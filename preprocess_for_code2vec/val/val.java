private static Bigquery.Builder beam_f7_0(BigQueryOptions options)
{    return new Bigquery.Builder(Transport.getTransport(), Transport.getJsonFactory(), chainHttpRequestInitializer(options.getGcpCredential(),     new RetryHttpRequestInitializer(ImmutableList.of(404)))).setApplicationName(options.getAppName()).setGoogleClientRequestInitializer(options.getGoogleApiTrace());}
private static Pubsub.Builder beam_f8_0(PubsubOptions options)
{    return new Pubsub.Builder(Transport.getTransport(), Transport.getJsonFactory(), chainHttpRequestInitializer(options.getGcpCredential(),     new RetryHttpRequestInitializer(ImmutableList.of(404)))).setRootUrl(options.getPubsubRootUrl()).setApplicationName(options.getAppName()).setGoogleClientRequestInitializer(options.getGoogleApiTrace());}
private static HttpRequestInitializer beam_f9_0(Credentials credential, HttpRequestInitializer httpRequestInitializer)
{    if (credential == null) {        return new ChainingHttpRequestInitializer(new NullCredentialInitializer(), httpRequestInitializer);    } else {        return new ChainingHttpRequestInitializer(new HttpCredentialsAdapter(credential), httpRequestInitializer);    }}
private void beam_f17_0()
{    System.out.println();    System.out.println("***********************************************************");    System.out.println("***********************************************************");    for (String message : pendingMessages) {        System.out.println(message);    }    System.out.println("***********************************************************");    System.out.println("***********************************************************");}
private static T beam_f18_0(AbstractGoogleClientRequest<T> request) throws IOException
{    try {        return request.execute();    } catch (GoogleJsonResponseException e) {        if (e.getStatusCode() == SC_NOT_FOUND) {            return null;        } else {            throw e;        }    }}
public PDone beam_f19_0(PCollection<String> input)
{    ResourceId resource = FileBasedSink.convertToFileResourceIfPossible(filenamePrefix);    TextIO.Write write = TextIO.write().to(new PerWindowFiles(resource)).withTempDirectory(resource.getCurrentDirectory()).withWindowedWrites();    if (numShards != null) {        write = write.withNumShards(numShards);    }    return input.apply(write);}
public Integer beam_f27_0(String input)
{    return (int) Math.pow(4, 5 - input.length());}
public int beam_f28_0(KV<String, List<CompletionCandidate>> elem, int numPartitions)
{    return elem.getKey().length() > minPrefix ? 0 : 1;}
public void beam_f29_0(ProcessContext c)
{    for (CompletionCandidate cc : c.element().getValue()) {        c.output(cc);    }}
public String beam_f37_0()
{    return "CompletionCandidate[" + value + ", " + count + "]";}
public void beam_f38_0(ProcessContext c)
{    Matcher m = Pattern.compile("#\\S+").matcher(c.element());    while (m.find()) {        c.output(m.group().substring(1));    }}
public void beam_f39_0(ProcessContext c)
{    List<TableRow> completions = new ArrayList<>();    for (CompletionCandidate cc : c.element().getValue()) {        completions.add(new TableRow().set("count", cc.getCount()).set("tag", cc.getValue()));    }    TableRow row = new TableRow().set("prefix", c.element().getKey()).set("tags", completions);    c.output(row);}
public void beam_f47_0(ProcessContext c, BoundedWindow window)
{    IntervalWindow w = (IntervalWindow) window;    int duration = new Duration(w.start(), w.end()).toPeriod().toStandardMinutes().getMinutes();    c.output(duration);}
protected static Map<String, WriteWindowedToBigQuery.FieldInfo<KV<String, Integer>>> beam_f48_0()
{    Map<String, WriteWindowedToBigQuery.FieldInfo<KV<String, Integer>>> tableConfigure = new HashMap<>();    tableConfigure.put("team", new WriteWindowedToBigQuery.FieldInfo<>("STRING", (c, w) -> c.element().getKey()));    tableConfigure.put("total_score", new WriteWindowedToBigQuery.FieldInfo<>("INTEGER", (c, w) -> c.element().getValue()));    tableConfigure.put("window_start", new WriteWindowedToBigQuery.FieldInfo<>("STRING", (c, w) -> {        IntervalWindow window = (IntervalWindow) w;        return GameConstants.DATE_TIME_FORMATTER.print(window.start());    }));    tableConfigure.put("processing_time", new WriteWindowedToBigQuery.FieldInfo<>("STRING", (c, w) -> GameConstants.DATE_TIME_FORMATTER.print(Instant.now())));    return tableConfigure;}
protected static Map<String, WriteWindowedToBigQuery.FieldInfo<Double>> beam_f49_0()
{    Map<String, WriteWindowedToBigQuery.FieldInfo<Double>> tableConfigure = new HashMap<>();    tableConfigure.put("window_start", new WriteWindowedToBigQuery.FieldInfo<>("STRING", (c, w) -> {        IntervalWindow window = (IntervalWindow) w;        return GameConstants.DATE_TIME_FORMATTER.print(window.start());    }));    tableConfigure.put("mean_duration", new WriteWindowedToBigQuery.FieldInfo<>("FLOAT", (c, w) -> c.element()));    return tableConfigure;}
 long beam_f57_0()
{    return startTimeInMillis + (expirationPeriod * 60L * 1000L);}
 String beam_f58_0()
{    int userNum = random.nextInt(numMembers);    return "user" + userNum + "_" + teamName;}
 int beam_f59_0()
{    return numMembers;}
public static void beam_f67_0(int numMessages, int delayInMillis) throws IOException
{    List<PubsubMessage> pubsubMessages = new ArrayList<>();    for (int i = 0; i < Math.max(1, numMessages); i++) {        Long currTime = System.currentTimeMillis();        String message = generateEvent(currTime, delayInMillis);        PubsubMessage pubsubMessage = new PubsubMessage().encodeData(message.getBytes("UTF-8"));        pubsubMessage.setAttributes(ImmutableMap.of(GameConstants.TIMESTAMP_ATTRIBUTE, Long.toString((currTime - delayInMillis) / 1000 * 1000)));        if (delayInMillis != 0) {            System.out.println(pubsubMessage.getAttributes());            System.out.println("late data for: " + message);        }        pubsubMessages.add(pubsubMessage);    }    PublishRequest publishRequest = new PublishRequest();    publishRequest.setMessages(pubsubMessages);    pubsub.projects().topics().publish(topic, publishRequest).execute();}
public static void beam_f68_0(String fileName, int numMessages, int delayInMillis) throws IOException
{    PrintWriter out = new PrintWriter(new OutputStreamWriter(new BufferedOutputStream(new FileOutputStream(fileName, true)), "UTF-8"));    try {        for (int i = 0; i < Math.max(1, numMessages); i++) {            Long currTime = System.currentTimeMillis();            String message = generateEvent(currTime, delayInMillis);            out.println(message);        }    } catch (Exception e) {        System.err.print("Error in writing generated events to file");        e.printStackTrace();    } finally {        out.flush();        out.close();    }}
public static void beam_f69_0(String[] args) throws IOException, InterruptedException
{    if (args.length < 3) {        System.out.println("Usage: Injector project-name (topic-name|none) (filename|none)");        System.exit(1);    }    boolean writeToFile = false;    boolean writeToPubsub = true;    project = args[0];    String topicName = args[1];    String fileName = args[2];        if ("none".equalsIgnoreCase(topicName)) {        writeToFile = true;        writeToPubsub = false;    }    if (writeToPubsub) {                pubsub = InjectorUtils.getClient();                topic = InjectorUtils.getFullyQualifiedTopicName(project, topicName);        InjectorUtils.createTopic(pubsub, topic);        System.out.println("Injecting to topic: " + topic);    } else {        if ("none".equalsIgnoreCase(fileName)) {            System.out.println("Filename not specified.");            System.exit(1);        }        System.out.println("Writing to file: " + fileName);    }    System.out.println("Starting Injector");        while (liveTeams.size() < NUM_LIVE_TEAMS) {        addLiveTeam();    }        for (int i = 0; true; i++) {        if (Thread.activeCount() > 10) {            System.err.println("I'm falling behind!");        }                final int numMessages;        final int delayInMillis;        if (i % LATE_DATA_RATE == 0) {                        delayInMillis = BASE_DELAY_IN_MILLIS + random.nextInt(FUZZY_DELAY_IN_MILLIS);            numMessages = 1;            System.out.println("DELAY(" + delayInMillis + ", " + numMessages + ")");        } else {            System.out.print(".");            delayInMillis = 0;            numMessages = MIN_QPS + random.nextInt(QPS_RANGE);        }        if (writeToFile) {                        publishDataToFile(fileName, numMessages, delayInMillis);        } else {                                    new Thread(() -> {                try {                    publishData(numMessages, delayInMillis);                } catch (IOException e) {                    System.err.println(e);                }            }).start();        }                Thread.sleep(THREAD_SLEEP_MS);    }}
protected static Map<String, WriteToBigQuery.FieldInfo<KV<String, Integer>>> beam_f77_0()
{    Map<String, WriteToBigQuery.FieldInfo<KV<String, Integer>>> tableConfigure = configureBigQueryWrite();    tableConfigure.put("processing_time", new WriteToBigQuery.FieldInfo<>("STRING", (c, w) -> GameConstants.DATE_TIME_FORMATTER.print(Instant.now())));    return tableConfigure;}
public static void beam_f78_0(String[] args) throws Exception
{    Options options = PipelineOptionsFactory.fromArgs(args).withValidation().as(Options.class);        options.setStreaming(true);    ExampleUtils exampleUtils = new ExampleUtils(options);    Pipeline pipeline = Pipeline.create(options);            PCollection<GameActionInfo> gameEvents = pipeline.apply(PubsubIO.readStrings().withTimestampAttribute(GameConstants.TIMESTAMP_ATTRIBUTE).fromTopic(options.getTopic())).apply("ParseGameEvent", ParDo.of(new ParseEventFn()));    gameEvents.apply("CalculateTeamScores", new CalculateTeamScores(Duration.standardMinutes(options.getTeamWindowDuration()), Duration.standardMinutes(options.getAllowedLateness()))).apply("WriteTeamScoreSums", new WriteWindowedToBigQuery<>(options.as(GcpOptions.class).getProject(), options.getDataset(), options.getLeaderBoardTableName() + "_team", configureWindowedTableWrite()));    gameEvents.apply("CalculateUserScores", new CalculateUserScores(Duration.standardMinutes(options.getAllowedLateness()))).apply("WriteUserScoreSums", new WriteToBigQuery<>(options.as(GcpOptions.class).getProject(), options.getDataset(), options.getLeaderBoardTableName() + "_user", configureGlobalWindowBigQueryWrite()));            PipelineResult result = pipeline.run();    exampleUtils.waitToFinish(result);}
public PCollection<KV<String, Integer>> beam_f79_0(PCollection<GameActionInfo> infos)
{    return infos.apply("LeaderboardTeamFixedWindows", Window.<GameActionInfo>into(FixedWindows.of(teamWindowDuration)).triggering(AfterWatermark.pastEndOfWindow().withEarlyFirings(AfterProcessingTime.pastFirstElementInPane().plusDelayOf(FIVE_MINUTES)).withLateFirings(AfterProcessingTime.pastFirstElementInPane().plusDelayOf(TEN_MINUTES))).withAllowedLateness(allowedLateness).accumulatingFiredPanes()).apply("ExtractTeamScore", new ExtractAndSumScore("team"));}
public Long beam_f87_0()
{    return this.timestamp;}
public String beam_f88_0(String keyname)
{    if ("team".equals(keyname)) {        return this.team;    } else {                return this.user;    }}
public boolean beam_f89_0(Object o)
{    if (this == o) {        return true;    }    if (o == null || o.getClass() != this.getClass()) {        return false;    }    GameActionInfo gameActionInfo = (GameActionInfo) o;    if (!this.getUser().equals(gameActionInfo.getUser())) {        return false;    }    if (!this.getTeam().equals(gameActionInfo.getTeam())) {        return false;    }    if (!this.getScore().equals(gameActionInfo.getScore())) {        return false;    }    return this.getTimestamp().equals(gameActionInfo.getTimestamp());}
public void beam_f97_0(ProcessContext c, BoundedWindow window)
{    TableRow row = new TableRow();    for (Map.Entry<String, FieldInfo<InputT>> entry : fieldInfo.entrySet()) {        String key = entry.getKey();        FieldInfo<InputT> fcnInfo = entry.getValue();        FieldFn<InputT> fcn = fcnInfo.getFieldFn();        row.set(key, fcn.apply(c, window));    }    c.output(row);}
protected TableSchema beam_f98_0()
{    List<TableFieldSchema> fields = new ArrayList<>();    for (Map.Entry<String, FieldInfo<InputT>> entry : fieldInfo.entrySet()) {        String key = entry.getKey();        FieldInfo<InputT> fcnInfo = entry.getValue();        String bqType = fcnInfo.getFieldType();        fields.add(new TableFieldSchema().setName(key).setType(bqType));    }    return new TableSchema().setFields(fields);}
public PDone beam_f99_0(PCollection<InputT> teamAndScore)
{    teamAndScore.apply("ConvertToRow", ParDo.of(new BuildRowFn())).apply(BigQueryIO.writeTableRows().to(getTable(projectId, datasetId, tableName)).withSchema(getSchema()).withCreateDisposition(CreateDisposition.CREATE_IF_NEEDED).withWriteDisposition(WriteDisposition.WRITE_APPEND));    return PDone.in(teamAndScore.getPipeline());}
public void beam_f107_0(ProcessContext c, BoundedWindow window)
{    TableRow row = new TableRow();    for (Map.Entry<String, FieldInfo<T>> entry : fieldInfo.entrySet()) {        String key = entry.getKey();        FieldInfo<T> fcnInfo = entry.getValue();        row.set(key, fcnInfo.getFieldFn().apply(c, window));    }    c.output(row);}
public PDone beam_f108_0(PCollection<T> teamAndScore)
{    teamAndScore.apply("ConvertToRow", ParDo.of(new BuildRowFn())).apply(BigQueryIO.writeTableRows().to(getTable(projectId, datasetId, tableName)).withSchema(getSchema()).withCreateDisposition(CreateDisposition.CREATE_IF_NEEDED).withWriteDisposition(WriteDisposition.WRITE_APPEND));    return PDone.in(teamAndScore.getPipeline());}
public void beam_f109_0(ProcessContext c)
{    String[] words = c.element().split(ExampleUtils.TOKENIZER_PATTERN, -1);    for (String word : words) {        if (!word.isEmpty()) {            c.output(word);        }    }}
public void beam_f117_1(ProcessContext c)
{    URI uri = c.element().getKey();    String line = c.element().getValue();    for (String word : line.split("\\W+", -1)) {                if ("love".equalsIgnoreCase(word)) {                    }        if (!word.isEmpty()) {            c.output(KV.of(uri, word.toLowerCase()));        }    }}
public void beam_f118_0(ProcessContext c)
{    URI uri = c.element().getKey().getKey();    String word = c.element().getKey().getValue();    Long occurrences = c.element().getValue();    c.output(KV.of(uri, KV.of(word, occurrences)));}
public void beam_f119_0(ProcessContext c)
{    URI uri = c.element().getKey();    Long wordTotal = c.element().getValue().getOnly(wordTotalsTag);    for (KV<String, Long> wordAndCount : c.element().getValue().getAll(wordCountsTag)) {        String word = wordAndCount.getKey();        Long wordCount = wordAndCount.getValue();        Double termFrequency = wordCount.doubleValue() / wordTotal.doubleValue();        c.output(KV.of(word, KV.of(uri, termFrequency)));    }}
public PCollection<KV<String, Long>> beam_f127_0(PCollection<String> actions)
{    return actions.apply(Window.into(Sessions.withGapDuration(Duration.standardHours(1)))).apply(Count.perElement());}
public PCollection<List<KV<String, Long>>> beam_f128_0(PCollection<KV<String, Long>> sessions)
{    SerializableComparator<KV<String, Long>> comparator = (o1, o2) -> ComparisonChain.start().compare(o1.getValue(), o2.getValue()).compare(o1.getKey(), o2.getKey()).result();    return sessions.apply(Window.into(CalendarWindows.months(1))).apply(Top.of(1, comparator).withoutDefaults());}
public void beam_f129_0(ProcessContext c, BoundedWindow window)
{    c.output(KV.of(c.element().getKey() + " : " + window, c.element().getValue()));}
public String beam_f137_0()
{    return this.lane;}
public String beam_f138_0()
{    return this.direction;}
public String beam_f139_0()
{    return this.freeway;}
public LaneInfo beam_f147_0(Iterable<LaneInfo> input)
{    Integer max = 0;    LaneInfo maxInfo = new LaneInfo();    for (LaneInfo item : input) {        Integer flow = item.getLaneFlow();        if (flow != null && (flow >= max)) {            max = flow;            maxInfo = item;        }    }    return maxInfo;}
public void beam_f148_0(ProcessContext c)
{    LaneInfo laneInfo = c.element().getValue();    TableRow row = new TableRow().set("station_id", c.element().getKey()).set("direction", laneInfo.getDirection()).set("freeway", laneInfo.getFreeway()).set("lane_max_flow", laneInfo.getLaneFlow()).set("lane", laneInfo.getLane()).set("avg_occ", laneInfo.getLaneAO()).set("avg_speed", laneInfo.getLaneAS()).set("total_flow", laneInfo.getTotalFlow()).set("recorded_timestamp", laneInfo.getRecordedTimestamp()).set("window_timestamp", c.timestamp().toString());    c.output(row);}
 static TableSchema beam_f149_0()
{    List<TableFieldSchema> fields = new ArrayList<>();    fields.add(new TableFieldSchema().setName("station_id").setType("STRING"));    fields.add(new TableFieldSchema().setName("direction").setType("STRING"));    fields.add(new TableFieldSchema().setName("freeway").setType("STRING"));    fields.add(new TableFieldSchema().setName("lane_max_flow").setType("INTEGER"));    fields.add(new TableFieldSchema().setName("lane").setType("STRING"));    fields.add(new TableFieldSchema().setName("avg_occ").setType("FLOAT"));    fields.add(new TableFieldSchema().setName("avg_speed").setType("FLOAT"));    fields.add(new TableFieldSchema().setName("total_flow").setType("INTEGER"));    fields.add(new TableFieldSchema().setName("window_timestamp").setType("TIMESTAMP"));    fields.add(new TableFieldSchema().setName("recorded_timestamp").setType("STRING"));    TableSchema schema = new TableSchema().setFields(fields);    return schema;}
public Double beam_f157_0()
{    return this.avgSpeed;}
public int beam_f158_0(StationSpeed other)
{    return Long.compare(this.timestamp, other.timestamp);}
public boolean beam_f159_0(Object object)
{    if (object == null) {        return false;    }    if (object.getClass() != getClass()) {        return false;    }    StationSpeed otherStationSpeed = (StationSpeed) object;    return Objects.equals(this.timestamp, otherStationSpeed.timestamp);}
public void beam_f167_0(ProcessContext c)
{    RouteInfo routeInfo = c.element().getValue();    TableRow row = new TableRow().set("avg_speed", routeInfo.getAvgSpeed()).set("slowdown_event", routeInfo.getSlowdownEvent()).set("route", c.element().getKey()).set("window_timestamp", c.timestamp().toString());    c.output(row);}
 static TableSchema beam_f168_0()
{    List<TableFieldSchema> fields = new ArrayList<>();    fields.add(new TableFieldSchema().setName("route").setType("STRING"));    fields.add(new TableFieldSchema().setName("avg_speed").setType("FLOAT"));    fields.add(new TableFieldSchema().setName("slowdown_event").setType("BOOLEAN"));    fields.add(new TableFieldSchema().setName("window_timestamp").setType("TIMESTAMP"));    return new TableSchema().setFields(fields);}
public PCollection<TableRow> beam_f169_0(PCollection<KV<String, StationSpeed>> stationSpeed)
{            PCollection<KV<String, Iterable<StationSpeed>>> timeGroup = stationSpeed.apply(GroupByKey.create());        PCollection<KV<String, RouteInfo>> stats = timeGroup.apply(ParDo.of(new GatherStats()));        PCollection<TableRow> results = stats.apply(ParDo.of(new FormatStatsFn()));    return results;}
private static String beam_f177_0(String[] inputItems, int index)
{    return inputItems.length > index ? inputItems[index] : null;}
private static Map<String, String> beam_f178_0()
{    Map<String, String> stations = new LinkedHashMap<>();        stations.put("1108413", "SDRoute1");        stations.put("1108699", "SDRoute2");    stations.put("1108702", "SDRoute2");    return stations;}
public void beam_f179_0(ProcessContext c)
{    TableRow row = c.element();    if ((Boolean) row.get("tornado")) {        c.output(Integer.parseInt((String) row.get("month")));    }}
public String beam_f187_0(Iterable<String> input)
{    StringBuilder all = new StringBuilder();    for (String item : input) {        if (!item.isEmpty()) {            if (all.length() == 0) {                all.append(item);            } else {                all.append(",");                all.append(item);            }        }    }    return all.toString();}
public static void beam_f188_0(String[] args) throws Exception
{    Options options = PipelineOptionsFactory.fromArgs(args).withValidation().as(Options.class);    Pipeline p = Pipeline.create(options);        List<TableFieldSchema> fields = new ArrayList<>();    fields.add(new TableFieldSchema().setName("word").setType("STRING"));    fields.add(new TableFieldSchema().setName("all_plays").setType("STRING"));    TableSchema schema = new TableSchema().setFields(fields);    p.apply(BigQueryIO.readTableRows().from(options.getInput())).apply(new PlaysForWord()).apply(BigQueryIO.writeTableRows().to(options.getOutput()).withSchema(schema).withCreateDisposition(BigQueryIO.Write.CreateDisposition.CREATE_IF_NEEDED).withWriteDisposition(BigQueryIO.Write.WriteDisposition.WRITE_TRUNCATE));    p.run().waitUntilFinish();}
public String beam_f189_0(PipelineOptions options)
{    if (options.getTempLocation() != null) {        return GcsPath.fromUri(options.getTempLocation()).resolve("deduped.txt").toString();    } else {        throw new IllegalArgumentException("Must specify --output or --tempLocation");    }}
public static void beam_f197_0(String[] args) throws Exception
{    Options options = PipelineOptionsFactory.fromArgs(args).withValidation().as(Options.class);    Pipeline p = Pipeline.create(options);    TableSchema schema = buildWeatherSchemaProjection();    p.apply(BigQueryIO.readTableRows().from(options.getInput())).apply(ParDo.of(new ProjectionFn())).apply(new BelowGlobalMean(options.getMonthFilter())).apply(BigQueryIO.writeTableRows().to(options.getOutput()).withSchema(schema).withCreateDisposition(BigQueryIO.Write.CreateDisposition.CREATE_IF_NEEDED).withWriteDisposition(BigQueryIO.Write.WriteDisposition.WRITE_TRUNCATE));    p.run().waitUntilFinish();}
 static PCollection<String> beam_f198_0(PCollection<TableRow> eventsTable, PCollection<TableRow> countryCodes) throws Exception
{    final TupleTag<String> eventInfoTag = new TupleTag<>();    final TupleTag<String> countryInfoTag = new TupleTag<>();            PCollection<KV<String, String>> eventInfo = eventsTable.apply(ParDo.of(new ExtractEventDataFn()));    PCollection<KV<String, String>> countryInfo = countryCodes.apply(ParDo.of(new ExtractCountryInfoFn()));        PCollection<KV<String, CoGbkResult>> kvpCollection = KeyedPCollectionTuple.of(eventInfoTag, eventInfo).and(countryInfoTag, countryInfo).apply(CoGroupByKey.create());            PCollection<KV<String, String>> finalResultCollection = kvpCollection.apply("Process", ParDo.of(new DoFn<KV<String, CoGbkResult>, KV<String, String>>() {        @ProcessElement        public void processElement(ProcessContext c) {            KV<String, CoGbkResult> e = c.element();            String countryCode = e.getKey();            String countryName = "none";            countryName = e.getValue().getOnly(countryInfoTag);            for (String eventInfo : c.element().getValue().getAll(eventInfoTag)) {                                c.output(KV.of(countryCode, "Country name: " + countryName + ", Event info: " + eventInfo));            }        }    }));        PCollection<String> formattedResults = finalResultCollection.apply("Format", ParDo.of(new DoFn<KV<String, String>, String>() {        @ProcessElement        public void processElement(ProcessContext c) {            String outputstring = "Country code: " + c.element().getKey() + ", " + c.element().getValue();            c.output(outputstring);        }    }));    return formattedResults;}
public void beam_f199_0(ProcessContext c)
{    KV<String, CoGbkResult> e = c.element();    String countryCode = e.getKey();    String countryName = "none";    countryName = e.getValue().getOnly(countryInfoTag);    for (String eventInfo : c.element().getValue().getAll(eventInfoTag)) {                c.output(KV.of(countryCode, "Country name: " + countryName + ", Event info: " + eventInfo));    }}
public static void beam_f207_0(String[] args) throws Exception
{    Options options = PipelineOptionsFactory.fromArgs(args).withValidation().as(Options.class);    Pipeline p = Pipeline.create(options);        List<TableFieldSchema> fields = new ArrayList<>();    fields.add(new TableFieldSchema().setName("month").setType("INTEGER"));    fields.add(new TableFieldSchema().setName("max_mean_temp").setType("FLOAT"));    TableSchema schema = new TableSchema().setFields(fields);    p.apply(BigQueryIO.readTableRows().from(options.getInput())).apply(new MaxMeanTemp()).apply(BigQueryIO.writeTableRows().to(options.getOutput()).withSchema(schema).withCreateDisposition(BigQueryIO.Write.CreateDisposition.CREATE_IF_NEEDED).withWriteDisposition(BigQueryIO.Write.WriteDisposition.WRITE_TRUNCATE));    p.run().waitUntilFinish();}
public PCollectionList<TableRow> beam_f208_0(PCollection<KV<String, Integer>> flowInfo)
{                                                                PCollection<TableRow> defaultTriggerResults = flowInfo.apply("Default", Window.<    KV<String, Integer>>into(FixedWindows.of(Duration.standardMinutes(windowDuration))).triggering(Repeatedly.forever(AfterWatermark.pastEndOfWindow())).withAllowedLateness(Duration.ZERO).discardingFiredPanes()).apply(new TotalFlow("default"));                                                                PCollection<TableRow> withAllowedLatenessResults = flowInfo.apply("WithLateData", Window.<KV<String, Integer>>into(FixedWindows.of(Duration.standardMinutes(windowDuration))).triggering(Repeatedly.forever(AfterWatermark.pastEndOfWindow())).discardingFiredPanes().withAllowedLateness(ONE_DAY)).apply(new TotalFlow("withAllowedLateness"));                                                            PCollection<TableRow> speculativeResults = flowInfo.apply("Speculative", Window.<KV<String, Integer>>into(FixedWindows.of(Duration.standardMinutes(windowDuration))).triggering(Repeatedly.forever(AfterProcessingTime.pastFirstElementInPane().plusDelayOf(ONE_MINUTE))).accumulatingFiredPanes().withAllowedLateness(ONE_DAY)).apply(new TotalFlow("speculative"));                                                                            PCollection<TableRow> sequentialResults = flowInfo.apply("Sequential", Window.<KV<String, Integer>>into(FixedWindows.of(Duration.standardMinutes(windowDuration))).triggering(AfterEach.inOrder(Repeatedly.forever(AfterProcessingTime.pastFirstElementInPane().plusDelayOf(ONE_MINUTE)).orFinally(AfterWatermark.pastEndOfWindow()), Repeatedly.forever(AfterProcessingTime.pastFirstElementInPane().plusDelayOf(FIVE_MINUTES)))).accumulatingFiredPanes().withAllowedLateness(ONE_DAY)).apply(new TotalFlow("sequential"));        PCollectionList<TableRow> resultsList = PCollectionList.of(defaultTriggerResults).and(withAllowedLatenessResults).and(speculativeResults).and(sequentialResults);    return resultsList;}
public PCollection<TableRow> beam_f209_0(PCollection<KV<String, Integer>> flowInfo)
{    PCollection<KV<String, Iterable<Integer>>> flowPerFreeway = flowInfo.apply(GroupByKey.create());    PCollection<KV<String, String>> results = flowPerFreeway.apply(ParDo.of(new DoFn<KV<String, Iterable<Integer>>, KV<String, String>>() {        @ProcessElement        public void processElement(ProcessContext c) throws Exception {            Iterable<Integer> flows = c.element().getValue();            Integer sum = 0;            Long numberOfRecords = 0L;            for (Integer value : flows) {                sum += value;                numberOfRecords++;            }            c.output(KV.of(c.element().getKey(), sum + "," + numberOfRecords));        }    }));    PCollection<TableRow> output = results.apply(ParDo.of(new FormatTotalFlow(triggerType)));    return output;}
private static Integer beam_f217_0(String number)
{    try {        return Integer.parseInt(number);    } catch (NumberFormatException e) {        return null;    }}
public void beam_f218_1(ProcessContext c)
{    if (filter.matcher(c.element().getKey()).matches()) {                                matchedWords.inc();        c.output(c.element());    } else {                                LOG.trace("Did not match: " + c.element().getKey());        unmatchedWords.inc();    }}
 static void beam_f219_0(WordCountOptions options)
{    Pipeline p = Pipeline.create(options);    PCollection<KV<String, Long>> filteredWords = p.apply("ReadLines", TextIO.read().from(options.getInputFile())).apply(new WordCount.CountWords()).apply(ParDo.of(new FilterTextFn(options.getFilterPattern())));    /*     * Concept #3: PAssert is a set of convenient PTransforms in the style of     * Hamcrest's collection matchers that can be used when writing Pipeline level tests     * to validate the contents of PCollections. PAssert is best used in unit tests     * with small data sets but is demonstrated here as a teaching tool.     *     * <p>Below we verify that the set of filtered words matches our expected counts. Note     * that PAssert does not provide any output and that successful completion of the     * Pipeline implies that the expectations were met. Learn more at     * https://beam.apache.org/documentation/pipelines/test-your-pipeline/ on how to test     * your Pipeline and see {@link DebuggingWordCountTest} for an example unit test.     */    List<KV<String, Long>> expectedResults = Arrays.asList(KV.of("Flourish", 3L), KV.of("stomach", 1L));    PAssert.that(filteredWords).containsInAnyOrder(expectedResults);    p.run().waitUntilFinish();}
public static String beam_f227_0(String name, Iterable<String> emails, Iterable<String> phones)
{    List<String> emailsList = new ArrayList<>();    for (String elem : emails) {        emailsList.add("'" + elem + "'");    }    Collections.sort(emailsList);    String emailsStr = "[" + String.join(", ", emailsList) + "]";    List<String> phonesList = new ArrayList<>();    for (String elem : phones) {        phonesList.add("'" + elem + "'");    }    Collections.sort(phonesList);    String phonesStr = "[" + String.join(", ", phonesList) + "]";    return name + "; " + emailsStr + "; " + phonesStr;}
public static PCollection<String> beam_f228_0(TupleTag<String> emailsTag, TupleTag<String> phonesTag, PCollection<KV<String, String>> emails, PCollection<KV<String, String>> phones)
{        PCollection<KV<String, CoGbkResult>> results = KeyedPCollectionTuple.of(emailsTag, emails).and(phonesTag, phones).apply(CoGroupByKey.create());    PCollection<String> contactLines = results.apply(ParDo.of(new DoFn<KV<String, CoGbkResult>, String>() {        @ProcessElement        public void processElement(ProcessContext c) {            KV<String, CoGbkResult> e = c.element();            String name = e.getKey();            Iterable<String> emailsIter = e.getValue().getAll(emailsTag);            Iterable<String> phonesIter = e.getValue().getAll(phonesTag);            String formattedResult = Snippets.formatCoGbkResults(name, emailsIter, phonesIter);            c.output(formattedResult);        }    }));        return contactLines;}
public void beam_f229_0(ProcessContext c)
{    KV<String, CoGbkResult> e = c.element();    String name = e.getKey();    Iterable<String> emailsIter = e.getValue().getAll(emailsTag);    Iterable<String> phonesIter = e.getValue().getAll(phonesTag);    String formattedResult = Snippets.formatCoGbkResults(name, emailsIter, phonesIter);    c.output(formattedResult);}
public void beam_f237_1(ProcessContext c)
{    MyOptions ops = c.getPipelineOptions().as(MyOptions.class);            }
public Collection<IntervalWindow> beam_f238_0(WindowFn.AssignContext c)
{        return Arrays.asList(new IntervalWindow(c.timestamp(), gapDuration));}
public Collection<IntervalWindow> beam_f239_0(AssignContext c)
{                Duration dataDrivenGap;    TableRow message = c.element();    try {        dataDrivenGap = Duration.standardSeconds(Long.parseLong(message.get("gap").toString()));    } catch (Exception e) {        dataDrivenGap = gapDuration;    }    return Arrays.asList(new IntervalWindow(c.timestamp(), dataDrivenGap));}
public String beam_f248_0()
{    return sourcePath;}
public void beam_f249_0(String sourcePath)
{    this.sourcePath = sourcePath;}
public String beam_f250_0()
{    return workerPath;}
public void beam_f258_1(ProcessContext c) throws Exception
{    try {                SubProcessCommandLineArgs commands = new SubProcessCommandLineArgs();        Command command = new Command(0, String.valueOf(c.element().getValue()));        commands.putCommand(command);                SubProcessKernel kernel = new SubProcessKernel(configuration, binaryName);                List<String> results = kernel.exec(commands);        for (String s : results) {            c.output(KV.of(c.element().getKey(), s));        }    } catch (Exception ex) {                throw ex;    }}
private static String beam_f259_0()
{    return "#!/bin/sh\n" + "filename=$1;\n" + "echo $2 >> $filename;";}
private static String beam_f260_0()
{    return "#!/bin/sh\n" + "filename=$1;\n" + "echo \"You again? Well ok, here is your word again.\" >> $2 >> $filename;";}
public String beam_f268_0()
{    return errFileLocation;}
public String beam_f269_0()
{    return outFileLocation;}
public Path beam_f270_0()
{    return errFile;}
private int beam_f278_0(SubProcessCommandLineArgs commands)
{    int size = 0;    for (SubProcessCommandLineArgs.Command c : commands.getParameters()) {        size += c.value.length();    }    return size;}
private Process beam_f279_1(ProcessBuilder builder, SubProcessCommandLineArgs commands, SubProcessIOFiles outPutFiles) throws Exception
{    try {        builder = prepareBuilder(builder, commands, outPutFiles);        Process process = builder.start();        boolean timeout = !process.waitFor(configuration.getWaitTime(), TimeUnit.SECONDS);        if (timeout) {            String log = String.format("Timeout waiting to run process with parameters %s . " + "Check to see if your timeout is long enough. Currently set at %s.", createLogEntryFromInputs(builder.command()), configuration.getWaitTime());            throw new Exception(log);        }        return process;    } catch (Exception ex) {                throw new Exception(ex);    }}
private List<String> beam_f280_1(Process process, ProcessBuilder builder, SubProcessIOFiles outPutFiles) throws Exception
{    List<String> results = new ArrayList<>();    try {                        if (process.exitValue() != 0) {            outPutFiles.copyOutPutFilesToBucket(configuration, FileUtils.toStringParams(builder));            String log = createLogEntryForProcessFailure(process, builder.command(), outPutFiles);            throw new Exception(log);        }                if (!Files.exists(outPutFiles.resultFile)) {            String log = createLogEntryForProcessFailure(process, builder.command(), outPutFiles);            outPutFiles.copyOutPutFilesToBucket(configuration, FileUtils.toStringParams(builder));            throw new Exception(log);        }                try (Stream<String> lines = Files.lines(outPutFiles.resultFile)) {            for (String line : (Iterable<String>) lines::iterator) {                results.add(line);            }        }        return results;    } catch (Exception ex) {        String log = String.format("Unexpected error runnng process. %s error message was %s", createLogEntryFromInputs(builder.command()), ex.getMessage());        throw new Exception(log);    }}
private static void beam_f288_1(String binaryName) throws IllegalStateException
{    if (!semaphores.containsKey(binaryName)) {        throw new IllegalStateException("Semaphore is NULL, check init logic in @Setup.");    }    try {        semaphores.get(binaryName).acquire();    } catch (InterruptedException ex) {            }}
private static void beam_f289_0(String binaryName) throws IllegalStateException
{    if (!semaphores.containsKey(binaryName)) {        throw new IllegalStateException("Semaphore is NULL, check init logic in @Setup.");    }    semaphores.get(binaryName).release();}
public void beam_f290_0()
{    CallingSubProcessUtils.releaseSemaphore(binaryName);}
public static String beam_f298_0(ProcessBuilder builder)
{    return String.join(",", builder.command());}
public static String beam_f299_1(SubProcessConfiguration configuration, Path fileToUpload) throws Exception
{    Path fileName;    if ((fileName = fileToUpload.getFileName()) == null) {        throw new IllegalArgumentException("FileName can not be null.");    }    ResourceId sourceFile = getFileResourceId(configuration.getWorkerPath(), fileName.toString());        ResourceId destinationFile = getFileResourceId(configuration.getSourcePath(), fileName.toString());        try {        return copyFile(sourceFile, destinationFile);    } catch (Exception ex) {                throw ex;    }}
public static String beam_f300_1(ExecutableFile execuableFile) throws Exception
{    ResourceId sourceFile = FileSystems.matchNewResource(execuableFile.getSourceGCSLocation(), false);    ResourceId destinationFile = FileSystems.matchNewResource(execuableFile.getDestinationLocation(), false);    try {                Path path = Paths.get(execuableFile.getDestinationLocation());        if (path.toFile().exists()) {                    }        copyFile(sourceFile, destinationFile);        path.toFile().setExecutable(true);        return path.toString();    } catch (Exception ex) {                throw ex;    }}
public static void beam_f308_0(String[] args) throws IOException
{    Options options = PipelineOptionsFactory.fromArgs(args).withValidation().as(Options.class);    runWindowedWordCount(options);}
public void beam_f309_0(@Element String element, OutputReceiver<String> receiver)
{    lineLenDist.update(element.length());    if (element.trim().isEmpty()) {        emptyLines.inc();    }        String[] words = element.split(ExampleUtils.TOKENIZER_PATTERN, -1);        for (String word : words) {        if (!word.isEmpty()) {            receiver.output(word);        }    }}
public String beam_f310_0(KV<String, Long> input)
{    return input.getKey() + ": " + input.getValue();}
public void beam_f318_0()
{    List<String> words = Arrays.asList("x", "x", "x", "xy", "xy", "xyz");    PCollection<String> input = p.apply(Create.of(words));    PCollection<KV<String, List<CompletionCandidate>>> output = input.apply(new ComputeTopCompletions(2, recursive));    PAssert.that(output).containsInAnyOrder(KV.of("x", parseList("x:3", "xy:2")), KV.of("xy", parseList("xy:2", "xyz:1")), KV.of("xyz", parseList("xyz:1")));    p.run().waitUntilFinish();}
public void beam_f319_0()
{    List<TimestampedValue<String>> words = Arrays.asList(TimestampedValue.of("xA", new Instant(1)), TimestampedValue.of("xA", new Instant(1)), TimestampedValue.of("xB", new Instant(1)), TimestampedValue.of("xB", new Instant(2)), TimestampedValue.of("xB", new Instant(2)));    PCollection<String> input = p.apply(Create.timestamped(words));    PCollection<KV<String, List<CompletionCandidate>>> output = input.apply(Window.into(SlidingWindows.of(new Duration(2)))).apply(new ComputeTopCompletions(2, recursive));    PAssert.that(output).containsInAnyOrder(    KV.of("x", parseList("xA:2", "xB:1")), KV.of("xA", parseList("xA:2")), KV.of("xB", parseList("xB:1")),     KV.of("x", parseList("xB:3", "xA:2")), KV.of("xA", parseList("xA:2")), KV.of("xB", parseList("xB:3")),     KV.of("x", parseList("xB:2")), KV.of("xB", parseList("xB:2")));    p.run().waitUntilFinish();}
private static List<CompletionCandidate> beam_f320_0(String... entries)
{    List<CompletionCandidate> all = new ArrayList<>();    for (String s : entries) {        String[] countValue = s.split(":", -1);        all.add(new CompletionCandidate(countValue[0], Integer.valueOf(countValue[1])));    }    return all;}
public void beam_f328_0()
{    TestStream<GameActionInfo> createEvents = TestStream.create(AvroCoder.of(GameActionInfo.class)).advanceWatermarkTo(baseTime).addElements(event(TestUser.BLUE_ONE, 3, Duration.standardSeconds(3)), event(TestUser.BLUE_ONE, 2, Duration.standardMinutes(1))).advanceProcessingTime(Duration.standardMinutes(10)).addElements(event(TestUser.RED_TWO, 5, Duration.standardMinutes(3))).advanceProcessingTime(Duration.standardMinutes(12)).addElements(event(TestUser.BLUE_TWO, 3, Duration.standardSeconds(22))).advanceProcessingTime(Duration.standardMinutes(10)).addElements(event(TestUser.RED_ONE, 4, Duration.standardMinutes(4)), event(TestUser.BLUE_TWO, 2, Duration.standardMinutes(2))).advanceWatermarkToInfinity();    PCollection<KV<String, Integer>> teamScores = p.apply(createEvents).apply(new CalculateTeamScores(TEAM_WINDOW_DURATION, ALLOWED_LATENESS));    String blueTeam = TestUser.BLUE_ONE.getTeam();    String redTeam = TestUser.RED_ONE.getTeam();    IntervalWindow window = new IntervalWindow(baseTime, TEAM_WINDOW_DURATION);        PAssert.that(teamScores).inWindow(window).containsInAnyOrder(KV.of(blueTeam, 10), /* The on-time blue pane */    KV.of(redTeam, 9), /* The on-time red pane */    KV.of(blueTeam, 5), /* The first blue speculative pane */    KV.of(blueTeam, 8), /* The second blue speculative pane */    KV.of(redTeam, 5));    PAssert.that(teamScores).inOnTimePane(window).containsInAnyOrder(KV.of(blueTeam, 10), KV.of(redTeam, 9));    p.run().waitUntilFinish();}
public void beam_f329_0()
{    BoundedWindow window = new IntervalWindow(baseTime, TEAM_WINDOW_DURATION);    TestStream<GameActionInfo> createEvents = TestStream.create(AvroCoder.of(GameActionInfo.class)).advanceWatermarkTo(baseTime).addElements(event(TestUser.BLUE_ONE, 3, Duration.standardSeconds(3)), event(TestUser.BLUE_TWO, 5, Duration.standardMinutes(8)), event(TestUser.RED_ONE, 4, Duration.standardMinutes(2)), event(TestUser.BLUE_ONE, 3, Duration.standardMinutes(5))).advanceWatermarkTo(baseTime.plus(TEAM_WINDOW_DURATION).minus(Duration.standardMinutes(1))).addElements(event(TestUser.RED_TWO, 2, Duration.ZERO), event(TestUser.RED_TWO, 5, Duration.standardMinutes(1)), event(TestUser.BLUE_TWO, 2, Duration.standardSeconds(90)), event(TestUser.RED_TWO, 3, Duration.standardMinutes(3))).advanceWatermarkTo(baseTime.plus(TEAM_WINDOW_DURATION).plus(Duration.standardMinutes(1))).advanceWatermarkToInfinity();    PCollection<KV<String, Integer>> teamScores = p.apply(createEvents).apply(new CalculateTeamScores(TEAM_WINDOW_DURATION, ALLOWED_LATENESS));    String blueTeam = TestUser.BLUE_ONE.getTeam();    String redTeam = TestUser.RED_ONE.getTeam();        PAssert.that(teamScores).inOnTimePane(window).containsInAnyOrder(KV.of(redTeam, 14), KV.of(blueTeam, 13));    p.run().waitUntilFinish();}
public void beam_f330_0()
{    Instant firstWindowCloses = baseTime.plus(ALLOWED_LATENESS).plus(TEAM_WINDOW_DURATION);    TestStream<GameActionInfo> createEvents = TestStream.create(AvroCoder.of(GameActionInfo.class)).advanceWatermarkTo(baseTime).addElements(event(TestUser.BLUE_ONE, 3, Duration.standardSeconds(3)), event(TestUser.BLUE_TWO, 5, Duration.standardMinutes(8))).advanceProcessingTime(Duration.standardMinutes(10)).advanceWatermarkTo(baseTime.plus(Duration.standardMinutes(3))).addElements(event(TestUser.RED_ONE, 3, Duration.standardMinutes(1)), event(TestUser.RED_ONE, 4, Duration.standardMinutes(2)), event(TestUser.BLUE_ONE, 3, Duration.standardMinutes(5))).advanceWatermarkTo(firstWindowCloses.minus(Duration.standardMinutes(1))).addElements(event(TestUser.RED_TWO, 2, Duration.ZERO), event(TestUser.RED_TWO, 5, Duration.standardMinutes(1)), event(TestUser.RED_TWO, 3, Duration.standardMinutes(3))).advanceProcessingTime(Duration.standardMinutes(12)).addElements(event(TestUser.RED_TWO, 9, Duration.standardMinutes(1)), event(TestUser.RED_TWO, 1, Duration.standardMinutes(3))).advanceWatermarkToInfinity();    PCollection<KV<String, Integer>> teamScores = p.apply(createEvents).apply(new CalculateTeamScores(TEAM_WINDOW_DURATION, ALLOWED_LATENESS));    BoundedWindow window = new IntervalWindow(baseTime, TEAM_WINDOW_DURATION);    String blueTeam = TestUser.BLUE_ONE.getTeam();    String redTeam = TestUser.RED_ONE.getTeam();    PAssert.that(teamScores).inWindow(window).satisfies(input -> {                        assertThat(input, hasItem(KV.of(blueTeam, 11)));        assertThat(input, hasItem(KV.of(redTeam, 27)));        return null;    });    PAssert.thatMap(teamScores).inOnTimePane(window).isEqualTo(ImmutableMap.<String, Integer>builder().put(redTeam, 7).put(blueTeam, 11).build());            PAssert.that(teamScores).inFinalPane(window).containsInAnyOrder(KV.of(redTeam, 27));    p.run().waitUntilFinish();}
public void beam_f338_0()
{    TestStream<KV<String, GameActionInfo>> createEvents = TestStream.create(KvCoder.of(StringUtf8Coder.of(), AvroCoder.of(GameActionInfo.class))).advanceWatermarkTo(baseTime).addElements(event(TestUser.RED_ONE, 50, Duration.standardSeconds(10)), event(TestUser.RED_TWO, 50, Duration.standardSeconds(20)), event(TestUser.BLUE_ONE, 70, Duration.standardSeconds(30)), event(TestUser.BLUE_TWO, 80, Duration.standardSeconds(40)), event(TestUser.BLUE_TWO, 50, Duration.standardSeconds(50))).advanceWatermarkToInfinity();    PCollection<KV<String, Integer>> teamScores = p.apply(createEvents).apply(ParDo.of(new UpdateTeamScoreFn(100)));    String redTeam = TestUser.RED_ONE.getTeam();    String blueTeam = TestUser.BLUE_ONE.getTeam();    PAssert.that(teamScores).inWindow(GlobalWindow.INSTANCE).containsInAnyOrder(KV.of(redTeam, 100), KV.of(blueTeam, 150), KV.of(blueTeam, 200));    p.run().waitUntilFinish();}
public void beam_f339_0()
{    TestStream<KV<String, GameActionInfo>> createEvents = TestStream.create(KvCoder.of(StringUtf8Coder.of(), AvroCoder.of(GameActionInfo.class))).advanceWatermarkTo(baseTime).addElements(event(TestUser.RED_ONE, 50, Duration.standardMinutes(1)), event(TestUser.RED_TWO, 50, Duration.standardMinutes(2)), event(TestUser.RED_ONE, 50, Duration.standardMinutes(3)), event(TestUser.RED_ONE, 60, Duration.standardMinutes(6)), event(TestUser.RED_TWO, 60, Duration.standardMinutes(7))).advanceWatermarkToInfinity();    Duration teamWindowDuration = Duration.standardMinutes(5);    PCollection<KV<String, Integer>> teamScores = p.apply(createEvents).apply(Window.<KV<String, GameActionInfo>>into(FixedWindows.of(teamWindowDuration))).apply(ParDo.of(new UpdateTeamScoreFn(100)));    String redTeam = TestUser.RED_ONE.getTeam();    String blueTeam = TestUser.BLUE_ONE.getTeam();    IntervalWindow window1 = new IntervalWindow(baseTime, teamWindowDuration);    IntervalWindow window2 = new IntervalWindow(window1.end(), teamWindowDuration);    PAssert.that(teamScores).inWindow(window1).containsInAnyOrder(KV.of(redTeam, 100));    PAssert.that(teamScores).inWindow(window2).containsInAnyOrder(KV.of(redTeam, 120));    p.run().waitUntilFinish();}
private TimestampedValue<KV<String, GameActionInfo>> beam_f340_0(TestUser user, int score, Duration baseTimeOffset)
{    return TimestampedValue.of(KV.of(user.getTeam(), new GameActionInfo(user.getUser(), user.getTeam(), score, baseTime.plus(baseTimeOffset).getMillis())), baseTime.plus(baseTimeOffset));}
public static void beam_f348_0()
{    PipelineOptionsFactory.register(TestPipelineOptions.class);}
public void beam_f349_0() throws Exception
{    TopWikipediaSessionsITOptions options = TestPipeline.testingPipelineOptions().as(TopWikipediaSessionsITOptions.class);    options.setWikiInput(DEFAULT_INPUT_10_FILES);    options.setOutput(FileSystems.matchNewResource(options.getTempRoot(), true).resolve(String.format("topwikisessions-it-%tF-%<tH-%<tM-%<tS-%<tL", new Date()), StandardResolveOptions.RESOLVE_DIRECTORY).resolve("output", StandardResolveOptions.RESOLVE_DIRECTORY).resolve("results", StandardResolveOptions.RESOLVE_FILE).toString());    options.setOnSuccessMatcher(new FileChecksumMatcher(DEFAULT_OUTPUT_CHECKSUM, options.getOutput() + "*-of-*"));    TopWikipediaSessions.run(options);}
public void beam_f350_0()
{    PCollection<String> output = p.apply(Create.of(Arrays.asList(new TableRow().set("timestamp", 0).set("contributor_username", "user1"), new TableRow().set("timestamp", 1).set("contributor_username", "user1"), new TableRow().set("timestamp", 2).set("contributor_username", "user1"), new TableRow().set("timestamp", 0).set("contributor_username", "user2"), new TableRow().set("timestamp", 1).set("contributor_username", "user2"), new TableRow().set("timestamp", 3601).set("contributor_username", "user2"), new TableRow().set("timestamp", 3602).set("contributor_username", "user2"), new TableRow().set("timestamp", 35 * 24 * 3600).set("contributor_username", "user3")))).apply(new TopWikipediaSessions.ComputeTopSessions(1.0));    PAssert.that(output).containsInAnyOrder(Arrays.asList("user1 : [1970-01-01T00:00:00.000Z..1970-01-01T01:00:02.000Z)" + " : 3 : 1970-01-01T00:00:00.000Z", "user3 : [1970-02-05T00:00:00.000Z..1970-02-05T01:00:00.000Z)" + " : 1 : 1970-02-01T00:00:00.000Z"));    p.run().waitUntilFinish();}
private void beam_f358_0(BigQueryTornadoesITOptions options) throws Exception
{    String query = String.format("SELECT month, tornado_count FROM [%s]", options.getOutput());    options.setOnSuccessMatcher(new BigqueryMatcher(options.getAppName(), options.getProject(), query, DEFAULT_OUTPUT_CHECKSUM));    BigQueryTornadoes.runBigQueryTornadoes(options);}
public void beam_f359_0() throws Exception
{    BigQueryTornadoesITOptions options = TestPipeline.testingPipelineOptions().as(BigQueryTornadoesITOptions.class);    options.setReadMethod(Method.EXPORT);    options.setOutput(String.format("%s.%s", "BigQueryTornadoesIT", "monthly_tornadoes_" + System.currentTimeMillis()));    runE2EBigQueryTornadoesTest(options);}
public void beam_f360_0() throws Exception
{    BigQueryTornadoesITOptions options = TestPipeline.testingPipelineOptions().as(BigQueryTornadoesITOptions.class);    options.setReadMethod(Method.DIRECT_READ);    options.setOutput(String.format("%s.%s", "BigQueryTornadoesIT", "monthly_tornadoes_storage_" + System.currentTimeMillis()));    runE2EBigQueryTornadoesTest(options);}
public void beam_f368_0()
{    List<String> strings = Arrays.asList();    PCollection<String> input = p.apply(Create.of(strings).withCoder(StringUtf8Coder.of()));    PCollection<String> output = input.apply(Distinct.create());    PAssert.that(output).empty();    p.run().waitUntilFinish();}
public void beam_f369_0()
{    PCollection<TableRow> input = p.apply(Create.of(row1, row2, row3));    PCollection<TableRow> results = input.apply(ParDo.of(new ProjectionFn()));    PAssert.that(results).containsInAnyOrder(outRow1, outRow2, outRow3);    p.run().waitUntilFinish();}
public void beam_f370_0()
{    PCollection<TableRow> input = p.apply(Create.of(outRow1, outRow2, outRow3));    PCollection<TableRow> results = input.apply(ParDo.of(new FilterSingleMonthDataFn(7)));    PAssert.that(results).containsInAnyOrder(outRow2);    p.run().waitUntilFinish();}
 static String beam_f378_0(TableRow row)
{    List<String> entries = Lists.newArrayListWithCapacity(row.size());    for (Map.Entry<String, Object> entry : row.entrySet()) {        entries.add(entry.getKey() + ":" + entry.getValue());    }    Collections.sort(entries);    return Joiner.on(",").join(entries);}
public void beam_f379_0(ProcessContext c) throws Exception
{    TableRow element = c.element();    TableRow row = new TableRow().set("trigger_type", element.get("trigger_type")).set("freeway", element.get("freeway")).set("total_flow", element.get("total_flow")).set("number_of_records", element.get("number_of_records")).set("isFirst", element.get("isFirst")).set("isLast", element.get("isLast")).set("timing", element.get("timing")).set("window", element.get("window"));    c.output(canonicalFormat(row));}
private String beam_f380_0(String filePath)
{    if (filePath.contains(":")) {        return filePath.replace("\\", "/").split(":", -1)[1];    }    return filePath;}
public void beam_f388_1(ProcessContext c) throws Exception
{    try {                SubProcessCommandLineArgs commands = new SubProcessCommandLineArgs();        Command command = new Command(0, String.valueOf(c.element().getValue()));        commands.putCommand(command);                SubProcessKernel kernel = new SubProcessKernel(configuration, binaryName);                List<String> results = kernel.exec(commands);        for (String s : results) {            c.output(KV.of(c.element().getKey(), s));        }    } catch (Exception ex) {                throw ex;    }}
private static String beam_f389_0()
{    return "#!/bin/sh\n" + "filename=$1;\n" + "echo $2 >> $filename;";}
private static String beam_f390_0()
{    return "#!/bin/sh\n" + "filename=$1;\n" + "echo $2 >> $filename;";}
private WindowedWordCountITOptions beam_f398_0() throws Exception
{    WindowedWordCountITOptions options = TestPipeline.testingPipelineOptions().as(WindowedWordCountITOptions.class);    options.setInputFile(DEFAULT_INPUT);    options.setTestTimeoutSeconds(1200L);    options.setMinTimestampMillis(0L);    options.setMinTimestampMillis(Duration.standardHours(1).getMillis());    options.setWindowSize(10);    options.setOutput(FileSystems.matchNewResource(options.getTempRoot(), true).resolve(String.format("WindowedWordCountIT.%s-%tFT%<tH:%<tM:%<tS.%<tL+%s", testName.getMethodName(), new Date(), ThreadLocalRandom.current().nextInt()), StandardResolveOptions.RESOLVE_DIRECTORY).resolve("output", StandardResolveOptions.RESOLVE_DIRECTORY).resolve("results", StandardResolveOptions.RESOLVE_FILE).toString());    return options;}
private WindowedWordCountITOptions beam_f399_0() throws Exception
{    WindowedWordCountITOptions options = defaultOptions();    options.setStreaming(true);    return options;}
private WindowedWordCountITOptions beam_f400_0() throws Exception
{    WindowedWordCountITOptions options = defaultOptions();        options.setStreaming(false);    return options;}
public void beam_f408_0() throws Exception
{    PCollection<String> input = p.apply(Create.of(WORDS).withCoder(StringUtf8Coder.of()));    PCollection<String> output = input.apply(new CountWords()).apply(MapElements.via(new FormatAsTextFn()));    PAssert.that(output).containsInAnyOrder(COUNTS_ARRAY);    p.run().waitUntilFinish();}
private String beam_f409_0(String filePath)
{    if (filePath.contains(":")) {        return filePath.replace("\\", "/").split(":", -1)[1];    }    return filePath;}
public void beam_f410_0() throws Exception
{    File inputFile = tmpFolder.newFile();    File outputFile = tmpFolder.newFile();    Files.write("stomach secret Flourish message Flourish here Flourish", inputFile, StandardCharsets.UTF_8);    DebuggingWordCount.WordCountOptions options = TestPipeline.testingPipelineOptions().as(DebuggingWordCount.WordCountOptions.class);    options.setInputFile(getFilePath(inputFile.getAbsolutePath()));    options.setOutput(getFilePath(outputFile.getAbsolutePath()));    DebuggingWordCount.runDebuggingWordCount(options);}
public static void beam_f418_0(String[] args)
{    PipelineOptions options = PipelineOptionsFactory.fromArgs(args).create();    Pipeline pipeline = Pipeline.create(options);    PCollection<Integer> numbers = pipeline.apply(Create.of(1, 2, 3, 4, 5, 6, 7, 8, 9, 10));    PCollection<Integer> output = applyTransform(numbers);    output.apply(Log.ofElements());    pipeline.run();}
 static PCollection<Integer> beam_f419_0(PCollection<Integer> input)
{    return input.apply(Max.integersGlobally());}
public void beam_f420_0()
{    Create.Values<Integer> values = Create.of(1, 2, 3, 4, 5, 6, 7, 8, 9, 10);    PCollection<Integer> numbers = testPipeline.apply(values);    PCollection<Integer> results = Task.applyTransform(numbers);    PAssert.that(results).containsInAnyOrder(10);    testPipeline.run().waitUntilFinish();}
 static PCollection<Integer> beam_f428_0(PCollection<Integer> input)
{    return input.apply(Sum.integersGlobally());}
public void beam_f429_0()
{    Create.Values<Integer> values = Create.of(1, 2, 3, 4, 5, 6, 7, 8, 9, 10);    PCollection<Integer> numbers = testPipeline.apply(values);    PCollection<Integer> results = Task.applyTransform(numbers);    PAssert.that(results).containsInAnyOrder(55);    testPipeline.run().waitUntilFinish();}
public static void beam_f430_0(String[] args)
{    PipelineOptions options = PipelineOptionsFactory.fromArgs(args).create();    Pipeline pipeline = Pipeline.create(options);    PCollection<Integer> numbers = pipeline.apply(Create.of(1, 2, 3, 4, 5, 6, 7, 8, 9, 10));    PCollection<Integer> output = applyTransform(numbers);    output.apply(Log.ofElements());    pipeline.run();}
 static PCollection<KV<String, String>> beam_f438_0(PCollection<String> input)
{    return input.apply(WithKeys.<String, String>of(fruit -> fruit.substring(0, 1)).withKeyType(strings()));}
public void beam_f439_0()
{    PCollection<String> numbers = testPipeline.apply(Create.of("apple", "banana", "cherry", "durian", "guava", "melon"));    PCollection<KV<String, String>> results = Task.applyTransform(numbers);    PAssert.that(results).containsInAnyOrder(KV.of("a", "apple"), KV.of("b", "banana"), KV.of("c", "cherry"), KV.of("d", "durian"), KV.of("g", "guava"), KV.of("m", "melon"));    testPipeline.run().waitUntilFinish();}
public static void beam_f440_0(String[] args)
{    PipelineOptions options = PipelineOptionsFactory.fromArgs(args).create();    Pipeline pipeline = Pipeline.create(options);    PCollection<Integer> numbers = pipeline.apply(Create.of(1, 2, 3, 4, 5));    PCollection<Integer> mult5Results = applyMultiply5Transform(numbers);    PCollection<Integer> mult10Results = applyMultiply10Transform(numbers);    mult5Results.apply("Log multiply 5", Log.ofElements("Multiplied by 5: "));    mult10Results.apply("Log multiply 10", Log.ofElements("Multiplied by 10: "));    pipeline.run();}
public void beam_f448_0()
{    PCollection<String> fruits = testPipeline.apply("Fruits", Create.of("apple", "banana", "cherry"));    PCollection<String> countries = testPipeline.apply("Countries", Create.of("australia", "brazil", "canada"));    PCollection<String> results = Task.applyTransform(fruits, countries);    PAssert.that(results).containsInAnyOrder("WordsAlphabet{alphabet='a', fruit='apple', country='australia'}", "WordsAlphabet{alphabet='b', fruit='banana', country='brazil'}", "WordsAlphabet{alphabet='c', fruit='cherry', country='canada'}");    testPipeline.run().waitUntilFinish();}
public static void beam_f449_0(String[] args)
{    PipelineOptions options = PipelineOptionsFactory.fromArgs(args).create();    Pipeline pipeline = Pipeline.create(options);    PCollection<BigInteger> numbers = pipeline.apply(Create.of(BigInteger.valueOf(10), BigInteger.valueOf(20), BigInteger.valueOf(30), BigInteger.valueOf(40), BigInteger.valueOf(50)));    PCollection<BigInteger> output = applyTransform(numbers);    output.apply(Log.ofElements());    pipeline.run();}
 static PCollection<BigInteger> beam_f450_0(PCollection<BigInteger> input)
{    return input.apply(Combine.globally(new SumBigIntegerFn()));}
public Integer beam_f458_0(Integer left, Integer right)
{    return left + right;}
public void beam_f459_0()
{    Create.Values<KV<String, Integer>> values = Create.of(KV.of(Task.PLAYER_1, 15), KV.of(Task.PLAYER_2, 10), KV.of(Task.PLAYER_1, 100), KV.of(Task.PLAYER_3, 25), KV.of(Task.PLAYER_2, 75));    PCollection<KV<String, Integer>> numbers = testPipeline.apply(values);    PCollection<KV<String, Integer>> results = Task.applyTransform(numbers);    PAssert.that(results).containsInAnyOrder(KV.of(Task.PLAYER_1, 115), KV.of(Task.PLAYER_2, 85), KV.of(Task.PLAYER_3, 25));    testPipeline.run().waitUntilFinish();}
public static void beam_f460_0(String[] args)
{    PipelineOptions options = PipelineOptionsFactory.fromArgs(args).create();    Pipeline pipeline = Pipeline.create(options);    PCollection<Integer> numbers = pipeline.apply(Create.of(10, 20, 50, 70, 90));    PCollection<Double> output = applyTransform(numbers);    output.apply(Log.ofElements());    pipeline.run();}
public void beam_f468_0()
{    Create.Values<Integer> values = Create.of(10, 20, 50, 70, 90);    PCollection<Integer> numbers = testPipeline.apply(values);    PCollection<Double> results = Task.applyTransform(numbers);    PAssert.that(results).containsInAnyOrder(48.0);    testPipeline.run().waitUntilFinish();}
public static void beam_f469_0(String[] args)
{    PipelineOptions options = PipelineOptionsFactory.fromArgs(args).create();    Pipeline pipeline = Pipeline.create(options);    PCollection<Integer> numbers = pipeline.apply(Create.of(10, 30, 50, 70, 90));    PCollection<Integer> output = applyTransform(numbers);    output.apply(Log.ofElements());    pipeline.run();}
 static PCollection<Integer> beam_f470_0(PCollection<Integer> input)
{    return input.apply(Combine.globally(new SumIntegerFn()));}
public static void beam_f478_0(String[] args)
{    PipelineOptions options = PipelineOptionsFactory.fromArgs(args).create();    Pipeline pipeline = Pipeline.create(options);    PCollection<String> wordsStartingWithA = pipeline.apply("Words starting with A", Create.of("apple", "ant", "arrow"));    PCollection<String> wordsStartingWithB = pipeline.apply("Words starting with B", Create.of("ball", "book", "bow"));    PCollection<String> output = applyTransform(wordsStartingWithA, wordsStartingWithB);    output.apply(Log.ofElements());    pipeline.run();}
 static PCollection<String> beam_f479_0(PCollection<String> words1, PCollection<String> words2)
{    return PCollectionList.of(words1).and(words2).apply(Flatten.pCollections());}
public void beam_f480_0()
{    PCollection<String> wordsStartingWithA = testPipeline.apply("Words starting with A", Create.of("apple", "ant", "arrow"));    PCollection<String> wordsStartingWithB = testPipeline.apply("Words starting with B", Create.of("ball", "book", "bow"));    PCollection<String> results = Task.applyTransform(wordsStartingWithA, wordsStartingWithB);    PAssert.that(results).containsInAnyOrder("apple", "ant", "arrow", "ball", "book", "bow");    testPipeline.run().waitUntilFinish();}
 static PCollection<Integer> beam_f488_0(PCollection<Integer> input)
{    return input.apply(MapElements.into(TypeDescriptors.integers()).via(number -> number * 5));}
public void beam_f489_0()
{    Create.Values<Integer> values = Create.of(10, 20, 30, 40, 50);    PCollection<Integer> numbers = testPipeline.apply(values);    PCollection<Integer> results = Task.applyTransform(numbers);    PAssert.that(results).containsInAnyOrder(50, 100, 150, 200, 250);    testPipeline.run().waitUntilFinish();}
public static void beam_f490_0(String[] args)
{    PipelineOptions options = PipelineOptionsFactory.fromArgs(args).create();    Pipeline pipeline = Pipeline.create(options);    PCollection<Integer> numbers = pipeline.apply(Create.of(1, 2, 3, 4, 5));    PCollection<Integer> output = applyTransform(numbers);    output.apply(Log.ofElements());    pipeline.run();}
public static void beam_f498_0(String[] args)
{    PipelineOptions options = PipelineOptionsFactory.fromArgs(args).create();    Pipeline pipeline = Pipeline.create(options);    PCollection<Integer> numbers = pipeline.apply(Create.of(1, 2, 3, 4, 5, 100, 110, 150, 250));    PCollectionList<Integer> partition = applyTransform(numbers);    partition.get(0).apply(Log.ofElements("Number > 100: "));    partition.get(1).apply(Log.ofElements("Number <= 100: "));    pipeline.run();}
 static PCollectionList<Integer> beam_f499_0(PCollection<Integer> input)
{    return input.apply(Partition.of(2, (PartitionFn<Integer>) (number, numPartitions) -> {        if (number > 100) {            return 0;        } else {            return 1;        }    }));}
public void beam_f500_0()
{    PCollection<Integer> numbers = testPipeline.apply(Create.of(1, 2, 3, 4, 5, 100, 110, 150, 250));    PCollectionList<Integer> results = Task.applyTransform(numbers);    PAssert.that(results.get(0)).containsInAnyOrder(110, 150, 250);    PAssert.that(results.get(1)).containsInAnyOrder(1, 2, 3, 4, 5, 100);    testPipeline.run().waitUntilFinish();}
 static PCollectionView<Map<String, String>> beam_f508_0(PCollection<KV<String, String>> citiesToCountries)
{    return citiesToCountries.apply(View.asMap());}
 static PCollection<Person> beam_f509_0(PCollection<Person> persons, PCollectionView<Map<String, String>> citiesToCountriesView)
{    return persons.apply(ParDo.of(new DoFn<Person, Person>() {        @ProcessElement        public void processElement(@Element Person person, OutputReceiver<Person> out, ProcessContext context) {            Map<String, String> citiesToCountries = context.sideInput(citiesToCountriesView);            String city = person.getCity();            String country = citiesToCountries.get(city);            out.output(new Person(person.getName(), city, country));        }    }).withSideInputs(citiesToCountriesView));}
public void beam_f510_0(@Element Person person, OutputReceiver<Person> out, ProcessContext context)
{    Map<String, String> citiesToCountries = context.sideInput(citiesToCountriesView);    String city = person.getCity();    String country = citiesToCountries.get(city);    out.output(new Person(person.getName(), city, country));}
public void beam_f518_0(@Element KV<String, Long> element, OutputReceiver<String> out)
{    out.output(element.getKey() + ":" + element.getValue());}
public void beam_f519_0()
{    Create.Values<String> lines = Create.of("apple orange grape banana apple banana", "banana orange banana papaya");    PCollection<String> linesPColl = testPipeline.apply(lines);    PCollection<String> results = Task.applyTransform(linesPColl);    PAssert.that(results).containsInAnyOrder("apple:2", "banana:4", "grape:1", "orange:2", "papaya:1");    testPipeline.run().waitUntilFinish();}
public static void beam_f520_0(String[] args)
{    PipelineOptions options = PipelineOptionsFactory.fromArgs(args).create();    Pipeline pipeline = Pipeline.create(options);    PCollection<String> output = setupPipeline(pipeline);    output.apply(Log.ofElements());    pipeline.run();}
public PCollection<String> beam_f528_0(PBegin input)
{    return input.apply(GenerateSequence.from(1).withRate(1, Duration.standardSeconds(1))).apply(MapElements.into(strings()).via(num -> "event"));}
public static void beam_f529_0(String[] args)
{    PipelineOptions options = PipelineOptionsFactory.fromArgs(args).create();    Pipeline pipeline = Pipeline.create(options);    PCollection<String> events = pipeline.apply(GenerateEvent.everySecond());    PCollection<Long> output = applyTransform(events);    output.apply(Log.ofElements());    pipeline.run();}
 static PCollection<Long> beam_f530_0(PCollection<String> events)
{    return events.apply(Window.<String>into(FixedWindows.of(Duration.standardDays(1))).triggering(AfterWatermark.pastEndOfWindow().withEarlyFirings(AfterProcessingTime.pastFirstElementInPane())).withAllowedLateness(Duration.ZERO).discardingFiredPanes()).apply(Combine.globally(Count.<String>combineFn()).withoutDefaults());}
 static GenerateEvent beam_f538_0()
{    return new GenerateEvent();}
public PCollection<String> beam_f539_0(PBegin input)
{    return input.apply(GenerateSequence.from(1).withRate(1, Duration.standardSeconds(1))).apply(MapElements.into(strings()).via(num -> "event"));}
public static void beam_f540_0(String[] args)
{    PipelineOptions options = PipelineOptionsFactory.fromArgs(args).create();    Pipeline pipeline = Pipeline.create(options);    PCollection<String> events = pipeline.apply(GenerateEvent.everySecond());    PCollection<Long> output = applyTransform(events);    output.apply(Log.ofElements());    pipeline.run();}
public Void beam_f548_0(Iterable<KV<String, Iterable<String>>> input)
{    List<Matcher<? super KV<String, Iterable<String>>>> matchers = new ArrayList<>();    for (KV<String, Iterable<String>> expected : expectedKvs) {        String[] values = toArray(expected.getValue(), String.class);        matchers.add(isKv(equalTo(expected.getKey()), containsInAnyOrder(values)));    }    assertThat(input, containsInAnyOrder(matchers));    return null;}
public static KvMatcher<K, V> beam_f549_0(Matcher<K> keyMatcher, Matcher<V> valueMatcher)
{    return new KvMatcher<>(keyMatcher, valueMatcher);}
public boolean beam_f550_0(KV<? extends K, ? extends V> kv)
{    return keyMatcher.matches(kv.getKey()) && valueMatcher.matches(kv.getValue());}
public boolean beam_f558_0(Object o)
{    if (this == o) {        return true;    }    if (o == null || getClass() != o.getClass()) {        return false;    }    Event event1 = (Event) o;    return id.equals(event1.id) && event.equals(event1.event) && date.equals(event1.date);}
public int beam_f559_0()
{    return Objects.hash(id, event, date);}
public String beam_f560_0()
{    return "Event{" + "id='" + id + '\'' + ", event='" + event + '\'' + ", date=" + date + '}';}
public String beam_f568_0()
{    return event;}
public void beam_f569_0(String event)
{    this.event = event;}
public DateTime beam_f570_0()
{    return date;}
public void beam_f578_0(@Element Event event, ProcessContext context, OutputReceiver<KV<Event, Instant>> out)
{    out.output(KV.of(event, context.timestamp()));}
public static void beam_f579_0(String[] args)
{    PipelineOptions options = PipelineOptionsFactory.fromArgs(args).create();    Pipeline pipeline = Pipeline.create(options);    PCollection<String> events = pipeline.apply(Create.timestamped(TimestampedValue.of("event", Instant.parse("2019-06-01T00:00:00+00:00")), TimestampedValue.of("event", Instant.parse("2019-06-01T00:00:00+00:00")), TimestampedValue.of("event", Instant.parse("2019-06-01T00:00:00+00:00")), TimestampedValue.of("event", Instant.parse("2019-06-01T00:00:00+00:00")), TimestampedValue.of("event", Instant.parse("2019-06-05T00:00:00+00:00")), TimestampedValue.of("event", Instant.parse("2019-06-05T00:00:00+00:00")), TimestampedValue.of("event", Instant.parse("2019-06-08T00:00:00+00:00")), TimestampedValue.of("event", Instant.parse("2019-06-08T00:00:00+00:00")), TimestampedValue.of("event", Instant.parse("2019-06-08T00:00:00+00:00")), TimestampedValue.of("event", Instant.parse("2019-06-10T00:00:00+00:00"))));    PCollection<KV<String, Long>> output = applyTransform(events);    output.apply(Log.ofElements());    pipeline.run();}
 static PCollection<KV<String, Long>> beam_f580_0(PCollection<String> events)
{    return events.apply(Window.into(FixedWindows.of(Duration.standardDays(1)))).apply(Count.perElement());}
public ApexRunnerResult beam_f588_0(final Pipeline pipeline)
{    pipeline.replaceAll(getOverrides());    final ApexPipelineTranslator translator = new ApexPipelineTranslator(options);    final AtomicReference<DAG> apexDAG = new AtomicReference<>();    final AtomicReference<File> tempDir = new AtomicReference<>();    StreamingApplication apexApp = (dag, conf) -> {        apexDAG.set(dag);        dag.setAttribute(DAGContext.APPLICATION_NAME, options.getApplicationName());        if (options.isEmbeddedExecution()) {                                    tempDir.set(Files.createTempDir());            dag.setAttribute(DAGContext.APPLICATION_PATH, tempDir.get().toURI().toString());        }        translator.translate(pipeline, dag);    };    Properties configProperties = new Properties();    try {        if (options.getConfigFile() != null) {            URI configURL = new URI(options.getConfigFile());            if (CLASSPATH_SCHEME.equals(configURL.getScheme())) {                InputStream is = this.getClass().getResourceAsStream(configURL.getPath());                if (is != null) {                    configProperties.load(is);                    is.close();                }            } else {                if (!configURL.isAbsolute()) {                                        File f = new File(options.getConfigFile());                    configURL = f.toURI();                }                try (InputStream is = configURL.toURL().openStream()) {                    configProperties.load(is);                }            }        }    } catch (IOException | URISyntaxException ex) {        throw new RuntimeException("Error loading properties", ex);    }    if (options.isEmbeddedExecution()) {        EmbeddedAppLauncher<?> launcher = Launcher.getLauncher(LaunchMode.EMBEDDED);        Attribute.AttributeMap launchAttributes = new Attribute.AttributeMap.DefaultAttributeMap();        launchAttributes.put(EmbeddedAppLauncher.RUN_ASYNC, true);        if (options.isEmbeddedExecutionDebugMode()) {                        launchAttributes.put(EmbeddedAppLauncher.HEARTBEAT_MONITORING, false);        }        Configuration conf = new Configuration(false);        ApexYarnLauncher.addProperties(conf, configProperties);        try {            if (translateOnly) {                launcher.prepareDAG(apexApp, conf);                return new ApexRunnerResult(launcher.getDAG(), null);            }            ApexRunner.ASSERTION_ERROR.set(null);            AppHandle apexAppResult = launcher.launchApp(apexApp, conf, launchAttributes);            return new ApexRunnerResult(apexDAG.get(), apexAppResult) {                @Override                protected void cleanupOnCancelOrFinish() {                    if (tempDir.get() != null) {                        FileUtils.deleteQuietly(tempDir.get());                    }                }            };        } catch (Exception e) {            Throwables.throwIfUnchecked(e);            throw new RuntimeException(e);        }    } else {        try {            ApexYarnLauncher yarnLauncher = new ApexYarnLauncher();            AppHandle apexAppResult = yarnLauncher.launchApp(apexApp, configProperties);            return new ApexRunnerResult(apexDAG.get(), apexAppResult);        } catch (IOException e) {            throw new RuntimeException("Failed to launch the application on YARN.", e);        }    }}
protected void beam_f589_0()
{    if (tempDir.get() != null) {        FileUtils.deleteQuietly(tempDir.get());    }}
public static CreateApexPCollectionView<ElemT, ViewT> beam_f590_0(PCollectionView<ViewT> view)
{    return new CreateApexPCollectionView<>(view);}
protected String beam_f598_0()
{    return "StreamingViewAsIterable";}
public PTransformReplacement<PCollection<T>, PCollection<T>> beam_f599_0(AppliedPTransform<PCollection<T>, PCollection<T>, CreatePCollectionView<T, Iterable<T>>> transform)
{    return PTransformReplacement.of(PTransformReplacements.getSingletonMainInput(transform), new StreamingViewAsIterable<>(transform.getTransform().getView()));}
public List<T> beam_f600_0()
{    return new ArrayList<>();}
public State beam_f608_0()
{    return state;}
public State beam_f609_0() throws IOException
{    apexApp.shutdown(ShutdownMode.KILL);    cleanupOnCancelOrFinish();    state = State.CANCELLED;    return state;}
public State beam_f610_0(@Nullable Duration duration)
{    long timeout = (duration == null || duration.getMillis() < 1) ? Long.MAX_VALUE : System.currentTimeMillis() + duration.getMillis();    try {        while (!apexApp.isFinished() && System.currentTimeMillis() < timeout) {            if (ApexRunner.ASSERTION_ERROR.get() != null) {                throw ApexRunner.ASSERTION_ERROR.get();            }            Thread.sleep(500);        }        if (apexApp.isFinished()) {            cleanupOnCancelOrFinish();            return State.DONE;        }        return null;    } catch (InterruptedException e) {        throw new RuntimeException(e);    }}
public static List<File> beam_f619_0() throws IOException
{    try (InputStream dependencyTree = ApexRunner.class.getResourceAsStream("dependency-tree")) {        try (BufferedReader br = new BufferedReader(new InputStreamReader(dependencyTree, StandardCharsets.UTF_8))) {            String line;            List<String> excludes = new ArrayList<>();            int excludeLevel = Integer.MAX_VALUE;            while ((line = br.readLine()) != null) {                for (int i = 0; i < line.length(); i++) {                    char c = line.charAt(i);                    if (Character.isLetter(c)) {                        if (i > excludeLevel) {                            excludes.add(line.substring(i));                        } else {                            if (line.substring(i).startsWith("org.apache.hadoop")) {                                excludeLevel = i;                                excludes.add(line.substring(i));                            } else {                                excludeLevel = Integer.MAX_VALUE;                            }                        }                        break;                    }                }            }            Set<String> excludeJarFileNames = Sets.newHashSet();            for (String exclude : excludes) {                List<String> strings = Splitter.on(':').splitToList(exclude);                String[] mvnc = strings.toArray(new String[strings.size()]);                String fileName = mvnc[1] + "-";                if (mvnc.length == 6) {                                        fileName += mvnc[4] + "-" + mvnc[3];                } else {                    fileName += mvnc[3];                }                fileName += ".jar";                excludeJarFileNames.add(fileName);            }            ClassLoader classLoader = ApexYarnLauncher.class.getClassLoader();            URL[] urls = ((URLClassLoader) classLoader).getURLs();            List<File> dependencyJars = new ArrayList<>();            for (URL url : urls) {                File f = new File(url.getFile());                                if (f.exists() && !excludeJarFileNames.contains(f.getName())) {                    dependencyJars.add(f);                }            }            return dependencyJars;        }    }}
public static void beam_f620_0(File dir, File jarFile) throws IOException
{    final Map<String, ?> env = Collections.singletonMap("create", "true");    if (jarFile.exists() && !jarFile.delete()) {        throw new RuntimeException("Failed to remove " + jarFile);    }    URI uri = URI.create("jar:" + jarFile.toURI());    try (final FileSystem zipfs = FileSystems.newFileSystem(uri, env)) {        File manifestFile = new File(dir, JarFile.MANIFEST_NAME);        Files.createDirectory(zipfs.getPath("META-INF"));        try (final OutputStream out = Files.newOutputStream(zipfs.getPath(JarFile.MANIFEST_NAME))) {            if (!manifestFile.exists()) {                new Manifest().write(out);            } else {                Files.copy(manifestFile.toPath(), out);            }        }        final Path root = dir.toPath();        Files.walkFileTree(root, new java.nio.file.SimpleFileVisitor<Path>() {            String relativePath;            @Override            public FileVisitResult preVisitDirectory(Path dir, BasicFileAttributes attrs) throws IOException {                relativePath = root.relativize(dir).toString();                if (!relativePath.isEmpty()) {                    if (!relativePath.endsWith("/")) {                        relativePath += "/";                    }                    if (!"META-INF/".equals(relativePath)) {                        final Path dstDir = zipfs.getPath(relativePath);                        Files.createDirectory(dstDir);                    }                }                return super.preVisitDirectory(dir, attrs);            }            @Override            public FileVisitResult visitFile(Path file, BasicFileAttributes attrs) throws IOException {                String name = relativePath + file.getFileName();                if (!JarFile.MANIFEST_NAME.equals(name)) {                    try (final OutputStream out = Files.newOutputStream(zipfs.getPath(name))) {                        Files.copy(file, out);                    }                }                return super.visitFile(file, attrs);            }            @Override            public FileVisitResult postVisitDirectory(Path dir, IOException exc) throws IOException {                relativePath = root.relativize(dir.getParent()).toString();                if (!relativePath.isEmpty() && !relativePath.endsWith("/")) {                    relativePath += "/";                }                return super.postVisitDirectory(dir, exc);            }        });    }}
public FileVisitResult beam_f621_0(Path dir, BasicFileAttributes attrs) throws IOException
{    relativePath = root.relativize(dir).toString();    if (!relativePath.isEmpty()) {        if (!relativePath.endsWith("/")) {            relativePath += "/";        }        if (!"META-INF/".equals(relativePath)) {            final Path dstDir = zipfs.getPath(relativePath);            Files.createDirectory(dstDir);        }    }    return super.preVisitDirectory(dir, attrs);}
private static void beam_f629_0(DAG from, DAG to)
{    checkArgument(from.getClass() == to.getClass(), "must be same class %s %s", from.getClass(), to.getClass());    Field[] fields = from.getClass().getDeclaredFields();    AccessibleObject.setAccessible(fields, true);    for (Field field : fields) {        if (!Modifier.isStatic(field.getModifiers())) {            try {                field.set(to, field.get(from));            } catch (IllegalArgumentException | IllegalAccessException e) {                throw new RuntimeException(e);            }        }    }}
public boolean beam_f630_0()
{    return finished;}
public void beam_f631_0()
{    try {        rc = p.waitFor();    } catch (Exception e) {        }    finished = true;}
public void beam_f639_1(PValue value, TransformHierarchy.Node producer)
{    }
private static void beam_f640_0(Class<TransformT> transformClass, TransformTranslator<? extends TransformT> transformTranslator)
{    if (transformTranslators.put(transformClass, transformTranslator) != null) {        throw new IllegalArgumentException("defining multiple translators for " + transformClass);    }}
private TransformTranslator<TransformT> beam_f641_0(Class<TransformT> transformClass)
{    return transformTranslators.get(transformClass);}
 static void beam_f649_0(List<PCollection<T>> collections, Map<PCollection<?>, Integer> unionTags, PCollection<T> finalCollection, TranslationContext context)
{    List<PCollection<T>> remainingCollections = Lists.newArrayList();    PCollection<T> firstCollection = null;    while (!collections.isEmpty()) {        for (PCollection<T> collection : collections) {            if (null == firstCollection) {                firstCollection = collection;            } else {                ApexFlattenOperator<T> operator = new ApexFlattenOperator<>();                context.addStream(firstCollection, operator.data1);                Integer unionTag = unionTags.get(firstCollection);                operator.data1Tag = (unionTag != null) ? unionTag : 0;                context.addStream(collection, operator.data2);                unionTag = unionTags.get(collection);                operator.data2Tag = (unionTag != null) ? unionTag : 0;                if (!collection.getCoder().equals(firstCollection.getCoder())) {                    throw new UnsupportedOperationException("coders don't match");                }                if (collections.size() > 2) {                    PCollection<T> intermediateCollection = PCollection.createPrimitiveOutputInternal(collection.getPipeline(), collection.getWindowingStrategy(), collection.isBounded(), collection.getCoder());                    context.addOperator(operator, operator.out, intermediateCollection);                    remainingCollections.add(intermediateCollection);                } else {                                        context.addOperator(operator, operator.out, finalCollection);                }                firstCollection = null;            }        }        if (firstCollection != null) {                        remainingCollections.add(firstCollection);            firstCollection = null;        }        if (remainingCollections.size() > 1) {            collections = remainingCollections;            remainingCollections = Lists.newArrayList();        } else {            collections = Lists.newArrayList();        }    }}
public void beam_f650_0(GroupByKey<K, V> transform, TranslationContext context)
{    PCollection<KV<K, V>> input = context.getInput();    ApexGroupByKeyOperator<K, V> group = new ApexGroupByKeyOperator<>(context.getPipelineOptions(), input, context.getStateBackend());    context.addOperator(group, group.output);    context.addStream(input, group.input);}
public void beam_f651_1(ApexStreamTuple<WindowedValue<InputT>> tuple)
{    if (tuple instanceof WatermarkTuple) {        WatermarkTuple<?> wmTuple = (WatermarkTuple<?>) tuple;        if (wmTuple.getTimestamp() > inputWM1) {            inputWM1 = wmTuple.getTimestamp();            if (inputWM1 <= inputWM2) {                                outputWM = inputWM1;                if (traceTuples) {                                    }                out.emit(tuple);            }        }        return;    }    if (traceTuples) {            }    if (data1Tag > 0 && tuple instanceof ApexStreamTuple.DataTuple) {        ((ApexStreamTuple.DataTuple<?>) tuple).setUnionTag(data1Tag);    }    out.emit(tuple);}
private void beam_f661_0(WindowedValue<KV<K, V>> windowedValue) throws Exception
{    final KV<K, V> kv = windowedValue.getValue();    final WindowedValue<V> updatedWindowedValue = WindowedValue.of(kv.getValue(), windowedValue.getTimestamp(), windowedValue.getWindows(), windowedValue.getPane());    timerInternals.setContext(kv.getKey(), this.keyCoder, this.inputWatermark, null);    ReduceFnRunner<K, V, Iterable<V>, BoundedWindow> reduceFnRunner = newReduceFnRunner(kv.getKey());    reduceFnRunner.processElements(Collections.singletonList(updatedWindowedValue));    reduceFnRunner.persist();}
public void beam_f662_0(K key, Collection<TimerData> timerData)
{    timerInternals.setContext(key, keyCoder, inputWatermark, null);    ReduceFnRunner<K, V, Iterable<V>, BoundedWindow> reduceFnRunner = newReduceFnRunner(key);    try {        reduceFnRunner.onTimers(timerData);    } catch (Exception e) {        Throwables.throwIfUnchecked(e);        throw new RuntimeException(e);    }    reduceFnRunner.persist();}
private void beam_f663_0(ApexStreamTuple.WatermarkTuple<?> mark)
{    this.inputWatermark = new Instant(mark.getTimestamp());    timerInternals.fireReadyTimers(this.inputWatermark.getMillis(), this, TimeDomain.EVENT_TIME);}
public void beam_f671_0(OperatorContext context)
{    this.traceTuples = ApexStreamTuple.Logging.isDebugEnabled(pipelineOptions.get().as(ApexPipelineOptions.class), this);    SideInputReader sideInputReader = NullSideInputReader.of(sideInputs);    if (!Iterables.isEmpty(sideInputs)) {        sideInputHandler = new SideInputHandler(Lists.newArrayList(sideInputs), sideInputStateInternals);        sideInputReader = sideInputHandler;    }    for (int i = 0; i < additionalOutputTags.size(); i++) {        @SuppressWarnings("unchecked")        DefaultOutputPort<ApexStreamTuple<?>> port = (DefaultOutputPort<ApexStreamTuple<?>>) additionalOutputPorts[i];        additionalOutputPortMapping.put(additionalOutputTags.get(i), port);    }    NoOpStepContext stepContext = new NoOpStepContext() {        @Override        public StateInternals stateInternals() {            return currentKeyStateInternals;        }        @Override        public TimerInternals timerInternals() {            return currentKeyTimerInternals;        }    };    DoFnRunner<InputT, OutputT> doFnRunner = DoFnRunners.simpleRunner(pipelineOptions.get(), doFn, sideInputReader, this, mainOutputTag, additionalOutputTags, stepContext, inputCoder, outputCoders, windowingStrategy, doFnSchemaInformation, sideInputMapping);    doFnInvoker = DoFnInvokers.invokerFor(doFn);    doFnInvoker.invokeSetup();    if (this.currentKeyStateInternals != null) {        StatefulDoFnRunner.CleanupTimer cleanupTimer = new StatefulDoFnRunner.TimeInternalsCleanupTimer(stepContext.timerInternals(), windowingStrategy);        @SuppressWarnings({ "rawtypes" })        Coder windowCoder = windowingStrategy.getWindowFn().windowCoder();        @SuppressWarnings({ "unchecked" })        StatefulDoFnRunner.StateCleaner<?> stateCleaner = new StatefulDoFnRunner.StateInternalsStateCleaner<>(doFn, stepContext.stateInternals(), windowCoder);        doFnRunner = DoFnRunners.defaultStatefulDoFnRunner(doFn, doFnRunner, windowingStrategy, cleanupTimer, stateCleaner);    }    pushbackDoFnRunner = SimplePushbackSideInputDoFnRunner.create(doFnRunner, Lists.newArrayList(sideInputs), sideInputHandler);    if (doFn instanceof ProcessFn) {        @SuppressWarnings("unchecked")        StateInternalsFactory<byte[]> stateInternalsFactory = (StateInternalsFactory<byte[]>) this.currentKeyStateInternals.getFactory();        @SuppressWarnings({ "rawtypes", "unchecked" })        ProcessFn<InputT, OutputT, Object, RestrictionTracker<Object, Object>> splittableDoFn = (ProcessFn) doFn;        splittableDoFn.setStateInternalsFactory(stateInternalsFactory);        TimerInternalsFactory<byte[]> timerInternalsFactory = key -> currentKeyTimerInternals;        splittableDoFn.setTimerInternalsFactory(timerInternalsFactory);        splittableDoFn.setProcessElementInvoker(new OutputAndTimeBoundedSplittableProcessElementInvoker<>(doFn, pipelineOptions.get(), new OutputWindowedValue<OutputT>() {            @Override            public void outputWindowedValue(OutputT output, Instant timestamp, Collection<? extends BoundedWindow> windows, PaneInfo pane) {                output(mainOutputTag, WindowedValue.of(output, timestamp, windows, pane));            }            @Override            public <AdditionalOutputT> void outputWindowedValue(TupleTag<AdditionalOutputT> tag, AdditionalOutputT output, Instant timestamp, Collection<? extends BoundedWindow> windows, PaneInfo pane) {                output(tag, WindowedValue.of(output, timestamp, windows, pane));            }        }, sideInputReader, Executors.newSingleThreadScheduledExecutor(Executors.defaultThreadFactory()), 10000, Duration.standardSeconds(10)));    }}
public StateInternals beam_f672_0()
{    return currentKeyStateInternals;}
public TimerInternals beam_f673_0()
{    return currentKeyTimerInternals;}
public void beam_f682_1(ApexStreamTuple<? extends WindowedValue<?>> tuple)
{    if (traceTuples) {            }    outputPort.emit(tuple);}
public static ApexProcessFnOperator<KV<K, V>> beam_f683_0(ApexPipelineOptions options)
{    ApexOperatorFn<KV<K, V>> fn = new ToKeyedWorkItems<>();    return new ApexProcessFnOperator<>(fn, options.isTupleTracingEnabled());}
public final void beam_f684_0(ApexStreamTuple<WindowedValue<KV<K, V>>> tuple, OutputEmitter<ApexStreamTuple<? extends WindowedValue<?>>> outputEmitter)
{    if (tuple instanceof ApexStreamTuple.WatermarkTuple) {        outputEmitter.emit(tuple);    } else {        for (WindowedValue<KV<K, V>> in : tuple.getValue().explodeWindows()) {            KeyedWorkItem<K, V> kwi = KeyedWorkItems.elementsWorkItem(in.getValue().getKey(), Collections.singletonList(in.withValue(in.getValue().getValue())));            outputEmitter.emit(ApexStreamTuple.DataTuple.of(in.withValue(kwi)));        }    }}
private void beam_f692_1(long mark)
{    if (mark > outputWatermark) {        outputWatermark = mark;        if (traceTuples) {                    }        output.emit(ApexStreamTuple.WatermarkTuple.of(mark));    }}
public void beam_f693_1()
{    if (outputWatermark >= BoundedWindow.TIMESTAMP_MAX_VALUE.getMillis()) {                if (traceTuples) {                    }        try {                        Thread.sleep(100);        } catch (InterruptedException e) {        }        BaseOperator.shutdown();    }}
public void beam_f694_0(OperatorContext context)
{    this.traceTuples = ApexStreamTuple.Logging.isDebugEnabled(pipelineOptions.get().as(ApexPipelineOptions.class), this);    try {        reader = source.createReader(this.pipelineOptions.get(), null);        available = reader.start();    } catch (IOException e) {        throw new RuntimeException(e);    }}
public void beam_f702_0(StateNamespace namespace, String timerId)
{    this.eventTimeTimeTimers.deleteTimer(getKeyBytes(this.currentKey), namespace, timerId);    this.processingTimeTimers.deleteTimer(getKeyBytes(this.currentKey), namespace, timerId);}
public void beam_f703_0(TimerData timerKey)
{    getTimerSet(timerKey.getDomain()).deleteTimer(getKeyBytes(this.currentKey), timerKey);}
public Instant beam_f704_0()
{    return Instant.now();}
public void beam_f712_0(Slice keyBytes, TimerData timerKey)
{    Set<Slice> timersForKey = activeTimers.get(keyBytes);    if (timersForKey != null) {        try {            Slice timerBytes = new Slice(CoderUtils.encodeToByteArray(timerDataCoder, timerKey));            timersForKey.add(timerBytes);            timersForKey.remove(timerBytes);        } catch (CoderException e) {            throw new RuntimeException(e);        }        if (timersForKey.isEmpty()) {            activeTimers.remove(keyBytes);        } else {            activeTimers.put(keyBytes, timersForKey);        }    }}
protected Map<Slice, Set<Slice>> beam_f713_0()
{    return activeTimers;}
public void beam_f714_0(ParDo.MultiOutput<InputT, OutputT> transform, TranslationContext context)
{    DoFn<InputT, OutputT> doFn = transform.getFn();    DoFnSignature signature = DoFnSignatures.getSignature(doFn.getClass());    if (signature.processElement().isSplittable()) {        throw new UnsupportedOperationException(String.format("%s does not support splittable DoFn: %s", ApexRunner.class.getSimpleName(), doFn));    }    if (signature.timerDeclarations().size() > 0) {        throw new UnsupportedOperationException(String.format("Found %s annotations on %s, but %s cannot yet be used with timers in the %s.", DoFn.TimerId.class.getSimpleName(), doFn.getClass().getName(), DoFn.class.getSimpleName(), ApexRunner.class.getSimpleName()));    }    Map<TupleTag<?>, PValue> outputs = context.getOutputs();    PCollection<InputT> input = context.getInput();    Iterable<PCollectionView<?>> sideInputs = transform.getSideInputs().values();    DoFnSchemaInformation doFnSchemaInformation;    doFnSchemaInformation = ParDoTranslation.getSchemaInformation(context.getCurrentTransform());    Map<String, PCollectionView<?>> sideInputMapping = ParDoTranslation.getSideInputMapping(context.getCurrentTransform());    Map<TupleTag<?>, Coder<?>> outputCoders = outputs.entrySet().stream().filter(e -> e.getValue() instanceof PCollection).collect(Collectors.toMap(e -> e.getKey(), e -> ((PCollection) e.getValue()).getCoder()));    ApexParDoOperator<InputT, OutputT> operator = new ApexParDoOperator<>(context.getPipelineOptions(), doFn, transform.getMainOutputTag(), transform.getAdditionalOutputTags().getAll(), input.getWindowingStrategy(), sideInputs, input.getCoder(), outputCoders, doFnSchemaInformation, sideInputMapping, context.getStateBackend());    Map<PCollection<?>, OutputPort<?>> ports = Maps.newHashMapWithExpectedSize(outputs.size());    for (Entry<TupleTag<?>, PValue> output : outputs.entrySet()) {        checkArgument(output.getValue() instanceof PCollection, "%s %s outputs non-PCollection %s of type %s", ParDo.MultiOutput.class.getSimpleName(), context.getFullName(), output.getValue(), output.getValue().getClass().getSimpleName());        PCollection<?> pc = (PCollection<?>) output.getValue();        if (output.getKey().equals(transform.getMainOutputTag())) {            ports.put(pc, operator.output);        } else {            int portIndex = 0;            for (TupleTag<?> tag : transform.getAdditionalOutputTags().getAll()) {                if (tag.equals(output.getKey())) {                    ports.put(pc, operator.additionalOutputPorts[portIndex]);                    break;                }                portIndex++;            }        }    }    context.addOperator(operator, ports);    context.addStream(context.getInput(), operator.input);    if (!Iterables.isEmpty(sideInputs)) {        addSideInputs(operator.sideInput1, sideInputs, context);    }}
public ApexPipelineOptions beam_f722_0()
{    return pipelineOptions;}
public String beam_f723_0()
{    return getCurrentTransform().getFullName();}
public Map<TupleTag<?>, PValue> beam_f724_0()
{    return getCurrentTransform().getInputs();}
public void beam_f732_0(PInput input, InputPort inputPort)
{    while (aliasCollections.containsKey(input)) {        input = aliasCollections.get(input);    }    Pair<OutputPortInfo, List<InputPortInfo>> stream = this.streams.get(input);    checkArgument(stream != null, "no upstream operator defined for " + input);    stream.getRight().add(new InputPortInfo(inputPort, getCurrentTransform()));}
public void beam_f733_0(PValue alias, PInput source)
{    aliasCollections.put(alias, source);}
public void beam_f734_0(DAG dag)
{    for (Map.Entry<String, Operator> nameAndOperator : this.operators.entrySet()) {        dag.addOperator(nameAndOperator.getKey(), nameAndOperator.getValue());    }    int streamIndex = 0;    for (Map.Entry<PCollection, Pair<OutputPortInfo, List<InputPortInfo>>> streamEntry : this.streams.entrySet()) {        List<InputPortInfo> destInfo = streamEntry.getValue().getRight();        InputPort[] sinks = new InputPort[destInfo.size()];        for (int i = 0; i < sinks.length; i++) {            sinks[i] = destInfo.get(i).port;        }        if (sinks.length > 0) {            DAG.StreamMeta streamMeta = dag.addStream("stream" + streamIndex++, streamEntry.getValue().getLeft().port, sinks);            if (pipelineOptions.isParDoFusionEnabled()) {                optimizeStreams(streamMeta, streamEntry);            }            for (InputPort port : sinks) {                PCollection pc = streamEntry.getKey();                Coder coder = pc.getCoder();                if (pc.getWindowingStrategy() != null) {                    coder = FullWindowedValueCoder.of(pc.getCoder(), pc.getWindowingStrategy().getWindowFn().windowCoder());                }                Coder<Object> wrapperCoder = ApexStreamTuple.ApexStreamTupleCoder.of(coder);                CoderAdapterStreamCodec streamCodec = new CoderAdapterStreamCodec(wrapperCoder);                dag.setInputPortAttribute(port, PortContext.STREAM_CODEC, streamCodec);            }        }    }}
public MapState<KeyT, ValueT> beam_f742_0(StateTag<MapState<KeyT, ValueT>> spec, Coder<KeyT> mapKeyCoder, Coder<ValueT> mapValueCoder)
{    throw new UnsupportedOperationException(String.format("%s is not supported", MapState.class.getSimpleName()));}
public CombiningState<InputT, AccumT, OutputT> beam_f743_0(StateTag<CombiningState<InputT, AccumT, OutputT>> address, Coder<AccumT> accumCoder, final CombineFn<InputT, AccumT, OutputT> combineFn)
{    return new ApexCombiningState<>(namespace, address, accumCoder, combineFn);}
public WatermarkHoldState beam_f744_0(StateTag<WatermarkHoldState> address, TimestampCombiner timestampCombiner)
{    return new ApexWatermarkHoldState<>(namespace, address, timestampCombiner);}
public T beam_f752_0()
{    return readValue();}
public void beam_f753_0(T input)
{    writeValue(input);}
public ApexWatermarkHoldState<W> beam_f754_0()
{    return this;}
public OutputT beam_f762_0()
{    return combineFn.extractOutput(getAccum());}
public void beam_f763_0(InputT input)
{    AccumT accum = combineFn.addInput(getAccum(), input);    writeValue(accum);}
public AccumT beam_f764_0()
{    AccumT accum = readValue();    if (accum == null) {        accum = combineFn.createAccumulator();    }    return accum;}
public void beam_f772_0(T input)
{    List<T> value = read();    value.add(input);    writeValue(value);}
public ReadableState<Boolean> beam_f773_0()
{    return new ReadableState<Boolean>() {        @Override        public ReadableState<Boolean> readLater() {            return this;        }        @Override        public Boolean read() {            return stateTable.get(namespace.stringKey(), address.getId()) == null;        }    };}
public ReadableState<Boolean> beam_f774_0()
{    return this;}
public int beam_f782_0()
{    return unionTag;}
public void beam_f783_0(int unionTag)
{    this.unionTag = unionTag;}
public String beam_f784_0()
{    return value.toString();}
public void beam_f792_0(ApexStreamTuple<T> value, OutputStream outStream) throws CoderException, IOException
{    encode(value, outStream, Context.NESTED);}
public void beam_f793_0(ApexStreamTuple<T> value, OutputStream outStream, Context context) throws CoderException, IOException
{    if (value instanceof WatermarkTuple) {        outStream.write(1);        new DataOutputStream(outStream).writeLong(((WatermarkTuple<?>) value).getTimestamp());    } else {        outStream.write(0);        outStream.write(((DataTuple<?>) value).unionTag);        valueCoder.encode(value.getValue(), outStream, context);    }}
public ApexStreamTuple<T> beam_f794_0(InputStream inStream) throws CoderException, IOException
{    return decode(inStream, Context.NESTED);}
public Slice beam_f802_0(Object wv)
{    ByteArrayOutputStream bos = new ByteArrayOutputStream();    try {        coder.encode(wv, bos, Context.OUTER);    } catch (IOException e) {        throw new RuntimeException(e);    }    return new Slice(bos.toByteArray());}
public int beam_f803_0(Object o)
{    return o.hashCode();}
public StateInternals beam_f804_0()
{    throw new UnsupportedOperationException("stateInternals is not supported");}
public T beam_f812_0()
{    return value;}
public void beam_f813_0(Kryo kryo, Output output)
{    try {        kryo.writeClass(output, coder.getClass());        kryo.writeObject(output, coder, JAVA_SERIALIZER);        coder.encode(value, output);    } catch (IOException e) {        throw new RuntimeException(e);    }}
public void beam_f814_0(Kryo kryo, Input input)
{    try {        @SuppressWarnings("unchecked")        Class<Coder<T>> type = kryo.readClass(input).getType();        coder = kryo.readObject(input, type, JAVA_SERIALIZER);        value = coder.decode(input);    } catch (IOException e) {        throw new RuntimeException(e);    }}
public Instant beam_f822_0() throws NoSuchElementException
{    return Instant.now();}
public Instant beam_f824_0()
{    return Instant.now();}
public CheckpointMark beam_f825_0()
{    return NOOP_CHECKPOINT_MARK;}
public void beam_f833_0() throws Exception
{        EmbeddedAppLauncher<?> embeddedLauncher = Launcher.getLauncher(LaunchMode.EMBEDDED);    StreamingApplication app = (dag, conf) -> dag.setAttribute(DAGContext.APPLICATION_NAME, "DummyApp");    Configuration conf = new Configuration(false);    DAG dag = embeddedLauncher.prepareDAG(app, conf);    Attribute.AttributeMap launchAttributes = new Attribute.AttributeMap.DefaultAttributeMap();    Properties configProperties = new Properties();    ApexYarnLauncher launcher = new ApexYarnLauncher();    launcher.launchApp(new MockApexYarnLauncherParams(dag, launchAttributes, configProperties));}
protected Launcher<?> beam_f834_0()
{    return new Launcher<AppHandle>() {        @Override        public AppHandle launchApp(StreamingApplication application, Configuration configuration, AttributeMap launchParameters) throws Launcher.LauncherException {            EmbeddedAppLauncher<?> embeddedLauncher = Launcher.getLauncher(LaunchMode.EMBEDDED);            DAG dag = embeddedLauncher.getDAG();            application.populateDAG(dag, new Configuration(false));            String appName = dag.getValue(DAGContext.APPLICATION_NAME);            Assert.assertEquals("DummyApp", appName);            return new AppHandle() {                @Override                public boolean isFinished() {                    return true;                }                @Override                public void shutdown(Launcher.ShutdownMode arg0) {                }            };        }    };}
public AppHandle beam_f835_0(StreamingApplication application, Configuration configuration, AttributeMap launchParameters) throws Launcher.LauncherException
{    EmbeddedAppLauncher<?> embeddedLauncher = Launcher.getLauncher(LaunchMode.EMBEDDED);    DAG dag = embeddedLauncher.getDAG();    application.populateDAG(dag, new Configuration(false));    String appName = dag.getValue(DAGContext.APPLICATION_NAME);    Assert.assertEquals("DummyApp", appName);    return new AppHandle() {        @Override        public boolean isFinished() {            return true;        }        @Override        public void shutdown(Launcher.ShutdownMode arg0) {        }    };}
public boolean beam_f844_0() throws IOException
{    index++;    currentRecord = texts[(int) index % (texts.length)];    currentTimestamp = new Instant(index * 1000);    try {                Thread.sleep(index);    } catch (InterruptedException e) {        throw new RuntimeException(e);    }    return true;}
public byte[] beam_f845_0() throws NoSuchElementException
{    return new byte[0];}
public String beam_f846_0() throws NoSuchElementException
{    return this.currentRecord;}
public static void beam_f855_0(String[] args)
{    WordCountOptions options = PipelineOptionsFactory.fromArgs(args).withValidation().as(WordCountOptions.class);    runWordCount(options);}
public void beam_f856_0() throws Exception
{    PipelineOptionsFactory.register(WordCountOptions.class);    WordCountOptions options = TestPipeline.testingPipelineOptions().as(WordCountOptions.class);    options.setRunner(TestApexRunner.class);    options.setApplicationName("StreamingWordCount");    String inputFile = WordCountTest.class.getResource("/words.txt").getFile();    options.setInputFile(new File(inputFile).getAbsolutePath());    String outputFilePrefix = "target/wordcountresult.txt";    options.setOutput(outputFilePrefix);    File outFile1 = new File(outputFilePrefix + "-00000-of-00002");    File outFile2 = new File(outputFilePrefix + "-00001-of-00002");    Assert.assertTrue(!outFile1.exists() || outFile1.delete());    Assert.assertTrue(!outFile2.exists() || outFile2.delete());    WordCountTest.runWordCount(options);    Assert.assertTrue("result files exist", outFile1.exists() && outFile2.exists());    HashSet<String> results = new HashSet<>();    results.addAll(Files.readAllLines(outFile1.toPath()));    results.addAll(Files.readAllLines(outFile2.toPath()));    HashSet<String> expectedOutput = Sets.newHashSet("foo - 5 @ 294247-01-09T04:00:54.775Z", "bar - 5 @ 294247-01-09T04:00:54.775Z");    Assert.assertEquals("expected output", expectedOutput, results);}
public void beam_f857_0(ProcessContext c)
{    RESULTS.put(c.element().getKey(), c.element().getValue());}
public void beam_f865_0() throws Exception
{    ApexPipelineOptions options = PipelineOptionsFactory.as(ApexPipelineOptions.class);    options.setApplicationName("GroupByKey");    options.setRunner(ApexRunner.class);    Pipeline p = Pipeline.create(options);    List<KV<String, Instant>> data = Lists.newArrayList(KV.of("foo", new Instant(1000)), KV.of("foo", new Instant(1000)), KV.of("foo", new Instant(2000)), KV.of("bar", new Instant(1000)), KV.of("bar", new Instant(2000)), KV.of("bar", new Instant(2000)));        List<KV<Instant, KV<String, Long>>> expected = Lists.newArrayList(KV.of(new Instant(1000), KV.of("foo", 2L)), KV.of(new Instant(1000), KV.of("bar", 1L)), KV.of(new Instant(2000), KV.of("foo", 1L)), KV.of(new Instant(2000), KV.of("bar", 2L)));    p.apply(Read.from(new TestSource(data, new Instant(5000)))).apply(Window.<String>into(FixedWindows.of(Duration.standardSeconds(1))).withTimestampCombiner(TimestampCombiner.LATEST)).apply(Count.perElement()).apply(ParDo.of(new KeyedByTimestamp<>())).apply(ParDo.of(new EmbeddedCollector()));    ApexRunnerResult result = (ApexRunnerResult) p.run();    result.getApexDAG();    long timeout = System.currentTimeMillis() + 30000;    while (System.currentTimeMillis() < timeout) {        if (EmbeddedCollector.RESULTS.containsAll(expected)) {            break;        }        Thread.sleep(1000);    }    Assert.assertEquals(Sets.newHashSet(expected), EmbeddedCollector.RESULTS);}
public void beam_f866_0(ProcessContext c) throws Exception
{    RESULTS.add(c.element());}
public void beam_f867_0(ProcessContext c) throws Exception
{    c.output(KV.of(c.timestamp(), c.element()));}
public String beam_f875_0() throws NoSuchElementException
{    collected = true;    return this.currentRecord;}
public Instant beam_f876_0() throws NoSuchElementException
{    return currentTimestamp;}
public Instant beam_f878_0()
{    if (!iterator.hasNext() && collected) {        return watermark;    } else {        return new Instant(0);    }}
public void beam_f886_0(ProcessContext c) throws Exception
{    RESULTS.add(c.element());}
private static Throwable beam_f887_0(Pipeline pipeline)
{        try {        pipeline.run();    } catch (AssertionError exc) {        return exc;    }    fail("assertion should have failed");    throw new RuntimeException("unreachable");}
public void beam_f888_0() throws Exception
{    ApexPipelineOptions options = PipelineOptionsFactory.create().as(ApexPipelineOptions.class);    options.setRunner(TestApexRunner.class);    Pipeline pipeline = Pipeline.create(options);    PCollection<Integer> pcollection = pipeline.apply(Create.of(1, 2, 3, 4));    PAssert.that(pcollection).containsInAnyOrder(2, 1, 4, 3, 7);    Throwable exc = runExpectingAssertionFailure(pipeline);    Pattern expectedPattern = Pattern.compile("Expected: iterable over \\[((<4>|<7>|<3>|<2>|<1>)(, )?){5}\\] in any order");        assertTrue("Expected error message from PAssert with substring matching " + expectedPattern + " but the message was \"" + exc.getMessage() + "\"", expectedPattern.matcher(exc.getMessage()).find());}
public void beam_f896_1() throws Exception
{    ApexPipelineOptions options = PipelineOptionsFactory.create().as(ApexPipelineOptions.class);    EmbeddedCollector.RESULTS.clear();    options.setApplicationName("ReadUnbound");    options.setRunner(ApexRunner.class);    Pipeline p = Pipeline.create(options);    List<String> collection = Lists.newArrayList("1", "2", "3", "4", "5");    CollectionSource<String> source = new CollectionSource<>(collection, StringUtf8Coder.of());    p.apply(Read.from(source)).apply(ParDo.of(new EmbeddedCollector()));    ApexRunnerResult result = (ApexRunnerResult) p.run();    DAG dag = result.getApexDAG();    DAG.OperatorMeta om = dag.getOperatorMeta("Read(CollectionSource)");    Assert.assertNotNull(om);    Assert.assertEquals(om.getOperator().getClass(), ApexReadUnboundedInputOperator.class);    long timeout = System.currentTimeMillis() + 30000;    while (System.currentTimeMillis() < timeout) {        if (EmbeddedCollector.RESULTS.containsAll(collection)) {            break;        }                Thread.sleep(1000);    }    Assert.assertEquals(Sets.newHashSet(collection), EmbeddedCollector.RESULTS);}
public void beam_f897_1() throws Exception
{    ApexPipelineOptions options = PipelineOptionsFactory.create().as(ApexPipelineOptions.class);    EmbeddedCollector.RESULTS.clear();    options.setApplicationName("ReadBounded");    options.setRunner(ApexRunner.class);    Pipeline p = Pipeline.create(options);    Set<Long> expected = ContiguousSet.create(Range.closedOpen(0L, 10L), DiscreteDomain.longs());    p.apply(GenerateSequence.from(0).to(10)).apply(ParDo.of(new EmbeddedCollector()));    ApexRunnerResult result = (ApexRunnerResult) p.run();    DAG dag = result.getApexDAG();    String operatorName = "GenerateSequence/Read(BoundedCountingSource)";    DAG.OperatorMeta om = dag.getOperatorMeta(operatorName);    Assert.assertNotNull(om);    Assert.assertEquals(om.getOperator().getClass(), ApexReadUnboundedInputOperator.class);    long timeout = System.currentTimeMillis() + 30000;    while (System.currentTimeMillis() < timeout) {        if (EmbeddedCollector.RESULTS.containsAll(expected)) {            break;        }                Thread.sleep(1000);    }    Assert.assertEquals(Sets.newHashSet(expected), EmbeddedCollector.RESULTS);}
public void beam_f898_0(ProcessContext c) throws Exception
{    RESULTS.add(c.element());}
protected StateInternals beam_f907_0()
{    return newStateInternals();}
public void beam_f915_0() throws Exception
{    ApexStateInternals.ApexStateInternalsFactory<String> sif = new ApexStateInternals.ApexStateBackend().newStateInternalsFactory(StringUtf8Coder.of());    ApexStateInternals<String> keyAndState = sif.stateInternalsForKey("dummy");    ValueState<String> value = keyAndState.state(NAMESPACE, STRING_VALUE_ADDR);    assertEquals(keyAndState.state(NAMESPACE, STRING_VALUE_ADDR), value);    value.write("hello");    ApexStateInternals.ApexStateInternalsFactory<String> cloned;    assertNotNull("Serialization", cloned = KryoCloneUtils.cloneObject(sif));    ApexStateInternals<String> clonedKeyAndState = cloned.stateInternalsForKey("dummy");    ValueState<String> clonedValue = clonedKeyAndState.state(NAMESPACE, STRING_VALUE_ADDR);    assertThat(clonedValue.read(), Matchers.equalTo("hello"));    assertEquals(clonedKeyAndState.state(NAMESPACE, STRING_VALUE_ADDR), value);}
public List<? extends UnboundedSource<T, CheckpointMark>> beam_f916_0(int desiredNumSplits, PipelineOptions options) throws Exception
{    return Collections.singletonList(this);}
public UnboundedSource<T, ?> beam_f924_0()
{    return source;}
public T beam_f925_0() throws NoSuchElementException
{    return current;}
public Instant beam_f926_0() throws NoSuchElementException
{    return Instant.now();}
public boolean beam_f936_0()
{    return completed.getCount() == 0;}
public void beam_f937_0() throws InterruptedException
{    completed.await();}
public StagingResult beam_f938_0()
{    Set<ArtifactMetadata> metadata = new HashSet<>();    Map<StagedFile, Throwable> failures = new HashMap<>();    for (Entry<StagedFile, CompletionStage<ArtifactMetadata>> stagedFileResult : futures.entrySet()) {        try {            metadata.add(MoreFutures.get(stagedFileResult.getValue()));        } catch (ExecutionException ee) {            failures.put(stagedFileResult.getKey(), ee.getCause());        } catch (InterruptedException ie) {            throw new AssertionError("This should never happen. " + "All of the futures are complete by construction", ie);        }    }    if (failures.isEmpty()) {        return StagingResult.success(metadata);    } else {        return StagingResult.failure(failures);    }}
public byte[] beam_f946_0(AvroCoder from)
{    return from.getSchema().toString().getBytes(Charsets.UTF_8);}
public AvroCoder beam_f947_0(List<Coder<?>> components, byte[] payload)
{    Schema schema = new Schema.Parser().parse(new String(payload, Charsets.UTF_8));    return AvroCoder.of(schema);}
public static String beam_f948_0(ProtocolMessageEnum value)
{    return value.getValueDescriptor().getOptions().getExtension(RunnerApi.beamUrn);}
public static Coder<?> beam_f956_0(RunnerApi.Coder protoCoder, RehydratedComponents components) throws IOException
{    String coderSpecUrn = protoCoder.getSpec().getUrn();    if (coderSpecUrn.equals(JAVA_SERIALIZED_CODER_URN)) {        return fromCustomCoder(protoCoder);    }    return fromKnownCoder(protoCoder, components);}
private static Coder<?> beam_f957_0(RunnerApi.Coder coder, RehydratedComponents components) throws IOException
{    String coderUrn = coder.getSpec().getUrn();    List<Coder<?>> coderComponents = new ArrayList<>();    for (String componentId : coder.getComponentCoderIdsList()) {        Coder<?> innerCoder = components.getCoder(componentId);        coderComponents.add(innerCoder);    }    Class<? extends Coder> coderType = KNOWN_CODER_URNS.inverse().get(coderUrn);    CoderTranslator<?> translator = KNOWN_TRANSLATORS.get(coderType);    checkArgument(translator != null, "Unknown Coder URN %s. Known URNs: %s", coderUrn, KNOWN_CODER_URNS.values());    return translator.fromComponents(coderComponents, coder.getSpec().getPayload().toByteArray());}
private static Coder<?> beam_f958_0(RunnerApi.Coder protoCoder) throws IOException
{    return (Coder<?>) SerializableUtils.deserializeFromByteArray(protoCoder.getSpec().getPayload().toByteArray(), "Custom Coder Bytes");}
 static CoderTranslator<IterableCoder<?>> beam_f966_0()
{    return new SimpleStructuredCoderTranslator<IterableCoder<?>>() {        @Override        public List<? extends Coder<?>> getComponents(IterableCoder<?> from) {            return Collections.singletonList(from.getElemCoder());        }        @Override        public IterableCoder<?> fromComponents(List<Coder<?>> components) {            return IterableCoder.of(components.get(0));        }    };}
public List<? extends Coder<?>> beam_f967_0(IterableCoder<?> from)
{    return Collections.singletonList(from.getElemCoder());}
public IterableCoder<?> beam_f968_0(List<Coder<?>> components)
{    return IterableCoder.of(components.get(0));}
public List<? extends Coder<?>> beam_f976_0(FullWindowedValueCoder<?> from)
{    return ImmutableList.of(from.getValueCoder(), from.getWindowCoder());}
public FullWindowedValueCoder<?> beam_f977_0(List<Coder<?>> components)
{    return WindowedValue.getFullCoder(components.get(0), (Coder<BoundedWindow>) components.get(1));}
public final T beam_f978_0(List<Coder<?>> components, byte[] payload)
{    return fromComponents(components);}
public String beam_f986_0(Combine.GroupedValues<?, ?, ?> transform)
{    return COMBINE_GROUPED_VALUES_TRANSFORM_URN;}
public FunctionSpec beam_f987_0(AppliedPTransform<?, ?, Combine.GroupedValues<?, ?, ?>> transform, SdkComponents components) throws IOException
{    if (transform.getTransform().getSideInputs().isEmpty()) {        GlobalCombineFn<?, ?, ?> combineFn = transform.getTransform().getFn();        Coder<?> accumulatorCoder = extractAccumulatorCoder(combineFn, (AppliedPTransform) transform);        return FunctionSpec.newBuilder().setUrn(getUrn(transform.getTransform())).setPayload(combinePayload(combineFn, accumulatorCoder, components).toByteString()).build();    } else {                return null;    }}
private static Coder<AccumT> beam_f988_0(GlobalCombineFn<InputT, AccumT, ?> combineFn, AppliedPTransform<PCollection<KV<K, Iterable<InputT>>>, ?, Combine.GroupedValues<K, InputT, ?>> transform) throws IOException
{    try {        @SuppressWarnings("unchecked")        PCollection<KV<K, Iterable<InputT>>> mainInput = (PCollection<KV<K, Iterable<InputT>>>) Iterables.getOnlyElement(TransformInputs.nonAdditionalInputs(transform));        KvCoder<K, Iterable<InputT>> kvCoder = (KvCoder<K, Iterable<InputT>>) mainInput.getCoder();        IterableCoder<InputT> iterCoder = (IterableCoder<InputT>) kvCoder.getValueCoder();        return combineFn.getAccumulatorCoder(transform.getPipeline().getCoderRegistry(), iterCoder.getElemCoder());    } catch (CannotProvideCoderException e) {        throw new IOException("Could not obtain a Coder for the accumulator", e);    }}
public static DeduplicatedFlattenFactory<T> beam_f996_0()
{    return new DeduplicatedFlattenFactory<>();}
public PTransformReplacement<PCollectionList<T>, PCollection<T>> beam_f997_0(AppliedPTransform<PCollectionList<T>, PCollection<T>, PCollections<T>> transform)
{    return PTransformReplacement.of(getInput(transform.getInputs(), transform.getPipeline()), new FlattenWithoutDuplicateInputs<>());}
private PCollectionList<T> beam_f998_0(Map<TupleTag<?>, PValue> inputs, Pipeline p)
{    PCollectionList<T> pCollections = PCollectionList.empty(p);    for (PValue input : inputs.values()) {        PCollection<T> pcollection = (PCollection<T>) input;        pCollections = pCollections.and(pcollection);    }    return pCollections;}
public static RunnerApi.DisplayData beam_f1006_0(DisplayData displayData)
{        return RunnerApi.DisplayData.newBuilder().addItems(RunnerApi.DisplayData.Item.newBuilder().setId(RunnerApi.DisplayData.Identifier.newBuilder().setKey("stubImplementation")).setLabel("Stub implementation").setType(RunnerApi.DisplayData.Type.Enum.BOOLEAN).setValue(Any.pack(BoolValue.newBuilder().setValue(true).build()))).build();}
public static EmptyFlattenAsCreateFactory<T> beam_f1007_0()
{    return (EmptyFlattenAsCreateFactory<T>) INSTANCE;}
public PTransformReplacement<PCollectionList<T>, PCollection<T>> beam_f1008_0(AppliedPTransform<PCollectionList<T>, PCollection<T>, PCollections<T>> transform)
{    checkArgument(transform.getInputs().isEmpty(), "Unexpected nonempty input %s for %s", transform.getInputs(), getClass().getSimpleName());    return PTransformReplacement.of(PCollectionList.empty(transform.getPipeline()), new CreateEmptyFromList<T>());}
public static Environment beam_f1016_0(String os, String arch, String command, Map<String, String> env)
{    ProcessPayload.Builder builder = ProcessPayload.newBuilder();    if (!Strings.isNullOrEmpty(os)) {        builder.setOs(os);    }    if (!Strings.isNullOrEmpty(arch)) {        builder.setArch(arch);    }    if (!Strings.isNullOrEmpty(command)) {        builder.setCommand(command);    }    if (env != null) {        builder.putAllEnv(env);    }    return Environment.newBuilder().setUrn(BeamUrns.getUrn(StandardEnvironments.Environments.PROCESS)).setPayload(builder.build().toByteString()).build();}
public static Optional<Environment> beam_f1017_0(String ptransformId, Components components)
{    try {        PTransform ptransform = components.getTransformsOrThrow(ptransformId);        String envId = KNOWN_URN_SPEC_EXTRACTORS.getOrDefault(ptransform.getSpec().getUrn(), DEFAULT_SPEC_EXTRACTOR).getEnvironmentId(ptransform);        if (Strings.isNullOrEmpty(envId)) {                        return Optional.empty();        } else {            return Optional.of(components.getEnvironmentsOrThrow(envId));        }    } catch (IOException e) {        throw new RuntimeException(e);    }}
public static Optional<Environment> beam_f1018_0(PTransform ptransform, RehydratedComponents components)
{    try {        String envId = KNOWN_URN_SPEC_EXTRACTORS.getOrDefault(ptransform.getSpec().getUrn(), DEFAULT_SPEC_EXTRACTOR).getEnvironmentId(ptransform);        if (!Strings.isNullOrEmpty(envId)) {                        return Optional.of(components.getEnvironment(envId));        } else {            return Optional.empty();        }    } catch (IOException e) {        throw new RuntimeException(e);    }}
public Map<String, String> beam_f1026_0()
{    return env;}
public static ExecutableStagePayload beam_f1027_0(AppliedPTransform<?, ?, ?> appliedTransform) throws IOException
{    RunnerApi.PTransform transform = PTransformTranslation.toProto(appliedTransform, SdkComponents.create(appliedTransform.getPipeline().getOptions()));    checkArgument(ExecutableStage.URN.equals(transform.getSpec().getUrn()));    return ExecutableStagePayload.parseFrom(transform.getSpec().getPayload());}
public static String beam_f1028_0(ExecutableStagePayload stagePayload)
{    StringBuilder sb = new StringBuilder();    RunnerApi.Components components = stagePayload.getComponents();    final int transformsCount = stagePayload.getTransformsCount();    sb.append("[").append(transformsCount).append("]");    Collection<String> names = new ArrayList<>();    for (int i = 0; i < transformsCount; i++) {        String name = components.getTransformsOrThrow(stagePayload.getTransforms(i)).getUniqueName();                name = name.replaceFirst("/ParMultiDo\\(Anonymous\\)$", "");        names.add(name);    }    sb.append(generateNameFromTransformNames(names, true));    return sb.toString();}
private static Object beam_f1036_0(Class<? extends ExternalTransformBuilder> builderClass) throws Exception
{    for (Method method : builderClass.getMethods()) {        if (method.getName().equals("buildExternal")) {            Preconditions.checkState(method.getParameterCount() == 1, "Build method for ExternalTransformBuilder %s must have exactly one parameter, but had %s parameters.", builderClass.getSimpleName(), method.getParameterCount());            Class<?> configurationClass = method.getParameterTypes()[0];            if (Object.class.equals(configurationClass)) {                continue;            }            return configurationClass.getDeclaredConstructor().newInstance();        }    }    throw new RuntimeException("Couldn't find build method on ExternalTransformBuilder.");}
 static void beam_f1037_0(Object config, ExternalTransforms.ExternalConfigurationPayload payload) throws Exception
{    Converter<String, String> camelCaseConverter = CaseFormat.LOWER_UNDERSCORE.converterTo(CaseFormat.LOWER_CAMEL);    for (Map.Entry<String, ExternalTransforms.ConfigValue> entry : payload.getConfigurationMap().entrySet()) {        String key = entry.getKey();        ExternalTransforms.ConfigValue value = entry.getValue();        String fieldName = camelCaseConverter.convert(key);        List<String> coderUrns = value.getCoderUrnList();        Preconditions.checkArgument(coderUrns.size() > 0, "No Coder URN provided.");        Coder coder = resolveCoder(coderUrns);        Class type = coder.getEncodedTypeDescriptor().getRawType();        String setterName = "set" + Character.toUpperCase(fieldName.charAt(0)) + fieldName.substring(1);        Method method;        try {                        method = config.getClass().getMethod(setterName, type);        } catch (NoSuchMethodException e) {            throw new RuntimeException(String.format("The configuration class %s is missing a setter %s for %s with type %s", config.getClass(), setterName, fieldName, coder.getEncodedTypeDescriptor().getType().getTypeName()), e);        }        method.invoke(config, coder.decode(entry.getValue().getPayload().newInput(), Coder.Context.NESTED));    }}
private static Coder beam_f1038_0(List<String> coderUrns) throws Exception
{    Preconditions.checkArgument(coderUrns.size() > 0, "No Coder URN provided.");    RunnerApi.Components.Builder componentsBuilder = RunnerApi.Components.newBuilder();    Deque<String> coderQueue = new ArrayDeque<>(coderUrns);    RunnerApi.Coder coder = buildProto(coderQueue, componentsBuilder);    RehydratedComponents rehydratedComponents = RehydratedComponents.forComponents(componentsBuilder.build());    return CoderTranslation.fromProto(coder, rehydratedComponents);}
public void beam_f1046_0(ExpansionApi.ExpansionRequest request, StreamObserver<ExpansionApi.ExpansionResponse> responseObserver)
{    try {        responseObserver.onNext(expand(request));        responseObserver.onCompleted();    } catch (RuntimeException exn) {        responseObserver.onError(exn);        throw exn;    }}
public static void beam_f1048_0(String[] args) throws Exception
{    int port = Integer.parseInt(args[0]);    System.out.println("Starting expansion service at localhost:" + port);    Server server = ServerBuilder.forPort(port).addService(new ExpansionService()).build();    server.start();    server.awaitTermination();}
private static int beam_f1049_0()
{    return namespaceCounter.getAndIncrement();}
 RunnerApi.PTransform beam_f1057_0()
{    return expandedTransform;}
 RunnerApi.Components beam_f1058_0()
{    return expandedComponents;}
 Map<PCollection, String> beam_f1059_0()
{    return externalPCollectionIdMap;}
public RunnerApi.PTransform beam_f1067_0(AppliedPTransform<?, ?, ?> appliedPTransform, List<AppliedPTransform<?, ?, ?>> subtransforms, SdkComponents components) throws IOException
{    checkArgument(canTranslate(appliedPTransform.getTransform()), "can only translate ExpandableTransform");    External.ExpandableTransform expandableTransform = (External.ExpandableTransform) appliedPTransform.getTransform();    String nameSpace = expandableTransform.getNamespace();    String impulsePrefix = expandableTransform.getImpulsePrefix();    ImmutableMap.Builder<String, String> pColRenameMapBuilder = ImmutableMap.builder();    RunnerApi.PTransform expandedTransform = expandableTransform.getExpandedTransform();    RunnerApi.Components expandedComponents = expandableTransform.getExpandedComponents();    Map<PCollection, String> externalPCollectionIdMap = expandableTransform.getExternalPCollectionIdMap();    for (PValue pcol : appliedPTransform.getInputs().values()) {        if (!(pcol instanceof PCollection)) {            throw new RuntimeException("unknown input type.");        }        pColRenameMapBuilder.put(externalPCollectionIdMap.get(pcol), components.registerPCollection((PCollection) pcol));    }    for (PValue pcol : appliedPTransform.getOutputs().values()) {        if (!(pcol instanceof PCollection)) {            throw new RuntimeException("unknown input type.");        }        pColRenameMapBuilder.put(externalPCollectionIdMap.get(pcol), components.registerPCollection((PCollection) pcol));    }    ImmutableMap<String, String> pColRenameMap = pColRenameMapBuilder.build();    RunnerApi.Components.Builder mergingComponentsBuilder = RunnerApi.Components.newBuilder();    for (Map.Entry<String, RunnerApi.Coder> entry : expandedComponents.getCodersMap().entrySet()) {        if (entry.getKey().startsWith(nameSpace)) {            mergingComponentsBuilder.putCoders(entry.getKey(), entry.getValue());        }    }    for (Map.Entry<String, RunnerApi.WindowingStrategy> entry : expandedComponents.getWindowingStrategiesMap().entrySet()) {        if (entry.getKey().startsWith(nameSpace)) {            mergingComponentsBuilder.putWindowingStrategies(entry.getKey(), entry.getValue());        }    }    for (Map.Entry<String, RunnerApi.Environment> entry : expandedComponents.getEnvironmentsMap().entrySet()) {        if (entry.getKey().startsWith(nameSpace)) {            mergingComponentsBuilder.putEnvironments(entry.getKey(), entry.getValue());        }    }    for (Map.Entry<String, RunnerApi.PCollection> entry : expandedComponents.getPcollectionsMap().entrySet()) {        if (entry.getKey().startsWith(nameSpace)) {            mergingComponentsBuilder.putPcollections(entry.getKey(), entry.getValue());        }    }    for (Map.Entry<String, RunnerApi.PTransform> entry : expandedComponents.getTransformsMap().entrySet()) {                if (entry.getKey().startsWith(impulsePrefix)) {            continue;        }        checkState(entry.getKey().startsWith(nameSpace), "unknown transform found");        RunnerApi.PTransform proto = entry.getValue();        RunnerApi.PTransform.Builder transformBuilder = RunnerApi.PTransform.newBuilder();        transformBuilder.setUniqueName(proto.getUniqueName()).setSpec(proto.getSpec()).addAllSubtransforms(proto.getSubtransformsList());        for (Map.Entry<String, String> inputEntry : proto.getInputsMap().entrySet()) {            transformBuilder.putInputs(inputEntry.getKey(), pColRenameMap.getOrDefault(inputEntry.getValue(), inputEntry.getValue()));        }        for (Map.Entry<String, String> outputEntry : proto.getOutputsMap().entrySet()) {            transformBuilder.putOutputs(outputEntry.getKey(), pColRenameMap.getOrDefault(outputEntry.getValue(), outputEntry.getValue()));        }        mergingComponentsBuilder.putTransforms(entry.getKey(), transformBuilder.build());    }    RunnerApi.PTransform.Builder rootTransformBuilder = RunnerApi.PTransform.newBuilder();    rootTransformBuilder.setUniqueName(expandedTransform.getUniqueName()).setSpec(expandedTransform.getSpec()).addAllSubtransforms(expandedTransform.getSubtransformsList()).putAllInputs(expandedTransform.getInputsMap());    for (Map.Entry<String, String> outputEntry : expandedTransform.getOutputsMap().entrySet()) {        rootTransformBuilder.putOutputs(outputEntry.getKey(), pColRenameMap.getOrDefault(outputEntry.getValue(), outputEntry.getValue()));    }    components.mergeFrom(mergingComponentsBuilder.build());    return rootTransformBuilder.build();}
public static TransformPayloadTranslator beam_f1068_0()
{    return new FlattenTranslator();}
public String beam_f1069_0(Flatten.PCollections<?> transform)
{    return PTransformTranslation.FLATTEN_TRANSFORM_URN;}
 PTransform beam_f1077_0(String uniqueName)
{    PTransform.Builder pt = PTransform.newBuilder().setUniqueName(uniqueName);    ExecutableStagePayload.Builder payload = ExecutableStagePayload.newBuilder();    payload.setEnvironment(getEnvironment());        PCollectionNode input = getInputPCollection();    pt.putInputs("input", getInputPCollection().getId());    payload.setInput(input.getId());    for (SideInputReference sideInput : getSideInputs()) {                        String outerLocalName = String.format("%s:%s", sideInput.transform().getId(), sideInput.localName());        pt.putInputs(outerLocalName, sideInput.collection().getId());        payload.addSideInputs(SideInputId.newBuilder().setTransformId(sideInput.transform().getId()).setLocalName(sideInput.localName()));    }    for (UserStateReference userState : getUserStates()) {        payload.addUserStates(UserStateId.newBuilder().setTransformId(userState.transform().getId()).setLocalName(userState.localName()));    }    for (TimerReference timer : getTimers()) {        payload.addTimers(TimerId.newBuilder().setTransformId(timer.transform().getId()).setLocalName(timer.localName()));    }    int outputIndex = 0;    for (PCollectionNode output : getOutputPCollections()) {        pt.putOutputs(String.format("materialized_%d", outputIndex), output.getId());        payload.addOutputs(output.getId());        outputIndex++;    }        for (PTransformNode transform : getTransforms()) {        payload.addTransforms(transform.getId());    }    payload.setComponents(getComponents().toBuilder().clearTransforms().putAllTransforms(getTransforms().stream().collect(Collectors.toMap(PTransformNode::getId, PTransformNode::getTransform))));    pt.setSpec(FunctionSpec.newBuilder().setUrn(ExecutableStage.URN).setPayload(payload.build().toByteString()).build());    return pt.build();}
 static ExecutableStage beam_f1078_0(ExecutableStagePayload payload)
{    Components components = payload.getComponents();    Environment environment = payload.getEnvironment();    PCollectionNode input = PipelineNode.pCollection(payload.getInput(), components.getPcollectionsOrThrow(payload.getInput()));    List<SideInputReference> sideInputs = payload.getSideInputsList().stream().map(sideInputId -> SideInputReference.fromSideInputId(sideInputId, components)).collect(Collectors.toList());    List<UserStateReference> userStates = payload.getUserStatesList().stream().map(userStateId -> UserStateReference.fromUserStateId(userStateId, components)).collect(Collectors.toList());    List<TimerReference> timers = payload.getTimersList().stream().map(timerId -> TimerReference.fromTimerId(timerId, components)).collect(Collectors.toList());    List<PTransformNode> transforms = payload.getTransformsList().stream().map(id -> PipelineNode.pTransform(id, components.getTransformsOrThrow(id))).collect(Collectors.toList());    List<PCollectionNode> outputs = payload.getOutputsList().stream().map(id -> PipelineNode.pCollection(id, components.getPcollectionsOrThrow(id))).collect(Collectors.toList());    return ImmutableExecutableStage.of(components, environment, input, sideInputs, userStates, timers, transforms, outputs);}
 static FusedPipeline beam_f1079_0(Components components, Set<ExecutableStage> environmentalStages, Set<PTransformNode> runnerStages)
{    return new AutoValue_FusedPipeline(components, environmentalStages, runnerStages);}
private static boolean beam_f1087_0(PTransformNode left, PTransformNode right, QueryablePipeline pipeline)
{    return pipeline.getEnvironment(left).equals(pipeline.getEnvironment(right));}
private static boolean beam_f1088_0(@SuppressWarnings("unused") PTransformNode flatten, @SuppressWarnings("unused") Environment environment, @SuppressWarnings("unused") PCollectionNode candidate, @SuppressWarnings("unused") Collection<PCollectionNode> stagePCollections, @SuppressWarnings("unused") QueryablePipeline pipeline)
{    return true;}
private static boolean beam_f1089_0(@SuppressWarnings("unused") PTransformNode cannotFuse, @SuppressWarnings("unused") Environment environment, @SuppressWarnings("unused") PCollectionNode candidate, @SuppressWarnings("unused") Collection<PCollectionNode> stagePCollections, @SuppressWarnings("unused") QueryablePipeline pipeline)
{    return false;}
 static DescendantConsumers beam_f1097_0(Set<PTransformNode> unfusible, NavigableSet<CollectionConsumer> fusible)
{    return new AutoValue_GreedyPipelineFuser_DescendantConsumers(unfusible, fusible);}
private NavigableSet<NavigableSet<CollectionConsumer>> beam_f1098_0(NavigableSet<CollectionConsumer> newConsumers)
/* Use a navigable set for consistent iteration order */{    Multimap<SiblingKey, NavigableSet<CollectionConsumer>> compatibleConsumers = HashMultimap.create();        for (CollectionConsumer newConsumer : newConsumers) {        SiblingKey key = new AutoValue_GreedyPipelineFuser_SiblingKey(newConsumer.consumedCollection(), pipeline.getEnvironment(newConsumer.consumingTransform()).get());        boolean foundSiblings = false;        for (Set<CollectionConsumer> existingConsumers : compatibleConsumers.get(key)) {            if (existingConsumers.stream().allMatch(            collectionConsumer -> GreedyPCollectionFusers.isCompatible(collectionConsumer.consumingTransform(), newConsumer.consumingTransform(), pipeline))) {                existingConsumers.add(newConsumer);                foundSiblings = true;                break;            }        }        if (!foundSiblings) {            NavigableSet<CollectionConsumer> newConsumerSet = new TreeSet<>();            newConsumerSet.add(newConsumer);            compatibleConsumers.put(key, newConsumerSet);        }    }            @SuppressWarnings("JdkObsolete")    NavigableSet<NavigableSet<CollectionConsumer>> orderedSiblings = new TreeSet<>(Comparator.comparing(NavigableSet::first));    orderedSiblings.addAll(compatibleConsumers.values());    return orderedSiblings;}
private ExecutableStage beam_f1099_0(Set<CollectionConsumer> mutuallyCompatible)
{    PCollectionNode rootCollection = mutuallyCompatible.iterator().next().consumedCollection();    return GreedyStageFuser.forGrpcPortRead(pipeline, rootCollection, mutuallyCompatible.stream().map(CollectionConsumer::consumingTransform).collect(Collectors.toSet()));}
public static ImmutableExecutableStage beam_f1107_0(Components components, Environment environment, PCollectionNode input, Collection<SideInputReference> sideInputs, Collection<UserStateReference> userStates, Collection<TimerReference> timers, Collection<PTransformNode> transforms, Collection<PCollectionNode> outputs)
{    Components prunedComponents = components.toBuilder().clearTransforms().putAllTransforms(transforms.stream().collect(Collectors.toMap(PTransformNode::getId, PTransformNode::getTransform))).build();    return of(prunedComponents, environment, input, sideInputs, userStates, timers, transforms, outputs);}
public static ImmutableExecutableStage beam_f1108_0(Components components, Environment environment, PCollectionNode input, Collection<SideInputReference> sideInputs, Collection<UserStateReference> userStates, Collection<TimerReference> timers, Collection<PTransformNode> transforms, Collection<PCollectionNode> outputs)
{    return new AutoValue_ImmutableExecutableStage(components, environment, input, ImmutableSet.copyOf(sideInputs), ImmutableSet.copyOf(userStates), ImmutableSet.copyOf(timers), ImmutableSet.copyOf(transforms), ImmutableSet.copyOf(outputs));}
public final NodeT beam_f1109_0(NodeT input)
{    if (type.isInstance(input)) {        return typedApply((T) input);    }    return input;}
private static String beam_f1117_0(String s)
{    return s.replace("\\", "\\\\").replace("\"", "\\\"").replace("\n", "\\l");}
public static List<List<NodeT>> beam_f1118_0(Network<NodeT, EdgeT> network)
{    ArrayDeque<List<NodeT>> paths = new ArrayDeque<>();        for (NodeT node : network.nodes()) {        if (network.inDegree(node) == 0) {            paths.add(ImmutableList.of(node));        }    }    List<List<NodeT>> distinctPathsFromRootsToLeaves = new ArrayList<>();    while (!paths.isEmpty()) {        List<NodeT> path = paths.removeFirst();        NodeT lastNode = path.get(path.size() - 1);        if (network.outDegree(lastNode) == 0) {            distinctPathsFromRootsToLeaves.add(new ArrayList<>(path));        } else {            for (EdgeT edge : network.outEdges(lastNode)) {                paths.addFirst(ImmutableList.<NodeT>builder().addAll(path).add(network.incidentNodes(edge).target()).build());            }        }    }    return distinctPathsFromRootsToLeaves;}
 static DeduplicationResult beam_f1119_0(QueryablePipeline pipeline, Collection<ExecutableStage> stages, Collection<PTransformNode> unfusedTransforms)
{    RunnerApi.Components.Builder unzippedComponents = pipeline.getComponents().toBuilder();    Multimap<PCollectionNode, StageOrTransform> pcollectionProducers = getProducers(pipeline, stages, unfusedTransforms);    Multimap<StageOrTransform, PCollectionNode> requiresNewOutput = HashMultimap.create();        for (Map.Entry<PCollectionNode, Collection<StageOrTransform>> collectionProducer : pcollectionProducers.asMap().entrySet()) {        if (collectionProducer.getValue().size() > 1) {            for (StageOrTransform producer : collectionProducer.getValue()) {                requiresNewOutput.put(producer, collectionProducer.getKey());            }        }    }    Map<ExecutableStage, ExecutableStage> updatedStages = new LinkedHashMap<>();    Map<String, PTransformNode> updatedTransforms = new LinkedHashMap<>();    Multimap<String, PCollectionNode> originalToPartial = HashMultimap.create();    for (Map.Entry<StageOrTransform, Collection<PCollectionNode>> deduplicationTargets : requiresNewOutput.asMap().entrySet()) {        if (deduplicationTargets.getKey().getStage() != null) {            StageDeduplication deduplication = deduplicatePCollections(deduplicationTargets.getKey().getStage(), deduplicationTargets.getValue(), unzippedComponents::containsPcollections);            for (Entry<String, PCollectionNode> originalToPartialReplacement : deduplication.getOriginalToPartialPCollections().entrySet()) {                originalToPartial.put(originalToPartialReplacement.getKey(), originalToPartialReplacement.getValue());                unzippedComponents.putPcollections(originalToPartialReplacement.getValue().getId(), originalToPartialReplacement.getValue().getPCollection());            }            updatedStages.put(deduplicationTargets.getKey().getStage(), deduplication.getUpdatedStage());        } else if (deduplicationTargets.getKey().getTransform() != null) {            PTransformDeduplication deduplication = deduplicatePCollections(deduplicationTargets.getKey().getTransform(), deduplicationTargets.getValue(), unzippedComponents::containsPcollections);            for (Entry<String, PCollectionNode> originalToPartialReplacement : deduplication.getOriginalToPartialPCollections().entrySet()) {                originalToPartial.put(originalToPartialReplacement.getKey(), originalToPartialReplacement.getValue());                unzippedComponents.putPcollections(originalToPartialReplacement.getValue().getId(), originalToPartialReplacement.getValue().getPCollection());            }            updatedTransforms.put(deduplicationTargets.getKey().getTransform().getId(), deduplication.getUpdatedTransform());        } else {            throw new IllegalStateException(String.format("%s with no %s or %s", StageOrTransform.class.getSimpleName(), ExecutableStage.class.getSimpleName(), PTransformNode.class.getSimpleName()));        }    }    Set<PTransformNode> introducedFlattens = new LinkedHashSet<>();    for (Map.Entry<String, Collection<PCollectionNode>> partialFlattenTargets : originalToPartial.asMap().entrySet()) {        String flattenId = SyntheticComponents.uniqueId("unzipped_flatten", unzippedComponents::containsTransforms);        PTransform flattenPartialPCollections = createFlattenOfPartials(flattenId, partialFlattenTargets.getKey(), partialFlattenTargets.getValue());        unzippedComponents.putTransforms(flattenId, flattenPartialPCollections);        introducedFlattens.add(PipelineNode.pTransform(flattenId, flattenPartialPCollections));    }    Components components = unzippedComponents.build();    return DeduplicationResult.of(components, introducedFlattens, updatedStages, updatedTransforms);}
private static Map<String, PCollectionNode> beam_f1127_0(Collection<PCollectionNode> duplicates, Predicate<String> existingPCollectionIds)
{    Map<String, PCollectionNode> unzippedOutputs = new LinkedHashMap<>();    Predicate<String> existingOrNewIds = existingPCollectionIds.or(id -> unzippedOutputs.values().stream().map(PCollectionNode::getId).anyMatch(id::equals));    for (PCollectionNode duplicateOutput : duplicates) {        String id = SyntheticComponents.uniqueId(duplicateOutput.getId(), existingOrNewIds);        PCollection partial = duplicateOutput.getPCollection().toBuilder().setUniqueName(id).build();                        PCollectionNode alreadyDeduplicated = unzippedOutputs.put(duplicateOutput.getId(), PipelineNode.pCollection(id, partial));        checkArgument(alreadyDeduplicated == null, "a duplicate should only appear once per stage");    }    return unzippedOutputs;}
private static ExecutableStage beam_f1128_0(ExecutableStage stage, Map<String, PCollectionNode> originalToPartial)
{    Collection<PTransformNode> updatedTransforms = new ArrayList<>();    for (PTransformNode transform : stage.getTransforms()) {        PTransform updatedTransform = updateOutputs(transform.getTransform(), originalToPartial);        updatedTransforms.add(PipelineNode.pTransform(transform.getId(), updatedTransform));    }    Collection<PCollectionNode> updatedOutputs = new ArrayList<>();    for (PCollectionNode output : stage.getOutputPCollections()) {        updatedOutputs.add(originalToPartial.getOrDefault(output.getId(), output));    }    RunnerApi.Components updatedStageComponents = stage.getComponents().toBuilder().clearTransforms().putAllTransforms(updatedTransforms.stream().collect(Collectors.toMap(PTransformNode::getId, PTransformNode::getTransform))).putAllPcollections(originalToPartial.values().stream().collect(Collectors.toMap(PCollectionNode::getId, PCollectionNode::getPCollection))).build();    return ImmutableExecutableStage.of(updatedStageComponents, stage.getEnvironment(), stage.getInputPCollection(), stage.getSideInputs(), stage.getUserStates(), stage.getTimers(), updatedTransforms, updatedOutputs);}
private static PTransform beam_f1129_0(PTransform transform, Map<String, PCollectionNode> originalToPartial)
{    PTransform.Builder updatedTransformBuilder = transform.toBuilder();    for (Map.Entry<String, String> output : transform.getOutputsMap().entrySet()) {        if (originalToPartial.containsKey(output.getValue())) {            updatedTransformBuilder.putOutputs(output.getKey(), originalToPartial.get(output.getValue()).getId());        }    }    return updatedTransformBuilder.build();}
public static void beam_f1137_0(RunnerApi.Pipeline p)
{    Components components = p.getComponents();    for (String transformId : p.getRootTransformIdsList()) {        checkArgument(components.containsTransforms(transformId), "Root transform id %s is unknown", transformId);    }    validateComponents("pipeline", components);}
private static void beam_f1138_0(String context, Components components)
{    {        Map<String, String> uniqueNamesById = Maps.newHashMap();        for (String transformId : components.getTransformsMap().keySet()) {            PTransform transform = components.getTransformsOrThrow(transformId);            String previousId = uniqueNamesById.put(transform.getUniqueName(), transformId);                                                checkArgument(previousId == null, "%s: Transforms %s and %s both have unique_name \"%s\"", context, transformId, previousId, transform.getUniqueName());            validateTransform(transformId, transform, components);        }    }    {        Map<String, String> uniqueNamesById = Maps.newHashMap();        for (String pcollectionId : components.getPcollectionsMap().keySet()) {            PCollection pc = components.getPcollectionsOrThrow(pcollectionId);            checkArgument(!pc.getUniqueName().isEmpty(), "%s: PCollection %s does not have a unique_name set", context, pcollectionId);            String previousId = uniqueNamesById.put(pc.getUniqueName(), pcollectionId);            checkArgument(previousId == null, "%s: PCollections %s and %s both have unique_name \"%s\"", context, pcollectionId, previousId, pc.getUniqueName());            checkArgument(components.containsCoders(pc.getCoderId()), "%s: PCollection %s uses unknown coder %s", context, pcollectionId, pc.getCoderId());            checkArgument(components.containsWindowingStrategies(pc.getWindowingStrategyId()), "%s: PCollection %s uses unknown windowing strategy %s", context, pcollectionId, pc.getWindowingStrategyId());        }    }    for (String strategyId : components.getWindowingStrategiesMap().keySet()) {        WindowingStrategy strategy = components.getWindowingStrategiesOrThrow(strategyId);        checkArgument(components.containsCoders(strategy.getWindowCoderId()), "%s: WindowingStrategy %s uses unknown coder %s", context, strategyId, strategy.getWindowCoderId());    }    for (String coderId : components.getCodersMap().keySet()) {        for (String componentCoderId : components.getCodersOrThrow(coderId).getComponentCoderIdsList()) {            checkArgument(components.containsCoders(componentCoderId), "%s: Coder %s uses unknown component coder %s", context, coderId, componentCoderId);        }    }}
private static void beam_f1139_0(String id, PTransform transform, Components components)
{    for (String subtransformId : transform.getSubtransformsList()) {        checkArgument(components.containsTransforms(subtransformId), "Transform %s references unknown subtransform %s", id, subtransformId);    }    for (String inputId : transform.getInputsMap().keySet()) {        String pcollectionId = transform.getInputsOrThrow(inputId);        checkArgument(components.containsPcollections(pcollectionId), "Transform %s input %s points to unknown PCollection %s", id, inputId, pcollectionId);    }    for (String outputId : transform.getOutputsMap().keySet()) {        String pcollectionId = transform.getOutputsOrThrow(outputId);        checkArgument(components.containsPcollections(pcollectionId), "Transform %s output %s points to unknown PCollection %s", id, outputId, pcollectionId);    }    String urn = transform.getSpec().getUrn();    if (VALIDATORS.containsKey(urn)) {        try {            VALIDATORS.get(urn).validate(id, transform, components);        } catch (Exception e) {            throw new RuntimeException(String.format("Failed to validate transform %s", id), e);        }    }}
public static QueryablePipeline beam_f1147_0(Components components)
{    return new QueryablePipeline(getPrimitiveTransformIds(components), components);}
public static QueryablePipeline beam_f1148_0(RunnerApi.Pipeline p)
{    return forTransforms(p.getRootTransformIdsList(), p.getComponents());}
public static QueryablePipeline beam_f1149_0(Collection<String> transformIds, Components components)
{    return new QueryablePipeline(transformIds, components);}
public Set<PTransformNode> beam_f1157_0(PCollectionNode pCollection)
{    return pipelineNetwork.successors(pCollection).stream().filter(consumer -> pipelineNetwork.edgesConnecting(pCollection, consumer).stream().anyMatch(PipelineEdge::isPerElement)).map(pipelineNode -> (PTransformNode) pipelineNode).collect(Collectors.toSet());}
public Set<PTransformNode> beam_f1158_0(PCollectionNode pCollection)
{    return pipelineNetwork.successors(pCollection).stream().filter(consumer -> pipelineNetwork.edgesConnecting(pCollection, consumer).stream().anyMatch(edge -> !edge.isPerElement())).map(pipelineNode -> (PTransformNode) pipelineNode).collect(Collectors.toSet());}
public Set<PCollectionNode> beam_f1159_0(PTransformNode ptransform)
{    return pipelineNetwork.inEdges(ptransform).stream().filter(PipelineEdge::isPerElement).map(edge -> (PCollectionNode) pipelineNetwork.incidentNodes(edge).source()).collect(Collectors.toSet());}
private Set<String> beam_f1167_0(PTransform transform)
{    if (PAR_DO_TRANSFORM_URN.equals(transform.getSpec().getUrn())) {        try {            return ParDoPayload.parseFrom(transform.getSpec().getPayload()).getTimerSpecsMap().keySet();        } catch (InvalidProtocolBufferException e) {            throw new RuntimeException(e);        }    } else {        return Collections.emptySet();    }}
public Optional<Environment> beam_f1168_0(PTransformNode parDo)
{    return Environments.getEnvironment(parDo.getId(), components);}
public boolean beam_f1169_0()
{    return true;}
public static UserStateReference beam_f1177_0(UserStateId userStateId, RunnerApi.Components components)
{    PTransform transform = components.getTransformsOrThrow(userStateId.getTransformId());    String mainInputCollectionId;    try {        mainInputCollectionId = transform.getInputsOrThrow(ParDoTranslation.getMainInputName(transform));    } catch (IOException e) {        throw new RuntimeException(e);    }    return UserStateReference.of(PipelineNode.pTransform(userStateId.getTransformId(), transform), userStateId.getLocalName(), PipelineNode.pCollection(mainInputCollectionId, components.getPcollectionsOrThrow(mainInputCollectionId)));}
public String beam_f1178_0(GroupByKey<?, ?> transform)
{    return PTransformTranslation.GROUP_BY_KEY_TRANSFORM_URN;}
public FunctionSpec beam_f1179_0(AppliedPTransform<?, ?, GroupByKey<?, ?>> transform, SdkComponents components)
{    return FunctionSpec.newBuilder().setUrn(getUrn(transform.getTransform())).build();}
public PCollection<T> beam_f1187_0(PBegin input)
{    return input.apply(Impulse.create()).apply(ParDo.of(new SplitBoundedSourceFn<>(source, DEFAULT_BUNDLE_SIZE_BYTES))).setCoder(new BoundedSourceCoder<>()).apply(Reshuffle.viaRandomKey()).apply(ParDo.of(new ReadFromBoundedSourceFn<>())).setCoder(source.getOutputCoder());}
public PTransformReplacement<PBegin, PCollection<T>> beam_f1188_0(AppliedPTransform<PBegin, PCollection<T>, PTransform<PBegin, PCollection<T>>> transform)
{    PBegin input = PBegin.in(transform.getPipeline());    BoundedSource<T> source;    try {        source = ReadTranslation.boundedSourceFromTransform(transform);    } catch (IOException e) {        throw new RuntimeException(e);    }    return PTransformReplacement.of(input, bounded(source));}
public Map<PValue, ReplacementOutput> beam_f1189_0(Map<TupleTag<?>, PValue> outputs, PCollection<T> newOutput)
{    return ReplacementOutputs.singleton(outputs, newOutput);}
public static Set<String> beam_f1197_0()
{    return MODEL_CODER_URNS;}
public static WindowedValueCoderComponents beam_f1198_0(Coder coder)
{    checkArgument(WINDOWED_VALUE_CODER_URN.equals(coder.getSpec().getUrn()));    return new AutoValue_ModelCoders_WindowedValueCoderComponents(coder.getComponentCoderIds(0), coder.getComponentCoderIds(1));}
public static Coder beam_f1199_0(String elementCoderId, String windowCoderId)
{    return Coder.newBuilder().setSpec(FunctionSpec.newBuilder().setUrn(WINDOWED_VALUE_CODER_URN)).addComponentCoderIds(elementCoderId).addComponentCoderIds(windowCoderId).build();}
public static ParDoPayload beam_f1207_0(AppliedPTransform<?, ?, ParDo.MultiOutput<?, ?>> appliedPTransform, SdkComponents components) throws IOException
{    final ParDo.MultiOutput<?, ?> parDo = appliedPTransform.getTransform();    final Pipeline pipeline = appliedPTransform.getPipeline();    final DoFn<?, ?> doFn = parDo.getFn();    final DoFnSignature signature = DoFnSignatures.getSignature(doFn.getClass());        Set<String> allInputs = appliedPTransform.getInputs().keySet().stream().map(TupleTag::getId).collect(Collectors.toSet());    Set<String> sideInputs = parDo.getSideInputs().values().stream().map(s -> s.getTagInternal().getId()).collect(Collectors.toSet());    Set<String> timerInputs = signature.timerDeclarations().keySet();    String mainInputName = Iterables.getOnlyElement(Sets.difference(allInputs, Sets.union(sideInputs, timerInputs)));    PCollection<?> mainInput = (PCollection<?>) appliedPTransform.getInputs().get(new TupleTag<>(mainInputName));    final DoFnSchemaInformation doFnSchemaInformation = ParDo.getDoFnSchemaInformation(doFn, mainInput);    return translateParDo(parDo, doFnSchemaInformation, pipeline, components);}
public static ParDoPayload beam_f1208_0(ParDo.MultiOutput<?, ?> parDo, DoFnSchemaInformation doFnSchemaInformation, Pipeline pipeline, SdkComponents components) throws IOException
{    final DoFn<?, ?> doFn = parDo.getFn();    final DoFnSignature signature = DoFnSignatures.getSignature(doFn.getClass());    final String restrictionCoderId;    if (signature.processElement().isSplittable()) {        final Coder<?> restrictionCoder = DoFnInvokers.invokerFor(doFn).invokeGetRestrictionCoder(pipeline.getCoderRegistry());        restrictionCoderId = components.registerCoder(restrictionCoder);    } else {        restrictionCoderId = "";    }    return payloadForParDoLike(new ParDoLike() {        @Override        public SdkFunctionSpec translateDoFn(SdkComponents newComponents) {            return ParDoTranslation.translateDoFn(parDo.getFn(), parDo.getMainOutputTag(), parDo.getSideInputs(), doFnSchemaInformation, newComponents);        }        @Override        public List<RunnerApi.Parameter> translateParameters() {            return ParDoTranslation.translateParameters(signature.processElement().extraParameters());        }        @Override        public Map<String, SideInput> translateSideInputs(SdkComponents components) {            Map<String, SideInput> sideInputs = new HashMap<>();            for (PCollectionView<?> sideInput : parDo.getSideInputs().values()) {                sideInputs.put(sideInput.getTagInternal().getId(), translateView(sideInput, components));            }            return sideInputs;        }        @Override        public Map<String, RunnerApi.StateSpec> translateStateSpecs(SdkComponents components) throws IOException {            Map<String, RunnerApi.StateSpec> stateSpecs = new HashMap<>();            for (Map.Entry<String, StateDeclaration> state : signature.stateDeclarations().entrySet()) {                RunnerApi.StateSpec spec = translateStateSpec(getStateSpecOrThrow(state.getValue(), doFn), components);                stateSpecs.put(state.getKey(), spec);            }            return stateSpecs;        }        @Override        public Map<String, RunnerApi.TimerSpec> translateTimerSpecs(SdkComponents newComponents) {            Map<String, RunnerApi.TimerSpec> timerSpecs = new HashMap<>();            for (Map.Entry<String, TimerDeclaration> timer : signature.timerDeclarations().entrySet()) {                RunnerApi.TimerSpec spec = translateTimerSpec(getTimerSpecOrThrow(timer.getValue(), doFn), newComponents);                timerSpecs.put(timer.getKey(), spec);            }            return timerSpecs;        }        @Override        public boolean isSplittable() {            return signature.processElement().isSplittable();        }        @Override        public String translateRestrictionCoderId(SdkComponents newComponents) {            return restrictionCoderId;        }    }, components);}
public SdkFunctionSpec beam_f1209_0(SdkComponents newComponents)
{    return ParDoTranslation.translateDoFn(parDo.getFn(), parDo.getMainOutputTag(), parDo.getSideInputs(), doFnSchemaInformation, newComponents);}
public static DoFn<?, ?> beam_f1217_0(ParDoPayload payload) throws InvalidProtocolBufferException
{    return doFnWithExecutionInformationFromProto(payload.getDoFn()).getDoFn();}
public static DoFn<?, ?> beam_f1218_0(AppliedPTransform<?, ?, ?> application) throws IOException
{    PTransform<?, ?> transform = application.getTransform();    if (transform instanceof ParDo.MultiOutput) {        return ((ParDo.MultiOutput<?, ?>) transform).getFn();    }    return getDoFn(getParDoPayload(application));}
public static DoFnSchemaInformation beam_f1219_0(AppliedPTransform<?, ?, ?> application)
{    try {        return getSchemaInformation(getParDoPayload(application));    } catch (IOException e) {        throw new RuntimeException(e);    }}
public static TupleTagList beam_f1227_0(AppliedPTransform<?, ?, ?> application) throws IOException
{    PTransform<?, ?> transform = application.getTransform();    if (transform instanceof ParDo.MultiOutput) {        return ((ParDo.MultiOutput<?, ?>) transform).getAdditionalOutputTags();    }    RunnerApi.PTransform protoTransform = PTransformTranslation.toProto(application, SdkComponents.create(application.getPipeline().getOptions()));    ParDoPayload payload = ParDoPayload.parseFrom(protoTransform.getSpec().getPayload());    TupleTag<?> mainOutputTag = getMainOutputTag(payload);    Set<String> outputTags = Sets.difference(protoTransform.getOutputsMap().keySet(), Collections.singleton(mainOutputTag.getId()));    ArrayList<TupleTag<?>> additionalOutputTags = new ArrayList<>();    for (String outputTag : outputTags) {        additionalOutputTags.add(new TupleTag<>(outputTag));    }    return TupleTagList.of(additionalOutputTags);}
public static Map<TupleTag<?>, Coder<?>> beam_f1228_0(AppliedPTransform<?, ?, ?> application)
{    return application.getOutputs().entrySet().stream().filter(e -> e.getValue() instanceof PCollection).collect(Collectors.toMap(e -> e.getKey(), e -> ((PCollection) e.getValue()).getCoder()));}
public static List<PCollectionView<?>> beam_f1229_0(AppliedPTransform<?, ?, ?> application) throws IOException
{    PTransform<?, ?> transform = application.getTransform();    if (transform instanceof ParDo.MultiOutput) {        return ((ParDo.MultiOutput<?, ?>) transform).getSideInputs().values().stream().collect(Collectors.toList());    }    SdkComponents sdkComponents = SdkComponents.create(application.getPipeline().getOptions());    RunnerApi.PTransform parDoProto = PTransformTranslation.toProto(application, sdkComponents);    ParDoPayload payload = ParDoPayload.parseFrom(parDoProto.getSpec().getPayload());    List<PCollectionView<?>> views = new ArrayList<>();    RehydratedComponents components = RehydratedComponents.forComponents(sdkComponents.toComponents());    for (Map.Entry<String, SideInput> sideInputEntry : payload.getSideInputsMap().entrySet()) {        String sideInputTag = sideInputEntry.getKey();        RunnerApi.SideInput sideInput = sideInputEntry.getValue();        PCollection<?> originalPCollection = checkNotNull((PCollection<?>) application.getInputs().get(new TupleTag<>(sideInputTag)), "no input with tag %s", sideInputTag);        views.add(PCollectionViewTranslation.viewFromProto(sideInput, sideInputTag, originalPCollection, parDoProto, components));    }    return views;}
public RunnerApi.StateSpec beam_f1237_0(Coder<?> keyCoder, Coder<?> valueCoder)
{    return builder.setMapSpec(RunnerApi.MapStateSpec.newBuilder().setKeyCoderId(registerCoderOrThrow(components, keyCoder)).setValueCoderId(registerCoderOrThrow(components, valueCoder))).build();}
public RunnerApi.StateSpec beam_f1238_0(Coder<?> elementCoder)
{    return builder.setSetSpec(RunnerApi.SetStateSpec.newBuilder().setElementCoderId(registerCoderOrThrow(components, elementCoder))).build();}
 static StateSpec<?> beam_f1239_0(RunnerApi.StateSpec stateSpec, RehydratedComponents components) throws IOException
{    switch(stateSpec.getSpecCase()) {        case VALUE_SPEC:            return StateSpecs.value(components.getCoder(stateSpec.getValueSpec().getCoderId()));        case BAG_SPEC:            return StateSpecs.bag(components.getCoder(stateSpec.getBagSpec().getElementCoderId()));        case COMBINING_SPEC:            FunctionSpec combineFnSpec = stateSpec.getCombiningSpec().getCombineFn().getSpec();            if (!combineFnSpec.getUrn().equals(CombineTranslation.JAVA_SERIALIZED_COMBINE_FN_URN)) {                throw new UnsupportedOperationException(String.format("Cannot create %s from non-Java %s: %s", StateSpec.class.getSimpleName(), Combine.CombineFn.class.getSimpleName(), combineFnSpec.getUrn()));            }            Combine.CombineFn<?, ?, ?> combineFn = (Combine.CombineFn<?, ?, ?>) SerializableUtils.deserializeFromByteArray(combineFnSpec.getPayload().toByteArray(), Combine.CombineFn.class.getSimpleName());                        return StateSpecs.combining((Coder) components.getCoder(stateSpec.getCombiningSpec().getAccumulatorCoderId()), combineFn);        case MAP_SPEC:            return StateSpecs.map(components.getCoder(stateSpec.getMapSpec().getKeyCoderId()), components.getCoder(stateSpec.getMapSpec().getValueCoderId()));        case SET_SPEC:            return StateSpecs.set(components.getCoder(stateSpec.getSetSpec().getElementCoderId()));        case SPEC_NOT_SET:        default:            throw new IllegalArgumentException(String.format("Unknown %s: %s", RunnerApi.StateSpec.class.getName(), stateSpec));    }}
public RunnerApi.Parameter beam_f1247_0(RestrictionTrackerParameter p)
{    return RunnerApi.Parameter.newBuilder().setType(Type.Enum.RESTRICTION_TRACKER).build();}
protected RunnerApi.Parameter beam_f1248_0(Parameter p)
{    return null;}
public static Map<String, SideInput> beam_f1249_0(List<PCollectionView<?>> views, SdkComponents components)
{    Map<String, SideInput> sideInputs = new HashMap<>();    for (PCollectionView<?> sideInput : views) {        sideInputs.put(sideInput.getTagInternal().getId(), ParDoTranslation.translateView(sideInput, components));    }    return sideInputs;}
public static ParDoPayload beam_f1257_0(ParDoLike parDo, SdkComponents components) throws IOException
{    return ParDoPayload.newBuilder().setDoFn(parDo.translateDoFn(components)).addAllParameters(parDo.translateParameters()).putAllStateSpecs(parDo.translateStateSpecs(components)).putAllTimerSpecs(parDo.translateTimerSpecs(components)).putAllSideInputs(parDo.translateSideInputs(components)).setSplittable(parDo.isSplittable()).setRestrictionCoderId(parDo.translateRestrictionCoderId(components)).build();}
public static RunnerApi.PCollection beam_f1258_0(PCollection<?> pCollection, SdkComponents components) throws IOException
{    String coderId = components.registerCoder(pCollection.getCoder());    String windowingStrategyId = components.registerWindowingStrategy(pCollection.getWindowingStrategy());    return RunnerApi.PCollection.newBuilder().setUniqueName(pCollection.getName()).setCoderId(coderId).setIsBounded(toProto(pCollection.isBounded())).setWindowingStrategyId(windowingStrategyId).build();}
public static PCollection<?> beam_f1259_0(RunnerApi.PCollection pCollection, Pipeline pipeline, RehydratedComponents components) throws IOException
{    Coder<?> coder = components.getCoder(pCollection.getCoderId());    return PCollection.createPrimitiveOutputInternal(pipeline, components.getWindowingStrategy(pCollection.getWindowingStrategyId()), fromProto(pCollection.getIsBounded()), (Coder) coder);}
public static PipelineOptions beam_f1267_0(Struct protoOptions)
{    try {        Map<String, TreeNode> mapWithoutUrns = new HashMap<>();        TreeNode rootOptions = MAPPER.readTree(JsonFormat.printer().print(protoOptions));        Iterator<String> optionsKeys = rootOptions.fieldNames();        while (optionsKeys.hasNext()) {            String optionKey = optionsKeys.next();            TreeNode optionValue = rootOptions.get(optionKey);            mapWithoutUrns.put(CaseFormat.LOWER_UNDERSCORE.to(CaseFormat.LOWER_CAMEL, optionKey.substring("beam:option:".length(), optionKey.length() - ":v1".length())), optionValue);        }        return MAPPER.readValue(MAPPER.writeValueAsString(ImmutableMap.of("options", mapWithoutUrns)), PipelineOptions.class);    } catch (IOException e) {        throw new RuntimeException("Failed to read PipelineOptions from Protocol", e);    }}
public static PipelineOptions beam_f1268_0(String optionsJson)
{    try {        Map<String, Object> probingOptionsMap = MAPPER.readValue(optionsJson, new TypeReference<Map<String, Object>>() {        });        if (probingOptionsMap.containsKey("options")) {                        return MAPPER.readValue(optionsJson, PipelineOptions.class);        } else {                        Struct.Builder builder = Struct.newBuilder();            JsonFormat.parser().merge(optionsJson, builder);            return fromProto(builder.build());        }    } catch (IOException e) {        throw new RuntimeException("Failed to read PipelineOptions from JSON", e);    }}
public static String beam_f1269_0(PipelineOptions options)
{    try {        return JsonFormat.printer().print(toProto(options));    } catch (InvalidProtocolBufferException e) {        throw new RuntimeException("Failed to convert PipelineOptions to JSON", e);    }}
public static RunnerApi.Pipeline beam_f1277_0(Pipeline pipeline, boolean useDeprecatedViewTransforms)
{    return toProto(pipeline, SdkComponents.create(pipeline.getOptions()), useDeprecatedViewTransforms);}
public static RunnerApi.Pipeline beam_f1278_0(Pipeline pipeline, SdkComponents components)
{    return toProto(pipeline, components, false);}
public static RunnerApi.Pipeline beam_f1279_0(final Pipeline pipeline, final SdkComponents components, boolean useDeprecatedViewTransforms)
{    final Collection<String> rootIds = new HashSet<>();    pipeline.traverseTopologically(new PipelineVisitor.Defaults() {        private final ListMultimap<Node, AppliedPTransform<?, ?, ?>> children = ArrayListMultimap.create();        @Override        public void leaveCompositeTransform(Node node) {            if (node.isRootNode()) {                for (AppliedPTransform<?, ?, ?> pipelineRoot : children.get(node)) {                    rootIds.add(components.getExistingPTransformId(pipelineRoot));                }            } else {                                children.put(node.getEnclosingNode(), node.toAppliedPTransform(pipeline));                try {                    components.registerPTransform(node.toAppliedPTransform(pipeline), children.get(node));                } catch (IOException e) {                    throw new RuntimeException(e);                }            }        }        @Override        public void visitPrimitiveTransform(Node node) {                        children.put(node.getEnclosingNode(), node.toAppliedPTransform(pipeline));            try {                components.registerPTransform(node.toAppliedPTransform(pipeline), Collections.emptyList());            } catch (IOException e) {                throw new IllegalStateException(e);            }        }    });    RunnerApi.Pipeline res = RunnerApi.Pipeline.newBuilder().setComponents(components.toComponents()).addAllRootTransformIds(rootIds).build();    if (!useDeprecatedViewTransforms) {                res = elideDeprecatedViews(res);    }        PipelineValidator.validate(res);    return res;}
public static PTransformMatcher beam_f1287_0(String urn)
{    return new EqualUrnPTransformMatcher(urn);}
public boolean beam_f1288_0(AppliedPTransform<?, ?, ?> application)
{    return urn.equals(PTransformTranslation.urnForTransformOrNull(application.getTransform()));}
public String beam_f1289_0()
{    return MoreObjects.toStringHelper(this).add("urn", urn).toString();}
public boolean beam_f1297_0(AppliedPTransform<?, ?, ?> application)
{    return false;}
public String beam_f1298_0()
{    return MoreObjects.toStringHelper("RequiresStableInputParDoSingleMatcher").toString();}
public static PTransformMatcher beam_f1299_0()
{    return new PTransformMatcher() {        @Override        public boolean matches(AppliedPTransform<?, ?, ?> application) {            PTransform<?, ?> transform = application.getTransform();            if (transform instanceof ParDo.MultiOutput) {                DoFn<?, ?> fn = ((ParDo.MultiOutput<?, ?>) transform).getFn();                DoFnSignature signature = DoFnSignatures.signatureForDoFn(fn);                return signature.processElement().requiresStableInput();            }            return false;        }        @Override        public boolean matchesDuringValidation(AppliedPTransform<?, ?, ?> application) {            return false;        }        @Override        public String toString() {            return MoreObjects.toStringHelper("RequiresStableInputParDoMultiMatcher").toString();        }    };}
public boolean beam_f1307_0(AppliedPTransform<?, ?, ?> application)
{    PTransform<?, ?> transform = application.getTransform();    if (transform instanceof ParDo.SingleOutput) {        DoFn<?, ?> fn = ((ParDo.SingleOutput<?, ?>) transform).getFn();        DoFnSignature signature = DoFnSignatures.signatureForDoFn(fn);        return signature.usesState() || signature.usesTimers();    }    return false;}
public String beam_f1308_0()
{    return MoreObjects.toStringHelper("StateOrTimerParDoSingleMatcher").toString();}
public static PTransformMatcher beam_f1309_0()
{    return new PTransformMatcher() {        @Override        public boolean matches(AppliedPTransform<?, ?, ?> application) {            PTransform<?, ?> transform = application.getTransform();            if (transform instanceof ParDo.MultiOutput) {                DoFn<?, ?> fn = ((ParDo.MultiOutput<?, ?>) transform).getFn();                DoFnSignature signature = DoFnSignatures.signatureForDoFn(fn);                return signature.processElement().isSplittable();            }            return false;        }        @Override        public String toString() {            return MoreObjects.toStringHelper("SplittableParDoMultiMatcher").toString();        }    };}
public String beam_f1317_0()
{    return MoreObjects.toStringHelper("SplittableProcessKeyedBoundedMatcher").toString();}
public static PTransformMatcher beam_f1318_0()
{    return new PTransformMatcher() {        @Override        public boolean matches(AppliedPTransform<?, ?, ?> application) {            PTransform<?, ?> transform = application.getTransform();            if (transform instanceof SplittableParDo.ProcessKeyedElements) {                DoFn<?, ?> fn = ((SplittableParDo.ProcessKeyedElements) transform).getFn();                DoFnSignature signature = DoFnSignatures.signatureForDoFn(fn);                return signature.processElement().isSplittable() && signature.isBoundedPerElement() == IsBounded.UNBOUNDED;            }            return false;        }        @Override        public String toString() {            return MoreObjects.toStringHelper("SplittableProcessKeyedUnboundedMatcher").toString();        }    };}
public boolean beam_f1319_0(AppliedPTransform<?, ?, ?> application)
{    PTransform<?, ?> transform = application.getTransform();    if (transform instanceof SplittableParDo.ProcessKeyedElements) {        DoFn<?, ?> fn = ((SplittableParDo.ProcessKeyedElements) transform).getFn();        DoFnSignature signature = DoFnSignatures.signatureForDoFn(fn);        return signature.processElement().isSplittable() && signature.isBoundedPerElement() == IsBounded.UNBOUNDED;    }    return false;}
public static PTransformMatcher beam_f1327_0(final Class<? extends DoFn> fnType)
{    return new PTransformMatcher() {        @Override        public boolean matches(AppliedPTransform<?, ?, ?> application) {            DoFn<?, ?> fn;            if (application.getTransform() instanceof ParDo.SingleOutput) {                fn = ((ParDo.SingleOutput) application.getTransform()).getFn();            } else if (application.getTransform() instanceof ParDo.MultiOutput) {                fn = ((ParDo.MultiOutput) application.getTransform()).getFn();            } else {                return false;            }            return fnType.equals(fn.getClass());        }        @Override        public String toString() {            return MoreObjects.toStringHelper("ParDoWithFnTypeMatcher").add("fnType", fnType).toString();        }    };}
public boolean beam_f1328_0(AppliedPTransform<?, ?, ?> application)
{    DoFn<?, ?> fn;    if (application.getTransform() instanceof ParDo.SingleOutput) {        fn = ((ParDo.SingleOutput) application.getTransform()).getFn();    } else if (application.getTransform() instanceof ParDo.MultiOutput) {        fn = ((ParDo.MultiOutput) application.getTransform()).getFn();    } else {        return false;    }    return fnType.equals(fn.getClass());}
public String beam_f1329_0()
{    return MoreObjects.toStringHelper("ParDoWithFnTypeMatcher").add("fnType", fnType).toString();}
public static PTransformMatcher beam_f1337_0()
{    return application -> {        if (WRITE_FILES_TRANSFORM_URN.equals(PTransformTranslation.urnForTransformOrNull(application.getTransform()))) {            try {                return WriteFilesTranslation.isRunnerDeterminedSharding((AppliedPTransform) application);            } catch (IOException exc) {                throw new RuntimeException(String.format("Transform with URN %s failed to parse: %s", WRITE_FILES_TRANSFORM_URN, application.getTransform()), exc);            }        }        return false;    };}
public static PCollection<T> beam_f1338_0(AppliedPTransform<? extends PCollection<? extends T>, ?, ?> application)
{    return getSingletonMainInput(application.getInputs(), application.getTransform().getAdditionalInputs().keySet());}
private static PCollection<T> beam_f1339_0(Map<TupleTag<?>, PValue> inputs, Set<TupleTag<?>> ignoredTags)
{    PCollection<T> mainInput = null;    for (Map.Entry<TupleTag<?>, PValue> input : inputs.entrySet()) {        if (!ignoredTags.contains(input.getKey())) {            checkArgument(mainInput == null, "Got multiple inputs that are not additional inputs for a " + "singleton main input: %s and %s", mainInput, input.getValue());            checkArgument(input.getValue() instanceof PCollection, "Unexpected input type %s", input.getValue().getClass());            mainInput = (PCollection<T>) input.getValue();        }    }    checkArgument(mainInput != null, "No main input found in inputs: Inputs %s, Side Input tags %s", inputs, ignoredTags);    return mainInput;}
public static String beam_f1347_0(RunnerApi.PTransform transform)
{    return transform.getSpec() == null ? null : transform.getSpec().getUrn();}
public String beam_f1348_0(PTransform<?, ?> transform)
{    return null;}
public boolean beam_f1349_0(PTransform<?, ?> pTransform)
{    return true;}
public RunnerApi.PTransform beam_f1357_0(AppliedPTransform<?, ?, ?> appliedPTransform, List<AppliedPTransform<?, ?, ?>> subtransforms, SdkComponents components) throws IOException
{    RunnerApi.PTransform.Builder transformBuilder = translateAppliedPTransform(appliedPTransform, subtransforms, components);    FunctionSpec spec = KNOWN_PAYLOAD_TRANSLATORS.get(appliedPTransform.getTransform().getClass()).translate(appliedPTransform, components);    if (spec != null) {        transformBuilder.setSpec(spec);    }    return transformBuilder.build();}
 static RunnerApi.PTransform.Builder beam_f1358_0(AppliedPTransform<?, ?, ?> appliedPTransform, List<AppliedPTransform<?, ?, ?>> subtransforms, SdkComponents components) throws IOException
{    RunnerApi.PTransform.Builder transformBuilder = RunnerApi.PTransform.newBuilder();    for (Map.Entry<TupleTag<?>, PValue> taggedInput : appliedPTransform.getInputs().entrySet()) {        checkArgument(taggedInput.getValue() instanceof PCollection, "Unexpected input type %s", taggedInput.getValue().getClass());        transformBuilder.putInputs(toProto(taggedInput.getKey()), components.registerPCollection((PCollection<?>) taggedInput.getValue()));    }    for (Map.Entry<TupleTag<?>, PValue> taggedOutput : appliedPTransform.getOutputs().entrySet()) {                if (taggedOutput.getValue() instanceof PCollection) {            checkArgument(taggedOutput.getValue() instanceof PCollection, "Unexpected output type %s", taggedOutput.getValue().getClass());            transformBuilder.putOutputs(toProto(taggedOutput.getKey()), components.registerPCollection((PCollection<?>) taggedOutput.getValue()));        }    }    for (AppliedPTransform<?, ?, ?> subtransform : subtransforms) {        transformBuilder.addSubtransforms(components.getExistingPTransformId(subtransform));    }    transformBuilder.setUniqueName(appliedPTransform.getFullName());    transformBuilder.setDisplayData(DisplayDataTranslation.toProto(DisplayData.from(appliedPTransform.getTransform())));    return transformBuilder;}
public static NotSerializable<?> beam_f1359_0(final String urn)
{    return new NotSerializable<PTransform<?, ?>>() {        @Override        public String getUrn(PTransform<?, ?> transform) {            return urn;        }    };}
public static SdkFunctionSpec beam_f1367_0(Source<?> source, SdkComponents components)
{    if (source instanceof BoundedSource) {        return toProto((BoundedSource) source, components);    } else if (source instanceof UnboundedSource) {        return toProto((UnboundedSource<?, ?>) source, components);    } else {        throw new IllegalArgumentException(String.format("Unknown %s type %s", Source.class.getSimpleName(), source.getClass()));    }}
private static SdkFunctionSpec beam_f1368_0(BoundedSource<?> source, SdkComponents components)
{    return SdkFunctionSpec.newBuilder().setEnvironmentId(components.getOnlyEnvironmentId()).setSpec(FunctionSpec.newBuilder().setUrn(JAVA_SERIALIZED_BOUNDED_SOURCE).setPayload(ByteString.copyFrom(SerializableUtils.serializeToByteArray(source))).build()).build();}
public static BoundedSource<?> beam_f1369_0(ReadPayload payload) throws InvalidProtocolBufferException
{    checkArgument(payload.getIsBounded().equals(IsBounded.Enum.BOUNDED));    return (BoundedSource<?>) SerializableUtils.deserializeFromByteArray(payload.getSource().getSpec().getPayload().toByteArray(), "BoundedSource");}
public String beam_f1377_0(Read.Unbounded<?> transform)
{    return PTransformTranslation.READ_TRANSFORM_URN;}
public FunctionSpec beam_f1378_0(AppliedPTransform<?, ?, Read.Unbounded<?>> transform, SdkComponents components)
{    ReadPayload payload = toProto(transform.getTransform(), components);    return RunnerApi.FunctionSpec.newBuilder().setUrn(getUrn(transform.getTransform())).setPayload(payload.toByteString()).build();}
public static TransformPayloadTranslator beam_f1379_0()
{    return new BoundedReadPayloadTranslator();}
public RehydratedComponents beam_f1387_0(Pipeline pipeline)
{    return new RehydratedComponents(components, pipeline);}
public PCollection<?> beam_f1388_0(String pCollectionId) throws IOException
{    try {        return pCollections.get(pCollectionId);    } catch (ExecutionException exc) {        throw new RuntimeException(exc);    }}
public WindowingStrategy<?, ?> beam_f1389_0(String windowingStrategyId) throws IOException
{    try {        return windowingStrategies.get(windowingStrategyId);    } catch (ExecutionException exc) {        throw new RuntimeException(exc);    }}
public void beam_f1399_0(TransformHierarchy.Node node)
{    exitBlock();    writeLine("}");}
public void beam_f1400_0(TransformHierarchy.Node node)
{    final int nodeId = nextNodeId++;    writeLine("%d [label=\"%s\"]", nodeId, escapeString(node.getTransform().getName()));    node.getOutputs().values().forEach(x -> valueToProducerNodeId.put(x, nodeId));    node.getInputs().forEach((key, value) -> {        final int producerId = valueToProducerNodeId.get(value);        String style = "solid";        if (node.getTransform().getAdditionalInputs().containsKey(key)) {            style = "dashed";        }        writeLine("%d -> %d [style=%s label=\"%s\"]", producerId, nodeId, style, "");    });}
private void beam_f1402_0()
{    writeLine("digraph {");    enterBlock();    writeLine("rankdir=LR");}
private String beam_f1410_0(RunnerApi.Pipeline pipeline)
{    final QueryablePipeline p = QueryablePipeline.forTransforms(pipeline.getRootTransformIdsList(), pipeline.getComponents());    begin();    for (PipelineNode.PTransformNode transform : p.getTopologicallyOrderedTransforms()) {        visitTransform(transform);    }    end();    return dotBuilder.toString();}
private void beam_f1411_0(PipelineNode.PTransformNode node)
{    final int nodeId = nextNodeId++;    final RunnerApi.PTransform transform = node.getTransform();    writeLine("%d [label=\"%s\\n%s\"]", nodeId, escapeString(transform.getUniqueName()), escapeString(transform.getSpec().getUrn()));    transform.getOutputsMap().values().forEach(x -> valueToProducerNodeId.put(x, nodeId));    transform.getInputsMap().forEach((key, value) -> {        final int producerId = valueToProducerNodeId.get(value);        String style = "solid";        writeLine("%d -> %d [style=%s label=\"%s\"]", producerId, nodeId, style, escapeString(value.substring(value.lastIndexOf('_') + 1)));    });}
private void beam_f1412_0()
{    writeLine("digraph {");    enterBlock();    writeLine("rankdir=LR");}
public String beam_f1420_0(Reshuffle<?, ?> transform)
{    return PTransformTranslation.RESHUFFLE_URN;}
public FunctionSpec beam_f1421_0(AppliedPTransform<?, ?, Reshuffle<?, ?>> transform, SdkComponents components)
{    return FunctionSpec.newBuilder().setUrn(getUrn(transform.getTransform())).build();}
public Map<? extends Class<? extends PTransform>, ? extends TransformPayloadTranslator> beam_f1422_0()
{    return Collections.singletonMap(Reshuffle.class, new ReshuffleTranslator());}
public boolean beam_f1430_0(Object other)
{    if (!(other instanceof PCollectionView)) {        return false;    }    @SuppressWarnings("unchecked")    PCollectionView<?> otherView = (PCollectionView<?>) other;    return tag.equals(otherView.getTagInternal());}
public int beam_f1431_0()
{    return Objects.hash(tag);}
public static SchemaApi.Schema beam_f1432_0(Schema schema)
{    String uuid = schema.getUUID() != null ? schema.getUUID().toString() : "";    SchemaApi.Schema.Builder builder = SchemaApi.Schema.newBuilder().setId(uuid);    for (Field field : schema.getFields()) {        SchemaApi.Field protoField = fieldToProto(field, schema.indexOf(field.getName()), schema.getEncodingPositions().get(field.getName()));        builder.addFields(protoField);    }    return builder.build();}
public static SdkComponents beam_f1440_0(RunnerApi.Components components)
{    return new SdkComponents(components, "");}
 static SdkComponents beam_f1441_0(RunnerApi.Components components, Map<String, AppliedPTransform<?, ?, ?>> transforms, Map<String, PCollection<?>> pCollections, Map<String, WindowingStrategy<?, ?>> windowingStrategies, Map<String, Coder<?>> coders, Map<String, Environment> environments)
{    SdkComponents sdkComponents = SdkComponents.create(components);    sdkComponents.transformIds.inverse().putAll(transforms);    sdkComponents.pCollectionIds.inverse().putAll(pCollections);    sdkComponents.windowingStrategyIds.inverse().putAll(windowingStrategies);    sdkComponents.coderIds.inverse().putAll(coders);    sdkComponents.environmentIds.inverse().putAll(environments);    return sdkComponents;}
public static SdkComponents beam_f1442_0(PipelineOptions options)
{    SdkComponents sdkComponents = new SdkComponents(RunnerApi.Components.getDefaultInstance(), "");    PortablePipelineOptions portablePipelineOptions = options.as(PortablePipelineOptions.class);    sdkComponents.defaultEnvironmentId = sdkComponents.registerEnvironment(Environments.createOrGetDefaultEnvironment(portablePipelineOptions.getDefaultEnvironmentType(), portablePipelineOptions.getDefaultEnvironmentConfig()));    return sdkComponents;}
public String beam_f1450_0(WindowingStrategy<?, ?> windowingStrategy) throws IOException
{    String existing = windowingStrategyIds.get(windowingStrategy);    if (existing != null) {        return existing;    }    String baseName = String.format("%s(%s)", NameUtils.approximateSimpleName(windowingStrategy), NameUtils.approximateSimpleName(windowingStrategy.getWindowFn()));    String name = uniqify(baseName, windowingStrategyIds.values());    windowingStrategyIds.put(windowingStrategy, name);    RunnerApi.WindowingStrategy windowingStrategyProto = WindowingStrategyTranslation.toProto(windowingStrategy, this);    componentsBuilder.putWindowingStrategies(name, windowingStrategyProto);    return name;}
public String beam_f1451_0(Coder<?> coder) throws IOException
{    String existing = coderIds.get(coder);    if (existing != null) {        return existing;    }    String baseName = NameUtils.approximateSimpleName(coder);    String name = uniqify(baseName, coderIds.values());    coderIds.put(coder, name);    RunnerApi.Coder coderProto = CoderTranslation.toProto(coder, this);    componentsBuilder.putCoders(name, coderProto);    return name;}
public String beam_f1452_0(Environment env)
{    String existing = environmentIds.get(env);    if (existing != null) {        return existing;    }    String name = uniqify(env.getUrn(), environmentIds.values());    environmentIds.put(env, name);    componentsBuilder.putEnvironments(name, env);    return name;}
public String beam_f1460_0()
{    return serializedPipelineOptions;}
public boolean beam_f1461_0(Object o)
{    if (this == o) {        return true;    }    if (o == null || getClass() != o.getClass()) {        return false;    }    SerializablePipelineOptions that = (SerializablePipelineOptions) o;    return serializedPipelineOptions.equals(that.serializedPipelineOptions);}
public int beam_f1462_0()
{    return Objects.hash(serializedPipelineOptions, options);}
public DoFn<InputT, OutputT> beam_f1470_0()
{    return fn;}
public Coder<InputT> beam_f1471_0()
{    return elementCoder;}
public Coder<RestrictionT> beam_f1472_0()
{    return restrictionCoder;}
public Map<TupleTag<?>, PValue> beam_f1480_0()
{    return PCollectionViews.toAdditionalInputs(sideInputs);}
public Map<? extends Class<? extends PTransform>, ? extends TransformPayloadTranslator> beam_f1481_0()
{    return ImmutableMap.<Class<? extends PTransform>, TransformPayloadTranslator>builder().put(ProcessKeyedElements.class, new ProcessKeyedElementsTranslator()).build();}
public static TransformPayloadTranslator beam_f1482_0()
{    return new ProcessKeyedElementsTranslator();}
public boolean beam_f1490_0()
{    return true;}
public String beam_f1491_0(SdkComponents newComponents)
{    return restrictionCoderId;}
public byte[] beam_f1492_0(T input)
{    byte[] key = new byte[128];    ThreadLocalRandom.current().nextBytes(key);    return key;}
public void beam_f1500_0()
{    invoker.invokeTeardown();    invoker = null;}
public PTransformReplacement<PCollection<KV<byte[], KV<InputT, RestrictionT>>>, PCollectionTuple> beam_f1501_0(AppliedPTransform<PCollection<KV<byte[], KV<InputT, RestrictionT>>>, PCollectionTuple, ProcessKeyedElements<InputT, OutputT, RestrictionT>> transform)
{    checkArgument(DoFnSignatures.signatureForDoFn(transform.getTransform().getFn()).isBoundedPerElement() == IsBounded.BOUNDED, "Expecting a bounded-per-element splittable DoFn");    return PTransformReplacement.of(PTransformReplacements.getSingletonMainInput(transform), new SplittableProcessNaive<>(transform.getTransform()));}
public Map<PValue, ReplacementOutput> beam_f1502_0(Map<TupleTag<?>, PValue> outputs, PCollectionTuple newOutput)
{    return ReplacementOutputs.tagged(outputs, newOutput);}
public void beam_f1510_0(@Nullable OutputT output, Instant timestamp, BoundedWindow window)
{    throw new UnsupportedOperationException("Output from FinishBundle for SDF is not supported");}
public void beam_f1511_0(TupleTag<T> tag, T output, Instant timestamp, BoundedWindow window)
{    throw new UnsupportedOperationException("Output from FinishBundle for SDF is not supported");}
public void beam_f1512_0()
{    invoker.invokeTeardown();}
public Object beam_f1520_0(int index)
{    throw new UnsupportedOperationException();}
public Instant beam_f1521_0(DoFn<InputT, OutputT> doFn)
{    return outerContext.timestamp();}
public OutputReceiver<OutputT> beam_f1522_0(DoFn<InputT, OutputT> doFn)
{    return new OutputReceiver<OutputT>() {        @Override        public void output(OutputT output) {            outerContext.output(output);        }        @Override        public void outputWithTimestamp(OutputT output, Instant timestamp) {            outerContext.outputWithTimestamp(output, timestamp);        }    };}
public RestrictionTracker<?, ?> beam_f1530_0()
{    return tracker;}
public PipelineOptions beam_f1531_0()
{    return outerContext.getPipelineOptions();}
public void beam_f1532_0(OutputT output)
{    outerContext.output(output);}
public DoFn<InputT, OutputT>.StartBundleContext beam_f1541_0(DoFn<InputT, OutputT> doFn)
{    throw new IllegalStateException();}
public DoFn<InputT, OutputT>.FinishBundleContext beam_f1542_0(DoFn<InputT, OutputT> doFn)
{    throw new IllegalStateException();}
public OutputReceiver<Row> beam_f1543_0(DoFn<InputT, OutputT> doFn)
{    throw new UnsupportedOperationException();}
 static TestStream.Event<T> beam_f1551_0(RunnerApi.TestStreamPayload.Event protoEvent, Coder<T> coder) throws IOException
{    switch(protoEvent.getEventCase()) {        case WATERMARK_EVENT:            return TestStream.WatermarkEvent.advanceTo(new Instant(protoEvent.getWatermarkEvent().getNewWatermark()));        case PROCESSING_TIME_EVENT:            return TestStream.ProcessingTimeEvent.advanceBy(Duration.millis(protoEvent.getProcessingTimeEvent().getAdvanceDuration()));        case ELEMENT_EVENT:            List<TimestampedValue<T>> decodedElements = new ArrayList<>();            for (RunnerApi.TestStreamPayload.TimestampedElement element : protoEvent.getElementEvent().getElementsList()) {                decodedElements.add(TimestampedValue.of(CoderUtils.decodeFromByteArray(coder, element.getEncodedElement().toByteArray()), new Instant(element.getTimestamp())));            }            return TestStream.ElementEvent.add(decodedElements);        case EVENT_NOT_SET:        default:            throw new IllegalArgumentException(String.format("Unsupported type of %s: %s", RunnerApi.TestStreamPayload.Event.class.getCanonicalName(), protoEvent.getEventCase()));    }}
public String beam_f1552_0(TestStream<?> transform)
{    return TEST_STREAM_TRANSFORM_URN;}
public RunnerApi.FunctionSpec beam_f1553_0(final AppliedPTransform<?, ?, TestStream<?>> transform, SdkComponents components) throws IOException
{    return translateTyped(transform.getTransform(), components);}
public Timer<T> beam_f1561_0(InputStream inStream) throws CoderException, IOException
{    Instant instant = InstantCoder.of().decode(inStream);    T value = payloadCoder.decode(inStream);    return Timer.of(instant, value);}
public List<? extends org.apache.beam.sdk.coders.Coder<?>> beam_f1562_0()
{    return Collections.singletonList(payloadCoder);}
public void beam_f1563_0() throws NonDeterministicException
{    verifyDeterministic(this, "Payload coder must be deterministic", payloadCoder);}
private RunnerApi.Trigger beam_f1571_0(Method evaluationMethod, Trigger trigger)
{    try {        return (RunnerApi.Trigger) evaluationMethod.invoke(this, trigger);    } catch (InvocationTargetException exc) {        if (exc.getCause() instanceof RuntimeException) {            throw (RuntimeException) exc.getCause();        } else {            throw new RuntimeException(exc.getCause());        }    } catch (IllegalAccessException exc) {        throw new IllegalStateException(String.format("Internal error: could not invoke %s", evaluationMethod));    }}
private Method beam_f1572_0(Class<?> clazz)
{    try {        return getClass().getDeclaredMethod("convertSpecific", clazz);    } catch (NoSuchMethodException exc) {        throw new IllegalArgumentException(String.format("Cannot translate trigger class %s to a runner-API proto.", clazz.getCanonicalName()), exc);    }}
private RunnerApi.Trigger beam_f1573_0(DefaultTrigger v)
{    return RunnerApi.Trigger.newBuilder().setDefault(RunnerApi.Trigger.Default.getDefaultInstance()).build();}
private RunnerApi.Trigger beam_f1581_0(AfterPane v)
{    return RunnerApi.Trigger.newBuilder().setElementCount(RunnerApi.Trigger.ElementCount.newBuilder().setElementCount(v.getElementCount())).build();}
private RunnerApi.Trigger beam_f1582_0(AfterWatermarkEarlyAndLate v)
{    RunnerApi.Trigger.AfterEndOfWindow.Builder builder = RunnerApi.Trigger.AfterEndOfWindow.newBuilder();    builder.setEarlyFirings(toProto(v.getEarlyTrigger()));    if (v.getLateTrigger() != null) {        builder.setLateFirings(toProto(v.getLateTrigger()));    }    return RunnerApi.Trigger.newBuilder().setAfterEndOfWindow(builder).build();}
private RunnerApi.Trigger beam_f1583_0(AfterEach v)
{    RunnerApi.Trigger.AfterEach.Builder builder = RunnerApi.Trigger.AfterEach.newBuilder();    for (Trigger subtrigger : v.subTriggers()) {        builder.addSubtriggers(toProto(subtrigger));    }    return RunnerApi.Trigger.newBuilder().setAfterEach(builder).build();}
protected Coder<T> beam_f1591_0()
{    return source.getDefaultOutputCoder();}
public String beam_f1592_0()
{    return String.format("Read(%s)", NameUtils.approximateSimpleName(source));}
public void beam_f1593_0(DisplayData.Builder builder)
{        builder.add(DisplayData.item("source", source.getClass())).include("source", source);}
public void beam_f1602_0(Checkpoint<T> value, OutputStream outStream) throws CoderException, IOException
{    elemsCoder.encode(value.residualElements, outStream);    sourceCoder.encode(value.residualSource, outStream);}
public Checkpoint<T> beam_f1603_0(InputStream inStream) throws CoderException, IOException
{    return new Checkpoint<>(elemsCoder.decode(inStream), sourceCoder.decode(inStream));}
public List<Coder<?>> beam_f1604_0()
{    return Arrays.asList(elemCoder);}
public Instant beam_f1612_0()
{    return done ? BoundedWindow.TIMESTAMP_MAX_VALUE : BoundedWindow.TIMESTAMP_MIN_VALUE;}
public Checkpoint<T> beam_f1613_0()
{    Checkpoint<T> newCheckpoint;    if (!residualElements.done()) {                        newCheckpoint = new Checkpoint<>(residualElements.getRestElements(), residualSource == null ? null : residualSource.getSource());    } else if (residualSource != null) {        newCheckpoint = residualSource.getCheckpointMark();    } else {        newCheckpoint = new Checkpoint<>(null, /* residualElements */        null);    }            init(newCheckpoint.residualElements, newCheckpoint.residualSource, options);    return newCheckpoint;}
public BoundedToUnboundedSourceAdapter<T> beam_f1614_0()
{    return BoundedToUnboundedSourceAdapter.this;}
private boolean beam_f1622_0() throws IOException
{    checkArgument(!closed, "advance() call on closed %s", getClass().getName());    if (readerDone) {        return false;    }    if (reader == null) {        reader = residualSource.createReader(options);        readerDone = !reader.start();    } else {        readerDone = !reader.advance();    }    return !readerDone;}
 T beam_f1623_0() throws NoSuchElementException
{    if (reader == null) {        throw new NoSuchElementException();    }    return reader.getCurrent();}
 Instant beam_f1624_0() throws NoSuchElementException
{    if (reader == null) {        throw new NoSuchElementException();    }    return reader.getCurrentTimestamp();}
public static UnsupportedOverrideFactory<InputT, OutputT, TransformT> beam_f1633_0(String message)
{    return new UnsupportedOverrideFactory<>(message);}
public PTransformReplacement<InputT, OutputT> beam_f1634_0(AppliedPTransform<InputT, OutputT, TransformT> transform)
{    throw new UnsupportedOperationException(message);}
public Map<PValue, ReplacementOutput> beam_f1635_0(Map<TupleTag<?>, PValue> outputs, OutputT newOutput)
{    throw new UnsupportedOperationException(message);}
public static TimestampCombiner beam_f1643_0(RunnerApi.OutputTime.Enum proto)
{    switch(proto) {        case EARLIEST_IN_PANE:            return TimestampCombiner.EARLIEST;        case END_OF_WINDOW:            return TimestampCombiner.END_OF_WINDOW;        case LATEST_IN_PANE:            return TimestampCombiner.LATEST;        case UNRECOGNIZED:        default:                        throw new IllegalArgumentException(String.format("Cannot convert unknown %s to %s: %s", RunnerApi.OutputTime.class.getCanonicalName(), OutputTime.class.getCanonicalName(), proto));    }}
public static SdkFunctionSpec beam_f1644_0(WindowFn<?, ?> windowFn, SdkComponents components)
{    ByteString serializedFn = ByteString.copyFrom(SerializableUtils.serializeToByteArray(windowFn));    if (windowFn instanceof GlobalWindows) {        return SdkFunctionSpec.newBuilder().setEnvironmentId(components.getOnlyEnvironmentId()).setSpec(FunctionSpec.newBuilder().setUrn(GLOBAL_WINDOWS_URN)).build();    } else if (windowFn instanceof FixedWindows) {        FixedWindowsPayload fixedWindowsPayload = FixedWindowsPayload.newBuilder().setSize(Durations.fromMillis(((FixedWindows) windowFn).getSize().getMillis())).setOffset(Timestamps.fromMillis(((FixedWindows) windowFn).getOffset().getMillis())).build();        return SdkFunctionSpec.newBuilder().setEnvironmentId(components.getOnlyEnvironmentId()).setSpec(FunctionSpec.newBuilder().setUrn(FIXED_WINDOWS_URN).setPayload(fixedWindowsPayload.toByteString())).build();    } else if (windowFn instanceof SlidingWindows) {        SlidingWindowsPayload slidingWindowsPayload = SlidingWindowsPayload.newBuilder().setSize(Durations.fromMillis(((SlidingWindows) windowFn).getSize().getMillis())).setOffset(Timestamps.fromMillis(((SlidingWindows) windowFn).getOffset().getMillis())).setPeriod(Durations.fromMillis(((SlidingWindows) windowFn).getPeriod().getMillis())).build();        return SdkFunctionSpec.newBuilder().setEnvironmentId(components.getOnlyEnvironmentId()).setSpec(FunctionSpec.newBuilder().setUrn(SLIDING_WINDOWS_URN).setPayload(slidingWindowsPayload.toByteString())).build();    } else if (windowFn instanceof Sessions) {        SessionsPayload sessionsPayload = SessionsPayload.newBuilder().setGapSize(Durations.fromMillis(((Sessions) windowFn).getGapDuration().getMillis())).build();        return SdkFunctionSpec.newBuilder().setEnvironmentId(components.getOnlyEnvironmentId()).setSpec(FunctionSpec.newBuilder().setUrn(SESSION_WINDOWS_URN).setPayload(sessionsPayload.toByteString())).build();    } else {        return SdkFunctionSpec.newBuilder().setEnvironmentId(components.getOnlyEnvironmentId()).setSpec(FunctionSpec.newBuilder().setUrn(SERIALIZED_JAVA_WINDOWFN_URN).setPayload(serializedFn)).build();    }}
public static RunnerApi.MessageWithComponents beam_f1645_0(WindowingStrategy<?, ?> windowingStrategy, SdkComponents components) throws IOException
{    RunnerApi.WindowingStrategy windowingStrategyProto = toProto(windowingStrategy, components);    return RunnerApi.MessageWithComponents.newBuilder().setWindowingStrategy(windowingStrategyProto).setComponents(components.toComponents()).build();}
public static WindowIntoPayload beam_f1653_0(AppliedPTransform<?, ?, ?> application)
{    RunnerApi.PTransform transformProto;    try {        SdkComponents components = SdkComponents.create(application.getPipeline().getOptions());        transformProto = PTransformTranslation.toProto(application, Collections.emptyList(), components);    } catch (IOException exc) {        throw new RuntimeException(exc);    }    checkArgument(PTransformTranslation.ASSIGN_WINDOWS_TRANSFORM_URN.equals(transformProto.getSpec().getUrn()), "Illegal attempt to extract %s from transform %s with name \"%s\" and URN \"%s\"", Window.Assign.class.getSimpleName(), application.getTransform(), application.getFullName(), transformProto.getSpec().getUrn());    try {        return WindowIntoPayload.parseFrom(transformProto.getSpec().getPayload());    } catch (InvalidProtocolBufferException exc) {        throw new IllegalStateException(String.format("%s translated %s with URN '%s' but payload was not a %s", PTransformTranslation.class.getSimpleName(), application, PTransformTranslation.ASSIGN_WINDOWS_TRANSFORM_URN, WindowIntoPayload.class.getSimpleName()), exc);    }}
public static WindowFn<?, ?> beam_f1654_0(AppliedPTransform<?, ?, ?> application)
{    return WindowingStrategyTranslation.windowFnFromProto(getWindowIntoPayload(application).getWindowFn());}
public static TransformPayloadTranslator beam_f1655_0()
{    return new WindowIntoPayloadTranslator();}
public boolean beam_f1663_0()
{    return transform.getNumShardsProvider() == null && transform.getComputeNumShards() == null;}
private static SdkFunctionSpec beam_f1664_0(FileBasedSink<?, ?, ?> sink)
{    return toProto(CUSTOM_JAVA_FILE_BASED_SINK_URN, sink);}
private static SdkFunctionSpec beam_f1665_0(String urn, Serializable serializable)
{    return SdkFunctionSpec.newBuilder().setSpec(FunctionSpec.newBuilder().setUrn(urn).setPayload(ByteString.copyFrom(SerializableUtils.serializeToByteArray(serializable))).build()).build();}
public FunctionSpec beam_f1673_0(SdkComponents components) throws IOException
{    return FunctionSpec.newBuilder().setUrn(WRITE_FILES_TRANSFORM_URN).setPayload(payloadForWriteFilesLike(this, components).toByteString()).build();}
public Map<TupleTag<?>, PValue> beam_f1674_0()
{    Map<TupleTag<?>, PValue> additionalInputs = new HashMap<>();    for (Map.Entry<String, SideInput> sideInputEntry : payload.getSideInputsMap().entrySet()) {        try {            additionalInputs.put(new TupleTag<>(sideInputEntry.getKey()), rehydratedComponents.getPCollection(protoTransform.getInputsOrThrow(sideInputEntry.getKey())));        } catch (IOException exc) {            throw new IllegalStateException(String.format("Could not find input with name %s for %s transform", sideInputEntry.getKey(), WriteFiles.class.getSimpleName()));        }    }    return additionalInputs;}
public SdkFunctionSpec beam_f1675_0(SdkComponents newComponents)
{        return payload.getSink();}
public void beam_f1683_0() throws IOException
{    stager = ArtifactServiceStager.overChannel(InProcessChannelBuilder.forName("service_stager").build(), 6);    service = new InMemoryArtifactStagerService();    server = InProcessServerBuilder.forName("service_stager").directExecutor().addService(service).build().start();}
public void beam_f1684_0()
{    server.shutdownNow();}
public void beam_f1685_0() throws Exception
{    String stagingSessionToken = "token";    File file = temp.newFile();    byte[] content = "foo-bar-baz".getBytes(StandardCharsets.UTF_8);    String contentSha256 = Hashing.sha256().newHasher().putBytes(content).hash().toString();    try (FileChannel contentChannel = new FileOutputStream(file).getChannel()) {        contentChannel.write(ByteBuffer.wrap(content));    }    stager.stage(stagingSessionToken, Collections.singleton(StagedFile.of(file, file.getName())));    assertThat(service.getStagedArtifacts().entrySet(), hasSize(1));    byte[] stagedContent = Iterables.getOnlyElement(service.getStagedArtifacts().values());    assertThat(stagedContent, equalTo(content));    ArtifactMetadata staged = service.getManifest().getArtifact(0);    assertThat(staged.getName(), equalTo(file.getName()));    String manifestSha256 = staged.getSha256();    assertThat(contentSha256, equalTo(manifestSha256));    assertThat(service.getManifest().getArtifactCount(), equalTo(1));    assertThat(staged, equalTo(Iterables.getOnlyElement(service.getStagedArtifacts().keySet())));}
public void beam_f1694_0() throws Exception
{    PCollection<Integer> input = pipeline.apply(Create.of(1, 2, 3));    input.apply(Combine.globally(combineFn));    final AtomicReference<AppliedPTransform<?, ?, Combine.Globally<?, ?>>> combine = new AtomicReference<>();    pipeline.traverseTopologically(new PipelineVisitor.Defaults() {        @Override        public void leaveCompositeTransform(Node node) {            if (node.getTransform() instanceof Combine.Globally) {                checkState(combine.get() == null);                combine.set((AppliedPTransform) node.toAppliedPTransform(getPipeline()));            }        }    });    checkState(combine.get() != null);    assertEquals(combineFn, combine.get().getTransform().getFn());    SdkComponents sdkComponents = SdkComponents.create();    sdkComponents.registerEnvironment(Environments.createDockerEnvironment("java"));    CombinePayload combineProto = CombineTranslation.CombineGloballyPayloadTranslator.payloadForCombineGlobally((AppliedPTransform) combine.get(), sdkComponents);    RunnerApi.Components componentsProto = sdkComponents.toComponents();    assertEquals(combineFn.getAccumulatorCoder(pipeline.getCoderRegistry(), input.getCoder()), getAccumulatorCoder(combineProto, RehydratedComponents.forComponents(componentsProto)));    assertEquals(combineFn, SerializableUtils.deserializeFromByteArray(combineProto.getCombineFn().getSpec().getPayload().toByteArray(), "CombineFn"));}
public void beam_f1695_0(Node node)
{    if (node.getTransform() instanceof Combine.Globally) {        checkState(combine.get() == null);        combine.set((AppliedPTransform) node.toAppliedPTransform(getPipeline()));    }}
public void beam_f1696_0() throws Exception
{    PCollection<Integer> input = pipeline.apply(Create.of(1, 2, 3));    CombineFnWithContext<Integer, int[], Integer> combineFn = new TestCombineFnWithContext();    input.apply(Combine.globally(combineFn).withoutDefaults());    final AtomicReference<AppliedPTransform<?, ?, Combine.Globally<?, ?>>> combine = new AtomicReference<>();    pipeline.traverseTopologically(new PipelineVisitor.Defaults() {        @Override        public void leaveCompositeTransform(Node node) {            if (node.getTransform() instanceof Combine.Globally) {                checkState(combine.get() == null);                combine.set((AppliedPTransform) node.toAppliedPTransform(getPipeline()));            }        }    });    checkState(combine.get() != null);    assertEquals(combineFn, combine.get().getTransform().getFn());    SdkComponents sdkComponents = SdkComponents.create();    sdkComponents.registerEnvironment(Environments.createDockerEnvironment("java"));    CombinePayload combineProto = CombineTranslation.CombineGloballyPayloadTranslator.payloadForCombineGlobally((AppliedPTransform) combine.get(), sdkComponents);    RunnerApi.Components componentsProto = sdkComponents.toComponents();    assertEquals(combineFn.getAccumulatorCoder(pipeline.getCoderRegistry(), input.getCoder()), getAccumulatorCoder(combineProto, RehydratedComponents.forComponents(componentsProto)));    assertEquals(combineFn, SerializableUtils.deserializeFromByteArray(combineProto.getCombineFn().getSpec().getPayload().toByteArray(), "CombineFn"));}
public Void beam_f1704_0(Void accumulator)
{    return accumulator;}
public Void beam_f1705_0(Iterable<Void> accumulators)
{    return null;}
public Void beam_f1706_0(Void accumulator, Integer input)
{    return accumulator;}
public int beam_f1714_0()
{    return TestCombineFnWithContext.class.hashCode();}
 static CommonCoder beam_f1715_0(@JsonProperty("urn") String urn, @JsonProperty("components") @Nullable List<CommonCoder> components, @JsonProperty("non_deterministic") @Nullable Boolean nonDeterministic)
{    return new AutoValue_CommonCoderTest_CommonCoder(checkNotNull(urn, "urn"), firstNonNull(components, Collections.emptyList()), firstNonNull(nonDeterministic, Boolean.FALSE));}
 static CommonCoderTestSpec beam_f1716_0(@JsonProperty("coder") CommonCoder coder, @JsonProperty("nested") @Nullable Boolean nested, @JsonProperty("examples") Map<String, Object> examples)
{    return new AutoValue_CommonCoderTest_CommonCoderTestSpec(coder, nested, examples);}
public void beam_f1724_0() throws IOException
{    assertCoderIsKnown(testSpec.getCoder());    Coder coder = instantiateCoder(testSpec.getCoder());    Object testValue = convertValue(testSpec.getValue(), testSpec.getCoder(), coder);    Context context = testSpec.getNested() ? Context.NESTED : Context.OUTER;    byte[] encoded = CoderUtils.encodeToByteArray(coder, testValue, context);    Object decodedValue = CoderUtils.decodeFromByteArray(coder, testSpec.getSerialized(), context);    if (!testSpec.getCoder().getNonDeterministic()) {        assertThat(testSpec.toString(), encoded, equalTo(testSpec.getSerialized()));    }    verifyDecodedValue(testSpec.getCoder(), decodedValue, testValue);}
private void beam_f1725_0(CommonCoder coder, Object expectedValue, Object actualValue)
{    String s = coder.getUrn();    if (s.equals(getUrn(StandardCoders.Enum.BYTES))) {        assertThat(expectedValue, equalTo(actualValue));    } else if (s.equals(getUrn(StandardCoders.Enum.STRING_UTF8))) {        assertEquals(expectedValue, actualValue);    } else if (s.equals(getUrn(StandardCoders.Enum.KV))) {        assertThat(actualValue, instanceOf(KV.class));        verifyDecodedValue(coder.getComponents().get(0), ((KV) expectedValue).getKey(), ((KV) actualValue).getKey());        verifyDecodedValue(coder.getComponents().get(0), ((KV) expectedValue).getValue(), ((KV) actualValue).getValue());    } else if (s.equals(getUrn(StandardCoders.Enum.VARINT))) {        assertEquals(expectedValue, actualValue);    } else if (s.equals(getUrn(StandardCoders.Enum.INTERVAL_WINDOW))) {        assertEquals(expectedValue, actualValue);    } else if (s.equals(getUrn(StandardCoders.Enum.ITERABLE))) {        assertThat(actualValue, instanceOf(Iterable.class));        CommonCoder componentCoder = coder.getComponents().get(0);        Iterator<Object> expectedValueIterator = ((Iterable<Object>) expectedValue).iterator();        for (Object value : (Iterable<Object>) actualValue) {            verifyDecodedValue(componentCoder, expectedValueIterator.next(), value);        }        assertFalse(expectedValueIterator.hasNext());    } else if (s.equals(getUrn(StandardCoders.Enum.TIMER))) {        assertEquals(((Timer) expectedValue).getTimestamp(), ((Timer) actualValue).getTimestamp());        assertThat(((Timer) expectedValue).getPayload(), equalTo(((Timer) actualValue).getPayload()));    } else if (s.equals(getUrn(StandardCoders.Enum.GLOBAL_WINDOW))) {        assertEquals(expectedValue, actualValue);    } else if (s.equals(getUrn(StandardCoders.Enum.WINDOWED_VALUE))) {        assertEquals(expectedValue, actualValue);    } else if (s.equals(getUrn(StandardCoders.Enum.DOUBLE))) {        assertEquals(expectedValue, actualValue);    } else {        throw new IllegalStateException("Unknown coder URN: " + coder.getUrn());    }}
private static String beam_f1726_0(Coder<T> coder, T value, Context context) throws CoderException
{    byte[] bytes = CoderUtils.encodeToByteArray(coder, value, context);    ObjectMapper mapper = new ObjectMapper();    mapper.configure(JsonGenerator.Feature.ESCAPE_NON_ASCII, true);    try {        return mapper.writeValueAsString(new String(bytes, StandardCharsets.ISO_8859_1));    } catch (JsonProcessingException e) {        throw new CoderException(String.format("Unable to encode %s with coder %s", value, coder), e);    }}
public void beam_f1734_0()
{    PTransformReplacement<PCollectionList<Long>, PCollection<Long>> replacement = factory.getReplacementTransform(AppliedPTransform.of("nonEmptyInput", Collections.emptyMap(), Collections.emptyMap(), Flatten.pCollections(), pipeline));    assertThat(replacement.getInput().getAll(), emptyIterable());}
public void beam_f1735_0()
{    PCollectionList<Long> nonEmpty = PCollectionList.of(pipeline.apply("unbounded", GenerateSequence.from(0))).and(pipeline.apply("bounded", GenerateSequence.from(0).to(100)));    thrown.expect(IllegalArgumentException.class);    thrown.expectMessage(nonEmpty.expand().toString());    thrown.expectMessage(EmptyFlattenAsCreateFactory.class.getSimpleName());    factory.getReplacementTransform(AppliedPTransform.of("nonEmptyInput", nonEmpty.expand(), Collections.emptyMap(), Flatten.pCollections(), pipeline));}
public void beam_f1736_0()
{    PCollection<Long> original = pipeline.apply("Original", GenerateSequence.from(0));    PCollection<Long> replacement = pipeline.apply("Replacement", GenerateSequence.from(0));    Map<PValue, ReplacementOutput> mapping = factory.mapOutputs(original.expand(), replacement);    assertThat(mapping, Matchers.hasEntry(replacement, ReplacementOutput.of(TaggedPValue.ofExpandedValue(original), TaggedPValue.ofExpandedValue(replacement))));}
public boolean beam_f1745_0(WindowFn<?, ?> other)
{    return false;}
public Coder<BoundedWindow> beam_f1746_0()
{    return null;}
public void beam_f1747_0() throws IOException
{    SdkComponents components = SdkComponents.create();    components.registerEnvironment(Environments.createDockerEnvironment("java"));    ReadPayload payload = ReadTranslation.toProto(Read.from(CountingSource.upTo(10)), components);    RehydratedComponents rehydratedComponents = RehydratedComponents.forComponents(components.toComponents());    PTransform builder = PTransform.newBuilder().setSpec(FunctionSpec.newBuilder().setUrn(PTransformTranslation.COMBINE_PER_KEY_TRANSFORM_URN).setPayload(payload.toByteString()).build()).build();    Environment env = Environments.getEnvironment(builder, rehydratedComponents).get();    assertThat(env, equalTo(components.toComponents().getEnvironmentsOrThrow(payload.getSource().getEnvironmentId())));}
public void beam_f1758_0()
{    Pipeline p = Pipeline.create();    p.apply(Impulse.create());    RunnerApi.Pipeline pipelineProto = PipelineTranslation.toProto(p);    String inputPcollId = Iterables.getOnlyElement(Iterables.getOnlyElement(pipelineProto.getComponents().getTransformsMap().values()).getOutputsMap().values());    ExpansionApi.ExpansionRequest request = ExpansionApi.ExpansionRequest.newBuilder().setComponents(pipelineProto.getComponents()).setTransform(RunnerApi.PTransform.newBuilder().setUniqueName(TEST_NAME).setSpec(RunnerApi.FunctionSpec.newBuilder().setUrn(TEST_URN)).putInputs("input", inputPcollId)).setNamespace(TEST_NAMESPACE).build();    ExpansionApi.ExpansionResponse response = expansionService.expand(request);    RunnerApi.PTransform expandedTransform = response.getTransform();    assertEquals(TEST_NAMESPACE + TEST_NAME, expandedTransform.getUniqueName());        assertEquals(inputPcollId, Iterables.getOnlyElement(expandedTransform.getInputsMap().values()));        assertNotEquals(expandedTransform.getSubtransformsCount(), 0);    for (String subtransform : expandedTransform.getSubtransformsList()) {        assertTrue(response.getComponents().containsTransforms(subtransform));    }        Set<String> originalIds = allIds(request.getComponents());    for (String id : allIds(response.getComponents())) {        assertTrue(id, id.startsWith(TEST_NAMESPACE) || originalIds.contains(id));    }}
public void beam_f1759_0()
{    ExternalTransforms.ExternalConfigurationPayload payload = ExternalTransforms.ExternalConfigurationPayload.newBuilder().putConfiguration("start", ExternalTransforms.ConfigValue.newBuilder().addCoderUrn(BeamUrns.getUrn(RunnerApi.StandardCoders.Enum.VARINT)).setPayload(ByteString.copyFrom(new byte[] { 0 })).build()).putConfiguration("stop", ExternalTransforms.ConfigValue.newBuilder().addCoderUrn(BeamUrns.getUrn(RunnerApi.StandardCoders.Enum.VARINT)).setPayload(ByteString.copyFrom(new byte[] { 1 })).build()).build();    Pipeline p = Pipeline.create();    RunnerApi.Pipeline pipelineProto = PipelineTranslation.toProto(p);    ExpansionApi.ExpansionRequest request = ExpansionApi.ExpansionRequest.newBuilder().setComponents(pipelineProto.getComponents()).setTransform(RunnerApi.PTransform.newBuilder().setUniqueName(TEST_NAME).setSpec(RunnerApi.FunctionSpec.newBuilder().setUrn(GenerateSequence.External.URN).setPayload(payload.toByteString()))).setNamespace(TEST_NAMESPACE).build();    ExpansionApi.ExpansionResponse response = expansionService.expand(request);    RunnerApi.PTransform expandedTransform = response.getTransform();    assertEquals(TEST_NAMESPACE + TEST_NAME, expandedTransform.getUniqueName());    assertThat(expandedTransform.getInputsCount(), Matchers.is(0));    assertThat(expandedTransform.getOutputsCount(), Matchers.is(1));    assertThat(expandedTransform.getSubtransformsCount(), Matchers.greaterThan(0));}
public void beam_f1760_0() throws Exception
{    ExternalTransforms.ExternalConfigurationPayload.Builder builder = ExternalTransforms.ExternalConfigurationPayload.newBuilder();    builder.putConfiguration("config_key1", ExternalTransforms.ConfigValue.newBuilder().addCoderUrn(BeamUrns.getUrn(RunnerApi.StandardCoders.Enum.VARINT)).setPayload(ByteString.copyFrom(new byte[] { 1 })).build());    List<byte[]> byteList = ImmutableList.of("testing", "compound", "coders").stream().map(str -> str.getBytes(Charsets.UTF_8)).collect(Collectors.toList());    IterableCoder<byte[]> compoundCoder = IterableCoder.of(ByteArrayCoder.of());    ByteArrayOutputStream baos = new ByteArrayOutputStream();    compoundCoder.encode(byteList, baos);    builder.putConfiguration("config_key2", ExternalTransforms.ConfigValue.newBuilder().addCoderUrn(BeamUrns.getUrn(RunnerApi.StandardCoders.Enum.ITERABLE)).addCoderUrn(BeamUrns.getUrn(RunnerApi.StandardCoders.Enum.BYTES)).setPayload(ByteString.copyFrom(baos.toByteArray())).build());    List<KV<byte[], Long>> byteKvList = ImmutableList.of("testing", "compound", "coders").stream().map(str -> KV.of(str.getBytes(Charsets.UTF_8), (long) str.length())).collect(Collectors.toList());    IterableCoder<KV<byte[], Long>> compoundCoder2 = IterableCoder.of(KvCoder.of(ByteArrayCoder.of(), VarLongCoder.of()));    ByteArrayOutputStream baos2 = new ByteArrayOutputStream();    compoundCoder2.encode(byteKvList, baos2);    builder.putConfiguration("config_key3", ExternalTransforms.ConfigValue.newBuilder().addCoderUrn(BeamUrns.getUrn(RunnerApi.StandardCoders.Enum.ITERABLE)).addCoderUrn(BeamUrns.getUrn(RunnerApi.StandardCoders.Enum.KV)).addCoderUrn(BeamUrns.getUrn(RunnerApi.StandardCoders.Enum.BYTES)).addCoderUrn(BeamUrns.getUrn(RunnerApi.StandardCoders.Enum.VARINT)).setPayload(ByteString.copyFrom(baos2.toByteArray())).build());    List<KV<List<Long>, byte[]>> byteKvListWithListKey = ImmutableList.of("testing", "compound", "coders").stream().map(str -> KV.of(Collections.singletonList((long) str.length()), str.getBytes(Charsets.UTF_8))).collect(Collectors.toList());    Coder compoundCoder3 = IterableCoder.of(KvCoder.of(IterableCoder.of(VarLongCoder.of()), ByteArrayCoder.of()));    ByteArrayOutputStream baos3 = new ByteArrayOutputStream();    compoundCoder3.encode(byteKvListWithListKey, baos3);    builder.putConfiguration("config_key4", ExternalTransforms.ConfigValue.newBuilder().addCoderUrn(BeamUrns.getUrn(RunnerApi.StandardCoders.Enum.ITERABLE)).addCoderUrn(BeamUrns.getUrn(RunnerApi.StandardCoders.Enum.KV)).addCoderUrn(BeamUrns.getUrn(RunnerApi.StandardCoders.Enum.ITERABLE)).addCoderUrn(BeamUrns.getUrn(RunnerApi.StandardCoders.Enum.VARINT)).addCoderUrn(BeamUrns.getUrn(RunnerApi.StandardCoders.Enum.BYTES)).setPayload(ByteString.copyFrom(baos3.toByteArray())).build());    ExternalTransforms.ExternalConfigurationPayload externalConfig = builder.build();    TestConfig config = new TestConfig();    ExpansionService.ExternalTransformRegistrarLoader.populateConfiguration(config, externalConfig);    assertThat(config.configKey1, Matchers.is(1L));    assertArrayEquals(Iterables.toArray(config.configKey2, byte[].class), byteList.toArray());    assertArrayEquals(Iterables.toArray(config.configKey3, KV.class), byteKvList.toArray());    assertArrayEquals(Iterables.toArray(config.configKey4, KV.class), byteKvListWithListKey.toArray());}
public void beam_f1768_0()
{    PCollection<String> col = testPipeline.apply(Create.of("1", "2", "3")).apply(External.of(TEST_URN_SIMPLE, new byte[] {}, localExpansionAddr));    PAssert.that(col).containsInAnyOrder("Simple(1)", "Simple(2)", "Simple(3)");    testPipeline.run();}
public void beam_f1769_0()
{    PCollection<String> pcol = testPipeline.apply(Create.of(1, 2, 3, 4, 5, 6)).apply("filter <=3", External.of(TEST_URN_LE, "3".getBytes(StandardCharsets.UTF_8), localExpansionAddr)).apply(MapElements.into(TypeDescriptors.strings()).via(Object::toString)).apply("put simple", External.of(TEST_URN_SIMPLE, new byte[] {}, localExpansionAddr));    PAssert.that(pcol).containsInAnyOrder("Simple(1)", "Simple(2)", "Simple(3)");    testPipeline.run();}
public void beam_f1770_0()
{    PCollectionTuple pTuple = testPipeline.apply(Create.of(1, 2, 3, 4, 5, 6)).apply(External.of(TEST_URN_MULTI, new byte[] {}, localExpansionAddr).withMultiOutputs());    PAssert.that(pTuple.get(new TupleTag<Integer>("even") {    })).containsInAnyOrder(2, 4, 6);    PAssert.that(pTuple.get(new TupleTag<Integer>("odd") {    })).containsInAnyOrder(1, 3, 5);    testPipeline.run();}
public void beam_f1778_0()
{    Map<TupleTag<?>, PValue> additionalInputs = ImmutableMap.of(new TupleTag<>("test_tag"), Pipeline.create().apply(Create.of("1")));    when(delegate.getAdditionalInputs()).thenReturn(additionalInputs);    assertThat(forwarding.getAdditionalInputs(), equalTo(additionalInputs));}
public void beam_f1779_0()
{    @SuppressWarnings("unchecked")    PipelineOptions options = mock(PipelineOptions.class);    doThrow(RuntimeException.class).when(delegate).validate(options);    thrown.expect(RuntimeException.class);    forwarding.validate(options);}
public void beam_f1780_0() throws Exception
{    @SuppressWarnings("unchecked")    PCollection<Integer> input = PCollection.createPrimitiveOutputInternal(null, /* pipeline */    WindowingStrategy.globalDefault(), PCollection.IsBounded.BOUNDED, null);    @SuppressWarnings("unchecked")    PCollection<String> output = PCollection.createPrimitiveOutputInternal(null, /* pipeline */    WindowingStrategy.globalDefault(), PCollection.IsBounded.BOUNDED, null);    @SuppressWarnings("unchecked")    Coder<String> outputCoder = mock(Coder.class);    when(delegate.expand(input)).thenReturn(output);    when(delegate.getDefaultOutputCoder(input, output)).thenReturn(outputCoder);    assertThat(forwarding.expand(input).getCoder(), equalTo(outputCoder));}
public ExecutableStageMatcher beam_f1788_0(String... transforms)
{    return new ExecutableStageMatcher(inputPCollectionId, sideInputIds, materializedPCollection, ImmutableList.copyOf(transforms));}
protected boolean beam_f1789_0(ExecutableStage item)
{    return item.getInputPCollection().getId().equals(inputPCollectionId) && containsInAnyOrder(sideInputIds.toArray()).matches(item.getSideInputs().stream().map(ref -> SideInputId.newBuilder().setTransformId(ref.transform().getId()).setLocalName(ref.localName()).build()).collect(Collectors.toSet())) && materializedPCollection.matches(item.getOutputPCollections().stream().map(PCollectionNode::getId).collect(Collectors.toSet())) && containsInAnyOrder(fusedTransforms.toArray(new String[0])).matches(item.getTransforms().stream().map(PTransformNode::getId).collect(Collectors.toSet()));}
public void beam_f1790_0(Description description)
{    description.appendText(String.format("An %s with input %s ", ExecutableStage.class.getSimpleName(), PCollection.class.getSimpleName())).appendText(inputPCollectionId).appendText(String.format(", output %ss ", PCollection.class.getSimpleName())).appendDescriptionOf(materializedPCollection).appendText(String.format(" and fused %ss ", PTransform.class.getSimpleName())).appendValueList("[", ", ", "]", fusedTransforms);}
public void beam_f1798_0()
{    Components components = partialComponents.toBuilder().putTransforms("mystery", PTransform.newBuilder().setSpec(FunctionSpec.newBuilder().setUrn(PTransformTranslation.PAR_DO_TRANSFORM_URN)).setUniqueName("Mystery").putInputs("input", "impulse.out").putOutputs("output", "mystery.out").build()).putPcollections("mystery.out", pc("mystery.out")).putTransforms("enigma", PTransform.newBuilder().setSpec(FunctionSpec.newBuilder().setUrn(PTransformTranslation.PAR_DO_TRANSFORM_URN)).setUniqueName("Enigma").putInputs("input", "impulse.out").putOutputs("output", "enigma.out").build()).putPcollections("enigma.out", pc("enigma.out")).build();    FusedPipeline fused = GreedyPipelineFuser.fuse(Pipeline.newBuilder().setComponents(components).build());    assertThat(fused.getRunnerExecutedTransforms(), containsInAnyOrder(PipelineNode.pTransform("impulse", components.getTransformsOrThrow("impulse")), PipelineNode.pTransform("mystery", components.getTransformsOrThrow("mystery")), PipelineNode.pTransform("enigma", components.getTransformsOrThrow("enigma"))));    assertThat(fused.getFusedStages(), emptyIterable());}
public void beam_f1799_0()
{    Components components = partialComponents.toBuilder().putTransforms("read", PTransform.newBuilder().setUniqueName("Read").putInputs("input", "impulse.out").putOutputs("output", "read.out").setSpec(FunctionSpec.newBuilder().setUrn(PTransformTranslation.PAR_DO_TRANSFORM_URN).setPayload(ParDoPayload.newBuilder().setDoFn(SdkFunctionSpec.newBuilder().setEnvironmentId("py")).build().toByteString())).build()).putPcollections("read.out", pc("read.out")).putTransforms("groupByKey", PTransform.newBuilder().setUniqueName("GroupByKey").putInputs("input", "read.out").putOutputs("output", "groupByKey.out").setSpec(FunctionSpec.newBuilder().setUrn(PTransformTranslation.GROUP_BY_KEY_TRANSFORM_URN)).build()).putPcollections("groupByKey.out", pc("groupByKey.out")).putTransforms("parDo", PTransform.newBuilder().setUniqueName("ParDo").putInputs("input", "groupByKey.out").putOutputs("output", "parDo.out").setSpec(FunctionSpec.newBuilder().setUrn(PTransformTranslation.PAR_DO_TRANSFORM_URN).setPayload(ParDoPayload.newBuilder().setDoFn(SdkFunctionSpec.newBuilder().setEnvironmentId("py")).build().toByteString())).build()).putPcollections("parDo.out", pc("parDo.out")).build();    FusedPipeline fused = GreedyPipelineFuser.fuse(Pipeline.newBuilder().setComponents(components).build());    assertThat(fused.getRunnerExecutedTransforms(), containsInAnyOrder(PipelineNode.pTransform("impulse", components.getTransformsOrThrow("impulse")), PipelineNode.pTransform("groupByKey", components.getTransformsOrThrow("groupByKey"))));    assertThat(fused.getFusedStages(), containsInAnyOrder(ExecutableStageMatcher.withInput("impulse.out").withOutputs("read.out").withTransforms("read"), ExecutableStageMatcher.withInput("groupByKey.out").withNoOutputs().withTransforms("parDo")));}
public void beam_f1800_0()
{    Components components = partialComponents.toBuilder().putTransforms("read", PTransform.newBuilder().setUniqueName("Read").putInputs("input", "impulse.out").putOutputs("output", "read.out").setSpec(FunctionSpec.newBuilder().setUrn(PTransformTranslation.PAR_DO_TRANSFORM_URN).setPayload(ParDoPayload.newBuilder().setDoFn(SdkFunctionSpec.newBuilder().setEnvironmentId("py")).build().toByteString())).build()).putPcollections("read.out", pc("read.out")).putTransforms("goTransform", PTransform.newBuilder().setUniqueName("GoTransform").putInputs("input", "read.out").putOutputs("output", "go.out").setSpec(FunctionSpec.newBuilder().setUrn(PTransformTranslation.PAR_DO_TRANSFORM_URN).setPayload(ParDoPayload.newBuilder().setDoFn(SdkFunctionSpec.newBuilder().setEnvironmentId("go")).build().toByteString())).build()).putPcollections("go.out", pc("go.out")).putTransforms("pyTransform", PTransform.newBuilder().setUniqueName("PyTransform").putInputs("input", "read.out").putOutputs("output", "py.out").setSpec(FunctionSpec.newBuilder().setUrn(PTransformTranslation.ASSIGN_WINDOWS_TRANSFORM_URN).setPayload(WindowIntoPayload.newBuilder().setWindowFn(SdkFunctionSpec.newBuilder().setEnvironmentId("py")).build().toByteString())).build()).putPcollections("py.out", pc("py.out")).build();    FusedPipeline fused = GreedyPipelineFuser.fuse(Pipeline.newBuilder().setComponents(components).build());        assertThat(fused.getRunnerExecutedTransforms(), hasSize(1));    assertThat(fused.getFusedStages(), hasSize(3));    assertThat(fused.getFusedStages(), containsInAnyOrder(ExecutableStageMatcher.withInput("impulse.out").withOutputs("read.out").withTransforms("read"), ExecutableStageMatcher.withInput("read.out").withNoOutputs().withTransforms("pyTransform"), ExecutableStageMatcher.withInput("read.out").withNoOutputs().withTransforms("goTransform")));}
public void beam_f1808_0()
{    Components components = partialComponents.toBuilder().putTransforms("read", PTransform.newBuilder().setUniqueName("Read").putInputs("input", "impulse.out").putOutputs("output", "read.out").setSpec(FunctionSpec.newBuilder().setUrn(PTransformTranslation.PAR_DO_TRANSFORM_URN).setPayload(ParDoPayload.newBuilder().setDoFn(SdkFunctionSpec.newBuilder().setEnvironmentId("py")).build().toByteString())).build()).putPcollections("read.out", pc("read.out")).putTransforms("goTransform", PTransform.newBuilder().setUniqueName("GoTransform").putInputs("input", "read.out").putOutputs("output", "go.out").setSpec(FunctionSpec.newBuilder().setUrn(PTransformTranslation.PAR_DO_TRANSFORM_URN).setPayload(ParDoPayload.newBuilder().setDoFn(SdkFunctionSpec.newBuilder().setEnvironmentId("go")).build().toByteString())).build()).putPcollections("go.out", pc("go.out")).putTransforms("pyTransform", PTransform.newBuilder().setUniqueName("PyTransform").putInputs("input", "read.out").putOutputs("output", "py.out").setSpec(FunctionSpec.newBuilder().setUrn(PTransformTranslation.ASSIGN_WINDOWS_TRANSFORM_URN).setPayload(WindowIntoPayload.newBuilder().setWindowFn(SdkFunctionSpec.newBuilder().setEnvironmentId("py")).build().toByteString())).build()).putPcollections("py.out", pc("py.out")).putTransforms("compositeMultiLang", PTransform.newBuilder().setUniqueName("CompositeMultiLang").putInputs("input", "impulse.out").putOutputs("pyOut", "py.out").putOutputs("goOut", "go.out").addSubtransforms("read").addSubtransforms("goTransform").addSubtransforms("pyTransform").build()).build();    FusedPipeline fused = GreedyPipelineFuser.fuse(Pipeline.newBuilder().addRootTransformIds("impulse").addRootTransformIds("compositeMultiLang").setComponents(components).build());        assertThat(fused.getRunnerExecutedTransforms(), hasSize(1));    assertThat(fused.getFusedStages(), hasSize(3));    assertThat(fused.getFusedStages(), containsInAnyOrder(ExecutableStageMatcher.withInput("impulse.out").withOutputs("read.out").withTransforms("read"), ExecutableStageMatcher.withInput("read.out").withNoOutputs().withTransforms("pyTransform"), ExecutableStageMatcher.withInput("read.out").withNoOutputs().withTransforms("goTransform")));}
public void beam_f1809_0() throws Exception
{    PCollection flattenOutput = pc("flatten.out");    PCollection read1Output = pc("read1.out");    PCollection read2Output = pc("read2.out");    PCollection impulse1Output = pc("impulse1.out");    PCollection impulse2Output = pc("impulse2.out");    PTransform flattenTransform = PTransform.newBuilder().setUniqueName("Flatten").putInputs(read1Output.getUniqueName(), read1Output.getUniqueName()).putInputs(read2Output.getUniqueName(), read2Output.getUniqueName()).putOutputs(flattenOutput.getUniqueName(), flattenOutput.getUniqueName()).setSpec(FunctionSpec.newBuilder().setUrn(PTransformTranslation.FLATTEN_TRANSFORM_URN).setPayload(WindowIntoPayload.newBuilder().setWindowFn(SdkFunctionSpec.newBuilder().setEnvironmentId("py")).build().toByteString())).build();    PTransform read1Transform = PTransform.newBuilder().setUniqueName("read1").putInputs(impulse1Output.getUniqueName(), impulse1Output.getUniqueName()).putOutputs(read1Output.getUniqueName(), read1Output.getUniqueName()).setSpec(FunctionSpec.newBuilder().setUrn(PTransformTranslation.PAR_DO_TRANSFORM_URN).setPayload(WindowIntoPayload.newBuilder().setWindowFn(SdkFunctionSpec.newBuilder().setEnvironmentId("py")).build().toByteString())).build();    PTransform read2Transform = PTransform.newBuilder().setUniqueName("read2").putInputs(impulse2Output.getUniqueName(), impulse2Output.getUniqueName()).putOutputs(read2Output.getUniqueName(), read2Output.getUniqueName()).setSpec(FunctionSpec.newBuilder().setUrn(PTransformTranslation.PAR_DO_TRANSFORM_URN).setPayload(WindowIntoPayload.newBuilder().setWindowFn(SdkFunctionSpec.newBuilder().setEnvironmentId("py")).build().toByteString())).build();    PTransform impulse1Transform = PTransform.newBuilder().setUniqueName("impulse1").putOutputs(impulse1Output.getUniqueName(), impulse1Output.getUniqueName()).setSpec(FunctionSpec.newBuilder().setUrn(PTransformTranslation.IMPULSE_TRANSFORM_URN).setPayload(WindowIntoPayload.newBuilder().setWindowFn(SdkFunctionSpec.newBuilder().setEnvironmentId("py")).build().toByteString())).build();    PTransform impulse2Transform = PTransform.newBuilder().setUniqueName("impulse2").putOutputs(impulse2Output.getUniqueName(), impulse2Output.getUniqueName()).setSpec(FunctionSpec.newBuilder().setUrn(PTransformTranslation.IMPULSE_TRANSFORM_URN).setPayload(WindowIntoPayload.newBuilder().setWindowFn(SdkFunctionSpec.newBuilder().setEnvironmentId("py")).build().toByteString())).build();    Pipeline impulse = Pipeline.newBuilder().addRootTransformIds(impulse1Transform.getUniqueName()).addRootTransformIds(impulse2Transform.getUniqueName()).addRootTransformIds(flattenTransform.getUniqueName()).setComponents(Components.newBuilder().putCoders("coder", Coder.newBuilder().build()).putCoders("windowCoder", Coder.newBuilder().build()).putWindowingStrategies("ws", WindowingStrategy.newBuilder().setWindowCoderId("windowCoder").build()).putEnvironments("py", Environments.createDockerEnvironment("py")).putPcollections(flattenOutput.getUniqueName(), flattenOutput).putTransforms(flattenTransform.getUniqueName(), flattenTransform).putPcollections(read1Output.getUniqueName(), read1Output).putTransforms(read1Transform.getUniqueName(), read1Transform).putPcollections(read2Output.getUniqueName(), read2Output).putTransforms(read2Transform.getUniqueName(), read2Transform).putPcollections(impulse1Output.getUniqueName(), impulse1Output).putTransforms(impulse1Transform.getUniqueName(), impulse1Transform).putPcollections(impulse2Output.getUniqueName(), impulse2Output).putTransforms(impulse2Transform.getUniqueName(), impulse2Transform).build()).build();    FusedPipeline fused = GreedyPipelineFuser.fuse(impulse);    assertThat(fused.getRunnerExecutedTransforms(), hasSize(2));    assertThat(fused.getFusedStages(), hasSize(2));    assertThat(fused.getFusedStages(), containsInAnyOrder(ExecutableStageMatcher.withInput(impulse1Output.getUniqueName()).withTransforms(flattenTransform.getUniqueName(), read1Transform.getUniqueName()), ExecutableStageMatcher.withInput(impulse2Output.getUniqueName()).withTransforms(flattenTransform.getUniqueName(), read2Transform.getUniqueName())));    assertThat(fused.getFusedStages().stream().flatMap(s -> s.getComponents().getTransformsOrThrow(flattenTransform.getUniqueName()).getInputsMap().values().stream()).collect(Collectors.toList()), containsInAnyOrder(read1Output.getUniqueName(), read2Output.getUniqueName()));}
public void beam_f1810_0()
{    partialComponents = Components.newBuilder().putTransforms("impulse", PTransform.newBuilder().putOutputs("output", "impulse.out").setSpec(FunctionSpec.newBuilder().setUrn(PTransformTranslation.IMPULSE_TRANSFORM_URN)).build()).putPcollections("impulse.out", impulseDotOut).build();}
public void beam_f1818_0()
{                                PTransform readTransform = PTransform.newBuilder().putInputs("input", "impulse.out").putOutputs("output", "read.out").setSpec(FunctionSpec.newBuilder().setUrn(PTransformTranslation.PAR_DO_TRANSFORM_URN).setPayload(ParDoPayload.newBuilder().setDoFn(SdkFunctionSpec.newBuilder().setEnvironmentId("common")).build().toByteString())).build();    PTransform otherEnvRead = PTransform.newBuilder().putInputs("impulse", "impulse.out").putOutputs("output", "envRead.out").setSpec(FunctionSpec.newBuilder().setUrn(PTransformTranslation.PAR_DO_TRANSFORM_URN).setPayload(ParDoPayload.newBuilder().setDoFn(SdkFunctionSpec.newBuilder().setEnvironmentId("rare")).build().toByteString())).build();    PTransform flattenTransform = PTransform.newBuilder().putInputs("readInput", "read.out").putInputs("otherEnvInput", "envRead.out").putOutputs("output", "flatten.out").setSpec(FunctionSpec.newBuilder().setUrn(PTransformTranslation.FLATTEN_TRANSFORM_URN)).build();    PTransform windowTransform = PTransform.newBuilder().putInputs("input", "flatten.out").putOutputs("output", "window.out").setSpec(FunctionSpec.newBuilder().setUrn(PTransformTranslation.ASSIGN_WINDOWS_TRANSFORM_URN).setPayload(WindowIntoPayload.newBuilder().setWindowFn(SdkFunctionSpec.newBuilder().setEnvironmentId("common")).build().toByteString())).build();    Components components = partialComponents.toBuilder().putTransforms("read", readTransform).putPcollections("read.out", PCollection.newBuilder().setUniqueName("read.out").build()).putTransforms("envRead", otherEnvRead).putPcollections("envRead.out", PCollection.newBuilder().setUniqueName("envRead.out").build()).putTransforms("flatten", flattenTransform).putPcollections("flatten.out", PCollection.newBuilder().setUniqueName("flatten.out").build()).putTransforms("window", windowTransform).putPcollections("window.out", PCollection.newBuilder().setUniqueName("window.out").build()).putEnvironments("common", Environments.createDockerEnvironment("common")).putEnvironments("rare", Environments.createDockerEnvironment("rare")).build();    QueryablePipeline p = QueryablePipeline.forPrimitivesIn(components);    ExecutableStage subgraph = GreedyStageFuser.forGrpcPortRead(p, impulseOutputNode, ImmutableSet.of(PipelineNode.pTransform("read", readTransform)));    assertThat(subgraph.getOutputPCollections(), emptyIterable());    assertThat(subgraph, hasSubtransforms("read", "flatten", "window"));            ExecutableStage readFromOtherEnv = GreedyStageFuser.forGrpcPortRead(p, impulseOutputNode, ImmutableSet.of(PipelineNode.pTransform("envRead", otherEnvRead)));    assertThat(readFromOtherEnv.getOutputPCollections(), contains(PipelineNode.pCollection("flatten.out", components.getPcollectionsOrThrow("flatten.out"))));    assertThat(readFromOtherEnv, hasSubtransforms("envRead", "flatten"));}
public void beam_f1819_0()
{                                    PTransform pyRead = PTransform.newBuilder().putInputs("input", "impulse.out").putOutputs("output", "pyRead.out").setSpec(FunctionSpec.newBuilder().setUrn(PTransformTranslation.PAR_DO_TRANSFORM_URN).setPayload(ParDoPayload.newBuilder().setDoFn(SdkFunctionSpec.newBuilder().setEnvironmentId("py")).build().toByteString()).build()).build();    PTransform goRead = PTransform.newBuilder().putInputs("input", "impulse.out").putOutputs("output", "goRead.out").setSpec(FunctionSpec.newBuilder().setUrn(PTransformTranslation.PAR_DO_TRANSFORM_URN).setPayload(ParDoPayload.newBuilder().setDoFn(SdkFunctionSpec.newBuilder().setEnvironmentId("go")).build().toByteString()).build()).build();    PTransform pyParDo = PTransform.newBuilder().putInputs("input", "flatten.out").putOutputs("output", "pyParDo.out").setSpec(FunctionSpec.newBuilder().setUrn(PTransformTranslation.PAR_DO_TRANSFORM_URN).setPayload(ParDoPayload.newBuilder().setDoFn(SdkFunctionSpec.newBuilder().setEnvironmentId("py")).build().toByteString()).build()).build();    PTransform goWindow = PTransform.newBuilder().putInputs("input", "flatten.out").putOutputs("output", "goWindow.out").setSpec(FunctionSpec.newBuilder().setUrn(PTransformTranslation.ASSIGN_WINDOWS_TRANSFORM_URN).setPayload(WindowIntoPayload.newBuilder().setWindowFn(SdkFunctionSpec.newBuilder().setEnvironmentId("go")).build().toByteString()).build()).build();    PCollection flattenPc = PCollection.newBuilder().setUniqueName("flatten.out").build();    Components components = partialComponents.toBuilder().putTransforms("pyRead", pyRead).putPcollections("pyRead.out", PCollection.newBuilder().setUniqueName("pyRead.out").build()).putTransforms("goRead", goRead).putPcollections("goRead.out", PCollection.newBuilder().setUniqueName("goRead.out").build()).putTransforms("flatten", PTransform.newBuilder().putInputs("py_input", "pyRead.out").putInputs("go_input", "goRead.out").putOutputs("output", "flatten.out").setSpec(FunctionSpec.newBuilder().setUrn(PTransformTranslation.FLATTEN_TRANSFORM_URN).build()).build()).putPcollections("flatten.out", flattenPc).putTransforms("pyParDo", pyParDo).putPcollections("pyParDo.out", PCollection.newBuilder().setUniqueName("pyParDo.out").build()).putTransforms("goWindow", goWindow).putPcollections("goWindow.out", PCollection.newBuilder().setUniqueName("goWindow.out").build()).putEnvironments("go", Environments.createDockerEnvironment("go")).putEnvironments("py", Environments.createDockerEnvironment("py")).build();    QueryablePipeline p = QueryablePipeline.forPrimitivesIn(components);    ExecutableStage readFromPy = GreedyStageFuser.forGrpcPortRead(p, impulseOutputNode, ImmutableSet.of(PipelineNode.pTransform("pyRead", pyRead)));    ExecutableStage readFromGo = GreedyStageFuser.forGrpcPortRead(p, impulseOutputNode, ImmutableSet.of(PipelineNode.pTransform("goRead", goRead)));    assertThat(readFromPy.getOutputPCollections(), contains(PipelineNode.pCollection("flatten.out", flattenPc)));                assertThat(readFromPy.getTransforms(), not(hasItem(PipelineNode.pTransform("pyParDo", pyParDo))));    assertThat(readFromGo.getOutputPCollections(), contains(PipelineNode.pCollection("flatten.out", flattenPc)));    assertThat(readFromGo.getTransforms(), not(hasItem(PipelineNode.pTransform("goWindow", goWindow))));}
public void beam_f1820_0()
{                    Environment env = Environments.createDockerEnvironment("common");    PTransform parDoTransform = PTransform.newBuilder().putInputs("input", "impulse.out").putOutputs("out", "parDo.out").setSpec(FunctionSpec.newBuilder().setUrn(PTransformTranslation.PAR_DO_TRANSFORM_URN).setPayload(ParDoPayload.newBuilder().setDoFn(SdkFunctionSpec.newBuilder().setEnvironmentId("common")).build().toByteString())).build();    PCollection parDoOutput = PCollection.newBuilder().setUniqueName("parDo.out").build();    QueryablePipeline p = QueryablePipeline.forPrimitivesIn(partialComponents.toBuilder().putTransforms("parDo", parDoTransform).putPcollections("parDo.out", parDoOutput).putTransforms("window", PTransform.newBuilder().putInputs("input", "parDo.out").putOutputs("output", "window.out").setSpec(FunctionSpec.newBuilder().setUrn(PTransformTranslation.ASSIGN_WINDOWS_TRANSFORM_URN).setPayload(WindowIntoPayload.newBuilder().setWindowFn(SdkFunctionSpec.newBuilder().setEnvironmentId("rare")).build().toByteString())).build()).putPcollections("window.out", PCollection.newBuilder().setUniqueName("window.out").build()).putEnvironments("rare", Environments.createDockerEnvironment("rare")).putEnvironments("common", env).build());    ExecutableStage subgraph = GreedyStageFuser.forGrpcPortRead(p, impulseOutputNode, p.getPerElementConsumers(impulseOutputNode));    assertThat(subgraph.getOutputPCollections(), contains(PipelineNode.pCollection("parDo.out", parDoOutput)));    assertThat(subgraph.getInputPCollection(), equalTo(impulseOutputNode));    assertThat(subgraph.getEnvironment(), equalTo(env));    assertThat(subgraph.getTransforms(), contains(PipelineNode.pTransform("parDo", parDoTransform)));}
protected boolean beam_f1828_0(ExecutableStage executableStage)
{        Set<String> stageTransforms = executableStage.getTransforms().stream().map(PTransformNode::getId).collect(Collectors.toSet());    return stageTransforms.containsAll(expectedTransforms) && expectedTransforms.containsAll(stageTransforms);}
public void beam_f1829_0(Description description)
{    description.appendText("ExecutableStage with subtransform ids: " + expectedTransforms);}
public void beam_f1830_0() throws Exception
{    Environment env = Environments.createDockerEnvironment("foo");    PTransform pt = PTransform.newBuilder().putInputs("input", "input.out").putInputs("side_input", "sideInput.in").putInputs("timer", "timer.pc").putOutputs("output", "output.out").putOutputs("timer", "timer.pc").setSpec(FunctionSpec.newBuilder().setUrn(PTransformTranslation.PAR_DO_TRANSFORM_URN).setPayload(ParDoPayload.newBuilder().setDoFn(RunnerApi.SdkFunctionSpec.newBuilder().setEnvironmentId("foo")).putSideInputs("side_input", RunnerApi.SideInput.getDefaultInstance()).putStateSpecs("user_state", RunnerApi.StateSpec.getDefaultInstance()).putTimerSpecs("timer", RunnerApi.TimerSpec.getDefaultInstance()).build().toByteString())).build();    PCollection input = PCollection.newBuilder().setUniqueName("input.out").build();    PCollection sideInput = PCollection.newBuilder().setUniqueName("sideInput.in").build();    PCollection timer = PCollection.newBuilder().setUniqueName("timer.pc").build();    PCollection output = PCollection.newBuilder().setUniqueName("output.out").build();    Components components = Components.newBuilder().putTransforms("pt", pt).putTransforms("other_pt", PTransform.newBuilder().setUniqueName("other").build()).putPcollections("input.out", input).putPcollections("sideInput.in", sideInput).putPcollections("timer.pc", timer).putPcollections("output.out", output).putEnvironments("foo", env).build();    PTransformNode transformNode = PipelineNode.pTransform("pt", pt);    SideInputReference sideInputRef = SideInputReference.of(transformNode, "side_input", PipelineNode.pCollection("sideInput.in", sideInput));    UserStateReference userStateRef = UserStateReference.of(transformNode, "user_state", PipelineNode.pCollection("input.out", input));    TimerReference timerRef = TimerReference.of(transformNode, "timer");    ImmutableExecutableStage stage = ImmutableExecutableStage.ofFullComponents(components, env, PipelineNode.pCollection("input.out", input), Collections.singleton(sideInputRef), Collections.singleton(userStateRef), Collections.singleton(timerRef), Collections.singleton(PipelineNode.pTransform("pt", pt)), Collections.singleton(PipelineNode.pCollection("output.out", output)));    assertThat(stage.getComponents().containsTransforms("pt"), is(true));    assertThat(stage.getComponents().containsTransforms("other_pt"), is(false));    PTransform stagePTransform = stage.toPTransform("foo");    assertThat(stagePTransform.getOutputsMap(), hasValue("output.out"));    assertThat(stagePTransform.getOutputsCount(), equalTo(1));    assertThat(stagePTransform.getInputsMap(), allOf(hasValue("input.out"), hasValue("sideInput.in")));    assertThat(stagePTransform.getInputsCount(), equalTo(2));    ExecutableStagePayload payload = ExecutableStagePayload.parseFrom(stagePTransform.getSpec().getPayload());    assertThat(payload.getTransformsList(), contains("pt"));    assertThat(ExecutableStage.fromPayload(payload), equalTo(stage));}
public void beam_f1838_0()
{        assertEquals(ImmutableSet.of("I", "J", "E", "G", "H", "K", "L"), Networks.reachableNodes(createNetwork(), ImmutableSet.of("I"), ImmutableSet.of("J")));}
public void beam_f1839_0()
{    MutableNetwork<String, String> network = createEmptyNetwork();    Networks.replaceDirectedNetworkNodes(network, String::toLowerCase);    assertThat(network.nodes(), empty());}
public void beam_f1840_0()
{    Function<String, String> function = input -> {        if ("E".equals(input) || "J".equals(input) || "M".equals(input) || "O".equals(input)) {            return input.toLowerCase();        }        checkArgument(!input.toLowerCase().equals(input), "All inputs should be in upper case, got %s. " + "This may indicate calling the function on multiple inputs", input);        return input;    };    MutableNetwork<String, String> network = createNetwork();    Networks.replaceDirectedNetworkNodes(network, function);    MutableNetwork<String, String> originalNetwork = createNetwork();    for (String node : originalNetwork.nodes()) {        assertEquals(originalNetwork.successors(node).stream().map(function).collect(Collectors.toCollection(HashSet::new)), network.successors(function.apply(node)));    }    assertEquals(network.nodes(), originalNetwork.nodes().stream().map(function).collect(Collectors.toCollection(HashSet::new)));}
public void beam_f1848_0()
{    RunnerApi.Pipeline p = Pipeline.newBuilder().addAllRootTransformIds(ImmutableList.of("first", "second")).setComponents(Components.newBuilder().putTransforms("first", PTransform.newBuilder().setSpec(FunctionSpec.newBuilder().setUrn("beam:first")).build()).putTransforms("second", PTransform.newBuilder().setSpec(FunctionSpec.newBuilder().setUrn("beam:second")).build()).putPcollections("intermediatePc", PCollection.newBuilder().setUniqueName("intermediate").build()).putCoders("coder", Coder.newBuilder().setSpec(FunctionSpec.getDefaultInstance()).build())).build();    PTransform secondReplacement = PTransform.newBuilder().addSubtransforms("second_sub").setSpec(FunctionSpec.newBuilder().setUrn("beam:second:replacement").setPayload(ByteString.copyFrom("foo-bar-baz".getBytes(StandardCharsets.UTF_8)))).build();    WindowingStrategy introducedWS = WindowingStrategy.newBuilder().setAccumulationMode(Enum.ACCUMULATING).build();    RunnerApi.Components extraComponents = Components.newBuilder().putPcollections("intermediatePc", PCollection.newBuilder().setUniqueName("intermediate_replacement").build()).putWindowingStrategies("new_ws", introducedWS).putTransforms("second_sub", PTransform.getDefaultInstance()).build();    Pipeline updated = ProtoOverrides.updateTransform("beam:second", p, new TestReplacer(secondReplacement, extraComponents));    PTransform updatedSecond = updated.getComponents().getTransformsOrThrow("second");    assertThat(updatedSecond, equalTo(secondReplacement));    assertThat(updated.getComponents().getWindowingStrategiesOrThrow("new_ws"), equalTo(introducedWS));    assertThat(updated.getComponents().getTransformsOrThrow("second_sub"), equalTo(PTransform.getDefaultInstance()));            assertThat(updated.getComponents().getPcollectionsOrThrow("intermediatePc").getUniqueName(), equalTo("intermediate_replacement"));        assertThat(updated.getComponents().getTransformsOrThrow("first"), equalTo(p.getComponents().getTransformsOrThrow("first")));    assertThat(updated.getComponents().getCodersOrThrow("coder"), equalTo(p.getComponents().getCodersOrThrow("coder")));    assertThat(updated.getRootTransformIdsList(), equalTo(p.getRootTransformIdsList()));}
public void beam_f1849_0()
{    RunnerApi.Pipeline p = Pipeline.newBuilder().addAllRootTransformIds(ImmutableList.of("first", "second")).setComponents(Components.newBuilder().putTransforms("first", PTransform.newBuilder().setSpec(FunctionSpec.newBuilder().setUrn("beam:first")).build()).putTransforms("second", PTransform.newBuilder().setSpec(FunctionSpec.newBuilder().setUrn("beam:repeated")).build()).putTransforms("third", PTransform.newBuilder().setSpec(FunctionSpec.newBuilder().setUrn("beam:repeated")).build()).putPcollections("intermediatePc", PCollection.newBuilder().setUniqueName("intermediate").build()).putCoders("coder", Coder.newBuilder().setSpec(FunctionSpec.getDefaultInstance()).build())).build();    ByteString newPayload = ByteString.copyFrom("foo-bar-baz".getBytes(StandardCharsets.UTF_8));    Pipeline updated = ProtoOverrides.updateTransform("beam:repeated", p, (transformId, existingComponents) -> {        String subtransform = String.format("%s_sub", transformId);        return MessageWithComponents.newBuilder().setPtransform(PTransform.newBuilder().setSpec(FunctionSpec.newBuilder().setUrn("beam:repeated:replacement").setPayload(newPayload)).addSubtransforms(subtransform)).setComponents(Components.newBuilder().putTransforms(subtransform, PTransform.newBuilder().setUniqueName(subtransform).build())).build();    });    PTransform updatedSecond = updated.getComponents().getTransformsOrThrow("second");    PTransform updatedThird = updated.getComponents().getTransformsOrThrow("third");    assertThat(updatedSecond, not(equalTo(p.getComponents().getTransformsOrThrow("second"))));    assertThat(updatedThird, not(equalTo(p.getComponents().getTransformsOrThrow("third"))));    assertThat(updatedSecond.getSubtransformsList(), contains("second_sub"));    assertThat(updatedSecond.getSpec().getPayload(), equalTo(newPayload));    assertThat(updatedThird.getSubtransformsList(), contains("third_sub"));    assertThat(updatedThird.getSpec().getPayload(), equalTo(newPayload));    assertThat(updated.getComponents().getTransformsMap(), hasKey("second_sub"));    assertThat(updated.getComponents().getTransformsMap(), hasKey("third_sub"));    assertThat(updated.getComponents().getTransformsOrThrow("second_sub").getUniqueName(), equalTo("second_sub"));    assertThat(updated.getComponents().getTransformsOrThrow("third_sub").getUniqueName(), equalTo("third_sub"));}
public void beam_f1850_0()
{    Pipeline p = Pipeline.newBuilder().addRootTransformIds("root").setComponents(Components.newBuilder().putTransforms("root", PTransform.newBuilder().addSubtransforms("sub_first").setSpec(FunctionSpec.newBuilder().setUrn("beam:composite")).build()).putTransforms("sub_first", PTransform.newBuilder().setSpec(FunctionSpec.newBuilder().setUrn("beam:inner")).build())).build();    Pipeline pipeline = ProtoOverrides.updateTransform("beam:composite", p, new TestReplacer(PTransform.newBuilder().addSubtransforms("foo").addSubtransforms("bar").setSpec(FunctionSpec.getDefaultInstance().newBuilderForType().setUrn("beam:composite")).build(), Components.getDefaultInstance()));    assertThat(pipeline.getComponents().getTransformsOrThrow("root").getSpec().getUrn(), equalTo("beam:composite"));    assertThat(pipeline.getComponents().getTransformsOrThrow("root").getSubtransformsList(), contains("foo", "bar"));}
public void beam_f1858_0()
{    Components components = Components.newBuilder().putPcollections("read_pc", RunnerApi.PCollection.getDefaultInstance()).putPcollections("pardo_out", RunnerApi.PCollection.getDefaultInstance()).putTransforms("root", PTransform.newBuilder().setSpec(FunctionSpec.newBuilder().setUrn(PTransformTranslation.IMPULSE_TRANSFORM_URN).build()).putOutputs("out", "read_pc").build()).putTransforms("multiConsumer", PTransform.newBuilder().putInputs("main_in", "read_pc").putInputs("side_in", "read_pc").putOutputs("out", "pardo_out").setSpec(FunctionSpec.newBuilder().setUrn(PTransformTranslation.PAR_DO_TRANSFORM_URN).setPayload(ParDoPayload.newBuilder().putSideInputs("side_in", SideInput.getDefaultInstance()).build().toByteString()).build()).build()).build();    QueryablePipeline qp = QueryablePipeline.forPrimitivesIn(components);    PCollectionNode multiInputPc = PipelineNode.pCollection("read_pc", components.getPcollectionsOrThrow("read_pc"));    PTransformNode multiConsumerPT = PipelineNode.pTransform("multiConsumer", components.getTransformsOrThrow("multiConsumer"));    SideInputReference sideInputRef = SideInputReference.of(multiConsumerPT, "side_in", multiInputPc);    assertThat(qp.getPerElementConsumers(multiInputPc), contains(multiConsumerPT));    assertThat(qp.getSideInputs(multiConsumerPT), contains(sideInputRef));}
public void beam_f1859_0()
{    Pipeline p = Pipeline.create();    PCollection<Long> longs = p.apply("BoundedRead", Read.from(CountingSource.upTo(100L)));    PCollectionList.of(longs).and(longs).and(longs).apply("flatten", Flatten.pCollections());    Components components = PipelineTranslation.toProto(p).getComponents();        String readOutput = getOnlyElement(components.getTransformsOrThrow("BoundedRead").getOutputsMap().values());    QueryablePipeline qp = QueryablePipeline.forPrimitivesIn(components);    Set<PTransformNode> consumers = qp.getPerElementConsumers(PipelineNode.pCollection(readOutput, components.getPcollectionsOrThrow(readOutput)));    assertThat(consumers.size(), equalTo(1));    assertThat(getOnlyElement(consumers).getTransform().getSpec().getUrn(), equalTo(PTransformTranslation.FLATTEN_TRANSFORM_URN));}
public void beam_f1860_0()
{    Pipeline p = Pipeline.create();    PCollection<Long> longs = p.apply("BoundedRead", Read.from(CountingSource.upTo(100L)));    PCollectionList.of(longs).and(longs).and(longs).apply("flatten", Flatten.pCollections());    Components components = PipelineTranslation.toProto(p).getComponents();    QueryablePipeline qp = QueryablePipeline.forPrimitivesIn(components);    String longsOutputName = getOnlyElement(PipelineNode.pTransform("BoundedRead", components.getTransformsOrThrow("BoundedRead")).getTransform().getOutputsMap().values());    PTransformNode longsProducer = PipelineNode.pTransform("BoundedRead", components.getTransformsOrThrow("BoundedRead"));    PCollectionNode longsOutput = PipelineNode.pCollection(longsOutputName, components.getPcollectionsOrThrow(longsOutputName));    String flattenOutputName = getOnlyElement(PipelineNode.pTransform("flatten", components.getTransformsOrThrow("flatten")).getTransform().getOutputsMap().values());    PTransformNode flattenProducer = PipelineNode.pTransform("flatten", components.getTransformsOrThrow("flatten"));    PCollectionNode flattenOutput = PipelineNode.pCollection(flattenOutputName, components.getPcollectionsOrThrow(flattenOutputName));    assertThat(qp.getProducer(longsOutput), equalTo(longsProducer));    assertThat(qp.getProducer(flattenOutput), equalTo(flattenProducer));}
public StreamObserver<ArtifactApi.PutArtifactRequest> beam_f1869_0(StreamObserver<ArtifactApi.PutArtifactResponse> responseObserver)
{    return new BufferingObserver(responseObserver);}
public void beam_f1870_0(ArtifactApi.CommitManifestRequest request, StreamObserver<ArtifactApi.CommitManifestResponse> responseObserver)
{    checkState(this.manifest.compareAndSet(null, request.getManifest()), "Already committed a %s %s", Manifest.class.getSimpleName(), manifest.get());    responseObserver.onNext(CommitManifestResponse.getDefaultInstance());    responseObserver.onCompleted();}
public Map<ArtifactMetadata, byte[]> beam_f1871_0()
{    return Collections.unmodifiableMap(artifactBytes);}
public void beam_f1880_0()
{    PCollection<BoundedSource<Long>> splits = p.apply(Impulse.create()).apply("SplitSource", /*                 * Split the source of 1 million longs into bundles of size 300 thousand bytes.                 * This should produce some small number of bundles, but more than one.                 */    ParDo.of(new SplitBoundedSourceFn<>(CountingSource.upTo(1_000_000L), 300_000L))).setCoder(new JavaReadViaImpulse.BoundedSourceCoder<>());    PAssert.that(splits).satisfies(input -> {        assertThat(Iterables.size(input), greaterThan(1));        return null;    });    p.run();}
public void beam_f1881_0()
{    BoundedSource<Long> source = CountingSource.upTo(10L);    PCollection<BoundedSource<Long>> sourcePC = p.apply(Create.of(source).withCoder(new JavaReadViaImpulse.BoundedSourceCoder<>()));    PCollection<Long> elems = sourcePC.apply(ParDo.of(new ReadFromBoundedSourceFn<>())).setCoder(VarLongCoder.of());    PAssert.that(elems).containsInAnyOrder(0L, 9L, 8L, 1L, 2L, 7L, 6L, 3L, 4L, 5L);    p.run();}
public void beam_f1882_0()
{    BoundedSource<Long> source = CountingSource.upTo(10L);        PCollection<Long> input = p.apply(Read.from(source));    PAssert.that(input).containsInAnyOrder(0L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L);    p.replaceAll(Collections.singletonList(JavaReadViaImpulse.boundedOverride()));    p.traverseTopologically(new Pipeline.PipelineVisitor() {        @Override        public void enterPipeline(Pipeline p) {        }        @Override        public CompositeBehavior enterCompositeTransform(TransformHierarchy.Node node) {            assertNotReadTransform(node.getTransform());            return CompositeBehavior.ENTER_TRANSFORM;        }        @Override        public void leaveCompositeTransform(TransformHierarchy.Node node) {        }        @Override        public void visitPrimitiveTransform(TransformHierarchy.Node node) {            assertNotReadTransform(node.getTransform());        }        @Override        public void visitValue(PValue value, TransformHierarchy.Node producer) {        }        @Override        public void leavePipeline(Pipeline pipeline) {        }    });    p.run();}
public Coder<Integer> beam_f1894_0()
{    return BigEndianIntegerCoder.of();}
public void beam_f1895_0() throws IOException
{    FullWindowedValueCoder<Iterable<KV<String, Integer>>> javaCoder = FullWindowedValueCoder.of(IterableCoder.of(KvCoder.of(StringUtf8Coder.of(), VarIntCoder.of())), IntervalWindowCoder.of());    MessageWithComponents coderAndComponents = CoderTranslation.toProto(javaCoder);    WindowedValueCoderComponents windowedValueCoderComponents = ModelCoders.getWindowedValueCoderComponents(coderAndComponents.getCoder());    Coder windowedCoder = ModelCoders.windowedValueCoder(windowedValueCoderComponents.elementCoderId(), windowedValueCoderComponents.windowCoderId());    assertThat(windowedCoder, equalTo(coderAndComponents.getCoder()));}
public void beam_f1896_0()
{    thrown.expect(IllegalArgumentException.class);    ModelCoders.getWindowedValueCoderComponents(Coder.newBuilder().setSpec(FunctionSpec.newBuilder().setUrn(ModelCoders.LENGTH_PREFIX_CODER_URN)).build());}
public static Iterable<ParDo.MultiOutput<?, ?>> beam_f1904_0()
{    return ImmutableList.of(ParDo.of(new DropElementsFn()).withOutputTags(new TupleTag<>(), TupleTagList.empty()), ParDo.of(new DropElementsFn()).withOutputTags(new TupleTag<>(), TupleTagList.empty()).withSideInputs(singletonSideInput, multimapSideInput), ParDo.of(new DropElementsFn()).withOutputTags(new TupleTag<>(), TupleTagList.of(new TupleTag<byte[]>() {    }).and(new TupleTag<Integer>() {    })).withSideInputs(singletonSideInput, multimapSideInput), ParDo.of(new DropElementsFn()).withOutputTags(new TupleTag<>(), TupleTagList.of(new TupleTag<byte[]>() {    }).and(new TupleTag<Integer>() {    })), ParDo.of(new SplittableDropElementsFn()).withOutputTags(new TupleTag<>(), TupleTagList.empty()), ParDo.of(new StateTimerDropElementsFn()).withOutputTags(new TupleTag<>(), TupleTagList.empty()));}
public void beam_f1905_0() throws Exception
{    SdkComponents components = SdkComponents.create();    components.registerEnvironment(Environments.createDockerEnvironment("java"));    ParDoPayload payload = ParDoTranslation.translateParDo(parDo, DoFnSchemaInformation.create(), p, components);    assertThat(ParDoTranslation.getDoFn(payload), equalTo(parDo.getFn()));    assertThat(ParDoTranslation.getMainOutputTag(payload), equalTo(parDo.getMainOutputTag()));    for (PCollectionView<?> view : parDo.getSideInputs().values()) {        payload.getSideInputsOrThrow(view.getTagInternal().getId());    }}
public void beam_f1906_0() throws Exception
{    Map<TupleTag<?>, PValue> inputs = new HashMap<>();    inputs.put(new TupleTag<KV<Long, String>>("mainInputName") {    }, mainInput);    inputs.putAll(parDo.getAdditionalInputs());    PCollectionTuple output = mainInput.apply(parDo);    SdkComponents sdkComponents = SdkComponents.create();    sdkComponents.registerEnvironment(Environments.createDockerEnvironment("java"));        RunnerApi.PTransform protoTransform = PTransformTranslation.toProto(AppliedPTransform.<PCollection<KV<Long, String>>, PCollection<Void>, MultiOutput>of("foo", inputs, output.expand(), parDo, p), sdkComponents);    RunnerApi.Components components = sdkComponents.toComponents();    RehydratedComponents rehydratedComponents = RehydratedComponents.forComponents(components);        ParDoPayload parDoPayload = ParDoPayload.parseFrom(protoTransform.getSpec().getPayload());    for (PCollectionView<?> view : parDo.getSideInputs().values()) {        SideInput sideInput = parDoPayload.getSideInputsOrThrow(view.getTagInternal().getId());        PCollectionView<?> restoredView = PCollectionViewTranslation.viewFromProto(sideInput, view.getTagInternal().getId(), view.getPCollection(), protoTransform, rehydratedComponents);        assertThat(restoredView.getTagInternal(), equalTo(view.getTagInternal()));        assertThat(restoredView.getViewFn(), instanceOf(view.getViewFn().getClass()));        assertThat(restoredView.getWindowMappingFn(), instanceOf(view.getWindowMappingFn().getClass()));        assertThat(restoredView.getWindowingStrategyInternal(), equalTo(view.getWindowingStrategyInternal().fixDefaults()));        assertThat(restoredView.getCoderInternal(), equalTo(view.getCoderInternal()));    }    String mainInputId = sdkComponents.registerPCollection(mainInput);    assertThat(ParDoTranslation.getMainInput(protoTransform, components), equalTo(components.getPcollectionsOrThrow(mainInputId)));    assertThat(ParDoTranslation.getMainInputName(protoTransform), equalTo("mainInputName"));        DoFnSignature signature = DoFnSignatures.signatureForDoFn(parDo.getFn());    for (String localTimerName : signature.timerDeclarations().keySet()) {        RunnerApi.PCollection timerPCollection = components.getPcollectionsOrThrow(String.format("foo.%s", localTimerName));        assertEquals(components.getPcollectionsOrThrow(mainInputId).getIsBounded(), timerPCollection.getIsBounded());        assertEquals(components.getPcollectionsOrThrow(mainInputId).getWindowingStrategyId(), timerPCollection.getWindowingStrategyId());        ModelCoders.KvCoderComponents timerKvCoderComponents = ModelCoders.getKvCoderComponents(components.getCodersOrThrow(timerPCollection.getCoderId()));        Coder<?> timerKeyCoder = CoderTranslation.fromProto(components.getCodersOrThrow(timerKvCoderComponents.keyCoderId()), rehydratedComponents);        assertEquals(VarLongCoder.of(), timerKeyCoder);        Coder<?> timerValueCoder = CoderTranslation.fromProto(components.getCodersOrThrow(timerKvCoderComponents.valueCoderId()), rehydratedComponents);        assertEquals(org.apache.beam.runners.core.construction.Timer.Coder.of(VoidCoder.of()), timerValueCoder);    }}
public RestrictionTracker<Integer, ?> beam_f1914_0(Integer restriction)
{    throw new UnsupportedOperationException("Should never be called; only to test translation");}
public boolean beam_f1915_0(Object other)
{    return other instanceof SplittableDropElementsFn;}
public int beam_f1916_0()
{    return SplittableDropElementsFn.class.hashCode();}
public void beam_f1926_0() throws Exception
{    SdkComponents sdkComponents = SdkComponents.create();    sdkComponents.registerEnvironment(Environments.createDockerEnvironment("java"));    RunnerApi.PCollection protoCollection = PCollectionTranslation.toProto(testCollection, sdkComponents);    RehydratedComponents protoComponents = RehydratedComponents.forComponents(sdkComponents.toComponents());    Coder<?> decodedCoder = protoComponents.getCoder(protoCollection.getCoderId());    WindowingStrategy<?, ?> decodedStrategy = protoComponents.getWindowingStrategy(protoCollection.getWindowingStrategyId());    IsBounded decodedIsBounded = PCollectionTranslation.isBounded(protoCollection);    assertThat(decodedCoder, equalTo(testCollection.getCoder()));    assertThat(decodedStrategy, equalTo(testCollection.getWindowingStrategy().fixDefaults()));    assertThat(decodedIsBounded, equalTo(testCollection.isBounded()));}
public Integer beam_f1927_0(InputStream inStream) throws IOException
{    return VarInt.decodeInt(inStream);}
public void beam_f1928_0(Integer value, OutputStream outStream) throws IOException
{    VarInt.encode(value, outStream);}
public Instant beam_f1937_0()
{    return ts;}
public WindowMappingFn<BoundedWindow> beam_f1938_0()
{    throw new UnsupportedOperationException();}
public void beam_f1939_0() throws Exception
{    SdkComponents sdkComponents = SdkComponents.create();    sdkComponents.registerEnvironment(Environments.createDockerEnvironment("java"));    assertEquals(new TestViewFn(), PCollectionViewTranslation.viewFnFromProto(ParDoTranslation.translateViewFn(new TestViewFn(), sdkComponents)));}
public void beam_f1947_0() throws Exception
{    options.getOptionsId();    Struct originalStruct = PipelineOptionsTranslation.toProto(options);    PipelineOptions deserializedStruct = PipelineOptionsTranslation.fromProto(originalStruct);    Struct reserializedStruct = PipelineOptionsTranslation.toProto(deserializedStruct);    assertThat(reserializedStruct.getFieldsMap(), equalTo(originalStruct.getFieldsMap()));}
public void beam_f1948_0() throws Exception
{    options.getOptionsId();    Struct originalStruct = PipelineOptionsTranslation.toProto(options);    String json = PipelineOptionsTranslation.toJson(options);    String legacyJson = MAPPER.writeValueAsString(options);    assertThat(PipelineOptionsTranslation.toProto(PipelineOptionsTranslation.fromJson(json)).getFieldsMap(), equalTo(originalStruct.getFieldsMap()));    assertThat(PipelineOptionsTranslation.toProto(PipelineOptionsTranslation.fromJson(legacyJson)).getFieldsMap(), equalTo(originalStruct.getFieldsMap()));}
public void beam_f1949_0() throws Exception
{    TestOptions options = PipelineOptionsFactory.as(TestOptions.class);    options.setExample(23);    Struct serialized = PipelineOptionsTranslation.toProto(options);    PipelineOptions deserialized = PipelineOptionsTranslation.fromProto(serialized);    assertThat(deserialized.as(TestOptions.class).getExample(), equalTo(23));}
public void beam_f1957_0() throws IOException
{    String nonexistentFilePath = tmpFolder.getRoot().getPath() + "/nonexistent/file";    String existingFilePath = tmpFolder.newFile("existingFile").getAbsolutePath();    String temporaryLocation = tmpFolder.newFolder().getAbsolutePath();    List<String> filesToStage = Arrays.asList(nonexistentFilePath, existingFilePath);    assertThrows("To-be-staged file does not exist: ", IllegalStateException.class, () -> PipelineResources.prepareFilesForStaging(filesToStage, temporaryLocation));}
public void beam_f1958_0() throws IOException
{    String directoryPath = tmpFolder.newFolder().getAbsolutePath();    String temporaryLocation = tmpFolder.newFolder().getAbsolutePath();    List<String> filesToStage = new ArrayList<>();    filesToStage.add(directoryPath);    List<String> result = PipelineResources.prepareFilesForStaging(filesToStage, temporaryLocation);    assertTrue(new File(result.get(0)).exists());    assertTrue(result.get(0).matches(".*\\.jar"));}
public void beam_f1959_0() throws IOException
{    thrown.expect(IllegalArgumentException.class);    thrown.expectMessage("Please provide temporary location for storing the jar files.");    List<String> filesToStage = new ArrayList<>();    filesToStage.add(tmpFolder.newFolder().getAbsolutePath());    PipelineResources.prepareFilesForStaging(filesToStage, null);}
private void beam_f1967_0(Coder<?> coder)
{    coders.add(coder);    if (CoderTranslation.KNOWN_CODER_URNS.containsKey(coder.getClass())) {        for (Coder<?> component : ((StructuredCoder<?>) coder).getComponents()) {            addCoders(component);        }    }}
private static Coder<?> beam_f1968_0(AppliedPTransform<?, ?, ?> transform) throws IOException
{    SdkComponents sdkComponents = SdkComponents.create(transform.getPipeline().getOptions());    String id = getCombinePayload(transform, sdkComponents).map(CombinePayload::getAccumulatorCoderId).orElseThrow(() -> new IOException("Transform does not contain an AccumulatorCoder"));    Components components = sdkComponents.toComponents();    return CoderTranslation.fromProto(components.getCodersOrThrow(id), RehydratedComponents.forComponents(components));}
private static Optional<CombinePayload> beam_f1969_0(AppliedPTransform<?, ?, ?> transform, SdkComponents components) throws IOException
{    RunnerApi.PTransform proto = PTransformTranslation.toProto(transform, Collections.emptyList(), components);        if (proto.hasSpec()) {        return Optional.of(CombinePayload.parseFrom(proto.getSpec().getPayload()));    } else {        return Optional.empty();    }}
public SomeTracker beam_f1979_0(Void restriction)
{    return null;}
public void beam_f1980_0(ProcessContext c, @StateId(stateId) ValueState<Integer> state)
{    Integer currentValue = MoreObjects.firstNonNull(state.read(), 0);    c.output(currentValue);    state.write(currentValue + 1);}
public void beam_f1981_0(ProcessContext context, @TimerId(timerId) Timer timer)
{    timer.offset(Duration.standardSeconds(1)).setRelative();    context.output(3);}
public void beam_f1989_0()
{    AppliedPTransform<?, ?, ?> parDoApplication = getAppliedTransform(ParDo.of(splittableDoFn).withOutputTags(new TupleTag<>(), TupleTagList.empty()));    assertThat(PTransformMatchers.splittableParDo().matches(parDoApplication), is(true));    assertThat(PTransformMatchers.stateOrTimerParDoMulti().matches(parDoApplication), is(false));    assertThat(PTransformMatchers.splittableParDoSingle().matches(parDoApplication), is(false));    assertThat(PTransformMatchers.stateOrTimerParDoSingle().matches(parDoApplication), is(false));}
public void beam_f1990_0()
{    AppliedPTransform<?, ?, ?> parDoApplication = getAppliedTransform(ParDo.of(doFnWithState).withOutputTags(new TupleTag<>(), TupleTagList.empty()));    assertThat(PTransformMatchers.stateOrTimerParDoMulti().matches(parDoApplication), is(true));    assertThat(PTransformMatchers.splittableParDoMulti().matches(parDoApplication), is(false));    assertThat(PTransformMatchers.splittableParDoSingle().matches(parDoApplication), is(false));    assertThat(PTransformMatchers.stateOrTimerParDoSingle().matches(parDoApplication), is(false));}
public void beam_f1991_0()
{    AppliedPTransform<?, ?, ?> statefulApplication = getAppliedTransform(ParDo.of(doFnWithState).withOutputTags(new TupleTag<>(), TupleTagList.empty()));    assertThat(PTransformMatchers.stateOrTimerParDo().matches(statefulApplication), is(true));    AppliedPTransform<?, ?, ?> splittableApplication = getAppliedTransform(ParDo.of(splittableDoFn).withOutputTags(new TupleTag<>(), TupleTagList.empty()));    assertThat(PTransformMatchers.stateOrTimerParDo().matches(splittableApplication), is(false));}
public void beam_f2002_0()
{    PCollection<Integer> input = p.apply(Create.of(1));    PCollectionView<Iterable<Integer>> view = input.apply(View.asIterable());    PTransformMatcher matcher = PTransformMatchers.createViewWithViewFn(view.getViewFn().getClass());    assertThat(matcher.matches(getAppliedTransform(View.asIterable())), is(false));}
public void beam_f2003_0()
{    AppliedPTransform application = AppliedPTransform.of("EmptyFlatten", Collections.emptyMap(), Collections.singletonMap(new TupleTag<Integer>(), PCollection.createPrimitiveOutputInternal(p, WindowingStrategy.globalDefault(), IsBounded.BOUNDED, VarIntCoder.of())), Flatten.pCollections(), p);    assertThat(PTransformMatchers.emptyFlatten().matches(application), is(true));}
public void beam_f2004_0()
{    AppliedPTransform application = AppliedPTransform.of("Flatten", Collections.singletonMap(new TupleTag<Integer>(), PCollection.createPrimitiveOutputInternal(p, WindowingStrategy.globalDefault(), IsBounded.BOUNDED, VarIntCoder.of())), Collections.singletonMap(new TupleTag<Integer>(), PCollection.createPrimitiveOutputInternal(p, WindowingStrategy.globalDefault(), IsBounded.BOUNDED, VarIntCoder.of())), Flatten.pCollections(), p);    assertThat(PTransformMatchers.emptyFlatten().matches(application), is(false));}
public ResourceId beam_f2012_0(int shardNumber, int numShards, BoundedWindow window, PaneInfo paneInfo, FileBasedSink.OutputFileHints outputFileHints)
{    throw new UnsupportedOperationException("should not be called");}
public ResourceId beam_f2013_0(int shardNumber, int numShards, FileBasedSink.OutputFileHints outputFileHints)
{    throw new UnsupportedOperationException("should not be called");}
public void beam_f2014_0()
{    AppliedPTransform<PCollection<Long>, ?, ?> application = AppliedPTransform.of("application", Collections.singletonMap(new TupleTag<Long>(), mainInput), Collections.singletonMap(new TupleTag<Long>(), output), ParDo.of(new TestDoFn()), pipeline);    PCollection<Long> input = PTransformReplacements.getSingletonMainInput(application);    assertThat(input, equalTo(mainInput));}
private RunnerApi.PTransform beam_f2023_0(ToAndFromProtoSpec spec, SdkComponents components) throws IOException
{    List<AppliedPTransform<?, ?, ?>> childTransforms = new ArrayList<>();    for (ToAndFromProtoSpec child : spec.getChildren()) {        childTransforms.add(child.getTransform());        System.out.println("Converting child " + child);        convert(child, components);                components.getExistingPTransformId(child.getTransform());    }    RunnerApi.PTransform convert = PTransformTranslation.toProto(spec.getTransform(), childTransforms, components);            components.registerPTransform(spec.getTransform(), childTransforms);    return convert;}
private static AppliedPTransform<?, ?, ?> beam_f2025_0(Pipeline pipeline)
{    GenerateSequence sequence = GenerateSequence.from(0);    PCollection<Long> pcollection = pipeline.apply(sequence);    return AppliedPTransform.of("Count", pipeline.begin().expand(), pcollection.expand(), sequence, pipeline);}
private static AppliedPTransform<?, ?, ?> beam_f2026_0(Pipeline pipeline)
{    Read.Unbounded<Long> transform = Read.from(CountingSource.unbounded());    PCollection<Long> pcollection = pipeline.apply(transform);    return AppliedPTransform.of("ReadTheCount", pipeline.begin().expand(), pcollection.expand(), transform, pipeline);}
public List<? extends BoundedSource<String>> beam_f2034_0(long desiredBundleSizeBytes, PipelineOptions options) throws Exception
{    throw new UnsupportedOperationException();}
public long beam_f2035_0(PipelineOptions options) throws Exception
{    throw new UnsupportedOperationException();}
public BoundedReader<String> beam_f2036_0(PipelineOptions options) throws IOException
{    throw new UnsupportedOperationException();}
public boolean beam_f2044_0(Object other)
{    return other != null && other.getClass().equals(TestUnboundedSource.class);}
public int beam_f2045_0()
{    return TestUnboundedSource.class.hashCode();}
public void beam_f2046_0(CheckpointMark value, OutputStream outStream) throws CoderException, IOException
{    throw new UnsupportedOperationException();}
public void beam_f2054_0()
{    assertEquals("digraph {" + "    rankdir=LR" + "}", PortablePipelineDotRenderer.toDotString(PipelineTranslation.toProto(p)).replaceAll(System.lineSeparator(), ""));}
public void beam_f2055_0()
{    p.apply(Create.timestamped(TimestampedValue.of(KV.of(1, 1), new Instant(1)))).apply(Window.into(FixedWindows.of(Duration.millis(10)))).apply(Sum.integersPerKey());    assertEquals("digraph {" + "    rankdir=LR" + "    0 [label=\"Create.TimestampedValues\\n\"]" + "    1 [label=\"Window.Into()\\n\"]" + "    0 -> 1 [style=solid label=\"Create.TimestampedValues/ParDo(ConvertTimestamps)/ParMultiDo(ConvertTimestamps).output\"]" + "    2 [label=\"Combine.perKey(SumInteger)\\nbeam:transform:combine_per_key:v1\"]" + "    1 -> 2 [style=solid label=\"Window.Into()/Window.Assign.out\"]" + "}", PortablePipelineDotRenderer.toDotString(PipelineTranslation.toProto(p)).replaceAll(System.lineSeparator(), ""));}
public void beam_f2056_0()
{    Map<PValue, ReplacementOutput> replacements = ReplacementOutputs.singleton(ints.expand(), replacementInts);    assertThat(replacements, Matchers.hasKey(replacementInts));    ReplacementOutput replacement = replacements.get(replacementInts);    Map.Entry<TupleTag<?>, PValue> taggedInts = Iterables.getOnlyElement(ints.expand().entrySet());    assertThat(replacement.getOriginal().getTag(), equalTo(taggedInts.getKey()));    assertThat(replacement.getOriginal().getValue(), equalTo(taggedInts.getValue()));    assertThat(replacement.getReplacement().getValue(), equalTo(replacementInts));}
public void beam_f2064_0() throws Exception
{    components = SdkComponents.create();    components.registerEnvironment(Environments.JAVA_SDK_HARNESS_ENVIRONMENT);}
public void beam_f2065_0() throws IOException
{    Coder<?> coder = KvCoder.of(StringUtf8Coder.of(), IterableCoder.of(SetCoder.of(ByteArrayCoder.of())));    String id = components.registerCoder(coder);    assertThat(components.registerCoder(coder), equalTo(id));    assertThat(id, not(isEmptyOrNullString()));    Coder<?> equalCoder = KvCoder.of(StringUtf8Coder.of(), IterableCoder.of(SetCoder.of(ByteArrayCoder.of())));    assertThat(components.registerCoder(equalCoder), equalTo(id));    Coder<?> otherCoder = VarLongCoder.of();    assertThat(components.registerCoder(otherCoder), not(equalTo(id)));    components.toComponents().getCodersOrThrow(id);    components.toComponents().getCodersOrThrow(components.registerCoder(otherCoder));}
public void beam_f2066_0() throws IOException
{    Create.Values<Integer> create = Create.of(1, 2, 3);    PCollection<Integer> pt = pipeline.apply(create);    String userName = "my_transform/my_nesting";    AppliedPTransform<?, ?, ?> transform = AppliedPTransform.of(userName, pipeline.begin().expand(), pt.expand(), create, pipeline);    String componentName = components.registerPTransform(transform, Collections.emptyList());    assertThat(componentName, equalTo(userName));    assertThat(components.getExistingPTransformId(transform), equalTo(componentName));}
public void beam_f2074_0() throws IOException
{    WindowingStrategy<?, ?> strategy = WindowingStrategy.globalDefault().withMode(AccumulationMode.ACCUMULATING_FIRED_PANES);    String name = components.registerWindowingStrategy(strategy);    String duplicateName = components.registerWindowingStrategy(WindowingStrategy.globalDefault().withMode(AccumulationMode.ACCUMULATING_FIRED_PANES));    assertThat(name, equalTo(duplicateName));}
public void beam_f2075_0() throws Exception
{    PipelineOptions options = PipelineOptionsFactory.fromArgs("--foo=testValue", "--ignoredField=overridden").as(MyOptions.class);    SerializablePipelineOptions serializableOptions = new SerializablePipelineOptions(options);    assertEquals("testValue", serializableOptions.get().as(MyOptions.class).getFoo());    assertEquals("overridden", serializableOptions.get().as(MyOptions.class).getIgnoredField());    SerializablePipelineOptions copy = SerializableUtils.clone(serializableOptions);    assertEquals("testValue", copy.get().as(MyOptions.class).getFoo());    assertEquals("not overridden", copy.get().as(MyOptions.class).getIgnoredField());}
public void beam_f2076_0() throws Exception
{    SerializablePipelineOptions first = new SerializablePipelineOptions(PipelineOptionsFactory.fromArgs("--foo=first").as(MyOptions.class));    SerializablePipelineOptions firstCopy = SerializableUtils.clone(first);    SerializablePipelineOptions second = new SerializablePipelineOptions(PipelineOptionsFactory.fromArgs("--foo=second").as(MyOptions.class));    SerializablePipelineOptions secondCopy = SerializableUtils.clone(second);    assertEquals("first", first.get().as(MyOptions.class).getFoo());    assertEquals("first", firstCopy.get().as(MyOptions.class).getFoo());    assertEquals("second", second.get().as(MyOptions.class).getFoo());    assertEquals("second", secondCopy.get().as(MyOptions.class).getFoo());    first.get().as(MyOptions.class).setFoo("new first");    firstCopy.get().as(MyOptions.class).setFoo("new firstCopy");    second.get().as(MyOptions.class).setFoo("new second");    secondCopy.get().as(MyOptions.class).setFoo("new secondCopy");    assertEquals("new first", first.get().as(MyOptions.class).getFoo());    assertEquals("new firstCopy", firstCopy.get().as(MyOptions.class).getFoo());    assertEquals("new second", second.get().as(MyOptions.class).getFoo());    assertEquals("new secondCopy", secondCopy.get().as(MyOptions.class).getFoo());}
public SomeRestriction beam_f2084_0()
{    return someRestriction;}
public SomeRestriction beam_f2085_0()
{    return someRestriction;}
public SomeRestriction beam_f2088_0(Integer element)
{    return null;}
public static Iterable<TestStream<?>> beam_f2096_0()
{    return ImmutableList.of(TestStream.create(VarIntCoder.of()).advanceWatermarkToInfinity(), TestStream.create(VarIntCoder.of()).advanceWatermarkTo(new Instant(42)).advanceWatermarkToInfinity(), TestStream.create(VarIntCoder.of()).addElements(TimestampedValue.of(3, new Instant(17))).advanceWatermarkToInfinity(), TestStream.create(StringUtf8Coder.of()).advanceProcessingTime(Duration.millis(82)).advanceWatermarkToInfinity());}
public void beam_f2097_0() throws Exception
{    SdkComponents components = SdkComponents.create();    components.registerEnvironment(Environments.createDockerEnvironment("java"));    RunnerApi.TestStreamPayload payload = TestStreamTranslation.payloadForTestStream(testStream, components);    verifyTestStreamEncoding(testStream, payload, RehydratedComponents.forComponents(components.toComponents()));}
public void beam_f2098_0() throws Exception
{    PCollection<String> output = p.apply(testStream);    AppliedPTransform<PBegin, PCollection<String>, TestStream<String>> appliedTestStream = AppliedPTransform.of("fakeName", PBegin.in(p).expand(), output.expand(), testStream, p);    SdkComponents components = SdkComponents.create();    components.registerEnvironment(Environments.createDockerEnvironment("java"));    RunnerApi.FunctionSpec spec = PTransformTranslation.toProto(appliedTestStream, components).getSpec();    assertThat(spec.getUrn(), equalTo(TEST_STREAM_TRANSFORM_URN));    RunnerApi.TestStreamPayload payload = TestStreamPayload.parseFrom(spec.getPayload());    verifyTestStreamEncoding(testStream, payload, RehydratedComponents.forComponents(components.toComponents()));}
public void beam_f2106_0()
{    Map<TupleTag<?>, PValue> allInputs = new HashMap<>();    PCollection<Integer> mainInts = pipeline.apply("MainInput", Create.of(12, 3));    allInputs.put(new TupleTag<Integer>() {    }, mainInts);    PCollection<Void> voids = pipeline.apply("VoidInput", Create.empty(VoidCoder.of()));    allInputs.put(new TupleTag<Void>() {    }, voids);    AppliedPTransform<PInput, POutput, TestTransform> transform = AppliedPTransform.of("additional-free", allInputs, Collections.emptyMap(), new TestTransform(), pipeline);    assertThat(TransformInputs.nonAdditionalInputs(transform), Matchers.containsInAnyOrder(voids, mainInts));}
public void beam_f2107_0()
{    Map<TupleTag<?>, PValue> additionalInputs = new HashMap<>();    additionalInputs.put(new TupleTag<String>() {    }, pipeline.apply(Create.of("1, 2", "3")));    additionalInputs.put(new TupleTag<Long>() {    }, pipeline.apply(GenerateSequence.from(3L)));    Map<TupleTag<?>, PValue> allInputs = new HashMap<>();    PCollection<Integer> mainInts = pipeline.apply("MainInput", Create.of(12, 3));    allInputs.put(new TupleTag<Integer>() {    }, mainInts);    PCollection<Void> voids = pipeline.apply("VoidInput", Create.empty(VoidCoder.of()));    allInputs.put(new TupleTag<Void>() {    }, voids);    allInputs.putAll(additionalInputs);    AppliedPTransform<PInput, POutput, TestTransform> transform = AppliedPTransform.of("additional", allInputs, Collections.emptyMap(), new TestTransform(additionalInputs), pipeline);    assertThat(TransformInputs.nonAdditionalInputs(transform), Matchers.containsInAnyOrder(mainInts, voids));}
public void beam_f2108_0()
{    Map<TupleTag<?>, PValue> additionalInputs = new HashMap<>();    additionalInputs.put(new TupleTag<String>() {    }, pipeline.apply(Create.of("1, 2", "3")));    additionalInputs.put(new TupleTag<Long>() {    }, pipeline.apply(GenerateSequence.from(3L)));    AppliedPTransform<PInput, POutput, TestTransform> transform = AppliedPTransform.of("additional-only", additionalInputs, Collections.emptyMap(), new TestTransform(additionalInputs), pipeline);    thrown.expect(IllegalArgumentException.class);    thrown.expectMessage("at least one");    TransformInputs.nonAdditionalInputs(transform);}
public void beam_f2116_0() throws Exception
{    long numElements = 100;    BoundedSource<Long> boundedSource = CountingSource.upTo(numElements);    UnboundedSource<Long, Checkpoint<Long>> unboundedSource = new BoundedToUnboundedSourceAdapter<>(boundedSource);    PCollection<Long> output = p.apply(Read.from(unboundedSource).withMaxNumRecords(numElements));        PAssert.thatSingleton(output.apply("Count", Count.globally())).isEqualTo(numElements);        PAssert.thatSingleton(output.apply(Distinct.create()).apply("UniqueCount", Count.globally())).isEqualTo(numElements);        PAssert.thatSingleton(output.apply("Min", Min.globally())).isEqualTo(0L);        PAssert.thatSingleton(output.apply("Max", Max.globally())).isEqualTo(numElements - 1);    p.run();}
public void beam_f2117_0() throws Exception
{    long numElements = 100;    BoundedSource<Long> countingSource = CountingSource.upTo(numElements);    List<Long> expected = Lists.newArrayList();    for (long i = 0; i < numElements; ++i) {        expected.add(i);    }    testBoundedToUnboundedSourceAdapterCheckpoint(countingSource, expected);}
public void beam_f2118_0() throws Exception
{    String baseName = "test-input";    File compressedFile = tmpFolder.newFile(baseName + ".gz");    byte[] input = generateInput(100);    writeFile(compressedFile, input);    BoundedSource<Byte> source = new UnsplittableSource(compressedFile.getPath(), 1);    List<Byte> expected = Lists.newArrayList();    for (byte i : input) {        expected.add(i);    }    testBoundedToUnboundedSourceAdapterCheckpoint(source, expected);}
public void beam_f2126_0() throws Exception
{    thrown.expect(NoSuchElementException.class);    BoundedSource<Long> countingSource = CountingSource.upTo(100);    BoundedToUnboundedSourceAdapter<Long> unboundedSource = new BoundedToUnboundedSourceAdapter<>(countingSource);    PipelineOptions options = PipelineOptionsFactory.create();    List<TimestampedValue<Long>> elements = ImmutableList.of(TimestampedValue.of(1L, new Instant(1L)));    Checkpoint<Long> checkpoint = new Checkpoint<>(elements, countingSource);    unboundedSource.createReader(options, checkpoint).getCurrent();}
private static byte[] beam_f2127_0(int size)
{        Random random = new Random(285930);    byte[] buff = new byte[size];    random.nextBytes(buff);    return buff;}
private static void beam_f2128_0(File file, byte[] input) throws IOException
{    try (OutputStream os = new FileOutputStream(file)) {        os.write(input);    }}
protected boolean beam_f2136_0() throws IOException
{    buff.clear();    if (channel.read(buff) != 1) {        return false;    }    current = buff.get(0);    offset += 1;    return true;}
protected long beam_f2137_0()
{    return offset;}
public void beam_f2138_0()
{    Bounded<Long> transform = Read.from(CountingSource.upTo(20L));    pipeline.apply(transform);    UnconsumedReads.ensureAllReadsConsumed(pipeline);    validateConsumed();}
public void beam_f2146_0()
{    thrown.expect(UnsupportedOperationException.class);    thrown.expectMessage(message);    factory.mapOutputs(Collections.emptyMap(), PDone.in(pipeline));}
private static ToProtoAndBackSpec beam_f2147_0(WindowingStrategy windowingStrategy)
{    return new AutoValue_WindowingStrategyTranslationTest_ToProtoAndBackSpec(windowingStrategy);}
public static Iterable<ToProtoAndBackSpec> beam_f2148_0()
{    return ImmutableList.of(toProtoAndBackSpec(WindowingStrategy.globalDefault()), toProtoAndBackSpec(WindowingStrategy.of(FixedWindows.of(Duration.millis(11)).withOffset(Duration.millis(3)))), toProtoAndBackSpec(WindowingStrategy.of(SlidingWindows.of(Duration.millis(37)).every(Duration.millis(3)).withOffset(Duration.millis(2)))), toProtoAndBackSpec(WindowingStrategy.of(Sessions.withGapDuration(Duration.millis(389)))), toProtoAndBackSpec(WindowingStrategy.of(REPRESENTATIVE_WINDOW_FN).withClosingBehavior(ClosingBehavior.FIRE_ALWAYS).withMode(AccumulationMode.ACCUMULATING_FIRED_PANES).withTrigger(REPRESENTATIVE_TRIGGER).withAllowedLateness(Duration.millis(71)).withTimestampCombiner(TimestampCombiner.EARLIEST)), toProtoAndBackSpec(WindowingStrategy.of(REPRESENTATIVE_WINDOW_FN).withClosingBehavior(ClosingBehavior.FIRE_IF_NON_EMPTY).withMode(AccumulationMode.DISCARDING_FIRED_PANES).withTrigger(REPRESENTATIVE_TRIGGER).withAllowedLateness(Duration.millis(93)).withTimestampCombiner(TimestampCombiner.LATEST)), toProtoAndBackSpec(WindowingStrategy.of(REPRESENTATIVE_WINDOW_FN).withClosingBehavior(ClosingBehavior.FIRE_IF_NON_EMPTY).withMode(AccumulationMode.RETRACTING_FIRED_PANES).withTrigger(REPRESENTATIVE_TRIGGER).withAllowedLateness(Duration.millis(100)).withTimestampCombiner(TimestampCombiner.LATEST)));}
public Coder<BoundedWindow> beam_f2156_0()
{    return (Coder) GlobalWindow.Coder.INSTANCE;}
public boolean beam_f2157_0(Object other)
{    return other != null && other.getClass().equals(this.getClass());}
public int beam_f2158_0()
{    return getClass().hashCode();}
public ResourceId beam_f2166_0(int shardNumber, int numShards, BoundedWindow window, PaneInfo paneInfo, OutputFileHints outputFileHints)
{    throw new UnsupportedOperationException("Should never be called.");}
public ResourceId beam_f2167_0(int shardNumber, int numShards, OutputFileHints outputFileHints)
{    throw new UnsupportedOperationException("Should never be called.");}
public boolean beam_f2168_0(Object other)
{    return other instanceof DummyFilenamePolicy;}
public PipelineOptions beam_f2176_0()
{    return options;}
public T beam_f2177_0(PCollectionView<T> view)
{    if (!sideInputReader.contains(view)) {        throw new IllegalArgumentException("calling sideInput() with unknown view");    }    BoundedWindow sideInputWindow = view.getWindowMappingFn().getSideInputWindow(mainInputWindow);    return sideInputReader.get(view, sideInputWindow);}
public String beam_f2178_0()
{    return combineFn.toString();}
public AccumT beam_f2186_0(AccumT accumulator, InputT input, PipelineOptions options, SideInputReader sideInputReader, Collection<? extends BoundedWindow> windows)
{    return combineFnWithContext.addInput(accumulator, input, createFromComponents(options, sideInputReader, Iterables.getOnlyElement(windows)));}
public AccumT beam_f2187_0(Iterable<AccumT> accumulators, PipelineOptions options, SideInputReader sideInputReader, Collection<? extends BoundedWindow> windows)
{    return combineFnWithContext.mergeAccumulators(accumulators, createFromComponents(options, sideInputReader, Iterables.getOnlyElement(windows)));}
public OutputT beam_f2188_0(AccumT accumulator, PipelineOptions options, SideInputReader sideInputReader, Collection<? extends BoundedWindow> windows)
{    return combineFnWithContext.extractOutput(accumulator, createFromComponents(options, sideInputReader, Iterables.getOnlyElement(windows)));}
public PCollection<KV<K, Iterable<WindowedValue<V>>>> beam_f2196_0(PCollection<KV<K, V>> input)
{    return PCollection.createPrimitiveOutputInternal(input.getPipeline(), input.getWindowingStrategy(), input.isBounded(), (Coder) GroupByKey.getOutputKvCoder(input.getCoder()));}
public PCollection<KV<K, Iterable<WindowedValue<V>>>> beam_f2197_0(PCollection<KV<K, Iterable<WindowedValue<V>>>> input)
{    return input.apply(ParDo.of(new DoFn<KV<K, Iterable<WindowedValue<V>>>, KV<K, Iterable<WindowedValue<V>>>>() {        @ProcessElement        public void processElement(ProcessContext c) {            KV<K, Iterable<WindowedValue<V>>> kvs = c.element();            K key = kvs.getKey();            Iterable<WindowedValue<V>> unsortedValues = kvs.getValue();            List<WindowedValue<V>> sortedValues = new ArrayList<>();            for (WindowedValue<V> value : unsortedValues) {                sortedValues.add(value);            }            sortedValues.sort(Comparator.comparing(WindowedValue::getTimestamp));            c.output(KV.of(key, sortedValues));        }    })).setCoder(input.getCoder());}
public void beam_f2198_0(ProcessContext c)
{    KV<K, Iterable<WindowedValue<V>>> kvs = c.element();    K key = kvs.getKey();    Iterable<WindowedValue<V>> unsortedValues = kvs.getValue();    List<WindowedValue<V>> sortedValues = new ArrayList<>();    for (WindowedValue<V> value : unsortedValues) {        sortedValues.add(value);    }    sortedValues.sort(Comparator.comparing(WindowedValue::getTimestamp));    c.output(KV.of(key, sortedValues));}
public static InMemoryStateInternals<K> beam_f2206_0(@Nullable K key)
{    return new InMemoryStateInternals<>(key);}
public K beam_f2207_0()
{    return key;}
protected StateBinder beam_f2208_0(StateNamespace namespace, StateContext<?> c)
{    return new InMemoryStateBinder(c);}
public CombiningState<InputT, AccumT, OutputT> beam_f2216_0(StateTag<CombiningState<InputT, AccumT, OutputT>> address, Coder<AccumT> accumCoder, final CombineFn<InputT, AccumT, OutputT> combineFn)
{    return new InMemoryCombiningState<>(combineFn, accumCoder);}
public WatermarkHoldState beam_f2217_0(StateTag<WatermarkHoldState> address, TimestampCombiner timestampCombiner)
{    return new InMemoryWatermarkHold(timestampCombiner);}
public CombiningState<InputT, AccumT, OutputT> beam_f2218_0(StateTag<CombiningState<InputT, AccumT, OutputT>> address, Coder<AccumT> accumCoder, CombineFnWithContext<InputT, AccumT, OutputT> combineFn)
{    return bindCombiningValue(address, accumCoder, CombineFnUtil.bindContext(combineFn, c));}
public void beam_f2226_0()
{            combinedHold = null;}
public Instant beam_f2227_0()
{    return combinedHold;}
public void beam_f2228_0(Instant outputTime)
{    combinedHold = combinedHold == null ? outputTime : timestampCombiner.combine(combinedHold, outputTime);}
public InMemoryCombiningState<InputT, AccumT, OutputT> beam_f2236_0()
{    return this;}
public void beam_f2237_0()
{            accum = combineFn.createAccumulator();    isCleared = true;}
public OutputT beam_f2238_0()
{    return combineFn.extractOutput(combineFn.mergeAccumulators(Arrays.asList(combineFn.createAccumulator(), accum)));}
public boolean beam_f2246_0()
{    return isCleared;}
public InMemoryCombiningState<InputT, AccumT, OutputT> beam_f2247_0()
{    InMemoryCombiningState<InputT, AccumT, OutputT> that = new InMemoryCombiningState<>(combineFn, accumCoder);    if (!this.isCleared) {        that.isCleared = this.isCleared;        that.addAccum(uncheckedClone(accumCoder, accum));    }    return that;}
public void beam_f2248_0()
{                                contents = new ArrayList<>();}
public InMemoryBag<T> beam_f2256_0()
{    InMemoryBag<T> that = new InMemoryBag<>(elemCoder);    for (T elem : this.contents) {        that.contents.add(uncheckedClone(elemCoder, elem));    }    return that;}
public void beam_f2257_0()
{    contents = new HashSet<>();}
public ReadableState<Boolean> beam_f2258_0(T t)
{    return ReadableStates.immediate(contents.contains(t));}
public ReadableState<Boolean> beam_f2266_0()
{    return this;}
public Boolean beam_f2267_0()
{    return contents.isEmpty();}
public InMemorySet<T> beam_f2268_0()
{    InMemorySet<T> that = new InMemorySet<>(elemCoder);    for (T elem : this.contents) {        that.contents.add(uncheckedClone(elemCoder, elem));    }    return that;}
public ReadableState<Iterable<T>> beam_f2276_0()
{    return this;}
public ReadableState<Iterable<K>> beam_f2277_0()
{    return CollectionViewState.of(contents.keySet());}
public ReadableState<Iterable<V>> beam_f2278_0()
{    return CollectionViewState.of(contents.values());}
public void beam_f2286_0(StateNamespace namespace, String timerId, Instant target, TimeDomain timeDomain)
{    setTimer(TimerData.of(timerId, namespace, target, timeDomain));}
public void beam_f2287_0(TimerData timerData)
{    WindowTracing.trace("{}.setTimer: {}", getClass().getSimpleName(), timerData);    @Nullable    TimerData existing = existingTimers.get(timerData.getNamespace(), timerData.getTimerId());    if (existing == null) {        existingTimers.put(timerData.getNamespace(), timerData.getTimerId(), timerData);        timersForDomain(timerData.getDomain()).add(timerData);    } else {        checkArgument(timerData.getDomain().equals(existing.getDomain()), "Attempt to set %s for time domain %s, but it is already set for time domain %s", timerData.getTimerId(), timerData.getDomain(), existing.getDomain());        if (!timerData.getTimestamp().equals(existing.getTimestamp())) {            NavigableSet<TimerData> timers = timersForDomain(timerData.getDomain());            timers.remove(existing);            timers.add(timerData);            existingTimers.put(timerData.getNamespace(), timerData.getTimerId(), timerData);        }    }}
public void beam_f2288_0(StateNamespace namespace, String timerId, TimeDomain timeDomain)
{    throw new UnsupportedOperationException("Canceling a timer by ID is not yet supported.");}
public void beam_f2296_0(Instant newOutputWatermark)
{    checkNotNull(newOutputWatermark);    final Instant adjustedOutputWatermark;    if (newOutputWatermark.isAfter(inputWatermarkTime)) {        WindowTracing.trace("{}.advanceOutputWatermark: clipping output watermark from {} to {}", getClass().getSimpleName(), newOutputWatermark, inputWatermarkTime);        adjustedOutputWatermark = inputWatermarkTime;    } else {        adjustedOutputWatermark = newOutputWatermark;    }    checkState(outputWatermarkTime == null || !adjustedOutputWatermark.isBefore(outputWatermarkTime), "Cannot move output watermark time backwards from %s to %s", outputWatermarkTime, adjustedOutputWatermark);    WindowTracing.trace("{}.advanceOutputWatermark: from {} to {}", getClass().getSimpleName(), outputWatermarkTime, adjustedOutputWatermark);    outputWatermarkTime = adjustedOutputWatermark;}
public void beam_f2297_0(Instant newProcessingTime) throws Exception
{    checkNotNull(newProcessingTime);    checkState(!newProcessingTime.isBefore(processingTime), "Cannot move processing time backwards from %s to %s", processingTime, newProcessingTime);    WindowTracing.trace("{}.advanceProcessingTime: from {} to {}", getClass().getSimpleName(), processingTime, newProcessingTime);    processingTime = newProcessingTime;}
public void beam_f2298_0(Instant newSynchronizedProcessingTime) throws Exception
{    checkNotNull(newSynchronizedProcessingTime);    checkState(!newSynchronizedProcessingTime.isBefore(synchronizedProcessingTime), "Cannot move processing time backwards from %s to %s", synchronizedProcessingTime, newSynchronizedProcessingTime);    WindowTracing.trace("{}.advanceProcessingTime: from {} to {}", getClass().getSimpleName(), synchronizedProcessingTime, newSynchronizedProcessingTime);    synchronizedProcessingTime = newSynchronizedProcessingTime;}
public void beam_f2306_0(KeyedWorkItem<K, ElemT> value, OutputStream outStream) throws CoderException, IOException
{    keyCoder.encode(value.key(), outStream);    timersCoder.encode(value.timersIterable(), outStream);    elemsCoder.encode(value.elementsIterable(), outStream);}
public KeyedWorkItem<K, ElemT> beam_f2307_0(InputStream inStream) throws CoderException, IOException
{    K key = keyCoder.decode(inStream);    Iterable<TimerData> timers = timersCoder.decode(inStream);    Iterable<WindowedValue<ElemT>> elems = elemsCoder.decode(inStream);    return KeyedWorkItems.workItem(key, timers, elems);}
public List<? extends Coder<?>> beam_f2308_0()
{    return ImmutableList.of(keyCoder, elemCoder, windowCoder);}
public Iterable<WindowedValue<ElemT>> beam_f2316_0()
{    return elements;}
public boolean beam_f2317_0(Object other)
{    if (other == null || !(other instanceof ComposedKeyedWorkItem)) {        return false;    }    KeyedWorkItem<?, ?> that = (KeyedWorkItem<?, ?>) other;    return Objects.equals(this.key, that.key()) && Iterables.elementsEqual(this.timersIterable(), that.timersIterable()) && Iterables.elementsEqual(this.elementsIterable(), that.elementsIterable());}
public int beam_f2318_0()
{    return Objects.hash(key, timers, elements);}
private boolean beam_f2326_0(BoundedWindow window)
{    Instant inputWM = timerInternals.currentInputWatermarkTime();    return LateDataUtils.garbageCollectionTime(window, windowingStrategy).isBefore(inputWM);}
public static Instant beam_f2327_0(BoundedWindow window, WindowingStrategy windowingStrategy)
{    return garbageCollectionTime(window, windowingStrategy.getAllowedLateness());}
public static Instant beam_f2328_0(BoundedWindow window, Duration allowedLateness)
{        if (GlobalWindow.INSTANCE.maxTimestamp().minus(allowedLateness).isBefore(window.maxTimestamp())) {        return GlobalWindow.INSTANCE.maxTimestamp();    } else {        return window.maxTimestamp().plus(allowedLateness);    }}
public void beam_f2336_0(W window)
{    Set<W> stateAddressWindows = activeWindowToStateAddressWindows.get(window);    checkState(stateAddressWindows != null, "Cannot ensure window %s is active since it is neither ACTIVE nor NEW", window);    if (stateAddressWindows.isEmpty()) {                stateAddressWindows.add(window);    }}
public void beam_f2337_0(W window)
{    if (!activeWindowToStateAddressWindows.containsKey(window)) {                Set<W> stateAddressWindows = new LinkedHashSet<>();        stateAddressWindows.add(window);        activeWindowToStateAddressWindows.put(window, stateAddressWindows);    }}
public void beam_f2338_0(W window, Iterable<W> stateAddressWindows)
{    if (!activeWindowToStateAddressWindows.containsKey(window)) {        activeWindowToStateAddressWindows.put(window, Sets.newLinkedHashSet(stateAddressWindows));    }}
public Set<W> beam_f2346_0(W window)
{    Set<W> stateAddressWindows = activeWindowToStateAddressWindows.get(window);    checkState(stateAddressWindows != null, "Window %s is not ACTIVE", window);    return stateAddressWindows;}
public W beam_f2347_0(W window)
{    Set<W> stateAddressWindows = activeWindowToStateAddressWindows.get(window);    checkState(stateAddressWindows != null, "Window %s is not ACTIVE", window);    W result = Iterables.getFirst(stateAddressWindows, null);    checkState(result != null, "Window %s is still NEW", window);    return result;}
public W beam_f2348_0(Collection<W> toBeMerged, W mergeResult)
{    Set<W> stateAddressWindows = activeWindowToStateAddressWindows.get(mergeResult);    if (stateAddressWindows != null && !stateAddressWindows.isEmpty()) {        return Iterables.getFirst(stateAddressWindows, null);    }    for (W mergedWindow : toBeMerged) {        stateAddressWindows = activeWindowToStateAddressWindows.get(mergedWindow);        if (stateAddressWindows != null && !stateAddressWindows.isEmpty()) {            return Iterables.getFirst(stateAddressWindows, null);        }    }    return mergeResult;}
public void beam_f2356_0()
{    inc(1);}
public void beam_f2357_0()
{    inc(-1);}
public void beam_f2358_0(long n)
{    inc(-1 * n);}
public boolean beam_f2366_0()
{                                    State state;    do {        state = dirty.get();    } while (state != State.CLEAN && !dirty.compareAndSet(state, State.COMMITTING));    return state != State.CLEAN;}
public void beam_f2367_0()
{    dirty.compareAndSet(State.COMMITTING, State.CLEAN);}
public boolean beam_f2368_0(Object object)
{    if (object instanceof DirtyState) {        DirtyState dirtyState = (DirtyState) object;        return Objects.equals(dirty.get(), dirtyState.dirty.get());    }    return false;}
public boolean beam_f2376_0(Object object)
{    if (object instanceof DistributionCell) {        DistributionCell distributionCell = (DistributionCell) object;        return Objects.equals(dirty, distributionCell.dirty) && Objects.equals(value.get(), distributionCell.value.get()) && Objects.equals(name, distributionCell.name);    }    return false;}
public int beam_f2377_0()
{    return Objects.hash(dirty, value.get(), name);}
public static DistributionData beam_f2378_0(long sum, long count, long min, long max)
{    return new AutoValue_DistributionData(sum, count, min, max);}
 synchronized void beam_f2386_0(ExecutorService executor)
{    if (executionSamplerFuture != null) {        return;    }    executionSamplerFuture = executor.submit(() -> {        lastSampleTimeMillis = clock.getMillis();        long targetTimeMillis = lastSampleTimeMillis + PERIOD_MS;        while (!Thread.interrupted()) {            long currentTimeMillis = clock.getMillis();            long difference = targetTimeMillis - currentTimeMillis;            if (difference > 0) {                try {                    Thread.sleep(difference);                } catch (InterruptedException e) {                    Thread.currentThread().interrupt();                }            } else {                                doSampling(currentTimeMillis - lastSampleTimeMillis);                lastSampleTimeMillis = currentTimeMillis;                targetTimeMillis = lastSampleTimeMillis + PERIOD_MS;            }        }        return null;    });}
public synchronized void beam_f2387_0()
{    if (executionSamplerFuture == null) {        return;    }    executionSamplerFuture.cancel(true);    try {        executionSamplerFuture.get(5 * PERIOD_MS, TimeUnit.MILLISECONDS);    } catch (CancellationException e) {        } catch (InterruptedException | TimeoutException e) {        throw new RuntimeException("Failed to stop state sampling after waiting 5 sampling periods.", e);    } catch (ExecutionException e) {        throw new RuntimeException("Exception in state sampler", e);    } finally {        executionSamplerFuture = null;    }}
 void beam_f2388_0(ExecutionStateTracker tracker)
{    this.activeTrackers.add(tracker);}
public int beam_f2397_0(ExecutionStateTracker o)
{    if (this.equals(o)) {        return 0;    } else {        return System.identityHashCode(this) - System.identityHashCode(o);    }}
public static ExecutionState beam_f2398_0()
{    ExecutionStateTracker tracker = CURRENT_TRACKERS.get(Thread.currentThread());    return tracker == null ? null : tracker.currentState;}
public Closeable beam_f2399_0()
{    return activate(Thread.currentThread());}
public long beam_f2407_0()
{    return millisSinceLastTransition;}
protected void beam_f2408_0(long millisSinceLastSample)
{                    ExecutionState state = currentState;    long transitionsAtThisSample = numTransitions;    if (transitionsAtThisSample != transitionsAtLastSample) {        millisSinceLastTransition = 0;        nextLullReportMs = LULL_REPORT_MS;        transitionsAtLastSample = transitionsAtThisSample;    }    updateMillisSinceLastTransition(millisSinceLastSample, state);}
private void beam_f2409_0(long millisSinceLastSample, ExecutionState state)
{            millisSinceLastTransition += millisSinceLastSample;    if (state != null) {        if (millisSinceLastTransition > nextLullReportMs) {            state.reportLull(trackedThread, millisSinceLastTransition);            nextLullReportMs += LULL_REPORT_MS;        }        state.takeSample(millisSinceLastSample);    }}
public static GaugeData beam_f2417_0(long value)
{    return new AutoValue_GaugeData(value, Instant.now());}
public static GaugeData beam_f2418_0()
{    return EmptyGaugeData.INSTANCE;}
public GaugeData beam_f2419_0(GaugeData other)
{    if (this.timestamp().isAfter(other.timestamp())) {        return this;    } else {        return other;    }}
public DistributionCell beam_f2427_0(MetricName metricName)
{    return distributions.get(metricName);}
public DistributionCell beam_f2428_0(MetricName metricName)
{    return distributions.tryGet(metricName);}
public GaugeCell beam_f2429_0(MetricName metricName)
{    return gauges.get(metricName);}
public void beam_f2437_0()
{    commitUpdates(counters);    commitUpdates(distributions);    commitUpdates(gauges);}
private ImmutableList<MetricUpdate<UpdateT>> beam_f2438_0(MetricsMap<MetricName, CellT> cells)
{    ImmutableList.Builder<MetricUpdate<UpdateT>> updates = ImmutableList.builder();    for (Map.Entry<MetricName, CellT> cell : cells.entries()) {        UpdateT update = checkNotNull(cell.getValue().getCumulative());        updates.add(MetricUpdate.create(MetricKey.create(stepName, cell.getKey()), update));    }    return updates.build();}
public MetricUpdates beam_f2439_0()
{    return MetricUpdates.create(extractCumulatives(counters), extractCumulatives(distributions), extractCumulatives(gauges));}
public MetricsContainerImpl beam_f2447_0()
{    return this.unboundContainer;}
public MetricsContainerImpl beam_f2448_0(String stepName)
{    if (stepName == null) {                return getUnboundContainer();    }    if (!metricsContainers.containsKey(stepName)) {        metricsContainers.put(stepName, new MetricsContainerImpl(stepName));    }    return metricsContainers.get(stepName);}
public void beam_f2449_0(MetricsContainerStepMap other)
{    for (Map.Entry<String, MetricsContainerImpl> container : other.metricsContainers.entrySet()) {        getContainer(container.getKey()).update(container.getValue());    }    getUnboundContainer().update(other.getUnboundContainer());}
private Iterable<MetricsContainerImpl> beam_f2457_0()
{    return concat(metricsContainers.values(), singleton(unboundContainer));}
private static void beam_f2458_0(Map<MetricKey, MetricResult<T>> metricResultMap, Iterable<MetricUpdate<T>> updates, BiFunction<T, T, T> combine)
{    for (MetricUpdate<T> metricUpdate : updates) {        MetricKey key = metricUpdate.getKey();        MetricResult<T> current = metricResultMap.get(key);        if (current == null) {            metricResultMap.put(key, MetricResult.attempted(key, metricUpdate.getUpdate()));        } else {            metricResultMap.put(key, current.addAttempted(metricUpdate.getUpdate(), combine));        }    }}
private static void beam_f2459_0(Map<MetricKey, MetricResult<T>> metricResultMap, Iterable<MetricUpdate<T>> updates, BiFunction<T, T, T> combine)
{    for (MetricUpdate<T> metricUpdate : updates) {        MetricKey key = metricUpdate.getKey();        MetricResult<T> current = metricResultMap.get(key);        if (current == null) {            throw new IllegalStateException(String.format("%s: existing 'attempted' result not found for 'committed' value %s", key, metricUpdate.getUpdate()));        }        metricResultMap.put(key, current.addCommitted(metricUpdate.getUpdate(), combine));    }}
private void beam_f2467_0()
{    pushMetrics();    if (!scheduledFuture.isCancelled()) {        scheduledFuture.cancel(true);    }}
private void beam_f2468_0()
{    pushMetrics();    if (pipelineResult != null) {        PipelineResult.State pipelineState = pipelineResult.getState();        if (pipelineState.isTerminal()) {            tearDown();        }    }}
private void beam_f2469_0()
{    if (!(metricsSink instanceof NoOpMetricsSink)) {        try {                        MetricResults metricResults = asAttemptedOnlyMetricResults(metricsContainerStepMap);            MetricQueryResults metricQueryResults = metricResults.allMetrics();            if ((Iterables.size(metricQueryResults.getDistributions()) != 0) || (Iterables.size(metricQueryResults.getGauges()) != 0) || (Iterables.size(metricQueryResults.getCounters()) != 0)) {                metricsSink.writeMetrics(metricQueryResults);            }        } catch (Exception e) {            MetricsPushException metricsPushException = new MetricsPushException(e);            metricsPushException.printStackTrace();        }    }}
public static MetricUpdates beam_f2477_0(Iterable<MetricUpdate<Long>> counterUpdates, Iterable<MetricUpdate<DistributionData>> distributionUpdates, Iterable<MetricUpdate<GaugeData>> gaugeUpdates)
{    return new AutoValue_MetricUpdates(counterUpdates, distributionUpdates, gaugeUpdates);}
private static String beam_f2478_0(MonitoringInfoSpecs.Enum value)
{    return value.getValueDescriptor().getOptions().getExtension(monitoringInfoSpec).getUrn();}
private static String beam_f2479_0(MonitoringInfo.MonitoringInfoLabels value)
{    return value.getValueDescriptor().getOptions().getExtension(labelProps).getName();}
public int beam_f2487_0()
{        return Objects.hash(urn, labels);}
public boolean beam_f2488_0(Object o)
{        if (o == this) {        return true;    }    if (!(o instanceof MonitoringInfoMetricName)) {        return false;    }    MonitoringInfoMetricName other = (MonitoringInfoMetricName) o;    return this.urn.equals(other.urn) && this.labels.equals(other.labels);}
public String beam_f2489_0()
{    StringBuilder builder = new StringBuilder();    builder.append(this.urn.toString());    builder.append(" ");    builder.append(this.labels.toString());    return builder.toString();}
public SimpleMonitoringInfoBuilder beam_f2498_0(String urn)
{    this.builder.setUrn(urn);    return this;}
public SimpleMonitoringInfoBuilder beam_f2499_0()
{    Instant time = Instant.now();    this.builder.getTimestampBuilder().setSeconds(time.getEpochSecond()).setNanos(time.getNano());    return this;}
public SimpleMonitoringInfoBuilder beam_f2500_0(long value)
{    this.builder.getMetricBuilder().getCounterDataBuilder().setInt64Value(value);    this.setInt64TypeUrn();    return this;}
public MonitoringInfo beam_f2508_0()
{    final MonitoringInfo result = this.builder.build();    if (validateAndDropInvalid && this.validator.validate(result).isPresent()) {        return null;    }    return result;}
public void beam_f2509_0(SimpleExecutionState state)
{    this.executionStates.add(state);}
public List<MonitoringInfo> beam_f2510_0()
{    List<MonitoringInfo> monitoringInfos = new ArrayList<MonitoringInfo>();    for (SimpleExecutionState state : executionStates) {        SimpleMonitoringInfoBuilder builder = new SimpleMonitoringInfoBuilder();        builder.setUrn(state.getUrn());        for (Map.Entry<String, String> entry : state.getLabels().entrySet()) {            builder.setLabel(entry.getKey(), entry.getValue());        }        builder.setInt64Value(state.getTotalMillis());        monitoringInfos.add(builder.build());    }    return monitoringInfos;}
public void beam_f2522_0(MergingStateAccessor<K, W> context)
{    StateMerging.mergeCombiningValues(context, PANE_ADDITIONS_TAG);}
public Set<W> beam_f2525_0()
{        throw new java.lang.UnsupportedOperationException();}
public boolean beam_f2526_0(W window)
{        return true;}
public boolean beam_f2540_0()
{    return views.isEmpty();}
public boolean beam_f2541_0(PCollectionView<T> view)
{    return views.contains(view);}
public Result beam_f2542_0(DoFnInvoker<InputT, OutputT> invoker, final WindowedValue<InputT> element, final RestrictionTracker<RestrictionT, PositionT> tracker)
{    final ProcessContext processContext = new ProcessContext(element, tracker);    DoFn.ProcessContinuation cont = invoker.invokeProcessElement(new DoFnInvoker.ArgumentProvider<InputT, OutputT>() {        @Override        public DoFn<InputT, OutputT>.ProcessContext processContext(DoFn<InputT, OutputT> doFn) {            return processContext;        }        @Override        public InputT element(DoFn<InputT, OutputT> doFn) {            return processContext.element();        }        @Override        public Object sideInput(String tagId) {            throw new UnsupportedOperationException("Not supported in SplittableDoFn");        }        @Override        public Object schemaElement(int index) {            throw new UnsupportedOperationException("Not supported in SplittableDoFn");        }        @Override        public Instant timestamp(DoFn<InputT, OutputT> doFn) {            return processContext.timestamp();        }        @Override        public TimeDomain timeDomain(DoFn<InputT, OutputT> doFn) {            throw new UnsupportedOperationException("Access to time domain not supported in ProcessElement");        }        @Override        public OutputReceiver<OutputT> outputReceiver(DoFn<InputT, OutputT> doFn) {            return DoFnOutputReceivers.windowedReceiver(processContext, null);        }        @Override        public OutputReceiver<Row> outputRowReceiver(DoFn<InputT, OutputT> doFn) {            throw new UnsupportedOperationException("Not supported in SplittableDoFn");        }        @Override        public MultiOutputReceiver taggedOutputReceiver(DoFn<InputT, OutputT> doFn) {            return DoFnOutputReceivers.windowedMultiReceiver(processContext, null);        }        @Override        public RestrictionTracker<?, ?> restrictionTracker() {            return processContext.tracker;        }                @Override        public BoundedWindow window() {            throw new UnsupportedOperationException("Access to window of the element not supported in Splittable DoFn");        }        @Override        public PaneInfo paneInfo(DoFn<InputT, OutputT> doFn) {            throw new UnsupportedOperationException("Access to pane of the element not supported in Splittable DoFn");        }        @Override        public PipelineOptions pipelineOptions() {            return pipelineOptions;        }        @Override        public StartBundleContext startBundleContext(DoFn<InputT, OutputT> doFn) {            throw new IllegalStateException("Should not access startBundleContext() from @" + DoFn.ProcessElement.class.getSimpleName());        }        @Override        public FinishBundleContext finishBundleContext(DoFn<InputT, OutputT> doFn) {            throw new IllegalStateException("Should not access finishBundleContext() from @" + DoFn.ProcessElement.class.getSimpleName());        }        @Override        public DoFn<InputT, OutputT>.OnTimerContext onTimerContext(DoFn<InputT, OutputT> doFn) {            throw new UnsupportedOperationException("Access to timers not supported in Splittable DoFn");        }        @Override        public State state(String stateId) {            throw new UnsupportedOperationException("Access to state not supported in Splittable DoFn");        }        @Override        public Timer timer(String timerId) {            throw new UnsupportedOperationException("Access to timers not supported in Splittable DoFn");        }    });    processContext.cancelScheduledCheckpoint();    @Nullable    KV<RestrictionT, Instant> residual = processContext.getTakenCheckpoint();    if (cont.shouldResume()) {        checkState(!processContext.hasClaimFailed, "After tryClaim() returned false, @ProcessElement must return stop(), " + "but returned resume()");        if (residual == null) {                        if (processContext.numClaimedBlocks > 0) {                residual = checkNotNull(processContext.takeCheckpointNow());                processContext.tracker.checkDone();            } else {                                                                                                                                                                                                                                residual = KV.of(tracker.currentRestriction(), processContext.getLastReportedWatermark());                        }        } else {                                                                                                                        processContext.tracker.checkDone();        }    } else {                                        processContext.tracker.checkDone();    }    if (residual == null) {                        checkState(!cont.shouldResume());        return new Result(null, cont, BoundedWindow.TIMESTAMP_MAX_VALUE);    }    return new Result(residual.getKey(), cont, residual.getValue());}
public OutputReceiver<Row> beam_f2550_0(DoFn<InputT, OutputT> doFn)
{    throw new UnsupportedOperationException("Not supported in SplittableDoFn");}
public MultiOutputReceiver beam_f2551_0(DoFn<InputT, OutputT> doFn)
{    return DoFnOutputReceivers.windowedMultiReceiver(processContext, null);}
public RestrictionTracker<?, ?> beam_f2552_0()
{    return processContext.tracker;}
public Timer beam_f2560_0(String timerId)
{    throw new UnsupportedOperationException("Access to timers not supported in Splittable DoFn");}
public void beam_f2561_0(PositionT position)
{    checkState(!hasClaimFailed, "Must not call tryClaim() after it has previously returned false");    if (numClaimedBlocks == 0) {                                        this.scheduledCheckpoint = executor.schedule((Runnable) this::takeCheckpointNow, maxDuration.getMillis(), TimeUnit.MILLISECONDS);    }    ++numClaimedBlocks;}
public void beam_f2562_0(PositionT position)
{    checkState(!hasClaimFailed, "Must not call tryClaim() after it has previously returned false");    hasClaimFailed = true;}
public synchronized void beam_f2570_0(Instant watermark)
{                            lastReportedWatermark = watermark;}
 synchronized Instant beam_f2571_0()
{    return lastReportedWatermark;}
public PipelineOptions beam_f2572_0()
{    return pipelineOptions;}
public ReadableState<PaneInfo> beam_f2580_0(ReduceFn<?, ?, ?, ?>.Context context, final boolean isFinal)
{    final Object key = context.key();    final ReadableState<PaneInfo> previousPaneFuture = context.state().access(PaneInfoTracker.PANE_INFO_TAG);    final Instant windowMaxTimestamp = context.window().maxTimestamp();    return new ReadableState<PaneInfo>() {        @Override        public ReadableState<PaneInfo> readLater() {            previousPaneFuture.readLater();            return this;        }        @Override        public PaneInfo read() {            PaneInfo previousPane = previousPaneFuture.read();            return describePane(key, windowMaxTimestamp, previousPane, isFinal);        }    };}
public ReadableState<PaneInfo> beam_f2581_0()
{    previousPaneFuture.readLater();    return this;}
public PaneInfo beam_f2582_0()
{    PaneInfo previousPane = previousPaneFuture.read();    return describePane(key, windowMaxTimestamp, previousPane, isFinal);}
private void beam_f2590_0()
{    if (nextElementComputed) {        return;    }    if (!iterator.hasNext()) {        return;    }    nextElement = iterator.next();    nextElementComputed = true;}
public DoFn<KeyedWorkItem<byte[], KV<InputT, RestrictionT>>, OutputT> beam_f2591_0()
{    return underlying.getFn();}
public void beam_f2592_0()
{    underlying.startBundle();}
public ReduceFn<K, InputT, OutputT, W>.Context beam_f2602_0(W window, StateStyle style)
{    return new ContextImpl(stateAccessor(window, style));}
public ReduceFn<K, InputT, OutputT, W>.ProcessValueContext beam_f2603_0(W window, InputT value, Instant timestamp, StateStyle style)
{    return new ProcessValueContextImpl(stateAccessor(window, style), value, timestamp);}
public ReduceFn<K, InputT, OutputT, W>.OnTriggerContext beam_f2604_0(W window, PaneInfo pane, StateStyle style, OnTriggerCallbacks<OutputT> callbacks)
{    return new OnTriggerContextImpl(stateAccessor(window, style), pane, callbacks);}
protected StateNamespace beam_f2612_0(W window)
{    return StateNamespaces.window(windowCoder, window);}
protected StateNamespace beam_f2613_0()
{    return windowNamespace;}
 W beam_f2614_0()
{    return context.window();}
public W beam_f2622_0()
{    return state.window();}
public WindowingStrategy<?, W> beam_f2623_0()
{    return windowingStrategy;}
public StateAccessor<K> beam_f2624_0()
{    return state;}
public Timers beam_f2632_0()
{    return timers;}
public K beam_f2633_0()
{    return key;}
public W beam_f2634_0()
{    return state.window();}
public MergingStateAccessor<K, W> beam_f2642_0()
{    return state;}
public W beam_f2643_0()
{    return state.window();}
public Timers beam_f2644_0()
{    return timers;}
public T beam_f2652_0(PCollectionView<T> view)
{    return sideInputReader.get(view, view.getWindowMappingFn().getSideInputWindow(mainInputWindow));}
public W beam_f2653_0()
{    return mainInputWindow;}
private static StateContext<W> beam_f2654_0(final W window)
{    return new StateContext<W>() {        @Override        public PipelineOptions getPipelineOptions() {            throw new IllegalArgumentException("cannot call getPipelineOptions() in a window only context");        }        @Override        public <T> T sideInput(PCollectionView<T> view) {            throw new IllegalArgumentException("cannot call sideInput() in a window only context");        }        @Override        public W window() {            return window;        }    };}
private Collection<W> beam_f2662_0(Set<W> windows) throws Exception
{    Collection<W> result = new ArrayList<>();        for (W window : windows) {        ReduceFn<K, InputT, OutputT, W>.Context directContext = contextFactory.base(window, StateStyle.DIRECT);        if (triggerRunner.shouldFire(directContext.window(), directContext.timers(), directContext.state())) {            result.add(window);        }    }    return result;}
public void beam_f2663_0(Iterable<WindowedValue<InputT>> values) throws Exception
{    if (!values.iterator().hasNext()) {        return;    }        Set<W> windows = collectWindows(values);            Map<W, W> windowToMergeResult = mergeWindows(windows);    if (!windowToMergeResult.isEmpty()) {                                List<W> addedWindows = new ArrayList<>(windowToMergeResult.size());        for (Map.Entry<W, W> entry : windowToMergeResult.entrySet()) {            windows.remove(entry.getKey());            addedWindows.add(entry.getValue());        }        windows.addAll(addedWindows);    }    prefetchWindowsForValues(windows);        Set<W> windowsToConsider = windowsThatAreOpen(windows);        for (WindowedValue<InputT> value : values) {        processElement(windowToMergeResult, value);    }        for (W mergedWindow : windowsToConsider) {        triggerRunner.prefetchShouldFire(mergedWindow, contextFactory.base(mergedWindow, StateStyle.DIRECT).state());    }        Collection<W> windowsToFire = windowsThatShouldFire(windowsToConsider);        for (W window : windowsToFire) {        prefetchEmit(contextFactory.base(window, StateStyle.DIRECT), contextFactory.base(window, StateStyle.RENAMED));    }        for (W window : windowsToFire) {        emit(contextFactory.base(window, StateStyle.DIRECT), contextFactory.base(window, StateStyle.RENAMED));    }                activeWindows.cleanupTemporaryWindows();}
public void beam_f2664_0()
{    activeWindows.persist();}
private void beam_f2672_0(Map<W, W> windowToMergeResult, WindowedValue<InputT> value) throws Exception
{    ImmutableSet<W> windows = toMergedWindows(windowToMergeResult, value.getWindows());        for (W window : windows) {        ReduceFn<K, InputT, OutputT, W>.ProcessValueContext directContext = contextFactory.forValue(window, value.getValue(), value.getTimestamp(), StateStyle.DIRECT);        if (triggerRunner.isClosed(directContext.state())) {                        droppedDueToClosedWindow.inc();            WindowTracing.debug("ReduceFnRunner.processElement: Dropping element at {} for key:{}; window:{} " + "since window is no longer active at inputWatermark:{}; outputWatermark:{}", value.getTimestamp(), key, window, timerInternals.currentInputWatermarkTime(), timerInternals.currentOutputWatermarkTime());            continue;        }        activeWindows.ensureWindowIsActive(window);        ReduceFn<K, InputT, OutputT, W>.ProcessValueContext renamedContext = contextFactory.forValue(window, value.getValue(), value.getTimestamp(), StateStyle.RENAMED);        nonEmptyPanes.recordContent(renamedContext.state());        scheduleGarbageCollectionTimer(directContext);                                watermarkHold.addHolds(renamedContext);                reduceFn.processValue(renamedContext);                triggerRunner.processValue(directContext.window(), directContext.timestamp(), directContext.timers(), directContext.state());                    }}
public boolean beam_f2673_0()
{    return activeWindows.isActive(directContext.window()) && !triggerRunner.isClosed(directContext.state());}
public void beam_f2674_0(Iterable<TimerData> timers) throws Exception
{    if (!timers.iterator().hasNext()) {        return;    }            Map<BoundedWindow, WindowActivation> windowActivations = new HashMap();    for (TimerData timer : timers) {        checkArgument(timer.getNamespace() instanceof WindowNamespace, "Expected timer to be in WindowNamespace, but was in %s", timer.getNamespace());        @SuppressWarnings("unchecked")        WindowNamespace<W> windowNamespace = (WindowNamespace<W>) timer.getNamespace();        W window = windowNamespace.getWindow();        WindowTracing.debug("{}: Received timer key:{}; window:{}; data:{} with " + "inputWatermark:{}; outputWatermark:{}", ReduceFnRunner.class.getSimpleName(), key, window, timer, timerInternals.currentInputWatermarkTime(), timerInternals.currentOutputWatermarkTime());                if (TimeDomain.EVENT_TIME != timer.getDomain() && windowIsExpired(window)) {            continue;        }                if (windowActivations.containsKey(window)) {            continue;        }        ReduceFn<K, InputT, OutputT, W>.Context directContext = contextFactory.base(window, StateStyle.DIRECT);        ReduceFn<K, InputT, OutputT, W>.Context renamedContext = contextFactory.base(window, StateStyle.RENAMED);        WindowActivation windowActivation = new WindowActivation(directContext, renamedContext);        windowActivations.put(window, windowActivation);                if (windowActivation.isGarbageCollection) {            triggerRunner.prefetchIsClosed(directContext.state());        } else {            triggerRunner.prefetchShouldFire(directContext.window(), directContext.state());        }    }        for (WindowActivation timer : windowActivations.values()) {        if (timer.windowIsActiveAndOpen()) {            ReduceFn<K, InputT, OutputT, W>.Context directContext = timer.directContext;            if (timer.isGarbageCollection) {                prefetchOnTrigger(directContext, timer.renamedContext);            } else if (triggerRunner.shouldFire(directContext.window(), directContext.timers(), directContext.state())) {                prefetchEmit(directContext, timer.renamedContext);            }        }    }        for (WindowActivation windowActivation : windowActivations.values()) {        ReduceFn<K, InputT, OutputT, W>.Context directContext = windowActivation.directContext;        ReduceFn<K, InputT, OutputT, W>.Context renamedContext = windowActivation.renamedContext;        if (windowActivation.isGarbageCollection) {            WindowTracing.debug("{}: Cleaning up for key:{}; window:{} with inputWatermark:{}; outputWatermark:{}", ReduceFnRunner.class.getSimpleName(), key, directContext.window(), timerInternals.currentInputWatermarkTime(), timerInternals.currentOutputWatermarkTime());            boolean windowIsActiveAndOpen = windowActivation.windowIsActiveAndOpen();            if (windowIsActiveAndOpen) {                                                                @Nullable                Instant newHold = onTrigger(directContext, renamedContext, true, /* isFinished */                windowActivation.isEndOfWindow);                checkState(newHold == null, "Hold placed at %s despite isFinished being true.", newHold);            }                                    clearAllState(directContext, renamedContext, windowIsActiveAndOpen);        } else {            WindowTracing.debug("{}.onTimers: Triggering for key:{}; window:{} at {} with " + "inputWatermark:{}; outputWatermark:{}", key, directContext.window(), timerInternals.currentInputWatermarkTime(), timerInternals.currentOutputWatermarkTime());            if (windowActivation.windowIsActiveAndOpen() && triggerRunner.shouldFire(directContext.window(), directContext.timers(), directContext.state())) {                emit(directContext, renamedContext);            }            if (windowActivation.isEndOfWindow) {                                                                                                                                                                checkState(windowingStrategy.getAllowedLateness().isLongerThan(Duration.ZERO), "Unexpected zero getAllowedLateness");                Instant cleanupTime = LateDataUtils.garbageCollectionTime(directContext.window(), windowingStrategy);                WindowTracing.debug("ReduceFnRunner.onTimer: Scheduling cleanup timer for key:{}; window:{} at {} with " + "inputWatermark:{}; outputWatermark:{}", key, directContext.window(), cleanupTime, timerInternals.currentInputWatermarkTime(), timerInternals.currentOutputWatermarkTime());                checkState(!cleanupTime.isAfter(BoundedWindow.TIMESTAMP_MAX_VALUE), "Cleanup time %s is beyond end-of-time", cleanupTime);                directContext.timers().setTimer(cleanupTime, TimeDomain.EVENT_TIME);            }        }    }}
private void beam_f2682_0(ReduceFn<?, ?, ?, W>.Context directContext)
{    Instant inputWM = timerInternals.currentInputWatermarkTime();    Instant gcTime = LateDataUtils.garbageCollectionTime(directContext.window(), windowingStrategy);    WindowTracing.trace("ReduceFnRunner.scheduleGarbageCollectionTimer: Scheduling at {} for " + "key:{}; window:{} where inputWatermark:{}; outputWatermark:{}", gcTime, key, directContext.window(), inputWM, timerInternals.currentOutputWatermarkTime());    checkState(!gcTime.isAfter(BoundedWindow.TIMESTAMP_MAX_VALUE), "Timer %s is beyond end-of-time", gcTime);    directContext.timers().setTimer(gcTime, TimeDomain.EVENT_TIME);}
private void beam_f2683_0(ReduceFn<?, ?, ?, W>.Context directContext)
{    WindowTracing.debug("ReduceFnRunner.cancelEndOfWindowAndGarbageCollectionTimers: Deleting timers for " + "key:{}; window:{} where inputWatermark:{}; outputWatermark:{}", key, directContext.window(), timerInternals.currentInputWatermarkTime(), timerInternals.currentOutputWatermarkTime());    Instant eow = directContext.window().maxTimestamp();    directContext.timers().deleteTimer(eow, TimeDomain.EVENT_TIME);    Instant gc = LateDataUtils.garbageCollectionTime(directContext.window(), windowingStrategy);    if (gc.isAfter(eow)) {        directContext.timers().deleteTimer(gc, TimeDomain.EVENT_TIME);    }}
private boolean beam_f2684_0(BoundedWindow w)
{    return timerInternals.currentInputWatermarkTime().isAfter(w.maxTimestamp().plus(windowingStrategy.getAllowedLateness()));}
public boolean beam_f2692_0(PCollectionView<?> sideInput, BoundedWindow window)
{    Set<BoundedWindow> readyWindows = stateInternals.state(StateNamespaces.global(), availableWindowsTags.get(sideInput)).read();    return readyWindows != null && readyWindows.contains(window);}
public boolean beam_f2693_0(PCollectionView<T> view)
{    return sideInputs.contains(view);}
public boolean beam_f2694_0()
{    return sideInputs.isEmpty();}
public void beam_f2702_0(String timerId, BoundedWindow window, Instant timestamp, TimeDomain timeDomain)
{                    Instant effectiveTimestamp;    switch(timeDomain) {        case EVENT_TIME:            effectiveTimestamp = timestamp;            break;        case PROCESSING_TIME:        case SYNCHRONIZED_PROCESSING_TIME:            effectiveTimestamp = stepContext.timerInternals().currentInputWatermarkTime();            break;        default:            throw new IllegalArgumentException(String.format("Unknown time domain: %s", timeDomain));    }    OnTimerArgumentProvider argumentProvider = new OnTimerArgumentProvider(window, effectiveTimestamp, timeDomain);    invoker.invokeOnTimer(timerId, argumentProvider);}
private void beam_f2703_0(WindowedValue<InputT> elem)
{        try {        invoker.invokeProcessElement(new DoFnProcessContext(elem));    } catch (Exception ex) {        throw wrapUserCodeException(ex);    }}
public void beam_f2704_0()
{        try {        invoker.invokeFinishBundle(new DoFnFinishBundleContext());    } catch (Throwable t) {                throw wrapUserCodeException(t);    }}
public PipelineOptions beam_f2712_0()
{    return getPipelineOptions();}
public DoFn<InputT, OutputT>.StartBundleContext beam_f2713_0(DoFn<InputT, OutputT> doFn)
{    return this;}
public DoFn<InputT, OutputT>.FinishBundleContext beam_f2714_0(DoFn<InputT, OutputT> doFn)
{    throw new UnsupportedOperationException("Cannot access FinishBundleContext outside of @FinishBundle method.");}
public OutputReceiver<Row> beam_f2722_0(DoFn<InputT, OutputT> doFn)
{    throw new UnsupportedOperationException("Cannot access output receiver outside of @ProcessElement method.");}
public MultiOutputReceiver beam_f2723_0(DoFn<InputT, OutputT> doFn)
{    throw new UnsupportedOperationException("Cannot access output receiver outside of @ProcessElement method.");}
public DoFn<InputT, OutputT>.OnTimerContext beam_f2724_0(DoFn<InputT, OutputT> doFn)
{    throw new UnsupportedOperationException("Cannot access OnTimerContext outside of @OnTimer methods.");}
public DoFn<InputT, OutputT>.StartBundleContext beam_f2732_0(DoFn<InputT, OutputT> doFn)
{    throw new UnsupportedOperationException("Cannot access StartBundleContext outside of @StartBundle method.");}
public DoFn<InputT, OutputT>.FinishBundleContext beam_f2733_0(DoFn<InputT, OutputT> doFn)
{    return this;}
public DoFn<InputT, OutputT>.ProcessContext beam_f2734_0(DoFn<InputT, OutputT> doFn)
{    throw new UnsupportedOperationException("Cannot access ProcessContext outside of @ProcessElement method.");}
public MultiOutputReceiver beam_f2742_0(DoFn<InputT, OutputT> doFn)
{    throw new UnsupportedOperationException("Cannot access outputReceiver in @FinishBundle method.");}
public DoFn<InputT, OutputT>.OnTimerContext beam_f2743_0(DoFn<InputT, OutputT> doFn)
{    throw new UnsupportedOperationException("Cannot access OnTimerContext outside of @OnTimer methods.");}
public RestrictionTracker<?, ?> beam_f2744_0()
{    throw new UnsupportedOperationException("Cannot access RestrictionTracker outside of @ProcessElement method.");}
public T beam_f2752_0(PCollectionView<T> view)
{    checkNotNull(view, "View passed to sideInput cannot be null");    BoundedWindow window = Iterables.getOnlyElement(windows());    return SimpleDoFnRunner.this.sideInput(view, view.getWindowMappingFn().getSideInputWindow(window));}
public PaneInfo beam_f2753_0()
{    return elem.getPane();}
public void beam_f2754_0(Instant watermark)
{    throw new UnsupportedOperationException("Only splittable DoFn's can use updateWatermark()");}
public BoundedWindow beam_f2762_0()
{    return Iterables.getOnlyElement(elem.getWindows());}
public PaneInfo beam_f2763_0(DoFn<InputT, OutputT> doFn)
{    return pane();}
public PipelineOptions beam_f2764_0()
{    return getPipelineOptions();}
public TimeDomain beam_f2772_0(DoFn<InputT, OutputT> doFn)
{    throw new UnsupportedOperationException("Cannot access time domain outside of @ProcessTimer method.");}
public OutputReceiver<OutputT> beam_f2773_0(DoFn<InputT, OutputT> doFn)
{    return DoFnOutputReceivers.windowedReceiver(this, mainOutputTag);}
public OutputReceiver<Row> beam_f2774_0(DoFn<InputT, OutputT> doFn)
{    return DoFnOutputReceivers.rowReceiver(this, mainOutputTag, mainOutputSchemaCoder);}
public BoundedWindow beam_f2782_0()
{    return window;}
public PaneInfo beam_f2783_0(DoFn<InputT, OutputT> doFn)
{    throw new UnsupportedOperationException("Cannot access paneInfo outside of @ProcessElement methods.");}
public PipelineOptions beam_f2784_0()
{    return getPipelineOptions();}
public Instant beam_f2792_0(DoFn<InputT, OutputT> doFn)
{    return timestamp();}
public TimeDomain beam_f2793_0(DoFn<InputT, OutputT> doFn)
{    return timeDomain();}
public OutputReceiver<OutputT> beam_f2794_0(DoFn<InputT, OutputT> doFn)
{    return DoFnOutputReceivers.windowedReceiver(this, mainOutputTag);}
public void beam_f2802_0(OutputT output)
{    output(mainOutputTag, output);}
public void beam_f2803_0(OutputT output, Instant timestamp)
{    outputWithTimestamp(mainOutputTag, output, timestamp);}
public void beam_f2804_0(TupleTag<T> tag, T output)
{    outputWindowedValue(tag, WindowedValue.of(output, timestamp, window(), PaneInfo.NO_FIRING));}
private Instant beam_f2812_0()
{    switch(spec.getTimeDomain()) {        case EVENT_TIME:            return timerInternals.currentInputWatermarkTime();        case PROCESSING_TIME:            return timerInternals.currentProcessingTime();        case SYNCHRONIZED_PROCESSING_TIME:            return timerInternals.currentSynchronizedProcessingTime();        default:            throw new IllegalStateException(String.format("Timer created for unknown time domain %s", spec.getTimeDomain()));    }}
public static SimplePushbackSideInputDoFnRunner<InputT, OutputT> beam_f2813_0(DoFnRunner<InputT, OutputT> underlying, Collection<PCollectionView<?>> views, ReadyCheckingSideInputReader sideInputReader)
{    return new SimplePushbackSideInputDoFnRunner<>(underlying, views, sideInputReader);}
public DoFn<InputT, OutputT> beam_f2814_0()
{    return underlying.getFn();}
public RunnerApi.FunctionSpec beam_f2822_0()
{    throw new UnsupportedOperationException(String.format("%s should never be serialized to proto", getClass().getSimpleName()));}
public PTransformReplacement<PCollection<KV<byte[], KV<InputT, RestrictionT>>>, PCollectionTuple> beam_f2823_0(AppliedPTransform<PCollection<KV<byte[], KV<InputT, RestrictionT>>>, PCollectionTuple, ProcessKeyedElements<InputT, OutputT, RestrictionT>> transform)
{    return PTransformReplacement.of(PTransformReplacements.getSingletonMainInput(transform), new SplittableProcessViaKeyedWorkItems<>(transform.getTransform()));}
public Map<PValue, ReplacementOutput> beam_f2824_0(Map<TupleTag<?>, PValue> outputs, PCollectionTuple newOutput)
{    return ReplacementOutputs.tagged(outputs, newOutput);}
public void beam_f2832_0(StateInternalsFactory<byte[]> stateInternalsFactory)
{    this.stateInternalsFactory = stateInternalsFactory;}
public void beam_f2833_0(TimerInternalsFactory<byte[]> timerInternalsFactory)
{    this.timerInternalsFactory = timerInternalsFactory;}
public void beam_f2834_0(SplittableProcessElementInvoker<InputT, OutputT, RestrictionT, PositionT> invoker)
{    this.processElementInvoker = invoker;}
public void beam_f2842_0(FinishBundleContext c) throws Exception
{    invoker.invokeFinishBundle(wrapContextAsFinishBundle(c));}
public void beam_f2843_0(final ProcessContext c)
{    byte[] key = c.element().key();    StateInternals stateInternals = stateInternalsFactory.stateInternalsForKey(key);    TimerInternals timerInternals = timerInternalsFactory.timerInternalsForKey(key);                TimerInternals.TimerData timer = Iterables.getOnlyElement(c.element().timersIterable(), null);    boolean isSeedCall = (timer == null);    StateNamespace stateNamespace;    if (isSeedCall) {        WindowedValue<KV<InputT, RestrictionT>> windowedValue = Iterables.getOnlyElement(c.element().elementsIterable());        BoundedWindow window = Iterables.getOnlyElement(windowedValue.getWindows());        stateNamespace = StateNamespaces.window((Coder<BoundedWindow>) inputWindowingStrategy.getWindowFn().windowCoder(), window);    } else {        stateNamespace = timer.getNamespace();    }    ValueState<WindowedValue<InputT>> elementState = stateInternals.state(stateNamespace, elementTag);    ValueState<RestrictionT> restrictionState = stateInternals.state(stateNamespace, restrictionTag);    WatermarkHoldState holdState = stateInternals.state(stateNamespace, watermarkHoldTag);    KV<WindowedValue<InputT>, RestrictionT> elementAndRestriction;    if (isSeedCall) {        WindowedValue<KV<InputT, RestrictionT>> windowedValue = Iterables.getOnlyElement(c.element().elementsIterable());        WindowedValue<InputT> element = windowedValue.withValue(windowedValue.getValue().getKey());        elementState.write(element);        elementAndRestriction = KV.of(element, windowedValue.getValue().getValue());    } else {                        elementState.readLater();        restrictionState.readLater();        elementAndRestriction = KV.of(elementState.read(), restrictionState.read());    }    final RestrictionTracker<RestrictionT, PositionT> tracker = invoker.invokeNewTracker(elementAndRestriction.getValue());    SplittableProcessElementInvoker<InputT, OutputT, RestrictionT, PositionT>.Result result = processElementInvoker.invokeProcessElement(invoker, elementAndRestriction.getKey(), tracker);        if (result.getResidualRestriction() == null) {                elementState.clear();        restrictionState.clear();        holdState.clear();        return;    }    restrictionState.write(result.getResidualRestriction());    @Nullable    Instant futureOutputWatermark = result.getFutureOutputWatermark();    if (futureOutputWatermark == null) {        futureOutputWatermark = elementAndRestriction.getKey().getTimestamp();    }    Instant wakeupTime = timerInternals.currentProcessingTime().plus(result.getContinuation().resumeDelay());    holdState.add(futureOutputWatermark);        timerInternals.setTimer(TimerInternals.TimerData.of(stateNamespace, wakeupTime, TimeDomain.PROCESSING_TIME));}
private DoFn<InputT, OutputT>.StartBundleContext beam_f2844_0(final StartBundleContext baseContext)
{    return fn.new StartBundleContext() {        @Override        public PipelineOptions getPipelineOptions() {            return baseContext.getPipelineOptions();        }    };}
public DoFn.ProcessContinuation beam_f2852_0()
{    return continuation;}
public Instant beam_f2853_0()
{    return futureOutputWatermark;}
private void beam_f2854_0(WindowFn<?, ?> windowFn)
{    if (!(windowFn instanceof NonMergingWindowFn)) {        throw new UnsupportedOperationException("MergingWindowFn is not supported for stateful DoFns, WindowFn is: " + windowFn);    }}
public void beam_f2862_0(InputT input, BoundedWindow window)
{    Instant gcTime = LateDataUtils.garbageCollectionTime(window, windowingStrategy);        gcTime = gcTime.plus(GC_DELAY_MS);    timerInternals.setTimer(StateNamespaces.window(windowCoder, window), GC_TIMER_ID, gcTime, TimeDomain.EVENT_TIME);}
public boolean beam_f2863_0(String timerId, BoundedWindow window, Instant timestamp, TimeDomain timeDomain)
{    boolean isEventTimer = timeDomain.equals(TimeDomain.EVENT_TIME);    Instant gcTime = LateDataUtils.garbageCollectionTime(window, windowingStrategy);    gcTime = gcTime.plus(GC_DELAY_MS);    return isEventTimer && GC_TIMER_ID.equals(timerId) && gcTime.equals(timestamp);}
public void beam_f2864_0(W window)
{    for (Map.Entry<String, DoFnSignature.StateDeclaration> entry : signature.stateDeclarations().entrySet()) {        try {            StateSpec<?> spec = (StateSpec<?>) entry.getValue().field().get(fn);            State state = stateInternals.state(StateNamespaces.window(windowCoder, window), StateTags.tagForSpec(entry.getKey(), (StateSpec) spec));            state.clear();        } catch (IllegalAccessException e) {            throw new RuntimeException(e);        }    }}
public static void beam_f2872_0(MergingStateAccessor<K, W> context, StateTag<StateT> address)
{    for (StateT state : context.accessInEachMergingWindow(address).values()) {        prefetchRead(state);    }}
public static void beam_f2873_0(MergingStateAccessor<K, W> context, StateTag<CombiningState<InputT, AccumT, OutputT>> address)
{    mergeCombiningValues(context.accessInEachMergingWindow(address).values(), context.access(address));}
public static void beam_f2874_0(Collection<CombiningState<InputT, AccumT, OutputT>> sources, CombiningState<InputT, AccumT, OutputT> result)
{    if (sources.isEmpty()) {                return;    }    if (sources.size() == 1 && sources.contains(result)) {                return;    }        List<ReadableState<AccumT>> futures = new ArrayList<>(sources.size());    for (CombiningState<InputT, AccumT, OutputT> source : sources) {        prefetchRead(source);    }        List<AccumT> accumulators = new ArrayList<>(futures.size());    for (CombiningState<InputT, AccumT, OutputT> source : sources) {        accumulators.add(source.getAccum());    }        AccumT merged = result.mergeAccumulators(accumulators);        for (CombiningState<InputT, AccumT, OutputT> source : sources) {        source.clear();    }        result.addAccum(merged);}
public static StateNamespace beam_f2882_0(Coder<W> windowCoder, W window)
{    return new WindowNamespace<>(windowCoder, window);}
public static StateNamespace beam_f2883_0(Coder<W> windowCoder, W window, int triggerIdx)
{    return new WindowAndTriggerNamespace<>(windowCoder, window, triggerIdx);}
public String beam_f2884_0()
{    return GLOBAL_STRING;}
public void beam_f2892_0(Appendable sb) throws IOException
{    sb.append('/').append(CoderUtils.encodeToBase64(windowCoder, window)).append('/');}
public Object beam_f2893_0()
{    return window;}
public boolean beam_f2894_0(Object obj)
{    if (obj == this) {        return true;    }    if (!(obj instanceof WindowNamespace)) {        return false;    }    WindowNamespace<?> that = (WindowNamespace<?>) obj;    return Objects.equals(this.windowStructuralValue(), that.windowStructuralValue());}
public Object beam_f2902_0()
{    return window;}
public boolean beam_f2903_0(Object obj)
{    if (obj == this) {        return true;    }    if (!(obj instanceof WindowAndTriggerNamespace)) {        return false;    }    WindowAndTriggerNamespace<?> that = (WindowAndTriggerNamespace<?>) obj;    return this.triggerIndex == that.triggerIndex && Objects.equals(this.windowStructuralValue(), that.windowStructuralValue());}
private Object beam_f2904_0()
{    return windowCoder.structuralValue(window);}
public Iterable<State> beam_f2912_0()
{    return stateTable.values();}
public boolean beam_f2913_0(StateNamespace namespace)
{    return stateTable.containsRow(namespace);}
public Map<StateTag, State> beam_f2914_0(StateNamespace namespace)
{                Map<Equivalence.Wrapper<StateTag>, State> row = stateTable.row(namespace);    HashMap<StateTag, State> result = new HashMap<>();    for (Map.Entry<Equivalence.Wrapper<StateTag>, State> entry : row.entrySet()) {        result.put(entry.getKey().get(), entry.getValue());    }    return result;}
public MapState<KeyT, ValueT> beam_f2922_0(String id, StateSpec<MapState<KeyT, ValueT>> spec, Coder<KeyT> mapKeyCoder, Coder<ValueT> mapValueCoder)
{    return binder.bindMap(tagForSpec(id, spec), mapKeyCoder, mapValueCoder);}
public CombiningState<InputT, AccumT, OutputT> beam_f2923_0(String id, StateSpec<CombiningState<InputT, AccumT, OutputT>> spec, Coder<AccumT> accumCoder, CombineFn<InputT, AccumT, OutputT> combineFn)
{    return binder.bindCombiningValue(tagForSpec(id, spec), accumCoder, combineFn);}
public CombiningState<InputT, AccumT, OutputT> beam_f2924_0(String id, StateSpec<CombiningState<InputT, AccumT, OutputT>> spec, Coder<AccumT> accumCoder, CombineFnWithContext<InputT, AccumT, OutputT> combineFn)
{    return binder.bindCombiningValueWithContext(tagForSpec(id, spec), accumCoder, combineFn);}
public static StateTag<SetState<T>> beam_f2932_0(String id, Coder<T> elemCoder)
{    return new SimpleStateTag<>(new StructuredId(id), StateSpecs.set(elemCoder));}
public static StateTag<MapState<K, V>> beam_f2933_0(String id, Coder<K> keyCoder, Coder<V> valueCoder)
{    return new SimpleStateTag<>(new StructuredId(id), StateSpecs.map(keyCoder, valueCoder));}
public static StateTag<WatermarkHoldState> beam_f2934_0(String id, TimestampCombiner timestampCombiner)
{    return new SimpleStateTag<>(new StructuredId(id), StateSpecs.watermarkStateInternal(timestampCombiner));}
public int beam_f2942_0()
{    return Objects.hash(kind, rawId);}
public StateT beam_f2943_0(StateTag.StateBinder binder)
{    return spec.bind(this.id.getRawId(), adaptTagBinder(binder));}
public String beam_f2944_0()
{    return id.getRawId();}
public void beam_f2952_0(MergingStateAccessor<K, W> state) throws Exception
{    StateMerging.prefetchBags(state, bufferTag);}
public void beam_f2953_0(OnMergeContext c) throws Exception
{    StateMerging.mergeBags(c.state(), bufferTag);}
public static SystemReduceFn<K, InputT, AccumT, OutputT, W> beam_f2954_0(final Coder<K> keyCoder, final AppliedCombineFn<K, InputT, AccumT, OutputT> combineFn)
{    final StateTag<CombiningState<InputT, AccumT, OutputT>> bufferTag;    if (combineFn.getFn() instanceof CombineFnWithContext) {        bufferTag = StateTags.makeSystemTagInternal(StateTags.combiningValueWithContext(BUFFER_NAME, combineFn.getAccumulatorCoder(), (CombineFnWithContext<InputT, AccumT, OutputT>) combineFn.getFn()));    } else {        bufferTag = StateTags.makeSystemTagInternal(StateTags.combiningValue(BUFFER_NAME, combineFn.getAccumulatorCoder(), (CombineFn<InputT, AccumT, OutputT>) combineFn.getFn()));    }    return new SystemReduceFn<K, InputT, AccumT, OutputT, W>(bufferTag) {        @Override        public void prefetchOnMerge(MergingStateAccessor<K, W> state) throws Exception {            StateMerging.prefetchCombiningValues(state, bufferTag);        }        @Override        public void onMerge(OnMergeContext c) throws Exception {            StateMerging.mergeCombiningValues(c.state(), bufferTag);        }    };}
public ReadableState<Boolean> beam_f2962_0(StateAccessor<K> state)
{    return state.access(bufferTag).isEmpty();}
public Set<StateTag> beam_f2963_0(StateNamespace namespace)
{    Set<StateTag> inUse = new HashSet<>();    for (Map.Entry<StateTag, State> entry : inMemoryState.getTagsInUse(namespace).entrySet()) {        if (!isEmptyForTesting(entry.getValue())) {            inUse.add(entry.getKey());        }    }    return inUse;}
public Set<StateNamespace> beam_f2964_0()
{    return inMemoryState.getNamespacesInUse();}
public List<? extends Coder<?>> beam_f2972_0()
{    return Arrays.asList(windowCoder);}
public void beam_f2973_0() throws NonDeterministicException
{    verifyDeterministic(this, "window coder must be deterministic", windowCoder);}
public static TriggerStateMachine beam_f2974_0(TriggerStateMachine... triggers)
{    return new AfterAllStateMachine(Arrays.asList(triggers));}
public TimeDomain beam_f2982_0()
{    return timeDomain;}
public List<SerializableFunction<Instant, Instant>> beam_f2983_0()
{    return timestampMappers;}
public AfterDelayFromFirstElementStateMachine beam_f2984_0(final Duration size, final Instant offset)
{    return newWith(new AlignFn(size, offset));}
public void beam_f2992_0(OnMergeContext c) throws Exception
{        if (c.trigger().isFinished()) {        StateMerging.clear(c.state(), DELAYED_UNTIL_TAG);                return;    }        StateMerging.mergeCombiningValues(c.state(), DELAYED_UNTIL_TAG);    Instant earliestTargetTime = c.state().access(DELAYED_UNTIL_TAG).read();    if (earliestTargetTime != null) {        c.setTimer(earliestTargetTime, timeDomain);    }}
public void beam_f2993_0(StateAccessor<?> state)
{    state.access(DELAYED_UNTIL_TAG).readLater();}
public void beam_f2994_0(TriggerContext c) throws Exception
{    c.state().access(DELAYED_UNTIL_TAG).clear();}
public Instant beam_f3002_0(Instant point)
{    long millisSinceStart = new Duration(offset, point).getMillis() % size.getMillis();    return millisSinceStart == 0 ? point : point.plus(size).minus(millisSinceStart);}
public boolean beam_f3003_0(Object object)
{    if (object == this) {        return true;    }    if (!(object instanceof AlignFn)) {        return false;    }    AlignFn other = (AlignFn) object;    return other.size.equals(this.size) && other.offset.equals(this.offset);}
public int beam_f3004_0()
{    return Objects.hash(size, offset);}
private void beam_f3012_0(TriggerContext context)
{    context.trigger().setFinished(context.trigger().firstUnfinishedSubTrigger() == null);}
public static TriggerStateMachine beam_f3013_0(TriggerStateMachine... triggers)
{    return new AfterFirstStateMachine(Arrays.asList(triggers));}
public static TriggerStateMachine beam_f3014_0(Iterable<? extends TriggerStateMachine> triggers)
{    return new AfterFirstStateMachine(ImmutableList.copyOf(triggers));}
public static AfterPaneStateMachine beam_f3022_0(int countElems)
{    return new AfterPaneStateMachine(countElems);}
public void beam_f3023_0(OnElementContext c) throws Exception
{    c.state().access(ELEMENTS_IN_PANE_TAG).add(1L);}
public void beam_f3024_0(MergingStateAccessor<?, ?> state)
{    super.prefetchOnMerge(state);    StateMerging.prefetchCombiningValues(state, ELEMENTS_IN_PANE_TAG);}
public int beam_f3032_0()
{    return Objects.hash(countElems);}
public void beam_f3033_0(TriggerStateMachine.TriggerContext context) throws Exception
{    clear(context);    context.trigger().setFinished(true);}
public Instant beam_f3034_0(TriggerStateMachine.TriggerContext context)
{    return context.currentProcessingTime();}
public String beam_f3042_0()
{    return "AfterSynchronizedProcessingTime.pastFirstElementInPane()";}
public boolean beam_f3043_0(Object obj)
{    return this == obj || obj instanceof AfterSynchronizedProcessingTimeStateMachine;}
public int beam_f3044_0()
{    return Objects.hashCode(AfterSynchronizedProcessingTimeStateMachine.class);}
public boolean beam_f3052_0(TriggerStateMachine.TriggerContext context) throws Exception
{    if (!context.trigger().isFinished(EARLY_INDEX)) {                return context.trigger().subTrigger(EARLY_INDEX).invokeShouldFire(context) || endOfWindowReached(context);    } else if (lateTrigger == null) {        return false;    } else {                return context.trigger().subTrigger(LATE_INDEX).invokeShouldFire(context);    }}
public void beam_f3053_0(TriggerStateMachine.TriggerContext context) throws Exception
{    if (!context.forTrigger(context.trigger().subTrigger(EARLY_INDEX)).trigger().isFinished()) {        onNonLateFiring(context);    } else if (lateTrigger != null) {        onLateFiring(context);    } else {                context.trigger().setFinished(true);    }}
public String beam_f3054_0()
{    StringBuilder builder = new StringBuilder(TO_STRING);    if (!(earlyTrigger instanceof NeverStateMachine)) {        builder.append(".withEarlyFirings(").append(earlyTrigger).append(")");    }    if (lateTrigger != null && !(lateTrigger instanceof NeverStateMachine)) {        builder.append(".withLateFirings(").append(lateTrigger).append(")");    }    return builder.toString();}
public boolean beam_f3062_0(Object obj)
{    return obj instanceof FromEndOfWindow;}
public int beam_f3063_0()
{    return Objects.hash(getClass());}
public boolean beam_f3064_0(TriggerStateMachine.TriggerContext context) throws Exception
{    return endOfWindowReached(context);}
private boolean beam_f3073_0(TriggerStateMachine.TriggerContext context)
{    return context.currentEventTime() != null && context.currentEventTime().isAfter(context.window().maxTimestamp());}
public static ExecutableTriggerStateMachine beam_f3075_0(TriggerStateMachine trigger)
{    return create(trigger, 0);}
private static ExecutableTriggerStateMachine beam_f3076_0(TriggerStateMachine trigger, int nextUnusedIndex)
{    return new ExecutableTriggerStateMachine(trigger, nextUnusedIndex);}
public ExecutableTriggerStateMachine beam_f3084_0(int index)
{    checkNotNull(subTriggers);    checkState(index > triggerIndex && index < firstIndexAfterSubtree, "Cannot find sub-trigger containing index not in this tree.");    ExecutableTriggerStateMachine previous = null;    for (ExecutableTriggerStateMachine subTrigger : subTriggers) {        if (index < subTrigger.triggerIndex) {            return previous;        }        previous = subTrigger;    }    return previous;}
public void beam_f3085_0(TriggerStateMachine.OnElementContext c) throws Exception
{    trigger.onElement(c.forTrigger(this));}
public void beam_f3086_0(TriggerStateMachine.OnMergeContext c) throws Exception
{    TriggerStateMachine.OnMergeContext subContext = c.forTrigger(this);    trigger.onMerge(subContext);}
public void beam_f3094_0(ExecutableTriggerStateMachine trigger, boolean value)
{    bitSet.set(trigger.getTriggerIndex(), value);}
public void beam_f3095_0(ExecutableTriggerStateMachine trigger)
{    bitSet.clear(trigger.getTriggerIndex(), trigger.getFirstIndexAfterSubtree());}
public FinishedTriggersBitSet beam_f3096_0()
{    return new FinishedTriggersBitSet((BitSet) bitSet.clone());}
public boolean beam_f3106_0(TriggerStateMachine.TriggerContext context)
{    return false;}
public void beam_f3107_0(TriggerStateMachine.TriggerContext context)
{    throw new UnsupportedOperationException(String.format("%s should never fire", getClass().getSimpleName()));}
public void beam_f3108_0(OnElementContext c) throws Exception
{    c.trigger().subTrigger(ACTUAL).invokeOnElement(c);    c.trigger().subTrigger(UNTIL).invokeOnElement(c);}
public void beam_f3116_0(OnMergeContext c) throws Exception
{    getRepeated(c).invokeOnMerge(c);}
public boolean beam_f3117_0(TriggerStateMachine.TriggerContext context) throws Exception
{    return getRepeated(context).invokeShouldFire(context);}
public void beam_f3118_0(TriggerContext context) throws Exception
{    getRepeated(context).invokeOnFire(context);    if (context.trigger().isFinished(REPEATED)) {                context.forTrigger(getRepeated(context)).trigger().resetTree();    }}
public void beam_f3129_0(StateAccessor<?> state)
{    if (subTriggers != null) {        for (TriggerStateMachine trigger : subTriggers) {            trigger.prefetchShouldFire(state);        }    }}
public void beam_f3130_0(StateAccessor<?> state)
{    if (subTriggers != null) {        for (TriggerStateMachine trigger : subTriggers) {            trigger.prefetchOnFire(state);        }    }}
public void beam_f3131_0(TriggerContext c) throws Exception
{    if (subTriggers != null) {        for (ExecutableTriggerStateMachine trigger : c.trigger().subTriggers()) {            trigger.invokeClear(c);        }    }}
public TriggerStateMachine.OnElementContext beam_f3139_0(W window, Timers timers, Instant elementTimestamp, ExecutableTriggerStateMachine rootTrigger, FinishedTriggers finishedSet)
{    return new OnElementContextImpl(window, timers, rootTrigger, finishedSet, elementTimestamp);}
public TriggerStateMachine.OnMergeContext beam_f3140_0(W window, Timers timers, ExecutableTriggerStateMachine rootTrigger, FinishedTriggers finishedSet, Map<W, FinishedTriggers> finishedSets)
{    return new OnMergeContextImpl(window, timers, rootTrigger, finishedSet, finishedSets);}
public StateAccessor<?> beam_f3141_0(W window, ExecutableTriggerStateMachine trigger)
{    return new StateAccessorImpl(window, trigger);}
public Iterable<ExecutableTriggerStateMachine> beam_f3149_0()
{    return FluentIterable.from(trigger.subTriggers()).filter(trigger -> !finishedSet.isFinished(trigger));}
public ExecutableTriggerStateMachine beam_f3150_0()
{    for (ExecutableTriggerStateMachine subTrigger : trigger.subTriggers()) {        if (!finishedSet.isFinished(subTrigger)) {            return subTrigger;        }    }    return null;}
public void beam_f3151_0() throws Exception
{    finishedSet.clearRecursively(trigger);    trigger.invokeClear(context);}
public boolean beam_f3159_0()
{    for (FinishedTriggers finishedSet : finishedSets.values()) {        if (finishedSet.isFinished(trigger)) {            return true;        }    }    return false;}
public boolean beam_f3160_0()
{    for (FinishedTriggers finishedSet : finishedSets.values()) {        if (!finishedSet.isFinished(trigger)) {            return false;        }    }    return true;}
protected StateNamespace beam_f3161_0(W window)
{    return StateNamespaces.windowAndTrigger(windowCoder, window, triggerIndex);}
public void beam_f3169_0(Instant timestamp, TimeDomain domain)
{    timers.deleteTimer(timestamp, domain);}
public Instant beam_f3170_0()
{    return timers.currentProcessingTime();}
public Instant beam_f3171_0()
{    return timers.currentSynchronizedProcessingTime();}
public void beam_f3179_0(Instant timestamp, TimeDomain domain)
{    timers.deleteTimer(timestamp, domain);}
public Instant beam_f3180_0()
{    return timers.currentProcessingTime();}
public Instant beam_f3181_0()
{    return timers.currentSynchronizedProcessingTime();}
public Instant beam_f3189_0()
{    return timers.currentProcessingTime();}
public Instant beam_f3190_0()
{    return timers.currentSynchronizedProcessingTime();}
public Instant beam_f3191_0()
{    return timers.currentEventTime();}
public void beam_f3199_0(W window, Instant timestamp, Timers timers, StateAccessor<?> state) throws Exception
{        FinishedTriggersBitSet finishedSet = readFinishedBits(state.access(FINISHED_BITS_TAG)).copy();    TriggerStateMachine.OnElementContext triggerContext = contextFactory.createOnElementContext(window, timers, timestamp, rootTrigger, finishedSet);    rootTrigger.invokeOnElement(triggerContext);    persistFinishedSet(state, finishedSet);}
public void beam_f3200_0(W window, Collection<W> mergingWindows, MergingStateAccessor<?, W> state)
{    if (isFinishedSetNeeded()) {        for (ValueState<?> value : state.accessInEachMergingWindow(FINISHED_BITS_TAG).values()) {            value.readLater();        }    }    rootTrigger.getSpec().prefetchOnMerge(contextFactory.createMergingStateAccessor(window, mergingWindows, rootTrigger));}
public void beam_f3201_0(W window, Timers timers, MergingStateAccessor<?, W> state) throws Exception
{        FinishedTriggersBitSet finishedSet = readFinishedBits(state.access(FINISHED_BITS_TAG)).copy();        ImmutableMap.Builder<W, FinishedTriggers> builder = ImmutableMap.builder();    for (Map.Entry<W, ValueState<BitSet>> entry : state.accessInEachMergingWindow(FINISHED_BITS_TAG).entrySet()) {                builder.put(entry.getKey(), readFinishedBits(entry.getValue()));                clearFinishedBits(entry.getValue());    }    ImmutableMap<W, FinishedTriggers> mergingFinishedSets = builder.build();    TriggerStateMachine.OnMergeContext mergeContext = contextFactory.createOnMergeContext(window, timers, rootTrigger, finishedSet, mergingFinishedSets);        rootTrigger.invokeOnMerge(mergeContext);    persistFinishedSet(state, finishedSet);}
private static TriggerStateMachine beam_f3209_0(RunnerApi.Trigger.AfterEndOfWindow trigger)
{    if (!trigger.hasEarlyFirings() && !trigger.hasLateFirings()) {        return AfterWatermarkStateMachine.pastEndOfWindow();    } else {        AfterWatermarkStateMachine.AfterWatermarkEarlyAndLate machine = AfterWatermarkStateMachine.pastEndOfWindow().withEarlyFirings(stateMachineForTrigger(trigger.getEarlyFirings()));        if (trigger.hasLateFirings()) {            machine = machine.withLateFirings(stateMachineForTrigger(trigger.getLateFirings()));        }        return machine;    }}
private static TriggerStateMachine beam_f3210_0(RunnerApi.Trigger.AfterProcessingTime trigger)
{    AfterDelayFromFirstElementStateMachine stateMachine = AfterProcessingTimeStateMachine.pastFirstElementInPane();    for (RunnerApi.TimestampTransform transform : trigger.getTimestampTransformsList()) {        switch(transform.getTimestampTransformCase()) {            case ALIGN_TO:                stateMachine = stateMachine.alignedTo(Duration.millis(transform.getAlignTo().getPeriod()), new Instant(transform.getAlignTo().getOffset()));                break;            case DELAY:                stateMachine = stateMachine.plusDelayOf(Duration.millis(transform.getDelay().getDelayMillis()));                break;            case TIMESTAMPTRANSFORM_NOT_SET:                throw new IllegalArgumentException(String.format("Required field 'timestamp_transform' not set in %s", transform));            default:                throw new IllegalArgumentException(String.format("Unknown timestamp transform case: %s", transform.getTimestampTransformCase()));        }    }    return stateMachine;}
private static List<TriggerStateMachine> beam_f3211_0(List<RunnerApi.Trigger> triggers)
{    List<TriggerStateMachine> stateMachines = new ArrayList<>(triggers.size());    for (RunnerApi.Trigger trigger : triggers) {        stateMachines.add(stateMachineForTrigger(trigger));    }    return stateMachines;}
private Instant beam_f3219_0(ReduceFn<?, ?, ?, W>.Context context, boolean paneIsEmpty)
{    Instant outputWM = timerInternals.currentOutputWatermarkTime();    Instant inputWM = timerInternals.currentInputWatermarkTime();    Instant gcHold = LateDataUtils.garbageCollectionTime(context.window(), windowingStrategy);    if (gcHold.isBefore(inputWM)) {        WindowTracing.trace("{}.addGarbageCollectionHold: gc hold would be before the input watermark " + "for key:{}; window: {}; inputWatermark: {}; outputWatermark: {}", getClass().getSimpleName(), context.key(), context.window(), inputWM, outputWM);        return null;    }    if (paneIsEmpty && context.windowingStrategy().getClosingBehavior() == ClosingBehavior.FIRE_IF_NON_EMPTY) {        WindowTracing.trace("WatermarkHold.addGarbageCollectionHold: garbage collection hold at {} is unnecessary " + "since empty pane and FIRE_IF_NON_EMPTY for key:{}; window:{}; inputWatermark:{}; " + "outputWatermark:{}", gcHold, context.key(), context.window(), inputWM, outputWM);        return null;    }    if (!gcHold.isBefore(BoundedWindow.TIMESTAMP_MAX_VALUE)) {                                gcHold = BoundedWindow.TIMESTAMP_MAX_VALUE.minus(Duration.millis(1L));    }    checkState(!gcHold.isBefore(inputWM), "Garbage collection hold %s cannot be before input watermark %s", gcHold, inputWM);    checkState(!gcHold.isAfter(BoundedWindow.TIMESTAMP_MAX_VALUE), "Garbage collection hold %s is beyond end-of-time", gcHold);        context.state().access(EXTRA_HOLD_TAG).add(gcHold);    WindowTracing.trace("WatermarkHold.addGarbageCollectionHold: garbage collection hold at {} is on time for " + "key:{}; window:{}; inputWatermark:{}; outputWatermark:{}", gcHold, context.key(), context.window(), inputWM, outputWM);    return gcHold;}
public void beam_f3220_0(MergingStateAccessor<?, W> context)
{    Map<W, WatermarkHoldState> map = context.accessInEachMergingWindow(elementHoldTag);    WatermarkHoldState result = context.access(elementHoldTag);    if (map.isEmpty()) {                return;    }    if (map.size() == 1 && map.values().contains(result) && result.getTimestampCombiner().dependsOnlyOnEarliestTimestamp()) {                return;    }    if (result.getTimestampCombiner().dependsOnlyOnWindow()) {                return;    }        for (WatermarkHoldState source : map.values()) {        source.readLater();    }}
public void beam_f3221_0(ReduceFn<?, ?, ?, W>.OnMergeContext context)
{    WindowTracing.debug("WatermarkHold.onMerge: for key:{}; window:{}; inputWatermark:{}; " + "outputWatermark:{}", context.key(), context.window(), timerInternals.currentInputWatermarkTime(), timerInternals.currentOutputWatermarkTime());    Collection<WatermarkHoldState> sources = context.state().accessInEachMergingWindow(elementHoldTag).values();    WatermarkHoldState result = context.state().access(elementHoldTag);    if (sources.isEmpty()) {        } else if (sources.size() == 1 && sources.contains(result) && result.getTimestampCombiner().dependsOnlyOnEarliestTimestamp()) {        } else if (result.getTimestampCombiner().dependsOnlyOnWindow()) {                for (WatermarkHoldState source : sources) {            source.clear();        }                addElementHold(BoundedWindow.TIMESTAMP_MIN_VALUE, context);    } else {                for (WatermarkHoldState source : sources) {            source.readLater();        }                Instant mergedHold = null;        for (ReadableState<Instant> source : sources) {            Instant sourceOutputTime = source.read();            if (sourceOutputTime != null) {                if (mergedHold == null) {                    mergedHold = sourceOutputTime;                } else {                    mergedHold = result.getTimestampCombiner().merge(context.window(), mergedHold, sourceOutputTime);                }            }        }                for (WatermarkHoldState source : sources) {            source.clear();        }                if (mergedHold != null) {            result.add(mergedHold);        }    }                            StateMerging.clear(context.state(), EXTRA_HOLD_TAG);    addGarbageCollectionHold(context, false);}
public void beam_f3229_0()
{    MultimapView<String, String> view = InMemoryMultimapSideInputView.fromIterable(StringUtf8Coder.of(), ImmutableList.of(KV.of("A", "a1"), KV.of("A", "a2"), KV.of("B", "b1")));    assertEquals(view.get("A"), ImmutableList.of("a1", "a2"));    assertEquals(view.get("B"), ImmutableList.of("b1"));    assertEquals(view.get("C"), ImmutableList.of());}
protected StateInternals beam_f3230_0()
{    return new InMemoryStateInternals<>("dummyKey");}
public void beam_f3231_0()
{    assertSameInstance(STRING_VALUE_ADDR);    assertSameInstance(SUM_INTEGER_ADDR);    assertSameInstance(STRING_BAG_ADDR);    assertSameInstance(STRING_SET_ADDR);    assertSameInstance(STRING_MAP_ADDR);    assertSameInstance(WATERMARK_EARLIEST_ADDR);}
public void beam_f3239_0() throws Exception
{    InMemoryTimerInternals underTest = new InMemoryTimerInternals();    TimerData eventTime = TimerData.of(NS1, new Instant(19), TimeDomain.EVENT_TIME);    TimerData processingTime = TimerData.of(NS1, new Instant(19), TimeDomain.PROCESSING_TIME);    underTest.setTimer(eventTime);    underTest.setTimer(eventTime);    underTest.setTimer(processingTime);    underTest.setTimer(processingTime);    underTest.advanceProcessingTime(new Instant(20));    underTest.advanceInputWatermark(new Instant(20));    assertThat(underTest.removeNextProcessingTimer(), equalTo(processingTime));    assertThat(underTest.removeNextProcessingTimer(), nullValue());    assertThat(underTest.removeNextEventTimer(), equalTo(eventTime));    assertThat(underTest.removeNextEventTimer(), nullValue());}
public void beam_f3240_0() throws Exception
{    CoderProperties.coderSerializable(KeyedWorkItemCoder.of(StringUtf8Coder.of(), VarIntCoder.of(), GlobalWindow.Coder.INSTANCE));}
public void beam_f3241_0() throws Exception
{    Iterable<TimerData> timers = ImmutableList.of(TimerData.of(StateNamespaces.global(), new Instant(500L), TimeDomain.EVENT_TIME));    Iterable<WindowedValue<Integer>> elements = ImmutableList.of(WindowedValue.valueInGlobalWindow(1), WindowedValue.valueInGlobalWindow(4), WindowedValue.valueInGlobalWindow(8));    KeyedWorkItemCoder<String, Integer> coder = KeyedWorkItemCoder.of(StringUtf8Coder.of(), VarIntCoder.of(), GlobalWindow.Coder.INSTANCE);    CoderProperties.coderDecodeEncodeEqual(coder, KeyedWorkItems.workItem("foo", timers, elements));    CoderProperties.coderDecodeEncodeEqual(coder, KeyedWorkItems.elementsWorkItem("foo", elements));    CoderProperties.coderDecodeEncodeEqual(coder, KeyedWorkItems.timersWorkItem("foo", timers));}
public void beam_f3249_0()
{    windowFn = Sessions.withGapDuration(Duration.millis(10));    state = InMemoryStateInternals.forKey("dummyKey");    set = new MergingActiveWindowSet<>(windowFn, state);    @SuppressWarnings("unchecked")    ActiveWindowSet.MergeCallback<IntervalWindow> callback = mock(ActiveWindowSet.MergeCallback.class);    this.callback = callback;}
public void beam_f3250_0()
{    set = null;    state = null;    windowFn = null;}
private void beam_f3251_0(long... instants)
{    for (final long instant : instants) {        System.out.println("ADD " + instant);        Sessions.AssignContext context = windowFn.new AssignContext() {            @Override            public Object element() {                return (Object) instant;            }            @Override            public Instant timestamp() {                return new Instant(instant);            }            @Override            public BoundedWindow window() {                return GlobalWindow.INSTANCE;            }        };        for (IntervalWindow window : windowFn.assignWindows(context)) {            set.ensureWindowExists(window);        }    }}
public void beam_f3259_0() throws Exception
{                                add(1, 2, 15);    assertEquals(ImmutableSet.of(window(1, 10), window(2, 10), window(15, 10)), set.getActiveAndNewWindows());    Map<IntervalWindow, IntervalWindow> map = merge(ImmutableList.of(window(1, 10), window(2, 10)), window(1, 11));    activate(map, 1, 2, 15);    assertEquals(ImmutableSet.of(window(1, 11), window(15, 10)), set.getActiveAndNewWindows());    assertEquals(ImmutableSet.of(window(1, 11)), set.readStateAddresses(window(1, 11)));    assertEquals(ImmutableSet.of(window(15, 10)), set.readStateAddresses(window(15, 10)));    cleanup();                        add(3);    assertEquals(ImmutableSet.of(window(3, 10), window(1, 11), window(15, 10)), set.getActiveAndNewWindows());    map = merge(ImmutableList.of(window(1, 11), window(3, 10)), window(1, 12));    activate(map, 3);    assertEquals(ImmutableSet.of(window(1, 12), window(15, 10)), set.getActiveAndNewWindows());    assertEquals(ImmutableSet.of(window(1, 11)), set.readStateAddresses(window(1, 12)));    assertEquals(ImmutableSet.of(window(15, 10)), set.readStateAddresses(window(15, 10)));    cleanup();                    add(8);    assertEquals(ImmutableSet.of(window(8, 10), window(1, 12), window(15, 10)), set.getActiveAndNewWindows());    map = merge(ImmutableList.of(window(1, 12), window(8, 10), window(15, 10)), window(1, 24));    activate(map, 8);    assertEquals(ImmutableSet.of(window(1, 24)), set.getActiveAndNewWindows());    assertEquals(ImmutableSet.of(window(1, 11)), set.readStateAddresses(window(1, 24)));    cleanup();                    add(9);    assertEquals(ImmutableSet.of(window(9, 10), window(1, 24)), set.getActiveAndNewWindows());    map = merge(ImmutableList.of(window(1, 24), window(9, 10)), window(1, 24));    activate(map, 9);    assertEquals(ImmutableSet.of(window(1, 24)), set.getActiveAndNewWindows());    assertEquals(ImmutableSet.of(window(1, 11)), set.readStateAddresses(window(1, 24)));    cleanup();                    add(1);    assertEquals(ImmutableSet.of(window(1, 10), window(1, 24)), set.getActiveAndNewWindows());    map = merge(ImmutableList.of(window(1, 10), window(1, 24)), window(1, 24));    activate(map, 1);    assertEquals(ImmutableSet.of(window(1, 24)), set.getActiveAndNewWindows());    assertEquals(ImmutableSet.of(window(1, 11)), set.readStateAddresses(window(1, 24)));    cleanup();        set.remove(window(1, 24));    cleanup();    assertTrue(set.getActiveAndNewWindows().isEmpty());}
public void beam_f3260_0()
{            set.addActiveForTesting(window(1, 12), ImmutableList.of(window(1, 10), window(2, 10), window(3, 10)));        assertTrue(set.isActive(window(1, 12)));    assertEquals(ImmutableSet.of(window(1, 10), window(2, 10), window(3, 10)), set.readStateAddresses(window(1, 12)));    assertEquals(window(1, 10), set.mergedWriteStateAddress(ImmutableList.of(window(1, 10), window(2, 10), window(3, 10)), window(1, 12)));    set.merged(window(1, 12));    cleanup();        assertEquals(ImmutableSet.of(window(1, 10)), set.readStateAddresses(window(1, 12)));}
public void beam_f3261_0()
{    cell.inc(5);    cell.inc(7);    assertThat(cell.getCumulative(), equalTo(12L));    assertThat("getCumulative is idempotent", cell.getCumulative(), equalTo(12L));    assertThat(cell.getDirty().beforeCommit(), equalTo(true));    cell.getDirty().afterCommit();    assertThat(cell.getDirty().beforeCommit(), equalTo(false));    assertThat(cell.getCumulative(), equalTo(12L));    cell.inc(30);    assertThat(cell.getCumulative(), equalTo(42L));    assertThat(cell.getDirty().beforeCommit(), equalTo(true));    cell.getDirty().afterCommit();    assertThat(cell.getDirty().beforeCommit(), equalTo(false));}
public void beam_f3269_0()
{    DistributionCell distributionCell = new DistributionCell(MetricName.named("namespace", "name"));    DistributionCell equal = new DistributionCell(MetricName.named("namespace", "name"));    Assert.assertEquals(distributionCell, equal);    Assert.assertEquals(distributionCell.hashCode(), equal.hashCode());}
public void beam_f3270_0()
{    DistributionCell distributionCell = new DistributionCell(MetricName.named("namespace", "name"));    Assert.assertNotEquals(distributionCell, new Object());    DistributionCell differentDirty = new DistributionCell(MetricName.named("namespace", "name"));    differentDirty.getDirty().beforeCommit();    Assert.assertNotEquals(distributionCell, differentDirty);    Assert.assertNotEquals(distributionCell.hashCode(), differentDirty.hashCode());    DistributionCell differentValue = new DistributionCell(MetricName.named("namespace", "name"));    differentValue.update(DistributionData.create(1, 1, 1, 1));    Assert.assertNotEquals(distributionCell, differentValue);    Assert.assertNotEquals(distributionCell.hashCode(), differentValue.hashCode());    DistributionCell differentName = new DistributionCell(MetricName.named("DIFFERENT", "DIFFERENT"));    Assert.assertNotEquals(distributionCell, differentName);    Assert.assertNotEquals(distributionCell.hashCode(), differentName.hashCode());}
public void beam_f3271_0()
{    clock = mock(MillisProvider.class);    sampler = ExecutionStateSampler.newForTest(clock);}
public void beam_f3279_0()
{    GaugeCell gaugeCell = new GaugeCell(MetricName.named("namespace", "name"));    GaugeCell equal = new GaugeCell(MetricName.named("namespace", "name"));    Assert.assertEquals(gaugeCell, equal);    Assert.assertEquals(gaugeCell.hashCode(), equal.hashCode());}
public void beam_f3280_0()
{    GaugeCell gaugeCell = new GaugeCell(MetricName.named("namespace", "name"));    Assert.assertNotEquals(gaugeCell, new Object());    GaugeCell differentDirty = new GaugeCell(MetricName.named("namespace", "name"));    differentDirty.getDirty().beforeCommit();    Assert.assertNotEquals(gaugeCell, differentDirty);    Assert.assertNotEquals(gaugeCell.hashCode(), differentDirty.hashCode());    GaugeCell differentGaugeValue = new GaugeCell(MetricName.named("namespace", "name"));    differentGaugeValue.update(GaugeData.create(1));    Assert.assertNotEquals(gaugeCell, differentGaugeValue);    Assert.assertNotEquals(gaugeCell.hashCode(), differentGaugeValue.hashCode());    GaugeCell differentName = new GaugeCell(MetricName.named("DIFFERENT", "DIFFERENT"));    Assert.assertNotEquals(gaugeCell, differentName);    Assert.assertNotEquals(gaugeCell.hashCode(), differentName.hashCode());}
public void beam_f3281_0()
{    MetricsEnvironment.setCurrentContainer(null);    assertNull(MetricsEnvironment.getCurrentContainer());    HashMap<String, String> labels = new HashMap<String, String>();    String urn = MonitoringInfoConstants.Urns.ELEMENT_COUNT;    MonitoringInfoMetricName name = MonitoringInfoMetricName.named(urn, labels);    Counter counter = LabeledMetrics.counter(name);    counter.inc();    counter.inc(5L);    counter.dec();    counter.dec(5L);}
public void beam_f3289_0()
{    MetricsContainerImpl testObject = new MetricsContainerImpl("step1");    HashMap<String, String> labels = new HashMap<String, String>();    labels.put(MonitoringInfoConstants.Labels.PCOLLECTION, "pcollection");    MetricName name = MonitoringInfoMetricName.named(MonitoringInfoConstants.Urns.ELEMENT_COUNT, labels);    CounterCell c1 = testObject.getCounter(name);    c1.inc(2L);    SimpleMonitoringInfoBuilder builder1 = new SimpleMonitoringInfoBuilder();    builder1.setUrn(MonitoringInfoConstants.Urns.ELEMENT_COUNT);    builder1.setLabel(MonitoringInfoConstants.Labels.PCOLLECTION, "pcollection");    builder1.setInt64Value(2);    ArrayList<MonitoringInfo> actualMonitoringInfos = new ArrayList<MonitoringInfo>();    for (MonitoringInfo mi : testObject.getMonitoringInfos()) {        actualMonitoringInfos.add(SimpleMonitoringInfoBuilder.copyAndClearTimestamp(mi));    }    assertThat(actualMonitoringInfos, containsInAnyOrder(builder1.build()));}
public void beam_f3290_0()
{    MetricsContainerImpl metricsContainerImpl = new MetricsContainerImpl("stepName");    MetricsContainerImpl equal = new MetricsContainerImpl("stepName");    Assert.assertEquals(metricsContainerImpl, equal);    Assert.assertEquals(metricsContainerImpl.hashCode(), equal.hashCode());}
public void beam_f3291_0()
{    MetricsContainerImpl metricsContainerImpl = new MetricsContainerImpl("stepName");    Assert.assertNotEquals(metricsContainerImpl, new Object());    MetricsContainerImpl differentStepName = new MetricsContainerImpl("DIFFERENT");    Assert.assertNotEquals(metricsContainerImpl, differentStepName);    Assert.assertNotEquals(metricsContainerImpl.hashCode(), differentStepName.hashCode());    MetricsContainerImpl differentCounters = new MetricsContainerImpl("stepName");    differentCounters.getCounter(MetricName.named("namespace", "name"));    Assert.assertNotEquals(metricsContainerImpl, differentCounters);    Assert.assertNotEquals(metricsContainerImpl.hashCode(), differentCounters.hashCode());    MetricsContainerImpl differentDistributions = new MetricsContainerImpl("stepName");    differentDistributions.getDistribution(MetricName.named("namespace", "name"));    Assert.assertNotEquals(metricsContainerImpl, differentDistributions);    Assert.assertNotEquals(metricsContainerImpl.hashCode(), differentDistributions.hashCode());    MetricsContainerImpl differentGauges = new MetricsContainerImpl("stepName");    differentGauges.getGauge(MetricName.named("namespace", "name"));    Assert.assertNotEquals(metricsContainerImpl, differentGauges);    Assert.assertNotEquals(metricsContainerImpl.hashCode(), differentGauges.hashCode());}
public void beam_f3299_0()
{    MetricsContainerStepMap metricsContainerStepMap = new MetricsContainerStepMap();    MetricsContainerStepMap equal = new MetricsContainerStepMap();    Assert.assertEquals(metricsContainerStepMap, equal);    Assert.assertEquals(metricsContainerStepMap.hashCode(), equal.hashCode());}
public void beam_f3300_0()
{    MetricsContainerStepMap metricsContainerStepMap = new MetricsContainerStepMap();    Assert.assertNotEquals(metricsContainerStepMap, new Object());    MetricsContainerStepMap differentMetricsContainers = new MetricsContainerStepMap();    differentMetricsContainers.getContainer("stepName");    Assert.assertNotEquals(metricsContainerStepMap, differentMetricsContainers);    Assert.assertNotEquals(metricsContainerStepMap.hashCode(), differentMetricsContainers.hashCode());    MetricsContainerStepMap differentUnboundedContainer = new MetricsContainerStepMap();    differentUnboundedContainer.getContainer(null).getCounter(MetricName.named("namespace", "name"));    Assert.assertNotEquals(metricsContainerStepMap, differentUnboundedContainer);    Assert.assertNotEquals(metricsContainerStepMap.hashCode(), differentUnboundedContainer.hashCode());}
private void beam_f3301_0(Iterable<T> iterable, int size)
{    assertThat(iterable, IsIterableWithSize.iterableWithSize(size));}
public void beam_f3309_0()
{    MetricsMap<String, AtomicLong> metricsMap = new MetricsMap<>(unusedKey -> new AtomicLong());    MetricsMap<String, AtomicLong> equal = new MetricsMap<>(unusedKey -> new AtomicLong());    Assert.assertEquals(metricsMap, equal);    Assert.assertEquals(metricsMap.hashCode(), equal.hashCode());}
public void beam_f3310_0()
{    MetricsMap<String, AtomicLong> metricsMap = new MetricsMap<>(unusedKey -> new AtomicLong());    Assert.assertNotEquals(metricsMap, new Object());    MetricsMap<String, AtomicLong> differentMetrics = new MetricsMap<>(unusedKey -> new AtomicLong());    differentMetrics.get("key");    Assert.assertNotEquals(metricsMap, differentMetrics);    Assert.assertNotEquals(metricsMap.hashCode(), differentMetrics.hashCode());}
private static Matcher<Map.Entry<String, AtomicLong>> beam_f3311_0(final String key, final AtomicLong value)
{    return new TypeSafeMatcher<Entry<String, AtomicLong>>() {        @Override        public void describeTo(Description description) {            description.appendText("Map.Entry{key=").appendValue(key).appendText(", value=").appendValue(value).appendText("}");        }        @Override        protected boolean matchesSafely(Entry<String, AtomicLong> item) {            return Objects.equals(key, item.getKey()) && Objects.equals(value, item.getValue());        }    };}
public static Matcher<MetricUpdate<T>> beam_f3319_0(final String name, final T update)
{    return new TypeSafeMatcher<MetricUpdate<T>>() {        @Override        protected boolean matchesSafely(MetricUpdate<T> item) {            return Objects.equals(name, item.getKey().metricName().getName()) && Objects.equals(update, item.getUpdate());        }        @Override        public void describeTo(Description description) {            description.appendText("MetricUpdate{name=").appendValue(name).appendText(", update=").appendValue(update).appendText("}");        }    };}
protected boolean beam_f3320_0(MetricUpdate<T> item)
{    return Objects.equals(name, item.getKey().metricName().getName()) && Objects.equals(update, item.getUpdate());}
public void beam_f3321_0(Description description)
{    description.appendText("MetricUpdate{name=").appendValue(name).appendText(", update=").appendValue(update).appendText("}");}
protected boolean beam_f3329_0(MonitoringInfo item)
{    if (item.getMetric().getCounterData().getInt64Value() < value) {        return false;    }    return true;}
public void beam_f3330_0(Description description)
{    description.appendText("value=").appendValue(value);}
public void beam_f3331_0()
{    HashMap<String, String> labels = new HashMap<String, String>();    String urn = MonitoringInfoConstants.Urns.ELEMENT_COUNT;    MonitoringInfoMetricName name = MonitoringInfoMetricName.named(urn, labels);    assertEquals(labels, name.getLabels());    assertEquals(urn, name.getUrn());        assertEquals(name, name);        urn = MonitoringInfoConstants.Urns.ELEMENT_COUNT;    labels = new HashMap<String, String>();    MonitoringInfoMetricName name2 = MonitoringInfoMetricName.named(urn, labels);    assertEquals(name, name2);    assertEquals(name.hashCode(), name2.hashCode());}
public static MonitoringInfoMetricName beam_f3339_0()
{    HashMap labels = new HashMap<String, String>();    labels.put(MonitoringInfoConstants.Labels.PCOLLECTION, "testPCollection");    MonitoringInfoMetricName name = MonitoringInfoMetricName.named(MonitoringInfoConstants.Urns.ELEMENT_COUNT, labels);    return name;}
public static MonitoringInfo beam_f3340_0(long value)
{    SimpleMonitoringInfoBuilder builder = new SimpleMonitoringInfoBuilder();    builder.setUrn(MonitoringInfoConstants.Urns.ELEMENT_COUNT);    builder.setLabel(MonitoringInfoConstants.Labels.PCOLLECTION, "testPCollection");    builder.setInt64Value(value);    return builder.build();}
public void beam_f3341_0()
{    String stateName = "myState";    HashMap<String, String> labelsMetadata = new HashMap<String, String>();    labelsMetadata.put("k1", "v1");    labelsMetadata.put("k2", "v2");    SimpleExecutionState testObject = new SimpleExecutionState(stateName, null, labelsMetadata);    assertEquals(testObject.getStateName(), stateName);    assertEquals(2, testObject.getLabels().size());    assertThat(testObject.getLabels(), hasEntry("k1", "v1"));    assertThat(testObject.getLabels(), hasEntry("k2", "v2"));}
public void beam_f3349_0() throws Exception
{    String testPTransformId = "pTransformId";    HashMap<String, String> labelsMetadata = new HashMap<String, String>();    labelsMetadata.put(MonitoringInfoConstants.Labels.PTRANSFORM, testPTransformId);    SimpleExecutionState startState = new SimpleExecutionState(ExecutionStateTracker.START_STATE_NAME, MonitoringInfoConstants.Urns.START_BUNDLE_MSECS, labelsMetadata);    SimpleExecutionState processState = new SimpleExecutionState(ExecutionStateTracker.PROCESS_STATE_NAME, MonitoringInfoConstants.Urns.PROCESS_BUNDLE_MSECS, labelsMetadata);    SimpleExecutionState finishState = new SimpleExecutionState(ExecutionStateTracker.FINISH_STATE_NAME, MonitoringInfoConstants.Urns.FINISH_BUNDLE_MSECS, labelsMetadata);    SimpleStateRegistry testObject = new SimpleStateRegistry();    testObject.register(startState);    testObject.register(processState);    testObject.register(finishState);    List<MonitoringInfo> testOutput = testObject.getExecutionTimeMonitoringInfos();    List<Matcher<MonitoringInfo>> matchers = new ArrayList<Matcher<MonitoringInfo>>();    SimpleMonitoringInfoBuilder builder = new SimpleMonitoringInfoBuilder();    builder.setUrn(MonitoringInfoConstants.Urns.START_BUNDLE_MSECS);    builder.setInt64Value(0);    builder.setLabel(MonitoringInfoConstants.Labels.PTRANSFORM, testPTransformId);    matchers.add(MonitoringInfoMatchers.matchSetFields(builder.build()));        builder = new SimpleMonitoringInfoBuilder();    builder.setUrn(MonitoringInfoConstants.Urns.PROCESS_BUNDLE_MSECS);    builder.setInt64Value(0);    builder.setLabel(MonitoringInfoConstants.Labels.PTRANSFORM, testPTransformId);    matchers.add(MonitoringInfoMatchers.matchSetFields(builder.build()));    builder = new SimpleMonitoringInfoBuilder();    builder.setUrn(MonitoringInfoConstants.Urns.FINISH_BUNDLE_MSECS);    builder.setInt64Value(0);    builder.setLabel(MonitoringInfoConstants.Labels.PTRANSFORM, testPTransformId);    matchers.add(MonitoringInfoMatchers.matchSetFields(builder.build()));    for (Matcher<MonitoringInfo> matcher : matchers) {        assertThat(testOutput, Matchers.hasItem(matcher));    }}
public void beam_f3350_0() throws Exception
{    testObject = new SpecMonitoringInfoValidator();}
public void beam_f3351_0()
{    MonitoringInfo testInput = MonitoringInfo.newBuilder().setUrn("beam:metric:user").putLabels(MonitoringInfoConstants.Labels.NAME, "anyCounter").putLabels(MonitoringInfoConstants.Labels.NAMESPACE, "namespace").setType("beam:metrics:bad_value").build();    assertTrue(testObject.validate(testInput).isPresent());}
private SplittableProcessElementInvoker<Void, String, OffsetRange, Long>.Result beam_f3359_0(int totalNumOutputs, Duration sleepBeforeFirstClaim, int numOutputsPerProcessCall, Duration sleepBeforeEachOutput)
{    SomeFn fn = new SomeFn(sleepBeforeFirstClaim, numOutputsPerProcessCall, sleepBeforeEachOutput);    OffsetRange initialRestriction = new OffsetRange(0, totalNumOutputs);    return runTest(fn, initialRestriction);}
private SplittableProcessElementInvoker<Void, String, OffsetRange, Long>.Result beam_f3360_0(DoFn<Void, String> fn, OffsetRange initialRestriction)
{    SplittableProcessElementInvoker<Void, String, OffsetRange, Long> invoker = new OutputAndTimeBoundedSplittableProcessElementInvoker<>(fn, PipelineOptionsFactory.create(), new OutputWindowedValue<String>() {        @Override        public void outputWindowedValue(String output, Instant timestamp, Collection<? extends BoundedWindow> windows, PaneInfo pane) {        }        @Override        public <AdditionalOutputT> void outputWindowedValue(TupleTag<AdditionalOutputT> tag, AdditionalOutputT output, Instant timestamp, Collection<? extends BoundedWindow> windows, PaneInfo pane) {        }    }, NullSideInputReader.empty(), Executors.newSingleThreadScheduledExecutor(), 1000, Duration.standardSeconds(3));    return invoker.invokeProcessElement(DoFnInvokers.invokerFor(fn), WindowedValue.of(null, Instant.now(), GlobalWindow.INSTANCE, PaneInfo.NO_FIRING), new OffsetRangeTracker(initialRestriction));}
public void beam_f3363_0() throws Exception
{    SplittableProcessElementInvoker<Void, String, OffsetRange, Long>.Result res = runTest(10000, Duration.ZERO, Integer.MAX_VALUE, Duration.ZERO);    assertFalse(res.getContinuation().shouldResume());    OffsetRange residualRange = res.getResidualRestriction();        assertEquals(1000, residualRange.getFrom());    assertEquals(10000, residualRange.getTo());}
public void beam_f3371_0() throws Exception
{    DoFn<Void, String> brokenFn = new DoFn<Void, String>() {        @ProcessElement        public void process(ProcessContext c, RestrictionTracker<OffsetRange, Long> tracker) {            assertFalse(tracker.tryClaim(6L));            c.output("foo");        }        @GetInitialRestriction        public OffsetRange getInitialRestriction(Void element) {            throw new UnsupportedOperationException("Should not be called in this test");        }    };    e.expectMessage("Output is not allowed after a failed tryClaim()");    runTest(brokenFn, new OffsetRange(0, 5));}
public void beam_f3372_0(ProcessContext c, RestrictionTracker<OffsetRange, Long> tracker)
{    assertFalse(tracker.tryClaim(6L));    c.output("foo");}
public OffsetRange beam_f3373_0(Void element)
{    throw new UnsupportedOperationException("Should not be called in this test");}
public void beam_f3381_0() throws Exception
{    WindowingStrategy<?, IntervalWindow> strategy = WindowingStrategy.of((WindowFn<?, IntervalWindow>) FixedWindows.of(Duration.millis(100))).withTimestampCombiner(TimestampCombiner.EARLIEST).withMode(AccumulationMode.ACCUMULATING_FIRED_PANES).withAllowedLateness(Duration.ZERO).withTrigger(Repeatedly.forever(AfterProcessingTime.pastFirstElementInPane().plusDelayOf(Duration.millis(10))));    ReduceFnTester<Integer, Integer, IntervalWindow> tester = ReduceFnTester.combining(strategy, Sum.ofIntegers(), VarIntCoder.of());    tester.advanceProcessingTime(new Instant(5000));        injectElement(tester, 2);    injectElement(tester, 5);    tester.advanceProcessingTime(new Instant(10000));    tester.assertHasOnlyGlobalAndStateFor(new IntervalWindow(new Instant(0), new Instant(100)));    assertThat(tester.extractOutput(), contains(isSingleWindowedValue(equalTo(7), 2, 0, 100, PaneInfo.createPane(true, false, Timing.EARLY, 0, 0))));}
public void beam_f3382_0() throws Exception
{        MetricsContainerImpl container = new MetricsContainerImpl("any");    MetricsEnvironment.setCurrentContainer(container);    ReduceFnTester<Integer, Iterable<Integer>, IntervalWindow> tester = ReduceFnTester.nonCombining(FixedWindows.of(Duration.millis(10)), mockTriggerStateMachine, AccumulationMode.DISCARDING_FIRED_PANES, Duration.millis(100), ClosingBehavior.FIRE_IF_NON_EMPTY);        injectElement(tester, 1);    when(mockTriggerStateMachine.shouldFire(anyTriggerContext())).thenReturn(true);    injectElement(tester, 2);    assertThat(tester.extractOutput(), contains(isSingleWindowedValue(containsInAnyOrder(1, 2), 1, 0, 10)));        when(mockTriggerStateMachine.shouldFire(anyTriggerContext())).thenReturn(true);    triggerShouldFinish(mockTriggerStateMachine);    injectElement(tester, 3);    assertThat(tester.extractOutput(), contains(isSingleWindowedValue(containsInAnyOrder(3), 3, 0, 10)));    assertTrue(tester.isMarkedFinished(firstWindow));    tester.assertHasOnlyGlobalAndFinishedSetsFor(firstWindow);        injectElement(tester, 4);    long droppedElements = container.getCounter(MetricName.named(ReduceFnRunner.class, ReduceFnRunner.DROPPED_DUE_TO_CLOSED_WINDOW)).getCumulative();    assertEquals(1, droppedElements);}
public void beam_f3383_0() throws Exception
{        ReduceFnTester<Integer, Iterable<Integer>, IntervalWindow> tester = ReduceFnTester.nonCombining(FixedWindows.of(Duration.millis(10)), mockTriggerStateMachine, AccumulationMode.ACCUMULATING_FIRED_PANES, Duration.millis(100), ClosingBehavior.FIRE_IF_NON_EMPTY);    injectElement(tester, 1);        when(mockTriggerStateMachine.shouldFire(anyTriggerContext())).thenReturn(true);    injectElement(tester, 2);        when(mockTriggerStateMachine.shouldFire(anyTriggerContext())).thenReturn(true);    triggerShouldFinish(mockTriggerStateMachine);    injectElement(tester, 3);        injectElement(tester, 4);    assertThat(tester.extractOutput(), contains(isSingleWindowedValue(containsInAnyOrder(1, 2), 1, 0, 10), isSingleWindowedValue(containsInAnyOrder(1, 2, 3), 3, 0, 10)));    assertTrue(tester.isMarkedFinished(firstWindow));    tester.assertHasOnlyGlobalAndFinishedSetsFor(firstWindow);}
public void beam_f3391_0() throws Exception
{    Duration allowedLateness = Duration.standardDays(365);    Duration windowSize = Duration.millis(10);    WindowFn<Object, IntervalWindow> windowFn = FixedWindows.of(windowSize);            final Instant elementTimestamp = GlobalWindow.INSTANCE.maxTimestamp().minus(allowedLateness).plus(1);    IntervalWindow window = Iterables.getOnlyElement(windowFn.assignWindows(windowFn.new AssignContext() {        @Override        public Object element() {            throw new UnsupportedOperationException();        }        @Override        public Instant timestamp() {            return elementTimestamp;        }        @Override        public BoundedWindow window() {            throw new UnsupportedOperationException();        }    }));    assertTrue(window.maxTimestamp().isBefore(GlobalWindow.INSTANCE.maxTimestamp()));    assertTrue(window.maxTimestamp().plus(allowedLateness).isAfter(GlobalWindow.INSTANCE.maxTimestamp()));        WindowingStrategy<?, IntervalWindow> strategy = WindowingStrategy.of((WindowFn<?, IntervalWindow>) windowFn).withTimestampCombiner(TimestampCombiner.EARLIEST).withTrigger(AfterWatermark.pastEndOfWindow().withLateFirings(Never.ever())).withMode(AccumulationMode.DISCARDING_FIRED_PANES).withAllowedLateness(allowedLateness);    ReduceFnTester<Integer, Integer, IntervalWindow> tester = ReduceFnTester.combining(strategy, Sum.ofIntegers(), VarIntCoder.of());    tester.injectElements(TimestampedValue.of(13, elementTimestamp));            tester.advanceInputWatermark(window.maxTimestamp());        assertThat(tester.extractOutput(), emptyIterable());    tester.injectElements(TimestampedValue.of(42, elementTimestamp));        tester.advanceInputWatermark(GlobalWindow.INSTANCE.maxTimestamp());    assertThat(tester.extractOutput(), contains(isWindowedValue(equalTo(55))));}
public Object beam_f3392_0()
{    throw new UnsupportedOperationException();}
public Instant beam_f3393_0()
{    return elementTimestamp;}
public void beam_f3401_0() throws Exception
{    Duration allowedLateness = Duration.standardMinutes(1);    Duration gapDuration = Duration.millis(10);    ReduceFnTester<Integer, Iterable<Integer>, IntervalWindow> tester = ReduceFnTester.nonCombining(WindowingStrategy.of(Sessions.withGapDuration(gapDuration)).withMode(AccumulationMode.DISCARDING_FIRED_PANES).withTrigger(Repeatedly.forever(AfterWatermark.pastEndOfWindow().withLateFirings(AfterPane.elementCountAtLeast(1)))).withAllowedLateness(allowedLateness));    tester.setAutoAdvanceOutputWatermark(false);    assertEquals(null, tester.getWatermarkHold());    assertEquals(null, tester.getOutputWatermark());    tester.advanceInputWatermark(new Instant(24));    injectElements(tester, 1);    assertThat(tester.getWatermarkHold(), nullValue());    injectElements(tester, 14);    assertThat(tester.getWatermarkHold(), nullValue());    injectElements(tester, 6, 16);            assertEquals(tester.getWatermarkHold(), new Instant(25));    injectElements(tester, 6, 21);        assertEquals(tester.getWatermarkHold(), new Instant(30));        tester.advanceInputWatermark(new Instant(31));    assertThat(tester.getWatermarkHold(), nullValue());        injectElements(tester, 0);    assertThat(tester.getWatermarkHold(), nullValue());        injectElements(tester, 32, 40);    assertEquals(tester.getWatermarkHold(), new Instant(49));        injectElements(tester, 24);    assertEquals(tester.getWatermarkHold(), new Instant(49));    tester.advanceInputWatermark(new Instant(50));    assertThat(tester.getWatermarkHold(), nullValue());}
public void beam_f3402_0() throws Exception
{    MetricsContainerImpl container = new MetricsContainerImpl("any");    MetricsEnvironment.setCurrentContainer(container);    Duration gapDuration = Duration.millis(10);    Duration allowedLateness = Duration.standardMinutes(100);    ReduceFnTester<Integer, Iterable<Integer>, IntervalWindow> tester = ReduceFnTester.nonCombining(WindowingStrategy.of(Sessions.withGapDuration(gapDuration)).withMode(AccumulationMode.DISCARDING_FIRED_PANES).withTrigger(Repeatedly.forever(AfterWatermark.pastEndOfWindow().withLateFirings(AfterPane.elementCountAtLeast(10)))).withAllowedLateness(allowedLateness));    tester.setAutoAdvanceOutputWatermark(false);        assertEquals(null, tester.getWatermarkHold());    assertEquals(null, tester.getOutputWatermark());    tester.advanceInputWatermark(new Instant(20));        injectElements(tester, Arrays.asList(3));    assertThat(tester.getWatermarkHold(), nullValue());    injectElements(tester, Arrays.asList(4));    Instant endOfWindow = new Instant(4).plus(gapDuration);        Instant expectedGcHold = endOfWindow.plus(allowedLateness).minus(1);    assertEquals(expectedGcHold, tester.getWatermarkHold());    tester.advanceInputWatermark(new Instant(1000));    assertEquals(expectedGcHold, tester.getWatermarkHold());}
public void beam_f3403_1() throws Exception
{    MetricsContainerImpl container = new MetricsContainerImpl("any");    MetricsEnvironment.setCurrentContainer(container);        Duration allowedLateness = Duration.standardMinutes(100);    long seed = ThreadLocalRandom.current().nextLong();        Random r = new Random(seed);    Duration gapDuration = Duration.millis(10 + r.nextInt(40));        ReduceFnTester<Integer, Iterable<Integer>, IntervalWindow> tester = ReduceFnTester.nonCombining(WindowingStrategy.of(Sessions.withGapDuration(gapDuration)).withMode(AccumulationMode.DISCARDING_FIRED_PANES).withTrigger(Repeatedly.forever(AfterWatermark.pastEndOfWindow().withLateFirings(AfterPane.elementCountAtLeast(1)))).withAllowedLateness(allowedLateness));    tester.setAutoAdvanceOutputWatermark(true);        assertEquals(null, tester.getWatermarkHold());    assertEquals(null, tester.getOutputWatermark());        List<Integer> times = new ArrayList<>();    int numTs = 3 + r.nextInt(100);    int maxTs = 1 + r.nextInt(400);            for (int i = numTs; i >= 0; --i) {        times.add(r.nextInt(maxTs));    }        int split = 0;    long watermark = 0;    while (split < times.size()) {        int nextSplit = split + r.nextInt(times.size());        if (nextSplit > times.size()) {            nextSplit = times.size();        }                injectElements(tester, times.subList(split, nextSplit));        if (r.nextInt(3) == 0) {            int nextWatermark = r.nextInt((int) (maxTs + gapDuration.getMillis()));            if (nextWatermark > watermark) {                Boolean enabled = r.nextBoolean();                                watermark = nextWatermark;                tester.setAutoAdvanceOutputWatermark(enabled);                tester.advanceInputWatermark(new Instant(watermark));            }        }        split = nextSplit;        Instant hold = tester.getWatermarkHold();        if (hold != null) {            assertThat(hold, greaterThanOrEqualTo(new Instant(watermark)));            assertThat(watermark, lessThan(maxTs + gapDuration.getMillis()));        }    }    tester.setAutoAdvanceOutputWatermark(true);    watermark = gapDuration.getMillis() + maxTs;    tester.advanceInputWatermark(new Instant(watermark));        if (tester.getWatermarkHold() != null) {        assertThat(tester.getWatermarkHold(), equalTo(new Instant(watermark).plus(allowedLateness)));    }        long droppedElements = container.getCounter(MetricName.named(ReduceFnRunner.class, ReduceFnRunner.DROPPED_DUE_TO_CLOSED_WINDOW)).getCumulative().longValue();    assertEquals(0, droppedElements);}
public void beam_f3411_0() throws Exception
{    ReduceFnTester<Integer, Iterable<Integer>, IntervalWindow> tester = ReduceFnTester.nonCombining(WindowingStrategy.of(FixedWindows.of(Duration.millis(10))).withTrigger(Repeatedly.forever(AfterPane.elementCountAtLeast(2)).orFinally(AfterWatermark.pastEndOfWindow())).withMode(AccumulationMode.DISCARDING_FIRED_PANES).withAllowedLateness(Duration.millis(100)).withClosingBehavior(ClosingBehavior.FIRE_ALWAYS));    tester.advanceInputWatermark(new Instant(0));        tester.injectElements(TimestampedValue.of(1, new Instant(1)), TimestampedValue.of(2, new Instant(2)));    assertThat(tester.extractOutput(), contains(WindowMatchers.valueWithPaneInfo(PaneInfo.createPane(true, false, Timing.EARLY, 0, -1))));    tester.advanceInputWatermark(new Instant(150));    assertThat(tester.extractOutput(), contains(WindowMatchers.valueWithPaneInfo(PaneInfo.createPane(false, true, Timing.ON_TIME, 1, 0))));}
public void beam_f3412_0() throws Exception
{    ReduceFnTester<Integer, Iterable<Integer>, IntervalWindow> tester = ReduceFnTester.nonCombining(FixedWindows.of(Duration.millis(10)), mockTriggerStateMachine, AccumulationMode.DISCARDING_FIRED_PANES, Duration.millis(100), ClosingBehavior.FIRE_IF_NON_EMPTY);    tester.advanceInputWatermark(new Instant(0));    when(mockTriggerStateMachine.shouldFire(anyTriggerContext())).thenReturn(true);    triggerShouldFinish(mockTriggerStateMachine);    injectElement(tester, 1);    assertThat(tester.extractOutput(), contains(WindowMatchers.valueWithPaneInfo(PaneInfo.createPane(true, true, Timing.EARLY))));}
public void beam_f3413_0() throws Exception
{    ReduceFnTester<Integer, Iterable<Integer>, IntervalWindow> tester = ReduceFnTester.nonCombining(FixedWindows.of(Duration.millis(10)), mockTriggerStateMachine, AccumulationMode.DISCARDING_FIRED_PANES, Duration.millis(100), ClosingBehavior.FIRE_IF_NON_EMPTY);    tester.advanceInputWatermark(new Instant(15));    when(mockTriggerStateMachine.shouldFire(anyTriggerContext())).thenReturn(true);    triggerShouldFinish(mockTriggerStateMachine);    injectElement(tester, 1);    assertThat(tester.extractOutput(), contains(WindowMatchers.valueWithPaneInfo(PaneInfo.createPane(true, true, Timing.LATE))));}
public void beam_f3421_0() throws Exception
{    MetricsContainerImpl container = new MetricsContainerImpl("any");    MetricsEnvironment.setCurrentContainer(container);            ReduceFnTester<Integer, Iterable<Integer>, IntervalWindow> tester = ReduceFnTester.nonCombining(FixedWindows.of(Duration.millis(10)), mockTriggerStateMachine, AccumulationMode.DISCARDING_FIRED_PANES, Duration.millis(100), ClosingBehavior.FIRE_IF_NON_EMPTY);        injectElement(tester, 1);    injectElement(tester, 2);    tester.advanceInputWatermark(new Instant(12));        when(mockTriggerStateMachine.shouldFire(anyTriggerContext())).thenReturn(true);    tester.fireTimer(firstWindow, new Instant(9), TimeDomain.EVENT_TIME);        when(mockTriggerStateMachine.shouldFire(anyTriggerContext())).thenReturn(true);    tester.fireTimer(firstWindow, new Instant(9), TimeDomain.EVENT_TIME);        when(mockTriggerStateMachine.shouldFire(anyTriggerContext())).thenReturn(true);    triggerShouldFinish(mockTriggerStateMachine);    injectElement(tester, 3);        List<WindowedValue<Iterable<Integer>>> output = tester.extractOutput();    assertThat(output.size(), equalTo(2));        assertThat(output.get(0), isSingleWindowedValue(containsInAnyOrder(1, 2), 1, 0, 10));        assertThat(output.get(1).getValue(), contains(3));    assertThat(output.get(1).getPane(), equalTo(PaneInfo.createPane(false, true, Timing.LATE, 1, 1)));    assertTrue(tester.isMarkedFinished(firstWindow));    tester.assertHasOnlyGlobalAndFinishedSetsFor(firstWindow);    long droppedElements = container.getCounter(MetricName.named(ReduceFnRunner.class, ReduceFnRunner.DROPPED_DUE_TO_CLOSED_WINDOW)).getCumulative();    assertEquals(0, droppedElements);}
public void beam_f3422_0() throws Exception
{    MetricsContainerImpl container = new MetricsContainerImpl("any");    MetricsEnvironment.setCurrentContainer(container);            ReduceFnTester<Integer, Iterable<Integer>, IntervalWindow> tester = ReduceFnTester.nonCombining(FixedWindows.of(Duration.millis(10)), mockTriggerStateMachine, AccumulationMode.ACCUMULATING_FIRED_PANES, Duration.millis(100), ClosingBehavior.FIRE_IF_NON_EMPTY);        injectElement(tester, 1);    injectElement(tester, 2);    tester.advanceInputWatermark(new Instant(12));        when(mockTriggerStateMachine.shouldFire(anyTriggerContext())).thenReturn(true);    tester.fireTimer(firstWindow, new Instant(9), TimeDomain.EVENT_TIME);    List<WindowedValue<Iterable<Integer>>> output = tester.extractOutput();    assertThat(output.size(), equalTo(1));    assertThat(output.get(0), isSingleWindowedValue(containsInAnyOrder(1, 2), 1, 0, 10));    assertThat(output.get(0).getPane(), equalTo(PaneInfo.createPane(true, false, Timing.ON_TIME, 0, 0)));            when(mockTriggerStateMachine.shouldFire(anyTriggerContext())).thenReturn(true);    tester.fireTimer(firstWindow, new Instant(9), TimeDomain.EVENT_TIME);    assertThat(tester.extractOutput().size(), equalTo(0));        when(mockTriggerStateMachine.shouldFire(anyTriggerContext())).thenReturn(true);    triggerShouldFinish(mockTriggerStateMachine);    injectElement(tester, 3);    output = tester.extractOutput();    assertThat(output.size(), equalTo(1));        assertThat(output.get(0).getValue(), containsInAnyOrder(1, 2, 3));    assertThat(output.get(0).getPane(), equalTo(PaneInfo.createPane(false, true, Timing.LATE, 1, 1)));    assertTrue(tester.isMarkedFinished(firstWindow));    tester.assertHasOnlyGlobalAndFinishedSetsFor(firstWindow);    long droppedElements = container.getCounter(MetricName.named(ReduceFnRunner.class, ReduceFnRunner.DROPPED_DUE_TO_CLOSED_WINDOW)).getCumulative();    assertEquals(0, droppedElements);}
public void beam_f3423_0() throws Exception
{    WindowingStrategy<?, IntervalWindow> strategy = WindowingStrategy.of((WindowFn<?, IntervalWindow>) FixedWindows.of(Duration.millis(10))).withTimestampCombiner(TimestampCombiner.EARLIEST).withTrigger(AfterEach.inOrder(Repeatedly.forever(AfterProcessingTime.pastFirstElementInPane().plusDelayOf(new Duration(5))).orFinally(AfterWatermark.pastEndOfWindow()), Repeatedly.forever(AfterProcessingTime.pastFirstElementInPane().plusDelayOf(new Duration(25))))).withMode(AccumulationMode.ACCUMULATING_FIRED_PANES).withAllowedLateness(Duration.millis(100));    ReduceFnTester<Integer, Integer, IntervalWindow> tester = ReduceFnTester.combining(strategy, Sum.ofIntegers(), VarIntCoder.of());    tester.advanceInputWatermark(new Instant(0));    tester.advanceProcessingTime(new Instant(0));        tester.injectElements(TimestampedValue.of(1, new Instant(1)), TimestampedValue.of(1, new Instant(3)), TimestampedValue.of(1, new Instant(7)), TimestampedValue.of(1, new Instant(5)));        tester.advanceProcessingTime(new Instant(6));        tester.advanceInputWatermark(new Instant(11));    List<WindowedValue<Integer>> output = tester.extractOutput();    assertEquals(2, output.size());    assertThat(output.get(0), isSingleWindowedValue(4, 1, 0, 10));    assertThat(output.get(1), isSingleWindowedValue(4, 9, 0, 10));    assertThat(output.get(0), WindowMatchers.valueWithPaneInfo(PaneInfo.createPane(true, false, Timing.EARLY, 0, -1)));    assertThat(output.get(1), WindowMatchers.valueWithPaneInfo(PaneInfo.createPane(false, false, Timing.ON_TIME, 1, 0)));}
private void beam_f3431_0(Context c)
{    assertThat(expectedValue, equalTo(c.getPipelineOptions().as(TestOptions.class).getValue()));    assertThat(c.sideInput(view), greaterThanOrEqualTo(100));}
public Integer beam_f3432_0(Context c)
{    verifyContext(c);    return 0;}
public Integer beam_f3433_0(Integer accumulator, Integer input, Context c)
{    verifyContext(c);    return accumulator + input;}
public static ReduceFnTester<Integer, OutputT, W> beam_f3441_0(WindowingStrategy<?, W> strategy, CombineFnWithContext<Integer, AccumT, OutputT> combineFn, Coder<OutputT> outputCoder, PipelineOptions options, SideInputReader sideInputReader) throws Exception
{    CoderRegistry registry = CoderRegistry.createDefault();        AppliedCombineFn.withInputCoder(combineFn, registry, KvCoder.of(StringUtf8Coder.of(), VarIntCoder.of()));    return combining(strategy, TriggerStateMachines.stateMachineForTrigger(TriggerTranslation.toProto(strategy.getTrigger())), combineFn, outputCoder, options, sideInputReader);}
public static ReduceFnTester<Integer, OutputT, W> beam_f3442_0(WindowingStrategy<?, W> strategy, TriggerStateMachine triggerStateMachine, CombineFnWithContext<Integer, AccumT, OutputT> combineFn, Coder<OutputT> outputCoder, PipelineOptions options, SideInputReader sideInputReader) throws Exception
{    CoderRegistry registry = CoderRegistry.createDefault();    AppliedCombineFn<String, Integer, AccumT, OutputT> fn = AppliedCombineFn.withInputCoder(combineFn, registry, KvCoder.of(StringUtf8Coder.of(), VarIntCoder.of()));    return new ReduceFnTester<>(strategy, triggerStateMachine, SystemReduceFn.combining(StringUtf8Coder.of(), fn), outputCoder, options, sideInputReader);}
public void beam_f3443_0(boolean autoAdvanceOutputWatermark)
{    this.autoAdvanceOutputWatermark = autoAdvanceOutputWatermark;}
public final void beam_f3451_0()
{    assertHasOnlyGlobalAndAllowedTags(Collections.emptySet(), Collections.emptySet());}
public final void beam_f3452_0(W... expectedWindows)
{    assertHasOnlyGlobalAndAllowedTags(ImmutableSet.copyOf(expectedWindows), ImmutableSet.of(PaneInfoTracker.PANE_INFO_TAG, WatermarkHold.watermarkHoldTagForTimestampCombiner(objectStrategy.getTimestampCombiner()), WatermarkHold.EXTRA_HOLD_TAG));}
private void beam_f3453_0(Set<W> expectedWindows, Set<StateTag<?>> allowedTags)
{    Set<StateNamespace> expectedWindowsSet = new HashSet<>();    Set<Equivalence.Wrapper<StateTag>> allowedEquivalentTags = new HashSet<>();    for (StateTag tag : allowedTags) {        allowedEquivalentTags.add(StateTags.ID_EQUIVALENCE.wrap(tag));    }    for (W expectedWindow : expectedWindows) {        expectedWindowsSet.add(windowNamespace(expectedWindow));    }    Map<StateNamespace, Set<Equivalence.Wrapper<StateTag>>> actualWindows = new HashMap<>();    for (StateNamespace namespace : stateInternals.getNamespacesInUse()) {        if (namespace instanceof StateNamespaces.GlobalNamespace) {            continue;        } else if (namespace instanceof StateNamespaces.WindowNamespace) {            Set<Equivalence.Wrapper<StateTag>> tagsInUse = new HashSet<>();            for (StateTag tag : stateInternals.getTagsInUse(namespace)) {                tagsInUse.add(StateTags.ID_EQUIVALENCE.wrap(tag));            }            if (tagsInUse.isEmpty()) {                continue;            }            actualWindows.put(namespace, tagsInUse);            Set<Equivalence.Wrapper<StateTag>> unexpected = Sets.difference(tagsInUse, allowedEquivalentTags);            if (unexpected.isEmpty()) {                continue;            } else {                fail(namespace + " has unexpected states: " + tagsInUse);            }        } else if (namespace instanceof StateNamespaces.WindowAndTriggerNamespace) {            Set<Equivalence.Wrapper<StateTag>> tagsInUse = new HashSet<>();            for (StateTag tag : stateInternals.getTagsInUse(namespace)) {                tagsInUse.add(StateTags.ID_EQUIVALENCE.wrap(tag));            }            assertTrue(namespace + " contains " + tagsInUse, tagsInUse.isEmpty());        } else {            fail("Unrecognized namespace " + namespace);        }    }    assertEquals("Still in use: " + actualWindows.toString(), expectedWindowsSet, actualWindows.keySet());}
public void beam_f3461_0(Instant newProcessingTime) throws Exception
{    timerInternals.advanceProcessingTime(newProcessingTime);}
public void beam_f3462_0(Instant newOutputWatermark) throws Exception
{    timerInternals.advanceOutputWatermark(newOutputWatermark);}
public void beam_f3463_0(Instant newProcessingTime) throws Exception
{    timerInternals.advanceProcessingTime(newProcessingTime);    ReduceFnRunner<String, InputT, OutputT, W> runner = createRunner();    while (true) {        TimerData timer;        List<TimerInternals.TimerData> timers = new ArrayList<>();        while ((timer = timerInternals.removeNextProcessingTimer()) != null) {            timers.add(timer);        }        if (timers.isEmpty()) {            break;        }        runner.onTimers(timers);    }    runner.persist();}
public Object beam_f3471_0()
{    return element;}
public Instant beam_f3472_0()
{    return timestamp;}
public BoundedWindow beam_f3473_0()
{    return window;}
private WindowedValue<Iterable<?>> beam_f3481_0(List<Object> values, Instant timestamp, BoundedWindow window)
{    return (WindowedValue) WindowedValue.of(values, timestamp, window, PaneInfo.NO_FIRING);}
public void beam_f3482_0()
{    MockitoAnnotations.initMocks(this);    when(mockStepContext.timerInternals()).thenReturn(mockTimerInternals);}
public void beam_f3483_0()
{    ThrowingDoFn fn = new ThrowingDoFn();    DoFnRunner<String, String> runner = new SimpleDoFnRunner<>(null, fn, NullSideInputReader.empty(), null, null, Collections.emptyList(), mockStepContext, null, Collections.emptyMap(), WindowingStrategy.of(new GlobalWindows()), DoFnSchemaInformation.create(), Collections.emptyMap());    thrown.expect(UserCodeException.class);    thrown.expectCause(is(fn.exceptionToThrow));    runner.processElement(WindowedValue.valueInGlobalWindow("anyValue"));}
public void beam_f3491_0()
{    SkewingDoFn fn = new SkewingDoFn(Duration.millis(Long.MAX_VALUE));    DoFnRunner<Duration, Duration> runner = new SimpleDoFnRunner<>(null, fn, NullSideInputReader.empty(), new ListOutputManager(), new TupleTag<>(), Collections.emptyList(), mockStepContext, null, Collections.emptyMap(), WindowingStrategy.of(new GlobalWindows()), DoFnSchemaInformation.create(), Collections.emptyMap());    runner.startBundle();    runner.processElement(WindowedValue.timestampedValueInGlobalWindow(Duration.millis(1L), new Instant(0)));    runner.processElement(WindowedValue.timestampedValueInGlobalWindow(Duration.millis(1L), BoundedWindow.TIMESTAMP_MIN_VALUE.plus(Duration.millis(1))));    runner.processElement(WindowedValue.timestampedValueInGlobalWindow(    Duration.millis(BoundedWindow.TIMESTAMP_MAX_VALUE.getMillis()).minus(Duration.millis(BoundedWindow.TIMESTAMP_MIN_VALUE.getMillis())), BoundedWindow.TIMESTAMP_MAX_VALUE));}
public void beam_f3492_0() throws Exception
{    throw exceptionToThrow;}
public void beam_f3493_0() throws Exception
{    throw exceptionToThrow;}
public void beam_f3501_0()
{    MockitoAnnotations.initMocks(this);    PCollection<Integer> created = p.apply(Create.of(1, 2, 3));    singletonView = created.apply(Window.into(new IdentitySideInputWindowFn())).apply(Sum.integersGlobally().asSingletonView());    underlying = new TestDoFnRunner<>();}
private SimplePushbackSideInputDoFnRunner<Integer, Integer> beam_f3502_0(ImmutableList<PCollectionView<?>> views)
{    SimplePushbackSideInputDoFnRunner<Integer, Integer> runner = SimplePushbackSideInputDoFnRunner.create(underlying, views, reader);    runner.startBundle();    return runner;}
public void beam_f3503_0()
{    PushbackSideInputDoFnRunner runner = createRunner(ImmutableList.of(singletonView));    assertThat(underlying.started, is(true));    assertThat(underlying.finished, is(false));    runner.finishBundle();    assertThat(underlying.finished, is(true));}
public void beam_f3511_0()
{    started = true;    inputElems = new ArrayList<>();    firedTimers = new ArrayList<>();}
public void beam_f3512_0(WindowedValue<InputT> elem)
{    inputElems.add(elem);}
public void beam_f3513_0(String timerId, BoundedWindow window, Instant timestamp, TimeDomain timeDomain)
{    firedTimers.add(TimerData.of(timerId, StateNamespaces.window(IntervalWindow.getCoder(), (IntervalWindow) window), timestamp, timeDomain));}
public boolean beam_f3522_0()
{    return true;}
public void beam_f3523_0() throws Exception
{    tester.close();}
 void beam_f3524_0(InputT element, RestrictionT restriction) throws Exception
{    startElement(WindowedValue.of(KV.of(element, restriction), currentProcessingTime, GlobalWindow.INSTANCE, PaneInfo.ON_TIME_AND_ONLY_FIRING));}
public void beam_f3532_0(ProcessContext c, RestrictionTracker<SomeRestriction, Void> tracker)
{    checkState(tracker.tryClaim(null));    c.output(c.element().toString() + "a");    c.output(c.element().toString() + "b");    c.output(c.element().toString() + "c");}
public SomeRestriction beam_f3533_0(Integer elem)
{    return new SomeRestriction();}
public void beam_f3534_0() throws Exception
{                DoFn<Integer, String> fn = new ToStringFn();    Instant base = Instant.now();    IntervalWindow w = new IntervalWindow(base.minus(Duration.standardMinutes(1)), base.plus(Duration.standardMinutes(1)));    ProcessFnTester<Integer, String, SomeRestriction, Void> tester = new ProcessFnTester<>(base, fn, BigEndianIntegerCoder.of(), SerializableCoder.of(SomeRestriction.class), MAX_OUTPUTS_PER_BUNDLE, MAX_BUNDLE_DURATION);    tester.startElement(WindowedValue.of(KV.of(42, new SomeRestriction()), base, Collections.singletonList(w), PaneInfo.ON_TIME_AND_ONLY_FIRING));    assertEquals(Arrays.asList(TimestampedValue.of("42a", base), TimestampedValue.of("42b", base), TimestampedValue.of("42c", base)), tester.peekOutputElementsInWindow(w));}
public ProcessContinuation beam_f3542_0(ProcessContext c, RestrictionTracker<OffsetRange, Long> tracker)
{    for (long i = tracker.currentRestriction().getFrom(), numIterations = 0; tracker.tryClaim(i); ++i, ++numIterations) {        c.output(String.valueOf(c.element() + i));        if (numIterations == numOutputsPerCall - 1) {            return resume();        }    }    return stop();}
public OffsetRange beam_f3543_0(Integer elem)
{    throw new UnsupportedOperationException("Expected to be supplied explicitly in this test");}
public void beam_f3544_0() throws Exception
{    DoFn<Integer, String> fn = new CounterFn(1);    Instant base = Instant.now();    dateTimeProvider.setDateTimeFixed(base.getMillis());    ProcessFnTester<Integer, String, OffsetRange, Long> tester = new ProcessFnTester<>(base, fn, BigEndianIntegerCoder.of(), SerializableCoder.of(OffsetRange.class), MAX_OUTPUTS_PER_BUNDLE, MAX_BUNDLE_DURATION);    tester.startElement(42, new OffsetRange(0, 3));    assertThat(tester.takeOutputElements(), contains("42"));    assertTrue(tester.advanceProcessingTimeBy(Duration.standardSeconds(1)));    assertThat(tester.takeOutputElements(), contains("43"));    assertTrue(tester.advanceProcessingTimeBy(Duration.standardSeconds(1)));    assertThat(tester.takeOutputElements(), contains("44"));    assertTrue(tester.advanceProcessingTimeBy(Duration.standardSeconds(1)));        assertEquals(0, tester.takeOutputElements().size());        assertFalse(tester.advanceProcessingTimeBy(Duration.standardSeconds(1)));}
public void beam_f3552_0()
{    assertEquals(State.INSIDE_BUNDLE, state);    state = State.OUTSIDE_BUNDLE;}
public void beam_f3553_0() throws Exception
{    DoFn<Integer, String> fn = new LifecycleVerifyingFn();    try (ProcessFnTester<Integer, String, SomeRestriction, Void> tester = new ProcessFnTester<>(Instant.now(), fn, BigEndianIntegerCoder.of(), SerializableCoder.of(SomeRestriction.class), MAX_OUTPUTS_PER_BUNDLE, MAX_BUNDLE_DURATION)) {        tester.startElement(42, new SomeRestriction());    }}
private static StateNamespace beam_f3554_0(IntervalWindow window)
{    return StateNamespaces.window((Coder) WINDOWING_STRATEGY.getWindowFn().windowCoder(), window);}
public void beam_f3562_0() throws Exception
{    ValueState<String> value = underTest.state(NAMESPACE_1, STRING_VALUE_ADDR);        assertThat(underTest.state(NAMESPACE_1, STRING_VALUE_ADDR), equalTo(value));    assertThat(underTest.state(NAMESPACE_2, STRING_VALUE_ADDR), not(equalTo(value)));    assertThat(value.read(), Matchers.nullValue());    value.write("hello");    assertThat(value.read(), equalTo("hello"));    value.write("world");    assertThat(value.read(), equalTo("world"));    value.clear();    assertThat(value.read(), Matchers.nullValue());    assertThat(underTest.state(NAMESPACE_1, STRING_VALUE_ADDR), equalTo(value));}
public void beam_f3563_0() throws Exception
{    BagState<String> value = underTest.state(NAMESPACE_1, STRING_BAG_ADDR);        assertThat(value, equalTo(underTest.state(NAMESPACE_1, STRING_BAG_ADDR)));    assertThat(value, not(equalTo(underTest.state(NAMESPACE_2, STRING_BAG_ADDR))));    assertThat(value.read(), Matchers.emptyIterable());    value.add("hello");    assertThat(value.read(), containsInAnyOrder("hello"));    value.add("world");    assertThat(value.read(), containsInAnyOrder("hello", "world"));    value.clear();    assertThat(value.read(), Matchers.emptyIterable());    assertThat(underTest.state(NAMESPACE_1, STRING_BAG_ADDR), equalTo(value));}
public void beam_f3564_0() throws Exception
{    BagState<String> value = underTest.state(NAMESPACE_1, STRING_BAG_ADDR);    assertThat(value.isEmpty().read(), Matchers.is(true));    ReadableState<Boolean> readFuture = value.isEmpty();    value.add("hello");    assertThat(readFuture.read(), Matchers.is(false));    value.clear();    assertThat(readFuture.read(), Matchers.is(true));}
public final K beam_f3572_0()
{    return key;}
public final V beam_f3573_0()
{    return value;}
public final String beam_f3574_0()
{    return key + "=" + value;}
public void beam_f3582_0() throws Exception
{    CombiningState<Integer, int[], Integer> value1 = underTest.state(NAMESPACE_1, SUM_INTEGER_ADDR);    CombiningState<Integer, int[], Integer> value2 = underTest.state(NAMESPACE_2, SUM_INTEGER_ADDR);    CombiningState<Integer, int[], Integer> value3 = underTest.state(NAMESPACE_3, SUM_INTEGER_ADDR);    assertThat(value1.getAccum(), Matchers.is(notNullValue()));    assertThat(value2.getAccum(), Matchers.is(notNullValue()));    assertThat(value3.getAccum(), Matchers.is(notNullValue()));    value1.add(5);    value2.add(10);    value1.add(6);    StateMerging.mergeCombiningValues(Arrays.asList(value1, value2), value3);        assertThat(value1.read(), equalTo(0));    assertThat(value2.read(), equalTo(0));    assertThat(value3.read(), equalTo(21));}
public void beam_f3583_0() throws Exception
{    CombiningState<Integer, Integer, Integer> value1 = underTest.state(NAMESPACE_1, SUM_INTEGER_CONTEXT_ADDR);    CombiningState<Integer, Integer, Integer> value2 = underTest.state(NAMESPACE_2, SUM_INTEGER_CONTEXT_ADDR);    assertThat(value1.getAccum(), Matchers.is(notNullValue()));    assertThat(value2.getAccum(), Matchers.is(notNullValue()));    value1.add(5);    value2.add(10);    value1.add(6);    assertThat(value1.read(), equalTo(11));    assertThat(value2.read(), equalTo(10));        StateMerging.mergeCombiningValues(Arrays.asList(value1, value2), value1);    assertThat(value1.read(), equalTo(21));    assertThat(value2.read(), equalTo(0));}
public void beam_f3584_0() throws Exception
{    CombiningState<Integer, Integer, Integer> value1 = underTest.state(NAMESPACE_1, SUM_INTEGER_CONTEXT_ADDR);    CombiningState<Integer, Integer, Integer> value2 = underTest.state(NAMESPACE_2, SUM_INTEGER_CONTEXT_ADDR);    CombiningState<Integer, Integer, Integer> value3 = underTest.state(NAMESPACE_3, SUM_INTEGER_CONTEXT_ADDR);    assertThat(value1.getAccum(), Matchers.is(notNullValue()));    assertThat(value2.getAccum(), Matchers.is(notNullValue()));    assertThat(value3.getAccum(), Matchers.is(notNullValue()));    value1.add(5);    value2.add(10);    value1.add(6);    StateMerging.mergeCombiningValues(Arrays.asList(value1, value2), value3);        assertThat(value1.read(), equalTo(0));    assertThat(value2.read(), equalTo(0));    assertThat(value3.read(), equalTo(21));}
public void beam_f3592_0(String value, OutputStream outStream) throws CoderException, IOException
{    realCoder.encode(value, outStream);}
public String beam_f3593_0(InputStream inStream) throws CoderException, IOException
{    return realCoder.decode(inStream);}
public List<? extends Coder<?>> beam_f3594_0()
{    return null;}
public void beam_f3603_0()
{    StateNamespace global = StateNamespaces.global();    StateNamespace intervalWindow = StateNamespaces.window(intervalCoder, intervalWindow(1000, 87392));    StateNamespace intervalWindowAndTrigger = StateNamespaces.windowAndTrigger(intervalCoder, intervalWindow(1000, 87392), 57);    StateNamespace globalWindow = StateNamespaces.window(GlobalWindow.Coder.INSTANCE, GlobalWindow.INSTANCE);    StateNamespace globalWindowAndTrigger = StateNamespaces.windowAndTrigger(GlobalWindow.Coder.INSTANCE, GlobalWindow.INSTANCE, 12);    assertEquals("/", global.stringKey());    assertEquals("/gAAAAAABVWD4ogU/", intervalWindow.stringKey());    assertEquals("/gAAAAAABVWD4ogU/1L/", intervalWindowAndTrigger.stringKey());    assertEquals("//", globalWindow.stringKey());    assertEquals("//C/", globalWindowAndTrigger.stringKey());}
public void beam_f3604_0()
{    StateNamespace window = StateNamespaces.window(intervalCoder, intervalWindow(1000, 87392));    StateNamespace windowAndTrigger = StateNamespaces.windowAndTrigger(intervalCoder, intervalWindow(1000, 87392), 57);    assertThat(windowAndTrigger.stringKey(), Matchers.startsWith(window.stringKey()));    assertThat(StateNamespaces.global().stringKey(), Matchers.not(Matchers.startsWith(window.stringKey())));}
public void beam_f3605_0()
{    StateNamespace window = StateNamespaces.window(GlobalWindow.Coder.INSTANCE, GlobalWindow.INSTANCE);    StateNamespace windowAndTrigger = StateNamespaces.windowAndTrigger(GlobalWindow.Coder.INSTANCE, GlobalWindow.INSTANCE, 57);    assertThat(windowAndTrigger.stringKey(), Matchers.startsWith(window.stringKey()));    assertThat(StateNamespaces.global().stringKey(), Matchers.not(Matchers.startsWith(window.stringKey())));}
public void beam_f3613_0()
{    StateTag<?> fooStringVarInt1 = StateTags.map("foo", StringUtf8Coder.of(), VarIntCoder.of());    StateTag<?> fooStringVarInt2 = StateTags.map("foo", StringUtf8Coder.of(), VarIntCoder.of());    StateTag<?> fooStringBigEndian = StateTags.map("foo", StringUtf8Coder.of(), BigEndianIntegerCoder.of());    StateTag<?> fooVarIntBigEndian = StateTags.map("foo", VarIntCoder.of(), BigEndianIntegerCoder.of());    StateTag<?> barStringVarInt = StateTags.map("bar", StringUtf8Coder.of(), VarIntCoder.of());    assertEquals(fooStringVarInt1, fooStringVarInt2);    assertEquals(fooStringVarInt1, fooStringBigEndian);    assertEquals(fooStringBigEndian, fooVarIntBigEndian);    assertEquals(fooStringVarInt1, fooVarIntBigEndian);    assertNotEquals(fooStringVarInt1, barStringVarInt);}
public void beam_f3614_0()
{    StateTag<?> foo1 = StateTags.watermarkStateInternal("foo", TimestampCombiner.EARLIEST);    StateTag<?> foo2 = StateTags.watermarkStateInternal("foo", TimestampCombiner.EARLIEST);    StateTag<?> bar = StateTags.watermarkStateInternal("bar", TimestampCombiner.EARLIEST);    StateTag<?> bar2 = StateTags.watermarkStateInternal("bar", TimestampCombiner.LATEST);        assertEquals(foo1, foo2);        assertNotEquals(foo1, bar);        assertEquals(bar, bar2);}
public void beam_f3615_0()
{    Combine.BinaryCombineIntegerFn maxFn = Max.ofIntegers();    Coder<Integer> input1 = VarIntCoder.of();    Coder<Integer> input2 = BigEndianIntegerCoder.of();    Combine.BinaryCombineIntegerFn minFn = Min.ofIntegers();    StateTag<?> fooCoder1Max1 = StateTags.combiningValueFromInputInternal("foo", input1, maxFn);    StateTag<?> fooCoder1Max2 = StateTags.combiningValueFromInputInternal("foo", input1, maxFn);    StateTag<?> fooCoder1Min = StateTags.combiningValueFromInputInternal("foo", input1, minFn);    StateTag<?> fooCoder2Max = StateTags.combiningValueFromInputInternal("foo", input2, maxFn);    StateTag<?> barCoder1Max = StateTags.combiningValueFromInputInternal("bar", input1, maxFn);        assertEquals(fooCoder1Max1, fooCoder1Max2);    assertEquals(StateTags.convertToBagTagInternal((StateTag) fooCoder1Max1), StateTags.convertToBagTagInternal((StateTag) fooCoder1Max2));        assertEquals(fooCoder1Max1, fooCoder1Min);    assertEquals(StateTags.convertToBagTagInternal((StateTag) fooCoder1Max1), StateTags.convertToBagTagInternal((StateTag) fooCoder1Min));        assertEquals(fooCoder1Max1, fooCoder2Max);    assertEquals(StateTags.convertToBagTagInternal((StateTag) fooCoder1Max1), StateTags.convertToBagTagInternal((StateTag) fooCoder2Max));        assertNotEquals(fooCoder1Max1, barCoder1Max);    assertNotEquals(StateTags.convertToBagTagInternal((StateTag) fooCoder1Max1), StateTags.convertToBagTagInternal((StateTag) barCoder1Max));}
public void beam_f3623_0()
{    Instant timestamp = new Instant(100);    StateNamespace namespace = StateNamespaces.global();    TimerData id0Timer = TimerData.of("id0", namespace, timestamp, TimeDomain.EVENT_TIME);    TimerData id1Timer = TimerData.of("id1", namespace, timestamp, TimeDomain.EVENT_TIME);    assertThat(id0Timer, lessThan(id1Timer));}
public void beam_f3624_0() throws Exception
{    tester = TriggerStateMachineTester.forTrigger(AfterAllStateMachine.of(AfterPaneStateMachine.elementCountAtLeast(1), AfterPaneStateMachine.elementCountAtLeast(2)), FixedWindows.of(Duration.millis(100)));    IntervalWindow window = new IntervalWindow(new Instant(0), new Instant(100));    tester.injectElements(1);    assertFalse(tester.shouldFire(window));    tester.injectElements(2);    assertTrue(tester.shouldFire(window));    tester.fireIfShouldFire(window);    assertTrue(tester.isMarkedFinished(window));}
public void beam_f3625_0() throws Exception
{    tester = TriggerStateMachineTester.forTrigger(AfterAllStateMachine.of(AfterPaneStateMachine.elementCountAtLeast(2), AfterPaneStateMachine.elementCountAtLeast(1)), FixedWindows.of(Duration.millis(100)));    IntervalWindow window = new IntervalWindow(new Instant(0), new Instant(100));    tester.injectElements(1);    assertFalse(tester.shouldFire(window));    tester.injectElements(2);    assertTrue(tester.shouldFire(window));    tester.fireIfShouldFire(window);    assertTrue(tester.isMarkedFinished(window));}
public void beam_f3633_0() throws Exception
{    tester = TriggerStateMachineTester.forTrigger(AfterFirstStateMachine.of(mockTrigger1, mockTrigger2), FixedWindows.of(Duration.millis(10)));    tester.injectElements(1);    IntervalWindow window = new IntervalWindow(new Instant(0), new Instant(10));    when(mockTrigger1.shouldFire(anyTriggerContext())).thenReturn(false);    when(mockTrigger2.shouldFire(anyTriggerContext())).thenReturn(false);        assertFalse(tester.shouldFire(window));        assertFalse(tester.isMarkedFinished(window));}
public void beam_f3634_0() throws Exception
{    tester = TriggerStateMachineTester.forTrigger(AfterFirstStateMachine.of(mockTrigger1, mockTrigger2), FixedWindows.of(Duration.millis(10)));    tester.injectElements(1);    IntervalWindow window = new IntervalWindow(new Instant(1), new Instant(11));    when(mockTrigger1.shouldFire(anyTriggerContext())).thenReturn(true);    when(mockTrigger2.shouldFire(anyTriggerContext())).thenReturn(false);        assertTrue(tester.shouldFire(window));    tester.fireIfShouldFire(window);    assertTrue(tester.isMarkedFinished(window));}
public void beam_f3635_0() throws Exception
{    tester = TriggerStateMachineTester.forTrigger(AfterFirstStateMachine.of(mockTrigger1, mockTrigger2), FixedWindows.of(Duration.millis(10)));    tester.injectElements(1);    IntervalWindow window = new IntervalWindow(new Instant(1), new Instant(11));    when(mockTrigger1.shouldFire(anyTriggerContext())).thenReturn(false);    when(mockTrigger2.shouldFire(anyTriggerContext())).thenReturn(true);        assertTrue(tester.shouldFire(window));        tester.fireIfShouldFire(window);    assertTrue(tester.isMarkedFinished(window));}
public void beam_f3643_0() throws Exception
{    SimpleTriggerStateMachineTester<IntervalWindow> tester = TriggerStateMachineTester.forTrigger(AfterProcessingTimeStateMachine.pastFirstElementInPane().plusDelayOf(Duration.millis(5)), FixedWindows.of(Duration.millis(10)));    tester.injectElements(1, 2, 3);    IntervalWindow window = new IntervalWindow(new Instant(0), new Instant(10));    tester.clearState(window);    tester.assertCleared(window);}
public void beam_f3644_0() throws Exception
{    SimpleTriggerStateMachineTester<IntervalWindow> tester = TriggerStateMachineTester.forTrigger(AfterProcessingTimeStateMachine.pastFirstElementInPane().plusDelayOf(Duration.millis(5)), Sessions.withGapDuration(Duration.millis(10)));    tester.advanceProcessingTime(new Instant(10));        tester.injectElements(1);    IntervalWindow firstWindow = new IntervalWindow(new Instant(1), new Instant(11));    assertFalse(tester.shouldFire(firstWindow));    tester.advanceProcessingTime(new Instant(12));        tester.injectElements(3);    IntervalWindow secondWindow = new IntervalWindow(new Instant(3), new Instant(13));    assertFalse(tester.shouldFire(secondWindow));    tester.mergeWindows();    IntervalWindow mergedWindow = new IntervalWindow(new Instant(1), new Instant(13));    tester.advanceProcessingTime(new Instant(16));    assertTrue(tester.shouldFire(mergedWindow));}
public void beam_f3645_0() throws Exception
{    TriggerStateMachine t1 = AfterProcessingTimeStateMachine.pastFirstElementInPane().plusDelayOf(Duration.standardMinutes(1L));    TriggerStateMachine t2 = AfterProcessingTimeStateMachine.pastFirstElementInPane().plusDelayOf(Duration.standardMinutes(1L));    assertTrue(t1.isCompatible(t2));}
private void beam_f3653_0(int... elements) throws Exception
{    for (int element : elements) {        doNothing().when(mockEarly).onElement(anyElementContext());        doNothing().when(mockLate).onElement(anyElementContext());        tester.injectElements(element);    }}
public void beam_f3654_0()
{    MockitoAnnotations.initMocks(this);}
public void beam_f3655_0(TriggerStateMachine mockTrigger, IntervalWindow window) throws Exception
{        when(mockTrigger.shouldFire(anyTriggerContext())).thenReturn(false);        assertFalse(tester.shouldFire(window));        when(mockTrigger.shouldFire(anyTriggerContext())).thenReturn(true);        assertTrue(tester.shouldFire(window));    tester.fireIfShouldFire(window);    assertFalse(tester.isMarkedFinished(window));}
public void beam_f3663_0() throws Exception
{    tester = TriggerStateMachineTester.forTrigger(AfterWatermarkStateMachine.pastEndOfWindow().withEarlyFirings(AfterPaneStateMachine.elementCountAtLeast(100)).withLateFirings(AfterPaneStateMachine.elementCountAtLeast(1)), Sessions.withGapDuration(Duration.millis(10)));    tester.injectElements(1);    tester.injectElements(5);    IntervalWindow firstWindow = new IntervalWindow(new Instant(1), new Instant(11));    IntervalWindow secondWindow = new IntervalWindow(new Instant(5), new Instant(15));    IntervalWindow mergedWindow = new IntervalWindow(new Instant(1), new Instant(15));        tester.advanceInputWatermark(new Instant(15));    assertTrue(tester.shouldFire(firstWindow));    assertTrue(tester.shouldFire(secondWindow));    tester.fireIfShouldFire(firstWindow);    tester.fireIfShouldFire(secondWindow);        assertFalse(tester.shouldFire(firstWindow));    assertFalse(tester.shouldFire(secondWindow));    tester.injectElements(1);    tester.injectElements(5);    assertTrue(tester.shouldFire(firstWindow));    assertTrue(tester.shouldFire(secondWindow));    tester.fireIfShouldFire(firstWindow);    tester.fireIfShouldFire(secondWindow);        tester.mergeWindows();        assertFalse(tester.shouldFire(mergedWindow));    tester.injectElements(1);    assertTrue(tester.shouldFire(mergedWindow));}
public void beam_f3664_0() throws Exception
{    tester = TriggerStateMachineTester.forTrigger(AfterWatermarkStateMachine.pastEndOfWindow().withEarlyFirings(mockEarly).withLateFirings(mockLate), Sessions.withGapDuration(Duration.millis(10)));    tester.injectElements(1);    tester.injectElements(5);        tester.mergeWindows();    verify(mockEarly).onMerge(Mockito.any(OnMergeContext.class));}
public void beam_f3665_0() throws Exception
{    tester = TriggerStateMachineTester.forTrigger(AfterWatermarkStateMachine.pastEndOfWindow().withEarlyFirings(AfterPaneStateMachine.elementCountAtLeast(100)).withLateFirings(AfterPaneStateMachine.elementCountAtLeast(1)), Sessions.withGapDuration(Duration.millis(10)));    tester.injectElements(1);    tester.injectElements(5);    IntervalWindow firstWindow = new IntervalWindow(new Instant(1), new Instant(11));    IntervalWindow secondWindow = new IntervalWindow(new Instant(5), new Instant(15));    IntervalWindow mergedWindow = new IntervalWindow(new Instant(1), new Instant(15));        tester.advanceInputWatermark(new Instant(11));    assertTrue(tester.shouldFire(firstWindow));    assertFalse(tester.shouldFire(secondWindow));    tester.fireIfShouldFire(firstWindow);        assertFalse(tester.shouldFire(firstWindow));    tester.injectElements(1);    assertTrue(tester.shouldFire(firstWindow));    tester.fireIfShouldFire(firstWindow);        tester.mergeWindows();        assertFalse(tester.shouldFire(mergedWindow));    tester.injectElements(1);    assertFalse(tester.shouldFire(mergedWindow));        tester.advanceInputWatermark(new Instant(15));    assertTrue(tester.shouldFire(mergedWindow));}
public void beam_f3673_0() throws Exception
{    tester = TriggerStateMachineTester.forTrigger(DefaultTriggerStateMachine.of(), Sessions.withGapDuration(Duration.millis(100)));    tester.injectElements(    1,     50);    tester.mergeWindows();    IntervalWindow firstWindow = new IntervalWindow(new Instant(1), new Instant(101));    IntervalWindow secondWindow = new IntervalWindow(new Instant(50), new Instant(150));    IntervalWindow mergedWindow = new IntervalWindow(new Instant(1), new Instant(150));        tester.advanceInputWatermark(new Instant(100));    assertFalse(tester.shouldFire(firstWindow));    assertFalse(tester.shouldFire(secondWindow));    assertFalse(tester.shouldFire(mergedWindow));        tester.advanceInputWatermark(new Instant(149));    assertTrue(tester.shouldFire(firstWindow));    assertFalse(tester.shouldFire(secondWindow));    assertFalse(tester.shouldFire(mergedWindow));        tester.advanceInputWatermark(new Instant(150));    assertTrue(tester.shouldFire(firstWindow));    assertTrue(tester.shouldFire(secondWindow));    assertTrue(tester.shouldFire(mergedWindow));        tester.fireIfShouldFire(mergedWindow);    assertTrue(tester.shouldFire(mergedWindow));    assertFalse(tester.isMarkedFinished(mergedWindow));}
public void beam_f3674_0() throws Exception
{    StubStateMachine t1 = new StubStateMachine();    ExecutableTriggerStateMachine executable = ExecutableTriggerStateMachine.create(t1);    assertEquals(0, executable.getTriggerIndex());}
public void beam_f3675_0() throws Exception
{    StubStateMachine t1 = new StubStateMachine();    StubStateMachine t2 = new StubStateMachine();    StubStateMachine t = new StubStateMachine(t1, t2);    ExecutableTriggerStateMachine executable = ExecutableTriggerStateMachine.create(t);    assertEquals(0, executable.getTriggerIndex());    assertEquals(1, executable.subTriggers().get(0).getTriggerIndex());    assertSame(t1, executable.subTriggers().get(0).getSpec());    assertEquals(2, executable.subTriggers().get(1).getTriggerIndex());    assertSame(t2, executable.subTriggers().get(1).getSpec());}
public static void beam_f3687_0(FinishedTriggers finishedSet)
{    ExecutableTriggerStateMachine trigger = ExecutableTriggerStateMachine.create(AfterAllStateMachine.of(AfterFirstStateMachine.of(AfterPaneStateMachine.elementCountAtLeast(3), AfterWatermarkStateMachine.pastEndOfWindow()), AfterAllStateMachine.of(AfterPaneStateMachine.elementCountAtLeast(10), AfterProcessingTimeStateMachine.pastFirstElementInPane())));        setFinishedRecursively(finishedSet, trigger);    assertTrue(finishedSet.isFinished(trigger));    assertTrue(finishedSet.isFinished(trigger.subTriggers().get(0)));    assertTrue(finishedSet.isFinished(trigger.subTriggers().get(0).subTriggers().get(0)));    assertTrue(finishedSet.isFinished(trigger.subTriggers().get(0).subTriggers().get(1)));        finishedSet.clearRecursively(trigger.subTriggers().get(1));        assertTrue(finishedSet.isFinished(trigger));    verifyFinishedRecursively(finishedSet, trigger.subTriggers().get(0));    verifyUnfinishedRecursively(finishedSet, trigger.subTriggers().get(1));}
private static void beam_f3688_0(FinishedTriggers finishedSet, ExecutableTriggerStateMachine trigger)
{    finishedSet.setFinished(trigger, true);    for (ExecutableTriggerStateMachine subTrigger : trigger.subTriggers()) {        setFinishedRecursively(finishedSet, subTrigger);    }}
private static void beam_f3689_0(FinishedTriggers finishedSet, ExecutableTriggerStateMachine trigger)
{    assertTrue(finishedSet.isFinished(trigger));    for (ExecutableTriggerStateMachine subTrigger : trigger.subTriggers()) {        verifyFinishedRecursively(finishedSet, subTrigger);    }}
public void beam_f3697_0() throws Exception
{    tester = TriggerStateMachineTester.forTrigger(new OrFinallyStateMachine(RepeatedlyStateMachine.forever(AfterPaneStateMachine.elementCountAtLeast(2)), AfterPaneStateMachine.elementCountAtLeast(100)), FixedWindows.of(Duration.millis(100)));    IntervalWindow window = new IntervalWindow(new Instant(0), new Instant(100));        tester.injectElements(1);    assertFalse(tester.shouldFire(window));    assertFalse(tester.isMarkedFinished(window));        tester.injectElements(2);    assertTrue(tester.shouldFire(window));    tester.fireIfShouldFire(window);    assertFalse(tester.isMarkedFinished(window));        tester.injectElements(3, 4);    assertTrue(tester.shouldFire(window));    tester.fireIfShouldFire(window);    assertFalse(tester.isMarkedFinished(window));}
public void beam_f3698_0() throws Exception
{    tester = TriggerStateMachineTester.forTrigger(AfterEachStateMachine.inOrder(AfterPaneStateMachine.elementCountAtLeast(5).orFinally(AfterWatermarkStateMachine.pastEndOfWindow()), RepeatedlyStateMachine.forever(AfterPaneStateMachine.elementCountAtLeast(1))), Sessions.withGapDuration(Duration.millis(10)));        tester.injectElements(1);    IntervalWindow firstWindow = new IntervalWindow(new Instant(1), new Instant(11));    assertFalse(tester.shouldFire(firstWindow));    tester.advanceInputWatermark(new Instant(11));    assertTrue(tester.shouldFire(firstWindow));    tester.fireIfShouldFire(firstWindow);        tester.injectElements(5);    IntervalWindow secondWindow = new IntervalWindow(new Instant(5), new Instant(15));    assertFalse(tester.shouldFire(secondWindow));        tester.mergeWindows();    IntervalWindow mergedWindow = new IntervalWindow(new Instant(1), new Instant(15));    assertFalse(tester.shouldFire(mergedWindow));        tester.injectElements(1, 2, 3, 4, 5);    tester.mergeWindows();    assertTrue(tester.shouldFire(mergedWindow));}
public void beam_f3699_0() throws Exception
{    tester = TriggerStateMachineTester.forTrigger(new OrFinallyStateMachine(RepeatedlyStateMachine.forever(AfterPaneStateMachine.elementCountAtLeast(2)), AfterPaneStateMachine.elementCountAtLeast(3)), FixedWindows.of(Duration.millis(10)));    IntervalWindow window = new IntervalWindow(new Instant(0), new Instant(10));        tester.injectElements(1);    assertFalse(tester.shouldFire(window));    assertFalse(tester.isMarkedFinished(window));        tester.injectElements(2);    assertTrue(tester.shouldFire(window));    tester.fireIfShouldFire(window);    assertFalse(tester.isMarkedFinished(window));        tester.injectElements(3);    assertTrue(tester.shouldFire(window));    tester.fireIfShouldFire(window);    assertTrue(tester.isMarkedFinished(window));}
public void beam_f3707_0() throws Exception
{    SimpleTriggerStateMachineTester<GlobalWindow> tester = TriggerStateMachineTester.forTrigger(RepeatedlyStateMachine.forever(AfterFirstStateMachine.of(AfterProcessingTimeStateMachine.pastFirstElementInPane().plusDelayOf(Duration.standardMinutes(15)), AfterPaneStateMachine.elementCountAtLeast(5))), new GlobalWindows());    GlobalWindow window = GlobalWindow.INSTANCE;    tester.injectElements(1);    assertFalse(tester.shouldFire(window));    tester.advanceProcessingTime(new Instant(0).plus(Duration.standardMinutes(15)));    assertTrue(tester.shouldFire(window));    tester.fireIfShouldFire(window);    assertFalse(tester.shouldFire(window));}
public void beam_f3708_0() throws Exception
{    SimpleTriggerStateMachineTester<GlobalWindow> tester = TriggerStateMachineTester.forTrigger(RepeatedlyStateMachine.forever(AfterPaneStateMachine.elementCountAtLeast(5)), new GlobalWindows());    GlobalWindow window = GlobalWindow.INSTANCE;    tester.injectElements(1);    assertFalse(tester.shouldFire(window));    tester.injectElements(2, 3, 4, 5);    assertTrue(tester.shouldFire(window));    tester.fireIfShouldFire(window);    assertFalse(tester.shouldFire(window));}
public void beam_f3709_0() throws Exception
{    SimpleTriggerStateMachineTester<GlobalWindow> tester = TriggerStateMachineTester.forTrigger(RepeatedlyStateMachine.forever(AfterProcessingTimeStateMachine.pastFirstElementInPane().plusDelayOf(Duration.standardMinutes(15))), new GlobalWindows());    GlobalWindow window = GlobalWindow.INSTANCE;    tester.injectElements(1);    assertFalse(tester.shouldFire(window));    tester.advanceProcessingTime(new Instant(0).plus(Duration.standardMinutes(15)));    assertTrue(tester.shouldFire(window));    tester.fireIfShouldFire(window);    assertFalse(tester.shouldFire(window));}
public String beam_f3717_0()
{    return name;}
public boolean beam_f3721_0(TriggerContext context) throws Exception
{    return false;}
public void beam_f3722_0()
{    int count = 37;    RunnerApi.Trigger trigger = RunnerApi.Trigger.newBuilder().setElementCount(RunnerApi.Trigger.ElementCount.newBuilder().setElementCount(count)).build();    AfterPaneStateMachine machine = (AfterPaneStateMachine) TriggerStateMachines.stateMachineForTrigger(trigger);    assertThat(machine.getElementCount(), equalTo(trigger.getElementCount().getElementCount()));}
public void beam_f3730_0()
{    RunnerApi.Trigger trigger = RunnerApi.Trigger.newBuilder().setAfterEndOfWindow(RunnerApi.Trigger.AfterEndOfWindow.newBuilder().setEarlyFirings(subtrigger1)).build();    AfterWatermarkStateMachine.AfterWatermarkEarlyAndLate machine = (AfterWatermarkStateMachine.AfterWatermarkEarlyAndLate) TriggerStateMachines.stateMachineForTrigger(trigger);    assertThat(machine, equalTo(AfterWatermarkStateMachine.pastEndOfWindow().withEarlyFirings(submachine1)));}
public void beam_f3731_0()
{    RunnerApi.Trigger trigger = RunnerApi.Trigger.newBuilder().setAfterEndOfWindow(RunnerApi.Trigger.AfterEndOfWindow.newBuilder().setEarlyFirings(subtrigger1).setLateFirings(subtrigger2)).build();    AfterWatermarkStateMachine.AfterWatermarkEarlyAndLate machine = (AfterWatermarkStateMachine.AfterWatermarkEarlyAndLate) TriggerStateMachines.stateMachineForTrigger(trigger);    assertThat(machine, equalTo(AfterWatermarkStateMachine.pastEndOfWindow().withEarlyFirings(submachine1).withLateFirings(submachine2)));}
public void beam_f3732_0()
{    RunnerApi.Trigger trigger = RunnerApi.Trigger.newBuilder().setOrFinally(RunnerApi.Trigger.OrFinally.newBuilder().setMain(subtrigger1).setFinally(subtrigger2)).build();    OrFinallyStateMachine machine = (OrFinallyStateMachine) TriggerStateMachines.stateMachineForTrigger(trigger);    assertThat(machine, equalTo(submachine1.orFinally(submachine2)));}
public static SimpleTriggerStateMachineTester<W> beam_f3746_0(TriggerStateMachine stateMachine, WindowFn<Object, W> windowFn) throws Exception
{    ExecutableTriggerStateMachine executableTriggerStateMachine = ExecutableTriggerStateMachine.create(stateMachine);                AccumulationMode mode = windowFn.isNonMerging() ? AccumulationMode.DISCARDING_FIRED_PANES : AccumulationMode.ACCUMULATING_FIRED_PANES;    return new SimpleTriggerStateMachineTester<>(executableTriggerStateMachine, windowFn, Duration.ZERO);}
public static TriggerStateMachineTester<InputT, W> beam_f3747_0(TriggerStateMachine stateMachine, WindowFn<Object, W> windowFn) throws Exception
{    ExecutableTriggerStateMachine executableTriggerStateMachine = ExecutableTriggerStateMachine.create(stateMachine);                AccumulationMode mode = windowFn.isNonMerging() ? AccumulationMode.DISCARDING_FIRED_PANES : AccumulationMode.ACCUMULATING_FIRED_PANES;    return new TriggerStateMachineTester<>(executableTriggerStateMachine, windowFn, Duration.ZERO);}
public void beam_f3748_0(W window) throws Exception
{    executableTrigger.invokeClear(contextFactory.base(window, new TestTimers(windowNamespace(window)), executableTrigger, getFinishedSet(window)));}
public final void beam_f3756_0(Collection<TimestampedValue<InputT>> values) throws Exception
{    for (TimestampedValue<InputT> value : values) {        WindowTracing.trace("TriggerTester.injectElements: {}", value);    }    List<WindowedValue<InputT>> windowedValues = Lists.newArrayListWithCapacity(values.size());    for (TimestampedValue<InputT> input : values) {        try {            InputT value = input.getValue();            Instant timestamp = input.getTimestamp();            Collection<W> assignedWindows = windowFn.assignWindows(new TestAssignContext<W>(windowFn, value, timestamp, GlobalWindow.INSTANCE));            for (W window : assignedWindows) {                activeWindows.addActiveForTesting(window);            }            windowedValues.add(WindowedValue.of(value, timestamp, assignedWindows, PaneInfo.NO_FIRING));        } catch (Exception e) {            throw new RuntimeException(e);        }    }    for (WindowedValue<InputT> windowedValue : windowedValues) {        for (BoundedWindow untypedWindow : windowedValue.getWindows()) {                        @SuppressWarnings("unchecked")            W window = mergeResult((W) untypedWindow);            TriggerStateMachine.OnElementContext context = contextFactory.createOnElementContext(window, new TestTimers(windowNamespace(window)), windowedValue.getTimestamp(), executableTrigger, getFinishedSet(window));            if (!context.trigger().isFinished()) {                executableTrigger.invokeOnElement(context);            }        }    }}
public boolean beam_f3757_0(W window) throws Exception
{    TriggerStateMachine.TriggerContext context = contextFactory.base(window, new TestTimers(windowNamespace(window)), executableTrigger, getFinishedSet(window));    executableTrigger.getSpec().prefetchShouldFire(context.state());    return executableTrigger.invokeShouldFire(context);}
public void beam_f3758_0(W window) throws Exception
{    TriggerStateMachine.TriggerContext context = contextFactory.base(window, new TestTimers(windowNamespace(window)), executableTrigger, getFinishedSet(window));    executableTrigger.getSpec().prefetchShouldFire(context.state());    if (executableTrigger.invokeShouldFire(context)) {        executableTrigger.getSpec().prefetchOnFire(context.state());        executableTrigger.invokeOnFire(context);        if (context.trigger().isFinished()) {            activeWindows.remove(window);            executableTrigger.invokeClear(context);        }    }}
public BoundedWindow beam_f3767_0()
{    return window;}
public void beam_f3768_0(Instant timestamp, TimeDomain timeDomain)
{    timerInternals.setTimer(TimerData.of(namespace, timestamp, timeDomain));}
public void beam_f3769_0(Instant timestamp, TimeDomain timeDomain)
{    timerInternals.deleteTimer(TimerData.of(namespace, timestamp, timeDomain));}
public static Matcher<WindowedValue<? extends T>> beam_f3777_0(Matcher<? super T> valueMatcher)
{    return new WindowedValueMatcher<>(valueMatcher, Matchers.anything(), Matchers.anything(), Matchers.anything());}
public static Matcher<WindowedValue<? extends T>> beam_f3778_0(T value, long timestamp, long windowStart, long windowEnd)
{    return WindowMatchers.isSingleWindowedValue(Matchers.equalTo(value), timestamp, windowStart, windowEnd);}
public static Matcher<WindowedValue<? extends T>> beam_f3779_0(T value, Instant timestamp, BoundedWindow window, PaneInfo paneInfo)
{    return WindowMatchers.isSingleWindowedValue(Matchers.equalTo(value), Matchers.equalTo(timestamp), Matchers.equalTo(window), Matchers.equalTo(paneInfo));}
public void beam_f3787_0(Description description)
{    description.appendText("WindowedValue(paneInfo = ").appendValue(paneInfo).appendText(")");}
protected boolean beam_f3788_0(WindowedValue<? extends T> item)
{    return Objects.equals(item.getPane(), paneInfo);}
protected void beam_f3789_0(WindowedValue<? extends T> item, Description mismatchDescription)
{    mismatchDescription.appendValue(item.getPane());}
public void beam_f3800_0()
{    executor.shutdown();}
public void beam_f3801_0(WindowedValue<BoundedSourceShard<OutputT>> element) throws Exception
{    BoundedSource<OutputT> source = element.getValue().getSource();    try (final BoundedReader<OutputT> reader = source.createReader(options)) {        boolean contentsRemaining = reader.start();        Future<BoundedSource<OutputT>> residualFuture = startDynamicSplitThread(source, reader);        UncommittedBundle<OutputT> output = evaluationContext.createBundle(outputPCollection);        while (contentsRemaining) {            output.add(WindowedValue.timestampedValueInGlobalWindow(reader.getCurrent(), reader.getCurrentTimestamp()));            contentsRemaining = reader.advance();        }        resultBuilder.addOutput(output);        try {            BoundedSource<OutputT> residual = residualFuture.get();            if (residual != null) {                resultBuilder.addUnprocessedElements(element.withValue(BoundedSourceShard.of(residual)));            }        } catch (ExecutionException exex) {                        throw UserCodeException.wrap(exex.getCause());        }    }}
private Future<BoundedSource<OutputT>> beam_f3802_0(BoundedSource<OutputT> source, BoundedReader<OutputT> reader) throws Exception
{    if (source.getEstimatedSizeBytes(options) > minimumDynamicSplitSize) {        return produceSplitExecutor.submit(new GenerateSplitAtHalfwayPoint<>(reader));    } else {        SettableFuture<BoundedSource<OutputT>> emptyFuture = SettableFuture.create();        emptyFuture.set(null);        return emptyFuture;    }}
public UncommittedBundle<T> beam_f3810_0(StructuralKey<K> key, PCollection<T> output)
{    return new CloningBundle<>(underlying.createKeyedBundle(key, output));}
public PCollection<T> beam_f3811_0()
{    return underlying.getPCollection();}
public UncommittedBundle<T> beam_f3812_0(WindowedValue<T> element)
{    try {                                WindowedValue<T> clone = element.withValue(CoderUtils.clone(coder, element.getValue()));        underlying.add(clone);    } catch (CoderException e) {        throw UserCodeException.wrap(e);    }    return this;}
public Object beam_f3820_0()
{    return key;}
public boolean beam_f3821_0()
{    return Iterables.isEmpty(table.values());}
private void beam_f3822_0()
{    Instant earliestHold = getEarliestWatermarkHold();    if (underlying.isPresent()) {        ReadThroughBinderFactory readThroughBinder = new ReadThroughBinderFactory<>(underlying.get());        binderFactory = readThroughBinder;        Instant earliestUnderlyingHold = readThroughBinder.readThroughAndGetEarliestHold(this);        if (earliestUnderlyingHold.isBefore(earliestHold)) {            earliestHold = earliestUnderlyingHold;        }    }    earliestWatermarkHold = Optional.of(earliestHold);    clearEmpty();    binderFactory = new InMemoryStateBinderFactory();    underlying = Optional.absent();}
public CombiningState<InputT, AccumT, OutputT> beam_f3830_0(StateTag<CombiningState<InputT, AccumT, OutputT>> address, Coder<AccumT> accumCoder, CombineFn<InputT, AccumT, OutputT> combineFn)
{    if (containedInUnderlying(namespace, address)) {        @SuppressWarnings("unchecked")        InMemoryState<? extends CombiningState<InputT, AccumT, OutputT>> existingState = (InMemoryState<? extends CombiningState<InputT, AccumT, OutputT>>) underlying.get().get(namespace, address, c);        return existingState.copy();    } else {        return new InMemoryCombiningState<>(combineFn, accumCoder);    }}
public BagState<T> beam_f3831_0(StateTag<BagState<T>> address, Coder<T> elemCoder)
{    if (containedInUnderlying(namespace, address)) {        @SuppressWarnings("unchecked")        InMemoryState<? extends BagState<T>> existingState = (InMemoryState<? extends BagState<T>>) underlying.get().get(namespace, address, c);        return existingState.copy();    } else {        return new InMemoryBag<>(elemCoder);    }}
public SetState<T> beam_f3832_0(StateTag<SetState<T>> address, Coder<T> elemCoder)
{    if (containedInUnderlying(namespace, address)) {        @SuppressWarnings("unchecked")        InMemoryState<? extends SetState<T>> existingState = (InMemoryState<? extends SetState<T>>) underlying.get().get(namespace, address, c);        return existingState.copy();    } else {        return new InMemorySet<>(elemCoder);    }}
public BagState<T> beam_f3840_0(StateTag<BagState<T>> address, Coder<T> elemCoder)
{    return underlying.get(namespace, address, c);}
public SetState<T> beam_f3841_0(StateTag<SetState<T>> address, Coder<T> elemCoder)
{    return underlying.get(namespace, address, c);}
public MapState<KeyT, ValueT> beam_f3842_0(StateTag<MapState<KeyT, ValueT>> address, Coder<KeyT> mapKeyCoder, Coder<ValueT> mapValueCoder)
{    return underlying.get(namespace, address, c);}
public TimerUpdate beam_f3850_0()
{    if (timerInternals == null) {        return TimerUpdate.empty();    }    return timerInternals.getTimerUpdate();}
public PTransformReplacement<PCollection<KV<KeyT, InputT>>, PCollection<KeyedWorkItem<KeyT, InputT>>> beam_f3851_0(AppliedPTransform<PCollection<KV<KeyT, InputT>>, PCollection<KeyedWorkItem<KeyT, InputT>>, GBKIntoKeyedWorkItems<KeyT, InputT>> transform)
{    return PTransformReplacement.of(PTransformReplacements.getSingletonMainInput(transform), new DirectGroupByKey.DirectGroupByKeyOnly<>());}
public static DirectGraph beam_f3852_0(Map<PCollection<?>, AppliedPTransform<?, ?, ?>> producers, Map<PCollectionView<?>, AppliedPTransform<?, ?, ?>> viewWriters, ListMultimap<PInput, AppliedPTransform<?, ?, ?>> perElementConsumers, Set<AppliedPTransform<?, ?, ?>> rootTransforms, Map<AppliedPTransform<?, ?, ?>, String> stepNames)
{    return new DirectGraph(producers, viewWriters, perElementConsumers, rootTransforms, stepNames);}
 Set<PCollectionView<?>> beam_f3860_0()
{    return viewWriters.keySet();}
 String beam_f3861_0(AppliedPTransform<?, ?, ?> step)
{    return stepNames.get(step);}
public CompositeBehavior beam_f3862_0(TransformHierarchy.Node node)
{    checkState(!finalized, "Attempting to traverse a pipeline (node %s) with a %s " + "which has already visited a Pipeline and is finalized", node.getFullName(), getClass().getSimpleName());    return CompositeBehavior.ENTER_TRANSFORM;}
public PCollection<KV<K, Iterable<V>>> beam_f3870_0(PCollection<KV<K, V>> input)
{                    WindowingStrategy<?, ?> inputWindowingStrategy = input.getWindowingStrategy();        return input.apply(new DirectGroupByKeyOnly<>()).apply("GroupAlsoByWindow", new DirectGroupAlsoByWindow<>(inputWindowingStrategy, outputWindowingStrategy));}
public PCollection<KeyedWorkItem<K, V>> beam_f3871_0(PCollection<KV<K, V>> input)
{    return PCollection.createPrimitiveOutputInternal(input.getPipeline(), WindowingStrategy.globalDefault(), input.isBounded(), KeyedWorkItemCoder.of(GroupByKey.getKeyCoder(input.getCoder()), GroupByKey.getInputValueCoder(input.getCoder()), input.getWindowingStrategy().getWindowFn().windowCoder()));}
public WindowingStrategy<?, ?> beam_f3872_0()
{    return inputWindowingStrategy;}
public void beam_f3880_0(final CommittedBundle<?> bundle, final UpdateT finalCumulative)
{    UpdateT current;    do {        current = finishedCommitted.get();    } while (!finishedCommitted.compareAndSet(current, aggregation.combine(asList(current, finalCumulative))));}
public ResultT beam_f3881_0()
{    return aggregation.extract(finishedCommitted.get());}
public Long beam_f3882_0()
{    return 0L;}
public GaugeResult beam_f3890_0(GaugeData data)
{    return data.extractResult();}
public MetricQueryResults beam_f3891_0(@Nullable MetricsFilter filter)
{    ImmutableList.Builder<MetricResult<Long>> counterResults = ImmutableList.builder();    for (Entry<MetricKey, DirectMetric<Long, Long>> counter : counters.entries()) {        maybeExtractResult(filter, counterResults, counter);    }    ImmutableList.Builder<MetricResult<DistributionResult>> distributionResults = ImmutableList.builder();    for (Entry<MetricKey, DirectMetric<DistributionData, DistributionResult>> distribution : distributions.entries()) {        maybeExtractResult(filter, distributionResults, distribution);    }    ImmutableList.Builder<MetricResult<GaugeResult>> gaugeResults = ImmutableList.builder();    for (Entry<MetricKey, DirectMetric<GaugeData, GaugeResult>> gauge : gauges.entries()) {        maybeExtractResult(filter, gaugeResults, gauge);    }    return MetricQueryResults.create(counterResults.build(), distributionResults.build(), gaugeResults.build());}
private void beam_f3892_0(MetricsFilter filter, ImmutableList.Builder<MetricResult<ResultT>> resultsBuilder, Map.Entry<MetricKey, ? extends DirectMetric<?, ResultT>> entry)
{    if (MetricFiltering.matches(filter, entry.getKey())) {        resultsBuilder.add(MetricResult.create(entry.getKey(), entry.getValue().extractCommitted(), entry.getValue().extractLatestAttempted()));    }}
 static BundleFactory beam_f3900_0(Set<Enforcement> enforcements, DirectGraph graph)
{    BundleFactory bundleFactory = enforcements.contains(Enforcement.ENCODABILITY) ? CloningBundleFactory.create() : ImmutableListBundleFactory.create();    if (enforcements.contains(Enforcement.IMMUTABILITY)) {        bundleFactory = ImmutabilityCheckingBundleFactory.create(bundleFactory, graph);    }    return bundleFactory;}
private static Map<String, Collection<ModelEnforcementFactory>> beam_f3901_0(Set<Enforcement> enabledEnforcements)
{    ImmutableMap.Builder<String, Collection<ModelEnforcementFactory>> enforcements = ImmutableMap.builder();    ImmutableList.Builder<ModelEnforcementFactory> enabledParDoEnforcements = ImmutableList.builder();    if (enabledEnforcements.contains(Enforcement.IMMUTABILITY)) {        enabledParDoEnforcements.add(ImmutabilityEnforcementFactory.create());    }    Collection<ModelEnforcementFactory> parDoEnforcements = enabledParDoEnforcements.build();    enforcements.put(PTransformTranslation.PAR_DO_TRANSFORM_URN, parDoEnforcements);    return enforcements.build();}
public boolean beam_f3902_0(PCollection<?> collection, DirectGraph graph)
{    return true;}
public MetricResults beam_f3910_0()
{    return evaluationContext.getMetrics();}
public State beam_f3911_0()
{    return waitUntilFinish(Duration.ZERO);}
public State beam_f3912_0()
{    this.state = executor.getPipelineState();    if (!this.state.isTerminal()) {        executor.stop();        this.state = executor.getPipelineState();    }    return executor.getPipelineState();}
public void beam_f3920_0(TimerData timerKey)
{    timerUpdateBuilder.deletedTimer(timerKey);}
public TimerUpdate beam_f3921_0()
{    return timerUpdateBuilder.build();}
public Instant beam_f3922_0()
{    return processingTimeClock.now();}
 static void beam_f3930_0(Pipeline pipeline)
{    validateTransforms(pipeline);}
 static void beam_f3931_0(PipelineOptions options)
{    evaluateDisplayData(options);}
private static void beam_f3932_0(Pipeline pipeline)
{    pipeline.traverseTopologically(Visitor.INSTANCE);}
public DoFn<?, ?> beam_f3940_0(Thread key) throws Exception
{    DoFn<?, ?> fn = (DoFn<?, ?>) SerializableUtils.deserializeFromByteArray(original, "DoFn Copy in thread " + key.getName());    DoFnInvokers.tryInvokeSetupFor(fn);    return fn;}
public void beam_f3941_0(RemovalNotification<Thread, DoFn<?, ?>> notification)
{    try {        DoFnInvokers.invokerFor(notification.getValue()).invokeTeardown();    } catch (Exception e) {        thrownOnTeardown.put(notification.getKey(), e);    }}
public static DoFnLifecycleManagerRemovingTransformEvaluator<InputT> beam_f3942_0(ParDoEvaluator<InputT> underlying, DoFnLifecycleManager lifecycleManager)
{    return new DoFnLifecycleManagerRemovingTransformEvaluator<>(underlying, lifecycleManager);}
public static EvaluationContext beam_f3950_0(Clock clock, BundleFactory bundleFactory, DirectGraph graph, Set<PValue> keyedPValues, ExecutorService executorService)
{    return new EvaluationContext(clock, bundleFactory, graph, keyedPValues, executorService);}
public void beam_f3951_0(Map<AppliedPTransform<?, ?, ?>, ? extends Iterable<CommittedBundle<?>>> initialInputs)
{    watermarkManager.initialize((Map) initialInputs);}
public CommittedResult<AppliedPTransform<?, ?, ?>> beam_f3952_0(CommittedBundle<?> completedBundle, Iterable<TimerData> completedTimers, TransformResult<?> result)
{    Iterable<? extends CommittedBundle<?>> committedBundles = commitBundles(result.getOutputBundles());    metrics.commitLogical(completedBundle, result.getLogicalMetricUpdates());        EnumSet<OutputType> outputTypes = EnumSet.copyOf(result.getOutputTypes());    if (Iterables.isEmpty(committedBundles)) {        outputTypes.remove(OutputType.BUNDLE);    } else {        outputTypes.add(OutputType.BUNDLE);    }    CommittedResult<AppliedPTransform<?, ?, ?>> committedResult = CommittedResult.create(result, getUnprocessedInput(completedBundle, result), committedBundles, outputTypes);        CopyOnAccessInMemoryStateInternals theirState = result.getState();    if (theirState != null) {        CopyOnAccessInMemoryStateInternals committedState = theirState.commit();        StepAndKey stepAndKey = StepAndKey.of(result.getTransform(), completedBundle.getKey());        if (!committedState.isEmpty()) {            applicationStateInternals.put(stepAndKey, committedState);        } else {            applicationStateInternals.remove(stepAndKey);        }    }            watermarkManager.updateWatermarks(completedBundle, result.getTimerUpdate().withCompletedTimers(completedTimers), committedResult.getExecutable(), committedResult.getUnprocessedInputs().orNull(), committedResult.getOutputs(), result.getWatermarkHold());    return committedResult;}
public boolean beam_f3960_0(PValue pValue)
{    return keyedPValues.contains(pValue);}
public PCollectionViewWriter<ElemT, ViewT> beam_f3961_0(PCollection<Iterable<ElemT>> input, final PCollectionView<ViewT> output)
{    return values -> sideInputContainer.write(output, values);}
public void beam_f3962_0(PCollection<?> value, BoundedWindow window, WindowingStrategy<?, ?> windowingStrategy, Runnable runnable)
{    AppliedPTransform<?, ?, ?> producing = graph.getProducer(value);    callbackExecutor.callOnGuaranteedFiring(producing, window, windowingStrategy, runnable);    fireAvailableCallbacks(producing);}
 void beam_f3970_0()
{    watermarkManager.refreshAll();    fireAllAvailableCallbacks();}
public Collection<FiredTimers<AppliedPTransform<?, ?, ?>>> beam_f3971_0()
{    forceRefresh();    return watermarkManager.extractFiredTimers();}
public boolean beam_f3972_0(AppliedPTransform<?, ?, ?> transform)
{        Instant stepWatermark = watermarkManager.getWatermarks(transform).getOutputWatermark();    return !stepWatermark.isBefore(BoundedWindow.TIMESTAMP_MAX_VALUE);}
public void beam_f3980_0(DirectGraph graph, RootProviderRegistry rootProviderRegistry)
{    int numTargetSplits = Math.max(3, targetParallelism);    ImmutableMap.Builder<AppliedPTransform<?, ?, ?>, ConcurrentLinkedQueue<CommittedBundle<?>>> pendingRootBundles = ImmutableMap.builder();    for (AppliedPTransform<?, ?, ?> root : graph.getRootTransforms()) {        ConcurrentLinkedQueue<CommittedBundle<?>> pending = new ConcurrentLinkedQueue<>();        try {            Collection<CommittedBundle<?>> initialInputs = rootProviderRegistry.getInitialInputs(root, numTargetSplits);            pending.addAll(initialInputs);        } catch (Exception e) {            throw UserCodeException.wrap(e);        }        pendingRootBundles.put(root, pending);    }    evaluationContext.initialize(pendingRootBundles.build());    final ExecutionDriver executionDriver = QuiescenceDriver.create(evaluationContext, graph, this, visibleUpdates, pendingRootBundles.build());    executorService.submit(new Runnable() {        @Override        public void run() {            DriverState drive = executionDriver.drive();            if (drive.isTermainal()) {                State newPipelineState = State.UNKNOWN;                switch(drive) {                    case FAILED:                        newPipelineState = State.FAILED;                        break;                    case SHUTDOWN:                        newPipelineState = State.DONE;                        break;                    case CONTINUE:                        throw new IllegalStateException(String.format("%s should not be a terminal state", DriverState.CONTINUE));                    default:                        throw new IllegalArgumentException(String.format("Unknown %s %s", DriverState.class.getSimpleName(), drive));                }                shutdownIfNecessary(newPipelineState);            } else {                executorService.submit(this);            }        }    });}
public void beam_f3981_0()
{    DriverState drive = executionDriver.drive();    if (drive.isTermainal()) {        State newPipelineState = State.UNKNOWN;        switch(drive) {            case FAILED:                newPipelineState = State.FAILED;                break;            case SHUTDOWN:                newPipelineState = State.DONE;                break;            case CONTINUE:                throw new IllegalStateException(String.format("%s should not be a terminal state", DriverState.CONTINUE));            default:                throw new IllegalArgumentException(String.format("Unknown %s %s", DriverState.class.getSimpleName(), drive));        }        shutdownIfNecessary(newPipelineState);    } else {        executorService.submit(this);    }}
public void beam_f3982_0(CommittedBundle<?> bundle, AppliedPTransform<?, ?, ?> consumer, CompletionCallback onComplete)
{    evaluateBundle(consumer, bundle, onComplete);}
public static VisibleExecutorUpdate beam_f3990_0(Exception e)
{    return new VisibleExecutorUpdate(null, e);}
public static VisibleExecutorUpdate beam_f3991_0(Error err)
{    return new VisibleExecutorUpdate(State.FAILED, err);}
public static VisibleExecutorUpdate beam_f3992_0()
{    return new VisibleExecutorUpdate(State.DONE, null);}
public TransformEvaluator<InputT> beam_f4000_0(AppliedPTransform<?, ?, ?> application, CommittedBundle<?> inputBundle)
{    @SuppressWarnings({ "cast", "unchecked", "rawtypes" })    TransformEvaluator<InputT> evaluator = (TransformEvaluator<InputT>) createInMemoryEvaluator((AppliedPTransform) application);    return evaluator;}
private TransformEvaluator<InputT> beam_f4002_0(final AppliedPTransform<PCollectionList<InputT>, PCollection<InputT>, PCollections<InputT>> application)
{    final UncommittedBundle<InputT> outputBundle = evaluationContext.createBundle((PCollection<InputT>) Iterables.getOnlyElement(application.getOutputs().values()));    final TransformResult<InputT> result = StepTransformResult.<InputT>withoutHold(application).addOutput(outputBundle).build();    return new FlattenEvaluator<>(outputBundle, result);}
public void beam_f4003_0(WindowedValue<InputT> element)
{    outputBundle.add(element);}
public void beam_f4012_0(TupleTag<AdditionalOutputT> tag, AdditionalOutputT output, Instant timestamp, Collection<? extends BoundedWindow> windows, PaneInfo pane)
{    throw new UnsupportedOperationException(String.format("%s should not use tagged outputs", DirectGroupAlsoByWindow.class.getSimpleName()));}
public TransformEvaluator<InputT> beam_f4013_0(AppliedPTransform<?, ?, ?> application, CommittedBundle<?> inputBundle)
{    @SuppressWarnings({ "cast", "unchecked", "rawtypes" })    TransformEvaluator<InputT> evaluator = createEvaluator((AppliedPTransform) application);    return evaluator;}
private TransformEvaluator<KV<K, V>> beam_f4015_0(final AppliedPTransform<PCollection<KV<K, V>>, PCollection<KeyedWorkItem<K, V>>, DirectGroupByKeyOnly<K, V>> application)
{    return new GroupByKeyOnlyEvaluator<>(evaluationContext, application);}
public PCollection<T> beam_f4023_0()
{    return underlying.getPCollection();}
public UncommittedBundle<T> beam_f4024_0(WindowedValue<T> element)
{    try {        mutationDetectors.put(element, MutationDetectors.forValueWithCoder(element.getValue(), coder));    } catch (CoderException e) {        throw new RuntimeException(e);    }    underlying.add(element);    return this;}
public CommittedBundle<T> beam_f4025_0(Instant synchronizedProcessingTime)
{    for (MutationDetector detector : mutationDetectors.values()) {        try {            detector.verifyUnmodified();        } catch (IllegalMutationException exn) {            throw new IllegalMutationException(String.format("PTransform %s mutated value %s after it was output (new value was %s)." + " Values must not be mutated in any way after being output.", graph.getProducer(underlying.getPCollection()).getFullName(), exn.getSavedValue(), exn.getNewValue()), exn.getSavedValue(), exn.getNewValue(), exn);        }    }    return underlying.commit(synchronizedProcessingTime);}
public UncommittedBundle<T> beam_f4033_0()
{    return UncommittedImmutableListBundle.create(null, StructuralKey.empty());}
public UncommittedBundle<T> beam_f4034_0(PCollection<T> output)
{    return UncommittedImmutableListBundle.create(output, StructuralKey.empty());}
public UncommittedBundle<T> beam_f4035_0(StructuralKey<K> key, PCollection<T> output)
{    return UncommittedImmutableListBundle.create(output, key);}
public CommittedBundle<T> beam_f4043_0(Iterable<WindowedValue<T>> elements)
{    return create(getPCollection(), getKey(), ImmutableList.copyOf(elements), minTimestamp(elements), getSynchronizedProcessingOutputWatermark());}
public int beam_f4044_0()
{    return System.identityHashCode(this);}
public boolean beam_f4045_0(Object obj)
{    return this == obj;}
public void beam_f4054_0(TransformHierarchy.Node node)
{    checkState(!finalized, "Attempted to use a %s that has already been finalized on a pipeline (visiting node %s)", KeyedPValueTrackingVisitor.class.getSimpleName(), node);    if (node.isRootNode()) {        finalized = true;    } else if (PRODUCES_KEYED_OUTPUTS.contains(node.getTransform().getClass())) {        Map<TupleTag<?>, PValue> outputs = node.getOutputs();        for (PValue output : outputs.values()) {            keyedValues.add(output);        }    }}
public void beam_f4055_0(PValue value, TransformHierarchy.Node producer)
{    boolean inputsAreKeyed = true;    for (PValue input : producer.getInputs().values()) {        inputsAreKeyed = inputsAreKeyed && keyedValues.contains(input);    }    if (PRODUCES_KEYED_OUTPUTS.contains(producer.getTransform().getClass()) || (isKeyPreserving(producer.getTransform()) && inputsAreKeyed)) {        keyedValues.add(value);    }}
public Set<PValue> beam_f4056_0()
{    checkState(finalized, "can't call getKeyedPValues before a Pipeline has been completely traversed");    return keyedValues;}
public String beam_f4064_0()
{    return "beam:directrunner:transforms:multistepcombine:v1";}
public RunnerApi.FunctionSpec beam_f4065_0()
{    return null;}
public PCollection<KV<K, OutputT>> beam_f4066_0(PCollection<KV<K, InputT>> input)
{    checkArgument(input.getCoder() instanceof KvCoder, "Expected input to have a %s of type %s, got %s", Coder.class.getSimpleName(), KvCoder.class.getSimpleName(), input.getCoder());    KvCoder<K, InputT> inputCoder = (KvCoder<K, InputT>) input.getCoder();    Coder<InputT> inputValueCoder = inputCoder.getValueCoder();    Coder<AccumT> accumulatorCoder;    try {        accumulatorCoder = combineFn.getAccumulatorCoder(input.getPipeline().getCoderRegistry(), inputValueCoder);    } catch (CannotProvideCoderException e) {        throw new IllegalStateException(String.format("Could not construct an Accumulator Coder with the provided %s %s", CombineFn.class.getSimpleName(), combineFn), e);    }    return input.apply(ParDo.of(new CombineInputs<>(combineFn, input.getWindowingStrategy().getTimestampCombiner(), inputCoder.getKeyCoder()))).setCoder(KvCoder.of(inputCoder.getKeyCoder(), accumulatorCoder)).apply(GroupByKey.create()).apply(new MergeAndExtractAccumulatorOutput<>(combineFn, outputCoder));}
public int beam_f4074_0()
{    return Objects.hash(window, key);}
 CombineFn<?, AccumT, OutputT> beam_f4075_0()
{    return combineFn;}
public PCollection<KV<K, OutputT>> beam_f4076_0(PCollection<KV<K, Iterable<AccumT>>> input)
{    return PCollection.createPrimitiveOutputInternal(input.getPipeline(), input.getWindowingStrategy(), input.isBounded(), outputCoder);}
public Instant beam_f4085_0()
{    return new Instant(baseMillis + (TimeUnit.MILLISECONDS.convert(System.nanoTime() - nanosAtBaseMillis, TimeUnit.NANOSECONDS)));}
public static DoFnRunnerFactory<InputT, OutputT> beam_f4086_0()
{    return (options, fn, sideInputs, sideInputReader, outputManager, mainOutputTag, additionalOutputTags, stepContext, schemaCoder, outputCoders, windowingStrategy, doFnSchemaInformation, sideInputMapping) -> {        DoFnRunner<InputT, OutputT> underlying = DoFnRunners.simpleRunner(options, fn, sideInputReader, outputManager, mainOutputTag, additionalOutputTags, stepContext, schemaCoder, outputCoders, windowingStrategy, doFnSchemaInformation, sideInputMapping);        return SimplePushbackSideInputDoFnRunner.create(underlying, sideInputs, sideInputReader);    };}
public static ParDoEvaluator<InputT> beam_f4087_0(EvaluationContext evaluationContext, PipelineOptions options, DirectStepContext stepContext, AppliedPTransform<?, ?, ?> application, Coder<InputT> inputCoder, WindowingStrategy<?, ? extends BoundedWindow> windowingStrategy, DoFn<InputT, OutputT> fn, StructuralKey<?> key, List<PCollectionView<?>> sideInputs, TupleTag<OutputT> mainOutputTag, List<TupleTag<?>> additionalOutputTags, Map<TupleTag<?>, PCollection<?>> outputs, DoFnSchemaInformation doFnSchemaInformation, Map<String, PCollectionView<?>> sideInputMapping, DoFnRunnerFactory<InputT, OutputT> runnerFactory)
{    BundleOutputManager outputManager = createOutputManager(evaluationContext, key, outputs);    ReadyCheckingSideInputReader sideInputReader = evaluationContext.createSideInputReader(sideInputs);    Map<TupleTag<?>, Coder<?>> outputCoders = outputs.entrySet().stream().collect(Collectors.toMap(e -> e.getKey(), e -> e.getValue().getCoder()));    PushbackSideInputDoFnRunner<InputT, OutputT> runner = runnerFactory.createRunner(options, fn, sideInputs, sideInputReader, outputManager, mainOutputTag, additionalOutputTags, stepContext, inputCoder, outputCoders, windowingStrategy, doFnSchemaInformation, sideInputMapping);    return create(runner, stepContext, application, outputManager);}
public TransformResult<InputT> beam_f4095_0()
{    try {        fnRunner.finishBundle();    } catch (Exception e) {        throw UserCodeException.wrap(e);    }    StepTransformResult.Builder<InputT> resultBuilder;    CopyOnAccessInMemoryStateInternals state = stepContext.commitState();    if (state != null) {        resultBuilder = StepTransformResult.<InputT>withHold(transform, state.getEarliestWatermarkHold()).withState(state);    } else {        resultBuilder = StepTransformResult.withoutHold(transform);    }    return resultBuilder.addOutput(outputManager.bundles.values()).withTimerUpdate(stepContext.getTimerUpdate()).addUnprocessedElements(unprocessedElements.build()).build();}
public static BundleOutputManager beam_f4096_0(Map<TupleTag<?>, UncommittedBundle<?>> outputBundles)
{    return new BundleOutputManager(outputBundles);}
public void beam_f4097_0(TupleTag<T> tag, WindowedValue<T> output)
{    checkArgument(bundles.containsKey(tag), "Unknown output tag %s", tag);    ((UncommittedBundle) bundles.get(tag)).add(output);}
public PTransformReplacement<PCollection<? extends InputT>, PCollectionTuple> beam_f4105_0(AppliedPTransform<PCollection<? extends InputT>, PCollectionTuple, PTransform<PCollection<? extends InputT>, PCollectionTuple>> application)
{    try {        return PTransformReplacement.of(PTransformReplacements.getSingletonMainInput(application), getReplacementForApplication(application));    } catch (IOException exc) {        throw new RuntimeException(exc);    }}
private PTransform<PCollection<? extends InputT>, PCollectionTuple> beam_f4106_0(AppliedPTransform<PCollection<? extends InputT>, PCollectionTuple, PTransform<PCollection<? extends InputT>, PCollectionTuple>> application) throws IOException
{    DoFn<InputT, OutputT> fn = (DoFn<InputT, OutputT>) ParDoTranslation.getDoFn(application);    DoFnSignature signature = DoFnSignatures.getSignature(fn.getClass());    if (signature.processElement().isSplittable()) {        return SplittableParDo.forAppliedParDo((AppliedPTransform) application);    } else if (signature.stateDeclarations().size() > 0 || signature.timerDeclarations().size() > 0) {        return new GbkThenStatefulParDo(fn, ParDoTranslation.getMainOutputTag(application), ParDoTranslation.getAdditionalOutputTags(application), ParDoTranslation.getSideInputs(application), ParDoTranslation.getSchemaInformation(application), ParDoTranslation.getSideInputMapping(application));    } else {        return application.getTransform();    }}
public Map<PValue, ReplacementOutput> beam_f4107_0(Map<TupleTag<?>, PValue> outputs, PCollectionTuple newOutput)
{    return ReplacementOutputs.tagged(outputs, newOutput);}
public Map<String, PCollectionView<?>> beam_f4115_0()
{    return sideInputMapping;}
public Map<TupleTag<?>, PValue> beam_f4116_0()
{    return PCollectionViews.toAdditionalInputs(sideInputs);}
public PCollectionTuple beam_f4117_0(PCollection<? extends KeyedWorkItem<K, KV<K, InputT>>> input)
{    return PCollectionTuple.ofPrimitiveOutputsInternal(input.getPipeline(), TupleTagList.of(getMainOutputTag()).and(getAdditionalOutputTags().getAll()),     Collections.emptyMap(), input.getWindowingStrategy(), input.isBounded());}
public BoundedWindow beam_f4125_0()
{    return window;}
public boolean beam_f4126_0(Object otherObject)
{    if (!(otherObject instanceof PCollectionViewWindow)) {        return false;    }    @SuppressWarnings("unchecked")    PCollectionViewWindow<T> other = (PCollectionViewWindow<T>) otherObject;    return getView().equals(other.getView()) && getWindow().equals(other.getWindow());}
public int beam_f4127_0()
{    return Objects.hash(getView(), getWindow());}
public final void beam_f4135_0(CommittedBundle<?> inputBundle, Exception e)
{    pendingWork.offer(WorkUpdate.fromException(e));    outstandingWork.decrementAndGet();}
public void beam_f4136_0(Error err)
{    pipelineMessageReceiver.failed(err);}
private static WorkUpdate beam_f4137_0(CommittedBundle<?> bundle, Collection<AppliedPTransform<?, ?, ?>> consumers)
{    return new AutoValue_QuiescenceDriver_WorkUpdate(Optional.of(bundle), consumers, Optional.absent());}
public Collection<CommittedBundle<?>> beam_f4145_0(AppliedPTransform<?, ?, ?> transform, int targetParallelism) throws Exception
{    String transformUrn = PTransformTranslation.urnForTransform(transform.getTransform());    RootInputProvider provider = checkNotNull(providers.get(transformUrn), "Tried to get a %s for a transform \"%s\", but there is no such provider", RootInputProvider.class.getSimpleName(), transformUrn);    return provider.getInitialInputs(transform, targetParallelism);}
public static SideInputContainer beam_f4146_0(final EvaluationContext context, Collection<PCollectionView<?>> containedViews)
{    for (PCollectionView<?> pCollectionView : containedViews) {        checkArgument(Materializations.MULTIMAP_MATERIALIZATION_URN.equals(pCollectionView.getViewFn().getMaterialization().getUrn()), "This handler is only capable of dealing with %s materializations " + "but was asked to handle %s for PCollectionView with tag %s.", Materializations.MULTIMAP_MATERIALIZATION_URN, pCollectionView.getViewFn().getMaterialization().getUrn(), pCollectionView.getTagInternal().getId());    }    LoadingCache<PCollectionViewWindow<?>, AtomicReference<Iterable<? extends WindowedValue<?>>>> viewByWindows = CacheBuilder.newBuilder().build(new CallbackSchedulingLoader(context));    return new SideInputContainer(containedViews, viewByWindows);}
public ReadyCheckingSideInputReader beam_f4147_0(Collection<PCollectionView<?>> newContainedViews)
{    if (!containedViews.containsAll(newContainedViews)) {        Set<PCollectionView<?>> currentlyContained = ImmutableSet.copyOf(containedViews);        Set<PCollectionView<?>> newRequested = ImmutableSet.copyOf(newContainedViews);        throw new IllegalArgumentException("Can't create a SideInputReader with unknown views " + Sets.difference(newRequested, currentlyContained));    }    return new SideInputContainerSideInputReader(newContainedViews);}
public T beam_f4155_0(final PCollectionView<T> view, final BoundedWindow window)
{    checkArgument(readerViews.contains(view), "call to get(PCollectionView) with unknown view: %s", view);    checkArgument(isReady(view, window), "calling get() on PCollectionView %s that is not ready in window %s", view, window);        @SuppressWarnings("unchecked")    Iterable<KV<?, ?>> elements = Iterables.transform((Iterable<WindowedValue<KV<?, ?>>>) viewContents.getUnchecked(PCollectionViewWindow.of(view, window)).get(), WindowedValue::getValue);    ViewFn<MultimapView, T> viewFn = (ViewFn<MultimapView, T>) view.getViewFn();    Coder<?> keyCoder = ((KvCoder<?, ?>) view.getCoderInternal()).getKeyCoder();    return (T) viewFn.apply(InMemoryMultimapSideInputView.fromIterable(keyCoder, (Iterable) elements));}
public boolean beam_f4156_0(PCollectionView<T> view)
{    return readerViews.contains(view);}
public boolean beam_f4157_0()
{    return readerViews.isEmpty();}
private static ParDoEvaluator.DoFnRunnerFactory<KeyedWorkItem<byte[], KV<InputT, RestrictionT>>, OutputT> beam_f4165_0()
{    return (options, fn, sideInputs, sideInputReader, outputManager, mainOutputTag, additionalOutputTags, stepContext, inputCoder, outputCoders, windowingStrategy, doFnSchemaInformation, sideInputMapping) -> {        ProcessFn<InputT, OutputT, RestrictionT, ?> processFn = (ProcessFn) fn;        return DoFnRunners.newProcessFnRunner(processFn, options, sideInputs, sideInputReader, outputManager, mainOutputTag, additionalOutputTags, stepContext, inputCoder, outputCoders, windowingStrategy, doFnSchemaInformation, sideInputMapping);    };}
public DoFnLifecycleManager beam_f4166_0(AppliedPTransform<?, ?, ?> appliedStatefulParDo) throws Exception
{            StatefulParDo<?, ?, ?> statefulParDo = (StatefulParDo<?, ?, ?>) appliedStatefulParDo.getTransform();    return DoFnLifecycleManager.of(statefulParDo.getDoFn());}
public TransformEvaluator<T> beam_f4167_0(AppliedPTransform<?, ?, ?> application, CommittedBundle<?> inputBundle) throws Exception
{    @SuppressWarnings({ "unchecked", "rawtypes" })    TransformEvaluator<T> evaluator = (TransformEvaluator<T>) createEvaluator((AppliedPTransform) application, (CommittedBundle) inputBundle);    return evaluator;}
public String beam_f4175_0()
{    return MoreObjects.toStringHelper(StepAndKey.class).add("step", step.getFullName()).add("key", key.getKey()).toString();}
public int beam_f4176_0()
{    return Objects.hash(step, key);}
public boolean beam_f4177_0(Object other)
{    if (other == this) {        return true;    } else if (!(other instanceof StepAndKey)) {        return false;    } else {        StepAndKey that = (StepAndKey) other;        return Objects.equals(this.step, that.step) && Objects.equals(this.key, that.key);    }}
public Builder<InputT> beam_f4185_0(WindowedValue<InputT>... unprocessed)
{    unprocessedElementsBuilder.addAll(Arrays.asList(unprocessed));    return this;}
public Builder<InputT> beam_f4186_0(Iterable<? extends WindowedValue<InputT>> unprocessed)
{    unprocessedElementsBuilder.addAll(unprocessed);    return this;}
public Builder<InputT> beam_f4187_0(UncommittedBundle<?> outputBundle, UncommittedBundle<?>... outputBundles)
{    bundlesBuilder.add(outputBundle);    bundlesBuilder.add(outputBundles);    return this;}
public Instant beam_f4196_0()
{    return currentTime.get();}
public Clock beam_f4197_0()
{    return new TestClock();}
public PTransformReplacement<PBegin, PCollection<T>> beam_f4198_0(AppliedPTransform<PBegin, PCollection<T>, PTransform<PBegin, PCollection<T>>> transform)
{    try {        return PTransformReplacement.of(transform.getPipeline().begin(), new DirectTestStream<>(runner, TestStreamTranslation.getTestStream(transform)));    } catch (IOException exc) {        throw new RuntimeException(String.format("Transform could not be converted to %s", TestStream.class.getSimpleName()), exc);    }}
public String beam_f4206_0(ProcessElements<?, ?, ?, ?> transform)
{    return SPLITTABLE_PROCESS_URN;}
public TransformEvaluator<InputT> beam_f4207_0(AppliedPTransform<?, ?, ?> application, CommittedBundle<?> inputBundle) throws Exception
{    checkState(!finished.get(), "Tried to get an evaluator for a finished TransformEvaluatorRegistry");    String urn = PTransformTranslation.urnForTransform(application.getTransform());    TransformEvaluatorFactory factory = checkNotNull(factories.get(urn), "No evaluator for PTransform \"%s\"", urn);    return factory.forApplication(application, inputBundle);}
public void beam_f4208_1() throws Exception
{    Collection<Exception> thrownInCleanup = new ArrayList<>();    for (TransformEvaluatorFactory factory : factories.values()) {        try {            factory.cleanup();        } catch (Exception e) {            if (e instanceof InterruptedException) {                Thread.currentThread().interrupt();            }            thrownInCleanup.add(e);        }    }    finished.set(true);    if (!thrownInCleanup.isEmpty()) {                Exception toThrow = null;        for (Exception e : thrownInCleanup) {            if (toThrow == null) {                toThrow = e;            } else {                toThrow.addSuppressed(e);            }        }        throw toThrow;    }}
private void beam_f4217_0()
{    if (currentlyEvaluating.get() == null) {                synchronized (this) {            TransformExecutor newWork = workQueue.poll();            if (active && newWork != null) {                if (currentlyEvaluating.compareAndSet(null, newWork)) {                    executor.submit(newWork);                } else {                    workQueue.offer(newWork);                }            }        }    }}
public String beam_f4218_0()
{    return MoreObjects.toStringHelper(SerialTransformExecutor.class).add("currentlyEvaluating", currentlyEvaluating).add("workQueue", workQueue).toString();}
public static UnboundedReadDeduplicator beam_f4219_0()
{    return new NeverDeduplicator();}
private UnboundedReader<OutputT> beam_f4228_0(UnboundedSourceShard<OutputT, CheckpointMarkT> shard) throws IOException
{    UnboundedReader<OutputT> existing = shard.getExistingReader();    if (existing == null) {        CheckpointMarkT checkpoint = shard.getCheckpoint();        if (checkpoint != null) {            checkpoint = CoderUtils.clone(shard.getSource().getCheckpointMarkCoder(), checkpoint);        }        return shard.getSource().createReader(options, checkpoint);    } else {        return existing;    }}
private boolean beam_f4229_0(UnboundedReader<OutputT> reader, UnboundedSourceShard<OutputT, CheckpointMarkT> shard) throws IOException
{    if (shard.getExistingReader() == null) {        return reader.start();    } else {        return shard.getExistingReader().advance();    }}
private CheckpointMarkT beam_f4230_0(UnboundedReader<OutputT> reader, Instant watermark, UnboundedSourceShard<OutputT, CheckpointMarkT> shard) throws IOException
{    final CheckpointMark oldMark = shard.getCheckpoint();    @SuppressWarnings("unchecked")    final CheckpointMarkT mark = (CheckpointMarkT) reader.getCheckpointMark();    if (oldMark != null) {        oldMark.finalizeCheckpoint();    }        if (!watermark.isBefore(BoundedWindow.TIMESTAMP_MAX_VALUE)) {        PCollection<OutputT> outputPc = (PCollection<OutputT>) getOnlyElement(transform.getOutputs().values());        evaluationContext.scheduleAfterOutputWouldBeProduced(outputPc, GlobalWindow.INSTANCE, outputPc.getWindowingStrategy(), () -> {            try {                mark.finalizeCheckpoint();            } catch (IOException e) {                throw new RuntimeException("Couldn't finalize checkpoint after the end of the Global Window", e);            }        });    }    return mark;}
public TransformResult<Iterable<InT>> beam_f4239_0()
{    writer.add(elements);    Builder resultBuilder = StepTransformResult.withoutHold(application);    if (!elements.isEmpty()) {        resultBuilder = resultBuilder.withAdditionalOutput(OutputType.PCOLLECTION_VIEW);    }    return resultBuilder.build();}
public PTransformReplacement<PCollection<ElemT>, PCollection<ElemT>> beam_f4240_0(AppliedPTransform<PCollection<ElemT>, PCollection<ElemT>, PTransform<PCollection<ElemT>, PCollection<ElemT>>> transform)
{    PCollectionView<ViewT> view;    try {        view = CreatePCollectionViewTranslation.getView(transform);    } catch (IOException exc) {        throw new RuntimeException(String.format("Could not extract %s from transform %s", PCollectionView.class.getSimpleName(), transform), exc);    }    return PTransformReplacement.of(PTransformReplacements.getSingletonMainInput(transform), new GroupAndWriteView<>(view));}
public Map<PValue, ReplacementOutput> beam_f4241_0(Map<TupleTag<?>, PValue> outputs, PCollection<ElemT> newOutput)
{    return ReplacementOutputs.singleton(outputs, newOutput);}
public static WatermarkCallback beam_f4249_0(BoundedWindow window, WindowingStrategy<?, W> strategy, Runnable callback)
{    @SuppressWarnings("unchecked")    Instant firingAfter = strategy.getTrigger().getWatermarkThatGuaranteesFiring((W) window);    return new WatermarkCallback(firingAfter, callback);}
public static WatermarkCallback beam_f4250_0(BoundedWindow window, WindowingStrategy<?, W> strategy, Runnable callback)
{            Instant firingAfter = window.maxTimestamp().plus(strategy.getAllowedLateness()).plus(1L);    return new WatermarkCallback(firingAfter, callback);}
public boolean beam_f4251_0(Instant currentWatermark)
{    return currentWatermark.isAfter(fireAfter) || currentWatermark.equals(BoundedWindow.TIMESTAMP_MAX_VALUE);}
public Instant beam_f4259_0()
{    return currentWatermark.get();}
public synchronized WatermarkUpdate beam_f4260_0()
{    Instant oldWatermark = currentWatermark.get();    Instant minInputWatermark = BoundedWindow.TIMESTAMP_MAX_VALUE;    for (Watermark inputWatermark : inputWatermarks) {        minInputWatermark = INSTANT_ORDERING.min(minInputWatermark, inputWatermark.get());    }    if (!pendingElements.isEmpty()) {        minInputWatermark = INSTANT_ORDERING.min(minInputWatermark, pendingElements.firstEntry().getElement().getMinimumTimestamp());    }    Instant newWatermark = INSTANT_ORDERING.max(oldWatermark, minInputWatermark);    currentWatermark.set(newWatermark);    return updateAndTrace(getName(), oldWatermark, newWatermark);}
private synchronized void beam_f4261_0(Bundle<?, ?> newPending)
{    pendingElements.add(newPending);}
public Instant beam_f4269_0()
{    return currentWatermark.get();}
public synchronized WatermarkUpdate beam_f4270_0()
{    Instant oldWatermark = currentWatermark.get();    Instant newWatermark = INSTANT_ORDERING.min(inputWatermark.get(), inputWatermark.getEarliestTimerTimestamp(), holds.getMinHold());    newWatermark = INSTANT_ORDERING.max(oldWatermark, newWatermark);    currentWatermark.set(newWatermark);    return updateAndTrace(getName(), oldWatermark, newWatermark);}
public synchronized String beam_f4271_0()
{    return MoreObjects.toStringHelper(AppliedPTransformOutputWatermark.class).add("holds", holds).add("currentWatermark", currentWatermark).toString();}
private synchronized Map<StructuralKey<?>, List<TimerData>> beam_f4279_0(TimeDomain domain, Instant firingTime)
{    Map<StructuralKey<?>, List<TimerData>> firedTimers;    switch(domain) {        case PROCESSING_TIME:            firedTimers = extractFiredTimers(firingTime, processingTimers);            break;        case SYNCHRONIZED_PROCESSING_TIME:            firedTimers = extractFiredTimers(INSTANT_ORDERING.min(firingTime, earliestHold.get()), synchronizedProcessingTimers);            break;        default:            throw new IllegalArgumentException("Called getFiredTimers on a Synchronized Processing Time watermark" + " and gave a non-processing time domain " + domain);    }    for (Map.Entry<StructuralKey<?>, ? extends Collection<TimerData>> firedTimer : firedTimers.entrySet()) {        pendingTimers.addAll(firedTimer.getValue());    }    return firedTimers;}
private Map<TimeDomain, NavigableSet<TimerData>> beam_f4280_0(StructuralKey<?> key)
{    NavigableSet<TimerData> processingQueue = processingTimers.computeIfAbsent(key, k -> new TreeSet<>());    NavigableSet<TimerData> synchronizedProcessingQueue = synchronizedProcessingTimers.computeIfAbsent(key, k -> new TreeSet<>());    EnumMap<TimeDomain, NavigableSet<TimerData>> result = new EnumMap<>(TimeDomain.class);    result.put(TimeDomain.PROCESSING_TIME, processingQueue);    result.put(TimeDomain.SYNCHRONIZED_PROCESSING_TIME, synchronizedProcessingQueue);    return result;}
public synchronized String beam_f4281_0()
{    return MoreObjects.toStringHelper(SynchronizedProcessingTimeInputWatermark.class).add("earliestHold", earliestHold).toString();}
private static Map<StructuralKey<?>, List<TimerData>> beam_f4289_0(Instant latestTime, Map<StructuralKey<?>, NavigableSet<TimerData>> objectTimers)
{    Map<StructuralKey<?>, List<TimerData>> result = new HashMap<>();    Set<StructuralKey<?>> emptyKeys = new HashSet<>();    for (Map.Entry<StructuralKey<?>, NavigableSet<TimerData>> pendingTimers : objectTimers.entrySet()) {        NavigableSet<TimerData> timers = pendingTimers.getValue();        if (!timers.isEmpty() && timers.first().getTimestamp().isBefore(latestTime)) {            ArrayList<TimerData> keyFiredTimers = new ArrayList<>();            result.put(pendingTimers.getKey(), keyFiredTimers);            while (!timers.isEmpty() && timers.first().getTimestamp().isBefore(latestTime)) {                keyFiredTimers.add(timers.first());                timers.remove(timers.first());            }        }        if (timers.isEmpty()) {            emptyKeys.add(pendingTimers.getKey());        }    }    objectTimers.keySet().removeAll(emptyKeys);    return result;}
public static WatermarkManager<ExecutableT, ? super CollectionT> beam_f4290_0(Clock clock, ExecutableGraph<ExecutableT, ? super CollectionT> graph, Function<ExecutableT, String> getName)
{    return new WatermarkManager<>(clock, graph, getName);}
private TransformWatermarks beam_f4291_0(CollectionT value)
{    return getTransformWatermark(graph.getProducer(value));}
private void beam_f4299_0()
{    refreshLock.lock();    try {        applyNUpdates(-1);    } finally {        refreshLock.unlock();    }}
private void beam_f4300_0(int numUpdates)
{    for (int i = 0; !pendingUpdates.isEmpty() && (i < numUpdates || numUpdates <= 0); i++) {        PendingWatermarkUpdate<ExecutableT, CollectionT> pending = pendingUpdates.poll();        applyPendingUpdate(pending);        pendingRefreshes.add(pending.getExecutable());    }}
private void beam_f4301_0(PendingWatermarkUpdate<ExecutableT, CollectionT> pending)
{    ExecutableT executable = pending.getExecutable();    Bundle<?, ? extends CollectionT> inputBundle = pending.getInputBundle();    updatePending(inputBundle, pending.getTimerUpdate(), executable, pending.getUnprocessedInputs(), pending.getOutputs());    TransformWatermarks transformWms = transformToWatermarks.get(executable);    transformWms.setEventTimeHold(inputBundle == null ? null : inputBundle.getKey(), pending.getEarliestHold());}
public int beam_f4309_0()
{    return Objects.hash(timestamp, key);}
public boolean beam_f4310_0(Object other)
{    if (other == null || !(other instanceof KeyedHold)) {        return false;    }    KeyedHold that = (KeyedHold) other;    return Objects.equals(this.timestamp, that.timestamp) && Objects.equals(this.key, that.key);}
public Instant beam_f4311_0()
{    return timestamp;}
public synchronized Instant beam_f4319_0()
{    latestSynchronizedOutputWm = INSTANT_ORDERING.max(latestSynchronizedOutputWm, INSTANT_ORDERING.min(clock.now(), synchronizedProcessingOutputWatermark.get()));    return latestSynchronizedOutputWm;}
private WatermarkUpdate beam_f4320_0()
{    inputWatermark.refresh();    synchronizedProcessingInputWatermark.refresh();    WatermarkUpdate eventOutputUpdate = outputWatermark.refresh();    WatermarkUpdate syncOutputUpdate = synchronizedProcessingOutputWatermark.refresh();    return eventOutputUpdate.union(syncOutputUpdate);}
private void beam_f4321_0(Object key, Instant newHold)
{    outputWatermark.updateHold(key, newHold);}
public static TimerUpdateBuilder beam_f4329_0(StructuralKey<?> key)
{    return new TimerUpdateBuilder(key);}
public TimerUpdateBuilder beam_f4330_0(Iterable<TimerData> completedTimers)
{    Iterables.addAll(this.completedTimers, completedTimers);    return this;}
public TimerUpdateBuilder beam_f4331_0(TimerData setTimer)
{    checkArgument(setTimer.getTimestamp().isBefore(BoundedWindow.TIMESTAMP_MAX_VALUE), "Got a timer for after the end of time (%s), got %s", BoundedWindow.TIMESTAMP_MAX_VALUE, setTimer.getTimestamp());    deletedTimers.remove(setTimer);    setTimers.add(setTimer);    return this;}
public int beam_f4339_0()
{    return Objects.hash(key, completedTimers, setTimers, deletedTimers);}
public boolean beam_f4340_0(Object other)
{    if (other == null || !(other instanceof TimerUpdate)) {        return false;    }    TimerUpdate that = (TimerUpdate) other;    return Objects.equals(this.key, that.key) && Objects.equals(this.completedTimers, that.completedTimers) && Objects.equals(this.setTimers, that.setTimers) && Objects.equals(this.deletedTimers, that.deletedTimers);}
public ExecutableT beam_f4341_0()
{    return executable;}
public void beam_f4350_0(WindowedValue<InputT> compressedElement) throws Exception
{    for (WindowedValue<InputT> element : compressedElement.explodeWindows()) {        Collection<? extends BoundedWindow> windows = assignWindows(windowFn, element);        outputBundle.add(WindowedValue.of(element.getValue(), element.getTimestamp(), windows, element.getPane()));    }}
private Collection<? extends BoundedWindow> beam_f4351_0(WindowFn<InputT, W> windowFn, WindowedValue<InputT> element) throws Exception
{    WindowFn<InputT, W>.AssignContext assignContext = new DirectAssignContext<>(windowFn, element);    return windowFn.assignWindows(assignContext);}
public TransformResult<InputT> beam_f4352_0() throws Exception
{    return StepTransformResult.<InputT>withoutHold(transform).addOutput(outputBundle).build();}
private int beam_f4360_0(long totalRecords)
{    if (totalRecords == 0) {                return 1;    }            int extraShards = extraShardsSupplier.get();    if (totalRecords < MIN_SHARDS_FOR_LOG + extraShards) {        return (int) totalRecords;    }        int floorLogRecs = (int) Math.log10(totalRecords);    return Math.max(floorLogRecs, MIN_SHARDS_FOR_LOG) + extraShards;}
public Integer beam_f4361_0()
{    return ThreadLocalRandom.current().nextInt(0, upperBound);}
public void beam_f4362_0()
{    MockitoAnnotations.initMocks(this);    source = CountingSource.upTo(10L);    longs = p.apply(Read.from(source));    options = PipelineOptionsFactory.create();    factory = new BoundedReadEvaluatorFactory(context, options, Long.MAX_VALUE);    bundleFactory = ImmutableListBundleFactory.create();    longsProducer = DirectGraphs.getProducer(longs);}
public void beam_f4370_0()
{    factory.cleanup();    assertThat(factory.executor.isShutdown(), is(true));}
public List<? extends OffsetBasedSource<T>> beam_f4371_0(long desiredBundleSizeBytes, PipelineOptions options) throws Exception
{    return ImmutableList.of(this);}
public long beam_f4372_0(PipelineOptions options) throws Exception
{    return elems.length;}
public boolean beam_f4380_0() throws IOException
{        if (index + 1 == sleepIndex && sleepIndex < getCurrentSource().elems.length) {        try {            dynamicallySplit.await();            while (initialSource.equals(getCurrentSource())) {                        }        } catch (InterruptedException e) {            Thread.currentThread().interrupt();            throw new IOException(e);        }    }    if (getCurrentSource().elems.length > index + 1) {        index++;        return true;    }    return false;}
public T beam_f4381_0() throws NoSuchElementException
{    return getCurrentSource().elems[index];}
public void beam_f4382_0() throws IOException
{    TestSource.readerClosed = true;}
public void beam_f4390_0()
{    PCollection<Record> pc = p.apply(Create.empty(new RecordNoDecodeCoder()));    UncommittedBundle<Record> bundle = factory.createKeyedBundle(StructuralKey.of("foo", StringUtf8Coder.of()), pc);    thrown.expect(UserCodeException.class);    thrown.expectCause(isA(CoderException.class));    thrown.expectMessage("Decode not allowed");    bundle.add(WindowedValue.valueInGlobalWindow(new Record()));}
public void beam_f4391_0(Record value, OutputStream outStream) throws IOException
{    throw new CoderException("Encode not allowed");}
public Record beam_f4392_0(InputStream inStream) throws IOException
{    return null;}
public boolean beam_f4403_0()
{    return false;}
public Object beam_f4404_0(Record value)
{    return value;}
public void beam_f4405_0(ProcessContext ctxt)
{    ctxt.output(ctxt.element());}
public void beam_f4413_0()
{    CopyOnAccessInMemoryStateInternals<String> underlying = CopyOnAccessInMemoryStateInternals.withUnderlying(key, null);    CopyOnAccessInMemoryStateInternals<String> internals = CopyOnAccessInMemoryStateInternals.withUnderlying(key, underlying);    StateNamespace namespace = new StateNamespaceForTest("foo");    StateTag<BagState<String>> bagTag = StateTags.bag("foo", StringUtf8Coder.of());    BagState<String> stringBag = internals.state(namespace, bagTag);    assertThat(stringBag.read(), emptyIterable());    stringBag.add("bar");    stringBag.add("baz");    assertThat(stringBag.read(), containsInAnyOrder("baz", "bar"));    BagState<String> reReadVoidBag = internals.state(namespace, bagTag);    assertThat(reReadVoidBag.read(), containsInAnyOrder("baz", "bar"));    BagState<String> underlyingState = underlying.state(namespace, bagTag);    assertThat(underlyingState.read(), emptyIterable());}
public void beam_f4414_0()
{    CopyOnAccessInMemoryStateInternals<String> underlying = CopyOnAccessInMemoryStateInternals.withUnderlying(key, null);    StateNamespace namespace = new StateNamespaceForTest("foo");    StateTag<ValueState<String>> valueTag = StateTags.value("foo", StringUtf8Coder.of());    ValueState<String> underlyingValue = underlying.state(namespace, valueTag);    assertThat(underlyingValue.read(), nullValue(String.class));    underlyingValue.write("bar");    assertThat(underlyingValue.read(), equalTo("bar"));    CopyOnAccessInMemoryStateInternals<String> internals = CopyOnAccessInMemoryStateInternals.withUnderlying(key, underlying);    ValueState<String> copyOnAccessState = internals.state(namespace, valueTag);    assertThat(copyOnAccessState.read(), equalTo("bar"));    copyOnAccessState.write("baz");    assertThat(copyOnAccessState.read(), equalTo("baz"));    assertThat(underlyingValue.read(), equalTo("bar"));    ValueState<String> reReadUnderlyingValue = underlying.state(namespace, valueTag);    assertThat(underlyingValue.read(), equalTo(reReadUnderlyingValue.read()));}
public void beam_f4415_0()
{    CopyOnAccessInMemoryStateInternals<String> underlying = CopyOnAccessInMemoryStateInternals.withUnderlying(key, null);    StateNamespace namespace = new StateNamespaceForTest("foo");    StateTag<BagState<Integer>> valueTag = StateTags.bag("foo", VarIntCoder.of());    BagState<Integer> underlyingValue = underlying.state(namespace, valueTag);    assertThat(underlyingValue.read(), emptyIterable());    underlyingValue.add(1);    assertThat(underlyingValue.read(), containsInAnyOrder(1));    CopyOnAccessInMemoryStateInternals<String> internals = CopyOnAccessInMemoryStateInternals.withUnderlying(key, underlying);    BagState<Integer> copyOnAccessState = internals.state(namespace, valueTag);    assertThat(copyOnAccessState.read(), containsInAnyOrder(1));    copyOnAccessState.add(4);    assertThat(copyOnAccessState.read(), containsInAnyOrder(4, 1));    assertThat(underlyingValue.read(), containsInAnyOrder(1));    BagState<Integer> reReadUnderlyingValue = underlying.state(namespace, valueTag);    assertThat(Lists.newArrayList(underlyingValue.read()), equalTo(Lists.newArrayList(reReadUnderlyingValue.read())));}
public void beam_f4423_0()
{    CopyOnAccessInMemoryStateInternals<String> underlying = CopyOnAccessInMemoryStateInternals.withUnderlying(key, null);    CopyOnAccessInMemoryStateInternals<String> internals = CopyOnAccessInMemoryStateInternals.withUnderlying(key, underlying);    StateNamespace namespace = new StateNamespaceForTest("foo");    StateTag<BagState<String>> bagTag = StateTags.bag("foo", StringUtf8Coder.of());    BagState<String> stringBag = underlying.state(namespace, bagTag);    assertThat(stringBag.read(), emptyIterable());    stringBag.add("bar");    stringBag.add("baz");    BagState<String> internalsState = internals.state(namespace, bagTag);    internalsState.add("eggs");    internalsState.add("ham");    internalsState.add("0x00ff00");    internalsState.add("&");    internals.commit();    BagState<String> reReadInternalState = internals.state(namespace, bagTag);    assertThat(reReadInternalState.read(), containsInAnyOrder("bar", "baz", "0x00ff00", "eggs", "&", "ham"));    BagState<String> reReadUnderlyingState = underlying.state(namespace, bagTag);    assertThat(reReadUnderlyingState.read(), containsInAnyOrder("bar", "baz"));}
public void beam_f4424_0()
{    CopyOnAccessInMemoryStateInternals<String> underlying = CopyOnAccessInMemoryStateInternals.withUnderlying(key, null);    CopyOnAccessInMemoryStateInternals<String> internals = CopyOnAccessInMemoryStateInternals.withUnderlying(key, underlying);    internals.commit();    StateNamespace namespace = new StateNamespaceForTest("foo");    StateTag<BagState<String>> bagTag = StateTags.bag("foo", StringUtf8Coder.of());    BagState<String> stringBag = underlying.state(namespace, bagTag);    assertThat(stringBag.read(), emptyIterable());    stringBag.add("bar");    stringBag.add("baz");    BagState<String> internalState = internals.state(namespace, bagTag);    assertThat(internalState.read(), emptyIterable());    BagState<String> reReadUnderlyingState = underlying.state(namespace, bagTag);    assertThat(reReadUnderlyingState.read(), containsInAnyOrder("bar", "baz"));}
public void beam_f4425_0()
{    CopyOnAccessInMemoryStateInternals<String> internals = CopyOnAccessInMemoryStateInternals.withUnderlying(key, null);    internals.commit();    assertThat(internals.isEmpty(), is(true));}
public Instant beam_f4433_0()
{    return new Instant(689743L);}
public void beam_f4434_0()
{    BoundedWindow first = new BoundedWindow() {        @Override        public Instant maxTimestamp() {            return new Instant(2048L);        }    };    BoundedWindow second = new BoundedWindow() {        @Override        public Instant maxTimestamp() {            return new Instant(689743L);        }    };    CopyOnAccessInMemoryStateInternals<String> underlying = CopyOnAccessInMemoryStateInternals.withUnderlying("foo", null);    StateTag<WatermarkHoldState> firstHoldAddress = StateTags.watermarkStateInternal("foo", TimestampCombiner.EARLIEST);    WatermarkHoldState firstHold = underlying.state(StateNamespaces.window(null, first), firstHoldAddress);    firstHold.add(new Instant(224L));    CopyOnAccessInMemoryStateInternals<String> internals = CopyOnAccessInMemoryStateInternals.withUnderlying("foo", underlying.commit());    StateTag<WatermarkHoldState> secondHoldAddress = StateTags.watermarkStateInternal("foo", TimestampCombiner.EARLIEST);    WatermarkHoldState secondHold = internals.state(StateNamespaces.window(null, second), secondHoldAddress);    secondHold.add(new Instant(24L));    internals.commit();    assertThat(internals.getEarliestWatermarkHold(), equalTo(new Instant(24L)));}
public Instant beam_f4435_0()
{    return new Instant(2048L);}
public void beam_f4443_0()
{    PCollection<String> created = p.apply(Create.of("foo", "bar"));    PCollection<Long> counted = p.apply(Read.from(CountingSource.upTo(1234L)));    PCollection<Long> unCounted = p.apply(GenerateSequence.from(0));    p.traverseTopologically(visitor);    DirectGraph graph = visitor.getGraph();    assertThat(graph.getRootTransforms(), hasSize(3));    assertThat(graph.getRootTransforms(), Matchers.containsInAnyOrder(new Object[] { graph.getProducer(created), graph.getProducer(counted), graph.getProducer(unCounted) }));    for (AppliedPTransform<?, ?, ?> root : graph.getRootTransforms()) {                assertThat(root.getInputs().entrySet(), emptyIterable());        assertThat(Iterables.getOnlyElement(root.getOutputs().values()), Matchers.<POutput>isOneOf(created, counted, unCounted));    }}
public void beam_f4444_0()
{    PCollections<String> flatten = Flatten.pCollections();    PCollectionList<String> emptyList = PCollectionList.empty(p);    PCollection<String> empty = emptyList.apply(flatten);    empty.setCoder(StringUtf8Coder.of());    p.traverseTopologically(visitor);    DirectGraph graph = visitor.getGraph();    assertThat(graph.getRootTransforms(), Matchers.containsInAnyOrder(new Object[] { graph.getProducer(empty) }));    AppliedPTransform<?, ?, ?> onlyRoot = Iterables.getOnlyElement(graph.getRootTransforms());    assertThat((Object) onlyRoot.getTransform(), equalTo(flatten));    assertThat(onlyRoot.getInputs().entrySet(), emptyIterable());    assertThat(onlyRoot.getOutputs(), equalTo(empty.expand()));}
public void beam_f4445_0()
{    PCollection<String> created = p.apply(Create.of("1", "2", "3"));    PCollection<String> transformed = created.apply(ParDo.of(new DoFn<String, String>() {        @ProcessElement        public void processElement(DoFn<String, String>.ProcessContext c) throws Exception {            c.output(Integer.toString(c.element().length()));        }    }));    PCollection<String> flattened = PCollectionList.of(created).and(transformed).apply(Flatten.pCollections());    p.traverseTopologically(visitor);    DirectGraph graph = visitor.getGraph();    AppliedPTransform<?, ?, ?> transformedProducer = graph.getProducer(transformed);    AppliedPTransform<?, ?, ?> flattenedProducer = graph.getProducer(flattened);    assertThat(graph.getPerElementConsumers(created), Matchers.containsInAnyOrder(new Object[] { transformedProducer, flattenedProducer }));    assertThat(graph.getPerElementConsumers(transformed), Matchers.containsInAnyOrder(new Object[] { flattenedProducer }));    assertThat(graph.getPerElementConsumers(flattened), emptyIterable());}
public void beam_f4453_0()
{    thrown.expect(IllegalStateException.class);    thrown.expectMessage("completely traversed");    thrown.expectMessage("get a graph");    visitor.getGraph();}
public void beam_f4454_0()
{    TestPipeline p = TestPipeline.create();    PCollection<KV<String, Integer>> input = p.apply(Create.of(KV.of("foo", 1)).withCoder(KvCoder.of(StringUtf8Coder.of(), VarIntCoder.of())));    PCollection<KV<String, Iterable<Integer>>> grouped = input.apply(GroupByKey.create());    AppliedPTransform<?, ?, ?> producer = DirectGraphs.getProducer(grouped);    PTransformReplacement<PCollection<KV<String, Integer>>, PCollection<KV<String, Iterable<Integer>>>> replacement = factory.getReplacementTransform((AppliedPTransform) producer);    assertThat(replacement.getInput(), Matchers.<PCollection<?>>equalTo(input));}
public void beam_f4455_0()
{    MockitoAnnotations.initMocks(this);    executor = Executors.newSingleThreadExecutor();    metrics = new DirectMetrics(executor);}
public void beam_f4463_0()
{    for (PipelineOptionsRegistrar registrar : Lists.newArrayList(ServiceLoader.load(PipelineOptionsRegistrar.class).iterator())) {        if (registrar instanceof Options) {            return;        }    }    fail("Expected to find " + Options.class);}
public void beam_f4464_0()
{    for (PipelineRunnerRegistrar registrar : Lists.newArrayList(ServiceLoader.load(PipelineRunnerRegistrar.class).iterator())) {        if (registrar instanceof Runner) {            return;        }    }    fail("Expected to find " + Runner.class);}
public void beam_f4465_0() throws Exception
{        @SuppressWarnings("unchecked")    final Set<String> allowed = ImmutableSet.of("org.apache.beam.sdk", "org.apache.beam.runners.direct", "org.joda.time", "javax.annotation", "java.math");    final Package thisPackage = getClass().getPackage();    final ClassLoader thisClassLoader = getClass().getClassLoader();    ApiSurface apiSurface = ApiSurface.ofPackage(thisPackage, thisClassLoader).pruningClass(Pipeline.class).pruningClass(PipelineRunner.class).pruningClass(PipelineOptions.class).pruningClass(PipelineOptionsRegistrar.class).pruningClass(PipelineOptions.DirectRunner.class).pruningClass(DisplayData.Builder.class).pruningClass(MetricResults.class).pruningClass(DirectGraphs.class).pruningClass(WatermarkManager.class).pruningPattern("org[.]apache[.]beam[.]runners[.]direct[.]portable.*").pruningPattern("org[.]apache[.]beam[.].*Test.*").pruningPattern("org[.]apache[.]beam[.].*IT").pruningPattern("java[.]io.*").pruningPattern("java[.]lang.*").pruningPattern("java[.]util.*");    assertThat(apiSurface, containsOnlyPackages(allowed));}
public String beam_f4473_0(KV<String, Long> input)
{    return String.format("%s: %s", input.getKey(), input.getValue());}
public void beam_f4474_0(ProcessContext c)
{    changed.getAndIncrement();}
public void beam_f4475_0()
{    Pipeline p = getPipeline();    SerializableFunction<Integer, byte[]> getBytes = input -> {        try {            return CoderUtils.encodeToByteArray(VarIntCoder.of(), input);        } catch (CoderException e) {            fail("Unexpected Coder Exception " + e);            throw new AssertionError("Unreachable");        }    };    TypeDescriptor<byte[]> td = new TypeDescriptor<byte[]>() {    };    PCollection<byte[]> foos = p.apply(Create.of(1, 1, 1, 2, 2, 3)).apply(MapElements.into(td).via(getBytes));    PCollection<byte[]> msync = p.apply(Create.of(1, -2, -8, -16)).apply(MapElements.into(td).via(getBytes));    PCollection<byte[]> bytes = PCollectionList.of(foos).and(msync).apply(Flatten.pCollections());    PCollection<KV<byte[], Long>> counts = bytes.apply(Count.perElement());    PCollection<KV<Integer, Long>> countsBackToString = counts.apply(MapElements.via(new SimpleFunction<KV<byte[], Long>, KV<Integer, Long>>() {        @Override        public KV<Integer, Long> apply(KV<byte[], Long> input) {            try {                return KV.of(CoderUtils.decodeFromByteArray(VarIntCoder.of(), input.getKey()), input.getValue());            } catch (CoderException e) {                fail("Unexpected Coder Exception " + e);                throw new AssertionError("Unreachable");            }        }    }));    Map<Integer, Long> expected = ImmutableMap.<Integer, Long>builder().put(1, 4L).put(2, 2L).put(3, 1L).put(-2, 1L).put(-8, 1L).put(-16, 1L).build();    PAssert.thatMap(countsBackToString).isEqualTo(expected);}
public void beam_f4484_0()
{    DoFn<Integer, Integer> brokenDoFn = new DoFn<Integer, Integer>() {        @ProcessElement        public void processElement(ProcessContext c) throws Exception {        }        @Override        public void populateDisplayData(DisplayData.Builder builder) {            throw new RuntimeException("oh noes!");        }    };    Pipeline p = getPipeline();    p.apply(Create.of(1, 2, 3)).apply(ParDo.of(brokenDoFn));    thrown.expectMessage(brokenDoFn.getClass().getName());    thrown.expectCause(ThrowableMessageMatcher.hasMessage(is("oh noes!")));    p.run();}
public void beam_f4486_0(DisplayData.Builder builder)
{    throw new RuntimeException("oh noes!");}
public void beam_f4487_0() throws Exception
{    Pipeline pipeline = getPipeline();    pipeline.apply(Create.of(42)).apply(ParDo.of(new DoFn<Integer, List<Integer>>() {        @ProcessElement        public void processElement(ProcessContext c) {            List<Integer> outputList = Arrays.asList(1, 2, 3, 4);            c.output(outputList);            outputList.set(0, 37);            c.output(outputList);        }    }));    thrown.expect(IllegalMutationException.class);    thrown.expectMessage("output");    thrown.expectMessage("must not be mutated");    pipeline.run();}
public void beam_f4495_0() throws Exception
{    Pipeline pipeline = getPipeline();    pipeline.apply(Create.of(Arrays.asList(1, 2, 3), Arrays.asList(4, 5, 6)).withCoder(ListCoder.of(VarIntCoder.of()))).apply(ParDo.of(new DoFn<List<Integer>, Integer>() {        @ProcessElement        public void processElement(ProcessContext c) {            List<Integer> inputList = c.element();            inputList.set(0, 37);            c.output(12);        }    }));    thrown.expect(IllegalMutationException.class);    thrown.expectMessage("Input");    thrown.expectMessage("must not be mutated");    pipeline.run();}
public void beam_f4496_0(ProcessContext c)
{    List<Integer> inputList = c.element();    inputList.set(0, 37);    c.output(12);}
public void beam_f4497_0() throws Exception
{    Pipeline pipeline = getPipeline();    pipeline.apply(Create.of(new byte[] { 0x1, 0x2, 0x3 }, new byte[] { 0x4, 0x5, 0x6 })).apply(ParDo.of(new DoFn<byte[], Integer>() {        @ProcessElement        public void processElement(ProcessContext c) {            byte[] inputArray = c.element();            inputArray[0] = 0xa;            c.output(13);        }    }));    thrown.expect(IllegalMutationException.class);    thrown.expectMessage("Input");    thrown.expectMessage("must not be mutated");    pipeline.run();}
public void beam_f4505_0(ProcessContext c)
{    TestSerializationOfOptions options = c.getPipelineOptions().as(TestSerializationOfOptions.class);    assertEquals("testValue", options.getFoo());    assertEquals("not overridden", options.getIgnoredField());    c.output(Integer.parseInt(c.element()));}
public Long beam_f4507_0(InputStream inStream) throws IOException
{    throw new CoderException("Cannot decode a long");}
public static BoundedSource<T> beam_f4508_0(BoundedSource<T> underlying)
{    return new MustSplitSource<>(underlying);}
public void beam_f4516_0()
{    TimerData eventTimer = TimerData.of(StateNamespaces.global(), new Instant(20145L), TimeDomain.EVENT_TIME);    TimerData processingTimer = TimerData.of(StateNamespaces.global(), new Instant(125555555L), TimeDomain.PROCESSING_TIME);    TimerData synchronizedProcessingTimer = TimerData.of(StateNamespaces.global(), new Instant(98745632189L), TimeDomain.SYNCHRONIZED_PROCESSING_TIME);    internals.deleteTimer(eventTimer);    internals.deleteTimer(processingTimer);    internals.deleteTimer(synchronizedProcessingTimer);    assertThat(internals.getTimerUpdate().getDeletedTimers(), containsInAnyOrder(eventTimer, synchronizedProcessingTimer, processingTimer));}
public void beam_f4517_0()
{    assertThat(internals.currentProcessingTime(), equalTo(clock.now()));    Instant oldProcessingTime = internals.currentProcessingTime();    clock.advance(Duration.standardHours(12));    assertThat(internals.currentProcessingTime(), equalTo(clock.now()));    assertThat(internals.currentProcessingTime(), equalTo(oldProcessingTime.plus(Duration.standardHours(12))));}
public void beam_f4518_0()
{    when(watermarks.getSynchronizedProcessingInputTime()).thenReturn(new Instant(12345L));    assertThat(internals.currentSynchronizedProcessingTime(), equalTo(new Instant(12345L)));}
public void beam_f4526_0() throws Exception
{    final TransformResult<String> result = StepTransformResult.<String>withoutHold(downstreamProducer).build();    final Collection<WindowedValue<String>> elementsProcessed = new ArrayList<>();    TransformEvaluator<String> evaluator = new TransformEvaluator<String>() {        @Override        public void processElement(WindowedValue<String> element) throws Exception {            elementsProcessed.add(element);        }        @Override        public TransformResult<String> finishBundle() throws Exception {            return result;        }    };    WindowedValue<String> foo = WindowedValue.valueInGlobalWindow("foo");    WindowedValue<String> spam = WindowedValue.valueInGlobalWindow("spam");    WindowedValue<String> third = WindowedValue.valueInGlobalWindow("third");    CommittedBundle<String> inputBundle = bundleFactory.createBundle(created).add(foo).add(spam).add(third).commit(Instant.now());    when(registry.<String>forApplication(downstreamProducer, inputBundle)).thenReturn(evaluator);    DirectTransformExecutor<String> executor = new DirectTransformExecutor<>(evaluationContext, registry, Collections.emptyList(), inputBundle, downstreamProducer, completionCallback, transformEvaluationState);    Future<?> future = Executors.newSingleThreadExecutor().submit(executor);    evaluatorCompleted.await();    future.get();    assertThat(elementsProcessed, containsInAnyOrder(spam, third, foo));    assertThat(completionCallback.handledResult, equalTo(result));    assertThat(completionCallback.handledException, is(nullValue()));}
public void beam_f4527_0(WindowedValue<String> element) throws Exception
{    elementsProcessed.add(element);}
public TransformResult<String> beam_f4528_0() throws Exception
{    return result;}
public void beam_f4538_0() throws Exception
{    final TransformResult<Object> result = StepTransformResult.withoutHold(createdProducer).build();    TransformEvaluator<Object> evaluator = new TransformEvaluator<Object>() {        @Override        public void processElement(WindowedValue<Object> element) throws Exception {        }        @Override        public TransformResult<Object> finishBundle() throws Exception {            return result;        }    };    WindowedValue<String> fooBytes = WindowedValue.valueInGlobalWindow("foo");    CommittedBundle<String> inputBundle = bundleFactory.createBundle(created).add(fooBytes).commit(Instant.now());    when(registry.forApplication(downstreamProducer, inputBundle)).thenReturn(evaluator);    DirectTransformExecutor<String> executor = new DirectTransformExecutor<>(evaluationContext, registry, Collections.<ModelEnforcementFactory>singleton(new ThrowingEnforcementFactory(ThrowingEnforcementFactory.When.AFTER_BUNDLE)), inputBundle, downstreamProducer, completionCallback, transformEvaluationState);    Future<?> task = Executors.newSingleThreadExecutor().submit(executor);    thrown.expectCause(isA(RuntimeException.class));    thrown.expectMessage("afterFinish");    task.get();}
public TransformResult<Object> beam_f4540_0() throws Exception
{    return result;}
public void beam_f4541_0() throws Exception
{    final TransformResult<Object> result = StepTransformResult.withoutHold(createdProducer).build();    TransformEvaluator<Object> evaluator = new TransformEvaluator<Object>() {        @Override        public void processElement(WindowedValue<Object> element) throws Exception {        }        @Override        public TransformResult<Object> finishBundle() throws Exception {            return result;        }    };    WindowedValue<String> fooBytes = WindowedValue.valueInGlobalWindow("foo");    CommittedBundle<String> inputBundle = bundleFactory.createBundle(created).add(fooBytes).commit(Instant.now());    when(registry.forApplication(downstreamProducer, inputBundle)).thenReturn(evaluator);    DirectTransformExecutor<String> executor = new DirectTransformExecutor<>(evaluationContext, registry, Collections.<ModelEnforcementFactory>singleton(new ThrowingEnforcementFactory(ThrowingEnforcementFactory.When.AFTER_ELEMENT)), inputBundle, downstreamProducer, completionCallback, transformEvaluationState);    Future<?> task = Executors.newSingleThreadExecutor().submit(executor);    thrown.expectCause(isA(RuntimeException.class));    thrown.expectMessage("afterElement");    task.get();}
public void beam_f4550_0(WindowedValue<T> element)
{    afterElements.add(element);}
public void beam_f4551_0(CommittedBundle<T> input, TransformResult<T> result, Iterable<? extends CommittedBundle<?>> outputs)
{    finishedBundles.add(result);}
public ModelEnforcement<T> beam_f4552_0(CommittedBundle<T> input, AppliedPTransform<?, ?, ?> consumer)
{    if (when == When.BEFORE_BUNDLE) {        throw new RuntimeException("forBundle");    }    return new ThrowingEnforcement<>();}
public void beam_f4560_0() throws Exception
{    ParDoEvaluator<Object> underlying = mock(ParDoEvaluator.class);    doThrow(IllegalArgumentException.class).when(underlying).finishBundle();    DoFn<?, ?> original = lifecycleManager.get();        assertThat(original, not(nullValue()));    TransformEvaluator<Object> evaluator = DoFnLifecycleManagerRemovingTransformEvaluator.wrapping(underlying, lifecycleManager);    try {        evaluator.finishBundle();    } catch (Exception e) {        assertThat(lifecycleManager.get(), not(Matchers.theInstance(original)));        return;    }    fail("Expected underlying evaluator to throw on method call");}
public void beam_f4562_0() throws Exception
{    DoFnLifecycleManager first = DoFnLifecycleManager.of(new ThrowsInCleanupFn("foo"));    DoFnLifecycleManager second = DoFnLifecycleManager.of(new ThrowsInCleanupFn("bar"));    DoFnLifecycleManager third = DoFnLifecycleManager.of(new ThrowsInCleanupFn("baz"));    first.get();    second.get();    third.get();    final Collection<Matcher<? super Throwable>> suppressions = new ArrayList<>();    suppressions.add(allOf(instanceOf(UserCodeException.class), new CausedByMatcher(new ThrowableMessageMatcher("foo"))));    suppressions.add(allOf(instanceOf(UserCodeException.class), new CausedByMatcher(new ThrowableMessageMatcher("bar"))));    suppressions.add(allOf(instanceOf(UserCodeException.class), new CausedByMatcher(new ThrowableMessageMatcher("baz"))));    thrown.expect(new BaseMatcher<Exception>() {        @Override        public void describeTo(Description description) {            description.appendText("Exception suppressing ").appendList("[", ", ", "]", suppressions);        }        @Override        public boolean matches(Object item) {            if (!(item instanceof Exception)) {                return false;            }            Exception that = (Exception) item;            return Matchers.containsInAnyOrder(suppressions).matches(ImmutableList.copyOf(that.getSuppressed()));        }    });    DoFnLifecycleManagers.removeAllFromManagers(ImmutableList.of(first, second, third));}
public void beam_f4563_0(Description description)
{    description.appendText("Exception suppressing ").appendList("[", ", ", "]", suppressions);}
public void beam_f4573_0() throws Exception
{    TestFn obtained = (TestFn) mgr.get();    assertThat(obtained, not(theInstance(fn)));    assertThat(obtained.setupCalled, is(true));    assertThat(obtained.teardownCalled, is(false));}
public void beam_f4574_0() throws Exception
{    TestFn obtained = (TestFn) mgr.get();    TestFn secondObtained = (TestFn) mgr.get();    assertThat(obtained, theInstance(secondObtained));    assertThat(obtained.setupCalled, is(true));    assertThat(obtained.teardownCalled, is(false));}
public void beam_f4575_0() throws Exception
{    CountDownLatch startSignal = new CountDownLatch(1);    ExecutorService executor = Executors.newCachedThreadPool();    List<Future<TestFn>> futures = new ArrayList<>();    for (int i = 0; i < 10; i++) {        futures.add(executor.submit(new GetFnCallable(mgr, startSignal)));    }    startSignal.countDown();    List<TestFn> fns = new ArrayList<>();    for (Future<TestFn> future : futures) {        fns.add(future.get(1L, TimeUnit.SECONDS));    }    for (TestFn fn : fns) {        assertThat(fn.setupCalled, is(true));        int sameInstances = 0;        for (TestFn otherFn : fns) {            if (otherFn == fn) {                sameInstances++;            }        }        assertThat(sameInstances, equalTo(1));    }}
public void beam_f4584_0()
{    checkState(setupCalled, "Cannot call teardown: not set up");    checkState(!teardownCalled, "Cannot call teardown: already torn down");    teardownCalled = true;}
public void beam_f4585_0()
{    DirectRunner runner = DirectRunner.fromOptions(PipelineOptionsFactory.create());    created = p.apply(Create.of(1, 2, 3));    downstream = created.apply(WithKeys.of("foo"));    view = created.apply(View.asIterable());    unbounded = p.apply(GenerateSequence.from(0));    p.replaceAll(runner.defaultTransformOverrides());    KeyedPValueTrackingVisitor keyedPValueTrackingVisitor = KeyedPValueTrackingVisitor.create();    p.traverseTopologically(keyedPValueTrackingVisitor);    BundleFactory bundleFactory = ImmutableListBundleFactory.create();    DirectGraphs.performDirectOverrides(p);    graph = DirectGraphs.getGraph(p);    context = EvaluationContext.create(NanosOffsetClock.create(), bundleFactory, graph, keyedPValueTrackingVisitor.getKeyedPValues(), Executors.newSingleThreadExecutor());    createdProducer = graph.getProducer(created);    downstreamProducer = graph.getProducer(downstream);    viewProducer = graph.getProducer(view);    unboundedProducer = graph.getProducer(unbounded);}
public void beam_f4586_0()
{    PCollectionViewWriter<?, Iterable<Integer>> viewWriter = context.createPCollectionViewWriter(PCollection.createPrimitiveOutputInternal(p, WindowingStrategy.globalDefault(), IsBounded.BOUNDED, IterableCoder.of(KvCoder.of(VoidCoder.of(), VarIntCoder.of()))), view);    BoundedWindow window = new TestBoundedWindow(new Instant(1024L));    BoundedWindow second = new TestBoundedWindow(new Instant(899999L));    ImmutableList.Builder<WindowedValue<?>> valuesBuilder = ImmutableList.builder();    for (Object materializedValue : materializeValuesFor(View.asIterable(), 1)) {        valuesBuilder.add(WindowedValue.of(materializedValue, new Instant(1222), window, PaneInfo.ON_TIME_AND_ONLY_FIRING));    }    for (Object materializedValue : materializeValuesFor(View.asIterable(), 2)) {        valuesBuilder.add(WindowedValue.of(materializedValue, new Instant(8766L), second, PaneInfo.createPane(true, false, Timing.ON_TIME, 0, 0)));    }    viewWriter.add((Iterable) valuesBuilder.build());    SideInputReader reader = context.createSideInputReader(ImmutableList.of(view));    assertThat(reader.get(view, window), containsInAnyOrder(1));    assertThat(reader.get(view, second), containsInAnyOrder(2));    ImmutableList.Builder<WindowedValue<?>> overwrittenValuesBuilder = ImmutableList.builder();    for (Object materializedValue : materializeValuesFor(View.asIterable(), 4444)) {        overwrittenValuesBuilder.add(WindowedValue.of(materializedValue, new Instant(8677L), second, PaneInfo.createPane(false, true, Timing.LATE, 1, 1)));    }    viewWriter.add((Iterable) overwrittenValuesBuilder.build());    assertThat(reader.get(view, second), containsInAnyOrder(2));        reader = context.createSideInputReader(ImmutableList.of(view));    assertThat(reader.get(view, second), containsInAnyOrder(4444));}
public void beam_f4594_0()
{    StructuralKey<String> key = StructuralKey.of("foo", StringUtf8Coder.of());    CommittedBundle<KV<String, Integer>> keyedBundle = context.createKeyedBundle(key, downstream).commit(Instant.now());    assertThat(keyedBundle.getKey(), equalTo(key));}
public void beam_f4595_0()
{    assertThat(context.isDone(unboundedProducer), is(false));    context.handleResult(null, ImmutableList.of(), StepTransformResult.withoutHold(unboundedProducer).build());    context.extractFiredTimers();    assertThat(context.isDone(unboundedProducer), is(true));}
public void beam_f4596_0()
{    assertThat(context.isDone(), is(false));    UncommittedBundle<Integer> rootBundle = context.createBundle(created);    rootBundle.add(WindowedValue.valueInGlobalWindow(1));    CommittedResult handleResult = context.handleResult(null, ImmutableList.of(), StepTransformResult.<Integer>withoutHold(createdProducer).addOutput(rootBundle).build());    @SuppressWarnings("unchecked")    CommittedBundle<Integer> committedBundle = (CommittedBundle<Integer>) Iterables.getOnlyElement(handleResult.getOutputs());    context.handleResult(null, ImmutableList.of(), StepTransformResult.withoutHold(unboundedProducer).build());    assertThat(context.isDone(), is(false));    for (AppliedPTransform<?, ?, ?> consumers : graph.getPerElementConsumers(created)) {        context.handleResult(committedBundle, ImmutableList.of(), StepTransformResult.withoutHold(consumers).build());    }    context.extractFiredTimers();    assertThat(context.isDone(), is(true));}
public boolean beam_f4604_0(Object item)
{    if (item == null || !(item instanceof WindowedValue)) {        return false;    }    WindowedValue<KeyedWorkItem<K, V>> that = (WindowedValue<KeyedWorkItem<K, V>>) item;    Multiset<WindowedValue<V>> myValues = HashMultiset.create();    Multiset<WindowedValue<V>> thatValues = HashMultiset.create();    for (WindowedValue<V> value : myWorkItem.elementsIterable()) {        myValues.add(value);    }    for (WindowedValue<V> value : that.getValue().elementsIterable()) {        thatValues.add(value);    }    try {        return myValues.equals(thatValues) && keyCoder.structuralValue(myWorkItem.key()).equals(keyCoder.structuralValue(that.getValue().key()));    } catch (Exception e) {        return false;    }}
public void beam_f4605_0(Description description)
{    description.appendText("KeyedWorkItem<K, V> containing key ").appendValue(myWorkItem.key()).appendText(" and values ").appendValueList("[", ", ", "]", myWorkItem.elementsIterable());}
public void beam_f4606_0() throws Exception
{    KV<String, Integer> firstFoo = KV.of("foo", -1);    KV<String, Integer> secondFoo = KV.of("foo", 1);    KV<String, Integer> thirdFoo = KV.of("foo", 3);    KV<String, Integer> firstBar = KV.of("bar", 22);    KV<String, Integer> secondBar = KV.of("bar", 12);    KV<String, Integer> firstBaz = KV.of("baz", Integer.MAX_VALUE);    PCollection<KV<String, Integer>> values = p.apply(Create.of(firstFoo, firstBar, secondFoo, firstBaz, secondBar, thirdFoo));    PCollection<KeyedWorkItem<String, Integer>> groupedKvs = values.apply(new DirectGroupByKeyOnly<>());    CommittedBundle<KV<String, Integer>> inputBundle = bundleFactory.createBundle(values).commit(Instant.now());    EvaluationContext evaluationContext = mock(EvaluationContext.class);    StructuralKey<String> fooKey = StructuralKey.of("foo", StringUtf8Coder.of());    UncommittedBundle<KeyedWorkItem<String, Integer>> fooBundle = bundleFactory.createKeyedBundle(fooKey, groupedKvs);    StructuralKey<String> barKey = StructuralKey.of("bar", StringUtf8Coder.of());    UncommittedBundle<KeyedWorkItem<String, Integer>> barBundle = bundleFactory.createKeyedBundle(barKey, groupedKvs);    StructuralKey<String> bazKey = StructuralKey.of("baz", StringUtf8Coder.of());    UncommittedBundle<KeyedWorkItem<String, Integer>> bazBundle = bundleFactory.createKeyedBundle(bazKey, groupedKvs);    when(evaluationContext.createKeyedBundle(fooKey, groupedKvs)).thenReturn(fooBundle);    when(evaluationContext.createKeyedBundle(barKey, groupedKvs)).thenReturn(barBundle);    when(evaluationContext.createKeyedBundle(bazKey, groupedKvs)).thenReturn(bazBundle);        @SuppressWarnings("unchecked")    Coder<String> keyCoder = ((KvCoder<String, Integer>) values.getCoder()).getKeyCoder();    TransformEvaluator<KV<String, Integer>> evaluator = new GroupByKeyOnlyEvaluatorFactory(evaluationContext).forApplication(DirectGraphs.getProducer(groupedKvs), inputBundle);    evaluator.processElement(WindowedValue.valueInGlobalWindow(firstFoo));    evaluator.processElement(WindowedValue.valueInGlobalWindow(secondFoo));    evaluator.processElement(WindowedValue.valueInGlobalWindow(thirdFoo));    evaluator.processElement(WindowedValue.valueInGlobalWindow(firstBar));    evaluator.processElement(WindowedValue.valueInGlobalWindow(secondBar));    evaluator.processElement(WindowedValue.valueInGlobalWindow(firstBaz));    evaluator.finishBundle();    assertThat(fooBundle.commit(Instant.now()).getElements(), contains(new KeyedWorkItemMatcher<>(KeyedWorkItems.elementsWorkItem("foo", ImmutableSet.of(WindowedValue.valueInGlobalWindow(-1), WindowedValue.valueInGlobalWindow(1), WindowedValue.valueInGlobalWindow(3))), keyCoder)));    assertThat(barBundle.commit(Instant.now()).getElements(), contains(new KeyedWorkItemMatcher<>(KeyedWorkItems.elementsWorkItem("bar", ImmutableSet.of(WindowedValue.valueInGlobalWindow(12), WindowedValue.valueInGlobalWindow(22))), keyCoder)));    assertThat(bazBundle.commit(Instant.now()).getElements(), contains(new KeyedWorkItemMatcher<>(KeyedWorkItems.elementsWorkItem("baz", ImmutableSet.of(WindowedValue.valueInGlobalWindow(Integer.MAX_VALUE))), keyCoder)));}
public void beam_f4614_0()
{    UncommittedBundle<byte[]> keyed = factory.createKeyedBundle(StructuralKey.of("mykey", StringUtf8Coder.of()), transformed);    byte[] array = new byte[] { 4, 8, 12 };    array[0] = Byte.MAX_VALUE;    WindowedValue<byte[]> windowedArray = WindowedValue.of(array, new Instant(891L), new IntervalWindow(new Instant(0), new Instant(1000)), PaneInfo.ON_TIME_AND_ONLY_FIRING);    keyed.add(windowedArray);    CommittedBundle<byte[]> committed = keyed.commit(Instant.now());    assertThat(committed.getElements(), containsInAnyOrder(windowedArray));}
public void beam_f4615_0()
{    UncommittedBundle<byte[]> intermediate = factory.createBundle(transformed);    byte[] array = new byte[] { 4, 8, 12 };    WindowedValue<byte[]> windowedArray = WindowedValue.of(array, new Instant(891L), new IntervalWindow(new Instant(0), new Instant(1000)), PaneInfo.ON_TIME_AND_ONLY_FIRING);    array[2] = -3;    intermediate.add(windowedArray);    CommittedBundle<byte[]> committed = intermediate.commit(Instant.now());    assertThat(committed.getElements(), containsInAnyOrder(windowedArray));}
public void beam_f4616_0()
{    UncommittedBundle<byte[]> keyed = factory.createKeyedBundle(StructuralKey.of("mykey", StringUtf8Coder.of()), transformed);    byte[] array = new byte[] { 4, 8, 12 };    WindowedValue<byte[]> windowedArray = WindowedValue.of(array, new Instant(891L), new IntervalWindow(new Instant(0), new Instant(1000)), PaneInfo.ON_TIME_AND_ONLY_FIRING);    keyed.add(windowedArray);    array[0] = Byte.MAX_VALUE;    thrown.expect(IllegalMutationException.class);    thrown.expectMessage("Values must not be mutated in any way after being output");    keyed.commit(Instant.now());}
public void beam_f4624_0()
{    created = p.apply(Create.of(1, 2, 3));    downstream = created.apply(WithKeys.of("foo"));}
private void beam_f4625_0(Coder<T> coder, T key) throws Exception
{    PCollection<Integer> pcollection = p.apply("Create", Create.of(1));    StructuralKey<?> skey = StructuralKey.of(key, coder);    UncommittedBundle<Integer> inFlightBundle = bundleFactory.createKeyedBundle(skey, pcollection);    CommittedBundle<Integer> bundle = inFlightBundle.commit(Instant.now());    assertThat(bundle.getKey(), equalTo(skey));}
public void beam_f4626_0() throws Exception
{    createKeyedBundle(VoidCoder.of(), null);}
public void beam_f4634_0()
{    Instant timestamp = BoundedWindow.TIMESTAMP_MAX_VALUE.plus(Duration.standardMinutes(2));    WindowedValue<Integer> value = WindowedValue.timestampedValueInGlobalWindow(1, timestamp);    UncommittedBundle<Integer> bundle = bundleFactory.createRootBundle();    thrown.expect(IllegalArgumentException.class);    thrown.expectMessage(timestamp.toString());    bundle.add(value);}
public void beam_f4635_0()
{    WindowedValue<Integer> firstValue = WindowedValue.valueInGlobalWindow(1);    WindowedValue<Integer> secondValue = WindowedValue.timestampedValueInGlobalWindow(2, new Instant(1000L));    CommittedBundle<Integer> committed = afterCommitGetElementsShouldHaveAddedElements(ImmutableList.of(firstValue, secondValue));    WindowedValue<Integer> firstReplacement = WindowedValue.of(9, new Instant(2048L), new IntervalWindow(new Instant(2044L), Instant.now()), PaneInfo.NO_FIRING);    WindowedValue<Integer> secondReplacement = WindowedValue.timestampedValueInGlobalWindow(-1, Instant.now());    CommittedBundle<Integer> withed = committed.withElements(ImmutableList.of(firstReplacement, secondReplacement));    assertThat(withed.getElements(), containsInAnyOrder(firstReplacement, secondReplacement));    assertThat(committed.getElements(), containsInAnyOrder(firstValue, secondValue));    assertThat(withed.getKey(), equalTo(committed.getKey()));    assertThat(withed.getPCollection(), equalTo(committed.getPCollection()));    assertThat(withed.getSynchronizedProcessingOutputWatermark(), equalTo(committed.getSynchronizedProcessingOutputWatermark()));    assertThat(withed.getMinimumTimestamp(), equalTo(new Instant(2048L)));}
public void beam_f4636_0()
{    UncommittedBundle<Integer> bundle = bundleFactory.createRootBundle();    bundle.add(WindowedValue.valueInGlobalWindow(1));    CommittedBundle<Integer> firstCommit = bundle.commit(Instant.now());    assertThat(firstCommit.getElements(), containsInAnyOrder(WindowedValue.valueInGlobalWindow(1)));    thrown.expect(IllegalStateException.class);    thrown.expectMessage("3");    thrown.expectMessage("committed");    bundle.add(WindowedValue.valueInGlobalWindow(3));}
public void beam_f4644_0()
{    PCollection<KV<Integer, Iterable<Void>>> unkeyed = p.apply(Create.of(KV.<Integer, Iterable<Void>>of(-1, Collections.emptyList())).withCoder(KvCoder.of(VarIntCoder.of(), IterableCoder.of(VoidCoder.of()))));    p.traverseTopologically(visitor);    assertThat(visitor.getKeyedPValues(), not(hasItem(unkeyed)));}
public void beam_f4645_0()
{    PCollection<KV<String, Iterable<Integer>>> onceKeyed = p.apply(Create.of(KV.of("hello", 42))).apply(GroupByKey.create()).apply(ParDo.of(new IdentityFn<>()));    p.traverseTopologically(visitor);    assertThat(visitor.getKeyedPValues(), not(hasItem(onceKeyed)));}
public void beam_f4646_0()
{    PCollection<KV<String, Iterable<WindowedValue<KV<String, Integer>>>>> input = p.apply(Create.of(KV.of("hello", (Iterable<WindowedValue<KV<String, Integer>>>) Collections.<WindowedValue<KV<String, Integer>>>emptyList())).withCoder(KvCoder.of(StringUtf8Coder.of(), IterableCoder.of(WindowedValue.getValueOnlyCoder(KvCoder.of(StringUtf8Coder.of(), VarIntCoder.of()))))));    PCollection<KeyedWorkItem<String, KV<String, Integer>>> unkeyed = input.apply(ParDo.of(new ParDoMultiOverrideFactory.ToKeyedWorkItem<>())).setCoder(KeyedWorkItemCoder.of(StringUtf8Coder.of(), KvCoder.of(StringUtf8Coder.of(), VarIntCoder.of()), GlobalWindow.Coder.INSTANCE));    p.traverseTopologically(visitor);    assertThat(visitor.getKeyedPValues(), not(hasItem(unkeyed)));}
public Instant beam_f4654_0()
{    return now;}
public void beam_f4655_0()
{    PCollection<KV<String, Long>> combined = pipeline.apply(Create.of(KV.of("foo", 1L), KV.of("bar", 2L), KV.of("bizzle", 3L), KV.of("bar", 4L), KV.of("bizzle", 11L))).apply(Combine.perKey(new MultiStepCombineFn()));    PAssert.that(combined).containsInAnyOrder(KV.of("foo", 1L), KV.of("bar", 6L), KV.of("bizzle", 14L));    pipeline.run();}
public void beam_f4656_0()
{    SlidingWindows windowFn = SlidingWindows.of(Duration.millis(6L)).every(Duration.millis(3L));    PCollection<KV<String, Long>> combined = pipeline.apply(Create.timestamped(TimestampedValue.of(KV.of("foo", 1L), new Instant(1L)), TimestampedValue.of(KV.of("bar", 2L), new Instant(2L)), TimestampedValue.of(KV.of("bizzle", 3L), new Instant(3L)), TimestampedValue.of(KV.of("bar", 4L), new Instant(4L)), TimestampedValue.of(KV.of("bizzle", 11L), new Instant(11L)))).apply(Window.into(windowFn)).apply(Combine.perKey(new MultiStepCombineFn()));    PAssert.that("Windows should combine only elements in their windows", combined).inWindow(new IntervalWindow(new Instant(0L), Duration.millis(6L))).containsInAnyOrder(KV.of("foo", 1L), KV.of("bar", 6L), KV.of("bizzle", 3L));    PAssert.that("Elements should appear in all the windows they are assigned to", combined).inWindow(new IntervalWindow(new Instant(-3L), Duration.millis(6L))).containsInAnyOrder(KV.of("foo", 1L), KV.of("bar", 2L));    PAssert.that(combined).inWindow(new IntervalWindow(new Instant(6L), Duration.millis(6L))).containsInAnyOrder(KV.of("bizzle", 11L));    PAssert.that(combined).containsInAnyOrder(KV.of("foo", 1L), KV.of("foo", 1L), KV.of("bar", 6L), KV.of("bar", 2L), KV.of("bar", 4L), KV.of("bizzle", 11L), KV.of("bizzle", 11L), KV.of("bizzle", 3L), KV.of("bizzle", 3L));    pipeline.run();}
private static MultiStepAccumulator beam_f4664_0(long value, boolean deserialized)
{    return new AutoValue_MultiStepCombineTest_MultiStepAccumulator(value, deserialized);}
 MultiStepAccumulator beam_f4665_0(MultiStepAccumulator other)
{    return MultiStepAccumulator.of(this.getValue() + other.getValue(), this.isDeserialized() || other.isDeserialized());}
public void beam_f4666_0(MultiStepAccumulator value, OutputStream outStream) throws CoderException, IOException
{    VarInt.encode(value.getValue(), outStream);}
public boolean beam_f4674_0()
{    return false;}
public boolean beam_f4675_0(PCollectionView<?> view, BoundedWindow window)
{    return window.equals(GlobalWindow.INSTANCE);}
public Instant beam_f4676_0()
{    return new Instant(789541L);}
public void beam_f4684_0()
{    PCollection<KV<String, String>> input = pipeline.apply(Create.empty(new TypeDescriptor<KV<String, String>>() {    }));    PCollectionView<Map<String, Iterable<String>>> newView = input.apply(View.asMultimap());    thrown.expect(IllegalArgumentException.class);    thrown.expectMessage("unknown views");    thrown.expectMessage(newView.toString());    container.createReaderForViews(ImmutableList.of(newView));}
public void beam_f4685_0()
{    thrown.expect(IllegalArgumentException.class);    thrown.expectMessage("unknown view: " + iterableView.toString());    container.createReaderForViews(ImmutableList.of(mapView)).get(iterableView, GlobalWindow.INSTANCE);}
public void beam_f4686_0() throws Exception
{    ImmutableList.Builder<WindowedValue<?>> valuesBuilder = ImmutableList.builder();    for (Object materializedValue : materializeValuesFor(View.asSingleton(), 2.875)) {        valuesBuilder.add(WindowedValue.of(materializedValue, FIRST_WINDOW.maxTimestamp().minus(200L), FIRST_WINDOW, PaneInfo.ON_TIME_AND_ONLY_FIRING));    }    for (Object materializedValue : materializeValuesFor(View.asSingleton(), 4.125)) {        valuesBuilder.add(WindowedValue.of(materializedValue, SECOND_WINDOW.maxTimestamp().minus(2_000_000L), SECOND_WINDOW, PaneInfo.ON_TIME_AND_ONLY_FIRING));    }    container.write(singletonView, valuesBuilder.build());    assertThat(container.createReaderForViews(ImmutableList.of(singletonView)).get(singletonView, FIRST_WINDOW), equalTo(2.875));    assertThat(container.createReaderForViews(ImmutableList.of(singletonView)).get(singletonView, SECOND_WINDOW), equalTo(4.125));}
private void beam_f4694_0(PCollectionView<?> view, BoundedWindow window)
{    doAnswer(invocation -> {        Object callback = invocation.getArguments()[3];        Runnable callbackRunnable = (Runnable) callback;        callbackRunnable.run();        return null;    }).when(context).scheduleAfterOutputWouldBeProduced(Mockito.eq(view), Mockito.eq(window), Mockito.eq(view.getWindowingStrategyInternal()), Mockito.any(Runnable.class));}
private CountDownLatch beam_f4695_0(PCollectionView<?> view, BoundedWindow window, final CountDownLatch onComplete)
{    final CountDownLatch runLatch = new CountDownLatch(1);    doAnswer(invocation -> {        Object callback = invocation.getArguments()[3];        final Runnable callbackRunnable = (Runnable) callback;        ListenableFuture<?> result = MoreExecutors.listeningDecorator(Executors.newSingleThreadExecutor()).submit(() -> {            try {                if (!runLatch.await(1500L, TimeUnit.MILLISECONDS)) {                    fail("Run latch didn't count down within timeout");                }                callbackRunnable.run();                onComplete.countDown();            } catch (InterruptedException e) {                throw new AssertionError("Unexpectedly interrupted while waiting for latch ", e);            }        });        return null;    }).when(context).scheduleAfterOutputWouldBeProduced(Mockito.eq(view), Mockito.eq(window), Mockito.eq(view.getWindowingStrategyInternal()), Mockito.any(Runnable.class));    return runLatch;}
private Future<ValueT> beam_f4696_0(final SideInputReader myReader, final PCollectionView<ValueT> view, final BoundedWindow window)
{    Callable<ValueT> callable = () -> myReader.get(view, window);    return Executors.newSingleThreadExecutor().submit(callable);}
public void beam_f4706_0()
{    TransformResult<Integer> result = StepTransformResult.<Integer>withoutHold(transform).build();    assertThat(result.getOutputTypes(), emptyIterable());}
public void beam_f4707_0()
{    context = mock(EvaluationContext.class);    runner = DirectRunner.fromOptions(TestPipeline.testingPipelineOptions());    factory = new TestStreamEvaluatorFactory(context);    bundleFactory = ImmutableListBundleFactory.create();}
public void beam_f4708_0() throws Exception
{    TestStream<Integer> testStream = TestStream.create(VarIntCoder.of()).addElements(1, 2, 3).advanceWatermarkTo(new Instant(0)).addElements(TimestampedValue.atMinimumTimestamp(4), TimestampedValue.atMinimumTimestamp(5), TimestampedValue.atMinimumTimestamp(6)).advanceProcessingTime(Duration.standardMinutes(10)).advanceWatermarkToInfinity();    PCollection<Integer> streamVals = p.apply(new DirectTestStream<>(runner, testStream));    TestClock clock = new TestClock();    when(context.getClock()).thenReturn(clock);    when(context.createRootBundle()).thenReturn(bundleFactory.createRootBundle());    when(context.createBundle(streamVals)).thenReturn(bundleFactory.createBundle(streamVals), bundleFactory.createBundle(streamVals));    AppliedPTransform<?, ?, ?> streamProducer = DirectGraphs.getProducer(streamVals);    Collection<CommittedBundle<?>> initialInputs = new TestStreamEvaluatorFactory.InputProvider(context).getInitialInputs(streamProducer, 1);    @SuppressWarnings("unchecked")    CommittedBundle<TestStreamIndex<Integer>> initialBundle = (CommittedBundle<TestStreamIndex<Integer>>) Iterables.getOnlyElement(initialInputs);    TransformEvaluator<TestStreamIndex<Integer>> firstEvaluator = factory.forApplication(streamProducer, initialBundle);    firstEvaluator.processElement(Iterables.getOnlyElement(initialBundle.getElements()));    TransformResult<TestStreamIndex<Integer>> firstResult = firstEvaluator.finishBundle();    WindowedValue<TestStreamIndex<Integer>> firstResidual = (WindowedValue<TestStreamIndex<Integer>>) Iterables.getOnlyElement(firstResult.getUnprocessedElements());    assertThat(firstResidual.getValue().getIndex(), equalTo(1));    assertThat(firstResidual.getTimestamp(), equalTo(BoundedWindow.TIMESTAMP_MIN_VALUE));    CommittedBundle<TestStreamIndex<Integer>> secondBundle = initialBundle.withElements(Collections.singleton(firstResidual));    TransformEvaluator<TestStreamIndex<Integer>> secondEvaluator = factory.forApplication(streamProducer, secondBundle);    secondEvaluator.processElement(firstResidual);    TransformResult<TestStreamIndex<Integer>> secondResult = secondEvaluator.finishBundle();    WindowedValue<TestStreamIndex<Integer>> secondResidual = (WindowedValue<TestStreamIndex<Integer>>) Iterables.getOnlyElement(secondResult.getUnprocessedElements());    assertThat(secondResidual.getValue().getIndex(), equalTo(2));    assertThat(secondResidual.getTimestamp(), equalTo(new Instant(0)));    CommittedBundle<TestStreamIndex<Integer>> thirdBundle = secondBundle.withElements(Collections.singleton(secondResidual));    TransformEvaluator<TestStreamIndex<Integer>> thirdEvaluator = factory.forApplication(streamProducer, thirdBundle);    thirdEvaluator.processElement(secondResidual);    TransformResult<TestStreamIndex<Integer>> thirdResult = thirdEvaluator.finishBundle();    WindowedValue<TestStreamIndex<Integer>> thirdResidual = (WindowedValue<TestStreamIndex<Integer>>) Iterables.getOnlyElement(thirdResult.getUnprocessedElements());    assertThat(thirdResidual.getValue().getIndex(), equalTo(3));    assertThat(thirdResidual.getTimestamp(), equalTo(new Instant(0)));    Instant start = clock.now();    CommittedBundle<TestStreamIndex<Integer>> fourthBundle = thirdBundle.withElements(Collections.singleton(thirdResidual));    TransformEvaluator<TestStreamIndex<Integer>> fourthEvaluator = factory.forApplication(streamProducer, fourthBundle);    fourthEvaluator.processElement(thirdResidual);    TransformResult<TestStreamIndex<Integer>> fourthResult = fourthEvaluator.finishBundle();    assertThat(clock.now(), equalTo(start.plus(Duration.standardMinutes(10))));    WindowedValue<TestStreamIndex<Integer>> fourthResidual = (WindowedValue<TestStreamIndex<Integer>>) Iterables.getOnlyElement(fourthResult.getUnprocessedElements());    assertThat(fourthResidual.getValue().getIndex(), equalTo(4));    assertThat(fourthResidual.getTimestamp(), equalTo(new Instant(0)));    CommittedBundle<TestStreamIndex<Integer>> fifthBundle = thirdBundle.withElements(Collections.singleton(fourthResidual));    TransformEvaluator<TestStreamIndex<Integer>> fifthEvaluator = factory.forApplication(streamProducer, fifthBundle);    fifthEvaluator.processElement(fourthResidual);    TransformResult<TestStreamIndex<Integer>> fifthResult = fifthEvaluator.finishBundle();    assertThat(Iterables.getOnlyElement(firstResult.getOutputBundles()).commit(Instant.now()).getElements(), Matchers.containsInAnyOrder(WindowedValue.valueInGlobalWindow(1), WindowedValue.valueInGlobalWindow(2), WindowedValue.valueInGlobalWindow(3)));    assertThat(Iterables.getOnlyElement(thirdResult.getOutputBundles()).commit(Instant.now()).getElements(), Matchers.containsInAnyOrder(WindowedValue.valueInGlobalWindow(4), WindowedValue.valueInGlobalWindow(5), WindowedValue.valueInGlobalWindow(6)));    assertThat(fifthResult.getOutputBundles(), Matchers.emptyIterable());    assertThat(fifthResult.getWatermarkHold(), equalTo(BoundedWindow.TIMESTAMP_MAX_VALUE));    assertThat(fifthResult.getUnprocessedElements(), Matchers.emptyIterable());}
public void beam_f4716_0()
{    byte[] id = new byte[] { -1, 2, 4, 22 };    UnboundedReadDeduplicator dedupper = NeverDeduplicator.create();    assertThat(dedupper.shouldOutput(id), is(true));    assertThat(dedupper.shouldOutput(id), is(true));}
public void beam_f4717_0()
{    byte[] id = new byte[] { -1, 2, 4, 22 };    UnboundedReadDeduplicator dedupper = CachedIdDeduplicator.create();    assertThat(dedupper.shouldOutput(id), is(true));    assertThat(dedupper.shouldOutput(id), is(false));}
public void beam_f4718_0() throws InterruptedException, ExecutionException
{    byte[] id = new byte[] { -1, 2, 4, 22 };    UnboundedReadDeduplicator dedupper = CachedIdDeduplicator.create();    final CountDownLatch startSignal = new CountDownLatch(1);    int numThreads = 50;    final CountDownLatch readyLatch = new CountDownLatch(numThreads);    final CountDownLatch finishLine = new CountDownLatch(numThreads);    ListeningExecutorService executor = MoreExecutors.listeningDecorator(Executors.newCachedThreadPool());    AtomicInteger successCount = new AtomicInteger();    AtomicInteger noOutputCount = new AtomicInteger();    List<ListenableFuture<?>> futures = new ArrayList<>();    for (int i = 0; i < numThreads; i++) {        futures.add(executor.submit(new TryOutputIdRunnable(dedupper, id, successCount, noOutputCount, readyLatch, startSignal, finishLine)));    }    readyLatch.await();    startSignal.countDown();    finishLine.await(10L, TimeUnit.SECONDS);    Futures.allAsList(futures).get();    executor.shutdownNow();        assertThat(successCount.get(), equalTo(1));        assertThat(noOutputCount.get(), lessThan(numThreads));}
public void beam_f4726_0() throws Exception
{    ContiguousSet<Long> elems = ContiguousSet.create(Range.closed(0L, 20L), DiscreteDomain.longs());    TestUnboundedSource<Long> source = new TestUnboundedSource<>(BigEndianLongCoder.of(), elems.toArray(new Long[0]));    PCollection<Long> pcollection = p.apply(Read.from(source));    AppliedPTransform<?, ?, ?> sourceTransform = DirectGraphs.getGraph(p).getProducer(pcollection);    when(context.createRootBundle()).thenReturn(bundleFactory.createRootBundle());    UncommittedBundle<Long> output = bundleFactory.createBundle(pcollection);    when(context.createBundle(pcollection)).thenReturn(output);    WindowedValue<UnboundedSourceShard<Long, TestCheckpointMark>> shard = WindowedValue.valueInGlobalWindow(UnboundedSourceShard.unstarted(source, NeverDeduplicator.create()));    CommittedBundle<UnboundedSourceShard<Long, TestCheckpointMark>> inputBundle = bundleFactory.<UnboundedSourceShard<Long, TestCheckpointMark>>createRootBundle().add(shard).commit(Instant.now());    UnboundedReadEvaluatorFactory factory = new UnboundedReadEvaluatorFactory(context, options, 0.0);    TransformEvaluator<UnboundedSourceShard<Long, TestCheckpointMark>> evaluator = factory.forApplication(sourceTransform, inputBundle);    evaluator.processElement(shard);    TransformResult<UnboundedSourceShard<Long, TestCheckpointMark>> result = evaluator.finishBundle();    CommittedBundle<UnboundedSourceShard<Long, TestCheckpointMark>> residual = inputBundle.withElements((Iterable<WindowedValue<UnboundedSourceShard<Long, TestCheckpointMark>>>) result.getUnprocessedElements());    TransformEvaluator<UnboundedSourceShard<Long, TestCheckpointMark>> secondEvaluator = factory.forApplication(sourceTransform, residual);    secondEvaluator.processElement(Iterables.getOnlyElement(residual.getElements()));    secondEvaluator.finishBundle();    assertThat(TestUnboundedSource.readerClosedCount, equalTo(2));    assertThat(Iterables.getOnlyElement(residual.getElements()).getValue().getCheckpoint().isFinalized(), is(true));}
public void beam_f4727_0() throws Exception
{    ContiguousSet<Long> elems = ContiguousSet.create(Range.closed(0L, 20L), DiscreteDomain.longs());    TestUnboundedSource<Long> source = new TestUnboundedSource<>(BigEndianLongCoder.of(), elems.toArray(new Long[0])).throwsOnClose();    PCollection<Long> pcollection = p.apply(Read.from(source));    AppliedPTransform<?, ?, ?> sourceTransform = DirectGraphs.getGraph(p).getProducer(pcollection);    when(context.createRootBundle()).thenReturn(bundleFactory.createRootBundle());    UncommittedBundle<Long> output = bundleFactory.createBundle(pcollection);    when(context.createBundle(pcollection)).thenReturn(output);    WindowedValue<UnboundedSourceShard<Long, TestCheckpointMark>> shard = WindowedValue.valueInGlobalWindow(UnboundedSourceShard.unstarted(source, NeverDeduplicator.create()));    CommittedBundle<UnboundedSourceShard<Long, TestCheckpointMark>> inputBundle = bundleFactory.<UnboundedSourceShard<Long, TestCheckpointMark>>createRootBundle().add(shard).commit(Instant.now());    UnboundedReadEvaluatorFactory factory = new UnboundedReadEvaluatorFactory(context, options, 0.0);    TransformEvaluator<UnboundedSourceShard<Long, TestCheckpointMark>> evaluator = factory.forApplication(sourceTransform, inputBundle);    thrown.expect(IOException.class);    thrown.expectMessage("throws on close");    evaluator.processElement(shard);}
public void beam_f4728_0() throws Exception
{    TestUnboundedSource.readerClosedCount = 0;    final TestUnboundedSource<String> source = new TestUnboundedSource<>(StringUtf8Coder.of());    source.advanceWatermarkToInfinity = true;    processElement(source);    assertEquals(1, TestUnboundedSource.readerClosedCount);        TestUnboundedSource.readerClosedCount = 0;}
public boolean beam_f4736_0()
{    return dedupes;}
public Coder<T> beam_f4737_0()
{    return coder;}
public TestUnboundedSource<T> beam_f4738_0()
{    return new TestUnboundedSource<>(coder, true, elems);}
public byte[] beam_f4746_0()
{    try {        return CoderUtils.encodeToByteArray(coder, getCurrent());    } catch (CoderException e) {        throw new RuntimeException(e);    }}
public void beam_f4747_0() throws IOException
{    try {        readerClosedCount++;                assertThat(closed, is(false));        if (throwOnClose) {            throw new IOException(String.format("%s throws on close", TestUnboundedSource.this));        }    } finally {        closed = true;    }}
public void beam_f4748_0() throws IOException
{    checkState(!finalized, "%s was finalized more than once", TestCheckpointMark.class.getSimpleName());    checkState(!decoded, "%s was finalized after being decoded", TestCheckpointMark.class.getSimpleName());    finalized = true;}
public void beam_f4756_0()
{    PCollection<Integer> created = p.apply(Create.of(1, 2, 3));    PCollection<Integer> summed = created.apply(Sum.integersGlobally());    DirectGraphs.performDirectOverrides(p);    DirectGraph graph = DirectGraphs.getGraph(p);    create = graph.getProducer(created);    sum = graph.getProducer(summed);}
public void beam_f4757_0() throws Exception
{    CountDownLatch latch = new CountDownLatch(1);    executor.callOnGuaranteedFiring(create, GlobalWindow.INSTANCE, WindowingStrategy.globalDefault(), new CountDownLatchCallback(latch));    executor.fireForWatermark(create, BoundedWindow.TIMESTAMP_MAX_VALUE);    assertThat(latch.await(500, TimeUnit.MILLISECONDS), equalTo(true));}
public void beam_f4758_0() throws Exception
{    CountDownLatch latch = new CountDownLatch(2);    WindowFn<Object, IntervalWindow> windowFn = FixedWindows.of(Duration.standardMinutes(10));    IntervalWindow window = new IntervalWindow(new Instant(0L), new Instant(0L).plus(Duration.standardMinutes(10)));    executor.callOnGuaranteedFiring(create, window, WindowingStrategy.of(windowFn), new CountDownLatchCallback(latch));    executor.callOnGuaranteedFiring(create, window, WindowingStrategy.of(windowFn), new CountDownLatchCallback(latch));    executor.fireForWatermark(create, new Instant(0L).plus(Duration.standardMinutes(10)));    assertThat(latch.await(500, TimeUnit.MILLISECONDS), equalTo(true));}
public void beam_f4766_0()
{    CommittedBundle<Integer> secondPcollectionBundle = multiWindowedBundle(intsToFlatten, -1);    manager.updateWatermarks(null, TimerUpdate.empty(), graph.getProducer(intsToFlatten), null, Collections.<CommittedBundle<?>>singleton(secondPcollectionBundle), BoundedWindow.TIMESTAMP_MAX_VALUE);    manager.refreshAll();        TransformWatermarks firstSourceWatermark = manager.getWatermarks(graph.getProducer(createdInts));    assertThat(firstSourceWatermark.getOutputWatermark(), not(greaterThan(BoundedWindow.TIMESTAMP_MIN_VALUE)));            TransformWatermarks secondSourceWatermark = manager.getWatermarks(graph.getProducer(intsToFlatten));    assertThat(secondSourceWatermark.getOutputWatermark(), not(lessThan(BoundedWindow.TIMESTAMP_MAX_VALUE)));        TransformWatermarks transformWatermark = manager.getWatermarks(graph.getProducer(flattened));    assertThat(transformWatermark.getInputWatermark(), not(greaterThan(BoundedWindow.TIMESTAMP_MIN_VALUE)));    assertThat(transformWatermark.getOutputWatermark(), not(greaterThan(BoundedWindow.TIMESTAMP_MIN_VALUE)));    CommittedBundle<Integer> flattenedBundleSecondCreate = multiWindowedBundle(flattened, -1);            manager.updateWatermarks(secondPcollectionBundle, TimerUpdate.empty(), graph.getProducer(flattened), secondPcollectionBundle.withElements(Collections.emptyList()), Collections.<CommittedBundle<?>>singleton(flattenedBundleSecondCreate), BoundedWindow.TIMESTAMP_MAX_VALUE);    TransformWatermarks transformAfterProcessing = manager.getWatermarks(graph.getProducer(flattened));    manager.updateWatermarks(secondPcollectionBundle, TimerUpdate.empty(), graph.getProducer(flattened), secondPcollectionBundle.withElements(Collections.emptyList()), Collections.<CommittedBundle<?>>singleton(flattenedBundleSecondCreate), BoundedWindow.TIMESTAMP_MAX_VALUE);    manager.refreshAll();    assertThat(transformAfterProcessing.getInputWatermark(), not(greaterThan(BoundedWindow.TIMESTAMP_MIN_VALUE)));    assertThat(transformAfterProcessing.getOutputWatermark(), not(greaterThan(BoundedWindow.TIMESTAMP_MIN_VALUE)));    Instant firstCollectionTimestamp = new Instant(10000);    CommittedBundle<Integer> firstPcollectionBundle = timestampedBundle(createdInts, TimestampedValue.of(5, firstCollectionTimestamp));            manager.updateWatermarks(null, TimerUpdate.empty(), graph.getProducer(createdInts), null, Collections.<CommittedBundle<?>>singleton(firstPcollectionBundle), BoundedWindow.TIMESTAMP_MAX_VALUE);    manager.refreshAll();    TransformWatermarks firstSourceWatermarks = manager.getWatermarks(graph.getProducer(createdInts));    assertThat(firstSourceWatermarks.getOutputWatermark(), not(lessThan(BoundedWindow.TIMESTAMP_MAX_VALUE)));            TransformWatermarks flattenAfterSourcesProduced = manager.getWatermarks(graph.getProducer(flattened));    assertThat(flattenAfterSourcesProduced.getInputWatermark(), not(greaterThan(firstCollectionTimestamp)));    assertThat(flattenAfterSourcesProduced.getOutputWatermark(), not(greaterThan(firstCollectionTimestamp)));                TransformWatermarks withBufferedElements = manager.getWatermarks(graph.getProducer(flattened));    assertThat(withBufferedElements.getInputWatermark(), equalTo(firstCollectionTimestamp));    assertThat(withBufferedElements.getOutputWatermark(), equalTo(firstCollectionTimestamp));    CommittedBundle<?> completedFlattenBundle = bundleFactory.createBundle(flattened).commit(BoundedWindow.TIMESTAMP_MAX_VALUE);    manager.updateWatermarks(firstPcollectionBundle, TimerUpdate.empty(), graph.getProducer(flattened), firstPcollectionBundle.withElements(Collections.emptyList()), Collections.<CommittedBundle<?>>singleton(completedFlattenBundle), BoundedWindow.TIMESTAMP_MAX_VALUE);    manager.refreshAll();    TransformWatermarks afterConsumingAllInput = manager.getWatermarks(graph.getProducer(flattened));    assertThat(afterConsumingAllInput.getInputWatermark(), not(lessThan(BoundedWindow.TIMESTAMP_MAX_VALUE)));    assertThat(afterConsumingAllInput.getOutputWatermark(), not(greaterThan(BoundedWindow.TIMESTAMP_MAX_VALUE)));}
public void beam_f4767_0()
{    PCollection<Integer> created = p.apply(Create.of(1, 2, 3));    PCollection<Integer> multiConsumer = PCollectionList.of(created).and(created).apply(Flatten.pCollections());    DirectGraphVisitor graphVisitor = new DirectGraphVisitor();    p.traverseTopologically(graphVisitor);    DirectGraph graph = graphVisitor.getGraph();    AppliedPTransform<?, ?, ?> theFlatten = graph.getProducer(multiConsumer);    WatermarkManager<AppliedPTransform<?, ?, ?>, ? super PCollection<?>> tstMgr = WatermarkManager.create(clock, graph, AppliedPTransform::getFullName);    CommittedBundle<Void> root = bundleFactory.<Void>createRootBundle().add(WindowedValue.valueInGlobalWindow(null)).commit(clock.now());    CommittedBundle<Integer> createBundle = bundleFactory.createBundle(created).add(WindowedValue.timestampedValueInGlobalWindow(1, new Instant(33536))).commit(clock.now());    Map<AppliedPTransform<?, ?, ?>, Collection<CommittedBundle<?>>> initialInputs = ImmutableMap.<AppliedPTransform<?, ?, ?>, Collection<CommittedBundle<?>>>builder().put(graph.getProducer(created), Collections.singleton(root)).build();    tstMgr.initialize((Map) initialInputs);    tstMgr.updateWatermarks(root, TimerUpdate.empty(), graph.getProducer(created), null, Collections.singleton(createBundle), BoundedWindow.TIMESTAMP_MAX_VALUE);    tstMgr.refreshAll();    TransformWatermarks flattenWms = tstMgr.getWatermarks(theFlatten);    assertThat(flattenWms.getInputWatermark(), equalTo(new Instant(33536)));    tstMgr.updateWatermarks(createBundle, TimerUpdate.empty(), theFlatten, null, Collections.emptyList(), BoundedWindow.TIMESTAMP_MAX_VALUE);    tstMgr.refreshAll();    assertThat(flattenWms.getInputWatermark(), equalTo(new Instant(33536)));    tstMgr.updateWatermarks(createBundle, TimerUpdate.empty(), theFlatten, null, Collections.emptyList(), BoundedWindow.TIMESTAMP_MAX_VALUE);    tstMgr.refreshAll();    assertThat(flattenWms.getInputWatermark(), equalTo(BoundedWindow.TIMESTAMP_MAX_VALUE));}
public void beam_f4768_0()
{    CommittedBundle<Integer> createdBundle = timestampedBundle(createdInts, TimestampedValue.of(1, new Instant(1_000_000L)), TimestampedValue.of(2, new Instant(1234L)), TimestampedValue.of(3, new Instant(-1000L)));    manager.updateWatermarks(null, TimerUpdate.empty(), graph.getProducer(createdInts), null, Collections.<CommittedBundle<?>>singleton(createdBundle), new Instant(Long.MAX_VALUE));    manager.refreshAll();    TransformWatermarks createdAfterProducing = manager.getWatermarks(graph.getProducer(createdInts));    assertThat(createdAfterProducing.getOutputWatermark(), not(lessThan(BoundedWindow.TIMESTAMP_MAX_VALUE)));    CommittedBundle<KV<String, Integer>> keyBundle = timestampedBundle(keyed, TimestampedValue.of(KV.of("MyKey", 1), new Instant(1_000_000L)), TimestampedValue.of(KV.of("MyKey", 2), new Instant(1234L)), TimestampedValue.of(KV.of("MyKey", 3), new Instant(-1000L)));    manager.updateWatermarks(createdBundle, TimerUpdate.empty(), graph.getProducer(keyed), createdBundle.withElements(Collections.emptyList()), Collections.<CommittedBundle<?>>singleton(keyBundle), BoundedWindow.TIMESTAMP_MAX_VALUE);    manager.refreshAll();    TransformWatermarks keyedWatermarks = manager.getWatermarks(graph.getProducer(keyed));    assertThat(keyedWatermarks.getInputWatermark(), not(lessThan(BoundedWindow.TIMESTAMP_MAX_VALUE)));    assertThat(keyedWatermarks.getOutputWatermark(), not(lessThan(BoundedWindow.TIMESTAMP_MAX_VALUE)));    TransformWatermarks filteredWatermarks = manager.getWatermarks(graph.getProducer(filtered));    assertThat(filteredWatermarks.getInputWatermark(), not(greaterThan(new Instant(-1000L))));    assertThat(filteredWatermarks.getOutputWatermark(), not(greaterThan(new Instant(-1000L))));    CommittedBundle<Integer> filteredBundle = timestampedBundle(filtered, TimestampedValue.of(2, new Instant(1234L)));    manager.updateWatermarks(createdBundle, TimerUpdate.empty(), graph.getProducer(filtered), createdBundle.withElements(Collections.emptyList()), Collections.<CommittedBundle<?>>singleton(filteredBundle), BoundedWindow.TIMESTAMP_MAX_VALUE);    manager.refreshAll();    TransformWatermarks filteredProcessedWatermarks = manager.getWatermarks(graph.getProducer(filtered));    assertThat(filteredProcessedWatermarks.getInputWatermark(), not(lessThan(BoundedWindow.TIMESTAMP_MAX_VALUE)));    assertThat(filteredProcessedWatermarks.getOutputWatermark(), not(lessThan(BoundedWindow.TIMESTAMP_MAX_VALUE)));}
public void beam_f4776_0()
{    manager.updateWatermarks(null, TimerUpdate.empty(), graph.getProducer(createdInts), null, Collections.<CommittedBundle<?>>singleton(bundleFactory.createBundle(createdInts).add(WindowedValue.valueInGlobalWindow(1)).commit(Instant.now())), BoundedWindow.TIMESTAMP_MAX_VALUE);    CommittedBundle<Integer> createdBundle = bundleFactory.createBundle(createdInts).add(WindowedValue.valueInGlobalWindow(1)).commit(Instant.now());    manager.updateWatermarks(createdBundle, TimerUpdate.empty(), graph.getProducer(keyed), createdBundle.withElements(Collections.emptyList()), Collections.emptyList(), null);    manager.refreshAll();    TransformWatermarks onTimeWatermarks = manager.getWatermarks(graph.getProducer(keyed));    assertThat(onTimeWatermarks.getInputWatermark(), equalTo(BoundedWindow.TIMESTAMP_MAX_VALUE));}
public void beam_f4777_0()
{    CommittedBundle<Integer> emptyCreateOutput = multiWindowedBundle(createdInts);    manager.updateWatermarks(null, TimerUpdate.empty(), graph.getProducer(createdInts), null, Collections.<CommittedBundle<?>>singleton(emptyCreateOutput), BoundedWindow.TIMESTAMP_MAX_VALUE);    manager.refreshAll();    TransformWatermarks updatedSourceWatermarks = manager.getWatermarks(graph.getProducer(createdInts));    assertThat(updatedSourceWatermarks.getOutputWatermark(), not(lessThan(BoundedWindow.TIMESTAMP_MAX_VALUE)));    TransformWatermarks finishedFilterWatermarks = manager.getWatermarks(graph.getProducer(filtered));    assertThat(finishedFilterWatermarks.getInputWatermark(), not(lessThan(BoundedWindow.TIMESTAMP_MAX_VALUE)));    assertThat(finishedFilterWatermarks.getOutputWatermark(), not(lessThan(BoundedWindow.TIMESTAMP_MAX_VALUE)));}
public void beam_f4778_0()
{    CommittedBundle<Integer> firstCreateOutput = multiWindowedBundle(createdInts, 1, 2);    manager.updateWatermarks(null, TimerUpdate.empty(), graph.getProducer(createdInts), null, Collections.<CommittedBundle<?>>singleton(firstCreateOutput), new Instant(12_000L));    CommittedBundle<Integer> firstFilterOutput = multiWindowedBundle(filtered);    manager.updateWatermarks(firstCreateOutput, TimerUpdate.empty(), graph.getProducer(filtered), firstCreateOutput.withElements(Collections.emptyList()), Collections.<CommittedBundle<?>>singleton(firstFilterOutput), new Instant(10_000L));    manager.refreshAll();    TransformWatermarks firstFilterWatermarks = manager.getWatermarks(graph.getProducer(filtered));    assertThat(firstFilterWatermarks.getInputWatermark(), not(lessThan(new Instant(12_000L))));    assertThat(firstFilterWatermarks.getOutputWatermark(), not(greaterThan(new Instant(10_000L))));    CommittedBundle<Integer> emptyCreateOutput = multiWindowedBundle(createdInts);    manager.updateWatermarks(null, TimerUpdate.empty(), graph.getProducer(createdInts), null, Collections.<CommittedBundle<?>>singleton(emptyCreateOutput), BoundedWindow.TIMESTAMP_MAX_VALUE);    manager.refreshAll();    TransformWatermarks updatedSourceWatermarks = manager.getWatermarks(graph.getProducer(createdInts));    assertThat(updatedSourceWatermarks.getOutputWatermark(), not(lessThan(BoundedWindow.TIMESTAMP_MAX_VALUE)));    TransformWatermarks finishedFilterWatermarks = manager.getWatermarks(graph.getProducer(filtered));    assertThat(finishedFilterWatermarks.getInputWatermark(), not(lessThan(BoundedWindow.TIMESTAMP_MAX_VALUE)));    assertThat(finishedFilterWatermarks.getOutputWatermark(), not(greaterThan(new Instant(10_000L))));}
public void beam_f4786_0()
{    Collection<FiredTimers<AppliedPTransform<?, ?, ?>>> initialTimers = manager.extractFiredTimers();        assertThat(initialTimers, emptyIterable());        CommittedBundle<Integer> createdBundle = multiWindowedBundle(filtered);    manager.updateWatermarks(null, TimerUpdate.empty(), graph.getProducer(createdInts), null, Collections.singleton(createdBundle), new Instant(1500L));    TimerData earliestTimer = TimerData.of(StateNamespaces.global(), new Instant(999L), TimeDomain.SYNCHRONIZED_PROCESSING_TIME);    TimerData middleTimer = TimerData.of(StateNamespaces.global(), new Instant(5000L), TimeDomain.SYNCHRONIZED_PROCESSING_TIME);    TimerData lastTimer = TimerData.of(StateNamespaces.global(), new Instant(10000L), TimeDomain.SYNCHRONIZED_PROCESSING_TIME);    StructuralKey<byte[]> key = StructuralKey.of(new byte[] { 2, -2, 22 }, ByteArrayCoder.of());    TimerUpdate update = TimerUpdate.builder(key).setTimer(lastTimer).setTimer(earliestTimer).setTimer(middleTimer).build();    manager.updateWatermarks(createdBundle, update, graph.getProducer(filtered), createdBundle.withElements(Collections.emptyList()), Collections.<CommittedBundle<?>>singleton(multiWindowedBundle(intsToFlatten)), new Instant(1000L));    manager.refreshAll();    Collection<FiredTimers<AppliedPTransform<?, ?, ?>>> firstFiredTimers = manager.extractFiredTimers();    assertThat(firstFiredTimers, not(emptyIterable()));    FiredTimers<AppliedPTransform<?, ?, ?>> firstFired = Iterables.getOnlyElement(firstFiredTimers);    assertThat(firstFired.getTimers(), contains(earliestTimer));    clock.set(new Instant(50_000L));    manager.updateWatermarks(null, TimerUpdate.empty(), graph.getProducer(createdInts), null, Collections.emptyList(), new Instant(50_000L));    manager.refreshAll();    Collection<FiredTimers<AppliedPTransform<?, ?, ?>>> secondFiredTimers = manager.extractFiredTimers();    assertThat(secondFiredTimers, not(emptyIterable()));    FiredTimers<AppliedPTransform<?, ?, ?>> secondFired = Iterables.getOnlyElement(secondFiredTimers);        assertThat(secondFired.getTimers(), contains(middleTimer, lastTimer));}
public void beam_f4787_0()
{    Collection<FiredTimers<AppliedPTransform<?, ?, ?>>> initialTimers = manager.extractFiredTimers();    assertThat(initialTimers, emptyIterable());    String timerId = "myTimer";    StructuralKey<?> key = StructuralKey.of(-12L, VarLongCoder.of());    TimerData initialTimer = TimerData.of(timerId, StateNamespaces.global(), new Instant(5000L), TimeDomain.PROCESSING_TIME);    TimerData overridingTimer = TimerData.of(timerId, StateNamespaces.global(), new Instant(10000L), TimeDomain.PROCESSING_TIME);    TimerUpdate initialUpdate = TimerUpdate.builder(key).setTimer(initialTimer).build();    TimerUpdate overridingUpdate = TimerUpdate.builder(key).setTimer(overridingTimer).build();    manager.updateWatermarks(null, initialUpdate, graph.getProducer(createdInts), null, Collections.emptyList(), new Instant(5000L));    manager.refreshAll();        manager.updateWatermarks(null, overridingUpdate, graph.getProducer(createdInts), null, Collections.emptyList(), new Instant(10000L));        clock.set(new Instant(50000L));    manager.refreshAll();    Collection<FiredTimers<AppliedPTransform<?, ?, ?>>> firedTimers = manager.extractFiredTimers();    assertThat(firedTimers, not(emptyIterable()));    FiredTimers<AppliedPTransform<?, ?, ?>> timers = Iterables.getOnlyElement(firedTimers);    assertThat(timers.getTimers(), contains(overridingTimer));}
public void beam_f4788_0()
{    Collection<FiredTimers<AppliedPTransform<?, ?, ?>>> initialTimers = manager.extractFiredTimers();    assertThat(initialTimers, emptyIterable());    String timerId = "myTimer";    StructuralKey<?> key = StructuralKey.of(-12L, VarLongCoder.of());    TimerData initialTimer = TimerData.of(timerId, StateNamespaces.global(), new Instant(1000L), TimeDomain.EVENT_TIME);    TimerData overridingTimer = TimerData.of(timerId, StateNamespaces.global(), new Instant(2000L), TimeDomain.EVENT_TIME);    TimerUpdate initialUpdate = TimerUpdate.builder(key).setTimer(initialTimer).build();    TimerUpdate overridingUpdate = TimerUpdate.builder(key).setTimer(overridingTimer).build();    CommittedBundle<Integer> createdBundle = multiWindowedBundle(filtered);    manager.updateWatermarks(createdBundle, initialUpdate, graph.getProducer(filtered), createdBundle.withElements(Collections.emptyList()), Collections.<CommittedBundle<?>>singleton(multiWindowedBundle(intsToFlatten)), new Instant(1000L));    manager.refreshAll();        manager.updateWatermarks(createdBundle, overridingUpdate, graph.getProducer(filtered), createdBundle.withElements(Collections.emptyList()), Collections.<CommittedBundle<?>>singleton(multiWindowedBundle(intsToFlatten)), new Instant(1000L));    manager.refreshAll();        manager.updateWatermarks(null, TimerUpdate.empty(), graph.getProducer(createdInts), null, Collections.singleton(createdBundle), new Instant(3000L));    manager.refreshAll();    Collection<FiredTimers<AppliedPTransform<?, ?, ?>>> firstFiredTimers = manager.extractFiredTimers();    assertThat(firstFiredTimers, not(emptyIterable()));    FiredTimers<AppliedPTransform<?, ?, ?>> firstFired = Iterables.getOnlyElement(firstFiredTimers);    assertThat(firstFired.getTimers(), contains(overridingTimer));}
public void beam_f4796_0()
{    TimerUpdateBuilder builder = TimerUpdate.builder(null);    TimerData timer = TimerData.of(StateNamespaces.global(), Instant.now(), TimeDomain.EVENT_TIME);    TimerUpdate built = builder.build();    builder.deletedTimer(timer);    assertThat(built.getDeletedTimers(), emptyIterable());    builder.build();    assertThat(built.getDeletedTimers(), emptyIterable());}
public void beam_f4797_0()
{    TimerUpdateBuilder builder = TimerUpdate.builder(null);    TimerData timer = TimerData.of(StateNamespaces.global(), Instant.now(), TimeDomain.EVENT_TIME);    TimerUpdate built = builder.build();    builder.withCompletedTimers(ImmutableList.of(timer));    assertThat(built.getCompletedTimers(), emptyIterable());    builder.build();    assertThat(built.getCompletedTimers(), emptyIterable());}
public void beam_f4798_0()
{    TimerUpdateBuilder builder = TimerUpdate.builder(null);    TimerData timer = TimerData.of(StateNamespaces.global(), Instant.now(), TimeDomain.EVENT_TIME);    TimerUpdate built = builder.build();    assertThat(built.getCompletedTimers(), emptyIterable());    assertThat(built.withCompletedTimers(ImmutableList.of(timer)).getCompletedTimers(), contains(timer));    assertThat(built.getCompletedTimers(), emptyIterable());}
private UncommittedBundle<Long> beam_f4806_0(PCollection<Long> output, CommittedBundle<Long> inputBundle)
{    UncommittedBundle<Long> outputBundle = bundleFactory.createBundle(output);    when(evaluationContext.createBundle(output)).thenReturn(outputBundle);    return outputBundle;}
private TransformResult<Long> beam_f4807_0(PCollection<Long> windowed, CommittedBundle<Long> inputBundle) throws Exception
{    TransformEvaluator<Long> evaluator = factory.forApplication(DirectGraphs.getProducer(windowed), inputBundle);    evaluator.processElement(valueInGlobalWindow);    evaluator.processElement(valueInGlobalAndTwoIntervalWindows);    evaluator.processElement(valueInIntervalWindow);    return evaluator.finishBundle();}
public Collection<BoundedWindow> beam_f4808_0(AssignContext c) throws Exception
{    if (c.window().equals(GlobalWindow.INSTANCE)) {        return Collections.singleton(new IntervalWindow(c.timestamp(), c.timestamp().plus(1L)));    }    return Collections.singleton(c.window());}
public void beam_f4816_0()
{    CalculateShardsFn fn = new CalculateShardsFn(0);    long input = 0L;    int output = 1;    PAssert.that(p.apply(Create.of(input)).apply(ParDo.of(fn))).containsInAnyOrder(output);    p.run().waitUntilFinish();}
public void beam_f4817_0()
{    CalculateShardsFn fn = new CalculateShardsFn(0);    long input = 1L;    int output = 1;    PAssert.that(p.apply(Create.of(input)).apply(ParDo.of(fn))).containsInAnyOrder(output);    p.run().waitUntilFinish();}
public void beam_f4818_0()
{    CalculateShardsFn fn = new CalculateShardsFn(0);    long input = 2L;    int output = 2;    PAssert.that(p.apply(Create.of(input)).apply(ParDo.of(fn))).containsInAnyOrder(output);    p.run().waitUntilFinish();}
public String beam_f4826_0()
{    StringBuilder messagePayload = new StringBuilder();        try {        messagePayload.append(createCommittedMessage());    } catch (UnsupportedOperationException e) {        if (!e.getMessage().contains("committed metrics")) {            throw e;        }    }    messagePayload.append(createAttemptedMessage());    return messagePayload.toString();}
protected String beam_f4827_0()
{    String metricMessage = String.format(Locale.US, "%s %s %s\n", createNormalizedMetricName(counter, "counter", valueType, CommittedOrAttemped.COMMITTED), counter.getCommitted(), metricTimestamp);    return metricMessage;}
protected String beam_f4828_0()
{    String metricMessage = String.format(Locale.US, "%s %s %s\n", createNormalizedMetricName(counter, "counter", valueType, CommittedOrAttemped.ATTEMPTED), counter.getAttempted(), metricTimestamp);    return metricMessage;}
public void beam_f4836_0(MetricName value, JsonGenerator gen, SerializerProvider provider) throws IOException
{    gen.writeStartObject();    gen.writeObjectField("name", value.name());    gen.writeObjectField("namespace", value.namespace());    gen.writeEndObject();}
public void beam_f4837_0(MetricKey value, JsonGenerator gen, SerializerProvider provider) throws IOException
{    gen.writeObjectField("name", value.metricName());    gen.writeObjectField("step", value.stepName());}
public void beam_f4838_0(MetricKey value, JsonGenerator gen, SerializerProvider provider) throws IOException
{    gen.writeStartObject();    inline(value, gen, provider);    gen.writeEndObject();}
public void beam_f4846_0()
{    graphiteServer.clear();}
public static void beam_f4847_0() throws IOException
{    graphiteServer.stop();}
public void beam_f4848_0() throws Exception
{    MetricQueryResults metricQueryResults = new CustomMetricQueryResults(true);    MetricsOptions pipelineOptions = PipelineOptionsFactory.create().as(MetricsOptions.class);    pipelineOptions.setMetricsGraphitePort(port);    pipelineOptions.setMetricsGraphiteHost("127.0.0.1");    MetricsGraphiteSink metricsGraphiteSink = new MetricsGraphiteSink(pipelineOptions);    CountDownLatch countDownLatch = new CountDownLatch(1);    graphiteServer.setCountDownLatch(countDownLatch);    metricsGraphiteSink.writeMetrics(metricQueryResults);    countDownLatch.await();    String join = String.join("\n", graphiteServer.getMessages());    String regexpr = "beam.counter.ns1.n1.committed.value 10 [0-9]+\\n" + "beam.counter.ns1.n1.attempted.value 20 [0-9]+\\n" + "beam.gauge.ns1.n3.committed.value 100 [0-9]+\\n" + "beam.gauge.ns1.n3.attempted.value 120 [0-9]+\\n" + "beam.distribution.ns1.n2.committed.min 5 [0-9]+\\n" + "beam.distribution.ns1.n2.attempted.min 3 [0-9]+\\n" + "beam.distribution.ns1.n2.committed.max 8 [0-9]+\\n" + "beam.distribution.ns1.n2.attempted.max 9 [0-9]+\\n" + "beam.distribution.ns1.n2.committed.count 2 [0-9]+\\n" + "beam.distribution.ns1.n2.attempted.count 4 [0-9]+\\n" + "beam.distribution.ns1.n2.committed.sum 10 [0-9]+\\n" + "beam.distribution.ns1.n2.attempted.sum 25 [0-9]+\\n" + "beam.distribution.ns1.n2.committed.mean 5.0 [0-9]+\\n" + "beam.distribution.ns1.n2.attempted.mean 6.25 [0-9]+";    assertTrue(join.matches(regexpr));}
public NetworkMockServer beam_f4856_0() throws IOException
{    serverSocket = new ServerSocket(port);    thread = new ServerThread(serverSocket, messages);    thread.start();    return this;}
public void beam_f4857_0() throws IOException
{    thread.shutdown();    serverSocket.close();}
public Collection<String> beam_f4858_0()
{    return messages;}
public T beam_f4866_0(T t, T reuse)
{    return copy(t);}
public int beam_f4867_0()
{    return -1;}
public void beam_f4868_0(T t, DataOutputView dataOutputView) throws IOException
{    DataOutputViewWrapper outputWrapper = new DataOutputViewWrapper(dataOutputView);    coder.encode(t, outputWrapper);}
public CompatibilityResult<T> beam_f4876_0(TypeSerializerConfigSnapshot configSnapshot)
{    if (snapshotConfiguration().equals(configSnapshot)) {        return CompatibilityResult.compatible();    }    return CompatibilityResult.requiresMigration();}
public int beam_f4877_0()
{    return VERSION;}
public boolean beam_f4878_0(Object o)
{    if (this == o) {        return true;    }    if (o == null || getClass() != o.getClass()) {        return false;    }    CoderTypeSerializerConfigSnapshot<?> that = (CoderTypeSerializerConfigSnapshot<?>) o;    return coderName != null ? coderName.equals(that.coderName) : that.coderName == null;}
public byte[] beam_f4886_0(byte[] from, byte[] reuse)
{    return copy(from);}
public int beam_f4887_0()
{    return -1;}
public void beam_f4888_0(byte[] record, DataOutputView target) throws IOException
{    if (record == null) {        throw new IllegalArgumentException("The record must not be null.");    }    final int len = record.length;    target.writeInt(len);    target.write(record);}
public static KeyedStateBackend<ByteBuffer> beam_f4907_0() throws Exception
{    MemoryStateBackend backend = new MemoryStateBackend();    AbstractKeyedStateBackend<ByteBuffer> keyedStateBackend = backend.createKeyedStateBackend(new DummyEnvironment("test", 1, 0), new JobID(), "test_op", new GenericTypeInfo<>(ByteBuffer.class).createSerializer(new ExecutionConfig()), 2, new KeyGroupRange(0, 1), new KvStateRegistry().createTaskRegistry(new JobID(), new JobVertexID()));    changeKey(keyedStateBackend);    return keyedStateBackend;}
private static void beam_f4908_0(KeyedStateBackend<ByteBuffer> keyedStateBackend) throws CoderException
{    keyedStateBackend.setCurrentKey(ByteBuffer.wrap(CoderUtils.encodeToByteArray(StringUtf8Coder.of(), UUID.randomUUID().toString())));}
public void beam_f4909_0() throws Exception
{    AtomicCoder<String> anonymousClassCoder = new AtomicCoder<String>() {        @Override        public void encode(String value, OutputStream outStream) throws CoderException, IOException {        }        @Override        public String decode(InputStream inStream) throws CoderException, IOException {            return "";        }    };    testWriteAndReadConfigSnapshot(anonymousClassCoder);}
public T beam_f4918_0(T t, T reuse)
{    return copy(t);}
public int beam_f4919_0()
{    return -1;}
public void beam_f4920_0(T t, DataOutputView dataOutputView) throws IOException
{    DataOutputViewWrapper outputWrapper = new DataOutputViewWrapper(dataOutputView);    coder.encode(t, outputWrapper);}
public TypeSerializerSchemaCompatibility<T> beam_f4928_0(TypeSerializer<T> newSerializer)
{        return TypeSerializerSchemaCompatibility.compatibleAsIs();}
public String beam_f4929_0()
{    return "CoderTypeSerializer{" + "coder=" + coder + '}';}
public boolean beam_f4930_0()
{    return true;}
public void beam_f4938_0(DataInputView source, DataOutputView target) throws IOException
{    final int len = source.readInt();    target.writeInt(len);    target.write(source, len);}
public TypeSerializerSnapshot<byte[]> beam_f4939_0()
{    return new TypeSerializerSnapshot<byte[]>() {        @Override        public int getCurrentVersion() {            return 1;        }        @Override        public void writeSnapshot(DataOutputView out) throws IOException {        }        @Override        public void readSnapshot(int readVersion, DataInputView in, ClassLoader userCodeClassLoader) throws IOException {        }        @Override        public TypeSerializer<byte[]> restoreSerializer() {            return new EncodedValueSerializer();        }        @Override        public TypeSerializerSchemaCompatibility<byte[]> resolveSchemaCompatibility(TypeSerializer<byte[]> newSerializer) {            return newSerializer instanceof EncodedValueSerializer ? TypeSerializerSchemaCompatibility.compatibleAsIs() : TypeSerializerSchemaCompatibility.compatibleAfterMigration();        }    };}
public int beam_f4940_0()
{    return 1;}
public void beam_f4961_0() throws Exception
{    AtomicCoder<String> anonymousClassCoder = new AtomicCoder<String>() {        @Override        public void encode(String value, OutputStream outStream) {        }        @Override        public String decode(InputStream inStream) {            return "";        }    };    testWriteAndReadConfigSnapshot(anonymousClassCoder);}
public String beam_f4963_0(InputStream inStream)
{    return "";}
public void beam_f4964_0() throws Exception
{    Coder<String> concreteClassCoder = StringUtf8Coder.of();    testWriteAndReadConfigSnapshot(concreteClassCoder);}
public Coder<List<T>> beam_f4972_0(CoderRegistry registry, Coder<T> inputCoder)
{    return ListCoder.of(inputCoder);}
public static CreateFlinkPCollectionView<ElemT, ViewT> beam_f4973_0(PCollectionView<ViewT> view)
{    return new CreateFlinkPCollectionView<>(view);}
public PCollection<List<ElemT>> beam_f4974_0(PCollection<List<ElemT>> input)
{    return PCollection.createPrimitiveOutputInternal(input.getPipeline(), input.getWindowingStrategy(), input.isBounded(), input.getCoder());}
private void beam_f4982_0(PTransform<?, ?> transform, TransformHierarchy.Node node, BatchTransformTranslator<?> translator)
{    @SuppressWarnings("unchecked")    T typedTransform = (T) transform;    @SuppressWarnings("unchecked")    BatchTransformTranslator<T> typedTranslator = (BatchTransformTranslator<T>) translator;        batchContext.setCurrentTransform(node.toAppliedPTransform(getPipeline()));    typedTranslator.translateNode(typedTransform, batchContext);}
private static BatchTransformTranslator<?> beam_f4983_0(TransformHierarchy.Node node)
{    @Nullable    PTransform<?, ?> transform = node.getTransform();        if (transform == null) {        return null;    }    return FlinkBatchTransformTranslators.getTranslator(transform);}
public BatchTranslationContext beam_f4984_0(JobInfo jobInfo, FlinkPipelineOptions pipelineOptions, @Nullable String confDir, List<String> filesToStage)
{    ExecutionEnvironment executionEnvironment = FlinkExecutionEnvironments.createBatchExecutionEnvironment(pipelineOptions, filesToStage, confDir);    return new BatchTranslationContext(jobInfo, pipelineOptions, executionEnvironment);}
public Collection<DataSet<?>> beam_f4992_0()
{    return danglingDataSets.stream().map(id -> dataSets.get(id)).collect(Collectors.toList());}
public Set<String> beam_f4993_0()
{    return urnToTransformTranslator.keySet();}
public boolean beam_f4994_0(RunnerApi.PTransform pTransform)
{    return PTransformTranslation.RESHUFFLE_URN.equals(PTransformTranslation.urnForTransformOrNull(pTransform));}
public List<T> beam_f5002_0(List<T> accumulator, T input)
{    accumulator.add(input);    return accumulator;}
public List<T> beam_f5003_0(Iterable<List<T>> accumulators)
{    List<T> result = createAccumulator();    for (List<T> accumulator : accumulators) {        result.addAll(accumulator);    }    return result;}
public List<T> beam_f5004_0(List<T> accumulator)
{    return accumulator;}
public void beam_f5012_0(PTransform<PBegin, PCollection<T>> transform, FlinkBatchTranslationContext context)
{    @SuppressWarnings("unchecked")    AppliedPTransform<PBegin, PCollection<T>, PTransform<PBegin, PCollection<T>>> application = (AppliedPTransform<PBegin, PCollection<T>, PTransform<PBegin, PCollection<T>>>) context.getCurrentTransform();    BoundedSource<T> source;    try {        source = ReadTranslation.boundedSourceFromTransform(application);    } catch (IOException e) {        throw new RuntimeException(e);    }    PCollection<T> output = context.getOutput(transform);    TypeInformation<WindowedValue<T>> typeInformation = context.getTypeInfo(output);    String fullName = getCurrentTransformName(context);    DataSource<WindowedValue<T>> dataSource = new DataSource<>(context.getExecutionEnvironment(), new SourceInputFormat<>(fullName, source, context.getPipelineOptions()), typeInformation, fullName);    context.setOutputDataSet(output, dataSource);}
public void beam_f5013_0(PTransform<PCollection<T>, PCollection<T>> transform, FlinkBatchTranslationContext context)
{    PValue input = context.getInput(transform);    TypeInformation<WindowedValue<T>> resultTypeInfo = context.getTypeInfo(context.getOutput(transform));    DataSet<WindowedValue<T>> inputDataSet = context.getInputDataSet(input);    @SuppressWarnings("unchecked")    final WindowingStrategy<T, ? extends BoundedWindow> windowingStrategy = (WindowingStrategy<T, ? extends BoundedWindow>) context.getOutput(transform).getWindowingStrategy();    WindowFn<T, ? extends BoundedWindow> windowFn = windowingStrategy.getWindowFn();    FlinkAssignWindows<T, ? extends BoundedWindow> assignWindowsFunction = new FlinkAssignWindows<>(windowFn);    DataSet<WindowedValue<T>> resultDataSet = inputDataSet.flatMap(assignWindowsFunction).name(context.getOutput(transform).getName()).returns(resultTypeInfo);    context.setOutputDataSet(context.getOutput(transform), resultDataSet);}
public void beam_f5014_0(PTransform<PCollection<KV<K, InputT>>, PCollection<KV<K, Iterable<InputT>>>> transform, FlinkBatchTranslationContext context)
{            DataSet<WindowedValue<KV<K, InputT>>> inputDataSet = context.getInputDataSet(context.getInput(transform));    Combine.CombineFn<InputT, List<InputT>, List<InputT>> combineFn = new Concatenate<>();    KvCoder<K, InputT> inputCoder = (KvCoder<K, InputT>) context.getInput(transform).getCoder();    Coder<List<InputT>> accumulatorCoder;    try {        accumulatorCoder = combineFn.getAccumulatorCoder(context.getInput(transform).getPipeline().getCoderRegistry(), inputCoder.getValueCoder());    } catch (CannotProvideCoderException e) {        throw new RuntimeException(e);    }    WindowingStrategy<?, ?> windowingStrategy = context.getInput(transform).getWindowingStrategy();    TypeInformation<WindowedValue<KV<K, List<InputT>>>> partialReduceTypeInfo = new CoderTypeInformation<>(WindowedValue.getFullCoder(KvCoder.of(inputCoder.getKeyCoder(), accumulatorCoder), windowingStrategy.getWindowFn().windowCoder()));    Grouping<WindowedValue<KV<K, InputT>>> inputGrouping = inputDataSet.groupBy(new KvKeySelector<>(inputCoder.getKeyCoder()));    @SuppressWarnings("unchecked")    WindowingStrategy<Object, BoundedWindow> boundedStrategy = (WindowingStrategy<Object, BoundedWindow>) windowingStrategy;    FlinkPartialReduceFunction<K, InputT, List<InputT>, ?> partialReduceFunction = new FlinkPartialReduceFunction<>(combineFn, boundedStrategy, Collections.emptyMap(), context.getPipelineOptions());    FlinkReduceFunction<K, List<InputT>, List<InputT>, ?> reduceFunction = new FlinkReduceFunction<>(combineFn, boundedStrategy, Collections.emptyMap(), context.getPipelineOptions());        String fullName = getCurrentTransformName(context);    GroupCombineOperator<WindowedValue<KV<K, InputT>>, WindowedValue<KV<K, List<InputT>>>> groupCombine = new GroupCombineOperator<>(inputGrouping, partialReduceTypeInfo, partialReduceFunction, "GroupCombine: " + fullName);    Grouping<WindowedValue<KV<K, List<InputT>>>> intermediateGrouping = groupCombine.groupBy(new KvKeySelector<>(inputCoder.getKeyCoder()));        GroupReduceOperator<WindowedValue<KV<K, List<InputT>>>, WindowedValue<KV<K, List<InputT>>>> outputDataSet = new GroupReduceOperator<>(intermediateGrouping, partialReduceTypeInfo, reduceFunction, fullName);    context.setOutputDataSet(context.getOutput(transform), outputDataSet);}
public void beam_f5022_0(PTransform<PCollection<KV<K, InputT>>, PCollection<KV<K, OutputT>>> transform, FlinkBatchTranslationContext context)
{    DataSet<WindowedValue<KV<K, InputT>>> inputDataSet = context.getInputDataSet(context.getInput(transform));    CombineFnBase.GlobalCombineFn<InputT, AccumT, OutputT> combineFn = ((Combine.PerKey) transform).getFn();    KvCoder<K, InputT> inputCoder = (KvCoder<K, InputT>) context.getInput(transform).getCoder();    Coder<AccumT> accumulatorCoder;    try {        accumulatorCoder = combineFn.getAccumulatorCoder(context.getInput(transform).getPipeline().getCoderRegistry(), inputCoder.getValueCoder());    } catch (CannotProvideCoderException e) {        throw new RuntimeException(e);    }    WindowingStrategy<?, ?> windowingStrategy = context.getInput(transform).getWindowingStrategy();    TypeInformation<WindowedValue<KV<K, AccumT>>> partialReduceTypeInfo = context.getTypeInfo(KvCoder.of(inputCoder.getKeyCoder(), accumulatorCoder), windowingStrategy);    Grouping<WindowedValue<KV<K, InputT>>> inputGrouping = inputDataSet.groupBy(new KvKeySelector<>(inputCoder.getKeyCoder()));            Map<PCollectionView<?>, WindowingStrategy<?, ?>> sideInputStrategies = new HashMap<>();    for (PCollectionView<?> sideInput : (List<PCollectionView<?>>) ((Combine.PerKey) transform).getSideInputs()) {        sideInputStrategies.put(sideInput, sideInput.getWindowingStrategyInternal());    }    WindowingStrategy<Object, BoundedWindow> boundedStrategy = (WindowingStrategy<Object, BoundedWindow>) windowingStrategy;    String fullName = getCurrentTransformName(context);    if (windowingStrategy.getWindowFn().isNonMerging()) {        FlinkPartialReduceFunction<K, InputT, AccumT, ?> partialReduceFunction = new FlinkPartialReduceFunction<>(combineFn, boundedStrategy, sideInputStrategies, context.getPipelineOptions());        FlinkReduceFunction<K, AccumT, OutputT, ?> reduceFunction = new FlinkReduceFunction<>(combineFn, boundedStrategy, sideInputStrategies, context.getPipelineOptions());                GroupCombineOperator<WindowedValue<KV<K, InputT>>, WindowedValue<KV<K, AccumT>>> groupCombine = new GroupCombineOperator<>(inputGrouping, partialReduceTypeInfo, partialReduceFunction, "GroupCombine: " + fullName);        transformSideInputs(((Combine.PerKey) transform).getSideInputs(), groupCombine, context);        TypeInformation<WindowedValue<KV<K, OutputT>>> reduceTypeInfo = context.getTypeInfo(context.getOutput(transform));        Grouping<WindowedValue<KV<K, AccumT>>> intermediateGrouping = groupCombine.groupBy(new KvKeySelector<>(inputCoder.getKeyCoder()));                GroupReduceOperator<WindowedValue<KV<K, AccumT>>, WindowedValue<KV<K, OutputT>>> outputDataSet = new GroupReduceOperator<>(intermediateGrouping, reduceTypeInfo, reduceFunction, fullName);        transformSideInputs(((Combine.PerKey) transform).getSideInputs(), outputDataSet, context);        context.setOutputDataSet(context.getOutput(transform), outputDataSet);    } else {                        RichGroupReduceFunction<WindowedValue<KV<K, InputT>>, WindowedValue<KV<K, OutputT>>> reduceFunction = new FlinkMergingNonShuffleReduceFunction<>(combineFn, boundedStrategy, sideInputStrategies, context.getPipelineOptions());        TypeInformation<WindowedValue<KV<K, OutputT>>> reduceTypeInfo = context.getTypeInfo(context.getOutput(transform));        Grouping<WindowedValue<KV<K, InputT>>> grouping = inputDataSet.groupBy(new KvKeySelector<>(inputCoder.getKeyCoder()));                GroupReduceOperator<WindowedValue<KV<K, InputT>>, WindowedValue<KV<K, OutputT>>> outputDataSet = new GroupReduceOperator<>(grouping, reduceTypeInfo, reduceFunction, fullName);        transformSideInputs(((Combine.PerKey) transform).getSideInputs(), outputDataSet, context);        context.setOutputDataSet(context.getOutput(transform), outputDataSet);    }}
public void beam_f5023_0(PTransform<PCollection<InputT>, PCollectionTuple> transform, FlinkBatchTranslationContext context)
{    DoFn<InputT, OutputT> doFn;    try {        doFn = (DoFn<InputT, OutputT>) ParDoTranslation.getDoFn(context.getCurrentTransform());    } catch (IOException e) {        throw new RuntimeException(e);    }    checkState(!DoFnSignatures.signatureForDoFn(doFn).processElement().isSplittable(), "Not expected to directly translate splittable DoFn, should have been overridden: %s", doFn);    DataSet<WindowedValue<InputT>> inputDataSet = context.getInputDataSet(context.getInput(transform));    Map<TupleTag<?>, PValue> outputs = context.getOutputs(transform);    TupleTag<?> mainOutputTag;    DoFnSchemaInformation doFnSchemaInformation;    Map<String, PCollectionView<?>> sideInputMapping;    try {        mainOutputTag = ParDoTranslation.getMainOutputTag(context.getCurrentTransform());    } catch (IOException e) {        throw new RuntimeException(e);    }    doFnSchemaInformation = ParDoTranslation.getSchemaInformation(context.getCurrentTransform());    sideInputMapping = ParDoTranslation.getSideInputMapping(context.getCurrentTransform());    Map<TupleTag<?>, Integer> outputMap = Maps.newHashMap();        outputMap.put(mainOutputTag, 0);    int count = 1;    for (TupleTag<?> tag : outputs.keySet()) {        if (!outputMap.containsKey(tag)) {            outputMap.put(tag, count++);        }    }        Map<Integer, TupleTag<?>> indexMap = Maps.newTreeMap();    for (Map.Entry<TupleTag<?>, Integer> entry : outputMap.entrySet()) {        indexMap.put(entry.getValue(), entry.getKey());    }        WindowingStrategy<?, ?> windowingStrategy = null;        List<Coder<?>> outputCoders = Lists.newArrayList();    for (TupleTag<?> tag : indexMap.values()) {        PValue taggedValue = outputs.get(tag);        checkState(taggedValue instanceof PCollection, "Within ParDo, got a non-PCollection output %s of type %s", taggedValue, taggedValue.getClass().getSimpleName());        PCollection<?> coll = (PCollection<?>) taggedValue;        outputCoders.add(coll.getCoder());        windowingStrategy = coll.getWindowingStrategy();    }    if (windowingStrategy == null) {        throw new IllegalStateException("No outputs defined.");    }    UnionCoder unionCoder = UnionCoder.of(outputCoders);    TypeInformation<WindowedValue<RawUnionValue>> typeInformation = new CoderTypeInformation<>(WindowedValue.getFullCoder(unionCoder, windowingStrategy.getWindowFn().windowCoder()));    List<PCollectionView<?>> sideInputs;    try {        sideInputs = ParDoTranslation.getSideInputs(context.getCurrentTransform());    } catch (IOException e) {        throw new RuntimeException(e);    }            Map<PCollectionView<?>, WindowingStrategy<?, ?>> sideInputStrategies = new HashMap<>();    for (PCollectionView<?> sideInput : sideInputs) {        sideInputStrategies.put(sideInput, sideInput.getWindowingStrategyInternal());    }    SingleInputUdfOperator<WindowedValue<InputT>, WindowedValue<RawUnionValue>, ?> outputDataSet;    boolean usesStateOrTimers;    try {        usesStateOrTimers = ParDoTranslation.usesStateOrTimers(context.getCurrentTransform());    } catch (IOException e) {        throw new RuntimeException(e);    }    Map<TupleTag<?>, Coder<?>> outputCoderMap = context.getOutputCoders();    String fullName = getCurrentTransformName(context);    if (usesStateOrTimers) {        KvCoder<?, ?> inputCoder = (KvCoder<?, ?>) context.getInput(transform).getCoder();        FlinkStatefulDoFnFunction<?, ?, OutputT> doFnWrapper = new FlinkStatefulDoFnFunction<>((DoFn) doFn, fullName, windowingStrategy, sideInputStrategies, context.getPipelineOptions(), outputMap, mainOutputTag, inputCoder, outputCoderMap, doFnSchemaInformation, sideInputMapping);                        Grouping<WindowedValue<InputT>> grouping = inputDataSet.groupBy(new KvKeySelector(inputCoder.getKeyCoder()));        outputDataSet = new GroupReduceOperator(grouping, typeInformation, doFnWrapper, fullName);    } else {        FlinkDoFnFunction<InputT, RawUnionValue> doFnWrapper = new FlinkDoFnFunction(doFn, fullName, windowingStrategy, sideInputStrategies, context.getPipelineOptions(), outputMap, mainOutputTag, context.getInput(transform).getCoder(), outputCoderMap, doFnSchemaInformation, sideInputMapping);        outputDataSet = new MapPartitionOperator<>(inputDataSet, typeInformation, doFnWrapper, fullName);    }    transformSideInputs(sideInputs, outputDataSet, context);    for (Entry<TupleTag<?>, PValue> output : outputs.entrySet()) {        pruneOutput(outputDataSet, context, outputMap.get(output.getKey()), (PCollection) output.getValue());    }}
private void beam_f5024_0(DataSet<WindowedValue<RawUnionValue>> taggedDataSet, FlinkBatchTranslationContext context, int integerTag, PCollection<T> collection)
{    TypeInformation<WindowedValue<T>> outputType = context.getTypeInfo(collection);    FlinkMultiOutputPruningFunction<T> pruningFunction = new FlinkMultiOutputPruningFunction<>(integerTag);    FlatMapOperator<WindowedValue<RawUnionValue>, WindowedValue<T>> pruningOperator = new FlatMapOperator<>(taggedDataSet, outputType, pruningFunction, collection.getName());    context.setOutputDataSet(collection, pruningOperator);}
public void beam_f5032_0(PValue value, DataSet<WindowedValue<T>> set)
{    if (!dataSets.containsKey(value)) {        dataSets.put(value, set);        danglingDataSets.put(value, set);    }}
public void beam_f5033_0(AppliedPTransform<?, ?, ?> currentTransform)
{    this.currentTransform = currentTransform;}
public AppliedPTransform<?, ?, ?> beam_f5034_0()
{    return currentTransform;}
 Map<TupleTag<?>, PValue> beam_f5042_0(PTransform<?, ?> transform)
{    return currentTransform.getOutputs();}
 T beam_f5043_0(PTransform<?, T> transform)
{    return (T) Iterables.getOnlyElement(currentTransform.getOutputs().values());}
public State beam_f5044_0()
{    return State.UNKNOWN;}
public static StreamExecutionEnvironment beam_f5052_0(FlinkPipelineOptions options, List<String> filesToStage)
{    return createStreamExecutionEnvironment(options, filesToStage, null);}
 static StreamExecutionEnvironment beam_f5053_1(FlinkPipelineOptions options, List<String> filesToStage, @Nullable String confDir)
{        String masterUrl = options.getFlinkMaster();    Configuration flinkConfiguration = getFlinkConfiguration(confDir);    final StreamExecutionEnvironment flinkStreamEnv;        if ("[local]".equals(masterUrl)) {        flinkStreamEnv = StreamExecutionEnvironment.createLocalEnvironment(getDefaultLocalParallelism(), flinkConfiguration);    } else if ("[auto]".equals(masterUrl)) {        flinkStreamEnv = StreamExecutionEnvironment.getExecutionEnvironment();    } else {        int defaultPort = flinkConfiguration.getInteger(RestOptions.PORT);        HostAndPort hostAndPort = HostAndPort.fromString(masterUrl).withDefaultPort(defaultPort);        flinkConfiguration.setInteger(RestOptions.PORT, hostAndPort.getPort());        final SavepointRestoreSettings savepointRestoreSettings;        if (options.getSavepointPath() != null) {            savepointRestoreSettings = SavepointRestoreSettings.forPath(options.getSavepointPath(), options.getAllowNonRestoredState());        } else {            savepointRestoreSettings = SavepointRestoreSettings.none();        }        flinkStreamEnv = new BeamFlinkRemoteStreamEnvironment(hostAndPort.getHost(), hostAndPort.getPort(), flinkConfiguration, savepointRestoreSettings, filesToStage.toArray(new String[filesToStage.size()]));            }        final int parallelism = determineParallelism(options.getParallelism(), flinkStreamEnv.getParallelism(), flinkConfiguration);    flinkStreamEnv.setParallelism(parallelism);    if (options.getMaxParallelism() > 0) {        flinkStreamEnv.setMaxParallelism(options.getMaxParallelism());    }        options.setParallelism(parallelism);    if (options.getObjectReuse()) {        flinkStreamEnv.getConfig().enableObjectReuse();    } else {        flinkStreamEnv.getConfig().disableObjectReuse();    }        flinkStreamEnv.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);            int numRetries = options.getNumberOfExecutionRetries();    if (numRetries != -1) {        flinkStreamEnv.setNumberOfExecutionRetries(numRetries);    }    long retryDelay = options.getExecutionRetryDelay();    if (retryDelay != -1) {        flinkStreamEnv.getConfig().setExecutionRetryDelay(retryDelay);    }                long checkpointInterval = options.getCheckpointingInterval();    if (checkpointInterval != -1) {        if (checkpointInterval < 1) {            throw new IllegalArgumentException("The checkpoint interval must be positive");        }        flinkStreamEnv.enableCheckpointing(checkpointInterval, CheckpointingMode.valueOf(options.getCheckpointingMode()));        if (options.getCheckpointTimeoutMillis() != -1) {            flinkStreamEnv.getCheckpointConfig().setCheckpointTimeout(options.getCheckpointTimeoutMillis());        }        boolean externalizedCheckpoint = options.isExternalizedCheckpointsEnabled();        boolean retainOnCancellation = options.getRetainExternalizedCheckpointsOnCancellation();        if (externalizedCheckpoint) {            flinkStreamEnv.getCheckpointConfig().enableExternalizedCheckpoints(retainOnCancellation ? ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION : ExternalizedCheckpointCleanup.DELETE_ON_CANCELLATION);        }        long minPauseBetweenCheckpoints = options.getMinPauseBetweenCheckpoints();        if (minPauseBetweenCheckpoints != -1) {            flinkStreamEnv.getCheckpointConfig().setMinPauseBetweenCheckpoints(minPauseBetweenCheckpoints);        }        boolean failOnCheckpointingErrors = options.getFailOnCheckpointingErrors();        flinkStreamEnv.getCheckpointConfig().setFailOnCheckpointingErrors(failOnCheckpointingErrors);    }    applyLatencyTrackingInterval(flinkStreamEnv.getConfig(), options);    if (options.getAutoWatermarkInterval() != null) {        flinkStreamEnv.getConfig().setAutoWatermarkInterval(options.getAutoWatermarkInterval());    }        if (options.getStateBackendFactory() != null) {        final StateBackend stateBackend = InstanceBuilder.ofType(FlinkStateBackendFactory.class).fromClass(options.getStateBackendFactory()).build().createStateBackend(options);        flinkStreamEnv.setStateBackend(stateBackend);    }    return flinkStreamEnv;}
private static int beam_f5054_1(final int pipelineOptionsParallelism, final int envParallelism, final Configuration configuration)
{    if (pipelineOptionsParallelism > 0) {        return pipelineOptionsParallelism;    }    if (envParallelism > 0) {                return envParallelism;    }    final int flinkConfigParallelism = configuration.getInteger(CoreOptions.DEFAULT_PARALLELISM.key(), -1);    if (flinkConfigParallelism > 0) {        return flinkConfigParallelism;    }        return 1;}
 String beam_f5062_0()
{    return flinkConfDir;}
public static void beam_f5063_0(String[] args) throws Exception
{            FileSystems.setDefaultPipelineOptions(PipelineOptionsFactory.create());    fromParams(args).run();}
private static void beam_f5064_0(CmdLineParser parser)
{    System.err.println(String.format("Usage: java %s arguments...", FlinkJobServerDriver.class.getSimpleName()));    parser.printUsage(System.err);    System.err.println();}
 JobGraph beam_f5072_0(Pipeline p)
{    translate(p);    return flinkStreamEnv.getStreamGraph().getJobGraph();}
 ExecutionEnvironment beam_f5073_0()
{    return flinkBatchEnv;}
 StreamExecutionEnvironment beam_f5074_0()
{    return flinkStreamEnv;}
public JobApi.MetricResults beam_f5082_0() throws UnsupportedOperationException
{    Iterable<MetricsApi.MonitoringInfo> monitoringInfos = this.getMetricsContainerStepMap().getMonitoringInfos();    return JobApi.MetricResults.newBuilder().addAllAttempted(monitoringInfos).build();}
public JobApi.MetricResults beam_f5083_1() throws UnsupportedOperationException
{        return JobApi.MetricResults.newBuilder().build();}
public static FlinkRunner beam_f5084_1(PipelineOptions options)
{    FlinkPipelineOptions flinkOptions = PipelineOptionsValidator.validate(FlinkPipelineOptions.class, options);    ArrayList<String> missing = new ArrayList<>();    if (flinkOptions.getAppName() == null) {        missing.add("appName");    }    if (missing.size() > 0) {        throw new IllegalArgumentException("Missing required values: " + Joiner.on(',').join(missing));    }    if (flinkOptions.getFilesToStage() == null) {        flinkOptions.setFilesToStage(detectClassPathResourcesToStage(FlinkRunner.class.getClassLoader()));                    }    return new FlinkRunner(flinkOptions);}
public CompositeBehavior beam_f5092_0(TransformHierarchy.Node node)
{    if (ptransformViewsWithNonDeterministicKeyCoders.contains(node.getTransform())) {        ptransformViewNamesWithNonDeterministicKeyCoders.add(node.getFullName());    }    return CompositeBehavior.ENTER_TRANSFORM;}
 JobGraph beam_f5093_0(Pipeline p)
{    FlinkPipelineExecutionEnvironment env = new FlinkPipelineExecutionEnvironment(options);    return env.getJobGraph(p);}
public Iterable<Class<? extends PipelineRunner<?>>> beam_f5094_0()
{    return ImmutableList.of(FlinkRunner.class, TestFlinkRunner.class);}
 MetricsContainerStepMap beam_f5102_0()
{    return (MetricsContainerStepMap) accumulators.get(FlinkMetricContainer.ACCUMULATOR_NAME);}
public void beam_f5103_0(Pipeline pipeline)
{        UnconsumedReads.ensureAllReadsConsumed(pipeline);    super.translate(pipeline);}
public CompositeBehavior beam_f5104_1(TransformHierarchy.Node node)
{        this.depth++;    PTransform<?, ?> transform = node.getTransform();    if (transform != null) {        StreamTransformTranslator<?> translator = FlinkStreamingTransformTranslators.getTranslator(transform);        if (translator != null && applyCanTranslate(transform, node, translator)) {            applyStreamingTransform(transform, node, translator);                        return CompositeBehavior.DO_NOT_ENTER_TRANSFORM;        }    }    return CompositeBehavior.ENTER_TRANSFORM;}
public Map<PValue, ReplacementOutput> beam_f5113_0(Map<TupleTag<?>, PValue> outputs, WriteFilesResult<DestinationT> newOutput)
{    return ReplacementOutputs.tagged(outputs, newOutput);}
 Map<Integer, Map<Integer, ShardedKey<Integer>>> beam_f5114_0()
{    return cache == null ? null : cache.asMap();}
 int beam_f5115_0()
{    return maxParallelism;}
public void beam_f5123_0(String pCollectionId, DataStream<T> dataStream)
{    dataStreams.put(pCollectionId, dataStream);}
public DataStream<T> beam_f5124_0(String pCollectionId)
{    DataStream<T> dataSet = (DataStream<T>) dataStreams.get(pCollectionId);    if (dataSet == null) {        throw new IllegalArgumentException(String.format("Unknown datastream for id %s.", pCollectionId));    }    return dataSet;}
public Set<String> beam_f5125_0()
{        return Sets.difference(urnToTransformTranslator.keySet(), ImmutableSet.of(PTransformTranslation.READ_TRANSFORM_URN));}
private void beam_f5133_0(String id, RunnerApi.Pipeline pipeline, StreamingTranslationContext context)
{    RunnerApi.PTransform transform = pipeline.getComponents().getTransformsOrThrow(id);    String outputCollectionId = Iterables.getOnlyElement(transform.getOutputsMap().values());    RunnerApi.ReadPayload payload;    try {        payload = RunnerApi.ReadPayload.parseFrom(transform.getSpec().getPayload());    } catch (IOException e) {        throw new RuntimeException("Failed to parse ReadPayload from transform", e);    }    Preconditions.checkState(payload.getIsBounded() != RunnerApi.IsBounded.Enum.BOUNDED, "Bounded reads should run inside an environment instead of being translated by the Runner.");    DataStream<WindowedValue<T>> source = translateUnboundedSource(transform.getUniqueName(), outputCollectionId, payload, pipeline, context.getPipelineOptions(), context.getExecutionEnvironment());    context.addDataStream(outputCollectionId, source);}
private static DataStream<WindowedValue<T>> beam_f5134_0(String transformName, String outputCollectionId, RunnerApi.ReadPayload payload, RunnerApi.Pipeline pipeline, PipelineOptions pipelineOptions, StreamExecutionEnvironment env)
{    final DataStream<WindowedValue<T>> source;    final DataStream<WindowedValue<ValueWithRecordId<T>>> nonDedupSource;    Coder<WindowedValue<T>> windowCoder = instantiateCoder(outputCollectionId, pipeline.getComponents());    TypeInformation<WindowedValue<T>> outputTypeInfo = new CoderTypeInformation<>(windowCoder);    WindowingStrategy windowStrategy = getWindowingStrategy(outputCollectionId, pipeline.getComponents());    TypeInformation<WindowedValue<ValueWithRecordId<T>>> withIdTypeInfo = new CoderTypeInformation<>(WindowedValue.getFullCoder(ValueWithRecordId.ValueWithRecordIdCoder.of(((WindowedValueCoder) windowCoder).getValueCoder()), windowStrategy.getWindowFn().windowCoder()));    UnboundedSource unboundedSource = ReadTranslation.unboundedSourceFromProto(payload);    try {        int parallelism = env.getMaxParallelism() > 0 ? env.getMaxParallelism() : env.getParallelism();        UnboundedSourceWrapper sourceWrapper = new UnboundedSourceWrapper<>(transformName, pipelineOptions, unboundedSource, parallelism);        nonDedupSource = env.addSource(sourceWrapper).name(transformName).uid(transformName).returns(withIdTypeInfo);        if (unboundedSource.requiresDeduping()) {            source = nonDedupSource.keyBy(new FlinkStreamingTransformTranslators.ValueWithRecordIdKeySelector<>()).transform("deduping", outputTypeInfo, new DedupingOperator<>()).uid(format("%s/__deduplicated__", transformName));        } else {            source = nonDedupSource.flatMap(new FlinkStreamingTransformTranslators.StripIdsMap<>()).returns(outputTypeInfo);        }    } catch (Exception e) {        throw new RuntimeException("Error while translating UnboundedSource: " + unboundedSource, e);    }    return source;}
private void beam_f5135_0(String id, RunnerApi.Pipeline pipeline, StreamingTranslationContext context)
{    RunnerApi.PTransform pTransform = pipeline.getComponents().getTransformsOrThrow(id);    TypeInformation<WindowedValue<byte[]>> typeInfo = new CoderTypeInformation<>(WindowedValue.getFullCoder(ByteArrayCoder.of(), GlobalWindow.Coder.INSTANCE));    boolean keepSourceAlive = !context.getPipelineOptions().isShutdownSourcesOnFinalWatermark();    SingleOutputStreamOperator<WindowedValue<byte[]>> source = context.getExecutionEnvironment().addSource(new ImpulseSourceFunction(keepSourceAlive), "Impulse").returns(typeInfo);    context.addDataStream(Iterables.getOnlyElement(pTransform.getOutputsMap().values()), source);}
public static FlinkStreamingPipelineTranslator.StreamTransformTranslator<?> beam_f5143_0(PTransform<?, ?> transform)
{    @Nullable    String urn = PTransformTranslation.urnForTransformOrNull(transform);    return urn == null ? null : TRANSLATORS.get(urn);}
private static String beam_f5144_0(FlinkStreamingTranslationContext context)
{    return context.getCurrentTransform().getFullName();}
public void beam_f5145_0(PTransform<PBegin, PCollection<T>> transform, FlinkStreamingTranslationContext context)
{    PCollection<T> output = context.getOutput(transform);    DataStream<WindowedValue<T>> source;    DataStream<WindowedValue<ValueWithRecordId<T>>> nonDedupSource;    TypeInformation<WindowedValue<T>> outputTypeInfo = context.getTypeInfo(context.getOutput(transform));    Coder<T> coder = context.getOutput(transform).getCoder();    TypeInformation<WindowedValue<ValueWithRecordId<T>>> withIdTypeInfo = new CoderTypeInformation<>(WindowedValue.getFullCoder(ValueWithRecordId.ValueWithRecordIdCoder.of(coder), output.getWindowingStrategy().getWindowFn().windowCoder()));    UnboundedSource<T, ?> rawSource;    try {        rawSource = ReadTranslation.unboundedSourceFromTransform((AppliedPTransform<PBegin, PCollection<T>, PTransform<PBegin, PCollection<T>>>) context.getCurrentTransform());    } catch (IOException e) {        throw new RuntimeException(e);    }    String fullName = getCurrentTransformName(context);    try {        int parallelism = context.getExecutionEnvironment().getMaxParallelism() > 0 ? context.getExecutionEnvironment().getMaxParallelism() : context.getExecutionEnvironment().getParallelism();        UnboundedSourceWrapper<T, ?> sourceWrapper = new UnboundedSourceWrapper<>(fullName, context.getPipelineOptions(), rawSource, parallelism);        nonDedupSource = context.getExecutionEnvironment().addSource(sourceWrapper).name(fullName).uid(fullName).returns(withIdTypeInfo);        if (rawSource.requiresDeduping()) {            source = nonDedupSource.keyBy(new ValueWithRecordIdKeySelector<>()).transform("deduping", outputTypeInfo, new DedupingOperator<>()).uid(format("%s/__deduplicated__", fullName));        } else {            source = nonDedupSource.flatMap(new StripIdsMap<>()).returns(outputTypeInfo);        }    } catch (Exception e) {        throw new RuntimeException("Error while translating UnboundedSource: " + rawSource, e);    }    context.setOutputDataStream(output, source);}
 static void beam_f5153_0(String transformName, DoFn<InputT, OutputT> doFn, PCollection<InputT> input, List<PCollectionView<?>> sideInputs, Map<TupleTag<?>, PValue> outputs, TupleTag<OutputT> mainOutputTag, List<TupleTag<?>> additionalOutputTags, DoFnSchemaInformation doFnSchemaInformation, Map<String, PCollectionView<?>> sideInputMapping, FlinkStreamingTranslationContext context, DoFnOperatorFactory<InputT, OutputT> doFnOperatorFactory)
{        WindowingStrategy<?, ?> windowingStrategy = input.getWindowingStrategy();    Map<TupleTag<?>, OutputTag<WindowedValue<?>>> tagsToOutputTags = Maps.newHashMap();    Map<TupleTag<?>, Coder<WindowedValue<?>>> tagsToCoders = Maps.newHashMap();                    Map<TupleTag<?>, Integer> tagsToIds = Maps.newHashMap();    int idCount = 0;    tagsToIds.put(mainOutputTag, idCount++);    for (Map.Entry<TupleTag<?>, PValue> entry : outputs.entrySet()) {        if (!tagsToOutputTags.containsKey(entry.getKey())) {            tagsToOutputTags.put(entry.getKey(), new OutputTag<WindowedValue<?>>(entry.getKey().getId(), (TypeInformation) context.getTypeInfo((PCollection<?>) entry.getValue())));            tagsToCoders.put(entry.getKey(), (Coder) context.getWindowedInputCoder((PCollection<OutputT>) entry.getValue()));            tagsToIds.put(entry.getKey(), idCount++);        }    }    SingleOutputStreamOperator<WindowedValue<OutputT>> outputStream;    Coder<WindowedValue<InputT>> windowedInputCoder = context.getWindowedInputCoder(input);    Coder<InputT> inputCoder = context.getInputCoder(input);    Map<TupleTag<?>, Coder<?>> outputCoders = context.getOutputCoders();    DataStream<WindowedValue<InputT>> inputDataStream = context.getInputDataStream(input);    Coder keyCoder = null;    KeySelector<WindowedValue<InputT>, ?> keySelector = null;    boolean stateful = false;    DoFnSignature signature = DoFnSignatures.getSignature(doFn.getClass());    if (signature.stateDeclarations().size() > 0 || signature.timerDeclarations().size() > 0) {                        keyCoder = ((KvCoder) input.getCoder()).getKeyCoder();        keySelector = new KvToByteBufferKeySelector(keyCoder);        inputDataStream = inputDataStream.keyBy(keySelector);        stateful = true;    } else if (doFn instanceof SplittableParDoViaKeyedWorkItems.ProcessFn) {                keyCoder = ByteArrayCoder.of();        keySelector = new WorkItemKeySelector<>(keyCoder);        stateful = true;    }    CoderTypeInformation<WindowedValue<OutputT>> outputTypeInformation = new CoderTypeInformation<>(context.getWindowedInputCoder((PCollection<OutputT>) outputs.get(mainOutputTag)));    if (sideInputs.isEmpty()) {        DoFnOperator<InputT, OutputT> doFnOperator = doFnOperatorFactory.createDoFnOperator(doFn, getCurrentTransformName(context), sideInputs, mainOutputTag, additionalOutputTags, context, windowingStrategy, tagsToOutputTags, tagsToCoders, tagsToIds, windowedInputCoder, inputCoder, outputCoders, keyCoder, keySelector, new HashMap<>(), /* side-input mapping */        doFnSchemaInformation, sideInputMapping);        outputStream = inputDataStream.transform(transformName, outputTypeInformation, doFnOperator);    } else {        Tuple2<Map<Integer, PCollectionView<?>>, DataStream<RawUnionValue>> transformedSideInputs = transformSideInputs(sideInputs, context);        DoFnOperator<InputT, OutputT> doFnOperator = doFnOperatorFactory.createDoFnOperator(doFn, getCurrentTransformName(context), sideInputs, mainOutputTag, additionalOutputTags, context, windowingStrategy, tagsToOutputTags, tagsToCoders, tagsToIds, windowedInputCoder, inputCoder, outputCoders, keyCoder, keySelector, transformedSideInputs.f0, doFnSchemaInformation, sideInputMapping);        if (stateful) {                                    KeyedStream keyedStream = (KeyedStream<?, InputT>) inputDataStream;            TwoInputTransformation<WindowedValue<KV<?, InputT>>, RawUnionValue, WindowedValue<OutputT>> rawFlinkTransform = new TwoInputTransformation(keyedStream.getTransformation(), transformedSideInputs.f1.broadcast().getTransformation(), transformName, doFnOperator, outputTypeInformation, keyedStream.getParallelism());            rawFlinkTransform.setStateKeyType(keyedStream.getKeyType());            rawFlinkTransform.setStateKeySelectors(keyedStream.getKeySelector(), null);            outputStream = new SingleOutputStreamOperator(keyedStream.getExecutionEnvironment(),             rawFlinkTransform) {            };            keyedStream.getExecutionEnvironment().addOperator(rawFlinkTransform);        } else {            outputStream = inputDataStream.connect(transformedSideInputs.f1.broadcast()).transform(transformName, outputTypeInformation, doFnOperator);        }    }    outputStream.uid(transformName);    context.setOutputDataStream(outputs.get(mainOutputTag), outputStream);    for (Map.Entry<TupleTag<?>, PValue> entry : outputs.entrySet()) {        if (!entry.getKey().equals(mainOutputTag)) {            context.setOutputDataStream(entry.getValue(), outputStream.getSideOutput(tagsToOutputTags.get(entry.getKey())));        }    }}
public void beam_f5154_0(PTransform<PCollection<InputT>, PCollectionTuple> transform, FlinkStreamingTranslationContext context)
{    DoFn<InputT, OutputT> doFn;    try {        doFn = (DoFn<InputT, OutputT>) ParDoTranslation.getDoFn(context.getCurrentTransform());    } catch (IOException e) {        throw new RuntimeException(e);    }    TupleTag<OutputT> mainOutputTag;    try {        mainOutputTag = (TupleTag<OutputT>) ParDoTranslation.getMainOutputTag(context.getCurrentTransform());    } catch (IOException e) {        throw new RuntimeException(e);    }    List<PCollectionView<?>> sideInputs;    try {        sideInputs = ParDoTranslation.getSideInputs(context.getCurrentTransform());    } catch (IOException e) {        throw new RuntimeException(e);    }    Map<String, PCollectionView<?>> sideInputMapping = ParDoTranslation.getSideInputMapping(context.getCurrentTransform());    TupleTagList additionalOutputTags;    try {        additionalOutputTags = ParDoTranslation.getAdditionalOutputTags(context.getCurrentTransform());    } catch (IOException e) {        throw new RuntimeException(e);    }    DoFnSchemaInformation doFnSchemaInformation;    doFnSchemaInformation = ParDoTranslation.getSchemaInformation(context.getCurrentTransform());    ParDoTranslationHelper.translateParDo(getCurrentTransformName(context), doFn, context.getInput(transform), sideInputs, context.getOutputs(transform), mainOutputTag, additionalOutputTags.getAll(), doFnSchemaInformation, sideInputMapping, context, (doFn1, stepName, sideInputs1, mainOutputTag1, additionalOutputTags1, context1, windowingStrategy, tagsToOutputTags, tagsToCoders, tagsToIds, windowedInputCoder, inputCoder, outputCoders1, keyCoder, keySelector, transformedSideInputs, doFnSchemaInformation1, sideInputMapping1) -> new DoFnOperator<>(doFn1, stepName, windowedInputCoder, inputCoder, outputCoders1, mainOutputTag1, additionalOutputTags1, new DoFnOperator.MultiOutputOutputManagerFactory<>(mainOutputTag1, tagsToOutputTags, tagsToCoders, tagsToIds), windowingStrategy, transformedSideInputs, sideInputs1, context1.getPipelineOptions(), keyCoder, keySelector, doFnSchemaInformation1, sideInputMapping1));}
public void beam_f5155_0(SplittableParDoViaKeyedWorkItems.ProcessElements<InputT, OutputT, RestrictionT, PositionT> transform, FlinkStreamingTranslationContext context)
{    ParDoTranslationHelper.translateParDo(getCurrentTransformName(context), transform.newProcessFn(transform.getFn()), context.getInput(transform), transform.getSideInputs(), context.getOutputs(transform), transform.getMainOutputTag(), transform.getAdditionalOutputTags().getAll(), DoFnSchemaInformation.create(), Collections.emptyMap(), context, (doFn, stepName, sideInputs, mainOutputTag, additionalOutputTags, context1, windowingStrategy, tagsToOutputTags, tagsToCoders, tagsToIds, windowedInputCoder, inputCoder, outputCoders1, keyCoder, keySelector, transformedSideInputs, doFnSchemaInformation, sideInputMapping) -> new SplittableDoFnOperator<>(doFn, stepName, windowedInputCoder, inputCoder, outputCoders1, mainOutputTag, additionalOutputTags, new DoFnOperator.MultiOutputOutputManagerFactory<>(mainOutputTag, tagsToOutputTags, tagsToCoders, tagsToIds), windowingStrategy, transformedSideInputs, sideInputs, context1.getPipelineOptions(), keyCoder, keySelector));}
public void beam_f5163_0(PTransform<PCollection<KV<K, InputT>>, PCollection<KeyedWorkItem<K, InputT>>> transform, FlinkStreamingTranslationContext context)
{    PCollection<KV<K, InputT>> input = context.getInput(transform);    KvCoder<K, InputT> inputKvCoder = (KvCoder<K, InputT>) input.getCoder();    SingletonKeyedWorkItemCoder<K, InputT> workItemCoder = SingletonKeyedWorkItemCoder.of(inputKvCoder.getKeyCoder(), inputKvCoder.getValueCoder(), input.getWindowingStrategy().getWindowFn().windowCoder());    WindowedValue.ValueOnlyWindowedValueCoder<SingletonKeyedWorkItem<K, InputT>> windowedWorkItemCoder = WindowedValue.getValueOnlyCoder(workItemCoder);    CoderTypeInformation<WindowedValue<SingletonKeyedWorkItem<K, InputT>>> workItemTypeInfo = new CoderTypeInformation<>(windowedWorkItemCoder);    DataStream<WindowedValue<KV<K, InputT>>> inputDataStream = context.getInputDataStream(input);    DataStream<WindowedValue<SingletonKeyedWorkItem<K, InputT>>> workItemStream = inputDataStream.flatMap(new ToKeyedWorkItemInGlobalWindow<>()).returns(workItemTypeInfo).name("ToKeyedWorkItem");    KeyedStream<WindowedValue<SingletonKeyedWorkItem<K, InputT>>, ByteBuffer> keyedWorkItemStream = workItemStream.keyBy(new WorkItemKeySelector<>(inputKvCoder.getKeyCoder()));    context.setOutputDataStream(context.getOutput(transform), keyedWorkItemStream);}
public void beam_f5164_0(WindowedValue<KV<K, InputT>> inWithMultipleWindows, Collector<WindowedValue<SingletonKeyedWorkItem<K, InputT>>> out) throws Exception
{        for (WindowedValue<KV<K, InputT>> in : inWithMultipleWindows.explodeWindows()) {        SingletonKeyedWorkItem<K, InputT> workItem = new SingletonKeyedWorkItem<>(in.getValue().getKey(), in.withValue(in.getValue().getValue()));        out.collect(WindowedValue.valueInGlobalWindow(workItem));    }}
public void beam_f5165_0(PTransform<PCollection<T>, PCollection<T>> transform, FlinkStreamingTranslationContext context)
{    Map<TupleTag<?>, PValue> allInputs = context.getInputs(transform);    if (allInputs.isEmpty()) {                                DataStreamSource<String> dummySource = context.getExecutionEnvironment().fromElements("dummy");        DataStream<WindowedValue<T>> result = dummySource.<WindowedValue<T>>flatMap((s, collector) -> {                }).returns(new CoderTypeInformation<>(WindowedValue.getFullCoder((Coder<T>) VoidCoder.of(), GlobalWindow.Coder.INSTANCE)));        context.setOutputDataStream(context.getOutput(transform), result);    } else {        DataStream<T> result = null;                                Map<DataStream<T>, Integer> duplicates = new HashMap<>();        for (PValue input : allInputs.values()) {            DataStream<T> current = context.getInputDataStream(input);            Integer oldValue = duplicates.put(current, 1);            if (oldValue != null) {                duplicates.put(current, oldValue + 1);            }        }        for (PValue input : allInputs.values()) {            DataStream<T> current = context.getInputDataStream(input);            final Integer timesRequired = duplicates.get(current);            if (timesRequired > 1) {                current = current.flatMap(new FlatMapFunction<T, T>() {                    private static final long serialVersionUID = 1L;                    @Override                    public void flatMap(T t, Collector<T> collector) throws Exception {                        collector.collect(t);                    }                });            }            result = (result == null) ? current : result.union(current);        }        context.setOutputDataStream(context.getOutput(transform), result);    }}
 void beam_f5173_0(TestStream<T> testStream, FlinkStreamingTranslationContext context)
{    Coder<T> valueCoder = testStream.getValueCoder();        TestStream.TestStreamCoder<T> testStreamCoder = TestStream.TestStreamCoder.of(valueCoder);    final byte[] payload;    try {        payload = CoderUtils.encodeToByteArray(testStreamCoder, testStream);    } catch (CoderException e) {        throw new RuntimeException("Could not encode TestStream.", e);    }    SerializableFunction<byte[], TestStream<T>> testStreamDecoder = bytes -> {        try {            return CoderUtils.decodeFromByteArray(TestStream.TestStreamCoder.of(valueCoder), bytes);        } catch (CoderException e) {            throw new RuntimeException("Can't decode TestStream payload.", e);        }    };    WindowedValue.FullWindowedValueCoder<T> elementCoder = WindowedValue.getFullCoder(valueCoder, GlobalWindow.Coder.INSTANCE);    DataStreamSource<WindowedValue<T>> source = context.getExecutionEnvironment().addSource(new TestStreamSource<>(testStreamDecoder, payload), new CoderTypeInformation<>(elementCoder));    context.setOutputDataStream(context.getOutput(testStream), source);}
 UnboundedSourceWrapper<OutputT, CheckpointMarkT> beam_f5174_0()
{    return unboundedSourceWrapper;}
public void beam_f5175_0(Configuration parameters) throws Exception
{    unboundedSourceWrapper.setRuntimeContext(getRuntimeContext());    unboundedSourceWrapper.open(parameters);}
public void beam_f5183_0(WindowedValue<ValueWithRecordId<OutputT>> element)
{    OutputT originalValue = element.getValue().getValue();    WindowedValue<OutputT> output = WindowedValue.of(originalValue, element.getTimestamp(), element.getWindows(), element.getPane());    ctx.collect(output);}
public void beam_f5184_0(WindowedValue<ValueWithRecordId<OutputT>> element, long timestamp)
{    OutputT originalValue = element.getValue().getValue();    WindowedValue<OutputT> output = WindowedValue.of(originalValue, element.getTimestamp(), element.getWindows(), element.getPane());    ctx.collectWithTimestamp(output, timestamp);}
public void beam_f5185_0(Watermark mark)
{    ctx.emitWatermark(mark);}
public void beam_f5193_0(AppliedPTransform<?, ?, ?> currentTransform)
{    this.currentTransform = currentTransform;}
public Coder<WindowedValue<T>> beam_f5194_0(PCollection<T> collection)
{    Coder<T> valueCoder = collection.getCoder();    return WindowedValue.getFullCoder(valueCoder, collection.getWindowingStrategy().getWindowFn().windowCoder());}
public Coder<T> beam_f5195_0(PCollection<T> collection)
{    return collection.getCoder();}
 static List<PTransformOverride> beam_f5203_0(FlinkPipelineOptions options)
{    ImmutableList.Builder<PTransformOverride> builder = ImmutableList.builder();    builder.add(PTransformOverride.of(PTransformMatchers.splittableParDo(), new SplittableParDo.OverrideFactory())).add(PTransformOverride.of(PTransformMatchers.urnEqualTo(PTransformTranslation.SPLITTABLE_PROCESS_KEYED_URN), options.isStreaming() ? new SplittableParDoViaKeyedWorkItems.OverrideFactory() : new SplittableParDoNaiveBounded.OverrideFactory()));    if (options.isStreaming()) {        builder.add(PTransformOverride.of(FlinkStreamingPipelineTranslator.StreamingShardedWriteFactory.writeFilesNeedsOverrides(), new FlinkStreamingPipelineTranslator.StreamingShardedWriteFactory(checkNotNull(options)))).add(PTransformOverride.of(PTransformMatchers.urnEqualTo(PTransformTranslation.CREATE_VIEW_TRANSFORM_URN), CreateStreamingFlinkView.Factory.INSTANCE));    }    return builder.build();}
public void beam_f5204_0()
{    try (Closeable ignored = MetricsEnvironment.scopedMetricsContainer(container.getMetricsContainer(stepName))) {        delegate.startBundle();    } catch (IOException e) {        throw new RuntimeException(e);    }}
public void beam_f5205_0(final WindowedValue<InputT> elem)
{    try (Closeable ignored = MetricsEnvironment.scopedMetricsContainer(container.getMetricsContainer(stepName))) {        delegate.processElement(elem);    } catch (IOException e) {        throw new RuntimeException(e);    }}
public MetricsContainerImpl beam_f5213_0(String stepName)
{    return metricsAccumulator != null ? metricsAccumulator.getLocalValue().getContainer(stepName) : null;}
public void beam_f5214_0(String stepName, List<MonitoringInfo> monitoringInfos)
{    getMetricsContainer(stepName).update(monitoringInfos);    updateMetrics(stepName);}
 void beam_f5215_0(String stepName)
{    MetricResults metricResults = asAttemptedOnlyMetricResults(metricsAccumulator.getLocalValue());    MetricQueryResults metricQueryResults = metricResults.queryMetrics(MetricsFilter.builder().addStep(stepName).build());    updateCounters(metricQueryResults.getCounters());    updateDistributions(metricQueryResults.getDistributions());    updateGauge(metricQueryResults.getGauges());}
public GaugeResult beam_f5223_0()
{    return data;}
public static String beam_f5224_0(Metric metric)
{    if (metric instanceof Counter) {        return Long.toString(((Counter) metric).getCount());    } else if (metric instanceof Gauge) {        return ((Gauge) metric).getValue().toString();    } else if (metric instanceof Meter) {        return Double.toString(((Meter) metric).getRate());    } else if (metric instanceof Histogram) {        HistogramStatistics stats = ((Histogram) metric).getStatistics();        return String.format("count=%d, min=%d, max=%d, mean=%f, stddev=%f, p50=%f, p75=%f, p95=%f", stats.size(), stats.getMin(), stats.getMax(), stats.getMean(), stats.getStdDev(), stats.getQuantile(0.5), stats.getQuantile(0.75), stats.getQuantile(0.95));    } else {        throw new IllegalStateException(String.format("Cannot remove unknown metric type %s. This indicates that the reporter " + "does not support this metric type.", metric.getClass().getName()));    }}
public void beam_f5225_0(MetricsContainerStepMap value)
{    metricsContainers.updateAll(value);}
public void beam_f5233_0(Pipeline pipeline)
{    super.translate(pipeline);}
public CompositeBehavior beam_f5234_0(TransformHierarchy.Node node)
{    return CompositeBehavior.ENTER_TRANSFORM;}
public void beam_f5236_1(TransformHierarchy.Node node)
{    AppliedPTransform<?, ?, ?> appliedPTransform = node.toAppliedPTransform(getPipeline());    if (hasUnboundedOutput(appliedPTransform)) {        Class<? extends PTransform> transformClass = node.getTransform().getClass();                hasUnboundedCollections = true;    }}
public OutputT beam_f5245_0(K key, AccumT accumulator, PipelineOptions options, SideInputReader sideInputReader, Collection<? extends BoundedWindow> windows)
{    return combineFnRunner.extractOutput(accumulator, options, sideInputReader, windows);}
public AccumT beam_f5246_0(K key, InputT value, PipelineOptions options, SideInputReader sideInputReader, Collection<? extends BoundedWindow> windows)
{    AccumT accumulator = combineFnRunner.createAccumulator(options, sideInputReader, windows);    return combineFnRunner.addInput(accumulator, value, options, sideInputReader, windows);}
public AccumT beam_f5247_0(K key, AccumT accumulator, InputT value, PipelineOptions options, SideInputReader sideInputReader, Collection<? extends BoundedWindow> windows)
{    return combineFnRunner.addInput(accumulator, value, options, sideInputReader, windows);}
public void beam_f5255_0(WindowedValue<T> input, Collector<WindowedValue<T>> collector) throws Exception
{    Collection<W> windows = windowFn.assignWindows(new FlinkAssignContext<>(windowFn, input));    for (W window : windows) {        collector.collect(WindowedValue.of(input.getValue(), input.getTimestamp(), window, input.getPane()));    }}
public void beam_f5256_0(Iterable<WindowedValue<InputT>> values, Collector<WindowedValue<OutputT>> out) throws Exception
{    RuntimeContext runtimeContext = getRuntimeContext();    DoFnRunners.OutputManager outputManager;    if (outputMap.size() == 1) {        outputManager = new FlinkDoFnFunction.DoFnOutputManager(out);    } else {                outputManager = new FlinkDoFnFunction.MultiDoFnOutputManager((Collector) out, outputMap);    }    List<TupleTag<?>> additionalOutputTags = Lists.newArrayList(outputMap.keySet());    DoFnRunner<InputT, OutputT> doFnRunner = DoFnRunners.simpleRunner(serializedOptions.get(), doFn, new FlinkSideInputReader(sideInputs, runtimeContext), outputManager, mainOutputTag, additionalOutputTags, new FlinkNoOpStepContext(), inputCoder, outputCoderMap, windowingStrategy, doFnSchemaInformation, sideInputMapping);    if ((serializedOptions.get().as(FlinkPipelineOptions.class)).getEnableMetrics()) {        doFnRunner = new DoFnRunnerWithMetricsUpdate<>(stepName, doFnRunner, getRuntimeContext());    }    doFnRunner.startBundle();    for (WindowedValue<InputT> value : values) {        doFnRunner.processElement(value);    }    doFnRunner.finishBundle();}
public void beam_f5257_0(Configuration parameters) throws Exception
{    doFnInvoker = DoFnInvokers.tryInvokeSetupFor(doFn);}
public void beam_f5265_0(ProcessBundleResponse response)
{    container.updateMetrics(stageName, response.getMonitoringInfosList());}
private StateRequestHandler beam_f5266_0(ExecutableStage executableStage, ProcessBundleDescriptors.ExecutableProcessBundleDescriptor processBundleDescriptor, RuntimeContext runtimeContext)
{    final StateRequestHandler sideInputHandler;    StateRequestHandlers.SideInputHandlerFactory sideInputHandlerFactory = BatchSideInputHandlerFactory.forStage(executableStage, runtimeContext::getBroadcastVariable);    try {        sideInputHandler = StateRequestHandlers.forSideInputHandlerFactory(ProcessBundleDescriptors.getSideInputs(executableStage), sideInputHandlerFactory);    } catch (IOException e) {        throw new RuntimeException("Failed to setup state handler", e);    }    final StateRequestHandler userStateHandler;    if (executableStage.getUserStates().size() > 0) {        bagUserStateHandlerFactory = new InMemoryBagUserStateFactory();        userStateHandler = StateRequestHandlers.forBagUserStateHandlerFactory(processBundleDescriptor, bagUserStateHandlerFactory);    } else {        userStateHandler = StateRequestHandler.unsupported();    }    EnumMap<StateKey.TypeCase, StateRequestHandler> handlerMap = new EnumMap<>(StateKey.TypeCase.class);    handlerMap.put(StateKey.TypeCase.MULTIMAP_SIDE_INPUT, sideInputHandler);    handlerMap.put(StateKey.TypeCase.BAG_USER_STATE, userStateHandler);    return StateRequestHandlers.delegateBasedUponType(handlerMap);}
public void beam_f5267_0(Iterable<WindowedValue<InputT>> iterable, Collector<RawUnionValue> collector) throws Exception
{    ReceiverFactory receiverFactory = new ReceiverFactory(collector, outputMap);    try (RemoteBundle bundle = stageBundleFactory.getBundle(receiverFactory, stateRequestHandler, progressHandler)) {        processElements(iterable, bundle);    }}
public StateInternals beam_f5275_0()
{    throw new UnsupportedOperationException("stateInternals is not supported");}
public TimerInternals beam_f5276_0()
{    throw new UnsupportedOperationException("timerInternals is not supported");}
public void beam_f5277_0(Iterable<WindowedValue<KV<K, InputT>>> elements, Collector<WindowedValue<KV<K, AccumT>>> out) throws Exception
{    PipelineOptions options = serializedOptions.get();    FlinkSideInputReader sideInputReader = new FlinkSideInputReader(sideInputs, getRuntimeContext());    AbstractFlinkCombineRunner<K, InputT, AccumT, AccumT, W> reduceRunner;    if (!windowingStrategy.getWindowFn().isNonMerging() && !windowingStrategy.getWindowFn().windowCoder().equals(IntervalWindow.getCoder())) {        reduceRunner = new HashingFlinkCombineRunner<>();    } else {        reduceRunner = new SortingFlinkCombineRunner<>();    }    reduceRunner.combine(new AbstractFlinkCombineRunner.PartialFlinkCombiner<>(combineFn), windowingStrategy, sideInputReader, options, elements, out);}
private void beam_f5285_0(InMemoryTimerInternals timerInternals, DoFnRunner<KV<K, V>, OutputT> runner) throws Exception
{    while (true) {        TimerInternals.TimerData timer;        boolean hasFired = false;        while ((timer = timerInternals.removeNextEventTimer()) != null) {            hasFired = true;            fireTimer(timer, runner);        }        while ((timer = timerInternals.removeNextProcessingTimer()) != null) {            hasFired = true;            fireTimer(timer, runner);        }        while ((timer = timerInternals.removeNextSynchronizedProcessingTimer()) != null) {            hasFired = true;            fireTimer(timer, runner);        }        if (!hasFired) {            break;        }    }}
private void beam_f5286_0(TimerInternals.TimerData timer, DoFnRunner<KV<K, V>, OutputT> doFnRunner)
{    StateNamespace namespace = timer.getNamespace();    checkArgument(namespace instanceof StateNamespaces.WindowNamespace);    BoundedWindow window = ((StateNamespaces.WindowNamespace) namespace).getWindow();    doFnRunner.onTimer(timer.getTimerId(), window, timer.getTimestamp(), timer.getDomain());}
public void beam_f5287_0(Configuration parameters) throws Exception
{    doFnInvoker = DoFnInvokers.tryInvokeSetupFor(dofn);}
public Iterable<V> beam_f5295_0(byte[] key, W window)
{    Iterable<KV<K, V>> values = (Iterable<KV<K, V>>) runnerHandler.getIterable(collection, window);    ArrayList<V> result = new ArrayList<>();        for (KV<K, V> kv : values) {        ByteArrayOutputStream bos = new ByteArrayOutputStream();        try {            keyCoder.encode(kv.getKey(), bos);            if (Arrays.equals(key, bos.toByteArray())) {                result.add(kv.getValue());            }        } catch (IOException ex) {            throw new RuntimeException(ex);        }    }    return result;}
public Coder<V> beam_f5296_0()
{    return valueCoder;}
public void beam_f5297_0(FlinkCombiner<K, InputT, AccumT, OutputT> flinkCombiner, WindowingStrategy<Object, W> windowingStrategy, SideInputReader sideInputReader, PipelineOptions options, Iterable<WindowedValue<KV<K, InputT>>> elements, Collector<WindowedValue<KV<K, OutputT>>> out) throws Exception
{    @SuppressWarnings("unchecked")    TimestampCombiner timestampCombiner = windowingStrategy.getTimestampCombiner();    WindowFn<Object, W> windowFn = windowingStrategy.getWindowFn();        List<WindowedValue<KV<K, InputT>>> inputs = new ArrayList<>();    Iterables.addAll(inputs, elements);    Set<W> windows = collectWindows(inputs);    Map<W, W> windowToMergeResult = mergeWindows(windowingStrategy, windows);        Map<W, Tuple2<AccumT, Instant>> mapState = new HashMap<>();    Iterator<WindowedValue<KV<K, InputT>>> iterator = inputs.iterator();    WindowedValue<KV<K, InputT>> currentValue = iterator.next();    K key = currentValue.getValue().getKey();    do {        for (BoundedWindow w : currentValue.getWindows()) {            @SuppressWarnings("unchecked")            W currentWindow = (W) w;            W mergedWindow = windowToMergeResult.get(currentWindow);            mergedWindow = mergedWindow == null ? currentWindow : mergedWindow;            Set<W> singletonW = Collections.singleton(mergedWindow);            Tuple2<AccumT, Instant> accumAndInstant = mapState.get(mergedWindow);            if (accumAndInstant == null) {                AccumT accumT = flinkCombiner.firstInput(key, currentValue.getValue().getValue(), options, sideInputReader, singletonW);                Instant windowTimestamp = timestampCombiner.assign(mergedWindow, windowFn.getOutputTime(currentValue.getTimestamp(), mergedWindow));                accumAndInstant = new Tuple2<>(accumT, windowTimestamp);                mapState.put(mergedWindow, accumAndInstant);            } else {                accumAndInstant.f0 = flinkCombiner.addInput(key, accumAndInstant.f0, currentValue.getValue().getValue(), options, sideInputReader, singletonW);                accumAndInstant.f1 = timestampCombiner.combine(accumAndInstant.f1, timestampCombiner.assign(mergedWindow, windowingStrategy.getWindowFn().getOutputTime(currentValue.getTimestamp(), mergedWindow)));            }        }        if (iterator.hasNext()) {            currentValue = iterator.next();        } else {            break;        }    } while (true);        for (Map.Entry<W, Tuple2<AccumT, Instant>> entry : mapState.entrySet()) {        AccumT accumulator = entry.getValue().f0;        Instant windowTimestamp = entry.getValue().f1;        out.collect(WindowedValue.of(KV.of(key, flinkCombiner.extractOutput(key, accumulator, options, sideInputReader, Collections.singleton(entry.getKey()))), windowTimestamp, entry.getKey(), PaneInfo.NO_FIRING));    }}
public Map<BoundedWindow, ViewT> beam_f5306_0(Iterable<WindowedValue<?>> inputValues)
{        Map<BoundedWindow, List<WindowedValue<KV<?, ?>>>> partitionedElements = new HashMap<>();    for (WindowedValue<KV<?, ?>> value : (Iterable<WindowedValue<KV<?, ?>>>) (Iterable) inputValues) {        for (BoundedWindow window : value.getWindows()) {            List<WindowedValue<KV<?, ?>>> windowedValues = partitionedElements.computeIfAbsent(window, k -> new ArrayList<>());            windowedValues.add(value);        }    }    Map<BoundedWindow, ViewT> resultMap = new HashMap<>();    for (Map.Entry<BoundedWindow, List<WindowedValue<KV<?, ?>>>> elements : partitionedElements.entrySet()) {        ViewFn<MultimapView, ViewT> viewFn = (ViewFn<MultimapView, ViewT>) view.getViewFn();        Coder keyCoder = ((KvCoder<?, ?>) view.getCoderInternal()).getKeyCoder();        resultMap.put(elements.getKey(), (ViewT) viewFn.apply(InMemoryMultimapSideInputView.fromIterable(keyCoder, (Iterable) elements.getValue().stream().map(WindowedValue::getValue).collect(Collectors.toList()))));    }    return resultMap;}
public void beam_f5307_0(FlinkCombiner<K, InputT, AccumT, OutputT> flinkCombiner, WindowingStrategy<Object, W> windowingStrategy, SideInputReader sideInputReader, PipelineOptions options, Iterable<WindowedValue<KV<K, InputT>>> elements, Collector<WindowedValue<KV<K, OutputT>>> out) throws Exception
{    @SuppressWarnings("unchecked")    TimestampCombiner timestampCombiner = (TimestampCombiner) windowingStrategy.getTimestampCombiner();    WindowFn<Object, W> windowFn = windowingStrategy.getWindowFn();                List<WindowedValue<KV<K, InputT>>> sortedInput = Lists.newArrayList();    for (WindowedValue<KV<K, InputT>> inputValue : elements) {        for (WindowedValue<KV<K, InputT>> exploded : inputValue.explodeWindows()) {            sortedInput.add(exploded);        }    }    sortedInput.sort(Comparator.comparing(o -> Iterables.getOnlyElement(o.getWindows()).maxTimestamp()));    if (!windowingStrategy.getWindowFn().isNonMerging()) {                                mergeWindow(sortedInput);    }        final Iterator<WindowedValue<KV<K, InputT>>> iterator = sortedInput.iterator();        WindowedValue<KV<K, InputT>> currentValue = iterator.next();    K key = currentValue.getValue().getKey();    W currentWindow = (W) Iterables.getOnlyElement(currentValue.getWindows());    InputT firstValue = currentValue.getValue().getValue();    AccumT accumulator = flinkCombiner.firstInput(key, firstValue, options, sideInputReader, currentValue.getWindows());        Instant windowTimestamp = timestampCombiner.assign(currentWindow, windowFn.getOutputTime(currentValue.getTimestamp(), currentWindow));    while (iterator.hasNext()) {        WindowedValue<KV<K, InputT>> nextValue = iterator.next();        W nextWindow = (W) Iterables.getOnlyElement(nextValue.getWindows());        if (currentWindow.equals(nextWindow)) {                        InputT value = nextValue.getValue().getValue();            accumulator = flinkCombiner.addInput(key, accumulator, value, options, sideInputReader, currentValue.getWindows());            windowTimestamp = timestampCombiner.combine(windowTimestamp, timestampCombiner.assign(currentWindow, windowFn.getOutputTime(nextValue.getTimestamp(), currentWindow)));        } else {                        out.collect(WindowedValue.of(KV.of(key, flinkCombiner.extractOutput(key, accumulator, options, sideInputReader, currentValue.getWindows())), windowTimestamp, currentWindow, PaneInfo.NO_FIRING));            currentWindow = nextWindow;            currentValue = nextValue;            InputT value = nextValue.getValue().getValue();            accumulator = flinkCombiner.firstInput(key, value, options, sideInputReader, currentValue.getWindows());            windowTimestamp = timestampCombiner.assign(currentWindow, windowFn.getOutputTime(nextValue.getTimestamp(), currentWindow));        }    }        out.collect(WindowedValue.of(KV.of(key, flinkCombiner.extractOutput(key, accumulator, options, sideInputReader, currentValue.getWindows())), windowTimestamp, currentWindow, PaneInfo.NO_FIRING));}
private void beam_f5308_0(List<WindowedValue<KV<K, InputT>>> elements)
{    int currentStart = 0;    IntervalWindow currentWindow = (IntervalWindow) Iterables.getOnlyElement(elements.get(0).getWindows());    for (int i = 1; i < elements.size(); i++) {        WindowedValue<KV<K, InputT>> nextValue = elements.get(i);        IntervalWindow nextWindow = (IntervalWindow) Iterables.getOnlyElement(nextValue.getWindows());        if (currentWindow.intersects(nextWindow)) {                        currentWindow = currentWindow.span(nextWindow);        } else {                        for (int j = i - 1; j >= currentStart; j--) {                WindowedValue<KV<K, InputT>> value = elements.get(j);                elements.set(j, WindowedValue.of(value.getValue(), value.getTimestamp(), currentWindow, value.getPane()));            }            currentStart = i;            currentWindow = nextWindow;        }    }    if (currentStart < elements.size() - 1) {                for (int j = elements.size() - 1; j >= currentStart; j--) {            WindowedValue<KV<K, InputT>> value = elements.get(j);            elements.set(j, WindowedValue.of(value.getValue(), value.getTimestamp(), currentWindow, value.getPane()));        }    }}
public int beam_f5316_0()
{    return 2;}
public boolean beam_f5317_0(Object o)
{    if (this == o) {        return true;    }    if (o == null || getClass() != o.getClass()) {        return false;    }    CoderTypeInformation that = (CoderTypeInformation) o;    return coder.equals(that.coder);}
public int beam_f5318_0()
{    return coder.hashCode();}
public int beam_f5326_0(byte[] first, byte[] second)
{    int len = Math.min(first.length, second.length);    for (int i = 0; i < len; i++) {        byte b1 = first[i];        byte b2 = second[i];        int result = (b1 < b2 ? -1 : (b1 == b2 ? 0 : 1));        if (result != 0) {            return ascending ? result : -result;        }    }    int result = first.length - second.length;    return ascending ? result : -result;}
public int beam_f5327_0(DataInputView firstSource, DataInputView secondSource) throws IOException
{    int lengthFirst = firstSource.readInt();    int lengthSecond = secondSource.readInt();    int len = Math.min(lengthFirst, lengthSecond);    for (int i = 0; i < len; i++) {        byte b1 = firstSource.readByte();        byte b2 = secondSource.readByte();        int result = (b1 < b2 ? -1 : (b1 == b2 ? 0 : 1));        if (result != 0) {            return ascending ? result : -result;        }    }    int result = lengthFirst - lengthSecond;    return ascending ? result : -result;}
public boolean beam_f5328_0()
{        return false;}
public TypeComparator<byte[]> beam_f5336_0()
{    return new EncodedValueComparator(ascending);}
public int beam_f5337_0(Object record, Object[] target, int index)
{    target[index] = record;    return 1;}
public TypeComparator[] beam_f5338_0()
{    return new TypeComparator[] { this.duplicate() };}
public boolean beam_f5346_0(Object other)
{    return other instanceof EncodedValueTypeInformation;}
public int beam_f5347_0()
{    return this.getClass().hashCode();}
public boolean beam_f5348_0(Object obj)
{    return obj instanceof EncodedValueTypeInformation;}
public boolean beam_f5358_0(long time, @Nonnull TimeUnit unit)
{    return true;}
public Condition beam_f5360_0()
{    throw new UnsupportedOperationException("Not implemented");}
public int beam_f5361_0() throws IOException
{    try {        return inputView.readUnsignedByte();    } catch (EOFException e) {                return -1;    }}
public GenericInputSplit[] beam_f5370_0(int numSplits)
{        return new GenericInputSplit[] { new GenericInputSplit(1, 1) };}
public InputSplitAssigner beam_f5371_0(GenericInputSplit[] genericInputSplits)
{    return new DefaultInputSplitAssigner(genericInputSplits);}
public void beam_f5372_0(GenericInputSplit genericInputSplit)
{    availableOutput = true;}
public float beam_f5381_0()
{    return BaseStatistics.AVG_RECORD_BYTES_UNKNOWN;}
public SourceInputSplit<T>[] beam_f5382_0(int numSplits) throws IOException
{    try {        long desiredSizeBytes = initialSource.getEstimatedSizeBytes(options) / numSplits;        List<? extends Source<T>> shards = initialSource.split(desiredSizeBytes, options);        int numShards = shards.size();        SourceInputSplit<T>[] sourceInputSplits = new SourceInputSplit[numShards];        for (int i = 0; i < numShards; i++) {            sourceInputSplits[i] = new SourceInputSplit<>(shards.get(i), i);        }        return sourceInputSplits;    } catch (Exception e) {        throw new IOException("Could not create input splits from Source.", e);    }}
public InputSplitAssigner beam_f5383_0(final SourceInputSplit[] sourceInputSplits)
{    return new DefaultInputSplitAssigner(sourceInputSplits);}
public void beam_f5391_0(StreamTask<?, ?> containingTask, StreamConfig config, Output<StreamRecord<WindowedValue<OutputT>>> output)
{        FlinkPipelineOptions options = serializedOptions.get().as(FlinkPipelineOptions.class);    FileSystems.setDefaultPipelineOptions(options);    super.setup(containingTask, config, output);}
public void beam_f5392_0(StateInitializationContext context) throws Exception
{    super.initializeState(context);    ListStateDescriptor<WindowedValue<InputT>> pushedBackStateDescriptor = new ListStateDescriptor<>("pushed-back-elements", new CoderTypeSerializer<>(windowedInputCoder));    if (keySelector != null) {        pushedBackElementsHandler = KeyedPushedBackElementsHandler.create(keySelector, getKeyedStateBackend(), pushedBackStateDescriptor);    } else {        ListState<WindowedValue<InputT>> listState = getOperatorStateBackend().getListState(pushedBackStateDescriptor);        pushedBackElementsHandler = NonKeyedPushedBackElementsHandler.create(listState);    }    setCurrentInputWatermark(BoundedWindow.TIMESTAMP_MIN_VALUE.getMillis());    setCurrentSideInputWatermark(BoundedWindow.TIMESTAMP_MIN_VALUE.getMillis());    setCurrentOutputWatermark(BoundedWindow.TIMESTAMP_MIN_VALUE.getMillis());    sideInputReader = NullSideInputReader.of(sideInputs);    if (!sideInputs.isEmpty()) {        FlinkBroadcastStateInternals sideInputStateInternals = new FlinkBroadcastStateInternals<>(getContainingTask().getIndexInSubtaskGroup(), getOperatorStateBackend());        sideInputHandler = new SideInputHandler(sideInputs, sideInputStateInternals);        sideInputReader = sideInputHandler;        Stream<WindowedValue<InputT>> pushedBack = pushedBackElementsHandler.getElements();        long min = pushedBack.map(v -> v.getTimestamp().getMillis()).reduce(Long.MAX_VALUE, Math::min);        setPushedBackWatermark(min);    } else {        setPushedBackWatermark(Long.MAX_VALUE);    }        if (keyCoder != null) {        keyedStateInternals = new FlinkStateInternals<>((KeyedStateBackend) getKeyedStateBackend(), keyCoder);        if (doFn != null) {            DoFnSignature signature = DoFnSignatures.getSignature(doFn.getClass());            FlinkStateInternals.EarlyBinder earlyBinder = new FlinkStateInternals.EarlyBinder(getKeyedStateBackend());            for (DoFnSignature.StateDeclaration value : signature.stateDeclarations().values()) {                StateSpec<?> spec = (StateSpec<?>) signature.stateDeclarations().get(value.id()).field().get(doFn);                spec.bind(value.id(), earlyBinder);            }        }        if (timerService == null) {            timerService = getInternalTimerService("beam-timer", new CoderTypeSerializer<>(timerCoder), this);        }        timerInternals = new FlinkTimerInternals();    }    outputManager = outputManagerFactory.create(output, getLockToAcquireForStateAccessDuringBundles(), getOperatorStateBackend(), getKeyedStateBackend(), keySelector);}
protected Lock beam_f5393_0()
{    return NoopLock.get();}
public final void beam_f5401_0(StreamRecord<WindowedValue<InputT>> streamRecord) throws Exception
{    checkInvokeStartBundle();    Iterable<WindowedValue<InputT>> justPushedBack = pushbackDoFnRunner.processElementInReadyWindows(streamRecord.getValue());    long min = pushedBackWatermark;    for (WindowedValue<InputT> pushedBackValue : justPushedBack) {        min = Math.min(min, pushedBackValue.getTimestamp().getMillis());        pushedBackElementsHandler.pushBack(pushedBackValue);    }    setPushedBackWatermark(min);    checkInvokeFinishBundleByCount();}
protected void beam_f5402_0(StreamRecord<RawUnionValue> streamRecord)
{    @SuppressWarnings("unchecked")    WindowedValue<Iterable<?>> value = (WindowedValue<Iterable<?>>) streamRecord.getValue().getValue();    PCollectionView<?> sideInput = sideInputTagMapping.get(streamRecord.getValue().getUnionTag());    sideInputHandler.addSideInputValue(sideInput, value);}
public final void beam_f5403_0(StreamRecord<RawUnionValue> streamRecord) throws Exception
{                    invokeFinishBundle();    checkInvokeStartBundle();        addSideInputValue(streamRecord);    List<WindowedValue<InputT>> newPushedBack = new ArrayList<>();    Iterator<WindowedValue<InputT>> it = pushedBackElementsHandler.getElements().iterator();    while (it.hasNext()) {        WindowedValue<InputT> element = it.next();                        setKeyContextElement1(new StreamRecord<>(element));        Iterable<WindowedValue<InputT>> justPushedBack = pushbackDoFnRunner.processElementInReadyWindows(element);        Iterables.addAll(newPushedBack, justPushedBack);    }    pushedBackElementsHandler.clear();    long min = Long.MAX_VALUE;    for (WindowedValue<InputT> pushedBackValue : newPushedBack) {        min = Math.min(min, pushedBackValue.getTimestamp().getMillis());        pushedBackElementsHandler.pushBack(pushedBackValue);    }    setPushedBackWatermark(min);    checkInvokeFinishBundleByCount();        processWatermark1(new Watermark(currentInputWatermark));}
private void beam_f5411_0()
{    long now = getProcessingTimeService().getCurrentProcessingTime();    if (now - lastFinishBundleTime >= maxBundleTimeMills) {        invokeFinishBundle();    }}
protected final void beam_f5412_0()
{    if (bundleStarted.compareAndSet(true, false)) {        pushbackDoFnRunner.finishBundle();        elementCount = 0L;        lastFinishBundleTime = getProcessingTimeService().getCurrentProcessingTime();                if (bundleFinishedCallback != null) {            bundleFinishedCallback.run();            bundleFinishedCallback = null;        }    }}
public final void beam_f5413_0(StateSnapshotContext context) throws Exception
{    if (requiresStableInput) {                        bufferingDoFnRunner.checkpoint(context.getCheckpointId());    }            outputManager.openBuffer();        while (bundleStarted.get()) {        invokeFinishBundle();    }    outputManager.closeBuffer();    super.snapshotState(context);}
 void beam_f5421_0()
{    this.openBuffer = true;}
 void beam_f5422_0()
{    this.openBuffer = false;}
public void beam_f5423_0(TupleTag<T> tag, WindowedValue<T> value)
{    if (!openBuffer) {        emit(tag, value);    } else {        buffer(KV.of(tagsToIds.get(tag), value));    }}
public BufferedOutputManager<OutputT> beam_f5431_0(Output<StreamRecord<WindowedValue<OutputT>>> output, Lock bufferLock, OperatorStateBackend operatorStateBackend, @Nullable KeyedStateBackend keyedStateBackend, @Nullable KeySelector keySelector) throws Exception
{    Preconditions.checkNotNull(output);    Preconditions.checkNotNull(bufferLock);    Preconditions.checkNotNull(operatorStateBackend);    Preconditions.checkState((keyedStateBackend == null) == (keySelector == null), "Either both KeyedStatebackend and Keyselector are provided or none.");    TaggedKvCoder taggedKvCoder = buildTaggedKvCoder();    ListStateDescriptor<KV<Integer, WindowedValue<?>>> taggedOutputPushbackStateDescriptor = new ListStateDescriptor<>("bundle-buffer-tag", new CoderTypeSerializer<>(taggedKvCoder));    final PushedBackElementsHandler<KV<Integer, WindowedValue<?>>> pushedBackElementsHandler;    if (keyedStateBackend != null) {                KeySelector<KV<Integer, WindowedValue<?>>, ?> taggedValueKeySelector = (KeySelector<KV<Integer, WindowedValue<?>>, Object>) value -> keySelector.getKey(value.getValue());        pushedBackElementsHandler = KeyedPushedBackElementsHandler.create(taggedValueKeySelector, keyedStateBackend, taggedOutputPushbackStateDescriptor);    } else {        ListState<KV<Integer, WindowedValue<?>>> listState = operatorStateBackend.getListState(taggedOutputPushbackStateDescriptor);        pushedBackElementsHandler = NonKeyedPushedBackElementsHandler.create(listState);    }    return new BufferedOutputManager<>(output, mainTag, tagsToOutputTags, tagsToIds, bufferLock, pushedBackElementsHandler);}
private TaggedKvCoder beam_f5432_0()
{    ImmutableMap.Builder<Integer, Coder<WindowedValue<?>>> idsToCodersBuilder = ImmutableMap.builder();    for (Map.Entry<TupleTag<?>, Integer> entry : tagsToIds.entrySet()) {        idsToCodersBuilder.put(entry.getValue(), tagsToCoders.get(entry.getKey()));    }    return new TaggedKvCoder(idsToCodersBuilder.build());}
public StateInternals beam_f5433_0()
{    return keyedStateInternals;}
public void beam_f5441_0(StateNamespace namespace, String timerId)
{    throw new UnsupportedOperationException("Canceling of a timer by ID is not yet supported.");}
public void beam_f5442_0(StateNamespace namespace, String timerId, TimeDomain timeDomain)
{    try {        cancelPendingTimerById(getContextTimerId(timerId, namespace));    } catch (Exception e) {        throw new RuntimeException("Failed to cancel timer", e);    }}
public void beam_f5443_0(TimerData timerKey)
{    cleanupPendingTimer(timerKey);    long time = timerKey.getTimestamp().getMillis();    switch(timerKey.getDomain()) {        case EVENT_TIME:            timerService.deleteEventTimeTimer(timerKey, adjustTimestampForFlink(time));            break;        case PROCESSING_TIME:        case SYNCHRONIZED_PROCESSING_TIME:            timerService.deleteProcessingTimeTimer(timerKey, adjustTimestampForFlink(time));            break;        default:            throw new UnsupportedOperationException("Unsupported time domain: " + timerKey.getDomain());    }}
public void beam_f5451_0(ProcessBundleProgressResponse progress)
{    flinkMetricContainer.updateMetrics(stepName, progress.getMonitoringInfosList());}
public void beam_f5452_0(ProcessBundleResponse response)
{    flinkMetricContainer.updateMetrics(stepName, response.getMonitoringInfosList());}
private StateRequestHandler beam_f5453_0(ExecutableStage executableStage)
{    final StateRequestHandler sideInputStateHandler;    if (executableStage.getSideInputs().size() > 0) {        checkNotNull(super.sideInputHandler);        StateRequestHandlers.SideInputHandlerFactory sideInputHandlerFactory = Preconditions.checkNotNull(FlinkStreamingSideInputHandlerFactory.forStage(executableStage, sideInputIds, super.sideInputHandler));        try {            sideInputStateHandler = StateRequestHandlers.forSideInputHandlerFactory(ProcessBundleDescriptors.getSideInputs(executableStage), sideInputHandlerFactory);        } catch (IOException e) {            throw new RuntimeException("Failed to initialize SideInputHandler", e);        }    } else {        sideInputStateHandler = StateRequestHandler.unsupported();    }    final StateRequestHandler userStateRequestHandler;    if (executableStage.getUserStates().size() > 0) {        if (keyedStateInternals == null) {            throw new IllegalStateException("Input must be keyed when user state is used");        }        userStateRequestHandler = StateRequestHandlers.forBagUserStateHandlerFactory(stageBundleFactory.getProcessBundleDescriptor(), new BagUserStateFactory(() -> UUID.randomUUID().toString(), keyedStateInternals, getKeyedStateBackend(), stateBackendLock));    } else {        userStateRequestHandler = StateRequestHandler.unsupported();    }    EnumMap<TypeCase, StateRequestHandler> handlerMap = new EnumMap<>(TypeCase.class);    handlerMap.put(TypeCase.MULTIMAP_SIDE_INPUT, sideInputStateHandler);    handlerMap.put(TypeCase.BAG_USER_STATE, userStateRequestHandler);    return StateRequestHandlers.delegateBasedUponType(handlerMap);}
private void beam_f5463_1(WindowedValue<InputT> timerElement, TimerInternals.TimerData timerData)
{    try {                        ByteBuffer encodedKey = (ByteBuffer) keySelector.getKey(timerElement);                try {            stateBackendLock.lock();            getKeyedStateBackend().setCurrentKey(encodedKey);            if (timerData.getTimestamp().isAfter(BoundedWindow.TIMESTAMP_MAX_VALUE)) {                timerInternals.deleteTimer(timerData.getNamespace(), timerData.getTimerId(), timerData.getDomain());            } else {                timerInternals.setTimer(timerData);            }        } finally {            stateBackendLock.unlock();        }    } catch (Exception e) {        throw new RuntimeException("Couldn't set timer", e);    }}
protected void beam_f5464_0(InternalTimer<ByteBuffer, TimerInternals.TimerData> timer)
{    final ByteBuffer encodedKey = timer.getKey();        try {        stateBackendLock.lock();        getKeyedStateBackend().setCurrentKey(encodedKey);        super.fireTimer(timer);    } finally {        stateBackendLock.unlock();    }}
public void beam_f5465_0() throws Exception
{        if (stageContext != null) {                try (AutoCloseable bundleFactoryCloser = stageBundleFactory;            AutoCloseable closable = stageContext) {                                    super.dispose();        } finally {            stageContext = null;        }    }}
public void beam_f5473_0()
{    try {                        remoteBundle.close();        emitResults();    } catch (Exception e) {        throw new RuntimeException("Failed to finish remote bundle", e);    } finally {        remoteBundle = null;    }}
 boolean beam_f5474_0()
{    return remoteBundle != null;}
private void beam_f5475_1()
{    KV<String, OutputT> result;    while ((result = outputQueue.poll()) != null) {        final String outputPCollectionId = Preconditions.checkNotNull(result.getKey());        TupleTag<?> tag = outputMap.get(outputPCollectionId);        WindowedValue windowedValue = Preconditions.checkNotNull((WindowedValue) result.getValue(), "Received a null value from the SDK harness for %s", outputPCollectionId);        if (tag != null) {                        outputManager.output(tag, windowedValue);        } else {            TimerSpec timerSpec = Preconditions.checkNotNull(timerOutputIdToSpecMap.get(outputPCollectionId), "Unknown Pcollectionid %s", outputPCollectionId);            Timer timer = Preconditions.checkNotNull((Timer) ((KV) windowedValue.getValue()).getValue(), "Received null Timer from SDK harness: %s", windowedValue);                        for (Object window : windowedValue.getWindows()) {                StateNamespace namespace = StateNamespaces.window(windowCoder, (BoundedWindow) window);                TimerInternals.TimerData timerData = TimerInternals.TimerData.of(timerSpec.inputCollectionId(), namespace, timer.getTimestamp(), timerSpec.getTimerSpec().getTimeDomain());                timerRegistration.accept(windowedValue, timerData);            }        }    }}
 void beam_f5483_1(StateInternals stateInternals, Consumer<ByteBuffer> keyContextConsumer)
{    while (!cleanupQueue.isEmpty()) {        KV<ByteBuffer, BoundedWindow> kv = cleanupQueue.remove();        if (LOG.isDebugEnabled()) {                    }        keyContextConsumer.accept(kv.getKey());        for (String userState : userStateNames) {            StateNamespace namespace = StateNamespaces.window(windowCoder, kv.getValue());            StateTag<BagState<Void>> bagStateStateTag = StateTags.bag(userState, VoidCoder.of());            BagState<?> state = stateInternals.state(namespace, bagStateStateTag);            state.clear();        }    }}
private static void beam_f5484_0(ExecutableStage executableStage, @Nullable KeyedStateBackend keyedStateBackend)
{    executableStage.getUserStates().forEach(ref -> {        try {            keyedStateBackend.getOrCreateKeyedState(StringSerializer.INSTANCE, new ListStateDescriptor<>(ref.localName(), new CoderTypeSerializer<>(ByteStringCoder.of())));        } catch (Exception e) {            throw new RuntimeException("Couldn't initialize user states.", e);        }    });}
public static ByteBuffer beam_f5486_0(K key, Coder<K> keyCoder)
{    checkNotNull(keyCoder, "Provided coder must not be null");    final byte[] keyBytes;    try {        keyBytes = CoderUtils.encodeToByteArray(keyCoder, key, Coder.Context.NESTED);    } catch (Exception e) {        throw new RuntimeException(String.format(Locale.ENGLISH, "Failed to encode key: %s", key), e);    }    return ByteBuffer.wrap(keyBytes);}
public void beam_f5495_0(StreamRecord<WindowedValue<ValueWithRecordId<T>>> streamRecord) throws Exception
{    ValueState<Long> dedupingState = getPartitionedState(dedupingStateDescriptor);    Long lastSeenTimestamp = dedupingState.value();    if (lastSeenTimestamp == null) {                WindowedValue<ValueWithRecordId<T>> value = streamRecord.getValue();        output.collect(streamRecord.replace(value.withValue(value.getValue().getValue())));    }    long currentProcessingTime = timerService.currentProcessingTime();    dedupingState.update(currentProcessingTime);    timerService.registerProcessingTimeTimer(VoidNamespace.INSTANCE, currentProcessingTime + MAX_RETENTION_SINCE_ACCESS);}
public void beam_f5497_0(InternalTimer<ByteBuffer, VoidNamespace> internalTimer) throws Exception
{    ValueState<Long> dedupingState = getPartitionedState(dedupingStateDescriptor);    Long lastSeenTimestamp = dedupingState.value();    if (lastSeenTimestamp != null && lastSeenTimestamp.equals(internalTimer.getTimestamp() - MAX_RETENTION_SINCE_ACCESS)) {        dedupingState.clear();    }}
public void beam_f5498_1(SourceContext<WindowedValue<byte[]>> ctx)
{            int subtaskCount = messageCount / getRuntimeContext().getNumberOfParallelSubtasks();        if (getRuntimeContext().getIndexOfThisSubtask() < (messageCount % getRuntimeContext().getNumberOfParallelSubtasks())) {        subtaskCount++;    }    while (!cancelled.get() && (messageCount == 0 || count < subtaskCount)) {        synchronized (ctx.getCheckpointLock()) {            ctx.collect(WindowedValue.valueInGlobalWindow(new byte[] {}));            count++;        }        try {            if (intervalMillis > 0) {                Thread.sleep(intervalMillis);            }        } catch (InterruptedException e) {                    }    }}
public void beam_f5506_0() throws Exception
{    try {        super.close();        if (localReaders != null) {            for (UnboundedSource.UnboundedReader<OutputT> reader : localReaders) {                reader.close();            }        }    } finally {        FlinkClassloading.deleteStaticCaches();    }}
public void beam_f5507_0()
{    isRunning = false;}
public void beam_f5508_0()
{    isRunning = false;}
 boolean beam_f5516_0()
{    return isRunning;}
public void beam_f5517_0(SourceContext<WindowedValue<ValueWithRecordId<OutputT>>> ctx)
{    context = ctx;}
public void beam_f5518_0(long checkpointId) throws Exception
{    List<CheckpointMarkT> checkpointMarks = pendingCheckpoints.get(checkpointId);    if (checkpointMarks != null) {                Iterator<Long> iterator = pendingCheckpoints.keySet().iterator();        long currentId;        do {            currentId = iterator.next();            iterator.remove();        } while (currentId != checkpointId);                for (CheckpointMarkT mark : checkpointMarks) {            mark.finalizeCheckpoint();        }    }}
 static NonKeyedPushedBackElementsHandler<T> beam_f5526_0(ListState<T> elementState)
{    return new NonKeyedPushedBackElementsHandler<>(elementState);}
public Stream<T> beam_f5527_0() throws Exception
{    return StreamSupport.stream(elementState.get().spliterator(), false);}
public void beam_f5528_0()
{    elementState.clear();}
public Coder<K> beam_f5536_0()
{    return keyCoder;}
public Coder<ElemT> beam_f5537_0()
{    return elemCoder;}
public void beam_f5538_0(SingletonKeyedWorkItem<K, ElemT> value, OutputStream outStream) throws CoderException, IOException
{    encode(value, outStream, Context.NESTED);}
public void beam_f5546_0(StateInitializationContext context) throws Exception
{    super.initializeState(context);    checkState(doFn instanceof ProcessFn);            StateInternalsFactory<byte[]> stateInternalsFactory = key -> (StateInternals) keyedStateInternals;        TimerInternalsFactory<byte[]> timerInternalsFactory = key -> timerInternals;    executorService = Executors.newSingleThreadScheduledExecutor(Executors.defaultThreadFactory());    ((ProcessFn) doFn).setStateInternalsFactory(stateInternalsFactory);    ((ProcessFn) doFn).setTimerInternalsFactory(timerInternalsFactory);    ((ProcessFn) doFn).setProcessElementInvoker(new OutputAndTimeBoundedSplittableProcessElementInvoker<>(doFn, serializedOptions.get(), new OutputWindowedValue<OutputT>() {        @Override        public void outputWindowedValue(OutputT output, Instant timestamp, Collection<? extends BoundedWindow> windows, PaneInfo pane) {            outputManager.output(mainOutputTag, WindowedValue.of(output, timestamp, windows, pane));        }        @Override        public <AdditionalOutputT> void outputWindowedValue(TupleTag<AdditionalOutputT> tag, AdditionalOutputT output, Instant timestamp, Collection<? extends BoundedWindow> windows, PaneInfo pane) {            outputManager.output(tag, WindowedValue.of(output, timestamp, windows, pane));        }    }, sideInputReader, executorService, 10000, Duration.standardSeconds(10)));}
public void beam_f5547_0(OutputT output, Instant timestamp, Collection<? extends BoundedWindow> windows, PaneInfo pane)
{    outputManager.output(mainOutputTag, WindowedValue.of(output, timestamp, windows, pane));}
public void beam_f5548_0(TupleTag<AdditionalOutputT> tag, AdditionalOutputT output, Instant timestamp, Collection<? extends BoundedWindow> windows, PaneInfo pane)
{    outputManager.output(tag, WindowedValue.of(output, timestamp, windows, pane));}
public int beam_f5556_0()
{    return Objects.hash(timerId, window, timestamp, timeDomain);}
public void beam_f5557_0(BufferedElement value, OutputStream outStream) throws IOException
{    if (value instanceof Element) {        outStream.write(ELEMENT_MAGIC_BYTE);        elementCoder.encode(((Element) value).element, outStream);    } else if (value instanceof Timer) {        outStream.write(TIMER_MAGIC_BYTE);        Timer timer = (Timer) value;        STRING_CODER.encode(timer.timerId, outStream);        windowCoder.encode(timer.window, outStream);        INSTANT_CODER.encode(timer.timestamp, outStream);        outStream.write(timer.timeDomain.ordinal());    } else {        throw new IllegalStateException("Unexpected element " + value);    }}
public BufferedElement beam_f5558_0(InputStream inStream) throws IOException
{    int firstByte = inStream.read();    switch(firstByte) {        case ELEMENT_MAGIC_BYTE:            return new Element(elementCoder.decode(inStream));        case TIMER_MAGIC_BYTE:            return new Timer(STRING_CODER.decode(inStream), windowCoder.decode(inStream), INSTANT_CODER.decode(inStream), TimeDomain.values()[inStream.read()]);        default:            throw new IllegalStateException("Unexpected byte while reading BufferedElement: " + firstByte);    }}
private void beam_f5569_0(long checkpointId, String internalId) throws Exception
{    notYetAcknowledgedSnapshots.addAll(Collections.singletonList(new CheckpointElement(internalId, checkpointId)));}
private List<CheckpointElement> beam_f5570_0(long checkpointId) throws Exception
{    List<CheckpointElement> toBeAcknowledged = new ArrayList<>();    List<CheckpointElement> checkpoints = new ArrayList<>();    for (CheckpointElement element : notYetAcknowledgedSnapshots.get()) {        if (element.checkpointId <= checkpointId) {            toBeAcknowledged.add(element);        } else {            checkpoints.add(element);        }    }    notYetAcknowledgedSnapshots.update(checkpoints);        toBeAcknowledged.sort(Comparator.comparingLong(o -> o.checkpointId));    return toBeAcknowledged;}
private static String beam_f5571_0()
{    return UUID.randomUUID().toString();}
public void beam_f5579_0()
{    elementState.clear();}
public K beam_f5580_0()
{    return null;}
public T beam_f5581_0(final StateNamespace namespace, StateTag<T> address, final StateContext<?> context)
{    return address.bind(new StateTag.StateBinder() {        @Override        public <T2> ValueState<T2> bindValue(StateTag<ValueState<T2>> address, Coder<T2> coder) {            return new FlinkBroadcastValueState<>(stateBackend, address, namespace, coder);        }        @Override        public <T2> BagState<T2> bindBag(StateTag<BagState<T2>> address, Coder<T2> elemCoder) {            return new FlinkBroadcastBagState<>(stateBackend, address, namespace, elemCoder);        }        @Override        public <T2> SetState<T2> bindSet(StateTag<SetState<T2>> address, Coder<T2> elemCoder) {            throw new UnsupportedOperationException(String.format("%s is not supported", SetState.class.getSimpleName()));        }        @Override        public <KeyT, ValueT> MapState<KeyT, ValueT> bindMap(StateTag<MapState<KeyT, ValueT>> spec, Coder<KeyT> mapKeyCoder, Coder<ValueT> mapValueCoder) {            throw new UnsupportedOperationException(String.format("%s is not supported", MapState.class.getSimpleName()));        }        @Override        public <InputT, AccumT, OutputT> CombiningState<InputT, AccumT, OutputT> bindCombiningValue(StateTag<CombiningState<InputT, AccumT, OutputT>> address, Coder<AccumT> accumCoder, Combine.CombineFn<InputT, AccumT, OutputT> combineFn) {            return new FlinkCombiningState<>(stateBackend, address, combineFn, namespace, accumCoder);        }        @Override        public <InputT, AccumT, OutputT> CombiningState<InputT, AccumT, OutputT> bindCombiningValueWithContext(StateTag<CombiningState<InputT, AccumT, OutputT>> address, Coder<AccumT> accumCoder, CombineWithContext.CombineFnWithContext<InputT, AccumT, OutputT> combineFn) {            return new FlinkCombiningStateWithContext<>(stateBackend, address, combineFn, namespace, accumCoder, FlinkBroadcastStateInternals.this, CombineContextFactory.createFromStateContext(context));        }        @Override        public WatermarkHoldState bindWatermark(StateTag<WatermarkHoldState> address, TimestampCombiner timestampCombiner) {            throw new UnsupportedOperationException(String.format("%s is not supported", WatermarkHoldState.class.getSimpleName()));        }    });}
 Map<String, T> beam_f5589_0() throws Exception
{    if (indexInSubtaskGroup == 0) {        return getMapFromBroadcastState();    } else {        Map<String, T> result = (Map<String, T>) stateForNonZeroOperator.get(name);                if (result == null) {            result = getMapFromBroadcastState();            if (result != null) {                stateForNonZeroOperator.put(name, result);                                flinkStateBackend.getUnionListState(flinkStateDescriptor).clear();            }        }        return result;    }}
 Map<String, T> beam_f5590_0() throws Exception
{    ListState<Map<String, T>> state = flinkStateBackend.getUnionListState(flinkStateDescriptor);    Iterable<Map<String, T>> iterable = state.get();    Map<String, T> ret = null;    if (iterable != null) {                Iterator<Map<String, T>> iterator = iterable.iterator();        if (iterator.hasNext()) {            ret = iterator.next();        }    }    return ret;}
 void beam_f5591_0(Map<String, T> map) throws Exception
{    if (indexInSubtaskGroup == 0) {        ListState<Map<String, T>> state = flinkStateBackend.getUnionListState(flinkStateDescriptor);        state.clear();        if (map.size() > 0) {            state.add(map);        }    } else {        if (map.isEmpty()) {            stateForNonZeroOperator.remove(name);                                } else {            stateForNonZeroOperator.put(name, map);        }    }}
public int beam_f5599_0()
{    int result = namespace.hashCode();    result = 31 * result + address.hashCode();    return result;}
public void beam_f5600_0()
{    clearInternal();}
public void beam_f5601_0(T input)
{    List<T> list = readInternal();    if (list == null) {        list = new ArrayList<>();    }    list.add(input);    writeInternal(list);}
public int beam_f5609_0()
{    int result = namespace.hashCode();    result = 31 * result + address.hashCode();    return result;}
public CombiningState<InputT, AccumT, OutputT> beam_f5610_0()
{    return this;}
public void beam_f5611_0(InputT value)
{    AccumT current = readInternal();    if (current == null) {        current = combineFn.createAccumulator();    }    current = combineFn.addInput(current, value);    writeInternal(current);}
public void beam_f5619_0()
{    clearInternal();}
public boolean beam_f5620_0(Object o)
{    if (this == o) {        return true;    }    if (o == null || getClass() != o.getClass()) {        return false;    }    FlinkCombiningState<?, ?, ?> that = (FlinkCombiningState<?, ?, ?>) o;    return namespace.equals(that.namespace) && address.equals(that.address);}
public int beam_f5621_0()
{    int result = namespace.hashCode();    result = 31 * result + address.hashCode();    return result;}
public Boolean beam_f5629_0()
{    try {        return readInternal() == null;    } catch (Exception e) {        throw new RuntimeException("Error reading state.", e);    }}
public ReadableState<Boolean> beam_f5630_0()
{    return this;}
public void beam_f5631_0()
{    clearInternal();}
public OutputT beam_f5639_0()
{    try {        AccumT accum = readInternal();        if (accum == null) {            accum = combineFn.createAccumulator(context);        }        return combineFn.extractOutput(accum, context);    } catch (Exception e) {        throw new RuntimeException("Error reading state.", e);    }}
public ReadableState<Boolean> beam_f5640_0()
{    return new ReadableState<Boolean>() {        @Override        public Boolean read() {            try {                return readInternal() == null;            } catch (Exception e) {                throw new RuntimeException("Error reading state.", e);            }        }        @Override        public ReadableState<Boolean> readLater() {            return this;        }    };}
public Boolean beam_f5641_0()
{    try {        return readInternal() == null;    } catch (Exception e) {        throw new RuntimeException("Error reading state.", e);    }}
public ValueState<T2> beam_f5649_0(String id, StateSpec<ValueState<T2>> spec, Coder<T2> coder)
{    return new FlinkValueState<>(flinkStateBackend, id, namespace, coder);}
public BagState<T2> beam_f5650_0(String id, StateSpec<BagState<T2>> spec, Coder<T2> elemCoder)
{    return new FlinkBagState<>(flinkStateBackend, id, namespace, elemCoder);}
public SetState<T2> beam_f5651_0(String id, StateSpec<SetState<T2>> spec, Coder<T2> elemCoder)
{    return new FlinkSetState<>(flinkStateBackend, id, namespace, elemCoder);}
public void beam_f5659_0()
{    try {        flinkStateBackend.getPartitionedState(namespace.stringKey(), StringSerializer.INSTANCE, flinkStateDescriptor).clear();    } catch (Exception e) {        throw new RuntimeException("Error clearing state.", e);    }}
public boolean beam_f5660_0(Object o)
{    if (this == o) {        return true;    }    if (o == null || getClass() != o.getClass()) {        return false;    }    FlinkValueState<?> that = (FlinkValueState<?>) o;    return namespace.equals(that.namespace) && stateId.equals(that.stateId);}
public int beam_f5661_0()
{    int result = namespace.hashCode();    result = 31 * result + stateId.hashCode();    return result;}
public ReadableState<Boolean> beam_f5669_0()
{    return this;}
public void beam_f5670_0()
{    try {        flinkStateBackend.getPartitionedState(namespace.stringKey(), StringSerializer.INSTANCE, flinkStateDescriptor).clear();    } catch (Exception e) {        throw new RuntimeException("Error clearing state.", e);    }}
public boolean beam_f5671_0(Object o)
{    if (this == o) {        return true;    }    if (o == null || getClass() != o.getClass()) {        return false;    }    FlinkBagState<?, ?> that = (FlinkBagState<?, ?>) o;    return namespace.equals(that.namespace) && stateId.equals(that.stateId);}
public ReadableState<Boolean> beam_f5679_0()
{    return new ReadableState<Boolean>() {        @Override        public Boolean read() {            try {                return flinkStateBackend.getPartitionedState(namespace.stringKey(), StringSerializer.INSTANCE, flinkStateDescriptor).value() == null;            } catch (Exception e) {                throw new RuntimeException("Error reading state.", e);            }        }        @Override        public ReadableState<Boolean> readLater() {            return this;        }    };}
public Boolean beam_f5680_0()
{    try {        return flinkStateBackend.getPartitionedState(namespace.stringKey(), StringSerializer.INSTANCE, flinkStateDescriptor).value() == null;    } catch (Exception e) {        throw new RuntimeException("Error reading state.", e);    }}
public ReadableState<Boolean> beam_f5681_0()
{    return this;}
public AccumT beam_f5689_0(Iterable<AccumT> accumulators)
{    return combineFn.mergeAccumulators(accumulators, context);}
public OutputT beam_f5690_0()
{    try {        org.apache.flink.api.common.state.ValueState<AccumT> state = flinkStateBackend.getPartitionedState(namespace.stringKey(), StringSerializer.INSTANCE, flinkStateDescriptor);        AccumT accum = state.value();        if (accum != null) {            return combineFn.extractOutput(accum, context);        } else {            return combineFn.extractOutput(combineFn.createAccumulator(context), context);        }    } catch (Exception e) {        throw new RuntimeException("Error reading state.", e);    }}
public ReadableState<Boolean> beam_f5691_0()
{    return new ReadableState<Boolean>() {        @Override        public Boolean read() {            try {                return flinkStateBackend.getPartitionedState(namespace.stringKey(), StringSerializer.INSTANCE, flinkStateDescriptor).value() == null;            } catch (Exception e) {                throw new RuntimeException("Error reading state.", e);            }        }        @Override        public ReadableState<Boolean> readLater() {            return this;        }    };}
public ReadableState<Boolean> beam_f5699_0()
{    return new ReadableState<Boolean>() {        @Override        public Boolean read() {            try {                return watermarkHoldsState.get(namespaceString) == null;            } catch (Exception e) {                throw new RuntimeException("Error reading state.", e);            }        }        @Override        public ReadableState<Boolean> readLater() {            return this;        }    };}
public Boolean beam_f5700_0()
{    try {        return watermarkHoldsState.get(namespaceString) == null;    } catch (Exception e) {        throw new RuntimeException("Error reading state.", e);    }}
public ReadableState<Boolean> beam_f5701_0()
{    return this;}
public ReadableState<ValueT> beam_f5709_0(final KeyT key, final ValueT value)
{    try {        ValueT current = flinkStateBackend.getPartitionedState(namespace.stringKey(), StringSerializer.INSTANCE, flinkStateDescriptor).get(key);        if (current == null) {            flinkStateBackend.getPartitionedState(namespace.stringKey(), StringSerializer.INSTANCE, flinkStateDescriptor).put(key, value);        }        return ReadableStates.immediate(current);    } catch (Exception e) {        throw new RuntimeException("Error put kv to state.", e);    }}
public void beam_f5710_0(KeyT key)
{    try {        flinkStateBackend.getPartitionedState(namespace.stringKey(), StringSerializer.INSTANCE, flinkStateDescriptor).remove(key);    } catch (Exception e) {        throw new RuntimeException("Error remove map state key.", e);    }}
public ReadableState<Iterable<KeyT>> beam_f5711_0()
{    return new ReadableState<Iterable<KeyT>>() {        @Override        public Iterable<KeyT> read() {            try {                Iterable<KeyT> result = flinkStateBackend.getPartitionedState(namespace.stringKey(), StringSerializer.INSTANCE, flinkStateDescriptor).keys();                return result != null ? ImmutableList.copyOf(result) : Collections.emptyList();            } catch (Exception e) {                throw new RuntimeException("Error get map state keys.", e);            }        }        @Override        public ReadableState<Iterable<KeyT>> readLater() {            return this;        }    };}
public ReadableState<Iterable<Map.Entry<KeyT, ValueT>>> beam_f5719_0()
{    return this;}
public void beam_f5720_0()
{    try {        flinkStateBackend.getPartitionedState(namespace.stringKey(), StringSerializer.INSTANCE, flinkStateDescriptor).clear();    } catch (Exception e) {        throw new RuntimeException("Error clearing state.", e);    }}
public boolean beam_f5721_0(Object o)
{    if (this == o) {        return true;    }    if (o == null || getClass() != o.getClass()) {        return false;    }    FlinkMapState<?, ?> that = (FlinkMapState<?, ?>) o;    return namespace.equals(that.namespace) && stateId.equals(that.stateId);}
public Boolean beam_f5729_0()
{    try {        Iterable<T> result = flinkStateBackend.getPartitionedState(namespace.stringKey(), StringSerializer.INSTANCE, flinkStateDescriptor).keys();        return result == null || Iterables.isEmpty(result);    } catch (Exception e) {        throw new RuntimeException("Error isEmpty from state.", e);    }}
public ReadableState<Boolean> beam_f5730_0()
{    return this;}
public Iterable<T> beam_f5731_0()
{    try {        Iterable<T> result = flinkStateBackend.getPartitionedState(namespace.stringKey(), StringSerializer.INSTANCE, flinkStateDescriptor).keys();        return result != null ? ImmutableList.copyOf(result) : Collections.emptyList();    } catch (Exception e) {        throw new RuntimeException("Error read from state.", e);    }}
public org.apache.beam.sdk.state.MapState<KeyT, ValueT> beam_f5739_0(String id, StateSpec<org.apache.beam.sdk.state.MapState<KeyT, ValueT>> spec, Coder<KeyT> mapKeyCoder, Coder<ValueT> mapValueCoder)
{    try {        keyedStateBackend.getOrCreateKeyedState(StringSerializer.INSTANCE, new MapStateDescriptor<>(id, new CoderTypeSerializer<>(mapKeyCoder), new CoderTypeSerializer<>(mapValueCoder)));    } catch (Exception e) {        throw new RuntimeException(e);    }    return null;}
public CombiningState<InputT, AccumT, OutputT> beam_f5740_0(String id, StateSpec<CombiningState<InputT, AccumT, OutputT>> spec, Coder<AccumT> accumCoder, Combine.CombineFn<InputT, AccumT, OutputT> combineFn)
{    try {        keyedStateBackend.getOrCreateKeyedState(StringSerializer.INSTANCE, new ValueStateDescriptor<>(id, new CoderTypeSerializer<>(accumCoder)));    } catch (Exception e) {        throw new RuntimeException(e);    }    return null;}
public CombiningState<InputT, AccumT, OutputT> beam_f5741_0(String id, StateSpec<CombiningState<InputT, AccumT, OutputT>> spec, Coder<AccumT> accumCoder, CombineWithContext.CombineFnWithContext<InputT, AccumT, OutputT> combineFn)
{    try {        keyedStateBackend.getOrCreateKeyedState(StringSerializer.INSTANCE, new ValueStateDescriptor<>(id, new CoderTypeSerializer<>(accumCoder)));    } catch (Exception e) {        throw new RuntimeException(e);    }    return null;}
protected TypeSerializer<byte[]> beam_f5749_0()
{    return new EncodedValueTypeInformation().createSerializer(new ExecutionConfig());}
protected void beam_f5750_0(String message, byte[] should, byte[] is)
{    Assert.assertArrayEquals(message, should, is);}
protected byte[][] beam_f5751_0()
{    StringUtf8Coder coder = StringUtf8Coder.of();    try {        return new byte[][] { CoderUtils.encodeToByteArray(coder, ""), CoderUtils.encodeToByteArray(coder, "Lorem Ipsum Dolor Omit Longer"), CoderUtils.encodeToByteArray(coder, "aaaa"), CoderUtils.encodeToByteArray(coder, "abcd"), CoderUtils.encodeToByteArray(coder, "abce"), CoderUtils.encodeToByteArray(coder, "abdd"), CoderUtils.encodeToByteArray(coder, "accd"), CoderUtils.encodeToByteArray(coder, "bbcd") };    } catch (CoderException e) {        throw new RuntimeException("Could not encode values.", e);    }}
public void beam_f5759_0()
{    FlinkPipelineOptions options = PipelineOptionsFactory.as(FlinkPipelineOptions.class);    options.setRunner(TestFlinkRunner.class);    ExecutionEnvironment bev = FlinkExecutionEnvironments.createBatchExecutionEnvironment(options, Collections.emptyList());    assertThat(bev, instanceOf(LocalEnvironment.class));    assertThat(options.getParallelism(), is(LocalStreamEnvironment.getDefaultLocalParallelism()));    assertThat(bev.getParallelism(), is(LocalStreamEnvironment.getDefaultLocalParallelism()));}
public void beam_f5760_0()
{    FlinkPipelineOptions options = PipelineOptionsFactory.as(FlinkPipelineOptions.class);    options.setRunner(TestFlinkRunner.class);    StreamExecutionEnvironment sev = FlinkExecutionEnvironments.createStreamExecutionEnvironment(options, Collections.emptyList());    assertThat(sev, instanceOf(LocalStreamEnvironment.class));    assertThat(options.getParallelism(), is(LocalStreamEnvironment.getDefaultLocalParallelism()));    assertThat(sev.getParallelism(), is(LocalStreamEnvironment.getDefaultLocalParallelism()));}
public void beam_f5761_0()
{    FlinkPipelineOptions options = PipelineOptionsFactory.as(FlinkPipelineOptions.class);    options.setRunner(FlinkRunner.class);    options.setFlinkMaster("host:1234");    ExecutionEnvironment bev = FlinkExecutionEnvironments.createBatchExecutionEnvironment(options, Collections.emptyList());    assertThat(bev, instanceOf(RemoteEnvironment.class));    assertThat(Whitebox.getInternalState(bev, "host"), is("host"));    assertThat(Whitebox.getInternalState(bev, "port"), is(1234));}
public void beam_f5769_0()
{    FlinkPipelineOptions options = PipelineOptionsFactory.as(FlinkPipelineOptions.class);    options.setRunner(FlinkRunner.class);    options.setFlinkMaster("192.168.1.1:1234");    ExecutionEnvironment bev = FlinkExecutionEnvironments.createBatchExecutionEnvironment(options, Collections.emptyList());    assertThat(Whitebox.getInternalState(bev, "host"), is("192.168.1.1"));    assertThat(Whitebox.getInternalState(bev, "port"), is(1234));    options.setFlinkMaster("192.168.1.1");    bev = FlinkExecutionEnvironments.createBatchExecutionEnvironment(options, Collections.emptyList());    assertThat(Whitebox.getInternalState(bev, "host"), is("192.168.1.1"));    assertThat(Whitebox.getInternalState(bev, "port"), is(RestOptions.PORT.defaultValue()));}
public void beam_f5770_0()
{    FlinkPipelineOptions options = PipelineOptionsFactory.as(FlinkPipelineOptions.class);    options.setRunner(FlinkRunner.class);    options.setFlinkMaster("192.168.1.1:1234");    ExecutionEnvironment bev = FlinkExecutionEnvironments.createBatchExecutionEnvironment(options, Collections.emptyList());    assertThat(Whitebox.getInternalState(bev, "host"), is("192.168.1.1"));    assertThat(Whitebox.getInternalState(bev, "port"), is(1234));    options.setFlinkMaster("192.168.1.1");    bev = FlinkExecutionEnvironments.createBatchExecutionEnvironment(options, Collections.emptyList());    assertThat(Whitebox.getInternalState(bev, "host"), is("192.168.1.1"));    assertThat(Whitebox.getInternalState(bev, "port"), is(RestOptions.PORT.defaultValue()));}
public void beam_f5771_0()
{    FlinkPipelineOptions options = PipelineOptionsFactory.as(FlinkPipelineOptions.class);    options.setRunner(FlinkRunner.class);    options.setFlinkMaster("[FE80:CD00:0000:0CDE:1257:0000:211E:729C]:1234");    ExecutionEnvironment bev = FlinkExecutionEnvironments.createBatchExecutionEnvironment(options, Collections.emptyList());    assertThat(Whitebox.getInternalState(bev, "host"), is("FE80:CD00:0000:0CDE:1257:0000:211E:729C"));    assertThat(Whitebox.getInternalState(bev, "port"), is(1234));    options.setFlinkMaster("FE80:CD00:0000:0CDE:1257:0000:211E:729C");    bev = FlinkExecutionEnvironments.createBatchExecutionEnvironment(options, Collections.emptyList());    assertThat(Whitebox.getInternalState(bev, "host"), is("FE80:CD00:0000:0CDE:1257:0000:211E:729C"));    assertThat(Whitebox.getInternalState(bev, "port"), is(RestOptions.PORT.defaultValue()));}
public void beam_f5779_0()
{    FlinkPipelineOptions options = PipelineOptionsFactory.as(FlinkPipelineOptions.class);    options.setRunner(TestFlinkRunner.class);    options.setFlinkMaster("[auto]");    FlinkPipelineExecutionEnvironment flinkEnv = new FlinkPipelineExecutionEnvironment(options);    Pipeline pipeline = Pipeline.create();    pipeline.apply(GenerateSequence.from(0).withRate(1, Duration.standardSeconds(1))).apply(ParDo.of(new DoFn<Long, String>() {        @ProcessElement        public void processElement(ProcessContext c) throws Exception {            c.output(Long.toString(c.element()));        }    })).apply(Window.into(FixedWindows.of(Duration.standardHours(1)))).apply(TextIO.write().withNumShards(1).withWindowedWrites().to("/dummy/path"));    flinkEnv.translate(pipeline);}
public void beam_f5780_0(ProcessContext c) throws Exception
{    c.output(Long.toString(c.element()));}
public void beam_f5781_0() throws IOException
{    FlinkPipelineOptions options = testPreparingResourcesToStage("localhost:8081", false);    assertThat(options.getFilesToStage().size(), is(2));    assertThat(options.getFilesToStage().get(0), matches(".*\\.jar"));}
public void beam_f5789_0()
{    boolean[] testParameters = { true, false };    for (boolean streaming : testParameters) {        FlinkPipelineOptions options = PipelineOptionsFactory.as(FlinkPipelineOptions.class);        options.setStreaming(streaming);        options.setRunner(FlinkRunner.class);        FlinkPipelineExecutionEnvironment flinkEnv = new FlinkPipelineExecutionEnvironment(options);        Pipeline p = Mockito.spy(Pipeline.create(options));        flinkEnv.translate(p);        ArgumentCaptor<ImmutableList> captor = ArgumentCaptor.forClass(ImmutableList.class);        Mockito.verify(p).replaceAll(captor.capture());        ImmutableList<PTransformOverride> overridesList = captor.getValue();        assertThat(overridesList.isEmpty(), is(false));        assertThat(overridesList.size(), is(FlinkTransformOverrides.getDefaultOverrides(options).size()));    }}
public void beam_f5790_0()
{    FlinkPipelineOptions options = PipelineOptionsFactory.as(FlinkPipelineOptions.class);    options.setStreaming(true);    options.setRunner(FlinkRunner.class);    FlinkPipelineExecutionEnvironment flinkEnv = new FlinkPipelineExecutionEnvironment(options);    Pipeline p = Pipeline.create(options);            p.apply(Create.of("test")).apply(TextIO.write().to("/tmp"));    p = Mockito.spy(p);        flinkEnv.translate(p);        ArgumentCaptor<ImmutableList> captor = ArgumentCaptor.forClass(ImmutableList.class);    Mockito.verify(p).replaceAll(captor.capture());    ImmutableList<PTransformOverride> overridesList = captor.getValue();    assertThat(overridesList, hasItem(new BaseMatcher<PTransformOverride>() {        @Override        public void describeTo(Description description) {        }        @Override        public boolean matches(Object actual) {            if (actual instanceof PTransformOverride) {                PTransformOverrideFactory overrideFactory = ((PTransformOverride) actual).getOverrideFactory();                if (overrideFactory instanceof FlinkStreamingPipelineTranslator.StreamingShardedWriteFactory) {                    FlinkStreamingPipelineTranslator.StreamingShardedWriteFactory factory = (FlinkStreamingPipelineTranslator.StreamingShardedWriteFactory) overrideFactory;                    return factory.options.getParallelism() > 0;                }            }            return false;        }    }));}
public boolean beam_f5792_0(Object actual)
{    if (actual instanceof PTransformOverride) {        PTransformOverrideFactory overrideFactory = ((PTransformOverride) actual).getOverrideFactory();        if (overrideFactory instanceof FlinkStreamingPipelineTranslator.StreamingShardedWriteFactory) {            FlinkStreamingPipelineTranslator.StreamingShardedWriteFactory factory = (FlinkStreamingPipelineTranslator.StreamingShardedWriteFactory) overrideFactory;            return factory.options.getParallelism() > 0;        }    }    return false;}
private FlinkPipelineOptions beam_f5800_0(String flinkMaster, String tempLocation, List<String> filesToStage)
{    FlinkPipelineOptions options = PipelineOptionsFactory.as(FlinkPipelineOptions.class);    options.setRunner(TestFlinkRunner.class);    options.setFlinkMaster(flinkMaster);    options.setTempLocation(tempLocation);    options.setFilesToStage(filesToStage);    return options;}
private static List<URL> beam_f5801_0(List<String> filePaths)
{    return filePaths.stream().map(file -> {        try {            return new File(file).getAbsoluteFile().toURI().toURL();        } catch (MalformedURLException e) {            throw new RuntimeException("Failed to convert to URL", e);        }    }).collect(Collectors.toList());}
public void beam_f5802_0()
{    FlinkPipelineOptions options = PipelineOptionsFactory.as(FlinkPipelineOptions.class);    assertThat(options.getParallelism(), is(-1));    assertThat(options.getMaxParallelism(), is(-1));    assertThat(options.getFlinkMaster(), is("[auto]"));    assertThat(options.getFilesToStage(), is(nullValue()));    assertThat(options.getLatencyTrackingInterval(), is(0L));    assertThat(options.isShutdownSourcesOnFinalWatermark(), is(false));    assertThat(options.getObjectReuse(), is(false));    assertThat(options.getCheckpointingMode(), is(CheckpointingMode.EXACTLY_ONCE.name()));    assertThat(options.getMinPauseBetweenCheckpoints(), is(-1L));    assertThat(options.getCheckpointingInterval(), is(-1L));    assertThat(options.getCheckpointTimeoutMillis(), is(-1L));    assertThat(options.getFailOnCheckpointingErrors(), is(true));    assertThat(options.getNumberOfExecutionRetries(), is(-1));    assertThat(options.getExecutionRetryDelay(), is(-1L));    assertThat(options.getRetainExternalizedCheckpointsOnCancellation(), is(false));    assertThat(options.getStateBackendFactory(), is(nullValue()));    assertThat(options.getMaxBundleSize(), is(1000L));    assertThat(options.getMaxBundleTimeMills(), is(1000L));    assertThat(options.getExecutionModeForBatch(), is(ExecutionMode.PIPELINED.name()));    assertThat(options.getSavepointPath(), is(nullValue()));    assertThat(options.getAllowNonRestoredState(), is(false));}
private JobID beam_f5810_0(Pipeline pipeline) throws Exception
{    JobGraph jobGraph = getJobGraph(pipeline);    flinkCluster.submitJob(jobGraph).get();    return jobGraph.getJobID();}
private String beam_f5811_0(JobID jobID) throws Exception
{    Exception exception = null;        for (int i = 0; i < 10; i++) {        try {            return flinkCluster.triggerSavepoint(jobID, null, false).get();        } catch (Exception e) {            exception = e;            Thread.sleep(100);        }    }    throw exception;}
private JobID beam_f5812_0(Pipeline pipeline, String savepointDir) throws ExecutionException, InterruptedException
{    JobGraph jobGraph = getJobGraph(pipeline);    SavepointRestoreSettings savepointSettings = SavepointRestoreSettings.forPath(savepointDir);    jobGraph.setSavepointRestoreSettings(savepointSettings);    return flinkCluster.submitJob(jobGraph).get().getJobID();}
public static void beam_f5820_1() throws Exception
{    Configuration config = new Configuration();        config.setInteger(RestOptions.PORT, 0);    config.setString(CheckpointingOptions.STATE_BACKEND, "filesystem");    String savepointPath = "file://" + tempFolder.getRoot().getAbsolutePath();                config.setString(CheckpointingOptions.CHECKPOINTS_DIRECTORY, savepointPath);        config.setString(CheckpointingOptions.SAVEPOINT_DIRECTORY, savepointPath);    MiniClusterConfiguration clusterConfig = new MiniClusterConfiguration.Builder().setConfiguration(config).setNumTaskManagers(2).setNumSlotsPerTaskManager(2).build();    flinkCluster = new MiniCluster(clusterConfig);    flinkCluster.start();}
public static void beam_f5821_0() throws Exception
{    flinkCluster.close();    flinkCluster = null;}
public void beam_f5822_0() throws Exception
{    for (JobStatusMessage jobStatusMessage : flinkCluster.listJobs().get()) {        if (jobStatusMessage.getJobState() == JobStatus.RUNNING) {            flinkCluster.cancelJob(jobStatusMessage.getJobId()).get();        }    }    while (!flinkCluster.listJobs().get().stream().allMatch(job -> job.getJobState().isTerminalState())) {        Thread.sleep(50);    }}
private String beam_f5830_0(JobID jobID) throws Exception
{    Exception exception = null;        for (int i = 0; i < 10; i++) {        try {            return flinkCluster.triggerSavepoint(jobID, null, true).get();        } catch (Exception e) {            exception = e;            Thread.sleep(100);        }    }    throw exception;}
private void beam_f5831_0(Pipeline pipeline, String savepointDir) throws ExecutionException, InterruptedException
{    JobGraph jobGraph = getJobGraph(pipeline);    SavepointRestoreSettings savepointSettings = SavepointRestoreSettings.forPath(savepointDir);    jobGraph.setSavepointRestoreSettings(savepointSettings);    flinkCluster.submitJob(jobGraph).get();}
private void beam_f5832_0(Pipeline pipeline, String savepointDir) throws Exception
{    FlinkPipelineOptions flinkOptions = pipeline.getOptions().as(FlinkPipelineOptions.class);    flinkOptions.setSavepointPath(savepointDir);    executePortable(pipeline);}
public void beam_f5840_0(ProcessContext context, @StateId("valueState") ValueState<Integer> intValueState, @StateId("bagState") BagState<Integer> intBagState)
{    Long value = Objects.requireNonNull(context.element().getValue());    if (value == 0L) {        intValueState.write(42);        intBagState.add(40);        intBagState.add(1);        intBagState.add(1);        oneShotLatch.countDown();    }}
public void beam_f5841_0()
{    int parallelism = 3;    assertThat(new FlinkAutoBalancedShardKeyShardingFunction<>(parallelism, -1, StringUtf8Coder.of()).getMaxParallelism(), equalTo(KeyGroupRangeAssignment.computeDefaultMaxParallelism(parallelism)));    assertThat(new FlinkAutoBalancedShardKeyShardingFunction<>(parallelism, 0, StringUtf8Coder.of()).getMaxParallelism(), equalTo(KeyGroupRangeAssignment.computeDefaultMaxParallelism(parallelism)));}
public void beam_f5842_0() throws Exception
{    FlinkAutoBalancedShardKeyShardingFunction<String, String> fn = new FlinkAutoBalancedShardKeyShardingFunction<>(2, 2, StringUtf8Coder.of());    assertNull(fn.getCache());    fn.assignShardKey("target/destination1", "one", 10);    fn.assignShardKey("target/destination2", "two", 10);    assertThat(fn.getCache().size(), equalTo(2));    assertThat(SerializableUtils.clone(fn).getCache(), nullValue());}
private FlinkStreamingPipelineTranslator.StreamTransformTranslator<PTransform<?, ?>> beam_f5850_0()
{    PTransformTranslation.RawPTransform<?, ?> t = mock(PTransformTranslation.RawPTransform.class);    when(t.getUrn()).thenReturn(PTransformTranslation.READ_TRANSFORM_URN);    return (FlinkStreamingPipelineTranslator.StreamTransformTranslator<PTransform<?, ?>>) FlinkStreamingTransformTranslators.getTranslator(t);}
public List<? extends BoundedSource<String>> beam_f5851_0(long desiredBundleSizeBytes, PipelineOptions options) throws Exception
{    List<BoundedSource<String>> splits = new ArrayList<>();    long remaining = bytes;    while (remaining > 0) {        remaining -= desiredBundleSizeBytes;        splits.add(this);    }    return splits;}
public long beam_f5852_0(PipelineOptions options) throws Exception
{    return bytes;}
private static FlinkTestPipeline beam_f5860_0(boolean streaming)
{    TestFlinkRunner flinkRunner = TestFlinkRunner.create(streaming);    return new FlinkTestPipeline(flinkRunner.getPipelineOptions());}
public void beam_f5861_0()
{    FlinkPipelineOptions options = PipelineOptionsFactory.as(FlinkPipelineOptions.class);    options.setRunner(TestFlinkRunner.class);    options.setFlinkMaster("[auto]");    options.setParallelism(5);    TestPipeline p = TestPipeline.fromOptions(options);    StreamingShardedWriteFactory<Object, Void, Object> factory = new StreamingShardedWriteFactory<>(p.getOptions());    WriteFiles<Object, Void, Object> original = WriteFiles.to(new TestSink(tmpFolder.toString()));    @SuppressWarnings("unchecked")    PCollection<Object> objs = (PCollection) p.apply(Create.empty(VoidCoder.of()));    AppliedPTransform<PCollection<Object>, WriteFilesResult<Void>, WriteFiles<Object, Void, Object>> originalApplication = AppliedPTransform.of("writefiles", objs.expand(), Collections.emptyMap(), original, p);    WriteFiles<Object, Void, Object> replacement = (WriteFiles<Object, Void, Object>) factory.getReplacementTransform(originalApplication).getTransform();    assertThat(replacement, not(equalTo((Object) original)));    assertThat(replacement.getNumShardsProvider().get(), is(10));}
public ResourceId beam_f5863_0(int shardNumber, int numShards, BoundedWindow window, PaneInfo paneInfo, OutputFileHints outputFileHints)
{    throw new UnsupportedOperationException("should not be called");}
public void beam_f5871_0()
{    FlinkMetricContainer flinkContainer = new FlinkMetricContainer(runtimeContext);    MetricsContainerImpl step = flinkContainer.getMetricsContainer("step");    MonitoringInfo intCounter = MonitoringInfo.newBuilder().setUrn(MonitoringInfoConstants.Urns.USER_COUNTER).putLabels(MonitoringInfoConstants.Labels.NAMESPACE, "ns1").putLabels(MonitoringInfoConstants.Labels.NAME, "int_counter").putLabels(MonitoringInfoConstants.Labels.PTRANSFORM, "step").setMetric(Metric.newBuilder().setCounterData(CounterData.newBuilder().setInt64Value(111))).build();    MonitoringInfo doubleCounter = MonitoringInfo.newBuilder().setUrn(MonitoringInfoConstants.Urns.USER_COUNTER).putLabels(MonitoringInfoConstants.Labels.NAMESPACE, "ns2").putLabels(MonitoringInfoConstants.Labels.NAME, "double_counter").putLabels(MonitoringInfoConstants.Labels.PTRANSFORM, "step").setMetric(Metric.newBuilder().setCounterData(CounterData.newBuilder().setDoubleValue(222))).build();    MonitoringInfo intDistribution = MonitoringInfo.newBuilder().setUrn(MonitoringInfoConstants.Urns.USER_COUNTER).putLabels(MonitoringInfoConstants.Labels.NAMESPACE, "ns3").putLabels(MonitoringInfoConstants.Labels.NAME, "int_distribution").putLabels(MonitoringInfoConstants.Labels.PTRANSFORM, "step").setMetric(Metric.newBuilder().setDistributionData(MetricsApi.DistributionData.newBuilder().setIntDistributionData(IntDistributionData.newBuilder().setSum(30).setCount(10).setMin(1).setMax(5)))).build();    MonitoringInfo doubleDistribution = MonitoringInfo.newBuilder().setUrn(MonitoringInfoConstants.Urns.USER_COUNTER).putLabels(MonitoringInfoConstants.Labels.NAMESPACE, "ns4").putLabels(MonitoringInfoConstants.Labels.NAME, "double_distribution").putLabels(MonitoringInfoConstants.Labels.PTRANSFORM, "step").setMetric(Metric.newBuilder().setDistributionData(MetricsApi.DistributionData.newBuilder().setDoubleDistributionData(DoubleDistributionData.newBuilder().setSum(30).setCount(10).setMin(1).setMax(5)))).build();            SimpleCounter counter = new SimpleCounter();    when(metricGroup.counter("ns1.int_counter")).thenReturn(counter);    flinkContainer.updateMetrics("step", ImmutableList.of(intCounter, doubleCounter, intDistribution, doubleDistribution));            verify(metricGroup).counter(eq("ns1.int_counter"));        assertThat(counter.getCount(), is(111L));        long count = ((CounterCell) step.tryGetCounter(MonitoringInfoMetricName.of(intCounter))).getCumulative();    assertThat(count, is(111L));            verify(metricGroup).gauge(eq("ns3.int_distribution"), argThat(new ArgumentMatcher<FlinkDistributionGauge>() {        @Override        public boolean matches(FlinkDistributionGauge argument) {            DistributionResult actual = ((FlinkDistributionGauge) argument).getValue();            DistributionResult expected = DistributionResult.create(30, 10, 1, 5);            return actual.equals(expected);        }    }));        DistributionData distributionData = ((DistributionCell) step.getDistribution(MonitoringInfoMetricName.of(intDistribution))).getCumulative();    assertThat(distributionData, is(DistributionData.create(30, 10, 1, 5)));}
public boolean beam_f5872_0(FlinkDistributionGauge argument)
{    DistributionResult actual = ((FlinkDistributionGauge) argument).getValue();    DistributionResult expected = DistributionResult.create(30, 10, 1, 5);    return actual.equals(expected);}
public void beam_f5873_0()
{    FlinkMetricContainer.FlinkDistributionGauge flinkGauge = new FlinkMetricContainer.FlinkDistributionGauge(DistributionResult.IDENTITY_ELEMENT);    when(metricGroup.gauge(eq("namespace.name"), anyObject())).thenReturn(flinkGauge);    FlinkMetricContainer container = new FlinkMetricContainer(runtimeContext);    MetricsContainer step = container.getMetricsContainer("step");    MetricName metricName = MetricName.named("namespace", "name");    Distribution distribution = step.getDistribution(metricName);    assertThat(flinkGauge.getValue(), is(DistributionResult.IDENTITY_ELEMENT));        container.updateMetrics("step");    distribution.update(42);    distribution.update(-23);    distribution.update(0);    distribution.update(1);    container.updateMetrics("step");    assertThat(flinkGauge.getValue().getMax(), is(42L));    assertThat(flinkGauge.getValue().getMin(), is(-23L));    assertThat(flinkGauge.getValue().getCount(), is(4L));    assertThat(flinkGauge.getValue().getSum(), is(20L));    assertThat(flinkGauge.getValue().getMean(), is(5.0));}
public void beam_f5881_0(ProcessContext ctxt)
{    ctxt.output((long) ctxt.element().length());}
public static Object[] beam_f5882_0()
{    return new Object[] { true, false };}
public static void beam_f5883_0()
{            flinkJobExecutor = MoreExecutors.listeningDecorator(Executors.newFixedThreadPool(1));}
public static void beam_f5891_1() throws InterruptedException
{    flinkJobExecutor.shutdown();    flinkJobExecutor.awaitTermination(10, TimeUnit.SECONDS);    if (!flinkJobExecutor.isShutdown()) {            }    flinkJobExecutor = null;}
public void beam_f5892_0() throws Exception
{    PipelineOptions options = PipelineOptionsFactory.create();    options.setRunner(CrashingRunner.class);    options.as(FlinkPipelineOptions.class).setFlinkMaster("[local]");    options.as(FlinkPipelineOptions.class).setStreaming(isStreaming);    options.as(FlinkPipelineOptions.class).setParallelism(2);    options.as(FlinkPipelineOptions.class).setShutdownSourcesOnFinalWatermark(true);    options.as(PortablePipelineOptions.class).setDefaultEnvironmentType(Environments.ENVIRONMENT_EMBEDDED);    final String timerId = "foo";    final String stateId = "sizzle";    final int offset = 5000;    final int timerOutput = 4093;        int numKeys = 50;    List<KV<String, Integer>> input = new ArrayList<>();    List<KV<String, Integer>> expectedOutput = new ArrayList<>();    for (Integer key = 0; key < numKeys; ++key) {                expectedOutput.add(KV.of(key.toString(), timerOutput));        for (int i = 0; i < 15; ++i) {                        input.add(KV.of(key.toString(), i));            expectedOutput.add(KV.of(key.toString(), i + offset));        }    }    Collections.shuffle(input);    DoFn<byte[], KV<String, Integer>> inputFn = new DoFn<byte[], KV<String, Integer>>() {        @ProcessElement        public void processElement(ProcessContext context) {            for (KV<String, Integer> stringIntegerKV : input) {                context.output(stringIntegerKV);            }        }    };    DoFn<KV<String, Integer>, KV<String, Integer>> testFn = new DoFn<KV<String, Integer>, KV<String, Integer>>() {        @TimerId(timerId)        private final TimerSpec spec = TimerSpecs.timer(TimeDomain.EVENT_TIME);        @StateId(stateId)        private final StateSpec<ValueState<String>> stateSpec = StateSpecs.value(StringUtf8Coder.of());        @ProcessElement        public void processElement(ProcessContext context, @TimerId(timerId) Timer timer, @StateId(stateId) ValueState<String> state, BoundedWindow window) {            timer.set(window.maxTimestamp());            state.write(context.element().getKey());            context.output(KV.of(context.element().getKey(), context.element().getValue() + offset));        }        @OnTimer(timerId)        public void onTimer(@StateId(stateId) ValueState<String> state, OutputReceiver<KV<String, Integer>> r) {            r.output(KV.of(state.read(), timerOutput));        }    };    final Pipeline pipeline = Pipeline.create(options);    PCollection<KV<String, Integer>> output = pipeline.apply(Impulse.create()).apply(ParDo.of(inputFn)).apply(ParDo.of(testFn));    PAssert.that(output).containsInAnyOrder(expectedOutput);            pipeline.replaceAll(Collections.singletonList(JavaReadViaImpulse.boundedOverride()));    RunnerApi.Pipeline pipelineProto = PipelineTranslation.toProto(pipeline);    JobInvocation jobInvocation = FlinkJobInvoker.createJobInvocation("id", "none", flinkJobExecutor, pipelineProto, options.as(FlinkPipelineOptions.class), new FlinkPipelineRunner(options.as(FlinkPipelineOptions.class), null, Collections.emptyList()));    jobInvocation.start();    while (jobInvocation.getState() != Enum.DONE) {        Thread.sleep(1000);    }    assertThat(jobInvocation.getState(), is(Enum.DONE));}
public void beam_f5893_0(ProcessContext context)
{    for (KV<String, Integer> stringIntegerKV : input) {        context.output(stringIntegerKV);    }}
public void beam_f5901_0() throws Exception
{    compareResultsByLinesInMemory(Joiner.on('\n').join(EXPECTED_RESULT), resultDir);}
public void beam_f5902_0() throws Exception
{    runProgram(resultPath);}
private static void beam_f5903_0(String resultPath)
{    Pipeline p = FlinkTestPipeline.createForStreaming();    p.apply(GenerateSequence.from(0).to(10)).apply(ParDo.of(new DoFn<Long, String>() {        @ProcessElement        public void processElement(ProcessContext c) throws Exception {            c.output(c.element().toString());        }    })).apply(TextIO.write().to(resultPath));    p.run();}
public void beam_f5911_0() throws Exception
{    final int numElements = 102;    final int firstBatchSize = 23;    final int secondBatchSize = numElements - firstBatchSize;    final Set<Long> emittedElements = new HashSet<>();    final Object checkpointLock = new Object();    PipelineOptions options = PipelineOptionsFactory.create();        BoundedSource<Long> source = CountingSource.upTo(numElements);    BoundedToUnboundedSourceAdapter<Long> unboundedSource = new BoundedToUnboundedSourceAdapter<>(source);    UnboundedSourceWrapper<Long, Checkpoint<Long>> flinkWrapper = new UnboundedSourceWrapper<>("stepName", options, unboundedSource, numSplits);    StreamSource<WindowedValue<ValueWithRecordId<Long>>, UnboundedSourceWrapper<Long, Checkpoint<Long>>> sourceOperator = new StreamSource<>(flinkWrapper);    AbstractStreamOperatorTestHarness<WindowedValue<ValueWithRecordId<Long>>> testHarness = new AbstractStreamOperatorTestHarness<>(sourceOperator, numTasks, /* max parallelism */    numTasks, /* parallelism */    0);    testHarness.setTimeCharacteristic(TimeCharacteristic.EventTime);        boolean readFirstBatchOfElements = false;    try {        testHarness.open();        sourceOperator.run(checkpointLock, new TestStreamStatusMaintainer(), new PartialCollector<>(emittedElements, firstBatchSize));    } catch (SuccessException e) {                readFirstBatchOfElements = true;    }    assertTrue("Did not successfully read first batch of elements.", readFirstBatchOfElements);        OperatorSubtaskState snapshot = testHarness.snapshot(0, 0);        final ArrayList<Integer> finalizeList = new ArrayList<>();    TestCountingSource.setFinalizeTracker(finalizeList);    testHarness.notifyOfCompletedCheckpoint(0);        BoundedSource<Long> restoredSource = CountingSource.upTo(numElements);    BoundedToUnboundedSourceAdapter<Long> restoredUnboundedSource = new BoundedToUnboundedSourceAdapter<>(restoredSource);    UnboundedSourceWrapper<Long, Checkpoint<Long>> restoredFlinkWrapper = new UnboundedSourceWrapper<>("stepName", options, restoredUnboundedSource, numSplits);    StreamSource<WindowedValue<ValueWithRecordId<Long>>, UnboundedSourceWrapper<Long, Checkpoint<Long>>> restoredSourceOperator = new StreamSource<>(restoredFlinkWrapper);        AbstractStreamOperatorTestHarness<WindowedValue<ValueWithRecordId<Long>>> restoredTestHarness = new AbstractStreamOperatorTestHarness<>(restoredSourceOperator, numTasks, /* max parallelism */    1, /* parallelism */    0);    restoredTestHarness.setTimeCharacteristic(TimeCharacteristic.EventTime);        restoredTestHarness.initializeState(snapshot);        boolean readSecondBatchOfElements = false;    try {        restoredTestHarness.open();        restoredSourceOperator.run(checkpointLock, new TestStreamStatusMaintainer(), new PartialCollector<>(emittedElements, secondBatchSize));    } catch (SuccessException e) {                readSecondBatchOfElements = true;    }    assertTrue("Did not successfully read second batch of elements.", readSecondBatchOfElements);        assertTrue(emittedElements.size() == numElements);}
public void beam_f5913_0(OutputTag<X> outputTag, StreamRecord<X> streamRecord)
{    collect((StreamRecord) streamRecord);}
public void beam_f5915_0(StreamRecord<WindowedValue<ValueWithRecordId<T>>> record)
{    emittedElements.add(record.getValue().getValue().getValue());    count++;    if (count >= elementsToConsumeLimit) {        throw new SuccessException();    }}
public void beam_f5924_0(ProcessContext c) throws Exception
{    KV<Void, Iterable<String>> elem = c.element();    StringBuilder str = new StringBuilder();    str.append("k: " + elem.getKey() + " v:");    for (String v : elem.getValue()) {        str.append(" " + v);    }    c.output(str.toString());}
public void beam_f5925_0()
{    FlinkPipelineOptions options = PipelineOptionsFactory.as(FlinkPipelineOptions.class);    options.setRunner(TestFlinkRunner.class);    options.setStreaming(true);    options.setShutdownSourcesOnFinalWatermark(true);    Pipeline pipeline = Pipeline.create(options);    PCollection<Integer> result = pipeline.apply(GenerateSequence.from(0).to(100).withTimestampFn(new SerializableFunction<Long, Instant>() {        @Override        public Instant apply(Long input) {            return new Instant(input);        }    })).apply(Window.into(FixedWindows.of(Duration.millis(10)))).apply(ParDo.of(new DoFn<Long, KV<String, Void>>() {        @ProcessElement        public void processElement(ProcessContext pc) {            pc.output(KV.of("hello", null));        }    })).apply(GroupByKey.create()).apply(ParDo.of(new DoFn<KV<String, Iterable<Void>>, Integer>() {        @ProcessElement        public void processElement(ProcessContext pc) {            int count = 0;            for (Void aVoid : pc.element().getValue()) {                assertNull("Element should be null", aVoid);                count++;            }            pc.output(count);        }    }));    PAssert.that(result).containsInAnyOrder(10, 10, 10, 10, 10, 10, 10, 10, 10, 10);    pipeline.run();}
public Instant beam_f5926_0(Long input)
{    return new Instant(input);}
public static Object[] beam_f5934_0()
{    return new Object[] { true, false };}
public void beam_f5935_0() throws Exception
{    MockitoAnnotations.initMocks(this);    when(runtimeContext.getDistributedCache()).thenReturn(distributedCache);    when(stageContext.getStageBundleFactory(any())).thenReturn(stageBundleFactory);    RemoteBundle remoteBundle = Mockito.mock(RemoteBundle.class);    when(stageBundleFactory.getBundle(any(), any(), any())).thenReturn(remoteBundle);    ImmutableMap input = ImmutableMap.builder().put("input", Mockito.mock(FnDataReceiver.class)).build();    when(remoteBundle.getInputReceivers()).thenReturn(input);    when(processBundleDescriptor.getTimerSpecs()).thenReturn(Collections.emptyMap());}
public void beam_f5936_0() throws Exception
{    FlinkExecutableStageFunction<Integer> function = getFunction(Collections.emptyMap());    function.open(new Configuration());    @SuppressWarnings("unchecked")    RemoteBundle bundle = Mockito.mock(RemoteBundle.class);    when(stageBundleFactory.getBundle(any(), any(), any())).thenReturn(bundle);    @SuppressWarnings("unchecked")    FnDataReceiver<WindowedValue<?>> receiver = Mockito.mock(FnDataReceiver.class);    when(bundle.getInputReceivers()).thenReturn(ImmutableMap.of("input", receiver));    Exception expected = new Exception();    doThrow(expected).when(bundle).close();    thrown.expect(is(expected));    function.mapPartition(Collections.emptyList(), collector);}
public void beam_f5945_0() throws Exception
{    FlinkExecutableStageFunction<Integer> function = getFunction(Collections.emptyMap());    function.open(new Configuration());    function.close();    verify(stageBundleFactory).getProcessBundleDescriptor();    verify(stageBundleFactory).close();    verifyNoMoreInteractions(stageBundleFactory);}
private FlinkExecutableStageFunction<Integer> beam_f5946_0(Map<String, Integer> outputMap)
{    FlinkExecutableStageContextFactory contextFactory = Mockito.mock(FlinkExecutableStageContextFactory.class);    when(contextFactory.get(any())).thenReturn(stageContext);    FlinkExecutableStageFunction<Integer> function = new FlinkExecutableStageFunction<>(stagePayload, jobInfo, outputMap, contextFactory, null);    function.setRuntimeContext(runtimeContext);    Whitebox.setInternalState(function, "stateRequestHandler", stateRequestHandler);    return function;}
public void beam_f5947_0()
{        assertThat(new ImpulseSourceFunction(false), instanceOf(SourceFunction.class));}
public boolean beam_f5955_0(WindowedValue<byte[]> o)
{    return o instanceof WindowedValue && Arrays.equals((byte[]) ((WindowedValue) o).getValue(), new byte[] {});}
public void beam_f5956_0() throws Exception
{    KeyedOneInputStreamOperatorTestHarness<ByteBuffer, WindowedValue<ValueWithRecordId<String>>, WindowedValue<String>> harness = getDebupingHarness();    harness.open();    String key1 = "key1";    String key2 = "key2";    harness.processElement(new StreamRecord<>(WindowedValue.valueInGlobalWindow(new ValueWithRecordId<>(key1, key1.getBytes(StandardCharsets.UTF_8)))));    harness.processElement(new StreamRecord<>(WindowedValue.valueInGlobalWindow(new ValueWithRecordId<>(key2, key2.getBytes(StandardCharsets.UTF_8)))));    harness.processElement(new StreamRecord<>(WindowedValue.valueInGlobalWindow(new ValueWithRecordId<>(key1, key1.getBytes(StandardCharsets.UTF_8)))));    assertThat(stripStreamRecordFromWindowedValue(harness.getOutput()), contains(WindowedValue.valueInGlobalWindow(key1), WindowedValue.valueInGlobalWindow(key2)));    OperatorSubtaskState snapshot = harness.snapshot(0L, 0L);    harness.close();    harness = getDebupingHarness();    harness.setup();    harness.initializeState(snapshot);    harness.open();    String key3 = "key3";    harness.processElement(new StreamRecord<>(WindowedValue.valueInGlobalWindow(new ValueWithRecordId<>(key2, key2.getBytes(StandardCharsets.UTF_8)))));    harness.processElement(new StreamRecord<>(WindowedValue.valueInGlobalWindow(new ValueWithRecordId<>(key3, key3.getBytes(StandardCharsets.UTF_8)))));    assertThat(stripStreamRecordFromWindowedValue(harness.getOutput()), contains(WindowedValue.valueInGlobalWindow(key3)));    harness.close();}
private KeyedOneInputStreamOperatorTestHarness<ByteBuffer, WindowedValue<ValueWithRecordId<String>>, WindowedValue<String>> beam_f5957_0() throws Exception
{    DedupingOperator<String> operator = new DedupingOperator<>();    return new KeyedOneInputStreamOperatorTestHarness<>(operator, value -> ByteBuffer.wrap(value.getValue().getId()), TypeInformation.of(ByteBuffer.class));}
public void beam_f5965_0() throws Exception
{    WindowingStrategy<Object, IntervalWindow> windowingStrategy = WindowingStrategy.of(FixedWindows.of(new Duration(10)));    DoFn<Integer, String> fn = new DoFn<Integer, String>() {        @StateId("state")        private final StateSpec<ValueState<String>> stateSpec = StateSpecs.value(StringUtf8Coder.of());        @ProcessElement        public void processElement(ProcessContext context) {            context.output(context.element().toString());        }    };    Coder<WindowedValue<Integer>> inputCoder = WindowedValue.getFullCoder(VarIntCoder.of(), windowingStrategy.getWindowFn().windowCoder());    Coder<WindowedValue<String>> outputCoder = WindowedValue.getFullCoder(StringUtf8Coder.of(), windowingStrategy.getWindowFn().windowCoder());    TupleTag<String> outputTag = new TupleTag<>("main-output");    DoFnOperator<Integer, String> doFnOperator = new DoFnOperator<>(fn, "stepName", inputCoder, null, Collections.emptyMap(), outputTag, Collections.emptyList(), new DoFnOperator.MultiOutputOutputManagerFactory<>(outputTag, outputCoder), windowingStrategy, new HashMap<>(), /* side-input mapping */    Collections.emptyList(), /* side inputs */    PipelineOptionsFactory.as(FlinkPipelineOptions.class), VarIntCoder.of(), /* key coder */    WindowedValue::getValue, DoFnSchemaInformation.create(), Collections.emptyMap());    OneInputStreamOperatorTestHarness<WindowedValue<Integer>, WindowedValue<String>> testHarness = new KeyedOneInputStreamOperatorTestHarness<>(doFnOperator, WindowedValue::getValue, new CoderTypeInformation<>(VarIntCoder.of()));    testHarness.open();    testHarness.processWatermark(0);    IntervalWindow window1 = new IntervalWindow(new Instant(0), Duration.millis(10));        testHarness.processElement(new StreamRecord<>(WindowedValue.of(13, new Instant(0), window1, PaneInfo.NO_FIRING)));    assertThat(stripStreamRecordFromWindowedValue(testHarness.getOutput()), contains(WindowedValue.of("13", new Instant(0), window1, PaneInfo.NO_FIRING)));    testHarness.getOutput().clear();    testHarness.processWatermark(9);        testHarness.processElement(new StreamRecord<>(WindowedValue.of(17, new Instant(0), window1, PaneInfo.NO_FIRING)));    assertThat(stripStreamRecordFromWindowedValue(testHarness.getOutput()), contains(WindowedValue.of("17", new Instant(0), window1, PaneInfo.NO_FIRING)));    testHarness.getOutput().clear();    testHarness.processWatermark(10);        testHarness.processElement(new StreamRecord<>(WindowedValue.of(17, new Instant(0), window1, PaneInfo.NO_FIRING)));    assertThat(stripStreamRecordFromWindowedValue(testHarness.getOutput()), emptyIterable());    testHarness.close();}
public void beam_f5966_0(ProcessContext context)
{    context.output(context.element().toString());}
public void beam_f5967_0() throws Exception
{    WindowingStrategy<Object, IntervalWindow> windowingStrategy = WindowingStrategy.of(FixedWindows.of(new Duration(10))).withAllowedLateness(Duration.ZERO);    final String timerId = "boo";    final String stateId = "dazzle";    final int offset = 5000;    final int timerOutput = 4093;    DoFn<KV<String, Integer>, KV<String, Integer>> fn = new DoFn<KV<String, Integer>, KV<String, Integer>>() {        @TimerId(timerId)        private final TimerSpec spec = TimerSpecs.timer(TimeDomain.EVENT_TIME);        @StateId(stateId)        private final StateSpec<ValueState<String>> stateSpec = StateSpecs.value(StringUtf8Coder.of());        @ProcessElement        public void processElement(ProcessContext context, @TimerId(timerId) Timer timer, @StateId(stateId) ValueState<String> state, BoundedWindow window) {            timer.set(window.maxTimestamp());            state.write(context.element().getKey());            context.output(KV.of(context.element().getKey(), context.element().getValue() + offset));        }        @OnTimer(timerId)        public void onTimer(OnTimerContext context, @StateId(stateId) ValueState<String> state) {            context.output(KV.of(state.read(), timerOutput));        }    };    WindowedValue.FullWindowedValueCoder<KV<String, Integer>> coder = WindowedValue.getFullCoder(KvCoder.of(StringUtf8Coder.of(), VarIntCoder.of()), windowingStrategy.getWindowFn().windowCoder());    TupleTag<KV<String, Integer>> outputTag = new TupleTag<>("main-output");    DoFnOperator<KV<String, Integer>, KV<String, Integer>> doFnOperator = new DoFnOperator<>(fn, "stepName", coder, null, Collections.emptyMap(), outputTag, Collections.emptyList(), new DoFnOperator.MultiOutputOutputManagerFactory<>(outputTag, coder), windowingStrategy, new HashMap<>(), /* side-input mapping */    Collections.emptyList(), /* side inputs */    PipelineOptionsFactory.as(FlinkPipelineOptions.class), StringUtf8Coder.of(), /* key coder */    kvWindowedValue -> kvWindowedValue.getValue().getKey(), DoFnSchemaInformation.create(), Collections.emptyMap());    KeyedOneInputStreamOperatorTestHarness<String, WindowedValue<KV<String, Integer>>, WindowedValue<KV<String, Integer>>> testHarness = new KeyedOneInputStreamOperatorTestHarness<>(doFnOperator, kvWindowedValue -> kvWindowedValue.getValue().getKey(), new CoderTypeInformation<>(StringUtf8Coder.of()));    testHarness.open();    testHarness.processWatermark(0);    assertEquals(0, testHarness.numKeyedStateEntries());    IntervalWindow window1 = new IntervalWindow(new Instant(0), Duration.millis(10));    testHarness.processElement(new StreamRecord<>(WindowedValue.of(KV.of("key1", 5), new Instant(1), window1, PaneInfo.NO_FIRING)));    testHarness.processElement(new StreamRecord<>(WindowedValue.of(KV.of("key2", 7), new Instant(3), window1, PaneInfo.NO_FIRING)));    assertThat(stripStreamRecordFromWindowedValue(testHarness.getOutput()), contains(WindowedValue.of(KV.of("key1", 5 + offset), new Instant(1), window1, PaneInfo.NO_FIRING), WindowedValue.of(KV.of("key2", 7 + offset), new Instant(3), window1, PaneInfo.NO_FIRING)));        assertEquals(4, testHarness.numKeyedStateEntries());    testHarness.getOutput().clear();                    testHarness.processWatermark(window1.maxTimestamp().plus(windowingStrategy.getAllowedLateness()).plus(StatefulDoFnRunner.TimeInternalsCleanupTimer.GC_DELAY_MS).getMillis() + 1);    assertThat(stripStreamRecordFromWindowedValue(testHarness.getOutput()), contains(WindowedValue.of(KV.of("key1", timerOutput), new Instant(9), window1, PaneInfo.NO_FIRING), WindowedValue.of(KV.of("key2", timerOutput), new Instant(9), window1, PaneInfo.NO_FIRING)));        assertEquals(0, testHarness.numKeyedStateEntries());    testHarness.close();}
public void beam_f5975_0() throws Exception
{    sideInputCheckpointing(() -> {        Coder<WindowedValue<String>> coder = WindowedValue.getFullCoder(StringUtf8Coder.of(), IntervalWindow.getCoder());        TupleTag<String> outputTag = new TupleTag<>("main-output");        ImmutableMap<Integer, PCollectionView<?>> sideInputMapping = ImmutableMap.<Integer, PCollectionView<?>>builder().put(1, view1).put(2, view2).build();        DoFnOperator<String, String> doFnOperator = new DoFnOperator<>(new IdentityDoFn<>(), "stepName", coder, null, Collections.emptyMap(), outputTag, Collections.emptyList(), new DoFnOperator.MultiOutputOutputManagerFactory<>(outputTag, coder), WindowingStrategy.globalDefault(), sideInputMapping, /* side-input mapping */        ImmutableList.of(view1, view2), /* side inputs */        PipelineOptionsFactory.as(FlinkPipelineOptions.class), null, null, DoFnSchemaInformation.create(), Collections.emptyMap());        return new TwoInputStreamOperatorTestHarness<>(doFnOperator);    });}
public void beam_f5976_0() throws Exception
{    sideInputCheckpointing(() -> {        Coder<WindowedValue<String>> coder = WindowedValue.getFullCoder(StringUtf8Coder.of(), IntervalWindow.getCoder());        TupleTag<String> outputTag = new TupleTag<>("main-output");        StringUtf8Coder keyCoder = StringUtf8Coder.of();        ImmutableMap<Integer, PCollectionView<?>> sideInputMapping = ImmutableMap.<Integer, PCollectionView<?>>builder().put(1, view1).put(2, view2).build();        DoFnOperator<String, String> doFnOperator = new DoFnOperator<>(new IdentityDoFn<>(), "stepName", coder, null, Collections.emptyMap(), outputTag, Collections.emptyList(), new DoFnOperator.MultiOutputOutputManagerFactory<>(outputTag, coder), WindowingStrategy.of(FixedWindows.of(Duration.millis(100))), sideInputMapping, /* side-input mapping */        ImmutableList.of(view1, view2), /* side inputs */        PipelineOptionsFactory.as(FlinkPipelineOptions.class), keyCoder, WindowedValue::getValue, DoFnSchemaInformation.create(), Collections.emptyMap());        return new KeyedTwoInputStreamOperatorTestHarness<>(doFnOperator, WindowedValue::getValue,         null, BasicTypeInfo.STRING_TYPE_INFO);    });}
 void beam_f5977_0(TestHarnessFactory<TwoInputStreamOperatorTestHarness<WindowedValue<String>, RawUnionValue, WindowedValue<String>>> harnessFactory) throws Exception
{    TwoInputStreamOperatorTestHarness<WindowedValue<String>, RawUnionValue, WindowedValue<String>> testHarness = harnessFactory.create();    testHarness.open();    IntervalWindow firstWindow = new IntervalWindow(new Instant(0), new Instant(100));    IntervalWindow secondWindow = new IntervalWindow(new Instant(0), new Instant(500));        testHarness.processElement2(new StreamRecord<>(new RawUnionValue(1, valuesInWindow(ImmutableList.of(KV.of((Void) null, "hello"), KV.of((Void) null, "ciao")), new Instant(0), firstWindow))));    testHarness.processElement2(new StreamRecord<>(new RawUnionValue(2, valuesInWindow(ImmutableList.of(KV.of((Void) null, "foo"), KV.of((Void) null, "bar")), new Instant(0), secondWindow))));            OperatorSubtaskState snapshot = testHarness.snapshot(0, 0);    testHarness = harnessFactory.create();    testHarness.initializeState(snapshot);    testHarness.open();        WindowedValue<String> helloElement = valueInWindow("Hello", new Instant(0), firstWindow);    WindowedValue<String> worldElement = valueInWindow("World", new Instant(1000), firstWindow);    testHarness.processElement1(new StreamRecord<>(helloElement));    testHarness.processElement1(new StreamRecord<>(worldElement));    assertThat(stripStreamRecordFromWindowedValue(testHarness.getOutput()), contains(helloElement, worldElement));    testHarness.close();}
public void beam_f5985_0() throws Exception
{    WindowedValue.ValueOnlyWindowedValueCoder<String> windowedValueCoder = WindowedValue.getValueOnlyCoder(StringUtf8Coder.of());    TupleTag<String> outputTag = new TupleTag<>("main-output");    FlinkPipelineOptions options = PipelineOptionsFactory.as(FlinkPipelineOptions.class);    options.setMaxBundleSize(2L);    options.setMaxBundleTimeMills(10L);    IdentityDoFn<String> doFn = new IdentityDoFn<String>() {        @FinishBundle        public void finishBundle(FinishBundleContext context) {            context.output("finishBundle", BoundedWindow.TIMESTAMP_MIN_VALUE, GlobalWindow.INSTANCE);        }    };    DoFnOperator.MultiOutputOutputManagerFactory<String> outputManagerFactory = new DoFnOperator.MultiOutputOutputManagerFactory(outputTag, WindowedValue.getFullCoder(StringUtf8Coder.of(), GlobalWindow.Coder.INSTANCE));    DoFnOperator<String, String> doFnOperator = new DoFnOperator<>(doFn, "stepName", windowedValueCoder, null, Collections.emptyMap(), outputTag, Collections.emptyList(), outputManagerFactory, WindowingStrategy.globalDefault(), new HashMap<>(), /* side-input mapping */    Collections.emptyList(), /* side inputs */    options, null, null, DoFnSchemaInformation.create(), Collections.emptyMap());    OneInputStreamOperatorTestHarness<WindowedValue<String>, WindowedValue<String>> testHarness = new OneInputStreamOperatorTestHarness<>(doFnOperator);    testHarness.open();    testHarness.processElement(new StreamRecord<>(WindowedValue.valueInGlobalWindow("a")));    testHarness.processElement(new StreamRecord<>(WindowedValue.valueInGlobalWindow("b")));    testHarness.processElement(new StreamRecord<>(WindowedValue.valueInGlobalWindow("c")));    assertThat(stripStreamRecordFromWindowedValue(testHarness.getOutput()), contains(WindowedValue.valueInGlobalWindow("a"), WindowedValue.valueInGlobalWindow("b"), WindowedValue.valueInGlobalWindow("finishBundle"), WindowedValue.valueInGlobalWindow("c")));        OperatorSubtaskState snapshot = testHarness.snapshot(0, 0);        PushedBackElementsHandler<KV<Integer, WindowedValue<?>>> pushedBackElementsHandler = doFnOperator.outputManager.pushedBackElementsHandler;    assertThat(pushedBackElementsHandler, instanceOf(NonKeyedPushedBackElementsHandler.class));    List<KV<Integer, WindowedValue<?>>> bufferedElements = pushedBackElementsHandler.getElements().collect(Collectors.toList());    assertThat(bufferedElements, contains(KV.of(0, WindowedValue.valueInGlobalWindow("finishBundle"))));    testHarness.close();    DoFnOperator<String, String> newDoFnOperator = new DoFnOperator<>(doFn, "stepName", windowedValueCoder, null, Collections.emptyMap(), outputTag, Collections.emptyList(), outputManagerFactory, WindowingStrategy.globalDefault(), new HashMap<>(), /* side-input mapping */    Collections.emptyList(), /* side inputs */    options, null, null, DoFnSchemaInformation.create(), Collections.emptyMap());    OneInputStreamOperatorTestHarness<WindowedValue<String>, WindowedValue<String>> newHarness = new OneInputStreamOperatorTestHarness<>(newDoFnOperator);        newHarness.initializeState(snapshot);    newHarness.open();        newHarness.processElement(new StreamRecord<>(WindowedValue.valueInGlobalWindow("d")));        newHarness.setProcessingTime(10);    assertThat(stripStreamRecordFromWindowedValue(newHarness.getOutput()), contains(WindowedValue.valueInGlobalWindow("finishBundle"), WindowedValue.valueInGlobalWindow("d"), WindowedValue.valueInGlobalWindow("finishBundle")));        newHarness.close();    assertThat(stripStreamRecordFromWindowedValue(newHarness.getOutput()), contains(WindowedValue.valueInGlobalWindow("finishBundle"), WindowedValue.valueInGlobalWindow("d"), WindowedValue.valueInGlobalWindow("finishBundle"), WindowedValue.valueInGlobalWindow("finishBundle")));            newDoFnOperator.dispose();    assertThat(stripStreamRecordFromWindowedValue(newHarness.getOutput()), contains(WindowedValue.valueInGlobalWindow("finishBundle"), WindowedValue.valueInGlobalWindow("d"), WindowedValue.valueInGlobalWindow("finishBundle"), WindowedValue.valueInGlobalWindow("finishBundle")));}
public void beam_f5986_0(FinishBundleContext context)
{    context.output("finishBundle", BoundedWindow.TIMESTAMP_MIN_VALUE, GlobalWindow.INSTANCE);}
public void beam_f5987_0() throws Exception
{    StringUtf8Coder keyCoder = StringUtf8Coder.of();    KvToByteBufferKeySelector keySelector = new KvToByteBufferKeySelector<>(keyCoder);    KvCoder<String, String> kvCoder = KvCoder.of(keyCoder, StringUtf8Coder.of());    WindowedValue.ValueOnlyWindowedValueCoder<KV<String, String>> windowedValueCoder = WindowedValue.getValueOnlyCoder(kvCoder);    TupleTag<String> outputTag = new TupleTag<>("main-output");    FlinkPipelineOptions options = PipelineOptionsFactory.as(FlinkPipelineOptions.class);    options.setMaxBundleSize(2L);    options.setMaxBundleTimeMills(10L);    IdentityDoFn<KV<String, String>> doFn = new IdentityDoFn<KV<String, String>>() {        @FinishBundle        public void finishBundle(FinishBundleContext context) {            context.output(KV.of("key2", "finishBundle"), BoundedWindow.TIMESTAMP_MIN_VALUE, GlobalWindow.INSTANCE);        }    };    DoFnOperator.MultiOutputOutputManagerFactory<KV<String, String>> outputManagerFactory = new DoFnOperator.MultiOutputOutputManagerFactory(outputTag, WindowedValue.getFullCoder(kvCoder, GlobalWindow.Coder.INSTANCE));    DoFnOperator<KV<String, String>, KV<String, String>> doFnOperator = new DoFnOperator(doFn, "stepName", windowedValueCoder, null, Collections.emptyMap(), outputTag, Collections.emptyList(), outputManagerFactory, WindowingStrategy.globalDefault(), new HashMap<>(), /* side-input mapping */    Collections.emptyList(), /* side inputs */    options, keyCoder, keySelector, DoFnSchemaInformation.create(), Collections.emptyMap());    OneInputStreamOperatorTestHarness<WindowedValue<KV<String, String>>, WindowedValue<KV<String, String>>> testHarness = new KeyedOneInputStreamOperatorTestHarness(doFnOperator, keySelector, keySelector.getProducedType());    testHarness.open();    testHarness.processElement(new StreamRecord(WindowedValue.valueInGlobalWindow(KV.of("key", "a"))));    testHarness.processElement(new StreamRecord(WindowedValue.valueInGlobalWindow(KV.of("key", "b"))));    testHarness.processElement(new StreamRecord(WindowedValue.valueInGlobalWindow(KV.of("key", "c"))));    assertThat(stripStreamRecordFromWindowedValue(testHarness.getOutput()), contains(WindowedValue.valueInGlobalWindow(KV.of("key", "a")), WindowedValue.valueInGlobalWindow(KV.of("key", "b")), WindowedValue.valueInGlobalWindow(KV.of("key2", "finishBundle")), WindowedValue.valueInGlobalWindow(KV.of("key", "c"))));        OperatorSubtaskState snapshot = testHarness.snapshot(0, 0);        PushedBackElementsHandler<KV<Integer, WindowedValue<?>>> pushedBackElementsHandler = doFnOperator.outputManager.pushedBackElementsHandler;    assertThat(pushedBackElementsHandler, instanceOf(KeyedPushedBackElementsHandler.class));    List<KV<Integer, WindowedValue<?>>> bufferedElements = pushedBackElementsHandler.getElements().collect(Collectors.toList());    assertThat(bufferedElements, contains(KV.of(0, WindowedValue.valueInGlobalWindow(KV.of("key2", "finishBundle")))));    testHarness.close();    doFnOperator = new DoFnOperator(doFn, "stepName", windowedValueCoder, null, Collections.emptyMap(), outputTag, Collections.emptyList(), outputManagerFactory, WindowingStrategy.globalDefault(), new HashMap<>(), /* side-input mapping */    Collections.emptyList(), /* side inputs */    options, keyCoder, keySelector, DoFnSchemaInformation.create(), Collections.emptyMap());    testHarness = new KeyedOneInputStreamOperatorTestHarness(doFnOperator, keySelector, keySelector.getProducedType());        testHarness.initializeState(snapshot);    testHarness.open();        testHarness.processElement(new StreamRecord<>(WindowedValue.valueInGlobalWindow(KV.of("key", "d"))));        testHarness.setProcessingTime(10);    assertThat(stripStreamRecordFromWindowedValue(testHarness.getOutput()), contains(    WindowedValue.valueInGlobalWindow(KV.of("key2", "finishBundle")), WindowedValue.valueInGlobalWindow(KV.of("key", "d")), WindowedValue.valueInGlobalWindow(KV.of("key2", "finishBundle"))));    testHarness.close();}
public void beam_f5995_0(ProcessContext context)
{    context.output(context.element());}
public void beam_f5996_0(FinishBundleContext context)
{    context.output(KV.of("key3", "finishBundle"), BoundedWindow.TIMESTAMP_MIN_VALUE, GlobalWindow.INSTANCE);}
public void beam_f5997_0()
{    TupleTag<String> outputTag = new TupleTag<>("main-output");    StringUtf8Coder keyCoder = StringUtf8Coder.of();    KvToByteBufferKeySelector keySelector = new KvToByteBufferKeySelector<>(keyCoder);    KvCoder<String, String> kvCoder = KvCoder.of(keyCoder, StringUtf8Coder.of());    WindowedValue.ValueOnlyWindowedValueCoder<KV<String, String>> windowedValueCoder = WindowedValue.getValueOnlyCoder(kvCoder);    DoFn<String, String> doFn = new DoFn<String, String>() {        @ProcessElement                @RequiresStableInput        public void processElement(ProcessContext context) {            context.output(context.element());        }    };    DoFnOperator.MultiOutputOutputManagerFactory<String> outputManagerFactory = new DoFnOperator.MultiOutputOutputManagerFactory(outputTag, WindowedValue.getFullCoder(StringUtf8Coder.of(), GlobalWindow.Coder.INSTANCE));    FlinkPipelineOptions options = PipelineOptionsFactory.as(FlinkPipelineOptions.class);        options.setCheckpointingInterval(-1L);    new DoFnOperator(doFn, "stepName", windowedValueCoder, null, Collections.emptyMap(), outputTag, Collections.emptyList(), outputManagerFactory, WindowingStrategy.globalDefault(), new HashMap<>(), /* side-input mapping */    Collections.emptyList(), /* side inputs */    options, keyCoder, keySelector, DoFnSchemaInformation.create(), Collections.emptyMap());}
private WindowedValue<Iterable<?>> beam_f6005_0(Iterable<?> values, Instant timestamp, BoundedWindow window)
{    return (WindowedValue) WindowedValue.of(values, timestamp, window, PaneInfo.NO_FIRING);}
private WindowedValue<T> beam_f6006_0(T value, Instant timestamp, BoundedWindow window)
{    return WindowedValue.of(value, timestamp, window, PaneInfo.NO_FIRING);}
public void beam_f6007_0()
{    MockitoAnnotations.initMocks(this);    when(runtimeContext.getDistributedCache()).thenReturn(distributedCache);    when(stageContext.getStageBundleFactory(any())).thenReturn(stageBundleFactory);    when(processBundleDescriptor.getTimerSpecs()).thenReturn(Collections.emptyMap());    when(processBundleDescriptor.getBagUserStateSpecs()).thenReturn(Collections.emptyMap());    when(stageBundleFactory.getProcessBundleDescriptor()).thenReturn(processBundleDescriptor);}
public ProcessBundleDescriptors.ExecutableProcessBundleDescriptor beam_f6015_0()
{    return processBundleDescriptor;}
public void beam_f6017_0() throws Exception
{    TupleTag<Integer> mainOutput = new TupleTag<>("main-output");    DoFnOperator.MultiOutputOutputManagerFactory<Integer> outputManagerFactory = new DoFnOperator.MultiOutputOutputManagerFactory(mainOutput, VoidCoder.of());    ExecutableStageDoFnOperator<Integer, Integer> operator = getOperator(mainOutput, Collections.emptyList(), outputManagerFactory);    OneInputStreamOperatorTestHarness<WindowedValue<Integer>, WindowedValue<Integer>> testHarness = new OneInputStreamOperatorTestHarness<>(operator);    RemoteBundle bundle = Mockito.mock(RemoteBundle.class);    when(bundle.getInputReceivers()).thenReturn(ImmutableMap.<String, FnDataReceiver<WindowedValue>>builder().put("input", Mockito.mock(FnDataReceiver.class)).build());    when(stageBundleFactory.getBundle(any(), any(), any())).thenReturn(bundle);    testHarness.open();    testHarness.close();    verify(stageBundleFactory).getProcessBundleDescriptor();    verify(stageBundleFactory).close();    verify(stageContext).close();        verify(stageBundleFactory).getBundle(any(), any(), any());    verify(bundle).getInputReceivers();    verify(bundle).close();    verifyNoMoreInteractions(stageBundleFactory);            operator.dispose();    verifyNoMoreInteractions(bundle);}
public void beam_f6018_0() throws Exception
{    TupleTag<Integer> mainOutput = new TupleTag<>("main-output");    DoFnOperator.MultiOutputOutputManagerFactory<Integer> outputManagerFactory = new DoFnOperator.MultiOutputOutputManagerFactory(mainOutput, VarIntCoder.of());    VarIntCoder keyCoder = VarIntCoder.of();    ExecutableStageDoFnOperator<Integer, Integer> operator = getOperator(mainOutput, Collections.emptyList(), outputManagerFactory, WindowingStrategy.globalDefault(), keyCoder, WindowedValue.getFullCoder(keyCoder, GlobalWindow.Coder.INSTANCE));    KeyedOneInputStreamOperatorTestHarness<Integer, WindowedValue<Integer>, WindowedValue<Integer>> testHarness = new KeyedOneInputStreamOperatorTestHarness(operator, val -> val, new CoderTypeInformation<>(keyCoder));    RemoteBundle bundle = Mockito.mock(RemoteBundle.class);    when(bundle.getInputReceivers()).thenReturn(ImmutableMap.<String, FnDataReceiver<WindowedValue>>builder().put("input", Mockito.mock(FnDataReceiver.class)).build());    when(stageBundleFactory.getBundle(any(), any(), any())).thenReturn(bundle);    testHarness.open();    Object doFnRunner = Whitebox.getInternalState(operator, "doFnRunner");    assertThat(doFnRunner, instanceOf(DoFnRunnerWithMetricsUpdate.class));        Object statefulDoFnRunner = Whitebox.getInternalState(doFnRunner, "delegate");    assertThat(statefulDoFnRunner, instanceOf(StatefulDoFnRunner.class));}
private static BeamFnApi.StateRequest beam_f6026_0(ByteString key, String userStateId) throws Exception
{    BeamFnApi.StateRequest.Builder builder = stateRequest(key, userStateId);    builder.setClear(BeamFnApi.StateClearRequest.newBuilder().build());    return builder.build();}
private static BeamFnApi.StateRequest.Builder beam_f6027_0(ByteString key, String userStateId) throws Exception
{    return BeamFnApi.StateRequest.newBuilder().setStateKey(BeamFnApi.StateKey.newBuilder().setBagUserState(BeamFnApi.StateKey.BagUserState.newBuilder().setTransformId("transform").setKey(key).setUserStateId(userStateId).setWindow(ByteString.copyFrom(CoderUtils.encodeToByteArray(GlobalWindow.Coder.INSTANCE, GlobalWindow.INSTANCE))).build()));}
private static ProcessBundleDescriptors.BagUserStateSpec beam_f6028_0(String userStateId)
{    ProcessBundleDescriptors.BagUserStateSpec bagUserStateMock = Mockito.mock(ProcessBundleDescriptors.BagUserStateSpec.class);    when(bagUserStateMock.keyCoder()).thenReturn(ByteStringCoder.of());    when(bagUserStateMock.valueCoder()).thenReturn(ByteStringCoder.of());    when(bagUserStateMock.transformId()).thenReturn("transformId");    when(bagUserStateMock.userStateId()).thenReturn(userStateId);    when(bagUserStateMock.windowCoder()).thenReturn(GlobalWindow.Coder.INSTANCE);    return bagUserStateMock;}
public void beam_f6036_0()
{    ByteString input = ByteString.copyFrom("hello world".getBytes(Charsets.UTF_8));    ByteBuffer encodedKey = FlinkKeyUtils.fromEncodedKey(input);    assertThat(encodedKey.array(), is(input.toByteArray()));}
public static void beam_f6037_0(List<Integer> finalizeTracker)
{    TestCountingSource.finalizeTracker = finalizeTracker;}
public TestCountingSource beam_f6038_0()
{    return new TestCountingSource(numMessagesPerShard, shardNumber, true, throwOnFirstSnapshot, -1);}
public void beam_f6046_0()
{    if (finalizeTracker != null) {        finalizeTracker.add(current);    }}
public Coder<CounterMark> beam_f6047_0()
{    return DelegateCoder.of(VarIntCoder.of(), new FromCounterMark(), new ToCounterMark());}
public boolean beam_f6048_0()
{    return dedup;}
public CounterMark beam_f6057_1()
{    if (throwOnFirstSnapshot && !thrown) {        thrown = true;                throw new RuntimeException("failed during checkpoint");    }        return new CounterMark(current);}
public long beam_f6058_0()
{    return 7L;}
public CountingSourceReader beam_f6059_1(PipelineOptions options, @Nullable CounterMark checkpointMark)
{    if (checkpointMark == null) {            } else {            }    return new CountingSourceReader(checkpointMark != null ? checkpointMark.current : -1);}
public static Collection<Object[]> beam_f6067_0()
{    /*       * Parameters for initializing the tests:       * {numTasks, numSplits}       * The test currently assumes powers of two for some assertions.       */    return Arrays.asList(new Object[][] { { 1, 1 }, { 1, 2 }, { 1, 4 }, { 2, 1 }, { 2, 2 }, { 2, 4 }, { 4, 1 }, { 4, 2 }, { 4, 4 } });}
public void beam_f6068_1() throws Exception
{    final int numElementsPerShard = 20;    FlinkPipelineOptions options = PipelineOptionsFactory.as(FlinkPipelineOptions.class);    options.setShutdownSourcesOnFinalWatermark(true);    final long[] numElementsReceived = { 0L };    final int[] numWatermarksReceived = { 0 };                TestCountingSource source = new TestCountingSource(numElementsPerShard).withFixedNumSplits(numSplits);    for (int subtaskIndex = 0; subtaskIndex < numTasks; subtaskIndex++) {        UnboundedSourceWrapper<KV<Integer, Integer>, TestCountingSource.CounterMark> flinkWrapper = new UnboundedSourceWrapper<>("stepName", options, source, numTasks);                        assertEquals(numSplits, flinkWrapper.getSplitSources().size());        StreamSource<WindowedValue<ValueWithRecordId<KV<Integer, Integer>>>, UnboundedSourceWrapper<KV<Integer, Integer>, TestCountingSource.CounterMark>> sourceOperator = new StreamSource<>(flinkWrapper);        AbstractStreamOperatorTestHarness<WindowedValue<ValueWithRecordId<KV<Integer, Integer>>>> testHarness = new AbstractStreamOperatorTestHarness<>(sourceOperator, numTasks, /* max parallelism */        numTasks, /* parallelism */        subtaskIndex);                                testHarness.getExecutionConfig().setAutoWatermarkInterval(10L);        testHarness.setProcessingTime(System.currentTimeMillis());        testHarness.setTimeCharacteristic(TimeCharacteristic.EventTime);                        Thread processingTimeUpdateThread = new Thread() {            @Override            public void run() {                while (true) {                    try {                                                synchronized (testHarness.getCheckpointLock()) {                            testHarness.setProcessingTime(System.currentTimeMillis());                        }                        Thread.sleep(100);                    } catch (InterruptedException e) {                                                break;                    } catch (Exception e) {                                                break;                    }                }            }        };        processingTimeUpdateThread.start();        try {            testHarness.open();            sourceOperator.run(testHarness.getCheckpointLock(), new TestStreamStatusMaintainer(), new Output<StreamRecord<WindowedValue<ValueWithRecordId<KV<Integer, Integer>>>>>() {                private boolean hasSeenMaxWatermark = false;                @Override                public void emitWatermark(Watermark watermark) {                                        if (!hasSeenMaxWatermark && watermark.getTimestamp() >= BoundedWindow.TIMESTAMP_MAX_VALUE.getMillis()) {                        numWatermarksReceived[0]++;                        hasSeenMaxWatermark = true;                    }                }                @Override                public <X> void collect(OutputTag<X> outputTag, StreamRecord<X> streamRecord) {                    collect((StreamRecord) streamRecord);                }                @Override                public void emitLatencyMarker(LatencyMarker latencyMarker) {                }                @Override                public void collect(StreamRecord<WindowedValue<ValueWithRecordId<KV<Integer, Integer>>>> windowedValueStreamRecord) {                    numElementsReceived[0]++;                }                @Override                public void close() {                }            });        } finally {            processingTimeUpdateThread.interrupt();            processingTimeUpdateThread.join();        }    }        assertEquals(numElementsPerShard * numSplits, numElementsReceived[0]);        assertEquals(numTasks, numWatermarksReceived[0]);}
public void beam_f6069_1()
{    while (true) {        try {                        synchronized (testHarness.getCheckpointLock()) {                testHarness.setProcessingTime(System.currentTimeMillis());            }            Thread.sleep(100);        } catch (InterruptedException e) {                        break;        } catch (Exception e) {                        break;        }    }}
public void beam_f6083_0(OutputTag<X> outputTag, StreamRecord<X> streamRecord)
{    collect((StreamRecord) streamRecord);}
public void beam_f6085_0(StreamRecord<WindowedValue<ValueWithRecordId<KV<Integer, Integer>>>> windowedValueStreamRecord)
{    emittedElements.add(windowedValueStreamRecord.getValue().getValue().getValue());    count++;    if (count >= numElements / 2) {        throw new SuccessException();    }}
public void beam_f6088_0(OutputTag<X> outputTag, StreamRecord<X> streamRecord)
{    collect((StreamRecord) streamRecord);}
public void beam_f6098_0() throws Exception
{    UnboundedReadFromBoundedSource.BoundedToUnboundedSourceAdapter<Long> source = new UnboundedReadFromBoundedSource.BoundedToUnboundedSourceAdapter<>(CountingSource.upTo(1000));    FlinkPipelineOptions options = PipelineOptionsFactory.as(FlinkPipelineOptions.class);    options.setShutdownSourcesOnFinalWatermark(true);    UnboundedSourceWrapper<Long, UnboundedReadFromBoundedSource.BoundedToUnboundedSourceAdapter.Checkpoint<Long>> sourceWrapper = new UnboundedSourceWrapper<>("sequentialRead", options, source, 4);    StreamingRuntimeContext runtimeContextMock = Mockito.mock(StreamingRuntimeContext.class);    Mockito.when(runtimeContextMock.getIndexOfThisSubtask()).thenReturn(0);    when(runtimeContextMock.getNumberOfParallelSubtasks()).thenReturn(2);    when(runtimeContextMock.getExecutionConfig()).thenReturn(new ExecutionConfig());    TestProcessingTimeService processingTimeService = new TestProcessingTimeService();    processingTimeService.setCurrentTime(0);    when(runtimeContextMock.getProcessingTimeService()).thenReturn(processingTimeService);    when(runtimeContextMock.getMetricGroup()).thenReturn(new UnregisteredMetricsGroup());    sourceWrapper.setRuntimeContext(runtimeContextMock);    sourceWrapper.open(new Configuration());    assertThat(sourceWrapper.getLocalReaders().size(), is(2));    List<Long> integers = new ArrayList<>();    sourceWrapper.run(new SourceFunction.SourceContext<WindowedValue<ValueWithRecordId<Long>>>() {        private final Object checkpointLock = new Object();        @Override        public void collect(WindowedValue<ValueWithRecordId<Long>> element) {            integers.add(element.getValue().getValue());        }        @Override        public void collectWithTimestamp(WindowedValue<ValueWithRecordId<Long>> element, long timestamp) {            throw new IllegalStateException("Should not collect with timestamp");        }        @Override        public void emitWatermark(Watermark mark) {        }        @Override        public void markAsTemporarilyIdle() {        }        @Override        public Object getCheckpointLock() {            return checkpointLock;        }        @Override        public void close() {        }    });                assertThat(integers.size(), is(500));    assertThat(integers, contains(LongStream.concat(LongStream.range(0, 250), LongStream.range(500, 750)).boxed().toArray()));}
public void beam_f6099_0(WindowedValue<ValueWithRecordId<Long>> element)
{    integers.add(element.getValue().getValue());}
public void beam_f6100_0(WindowedValue<ValueWithRecordId<Long>> element, long timestamp)
{    throw new IllegalStateException("Should not collect with timestamp");}
public void beam_f6111_0() throws Exception
{        KeyedOneInputStreamOperatorTestHarness<ByteBuffer, WindowedValue<KeyedWorkItem<Long, Long>>, WindowedValue<KV<Long, Long>>> testHarness = createTestHarness(getWindowDoFnOperator());    testHarness.open();        IntervalWindow window = new IntervalWindow(new Instant(0), Duration.millis(10_000));    testHarness.processWatermark(0L);    testHarness.processElement(Item.builder().key(1L).timestamp(1L).value(100L).window(window).build().toStreamRecord());    testHarness.processElement(Item.builder().key(1L).timestamp(2L).value(20L).window(window).build().toStreamRecord());    testHarness.processElement(Item.builder().key(2L).timestamp(3L).value(77L).window(window).build().toStreamRecord());        OperatorSubtaskState snapshot = testHarness.snapshot(0, 0);    testHarness.close();        testHarness = createTestHarness(getWindowDoFnOperator());    testHarness.initializeState(snapshot);    testHarness.open();        testHarness.processWatermark(10_000L);    Iterable<WindowedValue<KV<Long, Long>>> output = stripStreamRecordFromWindowedValue(testHarness.getOutput());    assertEquals(2, Iterables.size(output));    assertThat(output, containsInAnyOrder(WindowedValue.of(KV.of(1L, 120L), new Instant(9_999), window, PaneInfo.createPane(true, true, ON_TIME)), WindowedValue.of(KV.of(2L, 77L), new Instant(9_999), window, PaneInfo.createPane(true, true, ON_TIME))));        testHarness.close();}
public void beam_f6112_0() throws Exception
{        WindowDoFnOperator<Long, Long, Long> windowDoFnOperator = getWindowDoFnOperator();    KeyedOneInputStreamOperatorTestHarness<ByteBuffer, WindowedValue<KeyedWorkItem<Long, Long>>, WindowedValue<KV<Long, Long>>> testHarness = createTestHarness(windowDoFnOperator);    testHarness.open();    DoFnOperator<KeyedWorkItem<Long, Long>, KV<Long, Long>>.FlinkTimerInternals timerInternals = windowDoFnOperator.timerInternals;        IntervalWindow window = new IntervalWindow(new Instant(0), Duration.millis(100));    IntervalWindow window2 = new IntervalWindow(new Instant(100), Duration.millis(100));    testHarness.processWatermark(0L);    testHarness.processElement(Item.builder().key(1L).timestamp(1L).value(100L).window(window).build().toStreamRecord());    testHarness.processElement(Item.builder().key(1L).timestamp(150L).value(150L).window(window2).build().toStreamRecord());    assertThat(Iterables.size(timerInternals.pendingTimersById.keys()), is(2));        testHarness.processWatermark(200L);    assertThat(Iterables.size(timerInternals.pendingTimersById.keys()), is(0));        testHarness.close();}
private WindowDoFnOperator<Long, Long, Long> beam_f6113_0()
{    WindowingStrategy<Object, IntervalWindow> windowingStrategy = WindowingStrategy.of(FixedWindows.of(standardMinutes(1)));    TupleTag<KV<Long, Long>> outputTag = new TupleTag<>("main-output");    SystemReduceFn<Long, Long, long[], Long, BoundedWindow> reduceFn = SystemReduceFn.combining(VarLongCoder.of(), AppliedCombineFn.withInputCoder(Sum.ofLongs(), CoderRegistry.createDefault(), KvCoder.of(VarLongCoder.of(), VarLongCoder.of())));    Coder<IntervalWindow> windowCoder = windowingStrategy.getWindowFn().windowCoder();    SingletonKeyedWorkItemCoder<Long, Long> workItemCoder = SingletonKeyedWorkItemCoder.of(VarLongCoder.of(), VarLongCoder.of(), windowCoder);    FullWindowedValueCoder<SingletonKeyedWorkItem<Long, Long>> inputCoder = WindowedValue.getFullCoder(workItemCoder, windowCoder);    FullWindowedValueCoder<KV<Long, Long>> outputCoder = WindowedValue.getFullCoder(KvCoder.of(VarLongCoder.of(), VarLongCoder.of()), windowCoder);    return new WindowDoFnOperator<Long, Long, Long>(reduceFn, "stepName", (Coder) inputCoder, outputTag, emptyList(), new MultiOutputOutputManagerFactory<>(outputTag, outputCoder), windowingStrategy, emptyMap(), emptyList(), PipelineOptionsFactory.as(FlinkPipelineOptions.class), VarLongCoder.of(), new WorkItemKeySelector(VarLongCoder.of()));}
 Item beam_f6121_0()
{    Item item = new Item();    item.key = this.key;    item.value = this.value;    item.window = this.window;    item.timestamp = this.timestamp;    return item;}
public State beam_f6122_0()
{    if (!finished) {        return getGearpumpState();    } else {        return State.DONE;    }}
public State beam_f6123_0() throws IOException
{    if (!finished) {        app.shutDown();        finished = true;        return State.CANCELLED;    } else {        return State.DONE;    }}
private Config beam_f6131_0(Config config, Map<String, String> userSerializers)
{    Map<String, String> serializers = new HashMap<>();    serializers.put("org.apache.beam.sdk.util.WindowedValue$ValueInGlobalWindow", "");    serializers.put("org.apache.beam.sdk.util.WindowedValue$TimestampedValueInSingleWindow", "");    serializers.put("org.apache.beam.sdk.util.WindowedValue$TimestampedValueInGlobalWindow", "");    serializers.put("org.apache.beam.sdk.util.WindowedValue$TimestampedValueInMultipleWindows", "");    serializers.put("org.apache.beam.sdk.transforms.windowing.PaneInfo", "");    serializers.put("org.apache.beam.sdk.transforms.windowing.PaneInfo$Timing", "");    serializers.put("org.joda.time.Instant", "");    serializers.put("org.apache.beam.sdk.values.KV", "");    serializers.put("org.apache.beam.sdk.transforms.windowing.IntervalWindow", "");    serializers.put("org.apache.beam.sdk.values.TimestampedValue", "");    serializers.put("org.apache.beam.runners.gearpump.translators.utils.TranslatorUtils$RawUnionValue", "");    if (userSerializers != null && !userSerializers.isEmpty()) {        serializers.putAll(userSerializers);    }    return config.withValue(GEARPUMP_SERIALIZERS, ConfigValueFactory.fromMap(serializers));}
public Iterable<Class<? extends PipelineRunner<?>>> beam_f6132_0()
{    return ImmutableList.of(GearpumpRunner.class, TestGearpumpRunner.class);}
public Iterable<Class<? extends PipelineOptions>> beam_f6133_0()
{    return ImmutableList.of(GearpumpPipelineOptions.class);}
public List<T> beam_f6141_0(List<T> accumulator)
{    return accumulator;}
public Coder<List<T>> beam_f6142_0(CoderRegistry registry, Coder<T> inputCoder)
{    return ListCoder.of(inputCoder);}
public Coder<List<T>> beam_f6143_0(CoderRegistry registry, Coder<T> inputCoder)
{    return ListCoder.of(inputCoder);}
public void beam_f6151_0()
{    sideInputReader = new SideInputHandler(sideInputs, InMemoryStateInternals.<Void>forKey(null));    doFnInvoker = DoFnInvokers.invokerFor(doFn);    if (doFnInvoker != null) {        doFnInvoker.invokeSetup();    }    doFnRunner = doFnRunnerFactory.createRunner(sideInputReader);    pushedBackValues = new ArrayList<>();    outputManager.setup(mainOutput, sideOutputs);}
public void beam_f6152_0()
{    if (doFnInvoker != null) {        doFnInvoker.invokeTeardown();    }}
public Iterator<TranslatorUtils.RawUnionValue> beam_f6153_0(List<RawUnionValue> inputs)
{    outputManager.clear();    doFnRunner.startBundle();    for (RawUnionValue unionValue : inputs) {        final String tag = unionValue.getUnionTag();        if ("0".equals(tag)) {                        pushedBackValues.add((WindowedValue<InputT>) unionValue.getValue());        } else {                        PCollectionView<?> sideInput = tagsToSideInputs.get(unionValue.getUnionTag());            WindowedValue<Iterable<?>> sideInputValue = (WindowedValue<Iterable<?>>) unionValue.getValue();            sideInputReader.addSideInputValue(sideInput, sideInputValue);        }    }    for (PCollectionView<?> sideInput : sideInputs) {        for (WindowedValue<InputT> value : pushedBackValues) {            for (BoundedWindow win : value.getWindows()) {                BoundedWindow sideInputWindow = sideInput.getWindowMappingFn().getSideInputWindow(win);                if (!sideInputReader.isReady(sideInput, sideInputWindow)) {                    Object emptyValue = WindowedValue.of(new ArrayList<>(), value.getTimestamp(), sideInputWindow, value.getPane());                    sideInputReader.addSideInputValue(sideInput, (WindowedValue<Iterable<?>>) emptyValue);                }            }        }    }    List<WindowedValue<InputT>> nextPushedBackValues = new ArrayList<>();    for (WindowedValue<InputT> value : pushedBackValues) {        Iterable<WindowedValue<InputT>> values = doFnRunner.processElementInReadyWindows(value);        Iterables.addAll(nextPushedBackValues, values);    }    pushedBackValues.clear();    Iterables.addAll(pushedBackValues, nextPushedBackValues);    doFnRunner.finishBundle();    return outputManager.getOutputs();}
public void beam_f6161_1(TransformHierarchy.Node node)
{        PTransform transform = node.getTransform();    TransformTranslator translator = getTransformTranslator(transform.getClass());    if (null == translator) {        throw new IllegalStateException("no translator registered for " + transform);    }    translationContext.setCurrentTransform(node, getPipeline());    translator.translate(transform, translationContext);}
public void beam_f6162_1(PValue value, TransformHierarchy.Node producer)
{    }
private static void beam_f6163_0(Class<TransformT> transformClass, TransformTranslator<? extends TransformT> transformTranslator)
{    if (transformTranslators.put(transformClass, transformTranslator) != null) {        throw new IllegalArgumentException("defining multiple translators for " + transformClass);    }}
public KV<Instant, WindowedValue<KV<K, List<V>>>> beam_f6171_0()
{    return KV.of(null, null);}
public KV<Instant, WindowedValue<KV<K, List<V>>>> beam_f6172_0(KV<Instant, WindowedValue<KV<K, List<V>>>> accum, KV<Instant, WindowedValue<KV<K, V>>> iter)
{    if (accum.getKey() == null) {        WindowedValue<KV<K, V>> wv = iter.getValue();        KV<K, V> kv = wv.getValue();        V v = kv.getValue();        List<V> nv = Lists.newArrayList(v);        return KV.of(iter.getKey(), wv.withValue(KV.of(kv.getKey(), nv)));    }    Instant t1 = accum.getKey();    Instant t2 = iter.getKey();    final WindowedValue<KV<K, List<V>>> wv1 = accum.getValue();    final WindowedValue<KV<K, V>> wv2 = iter.getValue();    wv1.getValue().getValue().add(wv2.getValue().getValue());    final List<BoundedWindow> mergedWindows = new ArrayList<>();    if (!windowFn.isNonMerging()) {        try {            windowFn.mergeWindows(windowFn.new MergeContext() {                @Override                public Collection<BoundedWindow> windows() {                    ArrayList<BoundedWindow> windows = new ArrayList<>();                    windows.addAll(wv1.getWindows());                    windows.addAll(wv2.getWindows());                    return windows;                }                @Override                public void merge(Collection<BoundedWindow> toBeMerged, BoundedWindow mergeResult) throws Exception {                    mergedWindows.add(mergeResult);                }            });        } catch (Exception e) {            throw new RuntimeException(e);        }    } else {        mergedWindows.addAll(wv1.getWindows());    }    Instant timestamp = timestampCombiner.combine(t1, t2);    return KV.of(timestamp, WindowedValue.of(wv1.getValue(), timestamp, mergedWindows, wv1.getPane()));}
public Collection<BoundedWindow> beam_f6173_0()
{    ArrayList<BoundedWindow> windows = new ArrayList<>();    windows.addAll(wv1.getWindows());    windows.addAll(wv2.getWindows());    return windows;}
protected Source.Reader<OutputT> beam_f6181_0(PipelineOptions options) throws IOException
{    return source.createReader(options, null);}
private byte[] beam_f6182_0(Iterable<T> values, IterableCoder<T> coder)
{    try (ByteArrayOutputStream stream = new ByteArrayOutputStream()) {        coder.encode(values, stream, Coder.Context.OUTER);        return stream.toByteArray();    } catch (IOException ex) {        throw new RuntimeException(ex);    }}
private Iterable<T> beam_f6183_0(byte[] bytes) throws IOException
{    try (ByteArrayInputStream inputStream = new ByteArrayInputStream(bytes)) {        return iterableCoder.decode(inputStream, Coder.Context.OUTER);    } catch (IOException ex) {        throw new RuntimeException(ex);    }}
public Instant beam_f6191_0() throws NoSuchElementException
{    return getTimestamp(current);}
public Instant beam_f6193_0()
{    if (iterator.hasNext()) {        return getTimestamp(current);    } else {        return BoundedWindow.TIMESTAMP_MAX_VALUE;    }}
public CheckpointMark beam_f6194_0()
{    return CheckpointMark.NOOP_CHECKPOINT_MARK;}
public GearpumpPipelineOptions beam_f6202_0()
{    return pipelineOptions;}
public JavaStream<InputT> beam_f6203_0(PValue input)
{    return (JavaStream<InputT>) streams.get(input);}
public void beam_f6204_0(PValue output, JavaStream<OutputT> outputStream)
{    if (!streams.containsKey(output)) {        streams.put(output, outputStream);    } else {        throw new RuntimeException("set stream for duplicated output " + output);    }}
public StateInternals beam_f6212_0()
{    throw new UnsupportedOperationException();}
public TimerInternals beam_f6213_0()
{    throw new UnsupportedOperationException();}
public static Instant beam_f6214_0(org.joda.time.Instant time)
{    return Instant.ofEpochMilli(time.getMillis());}
public WindowedValue<OutputT> beam_f6222_0(RawUnionValue value)
{    return (WindowedValue<OutputT>) value.getValue();}
public RawUnionValue beam_f6223_0(WindowedValue<T> windowedValue)
{    return new RawUnionValue(tag, windowedValue);}
public String beam_f6224_0()
{    return unionTag;}
public Instant beam_f6232_0()
{    return value.getTimestamp();}
public BoundedWindow beam_f6233_0()
{    return Iterables.getOnlyElement(value.getWindows());}
public void beam_f6234_0()
{    String[] args = new String[] { String.format("--runner=%s", GearpumpRunner.class.getName()) };    PipelineOptions opts = PipelineOptionsFactory.fromArgs(args).create();    assertEquals(opts.getRunner(), GearpumpRunner.class);}
public void beam_f6242_0()
{    JavaStream javaStream = mock(JavaStream.class);    TranslationContext translationContext = mock(TranslationContext.class);    Map<TupleTag<?>, PValue> inputs = new HashMap<>();    TupleTag tag = mock(TupleTag.class);    PCollection mockCollection = mock(PCollection.class);    inputs.put(tag, mockCollection);    when(translationContext.getInputs()).thenReturn(inputs);    when(translationContext.getInputStream(mockCollection)).thenReturn(javaStream);    PValue mockOutput = mock(PValue.class);    when(translationContext.getOutput()).thenReturn(mockOutput);    translator.translate(transform, translationContext);    verify(translationContext, times(1)).setOutputStream(mockOutput, javaStream);}
public void beam_f6243_0()
{    String transformName = "transform";    when(transform.getName()).thenReturn(transformName);    JavaStream javaStream1 = mock(JavaStream.class);    JavaStream javaStream2 = mock(JavaStream.class);    JavaStream mergedStream = mock(JavaStream.class);    TranslationContext translationContext = mock(TranslationContext.class);    Map<TupleTag<?>, PValue> inputs = new HashMap<>();    TupleTag tag1 = mock(TupleTag.class);    PCollection mockCollection1 = mock(PCollection.class);    inputs.put(tag1, mockCollection1);    TupleTag tag2 = mock(TupleTag.class);    PCollection mockCollection2 = mock(PCollection.class);    inputs.put(tag2, mockCollection2);    PCollection output = mock(PCollection.class);    when(translationContext.getInputs()).thenReturn(inputs);    when(translationContext.getInputStream(mockCollection1)).thenReturn(javaStream1);    when(translationContext.getInputStream(mockCollection2)).thenReturn(javaStream2);    when(javaStream1.merge(javaStream2, 1, transformName)).thenReturn(mergedStream);    when(javaStream2.merge(javaStream1, 1, transformName)).thenReturn(mergedStream);    when(translationContext.getOutput()).thenReturn(output);    translator.translate(transform, translationContext);    verify(translationContext).setOutputStream(output, mergedStream);}
public void beam_f6244_0()
{    String transformName = "transform";    when(transform.getName()).thenReturn(transformName);    JavaStream javaStream1 = mock(JavaStream.class);    TranslationContext translationContext = mock(TranslationContext.class);    Map<TupleTag<?>, PValue> inputs = new HashMap<>();    TupleTag tag1 = mock(TupleTag.class);    PCollection mockCollection1 = mock(PCollection.class);    inputs.put(tag1, mockCollection1);    TupleTag tag2 = mock(TupleTag.class);    inputs.put(tag2, mockCollection1);    when(translationContext.getInputs()).thenReturn(inputs);    when(translationContext.getInputStream(mockCollection1)).thenReturn(javaStream1);    when(translationContext.getPipelineOptions()).thenReturn(PipelineOptionsFactory.as(GearpumpPipelineOptions.class));    translator.translate(transform, translationContext);    verify(javaStream1).map(any(MapFunction.class), eq("dummy"));    verify(javaStream1).merge(eq(null), eq(1), eq(transformName));}
public void beam_f6252_0()
{    GearpumpPipelineOptions options = PipelineOptionsFactory.create().as(GearpumpPipelineOptions.class);    ValuesSource<TimestampedValue<String>> valuesSource = new ValuesSource<>(TEST_VALUES, TimestampedValue.TimestampedValueCoder.of(StringUtf8Coder.of()));    SourceForTest<TimestampedValue<String>> sourceForTest = new SourceForTest<>(options, valuesSource);    sourceForTest.open(null, Instant.EPOCH);    for (int i = 0; i < TEST_VALUES.size(); i++) {        TimestampedValue<String> value = TEST_VALUES.get(i);                if (i < TEST_VALUES.size() - 1) {            Instant expectedWaterMark = TranslatorUtils.jodaTimeToJava8Time(value.getTimestamp());            Assert.assertEquals(expectedWaterMark, sourceForTest.getWatermark());        } else {            Assert.assertEquals(Watermark.MAX(), sourceForTest.getWatermark());        }        Message expectedMsg = new DefaultMessage(WindowedValue.timestampedValueInGlobalWindow(value, value.getTimestamp()), value.getTimestamp().getMillis());        Message message = sourceForTest.read();        Assert.assertEquals(expectedMsg, message);    }    Assert.assertNull(sourceForTest.read());    Assert.assertEquals(Watermark.MAX(), sourceForTest.getWatermark());}
public void beam_f6253_0()
{    GearpumpPipelineOptions options = PipelineOptionsFactory.create().as(GearpumpPipelineOptions.class);    Config config = ClusterConfig.master(null);    config = config.withValue(Constants.APPLICATION_TOTAL_RETRIES(), ConfigValueFactory.fromAnyRef(0));    options.setRemote(false);    options.setRunner(GearpumpRunner.class);    options.setParallelism(1);    Pipeline p = Pipeline.create(options);    List<String> values = Lists.newArrayList("1", "2", "3", "4", "5");    ValuesSource<String> source = new ValuesSource<>(values, StringUtf8Coder.of());    p.apply(Read.from(source)).apply(ParDo.of(new ResultCollector()));    p.run().waitUntilFinish();    Assert.assertEquals(Sets.newHashSet(values), ResultCollector.RESULTS);}
public void beam_f6254_0(ProcessContext c) throws Exception
{    RESULTS.add(c.element());}
public void beam_f6262_0()
{    WindowFn slidingWindows = Sessions.withGapDuration(Duration.millis(10));    WindowAssignTranslator.AssignWindows<String> assignWindows = new WindowAssignTranslator.AssignWindows(slidingWindows);    String value = "v1";    Instant timestamp = new Instant(1);    WindowedValue<String> windowedValue = WindowedValue.timestampedValueInGlobalWindow(value, timestamp);    ArrayList<WindowedValue<String>> expected = new ArrayList<>();    expected.add(WindowedValue.of(value, timestamp, new IntervalWindow(new Instant(1), new Instant(11)), PaneInfo.NO_FIRING));    Iterator<WindowedValue<String>> result = assignWindows.flatMap(windowedValue);    assertThat(expected, equalTo(Lists.newArrayList(result)));}
public void beam_f6263_0()
{    WindowFn slidingWindows = new GlobalWindows();    WindowAssignTranslator.AssignWindows<String> assignWindows = new WindowAssignTranslator.AssignWindows(slidingWindows);    String value = "v1";    Instant timestamp = new Instant(1);    WindowedValue<String> windowedValue = WindowedValue.timestampedValueInGlobalWindow(value, timestamp);    ArrayList<WindowedValue<String>> expected = new ArrayList<>();    expected.add(WindowedValue.timestampedValueInGlobalWindow(value, timestamp));    Iterator<WindowedValue<String>> result = assignWindows.flatMap(windowedValue);    assertThat(expected, equalTo(Lists.newArrayList(result)));}
public static PTransformOverrideFactory<PCollection<KV<K, InputT>>, PCollection<OutputT>, ParDo.SingleOutput<KV<K, InputT>, OutputT>> beam_f6264_0(DataflowPipelineOptions options)
{    return new SingleOutputOverrideFactory<>(isFnApi(options));}
public PCollection<OutputT> beam_f6272_0(PCollection<KV<K, InputT>> input)
{    DoFn<KV<K, InputT>, OutputT> fn = originalParDo.getFn();    verifyFnIsStateful(fn);    DataflowRunner.verifyStateSupported(fn);    DataflowRunner.verifyStateSupportForWindowingStrategy(input.getWindowingStrategy());    if (isFnApi) {        return input.apply(GroupByKey.create()).apply(ParDo.of(new ExpandGbkFn<>())).apply(originalParDo);    }    PTransform<PCollection<? extends KV<K, Iterable<KV<Instant, WindowedValue<KV<K, InputT>>>>>>, PCollection<OutputT>> statefulParDo = ParDo.of(new BatchStatefulDoFn<>(fn)).withSideInputs(originalParDo.getSideInputs());    return input.apply(new GbkBeforeStatefulParDo<>()).apply(statefulParDo);}
public PCollectionTuple beam_f6273_0(PCollection<KV<K, InputT>> input)
{    DoFn<KV<K, InputT>, OutputT> fn = originalParDo.getFn();    verifyFnIsStateful(fn);    DataflowRunner.verifyStateSupported(fn);    DataflowRunner.verifyStateSupportForWindowingStrategy(input.getWindowingStrategy());    if (isFnApi) {        return input.apply(GroupByKey.create()).apply(ParDo.of(new ExpandGbkFn<>())).apply(originalParDo);    }    PTransform<PCollection<? extends KV<K, Iterable<KV<Instant, WindowedValue<KV<K, InputT>>>>>>, PCollectionTuple> statefulParDo = ParDo.of(new BatchStatefulDoFn<>(fn)).withSideInputs(originalParDo.getSideInputs()).withOutputTags(originalParDo.getMainOutputTag(), originalParDo.getAdditionalOutputTags());    return input.apply(new GbkBeforeStatefulParDo<>()).apply(statefulParDo);}
public ParDo.MultiOutput<KV<K, InputT>, OutputT> beam_f6274_0()
{    return originalParDo;}
public TypeDescriptor<OutputT> beam_f6282_0()
{    return underlyingDoFn.getOutputTypeDescriptor();}
private static void beam_f6283_0(DoFn<InputT, OutputT> fn)
{    DoFnSignature signature = DoFnSignatures.getSignature(fn.getClass());            checkState(signature.usesState() || signature.usesTimers(), "%s used for %s that does not use state or timers.", BatchStatefulParDoOverrides.class.getSimpleName(), ParDo.class.getSimpleName());}
public void beam_f6284_0(ProcessContext c) throws Exception
{    Optional<Object> previousWindowStructuralValue = Optional.absent();    Optional<W> previousWindow = Optional.absent();    Map<K, WindowedValue<V>> map = new HashMap<>();    for (KV<W, WindowedValue<KV<K, V>>> kv : c.element().getValue()) {        Object currentWindowStructuralValue = windowCoder.structuralValue(kv.getKey());        if (previousWindowStructuralValue.isPresent() && !previousWindowStructuralValue.get().equals(currentWindowStructuralValue)) {                                    c.output(IsmRecord.of(ImmutableList.of(previousWindow.get()), valueInEmptyWindows(new TransformedMap<>(WindowedValueToValue.of(), map))));            map = new HashMap<>();        }                checkState(!map.containsKey(kv.getValue().getValue().getKey()), "Multiple values [%s, %s] found for single key [%s] within window [%s].", map.get(kv.getValue().getValue().getKey()), kv.getValue().getValue().getValue(), kv.getKey());        map.put(kv.getValue().getValue().getKey(), kv.getValue().withValue(kv.getValue().getValue().getValue()));        previousWindowStructuralValue = Optional.of(currentWindowStructuralValue);        previousWindow = Optional.of(kv.getKey());    }                c.output(IsmRecord.of(ImmutableList.of(previousWindow.get()), valueInEmptyWindows(new TransformedMap<>(WindowedValueToValue.of(), map))));}
private void beam_f6292_0(ProcessContext c, KV<KV<K, W>, WindowedValue<V>> value, long keyIndex)
{    IsmRecord<WindowedValue<V>> ismRecord = IsmRecord.of(ImmutableList.of(value.getKey().getKey(), value.getKey().getValue(), keyIndex), value.getValue());    c.output(ismRecord);}
private void beam_f6293_0(ProcessContext c, KV<KV<K, W>, WindowedValue<V>> value, long uniqueKeyCount)
{    c.output(outputForSize, KV.of(ismCoder.hash(ImmutableList.of(IsmFormat.getMetadataKey(), value.getKey().getValue())), KV.of(value.getKey().getValue(), uniqueKeyCount)));}
private void beam_f6294_0(ProcessContext c, KV<KV<K, W>, WindowedValue<V>> value)
{    c.output(outputForEntrySet, KV.of(ismCoder.hash(ImmutableList.of(IsmFormat.getMetadataKey(), value.getKey().getValue())), KV.of(value.getKey().getValue(), value.getKey().getKey())));}
protected String beam_f6302_0()
{    return "BatchViewAsMultimap";}
 static IsmRecordCoder<WindowedValue<V>> beam_f6303_0(Coder<? extends BoundedWindow> windowCoder, Coder<?> keyCoder, Coder<V> valueCoder)
{        return IsmRecordCoder.of(    1,     2,     ImmutableList.of(MetadataKeyCoder.of(keyCoder), windowCoder, BigEndianLongCoder.of()), FullWindowedValueCoder.of(valueCoder, windowCoder));}
public void beam_f6304_0(ProcessContext c) throws Exception
{    Optional<Object> previousWindowStructuralValue = Optional.absent();    T previousValue = null;    for (KV<W, WindowedValue<T>> next : c.element().getValue()) {        Object currentWindowStructuralValue = windowCoder.structuralValue(next.getKey());                        checkState(!previousWindowStructuralValue.isPresent() || !previousWindowStructuralValue.get().equals(currentWindowStructuralValue), "Multiple values [%s, %s] found for singleton within window [%s].", previousValue, next.getValue().getValue(), next.getKey());        c.output(IsmRecord.of(ImmutableList.of(next.getKey()), next.getValue()));        previousWindowStructuralValue = Optional.of(currentWindowStructuralValue);        previousValue = next.getValue().getValue();    }}
public PCollection<?> beam_f6312_0(PCollection<T> input)
{    return applyForIterableLike(runner, input, view);}
 static PCollection<?> beam_f6313_0(DataflowRunner runner, PCollection<T> input, PCollectionView<ViewT> view)
{    @SuppressWarnings("unchecked")    Coder<W> windowCoder = (Coder<W>) input.getWindowingStrategy().getWindowFn().windowCoder();    IsmRecordCoder<WindowedValue<T>> ismCoder = coderForListLike(windowCoder, input.getCoder());        if (input.getWindowingStrategy().getWindowFn() instanceof GlobalWindows) {        PCollection<IsmRecord<WindowedValue<T>>> reifiedPerWindowAndSorted = input.apply(ParDo.of(new ToIsmRecordForGlobalWindowDoFn<>()));        reifiedPerWindowAndSorted.setCoder(ismCoder);        runner.addPCollectionRequiringIndexedFormat(reifiedPerWindowAndSorted);        reifiedPerWindowAndSorted.apply(CreateDataflowView.forBatch(view));        return reifiedPerWindowAndSorted;    }    PCollection<IsmRecord<WindowedValue<T>>> reifiedPerWindowAndSorted = input.apply(new GroupByWindowHashAsKeyAndWindowAsSortKey<T, W>(ismCoder)).apply(ParDo.of(new ToIsmRecordForNonGlobalWindowDoFn<>(windowCoder)));    reifiedPerWindowAndSorted.setCoder(ismCoder);    runner.addPCollectionRequiringIndexedFormat(reifiedPerWindowAndSorted);    reifiedPerWindowAndSorted.apply(CreateDataflowView.forBatch(view));    return reifiedPerWindowAndSorted;}
protected String beam_f6314_0()
{    return "BatchViewAsList";}
public PCollection<KV<Integer, Iterable<KV<W, WindowedValue<T>>>>> beam_f6322_0(PCollection<T> input)
{    @SuppressWarnings("unchecked")    Coder<W> windowCoder = (Coder<W>) input.getWindowingStrategy().getWindowFn().windowCoder();    PCollection<KV<Integer, KV<W, WindowedValue<T>>>> rval = input.apply(ParDo.of(new UseWindowHashAsKeyAndWindowAsSortKeyDoFn<T, W>(ismCoderForHash)));    rval.setCoder(KvCoder.of(VarIntCoder.of(), KvCoder.of(windowCoder, FullWindowedValueCoder.of(input.getCoder(), windowCoder))));    return rval.apply(new GroupByKeyAndSortValuesOnly<>());}
public PCollection<KV<K1, Iterable<KV<K2, V>>>> beam_f6323_0(PCollection<KV<K1, KV<K2, V>>> input)
{    @SuppressWarnings("unchecked")    KvCoder<K1, KV<K2, V>> inputCoder = (KvCoder<K1, KV<K2, V>>) input.getCoder();    return PCollection.createPrimitiveOutputInternal(input.getPipeline(), WindowingStrategy.globalDefault(), IsBounded.BOUNDED, KvCoder.of(inputCoder.getKeyCoder(), IterableCoder.of(inputCoder.getValueCoder())));}
protected Map<K, V2> beam_f6324_0()
{    return transformedMap;}
public T beam_f6332_0()
{    return value;}
public Instant beam_f6333_0()
{    return BoundedWindow.TIMESTAMP_MIN_VALUE;}
public Collection<? extends BoundedWindow> beam_f6334_0()
{    return Collections.emptyList();}
public PCollectionView<ViewT> beam_f6342_0()
{    return view;}
public static DataflowClient beam_f6343_0(DataflowPipelineOptions options)
{    return new DataflowClient(options.getDataflowClient(), options);}
public Job beam_f6344_0(@Nonnull Job job) throws IOException
{    checkNotNull(job, "job");    Jobs.Create jobsCreate = dataflow.projects().locations().jobs().create(options.getProject(), options.getRegion(), job);    return jobsCreate.execute();}
public DataflowPipelineJob beam_f6352_0()
{    return job;}
private MetricQueryResults beam_f6353_0(List<MetricUpdate> metricUpdates, MetricsFilter filter)
{    return DataflowMetricQueryResultsFactory.create(dataflowPipelineJob, metricUpdates, filter).build();}
public MetricQueryResults beam_f6354_1(@Nullable MetricsFilter filter)
{    List<MetricUpdate> metricUpdates;    ImmutableList<MetricResult<Long>> counters = ImmutableList.of();    ImmutableList<MetricResult<DistributionResult>> distributions = ImmutableList.of();    ImmutableList<MetricResult<GaugeResult>> gauges = ImmutableList.of();    JobMetrics jobMetrics;    try {        jobMetrics = getJobMetrics();    } catch (IOException e) {                return MetricQueryResults.create(counters, distributions, gauges);    }    metricUpdates = firstNonNull(jobMetrics.getMetrics(), Collections.emptyList());    return populateMetricQueryResults(metricUpdates, filter);}
public static DataflowMetricQueryResultsFactory beam_f6362_0(DataflowPipelineJob dataflowPipelineJob, Iterable<MetricUpdate> metricUpdates, MetricsFilter filter)
{    return new DataflowMetricQueryResultsFactory(dataflowPipelineJob, metricUpdates, filter);}
private boolean beam_f6363_0(MetricUpdate metricUpdate)
{    return metricUpdate.getName().getContext().containsKey("tentative") && Objects.equal(metricUpdate.getName().getContext().get("tentative"), "true");}
private MetricKey beam_f6364_0(MetricUpdate metricUpdate)
{    String fullStepName = metricUpdate.getName().getContext().get("step");    if (dataflowPipelineJob.transformStepNames == null || !dataflowPipelineJob.transformStepNames.inverse().containsKey(fullStepName)) {                return null;    }    fullStepName = dataflowPipelineJob.transformStepNames.inverse().get(fullStepName).getFullName();    return MetricKey.create(fullStepName, MetricName.named(metricUpdate.getName().getContext().get("namespace"), metricUpdate.getName().getName()));}
public State beam_f6372_0()
{    return waitUntilFinish(Duration.millis(-1));}
public State beam_f6373_0(Duration duration)
{    try {        return waitUntilFinish(duration, new MonitoringUtil.LoggingHandler());    } catch (Exception e) {        if (e instanceof InterruptedException) {            Thread.currentThread().interrupt();        }        if (e instanceof RuntimeException) {            throw (RuntimeException) e;        }        throw new RuntimeException(e);    }}
 State beam_f6375_0(Duration duration, @Nullable MonitoringUtil.JobMessagesHandler messageHandler, Sleeper sleeper, NanoClock nanoClock) throws IOException, InterruptedException
{    return waitUntilFinish(duration, messageHandler, sleeper, nanoClock, new MonitoringUtil(dataflowClient));}
 State beam_f6383_0(BackOff attempts, Sleeper sleeper)
{    try {        return getStateWithRetries(attempts, sleeper);    } catch (IOException exn) {                return State.UNKNOWN;    }}
 State beam_f6384_0(BackOff attempts, Sleeper sleeper) throws IOException
{    if (terminalState != null) {        return terminalState;    }    Job job = getJobWithRetries(attempts, sleeper);    return MonitoringUtil.toState(job.getCurrentState());}
private Job beam_f6385_1(BackOff backoff, Sleeper sleeper) throws IOException
{        while (true) {        try {            Job job = dataflowClient.getJob(getJobId());            State currentState = MonitoringUtil.toState(job.getCurrentState());            if (currentState.isTerminal()) {                terminalState = currentState;                replacedByJob = new DataflowPipelineJob(dataflowClient, job.getReplacedByJobId(), dataflowOptions, transformStepNames);            }            return job;        } catch (IOException exn) {                                    if (!nextBackOff(sleeper, backoff)) {                throw exn;            }        }    }}
public Job beam_f6393_0()
{    return job;}
public RunnerApi.Pipeline beam_f6394_0()
{    return pipelineProto;}
public Map<AppliedPTransform<?, ?, ?>, String> beam_f6395_0()
{    return stepNames;}
public InputT beam_f6403_0(PTransform<InputT, ?> transform)
{    return (InputT) Iterables.getOnlyElement(TransformInputs.nonAdditionalInputs(getCurrentTransform(transform)));}
public Map<TupleTag<?>, PValue> beam_f6404_0(PTransform<?, OutputT> transform)
{    return getCurrentTransform(transform).getOutputs();}
public OutputT beam_f6405_0(PTransform<?, OutputT> transform)
{    return (OutputT) Iterables.getOnlyElement(getOutputs(transform).values());}
public SdkComponents beam_f6413_0()
{    return sdkComponents;}
public AppliedPTransform<?, ?, ?> beam_f6414_0(PValue value)
{    return checkNotNull(producers.get(value), "Unknown producer for value %s while translating step %s", value, currentTransform.getFullName());}
private String beam_f6415_0()
{    return "s" + (stepNames.size() + 1);}
public void beam_f6423_0(String name, String value)
{    addString(getProperties(), name, value);}
public void beam_f6424_0(String name, Long value)
{    addLong(getProperties(), name, value);}
public void beam_f6425_0(String name, Map<String, Object> elements)
{    addDictionary(getProperties(), name, elements);}
private static Map<String, Object> beam_f6433_0(Step step)
{    Map<String, Object> properties = step.getProperties();    if (properties == null) {        properties = new HashMap<>();        step.setProperties(properties);    }    return properties;}
public void beam_f6434_0(View.CreatePCollectionView transform, TranslationContext context)
{    translateTyped(transform, context);}
private void beam_f6435_0(View.CreatePCollectionView<ElemT, ViewT> transform, TranslationContext context)
{    StepTranslationContext stepContext = context.addStep(transform, "CollectionToSingleton");    PCollection<ElemT> input = context.getInput(transform);    stepContext.addInput(PropertyNames.PARALLEL_INPUT, input);    WindowingStrategy<?, ?> windowingStrategy = input.getWindowingStrategy();    stepContext.addInput(PropertyNames.WINDOWING_STRATEGY, byteArrayToJsonString(serializeWindowingStrategy(windowingStrategy, context.getPipelineOptions())));    stepContext.addInput(PropertyNames.IS_MERGING_WINDOW_FN, !windowingStrategy.getWindowFn().isNonMerging());    stepContext.addCollectionToSingletonOutput(input, PropertyNames.OUTPUT, transform.getView());}
private void beam_f6443_0(GroupByKeyAndSortValuesOnly<K1, K2, V> transform, TranslationContext context)
{    StepTranslationContext stepContext = context.addStep(transform, "GroupByKey");    PCollection<KV<K1, KV<K2, V>>> input = context.getInput(transform);    stepContext.addInput(PropertyNames.PARALLEL_INPUT, input);    stepContext.addOutput(PropertyNames.OUTPUT, context.getOutput(transform));    stepContext.addInput(PropertyNames.SORT_VALUES, true);        stepContext.addInput(PropertyNames.DISALLOW_COMBINER_LIFTING, true);}
public void beam_f6444_0(GroupByKey transform, TranslationContext context)
{    groupByKeyHelper(transform, context);}
private void beam_f6445_0(GroupByKey<K, V> transform, TranslationContext context)
{    StepTranslationContext stepContext = context.addStep(transform, "GroupByKey");    PCollection<KV<K, V>> input = context.getInput(transform);    stepContext.addInput(PropertyNames.PARALLEL_INPUT, input);    stepContext.addOutput(PropertyNames.OUTPUT, context.getOutput(transform));    WindowingStrategy<?, ?> windowingStrategy = input.getWindowingStrategy();    boolean isStreaming = context.getPipelineOptions().as(StreamingOptions.class).isStreaming();    boolean allowCombinerLifting = windowingStrategy.getWindowFn().isNonMerging() && windowingStrategy.getWindowFn().assignsToOneWindow();    if (isStreaming) {        allowCombinerLifting &= transform.fewKeys();                allowCombinerLifting &= (windowingStrategy.getTrigger() instanceof DefaultTrigger);    }    stepContext.addInput(PropertyNames.DISALLOW_COMBINER_LIFTING, !allowCombinerLifting);    stepContext.addInput(PropertyNames.SERIALIZED_FN, byteArrayToJsonString(serializeWindowingStrategy(windowingStrategy, context.getPipelineOptions())));    stepContext.addInput(PropertyNames.IS_MERGING_WINDOW_FN, !windowingStrategy.getWindowFn().isNonMerging());}
private void beam_f6453_0(SplittableParDo.ProcessKeyedElements<InputT, OutputT, RestrictionT> transform, TranslationContext context)
{    DoFnSchemaInformation doFnSchemaInformation;    doFnSchemaInformation = ParDoTranslation.getSchemaInformation(context.getCurrentTransform());    Map<String, PCollectionView<?>> sideInputMapping = ParDoTranslation.getSideInputMapping(context.getCurrentTransform());    StepTranslationContext stepContext = context.addStep(transform, "SplittableProcessKeyed");    Map<TupleTag<?>, Coder<?>> outputCoders = context.getOutputs(transform).entrySet().stream().collect(Collectors.toMap(Map.Entry::getKey, e -> ((PCollection) e.getValue()).getCoder()));    translateInputs(stepContext, context.getInput(transform), transform.getSideInputs(), context);    translateOutputs(context.getOutputs(transform), stepContext);    String ptransformId = context.getSdkComponents().getPTransformIdOrThrow(context.getCurrentTransform());    translateFn(stepContext, ptransformId, transform.getFn(), transform.getInputWindowingStrategy(), transform.getSideInputs(), transform.getElementCoder(), context, transform.getMainOutputTag(), outputCoders, doFnSchemaInformation, sideInputMapping);    stepContext.addInput(PropertyNames.RESTRICTION_CODER, translateCoder(transform.getRestrictionCoder(), context));}
private static void beam_f6454_0(StepTranslationContext stepContext, PCollection<?> input, Iterable<PCollectionView<?>> sideInputs, TranslationContext context)
{    stepContext.addInput(PropertyNames.PARALLEL_INPUT, input);    translateSideInputs(stepContext, sideInputs, context);}
private static void beam_f6455_0(StepTranslationContext stepContext, Iterable<PCollectionView<?>> sideInputs, TranslationContext context)
{    Map<String, Object> nonParInputs = new HashMap<>();    for (PCollectionView<?> view : sideInputs) {        nonParInputs.put(view.getTagInternal().getId(), context.asOutputReference(view, context.getProducer(view)));    }    stepContext.addInput(PropertyNames.NON_PARALLEL_INPUTS, nonParInputs);}
public CompositeBehavior beam_f6463_0(TransformHierarchy.Node node)
{    CompositeBehavior behavior = CompositeBehavior.ENTER_TRANSFORM;        if (!node.isRootNode() && node.toAppliedPTransform(getPipeline()).equals(application)) {                if (parents.isEmpty()) {            parent[0] = null;        } else {            parent[0] = parents.peekFirst();        }        behavior = CompositeBehavior.DO_NOT_ENTER_TRANSFORM;    }            parents.addFirst(node);    return behavior;}
public void beam_f6464_0(TransformHierarchy.Node node)
{    if (!node.isRootNode()) {        parents.removeFirst();    }}
public String beam_f6465_0()
{    return toStringHelper(CombineValuesWithParentCheckPTransformMatcher.class).toString();}
public Map<PValue, ReplacementOutput> beam_f6474_0(Map<TupleTag<?>, PValue> outputs, PValue newOutput)
{        return ImmutableMap.of();}
public PTransformReplacement<PCollection<InputT>, PCollection<OutputT>> beam_f6475_0(AppliedPTransform<PCollection<InputT>, PCollection<OutputT>, TransformT> transform)
{    PTransform<PCollection<InputT>, PCollection<OutputT>> rep = InstanceBuilder.ofType(replacement).withArg(DataflowRunner.class, runner).withArg((Class<TransformT>) transform.getTransform().getClass(), transform.getTransform()).build();    return PTransformReplacement.of(PTransformReplacements.getSingletonMainInput(transform), rep);}
private static PTransformMatcher beam_f6476_0(boolean fnApiEnabled)
{    if (fnApiEnabled) {        return new DataflowPTransformMatchers.CombineValuesWithParentCheckPTransformMatcher();    } else {        return new DataflowPTransformMatchers.CombineValuesWithoutSideInputsPTransformMatcher();    }}
private boolean beam_f6484_0(Pipeline p)
{    class BoundednessVisitor extends PipelineVisitor.Defaults {        IsBounded boundedness = IsBounded.BOUNDED;        @Override        public void visitValue(PValue value, Node producer) {            if (value instanceof PCollection) {                boundedness = boundedness.and(((PCollection) value).isBounded());            }        }    }    BoundednessVisitor visitor = new BoundednessVisitor();    p.traverseTopologically(visitor);    return visitor.boundedness == IsBounded.UNBOUNDED;}
public void beam_f6485_0(PValue value, Node producer)
{    if (value instanceof PCollection) {        boundedness = boundedness.and(((PCollection) value).isBounded());    }}
public DataflowPipelineTranslator beam_f6486_0()
{    return translator;}
public PTransformReplacement<PBegin, PCollection<PubsubMessage>> beam_f6496_0(AppliedPTransform<PBegin, PCollection<PubsubMessage>, PubsubUnboundedSource> transform)
{    return PTransformReplacement.of(transform.getPipeline().begin(), new StreamingPubsubIORead(transform.getTransform()));}
public Map<PValue, ReplacementOutput> beam_f6497_0(Map<TupleTag<?>, PValue> outputs, PCollection<PubsubMessage> newOutput)
{    return ReplacementOutputs.singleton(outputs, newOutput);}
 PubsubUnboundedSource beam_f6498_0()
{    return transform;}
public void beam_f6506_0(StreamingPubsubIOWrite transform, TranslationContext context)
{    checkArgument(context.getPipelineOptions().isStreaming(), "StreamingPubsubIOWrite is only for streaming pipelines.");    PubsubUnboundedSink overriddenTransform = transform.getOverriddenTransform();    StepTranslationContext stepContext = context.addStep(transform, "ParallelWrite");    stepContext.addInput(PropertyNames.FORMAT, "pubsub");    if (overriddenTransform.getTopicProvider().isAccessible()) {        stepContext.addInput(PropertyNames.PUBSUB_TOPIC, overriddenTransform.getTopic().getV1Beta1Path());    } else {        stepContext.addInput(PropertyNames.PUBSUB_TOPIC_OVERRIDE, ((NestedValueProvider) overriddenTransform.getTopicProvider()).propertyName());    }    if (overriddenTransform.getTimestampAttribute() != null) {        stepContext.addInput(PropertyNames.PUBSUB_TIMESTAMP_ATTRIBUTE, overriddenTransform.getTimestampAttribute());    }    if (overriddenTransform.getIdAttribute() != null) {        stepContext.addInput(PropertyNames.PUBSUB_ID_ATTRIBUTE, overriddenTransform.getIdAttribute());    }    stepContext.addInput(PropertyNames.PUBSUB_SERIALIZED_ATTRIBUTES_FN, byteArrayToJsonString(serializeToByteArray(new IdentityMessageFn())));            stepContext.addEncodingInput(WindowedValue.getValueOnlyCoder(VoidCoder.of()));    stepContext.addInput(PropertyNames.PARALLEL_INPUT, context.getInput(transform));}
public PTransformReplacement<PBegin, PCollection<T>> beam_f6507_0(AppliedPTransform<PBegin, PCollection<T>, Create.Values<T>> transform)
{    Create.Values<T> original = transform.getTransform();    PCollection<T> output = (PCollection) Iterables.getOnlyElement(transform.getOutputs().values());    return PTransformReplacement.of(transform.getPipeline().begin(), new StreamingFnApiCreate<>(original, output));}
public Map<PValue, ReplacementOutput> beam_f6508_0(Map<TupleTag<?>, PValue> outputs, PCollection<T> newOutput)
{    return ReplacementOutputs.singleton(outputs, newOutput);}
public PCollection<KV<K, Iterable<V>>> beam_f6516_0(PCollection<KV<K, V>> input)
{    return input.apply("GroupAll", GroupByKey.create()).apply("SplitIntoBatches", ParDo.of(new DoFn<KV<K, Iterable<V>>, KV<K, Iterable<V>>>() {        @ProcessElement        public void process(ProcessContext c) {                                    Iterator<List<V>> iterator = Iterators.partition(c.element().getValue().iterator(), (int) batchSize);                        while (iterator.hasNext()) {                c.output(KV.of(c.element().getKey(), iterator.next()));            }        }    }));}
public void beam_f6517_0(ProcessContext c)
{            Iterator<List<V>> iterator = Iterators.partition(c.element().getValue().iterator(), (int) batchSize);        while (iterator.hasNext()) {        c.output(KV.of(c.element().getKey(), iterator.next()));    }}
public PTransformReplacement<PBegin, PCollection<T>> beam_f6518_0(AppliedPTransform<PBegin, PCollection<T>, Read.Unbounded<T>> transform)
{    return PTransformReplacement.of(transform.getPipeline().begin(), new StreamingUnboundedRead<>(transform.getTransform()));}
public PCollection<T> beam_f6526_0(PCollection<ValueWithRecordId<T>> input)
{    return input.apply(WithKeys.of((ValueWithRecordId<T> value) -> Arrays.hashCode(value.getId()) % NUM_RESHARD_KEYS).withKeyType(TypeDescriptors.integers())).apply(Reshuffle.of()).apply("StripIds", ParDo.of(new DoFn<KV<Integer, ValueWithRecordId<T>>, T>() {        @ProcessElement        public void processElement(ProcessContext c) {            c.output(c.element().getValue().getValue());        }    }));}
public void beam_f6527_0(ProcessContext c)
{    c.output(c.element().getValue().getValue());}
public PTransformReplacement<PBegin, PCollection<T>> beam_f6528_0(AppliedPTransform<PBegin, PCollection<T>, Read.Bounded<T>> transform)
{    return PTransformReplacement.of(transform.getPipeline().begin(), new StreamingBoundedRead<>(transform.getTransform()));}
public void beam_f6536_0(ProcessContext c, BoundedWindow w) throws Exception
{    throw new UnsupportedOperationException(String.format("%s is a marker class only and should never be executed.", getClass().getName()));}
public String beam_f6537_0()
{    return "DataflowRunner#" + options.getJobName();}
private String beam_f6538_0(String jobName)
{    try {        ListJobsResponse listResult;        String token = null;        do {            listResult = dataflowClient.listJobs(token);            token = listResult.getNextPageToken();            for (Job job : listResult.getJobs()) {                if (job.getName().equals(jobName) && MonitoringUtil.toState(job.getCurrentState()).equals(State.RUNNING)) {                    return job.getId();                }            }        } while (token != null);    } catch (GoogleJsonResponseException e) {        throw new RuntimeException("Got error while looking up jobs: " + (e.getDetails() != null ? e.getDetails().getMessage() : e), e);    } catch (IOException e) {        throw new RuntimeException("Got error while looking up jobs: ", e);    }    throw new IllegalArgumentException("Could not find running job named " + jobName);}
public Map<PValue, ReplacementOutput> beam_f6546_0(Map<TupleTag<?>, PValue> outputs, WriteFilesResult<DestinationT> newOutput)
{    return ReplacementOutputs.tagged(outputs, newOutput);}
 static String beam_f6547_0(DataflowPipelineOptions options)
{    String workerHarnessContainerImage = options.getWorkerHarnessContainerImage();    String javaVersionId = Float.parseFloat(System.getProperty("java.specification.version")) >= 9 ? "java11" : "java";    if (!workerHarnessContainerImage.contains("IMAGE")) {        return workerHarnessContainerImage;    } else if (hasExperiment(options, "beam_fn_api")) {        return workerHarnessContainerImage.replace("IMAGE", "java");    } else if (options.isStreaming()) {        return workerHarnessContainerImage.replace("IMAGE", String.format("beam-%s-streaming", javaVersionId));    } else {        return workerHarnessContainerImage.replace("IMAGE", String.format("beam-%s-batch", javaVersionId));    }}
 static void beam_f6548_0(DoFn<?, ?> fn)
{    DoFnSignature signature = DoFnSignatures.getSignature(fn.getClass());    for (DoFnSignature.StateDeclaration stateDecl : signature.stateDeclarations().values()) {                if (stateDecl.stateType().isSubtypeOf(TypeDescriptor.of(MapState.class))) {            throw new UnsupportedOperationException(String.format("%s does not currently support %s", DataflowRunner.class.getSimpleName(), MapState.class.getSimpleName()));        }                if (stateDecl.stateType().isSubtypeOf(TypeDescriptor.of(SetState.class))) {            throw new UnsupportedOperationException(String.format("%s does not currently support %s", DataflowRunner.class.getSimpleName(), SetState.class.getSimpleName()));        }    }}
private static void beam_f6557_0(String property, Properties properties)
{    String value = System.getProperty(property);    if (value != null) {        properties.setProperty(property, value);    }}
private static int beam_f6558_0(DataflowPipelineOptions options)
{        int cores = 4;    if (options.getMaxNumWorkers() > 0) {        return options.getMaxNumWorkers() * cores;    } else if (options.getNumWorkers() > 0) {        return options.getNumWorkers() * cores;    } else {        return 5 * cores;    }}
public static com.google.api.services.dataflow.model.Source beam_f6559_1(Source<?> source, PipelineOptions options) throws Exception
{    com.google.api.services.dataflow.model.Source cloudSource = new com.google.api.services.dataflow.model.Source();        cloudSource.setSpec(CloudObject.forClass(CustomSources.class));    addString(cloudSource.getSpec(), SERIALIZED_SOURCE, encodeBase64String(serializeToByteArray(source)));    SourceMetadata metadata = new SourceMetadata();    if (source instanceof BoundedSource) {        BoundedSource<?> boundedSource = (BoundedSource<?>) source;                try {            metadata.setEstimatedSizeBytes(boundedSource.getEstimatedSizeBytes(options));        } catch (Exception e) {                    }    } else if (source instanceof UnboundedSource) {        UnboundedSource<?, ?> unboundedSource = (UnboundedSource<?, ?>) source;        metadata.setInfinite(true);        List<String> encodedSplits = new ArrayList<>();        int desiredNumSplits = getDesiredNumUnboundedSourceSplits(options.as(DataflowPipelineOptions.class));        for (UnboundedSource<?, ?> split : unboundedSource.split(desiredNumSplits, options)) {            encodedSplits.add(encodeBase64String(serializeToByteArray(split)));        }        checkArgument(!encodedSplits.isEmpty(), "UnboundedSources must have at least one split");        addStringList(cloudSource.getSpec(), SERIALIZED_SOURCE_SPLITS, encodedSplits);    } else {        throw new IllegalArgumentException("Unexpected source kind: " + source.getClass());    }    cloudSource.setMetadata(metadata);    return cloudSource;}
public List<Coder<?>> beam_f6567_0()
{    return keyComponentCoders;}
public Coder<T> beam_f6568_0(int index)
{    return (Coder<T>) keyComponentCoders.get(index);}
public Coder<V> beam_f6569_0()
{    return valueCoder;}
public void beam_f6577_0() throws Coder.NonDeterministicException
{    verifyDeterministic(this, "Key component coders expected to be deterministic.", keyComponentCoders);    verifyDeterministic(this, "Value coder expected to be deterministic.", valueCoder);}
public boolean beam_f6578_0()
{    for (Coder<?> keyComponentCoder : keyComponentCoders) {        if (!keyComponentCoder.consistentWithEquals()) {            return false;        }    }    return valueCoder.consistentWithEquals();}
public Object beam_f6579_0(IsmRecord<V> record)
{    checkNotNull(record);    checkState(record.getKeyComponents().size() == keyComponentCoders.size(), "Expected the number of key component coders %s " + "to match the number of key components %s.", keyComponentCoders.size(), record.getKeyComponents());    if (consistentWithEquals()) {        ArrayList<Object> keyComponentStructuralValues = new ArrayList<>();        for (int i = 0; i < keyComponentCoders.size(); ++i) {            keyComponentStructuralValues.add(getKeyComponentCoder(i).structuralValue(record.getKeyComponent(i)));        }        if (isMetadataKey(record.getKeyComponents())) {            return IsmRecord.meta(keyComponentStructuralValues, record.getMetadata());        } else {            return IsmRecord.of(keyComponentStructuralValues, valueCoder.structuralValue(record.getValue()));        }    }    return super.structuralValue(record);}
public static Object beam_f6587_0()
{    return METADATA_KEY;}
public static MetadataKeyCoder<K> beam_f6588_0(Coder<K> keyCoder)
{    checkNotNull(keyCoder);    return new MetadataKeyCoder<>(keyCoder);}
public Coder<K> beam_f6589_0()
{    return keyCoder;}
public long beam_f6597_0()
{    return blockOffset();}
public long beam_f6598_0()
{    checkState(indexOffset() >= 0, "Unable to fetch index offset because it was never specified.");    return indexOffset();}
public IsmShard beam_f6599_0(long indexOffset)
{    return of(id(), blockOffset(), indexOffset);}
public void beam_f6607_0(KeyPrefix value, OutputStream outStream) throws CoderException, IOException
{    VarInt.encode(value.getSharedKeySize(), outStream);    VarInt.encode(value.getUnsharedKeySize(), outStream);}
public KeyPrefix beam_f6608_0(InputStream inStream) throws CoderException, IOException
{    return KeyPrefix.of(VarInt.decodeInt(inStream), VarInt.decodeInt(inStream));}
public boolean beam_f6610_0()
{    return true;}
public boolean beam_f6619_0(Footer value)
{    return true;}
public long beam_f6620_0(Footer value) throws Exception
{    return Footer.FIXED_LENGTH;}
public Dataflow beam_f6621_0(PipelineOptions options)
{    return DataflowTransport.newDataflowClient(options.as(DataflowPipelineOptions.class)).build();}
public static WorkerLogLevelOverrides beam_f6629_0(Map<String, String> values)
{    checkNotNull(values, "Expected values to be not null.");    WorkerLogLevelOverrides overrides = new WorkerLogLevelOverrides();    for (Map.Entry<String, String> entry : values.entrySet()) {        try {            overrides.addOverrideForName(entry.getKey(), Level.valueOf(entry.getValue()));        } catch (IllegalArgumentException e) {            throw new IllegalArgumentException(String.format("Unsupported log level '%s' requested for %s. Must be one of %s.", entry.getValue(), entry.getKey(), Arrays.toString(Level.values())));        }    }    return overrides;}
public PTransformReplacement<PCollection<? extends InputT>, PCollection<OutputT>> beam_f6630_0(AppliedPTransform<PCollection<? extends InputT>, PCollection<OutputT>, SingleOutput<InputT, OutputT>> transform)
{    return PTransformReplacement.of(PTransformReplacements.getSingletonMainInput(transform), new ParDoSingle<>(transform.getTransform(), Iterables.getOnlyElement(transform.getOutputs().keySet()), PTransformReplacements.getSingletonMainOutput(transform).getCoder()));}
public PCollection<OutputT> beam_f6631_0(PCollection<? extends InputT> input)
{    return PCollection.createPrimitiveOutputInternal(input.getPipeline(), input.getWindowingStrategy(), input.isBounded(), outputCoder, onlyOutputTag);}
public RunnerApi.FunctionSpec beam_f6639_0(AppliedPTransform<?, ?, ParDoSingle<?, ?>> transform, SdkComponents components) throws IOException
{    RunnerApi.ParDoPayload payload = payloadForParDoSingle(transform, components);    return RunnerApi.FunctionSpec.newBuilder().setUrn(PAR_DO_TRANSFORM_URN).setPayload(payload.toByteString()).build();}
private static RunnerApi.ParDoPayload beam_f6640_0(final AppliedPTransform<?, ?, ParDoSingle<?, ?>> transform, SdkComponents components) throws IOException
{    final ParDoSingle<?, ?> parDo = transform.getTransform();    final DoFn<?, ?> doFn = parDo.getFn();    final DoFnSignature signature = DoFnSignatures.getSignature(doFn.getClass());    if (!hasExperiment(transform.getPipeline().getOptions(), "beam_fn_api")) {        checkArgument(!signature.processElement().isSplittable(), String.format("Not expecting a splittable %s: should have been overridden", ParDoSingle.class.getSimpleName()));    }        Set<String> allInputs = transform.getInputs().keySet().stream().map(TupleTag::getId).collect(Collectors.toSet());    Set<String> sideInputs = parDo.getSideInputs().values().stream().map(s -> s.getTagInternal().getId()).collect(Collectors.toSet());    Set<String> timerInputs = signature.timerDeclarations().keySet();    String mainInputName = Iterables.getOnlyElement(Sets.difference(allInputs, Sets.union(sideInputs, timerInputs)));    PCollection<?> mainInput = (PCollection<?>) transform.getInputs().get(new TupleTag<>(mainInputName));    final DoFnSchemaInformation doFnSchemaInformation = ParDo.getDoFnSchemaInformation(doFn, mainInput);    return ParDoTranslation.payloadForParDoLike(new ParDoTranslation.ParDoLike() {        @Override        public RunnerApi.SdkFunctionSpec translateDoFn(SdkComponents newComponents) {            return ParDoTranslation.translateDoFn(parDo.getFn(), parDo.getMainOutputTag(), parDo.getSideInputs(), doFnSchemaInformation, newComponents);        }        @Override        public List<RunnerApi.Parameter> translateParameters() {            return ParDoTranslation.translateParameters(signature.processElement().extraParameters());        }        @Override        public Map<String, RunnerApi.SideInput> translateSideInputs(SdkComponents components) {            return ParDoTranslation.translateSideInputs(parDo.getSideInputs().values().stream().collect(Collectors.toList()), components);        }        @Override        public Map<String, RunnerApi.StateSpec> translateStateSpecs(SdkComponents components) throws IOException {            Map<String, RunnerApi.StateSpec> stateSpecs = new HashMap<>();            for (Map.Entry<String, DoFnSignature.StateDeclaration> state : signature.stateDeclarations().entrySet()) {                RunnerApi.StateSpec spec = ParDoTranslation.translateStateSpec(getStateSpecOrThrow(state.getValue(), doFn), components);                stateSpecs.put(state.getKey(), spec);            }            return stateSpecs;        }        @Override        public Map<String, RunnerApi.TimerSpec> translateTimerSpecs(SdkComponents newComponents) {            Map<String, RunnerApi.TimerSpec> timerSpecs = new HashMap<>();            for (Map.Entry<String, DoFnSignature.TimerDeclaration> timer : signature.timerDeclarations().entrySet()) {                RunnerApi.TimerSpec spec = translateTimerSpec(getTimerSpecOrThrow(timer.getValue(), doFn), newComponents);                timerSpecs.put(timer.getKey(), spec);            }            return timerSpecs;        }        @Override        public boolean isSplittable() {            return signature.processElement().isSplittable();        }        @Override        public String translateRestrictionCoderId(SdkComponents newComponents) {            if (signature.processElement().isSplittable()) {                Coder<?> restrictionCoder = DoFnInvokers.invokerFor(doFn).invokeGetRestrictionCoder(transform.getPipeline().getCoderRegistry());                try {                    return newComponents.registerCoder(restrictionCoder);                } catch (IOException e) {                    throw new IllegalStateException(String.format("Unable to register restriction coder for %s.", transform.getFullName()), e);                }            }            return "";        }    }, components);}
public RunnerApi.SdkFunctionSpec beam_f6641_0(SdkComponents newComponents)
{    return ParDoTranslation.translateDoFn(parDo.getFn(), parDo.getMainOutputTag(), parDo.getSideInputs(), doFnSchemaInformation, newComponents);}
public void beam_f6649_0(Read.Bounded<?> transform, TranslationContext context)
{    translateReadHelper(transform.getSource(), transform, context);}
public static void beam_f6650_0(Source<T> source, PTransform<?, ? extends PCollection<?>> transform, TranslationContext context)
{    try {        StepTranslationContext stepContext = context.addStep(transform, "ParallelRead");        stepContext.addInput(PropertyNames.FORMAT, PropertyNames.CUSTOM_SOURCE_FORMAT);        stepContext.addInput(PropertyNames.SOURCE_STEP_INPUT, cloudSourceToDictionary(CustomSources.serializeToCloudSource(source, context.getPipelineOptions())));        stepContext.addOutput(PropertyNames.OUTPUT, context.getOutput(transform));    } catch (Exception e) {        throw new RuntimeException(e);    }}
private static Map<String, Object> beam_f6651_0(com.google.api.services.dataflow.model.Source source)
{            Map<String, Object> res = new HashMap<>();    addDictionary(res, PropertyNames.SOURCE_SPEC, source.getSpec());    if (source.getMetadata() != null) {        addDictionary(res, PropertyNames.SOURCE_METADATA, cloudSourceMetadataToDictionary(source.getMetadata()));    }    if (source.getDoesNotNeedSplitting() != null) {        addBoolean(res, PropertyNames.SOURCE_DOES_NOT_NEED_SPLITTING, source.getDoesNotNeedSplitting());    }    return res;}
public PCollectionTuple beam_f6659_0(PCollection<InputT> input)
{    return input.apply("Materialize input", Reshuffle.viaRandomKey()).apply("ParDo with stable input", appliedTransform.getTransform());}
public Map<PValue, ReplacementOutput> beam_f6660_0(Map<TupleTag<?>, PValue> outputs, PCollectionTuple newOutput)
{    return ReplacementOutputs.tagged(outputs, newOutput);}
public PTransformReplacement<PCollection<KV<K, V>>, PCollection<KV<K, V>>> beam_f6661_0(AppliedPTransform<PCollection<KV<K, V>>, PCollection<KV<K, V>>, Reshuffle<K, V>> transform)
{    return PTransformReplacement.of(PTransformReplacements.getSingletonMainInput(transform), new ReshuffleWithOnlyTrigger<>());}
public PCollection<ElemT> beam_f6669_0(PCollection<ElemT> input)
{    return input.apply(Combine.globally(new Concatenate<ElemT>()).withoutDefaults()).apply(ParDo.of(StreamingPCollectionViewWriterFn.create(view, input.getCoder()))).apply(CreateDataflowView.forStreaming(view));}
public List<T> beam_f6670_0()
{    return new ArrayList<>();}
public List<T> beam_f6671_0(List<T> accumulator, T input)
{    accumulator.add(input);    return accumulator;}
 DataflowPipelineJob beam_f6679_1(Pipeline pipeline, DataflowRunner runner)
{    updatePAssertCount(pipeline);    TestPipelineOptions testPipelineOptions = options.as(TestPipelineOptions.class);    final DataflowPipelineJob job;    job = runner.run(pipeline);        assertThat(job, testPipelineOptions.getOnCreateMatcher());    Boolean jobSuccess;    Optional<Boolean> allAssertionsPassed;    ErrorMonitorMessagesHandler messageHandler = new ErrorMonitorMessagesHandler(job, new MonitoringUtil.LoggingHandler());    if (options.isStreaming()) {        jobSuccess = waitForStreamingJobTermination(job, messageHandler);                allAssertionsPassed = Optional.absent();    } else {        jobSuccess = waitForBatchJobTermination(job, messageHandler);        allAssertionsPassed = checkForPAssertSuccess(job);    }        if (!allAssertionsPassed.isPresent()) {            } else if (!allAssertionsPassed.get()) {        throw new AssertionError(errorMessage(job, messageHandler));    }        if (!jobSuccess) {        throw new RuntimeException(errorMessage(job, messageHandler));    }        assertThat(job, testPipelineOptions.getOnSuccessMatcher());    return job;}
private boolean beam_f6680_1(final DataflowPipelineJob job, ErrorMonitorMessagesHandler messageHandler)
{                options.getExecutorService().submit(new CancelOnError(job, messageHandler));        State finalState;    try {        finalState = job.waitUntilFinish(Duration.standardSeconds(options.getTestTimeoutSeconds()), messageHandler);    } catch (IOException e) {        throw new RuntimeException(e);    } catch (InterruptedException e) {        Thread.interrupted();        return false;    }        if (finalState == null || !finalState.isTerminal()) {                try {            job.cancel();        } catch (IOException e) {            throw new RuntimeException(e);        }        return false;    } else {        return finalState == State.DONE && !messageHandler.hasSeenError();    }}
private boolean beam_f6681_0(DataflowPipelineJob job, ErrorMonitorMessagesHandler messageHandler)
{    {        try {            job.waitUntilFinish(Duration.standardSeconds(-1), messageHandler);        } catch (IOException e) {            throw new RuntimeException(e);        } catch (InterruptedException e) {            Thread.interrupted();            return false;        }        return job.getState() == State.DONE;    }}
 String beam_f6689_0()
{    return errorMessage.toString();}
public Void beam_f6690_1() throws Exception
{    while (true) {        State jobState = job.getState();                if (messageHandler.hasSeenError() && !job.getState().isTerminal()) {            job.cancel();                        return null;        }        if (jobState.isTerminal()) {            return null;        }        Thread.sleep(3000L);    }}
 boolean beam_f6691_0()
{    List<String> experiments = getPipelineOptions().getExperiments();    return experiments != null && experiments.contains("beam_fn_api");}
public static CloudKnownType beam_f6699_0(@Nullable String uri)
{    if (uri == null) {        return null;    }    return typesByUri.get(uri);}
private static Map<Class<?>, CloudKnownType> beam_f6700_0()
{    Map<Class<?>, CloudKnownType> result = new HashMap<>();    for (CloudKnownType ty : CloudKnownType.values()) {        for (Class<?> clazz : ty.classes) {            result.put(clazz, ty);        }    }    return result;}
public static CloudKnownType beam_f6701_0(Class<?> clazz)
{    return typesByClass.get(clazz);}
public static CloudObject beam_f6709_0(String value)
{    CloudObject result = forClassName(CloudKnownType.TEXT.getUri());    result.put(PropertyNames.SCALAR_FIELD_NAME, value);    return result;}
public static CloudObject beam_f6710_0(Boolean value)
{    CloudObject result = forClassName(CloudKnownType.BOOLEAN.getUri());    result.put(PropertyNames.SCALAR_FIELD_NAME, value);    return result;}
public static CloudObject beam_f6711_0(Long value)
{    CloudObject result = forClassName(CloudKnownType.INTEGER.getUri());    result.put(PropertyNames.SCALAR_FIELD_NAME, value);    return result;}
private static Map<String, CloudObjectTranslator<? extends Coder>> beam_f6719_0()
{    ImmutableMap.Builder<String, CloudObjectTranslator<? extends Coder>> builder = ImmutableMap.builder();    for (CoderCloudObjectTranslatorRegistrar coderRegistrar : ServiceLoader.load(CoderCloudObjectTranslatorRegistrar.class)) {        builder.putAll(coderRegistrar.classNamesToTranslators());    }    return builder.build();}
public static CloudObject beam_f6720_0(Coder<?> coder, @Nullable SdkComponents sdkComponents)
{    CloudObjectTranslator<Coder> translator = (CloudObjectTranslator<Coder>) CODER_TRANSLATORS.get(coder.getClass());    CloudObject encoding;    if (translator != null) {        encoding = translator.toCloudObject(coder, sdkComponents);    } else {        CloudObjectTranslator customCoderTranslator = CODER_TRANSLATORS.get(CustomCoder.class);        checkNotNull(customCoderTranslator, "No %s registered for %s, but it is in the %s", CloudObjectTranslator.class.getSimpleName(), CustomCoder.class.getSimpleName(), DefaultCoderCloudObjectTranslatorRegistrar.class.getSimpleName());        encoding = customCoderTranslator.toCloudObject(coder, sdkComponents);    }    if (sdkComponents != null && !DATAFLOW_KNOWN_CODERS.contains(coder.getClass())) {        try {            String coderId = sdkComponents.registerCoder(coder);            Structs.addString(encoding, PropertyNames.PIPELINE_PROTO_CODER_ID, coderId);        } catch (Exception e) {            throw new RuntimeException("Unable to register coder " + coder, e);        }    }    return encoding;}
public static Coder<?> beam_f6721_0(CloudObject cloudObject)
{    CloudObjectTranslator<? extends Coder> translator = CLOUD_OBJECT_CLASS_NAME_TRANSLATORS.get(cloudObject.getClassName());    checkArgument(translator != null, "Unknown %s class %s", Coder.class.getSimpleName(), cloudObject.getClassName());    return translator.fromCloudObject(cloudObject);}
public static CloudObjectTranslator<IterableCoder> beam_f6729_0()
{    return new CloudObjectTranslator<IterableCoder>() {        @Override        public CloudObject toCloudObject(IterableCoder target, SdkComponents sdkComponents) {            CloudObject result = CloudObject.forClassName(CloudObjectKinds.KIND_STREAM);            Structs.addBoolean(result, PropertyNames.IS_STREAM_LIKE, true);            return addComponents(result, Collections.<Coder<?>>singletonList(target.getElemCoder()), sdkComponents);        }        @Override        public IterableCoder fromCloudObject(CloudObject object) {            List<Coder<?>> components = getComponents(object);            checkArgument(components.size() == 1, "Expecting 1 component, got %s", components.size());            return IterableCoder.of(components.get(0));        }        @Override        public Class<? extends IterableCoder> getSupportedClass() {            return IterableCoder.class;        }        @Override        public String cloudObjectClassName() {            return CloudObjectKinds.KIND_STREAM;        }    };}
public CloudObject beam_f6730_0(IterableCoder target, SdkComponents sdkComponents)
{    CloudObject result = CloudObject.forClassName(CloudObjectKinds.KIND_STREAM);    Structs.addBoolean(result, PropertyNames.IS_STREAM_LIKE, true);    return addComponents(result, Collections.<Coder<?>>singletonList(target.getElemCoder()), sdkComponents);}
public IterableCoder beam_f6731_0(CloudObject object)
{    List<Coder<?>> components = getComponents(object);    checkArgument(components.size() == 1, "Expecting 1 component, got %s", components.size());    return IterableCoder.of(components.get(0));}
 static CloudObjectTranslator<GlobalWindow.Coder> beam_f6739_0()
{    return new CloudObjectTranslator<GlobalWindow.Coder>() {        @Override        public CloudObject toCloudObject(GlobalWindow.Coder target, SdkComponents sdkComponents) {            return addComponents(CloudObject.forClassName(CloudObjectKinds.KIND_GLOBAL_WINDOW), Collections.emptyList(), sdkComponents);        }        @Override        public GlobalWindow.Coder fromCloudObject(CloudObject object) {            return GlobalWindow.Coder.INSTANCE;        }        @Override        public Class<? extends GlobalWindow.Coder> getSupportedClass() {            return GlobalWindow.Coder.class;        }        @Override        public String cloudObjectClassName() {            return CloudObjectKinds.KIND_GLOBAL_WINDOW;        }    };}
public CloudObject beam_f6740_0(GlobalWindow.Coder target, SdkComponents sdkComponents)
{    return addComponents(CloudObject.forClassName(CloudObjectKinds.KIND_GLOBAL_WINDOW), Collections.emptyList(), sdkComponents);}
public GlobalWindow.Coder beam_f6741_0(CloudObject object)
{    return GlobalWindow.Coder.INSTANCE;}
 static CloudObjectTranslator<FullWindowedValueCoder> beam_f6749_0()
{    return new CloudObjectTranslator<FullWindowedValueCoder>() {        @Override        public CloudObject toCloudObject(FullWindowedValueCoder target, SdkComponents sdkComponents) {            CloudObject result = CloudObject.forClassName(CloudObjectKinds.KIND_WINDOWED_VALUE);            Structs.addBoolean(result, PropertyNames.IS_WRAPPER, true);            return addComponents(result, ImmutableList.<Coder<?>>of(target.getValueCoder(), target.getWindowCoder()), sdkComponents);        }        @Override        public FullWindowedValueCoder fromCloudObject(CloudObject object) {            List<Coder<?>> components = getComponents(object);            checkArgument(components.size() == 2, "Expecting 2 components, got " + components.size());            @SuppressWarnings("unchecked")            Coder<? extends BoundedWindow> window = (Coder<? extends BoundedWindow>) components.get(1);            return FullWindowedValueCoder.of(components.get(0), window);        }        @Override        public Class<? extends FullWindowedValueCoder> getSupportedClass() {            return FullWindowedValueCoder.class;        }        @Override        public String cloudObjectClassName() {            return CloudObjectKinds.KIND_WINDOWED_VALUE;        }    };}
public CloudObject beam_f6750_0(FullWindowedValueCoder target, SdkComponents sdkComponents)
{    CloudObject result = CloudObject.forClassName(CloudObjectKinds.KIND_WINDOWED_VALUE);    Structs.addBoolean(result, PropertyNames.IS_WRAPPER, true);    return addComponents(result, ImmutableList.<Coder<?>>of(target.getValueCoder(), target.getWindowCoder()), sdkComponents);}
public FullWindowedValueCoder beam_f6751_0(CloudObject object)
{    List<Coder<?>> components = getComponents(object);    checkArgument(components.size() == 2, "Expecting 2 components, got " + components.size());    @SuppressWarnings("unchecked")    Coder<? extends BoundedWindow> window = (Coder<? extends BoundedWindow>) components.get(1);    return FullWindowedValueCoder.of(components.get(0), window);}
 static CloudObjectTranslator<VarLongCoder> beam_f6759_0()
{    return new CloudObjectTranslator<VarLongCoder>() {        @Override        public CloudObject toCloudObject(VarLongCoder target, SdkComponents sdkComponents) {            return addComponents(CloudObject.forClass(target.getClass()), Collections.emptyList(), sdkComponents);        }        @Override        public VarLongCoder fromCloudObject(CloudObject object) {            return VarLongCoder.of();        }        @Override        public Class<? extends VarLongCoder> getSupportedClass() {            return VarLongCoder.class;        }        @Override        public String cloudObjectClassName() {            return CloudObject.forClass(VarLongCoder.class).getClassName();        }    };}
public CloudObject beam_f6760_0(VarLongCoder target, SdkComponents sdkComponents)
{    return addComponents(CloudObject.forClass(target.getClass()), Collections.emptyList(), sdkComponents);}
public VarLongCoder beam_f6761_0(CloudObject object)
{    return VarLongCoder.of();}
public static CloudObjectTranslator<T> beam_f6769_0(final Class<T> coderClass)
{        InstanceBuilder.ofType(coderClass).fromFactoryMethod("of").build();    return new CloudObjectTranslator<T>() {        @Override        public CloudObject toCloudObject(T target, SdkComponents sdkComponents) {            return CloudObject.forClass(coderClass);        }        @Override        public T fromCloudObject(CloudObject cloudObject) {            return InstanceBuilder.ofType(coderClass).fromFactoryMethod("of").build();        }        @Override        public Class<? extends T> getSupportedClass() {            return coderClass;        }        @Override        public String cloudObjectClassName() {            return CloudObject.forClass(coderClass).getClassName();        }    };}
public CloudObject beam_f6770_0(T target, SdkComponents sdkComponents)
{    return CloudObject.forClass(coderClass);}
public T beam_f6771_0(CloudObject cloudObject)
{    return InstanceBuilder.ofType(coderClass).fromFactoryMethod("of").build();}
public static CloudObjectTranslator<MapCoder> beam_f6779_0()
{    return new CloudObjectTranslator<MapCoder>() {        @Override        public CloudObject toCloudObject(MapCoder target, SdkComponents sdkComponents) {            CloudObject base = CloudObject.forClass(MapCoder.class);            return addComponents(base, ImmutableList.<Coder<?>>of(target.getKeyCoder(), target.getValueCoder()), sdkComponents);        }        @Override        public MapCoder<?, ?> fromCloudObject(CloudObject cloudObject) {            List<Coder<?>> components = getComponents(cloudObject);            checkArgument(components.size() == 2, "Expected 2 components for %s, got %s", MapCoder.class.getSimpleName(), components.size());            return MapCoder.of(components.get(0), components.get(1));        }        @Override        public Class<? extends MapCoder> getSupportedClass() {            return MapCoder.class;        }        @Override        public String cloudObjectClassName() {            return CloudObject.forClass(MapCoder.class).getClassName();        }    };}
public CloudObject beam_f6780_0(MapCoder target, SdkComponents sdkComponents)
{    CloudObject base = CloudObject.forClass(MapCoder.class);    return addComponents(base, ImmutableList.<Coder<?>>of(target.getKeyCoder(), target.getValueCoder()), sdkComponents);}
public MapCoder<?, ?> beam_f6781_0(CloudObject cloudObject)
{    List<Coder<?>> components = getComponents(cloudObject);    checkArgument(components.size() == 2, "Expected 2 components for %s, got %s", MapCoder.class.getSimpleName(), components.size());    return MapCoder.of(components.get(0), components.get(1));}
public static CloudObjectTranslator<UnionCoder> beam_f6789_0()
{    return new CloudObjectTranslator<UnionCoder>() {        @Override        public CloudObject toCloudObject(UnionCoder target, SdkComponents sdkComponents) {            return addComponents(CloudObject.forClass(UnionCoder.class), target.getElementCoders(), sdkComponents);        }        @Override        public UnionCoder fromCloudObject(CloudObject cloudObject) {            List<Coder<?>> elementCoders = getComponents(cloudObject);            return UnionCoder.of(elementCoders);        }        @Override        public Class<? extends UnionCoder> getSupportedClass() {            return UnionCoder.class;        }        @Override        public String cloudObjectClassName() {            return CloudObject.forClass(UnionCoder.class).getClassName();        }    };}
public CloudObject beam_f6790_0(UnionCoder target, SdkComponents sdkComponents)
{    return addComponents(CloudObject.forClass(UnionCoder.class), target.getElementCoders(), sdkComponents);}
public UnionCoder beam_f6791_0(CloudObject cloudObject)
{    List<Coder<?>> elementCoders = getComponents(cloudObject);    return UnionCoder.of(elementCoders);}
private CoGbkResultSchema beam_f6799_0(CloudObject cloudObject)
{    List<TupleTag<?>> tags = new ArrayList<>();    List<Map<String, Object>> serializedTags = Structs.getListOfMaps(cloudObject, PropertyNames.TUPLE_TAGS, Collections.emptyList());    for (Map<String, Object> serializedTag : serializedTags) {        TupleTag<?> tag = new TupleTag<>(Structs.getString(serializedTag, PropertyNames.VALUE));        tags.add(tag);    }    return CoGbkResultSchema.of(tags);}
public String beam_f6800_0()
{    return CloudObject.forClass(CoGbkResultCoder.class).getClassName();}
public String beam_f6801_0()
{    throw new UnsupportedOperationException(ERROR);}
public static CloudDebugger.Builder beam_f6809_0(DataflowPipelineOptions options)
{    return new CloudDebugger.Builder(getTransport(), getJsonFactory(), chainHttpRequestInitializer(options.getGcpCredential(), new RetryHttpRequestInitializer())).setApplicationName(options.getAppName()).setGoogleClientRequestInitializer(options.getGoogleApiTrace());}
private static HttpRequestInitializer beam_f6810_0(Credentials credential, HttpRequestInitializer httpRequestInitializer)
{    if (credential == null) {        NullCredentialInitializer.throwNullCredentialException();    }    return new ChainingHttpRequestInitializer(new HttpCredentialsAdapter(credential), httpRequestInitializer);}
public Map<String, CloudObjectTranslator<? extends Coder>> beam_f6811_0()
{    ImmutableMap.Builder<String, CloudObjectTranslator<? extends Coder>> nameToTranslators = ImmutableMap.builder();    for (CloudObjectTranslator<? extends Coder> translator : classesToTranslators().values()) {        nameToTranslators.put(translator.cloudObjectClassName(), translator);    }    return nameToTranslators.build();}
public int beam_f6819_0(JobMessage o1, JobMessage o2)
{    @Nullable    Instant t1 = fromCloudTime(o1.getTime());    if (t1 == null) {        return -1;    }    @Nullable    Instant t2 = fromCloudTime(o2.getTime());    if (t2 == null) {        return 1;    }    return t1.compareTo(t2);}
public List<JobMessage> beam_f6820_0(String jobId, long startTimestampMs) throws IOException
{        Instant startTimestamp = new Instant(startTimestampMs);    ArrayList<JobMessage> allMessages = new ArrayList<>();    String pageToken = null;    while (true) {        ListJobMessagesResponse response = dataflowClient.listJobMessages(jobId, pageToken);        if (response == null || response.getJobMessages() == null) {            return allMessages;        }        for (JobMessage m : response.getJobMessages()) {            @Nullable            Instant timestamp = fromCloudTime(m.getTime());            if (timestamp == null) {                continue;            }            if (timestamp.isAfter(startTimestamp)) {                allMessages.add(m);            }        }        if (response.getNextPageToken() == null) {            break;        } else {            pageToken = response.getNextPageToken();        }    }    allMessages.sort(new TimeStampComparator());    return allMessages;}
public static String beam_f6821_0(String projectName, String jobId)
{    return getJobMonitoringPageURL(projectName, "us-central1", jobId);}
private CompletionStage<PackageAttributes> beam_f6829_0(final DataflowPackage source, final String stagingPath)
{    return MoreFutures.supplyAsync(() -> {        final File file = new File(source.getLocation());        if (!file.exists()) {            throw new FileNotFoundException(String.format("Non-existent file to stage: %s", file.getAbsolutePath()));        }        PackageAttributes attributes = PackageAttributes.forFileToStage(file, stagingPath);        if (source.getName() != null) {            attributes = attributes.withPackageName(source.getName());        }        return attributes;    }, executorService);}
private boolean beam_f6830_0(PackageAttributes attributes) throws IOException
{    try {        long remoteLength = FileSystems.matchSingleFileSpec(attributes.getDestination().getLocation()).sizeBytes();        return remoteLength == attributes.getSize();    } catch (FileNotFoundException expected) {                return false;    }}
public CompletionStage<StagingResult> beam_f6831_0(final PackageAttributes attributes, final Sleeper retrySleeper, final CreateOptions createOptions)
{    return MoreFutures.supplyAsync(() -> stagePackageSynchronously(attributes, retrySleeper, createOptions), executorService);}
 static String beam_f6839_0(File classpathElement, String contentHash)
{    String fileName = Files.getNameWithoutExtension(classpathElement.getAbsolutePath());    String fileExtension = Files.getFileExtension(classpathElement.getAbsolutePath());    if (classpathElement.isDirectory()) {        return fileName + "-" + contentHash + ".jar";    } else if (fileExtension.isEmpty()) {        return fileName + "-" + contentHash;    }    return fileName + "-" + contentHash + "." + fileExtension;}
public static StagingResult beam_f6840_0(PackageAttributes attributes)
{    return new AutoValue_PackageUtil_StagingResult(attributes, true);}
public static StagingResult beam_f6841_0(PackageAttributes attributes)
{    return new AutoValue_PackageUtil_StagingResult(attributes, false);}
public RandomAccessData beam_f6849_0(InputStream inStream) throws CoderException, IOException
{    return decode(inStream, Coder.Context.NESTED);}
public RandomAccessData beam_f6850_0(InputStream inStream, Coder.Context context) throws CoderException, IOException
{    RandomAccessData rval = new RandomAccessData();    if (!context.isWholeStream) {        int length = VarInt.decodeInt(inStream);        rval.readFrom(inStream, 0, length);    } else {        ByteStreams.copy(inStream, rval.asOutputStream());    }    return rval;}
public boolean beam_f6852_0()
{    return true;}
public int beam_f6860_0()
{    return size;}
public void beam_f6861_0(int position)
{    ensureCapacity(position);    size = position;}
public void beam_f6862_0(int b) throws IOException
{    ensureCapacity(size + 1);    buffer[size] = (byte) b;    size += 1;}
public int beam_f6870_0()
{    int result = 1;    for (int i = 0; i < size; ++i) {        result = 31 * result + buffer[i];    }    return result;}
public String beam_f6871_0()
{    return MoreObjects.toStringHelper(this).add("buffer", Arrays.copyOf(buffer, size)).add("size", size).toString();}
private void beam_f6872_0(int minCapacity)
{        if (minCapacity <= buffer.length) {        return;    }            int newCapacity = (int) Math.min(Integer.MAX_VALUE - 8, buffer.length * 2L);    if (newCapacity < minCapacity) {        newCapacity = minCapacity;    }    buffer = Arrays.copyOf(buffer, newCapacity);}
public String beam_f6880_0()
{    return CloudObject.forClass(SerializableCoder.class).getClassName();}
public static String beam_f6881_0(Map<String, Object> map, String name)
{    return getValue(map, name, String.class, "a string");}
public static String beam_f6882_0(Map<String, Object> map, String name, @Nullable String defaultValue)
{    return getValue(map, name, String.class, "a string", defaultValue);}
public static Integer beam_f6890_0(Map<String, Object> map, String name, @Nullable Integer defaultValue)
{    return getValue(map, name, Integer.class, "an int", defaultValue);}
public static List<String> beam_f6891_0(Map<String, Object> map, String name, @Nullable List<String> defaultValue)
{    @Nullable    Object value = map.get(name);    if (value == null) {        if (map.containsKey(name)) {            throw new IncorrectTypeException(name, map, "a string or a list");        }        return defaultValue;    }    if (Data.isNull(value)) {                return Collections.emptyList();    }    @Nullable    String singletonString = decodeValue(value, String.class);    if (singletonString != null) {        return Collections.singletonList(singletonString);    }    if (!(value instanceof List)) {        throw new IncorrectTypeException(name, map, "a string or a list");    }    @SuppressWarnings("unchecked")    List<Object> elements = (List<Object>) value;    List<String> result = new ArrayList<>(elements.size());    for (Object o : elements) {        @Nullable        String s = decodeValue(o, String.class);        if (s == null) {            throw new IncorrectTypeException(name, map, "a list of strings");        }        result.add(s);    }    return result;}
public static Map<String, Object> beam_f6892_0(Map<String, Object> map, String name)
{    @Nullable    Map<String, Object> result = getObject(map, name, null);    if (result == null) {        throw new ParameterNotFoundException(name, map);    }    return result;}
public static void beam_f6900_0(Map<String, Object> map, String name, long value)
{    addObject(map, name, CloudObject.forInteger(value));}
public static void beam_f6901_0(Map<String, Object> map, String name, Map<String, Object> value)
{    map.put(name, value);}
public static void beam_f6902_0(Map<String, Object> map, String name)
{    map.put(name, Data.nullOf(Object.class));}
private static T beam_f6910_0(Map<String, Object> map, String name, Class<T> clazz, String type, @Nullable T defaultValue)
{    @Nullable    Object value = map.get(name);    if (value == null) {        if (map.containsKey(name)) {            throw new IncorrectTypeException(name, map, type);        }        return defaultValue;    }    T result = decodeValue(value, clazz);    if (result == null) {                throw new IncorrectTypeException(name, map, type);    }    return result;}
private static T beam_f6911_0(Object value, Class<T> clazz)
{    try {        if (value.getClass() == clazz) {                        return clazz.cast(value);        }        if (!(value instanceof Map)) {            return null;        }        @SuppressWarnings("unchecked")        Map<String, Object> map = (Map<String, Object>) value;        @Nullable        String typeName = (String) map.get(PropertyNames.OBJECT_TYPE_NAME);        if (typeName == null) {            return null;        }        @Nullable        CloudKnownType knownType = CloudKnownType.forUri(typeName);        if (knownType == null) {            return null;        }        @Nullable        Object scalar = map.get(PropertyNames.SCALAR_FIELD_NAME);        if (scalar == null) {            return null;        }        return knownType.parse(scalar, clazz);    } catch (ClassCastException e) {                return null;    }}
public static String beam_f6912_0(ReadableInstant instant)
{                    DateTime time = new DateTime(instant);    int millis = time.getMillisOfSecond();    if (millis == 0) {        return String.format("%04d-%02d-%02dT%02d:%02d:%02dZ", time.getYear(), time.getMonthOfYear(), time.getDayOfMonth(), time.getHourOfDay(), time.getMinuteOfHour(), time.getSecondOfMinute());    } else {        return String.format("%04d-%02d-%02dT%02d:%02d:%02d.%03dZ", time.getYear(), time.getMonthOfYear(), time.getDayOfMonth(), time.getHourOfDay(), time.getMinuteOfHour(), time.getSecondOfMinute(), millis);    }}
public void beam_f6920_0() throws Exception
{    DataflowPipelineOptions options = buildPipelineOptions("--experiments=beam_fn_api");    options.setRunner(DataflowRunner.class);    Pipeline pipeline = Pipeline.create(options);    TupleTag<Integer> mainOutputTag = new TupleTag<Integer>() {    };    TupleTag<Integer> sideOutputTag = new TupleTag<Integer>() {    };    DummyStatefulDoFn fn = new DummyStatefulDoFn();    pipeline.apply(Create.of(KV.of(1, 2))).apply(ParDo.of(fn).withOutputTags(mainOutputTag, TupleTagList.of(sideOutputTag)));    DataflowRunner runner = DataflowRunner.fromOptions(options);    runner.replaceTransforms(pipeline);    assertThat(findBatchStatefulDoFn(pipeline), equalTo((DoFn) fn));}
private static DummyStatefulDoFn beam_f6921_0(Pipeline p)
{    FindBatchStatefulDoFnVisitor findBatchStatefulDoFnVisitor = new FindBatchStatefulDoFnVisitor();    p.traverseTopologically(findBatchStatefulDoFnVisitor);    return (DummyStatefulDoFn) findBatchStatefulDoFnVisitor.getStatefulDoFn();}
public boolean beam_f6923_0(Object other)
{    return other instanceof DummyStatefulDoFn;}
public void beam_f6931_0() throws Exception
{    DoFnTester<KV<Integer, Iterable<KV<IntervalWindow, WindowedValue<Long>>>>, IsmRecord<WindowedValue<Long>>> doFnTester = DoFnTester.of(new BatchViewOverrides.BatchViewAsList.ToIsmRecordForNonGlobalWindowDoFn<Long, IntervalWindow>(IntervalWindow.getCoder()));    IntervalWindow windowA = new IntervalWindow(new Instant(0), new Instant(10));    IntervalWindow windowB = new IntervalWindow(new Instant(10), new Instant(20));    IntervalWindow windowC = new IntervalWindow(new Instant(20), new Instant(30));    Iterable<KV<Integer, Iterable<KV<IntervalWindow, WindowedValue<Long>>>>> inputElements = ImmutableList.of(KV.of(1, (Iterable<KV<IntervalWindow, WindowedValue<Long>>>) ImmutableList.of(KV.of(windowA, WindowedValue.of(110L, new Instant(1), windowA, PaneInfo.NO_FIRING)), KV.of(windowA, WindowedValue.of(111L, new Instant(3), windowA, PaneInfo.NO_FIRING)), KV.of(windowA, WindowedValue.of(112L, new Instant(4), windowA, PaneInfo.NO_FIRING)), KV.of(windowB, WindowedValue.of(120L, new Instant(12), windowB, PaneInfo.NO_FIRING)), KV.of(windowB, WindowedValue.of(121L, new Instant(14), windowB, PaneInfo.NO_FIRING)))), KV.of(2, (Iterable<KV<IntervalWindow, WindowedValue<Long>>>) ImmutableList.of(KV.of(windowC, WindowedValue.of(210L, new Instant(25), windowC, PaneInfo.NO_FIRING)))));        assertThat(doFnTester.processBundle(inputElements), contains(IsmRecord.of(ImmutableList.of(windowA, 0L), WindowedValue.of(110L, new Instant(1), windowA, PaneInfo.NO_FIRING)), IsmRecord.of(ImmutableList.of(windowA, 1L), WindowedValue.of(111L, new Instant(3), windowA, PaneInfo.NO_FIRING)), IsmRecord.of(ImmutableList.of(windowA, 2L), WindowedValue.of(112L, new Instant(4), windowA, PaneInfo.NO_FIRING)), IsmRecord.of(ImmutableList.of(windowB, 0L), WindowedValue.of(120L, new Instant(12), windowB, PaneInfo.NO_FIRING)), IsmRecord.of(ImmutableList.of(windowB, 1L), WindowedValue.of(121L, new Instant(14), windowB, PaneInfo.NO_FIRING)), IsmRecord.of(ImmutableList.of(windowC, 0L), WindowedValue.of(210L, new Instant(25), windowC, PaneInfo.NO_FIRING))));}
public void beam_f6932_0() throws Exception
{    TupleTag<KV<Integer, KV<IntervalWindow, Long>>> outputForSizeTag = new TupleTag<>();    TupleTag<KV<Integer, KV<IntervalWindow, Long>>> outputForEntrySetTag = new TupleTag<>();    Coder<Long> keyCoder = VarLongCoder.of();    Coder<IntervalWindow> windowCoder = IntervalWindow.getCoder();    IsmRecordCoder<WindowedValue<Long>> ismCoder = IsmRecordCoder.of(1, 2, ImmutableList.of(MetadataKeyCoder.of(keyCoder), IntervalWindow.getCoder(), BigEndianLongCoder.of()), FullWindowedValueCoder.of(VarLongCoder.of(), windowCoder));    DoFnTester<KV<Integer, Iterable<KV<KV<Long, IntervalWindow>, WindowedValue<Long>>>>, IsmRecord<WindowedValue<Long>>> doFnTester = DoFnTester.of(new BatchViewOverrides.BatchViewAsMultimap.ToIsmRecordForMapLikeDoFn<>(outputForSizeTag, outputForEntrySetTag, windowCoder, keyCoder, ismCoder, false));    IntervalWindow windowA = new IntervalWindow(new Instant(0), new Instant(10));    IntervalWindow windowB = new IntervalWindow(new Instant(10), new Instant(20));    IntervalWindow windowC = new IntervalWindow(new Instant(20), new Instant(30));    Iterable<KV<Integer, Iterable<KV<KV<Long, IntervalWindow>, WindowedValue<Long>>>>> inputElements = ImmutableList.of(KV.of(1, (Iterable<KV<KV<Long, IntervalWindow>, WindowedValue<Long>>>) ImmutableList.of(KV.of(KV.of(1L, windowA), WindowedValue.of(110L, new Instant(1), windowA, PaneInfo.NO_FIRING)),     KV.of(KV.of(1L, windowA), WindowedValue.of(111L, new Instant(2), windowA, PaneInfo.NO_FIRING)),     KV.of(KV.of(2L, windowA), WindowedValue.of(120L, new Instant(3), windowA, PaneInfo.NO_FIRING)),     KV.of(KV.of(2L, windowB), WindowedValue.of(210L, new Instant(11), windowB, PaneInfo.NO_FIRING)),     KV.of(KV.of(3L, windowB), WindowedValue.of(220L, new Instant(12), windowB, PaneInfo.NO_FIRING)))), KV.of(2, (Iterable<KV<KV<Long, IntervalWindow>, WindowedValue<Long>>>) ImmutableList.of(    KV.of(KV.of(4L, windowC), WindowedValue.of(330L, new Instant(21), windowC, PaneInfo.NO_FIRING)))));        assertThat(doFnTester.processBundle(inputElements), contains(IsmRecord.of(ImmutableList.of(1L, windowA, 0L), WindowedValue.of(110L, new Instant(1), windowA, PaneInfo.NO_FIRING)), IsmRecord.of(ImmutableList.of(1L, windowA, 1L), WindowedValue.of(111L, new Instant(2), windowA, PaneInfo.NO_FIRING)), IsmRecord.of(ImmutableList.of(2L, windowA, 0L), WindowedValue.of(120L, new Instant(3), windowA, PaneInfo.NO_FIRING)), IsmRecord.of(ImmutableList.of(2L, windowB, 0L), WindowedValue.of(210L, new Instant(11), windowB, PaneInfo.NO_FIRING)), IsmRecord.of(ImmutableList.of(3L, windowB, 0L), WindowedValue.of(220L, new Instant(12), windowB, PaneInfo.NO_FIRING)), IsmRecord.of(ImmutableList.of(4L, windowC, 0L), WindowedValue.of(330L, new Instant(21), windowC, PaneInfo.NO_FIRING))));        assertThat(doFnTester.takeOutputElements(outputForSizeTag), contains(KV.of(ismCoder.hash(ImmutableList.of(IsmFormat.getMetadataKey(), windowA)), KV.of(windowA, 2L)), KV.of(ismCoder.hash(ImmutableList.of(IsmFormat.getMetadataKey(), windowB)), KV.of(windowB, 2L)), KV.of(ismCoder.hash(ImmutableList.of(IsmFormat.getMetadataKey(), windowC)), KV.of(windowC, 1L))));        assertThat(doFnTester.takeOutputElements(outputForEntrySetTag), contains(KV.of(ismCoder.hash(ImmutableList.of(IsmFormat.getMetadataKey(), windowA)), KV.of(windowA, 1L)), KV.of(ismCoder.hash(ImmutableList.of(IsmFormat.getMetadataKey(), windowA)), KV.of(windowA, 2L)), KV.of(ismCoder.hash(ImmutableList.of(IsmFormat.getMetadataKey(), windowB)), KV.of(windowB, 2L)), KV.of(ismCoder.hash(ImmutableList.of(IsmFormat.getMetadataKey(), windowB)), KV.of(windowB, 3L)), KV.of(ismCoder.hash(ImmutableList.of(IsmFormat.getMetadataKey(), windowC)), KV.of(windowC, 4L))));}
public void beam_f6933_0() throws Exception
{    TupleTag<KV<Integer, KV<IntervalWindow, Long>>> outputForSizeTag = new TupleTag<>();    TupleTag<KV<Integer, KV<IntervalWindow, Long>>> outputForEntrySetTag = new TupleTag<>();    Coder<Long> keyCoder = VarLongCoder.of();    Coder<IntervalWindow> windowCoder = IntervalWindow.getCoder();    IsmRecordCoder<WindowedValue<Long>> ismCoder = IsmRecordCoder.of(1, 2, ImmutableList.of(MetadataKeyCoder.of(keyCoder), IntervalWindow.getCoder(), BigEndianLongCoder.of()), FullWindowedValueCoder.of(VarLongCoder.of(), windowCoder));    DoFnTester<KV<Integer, Iterable<KV<KV<Long, IntervalWindow>, WindowedValue<Long>>>>, IsmRecord<WindowedValue<Long>>> doFnTester = DoFnTester.of(new BatchViewOverrides.BatchViewAsMultimap.ToIsmRecordForMapLikeDoFn<>(outputForSizeTag, outputForEntrySetTag, windowCoder, keyCoder, ismCoder, true));    IntervalWindow windowA = new IntervalWindow(new Instant(0), new Instant(10));    Iterable<KV<Integer, Iterable<KV<KV<Long, IntervalWindow>, WindowedValue<Long>>>>> inputElements = ImmutableList.of(KV.of(1, (Iterable<KV<KV<Long, IntervalWindow>, WindowedValue<Long>>>) ImmutableList.of(KV.of(KV.of(1L, windowA), WindowedValue.of(110L, new Instant(1), windowA, PaneInfo.NO_FIRING)),     KV.of(KV.of(1L, windowA), WindowedValue.of(111L, new Instant(2), windowA, PaneInfo.NO_FIRING)))));    thrown.expect(IllegalStateException.class);    thrown.expectMessage("Unique keys are expected but found key");    doFnTester.processBundle(inputElements);}
private MetricUpdate beam_f6941_0(MetricUpdate update, String name, String namespace, String step, boolean tentative)
{    MetricStructuredName structuredName = new MetricStructuredName();    structuredName.setName(name);    structuredName.setOrigin("user");    ImmutableMap.Builder contextBuilder = new ImmutableMap.Builder<>();    contextBuilder.put("step", step).put("namespace", namespace);    if (tentative) {        contextBuilder.put("tentative", "true");    }    structuredName.setContext(contextBuilder.build());    update.setName(structuredName);    return update;}
private MetricUpdate beam_f6942_0(String name, String namespace, String step, Long sum, Long count, Long min, Long max, boolean tentative)
{    MetricUpdate update = new MetricUpdate();    ArrayMap<String, BigDecimal> distribution = ArrayMap.create();    distribution.add("count", new BigDecimal(count));    distribution.add("mean", new BigDecimal(sum / count));    distribution.add("sum", new BigDecimal(sum));    distribution.add("min", new BigDecimal(min));    distribution.add("max", new BigDecimal(max));    update.setDistribution(distribution);    return setStructuredName(update, name, namespace, step, tentative);}
private MetricUpdate beam_f6943_0(String name, String namespace, String step, long scalar, boolean tentative)
{    MetricUpdate update = new MetricUpdate();    update.setScalar(new BigDecimal(scalar));    return setStructuredName(update, name, namespace, step, tentative);}
private void beam_f6951_0(Duration pollingInterval, int retries, long timeSleptMillis)
{    long highSum = 0;    long lowSum = 0;    for (int i = 0; i < retries; i++) {        double currentInterval = pollingInterval.getMillis() * Math.pow(DataflowPipelineJob.DEFAULT_BACKOFF_EXPONENT, i);        double randomOffset = 0.5 * currentInterval;        highSum += Math.round(currentInterval + randomOffset);        lowSum += Math.round(currentInterval - randomOffset);    }    assertThat(timeSleptMillis, allOf(greaterThanOrEqualTo(lowSum), lessThanOrEqualTo(highSum)));}
public void beam_f6952_0() throws Exception
{    Dataflow.Projects.Locations.Jobs.Get statusRequest = mock(Dataflow.Projects.Locations.Jobs.Get.class);    Job statusResponse = new Job();    statusResponse.setCurrentState("JOB_STATE_" + State.DONE.name());    when(mockJobs.get(eq(PROJECT_ID), eq(REGION_ID), eq(JOB_ID))).thenReturn(statusRequest);    when(statusRequest.execute()).thenReturn(statusResponse);    MonitoringUtil.JobMessagesHandler jobHandler = mock(MonitoringUtil.JobMessagesHandler.class);    Dataflow.Projects.Locations.Jobs.Messages mockMessages = mock(Dataflow.Projects.Locations.Jobs.Messages.class);    Messages.List listRequest = mock(Dataflow.Projects.Locations.Jobs.Messages.List.class);    when(mockJobs.messages()).thenReturn(mockMessages);    when(mockMessages.list(eq(PROJECT_ID), eq(REGION_ID), eq(JOB_ID))).thenReturn(listRequest);    when(listRequest.setPageToken(eq((String) null))).thenReturn(listRequest);    when(listRequest.execute()).thenThrow(SocketTimeoutException.class);    DataflowPipelineJob job = new DataflowPipelineJob(DataflowClient.create(options), JOB_ID, options, ImmutableMap.of());    State state = job.waitUntilFinish(Duration.standardMinutes(5), jobHandler, fastClock, fastClock);    assertEquals(null, state);}
public State beam_f6953_0(State state) throws Exception
{    Dataflow.Projects.Locations.Jobs.Get statusRequest = mock(Dataflow.Projects.Locations.Jobs.Get.class);    Job statusResponse = new Job();    statusResponse.setCurrentState("JOB_STATE_" + state.name());    if (state == State.UPDATED) {        statusResponse.setReplacedByJobId(REPLACEMENT_JOB_ID);    }    when(mockJobs.get(eq(PROJECT_ID), eq(REGION_ID), eq(JOB_ID))).thenReturn(statusRequest);    when(statusRequest.execute()).thenReturn(statusResponse);    DataflowPipelineJob job = new DataflowPipelineJob(DataflowClient.create(options), JOB_ID, options, ImmutableMap.of());    return job.waitUntilFinish(Duration.standardMinutes(1), null, fastClock, fastClock);}
public void beam_f6961_0() throws Exception
{    Dataflow.Projects.Locations.Jobs.Get statusRequest = mock(Dataflow.Projects.Locations.Jobs.Get.class);    when(mockJobs.get(eq(PROJECT_ID), eq(REGION_ID), eq(JOB_ID))).thenReturn(statusRequest);    when(statusRequest.execute()).thenThrow(IOException.class);    DataflowPipelineJob job = new DataflowPipelineJob(DataflowClient.create(options), JOB_ID, options, ImmutableMap.of());    long startTime = fastClock.nanoTime();    State state = job.waitUntilFinish(Duration.millis(4), null, fastClock, fastClock);    assertEquals(null, state);    long timeDiff = TimeUnit.NANOSECONDS.toMillis(fastClock.nanoTime() - startTime);        assertEquals(4L, timeDiff);}
public void beam_f6962_0() throws Exception
{    Dataflow.Projects.Locations.Jobs.Get statusRequest = mock(Dataflow.Projects.Locations.Jobs.Get.class);    Job statusResponse = new Job();    statusResponse.setCurrentState("JOB_STATE_RUNNING");    when(mockJobs.get(eq(PROJECT_ID), eq(REGION_ID), eq(JOB_ID))).thenReturn(statusRequest);    when(statusRequest.execute()).thenReturn(statusResponse);    FastNanoClockAndFuzzySleeper clock = new FastNanoClockAndFuzzySleeper();    DataflowPipelineJob job = new DataflowPipelineJob(DataflowClient.create(options), JOB_ID, options, ImmutableMap.of());    long startTime = clock.nanoTime();    State state = job.waitUntilFinish(Duration.millis(4), null, clock, clock);    assertEquals(null, state);    long timeDiff = TimeUnit.NANOSECONDS.toMillis(clock.nanoTime() - startTime);        assertThat(timeDiff, lessThanOrEqualTo(4L));}
public void beam_f6963_0() throws Exception
{    Dataflow.Projects.Locations.Jobs.Get statusRequest = mock(Dataflow.Projects.Locations.Jobs.Get.class);    Job statusResponse = new Job();    statusResponse.setCurrentState("JOB_STATE_" + State.RUNNING.name());    when(mockJobs.get(eq(PROJECT_ID), eq(REGION_ID), eq(JOB_ID))).thenReturn(statusRequest);    when(statusRequest.execute()).thenReturn(statusResponse);    DataflowPipelineJob job = new DataflowPipelineJob(DataflowClient.create(options), JOB_ID, options, ImmutableMap.of());    assertEquals(State.RUNNING, job.getStateWithRetriesOrUnknownOnException(BackOffAdapter.toGcpBackOff(DataflowPipelineJob.STATUS_BACKOFF_FACTORY.backoff()), fastClock));}
public void beam_f6971_0() throws IOException
{    Dataflow.Projects.Locations.Jobs.Get statusRequest = mock(Dataflow.Projects.Locations.Jobs.Get.class);    Job statusResponse = new Job();    statusResponse.setCurrentState("JOB_STATE_RUNNING");    when(mockJobs.get(PROJECT_ID, REGION_ID, JOB_ID)).thenReturn(statusRequest);    when(statusRequest.execute()).thenReturn(statusResponse);    Dataflow.Projects.Locations.Jobs.Update update = mock(Dataflow.Projects.Locations.Jobs.Update.class);    when(mockJobs.update(eq(PROJECT_ID), eq(REGION_ID), eq(JOB_ID), any(Job.class))).thenReturn(update);    when(update.execute()).thenThrow(new IOException("Job has terminated in state SUCCESS"));    DataflowPipelineJob job = new DataflowPipelineJob(DataflowClient.create(options), JOB_ID, options, null);    State returned = job.cancel();    assertThat(returned, equalTo(State.RUNNING));    expectedLogs.verifyWarn("Cancel failed because job is already terminated.");}
public void beam_f6972_0() throws IOException
{    Dataflow.Projects.Locations.Jobs.Get statusRequest = mock(Dataflow.Projects.Locations.Jobs.Get.class);    Job statusResponse = new Job();    statusResponse.setCurrentState("JOB_STATE_FAILED");    when(mockJobs.get(PROJECT_ID, REGION_ID, JOB_ID)).thenReturn(statusRequest);    when(statusRequest.execute()).thenReturn(statusResponse);    Dataflow.Projects.Locations.Jobs.Update update = mock(Dataflow.Projects.Locations.Jobs.Update.class);    when(mockJobs.update(eq(PROJECT_ID), eq(REGION_ID), eq(JOB_ID), any(Job.class))).thenReturn(update);    when(update.execute()).thenThrow(new IOException());    DataflowPipelineJob job = new DataflowPipelineJob(DataflowClient.create(options), JOB_ID, options, null);    assertEquals(State.FAILED, job.cancel());    Job content = new Job();    content.setProjectId(PROJECT_ID);    content.setId(JOB_ID);    content.setRequestedState("JOB_STATE_CANCELLED");    verify(mockJobs).update(eq(PROJECT_ID), eq(REGION_ID), eq(JOB_ID), eq(content));    verify(mockJobs).get(PROJECT_ID, REGION_ID, JOB_ID);    verifyNoMoreInteractions(mockJobs);}
public void beam_f6973_0() throws Exception
{    DataflowPipelineJob job = new DataflowPipelineJob(mockDataflowClient, JOB_ID, options, null);    Sleeper sleeper = new ZeroSleeper();    NanoClock nanoClock = mock(NanoClock.class);    Instant separatingTimestamp = new Instant(42L);    JobMessage theMessage = infoMessage(separatingTimestamp, "nothing");    MonitoringUtil mockMonitor = mock(MonitoringUtil.class);    when(mockMonitor.getJobMessages(anyString(), anyLong())).thenReturn(ImmutableList.of(theMessage));        Job fakeJob = new Job();    fakeJob.setCurrentState("JOB_STATE_RUNNING");    when(mockDataflowClient.getJob(anyString())).thenReturn(fakeJob);        when(nanoClock.nanoTime()).thenReturn(0L).thenReturn(2000000000L);    job.waitUntilFinish(Duration.standardSeconds(1), mockHandler, sleeper, nanoClock, mockMonitor);    verify(mockHandler).process(ImmutableList.of(theMessage));            when(nanoClock.nanoTime()).thenReturn(3000000000L).thenReturn(6000000000L);    job.waitUntilFinish(Duration.standardSeconds(1), mockHandler, sleeper, nanoClock, mockMonitor);    verify(mockMonitor).getJobMessages(anyString(), eq(separatingTimestamp.getMillis()));}
private Pipeline beam_f6982_0(DataflowPipelineOptions options)
{    options.setRunner(DataflowRunner.class);    Pipeline p = Pipeline.create(options);    p.apply("ReadMyFile", TextIO.read().from("gs://bucket/object")).apply("WriteMyFile", TextIO.write().to("gs://bucket/object"));    DataflowRunner runner = DataflowRunner.fromOptions(options);    runner.replaceTransforms(p);    return p;}
private static Dataflow beam_f6983_0(ArgumentMatcher<Job> jobMatcher) throws IOException
{    Dataflow mockDataflowClient = mock(Dataflow.class);    Dataflow.Projects mockProjects = mock(Dataflow.Projects.class);    Dataflow.Projects.Jobs mockJobs = mock(Dataflow.Projects.Jobs.class);    Dataflow.Projects.Jobs.Create mockRequest = mock(Dataflow.Projects.Jobs.Create.class);    when(mockDataflowClient.projects()).thenReturn(mockProjects);    when(mockProjects.jobs()).thenReturn(mockJobs);    when(mockJobs.create(eq("someProject"), argThat(jobMatcher))).thenReturn(mockRequest);    Job resultJob = new Job();    resultJob.setId("newid");    when(mockRequest.execute()).thenReturn(resultJob);    return mockDataflowClient;}
private static DataflowPipelineOptions beam_f6984_0() throws IOException
{    GcsUtil mockGcsUtil = mock(GcsUtil.class);    when(mockGcsUtil.expand(any(GcsPath.class))).then(invocation -> ImmutableList.of((GcsPath) invocation.getArguments()[0]));    when(mockGcsUtil.bucketAccessible(any(GcsPath.class))).thenReturn(true);    DataflowPipelineOptions options = PipelineOptionsFactory.as(DataflowPipelineOptions.class);    options.setRunner(DataflowRunner.class);    options.setGcpCredential(new TestCredential());    options.setJobName("some-job-name");    options.setProject("some-project");    options.setRegion("some-region");    options.setTempLocation(GcsPath.fromComponents("somebucket", "some/path").toString());    options.setFilesToStage(new ArrayList<>());    options.setDataflowClient(buildMockDataflow(new IsValidCreateRequest()));    options.setGcsUtil(mockGcsUtil);        FileSystems.setDefaultPipelineOptions(options);    return options;}
public void beam_f6992_0() throws IOException
{    final String testZone = "test-zone-1";    DataflowPipelineOptions options = buildPipelineOptions();    options.setZone(testZone);    Pipeline p = buildPipeline(options);    p.traverseTopologically(new RecordingPipelineVisitor());    Job job = DataflowPipelineTranslator.fromOptions(options).translate(p, DataflowRunner.fromOptions(options), Collections.emptyList()).getJob();    assertEquals(1, job.getEnvironment().getWorkerPools().size());    assertEquals(testZone, job.getEnvironment().getWorkerPools().get(0).getZone());}
public void beam_f6993_0() throws IOException
{    final String testMachineType = "test-machine-type";    DataflowPipelineOptions options = buildPipelineOptions();    options.setWorkerMachineType(testMachineType);    Pipeline p = buildPipeline(options);    p.traverseTopologically(new RecordingPipelineVisitor());    Job job = DataflowPipelineTranslator.fromOptions(options).translate(p, DataflowRunner.fromOptions(options), Collections.emptyList()).getJob();    assertEquals(1, job.getEnvironment().getWorkerPools().size());    WorkerPool workerPool = job.getEnvironment().getWorkerPools().get(0);    assertEquals(testMachineType, workerPool.getMachineType());}
public void beam_f6994_0() throws IOException
{    final Integer diskSizeGb = 1234;    DataflowPipelineOptions options = buildPipelineOptions();    options.setDiskSizeGb(diskSizeGb);    Pipeline p = buildPipeline(options);    p.traverseTopologically(new RecordingPipelineVisitor());    Job job = DataflowPipelineTranslator.fromOptions(options).translate(p, DataflowRunner.fromOptions(options), Collections.emptyList()).getJob();    assertEquals(1, job.getEnvironment().getWorkerPools().size());    assertEquals(diskSizeGb, job.getEnvironment().getWorkerPools().get(0).getDiskSizeGb());}
public boolean beam_f7002_0()
{    return false;}
public String beam_f7003_0()
{    throw new RuntimeException("Should not be called.");}
public void beam_f7004_0() throws Exception
{    DataflowPipelineOptions options = buildPipelineOptions();    Pipeline pipeline = Pipeline.create(options);    DataflowPipelineTranslator t = DataflowPipelineTranslator.fromOptions(options);    pipeline.apply(TextIO.read().from(new TestValueProvider()));        t.translate(pipeline, DataflowRunner.fromOptions(options), Collections.emptyList());}
public void beam_f7014_0() throws Exception
{    DataflowPipelineOptions options = buildPipelineOptions();    DataflowPipelineTranslator translator = DataflowPipelineTranslator.fromOptions(options);    Pipeline pipeline = Pipeline.create(options);    DoFn<Integer, Integer> fn1 = new DoFn<Integer, Integer>() {        @ProcessElement        public void processElement(ProcessContext c) throws Exception {            c.output(c.element());        }        @Override        public void populateDisplayData(DisplayData.Builder builder) {            builder.add(DisplayData.item("foo", "bar")).add(DisplayData.item("foo2", DataflowPipelineTranslatorTest.class).withLabel("Test Class").withLinkUrl("http://www.google.com"));        }    };    DoFn<Integer, Integer> fn2 = new DoFn<Integer, Integer>() {        @ProcessElement        public void processElement(ProcessContext c) throws Exception {            c.output(c.element());        }        @Override        public void populateDisplayData(DisplayData.Builder builder) {            builder.add(DisplayData.item("foo3", 1234));        }    };    ParDo.SingleOutput<Integer, Integer> parDo1 = ParDo.of(fn1);    ParDo.SingleOutput<Integer, Integer> parDo2 = ParDo.of(fn2);    pipeline.apply(Create.of(1, 2, 3)).apply(parDo1).apply(parDo2);    DataflowRunner runner = DataflowRunner.fromOptions(options);    runner.replaceTransforms(pipeline);    Job job = translator.translate(pipeline, runner, Collections.emptyList()).getJob();    assertAllStepOutputsHaveUniqueIds(job);    List<Step> steps = job.getSteps();    assertEquals(3, steps.size());    Map<String, Object> parDo1Properties = steps.get(1).getProperties();    Map<String, Object> parDo2Properties = steps.get(2).getProperties();    assertThat(parDo1Properties, hasKey("display_data"));    @SuppressWarnings("unchecked")    Collection<Map<String, String>> fn1displayData = (Collection<Map<String, String>>) parDo1Properties.get("display_data");    @SuppressWarnings("unchecked")    Collection<Map<String, String>> fn2displayData = (Collection<Map<String, String>>) parDo2Properties.get("display_data");    ImmutableSet<ImmutableMap<String, Object>> expectedFn1DisplayData = ImmutableSet.of(ImmutableMap.<String, Object>builder().put("key", "foo").put("type", "STRING").put("value", "bar").put("namespace", fn1.getClass().getName()).build(), ImmutableMap.<String, Object>builder().put("key", "fn").put("label", "Transform Function").put("type", "JAVA_CLASS").put("value", fn1.getClass().getName()).put("shortValue", fn1.getClass().getSimpleName()).put("namespace", parDo1.getClass().getName()).build(), ImmutableMap.<String, Object>builder().put("key", "foo2").put("type", "JAVA_CLASS").put("value", DataflowPipelineTranslatorTest.class.getName()).put("shortValue", DataflowPipelineTranslatorTest.class.getSimpleName()).put("namespace", fn1.getClass().getName()).put("label", "Test Class").put("linkUrl", "http://www.google.com").build());    ImmutableSet<ImmutableMap<String, Object>> expectedFn2DisplayData = ImmutableSet.of(ImmutableMap.<String, Object>builder().put("key", "fn").put("label", "Transform Function").put("type", "JAVA_CLASS").put("value", fn2.getClass().getName()).put("shortValue", fn2.getClass().getSimpleName()).put("namespace", parDo2.getClass().getName()).build(), ImmutableMap.<String, Object>builder().put("key", "foo3").put("type", "INTEGER").put("value", 1234L).put("namespace", fn2.getClass().getName()).build());    assertEquals(expectedFn1DisplayData, ImmutableSet.copyOf(fn1displayData));    assertEquals(expectedFn2DisplayData, ImmutableSet.copyOf(fn2displayData));}
public void beam_f7015_0(ProcessContext c) throws Exception
{    c.output(c.element());}
public void beam_f7016_0(DisplayData.Builder builder)
{    builder.add(DisplayData.item("foo", "bar")).add(DisplayData.item("foo2", DataflowPipelineTranslatorTest.class).withLabel("Test Class").withLinkUrl("http://www.google.com"));}
public void beam_f7025_0()
{    PTransformMatcher matcher = new DataflowPTransformMatchers.CombineValuesWithParentCheckPTransformMatcher();    AppliedPTransform<?, ?, ?> groupedValues;    groupedValues = getCombineGroupedValuesFrom(createCombinePerKeyPipeline());    assertThat(matcher.matches(groupedValues), is(true));}
public void beam_f7026_0()
{    PTransformMatcher matcher = new DataflowPTransformMatchers.CombineValuesWithParentCheckPTransformMatcher();    AppliedPTransform<?, ?, ?> groupedValues;    groupedValues = getCombineGroupedValuesFrom(createCombineGroupedValuesPipeline());    assertThat(matcher.matches(groupedValues), is(false));    groupedValues = getCombineGroupedValuesFrom(createCombineGroupedValuesWithSideInputsPipeline());    assertThat(matcher.matches(groupedValues), is(false));    groupedValues = getCombineGroupedValuesFrom(createCombinePerKeyWithSideInputsPipeline());    assertThat(matcher.matches(groupedValues), is(false));}
private static TestPipeline beam_f7027_0()
{    TestPipeline pipeline = TestPipeline.create().enableAbandonedNodeEnforcement(false);    PCollection<KV<String, Integer>> input = pipeline.apply(Create.of(KV.of("key", 1))).setCoder(KvCoder.of(StringUtf8Coder.of(), VarIntCoder.of()));    input.apply(Combine.perKey(new SumCombineFn()));    return pipeline;}
public Integer beam_f7035_0(Iterable<Integer> accumulators)
{    Integer sum = 0;    for (Integer accum : accumulators) {        sum += accum;    }    return sum;}
public Integer beam_f7036_0(Integer accumulator)
{    return accumulator;}
public Integer beam_f7037_0(Context c)
{    return delegate.createAccumulator();}
private Dataflow beam_f7045_0() throws IOException
{    Dataflow mockDataflowClient = mock(Dataflow.class);    Dataflow.Projects mockProjects = mock(Dataflow.Projects.class);    Dataflow.Projects.Locations mockLocations = mock(Dataflow.Projects.Locations.class);    Dataflow.Projects.Locations.Jobs.Create mockRequest = mock(Dataflow.Projects.Locations.Jobs.Create.class);    Dataflow.Projects.Locations.Jobs.List mockList = mock(Dataflow.Projects.Locations.Jobs.List.class);    when(mockDataflowClient.projects()).thenReturn(mockProjects);    when(mockProjects.locations()).thenReturn(mockLocations);    when(mockLocations.jobs()).thenReturn(mockJobs);    when(mockJobs.create(eq(PROJECT_ID), eq(REGION_ID), isA(Job.class))).thenReturn(mockRequest);    when(mockJobs.list(eq(PROJECT_ID), eq(REGION_ID))).thenReturn(mockList);    when(mockList.setPageToken(any())).thenReturn(mockList);    when(mockList.execute()).thenReturn(new ListJobsResponse().setJobs(Arrays.asList(new Job().setName("oldjobname").setId("oldJobId").setCurrentState("JOB_STATE_RUNNING"))));    Job resultJob = new Job();    resultJob.setId("newid");    when(mockRequest.execute()).thenReturn(resultJob);    return mockDataflowClient;}
private GcsUtil beam_f7046_0() throws IOException
{    GcsUtil mockGcsUtil = mock(GcsUtil.class);    when(mockGcsUtil.create(any(GcsPath.class), anyString())).then(invocation -> FileChannel.open(Files.createTempFile("channel-", ".tmp"), StandardOpenOption.CREATE, StandardOpenOption.DELETE_ON_CLOSE));    when(mockGcsUtil.expand(any(GcsPath.class))).then(invocation -> ImmutableList.of((GcsPath) invocation.getArguments()[0]));    return mockGcsUtil;}
private DataflowPipelineOptions beam_f7047_0() throws IOException
{    DataflowPipelineOptions options = PipelineOptionsFactory.as(DataflowPipelineOptions.class);    options.setRunner(DataflowRunner.class);    options.setProject(PROJECT_ID);    options.setTempLocation(VALID_TEMP_BUCKET);    options.setRegion(REGION_ID);        options.setFilesToStage(new ArrayList<>());    options.setDataflowClient(buildMockDataflow());    options.setGcsUtil(mockGcsUtil);    options.setGcpCredential(new TestCredential());        FileSystems.setDefaultPipelineOptions(options);    return options;}
public JacksonIncompatible beam_f7055_0(JsonParser jsonParser, DeserializationContext deserializationContext) throws IOException, JsonProcessingException
{    return new JacksonIncompatible(jsonParser.readValueAs(String.class));}
public void beam_f7056_0(JacksonIncompatible jacksonIncompatible, JsonGenerator jsonGenerator, SerializerProvider serializerProvider) throws IOException, JsonProcessingException
{    jsonGenerator.writeString(jacksonIncompatible.value);}
public void beam_f7057_0() throws IOException
{    DataflowPipelineOptions options = buildPipelineOptions();    options.as(JacksonIncompatibleOptions.class).setJacksonIncompatible(new JacksonIncompatible("userCustomTypeTest"));    Pipeline p = Pipeline.create(options);    p.run();    ArgumentCaptor<Job> jobCaptor = ArgumentCaptor.forClass(Job.class);    Mockito.verify(mockJobs).create(eq(PROJECT_ID), eq(REGION_ID), jobCaptor.capture());    Map<String, Object> sdkPipelineOptions = jobCaptor.getValue().getEnvironment().getSdkPipelineOptions();    assertThat(sdkPipelineOptions, hasKey("options"));    Map<String, Object> optionsMap = (Map<String, Object>) sdkPipelineOptions.get("options");    assertThat(optionsMap, hasEntry("jacksonIncompatible", (Object) "userCustomTypeTest"));}
public void beam_f7065_0() throws IOException
{    thrown.expect(IllegalArgumentException.class);    thrown.expectMessage("Could not find running job named badjobname");    DataflowPipelineOptions options = buildPipelineOptions();    options.setUpdate(true);    options.setJobName("badJobName");    Pipeline p = buildDataflowPipeline(options);    p.run();}
public void beam_f7066_0() throws IOException
{    DataflowPipelineOptions options = buildPipelineOptions();    options.setUpdate(true);    options.setJobName("oldJobName");    Dataflow mockDataflowClient = options.getDataflowClient();    Dataflow.Projects.Locations.Jobs.Create mockRequest = mock(Dataflow.Projects.Locations.Jobs.Create.class);    when(mockDataflowClient.projects().locations().jobs().create(eq(PROJECT_ID), eq(REGION_ID), any(Job.class))).thenReturn(mockRequest);    final Job resultJob = new Job();    resultJob.setId("newid");        resultJob.setClientRequestId("different_request_id");    when(mockRequest.execute()).thenReturn(resultJob);    Pipeline p = buildDataflowPipeline(options);    thrown.expect(DataflowJobAlreadyUpdatedException.class);    thrown.expect(new TypeSafeMatcher<DataflowJobAlreadyUpdatedException>() {        @Override        public void describeTo(Description description) {            description.appendText("Expected job ID: " + resultJob.getId());        }        @Override        protected boolean matchesSafely(DataflowJobAlreadyUpdatedException item) {            return resultJob.getId().equals(item.getJob().getJobId());        }    });    thrown.expectMessage("The job named oldjobname with id: oldJobId has already been updated " + "into job id: newid and cannot be updated again.");    p.run();}
public void beam_f7067_0(Description description)
{    description.appendText("Expected job ID: " + resultJob.getId());}
public void beam_f7075_0() throws IOException
{    DataflowPipelineOptions options = buildPipelineOptions();    options.setSaveProfilesToGcs("file://my/staging/location");    try {        DataflowRunner.fromOptions(options);        fail("fromOptions should have failed");    } catch (IllegalArgumentException e) {        assertThat(e.getMessage(), containsString("Expected a valid 'gs://' path but was given"));    }    options.setSaveProfilesToGcs("my/staging/location");    try {        DataflowRunner.fromOptions(options);        fail("fromOptions should have failed");    } catch (IllegalArgumentException e) {        assertThat(e.getMessage(), containsString("Expected a valid 'gs://' path but was given"));    }}
public void beam_f7076_0() throws IOException
{    DataflowPipelineOptions options = buildPipelineOptions();    options.setGcpTempLocation(NON_EXISTENT_BUCKET);    thrown.expect(IllegalArgumentException.class);    thrown.expectMessage(containsString("Output path does not exist or is not writeable: " + NON_EXISTENT_BUCKET));    DataflowRunner.fromOptions(options);    ArgumentCaptor<Job> jobCaptor = ArgumentCaptor.forClass(Job.class);    Mockito.verify(mockJobs).create(eq(PROJECT_ID), eq(REGION_ID), jobCaptor.capture());    assertValidJob(jobCaptor.getValue());}
public void beam_f7077_0() throws IOException
{    DataflowPipelineOptions options = buildPipelineOptions();    options.setStagingLocation(NON_EXISTENT_BUCKET);    thrown.expect(IllegalArgumentException.class);    thrown.expectMessage(containsString("Output path does not exist or is not writeable: " + NON_EXISTENT_BUCKET));    DataflowRunner.fromOptions(options);    ArgumentCaptor<Job> jobCaptor = ArgumentCaptor.forClass(Job.class);    Mockito.verify(mockJobs).create(eq(PROJECT_ID), eq(REGION_ID), jobCaptor.capture());    assertValidJob(jobCaptor.getValue());}
public void beam_f7085_0()
{    DataflowPipelineOptions options = PipelineOptionsFactory.as(DataflowPipelineOptions.class);    options.setRunner(DataflowRunner.class);    options.setProject("foo-project");    thrown.expect(IllegalArgumentException.class);    thrown.expectMessage("DataflowRunner requires gcpTempLocation, " + "but failed to retrieve a value from PipelineOption");    DataflowRunner.fromOptions(options);}
public void beam_f7086_0() throws Exception
{    DataflowPipelineOptions options = PipelineOptionsFactory.as(DataflowPipelineOptions.class);    options.setRunner(DataflowRunner.class);    options.setGcpCredential(new TestCredential());    options.setProject("foo-project");    options.setGcpTempLocation(VALID_TEMP_BUCKET);    options.setGcsUtil(mockGcsUtil);    DataflowRunner.fromOptions(options);}
public void beam_f7087_0() throws Exception
{    DataflowPipelineOptions options = PipelineOptionsFactory.as(DataflowPipelineOptions.class);    options.setRunner(DataflowRunner.class);    options.setGcpCredential(new TestCredential());    options.setProject("foo-project");    options.setTempLocation(VALID_TEMP_BUCKET);    options.setGcsUtil(mockGcsUtil);    DataflowRunner.fromOptions(options);}
public void beam_f7095_0() throws IOException
{    DataflowPipelineOptions options = buildPipelineOptions();    Pipeline p = Pipeline.create(options);    p.apply(Create.of(Arrays.asList(1, 2, 3))).apply(new TestTransform());    thrown.expect(IllegalStateException.class);    thrown.expectMessage(containsString("no translator registered"));    DataflowPipelineTranslator.fromOptions(options).translate(p, DataflowRunner.fromOptions(options), Collections.emptyList());    ArgumentCaptor<Job> jobCaptor = ArgumentCaptor.forClass(Job.class);    Mockito.verify(mockJobs).create(eq(PROJECT_ID), eq(REGION_ID), jobCaptor.capture());    assertValidJob(jobCaptor.getValue());}
public void beam_f7096_0() throws IOException
{        DataflowPipelineOptions options = buildPipelineOptions();    Pipeline p = Pipeline.create(options);    TestTransform transform = new TestTransform();    p.apply(Create.of(Arrays.asList(1, 2, 3)).withCoder(BigEndianIntegerCoder.of())).apply(transform);    DataflowPipelineTranslator translator = DataflowRunner.fromOptions(options).getTranslator();    DataflowPipelineTranslator.registerTransformTranslator(TestTransform.class, (transform1, context) -> {        transform1.translated = true;                        TransformTranslator.StepTranslationContext stepContext = context.addStep(transform1, "TestTranslate");        stepContext.addOutput(PropertyNames.OUTPUT, context.getOutput(transform1));    });    translator.translate(p, DataflowRunner.fromOptions(options), Collections.emptyList());    assertTrue(transform.translated);}
private void beam_f7097_0(PipelineOptions options) throws Exception
{    Pipeline p = Pipeline.create(options);    p.apply(Create.of(KV.of(13, 42))).apply(ParDo.of(new DoFn<KV<Integer, Integer>, Void>() {        @StateId("fizzle")        private final StateSpec<MapState<Void, Void>> voidState = StateSpecs.map();        @ProcessElement        public void process() {        }    }));    thrown.expectMessage("MapState");    thrown.expect(UnsupportedOperationException.class);    p.run();}
public void beam_f7107_0() throws IOException
{    DataflowPipelineOptions options = buildPipelineOptions();    Pipeline p = Pipeline.create(options);    Create.TimestampedValues<String> transform = Create.timestamped(Arrays.asList(TimestampedValue.of("TestString", Instant.now())));    p.apply(transform);    CompositeTransformRecorder recorder = new CompositeTransformRecorder();    p.traverseTopologically(recorder);            assertThat("Expected to have seen CreateTimestamped composite transform.", recorder.getCompositeTransforms(), hasItem(transform));    assertThat("Expected to have two composites, CreateTimestamped and Create.Values", recorder.getCompositeTransforms(), hasItem(Matchers.<PTransform<?, ?>>isA((Class) Create.Values.class)));}
public void beam_f7108_0()
{    DataflowPipelineOptions options = PipelineOptionsFactory.as(DataflowPipelineOptions.class);    options.setJobName("TestJobName");    options.setProject("test-project");    options.setTempLocation("gs://test/temp/location");    options.setGcpCredential(new TestCredential());    options.setPathValidatorClass(NoopPathValidator.class);    options.setRunner(DataflowRunner.class);    assertEquals("DataflowRunner#testjobname", DataflowRunner.fromOptions(options).toString());}
public void beam_f7109_0() throws Exception
{    File existingFile = tmpFolder.newFile();    DataflowPipelineOptions options = PipelineOptionsFactory.as(DataflowPipelineOptions.class);    options.setJobName("TestJobName");    options.setGcpCredential(new TestCredential());    options.setPathValidatorClass(NoopPathValidator.class);    options.setProject("test-project");    options.setRunner(DataflowRunner.class);    options.setTemplateLocation(existingFile.getPath());    options.setTempLocation(tmpFolder.getRoot().getPath());    Pipeline p = Pipeline.create(options);    p.run();    expectedLogs.verifyInfo("Template successfully created");}
public void beam_f7118_0() throws Exception
{    PipelineOptions options = buildPipelineOptions();    options.as(StreamingOptions.class).setStreaming(false);    verifyMergingStatefulParDoRejected(options);}
public void beam_f7119_0()
{        assumeFalse(pipeline.getOptions().as(StreamingOptions.class).isStreaming());    final int batchSize = 2;    List<KV<String, Integer>> testValues = Arrays.asList(KV.of("A", 1), KV.of("B", 0), KV.of("A", 2), KV.of("A", 4), KV.of("A", 8));    PCollection<KV<String, Iterable<Integer>>> output = pipeline.apply(Create.of(testValues)).apply(GroupIntoBatches.ofSize(batchSize));    PAssert.thatMultimap(output).satisfies(new SerializableFunction<Map<String, Iterable<Iterable<Integer>>>, Void>() {        @Override        public Void apply(Map<String, Iterable<Iterable<Integer>>> input) {            assertEquals(2, input.size());            assertThat(input.keySet(), containsInAnyOrder("A", "B"));            Map<String, Integer> sums = new HashMap<>();            for (Map.Entry<String, Iterable<Iterable<Integer>>> entry : input.entrySet()) {                for (Iterable<Integer> batch : entry.getValue()) {                    assertThat(Iterables.size(batch), lessThanOrEqualTo(batchSize));                    for (Integer value : batch) {                        sums.put(entry.getKey(), value + sums.getOrDefault(entry.getKey(), 0));                    }                }            }            assertEquals(15, (int) sums.get("A"));            assertEquals(0, (int) sums.get("B"));            return null;        }    });    pipeline.run();    AtomicBoolean sawGroupIntoBatchesOverride = new AtomicBoolean(false);    pipeline.traverseTopologically(new PipelineVisitor.Defaults() {        @Override        public CompositeBehavior enterCompositeTransform(Node node) {            if (node.getTransform() instanceof BatchGroupIntoBatches) {                sawGroupIntoBatchesOverride.set(true);            }            return CompositeBehavior.ENTER_TRANSFORM;        }    });    assertTrue(sawGroupIntoBatchesOverride.get());}
public Void beam_f7120_0(Map<String, Iterable<Iterable<Integer>>> input)
{    assertEquals(2, input.size());    assertThat(input.keySet(), containsInAnyOrder("A", "B"));    Map<String, Integer> sums = new HashMap<>();    for (Map.Entry<String, Iterable<Iterable<Integer>>> entry : input.entrySet()) {        for (Iterable<Integer> batch : entry.getValue()) {            assertThat(Iterables.size(batch), lessThanOrEqualTo(batchSize));            for (Integer value : batch) {                sums.put(entry.getKey(), value + sums.getOrDefault(entry.getKey(), 0));            }        }    }    assertEquals(15, (int) sums.get("A"));    assertEquals(0, (int) sums.get("B"));    return null;}
public void beam_f7129_0()
{    DataflowPipelineOptions options = PipelineOptionsFactory.as(DataflowPipelineOptions.class);    options.setJobName("TestJobName");    assertEquals("TestJobName", options.getJobName());}
public void beam_f7130_0()
{    resetDateTimeProviderRule.setDateTimeFixed("2014-12-08T19:07:06.698Z");    System.getProperties().remove("user.name");    DataflowPipelineOptions options = PipelineOptionsFactory.as(DataflowPipelineOptions.class);    options.setAppName("TestApplication");    List<String> nameComponents = Splitter.on('-').splitToList(options.getJobName());    assertEquals(4, nameComponents.size());    assertEquals("testapplication", nameComponents.get(0));    assertEquals("", nameComponents.get(1));    assertEquals("1208190706", nameComponents.get(2));        Long.parseLong(nameComponents.get(3), 16);    assertTrue(options.getJobName().length() <= 40);}
public void beam_f7131_0()
{    resetDateTimeProviderRule.setDateTimeFixed("2014-12-08T19:07:06.698Z");    System.getProperties().put("user.name", "abcdeabcdeabcdeabcdeabcdeabcde");    DataflowPipelineOptions options = PipelineOptionsFactory.as(DataflowPipelineOptions.class);    options.setAppName("1234567890123456789012345678901234567890");    List<String> nameComponents = Splitter.on('-').splitToList(options.getJobName());    assertEquals(4, nameComponents.size());    assertEquals("a234567890123456789012345678901234567890", nameComponents.get(0));    assertEquals("abcdeabcdeabcdeabcdeabcdeabcde", nameComponents.get(1));    assertEquals("1208190706", nameComponents.get(2));        Long.parseLong(nameComponents.get(3), 16);}
public void beam_f7139_0()
{    DataflowPipelineOptions options = PipelineOptionsFactory.as(DataflowPipelineOptions.class);    options.setTempLocation("file://temp_location");    thrown.expect(IllegalArgumentException.class);    thrown.expectMessage("Error constructing default value for stagingLocation: " + "failed to retrieve gcpTempLocation.");    thrown.expectCause(hasMessage(containsString("Error constructing default value for gcpTempLocation")));    options.getStagingLocation();}
public void beam_f7140_0()
{    DataflowPipelineOptions options = PipelineOptionsFactory.as(DataflowPipelineOptions.class);    options.setGcpTempLocation("file://temp_location");    thrown.expect(IllegalArgumentException.class);    thrown.expectMessage("Error constructing default value for stagingLocation: gcpTempLocation is not" + " a valid GCS path");    thrown.expectCause(hasMessage(containsString("Expected a valid 'gs://' path")));    options.getStagingLocation();}
public void beam_f7141_0()
{    DataflowPipelineOptions options = PipelineOptionsFactory.as(DataflowPipelineOptions.class);    options.setProject("");    thrown.expect(IllegalArgumentException.class);    thrown.expectMessage("Error constructing default value for stagingLocation");    options.getStagingLocation();}
public void beam_f7149_0()
{    PCollectionView<Long> sideLong = pipeline.apply("LongSideInputVals", Create.of(-1L, -2L, -4L)).apply("SideLongView", Sum.longsGlobally().asSingletonView());    PCollectionView<List<String>> sideStrings = pipeline.apply("StringSideInputVals", Create.of("foo", "bar", "baz")).apply("SideStringsView", View.asList());    ParDo.SingleOutput<Integer, Long> originalTransform = ParDo.of(new ToLongFn()).withSideInputs(sideLong, sideStrings);    PCollection<? extends Integer> input = pipeline.apply(Create.of(1, 2, 3));    AppliedPTransform<PCollection<? extends Integer>, PCollection<Long>, ParDo.SingleOutput<Integer, Long>> application = AppliedPTransform.of("original", input.expand(), input.apply(originalTransform).expand(), originalTransform, pipeline);    PTransformReplacement<PCollection<? extends Integer>, PCollection<Long>> replacementTransform = factory.getReplacementTransform(application);    ParDoSingle<Integer, Long> parDoSingle = (ParDoSingle<Integer, Long>) replacementTransform.getTransform();    assertThat(parDoSingle.getSideInputs().values(), containsInAnyOrder(sideStrings, sideLong));}
public void beam_f7150_0()
{    DoFn<Integer, Long> originalFn = new ToLongFn();    ParDo.SingleOutput<Integer, Long> originalTransform = ParDo.of(originalFn);    PCollection<? extends Integer> input = pipeline.apply(Create.of(1, 2, 3));    AppliedPTransform<PCollection<? extends Integer>, PCollection<Long>, ParDo.SingleOutput<Integer, Long>> application = AppliedPTransform.of("original", input.expand(), input.apply(originalTransform).expand(), originalTransform, pipeline);    PTransformReplacement<PCollection<? extends Integer>, PCollection<Long>> replacementTransform = factory.getReplacementTransform(application);    ParDoSingle<Integer, Long> parDoSingle = (ParDoSingle<Integer, Long>) replacementTransform.getTransform();    assertThat(parDoSingle.getFn(), equalTo(originalTransform.getFn()));    assertThat(parDoSingle.getFn(), equalTo(originalFn));}
public void beam_f7151_0(ProcessContext ctxt)
{    ctxt.output(ctxt.element().longValue());}
public void beam_f7159_0() throws Exception
{    Pipeline p = Pipeline.create(options);    PCollection<Integer> pc = p.apply(Create.of(1, 2, 3));    PAssert.that(pc).containsInAnyOrder(1, 2, 3);    DataflowPipelineJob mockJob = Mockito.mock(DataflowPipelineJob.class);    when(mockJob.getState()).thenReturn(State.DONE);    when(mockJob.getProjectId()).thenReturn("test-project");    when(mockJob.getJobId()).thenReturn("test-job");    when(mockJob.waitUntilFinish(any(Duration.class), any(JobMessagesHandler.class))).thenAnswer(invocation -> {        JobMessage message = new JobMessage();        message.setMessageText("TransientError");        message.setTime(TimeUtil.toCloudTime(Instant.now()));        message.setMessageImportance("JOB_MESSAGE_ERROR");        ((JobMessagesHandler) invocation.getArguments()[1]).process(Arrays.asList(message));        return State.DONE;    });    DataflowRunner mockRunner = Mockito.mock(DataflowRunner.class);    when(mockRunner.run(any(Pipeline.class))).thenReturn(mockJob);    TestDataflowRunner runner = TestDataflowRunner.fromOptionsAndClient(options, mockClient);    when(mockClient.getJobMetrics(anyString())).thenReturn(generateMockMetricResponse(true, /* success */    true));    assertEquals(mockJob, runner.run(p, mockRunner));}
public void beam_f7160_0() throws Exception
{    Pipeline p = TestPipeline.create(options);    PCollection<Integer> pc = p.apply(Create.of(1, 2, 3));    PAssert.that(pc).containsInAnyOrder(1, 2, 3);    DataflowPipelineJob mockJob = Mockito.mock(DataflowPipelineJob.class);    when(mockJob.getState()).thenReturn(State.FAILED);    when(mockJob.getProjectId()).thenReturn("test-project");    when(mockJob.getJobId()).thenReturn("test-job");    DataflowRunner mockRunner = Mockito.mock(DataflowRunner.class);    when(mockRunner.run(any(Pipeline.class))).thenReturn(mockJob);    TestDataflowRunner runner = TestDataflowRunner.fromOptionsAndClient(options, mockClient);    when(mockClient.getJobMetrics(anyString())).thenReturn(generateMockMetricResponse(true, /* success */    false));    expectedException.expect(RuntimeException.class);    runner.run(p, mockRunner);            fail("AssertionError expected");}
public void beam_f7161_0() throws Exception
{    Pipeline p = TestPipeline.create(options);    PCollection<Integer> pc = p.apply(Create.of(1, 2, 3));    PAssert.that(pc).containsInAnyOrder(1, 2, 3);    DataflowPipelineJob mockJob = Mockito.mock(DataflowPipelineJob.class);    when(mockJob.getState()).thenReturn(State.RUNNING);    when(mockJob.getProjectId()).thenReturn("test-project");    when(mockJob.getJobId()).thenReturn("test-job");    when(mockJob.waitUntilFinish(any(Duration.class), any(JobMessagesHandler.class))).thenAnswer(invocation -> {        JobMessage message = new JobMessage();        message.setMessageText("FooException");        message.setTime(TimeUtil.toCloudTime(Instant.now()));        message.setMessageImportance("JOB_MESSAGE_ERROR");        ((JobMessagesHandler) invocation.getArguments()[1]).process(Arrays.asList(message));        return State.CANCELLED;    });    DataflowRunner mockRunner = Mockito.mock(DataflowRunner.class);    when(mockRunner.run(any(Pipeline.class))).thenReturn(mockJob);    when(mockClient.getJobMetrics(anyString())).thenReturn(generateMockMetricResponse(false, /* success */    true));    TestDataflowRunner runner = TestDataflowRunner.fromOptionsAndClient(options, mockClient);    try {        runner.run(p, mockRunner);    } catch (AssertionError expected) {        assertThat(expected.getMessage(), containsString("FooException"));        verify(mockJob, never()).cancel();        return;    }            fail("AssertionError expected");}
private JobMetrics beam_f7169_0(List<MetricUpdate> metricList)
{    JobMetrics jobMetrics = new JobMetrics();    jobMetrics.setMetrics(metricList);        jobMetrics.setFactory(Transport.getJsonFactory());    return jobMetrics;}
public void beam_f7170_0() throws Exception
{    DataflowPipelineJob job = spy(new DataflowPipelineJob(mockClient, "test-job", options, null));    Pipeline p = TestPipeline.create(options);    PCollection<Integer> pc = p.apply(Create.of(1, 2, 3));    PAssert.that(pc).containsInAnyOrder(1, 2, 3);    when(mockClient.getJobMetrics(anyString())).thenReturn(buildJobMetrics(generateMockMetrics(true, /* success */    true)));    TestDataflowRunner runner = TestDataflowRunner.fromOptionsAndClient(options, mockClient);    doReturn(State.DONE).when(job).getState();    assertThat(runner.checkForPAssertSuccess(job), equalTo(Optional.of(true)));}
public void beam_f7171_0() throws Exception
{    DataflowPipelineJob job = spy(new DataflowPipelineJob(mockClient, "test-job", options, null));    Pipeline p = TestPipeline.create(options);    PCollection<Integer> pc = p.apply(Create.of(1, 2, 3));    PAssert.that(pc).containsInAnyOrder(1, 2, 3);    when(mockClient.getJobMetrics(anyString())).thenReturn(buildJobMetrics(generateMockMetrics(false, /* success */    true)));    TestDataflowRunner runner = TestDataflowRunner.fromOptionsAndClient(options, mockClient);    doReturn(State.DONE).when(job).getState();    assertThat(runner.checkForPAssertSuccess(job), equalTo(Optional.of(false)));}
public void beam_f7179_0() throws Exception
{    options.setStreaming(true);    Pipeline p = TestPipeline.create(options);    PCollection<Integer> pc = p.apply(Create.of(1, 2, 3));    PAssert.that(pc).containsInAnyOrder(1, 2, 3);    final DataflowPipelineJob mockJob = Mockito.mock(DataflowPipelineJob.class);    when(mockJob.getState()).thenReturn(State.DONE);    when(mockJob.getProjectId()).thenReturn("test-project");    when(mockJob.getJobId()).thenReturn("test-job");    DataflowRunner mockRunner = Mockito.mock(DataflowRunner.class);    when(mockRunner.run(any(Pipeline.class))).thenReturn(mockJob);    TestDataflowRunner runner = TestDataflowRunner.fromOptionsAndClient(options, mockClient);    options.as(TestPipelineOptions.class).setOnSuccessMatcher(new TestSuccessMatcher(mockJob, 1));    when(mockJob.waitUntilFinish(any(Duration.class), any(JobMessagesHandler.class))).thenReturn(State.DONE);    when(mockClient.getJobMetrics(anyString())).thenReturn(generateMockMetricResponse(true, /* success */    true));    runner.run(p, mockRunner);}
public void beam_f7180_0() throws Exception
{    Pipeline p = TestPipeline.create(options);    PCollection<Integer> pc = p.apply(Create.of(1, 2, 3));    PAssert.that(pc).containsInAnyOrder(1, 2, 3);    final DataflowPipelineJob mockJob = Mockito.mock(DataflowPipelineJob.class);    when(mockJob.getState()).thenReturn(State.FAILED);    when(mockJob.getProjectId()).thenReturn("test-project");    when(mockJob.getJobId()).thenReturn("test-job");    DataflowRunner mockRunner = Mockito.mock(DataflowRunner.class);    when(mockRunner.run(any(Pipeline.class))).thenReturn(mockJob);    TestDataflowRunner runner = TestDataflowRunner.fromOptionsAndClient(options, mockClient);    options.as(TestPipelineOptions.class).setOnSuccessMatcher(new TestFailureMatcher());    when(mockClient.getJobMetrics(anyString())).thenReturn(generateMockMetricResponse(false, /* success */    true));    try {        runner.run(p, mockRunner);    } catch (AssertionError expected) {        verify(mockJob, Mockito.times(1)).waitUntilFinish(any(Duration.class), any(JobMessagesHandler.class));        return;    }    fail("Expected an exception on pipeline failure.");}
public void beam_f7181_0() throws Exception
{    options.setStreaming(true);    Pipeline p = TestPipeline.create(options);    PCollection<Integer> pc = p.apply(Create.of(1, 2, 3));    PAssert.that(pc).containsInAnyOrder(1, 2, 3);    final DataflowPipelineJob mockJob = Mockito.mock(DataflowPipelineJob.class);    when(mockJob.getState()).thenReturn(State.FAILED);    when(mockJob.getProjectId()).thenReturn("test-project");    when(mockJob.getJobId()).thenReturn("test-job");    DataflowRunner mockRunner = Mockito.mock(DataflowRunner.class);    when(mockRunner.run(any(Pipeline.class))).thenReturn(mockJob);    TestDataflowRunner runner = TestDataflowRunner.fromOptionsAndClient(options, mockClient);    options.as(TestPipelineOptions.class).setOnSuccessMatcher(new TestFailureMatcher());    when(mockJob.waitUntilFinish(any(Duration.class), any(JobMessagesHandler.class))).thenReturn(State.FAILED);    expectedException.expect(RuntimeException.class);    runner.run(p, mockRunner);}
public void beam_f7191_0()
{    MockitoAnnotations.initMocks(this);}
private Pipeline beam_f7192_0()
{    DataflowPipelineOptions options = PipelineOptionsFactory.as(DataflowPipelineOptions.class);    options.setRunner(DataflowRunner.class);    options.setProject("someproject");    options.setGcpTempLocation("gs://staging");    options.setPathValidatorClass(NoopPathValidator.class);    options.setDataflowClient(dataflow);    return Pipeline.create(options);}
private Pipeline beam_f7193_0()
{    DataflowPipelineOptions options = PipelineOptionsFactory.as(DataflowPipelineOptions.class);    options.setRunner(DataflowRunner.class);    options.setStreaming(true);    options.setProject("someproject");    options.setGcpTempLocation("gs://staging");    options.setPathValidatorClass(NoopPathValidator.class);    options.setDataflowClient(dataflow);    return Pipeline.create(options);}
public void beam_f7201_0()
{    testViewUnbounded(createTestBatchRunner(), View.asList());}
public void beam_f7202_0()
{    testViewUnbounded(createTestStreamingRunner(), View.asList());}
public void beam_f7203_0()
{    testViewUnbounded(createTestBatchRunner(), View.asMap());}
public void beam_f7211_0()
{    testViewNonmerging(createTestBatchRunner(), View.asList());}
public void beam_f7212_0()
{    testViewNonmerging(createTestStreamingRunner(), View.asList());}
public void beam_f7213_0()
{    testViewNonmerging(createTestBatchRunner(), View.asMap());}
public void beam_f7221_0() throws Exception
{    SdkComponents sdkComponents = SdkComponents.create();    CloudObject cloudObject = CloudObjects.asCloudObject(coder, sdkComponents);    Coder<?> fromCloudObject = CloudObjects.coderFromCloudObject(cloudObject);    assertEquals(coder.getClass(), fromCloudObject.getClass());    assertEquals(coder, fromCloudObject);    checkPipelineProtoCoderIds(coder, cloudObject, sdkComponents);}
private static void beam_f7222_0(Coder<?> coder, CloudObject cloudObject, SdkComponents sdkComponents) throws Exception
{    if (ModelCoderRegistrar.isKnownCoder(coder)) {        assertFalse(cloudObject.containsKey(PropertyNames.PIPELINE_PROTO_CODER_ID));    } else {        assertTrue(cloudObject.containsKey(PropertyNames.PIPELINE_PROTO_CODER_ID));        assertEquals(sdkComponents.registerCoder(coder), cloudObject.get(PropertyNames.PIPELINE_PROTO_CODER_ID));    }    List<? extends Coder<?>> coderArguments = coder.getCoderArguments();    Object cloudComponentsObject = cloudObject.get(PropertyNames.COMPONENT_ENCODINGS);    assertTrue(cloudComponentsObject instanceof List);    List<CloudObject> cloudComponents = (List<CloudObject>) cloudComponentsObject;    assertEquals(coderArguments.size(), cloudComponents.size());    for (int i = 0; i < coderArguments.size(); i++) {        checkPipelineProtoCoderIds(coderArguments.get(i), cloudComponents.get(i), sdkComponents);    }}
public Object beam_f7224_0(InputStream inStream) throws CoderException, IOException
{    return new Object();}
public void beam_f7234_0()
{    assertEquals(State.UNRECOGNIZED, MonitoringUtil.toState(null));}
public void beam_f7235_0()
{    assertEquals(State.UNRECOGNIZED, MonitoringUtil.toState("FOO_BAR_BAZ"));}
public void beam_f7236_0()
{    DataflowPipelineOptions options = PipelineOptionsFactory.create().as(DataflowPipelineOptions.class);    options.setProject(PROJECT_ID);    options.setRegion(REGION_ID);    options.setGcpCredential(new TestCredential());    String cancelCommand = MonitoringUtil.getGcloudCancelCommand(options, JOB_ID);    assertEquals("gcloud dataflow jobs --project=someProject cancel --region=thatRegion 1234", cancelCommand);}
public void beam_f7244_0() throws Exception
{    File tmpFile = makeFileWithContents("file", "This is a test!");    DataflowPackage target = makePackageAttributes(tmpFile, null).getDestination();    assertThat(target.getName(), RegexMatcher.matches("file-" + HASH_PATTERN));    assertThat(target.getLocation(), equalTo(STAGING_PATH + target.getName()));}
public void beam_f7245_0() throws Exception
{    File tmpDirectory = tmpFolder.newFolder("folder");    DataflowPackage target = makePackageAttributes(tmpDirectory, null).getDestination();    assertThat(target.getName(), RegexMatcher.matches("folder-" + HASH_PATTERN + ".jar"));    assertThat(target.getLocation(), equalTo(STAGING_PATH + target.getName()));}
public void beam_f7246_0() throws Exception
{    File tmpDirectory1 = tmpFolder.newFolder("folder1", "folderA");    makeFileWithContents("folder1/folderA/sameName", "This is a test!");    DataflowPackage target1 = makePackageAttributes(tmpDirectory1, null).getDestination();    File tmpDirectory2 = tmpFolder.newFolder("folder2", "folderA");    makeFileWithContents("folder2/folderA/sameName", "This is a test!");    DataflowPackage target2 = makePackageAttributes(tmpDirectory2, null).getDestination();    assertEquals(target1.getName(), target2.getName());    assertEquals(target1.getLocation(), target2.getLocation());}
public void beam_f7254_0() throws Exception
{    File tmpFile = makeFileWithContents("file.txt", "This is a test!");    when(mockGcsUtil.getObjects(anyListOf(GcsPath.class))).thenReturn(ImmutableList.of(StorageObjectOrIOException.create(new FileNotFoundException("some/path"))));    when(mockGcsUtil.create(any(GcsPath.class), anyString())).thenThrow(new IOException("Fake Exception: Upload error"));    try (PackageUtil directPackageUtil = PackageUtil.withExecutorService(MoreExecutors.newDirectExecutorService())) {        directPackageUtil.stageClasspathElements(ImmutableList.of(tmpFile.getAbsolutePath()), STAGING_PATH, fastNanoClockAndSleeper, createOptions);    } finally {        verify(mockGcsUtil).getObjects(anyListOf(GcsPath.class));        verify(mockGcsUtil, times(5)).create(any(GcsPath.class), anyString());        verifyNoMoreInteractions(mockGcsUtil);    }}
public void beam_f7255_0() throws Exception
{    File tmpFile = makeFileWithContents("file.txt", "This is a test!");    when(mockGcsUtil.getObjects(anyListOf(GcsPath.class))).thenReturn(ImmutableList.of(StorageObjectOrIOException.create(new FileNotFoundException("some/path"))));    when(mockGcsUtil.create(any(GcsPath.class), anyString())).thenThrow(new IOException("Failed to write to GCS path " + STAGING_PATH, googleJsonResponseException(HttpStatusCodes.STATUS_CODE_FORBIDDEN, "Permission denied", "Test message")));    try (PackageUtil directPackageUtil = PackageUtil.withExecutorService(MoreExecutors.newDirectExecutorService())) {        directPackageUtil.stageClasspathElements(ImmutableList.of(tmpFile.getAbsolutePath()), STAGING_PATH, fastNanoClockAndSleeper, createOptions);        fail("Expected RuntimeException");    } catch (RuntimeException e) {        assertThat("Expected RuntimeException wrapping IOException.", e.getCause(), instanceOf(RuntimeException.class));        assertThat("Expected IOException containing detailed message.", e.getCause().getCause(), instanceOf(IOException.class));        assertThat(e.getCause().getCause().getMessage(), Matchers.allOf(Matchers.containsString("Uploaded failed due to permissions error"), Matchers.containsString("Stale credentials can be resolved by executing 'gcloud auth application-default " + "login'")));    } finally {        verify(mockGcsUtil).getObjects(anyListOf(GcsPath.class));        verify(mockGcsUtil).create(any(GcsPath.class), anyString());        verifyNoMoreInteractions(mockGcsUtil);    }}
public void beam_f7256_0() throws Exception
{    Pipe pipe = Pipe.open();    File tmpFile = makeFileWithContents("file.txt", "This is a test!");    when(mockGcsUtil.getObjects(anyListOf(GcsPath.class))).thenReturn(ImmutableList.of(StorageObjectOrIOException.create(new FileNotFoundException("some/path"))));    when(mockGcsUtil.create(any(GcsPath.class), anyString())).thenThrow(    new IOException("Fake Exception: 410 Gone")).thenReturn(    pipe.sink());    try (PackageUtil directPackageUtil = PackageUtil.withExecutorService(MoreExecutors.newDirectExecutorService())) {        directPackageUtil.stageClasspathElements(ImmutableList.of(tmpFile.getAbsolutePath()), STAGING_PATH, fastNanoClockAndSleeper, createOptions);    } finally {        verify(mockGcsUtil).getObjects(anyListOf(GcsPath.class));        verify(mockGcsUtil, times(2)).create(any(GcsPath.class), anyString());        verifyNoMoreInteractions(mockGcsUtil);    }}
public void beam_f7264_0() throws Exception
{    RandomAccessData streamA = new RandomAccessData();    streamA.asOutputStream().write(TEST_DATA_A);    RandomAccessData streamB = new RandomAccessData();    streamB.asOutputStream().write(TEST_DATA_A);    CoderProperties.coderDecodeEncodeEqual(RandomAccessDataCoder.of(), streamA);    CoderProperties.coderDeterministic(RandomAccessDataCoder.of(), streamA, streamB);    CoderProperties.coderConsistentWithEquals(RandomAccessDataCoder.of(), streamA, streamB);    CoderProperties.coderSerializable(RandomAccessDataCoder.of());    CoderProperties.structuralValueConsistentWithEquals(RandomAccessDataCoder.of(), streamA, streamB);    assertTrue(RandomAccessDataCoder.of().isRegisterByteSizeObserverCheap(streamA));    assertEquals(4, RandomAccessDataCoder.of().getEncodedElementByteSize(streamA));}
public void beam_f7265_0() throws Exception
{    expectedException.expect(CoderException.class);    expectedException.expectMessage("Positive infinity can not be encoded");    RandomAccessDataCoder.of().encode(RandomAccessData.POSITIVE_INFINITY, new ByteArrayOutputStream(), Context.OUTER);}
public void beam_f7266_0() throws Exception
{    RandomAccessData streamA = new RandomAccessData();    streamA.asOutputStream().write(TEST_DATA_A);    RandomAccessData streamB = new RandomAccessData();    streamB.asOutputStream().write(TEST_DATA_B);    RandomAccessData streamC = new RandomAccessData();    streamC.asOutputStream().write(TEST_DATA_C);    assertTrue(RandomAccessData.UNSIGNED_LEXICOGRAPHICAL_COMPARATOR.compare(streamA, streamB) < 0);    assertTrue(RandomAccessData.UNSIGNED_LEXICOGRAPHICAL_COMPARATOR.compare(streamB, streamA) > 0);    assertTrue(RandomAccessData.UNSIGNED_LEXICOGRAPHICAL_COMPARATOR.compare(streamB, streamB) == 0);        assertEquals(2, RandomAccessData.UNSIGNED_LEXICOGRAPHICAL_COMPARATOR.commonPrefixLength(streamB, streamC));        assertTrue(RandomAccessData.UNSIGNED_LEXICOGRAPHICAL_COMPARATOR.compare(streamB, streamC, 3) == 0);        assertTrue(RandomAccessData.UNSIGNED_LEXICOGRAPHICAL_COMPARATOR.compare(streamA, RandomAccessData.POSITIVE_INFINITY) < 0);    assertTrue(RandomAccessData.UNSIGNED_LEXICOGRAPHICAL_COMPARATOR.compare(RandomAccessData.POSITIVE_INFINITY, RandomAccessData.POSITIVE_INFINITY) == 0);    assertTrue(RandomAccessData.UNSIGNED_LEXICOGRAPHICAL_COMPARATOR.compare(RandomAccessData.POSITIVE_INFINITY, streamA) > 0);}
public void beam_f7274_0() throws Exception
{    assertEquals(new RandomAccessData(new byte[] { 0x00, 0x01 }), new RandomAccessData(new byte[] { 0x00, 0x00 }).increment());    assertEquals(new RandomAccessData(new byte[] { 0x01, UnsignedBytes.MAX_VALUE }), new RandomAccessData(new byte[] { 0x00, UnsignedBytes.MAX_VALUE }).increment());        assertSame(RandomAccessData.POSITIVE_INFINITY, new RandomAccessData(new byte[0]).increment());    assertSame(RandomAccessData.POSITIVE_INFINITY, new RandomAccessData(new byte[] { UnsignedBytes.MAX_VALUE }).increment());    assertSame(RandomAccessData.POSITIVE_INFINITY, RandomAccessData.POSITIVE_INFINITY.increment());}
private List<Map<String, Object>> beam_f7275_0()
{    List<Map<String, Object>> objects = new ArrayList<>();    {        CloudObject o = CloudObject.forClassName("string");        addString(o, "singletonStringKey", "stringValue");        objects.add(o);    }    {        CloudObject o = CloudObject.forClassName("long");        addLong(o, "singletonLongKey", 42L);        objects.add(o);    }    return objects;}
private Map<String, Object> beam_f7276_0()
{    Map<String, Object> o = new HashMap<>();    addList(o, "emptyKey", Collections.emptyList());    addNull(o, "noStringsKey");    addString(o, "singletonStringKey", "stringValue");    addStringList(o, "multipleStringsKey", Arrays.asList("hi", "there", "bob"));    addLongs(o, "multipleLongsKey", 47L, 1L << 42, -5L);    addLong(o, "singletonLongKey", 42L);    addDouble(o, "singletonDoubleKey", Math.PI);    addBoolean(o, "singletonBooleanKey", true);    addNull(o, "noObjectsKey");    addList(o, "multipleObjectsKey", makeCloudObjects());    return o;}
public void beam_f7284_0()
{    assertEquals(Duration.millis(4000), fromCloudDuration("4s"));    assertEquals(Duration.millis(4001), fromCloudDuration("4.001s"));    assertEquals(Duration.millis(4001), fromCloudDuration("4.001000s"));    assertEquals(Duration.millis(4001), fromCloudDuration("4.001001s"));    assertEquals(Duration.millis(4001), fromCloudDuration("4.001000000s"));    assertEquals(Duration.millis(4001), fromCloudDuration("4.001000001s"));    assertNull(fromCloudDuration(""));    assertNull(fromCloudDuration("4"));    assertNull(fromCloudDuration("4.1"));    assertNull(fromCloudDuration("4.1s"));}
public static void beam_f7285_0(String[] args) throws Exception
{    org.apache.beam.runners.dataflow.worker.DataflowRunnerHarness.main(args);}
public static List<V> beam_f7286_0(List<V> list)
{    return list == null ? ImmutableList.<V>of() : list;}
protected void beam_f7294_0()
{    destroySynchronized();}
private synchronized void beam_f7295_0()
{    if (!destroyed) {        destroy();        destroyed = true;        nativePointer = 0;    }}
protected void beam_f7296_0()
{    destroy();}
public void beam_f7305_0() throws Exception
{    receiver = null;}
public AvroByteFileIterator beam_f7306_0() throws IOException
{    BoundedSource.BoundedReader<ByteBuffer> reader;    if (startPosition == 0 && endPosition == Long.MAX_VALUE) {                reader = avroSource.createReader(options);    } else {                reader = avroSource.createForSubrangeOfFile(FileSystems.matchSingleFileSpec(filename), startPosition, endPosition).createReader(options);    }    return new AvroByteFileIterator((AvroReader<ByteBuffer>) reader);}
public boolean beam_f7307_0() throws IOException
{    if (!reader.start()) {        current = Optional.empty();        return false;    }    updateCurrent();    return true;}
public NativeReader<?> beam_f7315_0(CloudObject spec, @Nullable Coder<?> coder, @Nullable PipelineOptions options, @Nullable DataflowExecutionContext executionContext, DataflowOperationContext operationContext) throws Exception
{    checkArgument(coder != null, "coder must not be null");    checkArgument(options != null, "options must not be null");    return create(spec, coder, options);}
 NativeReader<?> beam_f7316_0(CloudObject spec, Coder<?> coder, PipelineOptions options) throws Exception
{    String filename = getString(spec, WorkerPropertyNames.FILENAME);    long startOffset = getLong(spec, WorkerPropertyNames.START_OFFSET, 0L);    long endOffset = getLong(spec, WorkerPropertyNames.END_OFFSET, Long.MAX_VALUE);    return new AvroByteReader<>(filename, startOffset, endOffset, coder, options);}
public SinkWriter<T> beam_f7317_0() throws IOException
{    return new AvroByteFileWriter();}
private static DebugCapture.Manager beam_f7325_0(DataflowWorkerHarnessOptions options, List<DebugCapture.Capturable> debugCapturePages)
{    DebugCapture.Manager result = new DebugCapture.Manager(options, debugCapturePages);    result.start();    return result;}
private static Thread beam_f7326_0(MemoryMonitor memoryMonitor)
{    Thread result = new Thread(memoryMonitor);    result.setDaemon(true);    result.setPriority(Thread.MIN_PRIORITY);    result.setName("MemoryMonitor");    result.start();    return result;}
private Node beam_f7327_0()
{    return RemoteGrpcPortNode.create(RemoteGrpcPort.newBuilder().setApiServiceDescriptor(sdkHarnessRegistry.beamFnDataApiServiceDescriptor()).build(), idGenerator.getId());}
public static BatchModeExecutionContext beam_f7335_0(PipelineOptions options, CounterFactory counterFactory, String stageName)
{    BatchModeExecutionStateRegistry stateRegistry = new BatchModeExecutionStateRegistry();    return new BatchModeExecutionContext(counterFactory, CacheBuilder.newBuilder().maximumWeight(    1_000_000).weigher(Weighers.fixedWeightKeys(8)).softValues().recordStats().<Object, WeightedValue<?>>build(), CacheBuilder.newBuilder().weakValues().build(), ReaderRegistry.defaultRegistry(), options, new DataflowExecutionStateTracker(ExecutionStateSampler.newForTest(), stateRegistry.getState(NameContext.forStage(stageName), "other", null, ScopedProfiler.INSTANCE.emptyScope()), counterFactory, options, "test-work-item-id"), stateRegistry);}
public static BatchModeExecutionContext beam_f7336_0(PipelineOptions options, String stageName)
{    CounterFactory counterFactory = new CounterFactory();    return forTesting(options, counterFactory, stageName);}
public void beam_f7337_0(long millisSinceLastSample)
{    totalMillisInState += millisSinceLastSample;}
protected SideInputReader beam_f7345_0(Iterable<? extends SideInputInfo> sideInputInfos, DataflowOperationContext operationContext) throws Exception
{    return new LazilyInitializedSideInputReader(sideInputInfos, () -> {        try {            return IsmSideInputReader.of(sideInputInfos, options, BatchModeExecutionContext.this, readerFactory, operationContext);        } catch (Exception e) {            throw new RuntimeException(e);        }    });}
protected SideInputReader beam_f7346_0(Iterable<? extends PCollectionView<?>> sideInputViews)
{    throw new UnsupportedOperationException("Cannot call getSideInputReaderForViews for batch DataflowWorker: " + "the MapTask specification should have had SideInputInfo descriptors " + "for each side input, and a SideInputReader provided via getSideInputReader");}
public Cache<K, V> beam_f7347_0()
{    @SuppressWarnings({ "unchecked", "rawtypes" })    Cache<K, V> rval = (Cache) dataCache;    return rval;}
public DataflowStepContext beam_f7356_0()
{    return new UserStepContext(this);}
public StateInternals beam_f7357_0()
{    return wrapped.getUserStateInternals();}
public TimerInternals beam_f7358_0()
{    return wrapped.getUserTimerInternals();}
public void beam_f7366_0(Receiver... receivers) throws Exception
{    underlyingParDoFn.startBundle(receivers);}
public void beam_f7367_0(Object untypedElem) throws Exception
{    WindowedValue<?> windowedValue = (WindowedValue<?>) untypedElem;    KV<K, Iterable<KV<Instant, WindowedValue<V>>>> gbkElem = (KV<K, Iterable<KV<Instant, WindowedValue<V>>>>) windowedValue.getValue();        stepContext.setKey(gbkElem.getKey());    for (KV<Instant, WindowedValue<V>> timestampedElem : gbkElem.getValue()) {        underlyingParDoFn.processElement(timestampedElem.getValue());    }        underlyingParDoFn.processTimers();}
public void beam_f7369_0() throws Exception
{    underlyingParDoFn.finishBundle();}
private Function<Node, Node> beam_f7377_0(final Network<Node, Edge> network, final String stageName, final DataflowExecutionContext<?> executionContext, final JobBundleFactory jobBundleFactory)
{    return new TypeSafeNodeFunction<ExecutableStageNode>(ExecutableStageNode.class) {        @Override        public Node typedApply(ExecutableStageNode input) {            StageBundleFactory stageBundleFactory = jobBundleFactory.forStage(input.getExecutableStage());            Iterable<OutputReceiverNode> outputReceiverNodes = Iterables.filter(network.successors(input), OutputReceiverNode.class);            Map<String, OutputReceiver> outputReceiverMap = new HashMap<>();            Lists.newArrayList(outputReceiverNodes).stream().forEach(outputReceiverNode -> outputReceiverMap.put(outputReceiverNode.getPcollectionId(), outputReceiverNode.getOutputReceiver()));            DataflowOperationContext operationContextStage = executionContext.createOperationContext(NameContext.create(stageName, stageName, stageName, stageName));            TimerReceiver timerReceiver = new TimerReceiver(input.getExecutableStage().getComponents(), executionContext.getStepContext(operationContextStage).namespacedToUser(), stageBundleFactory);            ImmutableMap.Builder<String, DataflowOperationContext> ptransformIdToOperationContextBuilder = ImmutableMap.builder();            for (Map.Entry<String, NameContext> entry : input.getPTransformIdToPartialNameContextMap().entrySet()) {                NameContext fullNameContext = NameContext.create(stageName, entry.getValue().originalName(), entry.getValue().systemName(), entry.getValue().userName());                DataflowOperationContext operationContext = executionContext.createOperationContext(fullNameContext);                ptransformIdToOperationContextBuilder.put(entry.getKey(), operationContext);            }            ImmutableMap<String, DataflowOperationContext> ptransformIdToOperationContexts = ptransformIdToOperationContextBuilder.build();            ImmutableMap<String, SideInputReader> ptransformIdToSideInputReaders = buildPTransformIdToSideInputReadersMap(executionContext, input, ptransformIdToOperationContexts);            Map<RunnerApi.ExecutableStagePayload.SideInputId, PCollectionView<?>> ptransformIdToSideInputIdToPCollectionView = buildSideInputIdToPCollectionView(input);            return OperationNode.create(new ProcessRemoteBundleOperation(input.getExecutableStage(), executionContext.createOperationContext(NameContext.create(stageName, stageName, stageName, stageName)), stageBundleFactory, outputReceiverMap, timerReceiver, ptransformIdToSideInputReaders, ptransformIdToSideInputIdToPCollectionView));        }    };}
public Node beam_f7378_0(ExecutableStageNode input)
{    StageBundleFactory stageBundleFactory = jobBundleFactory.forStage(input.getExecutableStage());    Iterable<OutputReceiverNode> outputReceiverNodes = Iterables.filter(network.successors(input), OutputReceiverNode.class);    Map<String, OutputReceiver> outputReceiverMap = new HashMap<>();    Lists.newArrayList(outputReceiverNodes).stream().forEach(outputReceiverNode -> outputReceiverMap.put(outputReceiverNode.getPcollectionId(), outputReceiverNode.getOutputReceiver()));    DataflowOperationContext operationContextStage = executionContext.createOperationContext(NameContext.create(stageName, stageName, stageName, stageName));    TimerReceiver timerReceiver = new TimerReceiver(input.getExecutableStage().getComponents(), executionContext.getStepContext(operationContextStage).namespacedToUser(), stageBundleFactory);    ImmutableMap.Builder<String, DataflowOperationContext> ptransformIdToOperationContextBuilder = ImmutableMap.builder();    for (Map.Entry<String, NameContext> entry : input.getPTransformIdToPartialNameContextMap().entrySet()) {        NameContext fullNameContext = NameContext.create(stageName, entry.getValue().originalName(), entry.getValue().systemName(), entry.getValue().userName());        DataflowOperationContext operationContext = executionContext.createOperationContext(fullNameContext);        ptransformIdToOperationContextBuilder.put(entry.getKey(), operationContext);    }    ImmutableMap<String, DataflowOperationContext> ptransformIdToOperationContexts = ptransformIdToOperationContextBuilder.build();    ImmutableMap<String, SideInputReader> ptransformIdToSideInputReaders = buildPTransformIdToSideInputReadersMap(executionContext, input, ptransformIdToOperationContexts);    Map<RunnerApi.ExecutableStagePayload.SideInputId, PCollectionView<?>> ptransformIdToSideInputIdToPCollectionView = buildSideInputIdToPCollectionView(input);    return OperationNode.create(new ProcessRemoteBundleOperation(input.getExecutableStage(), executionContext.createOperationContext(NameContext.create(stageName, stageName, stageName, stageName)), stageBundleFactory, outputReceiverMap, timerReceiver, ptransformIdToSideInputReaders, ptransformIdToSideInputIdToPCollectionView));}
private Function<Node, Node> beam_f7379_0(final IdGenerator idGenerator, final InstructionRequestHandler instructionRequestHandler, final StateDelegator beamFnStateDelegator, final String stageName, final DataflowExecutionContext<?> executionContext)
{    return new TypeSafeNodeFunction<RegisterRequestNode>(RegisterRequestNode.class) {        @Override        public Node typedApply(RegisterRequestNode input) {            ImmutableMap.Builder<String, DataflowOperationContext> ptransformIdToOperationContextBuilder = ImmutableMap.builder();            ImmutableMap.Builder<String, DataflowStepContext> ptransformIdToStepContext = ImmutableMap.builder();            for (Map.Entry<String, NameContext> entry : input.getPTransformIdToPartialNameContextMap().entrySet()) {                NameContext fullNameContext = NameContext.create(stageName, entry.getValue().originalName(), entry.getValue().systemName(), entry.getValue().userName());                DataflowOperationContext operationContext = executionContext.createOperationContext(fullNameContext);                ptransformIdToOperationContextBuilder.put(entry.getKey(), operationContext);                ptransformIdToStepContext.put(entry.getKey(), executionContext.getStepContext(operationContext));            }            ImmutableMap.Builder<String, NameContext> pcollectionIdToNameContext = ImmutableMap.builder();            for (Map.Entry<String, NameContext> entry : input.getPCollectionToPartialNameContextMap().entrySet()) {                pcollectionIdToNameContext.put(entry.getKey(), NameContext.create(stageName, entry.getValue().originalName(), entry.getValue().systemName(), entry.getValue().userName()));            }            ImmutableMap<String, DataflowOperationContext> ptransformIdToOperationContexts = ptransformIdToOperationContextBuilder.build();            ImmutableMap<String, SideInputReader> ptransformIdToSideInputReaders = buildPTransformIdToSideInputReadersMap(executionContext, input, ptransformIdToOperationContexts);            ImmutableTable<String, String, PCollectionView<?>> ptransformIdToSideInputIdToPCollectionView = buildPTransformIdToSideInputIdToPCollectionView(input);            return OperationNode.create(new RegisterAndProcessBundleOperation(idGenerator, instructionRequestHandler, beamFnStateDelegator, input.getRegisterRequest(), ptransformIdToOperationContexts, ptransformIdToStepContext.build(), ptransformIdToSideInputReaders, ptransformIdToSideInputIdToPCollectionView, pcollectionIdToNameContext.build(),             executionContext.createOperationContext(NameContext.create(stageName, stageName, stageName, stageName))));        }    };}
 OperationNode beam_f7387_0(Network<Node, Edge> network, ParallelInstructionNode node, PipelineOptions options, ReaderFactory readerFactory, DataflowExecutionContext<?> executionContext, DataflowOperationContext operationContext) throws Exception
{    ParallelInstruction instruction = node.getParallelInstruction();    ReadInstruction read = instruction.getRead();    Source cloudSource = CloudSourceUtils.flattenBaseSpecs(read.getSource());    CloudObject sourceSpec = CloudObject.fromSpec(cloudSource.getSpec());    Coder<?> coder = CloudObjects.coderFromCloudObject(CloudObject.fromSpec(cloudSource.getCodec()));    NativeReader<?> reader = readerFactory.create(sourceSpec, coder, options, executionContext, operationContext);    OutputReceiver[] receivers = getOutputReceivers(network, node);    return OperationNode.create(ReadOperation.create(reader, receivers, operationContext));}
 OperationNode beam_f7388_0(ParallelInstructionNode node, PipelineOptions options, SinkFactory sinkFactory, DataflowExecutionContext executionContext, DataflowOperationContext context) throws Exception
{    ParallelInstruction instruction = node.getParallelInstruction();    WriteInstruction write = instruction.getWrite();    Coder<?> coder = CloudObjects.coderFromCloudObject(CloudObject.fromSpec(write.getSink().getCodec()));    CloudObject cloudSink = CloudObject.fromSpec(write.getSink().getSpec());    Sink<?> sink = sinkFactory.create(cloudSink, coder, options, executionContext, context);    return OperationNode.create(WriteOperation.create(sink, EMPTY_OUTPUT_RECEIVER_ARRAY, context));}
private OperationNode beam_f7389_0(Network<Node, Edge> network, ParallelInstructionNode node, PipelineOptions options, DataflowExecutionContext<?> executionContext, DataflowOperationContext operationContext) throws Exception
{    ParallelInstruction instruction = node.getParallelInstruction();    ParDoInstruction parDo = instruction.getParDo();    TupleTag<?> mainOutputTag = tupleTag(parDo.getMultiOutputInfos().get(0));    ImmutableMap.Builder<TupleTag<?>, Integer> outputTagsToReceiverIndicesBuilder = ImmutableMap.builder();    int successorOffset = 0;    for (Node successor : network.successors(node)) {        for (Edge edge : network.edgesConnecting(node, successor)) {            outputTagsToReceiverIndicesBuilder.put(tupleTag(((MultiOutputInfoEdge) edge).getMultiOutputInfo()), successorOffset);        }        successorOffset += 1;    }    ParDoFn fn = parDoFnFactory.create(options, CloudObject.fromSpec(parDo.getUserFn()), parDo.getSideInputs(), mainOutputTag, outputTagsToReceiverIndicesBuilder.build(), executionContext, operationContext);    OutputReceiver[] receivers = getOutputReceivers(network, node);    return OperationNode.create(new ParDoOperation(fn, receivers, operationContext));}
public void beam_f7397_0(T value, ElementByteSizeObserver observer) throws Exception
{    coder.registerByteSizeObserver(value, observer);}
public static ByteStringCoder beam_f7398_0()
{    return INSTANCE;}
public void beam_f7399_0(ByteString value, OutputStream os) throws IOException
{    VarInt.encode(value.size(), os);    value.writeTo(os);}
public void beam_f7407_0(ProcessContext c, BoundedWindow window)
{    KV<K, Iterable<InputT>> kv = c.element();    c.output(KV.of(kv.getKey(), applyCombineFn(combineFnRunner, kv.getValue(), window, c.getPipelineOptions())));}
private OutputT beam_f7408_0(GlobalCombineFnRunner<InputT, AccumT, OutputT> combineFnRunner, Iterable<InputT> inputs, BoundedWindow window, PipelineOptions options)
{    List<BoundedWindow> windows = Collections.singletonList(window);    AccumT accum = combineFnRunner.createAccumulator(options, sideInputReader, windows);    for (InputT input : inputs) {        accum = combineFnRunner.addInput(accum, input, options, sideInputReader, windows);    }    return combineFnRunner.extractOutput(accum, options, sideInputReader, windows);}
private static DoFnInfo<?, ?> beam_f7409_0(AppliedCombineFn<K, InputT, AccumT, OutputT> combineFn, SideInputReader sideInputReader)
{    GlobalCombineFnRunner<InputT, AccumT, OutputT> combineFnRunner = GlobalCombineFnRunners.create(combineFn.getFn());    DoFn<KV<K, Iterable<InputT>>, KV<K, AccumT>> doFn = new AddInputsDoFn<>(combineFnRunner, sideInputReader);    Coder<KV<K, Iterable<InputT>>> inputCoder = null;    if (combineFn.getKvCoder() != null) {        inputCoder = KvCoder.of(combineFn.getKvCoder().getKeyCoder(), IterableCoder.of(combineFn.getKvCoder().getValueCoder()));    }    return DoFnInfo.forFn(doFn, combineFn.getWindowingStrategy(), combineFn.getSideInputViews(), inputCoder,     Collections.emptyMap(), new TupleTag<>(PropertyNames.OUTPUT), DoFnSchemaInformation.create(), Collections.emptyMap());}
public boolean beam_f7417_0() throws IOException
{    return advance();}
public boolean beam_f7418_0() throws IOException
{    while (true) {                if (currentIterator != null && currentIterator.advance()) {                        return true;        }                if (currentIterator != null) {            currentIterator.close();            currentIterator = null;        }        if (!rangeTracker.tryReturnRecordAt(true, currentIteratorIndex + 1)) {            return false;        }        currentIteratorIndex++;        if (currentIteratorIndex == sources.size()) {                        return false;        }        Source currentSource = sources.get(currentIteratorIndex);        try {            Coder<?> coder = null;            if (currentSource.getCodec() != null) {                coder = CloudObjects.coderFromCloudObject(CloudObject.fromSpec(currentSource.getCodec()));            }            @SuppressWarnings("unchecked")            NativeReader<T> currentReader = (NativeReader<T>) readerFactory.create(CloudObject.fromSpec(currentSource.getSpec()), coder, options, executionContext, operationContext);            currentIterator = currentReader.iterator();        } catch (Exception e) {            throw new IOException("Failed to create a reader for source: " + currentSource, e);        }        if (!currentIterator.start()) {            currentIterator.close();            currentIterator = null;            continue;        }                return true;    }}
public T beam_f7419_0() throws NoSuchElementException
{    if (currentIterator == null) {        throw new NoSuchElementException();    }    return currentIterator.getCurrent();}
private static List<Source> beam_f7427_0(CloudObject spec) throws Exception
{    List<Source> subSources = new ArrayList<>();        List<Map<String, Object>> subSourceDictionaries = getListOfMaps(spec, WorkerPropertyNames.CONCAT_SOURCE_SOURCES, null);    if (subSourceDictionaries == null) {        return subSources;    }    for (Map<String, Object> subSourceDictionary : subSourceDictionaries) {                subSources.add(createSourceFromDictionary(subSourceDictionary));    }    return subSources;}
public static Source beam_f7428_0(Map<String, Object> dictionary) throws Exception
{    Source source = new Source();        CloudObject subSourceSpec = CloudObject.fromSpec(getObject(dictionary, PropertyNames.SOURCE_SPEC));    source.setSpec(subSourceSpec);        CloudObject subSourceEncoding = CloudObject.fromSpec(getObject(dictionary, PropertyNames.ENCODING, null));    if (subSourceEncoding != null) {        source.setCodec(subSourceEncoding);    }        List<Map<String, Object>> subSourceBaseSpecs = getListOfMaps(dictionary, WorkerPropertyNames.CONCAT_SOURCE_BASE_SPECS, null);    if (subSourceBaseSpecs != null) {        source.setBaseSpecs(subSourceBaseSpecs);    }        SourceMetadata metadata = new SourceMetadata();    Boolean infinite = getBoolean(dictionary, PropertyNames.SOURCE_IS_INFINITE, null);    if (infinite != null) {        metadata.setInfinite(infinite);    }    Long estimatedSizeBytes = getLong(dictionary, PropertyNames.SOURCE_ESTIMATED_SIZE_BYTES, null);    if (estimatedSizeBytes != null) {        metadata.setEstimatedSizeBytes(estimatedSizeBytes);    }    if (estimatedSizeBytes != null || infinite != null) {        source.setMetadata(metadata);    }        Boolean doesNotNeedSplitting = getBoolean(dictionary, PropertyNames.SOURCE_DOES_NOT_NEED_SPLITTING, null);    if (doesNotNeedSplitting != null) {        source.setDoesNotNeedSplitting(doesNotNeedSplitting);    }    return source;}
public static ContextActivationObserverRegistry beam_f7429_0()
{    return new ContextActivationObserverRegistry();}
public boolean beam_f7437_0()
{    return commitState.get() != CommitState.COMMITTED;}
public boolean beam_f7438_0()
{    return commitState.compareAndSet(CommitState.DIRTY, CommitState.COMMITTING);}
public boolean beam_f7439_0()
{    return commitState.compareAndSet(CommitState.COMMITTING, CommitState.COMMITTED);}
public Counter<Long, Long> beam_f7447_0(CounterName name)
{    return createCounter(name, new LongSumCounterValue());}
public Counter<Long, Long> beam_f7448_0(CounterName name)
{    return createCounter(name, new LongMinCounterValue());}
public Counter<Long, Long> beam_f7449_0(CounterName name)
{    return createCounter(name, new LongMaxCounterValue());}
public Counter<Double, Double> beam_f7457_0(CounterName name)
{    return createCounter(name, new DoubleMaxCounterValue());}
public Counter<Double, CounterMean<Double>> beam_f7458_0(CounterName name)
{    return createCounter(name, new DoubleMeanCounterValue());}
public Counter<Boolean, Boolean> beam_f7459_0(CounterName name)
{    return createCounter(name, new BooleanOrCounterValue());}
 static int beam_f7467_0(long value)
{    if (value == 0) {        return 0;    }    int log10Floor = LongMath.log10(value, RoundingMode.FLOOR);    long powerOfTen = LongMath.pow(10, log10Floor);    int bucketOffsetWithinPowerOf10;    if (value < 2 * powerOfTen) {                bucketOffsetWithinPowerOf10 = 0;    } else if (value < 5 * powerOfTen) {                bucketOffsetWithinPowerOf10 = 1;    } else {                bucketOffsetWithinPowerOf10 = 2;    }    return 1 + (log10Floor * BUCKETS_PER_10) + bucketOffsetWithinPowerOf10;}
private List<Long> beam_f7468_0(int bucketIndex)
{    int firstBucketOffset = getFirstBucketOffset();    List<Long> curBuckets = getBuckets();    ImmutableList.Builder<Long> newBuckets = ImmutableList.builder();    if (getBuckets().isEmpty()) {                newBuckets.add(1L);    } else if (bucketIndex < firstBucketOffset) {                newBuckets.add(1L);        for (int i = bucketIndex + 1; i < firstBucketOffset; i++) {            newBuckets.add(0L);        }        newBuckets.addAll(curBuckets);    } else if (bucketIndex >= firstBucketOffset + curBuckets.size()) {                newBuckets.addAll(curBuckets);        for (int i = firstBucketOffset + curBuckets.size(); i < bucketIndex; i++) {            newBuckets.add(0L);        }        newBuckets.add(1L);    } else {                int curIndex = firstBucketOffset;        for (Long curValue : curBuckets) {            long newValue = (bucketIndex == curIndex) ? curValue + 1 : curValue;            newBuckets.add(newValue);            curIndex++;        }    }    return newBuckets.build();}
protected AccumT beam_f7469_0(boolean delta)
{    return delta ? getAndReset() : getAggregate();}
public void beam_f7477_0(Long value)
{    aggregate.addAndGet(value);}
public Long beam_f7478_0()
{    return aggregate.getAndSet(0);}
public UpdateT beam_f7479_0(CounterName name, boolean delta, CounterUpdateExtractor<UpdateT> updateExtractor)
{    return updateExtractor.longSum(name, delta, extractValue(delta));}
public Integer beam_f7487_0()
{    return aggregate.getAndSet(Integer.MAX_VALUE);}
public UpdateT beam_f7488_0(CounterName name, boolean delta, CounterUpdateExtractor<UpdateT> updateExtractor)
{    return updateExtractor.intMin(name, delta, extractValue(delta));}
public void beam_f7489_0(Integer value)
{    int current;    int update;    do {        current = aggregate.get();        update = Math.max(value, current);    } while (update > current && !aggregate.compareAndSet(current, update));}
public Double beam_f7497_0()
{    return aggregate.get();}
public void beam_f7498_0(Double value)
{    double current;    double update;    do {        current = aggregate.get();        update = Math.min(value, current);    } while (update < current && !aggregate.compareAndSet(current, update));}
public Double beam_f7499_0()
{    return aggregate.getAndSet(Double.POSITIVE_INFINITY);}
protected CounterMean<Double> beam_f7507_0()
{    return DoubleCounterMean.ZERO;}
public UpdateT beam_f7508_0(CounterName name, boolean delta, CounterUpdateExtractor<UpdateT> updateExtractor)
{    return updateExtractor.doubleMean(name, delta, extractValue(delta));}
public void beam_f7509_0(Boolean value)
{    if (!value) {        aggregate.set(value);    }}
public Long beam_f7517_0()
{    return aggregate;}
public long beam_f7518_0()
{    return count;}
public CounterMean<Long> beam_f7519_0(Long value)
{    return new LongCounterMean(aggregate + value, count + 1);}
public CounterMean<Integer> beam_f7527_0(Integer sum, long newCount)
{    return new IntegerCounterMean(aggregate + sum, count + newCount);}
public boolean beam_f7528_0(Object obj)
{    if (obj == this) {        return true;    } else if (!(obj instanceof IntegerCounterMean)) {        return false;    }    IntegerCounterMean that = (IntegerCounterMean) obj;    return this.aggregate == that.aggregate && this.count == that.count;}
public int beam_f7529_0()
{    return Objects.hash(aggregate, count);}
public String beam_f7537_0()
{    return aggregate + "/" + count;}
public void beam_f7538_0(Long value)
{    CounterDistribution current;    CounterDistribution update;    do {        current = aggregate.get();        update = current.addValue(value);    } while (!aggregate.compareAndSet(current, update));}
public CounterDistribution beam_f7539_0()
{    return aggregate.get();}
public CounterName beam_f7547_0(NameContext context)
{    return create(name(), origin(), stepName(), prefix(), checkNotNull(context.originalName(), "Expected original name in context %s", context), contextSystemName(), originalRequestingStepName(), inputIndex());}
public CounterName beam_f7548_0(String originalName)
{    return create(name(), origin(), stepName(), prefix(), checkNotNull(originalName, "Expected original name in string %s", originalName), contextSystemName(), originalRequestingStepName(), inputIndex());}
public CounterName beam_f7549_0(NameContext context)
{    return create(name(), origin(), stepName(), prefix(), contextOriginalName(), checkNotNull(context.systemName(), "Expected system name in context %s", context), originalRequestingStepName(), inputIndex());}
public boolean beam_f7557_0()
{    return contextSystemName() != null;}
public boolean beam_f7558_0()
{    return originalRequestingStepName() != null;}
public boolean beam_f7559_0()
{    return usesContextOriginalName() || usesContextSystemName() || usesOriginalRequestingStepName();}
public List<UpdateT> beam_f7567_0(CounterUpdateExtractor<UpdateT> extractors)
{    return extractUpdatesImpl(true, true, extractors);}
public String beam_f7568_0()
{    return MoreObjects.toStringHelper(this).add("counters", counters.values()).toString();}
private CounterUpdate beam_f7569_0(CounterName name, boolean delta, String kind)
{    CounterUpdate counterUpdate = new CounterUpdate();    if (name.isStructured()) {        counterUpdate.setStructuredNameAndMetadata(getStructuredName(name, kind));    } else {        counterUpdate.setNameAndKind(getUnstructuredName(name, kind));    }    counterUpdate.setCumulative(!delta);    return counterUpdate;}
public CounterUpdate beam_f7577_0(CounterName name, boolean delta, Long value)
{    return longUpdate(name, delta, "MAX", value);}
public CounterUpdate beam_f7578_0(CounterName name, boolean delta, CounterMean<Long> value)
{    if (value.getCount() <= 0) {        return null;    }    return initUpdate(name, delta, "MEAN").setIntegerMean(new IntegerMean().setSum(longToSplitInt(value.getAggregate())).setCount(longToSplitInt(value.getCount())));}
public CounterUpdate beam_f7579_0(CounterName name, boolean delta, Integer value)
{    return longUpdate(name, delta, "SUM", value);}
public CounterUpdate beam_f7587_0(CounterName name, boolean delta, Boolean value)
{    return boolUpdate(name, delta, "OR", value);}
public CounterUpdate beam_f7588_0(CounterName name, boolean delta, Boolean value)
{    return boolUpdate(name, delta, "AND", value);}
public CounterUpdate beam_f7589_0(CounterName name, boolean delta, CounterFactory.CounterDistribution value)
{    DistributionUpdate distributionUpdate = new DistributionUpdate().setMin(longToSplitInt(value.getMin())).setMax(longToSplitInt(value.getMax())).setCount(longToSplitInt(value.getCount())).setSum(longToSplitInt(value.getSum())).setSumOfSquares(value.getSumOfSquares()).setHistogram(new Histogram().setFirstBucketOffset(value.getFirstBucketOffset()).setBucketCounts(ImmutableList.copyOf(value.getBuckets())));    return initUpdate(name, delta, "DISTRIBUTION").setDistribution(distributionUpdate);}
public void beam_f7597_0(CounterUpdate update, long shortId)
{    Long oldValue;    if (update.getNameAndKind() != null) {        String name = checkNotNull(update.getNameAndKind().getName(), "Counter update name should be non-null");        oldValue = unstructuredShortIdMap.putIfAbsent(name, shortId);    } else if (update.getStructuredNameAndMetadata() != null) {        CounterStructuredName name = checkNotNull(update.getStructuredNameAndMetadata().getName(), "Counter update structured-name should be non-null");        oldValue = structuredShortIdMap.putIfAbsent(name, shortId);    } else {        throw new IllegalArgumentException("CounterUpdate should have nameAndKind or structuredNameAndmetadata");    }    checkArgument(oldValue == null || oldValue.equals(shortId), "Received counter %s with incompatible short IDs. %s first ID and then %s", update, oldValue, shortId);}
public ParDoFn beam_f7598_0(PipelineOptions options, CloudObject cloudUserFn, List<SideInputInfo> sideInputInfos, TupleTag<?> mainOutputTag, Map<TupleTag<?>, Integer> outputTupleTagsToReceiverIndices, DataflowExecutionContext<?> executionContext, DataflowOperationContext operationContext) throws Exception
{    Coder<?> coder = CloudObjects.coderFromCloudObject(CloudObject.fromSpec(Structs.getObject(cloudUserFn, PropertyNames.ENCODING)));    checkState(coder instanceof IsmRecordCoder, "Expected to received an instanceof an IsmRecordCoder but got %s", coder);    return new CreateIsmShardKeyAndSortKeyParDoFn((IsmRecordCoder<?>) coder);}
public void beam_f7599_0(Receiver... receivers) throws Exception
{    checkState(receivers.length == 1, "%s.startBundle() called with %s receivers, expected exactly 1. " + "This is a bug in the Dataflow service", getClass().getSimpleName(), receivers.length);    this.receiver = receivers[0];}
public Boolean beam_f7610_1()
{    boolean success = true;    try {        do {                        success = doWork();            if (success) {                backOff.reset();            }                } while (success || BackOffUtils.next(sleeper, backOff));    } catch (IOException e) {                    } catch (InterruptedException e) {        Thread.currentThread().interrupt();            } catch (Throwable t) {            }    return false;}
private boolean beam_f7611_1()
{    try {                boolean success = worker.getAndPerformWork();                return success;    } catch (IOException e) {                        return false;    } catch (Exception e) {                        return false;    }}
 static void beam_f7612_1(DataflowWorkerHarnessOptions pipelineOptions, final BatchDataflowWorker worker, Sleeper sleeper) throws InterruptedException
{    int numThreads = chooseNumberOfThreads(pipelineOptions);    ExecutorService executor = pipelineOptions.getExecutorService();    final List<Callable<Boolean>> tasks = new ArrayList<>();            for (int i = 0; i < numThreads; ++i) {        tasks.add(new WorkerThread(worker, sleeper));    }            executor.invokeAll(tasks);    }
private void beam_f7620_0(ElementExecution execution)
{    long nextSnapshot = sharedState.latestSnapshot + 1;    executionStack.addLast(execution);    sharedState.executionJournal.add(execution, nextSnapshot);    sharedState.latestSnapshot = nextSnapshot;}
public void beam_f7621_0()
{        checkState(executionStack.size() > 1, "No processing elements currently tracked.");    ElementExecution execution = executionStack.removeLast();    long nextSnapshot = sharedState.latestSnapshot + 1;    sharedState.doneJournal.add(execution, nextSnapshot);    ElementExecution nextElement = executionStack.getLast();    sharedState.executionJournal.add(nextElement, nextSnapshot);    sharedState.latestSnapshot = nextSnapshot;}
public void beam_f7622_0(Duration sampleTime)
{    long snapshot = sharedState.latestSnapshot;    attributeProcessingTime(sampleTime, snapshot);                reportCounters(snapshot);    pruneJournals(snapshot);}
public T beam_f7630_0()
{    SnapshottedItem<T> next = iter.next();    checkValidSnapshot(next);    return next.item;}
public void beam_f7631_0()
{    iter.remove();}
private void beam_f7632_0(SnapshottedItem<T> next)
{    if (next.snapshot > snapshot) {        throw new NoSuchElementException();    }}
public T beam_f7640_0(DataflowOperationContext operationContext)
{    NameContext nameContext = operationContext.nameContext();    T context = cachedStepContexts.get(nameContext.systemName());    if (context == null) {        context = createStepContext(operationContext);        cachedStepContexts.put(nameContext.systemName(), context);    }    return context;}
public NameContext beam_f7641_0()
{    return nameContext;}
public DataflowOperationContext beam_f7642_0(NameContext nameContext)
{    MetricsContainer metricsContainer = metricsContainerRegistry.getContainer(checkNotNull(nameContext.originalName(), "All operations must have an original name, but %s doesn't.", nameContext));    return new DataflowOperationContext(counterFactory, nameContext, metricsContainer, executionStateTracker, executionStateRegistry);}
public static DataflowExecutionStateKey beam_f7650_0(NameContext nameContext, String stateName, @Nullable String requestingStepName, @Nullable Integer inputIndex)
{    return new AutoValue_DataflowExecutionStateKey(nameContext, stateName, requestingStepName, inputIndex);}
public int beam_f7651_0(DataflowExecutionStateKey o)
{    return ComparisonChain.start().compare(getStateName(), o.getStateName()).compare(getStepName().originalName(), o.getStepName().originalName(), Ordering.natural().nullsFirst()).compare(getStepName().stageName(), o.getStepName().stageName(), Ordering.natural().nullsFirst()).compare(getRequestingStepName(), o.getRequestingStepName(), Ordering.natural().nullsFirst()).compare(getInputIndex(), o.getInputIndex(), Ordering.natural().nullsFirst()).result();}
public DataflowOperationContext.DataflowExecutionState beam_f7652_0(final NameContext nameContext, final String stateName, final MetricsContainer container, final ProfileScope profileScope)
{    return getStateInternal(nameContext, stateName, null, null, container, profileScope);}
public ExecutionState beam_f7660_0(String stateName)
{    return executionStateRegistry.getState(nameContext, stateName, metricsContainer, profileScope);}
public Closeable beam_f7661_0()
{    return enter(startState);}
public Closeable beam_f7662_0()
{    return enter(processElementState);}
public NameContext beam_f7670_0()
{    return stepName;}
public void beam_f7671_0(boolean unusedPushing)
{    profileScope.activate();}
public ProfileScope beam_f7672_0()
{    return profileScope;}
public void beam_f7680_0(Object elem)
{    objectAndByteCounter.finishLazyUpdate(elem);}
 static String beam_f7681_0(String prefix)
{    return prefix + ELEMENT_COUNTER_NAME;}
 static String beam_f7682_0(String prefix)
{    return prefix + OBJECT_COUNTER_NAME;}
public MultimapView<K, V> beam_f7690_0(MultimapView<K, V> o)
{    return o;}
public TypeDescriptor<MultimapView<K, V>> beam_f7691_0()
{    throw new UnsupportedOperationException();}
public WindowMappingFn<?> beam_f7692_0()
{    throw new UnsupportedOperationException();}
public String beam_f7700_0()
{    throw new UnsupportedOperationException();}
public Pipeline beam_f7701_0()
{    throw new UnsupportedOperationException();}
public Map<TupleTag<?>, PValue> beam_f7702_0()
{    throw new UnsupportedOperationException();}
public void beam_f7710_0(String timerId, BoundedWindow window, Instant timestamp, TimeDomain timeDomain)
{    throw new UnsupportedOperationException("Unsupported for ProcessFn");}
public void beam_f7711_0()
{    simpleRunner.finishBundle();}
public DoFn<KeyedWorkItem<byte[], KV<InputT, RestrictionT>>, OutputT> beam_f7712_0()
{    return simpleRunner.getFn();}
public CounterName beam_f7720_0(NameContext nameContext)
{    Preconditions.checkNotNull(nameContext.systemName());    return CounterName.named(namePrefix + "-" + nameContext.systemName());}
public static DataflowWorkerHarnessOptions beam_f7721_1(Class<?> workerHarnessClass) throws Exception
{    /* Extract pipeline options. */    DataflowWorkerHarnessOptions pipelineOptions = WorkerPipelineOptionsFactory.createFromSystemProperties();    pipelineOptions.setAppName(workerHarnessClass.getSimpleName());    /* Configure logging with job-specific properties. */    DataflowWorkerLoggingMDC.setJobId(pipelineOptions.getJobId());    DataflowWorkerLoggingMDC.setWorkerId(pipelineOptions.getWorkerId());    ExperimentContext ec = ExperimentContext.parseFrom(pipelineOptions);    String experimentName = Experiment.EnableConscryptSecurityProvider.getName();    if (ec.isEnabled(Experiment.EnableConscryptSecurityProvider)) {        /* Enable fast SSL provider. */                Security.insertProviderAt(new OpenSSLProvider(), 1);    } else {            }    return pipelineOptions;}
public static void beam_f7722_0(Class<?> workerHarnessClass)
{    /* Set up exception handling tied to the workerHarnessClass. */    Thread.setDefaultUncaughtExceptionHandler(new WorkerUncaughtExceptionHandler(LoggerFactory.getLogger(workerHarnessClass)));        LogManager logManager = LogManager.getLogManager();    logManager.reset();    java.util.logging.Logger rootLogger = logManager.getLogger(ROOT_LOGGER_NAME);    for (Handler handler : rootLogger.getHandlers()) {        rootLogger.removeHandler(handler);    }    DataflowWorkerLoggingInitializer.initialize();}
protected long beam_f7730_0()
{    return fromCloudDuration(workItem.getReportStatusInterval()).getMillis();}
protected void beam_f7731_1() throws Exception
{    WorkItemServiceState result = workItemStatusClient.reportUpdate(dynamicSplitResultToReport, Duration.millis(requestedLeaseDurationMs));    if (result != null) {        if (result.getHotKeyDetection() != null && result.getHotKeyDetection().getUserStepName() != null) {            HotKeyDetection hotKeyDetection = result.getHotKeyDetection();            hotKeyLogger.logHotKeyDetection(hotKeyDetection.getUserStepName(), TimeUtil.fromCloudDuration(hotKeyDetection.getHotKeyAge()));        }                dynamicSplitResultToReport = null;        progressReportIntervalMs = nextProgressReportInterval(fromCloudDuration(result.getReportStatusInterval()).getMillis(), leaseRemainingTime(getLeaseExpirationTimestamp(result)));        ApproximateSplitRequest suggestedStopPoint = result.getSplitRequest();        if (suggestedStopPoint != null) {                        dynamicSplitResultToReport = worker.requestDynamicSplit(SourceTranslationUtils.toDynamicSplitRequest(suggestedStopPoint));        }    }}
private long beam_f7732_0(WorkItem workItem)
{    return fromCloudTime(workItem.getLeaseExpireTime()).getMillis();}
public void beam_f7740_0(long n)
{    value.addAndGet(n);}
public void beam_f7741_0()
{    inc(1);}
public void beam_f7742_0()
{    inc(-1);}
 void beam_f7750_0(DistributionData data)
{    DistributionData original;    do {        original = value.get();    } while (!value.compareAndSet(original, original.combine(data)));}
public void beam_f7751_0(long sum, long count, long min, long max)
{    update(DistributionData.create(sum, count, min, max));}
public DirtyState beam_f7752_0()
{    throw new UnsupportedOperationException(String.format("%s doesn't support the getDirty", getClass().getSimpleName()));}
private DoFnInfo<?, ?> beam_f7760_0() throws Exception
{    DoFnInfo<?, ?> fn;    fn = (DoFnInfo<?, ?>) SerializableUtils.deserializeFromByteArray(serializedFnInfo, null);    DoFnInvokers.invokerFor(fn.getDoFn()).invokeSetup();    return fn;}
public void beam_f7761_0(DoFnInfo<?, ?> fnInfo) throws Exception
{    if (fnInfo != null) {        fns.offer(fnInfo);    }}
public void beam_f7762_0(DoFnInfo<?, ?> fnInfo) throws Exception
{    if (fnInfo != null && fnInfo.getDoFn() != null) {        DoFnInvokers.invokerFor(fnInfo.getDoFn()).invokeTeardown();    }}
public PCollection<?> beam_f7772_0()
{    return delegate.getPCollection();}
public TupleTag<?> beam_f7773_0()
{    return delegate.getTagInternal();}
public ViewFn<?, T> beam_f7774_0()
{    return delegate.getViewFn();}
public void beam_f7782_0(PInput upstreamInput, PTransform<?, ?> upstreamTransform)
{    delegate.finishSpecifying(upstreamInput, upstreamTransform);}
private Iterable<PCollectionView<?>> beam_f7783_0(IdGenerator idGenerator, InstructionRequestHandler instructionRequestHandler, FnDataService beamFnDataService, ApiServiceDescriptor dataServiceApiServiceDescriptor, Coder<BoundedWindow> mainInputWindowCoder, Map<PCollectionView<?>, RunnerApi.SdkFunctionSpec> pCollectionViewsToWindowMappingFns)
{    ImmutableList.Builder<PCollectionView<?>> wrappedViews = ImmutableList.builder();    for (Map.Entry<PCollectionView<?>, RunnerApi.SdkFunctionSpec> entry : pCollectionViewsToWindowMappingFns.entrySet()) {        WindowMappingFn windowMappingFn = new FnApiWindowMappingFn(idGenerator, instructionRequestHandler, dataServiceApiServiceDescriptor, beamFnDataService, entry.getValue(), mainInputWindowCoder, entry.getKey().getWindowingStrategyInternal().getWindowFn().windowCoder());        wrappedViews.add(new ForwardingPCollectionView<Materializations.MultimapView>((PCollectionView) entry.getKey()) {            @Override            public WindowMappingFn<?> getWindowMappingFn() {                return windowMappingFn;            }        });    }    return wrappedViews.build();}
public WindowMappingFn<?> beam_f7784_0()
{    return windowMappingFn;}
private static int beam_f7792_0(int numShards, String filepattern)
{    if (numShards < 100000) {        return 5;    }    if (numShards < 1000000) {        return 6;    }    if (numShards < 10000000) {        return 7;    }    if (numShards < 100000000) {        return 8;    }    if (numShards < 1000000000) {        return 9;    }    throw new IllegalArgumentException("Unsupported number of shards: " + numShards + " in filepattern: " + filepattern);}
public Endpoints.ApiServiceDescriptor beam_f7793_0()
{    return apiServiceDescriptor;}
public StreamObserver<BeamFnApi.InstructionResponse> beam_f7794_1(StreamObserver<BeamFnApi.InstructionRequest> outboundObserver)
{        FnApiControlClient newClient = FnApiControlClient.forRequestObserver(headerAccessor.getSdkWorkerId(), streamObserverFactory.apply(outboundObserver));    try {        newClients.put(newClient);    } catch (InterruptedException e) {        Thread.currentThread().interrupt();        throw new RuntimeException(e);    }    return newClient.asResponseObserver();}
public DynamicSplitResult beam_f7803_0() throws Exception
{    return progressTracker.requestCheckpoint();}
public DynamicSplitResult beam_f7804_0(DynamicSplitRequest splitRequest) throws Exception
{    return progressTracker.requestDynamicSplit(splitRequest);}
public ReadOperation beam_f7805_0() throws Exception
{        for (Operation operation : operations) {        if (operation instanceof ReadOperation) {            return (ReadOperation) operation;        }    }    throw new IllegalStateException(String.format("ReadOperation not found in %s", operations));}
public List<CounterUpdate> beam_f7815_0()
{    return Collections.emptyList();}
public DynamicSplitResult beam_f7816_0() throws Exception
{    return readOperation.requestCheckpoint();}
public DynamicSplitResult beam_f7817_0(DynamicSplitRequest splitRequest) throws Exception
{    return readOperation.requestDynamicSplit(splitRequest);}
public MetricUpdates beam_f7825_0()
{    Map<MetricKey, MetricUpdate<Long>> snapshotCounterUpdates = deprecatedCounterUpdates;    Map<MetricKey, MetricUpdate<DistributionData>> snapshotDistributionUpdates = deprecatedDistributionUpdates;    Map<MetricKey, MetricUpdate<GaugeData>> snapshotGaugeUpdates = deprecatedGaugeUpdates;    return MetricUpdates.create(snapshotCounterUpdates.values(), snapshotDistributionUpdates.values(), snapshotGaugeUpdates.values());}
public DynamicSplitResult beam_f7826_0() throws Exception
{        return null;}
public DynamicSplitResult beam_f7827_0(DynamicSplitRequest splitRequest) throws Exception
{    return readOperation.requestDynamicSplit(splitRequest);}
private void beam_f7835_0()
{    assert numPoints > 2;    if (xs[1] < lowerBound) {                        int start = Arrays.binarySearch(xs, 0, numPoints, lowerBound);                if (start < 0) {            start = -start - 1;        }                        start--;        numPoints -= start;        System.arraycopy(xs, start, xs, 0, numPoints);        System.arraycopy(ys, start, ys, 0, numPoints);    } else {                        double[] newXs = new double[xs.length];        T[] newYs = (T[]) new Object[ys.length];        newXs[0] = xs[0];        newYs[0] = ys[0];        double dx = (xs[numPoints - 1] - xs[0]) / (numPoints / 2.0);        for (int i = 1; i < numPoints / 2; i++) {            double x = xs[0] + i * dx;            newXs[i] = x;            newYs[i] = interpolate(x);        }        newXs[numPoints / 2] = xs[numPoints - 1];        newYs[numPoints / 2] = ys[numPoints - 1];        xs = newXs;        ys = newYs;        numPoints = numPoints / 2 + 1;    }        Arrays.fill(xs, numPoints, ys.length, Double.NaN);        Arrays.fill(ys, numPoints, ys.length, null);}
private ProgressTracker beam_f7836_1()
{    ReadOperation readOperation;    RemoteGrpcPortWriteOperation grpcWriteOperation;    RegisterAndProcessBundleOperation bundleProcessOperation;    try {        readOperation = getReadOperation();    } catch (Exception exn) {        readOperation = null;                return new NullProgressTracker();    }        try {        grpcWriteOperation = Iterables.getOnlyElement(Iterables.filter(operations, RemoteGrpcPortWriteOperation.class));        bundleProcessOperation = Iterables.getOnlyElement(Iterables.filter(operations, RegisterAndProcessBundleOperation.class));    } catch (IllegalArgumentException | NoSuchElementException exn) {                grpcWriteOperation = null;        bundleProcessOperation = null;            }    if (grpcWriteOperation != null && bundleProcessOperation != null) {        return new SingularProcessBundleProgressTracker(readOperation, grpcWriteOperation, bundleProcessOperation);    } else {        return new ReadOperationProgressTracker(readOperation);    }}
private static boolean beam_f7837_0(String msg)
{    return msg != null && (msg.contains("Process bundle request not yet scheduled") || msg.contains("Unknown process bundle instruction") || msg.contains("unstarted operation"));}
public CounterUpdate beam_f7845_0(MonitoringInfo src)
{    String urn = src.getUrn();    MonitoringInfoToCounterUpdateTransformer transformer = counterTransformers.get(urn);    return transformer == null ? null : transformer.transform(src);}
protected Optional<String> beam_f7846_0(MonitoringInfo monitoringInfo)
{    Optional<String> validatorResult = specValidator.validate(monitoringInfo);    if (validatorResult.isPresent()) {        return validatorResult;    }    String urn = monitoringInfo.getUrn();    if (!urn.equals(SUPPORTED_URN)) {        throw new RuntimeException(String.format("Received unexpected counter urn: %s", urn));    }    if (!pcollectionIdToNameContext.containsKey(monitoringInfo.getLabelsMap().get(MonitoringInfoConstants.Labels.PCOLLECTION))) {        return Optional.of("Encountered ElementCount MonitoringInfo with unknown PCollectionId: " + monitoringInfo.toString());    }    return Optional.empty();}
public CounterUpdate beam_f7847_1(MonitoringInfo monitoringInfo)
{    Optional<String> validationResult = validate(monitoringInfo);    if (validationResult.isPresent()) {                return null;    }    IntDistributionData value = monitoringInfo.getMetric().getDistributionData().getIntDistributionData();    final String pcollectionId = monitoringInfo.getLabelsMap().get(MonitoringInfoConstants.Labels.PCOLLECTION);    final String pcollectionName = pcollectionIdToNameContext.get(pcollectionId).userName();    String counterName = pcollectionName + "-MeanByteCount";    NameAndKind name = new NameAndKind();    name.setName(counterName).setKind("MEAN");    return new CounterUpdate().setNameAndKind(name).setCumulative(true).setIntegerMean(new IntegerMean().setSum(longToSplitInt(value.getSum())).setCount(longToSplitInt(value.getCount())));}
public void beam_f7855_0() throws Exception
{    try (Closeable scope = context.enterStart()) {        super.start();        try {            remoteBundle = stageBundleFactory.getBundle(receiverFactory, stateRequestHandler, progressHandler);        } catch (Exception e) {            throw new RuntimeException("Failed to start remote bundle", e);        }    }}
public void beam_f7856_1(Object inputElement) throws Exception
{        String mainInputPCollectionId = executableStage.getInputPCollection().getId();    FnDataReceiver<WindowedValue<?>> mainInputReceiver = remoteBundle.getInputReceivers().get(mainInputPCollectionId);    try (Closeable scope = context.enterProcess()) {        mainInputReceiver.accept((WindowedValue<InputT>) inputElement);    } catch (Exception e) {        String err = String.format("Could not process element %s to receiver %s for pcollection %s with error %s", inputElement.toString(), mainInputReceiver.toString(), mainInputPCollectionId, e);                throw new RuntimeException(err, e);    }}
public void beam_f7857_0() throws Exception
{    try (Closeable scope = context.enterFinish()) {        try {                        remoteBundle.close();        } catch (Exception e) {            throw new RuntimeException("Failed to finish remote bundle", e);        }                timerReceiver.finish();    }}
public void beam_f7865_0() throws Exception
{    try (Closeable scope = context.enterFinish()) {                BeamFnApi.ProcessBundleResponse completedResponse = MoreFutures.get(getProcessBundleResponse(processBundleResponse));        if (completedResponse.getResidualRootsCount() > 0) {            throw new IllegalStateException("TODO: [BEAM-2939] residual roots in process bundle response not yet supported.");        }        deregisterStateHandler.deregister();        userStateData.clear();        processBundleId = null;        super.finish();    }}
public void beam_f7866_0() throws Exception
{    try (Closeable scope = context.enterAbort()) {        deregisterStateHandler.abort();        cancelIfNotNull(registerFuture);        cancelIfNotNull(processBundleResponse);        super.abort();    }}
public Map<String, NameContext> beam_f7867_0()
{    return this.pcollectionIdToNameContext;}
 double beam_f7875_0(BeamFnApi.Metrics metrics)
{    return metrics.getPtransformsOrDefault(grpcReadTransformId, BeamFnApi.Metrics.PTransform.getDefaultInstance()).getProcessedElements().getMeasured().getOutputElementCountsOrDefault(grpcReadTransformOutputName, 0);}
private CompletionStage<BeamFnApi.StateResponse.Builder> beam_f7876_0(StateRequest stateRequest)
{    switch(stateRequest.getStateKey().getTypeCase()) {        case BAG_USER_STATE:            return handleBagUserState(stateRequest);        case MULTIMAP_SIDE_INPUT:            return handleMultimapSideInput(stateRequest);        default:            throw new UnsupportedOperationException(String.format("Dataflow does not handle StateRequests of type %s", stateRequest.getStateKey().getTypeCase()));    }}
private CompletionStage<BeamFnApi.StateResponse.Builder> beam_f7877_0(StateRequest stateRequest)
{    checkState(stateRequest.getRequestCase() == RequestCase.GET, String.format("MultimapSideInput state requests only support '%s' requests, received '%s'", RequestCase.GET, stateRequest.getRequestCase()));    StateKey.MultimapSideInput multimapSideInputStateKey = stateRequest.getStateKey().getMultimapSideInput();    SideInputReader sideInputReader = ptransformIdToSideInputReader.get(multimapSideInputStateKey.getTransformId());    checkState(sideInputReader != null, String.format("Unknown PTransform '%s'", multimapSideInputStateKey.getTransformId()));    PCollectionView<Materializations.MultimapView<Object, Object>> view = (PCollectionView<Materializations.MultimapView<Object, Object>>) ptransformIdToSideInputIdToPCollectionView.get(multimapSideInputStateKey.getTransformId(), multimapSideInputStateKey.getSideInputId());    checkState(view != null, String.format("Unknown side input '%s' on PTransform '%s'", multimapSideInputStateKey.getSideInputId(), multimapSideInputStateKey.getTransformId()));    checkState(Materializations.MULTIMAP_MATERIALIZATION_URN.equals(view.getViewFn().getMaterialization().getUrn()), String.format("Unknown materialization for side input '%s' on PTransform '%s' with urn '%s'", multimapSideInputStateKey.getSideInputId(), multimapSideInputStateKey.getTransformId(), view.getViewFn().getMaterialization().getUrn()));    checkState(view.getCoderInternal() instanceof KvCoder, String.format("Materialization of side input '%s' on PTransform '%s' expects %s but received %s.", multimapSideInputStateKey.getSideInputId(), multimapSideInputStateKey.getTransformId(), KvCoder.class.getSimpleName(), view.getCoderInternal().getClass().getSimpleName()));    Coder<Object> keyCoder = ((KvCoder) view.getCoderInternal()).getKeyCoder();    Coder<Object> valueCoder = ((KvCoder) view.getCoderInternal()).getValueCoder();    BoundedWindow window;    try {                window = view.getWindowingStrategyInternal().getWindowFn().windowCoder().decode(multimapSideInputStateKey.getWindow().newInput());    } catch (IOException e) {        throw new IllegalArgumentException(String.format("Unable to decode window for side input '%s' on PTransform '%s'.", multimapSideInputStateKey.getSideInputId(), multimapSideInputStateKey.getTransformId()), e);    }    Object userKey;    try {                userKey = keyCoder.decode(multimapSideInputStateKey.getKey().newInput());    } catch (IOException e) {        throw new IllegalArgumentException(String.format("Unable to decode user key for side input '%s' on PTransform '%s'.", multimapSideInputStateKey.getSideInputId(), multimapSideInputStateKey.getTransformId()), e);    }    Materializations.MultimapView<Object, Object> sideInput = sideInputReader.get(view, window);    Iterable<Object> values = sideInput.get(userKey);    try {                return CompletableFuture.completedFuture(StateResponse.newBuilder().setGet(StateGetResponse.newBuilder().setData(encodeAndConcat(values, valueCoder))));    } catch (IOException e) {        throw new IllegalArgumentException(String.format("Unable to encode values for side input '%s' on PTransform '%s'.", multimapSideInputStateKey.getSideInputId(), multimapSideInputStateKey.getTransformId()), e);    }}
 static ByteString beam_f7885_0(Iterable<Object> values, Coder valueCoder) throws IOException
{    ByteString.Output out = ByteString.newOutput();    if (values != null) {        for (Object value : values) {            int size = out.size();            valueCoder.encode(value, out);                        if (size == out.size()) {                out.write(0);            }        }    }    return out.toByteString();}
public String beam_f7886_0()
{    return MoreObjects.toStringHelper(this).add("processBundleId", processBundleId).add("processBundleDescriptors", registerRequest.getProcessBundleDescriptorList()).toString();}
public FnDataReceiver<?> beam_f7887_0(String pCollectionId)
{    return receivedElement -> receive(pCollectionId, receivedElement);}
private static RunnerApi.Coder beam_f7895_0(RunnerApi.PTransform pTransform, String timerId, ProcessBundleDescriptor processBundleDescriptor)
{    String timerPCollectionId = pTransform.getInputsMap().get(timerId);    RunnerApi.PCollection timerPCollection = processBundleDescriptor.getPcollectionsMap().get(timerPCollectionId);    String windowingStrategyId = timerPCollection.getWindowingStrategyId();    RunnerApi.WindowingStrategy windowingStrategy = processBundleDescriptor.getWindowingStrategiesMap().get(windowingStrategyId);    String windowingCoderId = windowingStrategy.getWindowCoderId();    return processBundleDescriptor.getCodersMap().get(windowingCoderId);}
private Optional<String> beam_f7896_0(MonitoringInfo monitoringInfo)
{    Optional<String> validatorResult = specValidator.validate(monitoringInfo);    if (validatorResult.isPresent()) {        return validatorResult;    }    String urn = monitoringInfo.getUrn();    if (!urn.equals(BEAM_METRICS_USER_DISTRIBUTION_URN)) {        throw new RuntimeException(String.format("Received unexpected counter urn. Expected urn: %s, received: %s", BEAM_METRICS_USER_DISTRIBUTION_URN, urn));    }    final String ptransform = monitoringInfo.getLabelsMap().get(MonitoringInfoConstants.Labels.PTRANSFORM);    DataflowStepContext stepContext = transformIdMapping.get(ptransform);    if (stepContext == null) {        return Optional.of("Encountered user-counter MonitoringInfo with unknown ptransformId: " + monitoringInfo.toString());    }    return Optional.empty();}
public CounterUpdate beam_f7897_1(MonitoringInfo monitoringInfo)
{    Optional<String> validationResult = validate(monitoringInfo);    if (validationResult.isPresent()) {                return null;    }    IntDistributionData value = monitoringInfo.getMetric().getDistributionData().getIntDistributionData();    Map<String, String> miLabels = monitoringInfo.getLabelsMap();    final String ptransform = miLabels.get(MonitoringInfoConstants.Labels.PTRANSFORM);    final String counterName = miLabels.get(MonitoringInfoConstants.Labels.NAME);    final String counterNamespace = miLabels.get(MonitoringInfoConstants.Labels.NAMESPACE);    CounterStructuredNameAndMetadata name = new CounterStructuredNameAndMetadata();    DataflowStepContext stepContext = transformIdMapping.get(ptransform);    name.setName(new CounterStructuredName().setOrigin(Origin.USER.toString()).setName(counterName).setOriginalStepName(stepContext.getNameContext().originalName()).setOriginNamespace(counterNamespace)).setMetadata(new CounterMetadata().setKind("DISTRIBUTION"));    return new CounterUpdate().setStructuredNameAndMetadata(name).setCumulative(true).setDistribution(new DistributionUpdate().setMax(DataflowCounterUpdateExtractor.longToSplitInt(value.getMax())).setMin(DataflowCounterUpdateExtractor.longToSplitInt(value.getMin())).setSum(DataflowCounterUpdateExtractor.longToSplitInt(value.getSum())).setCount(DataflowCounterUpdateExtractor.longToSplitInt(value.getCount())));}
public boolean beam_f7906_0()
{    if (!future.isDone()) {        return false;    }    try {        return future.get().isDone();    } catch (CancellationException | ExecutionException e) {        return true;    } catch (InterruptedException e) {        Thread.currentThread().interrupt();        return false;    }}
public void beam_f7907_0()
{        if (future.cancel(true)) {        return;    } else {        try {            future.get().cancel();        } catch (CancellationException | ExecutionException e) {                        } catch (InterruptedException e) {                                    Thread.currentThread().interrupt();        }    }}
public void beam_f7908_0()
{    try {        future.get().complete();    } catch (CancellationException | ExecutionException e) {            } catch (InterruptedException e) {        Thread.currentThread().interrupt();        throw new RuntimeException(e);    }}
public void beam_f7917_1() throws Exception
{    try (Closeable scope = context.enterFinish()) {                inboundDataClient.awaitCompletion();        bundleId = null;        super.finish();    }}
public void beam_f7918_0() throws Exception
{    try (Closeable scope = context.enterAbort()) {        inboundDataClient.cancel();    }}
public boolean beam_f7919_0()
{    return true;}
public void beam_f7927_0(Object outputElem) throws Exception
{    try (Closeable scope = context.enterProcess()) {        maybeWait();        int numSent = elementsSent.incrementAndGet();        if (numSent == 1) {            firstElementSentMillis = currentTimeMillis.get();        } else if (numSent == 2) {            secondElementSentMillis = currentTimeMillis.get();        }        receiver.accept((WindowedValue<T>) outputElem);    }}
public void beam_f7928_0() throws Exception
{    try (Closeable scope = context.enterFinish()) {        receiver.close();        bundleId = null;        super.finish();    }}
public void beam_f7929_0() throws Exception
{    try (Closeable scope = context.enterAbort()) {        abortWait();        receiver.close();        super.abort();    }}
public void beam_f7937_0(BeamFnApi.LogEntry.List value)
{    DataflowWorkerLoggingMDC.setSdkHarnessId(sdkWorkerId);    for (BeamFnApi.LogEntry logEntry : value.getLogEntriesList()) {        clientLogger.accept(logEntry);    }    DataflowWorkerLoggingMDC.setSdkHarnessId(null);}
public void beam_f7938_1(Throwable t)
{                    completeIfNotNull(connectedClients.remove(this));}
public void beam_f7939_1()
{                    completeIfNotNull(connectedClients.remove(this));}
private static InstructionResponse beam_f7947_0(CompletionStage<InstructionResponse> responseFuture) throws ExecutionException, InterruptedException
{    InstructionResponse response = MoreFutures.get(responseFuture);    if (!Strings.isNullOrEmpty(response.getError())) {        throw new IllegalStateException(String.format("Client failed to process %s with error [%s].", response.getInstructionId(), response.getError()));    }    return response;}
public void beam_f7948_0(Receiver... receivers) throws Exception
{    delegate.startBundle(receivers);}
public void beam_f7949_0(Object elem) throws Exception
{    delegate.processElement(elem);}
private Environment beam_f7957_0(RunnerApi.Components components, Set<PTransformNode> sdkTransforms)
{    RehydratedComponents sdkComponents = RehydratedComponents.forComponents(components);    Environment env = null;    for (PTransformNode pTransformNode : sdkTransforms) {        env = Environments.getEnvironment(pTransformNode.getTransform(), sdkComponents).orElse(null);        if (env != null) {            break;        }    }    return env;}
private RunnerApi.FunctionSpec.Builder beam_f7958_0(CloudObject userFn)
{            String combinePTransformId = getString(userFn, PropertyNames.SERIALIZED_FN);    RunnerApi.PTransform combinePerKeyPTransform = pipeline.getComponents().getTransformsOrDefault(combinePTransformId, null);    checkArgument(combinePerKeyPTransform != null, "Transform with id \"%s\" not found in pipeline.", combinePTransformId);    checkArgument(combinePerKeyPTransform.getSpec().getUrn().equals(COMBINE_PER_KEY_URN), "Found transform \"%s\" for Combine instruction, " + "but that transform had unexpected URN \"%s\" (expected \"%s\")", combinePerKeyPTransform, combinePerKeyPTransform.getSpec().getUrn(), COMBINE_PER_KEY_URN);    RunnerApi.CombinePayload combinePayload;    try {        combinePayload = RunnerApi.CombinePayload.parseFrom(combinePerKeyPTransform.getSpec().getPayload());    } catch (InvalidProtocolBufferException exc) {        throw new RuntimeException("Combine did not have a CombinePayload", exc);    }    String phase = getString(userFn, WorkerPropertyNames.PHASE, CombinePhase.ALL);    String urn;    switch(phase) {        case CombinePhase.ALL:            urn = COMBINE_GROUPED_VALUES_URN;            break;        case CombinePhase.ADD:            urn = COMBINE_PRECOMBINE_URN;            break;        case CombinePhase.MERGE:            urn = COMBINE_MERGE_URN;            break;        case CombinePhase.EXTRACT:            urn = COMBINE_EXTRACT_URN;            break;        default:            throw new RuntimeException("Encountered unknown Combine Phase: " + phase);    }    return RunnerApi.FunctionSpec.newBuilder().setUrn(urn).setPayload(combinePayload.toByteString());}
private boolean beam_f7959_0(MutableNetwork<Node, Edge> input, InstructionOutputNode node)
{    return input.predecessors(node).stream().anyMatch(RemoteGrpcPortNode.class::isInstance);}
private AggregatedLocation beam_f7967_0(Node node, MutableNetwork<Node, Edge> network, Map<Node, AggregatedLocation> successorLocationsMap)
{    return getConnectedNodeLocations(node, network, successorLocationsMap, SearchDirection.SUCCESSORS);}
private AggregatedLocation beam_f7968_0(Node node, MutableNetwork<Node, Edge> network, Map<Node, AggregatedLocation> connectedLocationsMap, SearchDirection direction)
{        if (connectedLocationsMap.containsKey(node)) {        return connectedLocationsMap.get(node);    }    boolean hasSdkConnections = false;    boolean hasRunnerConnections = false;    Set<Node> connectedNodes;    if (direction == SearchDirection.SUCCESSORS) {        connectedNodes = network.successors(node);    } else {        connectedNodes = network.predecessors(node);    }        for (Node connectedNode : connectedNodes) {        if (connectedNode instanceof ParallelInstructionNode && ((ParallelInstructionNode) connectedNode).getExecutionLocation() != ExecutionLocation.UNKNOWN) {            ExecutionLocation executionLocation = ((ParallelInstructionNode) connectedNode).getExecutionLocation();            switch(executionLocation) {                case SDK_HARNESS:                    hasSdkConnections = true;                    break;                case RUNNER_HARNESS:                    hasRunnerConnections = true;                    break;                case AMBIGUOUS:                    hasSdkConnections = true;                    hasRunnerConnections = true;                    break;                default:                    throw new IllegalStateException("Unknown case " + executionLocation);            }        } else {            AggregatedLocation connectedLocation = getConnectedNodeLocations(connectedNode, network, connectedLocationsMap, direction);            switch(connectedLocation) {                case SDK_HARNESS:                    hasSdkConnections = true;                    break;                case RUNNER_HARNESS:                    hasRunnerConnections = true;                    break;                case BOTH:                    hasSdkConnections = true;                    hasRunnerConnections = true;                    break;                case NEITHER:                    break;                default:                    throw new IllegalStateException("Unknown case " + connectedLocation);            }        }                if (hasSdkConnections && hasRunnerConnections) {            break;        }    }        AggregatedLocation aggregatedLocation;    if (hasSdkConnections && hasRunnerConnections) {        aggregatedLocation = AggregatedLocation.BOTH;    } else if (hasSdkConnections) {        aggregatedLocation = AggregatedLocation.SDK_HARNESS;    } else if (hasRunnerConnections) {        aggregatedLocation = AggregatedLocation.RUNNER_HARNESS;    } else {        aggregatedLocation = AggregatedLocation.NEITHER;    }    connectedLocationsMap.put(node, aggregatedLocation);    return aggregatedLocation;}
public boolean beam_f7969_0(Node node)
{    return node instanceof ParallelInstructionNode && ((ParallelInstructionNode) node).getParallelInstruction().getFlatten() != null;}
public DefaultEdge beam_f7977_0()
{    return (DefaultEdge) super.clone();}
public static MultiOutputInfoEdge beam_f7978_0(MultiOutputInfo multiOutputInfo)
{    checkNotNull(multiOutputInfo);    return new AutoValue_Edges_MultiOutputInfoEdge(multiOutputInfo);}
public MultiOutputInfoEdge beam_f7979_0()
{    return (MultiOutputInfoEdge) super.clone();}
 static Function<Node, Node> beam_f7987_0(MutableNetwork<Node, Edge> network)
{    return new TypeSafeNodeFunction<InstructionOutputNode>(InstructionOutputNode.class) {        @Override        public InstructionOutputNode typedApply(InstructionOutputNode input) {            InstructionOutput cloudOutput = input.getInstructionOutput();            if (network.predecessors(input).stream().anyMatch(RemoteGrpcPortNode.class::isInstance) || network.successors(input).stream().anyMatch(RemoteGrpcPortNode.class::isInstance)) {                try {                    cloudOutput = forInstructionOutput(cloudOutput, false);                } catch (Exception e) {                    throw new RuntimeException(String.format("Failed to replace unknown coder with " + "LengthPrefixCoder for : {%s}", input.getInstructionOutput()), e);                }            }            return InstructionOutputNode.create(cloudOutput, input.getPcollectionId());        }    };}
public InstructionOutputNode beam_f7988_0(InstructionOutputNode input)
{    InstructionOutput cloudOutput = input.getInstructionOutput();    if (network.predecessors(input).stream().anyMatch(RemoteGrpcPortNode.class::isInstance) || network.successors(input).stream().anyMatch(RemoteGrpcPortNode.class::isInstance)) {        try {            cloudOutput = forInstructionOutput(cloudOutput, false);        } catch (Exception e) {            throw new RuntimeException(String.format("Failed to replace unknown coder with " + "LengthPrefixCoder for : {%s}", input.getInstructionOutput()), e);        }    }    return InstructionOutputNode.create(cloudOutput, input.getPcollectionId());}
private static Function<Node, Node> beam_f7989_0()
{    return new TypeSafeNodeFunction<InstructionOutputNode>(InstructionOutputNode.class) {        @Override        public Node typedApply(InstructionOutputNode input) {            InstructionOutput instructionOutput = input.getInstructionOutput();            try {                instructionOutput = forInstructionOutput(instructionOutput, true);            } catch (Exception e) {                throw new RuntimeException(String.format("Failed to replace unknown coder with " + "LengthPrefixCoder for : {%s}", input.getInstructionOutput()), e);            }            return InstructionOutputNode.create(instructionOutput, input.getPcollectionId());        }    };}
 static T beam_f7997_0(T cloudObject, Class<T> type) throws IOException
{    return cloudObject.getFactory().fromString(cloudObject.toString(), type);}
private static ParallelInstruction beam_f7998_0(JsonFactory factory, ParallelInstruction instruction)
{    try {        return factory.fromString(factory.toString(instruction), ParallelInstruction.class);    } catch (IOException e) {        throw new RuntimeException(e);    }}
public MutableNetwork<Node, Edge> beam_f7999_0(MapTask mapTask)
{    List<ParallelInstruction> parallelInstructions = Apiary.listOrEmpty(mapTask.getInstructions());    MutableNetwork<Node, Edge> network = NetworkBuilder.directed().allowsSelfLoops(false).allowsParallelEdges(true).expectedNodeCount(parallelInstructions.size() * 2).build();        ParallelInstructionNode[] instructionNodes = new ParallelInstructionNode[parallelInstructions.size()];    InstructionOutputNode[][] outputNodes = new InstructionOutputNode[parallelInstructions.size()][];    for (int i = 0; i < parallelInstructions.size(); ++i) {                        List<InstructionOutput> outputs = Apiary.listOrEmpty(parallelInstructions.get(i).getOutputs());        outputNodes[i] = new InstructionOutputNode[outputs.size()];        JsonFactory factory = MoreObjects.firstNonNull(mapTask.getFactory(), Transport.getJsonFactory());        ParallelInstruction parallelInstruction = clone(factory, parallelInstructions.get(i)).setOutputs(null);        ParallelInstructionNode instructionNode = ParallelInstructionNode.create(parallelInstruction, Nodes.ExecutionLocation.UNKNOWN);        instructionNodes[i] = instructionNode;        network.addNode(instructionNode);                for (int j = 0; j < outputs.size(); ++j) {            InstructionOutput instructionOutput = outputs.get(j);            InstructionOutputNode outputNode = InstructionOutputNode.create(instructionOutput, "generatedPcollection" + this.idGenerator.getId());            network.addNode(outputNode);            if (parallelInstruction.getParDo() != null) {                network.addEdge(instructionNode, outputNode, MultiOutputInfoEdge.create(parallelInstruction.getParDo().getMultiOutputInfos().get(j)));            } else {                network.addEdge(instructionNode, outputNode, DefaultEdge.create());            }            outputNodes[i][j] = outputNode;        }    }        for (ParallelInstructionNode instructionNode : instructionNodes) {        ParallelInstruction parallelInstruction = instructionNode.getParallelInstruction();        if (parallelInstruction.getFlatten() != null) {            for (InstructionInput input : Apiary.listOrEmpty(parallelInstruction.getFlatten().getInputs())) {                attachInput(input, network, instructionNode, outputNodes);            }        } else if (parallelInstruction.getParDo() != null) {            attachInput(parallelInstruction.getParDo().getInput(), network, instructionNode, outputNodes);        } else if (parallelInstruction.getPartialGroupByKey() != null) {            attachInput(parallelInstruction.getPartialGroupByKey().getInput(), network, instructionNode, outputNodes);        } else if (parallelInstruction.getRead() != null) {                } else if (parallelInstruction.getWrite() != null) {            attachInput(parallelInstruction.getWrite().getInput(), network, instructionNode, outputNodes);        } else {            throw new IllegalArgumentException(String.format("Unknown type of instruction %s for map task %s", parallelInstruction, mapTask));        }    }    return network;}
public static List<List<NodeT>> beam_f8007_0(Network<NodeT, EdgeT> network)
{    ArrayDeque<List<NodeT>> paths = new ArrayDeque<>();        for (NodeT node : network.nodes()) {        if (network.inDegree(node) == 0) {            paths.add(ImmutableList.of(node));        }    }    List<List<NodeT>> distinctPathsFromRootsToLeaves = new ArrayList<>();    while (!paths.isEmpty()) {        List<NodeT> path = paths.removeFirst();        NodeT lastNode = path.get(path.size() - 1);        if (network.outDegree(lastNode) == 0) {            distinctPathsFromRootsToLeaves.add(new ArrayList<>(path));        } else {            for (EdgeT edge : network.outEdges(lastNode)) {                paths.addFirst(ImmutableList.<NodeT>builder().addAll(path).add(network.incidentNodes(edge).target()).build());            }        }    }    return distinctPathsFromRootsToLeaves;}
public final boolean beam_f8008_0(Object obj)
{    return this == obj;}
public final int beam_f8009_0()
{    return super.hashCode();}
public void beam_f8017_0() throws IOException
{    baseGenerator.writeStartObject();}
public void beam_f8018_0() throws IOException
{    baseGenerator.writeEndObject();}
public void beam_f8019_0(String name) throws IOException
{    baseGenerator.writeFieldName(name);}
public void beam_f8027_0(BigDecimal v) throws IOException
{    baseGenerator.writeNumber(v);}
public void beam_f8028_0(String encodedValue) throws IOException
{    baseGenerator.writeNumber(encodedValue);}
public void beam_f8029_0() throws IOException
{    baseGenerator.enablePrettyPrint();}
public static RegisterRequestNode beam_f8037_0(BeamFnApi.RegisterRequest request, Map<String, NameContext> ptransformIdToPartialNameContextMap, Map<String, Iterable<SideInputInfo>> ptransformIdToSideInputInfoMap, Map<String, Iterable<PCollectionView<?>>> ptransformIdToPCollectionViewMap, Map<String, NameContext> pcollectionToPartialNameContextMap)
{    checkNotNull(request);    checkNotNull(ptransformIdToPartialNameContextMap);    return new AutoValue_Nodes_RegisterRequestNode(request, ptransformIdToPartialNameContextMap, ptransformIdToSideInputInfoMap, ptransformIdToPCollectionViewMap, pcollectionToPartialNameContextMap);}
public String beam_f8038_0()
{        return "RegisterRequestNode";}
public static ExecutableStageNode beam_f8039_0(ExecutableStage executableStage, Map<String, NameContext> ptransformIdToPartialNameContextMap, Map<String, Iterable<SideInputInfo>> ptransformIdToSideInputInfoMap, Map<String, Iterable<PCollectionView<?>>> pTransformIdToPCollectionViewMap)
{    checkNotNull(executableStage);    checkNotNull(ptransformIdToPartialNameContextMap);    return new AutoValue_Nodes_ExecutableStageNode(executableStage, ptransformIdToPartialNameContextMap, ptransformIdToSideInputInfoMap, pTransformIdToPCollectionViewMap);}
private static final void beam_f8047_0(RunnerApi.Pipeline pipeline, RunnerApi.PTransform originalPTransform, String sideInputTag, ProcessBundleDescriptor.Builder processBundleDescriptor, RunnerApi.PTransform.Builder updatedPTransform)
{    RunnerApi.PCollection sideInputPCollection = pipeline.getComponents().getPcollectionsOrThrow(originalPTransform.getInputsOrThrow(sideInputTag));    RunnerApi.WindowingStrategy sideInputWindowingStrategy = pipeline.getComponents().getWindowingStrategiesOrThrow(sideInputPCollection.getWindowingStrategyId());                            RunnerApi.Components.Builder componentsBuilder = pipeline.getComponents().toBuilder();    componentsBuilder.putAllCoders(processBundleDescriptor.getCodersMap());    String updatedSdkSideInputCoderId = LengthPrefixUnknownCoders.addLengthPrefixedCoder(sideInputPCollection.getCoderId(), componentsBuilder, false);    String updatedSdkSideInputWindowCoderId = LengthPrefixUnknownCoders.addLengthPrefixedCoder(sideInputWindowingStrategy.getWindowCoderId(), componentsBuilder, false);    processBundleDescriptor.putAllCoders(componentsBuilder.getCodersMap());    String updatedSdkWindowingStrategyId = SyntheticComponents.uniqueId(sideInputPCollection.getWindowingStrategyId() + "-runner_generated", processBundleDescriptor.getWindowingStrategiesMap().keySet()::contains);    processBundleDescriptor.putWindowingStrategies(updatedSdkWindowingStrategyId, sideInputWindowingStrategy.toBuilder().setWindowCoderId(updatedSdkSideInputWindowCoderId).build());    RunnerApi.PCollection updatedSdkSideInputPcollection = sideInputPCollection.toBuilder().setCoderId(updatedSdkSideInputCoderId).setWindowingStrategyId(updatedSdkWindowingStrategyId).build();            processBundleDescriptor.putPcollections(originalPTransform.getInputsOrThrow(sideInputTag), updatedSdkSideInputPcollection);    updatedPTransform.putInputs(sideInputTag, originalPTransform.getInputsOrThrow(sideInputTag));}
public MutableNetwork<Node, Edge> beam_f8048_0(MutableNetwork<Node, Edge> network)
{    for (Node node : ImmutableList.copyOf(Iterables.filter(network.nodes(), IsFlatten.INSTANCE))) {                        Node flattenPCollection = Iterables.getOnlyElement(network.successors(node));        for (Node successorInstruction : ImmutableList.copyOf(network.successors(flattenPCollection))) {            for (Edge edge : ImmutableList.copyOf(network.edgesConnecting(flattenPCollection, successorInstruction))) {                for (Node predecessorPCollection : ImmutableList.copyOf(network.predecessors(node))) {                    network.addEdge(predecessorPCollection, successorInstruction, edge.clone());                }            }        }                network.removeNode(flattenPCollection);        network.removeNode(node);    }    return network;}
public boolean beam_f8049_0(Node node)
{    return node instanceof ParallelInstructionNode && ((ParallelInstructionNode) node).getParallelInstruction().getFlatten() != null;}
public DoFn<InputT, OutputT> beam_f8059_0()
{    throw new UnsupportedOperationException(String.format("%s does not support getFn()", getClass().getCanonicalName()));}
public ParDoFn beam_f8060_1(PipelineOptions options, CloudObject cloudUserFn, @Nullable List<SideInputInfo> sideInputInfos, TupleTag<?> mainOutputTag, Map<TupleTag<?>, Integer> outputTupleTagsToReceiverIndices, final DataflowExecutionContext<?> executionContext, DataflowOperationContext operationContext) throws Exception
{    Map.Entry<TupleTag<?>, Integer> entry = Iterables.getOnlyElement(outputTupleTagsToReceiverIndices.entrySet());    checkArgument(entry.getKey().equals(mainOutputTag), "Output tags should reference only the main output tag: %s vs %s", entry.getKey(), mainOutputTag);    checkArgument(entry.getValue() == 0, "There should be a single receiver, but using receiver index %s", entry.getValue());    byte[] encodedWindowingStrategy = getBytes(cloudUserFn, PropertyNames.SERIALIZED_FN);    WindowingStrategy windowingStrategy;    try {        windowingStrategy = deserializeWindowingStrategy(encodedWindowingStrategy);    } catch (Exception e) {                if (DataflowRunner.hasExperiment(options.as(DataflowPipelineDebugOptions.class), "beam_fn_api")) {                        windowingStrategy = WindowingStrategy.globalDefault();        } else {            throw e;        }    }    byte[] serializedCombineFn = getBytes(cloudUserFn, WorkerPropertyNames.COMBINE_FN, null);    AppliedCombineFn<?, ?, ?, ?> combineFn = null;    if (serializedCombineFn != null) {        Object combineFnObj = SerializableUtils.deserializeFromByteArray(serializedCombineFn, "serialized combine fn");        checkArgument(combineFnObj instanceof AppliedCombineFn, "unexpected kind of AppliedCombineFn: " + combineFnObj.getClass().getName());        combineFn = (AppliedCombineFn<?, ?, ?, ?>) combineFnObj;    }    Map<String, Object> inputCoderObject = getObject(cloudUserFn, WorkerPropertyNames.INPUT_CODER);    Coder<?> inputCoder = CloudObjects.coderFromCloudObject(CloudObject.fromSpec(inputCoderObject));    checkArgument(inputCoder instanceof WindowedValueCoder, "Expected WindowedValueCoder for inputCoder, got: " + inputCoder.getClass().getName());    @SuppressWarnings("unchecked")    WindowedValueCoder<?> windowedValueCoder = (WindowedValueCoder<?>) inputCoder;    Coder<?> elemCoder = windowedValueCoder.getValueCoder();    checkArgument(elemCoder instanceof KvCoder, "Expected KvCoder for inputCoder, got: " + elemCoder.getClass().getName());    @SuppressWarnings("unchecked")    KvCoder<?, ?> kvCoder = (KvCoder<?, ?>) elemCoder;    boolean isStreamingPipeline = options.as(StreamingOptions.class).isStreaming();    SideInputReader sideInputReader = NullSideInputReader.empty();    @Nullable    AppliedCombineFn<?, ?, ?, ?> maybeMergingCombineFn = null;    if (combineFn != null) {        sideInputReader = executionContext.getSideInputReader(sideInputInfos, combineFn.getSideInputViews(), operationContext);        String phase = getString(cloudUserFn, WorkerPropertyNames.PHASE, CombinePhase.ALL);        checkArgument(phase.equals(CombinePhase.ALL) || phase.equals(CombinePhase.MERGE), "Unexpected phase: %s", phase);        if (phase.equals(CombinePhase.MERGE)) {            maybeMergingCombineFn = makeAppliedMergingFunction(combineFn);        } else {            maybeMergingCombineFn = combineFn;        }    }    StateInternalsFactory<?> stateInternalsFactory = key -> executionContext.getStepContext(operationContext).stateInternals();        GroupAlsoByWindowFn<?, ?> fn;        Coder<?> gabwInputCoder;        if (isStreamingPipeline) {        if (maybeMergingCombineFn == null) {            fn = StreamingGroupAlsoByWindowsDoFns.createForIterable(windowingStrategy, stateInternalsFactory, ((KvCoder) kvCoder).getValueCoder());            gabwInputCoder = WindmillKeyedWorkItem.FakeKeyedWorkItemCoder.of(kvCoder);        } else {            fn = StreamingGroupAlsoByWindowsDoFns.create(windowingStrategy, stateInternalsFactory, (AppliedCombineFn) maybeMergingCombineFn, ((KvCoder) kvCoder).getKeyCoder());            gabwInputCoder = WindmillKeyedWorkItem.FakeKeyedWorkItemCoder.of(((AppliedCombineFn) maybeMergingCombineFn).getKvCoder());        }    } else {        if (maybeMergingCombineFn == null) {            fn = BatchGroupAlsoByWindowsDoFns.createForIterable(windowingStrategy, stateInternalsFactory, ((KvCoder) kvCoder).getValueCoder());            gabwInputCoder = null;        } else {            fn = BatchGroupAlsoByWindowsDoFns.create(windowingStrategy, (AppliedCombineFn) maybeMergingCombineFn);            gabwInputCoder = null;        }    }        if (maybeMergingCombineFn != null) {        return new GroupAlsoByWindowsParDoFn(options, fn, windowingStrategy, ((AppliedCombineFn) maybeMergingCombineFn).getSideInputViews(), gabwInputCoder, sideInputReader, mainOutputTag, executionContext.getStepContext(operationContext));    } else {        return new GroupAlsoByWindowsParDoFn(options, fn, windowingStrategy, null, gabwInputCoder, sideInputReader, mainOutputTag, executionContext.getStepContext(operationContext));    }}
 static WindowingStrategy beam_f8061_0(byte[] encodedWindowingStrategy) throws InvalidProtocolBufferException
{    RunnerApi.MessageWithComponents strategyProto = RunnerApi.MessageWithComponents.parseFrom(encodedWindowingStrategy);    checkArgument(strategyProto.getRootCase() == RootCase.WINDOWING_STRATEGY, "Invalid windowing strategy: %s", strategyProto);    return WindowingStrategyTranslation.fromProto(strategyProto.getWindowingStrategy(), RehydratedComponents.forComponents(strategyProto.getComponents()));}
public Coder<List<AccumT>> beam_f8069_0(CoderRegistry registry, Coder<AccumT> inputCoder) throws CannotProvideCoderException
{    return ListCoder.of(accumCoder);}
public List<AccumT> beam_f8070_0(Context c)
{    ArrayList<AccumT> result = new ArrayList<>();    result.add(this.combineFnWithContext.createAccumulator(c));    return result;}
public List<AccumT> beam_f8071_0(List<AccumT> accumulator, AccumT input, Context c)
{    accumulator.add(input);    if (accumulator.size() < MAX_ACCUMULATOR_BUFFER_SIZE) {        return accumulator;    } else {        return mergeToSingleton(accumulator, c);    }}
public void beam_f8080_0() throws Exception
{    checkState(fnRunner != null);    fnRunner.finishBundle();    fnRunner = null;}
public void beam_f8081_0() throws Exception
{    fnRunner = null;}
private DoFnRunner<InputT, KV<K, Iterable<V>>> beam_f8082_0()
{    OutputManager outputManager = new OutputManager() {        @Override        public <T> void output(TupleTag<T> tag, WindowedValue<T> output) {            checkState(tag.equals(mainOutputTag), "Must only output to main output tag (%s), but was %s", tag, mainOutputTag);            try {                receiver.process(output);            } catch (Throwable t) {                throw new RuntimeException(t);            }        }    };    boolean hasStreamingSideInput = options.as(StreamingOptions.class).isStreaming() && !sideInputReader.isEmpty();    DoFnRunner<InputT, KV<K, Iterable<V>>> basicRunner = new GroupAlsoByWindowFnRunner<>(options, doFn, sideInputReader, outputManager, mainOutputTag, stepContext);    if (doFn instanceof StreamingGroupAlsoByWindowViaWindowSetFn) {        DoFnRunner<KeyedWorkItem<K, V>, KV<K, Iterable<V>>> streamingGABWRunner = (DoFnRunner<KeyedWorkItem<K, V>, KV<K, Iterable<V>>>) basicRunner;        if (hasStreamingSideInput) {            @SuppressWarnings("unchecked")            WindmillKeyedWorkItem.FakeKeyedWorkItemCoder<K, V> keyedWorkItemCoder = (WindmillKeyedWorkItem.FakeKeyedWorkItemCoder<K, V>) inputCoder;            StreamingSideInputFetcher<V, W> sideInputFetcher = new StreamingSideInputFetcher<>(sideInputViews, keyedWorkItemCoder.getElementCoder(), windowingStrategy, (StreamingModeExecutionContext.StreamingModeStepContext) stepContext);            streamingGABWRunner = new StreamingKeyedWorkItemSideInputDoFnRunner<>(streamingGABWRunner, keyedWorkItemCoder.getKeyCoder(), sideInputFetcher, stepContext);        }        return (DoFnRunner<InputT, KV<K, Iterable<V>>>) DoFnRunners.<K, V, Iterable<V>, W>lateDataDroppingRunner(streamingGABWRunner, stepContext.timerInternals(), windowingStrategy);    } else {        if (hasStreamingSideInput) {            return new StreamingSideInputDoFnRunner<>(basicRunner, new StreamingSideInputFetcher<>(sideInputViews, inputCoder, windowingStrategy, (StreamingModeExecutionContext.StreamingModeStepContext) stepContext));        } else {            return basicRunner;        }    }}
public boolean beam_f8090_0() throws IOException
{    return advance();}
public boolean beam_f8091_0() throws IOException
{    try (Closeable read = tracker.enterState(readState)) {        if (!groups.advance()) {            current = null;            return false;        }    }    K key = CoderUtils.decodeFromByteArray(parentReader.keyCoder, groups.getCurrent().key);    parentReader.executionContext.setKey(key);    current = new ValueInEmptyWindows<>(KV.<K, Reiterable<V>>of(key, new ValuesIterable(groups.getCurrent().values)));    return true;}
public WindowedValue<KV<K, Reiterable<V>>> beam_f8092_0() throws NoSuchElementException
{    if (current == null) {        throw new NoSuchElementException();    }    return current;}
public void beam_f8100_0()
{    base.remove();}
public ValuesIterator beam_f8101_0()
{    return new ValuesIterator(base.copy());}
public Map<String, ReaderFactory> beam_f8102_0()
{    return ImmutableMap.of("GroupingShuffleSource", new GroupingShuffleReaderFactory());}
public InMemoryReaderIterator beam_f8110_0() throws IOException
{    return new InMemoryReaderIterator();}
public boolean beam_f8111_0() throws IOException
{    Preconditions.checkState(lastReturnedIndex == null, "Already started");    if (!tracker.tryReturnRecordAt(true, startIndex)) {        current = Optional.empty();        return false;    }    current = Optional.of(decode(encodedElements.get(startIndex)));    lastReturnedIndex = startIndex;    return true;}
public boolean beam_f8112_0() throws IOException
{    Preconditions.checkNotNull(lastReturnedIndex, "Not started");    if (!tracker.tryReturnRecordAt(true, (long) lastReturnedIndex + 1)) {        current = Optional.empty();        return false;    }    ++lastReturnedIndex;    current = Optional.of(decode(encodedElements.get(lastReturnedIndex)));    return true;}
public NativeReader<?> beam_f8120_0(CloudObject spec, @Nullable Coder<?> coder, @Nullable PipelineOptions options, @Nullable DataflowExecutionContext executionContext, DataflowOperationContext operationContext) throws Exception
{    return create(spec, coder);}
 InMemoryReader<T> beam_f8121_0(CloudObject spec, Coder<T> coder) throws Exception
{    return new InMemoryReader<>(getStrings(spec, WorkerPropertyNames.ELEMENTS, Collections.<String>emptyList()), getInt(spec, WorkerPropertyNames.START_INDEX, null), getInt(spec, WorkerPropertyNames.END_INDEX, null), coder);}
public static IntrinsicMapTaskExecutor beam_f8122_0(List<Operation> operations, ExecutionStateTracker executionStateTracker)
{    return new IntrinsicMapTaskExecutor(operations, new CounterSet(), executionStateTracker);}
 OperationNode beam_f8130_0(ParallelInstructionNode node, PipelineOptions options, SinkFactory sinkFactory, DataflowExecutionContext executionContext, DataflowOperationContext context) throws Exception
{    ParallelInstruction instruction = node.getParallelInstruction();    WriteInstruction write = instruction.getWrite();    Coder<?> coder = CloudObjects.coderFromCloudObject(CloudObject.fromSpec(write.getSink().getCodec()));    CloudObject cloudSink = CloudObject.fromSpec(write.getSink().getSpec());    Sink<?> sink = sinkFactory.create(cloudSink, coder, options, executionContext, context);    return OperationNode.create(WriteOperation.create(sink, EMPTY_OUTPUT_RECEIVER_ARRAY, context));}
private OperationNode beam_f8131_0(Network<Node, Edge> network, ParallelInstructionNode node, PipelineOptions options, DataflowExecutionContext<?> executionContext, DataflowOperationContext operationContext) throws Exception
{    ParallelInstruction instruction = node.getParallelInstruction();    ParDoInstruction parDo = instruction.getParDo();    TupleTag<?> mainOutputTag = tupleTag(parDo.getMultiOutputInfos().get(0));    ImmutableMap.Builder<TupleTag<?>, Integer> outputTagsToReceiverIndicesBuilder = ImmutableMap.builder();    int successorOffset = 0;    for (Node successor : network.successors(node)) {        for (Edge edge : network.edgesConnecting(node, successor)) {            outputTagsToReceiverIndicesBuilder.put(tupleTag(((MultiOutputInfoEdge) edge).getMultiOutputInfo()), successorOffset);        }        successorOffset += 1;    }    ParDoFn fn = parDoFnFactory.create(options, CloudObject.fromSpec(parDo.getUserFn()), parDo.getSideInputs(), mainOutputTag, outputTagsToReceiverIndicesBuilder.build(), executionContext, operationContext);    OutputReceiver[] receivers = getOutputReceivers(network, node);    return OperationNode.create(new ParDoOperation(fn, receivers, operationContext));}
private static TupleTag<V> beam_f8132_0(MultiOutputInfo multiOutputInfo)
{    return new TupleTag<>(multiOutputInfo.getTag());}
public static Closeable beam_f8140_0(SideInputReadCounter readCounter)
{    SideInputReadCounter previousCounter = CURRENT_SIDE_INPUT_COUNTERS.get();    CURRENT_SIDE_INPUT_COUNTERS.set(readCounter);    return new Closeable() {        @Override        public void close() throws IOException {            if (previousCounter == null) {                CURRENT_SIDE_INPUT_COUNTERS.remove();            } else {                CURRENT_SIDE_INPUT_COUNTERS.set(previousCounter);            }        }    };}
public void beam_f8141_0() throws IOException
{    if (previousCounter == null) {        CURRENT_SIDE_INPUT_COUNTERS.remove();    } else {        CURRENT_SIDE_INPUT_COUNTERS.set(previousCounter);    }}
public static SideInputReadCounter beam_f8142_0()
{    SideInputReadCounter currentCounter = CURRENT_SIDE_INPUT_COUNTERS.get();    return currentCounter != null ? currentCounter : NoopSideInputReadCounter.INSTANCE;}
public IsmPrefixReaderIterator beam_f8150_0() throws IOException
{    SideInputReadCounter readCounter = IsmReader.getCurrentSideInputCounter();    return new LazyIsmPrefixReaderIterator(readCounter);}
public IsmPrefixReaderIterator beam_f8151_0(List<?> keyComponents) throws IOException
{    if (keyComponents.isEmpty()) {        return overKeyComponents(keyComponents, 0, new RandomAccessData(0));    }    RandomAccessData keyBytes = new RandomAccessData();    int shardId = coder.encodeAndHash(keyComponents, keyBytes);    return overKeyComponents(keyComponents, shardId, keyBytes);}
public IsmPrefixReaderIterator beam_f8152_0(List<?> keyComponents, int shardId, RandomAccessData keyBytes) throws IOException
{    checkNotNull(keyComponents);    checkNotNull(keyBytes);    SideInputReadCounter readCounter = IsmReader.getCurrentSideInputCounter();    if (keyComponents.isEmpty()) {        checkArgument(shardId == 0 && keyBytes.size() == 0, "Expected shard id to be 0 and key bytes to be empty " + "but got shard id %s and key bytes of length %s", shardId, keyBytes.size());    }    checkArgument(keyComponents.size() <= coder.getKeyComponentCoders().size(), "Expected at most %s key component(s) but received %s.", coder.getKeyComponentCoders().size(), keyComponents);    Optional<SeekableByteChannel> inChannel = initializeFooterAndShardIndex(Optional.<SeekableByteChannel>absent(), readCounter);        if (footer.getNumberOfKeys() == 0) {        return new EmptyIsmPrefixReaderIterator(keyComponents);    }        if (keyComponents.size() < coder.getNumberOfShardKeyCoders(keyComponents)) {        return new ShardAwareIsmPrefixReaderIterator(keyComponents, openIfNeeded(inChannel), readCounter);    }        if (!shardIdToShardMap.containsKey(shardId)) {        return new EmptyIsmPrefixReaderIterator(keyComponents);    }    inChannel = initializeForKeyedRead(shardId, inChannel, readCounter);    closeIfPresent(inChannel);    if (!bloomFilterMightContain(keyBytes)) {        return new EmptyIsmPrefixReaderIterator(keyComponents);    }                RandomAccessData floorKey = indexPerShard.get(shardId).floorKey(keyBytes);        RandomAccessData keyBytesUpperBound = keyBytes.increment();            Iterator<IsmShardKey> blockEntries = indexPerShard.get(shardId).subMap(floorKey, keyBytesUpperBound).values().iterator();    return new WithinShardIsmPrefixReaderIterator(keyComponents, keyBytes, keyBytesUpperBound, blockEntries, readCounter);}
private synchronized Optional<SeekableByteChannel> beam_f8160_0(Optional<SeekableByteChannel> inChannel) throws IOException
{    if (indexPerShard != null) {        checkState(bloomFilter != null, "Expected Bloom filter to have been initialized.");        return inChannel;    }    SeekableByteChannel rawChannel = openIfNeeded(inChannel);        position(rawChannel, footer.getBloomFilterPosition());    bloomFilter = ScalableBloomFilterCoder.of().decode(Channels.newInputStream(rawChannel));    indexPerShard = new HashMap<>();                Iterator<IsmShard> shardIterator = shardOffsetToShardMap.values().iterator();        if (!shardIterator.hasNext()) {        return Optional.of(rawChannel);    }                IsmShard currentShard = shardIterator.next();    while (shardIterator.hasNext()) {        IsmShard nextShard = shardIterator.next();        if (currentShard.getIndexOffset() == nextShard.getBlockOffset()) {            indexPerShard.put(currentShard.getId(), ImmutableSortedMap.<RandomAccessData, IsmShardKey>orderedBy(RandomAccessData.UNSIGNED_LEXICOGRAPHICAL_COMPARATOR).put(new RandomAccessData(0), new IsmShardKey(IsmReaderImpl.this.resourceId.toString(), new RandomAccessData(0), currentShard.getBlockOffset(), currentShard.getIndexOffset())).build());        }        currentShard = nextShard;    }        if (currentShard.getIndexOffset() == footer.getBloomFilterPosition()) {        indexPerShard.put(currentShard.getId(), ImmutableSortedMap.<RandomAccessData, IsmShardKey>orderedBy(RandomAccessData.UNSIGNED_LEXICOGRAPHICAL_COMPARATOR).put(new RandomAccessData(0), new IsmShardKey(IsmReaderImpl.this.resourceId.toString(), new RandomAccessData(0), currentShard.getBlockOffset(), currentShard.getIndexOffset())).build());    }    return Optional.of(rawChannel);}
public String beam_f8161_0()
{    return MoreObjects.toStringHelper(IsmShardKey.class).add("resource id", resourceId).add("firstKey", firstKey).add("startOffset", startOffset).add("endOffset", endOffset).toString();}
public boolean beam_f8162_0(Object obj)
{    if (!(obj instanceof IsmShardKey)) {        return false;    }    IsmShardKey key = (IsmShardKey) obj;    return startOffset == key.startOffset && endOffset == key.endOffset && Objects.equals(resourceId, key.resourceId) && Objects.equals(firstKey, key.firstKey);}
public WindowedValue<IsmRecord<V>> beam_f8170_0() throws IOException
{    RandomAccessData keyBytes = new RandomAccessData();    int shardId = coder.encodeAndHash(keyComponents, keyBytes);    Optional<SeekableByteChannel> inChannel = initializeFooterAndShardIndex(Optional.<SeekableByteChannel>absent(), readCounter);        if (!shardIdToShardMap.containsKey(shardId) || !bloomFilterMightContain(keyBytes)) {        return null;    }    inChannel = initializeForKeyedRead(shardId, inChannel, readCounter);    closeIfPresent(inChannel);    final NavigableMap<RandomAccessData, IsmShardKey> indexInShard = indexPerShard.get(shardId);    RandomAccessData end = keyBytes.increment();    final IsmShardKey cacheEntry = indexInShard.floorEntry(end).getValue();    NavigableMap<RandomAccessData, WindowedValue<IsmRecord<V>>> block;    try (Closeable readerCloser = IsmReader.setSideInputReadContext(readCounter)) {        block = fetch(cacheEntry);    }    RandomAccessData lastKey = block.lastKey();        if (RandomAccessData.UNSIGNED_LEXICOGRAPHICAL_COMPARATOR.compare(keyBytes, lastKey) > 0) {        return null;    }    Entry<RandomAccessData, WindowedValue<IsmRecord<V>>> rval = block.floorEntry(end);        if (RandomAccessData.UNSIGNED_LEXICOGRAPHICAL_COMPARATOR.commonPrefixLength(keyBytes, rval.getKey()) == keyBytes.size()) {        return rval.getValue();    }    return null;}
public boolean beam_f8171_0() throws IOException
{    return false;}
public boolean beam_f8172_0() throws IOException
{    return false;}
public boolean beam_f8180_0() throws IOException
{        while (iterator == null || !iterator.hasNext()) {                if (!blockEntriesIterator.hasNext()) {            return false;        }        NavigableMap<RandomAccessData, WindowedValue<IsmRecord<V>>> map;        try (Closeable counterCloseable = IsmReader.setSideInputReadContext(readCounter)) {            IsmShardKey nextBlock = blockEntriesIterator.next();            map = fetch(nextBlock);        }        SortedMap<RandomAccessData, WindowedValue<IsmRecord<V>>> submap = map.subMap(prefix, prefixUpperBound);        Collection<WindowedValue<IsmRecord<V>>> values = submap.values();        iterator = values.iterator();    }    current = Optional.of(iterator.next());    return true;}
public WindowedValue<IsmRecord<V>> beam_f8181_0() throws NoSuchElementException
{    if (!current.isPresent()) {        throw new NoSuchElementException();    }    return current.get();}
public boolean beam_f8182_0() throws IOException
{    checkState(delegate.start(), "Expected each shard to contain at least one entry");    return true;}
public RandomAccessData beam_f8190_0()
{    if (!current.isPresent()) {        throw new NoSuchElementException();    }    return keyBytes;}
private static void beam_f8191_0(InputStream inStream, RandomAccessData keyBytes) throws IOException
{    KeyPrefix keyPrefix = KeyPrefixCoder.of().decode(inStream);        keyBytes.readFrom(inStream, keyPrefix.getSharedKeySize(), /* start to overwrite the previous key at sharedKeySize */    keyPrefix.getUnsharedKeySize());        keyBytes.resetTo(keyPrefix.getSharedKeySize() + keyPrefix.getUnsharedKeySize());}
private void beam_f8192_0(Optional<SeekableByteChannel> inChannel) throws IOException
{    if (inChannel.isPresent()) {        inChannel.get().close();    }}
public SeekableByteChannel beam_f8201_0(long newPosition) throws IOException
{    checkArgument(newPosition >= offset && newPosition <= offset + data.length, "Cannot seek to position %s which is outside of cached data range [%s, %s].", newPosition, offset, offset + data.length);    localPosition = Ints.checkedCast(newPosition - offset);    return this;}
public long beam_f8202_0() throws IOException
{        return offset + data.length;}
public SeekableByteChannel beam_f8203_0(long size) throws IOException
{    throw new NonWritableChannelException();}
private T beam_f8211_0(TupleTag<?> viewTag, W window) throws IOException
{    @SuppressWarnings({ "rawtypes", "unchecked" })    List<IsmReader<WindowedValue<T>>> readers = (List) tagToIsmReaderMap.get(viewTag);    List<IsmReader<WindowedValue<T>>.IsmPrefixReaderIterator> readerIterators = findAndStartReaders(readers, ImmutableList.of(window));        if (readerIterators.isEmpty()) {        return (T) ImmutableMap.of();    }    checkState(readerIterators.size() == 1, "Expected only one Ism file to contain the singleton record for window %s", window);    return readerIterators.get(0).getCurrent().getValue().getValue().getValue();}
private List<T> beam_f8212_0(TupleTag<?> tag, W window) throws IOException
{    @SuppressWarnings({ "unchecked", "rawtypes" })    List<IsmReader<WindowedValue<T>>> readers = (List) tagToIsmReaderMap.get(tag);    List<IsmReader<WindowedValue<T>>.IsmPrefixReaderIterator> readerIterators = findAndStartReaders(readers, ImmutableList.of(window));    if (readerIterators.isEmpty()) {        return Collections.emptyList();    }    checkState(GlobalWindow.INSTANCE.equals(window) || readerIterators.size() == 1, "Expected to have had PCollectionView %s for window %s contained within one file.", tag, window);    return Collections.unmodifiableList(new ListOverReaderIterators<>(readerIterators, (WindowedValue<T> value) -> value.getValue()));}
private Map<K, V> beam_f8213_0(TupleTag<?> tag, W window) throws IOException
{    @SuppressWarnings({ "unchecked", "rawtypes" })    List<IsmReader<WindowedValue<V>>> readers = (List) tagToIsmReaderMap.get(tag);    if (readers.isEmpty()) {        return Collections.emptyMap();    }    @SuppressWarnings("unchecked")    Coder<K> keyCoder = ((MetadataKeyCoder<K>) readers.get(0).getCoder().getKeyComponentCoder(0)).getKeyCoder();    Long size = findMetadata(readers, ImmutableList.of(IsmFormat.getMetadataKey(), window, 0L), VarLongCoder.of());        if (size == null) {        return Collections.emptyMap();    }    return Collections.unmodifiableMap(new MapOverReaders<K, V, V, W>(window, new MapToValue<K, V>(), readers, keyCoder, size));}
public int beam_f8221_0()
{    return Ints.checkedCast(longSize());}
public Iterator<V> beam_f8222_0()
{    return listIterator();}
public ListIterator<V> beam_f8223_0()
{    return new ListIteratorOverReaderIterators();}
public int beam_f8231_0()
{    return Ints.checkedCast(position - 1);}
public void beam_f8232_0()
{    throw new UnsupportedOperationException();}
public void beam_f8233_0(V e)
{    throw new UnsupportedOperationException();}
public int beam_f8241_0()
{    return Ints.checkedCast(size);}
public boolean beam_f8242_0()
{    return position < size;}
public Entry<K, V2> beam_f8243_0()
{    if (!hasNext()) {        throw new NoSuchElementException();    }    final K key;    final V2 value;    try {        key = findMetadata(readers, ImmutableList.of(IsmFormat.getMetadataKey(), window, position + 1), keyCoder);        value = get(key);    } catch (IOException e) {        throw new IllegalStateException(e);    }            position += 1;    return new StructuralMapEntry<>(keyCoder, key, value);}
 long beam_f8251_0()
{    return BLOCK_SIZE_BYTES;}
public long beam_f8252_0(WindowedValue<IsmRecord<V>> windowedRecord) throws IOException
{        IsmRecord<V> record = windowedRecord.getValue();    checkArgument(coder.getKeyComponentCoders().size() == record.getKeyComponents().size());    List<Integer> keyOffsetPositions = new ArrayList<>();    final int currentShard = coder.encodeAndHash(record.getKeyComponents(), currentKeyBytes, keyOffsetPositions);        for (Integer offsetPosition : keyOffsetPositions) {        bloomFilterBuilder.put(currentKeyBytes.array(), 0, offsetPosition);    }        if (previousShard.isPresent() && currentShard != previousShard.get()) {                finishShard();    }    long positionOfCurrentKey = out.getCount();            int sharedKeySize;    if (!previousShard.isPresent()) {        sharedKeySize = 0;                        IsmShard ismShard = IsmShard.of(currentShard, positionOfCurrentKey);        checkState(shardKeyToShardMap.put(currentShard, ismShard) == null, "Unexpected insertion of keys %s for shard which already exists %s. " + "Ism files expect that all shards are written contiguously.", record.getKeyComponents(), ismShard);    } else {        sharedKeySize = commonPrefixLengthWithOrderCheck(previousKeyBytes, currentKeyBytes);    }        int unsharedKeySize = currentKeyBytes.size() - sharedKeySize;    KeyPrefix keyPrefix = KeyPrefix.of(sharedKeySize, unsharedKeySize);    KeyPrefixCoder.of().encode(keyPrefix, out);    currentKeyBytes.writeTo(out, sharedKeySize, unsharedKeySize);    if (IsmFormat.isMetadataKey(record.getKeyComponents())) {        ByteArrayCoder.of().encode(record.getMetadata(), out);    } else {        coder.getValueCoder().encode(record.getValue(), out);    }        if (positionOfCurrentKey > lastIndexedPosition + getBlockSize()) {                                int sharedIndexKeySize = commonPrefixLengthWithOrderCheck(lastIndexKeyBytes, currentKeyBytes);        int unsharedIndexKeySize = currentKeyBytes.size() - sharedIndexKeySize;        KeyPrefix indexKeyPrefix = KeyPrefix.of(sharedIndexKeySize, unsharedIndexKeySize);        KeyPrefixCoder.of().encode(indexKeyPrefix, indexOut.asOutputStream());        currentKeyBytes.writeTo(indexOut.asOutputStream(), sharedIndexKeySize, unsharedIndexKeySize);        VarInt.encode(positionOfCurrentKey, indexOut.asOutputStream());        lastIndexKeyBytes.resetTo(0);        currentKeyBytes.writeTo(lastIndexKeyBytes.asOutputStream(), 0, currentKeyBytes.size());        lastIndexedPosition = out.getCount();    }        previousShard = Optional.of(currentShard);        RandomAccessData temp = previousKeyBytes;    previousKeyBytes = currentKeyBytes;    currentKeyBytes = temp;    currentKeyBytes.resetTo(0);    numberOfKeysWritten += 1;    return out.getCount() - positionOfCurrentKey;}
private int beam_f8253_0(RandomAccessData prevKeyBytes, RandomAccessData currentKeyBytes)
{    int offset = RandomAccessData.UNSIGNED_LEXICOGRAPHICAL_COMPARATOR.commonPrefixLength(prevKeyBytes, currentKeyBytes);    int compare = RandomAccessData.UNSIGNED_LEXICOGRAPHICAL_COMPARATOR.compare(prevKeyBytes, currentKeyBytes, offset);    if (compare < 0) {        return offset;    } else if (compare == 0) {        throw new IllegalArgumentException(IsmSinkWriter.class.getSimpleName() + " expects keys to be written in strictly increasing order but was given " + prevKeyBytes + " as the previous key and " + currentKeyBytes + " as the current key. Expected " + prevKeyBytes.array()[offset + 1] + " <= " + currentKeyBytes.array()[offset + 1] + " at position " + (offset + 1) + ".");    } else {        throw new IllegalArgumentException(IsmSinkWriter.class.getSimpleName() + " expects keys to be written in strictly increasing order but was given " + prevKeyBytes + " as the previous key and " + currentKeyBytes + " as the current key. Expected length of previous key " + prevKeyBytes.size() + " <= " + currentKeyBytes.size() + " to current key.");    }}
public T beam_f8261_0(PCollectionView<T> view, BoundedWindow window)
{    return lazyInitSideInputReader.get().get(view, window);}
public boolean beam_f8262_0(PCollectionView<T> view)
{    return tupleTags.contains(view.getTagInternal());}
public boolean beam_f8263_0()
{    return tupleTags.isEmpty();}
public OutputStream beam_f8271_0()
{    try {        String filename = filepath + "." + formatter.format(new Date()) + ".log";        return new BufferedOutputStream(new FileOutputStream(new File(filename), true));    } catch (IOException e) {        throw new RuntimeException(e);    }}
private void beam_f8272_0() throws IOException
{    CountingOutputStream stream = new CountingOutputStream(outputStreamFactory.get());    generator = generatorFactory.createGenerator(stream, JsonEncoding.UTF8);    counter = stream;        generator.setPrettyPrinter(new MinimalPrettyPrinter(""));}
private void beam_f8273_0()
{    if (counter.getCount() < sizeLimit) {        return;    }    try {        JsonGenerator old = generator;        createOutputStream();        try {                        old.close();        } catch (IOException | RuntimeException e) {            reportFailure("Unable to close old log file", e, ErrorManager.CLOSE_FAILURE);        }    } catch (IOException | RuntimeException e) {        reportFailure("Unable to create new log file", e, ErrorManager.OPEN_FAILURE);    }}
public static DataflowWorkerLoggingHandler beam_f8281_0()
{    if (!initialized) {        throw new RuntimeException("getSdkLoggingHandler() called before initialize()");    }    return sdkLoggingHandler;}
private static Level beam_f8282_0(DataflowWorkerLoggingOptions.Level level)
{    return LEVELS.inverse().get(level);}
public static synchronized void beam_f8283_0()
{    if (!initialized) {        return;    }    configuredLoggers = Lists.newArrayList();    System.setOut(originalStdOut);    System.setErr(originalStdErr);    JulHandlerPrintStreamAdapterFactory.reset();    initialized = false;}
public static String beam_f8291_0()
{    return workerId.get();}
public static String beam_f8292_0()
{    return workId.get();}
public static String beam_f8293_0()
{    return sdkHarnessId.get();}
public T beam_f8301_0(String stepName)
{    return containers.computeIfAbsent(stepName, unused -> createContainer(stepName));}
public FluentIterable<T> beam_f8302_0()
{    return FluentIterable.from(containers.values());}
public Closeable beam_f8304_0(ExecutionStateTracker e)
{    return MetricsEnvironment.scopedMetricsContainer(new DataflowMetricsContainer(e));}
public boolean beam_f8312_0(Object other)
{    if (this == other) {        return true;    }    if (!(other instanceof KeyAndComputation)) {        return false;    }    KeyAndComputation that = (KeyAndComputation) other;    return this.key.equals(that.key) && this.computation.equals(that.computation);}
public int beam_f8313_0()
{    return Objects.hash(key, computation);}
public void beam_f8314_0()
{    if (useStreamingRequests) {        streamPool = new WindmillServerStub.StreamPool<>(NUM_STREAMS, STREAM_TIMEOUT, this.server::getDataStream);    } else {        for (int i = 0; i < NUM_THREADS; i++) {            readPool.add(new Thread("GetDataThread" + i) {                @Override                public void run() {                    getDataLoop();                }            });            readPool.get(i).start();        }    }}
public CounterSet beam_f8323_0()
{    return counterSet;}
public SourceOperationResponse beam_f8324_0()
{    return response;}
public void beam_f8325_0() throws Exception
{    SourceSplitRequest split = request.getSplit();    if (split != null) {        this.response = new SourceOperationResponse().setSplit(new SourceSplitResponse().setOutcome("SOURCE_SPLIT_OUTCOME_USE_CURRENT"));    } else {        throw new UnsupportedOperationException("Unsupported source operation request: " + request);    }}
public Integer beam_f8333_0(PipelineOptions options)
{    StreamingDataflowWorkerOptions streamingOptions = options.as(StreamingDataflowWorkerOptions.class);    return streamingOptions.isEnableStreamingEngine() ? Integer.MAX_VALUE : 1;}
public void beam_f8334_0(byte[] value)
{            int encodedLength = 2;    for (byte b : value) {        if ((b == ESCAPE1) || (b == ESCAPE2)) {            encodedLength += 2;        } else {            encodedLength++;        }    }    byte[] encodedArray = new byte[encodedLength];    int copyStart = 0;    int outIndex = 0;    for (int i = 0; i < value.length; i++) {        byte b = value[i];        if (b == ESCAPE1) {            System.arraycopy(value, copyStart, encodedArray, outIndex, i - copyStart);            outIndex += i - copyStart;            encodedArray[outIndex++] = ESCAPE1;            encodedArray[outIndex++] = NULL_CHARACTER;            copyStart = i + 1;        } else if (b == ESCAPE2) {            System.arraycopy(value, copyStart, encodedArray, outIndex, i - copyStart);            outIndex += i - copyStart;            encodedArray[outIndex++] = ESCAPE2;            encodedArray[outIndex++] = FF_CHARACTER;            copyStart = i + 1;        }    }    if (copyStart < value.length) {        System.arraycopy(value, copyStart, encodedArray, outIndex, value.length - copyStart);        outIndex += value.length - copyStart;    }    encodedArray[outIndex++] = ESCAPE1;    encodedArray[outIndex] = SEPARATOR;    encodedArrays.add(encodedArray);}
public void beam_f8335_0(long value)
{                    byte[] bufer = new byte[9];    int len = 0;    while (value != 0) {        len++;        bufer[9 - len] = (byte) (value & 0xff);        value >>>= 8;    }    bufer[9 - len - 1] = (byte) len;    len++;    byte[] encodedArray = new byte[len];    System.arraycopy(bufer, 9 - len, encodedArray, 0, len);    encodedArrays.add(encodedArray);}
public long beam_f8343_0()
{    if ((encodedArrays == null) || (encodedArrays.isEmpty()) || ((encodedArrays.get(0)).length - firstArrayPosition < 1)) {        throw new IllegalArgumentException("Invalid encoded byte array");    }    byte[] store = encodedArrays.get(0);    long xorMask = ((store[firstArrayPosition] & 0x80) == 0) ? ~0L : 0L;                int firstByte = (store[firstArrayPosition] & 0xff) ^ (int) (xorMask & 0xff);        int len;    long x;    if (firstByte != 0xff) {        len = 7 - log2Floor(firstByte ^ 0xff);        if (store.length - firstArrayPosition < len) {            throw new IllegalArgumentException("Invalid encoded byte array");        }                x = xorMask;        for (int i = firstArrayPosition; i < firstArrayPosition + len; i++) {            x = (x << 8) | (store[i] & 0xff);        }    } else {        len = 8;        if (store.length - firstArrayPosition < len) {            throw new IllegalArgumentException("Invalid encoded byte array");        }        int secondByte = (store[firstArrayPosition + 1] & 0xff) ^ (int) (xorMask & 0xff);        if (secondByte >= 0x80) {            if (secondByte < 0xc0) {                len = 9;            } else {                int thirdByte = (store[firstArrayPosition + 2] & 0xff) ^ (int) (xorMask & 0xff);                if (secondByte == 0xc0 && thirdByte < 0x80) {                    len = 10;                } else {                                        throw new IllegalArgumentException("Invalid encoded byte array");                }            }            if (store.length - firstArrayPosition < len) {                throw new IllegalArgumentException("Invalid encoded byte array");            }        }        x = Longs.fromByteArray(Arrays.copyOfRange(store, firstArrayPosition + len - 8, firstArrayPosition + len));    }        x ^= LENGTH_TO_MASK[len];    if (len != getSignedEncodingLength(x)) {        throw new IllegalArgumentException("Invalid encoded byte array");    }    if ((store.length - firstArrayPosition - len) == 0) {                encodedArrays.remove(0);        firstArrayPosition = 0;    } else {        firstArrayPosition = firstArrayPosition + len;    }    return x;}
public boolean beam_f8344_0()
{    if ((encodedArrays == null) || (encodedArrays.isEmpty()) || ((encodedArrays.get(0)).length - firstArrayPosition < 1)) {        throw new IllegalArgumentException("Invalid encoded byte array");    }    byte[] store = encodedArrays.get(0);    if (store.length - firstArrayPosition < 2) {        return false;    }    if ((store[firstArrayPosition] == ESCAPE2) && (store[firstArrayPosition + 1] == INFINITY)) {        if ((store.length - firstArrayPosition - 2) == 0) {                        encodedArrays.remove(0);            firstArrayPosition = 0;        } else {            firstArrayPosition = firstArrayPosition + 2;        }        return true;    } else {        return false;    }}
public byte[] beam_f8345_0()
{        if ((encodedArrays == null) || (encodedArrays.size() != 1)) {        throw new IllegalArgumentException("Invalid encoded byte array");    }    byte[] store = encodedArrays.get(0);    encodedArrays.remove(0);    assert encodedArrays.isEmpty();    return Arrays.copyOfRange(store, firstArrayPosition, store.length);}
 static ParDoFn beam_f8356_0(PipelineOptions options, KvCoder<K, ?> inputElementCoder, @Nullable AppliedCombineFn<K, InputT, AccumT, ?> combineFn, SideInputReader sideInputReader, Receiver receiver, @Nullable StepContext stepContext) throws Exception
{    Coder<K> keyCoder = inputElementCoder.getKeyCoder();    Coder<?> valueCoder = inputElementCoder.getValueCoder();    if (combineFn == null) {        @SuppressWarnings("unchecked")        Coder<InputT> inputCoder = (Coder<InputT>) valueCoder;        GroupingTable<?, ?, ?> groupingTable = GroupingTables.bufferingAndSampling(new WindowingCoderGroupingKeyCreator<>(keyCoder), PairInfo.create(), new CoderSizeEstimator<>(WindowedValue.getValueOnlyCoder(keyCoder)), new CoderSizeEstimator<>(inputCoder), 0.001);        return new SimplePartialGroupByKeyParDoFn<>(groupingTable, receiver);    } else {        GroupingTables.Combiner<WindowedValue<K>, InputT, AccumT, ?> valueCombiner = new ValueCombiner<>(GlobalCombineFnRunners.create(combineFn.getFn()), sideInputReader, options);        GroupingTable<WindowedValue<K>, InputT, AccumT> groupingTable = GroupingTables.combiningAndSampling(new WindowingCoderGroupingKeyCreator<>(keyCoder), PairInfo.create(), valueCombiner, new CoderSizeEstimator<>(WindowedValue.getValueOnlyCoder(keyCoder)), new CoderSizeEstimator<>(combineFn.getAccumulatorCoder()), 0.001);        if (sideInputReader.isEmpty()) {            return new SimplePartialGroupByKeyParDoFn<>(groupingTable, receiver);        } else if (options.as(StreamingOptions.class).isStreaming()) {            StreamingSideInputFetcher<KV<K, InputT>, ?> sideInputFetcher = new StreamingSideInputFetcher<>(combineFn.getSideInputViews(), combineFn.getKvCoder(), combineFn.getWindowingStrategy(), (StreamingModeExecutionContext.StreamingModeStepContext) stepContext);            return new StreamingSideInputPGBKParDoFn<>(groupingTable, receiver, sideInputFetcher);        } else {            return new BatchSideInputPGBKParDoFn<>(groupingTable, receiver);        }    }}
public AccumT beam_f8357_0(WindowedValue<K> windowedKey)
{    return this.combineFn.createAccumulator(options, sideInputReader, windowedKey.getWindows());}
public AccumT beam_f8358_0(WindowedValue<K> windowedKey, AccumT accumulator, InputT value)
{    return this.combineFn.addInput(accumulator, value, options, sideInputReader, windowedKey.getWindows());}
public Object beam_f8366_0(WindowedValue<K> key) throws Exception
{        return WindowedValue.of(coder.structuralValue(key.getValue()), ignored, key.getWindows(), key.getPane());}
protected void beam_f8367_0(long elementSize)
{    observedSize += elementSize;}
public long beam_f8368_0(T value) throws Exception
{        Observer observer = new Observer();    coder.registerByteSizeObserver(value, observer);    if (!observer.getIsLazy()) {        observer.advance();        return observer.observedSize;    } else {                        CountingOutputStream os = new CountingOutputStream(ByteStreams.nullOutputStream());        coder.encode(value, os);        return os.getCount();    }}
 PartitioningShuffleReaderIterator<K, V> beam_f8381_0(ShuffleEntryReader reader)
{    return new PartitioningShuffleReaderIterator<>(this, reader);}
public boolean beam_f8382_0() throws IOException
{    return advance();}
public boolean beam_f8383_0() throws IOException
{    if (!iterator.hasNext()) {        current = null;        return false;    }    ShuffleEntry record = iterator.next();    K key = CoderUtils.decodeFromByteArray(shuffleReader.keyCoder, record.getKey());    WindowedValue<V> windowedValue = CoderUtils.decodeFromByteArray(shuffleReader.windowedValueCoder, record.getValue());    shuffleReader.notifyElementRead(record.length());    current = windowedValue.withValue(KV.of(key, windowedValue.getValue()));    return true;}
public BoundedWindow beam_f8391_0()
{    return window;}
public boolean beam_f8392_0(Object otherObject)
{    if (!(otherObject instanceof PCollectionViewWindow)) {        return false;    }    @SuppressWarnings("unchecked")    PCollectionViewWindow<T> other = (PCollectionViewWindow<T>) otherObject;    return getView().equals(other.getView()) && getWindow().equals(other.getWindow());}
public int beam_f8393_0()
{    return Objects.hash(getView(), getWindow());}
public ProfileScope beam_f8401_0(ProfilerWrapper profiler)
{    return NoopProfileScope.NOOP;}
public ProfileScope beam_f8402_0(ProfilerWrapper profiler)
{    return NoopProfileScope.NOOP;}
private ProfilingState beam_f8403_1()
{    try {                profiler.getAttribute();                                return ProfilingState.PROFILING_PRESENT;    } catch (UnsatisfiedLinkError e) {                        return ProfilingState.PROFILING_ABSENT;    }}
protected WindowedValue<T> beam_f8412_0(Windmill.Message message) throws IOException
{    T value;    InputStream data = message.getData().newInput();    notifyElementRead(data.available());    if (parseFn != null) {        Pubsub.PubsubMessage pubsubMessage = Pubsub.PubsubMessage.parseFrom(data);        value = parseFn.apply(new PubsubMessage(pubsubMessage.getData().toByteArray(), pubsubMessage.getAttributesMap(), pubsubMessage.getMessageId()));    } else {        value = coder.decode(data, Coder.Context.OUTER);    }    return WindowedValue.timestampedValueInGlobalWindow(value, WindmillTimeUtils.windmillToHarnessTimestamp(message.getTimestamp()));}
public boolean beam_f8413_0()
{    return true;}
public Map<String, SinkFactory> beam_f8414_0()
{    Factory factory = new Factory();    return ImmutableMap.of("PubsubSink", factory, "org.apache.beam.runners.dataflow.worker.PubsubSink", factory);}
private void beam_f8422_1(KV<String, ByteString> key, CacheEntry entry)
{    try {        entry.reader.close();    } catch (IOException e) {            }}
 UnboundedSource.UnboundedReader<?> beam_f8423_0(String computationId, ByteString splitId, long cacheToken)
{    KV<String, ByteString> key = KV.of(computationId, splitId);    CacheEntry entry = cache.asMap().remove(key);    cache.cleanUp();    if (entry != null) {        if (entry.token == cacheToken) {            return entry.reader;        } else {                        closeReader(key, entry);        }    }    return null;}
 void beam_f8424_0(String computationId, ByteString splitId, long cacheToken, UnboundedSource.UnboundedReader<?> reader)
{    CacheEntry existing = cache.asMap().putIfAbsent(KV.of(computationId, splitId), new CacheEntry(reader, cacheToken));    Preconditions.checkState(existing == null, "Overwriting existing readers is not allowed");    cache.cleanUp();}
public static List<T> beam_f8432_0(NativeReader.NativeReaderIterator<T> reader, int n) throws IOException
{    return readNItemsFromIterator(reader, n, false);}
public static List<T> beam_f8433_0(NativeReader.NativeReaderIterator<T> reader, boolean started) throws IOException
{    return readNItemsFromIterator(reader, Integer.MAX_VALUE, started);}
public ParDoFn beam_f8434_0(PipelineOptions options, CloudObject cloudUserFn, @Nullable List<SideInputInfo> sideInputInfos, TupleTag<?> mainOutputTag, Map<TupleTag<?>, Integer> outputTupleTagsToReceiverIndices, DataflowExecutionContext<?> executionContext, DataflowOperationContext operationContext) throws Exception
{    return new ReifyTimestampAndWindowsParDoFn();}
public String beam_f8445_0()
{    return "kind:ism_record";}
private static CloudObjectTranslator<T> beam_f8446_0(final Class<T> coderClass)
{        InstanceBuilder.ofType(coderClass).fromFactoryMethod("of").build();    return new CloudObjectTranslator<T>() {        @Override        public CloudObject toCloudObject(T target, SdkComponents sdkComponents) {            throw new UnsupportedOperationException();        }        @Override        public T fromCloudObject(CloudObject cloudObject) {            return InstanceBuilder.ofType(coderClass).fromFactoryMethod("of").build();        }        @Override        public Class<? extends T> getSupportedClass() {            return coderClass;        }        @Override        public String cloudObjectClassName() {            return CloudObject.forClass(coderClass).getClassName();        }    };}
public CloudObject beam_f8447_0(T target, SdkComponents sdkComponents)
{    throw new UnsupportedOperationException();}
public void beam_f8455_1(FnApiControlClient controlClient)
{    Preconditions.checkNotNull(controlClient, "Control client can not be null.");    WorkCountingSdkWorkerHarness sdkWorkerHarness = new WorkCountingSdkWorkerHarness(controlClient);    workerMap.put(controlClient, sdkWorkerHarness);    workers.add(sdkWorkerHarness);    }
public void beam_f8456_1(FnApiControlClient controlClient)
{        WorkCountingSdkWorkerHarness worker = workerMap.remove(controlClient);    if (worker != null) {        worker.closed.set(true);        workers.remove(worker);    }                    sdkHarnessesAreHealthy.set(false);    }
public boolean beam_f8457_0()
{    return sdkHarnessesAreHealthy.get();}
public GrpcFnServer<GrpcStateService> beam_f8465_0()
{    return GrpcFnServer.create(beamFnStateService, beamFnDataApiServiceDescriptor());}
public FnApiControlClient beam_f8466_0()
{    return null;}
public String beam_f8467_0()
{    return null;}
public ApiServiceDescriptor beam_f8476_0()
{    return null;}
 static void beam_f8477_0()
{    ShuffleLibraryLoader.load();}
public static ShuffleKind beam_f8479_0(String shuffleKind) throws Exception
{    try {        return Enum.valueOf(ShuffleKind.class, shuffleKind.trim().toUpperCase());    } catch (IllegalArgumentException e) {        throw new Exception("unexpected shuffle_kind", e);    }}
public SinkWriter<WindowedValue<T>> beam_f8487_0() throws IOException
{    Preconditions.checkArgument(shuffleWriterConfig != null);    ApplianceShuffleWriter applianceWriter = new ApplianceShuffleWriter(shuffleWriterConfig, SHUFFLE_WRITER_BUFFER_SIZE, operationContext);    String datasetId = applianceWriter.getDatasetId();    return writer(applianceWriter, datasetId);}
public Map<String, SinkFactory> beam_f8488_0()
{    ShuffleSinkFactory factory = new ShuffleSinkFactory();    return ImmutableMap.of("ShuffleSink", factory, "org.apache.beam.runners.dataflow.worker.runners.worker.ShuffleSink", factory);}
public ShuffleSink<?> beam_f8489_0(CloudObject spec, Coder<?> coder, @Nullable PipelineOptions options, DataflowExecutionContext executionContext, DataflowOperationContext operationContext) throws Exception
{    checkArgument(coder != null, "coder must not be null");    @SuppressWarnings("unchecked")    Coder<WindowedValue<Object>> typedCoder = (Coder<WindowedValue<Object>>) coder;    return new ShuffleSink<>(options, decodeBase64(getString(spec, WorkerPropertyNames.SHUFFLE_WRITER_CONFIG, null)), parseShuffleKind(getString(spec, WorkerPropertyNames.SHUFFLE_KIND)), typedCoder, (BatchModeExecutionContext) executionContext, operationContext);}
public DoFnRunner<InputT, OutputT> beam_f8497_0(DoFn<InputT, OutputT> fn, PipelineOptions options, TupleTag<OutputT> mainOutputTag, List<TupleTag<?>> sideOutputTags, Iterable<PCollectionView<?>> sideInputViews, SideInputReader sideInputReader, Coder<InputT> inputCoder, Map<TupleTag<?>, Coder<?>> outputCoders, WindowingStrategy<?, ?> windowingStrategy, DataflowExecutionContext.DataflowStepContext stepContext, DataflowExecutionContext.DataflowStepContext userStepContext, OutputManager outputManager, DoFnSchemaInformation doFnSchemaInformation, Map<String, PCollectionView<?>> sideInputMapping)
{    DoFnRunner<InputT, OutputT> fnRunner = DoFnRunners.simpleRunner(options, fn, sideInputReader, outputManager, mainOutputTag, sideOutputTags, userStepContext, inputCoder, outputCoders, windowingStrategy, doFnSchemaInformation, sideInputMapping);    boolean hasStreamingSideInput = options.as(StreamingOptions.class).isStreaming() && !sideInputReader.isEmpty();    if (hasStreamingSideInput) {        return new StreamingSideInputDoFnRunner<>(fnRunner, new StreamingSideInputFetcher<>(sideInputViews, inputCoder, windowingStrategy, (StreamingModeExecutionContext.StreamingModeStepContext) userStepContext));    }    return fnRunner;}
private OutputsPerElementTracker beam_f8498_1()
{        if (!hasExperiment(OUTPUTS_PER_ELEMENT_EXPERIMENT)) {        return NoopOutputsPerElementTracker.INSTANCE;    }            return new OutputsPerElementTrackerImpl();}
private boolean beam_f8499_0(String experiment)
{    List<String> experiments = options.as(DataflowPipelineDebugOptions.class).getExperiments();    return experiments != null && experiments.contains(experiment);}
public void beam_f8510_0(TupleTag<T> tag, WindowedValue<T> output)
{    outputsPerElementTracker.onOutput();    Receiver receiver = getReceiverOrNull(tag);    if (receiver == null) {                                String outputName = "implicit-" + tag.getId();                                        OutputReceiver undeclaredReceiver = new OutputReceiver();        ElementCounter outputCounter = new DataflowOutputCounter(outputName, counterFactory, stepContext.getNameContext());        undeclaredReceiver.addOutputCounter(outputCounter);        undeclaredOutputs.put(tag, undeclaredReceiver);        receiver = undeclaredReceiver;    }    try {        receiver.process(output);    } catch (RuntimeException | Error e) {                throw e;    } catch (Exception e) {                throw new RuntimeException(e);    }}
public void beam_f8511_0(Object untypedElem) throws Exception
{    if (fnRunner == null) {                try (Closeable start = operationContext.enterStart()) {            reallyStartBundle();        }    }    WindowedValue<InputT> elem = (WindowedValue<InputT>) untypedElem;    if (fnSignature != null && fnSignature.stateDeclarations().size() > 0) {        registerStateCleanup((WindowingStrategy<?, BoundedWindow>) getDoFnInfo().getWindowingStrategy(), (Collection<BoundedWindow>) elem.getWindows());    }    outputsPerElementTracker.onProcessElement();    fnRunner.processElement(elem);    outputsPerElementTracker.onProcessElementSuccess();}
public void beam_f8512_0() throws Exception
{                        Coder<BoundedWindow> windowCoder = (Coder<BoundedWindow>) (fnInfo != null ? fnInfo : doFnInstanceManager.peek()).getWindowingStrategy().getWindowFn().windowCoder();    processTimers(TimerType.USER, userStepContext, windowCoder);    processTimers(TimerType.SYSTEM, stepContext, windowCoder);}
private void beam_f8520_0(WindowingStrategy<?, W> windowingStrategy, Collection<W> windowsToCleanup)
{    Coder<W> windowCoder = windowingStrategy.getWindowFn().windowCoder();    for (W window : windowsToCleanup) {                                stepContext.setStateCleanupTimer(CLEANUP_TIMER_ID, window, windowCoder, earliestAllowableCleanupTime(window, windowingStrategy));    }}
private Instant beam_f8521_0(BoundedWindow window, WindowingStrategy windowingStrategy)
{    return window.maxTimestamp().plus(windowingStrategy.getAllowedLateness()).plus(1L);}
 DoFnInfo<?, ?> beam_f8522_0()
{    return fnInfo;}
public void beam_f8530_0() throws IOException
{    underlyingWriter.close();}
public void beam_f8531_0() throws IOException
{    underlyingWriter.abort();}
public static SourceOperationExecutor beam_f8532_0(PipelineOptions options, SourceOperationRequest request, CounterSet counters, DataflowExecutionContext<?> executionContext, String stageName) throws Exception
{    boolean beamFnApi = DataflowRunner.hasExperiment(options.as(DataflowPipelineDebugOptions.class), "beam_fn_api");    Preconditions.checkNotNull(request, "SourceOperationRequest must be non-null");    Preconditions.checkNotNull(executionContext, "executionContext must be non-null");        if (beamFnApi) {        return new NoOpSourceOperationExecutor(request);    } else {        DataflowOperationContext operationContext = executionContext.createOperationContext(NameContext.create(stageName, request.getOriginalName(), request.getSystemName(), request.getName()));        return new WorkerCustomSourceOperationExecutor(options, request, counters, executionContext, operationContext);    }}
public int beam_f8540_0()
{    return cloudProgress.hashCode();}
public boolean beam_f8541_0(Object obj)
{    if (this == obj) {        return true;    } else if (!(obj instanceof DataflowReaderProgress)) {        return false;    } else {        return Objects.equals(cloudProgress, ((DataflowReaderProgress) obj).cloudProgress);    }}
public String beam_f8542_0()
{    return String.valueOf(cloudPosition);}
public DoFnInfo<?, ?> beam_f8550_0(CloudObject cloudUserFn) throws Exception
{    DoFnInfo<?, ?> doFnInfo = (DoFnInfo<?, ?>) deserializeFromByteArray(getBytes(cloudUserFn, PropertyNames.SERIALIZED_FN), "Serialized DoFnInfo");    Coder restrictionCoder = coderFromCloudObject(fromSpec(getObject(cloudUserFn, WorkerPropertyNames.RESTRICTION_CODER)));    ProcessFn processFn = new ProcessFn(doFnInfo.getDoFn(), doFnInfo.getInputCoder(), restrictionCoder, doFnInfo.getWindowingStrategy());    return DoFnInfo.forFn(processFn, doFnInfo.getWindowingStrategy(), doFnInfo.getSideInputViews(), KeyedWorkItemCoder.of(ByteArrayCoder.of(), KvCoder.of(doFnInfo.getInputCoder(), restrictionCoder), doFnInfo.getWindowingStrategy().getWindowFn().windowCoder()), doFnInfo.getOutputCoders(), doFnInfo.getMainOutput(), doFnInfo.getDoFnSchemaInformation(), doFnInfo.getSideInputMapping());}
public DoFnRunner<KeyedWorkItem<byte[], KV<InputT, RestrictionT>>, OutputT> beam_f8551_0(DoFn<KeyedWorkItem<byte[], KV<InputT, RestrictionT>>, OutputT> fn, PipelineOptions options, TupleTag<OutputT> mainOutputTag, List<TupleTag<?>> sideOutputTags, Iterable<PCollectionView<?>> sideInputViews, SideInputReader sideInputReader, Coder<KeyedWorkItem<byte[], KV<InputT, RestrictionT>>> inputCoder, Map<TupleTag<?>, Coder<?>> outputCoders, WindowingStrategy<?, ?> windowingStrategy, DataflowExecutionContext.DataflowStepContext stepContext, DataflowExecutionContext.DataflowStepContext userStepContext, OutputManager outputManager, DoFnSchemaInformation doFnSchemaInformation, Map<String, PCollectionView<?>> sideInputMapping)
{    ProcessFn<InputT, OutputT, RestrictionT, TrackerT> processFn = (ProcessFn<InputT, OutputT, RestrictionT, TrackerT>) fn;    processFn.setStateInternalsFactory(key -> (StateInternals) stepContext.stateInternals());    processFn.setTimerInternalsFactory(key -> stepContext.timerInternals());    processFn.setProcessElementInvoker(new OutputAndTimeBoundedSplittableProcessElementInvoker<>(processFn.getFn(), options, new OutputWindowedValue<OutputT>() {        @Override        public void outputWindowedValue(OutputT output, Instant timestamp, Collection<? extends BoundedWindow> windows, PaneInfo pane) {            outputManager.output(mainOutputTag, WindowedValue.of(output, timestamp, windows, pane));        }        @Override        public <T> void outputWindowedValue(TupleTag<T> tag, T output, Instant timestamp, Collection<? extends BoundedWindow> windows, PaneInfo pane) {            outputManager.output(tag, WindowedValue.of(output, timestamp, windows, pane));        }    }, sideInputReader, Executors.newSingleThreadScheduledExecutor(Executors.defaultThreadFactory()),     10000, Duration.standardSeconds(10)));    DoFnRunner<KeyedWorkItem<byte[], KV<InputT, RestrictionT>>, OutputT> simpleRunner = new SimpleDoFnRunner<>(options, processFn, sideInputReader, outputManager, mainOutputTag, sideOutputTags, userStepContext, inputCoder, outputCoders, processFn.getInputWindowingStrategy(), doFnSchemaInformation, sideInputMapping);    DoFnRunner<KeyedWorkItem<byte[], KV<InputT, RestrictionT>>, OutputT> fnRunner = new DataflowProcessFnRunner<>(simpleRunner);    boolean hasStreamingSideInput = options.as(StreamingOptions.class).isStreaming() && !sideInputReader.isEmpty();    KeyedWorkItemCoder<byte[], KV<InputT, RestrictionT>> kwiCoder = (KeyedWorkItemCoder<byte[], KV<InputT, RestrictionT>>) inputCoder;    if (hasStreamingSideInput) {        fnRunner = new StreamingKeyedWorkItemSideInputDoFnRunner<>(fnRunner, ByteArrayCoder.of(), new StreamingSideInputFetcher<>(sideInputViews, kwiCoder.getElementCoder(), processFn.getInputWindowingStrategy(), (StreamingModeExecutionContext.StreamingModeStepContext) userStepContext), userStepContext);    }    return fnRunner;}
public void beam_f8552_0(OutputT output, Instant timestamp, Collection<? extends BoundedWindow> windows, PaneInfo pane)
{    outputManager.output(mainOutputTag, WindowedValue.of(output, timestamp, windows, pane));}
public static SideInputCacheEntry beam_f8560_0()
{    return new SideInputCacheEntry(false, null, 0);}
public boolean beam_f8561_0()
{    return ready;}
public Optional<T> beam_f8562_0()
{    @SuppressWarnings("unchecked")    T typed = (T) value;    return ready ? Optional.fromNullable(typed) : null;}
 void beam_f8570_1()
{    if (!shouldSendCapture()) {        return;    }    try {        Payload payload = new Payload();        for (Capturable capturable : capturables) {            StringWriter content = new StringWriter();            PrintWriter writer = new PrintWriter(content);            capturable.captureData(writer);            writer.flush();            payload.pages.add(new Page(capturable.pageName(), content.toString()));        }        SendDebugCaptureRequest request = new SendDebugCaptureRequest().setComponentId(COMPONENT).setWorkerId(host).setData(JSON_FACTORY.toString(payload));        this.client.projects().locations().jobs().debug().sendCapture(project, region, job, request).execute();        synchronized (lock) {            lastCaptureUsec = DateTime.now().getMillis() * 1000;        }    } catch (Exception e) {            }}
public void beam_f8571_0(HttpServletRequest request, HttpServletResponse response) throws IOException
{    response.setContentType("text/html;charset=utf-8");    if (healthyIndicator.getAsBoolean()) {        response.setStatus(HttpServletResponse.SC_OK);        response.getWriter().println("ok");    } else {        response.setStatus(HttpServletResponse.SC_INTERNAL_SERVER_ERROR);        response.getWriter().println("internal server error");    }}
protected void beam_f8572_0(HttpServletRequest req, HttpServletResponse resp) throws IOException
{    String action = req.getParameter("action");    if (action == null || action.isEmpty()) {        resp.setContentType("text/html;charset=utf-8");        resp.setStatus(HttpServletResponse.SC_OK);        ServletOutputStream writer = resp.getOutputStream();        writer.println("<html>");        writer.println(String.format("Click <a href=\"%s\">here to download heap dump</a>", getPath("action=download")));        writer.println("</html>");        return;    } else if ("download".equals(action)) {        doDownload(resp);    }}
public int beam_f8580_0()
{    return Objects.hash(Arrays.deepHashCode(elements), state);}
public boolean beam_f8581_0(Object other)
{    if (other == this) {        return true;    } else if (!(other instanceof Stack)) {        return false;    } else {        Stack that = (Stack) other;        return state == that.state && Arrays.deepEquals(elements, that.elements);    }}
 static Map<Stack, List<String>> beam_f8582_0(Map<Thread, StackTraceElement[]> allStacks)
{    Map<Stack, List<String>> stacks = new HashMap<>();    for (Map.Entry<Thread, StackTraceElement[]> entry : allStacks.entrySet()) {        Thread thread = entry.getKey();        if (thread != Thread.currentThread()) {            Stack stack = new Stack(entry.getValue(), thread.getState());            List<String> threads = stacks.get(stack);            if (threads == null) {                threads = new ArrayList<>();                stacks.put(stack, threads);            }            threads.add(thread.toString());        }    }    return stacks;}
public void beam_f8590_0(String shortName, String longName, StatusDataProvider dataProvider)
{    statuszServlet.addDataProvider(shortName, longName, dataProvider);}
public List<Capturable> beam_f8591_0()
{    return Arrays.asList(threadzServlet, statuszServlet);}
protected void beam_f8592_0(HttpServletRequest request, HttpServletResponse response) throws IOException, ServletException
{    response.sendRedirect("/statusz");}
public V beam_f8600_0() throws InterruptedException
{    V result = queue.take();    limit.release(weigher.apply(result));    return result;}
public int beam_f8601_0()
{    return maxWeight - limit.availablePermits();}
public int beam_f8602_0()
{    return queue.size();}
public static StreamingDataflowWorker beam_f8610_0(DataflowWorkerHarnessOptions options, SdkHarnessRegistry sdkHarnessRegistry) throws IOException
{    return new StreamingDataflowWorker(Collections.emptyList(), IntrinsicMapTaskExecutorFactory.defaultFactory(), new DataflowWorkUnitClient(options, LOG), options.as(StreamingDataflowWorkerOptions.class), null, sdkHarnessRegistry, true, new HotKeyLogger());}
public void beam_f8611_1()
{        if (windmillServiceEnabled) {        streamingDispatchLoop();    } else {        dispatchLoop();    }    }
public void beam_f8612_0()
{    if (windmillServiceEnabled) {        streamingCommitLoop();    } else {        commitLoop();    }}
public void beam_f8620_0()
{    reportPeriodicWorkerUpdates();}
public void beam_f8621_0()
{    refreshActiveWork();}
public void beam_f8622_1()
{    List<Capturable> pages = statusPages.getDebugCapturePages();    if (pages.isEmpty()) {            }    Long timestamp = Instant.now().getMillis();    for (Capturable page : pages) {        PrintWriter writer = null;        try {            File outputFile = new File(options.getPeriodicStatusPageOutputDirectory(), ("StreamingDataflowWorker" + options.getWorkerId() + "_" + page.pageName() + timestamp.toString()).replaceAll("/", "_"));            writer = new PrintWriter(outputFile, UTF_8.name());            page.captureData(writer);        } catch (IOException e) {                    } finally {            if (writer != null) {                writer.close();            }        }    }}
private void beam_f8630_1()
{    while (running.get()) {        memoryMonitor.waitForResources("GetWork");        int backoff = 1;        Windmill.GetWorkResponse workResponse = null;        do {            try {                workResponse = getWork();                if (workResponse.getWorkCount() > 0) {                    break;                }            } catch (WindmillServerStub.RpcException e) {                            }            sleep(backoff);            backoff = Math.min(1000, backoff * 2);        } while (running.get());        for (final Windmill.ComputationWorkItems computationWork : workResponse.getWorkList()) {            final String computationId = computationWork.getComputationId();            final ComputationState computationState = getComputationState(computationId);            if (computationState == null) {                                continue;            }            final Instant inputDataWatermark = WindmillTimeUtils.windmillToHarnessWatermark(computationWork.getInputDataWatermark());            Preconditions.checkNotNull(inputDataWatermark);            @Nullable            final Instant synchronizedProcessingTime = WindmillTimeUtils.windmillToHarnessWatermark(computationWork.getDependentRealtimeInputWatermark());            for (final Windmill.WorkItem workItem : computationWork.getWorkList()) {                scheduleWorkItem(computationState, inputDataWatermark, synchronizedProcessingTime, workItem);            }        }    }}
 void beam_f8631_0()
{    while (running.get()) {        GetWorkStream stream = windmillServer.getWorkStream(Windmill.GetWorkRequest.newBuilder().setClientId(clientId).setMaxItems(MAX_GET_WORK_ITEMS).setMaxBytes(MAX_GET_WORK_FETCH_BYTES).build(), (String computation, @Nullable Instant inputDataWatermark, Instant synchronizedProcessingTime, Windmill.WorkItem workItem) -> {            memoryMonitor.waitForResources("GetWork");            scheduleWorkItem(getComputationState(computation), inputDataWatermark, synchronizedProcessingTime, workItem);        });        try {                                                stream.closeAfterDefaultTimeout();        } catch (InterruptedException e) {                }    }}
private void beam_f8632_0(final ComputationState computationState, final Instant inputDataWatermark, final Instant synchronizedProcessingTime, final Windmill.WorkItem workItem)
{    Preconditions.checkNotNull(inputDataWatermark);        @Nullable    final Instant outputDataWatermark = WindmillTimeUtils.windmillToHarnessWatermark(workItem.getOutputDataWatermark());    Preconditions.checkState(outputDataWatermark == null || !outputDataWatermark.isAfter(inputDataWatermark));    SdkWorkerHarness worker = sdkHarnessRegistry.getAvailableWorkerAndAssignWork();    if (workItem.hasHotKeyInfo()) {        Windmill.HotKeyInfo hotKeyInfo = workItem.getHotKeyInfo();        Duration hotKeyAge = Duration.millis(hotKeyInfo.getHotKeyAgeUsec() / 1000);                        String stepName = computationState.getMapTask().getInstructions().get(0).getName();        hotKeyLogger.logHotKeyDetection(stepName, hotKeyAge);    }    Work work = new Work(workItem) {        @Override        public void run() {            try {                process(worker, computationState, inputDataWatermark, outputDataWatermark, synchronizedProcessingTime, this);            } finally {                                sdkHarnessRegistry.completeWork(worker);            }        }    };    if (!computationState.activateWork(workItem.getKey(), work)) {                        sdkHarnessRegistry.completeWork(worker);    }}
private void beam_f8640_1(final SdkWorkerHarness worker, final ComputationState computationState, final Instant inputDataWatermark, @Nullable final Instant outputDataWatermark, @Nullable final Instant synchronizedProcessingTime, final Work work)
{    final Windmill.WorkItem workItem = work.getWorkItem();    final String computationId = computationState.getComputationId();    final ByteString key = workItem.getKey();    work.setState(State.PROCESSING);    DataflowWorkerLoggingMDC.setWorkId(TextFormat.escapeBytes(key) + "-" + Long.toString(workItem.getWorkToken()));    DataflowWorkerLoggingMDC.setStageName(computationId);        Windmill.WorkItemCommitRequest.Builder outputBuilder = Windmill.WorkItemCommitRequest.newBuilder().setKey(key).setShardingKey(workItem.getShardingKey()).setWorkToken(workItem.getWorkToken()).setCacheToken(workItem.getCacheToken());            callFinalizeCallbacks(workItem);    if (workItem.getSourceState().getOnlyFinalize()) {        outputBuilder.setSourceStateUpdates(Windmill.SourceState.newBuilder().setOnlyFinalize(true));        work.setState(State.COMMIT_QUEUED);        commitQueue.put(new Commit(outputBuilder.build(), computationState, work));        return;    }    long processingStartTimeNanos = System.nanoTime();    final MapTask mapTask = computationState.getMapTask();    StageInfo stageInfo = stageInfoMap.computeIfAbsent(mapTask.getStageName(), s -> new StageInfo(s, mapTask.getSystemName(), this));    ExecutionState executionState = null;    try {        executionState = computationState.getExecutionStateQueue(worker).poll();        if (executionState == null) {            MutableNetwork<Node, Edge> mapTaskNetwork = mapTaskToNetwork.apply(mapTask);            if (LOG.isDebugEnabled()) {                            }            ParallelInstructionNode readNode = (ParallelInstructionNode) Iterables.find(mapTaskNetwork.nodes(), node -> node instanceof ParallelInstructionNode && ((ParallelInstructionNode) node).getParallelInstruction().getRead() != null);            InstructionOutputNode readOutputNode = (InstructionOutputNode) Iterables.getOnlyElement(mapTaskNetwork.successors(readNode));            DataflowExecutionContext.DataflowExecutionStateTracker executionStateTracker = new DataflowExecutionContext.DataflowExecutionStateTracker(ExecutionStateSampler.instance(), stageInfo.executionStateRegistry.getState(NameContext.forStage(mapTask.getStageName()), "other", null, ScopedProfiler.INSTANCE.emptyScope()), stageInfo.deltaCounters, options, computationId);            StreamingModeExecutionContext context = new StreamingModeExecutionContext(pendingDeltaCounters, computationId, readerCache, !computationState.getTransformUserNameToStateFamily().isEmpty() ? computationState.getTransformUserNameToStateFamily() : stateNameMap, stateCache.forComputation(computationId), stageInfo.metricsContainerRegistry, executionStateTracker, stageInfo.executionStateRegistry, maxSinkBytes);            DataflowMapTaskExecutor mapTaskExecutor = mapTaskExecutorFactory.create(worker.getControlClientHandler(), worker.getGrpcDataFnServer(), sdkHarnessRegistry.beamFnDataApiServiceDescriptor(), worker.getGrpcStateFnServer(), mapTaskNetwork, options, mapTask.getStageName(), readerRegistry, sinkRegistry, context, pendingDeltaCounters, idGenerator);            ReadOperation readOperation = mapTaskExecutor.getReadOperation();                                    readOperation.setProgressUpdatePeriodMs(ReadOperation.DONT_UPDATE_PERIODICALLY);            Preconditions.checkState(mapTaskExecutor.supportsRestart(), "Streaming runner requires all operations support restart.");            Coder<?> readCoder;            readCoder = CloudObjects.coderFromCloudObject(CloudObject.fromSpec(readOutputNode.getInstructionOutput().getCodec()));            Coder<?> keyCoder = extractKeyCoder(readCoder);                        if (CustomSources.class.getName().equals(readNode.getParallelInstruction().getRead().getSource().getSpec().get("@type"))) {                NameContext nameContext = NameContext.create(mapTask.getStageName(), readNode.getParallelInstruction().getOriginalName(), readNode.getParallelInstruction().getSystemName(), readNode.getParallelInstruction().getName());                readOperation.receivers[0].addOutputCounter(new OutputObjectAndByteCounter(new IntrinsicMapTaskExecutorFactory.ElementByteSizeObservableCoder<>(readCoder), mapTaskExecutor.getOutputCounters(), nameContext).setSamplingPeriod(100).countBytes("dataflow_input_size-" + mapTask.getSystemName()));            }            executionState = new ExecutionState(mapTaskExecutor, context, keyCoder, executionStateTracker);        }        WindmillStateReader stateReader = new WindmillStateReader(metricTrackingWindmillServer, computationId, key, workItem.getShardingKey(), workItem.getWorkToken());        StateFetcher localStateFetcher = stateFetcher.byteTrackingView();                                                        @Nullable        Coder<?> keyCoder = executionState.getKeyCoder();        @Nullable        Object executionKey = keyCoder == null ? null : keyCoder.decode(key.newInput(), Coder.Context.OUTER);        executionState.getContext().start(executionKey, workItem, inputDataWatermark, outputDataWatermark, synchronizedProcessingTime, stateReader, localStateFetcher, outputBuilder);                executionState.getWorkExecutor().execute();        Iterables.addAll(this.pendingMonitoringInfos, executionState.getWorkExecutor().extractMetricUpdates());        commitCallbacks.putAll(executionState.getContext().flushState());                computationState.getExecutionStateQueue(worker).offer(executionState);        executionState = null;                work.setState(State.COMMIT_QUEUED);        WorkItemCommitRequest commitRequest = outputBuilder.build();        int byteLimit = maxWorkItemCommitBytes;        int commitSize = commitRequest.getSerializedSize();                windmillMaxObservedWorkItemCommitBytes.addValue(commitSize < 0 ? Integer.MAX_VALUE : commitSize);        if (commitSize < 0) {            throw KeyCommitTooLargeException.causedBy(computationId, byteLimit, commitRequest);        } else if (commitSize > byteLimit) {                                                KeyCommitTooLargeException e = KeyCommitTooLargeException.causedBy(computationId, byteLimit, commitRequest);            reportFailure(computationId, workItem, e);                    }        commitQueue.put(new Commit(commitRequest, computationState, work));                long stateBytesWritten = outputBuilder.clearOutputMessages().build().getSerializedSize();        long shuffleBytesRead = 0;        for (Windmill.InputMessageBundle bundle : workItem.getMessageBundlesList()) {            for (Windmill.Message message : bundle.getMessagesList()) {                shuffleBytesRead += message.getSerializedSize();            }        }        long stateBytesRead = stateReader.getBytesRead() + localStateFetcher.getBytesRead();        windmillShuffleBytesRead.addValue(shuffleBytesRead);        windmillStateBytesRead.addValue(stateBytesRead);        windmillStateBytesWritten.addValue(stateBytesWritten);            } catch (Throwable t) {        if (executionState != null) {            try {                executionState.getContext().invalidateCache();                executionState.getWorkExecutor().close();            } catch (Exception e) {                            } finally {                                executionState = null;            }        }        t = t instanceof UserCodeException ? t.getCause() : t;        boolean retryLocally = false;        if (KeyTokenInvalidException.isKeyTokenInvalidException(t)) {                    } else {                        LastExceptionDataProvider.reportException(t);                        if (!reportFailure(computationId, workItem, t)) {                            } else if (isOutOfMemoryError(t)) {                File heapDump = memoryMonitor.tryToDumpHeap();                            } else {                                retryLocally = true;            }        }        if (retryLocally) {                        sleep(retryLocallyDelayMs);            workUnitExecutor.forceExecute(work);        } else {                                    computationState.completeWork(key, workItem.getWorkToken());        }    } finally {                        long processingTimeMsecs = TimeUnit.NANOSECONDS.toMillis(System.nanoTime() - processingStartTimeNanos);        stageInfo.totalProcessingMsecs.addValue(processingTimeMsecs);                if (work.getWorkItem().hasTimers()) {            stageInfo.timerProcessingMsecs.addValue(processingTimeMsecs);        }        DataflowWorkerLoggingMDC.setWorkId(null);        DataflowWorkerLoggingMDC.setStageName(null);    }}
private void beam_f8641_0()
{    Map<ComputationState, Windmill.ComputationCommitWorkRequest.Builder> computationRequestMap = new HashMap<>();    while (running.get()) {        computationRequestMap.clear();        Windmill.CommitWorkRequest.Builder commitRequestBuilder = Windmill.CommitWorkRequest.newBuilder();        long commitBytes = 0;                Commit commit = null;        try {            commit = commitQueue.take();        } catch (InterruptedException e) {            Thread.currentThread().interrupt();            continue;        }        while (commit != null) {            ComputationState computationState = commit.getComputationState();            commit.getWork().setState(State.COMMITTING);            Windmill.ComputationCommitWorkRequest.Builder computationRequestBuilder = computationRequestMap.get(computationState);            if (computationRequestBuilder == null) {                computationRequestBuilder = commitRequestBuilder.addRequestsBuilder();                computationRequestBuilder.setComputationId(computationState.getComputationId());                computationRequestMap.put(computationState, computationRequestBuilder);            }            computationRequestBuilder.addRequests(commit.getRequest());                                    commitBytes += commit.getSize();            if (commitBytes >= TARGET_COMMIT_BUNDLE_BYTES) {                break;            }            commit = commitQueue.poll();        }        Windmill.CommitWorkRequest commitRequest = commitRequestBuilder.build();        LOG.trace("Commit: {}", commitRequest);        activeCommitBytes.set(commitBytes);        commitWork(commitRequest);        activeCommitBytes.set(0);        for (Map.Entry<ComputationState, Windmill.ComputationCommitWorkRequest.Builder> entry : computationRequestMap.entrySet()) {            ComputationState computationState = entry.getKey();            for (Windmill.WorkItemCommitRequest workRequest : entry.getValue().getRequestsList()) {                computationState.completeWork(workRequest.getKey(), workRequest.getWorkToken());            }        }    }}
private void beam_f8642_0()
{    StreamPool<CommitWorkStream> streamPool = new StreamPool<>(NUM_COMMIT_STREAMS, COMMIT_STREAM_TIMEOUT, windmillServer::commitWorkStream);    Commit commit = null;    while (running.get()) {                                CommitWorkStream commitStream = null;        int commits = 0;        while (running.get()) {                        if (commit == null) {                try {                    if (commits == 0) {                        commit = commitQueue.take();                    } else if (commits < 10) {                        commit = commitQueue.poll(20 - 2 * commits, TimeUnit.MILLISECONDS);                    } else {                        commit = commitQueue.poll();                    }                } catch (InterruptedException e) {                                        continue;                }                if (commit == null) {                                        break;                }            }            commits++;            final ComputationState state = commit.getComputationState();            final Windmill.WorkItemCommitRequest request = commit.getRequest();            final int size = commit.getSize();            commit.getWork().setState(State.COMMITTING);            if (commitStream == null) {                commitStream = streamPool.getStream();            }            if (commitStream.commitWorkItem(state.computationId, request, (Windmill.CommitStatus status) -> {                if (status != Windmill.CommitStatus.OK) {                    stateCache.forComputation(state.computationId).invalidate(request.getKey());                }                state.completeWork(request.getKey(), request.getWorkToken());                activeCommitBytes.addAndGet(-size);            })) {                                commit = null;                                                activeCommitBytes.addAndGet(size);            } else {                                break;            }        }        if (commitStream != null) {            commitStream.flush();            streamPool.releaseStream(commitStream);        }    }}
private void beam_f8650_1(String computation)
{    BackOff backoff = FluentBackoff.DEFAULT.withInitialBackoff(Duration.millis(100)).withMaxBackoff(Duration.standardMinutes(1)).withMaxCumulativeBackoff(Duration.standardMinutes(5)).backoff();    while (running.get()) {        try {            if (windmillServiceEnabled) {                getConfigFromDataflowService(computation);            } else {                getConfigFromWindmill(computation);            }            return;        } catch (IllegalArgumentException | IOException e) {                        try {                if (!BackOffUtils.next(Sleeper.DEFAULT, backoff)) {                    return;                }            } catch (IOException ioe) {                                return;            } catch (InterruptedException ie) {                Thread.currentThread().interrupt();                return;            }        }    }}
public Iterable<CounterUpdate> beam_f8651_0()
{    return Iterables.concat(pendingDeltaCounters.extractModifiedDeltaUpdates(DataflowCounterUpdateExtractor.INSTANCE), pendingCumulativeCounters.extractUpdates(false, DataflowCounterUpdateExtractor.INSTANCE));}
private Windmill.Exception beam_f8652_0(Throwable t)
{    Windmill.Exception.Builder builder = Windmill.Exception.newBuilder();    builder.addStackFrames(t.toString());    for (StackTraceElement frame : t.getStackTrace()) {        builder.addStackFrames(frame.toString());    }    if (t.getCause() != null) {        builder.setCause(buildExceptionReport(t.getCause()));    }    return builder.build();}
private void beam_f8660_0(CounterSet deltaCounters, CounterSet cumulativeCounters) throws IOException
{        windmillQuotaThrottling.addValue(windmillServer.getAndResetThrottleTime());    if (memoryMonitor.isThrashing()) {        memoryThrashing.addValue(1);    }    List<CounterUpdate> counterUpdates = new ArrayList<>(128);    if (publishCounters) {        stageInfoMap.values().forEach(s -> counterUpdates.addAll(s.extractCounterUpdates()));        counterUpdates.addAll(cumulativeCounters.extractUpdates(false, DataflowCounterUpdateExtractor.INSTANCE));        counterUpdates.addAll(deltaCounters.extractModifiedDeltaUpdates(DataflowCounterUpdateExtractor.INSTANCE));        if (hasExperiment(options, "beam_fn_api")) {            while (!this.pendingMonitoringInfos.isEmpty()) {                final CounterUpdate item = this.pendingMonitoringInfos.poll();                                if (item.getCumulative()) {                    item.setCumulative(false);                } else {                                        throw new UnsupportedOperationException("FnApi counters are expected to provide cumulative values." + " Please, update convertion to delta logic" + " if non-cumulative counter type is required.");                }                counterUpdates.add(item);            }        }    }                    ListMultimap<Object, CounterUpdate> counterMultimap = MultimapBuilder.hashKeys(counterUpdates.size()).linkedListValues().build();    boolean hasDuplicates = false;    for (CounterUpdate c : counterUpdates) {        Object key = getCounterUpdateKey(c);        if (counterMultimap.containsKey(key)) {            hasDuplicates = true;        }        counterMultimap.put(key, c);    }            Runnable extractUniqueCounters = () -> {        counterUpdates.clear();        for (Iterator<Object> iter = counterMultimap.keySet().iterator(); iter.hasNext(); ) {            List<CounterUpdate> counters = counterMultimap.get(iter.next());            counterUpdates.add(counters.get(0));            if (counters.size() == 1) {                                iter.remove();            } else {                                counters.remove(0);            }        }    };    if (hasDuplicates) {        extractUniqueCounters.run();    } else {                counterMultimap.clear();    }    List<Status> errors;    synchronized (pendingFailuresToReport) {        errors = new ArrayList<>(pendingFailuresToReport.size());        for (String stackTrace : pendingFailuresToReport) {            errors.add(new Status().setCode(            2).setMessage(stackTrace));        }                pendingFailuresToReport.clear();    }    WorkItemStatus workItemStatus = new WorkItemStatus().setWorkItemId(WINDMILL_COUNTER_UPDATE_WORK_ID).setErrors(errors).setCounterUpdates(counterUpdates);    workUnitClient.reportWorkItemStatus(workItemStatus);        while (!counterMultimap.isEmpty()) {        extractUniqueCounters.run();        workUnitClient.reportWorkItemStatus(new WorkItemStatus().setWorkItemId(WINDMILL_COUNTER_UPDATE_WORK_ID).setCounterUpdates(counterUpdates));    }}
private void beam_f8661_0()
{    Map<String, List<Windmill.KeyedGetDataRequest>> active = new HashMap<>();    Instant refreshDeadline = Instant.now().minus(Duration.millis(options.getActiveWorkRefreshPeriodMillis()));    for (Map.Entry<String, ComputationState> entry : computationMap.entrySet()) {        active.put(entry.getKey(), entry.getValue().getKeysToRefresh(refreshDeadline));    }    metricTrackingWindmillServer.refreshActiveWork(active);}
public String beam_f8662_0()
{    return computationId;}
public void beam_f8670_0() throws Exception
{    ExecutionState executionState;    for (ConcurrentLinkedQueue<ExecutionState> queue : executionStateQueues.values()) {        while ((executionState = queue.poll()) != null) {            executionState.getWorkExecutor().close();        }    }    executionStateQueues.clear();}
public DataflowWorkExecutor beam_f8671_0()
{    return workExecutor;}
public StreamingModeExecutionContext beam_f8672_0()
{    return context;}
public static GroupAlsoByWindowFn<KeyedWorkItem<K, InputT>, KV<K, OutputT>> beam_f8680_0(final WindowingStrategy<?, W> windowingStrategy, StateInternalsFactory<K> stateInternalsFactory, final AppliedCombineFn<K, InputT, AccumT, OutputT> combineFn, final Coder<K> keyCoder)
{    Preconditions.checkNotNull(combineFn);    return StreamingGroupAlsoByWindowViaWindowSetFn.create(windowingStrategy, stateInternalsFactory, SystemReduceFn.combining(keyCoder, combineFn));}
public static GroupAlsoByWindowFn<KeyedWorkItem<K, V>, KV<K, Iterable<V>>> beam_f8681_0(final WindowingStrategy<?, W> windowingStrategy, StateInternalsFactory<K> stateInternalsFactory, Coder<V> inputCoder)
{        if (StreamingGroupAlsoByWindowReshuffleFn.isReshuffle(windowingStrategy)) {        return new StreamingGroupAlsoByWindowReshuffleFn<>();    } else {        return StreamingGroupAlsoByWindowViaWindowSetFn.create(windowingStrategy, stateInternalsFactory, SystemReduceFn.buffering(inputCoder));    }}
public static GroupAlsoByWindowFn<KeyedWorkItem<K, InputT>, KV<K, OutputT>> beam_f8682_0(WindowingStrategy<?, W> strategy, StateInternalsFactory<K> stateInternalsFactory, SystemReduceFn<K, InputT, ?, OutputT, W> reduceFn)
{    return new StreamingGroupAlsoByWindowViaWindowSetFn<>(strategy, stateInternalsFactory, reduceFn);}
private boolean beam_f8690_0(Iterable<TimerData> timers, Iterable<WindowedValue<InputT>> elements)
{    return !timers.iterator().hasNext() && !elements.iterator().hasNext();}
public DoFn<KeyedWorkItem<K, InputT>, OutputT> beam_f8691_0()
{    return simpleDoFnRunner.getFn();}
public void beam_f8692_0(Thread trackedThread, long millis)
{    super.reportLull(trackedThread, millis);            String errorMessage = getLullMessage(trackedThread, Duration.millis(millis));    worker.addFailure(errorMessage);}
protected SideInputReader beam_f8700_0(Iterable<? extends PCollectionView<?>> views)
{    return StreamingModeSideInputReader.of(views, this);}
private Optional<T> beam_f8701_0(PCollectionView<T> view, BoundedWindow sideInputWindow, String stateFamily, StateFetcher.SideInputState state, Supplier<Closeable> scopedReadStateSupplier)
{    Map<BoundedWindow, Object> tagCache = sideInputCache.get(view.getTagInternal());    if (tagCache == null) {        tagCache = new HashMap<>();        sideInputCache.put(view.getTagInternal(), tagCache);    }    if (tagCache.containsKey(sideInputWindow)) {        @SuppressWarnings("unchecked")        T typed = (T) tagCache.get(sideInputWindow);        return Optional.fromNullable(typed);    } else {        if (state == StateFetcher.SideInputState.CACHED_IN_WORKITEM) {            throw new IllegalStateException("Expected side input to be cached. Tag: " + view.getTagInternal().getId());        }        Optional<T> fetched = stateFetcher.fetchSideInput(view, sideInputWindow, stateFamily, state, scopedReadStateSupplier);        if (fetched != null) {            tagCache.put(sideInputWindow, fetched.orNull());        }        return fetched;    }}
public Iterable<Windmill.GlobalDataId> beam_f8702_0()
{    return work.getGlobalDataIdNotificationsList();}
public void beam_f8710_1()
{    ByteString key = getSerializedKey();    if (key != null) {        readerCache.invalidateReader(computationId, key);        if (activeReader != null) {            try {                activeReader.close();            } catch (IOException e) {                            }        }        activeReader = null;        stateCache.invalidate(key);    }}
public UnboundedSource.CheckpointMark beam_f8711_0(Coder<? extends UnboundedSource.CheckpointMark> coder)
{    try {        ByteString state = work.getSourceState().getState();        if (state.isEmpty()) {            return null;        }        return coder.decode(state.newInput(), Coder.Context.OUTER);    } catch (IOException e) {        throw new RuntimeException("Exception while decoding checkpoint", e);    }}
public Map<Long, Runnable> beam_f8712_0()
{    Map<Long, Runnable> callbacks = new HashMap<>();    for (StepContext stepContext : getAllStepContexts()) {        stepContext.flushState();    }    if (activeReader != null) {        Windmill.SourceState.Builder sourceStateBuilder = outputBuilder.getSourceStateUpdatesBuilder();        final UnboundedSource.CheckpointMark checkpointMark = activeReader.getCheckpointMark();        final Instant watermark = activeReader.getWatermark();        long id = ThreadLocalRandom.current().nextLong();        sourceStateBuilder.addFinalizeIds(id);        callbacks.put(id, () -> {            try {                checkpointMark.finalizeCheckpoint();            } catch (IOException e) {                throw new RuntimeException("Exception while finalizing checkpoint", e);            }        });        @SuppressWarnings("unchecked")        Coder<UnboundedSource.CheckpointMark> checkpointCoder = ((UnboundedSource<?, UnboundedSource.CheckpointMark>) activeReader.getCurrentSource()).getCheckpointMarkCoder();        if (checkpointCoder != null) {            ByteString.Output stream = ByteString.newOutput();            try {                checkpointCoder.encode(checkpointMark, stream, Coder.Context.OUTER);            } catch (IOException e) {                throw new RuntimeException("Exception while encoding checkpoint", e);            }            sourceStateBuilder.setState(stream.toByteString());        }        outputBuilder.setSourceWatermark(WindmillTimeUtils.harnessToWindmillTimestamp(watermark));        backlogBytes = activeReader.getSplitBacklogBytes();        if (backlogBytes == UnboundedSource.UnboundedReader.BACKLOG_UNKNOWN && WorkerCustomSources.isFirstUnboundedSourceSplit(getSerializedKey())) {                        backlogBytes = activeReader.getTotalBacklogBytes();        }        outputBuilder.setSourceBacklogBytes(backlogBytes);        readerCache.cacheReader(computationId, getSerializedKey(), getWork().getCacheToken(), activeReader);        activeReader = null;    }    return callbacks;}
public DataflowStepContext beam_f8720_0()
{    return new UserStepContext(this);}
public Iterable<Windmill.GlobalDataId> beam_f8721_0()
{    return StreamingModeExecutionContext.this.getSideInputNotifications();}
public void beam_f8722_0(TupleTag<?> tag, Iterable<T> data, Coder<Iterable<T>> dataCoder, W window, Coder<W> windowCoder) throws IOException
{    if (getSerializedKey().size() != 0) {        throw new IllegalStateException("writePCollectionViewData must follow a Combine.globally");    }    ByteString.Output dataStream = ByteString.newOutput();    dataCoder.encode(data, dataStream, Coder.Context.OUTER);    ByteString.Output windowStream = ByteString.newOutput();    windowCoder.encode(window, windowStream, Coder.Context.OUTER);    if (stateFamily == null) {        throw new IllegalStateException("Tried to write view data for stateless step: " + getNameContext());    }    Windmill.GlobalData.Builder builder = Windmill.GlobalData.newBuilder().setDataId(Windmill.GlobalDataId.newBuilder().setTag(tag.getId()).setVersion(windowStream.toByteString()).build()).setData(dataStream.toByteString()).setStateFamily(stateFamily);    outputBuilder.addGlobalDataUpdates(builder.build());}
public void beam_f8730_0(GlobalDataRequest blocked)
{    wrapped.addBlockingSideInput(blocked);}
public void beam_f8731_0(Iterable<GlobalDataRequest> blocked)
{    wrapped.addBlockingSideInputs(blocked);}
public StateInternals beam_f8732_0()
{    return wrapped.stateInternals();}
public T beam_f8740_0(PCollectionView<T> view, BoundedWindow window)
{    if (!contains(view)) {        throw new RuntimeException("get() called with unknown view");    }        return context.fetchSideInput(view, window, null, /* unused stateFamily */    StateFetcher.SideInputState.CACHED_IN_WORKITEM, null).orNull();}
public boolean beam_f8741_0(PCollectionView<T> view)
{    return viewSet.contains(view);}
public boolean beam_f8742_0()
{    return viewSet.isEmpty();}
public DoFn<InputT, OutputT> beam_f8753_0()
{    return simpleDoFnRunner.getFn();}
 static StateTag<ValueState<Map<W, Set<GlobalDataRequest>>>> beam_f8754_0(Coder<W> mainWindowCoder)
{    return StateTags.value("blockedMap", MapCoder.of(mainWindowCoder, SetCoder.of(GlobalDataRequestCoder.of())));}
public Set<W> beam_f8755_0()
{    Set<W> readyWindows = new HashSet<>();    for (Windmill.GlobalDataId id : stepContext.getSideInputNotifications()) {        if (sideInputViews.get(id.getTag()) == null) {                        continue;        }        for (Map.Entry<W, Set<Windmill.GlobalDataRequest>> entry : blockedMap().entrySet()) {            Set<Windmill.GlobalDataRequest> windowBlockedSet = entry.getValue();            Set<Windmill.GlobalDataRequest> found = new HashSet<>();            for (Windmill.GlobalDataRequest request : windowBlockedSet) {                if (id.equals(request.getDataId())) {                    found.add(request);                }            }            windowBlockedSet.removeAll(found);            if (windowBlockedSet.isEmpty()) {                                                                W window = entry.getKey();                boolean allSideInputsCached = true;                for (PCollectionView<?> view : sideInputViews.values()) {                    if (!stepContext.issueSideInputFetch(view, window, StateFetcher.SideInputState.KNOWN_READY)) {                        Windmill.GlobalDataRequest request = buildGlobalDataRequest(view, window);                        stepContext.addBlockingSideInput(request);                        windowBlockedSet.add(request);                        allSideInputsCached = false;                    }                }                if (allSideInputsCached) {                    readyWindows.add(window);                }            }        }    }    return readyWindows;}
 Set<W> beam_f8763_0()
{    return blockedMap().keySet();}
 BagState<WindowedValue<InputT>> beam_f8764_0(W window)
{    return stepContext.stateInternals().state(StateNamespaces.window(mainWindowCoder, window), elementsAddr);}
 WatermarkHoldState beam_f8765_0(W window)
{    return stepContext.stateInternals().state(StateNamespaces.window(mainWindowCoder, window), watermarkHoldingAddr);}
private Parser<Windmill.GlobalDataRequest> beam_f8773_0()
{    if (memoizedParser == null) {        memoizedParser = Windmill.GlobalDataRequest.getDefaultInstance().getParserForType();    }    return memoizedParser;}
public static MetricsContainerRegistry<StreamingStepMetricsContainer> beam_f8774_0()
{    return new MetricsContainerRegistry<StreamingStepMetricsContainer>() {        @Override        protected StreamingStepMetricsContainer createContainer(String stepName) {            return new StreamingStepMetricsContainer(stepName);        }    };}
protected StreamingStepMetricsContainer beam_f8775_0(String stepName)
{    return new StreamingStepMetricsContainer(stepName);}
public CounterUpdate beam_f8783_0(@Nonnull Map.Entry<MetricName, DeltaDistributionCell> entry)
{    DistributionData value = entry.getValue().getAndReset();    if (value.count() == 0) {        return null;    }    return MetricsToCounterUpdateConverter.fromDistribution(MetricKey.create(stepName, entry.getKey()), false, value);}
public static Iterable<CounterUpdate> beam_f8784_0(MetricsContainerRegistry<StreamingStepMetricsContainer> metricsContainerRegistry)
{    return metricsContainerRegistry.getContainers().transformAndConcat(StreamingStepMetricsContainer::extractUpdates);}
public ParDoFn beam_f8785_0(PipelineOptions options, CloudObject cloudUserFn, List<SideInputInfo> sideInputInfos, TupleTag<?> mainOutputTag, Map<TupleTag<?>, Integer> outputTupleTagsToReceiverIndices, DataflowExecutionContext<?> executionContext, DataflowOperationContext operationContext) throws Exception
{    Coder<?> coder = CloudObjects.coderFromCloudObject(CloudObject.fromSpec(Structs.getObject(cloudUserFn, PropertyNames.ENCODING)));    checkState(coder instanceof IsmRecordCoder, "Expected to received an instanceof an %s but got %s", IsmRecordCoder.class.getSimpleName(), coder);    IsmRecordCoder<?> ismRecordCoder = (IsmRecordCoder<?>) coder;    return new ToIsmRecordForMultimapParDoFn(KvCoder.of(ismRecordCoder.getCoderArguments().get(0), ismRecordCoder.getCoderArguments().get(1)));}
public void beam_f8796_0() throws IOException
{    entryReader.close();}
public Map<String, ReaderFactory> beam_f8797_0()
{    return ImmutableMap.of("UngroupedShuffleSource", new UngroupedShuffleReaderFactory());}
public NativeReader<?> beam_f8798_0(CloudObject spec, @Nullable Coder<?> coder, @Nullable PipelineOptions options, @Nullable DataflowExecutionContext executionContext, DataflowOperationContext operationContext) throws Exception
{    return create(options, spec, coder, executionContext, operationContext);}
 static UserParDoFnFactory beam_f8806_0()
{    return new UserParDoFnFactory(new UserDoFnExtractor(), SimpleDoFnRunnerFactory.INSTANCE);}
public DoFnInfo<?, ?> beam_f8807_0(CloudObject cloudUserFn)
{    return (DoFnInfo<?, ?>) SerializableUtils.deserializeFromByteArray(getBytes(cloudUserFn, PropertyNames.SERIALIZED_FN), "Serialized DoFnInfo");}
public ParDoFn beam_f8808_0(PipelineOptions options, CloudObject cloudUserFn, @Nullable List<SideInputInfo> sideInputInfos, TupleTag<?> mainOutputTag, Map<TupleTag<?>, Integer> outputTupleTagsToReceiverIndices, DataflowExecutionContext<?> executionContext, DataflowOperationContext operationContext) throws Exception
{    DoFnInstanceManager instanceManager = fnCache.get(operationContext.nameContext().systemName(), () -> DoFnInstanceManagers.cloningPool(doFnExtractor.getDoFnInfo(cloudUserFn)));    DoFnInfo<?, ?> doFnInfo = instanceManager.peek();    DataflowExecutionContext.DataflowStepContext stepContext = executionContext.getStepContext(operationContext);    Iterable<PCollectionView<?>> sideInputViews = doFnInfo.getSideInputViews();    SideInputReader sideInputReader = executionContext.getSideInputReader(sideInputInfos, sideInputViews, operationContext);    if (doFnInfo.getDoFn() instanceof BatchStatefulParDoOverrides.BatchStatefulDoFn) {                                BatchStatefulParDoOverrides.BatchStatefulDoFn fn = (BatchStatefulParDoOverrides.BatchStatefulDoFn) doFnInfo.getDoFn();        DoFn underlyingFn = fn.getUnderlyingDoFn();        return new BatchModeUngroupingParDoFn((BatchModeExecutionContext.StepContext) stepContext, new SimpleParDoFn(options, DoFnInstanceManagers.singleInstance(doFnInfo.withFn(underlyingFn)), sideInputReader, doFnInfo.getMainOutput(), outputTupleTagsToReceiverIndices, stepContext, operationContext, doFnInfo.getDoFnSchemaInformation(), doFnInfo.getSideInputMapping(), runnerFactory));    } else if (doFnInfo.getDoFn() instanceof StreamingPCollectionViewWriterFn) {                                checkArgument(stepContext instanceof StreamingModeExecutionContext.StreamingModeStepContext, "stepContext must be a StreamingModeStepContext to use StreamingPCollectionViewWriterFn");        DataflowRunner.StreamingPCollectionViewWriterFn<Object> writerFn = (StreamingPCollectionViewWriterFn<Object>) doFnInfo.getDoFn();        return new StreamingPCollectionViewWriterParDoFn((StreamingModeExecutionContext.StreamingModeStepContext) stepContext, writerFn.getView().getTagInternal(), writerFn.getDataCoder(), (Coder<BoundedWindow>) doFnInfo.getWindowingStrategy().getWindowFn().windowCoder());    } else {        return new SimpleParDoFn(options, instanceManager, sideInputReader, doFnInfo.getMainOutput(), outputTupleTagsToReceiverIndices, stepContext, operationContext, doFnInfo.getDoFnSchemaInformation(), doFnInfo.getSideInputMapping(), runnerFactory);    }}
public OutputT beam_f8816_0(AccumT accumulator, BoundedWindow w)
{    return keyedCombineFn.extractOutput(accumulator);}
public AccumT beam_f8817_0(BoundedWindow w)
{    CombineWithContext.Context context = createFromComponents(options, sideInputReader, w);    return keyedCombineFnWithContext.createAccumulator(context);}
public AccumT beam_f8818_0(AccumT accumulator, InputT input, BoundedWindow w)
{    CombineWithContext.Context context = createFromComponents(options, sideInputReader, w);    return keyedCombineFnWithContext.addInput(accumulator, input, context);}
public static BatchGroupAlsoByWindowFn<K, V, Iterable<V>> beam_f8826_0(WindowingStrategy<?, W> windowingStrategy, StateInternalsFactory<K> stateInternalsFactory, Coder<V> inputCoder)
{        if (BatchGroupAlsoByWindowReshuffleFn.isReshuffle(windowingStrategy)) {        return new BatchGroupAlsoByWindowReshuffleFn<>();    } else if (BatchGroupAlsoByWindowViaIteratorsFn.isSupported(windowingStrategy)) {        return new BatchGroupAlsoByWindowViaIteratorsFn<K, V, W>(windowingStrategy);    }    return new BatchGroupAlsoByWindowViaOutputBufferFn<>(windowingStrategy, stateInternalsFactory, SystemReduceFn.buffering(inputCoder));}
public static BatchGroupAlsoByWindowFn<K, InputT, OutputT> beam_f8827_0(final WindowingStrategy<?, W> windowingStrategy, final AppliedCombineFn<K, InputT, AccumT, OutputT> combineFn)
{    checkNotNull(combineFn);    return new BatchGroupAlsoByWindowAndCombineFn<>(windowingStrategy, combineFn.getFn());}
public static boolean beam_f8828_0(WindowingStrategy<?, ?> strategy)
{    if (!strategy.getWindowFn().isNonMerging()) {        return false;    }        if (!strategy.getTimestampCombiner().dependsOnlyOnEarliestTimestamp() && !strategy.getTimestampCombiner().dependsOnlyOnWindow()) {        return false;    }    return true;}
private void beam_f8836_0()
{    while (iterator.hasNext()) {        WindowedValue<V> peek = iterator.peek();        if (peek.getTimestamp().isAfter(window.maxTimestamp())) {                        break;        }        if (!(peek.getWindows().size() == 1 && peek.getWindows().contains(window))) {                                    iterator = iterator.copy();        }        if (!peek.getWindows().contains(window)) {                        iterator.next();        } else {                        break;        }    }}
public T beam_f8837_0()
{    return list.get(index++);}
public boolean beam_f8838_0()
{    return index < list.size();}
public void beam_f8846_0(Thread t, Runnable r)
{    semaphore.release();}
public static Source beam_f8847_0(Source source)
{    if (source.getBaseSpecs() == null) {        return source;    }    Map<String, Object> params = new HashMap<>();    for (Map<String, Object> baseSpec : source.getBaseSpecs()) {        params.putAll(baseSpec);    }    params.putAll(source.getSpec());    Source result = source.clone();    result.setSpec(params);    result.setBaseSpecs(null);    return result;}
protected ForwardingReiterator<T> beam_f8848_0()
{    ForwardingReiterator<T> result;    try {        @SuppressWarnings("unchecked")        ForwardingReiterator<T> declResult = (ForwardingReiterator<T>) super.clone();        result = declResult;    } catch (CloneNotSupportedException e) {        throw new AssertionError("Object.clone() for a ForwardingReiterator<T> threw " + "CloneNotSupportedException; this should not happen, " + "since ForwardingReiterator<T> implements Cloneable.", e);    }    result.base = base.copy();    return result;}
public boolean beam_f8856_0()
{    return false;}
public Object beam_f8857_0()
{    throw new NoSuchElementException();}
public void beam_f8858_0()
{    throw new IllegalArgumentException();}
public ShuffleEntry beam_f8866_0() throws NoSuchElementException
{    fillEntriesIfNeeded();    return entries.next();}
public void beam_f8867_0() throws UnsupportedOperationException
{    throw new UnsupportedOperationException();}
public ShuffleReadIterator beam_f8868_0()
{    return new ShuffleReadIterator(this);}
public ByteArrayShufflePosition beam_f8877_0()
{    return new ByteArrayShufflePosition(Bytes.concat(position, new byte[] { 0 }));}
public boolean beam_f8878_0(Object o)
{    if (this == o) {        return true;    }    if (o instanceof ByteArrayShufflePosition) {        ByteArrayShufflePosition that = (ByteArrayShufflePosition) o;        return Arrays.equals(this.position, that.position);    }    return false;}
public int beam_f8879_0()
{    return Arrays.hashCode(position);}
public void beam_f8890_0(Object elem) throws Exception
{    try (Closeable scope = context.enterProcess()) {        checkStarted();        Receiver receiver = receivers[0];        if (receiver != null) {            receiver.process(elem);        }    }}
public boolean beam_f8891_0()
{    return true;}
protected void beam_f8892_0(ShuffleEntry entry)
{    notifyElementRead(entry.length());}
protected void beam_f8900_0(ShuffleEntry entry)
{    ValuesIterator.this.notifyValueReturned(entry.length());}
public boolean beam_f8901_0()
{    if (cachedHasNext != null) {        return cachedHasNext;    }    cachedHasNext = advance();    return cachedHasNext;}
private boolean beam_f8902_0()
{            Reiterator<ShuffleEntry> possibleStartOfNextKey = valuesIterator.copy();    if (valuesIterator.hasNext()) {        ShuffleEntry entry = valuesIterator.next();        if (Arrays.equals(entry.getKey(), expectedKeyBytes)) {            byteSizeRead += entry.length();            tracker.saw(entry);            current = entry;            return true;        }    }        if (expectedKeyBytes == parent.currentKeyBytes) {                parent.shuffleIterator = possibleStartOfNextKey;                        parent.totalByteSizeOfEntriesForCurrentKey = byteSizeRead;        }    return false;}
public synchronized boolean beam_f8910_0()
{    return getStopPosition() == getLastGroupStart();}
public synchronized long beam_f8911_0()
{    if (!isStarted()) {        return 0;    } else if (isDone()) {        return splitPointsSeen;    } else {                checkState(splitPointsSeen > 0, "A started rangeTracker should have seen > 0 split points (is %s)", splitPointsSeen);        return splitPointsSeen - 1;    }}
public synchronized boolean beam_f8912_0(boolean isAtSplitPoint, @Nullable ShufflePosition groupStart)
{    if (lastGroupStart == null && !isAtSplitPoint) {        throw new IllegalStateException(String.format("The first group [at %s] must be at a split point", groupStart.toString()));    }    if (this.startPosition != null && groupStart.compareTo(this.startPosition) < 0) {        throw new IllegalStateException(String.format("Trying to return record at %s which is before the starting position at %s", groupStart, this.startPosition));    }    int comparedToLast = (lastGroupStart == null) ? 1 : groupStart.compareTo(this.lastGroupStart);    if (comparedToLast < 0) {        throw new IllegalStateException(String.format("Trying to return group at %s which is before the last-returned group at %s", groupStart, this.lastGroupStart));    }    if (isAtSplitPoint) {        splitPointsSeen++;        if (comparedToLast == 0) {            throw new IllegalStateException(String.format("Trying to return a group at a split point with same position as the " + "previous group: both at %s, last group was %s", groupStart, lastGroupWasAtSplitPoint ? "at a split point." : "not at a split point."));        }        if (stopPosition != null && groupStart.compareTo(stopPosition) >= 0) {            return false;        }    } else {        checkState(comparedToLast == 0,         "Trying to return a group not at a split point, but with a different position " + "than the previous group: last group was %s at %s, current at %s", lastGroupWasAtSplitPoint ? "a split point" : "a non-split point", lastGroupStart, groupStart);    }    this.lastGroupStart = groupStart;    this.lastGroupWasAtSplitPoint = isAtSplitPoint;    return true;}
public static GroupingTable<K, InputT, AccumT> beam_f8920_0(GroupingKeyCreator<? super K> groupingKeyCreator, PairInfo pairInfo, Combiner<? super K, InputT, AccumT, ?> combineFn, SizeEstimator<? super K> keySizer, SizeEstimator<? super AccumT> accumulatorSizer)
{    return new CombiningGroupingTable<>(DEFAULT_MAX_GROUPING_TABLE_BYTES, groupingKeyCreator, pairInfo, combineFn, keySizer, accumulatorSizer);}
public static GroupingTable<K, InputT, AccumT> beam_f8921_0(GroupingKeyCreator<? super K> groupingKeyCreator, PairInfo pairInfo, Combiner<? super K, InputT, AccumT, ?> combineFn, SizeEstimator<? super K> keySizer, SizeEstimator<? super AccumT> accumulatorSizer, double sizeEstimatorSampleRate)
{    return new CombiningGroupingTable<>(DEFAULT_MAX_GROUPING_TABLE_BYTES, groupingKeyCreator, pairInfo, combineFn, new SamplingSizeEstimator<>(keySizer, sizeEstimatorSampleRate, 1.0), new SamplingSizeEstimator<>(accumulatorSizer, sizeEstimatorSampleRate, 1.0));}
public void beam_f8922_0(Object pair, Receiver receiver) throws Exception
{    put((K) pairInfo.getKeyFromInputPair(pair), (InputT) pairInfo.getValueFromInputPair(pair), receiver);}
public List<V> beam_f8930_0()
{    return values;}
public long beam_f8931_0()
{    return size;}
public void beam_f8933_0(V value) throws Exception
{    values.add(value);    size += BYTES_PER_JVM_WORD + valueSizer.estimateSize(value);}
public long beam_f8941_0(T element) throws Exception
{    if (sampleNow()) {        return recordSample(underlying.estimateSize(element));    } else {        return estimate;    }}
private boolean beam_f8942_0()
{    totalElements++;    return --nextSample < 0;}
private long beam_f8943_0(long value)
{    sampledElements += 1;    sampledSum += value;    sampledSumSquares += value * value;    estimate = (long) Math.ceil(sampledSum / (double) sampledElements);    long target = desiredSampleSize();    if (sampledElements < minSampled || sampledElements < target) {                nextSample = 0;    } else {        double rate = cap(minSampleRate, maxSampleRate, Math.max(        1.0 / (totalElements - minSampled + 1),         target / (double) totalElements));                                                nextSample = rate == 1.0 ? 0 : (long) Math.floor(Math.log(random.nextDouble()) / Math.log(1 - rate));    }    return value;}
public NativeReader.DynamicSplitResult beam_f8951_0(NativeReader.DynamicSplitRequest splitRequest) throws Exception
{    return getReadOperation().requestDynamicSplit(splitRequest);}
public ReadOperation beam_f8952_0() throws Exception
{    if (operations == null || operations.isEmpty()) {        throw new IllegalStateException("Map task has no operation.");    }    Operation readOperation = operations.get(0);    if (!(readOperation instanceof ReadOperation)) {        throw new IllegalStateException("First operation in the map task is not a ReadOperation.");    }    return (ReadOperation) readOperation;}
public boolean beam_f8953_0()
{    for (Operation op : operations) {        if (!op.supportsRestart()) {            return false;        }    }    return true;}
public Position beam_f8962_0()
{    return acceptedPosition;}
public String beam_f8963_0()
{    return String.valueOf(acceptedPosition);}
public int beam_f8964_0()
{    return acceptedPosition.hashCode();}
 boolean beam_f8972_0()
{    return (initializationState == InitializationState.ABORTED);}
public void beam_f8973_0() throws Exception
{    synchronized (initializationStateLock) {        checkUnstarted();        initializationState = InitializationState.STARTED;    }}
public void beam_f8974_0() throws Exception
{    synchronized (initializationStateLock) {        checkStarted();        initializationState = InitializationState.FINISHED;    }}
public Counter<Long, ?> beam_f8982_0()
{    return objectCount;}
public Counter<Long, Long> beam_f8983_0()
{    return byteCount;}
public Counter<Long, CounterMean<Long>> beam_f8984_0()
{    return meanByteCount;}
public void beam_f8992_0(Object elem) throws Exception
{    for (ElementCounter counter : outputCounters) {        counter.update(elem);    }        for (Receiver out : outputs) {        if (out != null) {            out.process(elem);        }    }    for (ElementCounter counter : outputCounters) {        counter.finishLazyUpdate(elem);    }}
public int beam_f8993_0()
{    return outputs.size();}
public Receiver beam_f8994_0()
{    if (outputs.size() != 1) {        throw new AssertionError("only one receiver expected");    }    return outputs.get(0);}
public ProgressTracker<T> beam_f9002_0()
{    return new Tracker(nextElementIndex);}
public void beam_f9003_0(T element)
{    long thisElementIndex = nextElementIndex;    nextElementIndex++;    if (thisElementIndex == nextIndexToReport) {        nextIndexToReport = nextElementIndex;        report(element);    }}
public T beam_f9004_0()
{    T result = super.next();    tracker.saw(result);    return result;}
public void beam_f9012_0() throws Exception
{    try (Closeable scope = context.enterStart()) {        super.start();        runReadLoop();    }}
public boolean beam_f9013_0()
{    return reader.supportsRestart();}
protected void beam_f9014_1() throws Exception
{    try (Closeable scope = context.enterProcess()) {        Receiver receiver = receivers[0];        if (receiver == null) {                        return;        }                        NativeReader.NativeReaderIterator<?> iterator = reader.iterator();        synchronized (initializationStateLock) {            readerIterator = new SynchronizedReaderIterator<>(iterator, progress);        }        Runnable setProgressFromIterator = new Runnable() {            @Override            public void run() {                readerIterator.setProgressFromIterator();            }        };        try (AutoCloseable updater = schedulePeriodicActivity(scheduler, setProgressFromIterator, progressUpdatePeriodMs)) {                        readerIterator.setProgressFromIterator();            for (boolean more = readerIterator.start(); more; more = readerIterator.advance()) {                if (abortRead.get()) {                    throw new InterruptedException("Read loop was aborted.");                }                if (progressUpdatePeriodMs == UPDATE_ON_EACH_ITERATION) {                    readerIterator.setProgressFromIterator();                }                receiver.process(readerIterator.getCurrent());            }                        readerIterator.setProgressFromIterator();        } finally {            scheduler.shutdown();            scheduler.awaitTermination(1, TimeUnit.MINUTES);            if (!scheduler.isTerminated()) {                                scheduler.awaitTermination(Long.MAX_VALUE, TimeUnit.SECONDS);                            }        }    }}
public void beam_f9024_0(Observable obs, Object obj)
{    Preconditions.checkArgument(obs == reader, "unexpected observable");    Preconditions.checkArgument(obj instanceof Long, "unexpected parameter object");    byteCount.addValue((Long) obj);}
public synchronized boolean beam_f9025_0() throws IOException
{    return readerIterator.start();}
public synchronized void beam_f9026_0() throws IOException
{    readerIterator.close();}
public String beam_f9034_0()
{    return MoreObjects.toStringHelper(this).add("reader", reader).add("readerIterator", readerIterator).add("byteCount", byteCount).toString();}
public void beam_f9035_0(Operation source, int outputNum)
{    checkUnstarted();    OutputReceiver fanOut = source.receivers[outputNum];    fanOut.addOutput(this);}
public ShufflePosition beam_f9036_0()
{    return position;}
public int beam_f9044_0()
{    return getClass().hashCode() + (position == null ? 0 : position.hashCode()) + (key == null ? 0 : Arrays.hashCode(key)) + (secondaryKey == null ? 0 : Arrays.hashCode(secondaryKey)) + (value == null ? 0 : Arrays.hashCode(value));}
private void beam_f9045_0()
{    if (this.experimentEnabled) {        ExecutionStateTracker.ExecutionState currentState = ExecutionStateTracker.getCurrentExecutionState();        String currentStateName = null;        if (currentState instanceof DataflowExecutionState) {            currentStateName = ((DataflowExecutionState) currentState).getStepName().originalName();        }        if (this.currentCounter != null && currentStateName == this.currentCounter.getName().originalRequestingStepName()) {                        return;        }        CounterName name = ShuffleReadCounter.generateCounterName(this.originalShuffleStepName, currentStateName);        this.currentCounter = this.counterSet.longSum(name);    } else {        this.currentCounter = this.legacyPerOperationPerDatasetBytesCounter;    }}
public void beam_f9046_0(long n)
{    checkState();    if (this.currentCounter == null) {        return;    }    this.currentCounter.addValue(n);}
 NativeReader.DynamicSplitResult beam_f9057_0() throws Exception
{        return null;}
 NativeReader.DynamicSplitResult beam_f9058_0(NativeReader.DynamicSplitRequest splitRequest) throws Exception
{        return null;}
 List<Integer> beam_f9061_0()
{    return Lists.newArrayList();}
private void beam_f9069_0()
{        synchronized (executor) {        if (executor.isShutdown()) {            return;        }        try {            checkForPeriodicCheckpoint();            tryCheckpointIfNeeded();            reportProgress();        } finally {            scheduleNextUpdate();        }    }}
private void beam_f9070_1()
{    if (clock.currentTimeMillis() >= nextPeriodicCheckpointTimeMs) {                if (checkpointState == CheckpointState.CHECKPOINT_NOT_REQUESTED) {            checkpointState = CheckpointState.CHECKPOINT_REQUESTED;        }        nextPeriodicCheckpointTimeMs = Long.MAX_VALUE;    }}
protected boolean beam_f9071_1()
{    if (checkpointState == CheckpointState.CHECKPOINT_REQUESTED && worker != null) {                try {            NativeReader.DynamicSplitResult checkpointPos = worker.requestCheckpoint();            if (checkpointPos != null) {                                dynamicSplitResultToReport = checkpointPos;                checkpointState = CheckpointState.CHECKPOINT_SUCCESSFUL;                return true;            }        } catch (Throwable e) {                    }    }    return false;}
private static CounterName beam_f9079_0(OperationContext context)
{    return CounterName.named(context.nameContext().systemName() + "-ByteCount");}
public static WriteOperation beam_f9080_0(Sink<?> sink, OutputReceiver[] receivers, OperationContext context)
{    return new WriteOperation(sink, receivers, context, bytesCounterName(context));}
public static WriteOperation beam_f9081_0(Sink<?> sink, OperationContext context)
{    return create(sink, new OutputReceiver[] {}, context);}
public Counter<Long, Long> beam_f9090_0()
{    return byteCount;}
public int beam_f9091_0()
{    return INVALID_SINK_INDEX;}
public String beam_f9092_0()
{    return MoreObjects.toStringHelper(this).add("sink", sink).add("writer", writer).add("byteCount", byteCount).toString();}
private void beam_f9100_0(File srcPath, ResourceId destination) throws IOException
{    StandardCreateOptions createOptions = StandardCreateOptions.builder().setMimeType("application/octet-stream").build();    try (WritableByteChannel dst = FileSystems.create(destination, createOptions)) {        try (ReadableByteChannel src = Channels.newChannel(new FileInputStream(srcPath))) {            ByteStreams.copy(src, dst);        }    }}
public void beam_f9101_0()
{    synchronized (waitingForStateChange) {        isRunning.set(false);        waitingForStateChange.notifyAll();    }}
public boolean beam_f9102_0()
{    return isThrashing.get();}
public void beam_f9110_1(String context)
{    if (!isThrashing.get()) {        return;    }    numPushbacks.incrementAndGet();        synchronized (waitingForResources) {        boolean interrupted = false;        try {                        while (isThrashing.get()) {                try {                    waitingForResources.wait();                } catch (InterruptedException e1) {                    interrupted = true;                                    }            }        } finally {            if (interrupted) {                Thread.currentThread().interrupt();            }        }    }    }
private static File beam_f9111_0()
{    File defaultPath = new File(System.getProperty("java.io.tmpdir", DEFAULT_LOGGING_DIR));    String jsonLogFile = System.getProperty("dataflow.worker.logging.filepath");    if (jsonLogFile == null) {        return defaultPath;    }    File logPath = new File(jsonLogFile).getParentFile();    if (logPath == null) {        return defaultPath;    }    return logPath;}
public File beam_f9112_0() throws MalformedObjectNameException, InstanceNotFoundException, ReflectionException, MBeanException, IOException
{    return dumpHeap(localDumpFolder);}
public boolean beam_f9121_0(byte[] buf, int offset, int length)
{    ByteBuffer byteBuffer = ByteBuffer.wrap(buf, offset, length);    return mightContain(byteBuffer);}
public boolean beam_f9122_0(ByteBuffer byteBuffer)
{    return bloomFilter.mightContain(byteBuffer);}
public boolean beam_f9123_0(Object other)
{    if (other == this) {        return true;    }    if (!(other instanceof ScalableBloomFilter)) {        return false;    }    ScalableBloomFilter scalableBloomFilter = (ScalableBloomFilter) other;    return Objects.equals(bloomFilter, scalableBloomFilter.bloomFilter);}
public ScalableBloomFilter beam_f9131_0()
{    int bloomFilterToUse = Long.SIZE - Long.numberOfLeadingZeros(numberOfInsertions);    if (Long.bitCount(numberOfInsertions) == 1) {        bloomFilterToUse -= 1;    }    bloomFilterToUse = Math.min(bloomFilterToUse, bloomFilters.size() - 1);    return new ScalableBloomFilter(bloomFilters.get(bloomFilterToUse));}
public void beam_f9132_0(ByteBuffer from, PrimitiveSink into)
{    into.putBytes(from.array(), from.position(), from.remaining());}
public static TimerOrElementCoder<T> beam_f9133_0(Coder<T> elemCoder)
{    return new TimerOrElementCoder<>(elemCoder);}
public PaneInfo beam_f9141_0()
{    return PaneInfo.NO_FIRING;}
public T beam_f9142_0()
{    return value;}
public WindowedValue<NewT> beam_f9143_0(NewT newValue)
{    return new ValueInEmptyWindows<>(newValue);}
public void beam_f9151_0(Object untypedElem) throws Exception
{    @SuppressWarnings("unchecked")    WindowedValue<KV<K, V>> elem = (WindowedValue) untypedElem;    V userValue = elem.getValue().getValue();    receiver.process(elem.withValue(userValue));}
public static Weigher<Object, Weighted> beam_f9155_0(final int keyWeight)
{    return (key, value) -> (int) Math.min(keyWeight + value.getWeight(), Integer.MAX_VALUE);}
public static Weigher<Weighted, Weighted> beam_f9156_0()
{    return (key, value) -> (int) Math.min(key.getWeight() + value.getWeight(), Integer.MAX_VALUE);}
private boolean beam_f9164_0()
{    return options.isEnableStreamingEngine();}
public synchronized void beam_f9165_1(Set<HostAndPort> endpoints) throws IOException
{    Preconditions.checkNotNull(endpoints);    if (endpoints.equals(this.endpoints)) {                return;    }        if (this.endpoints != null) {            }    initializeWindmillService(endpoints);}
public synchronized boolean beam_f9166_0()
{    return !stubList.isEmpty();}
public boolean beam_f9174_0()
{    return credentials.hasRequestMetadata();}
public boolean beam_f9175_0()
{    return credentials.hasRequestMetadataOnly();}
public void beam_f9176_0() throws IOException
{    credentials.refresh();}
public void beam_f9184_0(PrintWriter writer)
{    writer.write("Active Streams:<br>");    for (AbstractWindmillStream<?, ?> stream : streamRegistry) {        stream.appendSummaryHtml(writer);        writer.write("<br>");    }}
private BackOff beam_f9185_0()
{    return FluentBackoff.DEFAULT.withInitialBackoff(MIN_BACKOFF).withMaxBackoff(maxBackoff).backoff();}
private ResponseT beam_f9186_1(Supplier<ResponseT> function)
{    BackOff backoff = grpcBackoff();    int rpcErrors = 0;    while (true) {        try {            return function.get();        } catch (StatusRuntimeException e) {            try {                if (++rpcErrors % 20 == 0) {                                    }                if (!BackOffUtils.next(Sleeper.DEFAULT, backoff)) {                    throw new WindmillServerStub.RpcException(e);                }            } catch (IOException | InterruptedException i) {                if (i instanceof InterruptedException) {                    Thread.currentThread().interrupt();                }                WindmillServerStub.RpcException rpcException = new WindmillServerStub.RpcException(e);                rpcException.addSuppressed(i);                throw rpcException;            }        }    }}
public ReportStatsResponse beam_f9194_0(ReportStatsRequest request)
{    if (syncApplianceStub == null) {        throw new RpcException(new UnsupportedOperationException("ReportStats not supported with windmill service."));    } else {        return callWithBackoff(() -> syncApplianceStub.withDeadlineAfter(unaryDeadlineSeconds, TimeUnit.SECONDS).reportStats(request));    }}
public long beam_f9195_0()
{    return getWorkThrottleTimer.getAndResetThrottleTime() + getDataThrottleTimer.getAndResetThrottleTime() + commitWorkThrottleTimer.getAndResetThrottleTime();}
private JobHeader beam_f9196_0()
{    return JobHeader.newBuilder().setJobId(options.getJobId()).setProjectId(options.getProject()).setWorkerId(options.getWorkerId()).build();}
public void beam_f9204_0()
{    onStreamFinished(null);}
private void beam_f9205_1(@Nullable Throwable t)
{    synchronized (this) {        if (clientClosed.get() && !hasPendingRequests()) {            streamRegistry.remove(AbstractWindmillStream.this);            finishLatch.countDown();            return;        }    }    if (t != null) {        Status status = null;        if (t instanceof StatusRuntimeException) {            status = ((StatusRuntimeException) t).getStatus();        }        if (errorCount.incrementAndGet() % logEveryNStreamFailures == 0) {                    }                if (status != null && status.getCode() == Status.Code.RESOURCE_EXHAUSTED) {            startThrottleTimer();        }        try {            long sleep = backoff.nextBackOffMillis();            sleepUntil.set(Instant.now().getMillis() + sleep);            Thread.sleep(sleep);        } catch (InterruptedException e) {            Thread.currentThread().interrupt();        } catch (IOException e) {                }    }    executor.execute(AbstractWindmillStream.this::startStream);}
public final synchronized void beam_f9206_0()
{        clientClosed.set(true);    requestObserver.onCompleted();}
protected void beam_f9214_0()
{    getWorkThrottleTimer.start();}
private void beam_f9215_0(Windmill.ComputationWorkItemMetadata metadata)
{    this.computation = metadata.getComputationId();    this.inputDataWatermark = WindmillTimeUtils.windmillToHarnessWatermark(metadata.getInputDataWatermark());    this.synchronizedProcessingTime = WindmillTimeUtils.windmillToHarnessWatermark(metadata.getDependentRealtimeInputWatermark());}
public void beam_f9216_0(StreamingGetWorkResponseChunk chunk)
{    if (chunk.hasComputationMetadata()) {        setMetadata(chunk.getComputationMetadata());    }    this.data = data.concat(chunk.getSerializedWorkItem());    this.bufferedSize += chunk.getSerializedWorkItem().size();}
public KeyedGetDataResponse beam_f9224_0(String computation, KeyedGetDataRequest request)
{    return issueRequest(new QueuedRequest(computation, request), KeyedGetDataResponse::parseFrom);}
public GlobalData beam_f9225_0(GlobalDataRequest request)
{    return issueRequest(new QueuedRequest(request), GlobalData::parseFrom);}
public void beam_f9226_0(Map<String, List<KeyedGetDataRequest>> active)
{    long builderBytes = 0;    StreamingGetDataRequest.Builder builder = StreamingGetDataRequest.newBuilder();    for (Map.Entry<String, List<KeyedGetDataRequest>> entry : active.entrySet()) {        for (KeyedGetDataRequest request : entry.getValue()) {                        long bytes = (long) entry.getKey().length() + request.getSerializedSize() + 10;            if (builderBytes > 0 && (builderBytes + bytes > GET_DATA_STREAM_CHUNK_SIZE || builder.getRequestIdCount() >= streamingRpcBatchLimit)) {                send(builder.build());                builderBytes = 0;                builder.clear();            }            builderBytes += bytes;            builder.addStateRequest(ComputationGetDataRequest.newBuilder().setComputationId(entry.getKey()).addRequests(request));        }    }    if (builderBytes > 0) {        send(builder.build());    }}
 void beam_f9234_0()
{    flushInternal(queue);    queuedBytes = 0;}
public void beam_f9235_0(PrintWriter writer)
{    writer.format("CommitWorkStream: %d pending", pending.size());}
protected synchronized void beam_f9236_0()
{    send(StreamingCommitWorkRequest.newBuilder().setHeader(makeHeader()).build());    Batcher resendBatcher = new Batcher();    for (Map.Entry<Long, PendingRequest> entry : pending.entrySet()) {        if (!resendBatcher.canAccept(entry.getValue())) {            resendBatcher.flush();        }        resendBatcher.add(entry.getKey(), entry.getValue());    }    resendBatcher.flush();}
private void beam_f9244_0(Map<Long, PendingRequest> requests)
{    StreamingCommitWorkRequest.Builder requestBuilder = StreamingCommitWorkRequest.newBuilder();    String lastComputation = null;    for (Map.Entry<Long, PendingRequest> entry : requests.entrySet()) {        PendingRequest request = entry.getValue();        StreamingCommitRequestChunk.Builder chunkBuilder = requestBuilder.addCommitChunkBuilder();        if (lastComputation == null || !lastComputation.equals(request.computation)) {            chunkBuilder.setComputationId(request.computation);            lastComputation = request.computation;        }        chunkBuilder.setRequestId(entry.getKey());        chunkBuilder.setShardingKey(request.request.getShardingKey());        chunkBuilder.setSerializedWorkItemCommit(request.request.toByteString());    }    StreamingCommitWorkRequest request = requestBuilder.build();    try {        synchronized (this) {            pending.putAll(requests);            send(request);        }    } catch (IllegalStateException e) {        }}
private void beam_f9245_0(final long id, PendingRequest pendingRequest)
{    Preconditions.checkNotNull(pendingRequest.computation);    final ByteString serializedCommit = pendingRequest.request.toByteString();    synchronized (this) {        pending.put(id, pendingRequest);        for (int i = 0; i < serializedCommit.size(); i += COMMIT_STREAM_CHUNK_SIZE) {            int end = i + COMMIT_STREAM_CHUNK_SIZE;            ByteString chunk = serializedCommit.substring(i, Math.min(end, serializedCommit.size()));            StreamingCommitRequestChunk.Builder chunkBuilder = StreamingCommitRequestChunk.newBuilder().setRequestId(id).setSerializedWorkItemCommit(chunk).setComputationId(pendingRequest.computation).setShardingKey(pendingRequest.request.getShardingKey());            int remaining = serializedCommit.size() - end;            if (remaining > 0) {                chunkBuilder.setRemainingBytesForWorkItem(remaining);            }            StreamingCommitWorkRequest requestChunk = StreamingCommitWorkRequest.newBuilder().addCommitChunk(chunkBuilder).build();            try {                send(requestChunk);            } catch (IllegalStateException e) {                                break;            }        }    }}
public boolean beam_f9246_0()
{    if (current != null) {        return true;    }    try {        current = queue.take();        if (current != POISON_PILL) {            return true;        }        if (cancelled.get()) {            throw new CancellationException();        }        if (complete.get()) {            return false;        }        throw new IllegalStateException("Got poison pill but stream is not done.");    } catch (InterruptedException e) {        Thread.currentThread().interrupt();        throw new CancellationException();    }}
public void beam_f9254_0() throws IOException
{    stream.close();}
public synchronized void beam_f9255_0()
{    if (!throttled()) {                startTime = Instant.now().getMillis();    }}
public synchronized void beam_f9256_0()
{    if (throttled()) {                totalTime += Instant.now().getMillis() - startTime;        startTime = -1;    }}
public Windmill.GetDataResponse beam_f9265_0(Windmill.GetDataRequest dataRequest)
{    try {        byte[] requestBytes = dataRequest.toByteArray();        return Windmill.GetDataResponse.newBuilder().mergeFrom(getDataImpl(nativePointer, requestBytes, requestBytes.length)).build();    } catch (IOException e) {        throw new RuntimeException("Proto deserialization failed: " + e);    }}
public Windmill.CommitWorkResponse beam_f9266_0(Windmill.CommitWorkRequest commitRequest)
{    try {        byte[] requestBytes = commitRequest.toByteArray();        return Windmill.CommitWorkResponse.newBuilder().mergeFrom(commitWorkImpl(nativePointer, requestBytes, requestBytes.length)).build();    } catch (IOException e) {        throw new RuntimeException("Proto deserialization failed: " + e);    }}
public Windmill.GetConfigResponse beam_f9267_0(Windmill.GetConfigRequest configRequest)
{    try {        byte[] requestBytes = configRequest.toByteArray();        return Windmill.GetConfigResponse.newBuilder().mergeFrom(getConfigImpl(nativePointer, requestBytes, requestBytes.length)).build();    } catch (IOException e) {        throw new RuntimeException("Proto deserialization failed: " + e);    }}
public K beam_f9276_0()
{    return key;}
public Iterable<TimerData> beam_f9277_0()
{    FluentIterable<Timer> allTimers = FluentIterable.from(workItem.getTimers().getTimersList());    FluentIterable<Timer> eventTimers = allTimers.filter(IS_WATERMARK);    FluentIterable<Timer> nonEventTimers = allTimers.filter(Predicates.not(IS_WATERMARK));    return eventTimers.append(nonEventTimers).transform(timer -> WindmillTimerInternals.windmillTimerToTimerData(WindmillNamespacePrefix.SYSTEM_NAMESPACE_PREFIX, timer, windowCoder));}
public Iterable<WindowedValue<ElemT>> beam_f9278_0()
{    return FluentIterable.from(workItem.getMessageBundlesList()).transformAndConcat(Windmill.InputMessageBundle::getMessagesList).transform(message -> {        try {            Instant timestamp = WindmillTimeUtils.windmillToHarnessTimestamp(message.getTimestamp());            Collection<? extends BoundedWindow> windows = WindmillSink.decodeMetadataWindows(windowsCoder, message.getMetadata());            PaneInfo pane = WindmillSink.decodeMetadataPane(message.getMetadata());            InputStream inputStream = message.getData().newInput();            ElemT value = valueCoder.decode(inputStream, Coder.Context.OUTER);            return WindowedValue.of(value, timestamp, windows, pane);        } catch (IOException e) {            throw new RuntimeException(e);        }    });}
public boolean beam_f9286_0(KeyedWorkItem<K, ElemT> value)
{    return true;}
public void beam_f9287_0(KeyedWorkItem<K, ElemT> value, ElementByteSizeObserver observer) throws Exception
{    if (value instanceof WindmillKeyedWorkItem) {        long serializedSize = ((WindmillKeyedWorkItem<?, ?>) value).workItem.getSerializedSize();        observer.update(serializedSize);    } else {        throw new UnsupportedOperationException();    }}
public Coder<K> beam_f9289_0()
{    return kvCoder.getKeyCoder();}
public static PaneInfo beam_f9297_0(ByteString metadata) throws IOException
{    InputStream inStream = metadata.newInput();    return PaneInfoCoder.INSTANCE.decode(inStream);}
public static Collection<? extends BoundedWindow> beam_f9298_0(Coder<Collection<? extends BoundedWindow>> windowsCoder, ByteString metadata) throws IOException
{    InputStream inStream = metadata.newInput();    PaneInfoCoder.INSTANCE.decode(inStream);    return windowsCoder.decode(inStream, Coder.Context.OUTER);}
public Map<String, SinkFactory> beam_f9299_0()
{    Factory factory = new Factory();    return ImmutableMap.of("WindmillSink", factory, "org.apache.beam.runners.dataflow.worker.WindmillSink", factory);}
public long beam_f9307_0()
{    return displayedWeight;}
public void beam_f9308_0(ByteString processingKey)
{    synchronized (this) {        ComputationKey key = new ComputationKey(computation, processingKey);        for (StateId id : keyIndex.get(key)) {            stateCache.invalidate(id);        }        keyIndex.removeAll(key);    }}
public ForKey beam_f9309_0(ByteString key, String stateFamily, long cacheToken, long workToken)
{    return new ForKey(computation, key, stateFamily, cacheToken, workToken);}
public int beam_f9317_0()
{    return Objects.hash(computation, key);}
public boolean beam_f9318_0(Object other)
{    if (other instanceof StateId) {        StateId otherId = (StateId) other;        return computationKey.equals(otherId.computationKey) && stateFamily.equals(otherId.stateFamily) && namespaceKey.equals(otherId.namespaceKey);    }    return false;}
public ComputationKey beam_f9319_0()
{    return computationKey;}
public long beam_f9327_0()
{    return lastWorkToken;}
public boolean beam_f9328_0(Object other)
{    if (!(other instanceof NamespacedTag)) {        return false;    }    NamespacedTag<?> that = (NamespacedTag<?>) other;    return namespace.equals(that.namespace) && tag.equals(that.tag);}
public int beam_f9329_0()
{    return Objects.hash(namespace, tag);}
public MapState<KeyT, ValueT> beam_f9337_0(StateTag<MapState<KeyT, ValueT>> spec, Coder<KeyT> mapKeyCoder, Coder<ValueT> mapValueCoder)
{    throw new UnsupportedOperationException(String.format("%s is not supported", MapState.class.getSimpleName()));}
public WatermarkHoldState beam_f9338_0(StateTag<WatermarkHoldState> address, TimestampCombiner timestampCombiner)
{    WindmillWatermarkHold result = (WindmillWatermarkHold) cache.get(namespace, address);    if (result == null) {        result = new WindmillWatermarkHold(namespace, address, stateFamily, timestampCombiner, isNewKey);    }    result.initializeForWorkItem(reader, scopedReadStateSupplier);    return result;}
public CombiningState<InputT, AccumT, OutputT> beam_f9339_0(StateTag<CombiningState<InputT, AccumT, OutputT>> address, Coder<AccumT> accumCoder, CombineFn<InputT, AccumT, OutputT> combineFn)
{    WindmillCombiningState<InputT, AccumT, OutputT> result = new WindmillCombiningState<InputT, AccumT, OutputT>(namespace, address, stateFamily, accumCoder, combineFn, cache, isNewKey);    result.initializeForWorkItem(reader, scopedReadStateSupplier);    return result;}
public final Future<WorkItemCommitRequest> beam_f9347_0(WindmillStateCache.ForKey cache) throws IOException
{    return Futures.immediateFuture(persistDirectly(cache));}
public T beam_f9348_0(StateNamespace namespace, StateTag<T> address)
{    return workItemState.get(namespace, address, StateContexts.nullContext());}
public T beam_f9349_0(StateNamespace namespace, StateTag<T> address, StateContext<?> c)
{    return workItemState.get(namespace, address, c);}
private Iterable<T> beam_f9357_0(Future<Iterable<T>> persistedData)
{    try (Closeable scope = scopedReadState()) {        if (cachedValues != null) {            return cachedValues.snapshot();        }        Iterable<T> data = persistedData.get();        if (data instanceof Weighted) {                        cachedValues = new ConcatIterables<>();            cachedValues.extendWith(data);            encodedSize = ((Weighted) data).getWeight();            return cachedValues.snapshot();        } else {                        return data;        }    } catch (InterruptedException | ExecutionException | IOException e) {        if (e instanceof InterruptedException) {            Thread.currentThread().interrupt();        }        throw new RuntimeException("Unable to read state", e);    }}
public boolean beam_f9358_0()
{    return cachedValues != null;}
public WindmillBag<T> beam_f9359_0()
{    getFuture();    return this;}
public void beam_f9367_0(Iterable<T> iterable)
{    iterables.add(iterable);}
public Iterator<T> beam_f9368_0()
{    return Iterators.concat(Iterables.transform(iterables, Iterable::iterator).iterator());}
public Iterable<T> beam_f9369_0()
{    final int limit = iterables.size();    final List<Iterable<T>> iterablesList = iterables;    return () -> Iterators.concat(Iterators.transform(Iterators.limit(iterablesList.iterator(), limit), Iterable::iterator));}
private Future<Instant> beam_f9377_0()
{    return cachedValue != null ? Futures.immediateFuture(cachedValue.orNull()) : reader.watermarkFuture(stateKey, stateFamily);}
private Future<WorkItemCommitRequest> beam_f9378_0()
{    boolean windmillCanCombine = false;                windmillCanCombine |= timestampCombiner.dependsOnlyOnWindow();                        windmillCanCombine |= timestampCombiner.dependsOnlyOnEarliestTimestamp();    if (windmillCanCombine) {                WorkItemCommitRequest.Builder commitBuilder = WorkItemCommitRequest.newBuilder();        commitBuilder.addWatermarkHoldsBuilder().setTag(stateKey).setStateFamily(stateFamily).addTimestamps(WindmillTimeUtils.harnessToWindmillTimestamp(localAdditions));        if (cachedValue != null) {            cachedValue = Optional.of(cachedValue.isPresent() ? timestampCombiner.combine(cachedValue.get(), localAdditions) : localAdditions);        }        return Futures.immediateFuture(commitBuilder.buildPartial());    } else {                return Futures.lazyTransform((cachedValue != null) ? Futures.immediateFuture(cachedValue.orNull()) : reader.watermarkFuture(stateKey, stateFamily), priorHold -> {            cachedValue = Optional.of((priorHold != null) ? timestampCombiner.combine(priorHold, localAdditions) : localAdditions);            WorkItemCommitRequest.Builder commitBuilder = WorkItemCommitRequest.newBuilder();            commitBuilder.addWatermarkHoldsBuilder().setTag(stateKey).setStateFamily(stateFamily).setReset(true).addTimestamps(WindmillTimeUtils.harnessToWindmillTimestamp(cachedValue.get()));            return commitBuilder.buildPartial();        });    }}
 void beam_f9379_0(WindmillStateReader reader, Supplier<Closeable> scopedReadStateSupplier)
{    super.initializeForWorkItem(reader, scopedReadStateSupplier);    this.bag.initializeForWorkItem(reader, scopedReadStateSupplier);}
public ReadableState<Boolean> beam_f9387_0()
{    final ReadableState<Boolean> bagIsEmpty = bag.isEmpty();    return new ReadableState<Boolean>() {        @Override        public ReadableState<Boolean> readLater() {            bagIsEmpty.readLater();            return this;        }        @Override        public Boolean read() {            return !hasLocalAdditions && bagIsEmpty.read();        }    };}
public ReadableState<Boolean> beam_f9388_0()
{    bagIsEmpty.readLater();    return this;}
public Boolean beam_f9389_0()
{    return !hasLocalAdditions && bagIsEmpty.read();}
private SettableFuture<FutureT> beam_f9397_0()
{    return future;}
private SettableFuture<FutureT> beam_f9398_0(StateTag stateTag)
{    if (future.isDone()) {        throw new IllegalStateException("Future for " + stateTag + " is already done");    }    return future;}
private Coder<ElemT> beam_f9399_0()
{    if (coder == null) {        throw new IllegalStateException("Coder has already been cleared from cache");    }    Coder<ElemT> result = coder;    coder = null;    return result;}
public T beam_f9407_0() throws InterruptedException, ExecutionException
{    if (!delegate().isDone() && reader != null) {                reader.startBatchAndBlock();    }    reader = null;    return super.get();}
public T beam_f9408_0(long timeout, TimeUnit unit) throws InterruptedException, ExecutionException, TimeoutException
{    if (!delegate().isDone() && reader != null) {                reader.startBatchAndBlock();    }    reader = null;    return super.get(timeout, unit);}
private Future<T> beam_f9409_0(final Future<T> future)
{    if (future.isDone()) {                return future;    } else {                return new WrappedFuture<>(this, future);    }}
public boolean beam_f9417_0(T elem)
{    throw new UnsupportedOperationException("Must use AddWeighted()");}
public long beam_f9418_0()
{    return weight;}
public void beam_f9419_0(T elem, long weight)
{    delegate.add(elem);    this.weight += weight;}
public void beam_f9427_0(TimerData timerKey)
{    timers.put(timerKey.getTimerId(), timerKey.getNamespace(), timerKey);    timerStillPresent.put(timerKey.getTimerId(), timerKey.getNamespace(), true);}
public void beam_f9428_0(StateNamespace namespace, String timerId, Instant timestamp, TimeDomain timeDomain)
{    timers.put(timerId, namespace, TimerData.of(timerId, namespace, timestamp, timeDomain));    timerStillPresent.put(timerId, namespace, true);}
public void beam_f9429_0(TimerData timerKey)
{    timers.put(timerKey.getTimerId(), timerKey.getNamespace(), timerKey);    timerStillPresent.put(timerKey.getTimerId(), timerKey.getNamespace(), false);}
public static boolean beam_f9437_0(Windmill.Timer timer)
{    return timer.getTag().startsWith(WindmillNamespacePrefix.SYSTEM_NAMESPACE_PREFIX.byteString());}
public static boolean beam_f9438_0(Windmill.Timer timer)
{    return timer.getTag().startsWith(WindmillNamespacePrefix.USER_NAMESPACE_PREFIX.byteString());}
 static Timer.Builder beam_f9439_0(@Nullable String stateFamily, WindmillNamespacePrefix prefix, TimerData timerData, Timer.Builder builder)
{    builder.setTag(timerTag(prefix, timerData)).setType(timerType(timerData.getDomain()));    if (stateFamily != null) {        builder.setStateFamily(stateFamily);    }    builder.setTimestamp(WindmillTimeUtils.harnessToWindmillTimestamp(timerData.getTimestamp()));    return builder;}
public static Instant beam_f9447_0(long timestampUs)
{        Preconditions.checkArgument(timestampUs != Long.MIN_VALUE);    Instant result = new Instant(divideAndRoundDown(timestampUs, 1000));    if (result.isAfter(BoundedWindow.TIMESTAMP_MAX_VALUE)) {                return BoundedWindow.TIMESTAMP_MAX_VALUE;    }    return result;}
public static long beam_f9448_0(Instant timestamp)
{    if (!timestamp.isBefore(BoundedWindow.TIMESTAMP_MAX_VALUE)) {                return Long.MAX_VALUE;    } else if (timestamp.getMillis() < Long.MIN_VALUE / 1000) {        return Long.MIN_VALUE + 1;    } else {        return timestamp.getMillis() * 1000;    }}
private static long beam_f9449_0(long number, long divisor)
{    return number / divisor - ((number < 0 && number % divisor != 0) ? 1 : 0);}
public boolean beam_f9457_0() throws IOException
{    current = value;    return true;}
public boolean beam_f9458_0() throws IOException
{    current = null;    return false;}
public WindowedValue<KeyedWorkItem<K, T>> beam_f9459_0()
{    if (current == null) {        throw new NoSuchElementException();    }    return value;}
public static DynamicSourceSplit beam_f9467_0(BoundedSourceSplit<?> sourceSplitResult)
{    DynamicSourceSplit sourceSplit = new DynamicSourceSplit();    com.google.api.services.dataflow.model.Source primarySource;    com.google.api.services.dataflow.model.Source residualSource;    try {        primarySource = serializeSplitToCloudSource(sourceSplitResult.primary);        residualSource = serializeSplitToCloudSource(sourceSplitResult.residual);    } catch (Exception e) {        throw new RuntimeException("Failed to serialize one of the parts of the source split", e);    }    sourceSplit.setPrimary(new DerivedSource().setDerivationMode("SOURCE_DERIVATION_MODE_INDEPENDENT").setSource(primarySource));    sourceSplit.setResidual(new DerivedSource().setDerivationMode("SOURCE_DERIVATION_MODE_INDEPENDENT").setSource(residualSource));    return sourceSplit;}
private static com.google.api.services.dataflow.model.Source beam_f9468_1(BoundedSource<?> source) throws Exception
{    com.google.api.services.dataflow.model.Source cloudSource = new com.google.api.services.dataflow.model.Source();    cloudSource.setSpec(CloudObject.forClass(CustomSources.class));    addString(cloudSource.getSpec(), SERIALIZED_SOURCE, encodeBase64String(serializeToByteArray(source)));    SourceMetadata metadata = new SourceMetadata();        try {        metadata.setEstimatedSizeBytes(source.getEstimatedSizeBytes(PipelineOptionsFactory.create()));    } catch (Exception e) {            }    cloudSource.setMetadata(metadata);    return cloudSource;}
public static SourceOperationResponse beam_f9469_0(SourceSplitRequest request, PipelineOptions options) throws Exception
{    return performSplitWithApiLimit(request, options, DEFAULT_NUM_BUNDLES_LIMIT, DATAFLOW_SPLIT_RESPONSE_API_SIZE_LIMIT);}
public NativeReaderIterator<WindowedValue<Object>> beam_f9477_0() throws IOException
{    return new BoundedReaderIterator<>(((BoundedSource<Object>) source).createReader(options));}
public static boolean beam_f9478_0(ByteString splitKey)
{    return splitKey.equals(firstSplitKey);}
public NativeReaderIterator<WindowedValue<ValueWithRecordId<T>>> beam_f9479_0() throws IOException
{    UnboundedSource.UnboundedReader<T> reader = (UnboundedSource.UnboundedReader<T>) context.getCachedReader();    final boolean started = reader != null;    if (reader == null) {        String key = context.getSerializedKey().toStringUtf8();                int splitIndex = Integer.parseInt(key.substring(0, 16), 16) - 1;        UnboundedSource<T, UnboundedSource.CheckpointMark> splitSource = parseSource(splitIndex);        UnboundedSource.CheckpointMark checkpoint = null;        if (splitSource.getCheckpointMarkCoder() != null) {            checkpoint = context.getReaderCheckpoint(splitSource.getCheckpointMarkCoder());        }        reader = splitSource.createReader(options, checkpoint);    }    context.setActiveReader(reader);    return new UnboundedReaderIterator<>(reader, context, started);}
private static List<BoundedSource<T>> beam_f9487_0(List<BoundedSource<T>> bundles, int maxBundles)
{    List<BoundedSource<T>> splittableBoundedSources = new ArrayList<>();                        int numElementsToPutIntoBundle = maxBundles;    while (bundles.size() > (long) numElementsToPutIntoBundle * maxBundles) {        numElementsToPutIntoBundle *= maxBundles;    }        int startIndex = 0;            int remainingCapacity = numElementsToPutIntoBundle / maxBundles * (maxBundles - splittableBoundedSources.size() - 1);    for (; startIndex < bundles.size() - numElementsToPutIntoBundle - remainingCapacity; startIndex += numElementsToPutIntoBundle) {        splittableBoundedSources.add(new SplittableOnlyBoundedSource<>(bundles.subList(startIndex, startIndex + numElementsToPutIntoBundle)));        remainingCapacity = numElementsToPutIntoBundle / maxBundles * (maxBundles - splittableBoundedSources.size() - 1);    }            splittableBoundedSources.add(new SplittableOnlyBoundedSource<>(bundles.subList(startIndex, bundles.size() - remainingCapacity)));    startIndex = bundles.size() - remainingCapacity;        numElementsToPutIntoBundle /= maxBundles;    for (; startIndex < bundles.size(); startIndex += numElementsToPutIntoBundle) {        if (numElementsToPutIntoBundle == 1) {            splittableBoundedSources.add(bundles.get(startIndex));        } else {            splittableBoundedSources.add(new SplittableOnlyBoundedSource<>(bundles.subList(startIndex, startIndex + numElementsToPutIntoBundle)));        }    }    return splittableBoundedSources;}
 static Source<?> beam_f9488_1(Map<String, Object> spec) throws Exception
{    Source<?> source = (Source<?>) deserializeFromByteArray(Base64.decodeBase64(getString(spec, SERIALIZED_SOURCE)), "Source");    try {        source.validate();    } catch (Exception e) {                throw e;    }    return source;}
public boolean beam_f9489_0() throws IOException
{    try {        return reader.start();    } catch (Exception e) {        throw new IOException("Failed to start reading from source: " + reader.getCurrentSource(), e);    }}
public double beam_f9497_0()
{    return Double.NaN;}
public boolean beam_f9498_0() throws IOException
{    if (started) {                return advance();    }    try {        if (!reader.start()) {            return false;        }    } catch (Exception e) {        throw new IOException("Failed to start reading from source: " + reader.getCurrentSource(), e);    }    elemsRead++;    return true;}
public boolean beam_f9499_0() throws IOException
{    if (elemsRead >= maxUnboundedBundleSize || Instant.now().isAfter(endTime) || context.isSinkFullHintSet()) {        return false;    }    BackOff backoff = BACKOFF_FACTORY.backoff();    while (true) {        try {            if (reader.advance()) {                elemsRead++;                return true;            }        } catch (Exception e) {            throw new IOException("Failed to advance source: " + reader.getCurrentSource(), e);        }        long nextBackoff = backoff.nextBackOffMillis();        if (nextBackoff == BackOff.STOP) {            return false;        }        Uninterruptibles.sleepUninterruptibly(nextBackoff, TimeUnit.MILLISECONDS);    }}
public synchronized void beam_f9508_0(DataflowWorkExecutor worker, BatchModeExecutionContext executionContext)
{    checkArgument(worker != null, "worker must be non-null");    checkState(this.worker == null, "Can only call setWorker once");    this.worker = worker;    this.executionContext = executionContext;}
public synchronized WorkItemServiceState beam_f9509_1(Throwable e) throws IOException
{    checkState(!finalStateSent, "cannot reportUpdates after sending a final state");    WorkItemStatus status = createStatusUpdate(true);            Throwable t = e instanceof UserCodeException ? e.getCause() : e;    Status error = new Status();        error.setCode(2);        String logPrefix = String.format("Failure processing work item %s", uniqueWorkId());    if (isOutOfMemoryError(t)) {        String message = "An OutOfMemoryException occurred. Consider specifying higher memory " + "instances in PipelineOptions.\n";                error.setMessage(message + DataflowWorkerLoggingHandler.formatException(t));    } else {                error.setMessage(DataflowWorkerLoggingHandler.formatException(t));    }    status.setErrors(ImmutableList.of(error));    return execute(status);}
public synchronized WorkItemServiceState beam_f9510_1() throws IOException
{    checkState(!finalStateSent, "cannot reportSuccess after sending a final state");    checkState(worker != null, "setWorker should be called before reportSuccess");    WorkItemStatus status = createStatusUpdate(true);    if (worker instanceof SourceOperationExecutor) {                        SourceOperationResponse response = ((SourceOperationExecutor) worker).getResponse();        if (response != null) {            status.setSourceOperationResponse(response);        }    }        return execute(status);}
 synchronized void beam_f9518_0(WorkItemStatus status)
{    if (worker == null) {        return;    }        boolean isFinalUpdate = Boolean.TRUE.equals(status.getCompleted());    Map<Object, CounterUpdate> counterUpdatesMap = new HashMap<>();    final Consumer<CounterUpdate> appendCounterUpdate = x -> counterUpdatesMap.put(x.getStructuredNameAndMetadata() == null ? x.getNameAndKind() : x.getStructuredNameAndMetadata(), x);        extractCounters(worker.getOutputCounters()).forEach(appendCounterUpdate);        extractMetrics(isFinalUpdate).forEach(appendCounterUpdate);        extractMsecCounters(isFinalUpdate).forEach(appendCounterUpdate);                    worker.extractMetricUpdates().forEach(appendCounterUpdate);    status.setCounterUpdates(ImmutableList.copyOf(counterUpdatesMap.values()));}
private synchronized Iterable<CounterUpdate> beam_f9519_0(@Nullable CounterSet counters)
{    if (counters == null) {        return Collections.emptyList();    }                            boolean delta = false;    return counters.extractUpdates(delta, DataflowCounterUpdateExtractor.INSTANCE);}
private synchronized Iterable<CounterUpdate> beam_f9520_0(boolean isFinalUpdate)
{    return executionContext == null ? Collections.emptyList() : executionContext.extractMetricUpdates(isFinalUpdate);}
public void beam_f9528_0() throws Throwable
{    ExecutorService service = Executors.newSingleThreadExecutor();    final TestExecutorService testService = TestExecutors.from(() -> service);    final AtomicBoolean taskRan = new AtomicBoolean();    testService.apply(new Statement() {        @Override        public void evaluate() throws Throwable {            testService.submit(() -> taskRan.set(true));        }    }, null).evaluate();    assertTrue(service.isTerminated());    assertTrue(taskRan.get());}
public void beam_f9529_0() throws Throwable
{    testService.submit(() -> taskRan.set(true));}
public void beam_f9530_0() throws Throwable
{    ExecutorService service = Executors.newSingleThreadExecutor();    final TestExecutorService testService = TestExecutors.from(() -> service);    final AtomicBoolean taskStarted = new AtomicBoolean();    final AtomicBoolean taskWasInterrupted = new AtomicBoolean();    try {        testService.apply(new Statement() {            @Override            public void evaluate() throws Throwable {                testService.submit(this::taskToRun);            }            private void taskToRun() {                taskStarted.set(true);                try {                    while (true) {                        Thread.sleep(10000);                    }                } catch (InterruptedException e) {                    taskWasInterrupted.set(true);                    return;                }            }        }, null).evaluate();        fail();    } catch (IllegalStateException e) {        assertEquals(IllegalStateException.class, e.getClass());        assertEquals("Test executor failed to shutdown cleanly.", e.getMessage());    }    assertTrue(service.isShutdown());}
public static Builder<T> beam_f9538_0(Consumer<T> onNext)
{    return new Builder<>(new ForwardingCallStreamObserver<>(onNext, TestStreams::noop, TestStreams::noop, TestStreams::returnTrue));}
public Builder<T> beam_f9539_0(Supplier<Boolean> isReady)
{    return new Builder<>(new ForwardingCallStreamObserver<>(observer.onNext, observer.onError, observer.onCompleted, isReady));}
public Builder<T> beam_f9540_0(Runnable onCompleted)
{    return new Builder<>(new ForwardingCallStreamObserver<>(observer.onNext, observer.onError, onCompleted, observer.isReady));}
public boolean beam_f9550_0()
{    return isReady.get();}
public void beam_f9555_0()
{    AtomicBoolean onNextWasCalled = new AtomicBoolean();    TestStreams.withOnNext(onNextWasCalled::set).build().onNext(true);    assertTrue(onNextWasCalled.get());}
public void beam_f9556_0()
{    final AtomicBoolean isReadyWasCalled = new AtomicBoolean();    assertFalse(TestStreams.withOnNext(null).withIsReady(() -> isReadyWasCalled.getAndSet(true)).build().isReady());    assertTrue(isReadyWasCalled.get());}
public void beam_f9564_0()
{    FixMultiOutputInfosOnParDoInstructions function = new FixMultiOutputInfosOnParDoInstructions(IdGenerators.decrementingLongs());    MapTask output = function.apply(createMapTaskWithParDo(1));    assertEquals(createMapTaskWithParDo(1, "-1"), output);    output = function.apply(createMapTaskWithParDo(1));    assertEquals(createMapTaskWithParDo(1, "-2"), output);}
public void beam_f9565_0()
{    FixMultiOutputInfosOnParDoInstructions function = new FixMultiOutputInfosOnParDoInstructions(IdGenerators.decrementingLongs());    thrown.expect(IllegalArgumentException.class);    thrown.expectMessage("Invalid ParDoInstruction");    thrown.expectMessage("2 outputs specified");    function.apply(createMapTaskWithParDo(2));}
private static MapTask beam_f9566_0(int numOutputs, String... tags)
{    ParDoInstruction parDoInstruction = new ParDoInstruction();    parDoInstruction.setNumOutputs(numOutputs);    List<MultiOutputInfo> multiOutputInfos = new ArrayList<>(tags.length);    for (String tag : tags) {        MultiOutputInfo multiOutputInfo = new MultiOutputInfo();        multiOutputInfo.setTag(tag);        multiOutputInfos.add(multiOutputInfo);    }    parDoInstruction.setMultiOutputInfos(multiOutputInfos);    ParallelInstruction instruction = new ParallelInstruction();    instruction.setParDo(parDoInstruction);    MapTask mapTask = new MapTask();    mapTask.setInstructions(ImmutableList.of(instruction));    return mapTask;}
public void beam_f9574_0() throws Exception
{    String[] names = { "sum_counter" };    String[] kinds = { "sum", "max" };    long[] deltas = { 122 };    try {        counters.importCounters(names, kinds, deltas);    } catch (AssertionError e) {        }}
public void beam_f9575_0() throws Exception
{    String[] names = { "sum_counter" };    String[] kinds = { "sum_int" };    long[] deltas = { 122 };    thrown.expect(IllegalArgumentException.class);    thrown.expectMessage("sum_int");    counters.importCounters(names, kinds, deltas);}
 NativeReader<?> beam_f9576_0(String filename, @Nullable Long start, @Nullable Long end, CloudObject encoding) throws Exception
{    CloudObject spec = CloudObject.forClassName("AvroSource");    addString(spec, "filename", filename);    if (start != null) {        addLong(spec, "start_offset", start);    }    if (end != null) {        addLong(spec, "end_offset", end);    }    Source cloudSource = new Source();    cloudSource.setSpec(spec);    cloudSource.setCodec(encoding);    NativeReader<?> reader = ReaderRegistry.defaultRegistry().create(cloudSource, PipelineOptionsFactory.create(),     null, null);    return reader;}
private List<List<String>> beam_f9584_0(int numBlocks, int blockSizeBytes, int averageLineSizeBytes)
{    Random random = new Random(0);    List<List<String>> blocks = new ArrayList<>(numBlocks);    for (int blockNum = 0; blockNum < numBlocks; blockNum++) {        int numLines = blockSizeBytes / averageLineSizeBytes;        List<String> lines = new ArrayList<>(numLines);        for (int lineNum = 0; lineNum < numLines; lineNum++) {            int numChars = random.nextInt(averageLineSizeBytes * 2);            StringBuilder sb = new StringBuilder();            for (int charNum = 0; charNum < numChars; charNum++) {                sb.appendCodePoint(random.nextInt('z' - 'a' + 1) + 'a');            }            lines.add(sb.toString());        }        blocks.add(lines);    }    return blocks;}
public void beam_f9585_0() throws Exception
{    runTestRead(generateInputBlocks(3, 50, 5), StringUtf8Coder.of(), true);}
public void beam_f9586_0() throws Exception
{    runTestRead(generateInputBlocks(10, 128 * 1024, 100), StringUtf8Coder.of(), false);}
public void beam_f9594_0() throws Exception
{    runTestWriteFile(TestUtils.INTS, BigEndianIntegerCoder.of());}
public void beam_f9595_0() throws Exception
{    runTestWriteFile(TestUtils.NO_INTS, BigEndianIntegerCoder.of());}
public void beam_f9596_0()
{    MockitoAnnotations.initMocks(this);    options = PipelineOptionsFactory.as(DataflowWorkerHarnessOptions.class);}
public void beam_f9605_0()
{    BatchModeExecutionContext executionContext = BatchModeExecutionContext.forTesting(PipelineOptionsFactory.create(), "testStage");    DataflowOperationContext operationContext = executionContext.createOperationContext(NameContextsForTests.nameContextForTest());    Distribution distribution = operationContext.metricsContainer().getDistribution(MetricName.named("namespace", "some-distribution"));    distribution.update(2);    distribution.update(8);    final CounterUpdate expected = new CounterUpdate().setStructuredNameAndMetadata(new CounterStructuredNameAndMetadata().setName(new CounterStructuredName().setOrigin("USER").setOriginNamespace("namespace").setName("some-distribution").setOriginalStepName("originalName")).setMetadata(new CounterMetadata().setKind("DISTRIBUTION"))).setCumulative(true).setDistribution(new DistributionUpdate().setCount(longToSplitInt(2)).setMax(longToSplitInt(8)).setMin(longToSplitInt(2)).setSum(longToSplitInt(10)));    assertThat(executionContext.extractMetricUpdates(false), containsInAnyOrder(expected));}
public void beam_f9606_0()
{    BatchModeExecutionContext executionContext = BatchModeExecutionContext.forTesting(PipelineOptionsFactory.create(), "testStage");    MetricsContainer metricsContainer = Mockito.mock(MetricsContainer.class);    ProfileScope otherScope = Mockito.mock(ProfileScope.class);    ProfileScope profileScope = Mockito.mock(ProfileScope.class);    ExecutionState start1 = executionContext.executionStateRegistry.getState(NameContext.create("stage", "original-1", "system-1", "user-1"), ExecutionStateTracker.START_STATE_NAME, metricsContainer, profileScope);    ExecutionState process1 = executionContext.executionStateRegistry.getState(NameContext.create("stage", "original-1", "system-1", "user-1"), ExecutionStateTracker.PROCESS_STATE_NAME, metricsContainer, profileScope);    ExecutionState start2 = executionContext.executionStateRegistry.getState(NameContext.create("stage", "original-2", "system-2", "user-2"), ExecutionStateTracker.START_STATE_NAME, metricsContainer, profileScope);    ExecutionState other = executionContext.executionStateRegistry.getState(NameContext.forStage("stage"), "other", null, NoopProfileScope.NOOP);    other.takeSample(120);    start1.takeSample(100);    process1.takeSample(500);    assertThat(executionContext.extractMsecCounters(false), containsInAnyOrder(msecStage("other-msecs", "stage", 120), msec("start-msecs", "stage", "original-1", 100), msec("process-msecs", "stage", "original-1", 500)));    process1.takeSample(200);    start2.takeSample(200);    assertThat(executionContext.extractMsecCounters(false), containsInAnyOrder(msec("process-msecs", "stage", "original-1", 500 + 200), msec("start-msecs", "stage", "original-2", 200)));    process1.takeSample(300);    assertThat(executionContext.extractMsecCounters(true), hasItems(msecStage("other-msecs", "stage", 120), msec("start-msecs", "stage", "original-1", 100), msec("process-msecs", "stage", "original-1", 500 + 200 + 300), msec("start-msecs", "stage", "original-2", 200)));}
public void beam_f9607_0()
{    BatchModeExecutionContext executionContext = BatchModeExecutionContext.forTesting(PipelineOptionsFactory.create(), "testStage");    DataflowOperationContext operationContext = executionContext.createOperationContext(NameContextsForTests.nameContextForTest());    Counter counter = operationContext.metricsContainer().getCounter(MetricName.named(BatchModeExecutionContext.DATASTORE_THROTTLE_TIME_NAMESPACE, "cumulativeThrottlingSeconds"));    counter.inc(12);    counter.inc(17);    counter.inc(1);    assertEquals(30L, (long) executionContext.extractThrottleTime());}
public String beam_f9615_0()
{    return String.format("%.1f", count == 0 ? 0.0 : sum / count);}
public int beam_f9616_0()
{    return Objects.hash(count, sum);}
public boolean beam_f9617_0(Object obj)
{    if (obj == this) {        return true;    }    if (!(obj instanceof CountSum)) {        return false;    }    CountSum other = (CountSum) obj;    return (this.count == other.count) && (Math.abs(this.sum - other.sum) < 0.1);}
public void beam_f9626_0() throws Exception
{    TestReceiver receiver = new TestReceiver();    Combine.CombineFn<Integer, CountSum, String> combiner = (new MeanInts());    ParDoFn combineParDoFn = createCombineValuesFn(CombinePhase.ALL, combiner, StringUtf8Coder.of(), BigEndianIntegerCoder.of(), new CountSumCoder(), WindowingStrategy.globalDefault());    combineParDoFn.startBundle(receiver);    combineParDoFn.processElement(WindowedValue.valueInGlobalWindow(KV.of("a", Arrays.asList(5, 6, 7))));    combineParDoFn.processElement(WindowedValue.valueInGlobalWindow(KV.of("b", Arrays.asList(1, 3, 7))));    combineParDoFn.processElement(WindowedValue.valueInGlobalWindow(KV.of("c", Arrays.asList(3, 6, 8, 9))));    combineParDoFn.finishBundle();    Object[] expectedReceivedElems = { WindowedValue.valueInGlobalWindow(KV.of("a", String.format("%.1f", 6.0))), WindowedValue.valueInGlobalWindow(KV.of("b", String.format("%.1f", 3.7))), WindowedValue.valueInGlobalWindow(KV.of("c", String.format("%.1f", 6.5))) };    assertArrayEquals(expectedReceivedElems, receiver.receivedElems.toArray());}
public void beam_f9627_0() throws Exception
{    TestReceiver receiver = new TestReceiver();    MeanInts mean = new MeanInts();    Combine.CombineFn<Integer, CountSum, String> combiner = mean;    ParDoFn combineParDoFn = createCombineValuesFn(CombinePhase.ADD, combiner, StringUtf8Coder.of(), BigEndianIntegerCoder.of(), new CountSumCoder(), WindowingStrategy.globalDefault());    combineParDoFn.startBundle(receiver);    combineParDoFn.processElement(WindowedValue.valueInGlobalWindow(KV.of("a", Arrays.asList(5, 6, 7))));    combineParDoFn.processElement(WindowedValue.valueInGlobalWindow(KV.of("b", Arrays.asList(1, 3, 7))));    combineParDoFn.processElement(WindowedValue.valueInGlobalWindow(KV.of("c", Arrays.asList(3, 6, 8, 9))));    combineParDoFn.finishBundle();    Object[] expectedReceivedElems = { WindowedValue.valueInGlobalWindow(KV.of("a", new CountSum(3, 18))), WindowedValue.valueInGlobalWindow(KV.of("b", new CountSum(3, 11))), WindowedValue.valueInGlobalWindow(KV.of("c", new CountSum(4, 26))) };    assertArrayEquals(expectedReceivedElems, receiver.receivedElems.toArray());}
public void beam_f9628_0() throws Exception
{    TestReceiver receiver = new TestReceiver();    MeanInts mean = new MeanInts();    Combine.CombineFn<Integer, CountSum, String> combiner = mean;    ParDoFn combineParDoFn = createCombineValuesFn(CombinePhase.MERGE, combiner, StringUtf8Coder.of(), BigEndianIntegerCoder.of(), new CountSumCoder(), WindowingStrategy.globalDefault());    combineParDoFn.startBundle(receiver);    combineParDoFn.processElement(WindowedValue.valueInGlobalWindow(KV.of("a", Arrays.asList(new CountSum(3, 6), new CountSum(2, 9), new CountSum(1, 12)))));    combineParDoFn.processElement(WindowedValue.valueInGlobalWindow(KV.of("b", Arrays.asList(new CountSum(2, 20), new CountSum(1, 1)))));    combineParDoFn.finishBundle();    Object[] expectedReceivedElems = { WindowedValue.valueInGlobalWindow(KV.of("a", new CountSum(6, 27))), WindowedValue.valueInGlobalWindow(KV.of("b", new CountSum(3, 21))) };    assertArrayEquals(expectedReceivedElems, receiver.receivedElems.toArray());}
public boolean beam_f9636_0()
{    if (lastIterator != null) {        return lastIterator.isClosed;    }        return true;}
public NativeReaderIterator<T> beam_f9637_0() throws IOException
{    lastIterator = new TestIterator(readerDelegator.iterator());    return lastIterator;}
public boolean beam_f9638_0() throws IOException
{    currentIndex++;    if (currentIndex == recordToFailAt) {        throw new IOException("Failing at record " + currentIndex);    }    return iteratorImpl.start();}
private TestReader<String> beam_f9646_0(long recordsPerReader, long recordToFailAt, boolean failWhenClosing, List<String> expectedData) throws Exception
{    List<String> records = new ArrayList<>();    for (int i = 0; i < recordsPerReader; i++) {        String record = "Record" + i;        records.add(record);        if (recordToFailAt < 0 || i < recordToFailAt) {            expectedData.add(record);        }    }    return new TestReader<String>(records, StringUtf8Coder.of(), recordToFailAt, failWhenClosing);}
private static void beam_f9647_0(List<TestReader<?>> readers)
{    for (TestReader<?> reader : readers) {        if (!reader.isClosedOrUnopened()) {            throw new AssertionError("At least one reader was not closed");        }    }}
private Source beam_f9648_0(TestReader<String> testReader)
{    Source source = new Source();    CloudObject specObj = CloudObject.forClass(TestReader.class);    specObj.put(READER_OBJECT, testReader);    source.setSpec(specObj);    return source;}
public void beam_f9656_0() throws Exception
{    testReadersOfSizes(10, 5, 20, 0);}
public void beam_f9657_0() throws Exception
{    testReadersOfSizes(10, 0, 20, 30);}
public void beam_f9658_0() throws Exception
{    testReadersOfSizes(10, 0, 20, 0, 30, 0, 40);}
public void beam_f9666_0() throws Exception
{    runUpdateStopPositionTest(10);}
public void beam_f9667_0() throws Exception
{    runUpdateStopPositionTest(10, 10, 10);}
public void beam_f9668_0() throws Exception
{    runUpdateStopPositionTest(10, 30, 20, 15, 7);}
public void beam_f9677_0()
{    Counter<?, ?> c1 = counterSet.longSum(name1);    Counter<?, ?> c2 = counterSet.intMax(name2);    assertThat(counterSet.size(), equalTo(2L));}
public void beam_f9678_0()
{    CounterName cn1 = CounterName.named("myCounter1, origin=");    CounterName cn2 = CounterName.named("myCounter1").withOrigin(", origin=");    assertThat(cn1.getFlatName(), equalTo(cn1.getFlatName()));    Counter<?, ?> c1 = counterSet.longSum(cn1);    assertThat(c1, not(equalTo(counterSet.getExistingCounter(cn2))));}
public void beam_f9679_0()
{    CounterName cn1 = CounterName.named("myCounter1");    CounterName cn2 = CounterName.named("myCounter1");    Counter<?, ?> c1 = counterSet.longSum(cn1);    assertThat(c1, equalTo(counterSet.getExistingCounter(cn2)));}
public void beam_f9687_0()
{    Counter<Double, Double> c = counters.doubleMax(name);    assertEquals(Double.NEGATIVE_INFINITY, c.getAggregate(), EPSILON);    c.addValue(Math.E).addValue(Math.PI).addValue(Double.NEGATIVE_INFINITY);    assertEquals(Math.PI, c.getAggregate(), EPSILON);    c.getAndReset();    c.addValue(Math.sqrt(12345)).addValue(2 * Math.PI).addValue(3 * Math.E);    assertEquals(Math.sqrt(12345), c.getAggregate(), EPSILON);    assertEquals("getAndReset should return previous value", Math.sqrt(12345), c.getAndReset(), EPSILON);    assertEquals(Double.NEGATIVE_INFINITY, c.getAggregate(), EPSILON);    c.addValue(7 * Math.PI).addValue(5 * Math.E);    assertEquals(7 * Math.PI, c.getAggregate(), EPSILON);    c.getAndReset();    c.addValue(Math.sqrt(17)).addValue(171.0).addValue(49.0);    assertEquals(171.0, c.getAggregate(), EPSILON);}
public void beam_f9688_0()
{    Counter<Long, Long> c = counters.longMin(name);    assertEquals(Long.MAX_VALUE, (long) c.getAggregate());    c.addValue(13L).addValue(42L).addValue(Long.MAX_VALUE);    assertEquals(13L, (long) c.getAggregate());    c.getAndReset();    c.addValue(120L).addValue(17L).addValue(37L);    assertEquals(17L, (long) c.getAggregate());    assertEquals("getAndReset should return previous value", 17L, (long) c.getAndReset());    assertEquals("getAndReset should have reset the value", Long.MAX_VALUE, (long) c.getAggregate());    c.addValue(42L).addValue(18L);    assertEquals(18L, (long) c.getAggregate());}
public void beam_f9689_0()
{    Counter<Double, Double> c = counters.doubleMin(name);    assertEquals(Double.POSITIVE_INFINITY, c.getAggregate(), EPSILON);    c.addValue(Math.E).addValue(Math.PI).addValue(Double.POSITIVE_INFINITY);    assertEquals(Math.E, c.getAggregate(), EPSILON);    c.getAndReset();    c.addValue(Math.sqrt(12345)).addValue(2 * Math.PI).addValue(3 * Math.E);    assertEquals(2 * Math.PI, c.getAggregate(), EPSILON);    assertEquals("getAndReset should return previous value", 2 * Math.PI, c.getAndReset(), EPSILON);    assertEquals("getAndReset should have reset the value", Double.POSITIVE_INFINITY, c.getAggregate(), EPSILON);    c.getAndReset();    c.addValue(Math.sqrt(17)).addValue(171.0).addValue(0.0);    assertEquals(0.0, c.getAggregate(), EPSILON);}
public void beam_f9697_0()
{    verifyDirtyBit(counters.longSum(CounterName.named("long-sum")), 1L);    verifyDirtyBit(counters.longMean(CounterName.named("long-mean")), 1L);    verifyDirtyBit(counters.doubleSum(CounterName.named("double-sum")), 1.0);    verifyDirtyBit(counters.doubleMean(CounterName.named("double-mean")), 1.0);    verifyDirtyBit(counters.intSum(CounterName.named("int-sum")), 1);    verifyDirtyBit(counters.intMean(CounterName.named("int-mean")), 1);    verifyDirtyBit(counters.booleanAnd(CounterName.named("and")), true);}
private void beam_f9698_0(Counter<InputT, ?> counter, InputT sampleValue)
{    String name = String.format("counter '%s'", counter.getName().name());        assertFalse(String.format("%s should not be dirty on initialization.", name), counter.isDirty());    assertEquals(String.format("%s should not be COMMITTED on initialization.", name), CommitState.COMMITTED, counter.commitState.get());        counter.addValue(sampleValue);    assertTrue(String.format("%s should be dirty after mutating.", name), counter.isDirty());    assertEquals(String.format("%s should have DIRTY state after mutating.", name), CommitState.DIRTY, counter.commitState.get());        assertTrue(String.format("Committing %s should succeed when in DIRTY state.", name), counter.committing());    assertTrue(String.format("%s should be dirty after committing.", name), counter.isDirty());    assertEquals(String.format("%s should have COMMITTING state after mutating.", name), CommitState.COMMITTING, counter.commitState.get());        counter.addValue(sampleValue);    assertFalse(String.format("Marking %s committed should succeed after mutating.", name), counter.committed());    assertTrue(String.format("%s should be dirty after marking committed.", name), counter.isDirty());    assertEquals(String.format("%s state should be DIRTY after marking committed.", name), CommitState.DIRTY, counter.commitState.get());        assertTrue(String.format("Committing %s should succeed when in DIRTY state.", name), counter.committing());    assertTrue(String.format("Marking %s committed should succeed after committing.", name), counter.committed());    assertFalse(String.format("%s should be dirty after being marked committed.", name), counter.isDirty());    assertEquals(String.format("%s should have COMMITTED state after marking committed.", name), CommitState.COMMITTED, counter.commitState.get());}
public void beam_f9699_0()
{    Counter<?, ?> unstructured = counters.intSum(name);    Counter<?, ?> structuredOriginal = counters.intSum(name.withOriginalName(NameContextsForTests.nameContextForTest()));    Counter<?, ?> structuredSystem = counters.intSum(name.withSystemName(NameContextsForTests.nameContextForTest()));    Counter<?, ?> structuredCompatible = counters.intSum(name.withOriginalName(NameContextsForTests.nameContextForTest()));        assertFalse(unstructured.equals(structuredOriginal));    assertFalse(unstructured.equals(structuredSystem));    assertFalse(unstructured.equals(structuredCompatible));        assertEquals(structuredOriginal, structuredCompatible);    assertFalse(structuredOriginal.equals(structuredSystem));        assertFalse(structuredSystem.equals(structuredCompatible));}
private void beam_f9707_0(final WorkItemStatus status, Long... shortIds)
{    List<CounterUpdate> updates = status.getCounterUpdates();    assertTrue(updates.size() == shortIds.length);    for (int i = 0; i < updates.size(); i++) {        assertNull(status.getCounterUpdates().get(i).getNameAndKind());        assertTrue(status.getCounterUpdates().get(i).getShortId().longValue() == shortIds[i]);    }}
public void beam_f9708_0()
{    CounterShortIdCache shortIdCache = new CounterShortIdCache();    ReportWorkItemStatusRequest request = new ReportWorkItemStatusRequest();    ReportWorkItemStatusResponse reply = new ReportWorkItemStatusResponse();        request.setWorkItemStatuses(createWorkStatusNameAndKind(new String[] { "counter", "counter1" }, new String[] {}, new String[] { "counter2" }));    reply.setWorkItemServiceStates(createWorkServiceState(new Long[] { 1000L, 1001L }, new Long[] {}, new Long[] { 1002L }));        WorkItemStatus status1 = request.getWorkItemStatuses().get(0);    WorkItemStatus status2 = request.getWorkItemStatuses().get(1);    WorkItemStatus status3 = request.getWorkItemStatuses().get(2);    shortIdCache.shortenIdsIfAvailable(status1.getCounterUpdates());    for (CounterUpdate update : status1.getCounterUpdates()) {        assertNull(update.getShortId());    }        shortIdCache.storeNewShortIds(request, reply);    shortIdCache.shortenIdsIfAvailable(status1.getCounterUpdates());    shortIdCache.shortenIdsIfAvailable(status2.getCounterUpdates());    shortIdCache.shortenIdsIfAvailable(status3.getCounterUpdates());    checkStatusAndShortIds(status1, 1000L, 1001L);    checkStatusAndShortIds(status2);    checkStatusAndShortIds(status3, 1002L);}
public void beam_f9709_0()
{    CounterShortIdCache shortIdCache = new CounterShortIdCache();    shortIdCache.shortenIdsIfAvailable(null);}
public void beam_f9717_0(FakeWorker worker) throws Exception
{    final int numWorkers = Math.max(Runtime.getRuntime().availableProcessors(), 1);    final AtomicInteger sleepCount = new AtomicInteger(0);    final AtomicInteger illegalIntervalCount = new AtomicInteger(0);    DataflowBatchWorkerHarness.processWork(pipelineOptions, worker, millis -> {        if ((millis > DataflowBatchWorkerHarness.BACKOFF_MAX_INTERVAL_MILLIS * 1.5)) {                                    illegalIntervalCount.incrementAndGet();        }        if (sleepCount.incrementAndGet() > 1000) {            throw new InterruptedException("Stopping the retry loop.");        }    });        assertEquals(numWorkers + 1000, worker.getNumberOfCallsToGetAndPerformWork());    assertEquals(0, illegalIntervalCount.get());}
public void beam_f9718_0() throws Exception
{    FakeWorker fakeWorker = new FakeWorker(pipelineOptions, false);    runTestThatWeRetryIfTaskExecutionFailsAgainAndAgain(fakeWorker);}
public void beam_f9719_0() throws Exception
{    FakeWorker fakeWorker = new FakeWorker(pipelineOptions, new IOException());    runTestThatWeRetryIfTaskExecutionFailsAgainAndAgain(fakeWorker);}
public void beam_f9727_0() throws IOException
{    List<String> experiments = options.getExperiments();    experiments.remove(DataflowElementExecutionTracker.TIME_PER_ELEMENT_EXPERIMENT);    options.setExperiments(experiments);    tracker = DataflowElementExecutionTracker.create(counters, options);    NameContext step = createStep("A");    tracker.enter(step);    tracker.exit();    tracker.takeSample(10);    assertThat(getCounter(step), nullValue());}
public void beam_f9728_0() throws IOException
{    NameContext stepA = createStep("A");    NameContext stepB = createStep("B");    tracker.enter(stepA);    tracker.exit();    tracker.enter(stepB);    tracker.exit();        tracker.takeSample(50);    assertThat(getCounterValue(stepA), equalTo(distribution(10)));    assertThat(getCounterValue(stepB), equalTo(distribution(10)));}
public void beam_f9729_0() throws IOException
{    NameContext stepA = createStep("A");    NameContext stepB = createStep("B");    tracker.enter(stepA);    tracker.enter(stepB);    tracker.exit();    tracker.enter(stepB);    tracker.exit();    tracker.exit();        tracker.takeSample(70);    assertThat(getCounterValue(stepA), equalTo(distribution(30)));    assertThat(getCounterValue(stepB), equalTo(distribution(10, 10)));}
public void beam_f9737_0() throws IOException
{    AutoRegistrationClass.WAS_CALLED = true;}
public Closeable beam_f9738_0(ExecutionStateTracker e)
{    return this;}
public void beam_f9739_0() throws IOException
{    AutoRegistrationClassNotActive.WAS_CALLED = true;}
public void beam_f9747_0() throws IOException
{    enableTimePerElementExperiment();    ExecutionStateTracker tracker = createTracker();    try (Closeable c1 = tracker.activate(new Thread())) {        try (Closeable c2 = tracker.enterState(step1Process)) {        }        sampler.doSampling(30);        }    assertElementProcessingTimeCounter(step1, 10, 4);}
public void beam_f9748_0() throws IOException
{    enableTimePerElementExperiment();    ExecutionStateTracker tracker = createTracker();    try (Closeable c1 = tracker.activate(new Thread())) {        try (Closeable c2 = tracker.enterState(step1Process)) {            sampler.doSampling(100);            assertThat(step1Process.getTotalMillis(), equalTo(100L));        }    }    sampler.doSampling(100);    assertThat(step1Process.getTotalMillis(), equalTo(100L));}
private void beam_f9749_0()
{    options.as(DataflowPipelineDebugOptions.class).setExperiments(Lists.newArrayList(DataflowElementExecutionTracker.TIME_PER_ELEMENT_EXPERIMENT));}
public void beam_f9757_0()
{    operationContext.enterStart();    assertThat(stateTracker.getCurrentState(), sameInstance(startState));    verify(profileScope).activate();}
public void beam_f9758_0() throws IOException
{    operationContext.enterStart().close();    assertThat(stateTracker.getCurrentState(), sameInstance(otherState));    verify(profileScope).activate();    verify(emptyScope).activate();}
public void beam_f9759_0()
{    operationContext.enterStart();    operationContext.enterProcess();    operationContext.enterFinish();    operationContext.enterAbort();    assertThat(stateTracker.getCurrentState(), sameInstance(abortState));        verify(profileScope, times(4)).activate();}
public void beam_f9768_0()
{    assertEquals(CODER.getValueCoder(), DataflowPortabilityPCollectionView.with(TAG, CODER).getCoderInternal());}
public void beam_f9769_0()
{    assertEquals(CODER.getWindowCoder(), DataflowPortabilityPCollectionView.with(TAG, CODER).getWindowingStrategyInternal().getWindowFn().windowCoder());}
public void beam_f9770_0()
{    assertEquals(TAG, DataflowPortabilityPCollectionView.with(TAG, CODER).getTagInternal());}
public void beam_f9778_0() throws TextFormat.ParseException
{    Endpoints.ApiServiceDescriptor descriptor = Endpoints.ApiServiceDescriptor.newBuilder().setUrl("some_test_url").build();    Endpoints.ApiServiceDescriptor decoded = DataflowWorkerHarnessHelper.parseApiServiceDescriptorFromText(descriptor.toString());    assertThat(decoded, equalTo(descriptor));    assertThat(decoded.getUrl(), equalTo("some_test_url"));}
public void beam_f9779_0()
{    MockitoAnnotations.initMocks(this);    startTime = 0L;    clock = new FixedClock(startTime);    executor = new StubbedExecutor(clock);    WorkItem workItem = new WorkItem();    workItem.setProjectId(PROJECT_ID);    workItem.setJobId(JOB_ID);    workItem.setId(WORK_ID);    workItem.setLeaseExpireTime(toCloudTime(new Instant(clock.currentTimeMillis() + 1000)));    workItem.setReportStatusInterval(toCloudDuration(Duration.millis(300)));    workItem.setInitialReportIndex(1L);    progressUpdater = new DataflowWorkProgressUpdater(workItemStatusClient, workItem, worker, executor.getExecutor(), clock, hotKeyLogger) {                @Override        protected long getMinReportingInterval() {            return 100;        }        @Override        protected long getLeaseRenewalLatencyMargin() {            return 150;        }    };}
protected long beam_f9780_0()
{    return 100;}
public void beam_f9788_0() throws Exception
{    MockitoAnnotations.initMocks(this);    when(transport.buildRequest(anyString(), anyString())).thenReturn(request);    doCallRealMethod().when(request).getContentAsString();    Dataflow service = new Dataflow(transport, Transport.getJsonFactory(), null);    pipelineOptions = PipelineOptionsFactory.as(DataflowWorkerHarnessOptions.class);    pipelineOptions.setProject(PROJECT_ID);    pipelineOptions.setJobId(JOB_ID);    pipelineOptions.setWorkerId(WORKER_ID);    pipelineOptions.setGcpCredential(new TestCredential());    pipelineOptions.setDataflowClient(service);    pipelineOptions.setRegion("us-central1");}
public void beam_f9789_0() throws Exception
{    WorkItem workItem = createWorkItem(PROJECT_ID, JOB_ID);    when(request.execute()).thenReturn(generateMockResponse(workItem));    WorkUnitClient client = new DataflowWorkUnitClient(pipelineOptions, LOG);    assertEquals(Optional.of(workItem), client.getWorkItem());    LeaseWorkItemRequest actualRequest = Transport.getJsonFactory().fromString(request.getContentAsString(), LeaseWorkItemRequest.class);    assertEquals(WORKER_ID, actualRequest.getWorkerId());    assertEquals(ImmutableList.<String>of(WORKER_ID, "remote_source", "custom_source"), actualRequest.getWorkerCapabilities());    assertEquals(ImmutableList.<String>of("map_task", "seq_map_task", "remote_source_task"), actualRequest.getWorkItemTypes());    assertEquals("1234", DataflowWorkerLoggingMDC.getWorkId());}
public void beam_f9790_0() throws Exception
{    WorkUnitClient client = new DataflowWorkUnitClient(pipelineOptions, LOG);        final String stageName = "test_stage_name";    MapTask mapTask = new MapTask();    mapTask.setStageName(stageName);    WorkItem workItem = createWorkItem(PROJECT_ID, JOB_ID);    workItem.setMapTask(mapTask);    when(request.execute()).thenReturn(generateMockResponse(workItem));    assertEquals(Optional.of(workItem), client.getWorkItem());    assertEquals(stageName, DataflowWorkerLoggingMDC.getStageName());}
public void beam_f9799_0() throws Exception
{        String stringFieldValue = "some state";    long longFieldValue = 42L;    TestDoFn fn = new TestDoFn(stringFieldValue, longFieldValue);    String serializedFn = StringUtils.byteArrayToJsonString(SerializableUtils.serializeToByteArray(DoFnInfo.forFn(fn, WindowingStrategy.globalDefault(), null, /* side input views */    null, /* input coder */    new TupleTag<>("output"), /* main output */    DoFnSchemaInformation.create(), Collections.emptyMap())));    CloudObject cloudUserFn = CloudObject.forClassName("DoFn");    addString(cloudUserFn, "serialized_fn", serializedFn);        ParDoFn parDoFn = DEFAULT_FACTORY.create(DEFAULT_OPTIONS, cloudUserFn, null, MAIN_OUTPUT, ImmutableMap.<TupleTag<?>, Integer>of(MAIN_OUTPUT, 0), DEFAULT_EXECUTION_CONTEXT, TestOperationContext.create(counterSet));        assertThat(parDoFn, instanceOf(SimpleParDoFn.class));                SimpleParDoFn simpleParDoFn = (SimpleParDoFn) parDoFn;    parDoFn.startBundle(new OutputReceiver());        parDoFn.processElement(WindowedValue.valueInGlobalWindow("foo"));    @SuppressWarnings("rawtypes")    DoFnInfo doFnInfo = simpleParDoFn.getDoFnInfo();    DoFn innerDoFn = (TestDoFn) doFnInfo.getDoFn();    assertThat(innerDoFn, instanceOf(TestDoFn.class));    assertThat(doFnInfo.getWindowingStrategy().getWindowFn(), instanceOf(GlobalWindows.class));    assertThat(doFnInfo.getWindowingStrategy().getTrigger(), instanceOf(DefaultTrigger.class));        TestDoFn actualTestDoFn = (TestDoFn) innerDoFn;    assertEquals(stringFieldValue, actualTestDoFn.stringField);    assertEquals(longFieldValue, actualTestDoFn.longField);}
public void beam_f9800_0() throws Exception
{        CloudObject cloudUserFn = CloudObject.forClassName("UnknownKindOfDoFn");    try {        DEFAULT_FACTORY.create(DEFAULT_OPTIONS, cloudUserFn, null, MAIN_OUTPUT, ImmutableMap.<TupleTag<?>, Integer>of(MAIN_OUTPUT, 0), DEFAULT_EXECUTION_CONTEXT, TestOperationContext.create(counterSet));        fail("should have thrown an exception");    } catch (Exception exn) {        assertThat(exn.toString(), Matchers.containsString("No known ParDoFnFactory"));    }}
public void beam_f9801_0()
{    cell = new DeltaCounterCell(MetricName.named("hello", "world"));}
public void beam_f9810_0() throws Exception
{    DoFnInfo<?, ?> info = DoFnInfo.forFn(initialFn, WindowingStrategy.globalDefault(), null, /* side input views */    null, /* input coder */    new TupleTag<>(PropertyNames.OUTPUT), /* main output id */    DoFnSchemaInformation.create(), Collections.emptyMap());    DoFnInstanceManager mgr = DoFnInstanceManagers.cloningPool(info);    DoFnInfo<?, ?> firstInfo = mgr.get();    DoFnInfo<?, ?> secondInfo = mgr.get();    assertThat(firstInfo, not(Matchers.<DoFnInfo<?, ?>>theInstance(secondInfo)));    assertThat(firstInfo.getDoFn(), not(theInstance(secondInfo.getDoFn())));}
public void beam_f9811_0()
{    Set<String> experimentNames = new HashSet<>();    ExperimentContext ec = ExperimentContext.parseFrom(experimentNames);        for (Experiment experiment : Experiment.values()) {        assertFalse(ec.isEnabled(experiment));    }        for (Experiment experiment : Experiment.values()) {        experimentNames.add(experiment.getName());    }    ec = ExperimentContext.parseFrom(experimentNames);        for (Experiment experiment : Experiment.values()) {        assertTrue(ec.isEnabled(experiment));    }}
public void beam_f9812_0()
{    PipelineOptions options = PipelineOptionsFactory.create();    options.as(DataflowPipelineDebugOptions.class).setExperiments(Lists.newArrayList(Experiment.IntertransformIO.getName()));    ExperimentContext ec = ExperimentContext.parseFrom(options);    assertTrue(ec.isEnabled(Experiment.IntertransformIO));}
private void beam_f9820_0(Windmill.CommitWorkRequest request)
{    for (ComputationCommitWorkRequest computationRequest : request.getRequestsList()) {        for (WorkItemCommitRequest commit : computationRequest.getRequestsList()) {            errorCollector.checkThat(commit.hasWorkToken(), equalTo(true));            errorCollector.checkThat(commit.getShardingKey(), allOf(greaterThan(0L), lessThan(Long.MAX_VALUE)));            errorCollector.checkThat(commit.getCacheToken(), not(equalTo(0L)));        }    }}
public CommitWorkResponse beam_f9821_1(Windmill.CommitWorkRequest request)
{        validateCommitWorkRequest(request);    for (ComputationCommitWorkRequest computationRequest : request.getRequestsList()) {        for (WorkItemCommitRequest commit : computationRequest.getRequestsList()) {            commitsReceived.put(commit.getWorkToken(), commit);        }    }    CommitWorkResponse response = CommitWorkResponse.newBuilder().build();        return response;}
public Windmill.GetConfigResponse beam_f9822_0(Windmill.GetConfigRequest request)
{    return Windmill.GetConfigResponse.newBuilder().build();}
public GetDataStream beam_f9830_0()
{    Instant startTime = Instant.now();    return new GetDataStream() {        @Override        public Windmill.KeyedGetDataResponse requestKeyedData(String computation, KeyedGetDataRequest request) {            Windmill.GetDataRequest getDataRequest = GetDataRequest.newBuilder().addRequests(ComputationGetDataRequest.newBuilder().setComputationId(computation).addRequests(request).build()).build();            GetDataResponse getDataResponse = getData(getDataRequest);            if (getDataResponse.getDataList().isEmpty()) {                return null;            }            assertEquals(1, getDataResponse.getDataCount());            if (getDataResponse.getData(0).getDataList().isEmpty()) {                return null;            }            assertEquals(1, getDataResponse.getData(0).getDataCount());            return getDataResponse.getData(0).getData(0);        }        @Override        public Windmill.GlobalData requestGlobalData(Windmill.GlobalDataRequest request) {            Windmill.GetDataRequest getDataRequest = GetDataRequest.newBuilder().addGlobalDataFetchRequests(request).build();            GetDataResponse getDataResponse = getData(getDataRequest);            if (getDataResponse.getGlobalDataList().isEmpty()) {                return null;            }            assertEquals(1, getDataResponse.getGlobalDataCount());            return getDataResponse.getGlobalData(0);        }        @Override        public void refreshActiveWork(Map<String, List<KeyedGetDataRequest>> active) {        }        @Override        public void close() {        }        @Override        public boolean awaitTermination(int time, TimeUnit unit) {            return true;        }        @Override        public void closeAfterDefaultTimeout() {        }        @Override        public Instant startTime() {            return startTime;        }    };}
public Windmill.KeyedGetDataResponse beam_f9831_0(String computation, KeyedGetDataRequest request)
{    Windmill.GetDataRequest getDataRequest = GetDataRequest.newBuilder().addRequests(ComputationGetDataRequest.newBuilder().setComputationId(computation).addRequests(request).build()).build();    GetDataResponse getDataResponse = getData(getDataRequest);    if (getDataResponse.getDataList().isEmpty()) {        return null;    }    assertEquals(1, getDataResponse.getDataCount());    if (getDataResponse.getData(0).getDataList().isEmpty()) {        return null;    }    assertEquals(1, getDataResponse.getData(0).getDataCount());    return getDataResponse.getData(0).getData(0);}
public Windmill.GlobalData beam_f9832_0(Windmill.GlobalDataRequest request)
{    Windmill.GetDataRequest getDataRequest = GetDataRequest.newBuilder().addGlobalDataFetchRequests(request).build();    GetDataResponse getDataResponse = getData(getDataRequest);    if (getDataResponse.getGlobalDataList().isEmpty()) {        return null;    }    assertEquals(1, getDataResponse.getGlobalDataCount());    return getDataResponse.getGlobalData(0);}
public Map<Long, WorkItemCommitRequest> beam_f9846_1(int numCommits)
{        int maxTries = 10;    while (maxTries-- > 0 && commitsReceived.size() < commitsRequested + numCommits) {        Uninterruptibles.sleepUninterruptibly(1000, TimeUnit.MILLISECONDS);    }    assertFalse("Should have received " + numCommits + " more commits beyond " + commitsRequested + " commits already seen, but after 10s have only seen " + commitsReceived + ". Exceptions seen: " + exceptions, commitsReceived.size() < commitsRequested + numCommits);    commitsRequested += numCommits;        return commitsReceived;}
public void beam_f9847_0(int i)
{    expectedExceptionCount.getAndAdd(i);}
public Windmill.Exception beam_f9848_0() throws InterruptedException
{    return exceptions.take();}
public void beam_f9856_0() throws Exception
{    exception.expect(IllegalArgumentException.class);    exception.expectMessage("Unsupported number of shards: 2000000000 " + "in filepattern: gs://bucket/object@2000000000.ism");    Filepatterns.expandAtNFilepattern("gs://bucket/object@2000000000.ism");}
public void beam_f9857_0() throws Exception
{    assertThat(Filepatterns.expandAtNFilepattern("gs://bucket/file@2.ism"), contains("gs://bucket/file-00000-of-00002.ism", "gs://bucket/file-00001-of-00002.ism"));}
public void beam_f9858_0() throws Exception
{    Iterable<String> iterable = Filepatterns.expandAtNFilepattern("gs://bucket/file@200000.ism");    assertThat(iterable, Matchers.<String>iterableWithSize(200000));    assertThat(iterable, hasItems("gs://bucket/file-003232-of-200000.ism", "gs://bucket/file-199999-of-200000.ism"));}
public void beam_f9867_0() throws Exception
{    final String stepName = "fakeStepNameWithUserMetrics";    final String namespace = "sdk/whatever";    final String name = "someCounter";    final int firstCounterValue = 42;    final int secondCounterValue = 77;    final CountDownLatch progressSentTwiceLatch = new CountDownLatch(2);    final CountDownLatch processBundleLatch = new CountDownLatch(1);    final Metrics.User.MetricName metricName = Metrics.User.MetricName.newBuilder().setNamespace(namespace).setName(name).build();    InstructionRequestHandler instructionRequestHandler = new InstructionRequestHandler() {        @Override        public CompletionStage<InstructionResponse> handle(InstructionRequest request) {            switch(request.getRequestCase()) {                case REGISTER:                    return CompletableFuture.completedFuture(responseFor(request).build());                case PROCESS_BUNDLE:                    return MoreFutures.supplyAsync(() -> {                        processBundleLatch.await();                        return responseFor(request).setProcessBundle(BeamFnApi.ProcessBundleResponse.getDefaultInstance()).build();                    });                case PROCESS_BUNDLE_PROGRESS:                    progressSentTwiceLatch.countDown();                    return CompletableFuture.completedFuture(responseFor(request).setProcessBundleProgress(BeamFnApi.ProcessBundleProgressResponse.newBuilder().setMetrics(Metrics.newBuilder().putPtransforms(GRPC_READ_ID, FAKE_ELEMENT_COUNT_METRICS).putPtransforms(stepName, Metrics.PTransform.newBuilder().addUser(Metrics.User.newBuilder().setMetricName(metricName).setCounterData(Metrics.User.CounterData.newBuilder().setValue(progressSentTwiceLatch.getCount() > 0 ? firstCounterValue : secondCounterValue))).build()))).build());                default:                                        return new CompletableFuture<>();            }        }        @Override        public void close() {        }    };    when(grpcPortWriteOperation.processedElementsConsumer()).thenReturn(elementsConsumed -> {    });    RegisterAndProcessBundleOperation processOperation = new RegisterAndProcessBundleOperation(IdGenerators.decrementingLongs(), instructionRequestHandler, mockBeamFnStateDelegator, REGISTER_REQUEST, ImmutableMap.of(), ImmutableMap.of(), ImmutableMap.of(), ImmutableTable.of(), ImmutableMap.of(), mockContext);    BeamFnMapTaskExecutor mapTaskExecutor = BeamFnMapTaskExecutor.forOperations(ImmutableList.of(readOperation, grpcPortWriteOperation, processOperation), executionStateTracker);            CompletionStage<Void> doneFuture = MoreFutures.runAsync(mapTaskExecutor::execute);    progressSentTwiceLatch.await();    Iterable<CounterUpdate> metricsCounterUpdates = Collections.emptyList();    while (Iterables.size(metricsCounterUpdates) == 0) {        Thread.sleep(ReadOperation.DEFAULT_PROGRESS_UPDATE_PERIOD_MS);        metricsCounterUpdates = mapTaskExecutor.extractMetricUpdates();    }    assertThat(metricsCounterUpdates, contains(new CounterHamcrestMatchers.CounterUpdateIntegerValueMatcher(secondCounterValue)));        processBundleLatch.countDown();    MoreFutures.get(doneFuture);}
public CompletionStage<InstructionResponse> beam_f9868_0(InstructionRequest request)
{    switch(request.getRequestCase()) {        case REGISTER:            return CompletableFuture.completedFuture(responseFor(request).build());        case PROCESS_BUNDLE:            return MoreFutures.supplyAsync(() -> {                processBundleLatch.await();                return responseFor(request).setProcessBundle(BeamFnApi.ProcessBundleResponse.getDefaultInstance()).build();            });        case PROCESS_BUNDLE_PROGRESS:            progressSentTwiceLatch.countDown();            return CompletableFuture.completedFuture(responseFor(request).setProcessBundleProgress(BeamFnApi.ProcessBundleProgressResponse.newBuilder().setMetrics(Metrics.newBuilder().putPtransforms(GRPC_READ_ID, FAKE_ELEMENT_COUNT_METRICS).putPtransforms(stepName, Metrics.PTransform.newBuilder().addUser(Metrics.User.newBuilder().setMetricName(metricName).setCounterData(Metrics.User.CounterData.newBuilder().setValue(progressSentTwiceLatch.getCount() > 0 ? firstCounterValue : secondCounterValue))).build()))).build());        default:                        return new CompletableFuture<>();    }}
public void beam_f9870_0() throws Exception
{    final String stepName = "fakeStepNameWithUserMetrics";    final String namespace = "sdk/whatever";    final String name = "someCounter";    final int counterValue = 42;    final int finalCounterValue = 77;    final CountDownLatch progressSentLatch = new CountDownLatch(1);    final CountDownLatch processBundleLatch = new CountDownLatch(1);    final Metrics.User.MetricName metricName = Metrics.User.MetricName.newBuilder().setNamespace(namespace).setName(name).build();    InstructionRequestHandler instructionRequestHandler = new InstructionRequestHandler() {        @Override        public CompletionStage<InstructionResponse> handle(InstructionRequest request) {            switch(request.getRequestCase()) {                case REGISTER:                    return CompletableFuture.completedFuture(responseFor(request).build());                case PROCESS_BUNDLE:                    return MoreFutures.supplyAsync(() -> {                        processBundleLatch.await();                        return responseFor(request).setProcessBundle(BeamFnApi.ProcessBundleResponse.newBuilder().setMetrics(Metrics.newBuilder().putPtransforms(GRPC_READ_ID, FAKE_ELEMENT_COUNT_METRICS).putPtransforms(stepName, Metrics.PTransform.newBuilder().addUser(Metrics.User.newBuilder().setMetricName(metricName).setCounterData(Metrics.User.CounterData.newBuilder().setValue(finalCounterValue))).build()))).build();                    });                case PROCESS_BUNDLE_PROGRESS:                    progressSentLatch.countDown();                    return CompletableFuture.completedFuture(responseFor(request).setProcessBundleProgress(BeamFnApi.ProcessBundleProgressResponse.newBuilder().setMetrics(Metrics.newBuilder().putPtransforms(GRPC_READ_ID, FAKE_ELEMENT_COUNT_METRICS).putPtransforms(stepName, Metrics.PTransform.newBuilder().addUser(Metrics.User.newBuilder().setMetricName(metricName).setCounterData(Metrics.User.CounterData.newBuilder().setValue(counterValue))).build()))).build());                default:                                        return new CompletableFuture<>();            }        }        @Override        public void close() {        }    };    RegisterAndProcessBundleOperation processOperation = new RegisterAndProcessBundleOperation(IdGenerators.decrementingLongs(), instructionRequestHandler, mockBeamFnStateDelegator, REGISTER_REQUEST, ImmutableMap.of(), ImmutableMap.of(), ImmutableMap.of(), ImmutableTable.of(), ImmutableMap.of(), mockContext);    BeamFnMapTaskExecutor mapTaskExecutor = BeamFnMapTaskExecutor.forOperations(ImmutableList.of(readOperation, grpcPortWriteOperation, processOperation), executionStateTracker);            CompletionStage<Void> doneFuture = MoreFutures.runAsync(mapTaskExecutor::execute);    progressSentLatch.await();        Iterable<CounterUpdate> metricsCounterUpdates = Collections.emptyList();    while (Iterables.size(metricsCounterUpdates) == 0) {        Thread.sleep(ReadOperation.DEFAULT_PROGRESS_UPDATE_PERIOD_MS);        metricsCounterUpdates = mapTaskExecutor.extractMetricUpdates();    }        processBundleLatch.countDown();    MoreFutures.get(doneFuture);    metricsCounterUpdates = mapTaskExecutor.extractMetricUpdates();    assertThat(Iterables.size(metricsCounterUpdates), equalTo(1));    assertThat(metricsCounterUpdates, contains(new CounterHamcrestMatchers.CounterUpdateIntegerValueMatcher(finalCounterValue)));}
public String beam_f9880_0()
{    return valuesPrefix + "UserName";}
public TimerData beam_f9881_0(Coder<W> windowCoder)
{    return null;}
public DataflowStepContext beam_f9883_0()
{    return this;}
public void beam_f9891_0()
{    ImmutableMap<String, SideInputReader> sideInputReadersMap = ImmutableMap.<String, SideInputReader>builder().build();    ImmutableMap<RunnerApi.ExecutableStagePayload.SideInputId, PCollectionView<?>> sideInputIdToPCollectionViewMap = ImmutableMap.<RunnerApi.ExecutableStagePayload.SideInputId, PCollectionView<?>>builder().build();    DataflowSideInputHandlerFactory factory = DataflowSideInputHandlerFactory.of(sideInputReadersMap, sideInputIdToPCollectionViewMap);    thrown.expect(instanceOf(IllegalStateException.class));    factory.forSideInput(TRANSFORM_ID, SIDE_INPUT_NAME, MULTIMAP_ACCESS, KvCoder.of(VoidCoder.of(), VoidCoder.of()), GlobalWindow.Coder.INSTANCE);}
public void beam_f9892_0()
{    ImmutableMap<String, SideInputReader> sideInputReadersMap = ImmutableMap.<String, SideInputReader>builder().put(TRANSFORM_ID, fakeSideInputReader).build();    ImmutableMap<RunnerApi.ExecutableStagePayload.SideInputId, PCollectionView<?>> sideInputIdToPCollectionViewMap = ImmutableMap.<RunnerApi.ExecutableStagePayload.SideInputId, PCollectionView<?>>builder().put(sideInputId, view).build();    DataflowSideInputHandlerFactory factory = DataflowSideInputHandlerFactory.of(sideInputReadersMap, sideInputIdToPCollectionViewMap);    SideInputHandler<Integer, GlobalWindow> handler = factory.forSideInput(TRANSFORM_ID, SIDE_INPUT_NAME, MULTIMAP_ACCESS, KvCoder.of(StringUtf8Coder.of(), StringUtf8Coder.of()), GlobalWindow.Coder.INSTANCE);    Iterable<Integer> result = handler.get(ENCODED_FOO2, GlobalWindow.INSTANCE);    assertThat(result, emptyIterable());}
public void beam_f9893_0()
{    ImmutableMap<String, SideInputReader> sideInputReadersMap = ImmutableMap.<String, SideInputReader>builder().put(TRANSFORM_ID, fakeSideInputReader).build();    ImmutableMap<RunnerApi.ExecutableStagePayload.SideInputId, PCollectionView<?>> sideInputIdToPCollectionViewMap = ImmutableMap.<RunnerApi.ExecutableStagePayload.SideInputId, PCollectionView<?>>builder().put(sideInputId, view).build();    DataflowSideInputHandlerFactory factory = DataflowSideInputHandlerFactory.of(sideInputReadersMap, sideInputIdToPCollectionViewMap);    StateRequestHandlers.SideInputHandler handler = factory.forSideInput(TRANSFORM_ID, SIDE_INPUT_NAME, MULTIMAP_ACCESS, KvCoder.of(StringUtf8Coder.of(), VarIntCoder.of()), GlobalWindow.Coder.INSTANCE);    Iterable<String> result = handler.get(ENCODED_FOO, GlobalWindow.INSTANCE);    assertThat(result, containsInAnyOrder(1, 4, 3));}
public void beam_f9901_0()
{    Map<String, MonitoringInfoToCounterUpdateTransformer> genericTransformers = new HashMap<>();    final String validUrn = "urn1";    genericTransformers.put(validUrn, mockTransformer1);    genericTransformers.put("any:other:urn", mockTransformer2);    FnApiMonitoringInfoToCounterUpdateTransformer testObject = new FnApiMonitoringInfoToCounterUpdateTransformer(genericTransformers);    CounterUpdate expectedResult = new CounterUpdate();    when(mockTransformer1.transform(any())).thenReturn(expectedResult);    MonitoringInfo monitoringInfo = MonitoringInfo.newBuilder().setUrn(validUrn).putLabels(MonitoringInfoConstants.Labels.PTRANSFORM, "anyValue").build();    CounterUpdate result = testObject.transform(monitoringInfo);    assertSame(expectedResult, result);}
public void beam_f9902_0()
{    Map<String, MonitoringInfoToCounterUpdateTransformer> genericTransformers = new HashMap<>();    genericTransformers.put("beam:metric:user", mockTransformer2);    FnApiMonitoringInfoToCounterUpdateTransformer testObject = new FnApiMonitoringInfoToCounterUpdateTransformer(genericTransformers);    CounterUpdate expectedResult = new CounterUpdate();    when(mockTransformer1.transform(any())).thenReturn(expectedResult);    MonitoringInfo monitoringInfo = MonitoringInfo.newBuilder().setUrn("any:other:urn").putLabels(MonitoringInfoConstants.Labels.PTRANSFORM, "anyValue").build();    CounterUpdate result = testObject.transform(monitoringInfo);    assertSame(null, result);}
public void beam_f9903_0() throws Exception
{    MockitoAnnotations.initMocks(this);}
public void beam_f9911_0()
{    Map<String, String> counterNameMapping = new HashMap<>();    counterNameMapping.put("beam:counter:supported", "supportedCounter");    Map<String, DataflowStepContext> stepContextMapping = new HashMap<>();    MSecMonitoringInfoToCounterUpdateTransformer testObject = new MSecMonitoringInfoToCounterUpdateTransformer(mockSpecValidator, stepContextMapping);    when(mockSpecValidator.validate(any())).thenReturn(Optional.empty());    MonitoringInfo monitoringInfo = MonitoringInfo.newBuilder().setUrn("beam:counter:unsupported").putLabels(MonitoringInfoConstants.Labels.PTRANSFORM, "anyValue").build();    exception.expect(RuntimeException.class);    testObject.transform(monitoringInfo);}
public void beam_f9912_0()
{        Map<String, String> counterNameMapping = new HashMap<>();    counterNameMapping.put("beam:counter:supported", "supportedCounter");    Map<String, DataflowStepContext> stepContextMapping = new HashMap<>();    NameContext nc = NameContext.create("anyStageName", "anyOriginalName", "anySystemName", "anyUserName");    DataflowStepContext dsc = mock(DataflowStepContext.class);    when(dsc.getNameContext()).thenReturn(nc);    stepContextMapping.put("anyValue", dsc);    MSecMonitoringInfoToCounterUpdateTransformer testObject = new MSecMonitoringInfoToCounterUpdateTransformer(mockSpecValidator, stepContextMapping, counterNameMapping);    when(mockSpecValidator.validate(any())).thenReturn(Optional.empty());        MonitoringInfo monitoringInfo = MonitoringInfo.newBuilder().setUrn("beam:counter:supported").putLabels(MonitoringInfoConstants.Labels.PTRANSFORM, "anyValue").build();    CounterUpdate result = testObject.transform(monitoringInfo);        assertNotEquals(null, result);    assertEquals("{cumulative=true, integer={highBits=0, lowBits=0}, " + "structuredNameAndMetadata={metadata={kind=SUM}, " + "name={executionStepName=anyStageName, name=supportedCounter, origin=SYSTEM, " + "originalStepName=anyOriginalName}}}", result.toString());}
public void beam_f9913_0()
{    Map<String, DataflowStepContext> stepContextMapping = new HashMap<>();    MSecMonitoringInfoToCounterUpdateTransformer testObject = new MSecMonitoringInfoToCounterUpdateTransformer(mockSpecValidator, stepContextMapping);    Map<String, String> result = testObject.createKnownUrnToCounterNameMapping();    assertEquals("process-msecs", result.get("beam:metric:pardo_execution_time:process_bundle_msecs:v1"));    assertEquals("finish-msecs", result.get("beam:metric:pardo_execution_time:finish_bundle_msecs:v1"));    assertEquals("start-msecs", result.get("beam:metric:pardo_execution_time:start_bundle_msecs:v1"));}
public CompletionStage<InstructionResponse> beam_f9921_0(InstructionRequest request)
{    CompletableFuture<InstructionResponse> responseFuture = new CompletableFuture<>();    completeFuture(request, responseFuture);    return responseFuture;}
public void beam_f9923_0() throws Exception
{    List<BeamFnApi.InstructionRequest> requests = new ArrayList<>();    IdGenerator idGenerator = makeIdGeneratorStartingFrom(777L);    RegisterAndProcessBundleOperation operation = new RegisterAndProcessBundleOperation(idGenerator, new InstructionRequestHandler() {        @Override        public CompletionStage<InstructionResponse> handle(InstructionRequest request) {            requests.add(request);            switch(request.getRequestCase()) {                case REGISTER:                    return CompletableFuture.completedFuture(responseFor(request).setRegister(BeamFnApi.RegisterResponse.getDefaultInstance()).build());                case PROCESS_BUNDLE:                    return CompletableFuture.completedFuture(responseFor(request).setProcessBundle(BeamFnApi.ProcessBundleResponse.getDefaultInstance()).build());                default:                                        return new CompletableFuture<>();            }        }        @Override        public void close() {        }    }, mockBeamFnStateDelegator, REGISTER_REQUEST, ImmutableMap.of(), ImmutableMap.of(), ImmutableMap.of(), ImmutableTable.of(), ImmutableMap.of(), mockContext);        assertThat(requests, empty());    operation.start();    assertEquals(requests.get(0), BeamFnApi.InstructionRequest.newBuilder().setInstructionId("777").setRegister(REGISTER_REQUEST).build());    assertEquals(requests.get(1), BeamFnApi.InstructionRequest.newBuilder().setInstructionId("778").setProcessBundle(BeamFnApi.ProcessBundleRequest.newBuilder().setProcessBundleDescriptorId("555")).build());    operation.finish();        operation.start();    assertEquals(requests.get(2), BeamFnApi.InstructionRequest.newBuilder().setInstructionId("779").setProcessBundle(BeamFnApi.ProcessBundleRequest.newBuilder().setProcessBundleDescriptorId("555")).build());    operation.finish();}
public CompletionStage<InstructionResponse> beam_f9924_0(InstructionRequest request)
{    requests.add(request);    switch(request.getRequestCase()) {        case REGISTER:            return CompletableFuture.completedFuture(responseFor(request).setRegister(BeamFnApi.RegisterResponse.getDefaultInstance()).build());        case PROCESS_BUNDLE:            return CompletableFuture.completedFuture(responseFor(request).setProcessBundle(BeamFnApi.ProcessBundleResponse.getDefaultInstance()).build());        default:                        return new CompletableFuture<>();    }}
public CompletionStage<InstructionResponse> beam_f9936_0(InstructionRequest request)
{    switch(request.getRequestCase()) {        case REGISTER:            return CompletableFuture.completedFuture(responseFor(request).build());        case PROCESS_BUNDLE:            return MoreFutures.supplyAsync(() -> {                StateRequest partialRequest = StateRequest.newBuilder().setStateKey(StateKey.newBuilder().setBagUserState(StateKey.BagUserState.newBuilder().setTransformId("testPTransformId").setWindow(ByteString.EMPTY).setUserStateId("testUserStateId"))).buildPartial();                StateRequest get = partialRequest.toBuilder().setGet(StateGetRequest.getDefaultInstance()).build();                StateRequest clear = partialRequest.toBuilder().setClear(StateClearRequest.getDefaultInstance()).build();                StateRequest append = partialRequest.toBuilder().setAppend(StateAppendRequest.newBuilder().setData(ByteString.copyFromUtf8("ABC"))).build();                StateRequestHandler stateHandler = stateHandlerCaptor.getValue();                StateResponse.Builder getWhenEmptyResponse = MoreFutures.get(stateHandler.handle(get));                assertEquals(ByteString.EMPTY, getWhenEmptyResponse.getGet().getData());                StateResponse.Builder appendWhenEmptyResponse = MoreFutures.get(stateHandler.handle(append));                assertNotNull(appendWhenEmptyResponse);                StateResponse.Builder appendWhenEmptyResponse2 = MoreFutures.get(stateHandler.handle(append));                assertNotNull(appendWhenEmptyResponse2);                StateResponse.Builder getWhenHasValueResponse = MoreFutures.get(stateHandler.handle(get));                assertEquals(ByteString.copyFromUtf8("ABC").concat(ByteString.copyFromUtf8("ABC")), getWhenHasValueResponse.getGet().getData());                StateResponse.Builder clearResponse = MoreFutures.get(stateHandler.handle(clear));                assertNotNull(clearResponse);                return responseFor(request).setProcessBundle(BeamFnApi.ProcessBundleResponse.getDefaultInstance()).build();            });        default:                        return new CompletableFuture<>();    }}
public void beam_f9938_0() throws Exception
{    IdGenerator idGenerator = makeIdGeneratorStartingFrom(777L);    ExecutorService executorService = Executors.newCachedThreadPool();    DataflowStepContext mockStepContext = mock(DataflowStepContext.class);    DataflowStepContext mockUserStepContext = mock(DataflowStepContext.class);    when(mockStepContext.namespacedToUser()).thenReturn(mockUserStepContext);    CountDownLatch waitForStateHandler = new CountDownLatch(1);        InstructionRequestHandler fakeClient = new InstructionRequestHandler() {        @Override        public CompletionStage<InstructionResponse> handle(InstructionRequest request) {            switch(request.getRequestCase()) {                case REGISTER:                    return CompletableFuture.completedFuture(responseFor(request).build());                case PROCESS_BUNDLE:                    return MoreFutures.supplyAsync(() -> {                        StateKey getKey = StateKey.newBuilder().setMultimapSideInput(StateKey.MultimapSideInput.newBuilder().setTransformId("testPTransformId").setSideInputId("testSideInputId").setWindow(ByteString.copyFrom(CoderUtils.encodeToByteArray(GlobalWindow.Coder.INSTANCE, GlobalWindow.INSTANCE))).setKey(ByteString.copyFrom(CoderUtils.encodeToByteArray(ByteArrayCoder.of(), "ABC".getBytes(StandardCharsets.UTF_8), Coder.Context.NESTED)))).build();                        StateRequest getRequest = StateRequest.newBuilder().setStateKey(getKey).setGet(StateGetRequest.getDefaultInstance()).build();                        waitForStateHandler.await();                        StateRequestHandler stateHandler = stateHandlerCaptor.getValue();                        StateResponse.Builder getResponse = MoreFutures.get(stateHandler.handle(getRequest));                        assertEquals(encodeAndConcat(Arrays.asList("X", "Y", "Z"), StringUtf8Coder.of()), getResponse.getGet().getData());                        return responseFor(request).setProcessBundle(BeamFnApi.ProcessBundleResponse.getDefaultInstance()).build();                    });                default:                                        return new CompletableFuture<>();            }        }        @Override        public void close() {        }    };    SideInputReader fakeSideInputReader = new SideInputReader() {        @Nullable        @Override        public <T> T get(PCollectionView<T> view, BoundedWindow window) {            assertEquals(GlobalWindow.INSTANCE, window);            assertEquals("testSideInputId", view.getTagInternal().getId());            return (T) InMemoryMultimapSideInputView.fromIterable(ByteArrayCoder.of(), ImmutableList.of(KV.of("ABC".getBytes(StandardCharsets.UTF_8), "X"), KV.of("ABC".getBytes(StandardCharsets.UTF_8), "Y"), KV.of("ABC".getBytes(StandardCharsets.UTF_8), "Z")));        }        @Override        public <T> boolean contains(PCollectionView<T> view) {            return "testSideInputId".equals(view.getTagInternal().getId());        }        @Override        public boolean isEmpty() {            return false;        }    };    RegisterAndProcessBundleOperation operation = new RegisterAndProcessBundleOperation(idGenerator, fakeClient, mockBeamFnStateDelegator, REGISTER_REQUEST, ImmutableMap.of(), ImmutableMap.of("testPTransformId", mockStepContext), ImmutableMap.of("testPTransformId", fakeSideInputReader), ImmutableTable.of("testPTransformId", "testSideInputId", DataflowPortabilityPCollectionView.with(new TupleTag<>("testSideInputId"), FullWindowedValueCoder.of(KvCoder.of(ByteArrayCoder.of(), StringUtf8Coder.of()), GlobalWindow.Coder.INSTANCE))), ImmutableMap.of(), mockContext);    operation.start();    verify(mockBeamFnStateDelegator).registerForProcessBundleInstructionId(eq("778"), stateHandlerCaptor.capture());    waitForStateHandler.countDown();        operation.finish();        assertEquals(stateServiceRegisterCounter.get(), stateServiceDeregisterCounter.get());    assertEquals(0, stateServiceAbortCounter.get());}
public CompletionStage<InstructionResponse> beam_f9939_0(InstructionRequest request)
{    switch(request.getRequestCase()) {        case REGISTER:            return CompletableFuture.completedFuture(responseFor(request).build());        case PROCESS_BUNDLE:            return MoreFutures.supplyAsync(() -> {                StateKey getKey = StateKey.newBuilder().setMultimapSideInput(StateKey.MultimapSideInput.newBuilder().setTransformId("testPTransformId").setSideInputId("testSideInputId").setWindow(ByteString.copyFrom(CoderUtils.encodeToByteArray(GlobalWindow.Coder.INSTANCE, GlobalWindow.INSTANCE))).setKey(ByteString.copyFrom(CoderUtils.encodeToByteArray(ByteArrayCoder.of(), "ABC".getBytes(StandardCharsets.UTF_8), Coder.Context.NESTED)))).build();                StateRequest getRequest = StateRequest.newBuilder().setStateKey(getKey).setGet(StateGetRequest.getDefaultInstance()).build();                waitForStateHandler.await();                StateRequestHandler stateHandler = stateHandlerCaptor.getValue();                StateResponse.Builder getResponse = MoreFutures.get(stateHandler.handle(getRequest));                assertEquals(encodeAndConcat(Arrays.asList("X", "Y", "Z"), StringUtf8Coder.of()), getResponse.getGet().getData());                return responseFor(request).setProcessBundle(BeamFnApi.ProcessBundleResponse.getDefaultInstance()).build();            });        default:                        return new CompletableFuture<>();    }}
private InstructionResponse.Builder beam_f9950_0(BeamFnApi.InstructionRequest request)
{    return BeamFnApi.InstructionResponse.newBuilder().setInstructionId(request.getInstructionId());}
private void beam_f9951_0(BeamFnApi.InstructionRequest request, CompletableFuture<InstructionResponse> response)
{    response.complete(BeamFnApi.InstructionResponse.newBuilder().setInstructionId(request.getInstructionId()).build());}
public void beam_f9952_0() throws Exception
{    InstructionRequestHandler mockInstructionRequestHandler = mock(InstructionRequestHandler.class);    RegisterAndProcessBundleOperation operation = new RegisterAndProcessBundleOperation(IdGenerators.decrementingLongs(), mockInstructionRequestHandler, mockBeamFnStateDelegator, REGISTER_REQUEST, ImmutableMap.of(), ImmutableMap.of(), ImmutableMap.of(), ImmutableTable.of(), ImmutableMap.of(), mockContext);    assertEquals(ProcessBundleProgressResponse.getDefaultInstance(), MoreFutures.get(operation.getProcessBundleProgress()));}
public void beam_f9960_0() throws Exception
{    ThreadFactory threadFactory = new ThreadFactoryBuilder().setDaemon(true).build();    ExecutorService serverExecutor = Executors.newCachedThreadPool(threadFactory);    InProcessServerFactory serverFactory = InProcessServerFactory.create();    dataServer = GrpcFnServer.allocatePortAndCreateFor(GrpcDataService.create(serverExecutor, OutboundObserverFactory.serverDirect()), serverFactory);    loggingServer = GrpcFnServer.allocatePortAndCreateFor(GrpcLoggingService.forWriter(Slf4jLogWriter.getDefault()), serverFactory);    stateDelegator = GrpcStateService.create();    ControlClientPool clientPool = MapControlClientPool.create();    controlServer = GrpcFnServer.allocatePortAndCreateFor(FnApiControlClientPoolService.offeringClientsToPool(clientPool.getSink(), GrpcContextHeaderAccessorProvider.getHeaderAccessor()), serverFactory);        sdkHarnessExecutor = Executors.newSingleThreadExecutor(threadFactory);    sdkHarnessExecutorFuture = sdkHarnessExecutor.submit(() -> {        try {            FnHarness.main("id", PipelineOptionsFactory.create(), loggingServer.getApiServiceDescriptor(), controlServer.getApiServiceDescriptor(), InProcessManagedChannelFactory.create(), OutboundObserverFactory.clientDirect());        } catch (Exception e) {            throw new RuntimeException(e);        }    });    InstructionRequestHandler controlClient = clientPool.getSource().take("", java.time.Duration.ofSeconds(2));    client = SdkHarnessClient.usingFnApiClient(controlClient, dataServer.getService());}
public void beam_f9961_0() throws Exception
{    controlServer.close();    dataServer.close();    loggingServer.close();    sdkHarnessExecutor.shutdownNow();    try {        sdkHarnessExecutorFuture.get();    } catch (ExecutionException e) {        if (e.getCause() instanceof RuntimeException && e.getCause().getCause() instanceof InterruptedException) {                } else {            throw e;        }    }}
public void beam_f9962_0() throws Exception
{    final String timerId = "timerId";    Pipeline p = Pipeline.create();    PCollection<Integer> output = p.apply("impulse", Impulse.create()).apply("create", ParDo.of(new DoFn<byte[], KV<String, Integer>>() {        @ProcessElement        public void process(ProcessContext ctxt) {        }    })).apply("timer", ParDo.of(new DoFn<KV<String, Integer>, Integer>() {        @TimerId(timerId)        private final TimerSpec spec = TimerSpecs.timer(TimeDomain.EVENT_TIME);        @ProcessElement        public void processElement(@TimerId(timerId) Timer timer, OutputReceiver<Integer> r) {            timer.offset(Duration.standardSeconds(1)).setRelative();        }        @OnTimer(timerId)        public void onTimer(@TimerId(timerId) Timer timer, TimeDomain timeDomain, OutputReceiver<Integer> r) {            r.output(0);        }    }));    RunnerApi.Pipeline pipelineProto = PipelineTranslation.toProto(p);    FusedPipeline fused = GreedyPipelineFuser.fuse(pipelineProto);    Optional<ExecutableStage> optionalStage = Iterables.tryFind(fused.getFusedStages(), (ExecutableStage stage) -> !stage.getTimers().isEmpty());    checkState(optionalStage.isPresent(), "Expected a stage with timers.");    ExecutableStage stage = optionalStage.get();    ProcessBundleDescriptors.ExecutableProcessBundleDescriptor descriptor = ProcessBundleDescriptors.fromExecutableStage("test_stage", stage, dataServer.getApiServiceDescriptor());    TimerReceiver timerReceiver = Mockito.spy(new TimerReceiver(stage.getComponents(), buildDataflowStepContext(), buildStageBundleFactory(client, descriptor, stateDelegator)));    Map<String, ProcessBundleDescriptors.TimerSpec> timerSpecMap = new HashMap<>();    descriptor.getTimerSpecs().values().forEach(transformTimerMap -> {        for (ProcessBundleDescriptors.TimerSpec timerSpec : transformTimerMap.values()) {            timerSpecMap.put(timerSpec.timerId(), timerSpec);        }    });    String timerOutputPCollection = timerSpecMap.get(timerId).outputCollectionId();    String timerInputPCollection = timerSpecMap.get(timerId).inputCollectionId();        long testTimerOffset = 123456;        Object timer = timerBytes("X", testTimerOffset);    Object windowedTimer = WindowedValue.valueInGlobalWindow(timer);        assertTrue(timerReceiver.receive(timerOutputPCollection, windowedTimer));        Object expected = WindowedValue.of(timer, BoundedWindow.TIMESTAMP_MIN_VALUE.plus(testTimerOffset), GlobalWindow.INSTANCE, PaneInfo.NO_FIRING);    Mockito.verify(timerReceiver, Mockito.never()).fireTimer(timerInputPCollection, (WindowedValue<KV<Object, org.apache.beam.runners.core.construction.Timer>>) expected);            timerReceiver.finish();    Mockito.verify(timerReceiver).fireTimer(timerInputPCollection, (WindowedValue<KV<Object, org.apache.beam.runners.core.construction.Timer>>) expected);}
public ProcessBundleDescriptors.ExecutableProcessBundleDescriptor beam_f9972_0()
{    return processBundleDescriptor;}
public TimerInternals.TimerData beam_f9974_0(Coder<W> windowCoder)
{    try {        timerInternals.advanceInputWatermark(BoundedWindow.TIMESTAMP_MAX_VALUE);    } catch (Exception e) {        throw new IllegalStateException("Exception thrown advancing watermark", e);    }    return timerInternals.removeNextEventTimer();}
public DataflowExecutionContext.DataflowStepContext beam_f9975_0()
{    return this;}
public void beam_f9983_0() throws Exception
{    MockitoAnnotations.initMocks(this);}
public void beam_f9984_0()
{    Map<String, DataflowStepContext> stepContextMapping = new HashMap<>();    UserDistributionMonitoringInfoToCounterUpdateTransformer testObject = new UserDistributionMonitoringInfoToCounterUpdateTransformer(mockSpecValidator, stepContextMapping);    Optional<String> error = Optional.of("Error text");    when(mockSpecValidator.validate(any())).thenReturn(error);    assertEquals(null, testObject.transform(null));}
public void beam_f9985_0()
{    Map<String, DataflowStepContext> stepContextMapping = new HashMap<>();    MonitoringInfo monitoringInfo = MonitoringInfo.newBuilder().setUrn("beam:metric:element_count:v1").build();    UserDistributionMonitoringInfoToCounterUpdateTransformer testObject = new UserDistributionMonitoringInfoToCounterUpdateTransformer(mockSpecValidator, stepContextMapping);    when(mockSpecValidator.validate(any())).thenReturn(Optional.empty());    exception.expect(RuntimeException.class);    testObject.transform(monitoringInfo);}
public void beam_f9993_0() throws Exception
{    Endpoints.ApiServiceDescriptor descriptor = Endpoints.ApiServiceDescriptor.newBuilder().setUrl(UUID.randomUUID().toString()).build();    PipelineOptions options = PipelineOptionsFactory.create();    service = new BeamFnDataGrpcService(options, descriptor, ServerStreamObserverFactory.fromOptions(options)::from, GrpcContextHeaderAccessorProvider.getHeaderAccessor());    server = createServer(service, descriptor);}
public void beam_f9994_0()
{    server.shutdownNow();}
public void beam_f9995_0() throws Exception
{    BlockingQueue<Elements> clientInboundElements = new LinkedBlockingQueue<>();    ExecutorService executorService = Executors.newCachedThreadPool();    CountDownLatch waitForInboundElements = new CountDownLatch(1);    int numberOfClients = 3;    for (int client = 0; client < numberOfClients; ++client) {        executorService.submit(() -> {            ManagedChannel channel = InProcessChannelBuilder.forName(service.getApiServiceDescriptor().getUrl()).build();            StreamObserver<BeamFnApi.Elements> outboundObserver = BeamFnDataGrpc.newStub(channel).data(TestStreams.withOnNext(clientInboundElements::add).build());            waitForInboundElements.await();            outboundObserver.onCompleted();            return null;        });    }    for (int i = 0; i < 3; ++i) {        CloseableFnDataReceiver<WindowedValue<String>> consumer = service.getDataService(DEFAULT_CLIENT).send(LogicalEndpoint.of(Integer.toString(i), TRANSFORM_ID), CODER);        consumer.accept(valueInGlobalWindow("A" + i));        consumer.accept(valueInGlobalWindow("B" + i));        consumer.accept(valueInGlobalWindow("C" + i));        consumer.close();    }            List<Elements> copy = new ArrayList<>();    for (int i = 0; i < numberOfClients; ++i) {        copy.add(clientInboundElements.take());    }    assertThat(copy, containsInAnyOrder(elementsWithData("0"), elementsWithData("1"), elementsWithData("2")));    waitForInboundElements.countDown();}
public void beam_f10003_0()
{    assertTrue(operation.supportsRestart());}
public void beam_f10004_0() throws Exception
{    InboundDataClient inboundDataClient = CompletableFutureInboundDataClient.create();    when(beamFnDataService.receive(any(), Matchers.<Coder<WindowedValue<String>>>any(), any())).thenReturn(inboundDataClient);    when(bundleIdSupplier.getId()).thenReturn(BUNDLE_ID);    operation.start();    verify(beamFnDataService).receive(eq(LogicalEndpoint.of(BUNDLE_ID, TRANSFORM_ID)), eq(CODER), consumerCaptor.capture());    Future<Void> operationFinish = Executors.newSingleThreadExecutor().submit(() -> {        operation.finish();        return null;    });    consumerCaptor.getValue().accept(valueInGlobalWindow("ABC"));    consumerCaptor.getValue().accept(valueInGlobalWindow("DEF"));    consumerCaptor.getValue().accept(valueInGlobalWindow("GHI"));            Thread.sleep(100L);    assertFalse(operationFinish.isDone());    inboundDataClient.complete();    operationFinish.get();    verify(bundleIdSupplier, times(1)).getId();    assertThat(testReceiver.outputElems, contains(valueInGlobalWindow("ABC"), valueInGlobalWindow("DEF"), valueInGlobalWindow("GHI")));        when(bundleIdSupplier.getId()).thenReturn(BUNDLE_ID_2);    operation.start();    verify(beamFnDataService).receive(eq(LogicalEndpoint.of(BUNDLE_ID_2, TRANSFORM_ID)), eq(CODER), consumerCaptor.capture());}
public void beam_f10005_0() throws Exception
{    InboundDataClient inboundDataClient = CompletableFutureInboundDataClient.create();    when(beamFnDataService.receive(any(), Matchers.<Coder<WindowedValue<String>>>any(), any())).thenReturn(inboundDataClient);    when(bundleIdSupplier.getId()).thenReturn(BUNDLE_ID);    operation.start();    verify(beamFnDataService).receive(eq(LogicalEndpoint.of(BUNDLE_ID, TRANSFORM_ID)), eq(CODER), consumerCaptor.capture());    assertFalse(inboundDataClient.isDone());    operation.abort();    assertTrue(inboundDataClient.isDone());    verify(bundleIdSupplier, times(1)).getId();}
private Endpoints.ApiServiceDescriptor beam_f10014_0() throws Exception
{    InetAddress address = InetAddress.getLoopbackAddress();    try (ServerSocket socket = new ServerSocket(0, -1, address)) {        return Endpoints.ApiServiceDescriptor.newBuilder().setUrl(HostAndPort.fromParts(address.getHostAddress(), socket.getLocalPort()).toString()).build();    }}
public void beam_f10015_0()
{    server.shutdownNow();}
public void beam_f10016_0() throws Exception
{    ConcurrentLinkedQueue<BeamFnApi.LogEntry> logs = new ConcurrentLinkedQueue<>();    try (BeamFnLoggingService service = new BeamFnLoggingService(findOpenPort(), logs::add, ServerStreamObserverFactory.fromOptions(PipelineOptionsFactory.create())::from, GrpcContextHeaderAccessorProvider.getHeaderAccessor())) {        server = createServer(service, service.getApiServiceDescriptor());        Collection<Callable<Void>> tasks = new ArrayList<>();        for (int i = 1; i <= 3; ++i) {            int instructionId = i;            tasks.add(() -> {                CountDownLatch waitForServerHangup = new CountDownLatch(1);                ManagedChannel channel = InProcessChannelBuilder.forName(service.getApiServiceDescriptor().getUrl()).build();                StreamObserver<BeamFnApi.LogEntry.List> outboundObserver = BeamFnLoggingGrpc.newStub(channel).logging(TestStreams.withOnNext(BeamFnLoggingServiceTest::discardMessage).withOnCompleted(waitForServerHangup::countDown).build());                outboundObserver.onNext(createLogsWithIds(instructionId, -instructionId));                outboundObserver.onCompleted();                waitForServerHangup.await();                return null;            });        }        ExecutorService executorService = Executors.newCachedThreadPool();        executorService.invokeAll(tasks);        assertThat(logs, containsInAnyOrder(createLogWithId(1L), createLogWithId(2L), createLogWithId(3L), createLogWithId(-1L), createLogWithId(-2L), createLogWithId(-3L)));    }}
public void beam_f10025_0()
{    StreamObserver<String> observer = ServerStreamObserverFactory.fromOptions(PipelineOptionsFactory.fromArgs(new String[] { "--experiments=beam_fn_api_buffered_stream" }).create()).from(mockResponseObserver);    assertThat(observer, instanceOf(BufferingStreamObserver.class));}
public void beam_f10026_0()
{    StreamObserver<String> observer = ServerStreamObserverFactory.fromOptions(PipelineOptionsFactory.fromArgs(new String[] { "--experiments=beam_fn_api_buffered_stream," + "beam_fn_api_buffered_stream_buffer_size=1" }).create()).from(mockResponseObserver);    assertThat(observer, instanceOf(BufferingStreamObserver.class));    assertEquals(1, ((BufferingStreamObserver<String>) observer).getBufferSize());}
public void beam_f10027_0() throws Exception
{    TestSdkHarness testSdkHarness = new TestSdkHarness(GlobalWindow.INSTANCE);    FnApiWindowMappingFn windowMappingFn = new FnApiWindowMappingFn(IdGenerators.decrementingLongs(), testSdkHarness, DATA_SERVICE, testSdkHarness, WINDOW_MAPPING_SPEC, IntervalWindowCoder.of(), GlobalWindow.Coder.INSTANCE);        BoundedWindow inputWindow = new IntervalWindow(Instant.now(), Duration.standardMinutes(1));    assertEquals(GlobalWindow.INSTANCE, windowMappingFn.getSideInputWindow(inputWindow));    assertEquals(inputWindow, ((KV) testSdkHarness.getInputValues().get(0).getValue()).getValue());        BoundedWindow inputWindow2 = new IntervalWindow(Instant.now(), Duration.standardMinutes(2));    assertEquals(GlobalWindow.INSTANCE, windowMappingFn.getSideInputWindow(inputWindow2));    assertEquals(inputWindow2, ((KV) testSdkHarness.getInputValues().get(1).getValue()).getValue());        assertEquals(GlobalWindow.INSTANCE, windowMappingFn.getSideInputWindow(inputWindow));    assertEquals(2, testSdkHarness.getInputValues().size());}
public void beam_f10038_0() throws Exception
{                                MutableNetwork<Node, Edge> network = createEmptyNetwork();    Node sdk1 = createSdkNode("sdk1");    Node sdk2 = createSdkNode("sdk2");    Node sdk3 = createSdkNode("sdk3");    Node sdk1Out = createPCollection("sdk1.out");    Node sdk2Out = createPCollection("sdk2.out");    Node sdk3Out = createPCollection("sdk3.out");    Node runner1 = createRunnerNode("runner1");    Node runner2 = createRunnerNode("runner2");    Node runner3 = createRunnerNode("runner3");    Node runner4 = createRunnerNode("runner4");    Node runner1Out = createPCollection("runner1.out");    Node runner2Out = createPCollection("runner2.out");    Node runner3Out = createPCollection("runner3.out");    Node runner4Out = createPCollection("runner4.out");    Node ambiguousFlatten = createFlatten("ambiguous_flatten", ExecutionLocation.AMBIGUOUS);    Node ambiguousFlattenOut = createPCollection("ambiguous_flatten.out");    Node sdkFlatten = createFlatten("sdk_flatten", ExecutionLocation.SDK_HARNESS);    Node sdkFlattenOut = createPCollection("sdk_flatten.out");    Node runnerFlatten = createFlatten("runner_flatten", ExecutionLocation.RUNNER_HARNESS);    Node runnerFlattenOut = createPCollection("runner_flatten.out");    network.addNode(sdk1);    network.addNode(sdk2);    network.addNode(sdk3);    network.addNode(sdk1Out);    network.addNode(sdk2Out);    network.addNode(sdk3Out);    network.addNode(runner1);    network.addNode(runner2);    network.addNode(runner3);    network.addNode(runner4);    network.addNode(runner1Out);    network.addNode(runner2Out);    network.addNode(runner3Out);    network.addNode(runner4Out);    network.addNode(ambiguousFlatten);    network.addNode(ambiguousFlattenOut);    network.addNode(sdkFlatten);    network.addNode(sdkFlattenOut);    network.addNode(runnerFlatten);    network.addNode(runnerFlattenOut);    network.addEdge(sdk1, sdk1Out, DefaultEdge.create());    network.addEdge(sdk2, sdk2Out, DefaultEdge.create());    network.addEdge(sdk3, sdk3Out, DefaultEdge.create());    network.addEdge(runner1, runner1Out, DefaultEdge.create());    network.addEdge(runner2, runner2Out, DefaultEdge.create());    network.addEdge(runner3, runner3Out, DefaultEdge.create());    network.addEdge(runner4, runner4Out, DefaultEdge.create());    network.addEdge(ambiguousFlatten, ambiguousFlattenOut, DefaultEdge.create());    network.addEdge(sdkFlatten, sdkFlattenOut, DefaultEdge.create());    network.addEdge(runnerFlatten, runnerFlattenOut, DefaultEdge.create());    network.addEdge(sdk1Out, ambiguousFlatten, DefaultEdge.create());    network.addEdge(runner1Out, ambiguousFlatten, DefaultEdge.create());    network.addEdge(ambiguousFlattenOut, sdkFlatten, DefaultEdge.create());    network.addEdge(sdk2Out, sdkFlatten, DefaultEdge.create());    network.addEdge(sdkFlattenOut, sdk3, DefaultEdge.create());    network.addEdge(ambiguousFlattenOut, runner2, DefaultEdge.create());    network.addEdge(runner2Out, runnerFlatten, DefaultEdge.create());    network.addEdge(runner3Out, runnerFlatten, DefaultEdge.create());    network.addEdge(runnerFlattenOut, runner4, DefaultEdge.create());        List<List<Node>> originalPaths = Networks.allPathsFromRootsToLeaves(network);    network = new CloneAmbiguousFlattensFunction().apply(network);    for (Node node : network.nodes()) {        if (node instanceof ParallelInstructionNode && ((ParallelInstructionNode) node).getParallelInstruction().getFlatten() != null) {            ParallelInstructionNode castNode = ((ParallelInstructionNode) node);            assertTrue("Ambiguous flatten not removed from network.", castNode.getExecutionLocation() != ExecutionLocation.AMBIGUOUS);            if ("sdk_flatten".equals(castNode.getParallelInstruction().getName())) {                assertSame("SDK flatten has been incorrectly modified.", sdkFlatten, castNode);            } else if ("runner_flatten".equals(castNode.getParallelInstruction().getName())) {                assertSame("Runner flatten has been incorrectly modified.", runnerFlatten, castNode);            }        }    }    assertEquals(originalPaths.size(), Networks.allPathsFromRootsToLeaves(network).size());}
public void beam_f10039_0() throws Exception
{                            MutableNetwork<Node, Edge> network = createEmptyNetwork();    Node sdk1 = createSdkNode("sdk1");    Node sdk2 = createSdkNode("sdk2");    Node sdk3 = createSdkNode("sdk3");    Node sdk4 = createSdkNode("sdk4");    Node sdk1Out = createPCollection("sdk1.out");    Node sdk2Out = createPCollection("sdk2.out");    Node sdk3Out = createPCollection("sdk3.out");    Node sdk4Out = createPCollection("sdk4.out");    Node runner1 = createRunnerNode("runner1");    Node runner2 = createRunnerNode("runner2");    Node runner3 = createRunnerNode("runner3");    Node runner4 = createRunnerNode("runner4");    Node runner1Out = createPCollection("runner1.out");    Node runner2Out = createPCollection("runner2.out");    Node runner3Out = createPCollection("runner3.out");    Node runner4Out = createPCollection("runner4.out");    Node ambiguousFlatten1 = createFlatten("ambiguous_flatten1", ExecutionLocation.AMBIGUOUS);    Node ambiguousFlatten2 = createFlatten("ambiguous_flatten2", ExecutionLocation.AMBIGUOUS);    Node ambiguousFlatten3 = createFlatten("ambiguous_flatten3", ExecutionLocation.AMBIGUOUS);    Node ambiguousFlatten1Out = createPCollection("ambiguous_flatten1.out");    Node ambiguousFlatten2Out = createPCollection("ambiguous_flatten2.out");    Node ambiguousFlatten3Out = createPCollection("ambiguous_flatten3.out");    network.addNode(sdk1);    network.addNode(sdk2);    network.addNode(sdk3);    network.addNode(sdk4);    network.addNode(sdk1Out);    network.addNode(sdk2Out);    network.addNode(sdk3Out);    network.addNode(sdk4Out);    network.addNode(runner1);    network.addNode(runner2);    network.addNode(runner3);    network.addNode(runner4);    network.addNode(runner1Out);    network.addNode(runner2Out);    network.addNode(runner3Out);    network.addNode(runner4Out);    network.addNode(ambiguousFlatten1);    network.addNode(ambiguousFlatten2);    network.addNode(ambiguousFlatten3);    network.addNode(ambiguousFlatten1Out);    network.addNode(ambiguousFlatten2Out);    network.addNode(ambiguousFlatten3Out);    network.addEdge(sdk1, sdk1Out, DefaultEdge.create());    network.addEdge(sdk2, sdk2Out, DefaultEdge.create());    network.addEdge(sdk3, sdk3Out, DefaultEdge.create());    network.addEdge(sdk4, sdk4Out, DefaultEdge.create());    network.addEdge(runner1, runner1Out, DefaultEdge.create());    network.addEdge(runner2, runner2Out, DefaultEdge.create());    network.addEdge(runner3, runner3Out, DefaultEdge.create());    network.addEdge(runner4, runner4Out, DefaultEdge.create());    network.addEdge(ambiguousFlatten1, ambiguousFlatten1Out, DefaultEdge.create());    network.addEdge(ambiguousFlatten2, ambiguousFlatten2Out, DefaultEdge.create());    network.addEdge(ambiguousFlatten3, ambiguousFlatten3Out, DefaultEdge.create());    network.addEdge(sdk1Out, ambiguousFlatten1, DefaultEdge.create());    network.addEdge(runner1Out, ambiguousFlatten1, DefaultEdge.create());    network.addEdge(sdk2Out, ambiguousFlatten2, DefaultEdge.create());    network.addEdge(runner2Out, ambiguousFlatten2, DefaultEdge.create());    network.addEdge(ambiguousFlatten1Out, sdk3, DefaultEdge.create());    network.addEdge(ambiguousFlatten1Out, ambiguousFlatten3, DefaultEdge.create());    network.addEdge(ambiguousFlatten2Out, ambiguousFlatten3, DefaultEdge.create());    network.addEdge(ambiguousFlatten2Out, runner3, DefaultEdge.create());    network.addEdge(ambiguousFlatten3Out, sdk4, DefaultEdge.create());    network.addEdge(ambiguousFlatten3Out, runner4, DefaultEdge.create());        List<List<Node>> originalPaths = Networks.allPathsFromRootsToLeaves(network);    network = new CloneAmbiguousFlattensFunction().apply(network);    for (Node node : network.nodes()) {        if (node instanceof ParallelInstructionNode && ((ParallelInstructionNode) node).getParallelInstruction().getFlatten() != null) {            ParallelInstructionNode castNode = ((ParallelInstructionNode) node);            assertTrue("Ambiguous flatten not removed from network.", castNode.getExecutionLocation() != ExecutionLocation.AMBIGUOUS);        }    }    assertEquals(originalPaths.size(), Networks.allPathsFromRootsToLeaves(network).size());}
private static MutableNetwork<Node, Edge> beam_f10040_0()
{    return NetworkBuilder.directed().allowsSelfLoops(false).allowsParallelEdges(true).<Node, Edge>build();}
public void beam_f10048_0()
{    Node readNode = createReadNode("Read", Nodes.ExecutionLocation.RUNNER_HARNESS);    Edge readNodeEdge = DefaultEdge.create();    Node readNodeOut = createInstructionOutputNode("Read.out");    Edge readNodeOutEdge = DefaultEdge.create();    Node parDoNode = createParDoNode("ParDo", Nodes.ExecutionLocation.RUNNER_HARNESS);    Edge parDoNodeEdge = DefaultEdge.create();    Node parDoNodeOut = createInstructionOutputNode("ParDo.out");        MutableNetwork<Node, Edge> expectedNetwork = createEmptyNetwork();    expectedNetwork.addNode(readNode);    expectedNetwork.addNode(readNodeOut);    expectedNetwork.addNode(parDoNode);    expectedNetwork.addNode(parDoNodeOut);    expectedNetwork.addEdge(readNode, readNodeOut, readNodeEdge);    expectedNetwork.addEdge(readNodeOut, parDoNode, readNodeOutEdge);    expectedNetwork.addEdge(parDoNode, parDoNodeOut, parDoNodeEdge);    MutableNetwork<Node, Edge> appliedNetwork = createRegisterFnOperation.apply(Graphs.copyOf(expectedNetwork));    assertNetworkMaintainsBipartiteStructure(appliedNetwork);    assertEquals(String.format("Expected network %s but got network %s", expectedNetwork, appliedNetwork), expectedNetwork, appliedNetwork);}
public void beam_f10049_0()
{    Node sdkPortionNode = TestNode.create("SdkPortion");    @SuppressWarnings({ "unchecked", "rawtypes" })    ArgumentCaptor<MutableNetwork<Node, Edge>> networkCapture = ArgumentCaptor.forClass((Class) MutableNetwork.class);    when(registerFnOperationFunction.apply(networkCapture.capture())).thenReturn(sdkPortionNode);        Node readNode = createReadNode("Read", Nodes.ExecutionLocation.SDK_HARNESS);    Edge readNodeEdge = DefaultEdge.create();    Node readNodeOut = createInstructionOutputNode("Read.out");    Edge readNodeOutEdge = DefaultEdge.create();    Node parDoNode = createParDoNode("ParDo", Nodes.ExecutionLocation.SDK_HARNESS);    Edge parDoNodeEdge = DefaultEdge.create();    Node parDoNodeOut = createInstructionOutputNode("ParDo.out");    MutableNetwork<Node, Edge> network = createEmptyNetwork();    network.addNode(readNode);    network.addNode(readNodeOut);    network.addNode(parDoNode);    network.addNode(parDoNodeOut);    network.addEdge(readNode, readNodeOut, readNodeEdge);    network.addEdge(readNodeOut, parDoNode, readNodeOutEdge);    network.addEdge(parDoNode, parDoNodeOut, parDoNodeEdge);    MutableNetwork<Node, Edge> expectedNetwork = createEmptyNetwork();    expectedNetwork.addNode(sdkPortionNode);    MutableNetwork<Node, Edge> appliedNetwork = createRegisterFnOperation.apply(Graphs.copyOf(network));    assertNetworkMaintainsBipartiteStructure(appliedNetwork);    assertNetworkMaintainsBipartiteStructure(networkCapture.getValue());    assertEquals(String.format("Expected network %s but got network %s", expectedNetwork, appliedNetwork), expectedNetwork, appliedNetwork);    assertEquals(String.format("Expected network %s but got network %s", network, networkCapture.getValue()), network, networkCapture.getValue());}
public void beam_f10050_0()
{    Node sdkPortion = TestNode.create("SdkPortion");    @SuppressWarnings({ "unchecked", "rawtypes" })    ArgumentCaptor<MutableNetwork<Node, Edge>> networkCapture = ArgumentCaptor.forClass((Class) MutableNetwork.class);    when(registerFnOperationFunction.apply(networkCapture.capture())).thenReturn(sdkPortion);    Node firstPort = TestNode.create("FirstPort");    Node secondPort = TestNode.create("SecondPort");    when(portSupplier.get()).thenReturn(firstPort, secondPort);    Node readNode = createReadNode("Read", Nodes.ExecutionLocation.RUNNER_HARNESS);    Edge readNodeEdge = DefaultEdge.create();    Node readNodeOut = createInstructionOutputNode("Read.out");    Edge readNodeOutEdge = DefaultEdge.create();    Node sdkParDoNode = createParDoNode("SdkParDo", Nodes.ExecutionLocation.SDK_HARNESS);    Edge sdkParDoNodeEdge = DefaultEdge.create();    Node sdkParDoNodeOut = createInstructionOutputNode("SdkParDo.out");    Edge sdkParDoNodeOutEdge = DefaultEdge.create();    Node runnerParDoNode = createParDoNode("RunnerParDo", Nodes.ExecutionLocation.RUNNER_HARNESS);    Edge runnerParDoNodeEdge = DefaultEdge.create();    Node runnerParDoNodeOut = createInstructionOutputNode("RunnerParDo.out");        MutableNetwork<Node, Edge> network = createEmptyNetwork();    network.addNode(readNode);    network.addNode(readNodeOut);    network.addNode(sdkParDoNodeOut);    network.addNode(sdkParDoNodeOut);    network.addNode(runnerParDoNode);    network.addNode(runnerParDoNodeOut);    network.addEdge(readNode, readNodeOut, readNodeEdge);    network.addEdge(readNodeOut, sdkParDoNode, readNodeOutEdge);    network.addEdge(sdkParDoNode, sdkParDoNodeOut, sdkParDoNodeEdge);    network.addEdge(sdkParDoNodeOut, runnerParDoNode, sdkParDoNodeOutEdge);    network.addEdge(runnerParDoNode, runnerParDoNodeOut, runnerParDoNodeEdge);    MutableNetwork<Node, Edge> appliedNetwork = createRegisterFnOperation.apply(Graphs.copyOf(network));    assertNetworkMaintainsBipartiteStructure(appliedNetwork);        Node newOutA = Iterables.getOnlyElement(appliedNetwork.predecessors(firstPort));    Node newOutB = Iterables.getOnlyElement(appliedNetwork.successors(secondPort));        assertThat(appliedNetwork.nodes(), containsInAnyOrder(readNode, newOutA, firstPort, sdkPortion, secondPort, newOutB, runnerParDoNode, runnerParDoNodeOut));    assertThat(appliedNetwork.successors(readNode), containsInAnyOrder(newOutA));    assertThat(appliedNetwork.successors(newOutA), containsInAnyOrder(firstPort));    assertThat(appliedNetwork.successors(firstPort), containsInAnyOrder(sdkPortion));    assertThat(appliedNetwork.successors(sdkPortion), containsInAnyOrder(secondPort));    assertThat(appliedNetwork.successors(secondPort), containsInAnyOrder(newOutB));    assertThat(appliedNetwork.successors(newOutB), containsInAnyOrder(runnerParDoNode));    assertThat(appliedNetwork.successors(runnerParDoNode), containsInAnyOrder(runnerParDoNodeOut));    assertThat(appliedNetwork.edgesConnecting(firstPort, sdkPortion), everyItem(Matchers.<Edges.Edge>instanceOf(HappensBeforeEdge.class)));    assertThat(appliedNetwork.edgesConnecting(sdkPortion, secondPort), everyItem(Matchers.<Edges.Edge>instanceOf(HappensBeforeEdge.class)));    MutableNetwork<Node, Edge> sdkSubnetwork = networkCapture.getValue();    assertNetworkMaintainsBipartiteStructure(sdkSubnetwork);    Node sdkNewOutA = Iterables.getOnlyElement(sdkSubnetwork.successors(firstPort));    Node sdkNewOutB = Iterables.getOnlyElement(sdkSubnetwork.predecessors(secondPort));        assertThat(sdkSubnetwork.nodes(), containsInAnyOrder(firstPort, sdkNewOutA, sdkParDoNode, sdkNewOutB, secondPort));    assertThat(sdkSubnetwork.successors(firstPort), containsInAnyOrder(sdkNewOutA));    assertThat(sdkSubnetwork.successors(sdkNewOutA), containsInAnyOrder(sdkParDoNode));    assertThat(sdkSubnetwork.successors(sdkParDoNode), containsInAnyOrder(sdkNewOutB));    assertThat(sdkSubnetwork.successors(sdkNewOutB), containsInAnyOrder(secondPort));}
public static TestNode beam_f10058_0(String value)
{    return new AutoValue_CreateRegisterFnOperationFunctionTest_TestNode(value);}
public String beam_f10059_0()
{    return hashCode() + " " + getName();}
public void beam_f10060_0() throws Exception
{    assertEquals(createEmptyNetwork(), new DeduceFlattenLocationsFunction().apply(createEmptyNetwork()));}
public void beam_f10068_0() throws Exception
{        assertSingleFlattenLocationDeduction(ExecutionLocation.RUNNER_HARNESS, ExecutionLocation.UNKNOWN, ExecutionLocation.RUNNER_HARNESS);}
public void beam_f10069_0() throws Exception
{            assertSingleFlattenLocationDeduction(ExecutionLocation.AMBIGUOUS, ExecutionLocation.SDK_HARNESS, ExecutionLocation.SDK_HARNESS);}
public void beam_f10070_0() throws Exception
{            assertSingleFlattenLocationDeduction(ExecutionLocation.AMBIGUOUS, ExecutionLocation.RUNNER_HARNESS, ExecutionLocation.RUNNER_HARNESS);}
private static MutableNetwork<Node, Edge> beam_f10078_0()
{    return NetworkBuilder.directed().allowsSelfLoops(false).allowsParallelEdges(true).<Node, Edge>build();}
private static void beam_f10079_0(ExecutionLocation predecessorLocations, ExecutionLocation successorLocations, ExecutionLocation expectedLocation) throws Exception
{    MutableNetwork<Node, Edge> network = createSingleFlattenNetwork(predecessorLocations, successorLocations);    network = new DeduceFlattenLocationsFunction().apply(network);    ExecutionLocation flattenLocation = getExecutionLocationOf("flatten", network);    assertEquals(expectedLocation, flattenLocation);}
private static MutableNetwork<Node, Edge> beam_f10080_0(ExecutionLocation predecessorLocations, ExecutionLocation successorLocations) throws Exception
{    MutableNetwork<Node, Edge> network = createEmptyNetwork();    Node flatten = createFlatten("flatten");    Node flattenOutput = createPCollection("pcollection");    network.addNode(flatten);    network.addNode(flattenOutput);    network.addEdge(flatten, flattenOutput, DefaultEdge.create());    if (predecessorLocations == ExecutionLocation.SDK_HARNESS || predecessorLocations == ExecutionLocation.AMBIGUOUS) {        Node node = createSdkNode("sdk_predecessor");        Node out = createPCollection("sdk_predecessor.out");        network.addNode(node);        network.addNode(out);        network.addEdge(node, out, DefaultEdge.create());        network.addEdge(out, flatten, DefaultEdge.create());    }    if (predecessorLocations == ExecutionLocation.RUNNER_HARNESS || predecessorLocations == ExecutionLocation.AMBIGUOUS) {        Node node = createRunnerNode("runner_predecessor");        Node out = createPCollection("runner_predecessor.out");        network.addNode(node);        network.addNode(out);        network.addEdge(node, out, DefaultEdge.create());        network.addEdge(out, flatten, DefaultEdge.create());    }    if (successorLocations == ExecutionLocation.SDK_HARNESS || successorLocations == ExecutionLocation.AMBIGUOUS) {        Node node = createSdkNode("sdk_successor");        Node out = createPCollection("sdk_successor.out");        network.addNode(node);        network.addNode(out);        network.addEdge(flatten, node, DefaultEdge.create());        network.addEdge(node, out, DefaultEdge.create());    }    if (successorLocations == ExecutionLocation.RUNNER_HARNESS || successorLocations == ExecutionLocation.AMBIGUOUS) {        Node node = createRunnerNode("runner_successor");        Node out = createPCollection("runner_successor.out");        network.addNode(node);        network.addNode(out);        network.addEdge(flatten, node, DefaultEdge.create());        network.addEdge(node, out, DefaultEdge.create());    }    return network;}
public void beam_f10088_0()
{    assertEquals(createEmptyNetwork(), new DeduceNodeLocationsFunction().apply(createEmptyNetwork()));}
public void beam_f10089_0() throws Exception
{    Node unknown = createReadNode("Unknown", CUSTOM_SOURCE);    MutableNetwork<Node, Edge> network = createEmptyNetwork();    network.addNode(unknown);    Network<Node, Edge> inputNetwork = ImmutableNetwork.copyOf(network);    network = new DeduceNodeLocationsFunction().apply(network);    assertThatNetworksAreIdentical(inputNetwork, network);    for (Node node : ImmutableList.copyOf(network.nodes())) {        assertNodesIdenticalExceptForExecutionLocation(unknown, node);        assertThatLocationIsProperlyDeduced(node, ExecutionLocation.SDK_HARNESS);    }}
public void beam_f10090_0() throws Exception
{    Node unknown = createReadNode("Unknown", RUNNER_SOURCE);    MutableNetwork<Node, Edge> network = createEmptyNetwork();    network.addNode(unknown);    Network<Node, Edge> inputNetwork = ImmutableNetwork.copyOf(network);    network = new DeduceNodeLocationsFunction().apply(network);    assertThatNetworksAreIdentical(inputNetwork, network);    for (Node node : ImmutableList.copyOf(network.nodes())) {        assertNodesIdenticalExceptForExecutionLocation(unknown, node);        assertThatLocationIsProperlyDeduced(node, ExecutionLocation.RUNNER_HARNESS);    }}
private void beam_f10098_0(Network<Node, Edge> oldNetwork, Network<Node, Edge> newNetwork)
{        assertEquals(oldNetwork.nodes().size(), newNetwork.nodes().size());    assertEquals(oldNetwork.edges().size(), newNetwork.edges().size());        List<List<Equivalence.Wrapper<Node>>> oldPaths = allPathsWithWrappedNodes(oldNetwork);    List<List<Equivalence.Wrapper<Node>>> newPaths = allPathsWithWrappedNodes(newNetwork);    assertThat(oldPaths, containsInAnyOrder(newPaths.toArray()));}
private List<List<Equivalence.Wrapper<Node>>> beam_f10099_0(Network<Node, Edge> network)
{    List<List<Node>> paths = Networks.allPathsFromRootsToLeaves(network);    List<List<Equivalence.Wrapper<Node>>> wrappedPaths = new ArrayList<>();    for (List<Node> path : paths) {        List<Equivalence.Wrapper<Node>> wrappedPath = new ArrayList<>();        for (Node node : path) {            wrappedPath.add(NODE_EQUIVALENCE.wrap(node));        }        wrappedPaths.add(wrappedPath);    }    return wrappedPaths;}
private void beam_f10100_0(Network<Node, Edge> network)
{    for (Node node : network.nodes()) {        if (node instanceof ParallelInstructionNode) {                        if (((ParallelInstructionNode) node).getParallelInstruction().getFlatten() != null) {                assertTrue(((ParallelInstructionNode) node).getExecutionLocation() == ExecutionLocation.UNKNOWN);            } else {                assertTrue(((ParallelInstructionNode) node).getExecutionLocation() != ExecutionLocation.UNKNOWN);            }        }    }}
public void beam_f10108_0() throws Exception
{    Pipeline p = Pipeline.create();    PCollection<String> pc = p.apply(Create.of("a", "b", "c"));    PCollectionView<Iterable<String>> pcView = pc.apply(View.asIterable());    pc.apply(ParDo.of(new TestDoFn(pcView)).withSideInputs(pcView));    RunnerApi.Pipeline pipeline = PipelineTranslation.toProto(p);    Node predecessor = createParDoNode("predecessor");    InstructionOutputNode mainInput = InstructionOutputNode.create(new InstructionOutput(), "fakeId");    Node sideInputParDo = createParDoNode(findParDoWithSideInput(pipeline));    MutableNetwork<Node, Edge> network = createEmptyNetwork();    network.addNode(predecessor);    network.addNode(mainInput);    network.addNode(sideInputParDo);    network.addEdge(predecessor, mainInput, DefaultEdge.create());    network.addEdge(mainInput, sideInputParDo, DefaultEdge.create());    Network<Node, Edge> inputNetwork = ImmutableNetwork.copyOf(network);    network = InsertFetchAndFilterStreamingSideInputNodes.with(pipeline).forNetwork(network);    Node mainInputClone = InstructionOutputNode.create(mainInput.getInstructionOutput(), "fakeId");    Node fetchAndFilter = FetchAndFilterStreamingSideInputsNode.create(pcView.getWindowingStrategyInternal(), ImmutableMap.of(pcView, ParDoTranslation.translateWindowMappingFn(pcView.getWindowMappingFn(), SdkComponents.create(PipelineOptionsFactory.create()))), NameContextsForTests.nameContextForTest());    MutableNetwork<Node, Edge> expectedNetwork = createEmptyNetwork();    expectedNetwork.addNode(predecessor);    expectedNetwork.addNode(mainInputClone);    expectedNetwork.addNode(fetchAndFilter);    expectedNetwork.addNode(mainInput);    expectedNetwork.addNode(sideInputParDo);    expectedNetwork.addEdge(predecessor, mainInputClone, DefaultEdge.create());    expectedNetwork.addEdge(mainInputClone, fetchAndFilter, DefaultEdge.create());    expectedNetwork.addEdge(fetchAndFilter, mainInput, DefaultEdge.create());    expectedNetwork.addEdge(mainInput, sideInputParDo, DefaultEdge.create());    assertThatNetworksAreIdentical(expectedNetwork, network);}
public void beam_f10109_0() throws Exception
{    Pipeline p = Pipeline.create();    PCollection<String> pc = p.apply(Create.of("a", "b", "c"));    pc.apply(ParDo.of(new TestDoFn(null)));    RunnerApi.Pipeline pipeline = PipelineTranslation.toProto(p);    Node predecessor = createParDoNode("predecessor");    Node mainInput = InstructionOutputNode.create(new InstructionOutput(), "fakeId");    Node sideInputParDo = createParDoNode("noSideInput");    MutableNetwork<Node, Edge> network = createEmptyNetwork();    network.addNode(predecessor);    network.addNode(mainInput);    network.addNode(sideInputParDo);    network.addEdge(predecessor, mainInput, DefaultEdge.create());    network.addEdge(mainInput, sideInputParDo, DefaultEdge.create());    Network<Node, Edge> inputNetwork = ImmutableNetwork.copyOf(network);    network = InsertFetchAndFilterStreamingSideInputNodes.with(pipeline).forNetwork(network);    assertThatNetworksAreIdentical(inputNetwork, network);}
private String beam_f10110_0(RunnerApi.Pipeline pipeline)
{    for (Map.Entry<String, RunnerApi.PTransform> entry : pipeline.getComponents().getTransformsMap().entrySet()) {        if (!PTransformTranslation.PAR_DO_TRANSFORM_URN.equals(entry.getValue().getSpec().getUrn())) {            continue;        }        try {            ParDoPayload payload = ParDoPayload.parseFrom(entry.getValue().getSpec().getPayload());            if (!payload.getSideInputsMap().isEmpty()) {                return entry.getKey();            }        } catch (InvalidProtocolBufferException e) {            throw new IllegalStateException(String.format("Failed to parse PTransform %s", entry));        }    }    throw new IllegalStateException("No side input ptransform found");}
public void beam_f10119_0() throws Exception
{    Map<String, Object> lengthPrefixedCoderCloudObject = forCodec(CloudObjects.asCloudObject(windowedValueCoder, /*sdkComponents=*/    null), false);    assertEquals(CloudObjects.asCloudObject(prefixedWindowedValueCoder, /*sdkComponents=*/    null), lengthPrefixedCoderCloudObject);}
public void beam_f10120_0() throws Exception
{    Coder<WindowedValue<KV<String, Integer>>> windowedValueCoder = WindowedValue.getFullCoder(KvCoder.of(StringUtf8Coder.of(), LengthPrefixCoder.of(VarIntCoder.of())), GlobalWindow.Coder.INSTANCE);    Map<String, Object> lengthPrefixedCoderCloudObject = forCodec(CloudObjects.asCloudObject(windowedValueCoder, /*sdkComponents=*/    null), false);    Coder<WindowedValue<KV<String, Integer>>> expectedCoder = WindowedValue.getFullCoder(KvCoder.of(StringUtf8Coder.of(), LengthPrefixCoder.of(VarIntCoder.of())), GlobalWindow.Coder.INSTANCE);    assertEquals(CloudObjects.asCloudObject(expectedCoder, /*sdkComponents=*/    null), lengthPrefixedCoderCloudObject);}
public void beam_f10121_0() throws Exception
{    Coder<WindowedValue<KV<String, Integer>>> windowedValueCoder = WindowedValue.getFullCoder(KvCoder.of(StringUtf8Coder.of(), VarIntCoder.of()), GlobalWindow.Coder.INSTANCE);    Map<String, Object> lengthPrefixedCoderCloudObject = forCodec(CloudObjects.asCloudObject(windowedValueCoder, /*sdkComponents=*/    null), true);    assertEquals(CloudObjects.asCloudObject(prefixedAndReplacedWindowedValueCoder, /*sdkComponents=*/    null), lengthPrefixedCoderCloudObject);}
public void beam_f10129_0()
{    MutableNetwork<Node, Edge> network = createEmptyNetwork();    network.addNode(instructionOutputNode);    network.addNode(grpcPortNode);    network.addEdge(instructionOutputNode, grpcPortNode, DefaultEdge.create());    assertEquals(CloudObjects.asCloudObject(prefixedWindowedValueCoder, /*sdkComponents=*/    null), ((InstructionOutputNode) forInstructionOutputNode(network).apply(instructionOutputNode)).getInstructionOutput().getCodec());}
public void beam_f10130_0()
{    MutableNetwork<Node, Edge> network = createEmptyNetwork();    ParallelInstructionNode readNode = createReadNode("read", "source", windowedValueCoder);    network.addNode(instructionOutputNode);    network.addNode(readNode);    network.addEdge(readNode, instructionOutputNode, DefaultEdge.create());    assertEquals(CloudObjects.asCloudObject(windowedValueCoder, /*sdkComponents=*/    null), ((InstructionOutputNode) forInstructionOutputNode(network).apply(instructionOutputNode)).getInstructionOutput().getCodec());}
public void beam_f10131_0()
{    List<SideInputInfo> prefixedSideInputInfos = LengthPrefixUnknownCoders.forSideInputInfos(ImmutableList.of(createSideInputInfosWithCoders(windowedValueCoder, prefixedWindowedValueCoder)), false);    assertEquals(ImmutableList.of(createSideInputInfosWithCoders(prefixedWindowedValueCoder, prefixedWindowedValueCoder)), prefixedSideInputInfos);    List<SideInputInfo> prefixedAndReplacedSideInputInfos = LengthPrefixUnknownCoders.forSideInputInfos(ImmutableList.of(createSideInputInfosWithCoders(windowedValueCoder, prefixedWindowedValueCoder)), true);    assertEquals(ImmutableList.of(createSideInputInfosWithCoders(prefixedAndReplacedWindowedValueCoder, prefixedAndReplacedWindowedValueCoder)), prefixedAndReplacedSideInputInfos);}
public void beam_f10139_0()
{                InstructionOutput readOutputA = createInstructionOutput("ReadA.out");    ParallelInstruction readA = createParallelInstruction("ReadA", readOutputA);    readA.setRead(new ReadInstruction());    InstructionOutput readOutputB = createInstructionOutput("ReadB.out");    ParallelInstruction readB = createParallelInstruction("ReadB", readOutputB);    readB.setRead(new ReadInstruction());    FlattenInstruction flattenInstruction = new FlattenInstruction();    flattenInstruction.setInputs(ImmutableList.of(    createInstructionInput(0, 0),     createInstructionInput(1, 0)));    InstructionOutput flattenOutput = createInstructionOutput("Flatten.out");    ParallelInstruction flatten = createParallelInstruction("Flatten", flattenOutput);    flatten.setFlatten(flattenInstruction);    MapTask mapTask = new MapTask();    mapTask.setInstructions(ImmutableList.of(readA, readB, flatten));    mapTask.setFactory(Transport.getJsonFactory());    Network<Node, Edge> network = new MapTaskToNetworkFunction(IdGenerators.decrementingLongs()).apply(mapTask);    assertNetworkProperties(network);    assertEquals(6, network.nodes().size());    assertEquals(5, network.edges().size());    ParallelInstructionNode readANode = get(network, readA);    InstructionOutputNode readOutputANode = getOnlySuccessor(network, readANode);    assertEquals(readOutputA, readOutputANode.getInstructionOutput());    ParallelInstructionNode readBNode = get(network, readB);    InstructionOutputNode readOutputBNode = getOnlySuccessor(network, readBNode);    assertEquals(readOutputB, readOutputBNode.getInstructionOutput());        assertEquals(network.successors(readOutputANode), network.successors(readOutputBNode));    ParallelInstructionNode flattenNode = getOnlySuccessor(network, readOutputANode);    InstructionOutputNode flattenOutputNode = getOnlySuccessor(network, flattenNode);    assertEquals(flattenOutput, flattenOutputNode.getInstructionOutput());}
public void beam_f10140_0()
{                InstructionOutput readOutput = createInstructionOutput("Read.out");    ParallelInstruction read = createParallelInstruction("Read", readOutput);    read.setRead(new ReadInstruction());    FlattenInstruction flattenInstruction = new FlattenInstruction();    flattenInstruction.setInputs(ImmutableList.of(    createInstructionInput(0, 0),     createInstructionInput(0, 0),     createInstructionInput(0, 0)));    InstructionOutput flattenOutput = createInstructionOutput("Flatten.out");    ParallelInstruction flatten = createParallelInstruction("Flatten", flattenOutput);    flatten.setFlatten(flattenInstruction);    MapTask mapTask = new MapTask();    mapTask.setInstructions(ImmutableList.of(read, flatten));    mapTask.setFactory(Transport.getJsonFactory());    Network<Node, Edge> network = new MapTaskToNetworkFunction(IdGenerators.decrementingLongs()).apply(mapTask);    assertNetworkProperties(network);    assertEquals(4, network.nodes().size());    assertEquals(5, network.edges().size());    ParallelInstructionNode readNode = get(network, read);    InstructionOutputNode readOutputNode = getOnlySuccessor(network, readNode);    assertEquals(readOutput, readOutputNode.getInstructionOutput());    ParallelInstructionNode flattenNode = getOnlySuccessor(network, readOutputNode);        assertEquals(3, network.edgesConnecting(readOutputNode, flattenNode).size());    InstructionOutputNode flattenOutputNode = getOnlySuccessor(network, flattenNode);    assertEquals(flattenOutput, flattenOutputNode.getInstructionOutput());}
public void beam_f10141_0()
{    InstructionOutput readOutput = createInstructionOutput("Read.out");    ParallelInstruction read = createParallelInstruction("Read", readOutput);    read.setRead(new ReadInstruction());    WriteInstruction writeInstruction = new WriteInstruction();        writeInstruction.setInput(createInstructionInput(0, 0));    ParallelInstruction write = createParallelInstruction("Write");    write.setWrite(writeInstruction);    MapTask mapTask = new MapTask();    mapTask.setInstructions(ImmutableList.of(read, write));    mapTask.setFactory(Transport.getJsonFactory());    Network<Node, Edge> network = new MapTaskToNetworkFunction(IdGenerators.decrementingLongs()).apply(mapTask);    assertNetworkProperties(network);    assertEquals(3, network.nodes().size());    assertEquals(2, network.edges().size());    ParallelInstructionNode readNode = get(network, read);    InstructionOutputNode readOutputNode = getOnlySuccessor(network, readNode);    assertEquals(readOutput, readOutputNode.getInstructionOutput());    ParallelInstructionNode writeNode = getOnlySuccessor(network, readOutputNode);    assertNotNull(writeNode);}
private static InstructionOutput beam_f10149_0(String name)
{    InstructionOutput rval = new InstructionOutput();    rval.setName(name);    return rval;}
private static ParallelInstruction beam_f10150_0(String name, InstructionOutput... outputs)
{    ParallelInstruction rval = new ParallelInstruction();    rval.setName(name);    rval.setOutputs(Arrays.asList(outputs));    return rval;}
private static MultiOutputInfo beam_f10151_0(String tag)
{    MultiOutputInfo rval = new MultiOutputInfo();    rval.setTag(tag);    return rval;}
public void beam_f10159_0()
{    MutableNetwork<String, String> network = createEmptyNetwork();    Networks.replaceDirectedNetworkNodes(network, new Function<String, String>() {        @Override        @Nullable        public String apply(@Nullable String input) {            return input.toLowerCase();        }    });    assertThat(network.nodes(), empty());}
public String beam_f10160_0(@Nullable String input)
{    return input.toLowerCase();}
public void beam_f10161_0()
{    Function<String, String> function = new Function<String, String>() {        @Override        public String apply(String input) {            return "E".equals(input) || "J".equals(input) || "M".equals(input) || "O".equals(input) ? input.toLowerCase() : input;        }    };    Function<String, String> spiedFunction = spy(function);    MutableNetwork<String, String> network = createNetwork();    Networks.replaceDirectedNetworkNodes(network, spiedFunction);    MutableNetwork<String, String> originalNetwork = createNetwork();    for (String node : originalNetwork.nodes()) {        assertEquals(originalNetwork.successors(node).stream().map(function).collect(Collectors.toCollection(HashSet::new)), network.successors(function.apply(node)));                verify(spiedFunction, times(1)).apply(node);    }    assertEquals(network.nodes(), originalNetwork.nodes().stream().map(function).collect(Collectors.toCollection(HashSet::new)));}
public void beam_f10169_0()
{    Operation param = mock(Operation.class);    assertSame(param, OperationNode.create(param).getOperation());    assertNotEquals(OperationNode.create(param), OperationNode.create(param));}
public void beam_f10170_0()
{    BeamFnApi.RegisterRequest param = BeamFnApi.RegisterRequest.getDefaultInstance();    Map<String, NameContext> nameContexts = ImmutableMap.of("ABC", NameContext.create(null, "originalName", "systemName", "userName"));    Map<String, Iterable<SideInputInfo>> sideInputInfos = ImmutableMap.of("DEF", ImmutableList.of(new SideInputInfo()));    Map<String, Iterable<PCollectionView<?>>> pcollectionViews = ImmutableMap.of("GHI", ImmutableList.of(DataflowPortabilityPCollectionView.with(new TupleTag<>("JKL"), FullWindowedValueCoder.of(KvCoder.of(StringUtf8Coder.of(), StringUtf8Coder.of()), GlobalWindow.Coder.INSTANCE))));    assertSame(param, RegisterRequestNode.create(param, nameContexts, sideInputInfos, pcollectionViews, ImmutableMap.of()).getRegisterRequest());    assertSame(nameContexts, RegisterRequestNode.create(param, nameContexts, sideInputInfos, pcollectionViews, ImmutableMap.of()).getPTransformIdToPartialNameContextMap());    assertSame(sideInputInfos, RegisterRequestNode.create(param, nameContexts, sideInputInfos, pcollectionViews, ImmutableMap.of()).getPTransformIdToSideInputInfoMap());    assertSame(pcollectionViews, RegisterRequestNode.create(param, nameContexts, sideInputInfos, pcollectionViews, ImmutableMap.of()).getPTransformIdToPCollectionViewMap());    assertNotEquals(RegisterRequestNode.create(param, nameContexts, sideInputInfos, pcollectionViews, ImmutableMap.of()), RegisterRequestNode.create(param, nameContexts, sideInputInfos, pcollectionViews, ImmutableMap.of()));}
public void beam_f10171_0()
{    WindowingStrategy windowingStrategy = WindowingStrategy.globalDefault();    Map<PCollectionView<?>, RunnerApi.SdkFunctionSpec> pcollectionViewsToWindowMappingFns = ImmutableMap.of(mock(PCollectionView.class), SdkFunctionSpec.newBuilder().setSpec(FunctionSpec.newBuilder().setUrn("beam:test:urn:1.0")).build());    NameContext nameContext = NameContextsForTests.nameContextForTest();    assertSame(FetchAndFilterStreamingSideInputsNode.create(windowingStrategy, pcollectionViewsToWindowMappingFns, nameContext).getWindowingStrategy(), windowingStrategy);    assertSame(FetchAndFilterStreamingSideInputsNode.create(windowingStrategy, pcollectionViewsToWindowMappingFns, nameContext).getPCollectionViewsToWindowMappingFns(), pcollectionViewsToWindowMappingFns);    assertSame(FetchAndFilterStreamingSideInputsNode.create(windowingStrategy, pcollectionViewsToWindowMappingFns, nameContext).getNameContext(), nameContext);}
private static MutableNetwork<Node, Edge> beam_f10179_0()
{    return NetworkBuilder.directed().allowsSelfLoops(false).allowsParallelEdges(true).<Node, Edge>build();}
public void beam_f10180_0() throws Exception
{            Map<String, Object> valueCombiningFn = new HashMap<>();    Node out1 = createInstructionOutputNode("out1");    String pgbkName = "precombine_pgbk";    Node precombinePgbk = createPrecombinePgbkNode(pgbkName, valueCombiningFn);    Node out2 = createInstructionOutputNode("out2");    MutableNetwork<Node, Edge> network = createEmptyNetwork();    network.addNode(out1);    network.addNode(precombinePgbk);    network.addNode(out2);    network.addEdge(out1, precombinePgbk, DefaultEdge.create());    network.addEdge(precombinePgbk, out2, DefaultEdge.create());    Network<Node, Edge> inputNetwork = ImmutableNetwork.copyOf(network);    network = new ReplacePgbkWithPrecombineFunction().apply(network);        assertEquals(inputNetwork.nodes().size(), network.nodes().size());    assertEquals(inputNetwork.edges().size(), network.edges().size());    List<List<Node>> oldPaths = Networks.allPathsFromRootsToLeaves(inputNetwork);    List<List<Node>> newPaths = Networks.allPathsFromRootsToLeaves(network);    assertEquals(oldPaths.size(), newPaths.size());        for (Node node : network.nodes()) {        if (node instanceof ParallelInstructionNode) {            ParallelInstructionNode createdCombineNode = (ParallelInstructionNode) node;            ParallelInstruction parallelInstruction = createdCombineNode.getParallelInstruction();            assertEquals(parallelInstruction.getName(), pgbkName);            assertNull(parallelInstruction.getPartialGroupByKey());            assertNotNull(parallelInstruction.getParDo());            ParDoInstruction parDoInstruction = parallelInstruction.getParDo();            assertEquals(parDoInstruction.getUserFn(), valueCombiningFn);            break;        }    }}
public void beam_f10181_0() throws Exception
{            Node out1 = createInstructionOutputNode("out1");    Node pgbk = createPrecombinePgbkNode("pgbk", null);    Node out2 = createInstructionOutputNode("out2");    MutableNetwork<Node, Edge> network = createEmptyNetwork();    network.addNode(out1);    network.addNode(pgbk);    network.addNode(out2);    network.addEdge(out1, pgbk, DefaultEdge.create());    network.addEdge(pgbk, out2, DefaultEdge.create());    Network<Node, Edge> inputNetwork = ImmutableNetwork.copyOf(network);    network = new ReplacePgbkWithPrecombineFunction().apply(network);        List<List<Node>> oldPaths = Networks.allPathsFromRootsToLeaves(inputNetwork);    List<List<Node>> newPaths = Networks.allPathsFromRootsToLeaves(network);    assertThat(oldPaths, containsInAnyOrder(newPaths.toArray()));        for (Node node : network.nodes()) {        if (node instanceof ParallelInstructionNode) {            ParallelInstructionNode newNode = (ParallelInstructionNode) node;            ParallelInstruction parallelInstruction = newNode.getParallelInstruction();            assertEquals(parallelInstruction, ((ParallelInstructionNode) pgbk).getParallelInstruction());            break;        }    }}
public void beam_f10189_0() throws IOException
{    trackerCleanup.close();}
private List<ShuffleEntry> beam_f10190_0(List<KV<Integer, List<KV<Integer, Integer>>>> input, boolean sortValues) throws Exception
{    Coder<WindowedValue<KV<Integer, KV<Integer, Integer>>>> sinkElemCoder = WindowedValue.getFullCoder(KvCoder.of(BigEndianIntegerCoder.of(), KvCoder.of(BigEndianIntegerCoder.of(), BigEndianIntegerCoder.of())), IntervalWindow.getCoder());        BatchModeExecutionContext executionContext = BatchModeExecutionContext.forTesting(PipelineOptionsFactory.create(), "STAGE");    ShuffleSink<KV<Integer, KV<Integer, Integer>>> shuffleSink = new ShuffleSink<>(PipelineOptionsFactory.create(), null, sortValues ? ShuffleKind.GROUP_KEYS_AND_SORT_VALUES : ShuffleKind.GROUP_KEYS, sinkElemCoder, executionContext, TestOperationContext.create());    TestShuffleWriter shuffleWriter = new TestShuffleWriter();    int kvCount = 0;    List<Long> actualSizes = new ArrayList<>();    try (Sink.SinkWriter<WindowedValue<KV<Integer, KV<Integer, Integer>>>> shuffleSinkWriter = shuffleSink.writer(shuffleWriter, "dataset")) {        for (KV<Integer, List<KV<Integer, Integer>>> kvs : input) {            Integer key = kvs.getKey();            for (KV<Integer, Integer> value : kvs.getValue()) {                ++kvCount;                actualSizes.add(shuffleSinkWriter.add(WindowedValue.of(KV.of(key, value), timestamp, Lists.newArrayList(window), PaneInfo.NO_FIRING)));            }        }    }    List<ShuffleEntry> records = shuffleWriter.getRecords();    assertEquals(kvCount, records.size());    assertEquals(shuffleWriter.getSizes(), actualSizes);    return records;}
private List<KV<Integer, List<KV<Integer, Integer>>>> beam_f10191_0(BatchModeExecutionContext context, TestShuffleReader shuffleReader, GroupingShuffleReader<Integer, KV<Integer, Integer>> groupingShuffleReader, Coder<WindowedValue<KV<Integer, Iterable<KV<Integer, Integer>>>>> coder, ValuesToRead valuesToRead) throws Exception
{    CounterSet counterSet = new CounterSet();    Counter<Long, ?> elementByteSizeCounter = counterSet.longSum(CounterName.named("element-byte-size-counter"));    CounterBackedElementByteSizeObserver elementObserver = new CounterBackedElementByteSizeObserver(elementByteSizeCounter);    List<KV<Integer, List<KV<Integer, Integer>>>> actual = new ArrayList<>();    assertFalse(shuffleReader.isClosed());    try (GroupingShuffleReaderIterator<Integer, KV<Integer, Integer>> iter = groupingShuffleReader.iterator(shuffleReader)) {        Iterable<KV<Integer, Integer>> prevValuesIterable = null;        Iterator<KV<Integer, Integer>> prevValuesIterator = null;        for (boolean more = iter.start(); more; more = iter.advance()) {                        iter.getCurrent();            iter.getCurrent();                        @SuppressWarnings({ "rawtypes", "unchecked" })            WindowedValue<KV<Integer, Iterable<KV<Integer, Integer>>>> windowedValue = (WindowedValue) iter.getCurrent();                                    coder.registerByteSizeObserver(windowedValue, elementObserver);            assertTrue(elementObserver.getIsLazy());                        assertEquals(BoundedWindow.TIMESTAMP_MIN_VALUE, windowedValue.getTimestamp());            assertEquals(0, windowedValue.getWindows().size());            KV<Integer, Iterable<KV<Integer, Integer>>> elem = windowedValue.getValue();            Integer key = elem.getKey();            List<KV<Integer, Integer>> values = new ArrayList<>();            if (valuesToRead.ordinal() > ValuesToRead.SKIP_VALUES.ordinal()) {                if (prevValuesIterable != null) {                                        prevValuesIterable.iterator();                }                if (prevValuesIterator != null) {                                        prevValuesIterator.hasNext();                }                Iterable<KV<Integer, Integer>> valuesIterable = elem.getValue();                Iterator<KV<Integer, Integer>> valuesIterator = valuesIterable.iterator();                if (valuesToRead.ordinal() >= ValuesToRead.READ_ONE_VALUE.ordinal()) {                    while (valuesIterator.hasNext()) {                        assertTrue(valuesIterator.hasNext());                        assertTrue(valuesIterator.hasNext());                        assertEquals("BatchModeExecutionContext key", key, context.getKey());                        values.add(valuesIterator.next());                        if (valuesToRead == ValuesToRead.READ_ONE_VALUE) {                            break;                        }                    }                    if (valuesToRead.ordinal() >= ValuesToRead.READ_ALL_VALUES.ordinal()) {                        assertFalse(valuesIterator.hasNext());                        assertFalse(valuesIterator.hasNext());                        try {                            valuesIterator.next();                            fail("Expected NoSuchElementException");                        } catch (NoSuchElementException exn) {                                                }                                                valuesIterable.iterator();                    }                }                if (valuesToRead == ValuesToRead.READ_ALL_VALUES_TWICE) {                                        valuesIterator = valuesIterable.iterator();                    while (valuesIterator.hasNext()) {                        assertTrue(valuesIterator.hasNext());                        assertTrue(valuesIterator.hasNext());                        assertEquals("BatchModeExecutionContext key", key, context.getKey());                        valuesIterator.next();                    }                    assertFalse(valuesIterator.hasNext());                    assertFalse(valuesIterator.hasNext());                    try {                        valuesIterator.next();                        fail("Expected NoSuchElementException");                    } catch (NoSuchElementException exn) {                                        }                }                prevValuesIterable = valuesIterable;                prevValuesIterator = valuesIterator;            }            actual.add(KV.of(key, values));        }        assertFalse(iter.advance());        assertFalse(iter.advance());        try {            iter.getCurrent();            fail("Expected NoSuchElementException");        } catch (NoSuchElementException exn) {                }    }    assertTrue(shuffleReader.isClosed());    return actual;}
public void beam_f10199_0() throws Exception
{    runTestReadFromShuffle(KVS, false, /* do not sort values */    ValuesToRead.SKIP_VALUES);    runTestReadFromShuffle(KVS, true, /* sort values */    ValuesToRead.SKIP_VALUES);}
private void beam_f10200_0(TestShuffleReadCounterFactory factory, long expectedReadBytes)
{    Map<String, Long> expectedReadBytesMap = new HashMap<>();    expectedReadBytesMap.put(MOCK_ORIGINAL_NAME_FOR_EXECUTING_STEP1, expectedReadBytes);    expectShuffleReadCounterEquals(factory, expectedReadBytesMap);}
private void beam_f10201_0(TestShuffleReadCounterFactory factory, Map<String, Long> expectedReadBytesForOriginal)
{    ShuffleReadCounter src = factory.getOnlyShuffleReadCounterOrNull();    assertNotNull(src);        if (src.legacyPerOperationPerDatasetBytesCounter != null) {        assertEquals(0, (long) src.legacyPerOperationPerDatasetBytesCounter.getAggregate());    }            assertEquals(expectedReadBytesForOriginal.size(), (long) src.counterSet.size());    Iterator it = expectedReadBytesForOriginal.entrySet().iterator();    while (it.hasNext()) {        Map.Entry<String, Long> pair = (Map.Entry) it.next();        Counter counter = src.counterSet.getExistingCounter(ShuffleReadCounter.generateCounterName(ORIGINAL_SHUFFLE_STEP_NAME, pair.getKey()));        assertEquals(pair.getValue(), counter.getAggregate());    }}
public void beam_f10209_0() throws Exception
{    runTestBytesReadCounter(KVS, true, /* sort values */    ValuesToRead.READ_ONE_VALUE, 200L);}
public void beam_f10210_0() throws Exception
{    runTestBytesReadCounter(KVS, false, /* do not sort values */    ValuesToRead.SKIP_VALUES, 200L);}
public void beam_f10211_0() throws Exception
{    runTestBytesReadCounter(KVS, true, /* sort values */    ValuesToRead.SKIP_VALUES, 200L);}
private Position beam_f10219_0(byte[] position) throws Exception
{    return new Position().setShufflePosition(encodeBase64URLSafeString(position));}
public void beam_f10220_0() throws Exception
{    PipelineOptions options = PipelineOptionsFactory.create();    BatchModeExecutionContext context = BatchModeExecutionContext.forTesting(options, "testStage");    TestOperationContext operationContext = TestOperationContext.create();    GroupingShuffleReader<Integer, Integer> groupingShuffleReader = new GroupingShuffleReader<>(options, null, null, null, WindowedValue.getFullCoder(KvCoder.of(BigEndianIntegerCoder.of(), IterableCoder.of(BigEndianIntegerCoder.of())), IntervalWindow.getCoder()), context, operationContext, ShuffleReadCounterFactory.INSTANCE, false);    groupingShuffleReader.perOperationPerDatasetBytesCounter = operationContext.counterFactory().longSum(CounterName.named("dax-shuffle-test-wf-read-bytes"));    TestShuffleReader shuffleReader = new TestShuffleReader();    final int kNumRecords = 10;    final int kFirstShard = 0;    final int kSecondShard = 1;        for (int i = 0; i < kNumRecords; ++i) {        byte[] keyByte = CoderUtils.encodeToByteArray(BigEndianIntegerCoder.of(), i);        ShuffleEntry entry = new ShuffleEntry(fabricatePosition(kFirstShard, keyByte), keyByte, EMPTY_BYTE_ARRAY, keyByte);        shuffleReader.addEntry(entry);    }    for (int i = kNumRecords; i < 2 * kNumRecords; ++i) {        byte[] keyByte = CoderUtils.encodeToByteArray(BigEndianIntegerCoder.of(), i);        ShuffleEntry entry = new ShuffleEntry(fabricatePosition(kSecondShard, keyByte), keyByte, EMPTY_BYTE_ARRAY, keyByte);        shuffleReader.addEntry(entry);    }    int i = 0;    assertFalse(shuffleReader.isClosed());    try (GroupingShuffleReaderIterator<Integer, Integer> iter = groupingShuffleReader.iterator(shuffleReader)) {                assertTrue(iter.start());        ++i;        assertNull(iter.requestDynamicSplit(splitRequestAtPosition(new Position())));                NativeReader.DynamicSplitResult dynamicSplitResult = iter.requestDynamicSplit(splitRequestAtPosition(makeShufflePosition(kSecondShard, null)));        assertNotNull(dynamicSplitResult);        assertEquals(encodeBase64URLSafeString(fabricatePosition(kSecondShard).getPosition()), positionFromSplitResult(dynamicSplitResult).getShufflePosition());        for (; iter.advance(); ++i) {                                    iter.getCurrent();            iter.getCurrent();            KV<Integer, Reiterable<Integer>> elem = iter.getCurrent().getValue();            int key = elem.getKey();            assertEquals(key, i);            Reiterable<Integer> valuesIterable = elem.getValue();            Reiterator<Integer> valuesIterator = valuesIterable.iterator();            int j = 0;            while (valuesIterator.hasNext()) {                assertTrue(valuesIterator.hasNext());                assertTrue(valuesIterator.hasNext());                int value = valuesIterator.next();                assertEquals(value, i);                ++j;            }            assertFalse(valuesIterator.hasNext());            assertFalse(valuesIterator.hasNext());            assertEquals(1, j);        }        assertFalse(iter.advance());    }    assertTrue(shuffleReader.isClosed());    assertEquals(i, kNumRecords);            assertEquals(80L, (long) groupingShuffleReader.perOperationPerDatasetBytesCounter.getAggregate());}
public void beam_f10221_0() throws Exception
{        List<ByteArrayShufflePosition> positionsList = new ArrayList<>();    PipelineOptions options = PipelineOptionsFactory.create();    BatchModeExecutionContext context = BatchModeExecutionContext.forTesting(options, "testStage");    TestOperationContext operationContext = TestOperationContext.create();    GroupingShuffleReader<Integer, Integer> groupingShuffleReader = new GroupingShuffleReader<>(options, null, null, null, WindowedValue.getFullCoder(KvCoder.of(BigEndianIntegerCoder.of(), IterableCoder.of(BigEndianIntegerCoder.of())), IntervalWindow.getCoder()), context, operationContext, ShuffleReadCounterFactory.INSTANCE, false);    TestShuffleReader shuffleReader = new TestShuffleReader();    final int kNumRecords = 10;    for (int i = 0; i < kNumRecords; ++i) {        ByteArrayShufflePosition position = fabricatePosition(i);        byte[] keyByte = CoderUtils.encodeToByteArray(BigEndianIntegerCoder.of(), i);        positionsList.add(position);        ShuffleEntry entry = new ShuffleEntry(position, keyByte, EMPTY_BYTE_ARRAY, keyByte);        shuffleReader.addEntry(entry);    }    assertFalse(shuffleReader.isClosed());    try (GroupingShuffleReaderIterator<Integer, Integer> iter = groupingShuffleReader.iterator(shuffleReader)) {        Integer i = 0;        for (boolean more = iter.start(); more; more = iter.advance()) {            ApproximateReportedProgress progress = readerProgressToCloudProgress(iter.getProgress());            assertNotNull(progress.getPosition().getShufflePosition());                        assertEquals(positionsList.get(i).encodeBase64(), progress.getPosition().getShufflePosition());            WindowedValue<KV<Integer, Reiterable<Integer>>> elem = iter.getCurrent();            assertEquals(i, elem.getValue().getKey());            i++;        }        assertFalse(iter.advance());                Position proposedSplitPosition = new Position();        String stop = encodeBase64URLSafeString(fabricatePosition(0).getPosition());        proposedSplitPosition.setShufflePosition(stop);        assertNull(iter.requestDynamicSplit(toDynamicSplitRequest(approximateSplitRequestAtPosition(proposedSplitPosition))));    }    assertTrue(shuffleReader.isClosed());}
public void beam_f10229_0() throws Exception
{    runTestCreateInMemoryReader(Arrays.asList("hi", "there", "bob"), null, null, 0, 3, StringUtf8Coder.of());}
public void beam_f10230_0() throws Exception
{    runTestCreateInMemoryReader(Arrays.asList(33, 44, 55, 66, 77, 88), 1L, 3L, 1, 3, BigEndianIntegerCoder.of());}
 static List<String> beam_f10231_0(List<T> elements, Coder<T> coder) throws Exception
{    List<String> encodedElements = new ArrayList<>();    for (T element : elements) {        byte[] encodedElement = encodeToByteArray(coder, element);        String encodedElementString = byteArrayToJsonString(encodedElement);        encodedElements.add(encodedElementString);    }    return encodedElements;}
public void beam_f10239_0() throws Exception
{    runTestReadInMemory(Arrays.asList(33, 44, 55, 66, 77, 88), 2, 2, Arrays.<Integer>asList(), Arrays.<Integer>asList(), BigEndianIntegerCoder.of());}
public void beam_f10240_0() throws Exception
{    runTestReadInMemory(Arrays.<Integer>asList(), null, null, Arrays.<Integer>asList(), Arrays.<Integer>asList(), BigEndianIntegerCoder.of());}
public void beam_f10241_0() throws Exception
{    runTestReadInMemory(Arrays.<Integer>asList(), 0, 0, Arrays.<Integer>asList(), Arrays.<Integer>asList(), BigEndianIntegerCoder.of());}
 static ParallelInstruction beam_f10249_0(String name)
{    return createReadInstruction(name, ReaderFactoryTest.TestReaderFactory.class);}
 static ParallelInstruction beam_f10250_0(String name, Class<? extends ReaderFactory> readerFactoryClass)
{    CloudObject spec = CloudObject.forClass(readerFactoryClass);    Source cloudSource = new Source();    cloudSource.setSpec(spec);    cloudSource.setCodec(windowedStringCoder);    ReadInstruction readInstruction = new ReadInstruction();    readInstruction.setSource(cloudSource);    InstructionOutput output = new InstructionOutput();    output.setName("read_output_name");    output.setCodec(windowedStringCoder);    output.setOriginalName("originalName");    output.setSystemName("systemName");    ParallelInstruction instruction = new ParallelInstruction();    instruction.setSystemName(name);    instruction.setOriginalName(name + "OriginalName");    instruction.setRead(readInstruction);    instruction.setOutputs(Arrays.asList(output));    return instruction;}
public void beam_f10251_0() throws Exception
{    ParallelInstructionNode instructionNode = ParallelInstructionNode.create(createReadInstruction("Read"), ExecutionLocation.UNKNOWN);    when(network.successors(instructionNode)).thenReturn(ImmutableSet.<Node>of(IntrinsicMapTaskExecutorFactory.createOutputReceiversTransform(STAGE, counterSet).apply(InstructionOutputNode.create(instructionNode.getParallelInstruction().getOutputs().get(0), PCOLLECTION_ID))));    when(network.outDegree(instructionNode)).thenReturn(1);    Node operationNode = mapTaskExecutorFactory.createOperationTransformForParallelInstructionNodes(STAGE, network, PipelineOptionsFactory.create(), readerRegistry, sinkRegistry, BatchModeExecutionContext.forTesting(options, counterSet, "testStage")).apply(instructionNode);    assertThat(operationNode, instanceOf(OperationNode.class));    assertThat(((OperationNode) operationNode).getOperation(), instanceOf(ReadOperation.class));    ReadOperation readOperation = (ReadOperation) ((OperationNode) operationNode).getOperation();    assertEquals(1, readOperation.receivers.length);    assertEquals(0, readOperation.receivers[0].getReceiverCount());    assertEquals(Operation.InitializationState.UNSTARTED, readOperation.initializationState);    assertThat(readOperation.reader, instanceOf(ReaderFactoryTest.TestReader.class));    counterSet.extractUpdates(false, updateExtractor);    verifyOutputCounters(updateExtractor, "read_output_name");    verify(updateExtractor).longSum(eq(named("Read-ByteCount")), anyBoolean(), anyLong());    verifyNoMoreInteractions(updateExtractor);}
 static ParallelInstruction beam_f10260_0(int producerIndex, int producerOutputNum, String systemName)
{    return createParDoInstruction(producerIndex, producerOutputNum, systemName, "");}
 static ParallelInstruction beam_f10261_0(int producerIndex, int producerOutputNum, String systemName, String userName)
{    InstructionInput cloudInput = new InstructionInput();    cloudInput.setProducerInstructionIndex(producerIndex);    cloudInput.setOutputNum(producerOutputNum);    TestDoFn fn = new TestDoFn();    String serializedFn = StringUtils.byteArrayToJsonString(SerializableUtils.serializeToByteArray(DoFnInfo.forFn(fn, WindowingStrategy.globalDefault(), null, /* side input views */    null, /* input coder */    new TupleTag<>(PropertyNames.OUTPUT), /* main output id */    DoFnSchemaInformation.create(), Collections.emptyMap())));    CloudObject cloudUserFn = CloudObject.forClassName("DoFn");    addString(cloudUserFn, PropertyNames.SERIALIZED_FN, serializedFn);    MultiOutputInfo mainOutputTag = new MultiOutputInfo();    mainOutputTag.setTag("1");    ParDoInstruction parDoInstruction = new ParDoInstruction();    parDoInstruction.setInput(cloudInput);    parDoInstruction.setNumOutputs(1);    parDoInstruction.setMultiOutputInfos(ImmutableList.of(mainOutputTag));    parDoInstruction.setUserFn(cloudUserFn);    InstructionOutput output = new InstructionOutput();    output.setName(systemName + "_output");    output.setCodec(windowedStringCoder);    output.setOriginalName("originalName");    output.setSystemName("systemName");    ParallelInstruction instruction = new ParallelInstruction();    instruction.setParDo(parDoInstruction);    instruction.setOutputs(Arrays.asList(output));    instruction.setSystemName(systemName);    instruction.setOriginalName(systemName + "OriginalName");    instruction.setName(userName);    return instruction;}
public void beam_f10262_0() throws Exception
{    int producerIndex = 1;    int producerOutputNum = 2;    BatchModeExecutionContext context = BatchModeExecutionContext.forTesting(options, counterSet, "testStage");    ParallelInstructionNode instructionNode = ParallelInstructionNode.create(createParDoInstruction(producerIndex, producerOutputNum, "DoFn"), ExecutionLocation.UNKNOWN);    Node outputReceiverNode = IntrinsicMapTaskExecutorFactory.createOutputReceiversTransform(STAGE, counterSet).apply(InstructionOutputNode.create(instructionNode.getParallelInstruction().getOutputs().get(0), PCOLLECTION_ID));    when(network.successors(instructionNode)).thenReturn(ImmutableSet.of(outputReceiverNode));    when(network.outDegree(instructionNode)).thenReturn(1);    when(network.edgesConnecting(instructionNode, outputReceiverNode)).thenReturn(ImmutableSet.<Edge>of(MultiOutputInfoEdge.create(instructionNode.getParallelInstruction().getParDo().getMultiOutputInfos().get(0))));    Node operationNode = mapTaskExecutorFactory.createOperationTransformForParallelInstructionNodes(STAGE, network, options, readerRegistry, sinkRegistry, context).apply(instructionNode);    assertThat(operationNode, instanceOf(OperationNode.class));    assertThat(((OperationNode) operationNode).getOperation(), instanceOf(ParDoOperation.class));    ParDoOperation parDoOperation = (ParDoOperation) ((OperationNode) operationNode).getOperation();    assertEquals(1, parDoOperation.receivers.length);    assertEquals(0, parDoOperation.receivers[0].getReceiverCount());    assertEquals(Operation.InitializationState.UNSTARTED, parDoOperation.initializationState);}
public NativeReader.DynamicSplitResult beam_f10270_0(NativeReader.DynamicSplitRequest splitRequest)
{        return new NativeReader.DynamicSplitResultWithPosition(cloudPositionToReaderPosition(splitRequestToApproximateSplitRequest(splitRequest).getPosition()));}
public void beam_f10271_0(ApproximateReportedProgress progress)
{    this.progress = progress;}
public void beam_f10272_0() throws Exception
{    List<String> log = new ArrayList<>();    Operation o1 = Mockito.mock(Operation.class);    Operation o2 = Mockito.mock(Operation.class);    Operation o3 = Mockito.mock(Operation.class);    List<Operation> operations = Arrays.asList(new Operation[] { o1, o2, o3 });    ExecutionStateTracker stateTracker = Mockito.mock(ExecutionStateTracker.class);    try (IntrinsicMapTaskExecutor executor = IntrinsicMapTaskExecutor.withSharedCounterSet(operations, counterSet, stateTracker)) {        executor.execute();    }    InOrder inOrder = Mockito.inOrder(stateTracker, o1, o2, o3);    inOrder.verify(o3).start();    inOrder.verify(o2).start();    inOrder.verify(o1).start();    inOrder.verify(o1).finish();    inOrder.verify(o2).finish();    inOrder.verify(o3).finish();}
public void beam_f10285_0() throws Exception
{    super.start();    try (Closeable scope = context.enterStart()) {        Metrics.counter("TestMetric", "MetricCounter").inc(1L);    }}
public void beam_f10286_0() throws Exception
{    super.start();    try (Closeable scope = context.enterStart()) {        Metrics.counter("TestMetric", "MetricCounter").inc(2L);    }}
public void beam_f10287_0() throws Exception
{    super.start();    try (Closeable scope = context.enterStart()) {        Metrics.counter("TestMetric", "MetricCounter").inc(3L);    }}
public void beam_f10295_0() throws Exception
{    expectedException.expect(IllegalArgumentException.class);    expectedException.expectMessage("is expected to be deterministic");    IsmFormat.validateCoderIsCompatible(IsmRecordCoder.of(    1,     0, ImmutableList.<Coder<?>>of(NON_DETERMINISTIC_CODER, ByteArrayCoder.of()), ByteArrayCoder.of()));}
public void beam_f10296_0() throws Exception
{    expectedException.expect(IllegalArgumentException.class);    expectedException.expectMessage("is expected to be deterministic");    IsmFormat.validateCoderIsCompatible(IsmRecordCoder.of(    1,     0, ImmutableList.<Coder<?>>of(ByteArrayCoder.of(), NON_DETERMINISTIC_CODER), ByteArrayCoder.of()));}
public void beam_f10297_0() throws Exception
{    KeyPrefix keyPrefixA = KeyPrefix.of(5, 7);    KeyPrefix keyPrefixB = KeyPrefix.of(5, 7);    CoderProperties.coderDecodeEncodeEqual(KeyPrefixCoder.of(), keyPrefixA);    CoderProperties.coderDeterministic(KeyPrefixCoder.of(), keyPrefixA, keyPrefixB);    CoderProperties.coderConsistentWithEquals(KeyPrefixCoder.of(), keyPrefixA, keyPrefixB);    CoderProperties.coderSerializable(KeyPrefixCoder.of());    CoderProperties.structuralValueConsistentWithEquals(KeyPrefixCoder.of(), keyPrefixA, keyPrefixB);    assertTrue(KeyPrefixCoder.of().isRegisterByteSizeObserverCheap(keyPrefixA));    assertEquals(2, KeyPrefixCoder.of().getEncodedElementByteSize(keyPrefixA));}
public void beam_f10305_0() throws Exception
{    IsmRecordCoder<String> coderWithMetadata = IsmRecordCoder.of(2, 2, ImmutableList.<Coder<?>>of(MetadataKeyCoder.of(StringUtf8Coder.of()), StringUtf8Coder.of()), StringUtf8Coder.of());    expectedException.expect(IllegalArgumentException.class);    expectedException.expectMessage("Expected at least");    coderWithMetadata.hash(ImmutableList.of(IsmFormat.getMetadataKey()));}
public void beam_f10306_0() throws Exception
{    IsmRecord<String> ismRecord = IsmRecord.of(ImmutableList.of("0", "too many"), "1");    IsmRecordCoder<String> coder = IsmRecordCoder.of(1, 0, ImmutableList.<Coder<?>>of(StringUtf8Coder.of()), StringUtf8Coder.of());    expectedException.expect(CoderException.class);    expectedException.expectMessage("Expected 1 key component(s) but received key");    coder.encode(ismRecord, new ByteArrayOutputStream());}
public void beam_f10307_0()
{    IsmRecord<String> ismRecordA = IsmRecord.of(ImmutableList.of("0"), "1");    IsmRecord<String> ismRecordB = IsmRecord.of(ImmutableList.of("0"), "1");    IsmRecord<String> ismRecordC = IsmRecord.of(ImmutableList.of("3"), "4");    IsmRecord<String> ismRecordAWithMeta = IsmRecord.meta(ImmutableList.of(IsmFormat.getMetadataKey(), "0"), "2".getBytes(StandardCharsets.UTF_8));    IsmRecord<String> ismRecordBWithMeta = IsmRecord.meta(ImmutableList.of(IsmFormat.getMetadataKey(), "0"), "2".getBytes(StandardCharsets.UTF_8));    IsmRecord<String> ismRecordCWithMeta = IsmRecord.meta(ImmutableList.of(IsmFormat.getMetadataKey(), "0"), "5".getBytes(StandardCharsets.UTF_8));    assertEquals(ismRecordA, ismRecordB);    assertEquals(ismRecordAWithMeta, ismRecordBWithMeta);    assertNotEquals(ismRecordA, ismRecordAWithMeta);    assertNotEquals(ismRecordA, ismRecordC);    assertNotEquals(ismRecordAWithMeta, ismRecordCWithMeta);    assertEquals(ismRecordA.hashCode(), ismRecordB.hashCode());    assertEquals(ismRecordAWithMeta.hashCode(), ismRecordBWithMeta.hashCode());    assertNotEquals(ismRecordA.hashCode(), ismRecordAWithMeta.hashCode());    assertNotEquals(ismRecordA.hashCode(), ismRecordC.hashCode());    assertNotEquals(ismRecordAWithMeta.hashCode(), ismRecordCWithMeta.hashCode());    assertThat(ismRecordA.toString(), allOf(containsString("keyComponents=[0]"), containsString("value=1")));    assertThat(ismRecordAWithMeta.toString(), allOf(containsString("keyComponents=[META, 0]"), containsString("metadata=")));}
private CloudObject beam_f10315_0(String filename)
{    return CloudObject.fromSpec(ImmutableMap.<String, Object>of(PropertyNames.OBJECT_TYPE_NAME, "IsmSource", WorkerPropertyNames.FILENAME, filename));}
public void beam_f10316_0()
{    cache = CacheBuilder.newBuilder().weigher(Weighers.fixedWeightKeys(1)).maximumWeight(10_000).build();    executionContext = BatchModeExecutionContext.forTesting(PipelineOptionsFactory.as(DataflowPipelineOptions.class), "testStage");    DataflowExecutionState state = executionContext.getExecutionStateRegistry().getState(NameContextsForTests.nameContextForTest(), "test", null, NoopProfileScope.NOOP);    operationContext = executionContext.createOperationContext(NameContextsForTests.nameContextForTest());    stateCloseable = executionContext.getExecutionStateTracker().enterState(state);    sideInputReadCounter = new DataflowSideInputReadCounter(executionContext, operationContext, 1);}
public void beam_f10317_0() throws IOException
{    stateCloseable.close();}
public void beam_f10325_0() throws Exception
{    Random random = new Random(2348238943L);    for (int i : Arrays.asList(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)) {        int minElements = (int) Math.pow(2, i);        int valueSize = 128;                writeElementsToFileAndFindLastElementPerPrimaryKey(dataGenerator(7, minElements + random.nextInt(minElements), 8, /* max key size */        valueSize));    }}
public void beam_f10326_0() throws Exception
{    File tmpFile = tmpFolder.newFile();    List<IsmRecord<byte[]>> data = new ArrayList<>();    data.add(IsmRecord.<byte[]>of(ImmutableList.of(EMPTY, new byte[] { 0x04 }), EMPTY));    data.add(IsmRecord.<byte[]>of(ImmutableList.of(EMPTY, new byte[] { 0x08 }), EMPTY));    writeElementsToFile(data, tmpFile);    IsmReader<byte[]> reader = new IsmReaderImpl<byte[]>(FileSystems.matchSingleFileSpec(tmpFile.getAbsolutePath()).resourceId(), CODER, cache);        assertFalse(reader.overKeyComponents(ImmutableList.of(EMPTY, new byte[] { 0x02 })).start());        assertFalse(reader.overKeyComponents(ImmutableList.of(EMPTY, new byte[] { 0x06 })).start());        assertFalse(reader.overKeyComponents(ImmutableList.of(EMPTY, new byte[] { 0x10 })).start());}
public void beam_f10327_0() throws Exception
{    File tmpFile = tmpFolder.newFile();    List<IsmRecord<byte[]>> data = new ArrayList<>();    data.add(IsmRecord.<byte[]>of(ImmutableList.of(EMPTY, new byte[] { 0x04 }), EMPTY));    data.add(IsmRecord.<byte[]>of(ImmutableList.of(EMPTY, new byte[] { 0x08 }), EMPTY));    writeElementsToFile(data, tmpFile);    IsmReader<byte[]> reader = new IsmReaderImpl<byte[]>(FileSystems.matchSingleFileSpec(tmpFile.getAbsolutePath()).resourceId(), CODER, cache) {                @Override        boolean bloomFilterMightContain(RandomAccessData keyBytes) {            return true;        }    };        assertFalse(reader.overKeyComponents(ImmutableList.of(EMPTY, new byte[] { 0x02 })).start());        assertFalse(reader.overKeyComponents(ImmutableList.of(EMPTY, new byte[] { 0x06 })).start());        assertFalse(reader.overKeyComponents(ImmutableList.of(EMPTY, new byte[] { 0x10 })).start());}
private static void beam_f10335_0(IsmRecord<byte[]> actual, IsmRecord<byte[]> expected)
{    assertEquals(expected.getKeyComponents().size(), actual.getKeyComponents().size());    for (int i = 0; i < expected.getKeyComponents().size(); ++i) {        if (actual.getKeyComponent(i) != expected.getKeyComponent(i)) {            assertArrayEquals((byte[]) actual.getKeyComponent(i), (byte[]) expected.getKeyComponent(i));        }    }    if (IsmFormat.isMetadataKey(expected.getKeyComponents())) {        assertArrayEquals(actual.getMetadata(), expected.getMetadata());    } else {        assertArrayEquals(actual.getValue(), expected.getValue());    }}
public boolean beam_f10336_0(IsmRecord<byte[]> input)
{    byte[] secondKey = (byte[]) input.getKeyComponent(1);    return secondKey[secondKey.length - 1] % 2 == 0;}
private void beam_f10337_0(Iterable<IsmRecord<byte[]>> elements) throws Exception
{    File tmpFile = tmpFolder.newFile();    List<IsmRecord<byte[]>> oddSecondaryKeys = new ArrayList<>(ImmutableList.copyOf(Iterables.filter(elements, Predicates.not(EvenFilter.INSTANCE))));    List<IsmRecord<byte[]>> evenSecondaryKeys = new ArrayList<>(ImmutableList.copyOf(Iterables.filter(elements, EvenFilter.INSTANCE)));    writeElementsToFile(oddSecondaryKeys, tmpFile);    IsmReader<byte[]> reader = new IsmReaderImpl<>(FileSystems.matchSingleFileSpec(tmpFile.getAbsolutePath()).resourceId(), CODER, cache);        Collections.shuffle(oddSecondaryKeys);    for (IsmRecord<byte[]> expectedNext : oddSecondaryKeys) {        IsmReader<byte[]>.IsmPrefixReaderIterator iterator = reader.overKeyComponents(expectedNext.getKeyComponents());        assertTrue(iterator.start());        assertIsmEquals(iterator.getCurrent().getValue(), expectedNext);    }    Collections.shuffle(oddSecondaryKeys);        IsmReader<byte[]>.IsmPrefixReaderIterator readerIterator = reader.overKeyComponents(ImmutableList.of());    for (IsmRecord<byte[]> expectedNext : oddSecondaryKeys) {        assertIsmEquals(readerIterator.get(expectedNext.getKeyComponents()).getValue(), expectedNext);    }        Collections.shuffle(evenSecondaryKeys);    for (IsmRecord<byte[]> missingNext : evenSecondaryKeys) {        assertFalse(reader.overKeyComponents(missingNext.getKeyComponents()).start());    }    Collections.shuffle(evenSecondaryKeys);        readerIterator = reader.overKeyComponents(ImmutableList.of());    for (IsmRecord<byte[]> missingNext : evenSecondaryKeys) {        assertNull(readerIterator.get(missingNext.getKeyComponents()));    }}
public void beam_f10345_0() throws Exception
{    final int offset = 10;    try (SeekableByteChannel channel = new CachedTailSeekableByteChannel(offset, new byte[] { 0, 1, 2 })) {        ByteBuffer buffer = ByteBuffer.allocate(1);        channel.position(offset);        assertEquals(1, channel.read(buffer));        assertEquals(0, buffer.get(0));        assertEquals(offset + 1, channel.position());        buffer.clear();        assertEquals(1, channel.read(buffer));        assertEquals(1, buffer.get(0));        assertEquals(offset + 2, channel.position());        buffer.clear();        assertEquals(1, channel.read(buffer));        assertEquals(2, buffer.get(0));        assertEquals(offset + 3, channel.position());        buffer.clear();                channel.position(offset + 1);        assertEquals(1, channel.read(buffer));        assertEquals(1, buffer.get(0));        assertEquals(offset + 2, channel.position());        buffer.clear();        assertEquals(1, channel.read(buffer));        assertEquals(2, buffer.get(0));        assertEquals(offset + 3, channel.position());        buffer.clear();                assertEquals(-1, channel.read(buffer));        buffer.clear();                channel.position(offset + 3);        assertEquals(-1, channel.read(buffer));        buffer.clear();    }}
public void beam_f10346_0() throws Exception
{    try (SeekableByteChannel channel = new CachedTailSeekableByteChannel(1, new byte[0])) {                channel.position(1);        expectedException.expect(IllegalArgumentException.class);        channel.position(0);    }}
public void beam_f10347_0() throws Exception
{    try (SeekableByteChannel channel = new CachedTailSeekableByteChannel(1, new byte[0])) {                channel.position(1);        expectedException.expect(IllegalArgumentException.class);        channel.position(2);    }}
public void beam_f10355_0() throws Exception
{    Coder<WindowedValue<Long>> valueCoder = WindowedValue.getFullCoder(VarLongCoder.of(), GLOBAL_WINDOW_CODER);    IsmRecordCoder<WindowedValue<Long>> ismCoder = IsmRecordCoder.of(1, 0, ImmutableList.of(GLOBAL_WINDOW_CODER, BigEndianLongCoder.of()), valueCoder);    final List<KV<Long, WindowedValue<Long>>> firstElements = Arrays.asList(KV.of(0L, valueInGlobalWindow(12L)), KV.of(1L, valueInGlobalWindow(22L)), KV.of(2L, valueInGlobalWindow(32L)));    final List<KV<Long, WindowedValue<Long>>> secondElements = Arrays.asList(KV.of(0L, valueInGlobalWindow(42L)), KV.of(1L, valueInGlobalWindow(52L)), KV.of(2L, valueInGlobalWindow(62L)));    final PCollectionView<Iterable<Long>> view = Pipeline.create().apply(Create.empty(VarLongCoder.of())).apply(View.asIterable());    Source sourceA = initInputFile(fromKvsForList(firstElements), ismCoder);    Source sourceB = initInputFile(fromKvsForList(secondElements), ismCoder);    final IsmSideInputReader reader = sideInputReader(view.getTagInternal().getId(), sourceA, sourceB);    List<Callable<Iterable<Long>>> tasks = new ArrayList<>();    for (int i = 0; i < NUM_THREADS; ++i) {        tasks.add(() -> {                                    Iterable<Long> value = reader.get(view, GlobalWindow.INSTANCE);            verifyIterable(toValueList(concat(firstElements, secondElements)), value);                        assertSame(reader.get(view, GlobalWindow.INSTANCE), value);            return value;        });    }    List<Future<Iterable<Long>>> results = pipelineOptions.getExecutorService().invokeAll(tasks);    Iterable<Long> value = results.get(0).get();        for (Future<Iterable<Long>> result : results) {        assertSame(value, result.get());    }}
public void beam_f10356_0() throws Exception
{    Coder<WindowedValue<Long>> valueCoder = WindowedValue.getFullCoder(VarLongCoder.of(), GLOBAL_WINDOW_CODER);    IsmRecordCoder<WindowedValue<Long>> ismCoder = IsmRecordCoder.of(1, 0, ImmutableList.of(GLOBAL_WINDOW_CODER, BigEndianLongCoder.of()), valueCoder);    final List<KV<Long, WindowedValue<Long>>> firstElements = Arrays.asList(KV.of(0L, valueInGlobalWindow(12L)), KV.of(1L, valueInGlobalWindow(22L)), KV.of(2L, valueInGlobalWindow(32L)));    final List<KV<Long, WindowedValue<Long>>> secondElements = Arrays.asList(KV.of(0L, valueInGlobalWindow(42L)), KV.of(1L, valueInGlobalWindow(52L)), KV.of(2L, valueInGlobalWindow(62L)));    final PCollectionView<Iterable<Long>> view = Pipeline.create().apply(Create.empty(VarLongCoder.of())).apply(View.asIterable());    String tmpFilePrefix = tmpFolder.newFile().getPath();    initInputFile(fromKvsForList(firstElements), ismCoder, tmpFilePrefix + "-00000-of-00002.ism");    initInputFile(fromKvsForList(secondElements), ismCoder, tmpFilePrefix + "-00001-of-00002.ism");    Source source = newIsmSource(ismCoder, tmpFilePrefix + "@2.ism");    final IsmSideInputReader reader = sideInputReader(view.getTagInternal().getId(), source);    List<Callable<Iterable<Long>>> tasks = new ArrayList<>();    for (int i = 0; i < NUM_THREADS; ++i) {        tasks.add(() -> {                                    Iterable<Long> value = reader.get(view, GlobalWindow.INSTANCE);            verifyIterable(toValueList(concat(firstElements, secondElements)), value);                        assertSame(reader.get(view, GlobalWindow.INSTANCE), value);            return value;        });    }    List<Future<Iterable<Long>>> results = pipelineOptions.getExecutorService().invokeAll(tasks);    Iterable<Long> value = results.get(0).get();        for (Future<Iterable<Long>> result : results) {        assertSame(value, result.get());    }}
public void beam_f10357_0() throws Exception
{    Coder<WindowedValue<Long>> valueCoder = WindowedValue.getFullCoder(VarLongCoder.of(), INTERVAL_WINDOW_CODER);    IsmRecordCoder<WindowedValue<Long>> ismCoder = IsmRecordCoder.of(1, 0, ImmutableList.of(INTERVAL_WINDOW_CODER, BigEndianLongCoder.of()), valueCoder);    final List<KV<Long, WindowedValue<Long>>> firstElements = Arrays.asList(KV.of(0L, valueInIntervalWindow(12, 10)), KV.of(1L, valueInIntervalWindow(22, 10)), KV.of(2L, valueInIntervalWindow(32, 10)));    final List<KV<Long, WindowedValue<Long>>> secondElements = Arrays.asList(KV.of(0L, valueInIntervalWindow(42, 20)), KV.of(1L, valueInIntervalWindow(52, 20)), KV.of(2L, valueInIntervalWindow(62, 20)));    final List<KV<Long, WindowedValue<Long>>> thirdElements = Arrays.asList(KV.of(0L, valueInIntervalWindow(42L, 30)), KV.of(1L, valueInIntervalWindow(52L, 30)), KV.of(2L, valueInIntervalWindow(62L, 30)));    final PCollectionView<Iterable<Long>> view = Pipeline.create().apply(Create.empty(VarLongCoder.of())).apply(Window.into(FixedWindows.of(Duration.millis(10)))).apply(View.asIterable());    Source sourceA = initInputFile(fromKvsForList(concat(firstElements, secondElements)), ismCoder);    Source sourceB = initInputFile(fromKvsForList(thirdElements), ismCoder);    final IsmSideInputReader reader = sideInputReader(view.getTagInternal().getId(), sourceA, sourceB);    List<Callable<Map<BoundedWindow, Iterable<Long>>>> tasks = new ArrayList<>();    for (int i = 0; i < NUM_THREADS; ++i) {        tasks.add(() -> {                                    Iterable<Long> firstValues = reader.get(view, intervalWindow(10));            Iterable<Long> secondValues = reader.get(view, intervalWindow(20));            Iterable<Long> thirdValues = reader.get(view, intervalWindow(30));            verifyIterable(toValueList(firstElements), firstValues);            verifyIterable(toValueList(secondElements), secondValues);            verifyIterable(toValueList(thirdElements), thirdValues);                        assertSame(firstValues, reader.get(view, intervalWindow(10)));            assertSame(secondValues, reader.get(view, intervalWindow(20)));            assertSame(thirdValues, reader.get(view, intervalWindow(30)));            return ImmutableMap.<BoundedWindow, Iterable<Long>>of(intervalWindow(10), firstValues, intervalWindow(20), secondValues, intervalWindow(30), thirdValues);        });    }    List<Future<Map<BoundedWindow, Iterable<Long>>>> results = pipelineOptions.getExecutorService().invokeAll(tasks);    Map<BoundedWindow, Iterable<Long>> value = results.get(0).get();        for (Future<Map<BoundedWindow, Iterable<Long>>> result : results) {        assertEquals(value, result.get());        for (Map.Entry<BoundedWindow, Iterable<Long>> entry : result.get().entrySet()) {            assertSame(value.get(entry.getKey()), entry.getValue());        }    }}
public void beam_f10365_0() throws Exception
{        CounterUpdate expectedSideInputMsecUpdate = new CounterUpdate().setStructuredNameAndMetadata(new CounterStructuredNameAndMetadata().setMetadata(new CounterMetadata().setKind("SUM")).setName(new CounterStructuredName().setOrigin("SYSTEM").setName("read-sideinput-msecs").setOriginalStepName("originalName").setExecutionStepName("stageName").setOriginalRequestingStepName("originalName2").setInputIndex(1))).setCumulative(true).setInteger(new SplitInt64().setHighBits(0).setLowBits(0L));    CounterName expectedCounterName = CounterName.named("read-sideinput-byte-count").withOriginalName(operationContext.nameContext()).withOrigin("SYSTEM").withOriginalRequestingStepName("originalName2").withInputIndex(1);        Coder<WindowedValue<Long>> valueCoder = WindowedValue.getFullCoder(VarLongCoder.of(), GLOBAL_WINDOW_CODER);    IsmRecordCoder<WindowedValue<Long>> ismCoder = IsmRecordCoder.of(1, 0, ImmutableList.of(GLOBAL_WINDOW_CODER, BigEndianLongCoder.of()), valueCoder);        DataflowExecutionState state2 = executionContext.getExecutionStateRegistry().getState(NameContext.create("stageName", "originalName2", "systemName2", "userName2"), "process", null, NoopProfileScope.NOOP);    final List<KV<Long, WindowedValue<Long>>> firstElements = Arrays.asList(KV.of(0L, valueInGlobalWindow(0L)));    final List<KV<Long, WindowedValue<Long>>> secondElements = new ArrayList<>();    for (long i = 0; i < 100; i++) {        secondElements.add(KV.of(i, valueInGlobalWindow(i * 10)));    }    final PCollectionView<Iterable<Long>> view = Pipeline.create().apply(Create.empty(VarLongCoder.of())).apply(View.asIterable());    Source sourceA = initInputFile(fromKvsForList(firstElements), ismCoder);    Source sourceB = initInputFile(fromKvsForList(secondElements), ismCoder);    try (Closeable state2Closeable = executionContext.getExecutionStateTracker().enterState(state2)) {        final IsmSideInputReader reader = serialSideInputReader(view.getTagInternal().getId(), sourceA, sourceB);                        Iterable<Long> value = reader.get(view, GlobalWindow.INSTANCE);        verifyIterable(toValueList(concat(firstElements, secondElements)), value);                assertSame(reader.get(view, GlobalWindow.INSTANCE), value);        Iterable<CounterUpdate> counterUpdates = executionContext.getExecutionStateRegistry().extractUpdates(true);        assertThat(counterUpdates, hasItem(expectedSideInputMsecUpdate));        Counter<?, ?> expectedCounter = counterFactory.getExistingCounter(expectedCounterName);        assertNotNull(expectedCounter);    }}
public void beam_f10366_0() throws Exception
{    Coder<WindowedValue<Long>> valueCoder = WindowedValue.getFullCoder(VarLongCoder.of(), GLOBAL_WINDOW_CODER);    final WindowedValue<Long> element = valueInGlobalWindow(42L);    final PCollectionView<Long> view = Pipeline.create().apply(Create.empty(VarLongCoder.of())).apply(View.asSingleton());    final Source source = initInputFile(fromValues(Arrays.asList(element)), IsmRecordCoder.of(1, 0, ImmutableList.<Coder<?>>of(GLOBAL_WINDOW_CODER), valueCoder));    final Source emptySource = initInputFile(fromValues(Arrays.asList()), IsmRecordCoder.of(1, 0, ImmutableList.<Coder<?>>of(GLOBAL_WINDOW_CODER), valueCoder));    final IsmSideInputReader reader = sideInputReader(view.getTagInternal().getId(), source, emptySource);    assertTrue(reader.tagToIsmReaderMap.containsKey(view.getTagInternal()));    assertEquals(1, reader.tagToIsmReaderMap.get(view.getTagInternal()).size());    assertEquals(FileSystems.matchSingleFileSpec(getString(source.getSpec(), WorkerPropertyNames.FILENAME)).resourceId(), reader.tagToIsmReaderMap.get(view.getTagInternal()).get(0).getResourceId());    assertTrue(reader.tagToEmptyIsmReaderMap.containsKey(view.getTagInternal()));    assertEquals(1, reader.tagToEmptyIsmReaderMap.get(view.getTagInternal()).size());    assertEquals(FileSystems.matchSingleFileSpec(getString(emptySource.getSpec(), WorkerPropertyNames.FILENAME)).resourceId(), reader.tagToEmptyIsmReaderMap.get(view.getTagInternal()).get(0).getResourceId());}
public T beam_f10367_0(Collection<WindowedValue<T>> input)
{    return Iterables.getOnlyElement(input).getValue();}
private static IntervalWindow beam_f10375_0(long start)
{    return new IntervalWindow(new Instant(start), new Instant(start + 1));}
private static BoundedWindow beam_f10376_0(WindowedValue<?> windowedValue)
{    return windowedValue.getWindows().iterator().next();}
 List<IsmRecord<WindowedValue<T>>> beam_f10377_0(Iterable<WindowedValue<T>> elements)
{    List<IsmRecord<WindowedValue<T>>> rval = new ArrayList<>();    for (WindowedValue<T> element : elements) {        rval.add(IsmRecord.of(ImmutableList.of(windowOf(element)), element));    }    return rval;}
private Source beam_f10385_0(IsmRecordCoder<WindowedValue<V>> coder, String tmpFilePath)
{    Source source = new Source();    source.setCodec(CloudObjects.asCloudObject(WindowedValue.getFullCoder(coder, GLOBAL_WINDOW_CODER), /*sdkComponents=*/    null));    source.setSpec(new HashMap<String, Object>());    source.getSpec().put(PropertyNames.OBJECT_TYPE_NAME, "IsmSource");    source.getSpec().put(WorkerPropertyNames.FILENAME, tmpFilePath);    return source;}
private SideInputInfo beam_f10386_0(String tagId, Source... sources)
{    SideInputInfo sideInputInfo = new SideInputInfo();    sideInputInfo.setTag(tagId);    sideInputInfo.setKind(new HashMap<String, Object>());    if (sources.length == 1) {        sideInputInfo.getKind().put(PropertyNames.OBJECT_TYPE_NAME, "singleton");    } else {        sideInputInfo.getKind().put(PropertyNames.OBJECT_TYPE_NAME, "collection");    }    sideInputInfo.setSources(new ArrayList<>(Arrays.asList(sources)));    return sideInputInfo;}
private IsmSideInputReader beam_f10387_0(String tagId, Source... sources) throws Exception
{    return IsmSideInputReader.forTest(Arrays.asList(toSideInputInfo(tagId, sources)), pipelineOptions, executionContext, ReaderRegistry.defaultRegistry(), operationContext, MoreExecutors.newDirectExecutorService());}
public void beam_f10395_0() throws Exception
{    expectedException.expect(IllegalArgumentException.class);    expectedException.expectMessage("is expected to be deterministic");    new IsmSink<>(FileSystems.matchNewResource(tmpFolder.newFile().getPath(), false), IsmRecordCoder.of(1, 0, ImmutableList.<Coder<?>>of(ByteArrayCoder.of(), NON_DETERMINISTIC_CODER), ByteArrayCoder.of()), BLOOM_FILTER_SIZE_LIMIT);}
public void beam_f10396_0() throws Exception
{    KeyTokenInvalidException exception = new KeyTokenInvalidException("test");    RuntimeException keyTokenCauseException = new RuntimeException("key token cause", exception);    assertTrue(KeyTokenInvalidException.isKeyTokenInvalidException(exception));    assertTrue(KeyTokenInvalidException.isKeyTokenInvalidException(keyTokenCauseException));    assertFalse(KeyTokenInvalidException.isKeyTokenInvalidException(new RuntimeException("non key token")));}
public void beam_f10397_0()
{    MockitoAnnotations.initMocks(this);}
public void beam_f10405_0() throws IOException
{    DataflowWorkerLoggingMDC.setJobId("testJobId");    ByteArrayOutputStream first = new ByteArrayOutputStream();    ByteArrayOutputStream second = new ByteArrayOutputStream();    LogRecord record = createLogRecord("test.message", null);    String expected = "{\"timestamp\":{\"seconds\":0,\"nanos\":1000000},\"severity\":\"INFO\"," + "\"message\":\"test.message\",\"thread\":\"2\",\"job\":\"testJobId\"," + "\"logger\":\"LoggerName\"}" + System.lineSeparator();    FixedOutputStreamFactory factory = new FixedOutputStreamFactory(first, second);    DataflowWorkerLoggingHandler handler = new DataflowWorkerLoggingHandler(factory, expected.length() + 1);            handler.publish(record);    handler.publish(record);    handler.publish(record);    assertEquals(expected + expected, new String(first.toByteArray(), StandardCharsets.UTF_8));    assertEquals(expected, new String(second.toByteArray(), StandardCharsets.UTF_8));}
public void beam_f10406_0() throws IOException
{    DataflowWorkerLoggingMDC.setJobId("testJobId");    assertEquals("{\"timestamp\":{\"seconds\":0,\"nanos\":1000000},\"severity\":\"INFO\"," + "\"message\":\"test.message\",\"thread\":\"2\",\"job\":\"testJobId\"," + "\"logger\":\"LoggerName\"}" + System.lineSeparator(), createJson(createLogRecord("test.message", null)));}
public void beam_f10407_0() throws IOException
{    DataflowExecutionState state = new TestDataflowExecutionState(nameContextForTest(), "activity");    tracker.enterState(state);    String testJobId = "testJobId";    String testStage = "testStage";    String testWorkerId = "testWorkerId";    String testWorkId = "testWorkId";    DataflowWorkerLoggingMDC.setJobId(testJobId);    DataflowWorkerLoggingMDC.setStageName(testStage);    DataflowWorkerLoggingMDC.setWorkerId(testWorkerId);    DataflowWorkerLoggingMDC.setWorkId(testWorkId);    createLogRecord("test.message", null);    assertEquals(String.format("{\"timestamp\":{\"seconds\":0,\"nanos\":1000000},\"severity\":\"INFO\"," + "\"message\":\"test.message\",\"thread\":\"2\",\"job\":\"%s\"," + "\"stage\":\"%s\",\"step\":\"%s\",\"worker\":\"%s\"," + "\"work\":\"%s\",\"logger\":\"LoggerName\"}" + System.lineSeparator(), testJobId, testStage, NameContextsForTests.USER_NAME, testWorkerId, testWorkId), createJson(createLogRecord("test.message", null)));}
private Throwable beam_f10415_0()
{    Throwable throwable = new Throwable("exception.test.message");    throwable.setStackTrace(new StackTraceElement[] { new StackTraceElement("declaringClass1", "method1", "file1.java", 1), new StackTraceElement("declaringClass2", "method2", "file2.java", 1), new StackTraceElement("declaringClass3", "method3", "file3.java", 1) });    return throwable;}
private LogRecord beam_f10416_0(String message, Throwable throwable, Object... params)
{    LogRecord logRecord = new LogRecord(Level.INFO, message);    logRecord.setLoggerName("LoggerName");    logRecord.setMillis(1L);    logRecord.setThreadID(2);    logRecord.setThrown(throwable);    logRecord.setParameters(params);    return logRecord;}
private BeamFnApi.LogEntry beam_f10417_0(String message)
{    return BeamFnApi.LogEntry.newBuilder().setLogLocation("LoggerName").setSeverity(BeamFnApi.LogEntry.Severity.Enum.INFO).setMessage(message).setInstructionId("1").setThread("2").setTimestamp(Timestamp.newBuilder().setSeconds(0).setNanos(1 * 1000000)).build();}
public void beam_f10425_0() throws Throwable
{    System.out.println("afterInitialization");    verifyLogOutput("afterInitialization");}
public void beam_f10426_0() throws Throwable
{    System.err.println("afterInitialization");    verifyLogOutput("afterInitialization");}
public void beam_f10427_0() throws IOException
{    DataflowWorkerLoggingOptions options = PipelineOptionsFactory.as(DataflowWorkerLoggingOptions.class);    options.setDefaultWorkerLogLevel(DataflowWorkerLoggingOptions.Level.ERROR);    DataflowWorkerLoggingInitializer.configure(options);    System.out.println("sys.out");    System.err.println("sys.err");    List<String> actualLines = retrieveLogLines();    assertThat(actualLines, not(hasItem(containsString("sys.out"))));    assertThat(actualLines, hasItem(containsString("sys.err")));}
private void beam_f10437_0(String substring) throws IOException
{    List<String> logLines = retrieveLogLines();    assertThat(logLines, hasItem(containsString(substring)));}
private List<String> beam_f10438_0() throws IOException
{    List<String> allLogLines = Lists.newArrayList();    for (File logFile : logFolder.getRoot().listFiles()) {        allLogLines.addAll(Files.readAllLines(logFile.toPath(), StandardCharsets.UTF_8));    }    return allLogLines;}
public void beam_f10439_0()
{    JulHandlerPrintStreamAdapterFactory.reset();    handler = new LogSaver();}
public void beam_f10447_0()
{    try (PrintStream printStream = createPrintStreamAdapter()) {        printStream.print("blah");    }    assertThat(handler.getLogs(), hasLogItem("blah"));}
private PrintStream beam_f10448_0()
{    return JulHandlerPrintStreamAdapterFactory.create(handler, LOGGER_NAME, Level.INFO);}
public static LogRecordMatcher beam_f10449_0(Level level, String substring)
{    return new LogRecordMatcher(level, substring);}
public void beam_f10457_0()
{    Set<LogRecord> records = Sets.newHashSet(new LogRecord(Level.SEVERE, "error"));    assertThat(records, LogRecordMatcher.hasLogItem(Level.SEVERE, "error"));}
public void beam_f10458_0()
{    LogRecord record = new LogRecord(Level.WARNING, "abc");    try {        assertThat(record, LogRecordMatcher.hasLog(Level.CONFIG, ""));    } catch (AssertionError e) {        return;    }    fail("Expected exception not thrown");}
public void beam_f10459_0()
{    LogRecord record = new LogRecord(Level.INFO, "foo");    try {        assertThat(record, LogRecordMatcher.hasLog("bar"));    } catch (AssertionError e) {        return;    }    fail("Expected exception not thrown");}
public static NameContext beam_f10469_0()
{    return NameContext.create(STAGE_NAME, ORIGINAL_NAME, SYSTEM_NAME, USER_NAME);}
public void beam_f10470_0()
{    options = PipelineOptionsFactory.fromArgs(new String[] { "--experiments=beam_fn_api" }).create();    SourceSplitRequest splitRequest = new SourceSplitRequest();    SourceOperationRequest operationRequest = new SourceOperationRequest().setSplit(splitRequest);    executor = new NoOpSourceOperationExecutor(operationRequest);}
public void beam_f10471_0() throws Exception
{    executor.execute();    SourceOperationResponse response = executor.getResponse();    assertEquals("SOURCE_SPLIT_OUTCOME_USE_CURRENT", response.getSplit().getOutcome());}
private static void beam_f10479_0(long expectedNum, String encodedHexString)
{    OrderedCode orderedCode = new OrderedCode(bytesFromHexString(encodedHexString));    assertEquals("Unexpected value when decoding 0x" + encodedHexString, expectedNum, orderedCode.readSignedNumIncreasing());    assertFalse("Unexpected encoded bytes remain after decoding 0x" + encodedHexString, orderedCode.hasRemainingEncodedBytes());}
public void beam_f10480_0()
{    assertDecodedSignedNumIncreasingEquals(Long.MIN_VALUE, "003f8000000000000000");    assertDecodedSignedNumIncreasingEquals(Long.MIN_VALUE + 1, "003f8000000000000001");    assertDecodedSignedNumIncreasingEquals(Integer.MIN_VALUE - 1L, "077fffffff");    assertDecodedSignedNumIncreasingEquals(Integer.MIN_VALUE, "0780000000");    assertDecodedSignedNumIncreasingEquals(Integer.MIN_VALUE + 1, "0780000001");    assertDecodedSignedNumIncreasingEquals(-65, "3fbf");    assertDecodedSignedNumIncreasingEquals(-64, "40");    assertDecodedSignedNumIncreasingEquals(-63, "41");    assertDecodedSignedNumIncreasingEquals(-3, "7d");    assertDecodedSignedNumIncreasingEquals(-2, "7e");    assertDecodedSignedNumIncreasingEquals(-1, "7f");    assertDecodedSignedNumIncreasingEquals(0, "80");    assertDecodedSignedNumIncreasingEquals(1, "81");    assertDecodedSignedNumIncreasingEquals(2, "82");    assertDecodedSignedNumIncreasingEquals(3, "83");    assertDecodedSignedNumIncreasingEquals(63, "bf");    assertDecodedSignedNumIncreasingEquals(64, "c040");    assertDecodedSignedNumIncreasingEquals(65, "c041");    assertDecodedSignedNumIncreasingEquals(Integer.MAX_VALUE - 1, "f87ffffffe");    assertDecodedSignedNumIncreasingEquals(Integer.MAX_VALUE, "f87fffffff");    assertDecodedSignedNumIncreasingEquals(Integer.MAX_VALUE + 1L, "f880000000");    assertDecodedSignedNumIncreasingEquals(Long.MAX_VALUE - 1, "ffc07ffffffffffffffe");    assertDecodedSignedNumIncreasingEquals(Long.MAX_VALUE, "ffc07fffffffffffffff");}
private static void beam_f10481_0(long num)
{    OrderedCode orderedCode = new OrderedCode();    orderedCode.writeSignedNumIncreasing(num);    assertEquals("Unexpected result when decoding writeSignedNumIncreasing(" + num + ")", num, orderedCode.readSignedNumIncreasing());    assertFalse("Unexpected remaining encoded bytes after decoding " + num, orderedCode.hasRemainingEncodedBytes());}
public void beam_f10489_0()
{    byte[] ffChar = { OrderedCode.FF_CHARACTER };    byte[] nullChar = { OrderedCode.NULL_CHARACTER };    byte[] separatorEncoded = { OrderedCode.ESCAPE1, OrderedCode.SEPARATOR };    byte[] ffCharEncoded = { OrderedCode.ESCAPE1, OrderedCode.NULL_CHARACTER };    byte[] nullCharEncoded = { OrderedCode.ESCAPE2, OrderedCode.FF_CHARACTER };    byte[] infinityEncoded = { OrderedCode.ESCAPE2, OrderedCode.INFINITY };    OrderedCode orderedCode = new OrderedCode();    orderedCode.writeBytes(ffChar);    orderedCode.writeBytes(nullChar);    orderedCode.writeInfinity();    assertArrayEquals(orderedCode.getEncodedBytes(), Bytes.concat(ffCharEncoded, separatorEncoded, nullCharEncoded, separatorEncoded, infinityEncoded));    assertArrayEquals(orderedCode.readBytes(), ffChar);    assertArrayEquals(orderedCode.readBytes(), nullChar);    assertTrue(orderedCode.readInfinity());    orderedCode = new OrderedCode(Bytes.concat(ffCharEncoded, separatorEncoded));    assertArrayEquals(orderedCode.readBytes(), ffChar);    orderedCode = new OrderedCode(Bytes.concat(nullCharEncoded, separatorEncoded));    assertArrayEquals(orderedCode.readBytes(), nullChar);    byte[] invalidEncodingForRead = { OrderedCode.ESCAPE2, OrderedCode.ESCAPE2, OrderedCode.ESCAPE1, OrderedCode.SEPARATOR };    orderedCode = new OrderedCode(invalidEncodingForRead);    try {        orderedCode.readBytes();        fail("Should have failed.");    } catch (Exception e) {        }    assertTrue(orderedCode.hasRemainingEncodedBytes());}
public void beam_f10490_0()
{    byte[] bytes = { 'a', 'b', 'c' };    long number = 12345;        OrderedCode orderedCode = new OrderedCode();    assertFalse(orderedCode.hasRemainingEncodedBytes());        orderedCode.writeBytes(bytes);    assertTrue(orderedCode.hasRemainingEncodedBytes());    assertArrayEquals(orderedCode.readBytes(), bytes);    assertFalse(orderedCode.hasRemainingEncodedBytes());    orderedCode.writeNumIncreasing(number);    assertTrue(orderedCode.hasRemainingEncodedBytes());    assertEquals(orderedCode.readNumIncreasing(), number);    assertFalse(orderedCode.hasRemainingEncodedBytes());    orderedCode.writeSignedNumIncreasing(number);    assertTrue(orderedCode.hasRemainingEncodedBytes());    assertEquals(orderedCode.readSignedNumIncreasing(), number);    assertFalse(orderedCode.hasRemainingEncodedBytes());    orderedCode.writeInfinity();    assertTrue(orderedCode.hasRemainingEncodedBytes());    assertTrue(orderedCode.readInfinity());    assertFalse(orderedCode.hasRemainingEncodedBytes());    orderedCode.writeTrailingBytes(bytes);    assertTrue(orderedCode.hasRemainingEncodedBytes());    assertArrayEquals(orderedCode.readTrailingBytes(), bytes);    assertFalse(orderedCode.hasRemainingEncodedBytes());        orderedCode.writeBytes(bytes);    orderedCode.writeBytes(bytes);    assertTrue(orderedCode.hasRemainingEncodedBytes());    assertArrayEquals(orderedCode.readBytes(), bytes);    assertArrayEquals(orderedCode.readBytes(), bytes);    assertFalse(orderedCode.hasRemainingEncodedBytes());}
public void beam_f10491_0() throws Exception
{    ParDoFn parDoFn = new PairWithConstantKeyDoFnFactory().create(null, /* pipeline options */    CloudObject.fromSpec(ImmutableMap.of(PropertyNames.OBJECT_TYPE_NAME, "PairWithConstantKeyDoFn", WorkerPropertyNames.ENCODED_KEY, StringUtils.byteArrayToJsonString(CoderUtils.encodeToByteArray(BigEndianIntegerCoder.of(), 42)), PropertyNames.ENCODING, ImmutableMap.of(PropertyNames.OBJECT_TYPE_NAME, "kind:fixed_big_endian_int32"))), null, /* side input infos */    null, /* main output tag */    null, /* output tag to receiver index */    null, /* exection context */    null);    List<Object> outputReceiver = new ArrayList<>();    parDoFn.startBundle(outputReceiver::add);    parDoFn.processElement(valueInGlobalWindow(43));    assertThat(outputReceiver, contains(valueInGlobalWindow(KV.of(42, 43))));}
public void beam_f10499_0() throws Exception
{    doAnswer(invocation -> {        Object[] args = invocation.getArguments();        ((ElementByteSizeObserver) args[1]).update(5);                return null;    }).when(mockCoder).registerByteSizeObserver(Matchers.eq("apple"), Matchers.<ElementByteSizeObserver>any());    CoderSizeEstimator<String> estimator = new CoderSizeEstimator(mockCoder);    assertEquals(5, estimator.estimateSize("apple"));}
public void beam_f10500_0() throws Exception
{        doAnswer(invocation -> {        Object[] args = invocation.getArguments();                ((ElementByteSizeObserver) args[1]).setLazy();        return null;    }).when(mockCoder).registerByteSizeObserver(Matchers.eq("apple"), Matchers.<ElementByteSizeObserver>any());        doAnswer(invocation -> {        Object[] args = invocation.getArguments();        String value = (String) args[0];        OutputStream os = (OutputStream) args[1];        os.write(value.getBytes(StandardCharsets.UTF_8));        return null;    }).when(mockCoder).encode(Matchers.eq("apple"), Matchers.<OutputStream>any());    CoderSizeEstimator<String> estimator = new CoderSizeEstimator(mockCoder);        assertEquals(5L, estimator.estimateSize("apple"));}
public Integer beam_f10501_0(WindowedValue<String> key)
{    return 0;}
public void beam_f10509_0() throws Exception
{    assertEquals("The Profiler class must stay in this package since the implementation is via JNI",     Joiner.on(".").join("org", "apache", "beam", "runners", "dataflow", "worker", "profiler"), Profiler.class.getPackage().getName());}
public void beam_f10510_0() throws Exception
{    MockProfiler profiler = new MockProfiler(false);    ScopedProfiler underTest = new ScopedProfiler(profiler);    ProfileScope original = underTest.currentScope();    underTest.registerScope("attribute").activate();    original.activate();    assertThat(profiler.interactions, Matchers.equalTo(1));}
public void beam_f10511_0() throws Exception
{    MockProfiler profiler = new MockProfiler(true);    ScopedProfiler underTest = new ScopedProfiler(profiler);    ProfileScope original = underTest.currentScope();    underTest.registerScope("attribute").activate();    assertThat(profiler.currentState, Matchers.equalTo(profiler.registeredAttributes.get("attribute")));    original.activate();    assertThat(profiler.currentState, Matchers.equalTo(0));}
public void beam_f10519_0() throws Exception
{    testReadWith(null);}
public void beam_f10520_0() throws Exception
{    testReadWith("");}
public void beam_f10521_0() throws Exception
{    MockitoAnnotations.initMocks(this);}
public NativeReader<?> beam_f10529_0(CloudObject spec, @Nullable Coder<?> coder, @Nullable PipelineOptions options, @Nullable DataflowExecutionContext executionContext, DataflowOperationContext operationContext)
{    return new TestReader();}
public NativeReaderIterator<Integer> beam_f10530_0()
{    return new TestReaderIterator();}
public boolean beam_f10531_0()
{    return false;}
public void beam_f10539_0() throws Exception
{    CloudObject spec = CloudObject.forClass(TestReaderFactory.class);    Source cloudSource = new Source();    cloudSource.setSpec(spec);    cloudSource.setCodec(CloudObjects.asCloudObject(BigEndianIntegerCoder.of(), /*sdkComponents=*/    null));    PipelineOptions options = PipelineOptionsFactory.create();    ReaderRegistry registry = ReaderRegistry.defaultRegistry().register(TestReaderFactory.class.getName(), new TestReaderFactory());    NativeReader<?> reader = registry.create(cloudSource, PipelineOptionsFactory.create(), BatchModeExecutionContext.forTesting(options, "testStage"), null);    Assert.assertThat(reader, new IsInstanceOf(TestReader.class));}
public void beam_f10540_0() throws Exception
{    CloudObject spec = CloudObject.forClassName("UnknownSource");    Source cloudSource = new Source();    cloudSource.setSpec(spec);    cloudSource.setCodec(CloudObjects.asCloudObject(StringUtf8Coder.of(), /*sdkComponents=*/    null));    try {        PipelineOptions options = PipelineOptionsFactory.create();        ReaderRegistry.defaultRegistry().create(cloudSource, options, BatchModeExecutionContext.forTesting(options, "testStage"), null);        Assert.fail("should have thrown an exception");    } catch (Exception exn) {        Assert.assertThat(exn.toString(), CoreMatchers.containsString("Unable to create a Reader"));    }}
public static Position beam_f10541_0(@Nullable Long index)
{    return new Position().setRecordIndex(index);}
public static ApproximateSplitRequest beam_f10549_0(@Nullable Long byteOffset)
{    return approximateSplitRequestAtPosition(positionAtByteOffset(byteOffset));}
public static ApproximateReportedProgress beam_f10550_0(@Nullable Integer index, @Nullable Position innerPosition)
{    return approximateProgressAtPosition(positionAtConcatPosition(index, innerPosition));}
public static ApproximateSplitRequest beam_f10551_0(@Nullable Integer index, @Nullable Position innerPosition)
{    return approximateSplitRequestAtPosition(positionAtConcatPosition(index, innerPosition));}
public static Position beam_f10559_0(NativeReader.DynamicSplitResult dynamicSplitResult)
{    return toCloudPosition(((NativeReader.DynamicSplitResultWithPosition) dynamicSplitResult).getAcceptedPosition());}
public static Position beam_f10560_0(NativeReader.Progress progress)
{    return readerProgressToCloudProgress(progress).getPosition();}
public static NativeReader.DynamicSplitRequest beam_f10561_0(double fraction)
{    return toDynamicSplitRequest(approximateSplitRequestAtFraction(fraction));}
public void beam_f10569_0()
{    Coder<?> coder = CloudObjects.coderFromCloudObject(CloudObject.fromSpec(ImmutableMap.of(PropertyNames.OBJECT_TYPE_NAME, "kind:fixed_big_endian_int64")));    assertThat(coder, instanceOf(BigEndianLongCoder.class));}
public void beam_f10570_0()
{    Coder<?> coder = CloudObjects.coderFromCloudObject(CloudObject.fromSpec(ImmutableMap.of(PropertyNames.OBJECT_TYPE_NAME, "kind:void")));    assertThat(coder, instanceOf(VoidCoder.class));}
public void beam_f10571_0()
{    Coder<?> coder = CloudObjects.coderFromCloudObject(CloudObject.fromSpec(ImmutableMap.of(PropertyNames.OBJECT_TYPE_NAME, "kind:ism_record", "num_shard_key_coders", 1L, PropertyNames.COMPONENT_ENCODINGS, ImmutableList.of(ImmutableMap.of("@type", "kind:var_int32"), ImmutableMap.of("@type", "kind:fixed_big_endian_int32"), ImmutableMap.of("@type", "kind:bytes")))));    assertThat(coder, instanceOf(IsmRecordCoder.class));    IsmRecordCoder<?> ismRecordCoder = (IsmRecordCoder<?>) coder;    assertEquals(1, ismRecordCoder.getNumberOfShardKeyCoders(ImmutableList.of()));        assertEquals(0, ismRecordCoder.getNumberOfShardKeyCoders(ImmutableList.of(IsmFormat.getMetadataKey())));    assertEquals(ImmutableList.of(VarIntCoder.of(), BigEndianIntegerCoder.of(), ByteArrayCoder.of()), ismRecordCoder.getCoderArguments());}
public void beam_f10579_0() throws Exception
{    runTestCreateUngroupedShuffleReader(new byte[] { (byte) 0xE2 }, "aaa", "zzz", BigEndianIntegerCoder.of());}
public void beam_f10580_0() throws Exception
{    runTestCreateGroupingShuffleReader(new byte[] { (byte) 0xE1 }, null, null, BigEndianIntegerCoder.of(), StringUtf8Coder.of());}
public void beam_f10581_0() throws Exception
{    runTestCreateGroupingShuffleReader(new byte[] { (byte) 0xE2 }, "aaa", "zzz", BigEndianIntegerCoder.of(), KvCoder.of(StringUtf8Coder.of(), VoidCoder.of()));}
public void beam_f10589_0() throws Exception
{    FullWindowedValueCoder<?> coder = WindowedValue.getFullCoder(StringUtf8Coder.of(), IntervalWindow.getCoder());    runTestCreateUngroupingShuffleSink(new byte[] { (byte) 0xE1 }, coder, coder);}
public void beam_f10590_0() throws Exception
{    runTestCreatePartitioningShuffleSink(new byte[] { (byte) 0xE2 }, BigEndianIntegerCoder.of(), StringUtf8Coder.of());}
public void beam_f10591_0() throws Exception
{    runTestCreateGroupingShuffleSink(new byte[] { (byte) 0xE2 }, BigEndianIntegerCoder.of(), WindowedValue.getFullCoder(StringUtf8Coder.of(), IntervalWindow.getCoder()));}
public void beam_f10599_0() throws Exception
{    runTestWriteGroupingShuffleSink(KVS);}
public void beam_f10600_0() throws Exception
{    runTestWriteGroupingSortingShuffleSink(NO_SORTING_KVS);}
public void beam_f10601_0() throws Exception
{    runTestWriteGroupingSortingShuffleSink(SORTING_KVS);}
private void beam_f10609_0(String s)
{    nestedFunctionBeta(s);}
public void beam_f10610_0()
{    nestedFunctionAlpha("test error in initialize");}
public void beam_f10611_0(ProcessContext c)
{    nestedFunctionBeta("test error in process");}
public void beam_f10619_0() throws Exception
{    ExecutionStateTracker tracker = ExecutionStateTracker.newForTest();    TestOperationContext operationContext = TestOperationContext.create(new CounterSet(), NameContextsForTests.nameContextForTest(), new MetricsContainerImpl(NameContextsForTests.ORIGINAL_NAME), tracker);    class StateTestingDoFn extends DoFn<Integer, String> {        private boolean startCalled = false;        @StartBundle        public void startBundle() throws Exception {            startCalled = true;            assertThat(tracker.getCurrentState(), equalTo(operationContext.getStartState()));        }        @ProcessElement        public void processElement(ProcessContext c) throws Exception {            assertThat(startCalled, equalTo(true));            assertThat(tracker.getCurrentState(), equalTo(operationContext.getProcessState()));        }    }    StateTestingDoFn fn = new StateTestingDoFn();    DoFnInfo<?, ?> fnInfo = DoFnInfo.forFn(fn, WindowingStrategy.globalDefault(), null, /* side input views */    null, /* input coder */    MAIN_OUTPUT, DoFnSchemaInformation.create(), Collections.emptyMap());    ParDoFn userParDoFn = new SimpleParDoFn<>(options, DoFnInstanceManagers.singleInstance(fnInfo), NullSideInputReader.empty(), MAIN_OUTPUT, ImmutableMap.of(MAIN_OUTPUT, 0, new TupleTag<>("declared"), 1), BatchModeExecutionContext.forTesting(options, operationContext.counterFactory(), "testStage").getStepContext(operationContext), operationContext, DoFnSchemaInformation.create(), Collections.emptyMap(), SimpleDoFnRunnerFactory.INSTANCE);        try (Closeable trackerCloser = tracker.activate()) {        try (Closeable processCloser = operationContext.enterProcess()) {            userParDoFn.processElement(WindowedValue.valueInGlobalWindow(5));        }    }}
public void beam_f10620_0() throws Exception
{    startCalled = true;    assertThat(tracker.getCurrentState(), equalTo(operationContext.getStartState()));}
public void beam_f10621_0(ProcessContext c) throws Exception
{    assertThat(startCalled, equalTo(true));    assertThat(tracker.getCurrentState(), equalTo(operationContext.getProcessState()));}
public void beam_f10629_0() throws Exception
{    CloudObject spec = CloudObject.forClassName("AvroSink");    addString(spec, "filename", "/path/to/file.txt");    SizeReportingSinkWrapper<?> sink = SinkRegistry.defaultRegistry().create(spec, StringUtf8Coder.of(), options, BatchModeExecutionContext.forTesting(options, "testStage"), TestOperationContext.create());    Assert.assertThat(sink.getUnderlyingSink(), new IsInstanceOf(AvroByteSink.class));}
public void beam_f10630_0() throws Exception
{    CloudObject spec = CloudObject.forClassName("UnknownSink");    com.google.api.services.dataflow.model.Sink cloudSink = new com.google.api.services.dataflow.model.Sink();    cloudSink.setSpec(spec);    cloudSink.setCodec(CloudObjects.asCloudObject(StringUtf8Coder.of(), /*sdkComponents=*/    null));    try {        SinkRegistry.defaultRegistry().create(spec, StringUtf8Coder.of(), options, BatchModeExecutionContext.forTesting(options, "testStage"), TestOperationContext.create());        Assert.fail("should have thrown an exception");    } catch (Exception exn) {        Assert.assertThat(exn.toString(), CoreMatchers.containsString("Unable to create a Sink"));    }}
public void beam_f10631_0()
{    MockitoAnnotations.initMocks(this);}
private Windmill.GlobalData beam_f10639_0(String tag, ByteString version, boolean isReady, ByteString data)
{    Windmill.GlobalData.Builder builder = Windmill.GlobalData.newBuilder().setDataId(Windmill.GlobalDataId.newBuilder().setTag(tag).setVersion(version).build());    if (isReady) {        builder.setIsReady(true).setData(data);    } else {        builder.setIsReady(false);    }    return builder.build();}
private Windmill.GlobalDataRequest beam_f10640_0(String tag, ByteString version)
{    Windmill.GlobalDataId id = Windmill.GlobalDataId.newBuilder().setTag(tag).setVersion(version).build();    return Windmill.GlobalDataRequest.newBuilder().setDataId(id).setStateFamily(STATE_FAMILY).setExistenceWatermarkDeadline(TimeUnit.MILLISECONDS.toMicros(GlobalWindow.INSTANCE.maxTimestamp().getMillis())).build();}
private Dataflow beam_f10641_0() throws Exception
{    Dataflow mockDataflowClient = mock(Dataflow.class);    Dataflow.Projects mockProjects = mock(Dataflow.Projects.class);    Dataflow.Projects.Locations mockRegion = mock(Dataflow.Projects.Locations.class);    Dataflow.Projects.Locations.Jobs mockJobs = mock(Dataflow.Projects.Locations.Jobs.class);    Dataflow.Projects.Locations.Jobs.Debug mockDebug = mock(Dataflow.Projects.Locations.Jobs.Debug.class);    Dataflow.Projects.Locations.Jobs.Debug.GetConfig mockGetConfig = mock(Dataflow.Projects.Locations.Jobs.Debug.GetConfig.class);    when(mockDataflowClient.projects()).thenReturn(mockProjects);    when(mockProjects.locations()).thenReturn(mockRegion);    when(mockRegion.jobs()).thenReturn(mockJobs);    when(mockJobs.debug()).thenReturn(mockDebug);    when(mockDebug.getConfig(eq(PROJECT_ID), eq(REGION), eq(JOB_ID), isA(GetDebugConfigRequest.class))).thenReturn(mockGetConfig);    when(mockDebug.sendCapture(eq(PROJECT_ID), eq(REGION), eq(JOB_ID), isA(SendDebugCaptureRequest.class))).thenReturn(mockSendCapture);    when(mockGetConfig.execute()).thenReturn(fakeGetConfigResponse);    return mockDataflowClient;}
public void beam_f10649_0() throws Exception
{    String response = getPage("/threadz");    assertThat(response, containsString("HTTP/1.1 200 OK"));    assertThat("Test method should appear in stack trace", response, containsString("WorkerStatusPagesTest.testThreadz"));}
public void beam_f10650_0() throws Exception
{    String response = getPage("/healthz");    assertThat(response, containsString("HTTP/1.1 200 OK"));    assertThat(response, containsString("ok"));}
public void beam_f10651_0() throws Exception
{        wsp.stop();    wsp = new WorkerStatusPages(server, mockMemoryMonitor, () -> false);    wsp.start();    String response = getPage("/healthz");    assertThat(response, containsString("HTTP/1.1 500 Server Error"));    assertThat(response, containsString("internal server error"));}
private String beam_f10659_0(long index)
{    return DEFAULT_DATA_STRING + index;}
private ParallelInstruction beam_f10660_0(Coder<?> coder)
{    CloudObject timerCloudObject = CloudObject.forClassName("com.google.cloud.dataflow.sdk.util.TimerOrElement$TimerOrElementCoder");    List<CloudObject> component = Collections.singletonList(CloudObjects.asCloudObject(coder, /*sdkComponents=*/    null));    Structs.addList(timerCloudObject, PropertyNames.COMPONENT_ENCODINGS, component);    CloudObject encodedCoder = CloudObject.forClassName("kind:windowed_value");    Structs.addBoolean(encodedCoder, PropertyNames.IS_WRAPPER, true);    Structs.addList(encodedCoder, PropertyNames.COMPONENT_ENCODINGS, ImmutableList.of(timerCloudObject, CloudObjects.asCloudObject(IntervalWindowCoder.of(), /*sdkComponents=*/    null)));    return new ParallelInstruction().setSystemName(DEFAULT_SOURCE_SYSTEM_NAME).setOriginalName(DEFAULT_SOURCE_ORIGINAL_NAME).setRead(new ReadInstruction().setSource(new Source().setSpec(CloudObject.forClass(WindowingWindmillReader.class)).setCodec(encodedCoder))).setOutputs(Arrays.asList(new InstructionOutput().setName(Long.toString(idGenerator.get())).setCodec(encodedCoder).setOriginalName(DEFAULT_OUTPUT_ORIGINAL_NAME).setSystemName(DEFAULT_OUTPUT_SYSTEM_NAME)));}
private ParallelInstruction beam_f10661_0(Coder<?> coder)
{    return new ParallelInstruction().setSystemName(DEFAULT_SOURCE_SYSTEM_NAME).setOriginalName(DEFAULT_SOURCE_ORIGINAL_NAME).setRead(new ReadInstruction().setSource(new Source().setSpec(CloudObject.forClass(UngroupedWindmillReader.class)).setCodec(CloudObjects.asCloudObject(WindowedValue.getFullCoder(coder, IntervalWindow.getCoder()), /*sdkComponents=*/    null)))).setOutputs(Arrays.asList(new InstructionOutput().setName(Long.toString(idGenerator.get())).setOriginalName(DEFAULT_OUTPUT_ORIGINAL_NAME).setSystemName(DEFAULT_OUTPUT_SYSTEM_NAME).setCodec(CloudObjects.asCloudObject(WindowedValue.getFullCoder(coder, IntervalWindow.getCoder()), /*sdkComponents=*/    null))));}
private Windmill.GetWorkResponse beam_f10669_0(int workToken, long inputWatermark, long outputWatermark, List<Long> inputs, List<Timer> timers) throws Exception
{        Windmill.WorkItem.Builder builder = Windmill.WorkItem.newBuilder();    builder.setKey(DEFAULT_KEY_BYTES);    builder.setShardingKey(1);    builder.setCacheToken(1);    builder.setWorkToken(workToken);    builder.setOutputDataWatermark(outputWatermark * 1000);    if (!inputs.isEmpty()) {        InputMessageBundle.Builder messageBuilder = Windmill.InputMessageBundle.newBuilder().setSourceComputationId(DEFAULT_SOURCE_COMPUTATION_ID);        for (Long input : inputs) {            messageBuilder.addMessages(Windmill.Message.newBuilder().setTimestamp(input).setData(ByteString.copyFromUtf8(dataStringForIndex(input))).setMetadata(addPaneTag(PaneInfo.NO_FIRING, intervalWindowBytes(new IntervalWindow(new Instant(input), new Instant(input).plus(Duration.millis(10)))))));        }        builder.addMessageBundles(messageBuilder);    }    if (!timers.isEmpty()) {        builder.setTimers(Windmill.TimerBundle.newBuilder().addAllTimers(timers));    }    return Windmill.GetWorkResponse.newBuilder().addWork(Windmill.ComputationWorkItems.newBuilder().setComputationId(DEFAULT_COMPUTATION_ID).setInputDataWatermark(inputWatermark * 1000).addWork(builder)).build();}
private Windmill.GetWorkResponse beam_f10670_0(int index, long timestamp) throws Exception
{    return makeInput(index, timestamp, keyStringForIndex(index));}
private Windmill.GetWorkResponse beam_f10671_0(int index, long timestamp, String key) throws Exception
{    return buildInput("work {" + "  computation_id: \"" + DEFAULT_COMPUTATION_ID + "\"" + "  input_data_watermark: 0" + "  work {" + "    key: \"" + key + "\"" + "    sharding_key: 17" + "    work_token: " + index + "    cache_token: 3" + "    hot_key_info {" + "      hot_key_age_usec: 1000000" + "    }" + "    message_bundles {" + "      source_computation_id: \"" + DEFAULT_SOURCE_COMPUTATION_ID + "\"" + "      messages {" + "        timestamp: " + timestamp + "        data: \"data" + index + "\"" + "      }" + "    }" + "  }" + "}", CoderUtils.encodeToByteArray(CollectionCoder.of(IntervalWindow.getCoder()), Arrays.asList(DEFAULT_WINDOW)));}
private StreamingDataflowWorkerOptions beam_f10679_0(FakeWindmillServer server, String... args)
{    List<String> argsList = Lists.newArrayList(args);    if (streamingEngine) {        argsList.add("--experiments=enable_streaming_engine");    }    StreamingDataflowWorkerOptions options = PipelineOptionsFactory.fromArgs(argsList.toArray(new String[0])).as(StreamingDataflowWorkerOptions.class);    options.setAppName("StreamingWorkerHarnessTest");    options.setJobId("test_job_id");    options.setStreaming(true);    options.setWindmillServerStub(server);    options.setActiveWorkRefreshPeriodMillis(0);    return options;}
private StreamingDataflowWorker beam_f10680_0(List<ParallelInstruction> instructions, StreamingDataflowWorkerOptions options, boolean publishCounters) throws Exception
{    StreamingDataflowWorker worker = new StreamingDataflowWorker(Arrays.asList(defaultMapTask(instructions)), IntrinsicMapTaskExecutorFactory.defaultFactory(), mockWorkUnitClient, options, null, /* pipeline */    SdkHarnessRegistries.emptySdkHarnessRegistry(), publishCounters, hotKeyLogger);    worker.addStateNameMappings(ImmutableMap.of(DEFAULT_PARDO_USER_NAME, DEFAULT_PARDO_STATE_FAMILY));    return worker;}
public void beam_f10681_0() throws Exception
{    List<ParallelInstruction> instructions = Arrays.asList(makeSourceInstruction(StringUtf8Coder.of()), makeSinkInstruction(StringUtf8Coder.of(), 0));    FakeWindmillServer server = new FakeWindmillServer(errorCollector);    StreamingDataflowWorkerOptions options = createTestingPipelineOptions(server);    StreamingDataflowWorker worker = makeWorker(instructions, options, true);    worker.start();    final int numIters = 2000;    for (int i = 0; i < numIters; ++i) {        server.addWorkToOffer(makeInput(i, TimeUnit.MILLISECONDS.toMicros(i)));    }    Map<Long, Windmill.WorkItemCommitRequest> result = server.waitForAndGetCommits(numIters);    worker.stop();    for (int i = 0; i < numIters; ++i) {        assertTrue(result.containsKey((long) i));        assertEquals(makeExpectedOutput(i, TimeUnit.MILLISECONDS.toMicros(i)).build(), result.get((long) i));    }    verify(hotKeyLogger, atLeastOnce()).logHotKeyDetection(nullable(String.class), any());}
public void beam_f10689_0() throws Exception
{    if (streamingEngine) {                return;    }    KvCoder<String, String> kvCoder = KvCoder.of(StringUtf8Coder.of(), StringUtf8Coder.of());    List<ParallelInstruction> instructions = Arrays.asList(makeSourceInstruction(kvCoder), makeDoFnInstruction(new KeyTokenInvalidFn(), 0, kvCoder), makeSinkInstruction(kvCoder, 1));    FakeWindmillServer server = new FakeWindmillServer(errorCollector);    server.addWorkToOffer(makeInput(0, 0, "key"));    StreamingDataflowWorker worker = makeWorker(instructions, createTestingPipelineOptions(server), true);    worker.start();    server.waitForEmptyWorkQueue();    server.addWorkToOffer(makeInput(1, 0, "key"));    Map<Long, Windmill.WorkItemCommitRequest> result = server.waitForAndGetCommits(1);    assertEquals(makeExpectedOutput(1, 0, "key", "key").build(), result.get(1L));    assertEquals(1, result.size());}
public void beam_f10690_0(ProcessContext c)
{    if (c.element().getKey().equals("large_key")) {        StringBuilder s = new StringBuilder();        for (int i = 0; i < 100; ++i) s.append("large_commit");        c.output(KV.of(c.element().getKey(), s.toString()));    } else {        c.output(c.element());    }}
public void beam_f10691_0() throws Exception
{    KvCoder<String, String> kvCoder = KvCoder.of(StringUtf8Coder.of(), StringUtf8Coder.of());    List<ParallelInstruction> instructions = Arrays.asList(makeSourceInstruction(kvCoder), makeDoFnInstruction(new LargeCommitFn(), 0, kvCoder), makeSinkInstruction(kvCoder, 1));    FakeWindmillServer server = new FakeWindmillServer(errorCollector);    server.setExpectedExceptionCount(1);    StreamingDataflowWorker worker = makeWorker(instructions, createTestingPipelineOptions(server), true);    worker.setMaxWorkItemCommitBytes(1000);    worker.start();    server.addWorkToOffer(makeInput(1, 0, "large_key"));    server.addWorkToOffer(makeInput(2, 0, "key"));    server.waitForEmptyWorkQueue();    Map<Long, Windmill.WorkItemCommitRequest> result = server.waitForAndGetCommits(1);    assertEquals(2, result.size());    assertEquals(makeExpectedOutput(2, 0, "key", "key").build(), result.get(2L));    assertTrue(result.containsKey(1L));    assertEquals("large_key", result.get(1L).getKey().toStringUtf8());    assertTrue(result.get(1L).getSerializedSize() > 1000);        int maxTries = 10;    while (--maxTries > 0) {        worker.reportPeriodicWorkerUpdates();        Uninterruptibles.sleepUninterruptibly(1000, TimeUnit.MILLISECONDS);    }        ArgumentCaptor<WorkItemStatus> workItemStatusCaptor = ArgumentCaptor.forClass(WorkItemStatus.class);    verify(mockWorkUnitClient, atLeast(2)).reportWorkItemStatus(workItemStatusCaptor.capture());    List<WorkItemStatus> capturedStatuses = workItemStatusCaptor.getAllValues();    boolean foundErrors = false;    for (WorkItemStatus status : capturedStatuses) {        if (!status.getErrors().isEmpty()) {            assertFalse(foundErrors);            foundErrors = true;            String errorMessage = status.getErrors().get(0).getMessage();            assertThat(errorMessage, Matchers.containsString("KeyCommitTooLargeException"));        }    }    assertTrue(foundErrors);}
private void beam_f10699_0(WorkItemCommitRequest commit, WatermarkHold... watermarkHolds)
{    assertThat(commit.getWatermarkHoldsList(), Matchers.containsInAnyOrder(watermarkHolds));}
private Timer beam_f10700_0(String tagPrefix, long timestampMillis)
{    return buildWatermarkTimer(tagPrefix, timestampMillis, false);}
private Timer beam_f10701_0(String tagPrefix, long timestampMillis, boolean delete)
{    Timer.Builder builder = Timer.newBuilder().setTag(ByteString.copyFromUtf8(tagPrefix + ":" + timestampMillis)).setType(Type.WATERMARK).setStateFamily("MergeWindows");    if (!delete) {        builder.setTimestamp(timestampMillis * 1000);    }    return builder.build();}
public void beam_f10709_0(ProcessContext c)
{    KV<Integer, Integer> elem = c.element().getValue();    c.output(elem.getKey() + ":" + elem.getValue());}
private List<ParallelInstruction> beam_f10710_0() throws Exception
{    return makeUnboundedSourcePipeline(1, new PrintFn());}
private List<ParallelInstruction> beam_f10711_0(int numMessagesPerShard, DoFn<ValueWithRecordId<KV<Integer, Integer>>, String> doFn) throws Exception
{    DataflowPipelineOptions options = PipelineOptionsFactory.create().as(DataflowPipelineOptions.class);    options.setNumWorkers(1);    CloudObject codec = CloudObjects.asCloudObject(WindowedValue.getFullCoder(ValueWithRecordId.ValueWithRecordIdCoder.of(KvCoder.of(VarIntCoder.of(), VarIntCoder.of())), GlobalWindow.Coder.INSTANCE), /*sdkComponents=*/    null);    return Arrays.asList(new ParallelInstruction().setSystemName("Read").setOriginalName("OriginalReadName").setRead(new ReadInstruction().setSource(CustomSources.serializeToCloudSource(new TestCountingSource(numMessagesPerShard), options).setCodec(codec))).setOutputs(Arrays.asList(new InstructionOutput().setName("read_output").setOriginalName(DEFAULT_OUTPUT_ORIGINAL_NAME).setSystemName(DEFAULT_OUTPUT_SYSTEM_NAME).setCodec(codec))), makeDoFnInstruction(doFn, 0, StringUtf8Coder.of(), WindowingStrategy.globalDefault()), makeSinkInstruction(StringUtf8Coder.of(), 1, GlobalWindow.Coder.INSTANCE));}
public void beam_f10720_0(ProcessContext c) throws Exception
{    Thread.sleep(1000);    c.output(c.element());}
public void beam_f10721_0() throws Exception
{    if (streamingEngine) {                return;    }    List<ParallelInstruction> instructions = Arrays.asList(makeSourceInstruction(StringUtf8Coder.of()), makeDoFnInstruction(new SlowDoFn(), 0, StringUtf8Coder.of()), makeSinkInstruction(StringUtf8Coder.of(), 0));    FakeWindmillServer server = new FakeWindmillServer(errorCollector);    StreamingDataflowWorkerOptions options = createTestingPipelineOptions(server);    options.setActiveWorkRefreshPeriodMillis(100);    StreamingDataflowWorker worker = makeWorker(instructions, options, true);    worker.start();    server.addWorkToOffer(makeInput(0, TimeUnit.MILLISECONDS.toMicros(0)));    server.waitForAndGetCommits(1);    worker.stop();            assertThat(server.numGetDataRequests(), greaterThan(0));}
public void beam_f10722_0(ProcessContext c)
{    char[] chars = new char[inflatedSize];    Arrays.fill(chars, ' ');    c.output(new String(chars));}
private WindowedValue<KeyedWorkItem<String, T>> beam_f10730_0(WorkItem.Builder workItem, Coder<T> valueCoder)
{    @SuppressWarnings({ "unchecked", "rawtypes" })    Coder<Collection<? extends BoundedWindow>> wildcardWindowsCoder = (Coder) windowsCoder;    return new ValueInEmptyWindows<>((KeyedWorkItem<String, T>) new WindmillKeyedWorkItem<>(KEY, workItem.build(), windowCoder, wildcardWindowsCoder, valueCoder));}
public void beam_f10731_0() throws Exception
{    TupleTag<KV<String, Iterable<String>>> outputTag = new TupleTag<>();    ListOutputManager outputManager = new ListOutputManager();    DoFnRunner<KeyedWorkItem<String, String>, KV<String, Iterable<String>>> runner = makeRunner(outputTag, outputManager, WindowingStrategy.of(FixedWindows.of(Duration.millis(10))));    when(mockTimerInternals.currentInputWatermarkTime()).thenReturn(new Instant(0));    runner.startBundle();    WorkItem.Builder workItem1 = WorkItem.newBuilder();    workItem1.setKey(ByteString.copyFromUtf8(KEY));    workItem1.setWorkToken(WORK_TOKEN);    InputMessageBundle.Builder messageBundle = workItem1.addMessageBundlesBuilder();    messageBundle.setSourceComputationId(SOURCE_COMPUTATION_ID);    Coder<String> valueCoder = StringUtf8Coder.of();    addElement(messageBundle, Arrays.asList(window(0, 10)), new Instant(1), valueCoder, "v1");    addElement(messageBundle, Arrays.asList(window(0, 10)), new Instant(2), valueCoder, "v2");    addElement(messageBundle, Arrays.asList(window(0, 10)), new Instant(0), valueCoder, "v0");    addElement(messageBundle, Arrays.asList(window(10, 20)), new Instant(13), valueCoder, "v3");    runner.processElement(createValue(workItem1, valueCoder));    runner.finishBundle();    runner.startBundle();    WorkItem.Builder workItem2 = WorkItem.newBuilder();    workItem2.setKey(ByteString.copyFromUtf8(KEY));    workItem2.setWorkToken(WORK_TOKEN);    addTimer(workItem2, window(0, 10), new Instant(9), Timer.Type.WATERMARK);    addTimer(workItem2, window(10, 20), new Instant(19), Timer.Type.WATERMARK);    when(mockTimerInternals.currentInputWatermarkTime()).thenReturn(new Instant(20));    runner.processElement(createValue(workItem2, valueCoder));    runner.finishBundle();    List<WindowedValue<KV<String, Iterable<String>>>> result = outputManager.getOutput(outputTag);    assertThat(result.size(), equalTo(2));    assertThat(result, containsInAnyOrder(WindowMatchers.isSingleWindowedValue(isKv(equalTo(KEY), containsInAnyOrder("v0", "v1", "v2")), equalTo(window(0, 10).maxTimestamp()), equalTo(window(0, 10))), WindowMatchers.isSingleWindowedValue(isKv(equalTo(KEY), containsInAnyOrder("v3")), equalTo(window(10, 20).maxTimestamp()), equalTo(window(10, 20)))));}
public void beam_f10732_0() throws Exception
{    TupleTag<KV<String, Iterable<String>>> outputTag = new TupleTag<>();    ListOutputManager outputManager = new ListOutputManager();    DoFnRunner<KeyedWorkItem<String, String>, KV<String, Iterable<String>>> runner = makeRunner(outputTag, outputManager, WindowingStrategy.of(SlidingWindows.of(Duration.millis(20)).every(Duration.millis(10))).withTimestampCombiner(TimestampCombiner.EARLIEST));    when(mockTimerInternals.currentInputWatermarkTime()).thenReturn(new Instant(5));    runner.startBundle();    WorkItem.Builder workItem1 = WorkItem.newBuilder();    workItem1.setKey(ByteString.copyFromUtf8(KEY));    workItem1.setWorkToken(WORK_TOKEN);    InputMessageBundle.Builder messageBundle = workItem1.addMessageBundlesBuilder();    messageBundle.setSourceComputationId(SOURCE_COMPUTATION_ID);    Coder<String> valueCoder = StringUtf8Coder.of();    addElement(messageBundle, Arrays.asList(window(-10, 10), window(0, 20)), new Instant(5), valueCoder, "v1");    addElement(messageBundle, Arrays.asList(window(-10, 10), window(0, 20)), new Instant(2), valueCoder, "v0");    addElement(messageBundle, Arrays.asList(window(0, 20), window(10, 30)), new Instant(15), valueCoder, "v2");    runner.processElement(createValue(workItem1, valueCoder));    runner.finishBundle();    runner.startBundle();    WorkItem.Builder workItem2 = WorkItem.newBuilder();    workItem2.setKey(ByteString.copyFromUtf8(KEY));    workItem2.setWorkToken(WORK_TOKEN);    addTimer(workItem2, window(-10, 10), new Instant(9), Timer.Type.WATERMARK);    addTimer(workItem2, window(0, 20), new Instant(19), Timer.Type.WATERMARK);    addTimer(workItem2, window(10, 30), new Instant(29), Timer.Type.WATERMARK);    when(mockTimerInternals.currentInputWatermarkTime()).thenReturn(new Instant(30));    runner.processElement(createValue(workItem2, valueCoder));    runner.finishBundle();    List<WindowedValue<KV<String, Iterable<String>>>> result = outputManager.getOutput(outputTag);    assertThat(result.size(), equalTo(3));    assertThat(result, containsInAnyOrder(WindowMatchers.isSingleWindowedValue(isKv(equalTo(KEY), containsInAnyOrder("v0", "v1")), equalTo(new Instant(2)), equalTo(window(-10, 10))),     WindowMatchers.isSingleWindowedValue(isKv(equalTo(KEY), containsInAnyOrder("v0", "v1", "v2")), equalTo(window(-10, 10).maxTimestamp().plus(1)), equalTo(window(0, 20))), WindowMatchers.isSingleWindowedValue(isKv(equalTo(KEY), containsInAnyOrder("v2")), equalTo(window(0, 20).maxTimestamp().plus(1)), equalTo(window(10, 30)))));}
public Long beam_f10740_0(Long accumulator)
{    return accumulator;}
public void beam_f10741_0() throws Exception
{    TupleTag<KV<String, Long>> outputTag = new TupleTag<>();    CombineFn<Long, ?, Long> combineFn = new SumLongs();    CoderRegistry registry = CoderRegistry.createDefault();    AppliedCombineFn<String, Long, ?, Long> appliedCombineFn = AppliedCombineFn.withInputCoder(combineFn, registry, KvCoder.of(StringUtf8Coder.of(), BigEndianLongCoder.of()));    ListOutputManager outputManager = new ListOutputManager();    DoFnRunner<KeyedWorkItem<String, Long>, KV<String, Long>> runner = makeRunner(outputTag, outputManager, WindowingStrategy.of(Sessions.withGapDuration(Duration.millis(10))), appliedCombineFn);    when(mockTimerInternals.currentInputWatermarkTime()).thenReturn(new Instant(0));    runner.startBundle();    WorkItem.Builder workItem1 = WorkItem.newBuilder();    workItem1.setKey(ByteString.copyFromUtf8(KEY));    workItem1.setWorkToken(WORK_TOKEN);    InputMessageBundle.Builder messageBundle = workItem1.addMessageBundlesBuilder();    messageBundle.setSourceComputationId(SOURCE_COMPUTATION_ID);    Coder<Long> valueCoder = BigEndianLongCoder.of();    addElement(messageBundle, Arrays.asList(window(0, 10)), new Instant(0), valueCoder, 1L);    addElement(messageBundle, Arrays.asList(window(5, 15)), new Instant(5), valueCoder, 2L);    addElement(messageBundle, Arrays.asList(window(15, 25)), new Instant(15), valueCoder, 3L);    addElement(messageBundle, Arrays.asList(window(3, 13)), new Instant(3), valueCoder, 4L);    runner.processElement(createValue(workItem1, valueCoder));    runner.finishBundle();    runner.startBundle();    WorkItem.Builder workItem2 = WorkItem.newBuilder();    workItem2.setKey(ByteString.copyFromUtf8(KEY));    workItem2.setWorkToken(WORK_TOKEN);            addTimer(workItem2, window(0, 15), new Instant(14), Timer.Type.WATERMARK);    addTimer(workItem2, window(15, 25), new Instant(24), Timer.Type.WATERMARK);    when(mockTimerInternals.currentInputWatermarkTime()).thenReturn(new Instant(25));    runner.processElement(createValue(workItem2, valueCoder));    runner.finishBundle();    List<WindowedValue<KV<String, Long>>> result = outputManager.getOutput(outputTag);    assertThat(result.size(), equalTo(2));    assertThat(result, containsInAnyOrder(WindowMatchers.isSingleWindowedValue(isKv(equalTo(KEY), equalTo(7L)), equalTo(window(0, 15).maxTimestamp()), equalTo(window(0, 15))), WindowMatchers.isSingleWindowedValue(isKv(equalTo(KEY), equalTo(3L)), equalTo(window(15, 25).maxTimestamp()), equalTo(window(15, 25)))));}
private DoFnRunner<KeyedWorkItem<String, String>, KV<String, Iterable<String>>> beam_f10742_0(TupleTag<KV<String, Iterable<String>>> outputTag, DoFnRunners.OutputManager outputManager, WindowingStrategy<? super String, IntervalWindow> windowingStrategy) throws Exception
{    GroupAlsoByWindowFn<KeyedWorkItem<String, String>, KV<String, Iterable<String>>> fn = StreamingGroupAlsoByWindowsDoFns.createForIterable(windowingStrategy, new StepContextStateInternalsFactory<String>(stepContext), StringUtf8Coder.of());    return makeRunner(outputTag, outputManager, windowingStrategy, fn);}
private void beam_f10750_0(InputMessageBundle.Builder messageBundle, Collection<IntervalWindow> windows, Instant timestamp, Coder<V> valueCoder, V value) throws IOException
{    @SuppressWarnings({ "unchecked", "rawtypes" })    Coder<Collection<? extends BoundedWindow>> windowsCoder = (Coder) CollectionCoder.of(windowCoder);    ByteString.Output dataOutput = ByteString.newOutput();    valueCoder.encode(value, dataOutput, Context.OUTER);    messageBundle.addMessagesBuilder().setMetadata(WindmillSink.encodeMetadata(windowsCoder, windows, PaneInfo.NO_FIRING)).setData(dataOutput.toByteString()).setTimestamp(WindmillTimeUtils.harnessToWindmillTimestamp(timestamp));}
private WindowedValue<KeyedWorkItem<String, T>> beam_f10751_0(WorkItem.Builder workItem, Coder<T> valueCoder)
{    @SuppressWarnings({ "unchecked", "rawtypes" })    Coder<Collection<? extends BoundedWindow>> wildcardWindowsCoder = (Coder) windowsCoder;    return new ValueInEmptyWindows<>((KeyedWorkItem<String, T>) new WindmillKeyedWorkItem<>(KEY, workItem.build(), windowCoder, wildcardWindowsCoder, valueCoder));}
public void beam_f10752_0() throws Exception
{    TupleTag<KV<String, Iterable<String>>> outputTag = new TupleTag<>();    ListOutputManager outputManager = new ListOutputManager();    DoFnRunner<KeyedWorkItem<String, String>, KV<String, Iterable<String>>> runner = makeRunner(outputTag, outputManager, WindowingStrategy.of(FixedWindows.of(Duration.millis(10))));    runner.startBundle();    WorkItem.Builder workItem = WorkItem.newBuilder();    workItem.setKey(ByteString.copyFromUtf8(KEY));    workItem.setWorkToken(WORK_TOKEN);    InputMessageBundle.Builder messageBundle = workItem.addMessageBundlesBuilder();    messageBundle.setSourceComputationId(SOURCE_COMPUTATION_ID);    Coder<String> valueCoder = StringUtf8Coder.of();    addElement(messageBundle, Arrays.asList(window(0, 10)), new Instant(1), valueCoder, "v1");    addElement(messageBundle, Arrays.asList(window(0, 10)), new Instant(2), valueCoder, "v2");    addElement(messageBundle, Arrays.asList(window(0, 10)), new Instant(0), valueCoder, "v0");    addElement(messageBundle, Arrays.asList(window(10, 20)), new Instant(13), valueCoder, "v3");    runner.processElement(createValue(workItem, valueCoder));    runner.finishBundle();    List<WindowedValue<KV<String, Iterable<String>>>> result = outputManager.getOutput(outputTag);    assertEquals(4, result.size());    WindowedValue<KV<String, Iterable<String>>> item0 = result.get(0);    assertEquals(KEY, item0.getValue().getKey());    assertThat(item0.getValue().getValue(), Matchers.containsInAnyOrder("v1"));    assertEquals(new Instant(1), item0.getTimestamp());    assertThat(item0.getWindows(), Matchers.<BoundedWindow>contains(window(0, 10)));    WindowedValue<KV<String, Iterable<String>>> item1 = result.get(1);    assertEquals(KEY, item1.getValue().getKey());    assertThat(item1.getValue().getValue(), Matchers.containsInAnyOrder("v2"));    assertEquals(new Instant(2), item1.getTimestamp());    assertThat(item1.getWindows(), Matchers.<BoundedWindow>contains(window(0, 10)));    WindowedValue<KV<String, Iterable<String>>> item2 = result.get(2);    assertEquals(KEY, item2.getValue().getKey());    assertThat(item2.getValue().getValue(), Matchers.containsInAnyOrder("v0"));    assertEquals(new Instant(0), item2.getTimestamp());    assertThat(item2.getWindows(), Matchers.<BoundedWindow>contains(window(0, 10)));    WindowedValue<KV<String, Iterable<String>>> item3 = result.get(3);    assertEquals(KEY, item3.getValue().getKey());    assertThat(item3.getValue().getValue(), Matchers.containsInAnyOrder("v3"));    assertEquals(new Instant(13), item3.getTimestamp());    assertThat(item3.getWindows(), Matchers.<BoundedWindow>contains(window(10, 20)));}
private TimerData beam_f10760_0(IntervalWindow window, Instant timestamp, Timer.Type type)
{    return TimerData.of(StateNamespaces.window(IntervalWindow.getCoder(), window), timestamp, type == Windmill.Timer.Type.WATERMARK ? TimeDomain.EVENT_TIME : TimeDomain.PROCESSING_TIME);}
private IntervalWindow beam_f10761_0(long start, long end)
{    return new IntervalWindow(new Instant(start), new Instant(end));}
private StreamingKeyedWorkItemSideInputDoFnRunner<String, Integer, KV<String, Integer>, IntervalWindow> beam_f10762_0(DoFnRunners.OutputManager outputManager) throws Exception
{    CoderRegistry registry = CoderRegistry.createDefault();    Coder<String> keyCoder = StringUtf8Coder.of();    Coder<Integer> inputCoder = BigEndianIntegerCoder.of();    AppliedCombineFn<String, Integer, ?, Integer> combineFn = AppliedCombineFn.withInputCoder(Sum.ofIntegers(), registry, KvCoder.of(keyCoder, inputCoder));    WindowingStrategy<Object, IntervalWindow> windowingStrategy = WindowingStrategy.of(WINDOW_FN);    @SuppressWarnings("rawtypes")    StreamingGroupAlsoByWindowViaWindowSetFn doFn = (StreamingGroupAlsoByWindowViaWindowSetFn) StreamingGroupAlsoByWindowsDoFns.create(windowingStrategy, key -> state, combineFn, keyCoder);    DoFnRunner<KeyedWorkItem<String, Integer>, KV<String, Integer>> simpleDoFnRunner = new GroupAlsoByWindowFnRunner<>(PipelineOptionsFactory.create(), doFn.asDoFn(), mockSideInputReader, outputManager, mainOutputTag, stepContext);    return new StreamingKeyedWorkItemSideInputDoFnRunner<String, Integer, KV<String, Integer>, IntervalWindow>(simpleDoFnRunner, keyCoder, sideInputFetcher, stepContext);}
private CounterUpdate beam_f10770_0(String counterName, String stageName, long value)
{    return new CounterUpdate().setStructuredNameAndMetadata(new CounterStructuredNameAndMetadata().setName(new CounterStructuredName().setOrigin("SYSTEM").setName(counterName).setExecutionStepName(stageName)).setMetadata(new CounterMetadata().setKind("SUM"))).setCumulative(false).setInteger(longToSplitInt(value));}
public void beam_f10771_0() throws InterruptedException, ExecutionException
{    long numUpdates = 1_000_000;    StreamingModeExecutionState state = new StreamingModeExecutionState(NameContextsForTests.nameContextForTest(), "testState", null, NoopProfileScope.NOOP, null);    ExecutorService executor = Executors.newFixedThreadPool(2);    AtomicBoolean doneWriting = new AtomicBoolean(false);    Callable<Long> reader = () -> {        long count = 0;        boolean isLastRead;        do {            isLastRead = doneWriting.get();            CounterUpdate update = state.extractUpdate(false);            if (update != null) {                count += splitIntToLong(update.getInteger());            }        } while (!isLastRead);        return count;    };    Runnable writer = () -> {        for (int i = 0; i < numUpdates; i++) {            state.takeSample(1L);        }        doneWriting.set(true);    };        List<Future<Long>> results = executor.invokeAll(Lists.newArrayList(reader, Executors.callable(writer, 0L)), 2, TimeUnit.SECONDS);    long count = results.get(0).get();    assertThat(count, equalTo(numUpdates));}
public void beam_f10772_0()
{            StreamingModeExecutionState state = new StreamingModeExecutionState(NameContextsForTests.nameContextForTest(), "testState", null, NoopProfileScope.NOOP, null);    ExecutionStateSampler sampler = ExecutionStateSampler.newForTest();    try {        sampler.start();        ExecutionStateTracker tracker = new ExecutionStateTracker(sampler);        Thread executionThread = new Thread();        executionThread.setName("looping-thread-for-test");        tracker.activate(executionThread);        tracker.enterState(state);                for (int i = 0; i < 3; i++) {            CounterUpdate update = null;            while (update == null) {                update = state.extractUpdate(false);            }            long newValue = splitIntToLong(update.getInteger());            assertThat(newValue, Matchers.greaterThan(0L));        }    } finally {        sampler.stop();    }}
private StreamingSideInputDoFnRunner<String, String, IntervalWindow> beam_f10780_0(DoFnRunners.OutputManager outputManager, List<PCollectionView<String>> views, StreamingSideInputFetcher<String, IntervalWindow> sideInputFetcher) throws Exception
{    return createRunner(WINDOW_FN, outputManager, views, sideInputFetcher);}
private StreamingSideInputDoFnRunner<String, String, IntervalWindow> beam_f10781_0(WindowFn<?, ?> windowFn, DoFnRunners.OutputManager outputManager, List<PCollectionView<String>> views, StreamingSideInputFetcher<String, IntervalWindow> sideInputFetcher) throws Exception
{    DoFnRunner<String, String> simpleDoFnRunner = DoFnRunners.simpleRunner(PipelineOptionsFactory.create(), new SideInputFn(views), mockSideInputReader, outputManager, mainOutputTag, Arrays.<TupleTag<?>>asList(), stepContext, null, Collections.emptyMap(), WindowingStrategy.of(windowFn), DoFnSchemaInformation.create(), Collections.emptyMap());    return new StreamingSideInputDoFnRunner<>(simpleDoFnRunner, sideInputFetcher);}
private StreamingSideInputFetcher<String, IntervalWindow> beam_f10782_0(List<PCollectionView<String>> views) throws Exception
{    @SuppressWarnings({ "unchecked", "rawtypes" })    Iterable<PCollectionView<?>> typedViews = (Iterable) views;    return new StreamingSideInputFetcher<>(typedViews, StringUtf8Coder.of(), WindowingStrategy.of(WINDOW_FN), stepContext);}
private PCollectionView<String> beam_f10790_0()
{    return TestPipeline.create().apply(Create.empty(StringUtf8Coder.of())).apply(Window.<String>into(WINDOW_FN)).apply(View.<String>asSingleton());}
private WindowedValue<String> beam_f10791_0(String element, long timestamp)
{    return WindowedValue.of(element, new Instant(timestamp), Arrays.asList(createWindow(timestamp)), PaneInfo.NO_FIRING);}
private TimerData beam_f10792_0(long timestamp)
{    return TimerData.of(StateNamespaces.window(IntervalWindow.getCoder(), createWindow(timestamp)), new Instant(timestamp), TimeDomain.EVENT_TIME);}
public void beam_f10800_0()
{    evaluateRan[0] = true;        assertNull("null JobId", DataflowWorkerLoggingMDC.getJobId());    assertNull("null StageName", DataflowWorkerLoggingMDC.getStageName());    assertNull("null WorkerId", DataflowWorkerLoggingMDC.getWorkerId());    assertNull("null WorkId", DataflowWorkerLoggingMDC.getWorkId());        DataflowWorkerLoggingMDC.setJobId("newJobId");    DataflowWorkerLoggingMDC.setStageName("newStageName");    DataflowWorkerLoggingMDC.setWorkerId("newWorkerId");    DataflowWorkerLoggingMDC.setWorkId("newWorkId");        assertEquals("newJobId", DataflowWorkerLoggingMDC.getJobId());    assertEquals("newStageName", DataflowWorkerLoggingMDC.getStageName());    assertEquals("newWorkerId", DataflowWorkerLoggingMDC.getWorkerId());    assertEquals("newWorkId", DataflowWorkerLoggingMDC.getWorkId());}
public static void beam_f10801_0(List<Integer> finalizeTracker)
{    TestCountingSource.finalizeTracker = finalizeTracker;}
public TestCountingSource beam_f10802_0()
{    return new TestCountingSource(numMessagesPerShard, shardNumber, true, throwOnFirstSnapshot, true);}
public boolean beam_f10810_0()
{    return dedup;}
public boolean beam_f10811_0()
{    return advance();}
public boolean beam_f10812_0()
{    if (current >= numMessagesPerShard - 1) {        return false;    }        if (current >= 0 && dedup && ThreadLocalRandom.current().nextInt(5) == 0) {        return true;    }    current++;    return true;}
public CountingSourceReader beam_f10821_1(PipelineOptions options, @Nullable CounterMark checkpointMark)
{    if (checkpointMark == null) {            } else {            }    return new CountingSourceReader(checkpointMark != null ? checkpointMark.current : -1);}
public Coder<KV<Integer, Integer>> beam_f10823_0()
{    return KvCoder.of(VarIntCoder.of(), VarIntCoder.of());}
public void beam_f10824_0(long millisSinceLastSample)
{    totalMillis += millisSinceLastSample;}
public MetricsContainerImpl beam_f10833_0()
{    return metricsContainer;}
public CounterSet beam_f10834_0()
{    return counterSet;}
public ExecutionState beam_f10835_0()
{    return newExecutionState(ExecutionStateTracker.START_STATE_NAME);}
public void beam_f10843_0(ShuffleEntry entry)
{    TreeMap<byte[], List<ShuffleEntry>> valuesBySecondaryKey = records.get(entry.getKey());    if (valuesBySecondaryKey == null) {        valuesBySecondaryKey = new TreeMap<>(SHUFFLE_KEY_COMPARATOR);        records.put(entry.getKey(), valuesBySecondaryKey);    }    List<ShuffleEntry> values = valuesBySecondaryKey.get(entry.getSecondaryKey());    if (values == null) {        values = new ArrayList<>();        valuesBySecondaryKey.put(entry.getSecondaryKey(), values);    }    values.add(entry);}
public Iterator<ShuffleEntry> beam_f10844_0()
{    return read((byte[]) null, (byte[]) null);}
public Reiterator<ShuffleEntry> beam_f10845_0(@Nullable ShufflePosition startPosition, @Nullable ShufflePosition endPosition)
{    if (closed) {        throw new RuntimeException("Cannot read from a closed reader.");    }    return read(ByteArrayShufflePosition.getPosition(startPosition), ByteArrayShufflePosition.getPosition(endPosition));}
public void beam_f10853_0()
{    throw new UnsupportedOperationException();}
 void beam_f10854_0(List<KV<String, KV<String, String>>> expected, List<KV<String, KV<String, String>>> outOfRange, String startKey, String endKey)
{    TestShuffleReader shuffleReader = new TestShuffleReader();    List<KV<String, KV<String, String>>> expectedCopy = new ArrayList<>(expected);    expectedCopy.addAll(outOfRange);    Collections.shuffle(expectedCopy);    for (KV<String, KV<String, String>> entry : expectedCopy) {        shuffleReader.addEntry(entry.getKey(), entry.getValue().getKey(), /* secondary key */        entry.getValue().getValue());    }    testIteratorReturnsValues(shuffleReader.read(startKey, endKey), expected);    testIteratorIntermediateCopiesReturnTails(shuffleReader.read(startKey, endKey), expected);}
private void beam_f10855_0(Reiterator<ShuffleEntry> iter, List<KV<String, KV<String, String>>> expected)
{    List<KV<String, KV<String, String>>> actual = readShuffleEntries(iter);    try {        iter.next();        fail("should have failed");    } catch (NoSuchElementException exn) {        }    assertEquals(expected, actual);}
public void beam_f10863_0()
{    runTest(NO_ENTRIES, OUT_OF_RANGE_ENTRIES, START_KEY, END_KEY);}
public void beam_f10864_0(byte[] chunk) throws IOException
{    if (closed) {        throw new AssertionError("shuffle writer already closed");    }    DataInputStream dais = new DataInputStream(new ByteArrayInputStream(chunk));    while (dais.available() > 0) {        byte[] key = new byte[dais.readInt()];        dais.readFully(key);        byte[] sortKey = new byte[dais.readInt()];        dais.readFully(sortKey);        byte[] value = new byte[dais.readInt()];        dais.readFully(value);        ShuffleEntry entry = new ShuffleEntry(key, sortKey, value);        records.add(entry);        long size = entry.length();        sizes.add(size);    }}
public void beam_f10865_0()
{    if (closed) {        throw new AssertionError("shuffle writer already closed");    }    closed = true;}
public void beam_f10873_0()
{    state = State.SET_UP;}
public void beam_f10874_0()
{    assertThat(state, anyOf(equalTo(State.SET_UP), equalTo(State.FINISHED)));    state = State.STARTED;}
public void beam_f10875_0(ProcessContext c)
{    assertThat(state, anyOf(equalTo(State.STARTED), equalTo(State.PROCESSING)));    state = State.PROCESSING;    String value = "processing: " + c.element();    c.output(value);    for (TupleTag<String> additionalOutputTupleTag : outputTags) {        c.output(additionalOutputTupleTag, additionalOutputTupleTag.getId() + ": " + value);    }}
public void beam_f10884_0() throws Exception
{    PipelineOptions options = PipelineOptionsFactory.create();    CounterSet counters = new CounterSet();    DoFn<?, ?> initialFn = new TestStatefulDoFn();    CloudObject cloudObject = getCloudObject(initialFn, WindowingStrategy.globalDefault().withWindowFn(FixedWindows.of(Duration.millis(10))));    TimerInternals timerInternals = mock(TimerInternals.class);    DataflowStepContext stepContext = mock(DataflowStepContext.class);    when(stepContext.timerInternals()).thenReturn(timerInternals);    DataflowExecutionContext<DataflowStepContext> executionContext = mock(DataflowExecutionContext.class);    TestOperationContext operationContext = TestOperationContext.create(counters);    when(executionContext.getStepContext(operationContext)).thenReturn(stepContext);    when(executionContext.getSideInputReader(any(), any(), any())).thenReturn(NullSideInputReader.empty());    ParDoFn parDoFn = factory.create(options, cloudObject, Collections.emptyList(), MAIN_OUTPUT, ImmutableMap.of(MAIN_OUTPUT, 0), executionContext, operationContext);    Receiver rcvr = new OutputReceiver();    parDoFn.startBundle(rcvr);    IntervalWindow firstWindow = new IntervalWindow(new Instant(0), new Instant(10));    parDoFn.processElement(WindowedValue.of("foo", new Instant(1), firstWindow, PaneInfo.NO_FIRING));    verify(stepContext).setStateCleanupTimer(SimpleParDoFn.CLEANUP_TIMER_ID, firstWindow, IntervalWindow.getCoder(), firstWindow.maxTimestamp().plus(1L));}
public void beam_f10885_0() throws Exception
{    PipelineOptions options = PipelineOptionsFactory.create();    CounterSet counters = new CounterSet();    DoFn<?, ?> initialFn = new TestStatefulDoFn();    CloudObject cloudObject = getCloudObject(initialFn, WindowingStrategy.of(FixedWindows.of(Duration.millis(10))));    StateInternals stateInternals = InMemoryStateInternals.forKey("dummy");        DataflowStepContext stepContext = mock(DataflowStepContext.class);        DataflowStepContext userStepContext = mock(DataflowStepContext.class);    when(stepContext.namespacedToUser()).thenReturn(userStepContext);    when(userStepContext.stateInternals()).thenReturn((StateInternals) stateInternals);    DataflowExecutionContext<DataflowStepContext> executionContext = mock(DataflowExecutionContext.class);    TestOperationContext operationContext = TestOperationContext.create(counters);    when(executionContext.getStepContext(operationContext)).thenReturn(stepContext);    when(executionContext.getSideInputReader(any(), any(), any())).thenReturn(NullSideInputReader.empty());    ParDoFn parDoFn = factory.create(options, cloudObject, Collections.emptyList(), MAIN_OUTPUT, ImmutableMap.of(MAIN_OUTPUT, 0), executionContext, operationContext);    Receiver rcvr = new OutputReceiver();    parDoFn.startBundle(rcvr);    IntervalWindow firstWindow = new IntervalWindow(new Instant(0), new Instant(9));    IntervalWindow secondWindow = new IntervalWindow(new Instant(10), new Instant(19));    Coder<IntervalWindow> windowCoder = IntervalWindow.getCoder();    StateNamespace firstWindowNamespace = StateNamespaces.window(windowCoder, firstWindow);    StateNamespace secondWindowNamespace = StateNamespaces.window(windowCoder, secondWindow);    StateTag<ValueState<String>> tag = StateTags.tagForSpec(TestStatefulDoFn.STATE_ID, StateSpecs.value(StringUtf8Coder.of()));            stateInternals.state(firstWindowNamespace, tag).write("first");    stateInternals.state(secondWindowNamespace, tag).write("second");    when(userStepContext.getNextFiredTimer(windowCoder)).thenReturn(null);    when(stepContext.getNextFiredTimer(windowCoder)).thenReturn(TimerData.of(SimpleParDoFn.CLEANUP_TIMER_ID, firstWindowNamespace, firstWindow.maxTimestamp().plus(1L), TimeDomain.EVENT_TIME)).thenReturn(null);        parDoFn.processTimers();    assertThat(stateInternals.state(firstWindowNamespace, tag).read(), nullValue());    assertThat(stateInternals.state(secondWindowNamespace, tag).read(), equalTo("second"));    when(stepContext.getNextFiredTimer((Coder) windowCoder)).thenReturn(TimerData.of(SimpleParDoFn.CLEANUP_TIMER_ID, secondWindowNamespace, secondWindow.maxTimestamp().plus(1L), TimeDomain.EVENT_TIME)).thenReturn(null);        parDoFn.processTimers();    assertThat(stateInternals.state(firstWindowNamespace, tag).read(), nullValue());    assertThat(stateInternals.state(secondWindowNamespace, tag).read(), nullValue());}
public StateInternals beam_f10886_0(K key)
{    return InMemoryStateInternals.forKey(key);}
public BatchGroupAlsoByWindowFn<String, String, Iterable<String>> beam_f10894_0(WindowingStrategy<?, W> windowingStrategy, StateInternalsFactory<String> stateInternalsFactory)
{        return new BatchGroupAlsoByWindowReshuffleFn<String, String, W>();}
private static List<WindowedValue<KV<K, OutputT>>> beam_f10895_0(GroupAlsoByWindowDoFnFactory<K, InputT, OutputT> gabwFactory, WindowingStrategy<?, W> windowingStrategy, K key, WindowedValue<InputT>... values)
{    TupleTag<KV<K, OutputT>> outputTag = new TupleTag<>();    ListOutputManager outputManager = new ListOutputManager();    DoFnRunner<KV<K, Iterable<WindowedValue<InputT>>>, KV<K, OutputT>> runner = makeRunner(gabwFactory, windowingStrategy, outputTag, outputManager);    runner.startBundle();    if (values.length > 0) {        runner.processElement(new ValueInEmptyWindows<>(KV.of(key, (Iterable<WindowedValue<InputT>>) Arrays.<WindowedValue<InputT>>asList(values))));    }    runner.finishBundle();    List<WindowedValue<KV<K, OutputT>>> result = outputManager.getOutput(outputTag);        for (WindowedValue<KV<K, OutputT>> elem : result) {        assertThat(elem.getValue().getKey(), equalTo(key));    }    return result;}
private static DoFnRunner<KV<K, Iterable<WindowedValue<InputT>>>, KV<K, OutputT>> beam_f10896_0(GroupAlsoByWindowDoFnFactory<K, InputT, OutputT> fnFactory, WindowingStrategy<?, W> windowingStrategy, TupleTag<KV<K, OutputT>> outputTag, DoFnRunners.OutputManager outputManager)
{    final StepContext stepContext = new TestStepContext(STEP_NAME);    StateInternalsFactory<K> stateInternalsFactory = key -> stepContext.stateInternals();    BatchGroupAlsoByWindowFn<K, InputT, OutputT> fn = fnFactory.forStrategy(windowingStrategy, stateInternalsFactory);    return new GroupAlsoByWindowFnRunner<>(PipelineOptionsFactory.create(), fn, NullSideInputReader.empty(), outputManager, outputTag, stepContext);}
public void beam_f10904_0() throws Exception
{    CombineFn<Long, ?, Long> combineFn = Sum.ofLongs();    AppliedCombineFn<String, Long, ?, Long> appliedFn = AppliedCombineFn.withInputCoder(combineFn, CoderRegistry.createDefault(), KvCoder.of(StringUtf8Coder.of(), VarLongCoder.of()));    GroupAlsoByWindowProperties.combinesElementsInSlidingWindows(new CombiningGABWViaOutputBufferDoFnFactory<>(StringUtf8Coder.of(), appliedFn), combineFn);}
public void beam_f10905_0() throws Exception
{    CombineFn<Long, ?, Long> combineFn = Sum.ofLongs();    AppliedCombineFn<String, Long, ?, Long> appliedFn = AppliedCombineFn.withInputCoder(combineFn, CoderRegistry.createDefault(), KvCoder.of(StringUtf8Coder.of(), VarLongCoder.of()));    GroupAlsoByWindowProperties.combinesElementsPerSession(new CombiningGABWViaOutputBufferDoFnFactory<>(StringUtf8Coder.of(), appliedFn), combineFn);}
public void beam_f10906_0() throws Exception
{    CombineFn<Long, ?, Long> combineFn = Sum.ofLongs();    AppliedCombineFn<String, Long, ?, Long> appliedFn = AppliedCombineFn.withInputCoder(combineFn, CoderRegistry.createDefault(), KvCoder.of(StringUtf8Coder.of(), VarLongCoder.of()));    GroupAlsoByWindowProperties.combinesElementsPerSessionWithEndOfWindowTimestamp(new CombiningGABWViaOutputBufferDoFnFactory<>(StringUtf8Coder.of(), appliedFn), combineFn);}
private TaggedReiteratorList beam_f10914_0(int repeat, String... values)
{    ArrayList<TaggedValue> taggedValues = new ArrayList<>();    for (int tag = 0; tag < repeat; tag++) {        for (String value : values) {            taggedValues.add(new TaggedValue(tag, value + tag));        }    }    return new TaggedReiteratorList(new TestReiterator(taggedValues.toArray(new TaggedValue[0])), new TaggedValueExtractor());}
private List<T> beam_f10915_0(Iterator<T> iter)
{    return asList(iter, Integer.MAX_VALUE);}
private List<T> beam_f10916_0(Iterator<T> iter, int limit)
{    List<T> list = new ArrayList<>();    for (int i = 0; i < limit && iter.hasNext(); i++) {        list.add(iter.next());    }    return list;}
public String beam_f10924_0(TaggedValue elem)
{    return elem.value;}
public void beam_f10925_0()
{    MockitoAnnotations.initMocks(this);    reader = new BatchingShuffleEntryReader(batchReader);}
public void beam_f10926_0() throws Exception
{    ShuffleEntry e1 = new ShuffleEntry(KEY, SKEY, VALUE);    ShuffleEntry e2 = new ShuffleEntry(KEY, SKEY, VALUE);    ArrayList<ShuffleEntry> entries = new ArrayList<>();    entries.add(e1);    entries.add(e2);    when(batchReader.read(START_POSITION, END_POSITION)).thenReturn(new ShuffleBatchReader.Batch(entries, null));    List<ShuffleEntry> results = newArrayList(reader.read(START_POSITION, END_POSITION));    assertThat(results, contains(e1, e2));}
public NativeReaderIterator<String> beam_f10934_0()
{    return new TestReaderIterator(inputs);}
public boolean beam_f10935_0()
{    if (inputs.isEmpty()) {        return false;    }    notifyElementRead(getCurrent().length());    return true;}
public boolean beam_f10936_0() throws IOException
{    if (currentIndex >= inputs.size() - 1) {        return false;    }    ++currentIndex;    notifyElementRead(getCurrent().length());    return true;}
public void beam_f10944_0()
{    assertFalse(closed);    closed = true;}
public void beam_f10945_0() throws IOException
{    close();}
public void beam_f10946_0() throws Exception
{    TestOutputReceiver receiver = new TestOutputReceiver(counterSet, NameContext.create("test", "receiver", "receiver", "receiver"));    OperationContext context = TestOperationContext.create(counterSet, nameContext);    FlattenOperation flattenOperation = new FlattenOperation(receiver, context);    flattenOperation.start();    flattenOperation.process("hi");    flattenOperation.process("there");    flattenOperation.process("");    flattenOperation.process("bob");    flattenOperation.finish();    Assert.assertThat(receiver.outputElems, CoreMatchers.<Object>hasItems("hi", "there", "", "bob"));    CounterUpdateExtractor<?> updateExtractor = Mockito.mock(CounterUpdateExtractor.class);    counterSet.extractUpdates(false, updateExtractor);    verify(updateExtractor).longSum(getObjectCounterName("test_receiver_out"), false, 4L);    verify(updateExtractor).longMean(getMeanByteCounterName("test_receiver_out"), false, LongCounterMean.ZERO.addValue(14L, 4));    verifyNoMoreInteractions(updateExtractor);}
private void beam_f10954_0(String mockOriginalName)
{    DataflowExecutionState state = new TestDataflowExecutionState(NameContext.create(MOCK_STAGE_NAME, mockOriginalName, MOCK_SYSTEM_NAME, MOCK_USER_NAME), "activity");    tracker.enterState(state);}
private static ShuffleEntry beam_f10955_0(String key, String value)
{    return new ShuffleEntry(/* use key itself as position */    ByteArrayShufflePosition.of(key.getBytes(Charsets.UTF_8)), key.getBytes(Charsets.UTF_8), new byte[0], value.getBytes(Charsets.UTF_8));}
public void beam_f10956_0() throws Exception
{    setCurrentExecutionState(MOCK_ORIGINAL_NAME_FOR_EXECUTING_STEP1);    MockitoAnnotations.initMocks(this);    PipelineOptions options = PipelineOptionsFactory.create();    options.as(DataflowPipelineDebugOptions.class).setExperiments(Lists.newArrayList(Experiment.IntertransformIO.getName()));    BatchModeExecutionContext spyExecutionContext = Mockito.spy(BatchModeExecutionContext.forTesting(options, "STAGE"));    ArrayList<ShuffleEntry> entries = new ArrayList<>();    entries.add(shuffleEntry("k1", "v11"));    entries.add(shuffleEntry("k1", "v12"));    entries.add(shuffleEntry("k1", "v13"));    when(reader.read(START_POSITION, END_POSITION)).thenReturn(new ListReiterator<>(entries, 0));    final ShuffleReadCounter shuffleReadCounter = new ShuffleReadCounter(ORIGINAL_SHUFFLE_STEP_NAME, true, null);    iterator = new GroupingShuffleEntryIterator(reader, START_POSITION, END_POSITION) {        @Override        protected void notifyElementRead(long byteSize) {                }        @Override        protected void commitBytesRead(long bytes) {            shuffleReadCounter.addBytesRead(bytes);        }    };    assertTrue(iterator.advance());    KeyGroupedShuffleEntries k1Entries = iterator.getCurrent();    Reiterator<ShuffleEntry> values1 = k1Entries.values.iterator();    assertTrue(values1.hasNext());    Reiterator<ShuffleEntry> values1Copy1 = values1.copy();    Reiterator<ShuffleEntry> values1Copy2 = values1.copy();    ShuffleEntry expectedEntry = values1.next();        assertFalse(iterator.advance());        assertTrue(values1Copy1.hasNext());    assertEquals(expectedEntry, values1Copy1.next());    assertEquals(expectedEntry, values1Copy2.next());    Map<String, Long> expectedReadBytesMap = new HashMap<>();    expectedReadBytesMap.put(MOCK_ORIGINAL_NAME_FOR_EXECUTING_STEP1, 15L);            assertEquals(expectedReadBytesMap.size(), (long) shuffleReadCounter.counterSet.size());    Iterator it = expectedReadBytesMap.entrySet().iterator();    while (it.hasNext()) {        Map.Entry<String, Long> pair = (Map.Entry) it.next();        Counter counter = shuffleReadCounter.counterSet.getExistingCounter(ShuffleReadCounter.generateCounterName(ORIGINAL_SHUFFLE_STEP_NAME, pair.getKey()));        assertEquals(pair.getValue(), counter.getAggregate());    }}
public void beam_f10968_0() throws Exception
{    GroupingShuffleRangeTracker tracker = new GroupingShuffleRangeTracker(ofBytes(1, 0, 0), ofBytes(5, 0, 0));    assertTrue(tracker.tryReturnRecordAt(true, ofBytes(1, 2, 3)));    assertTrue(tracker.tryReturnRecordAt(true, ofBytes(1, 2, 5)));    assertTrue(tracker.tryReturnRecordAt(true, ofBytes(3, 6, 8, 10)));    assertTrue(tracker.tryReturnRecordAt(true, ofBytes(4, 255, 255, 255, 255)));        assertFalse(tracker.copy().tryReturnRecordAt(true, ofBytes(5, 0, 0)));    assertFalse(tracker.copy().tryReturnRecordAt(true, ofBytes(5, 0, 1)));    assertFalse(tracker.copy().tryReturnRecordAt(true, ofBytes(6, 0, 0)));}
public void beam_f10969_0() throws Exception
{    GroupingShuffleRangeTracker tracker = new GroupingShuffleRangeTracker(ofBytes(1, 0, 0), ofBytes(5, 0, 0));    assertTrue(tracker.tryReturnRecordAt(true, ofBytes(1, 2, 3)));    assertTrue(tracker.tryReturnRecordAt(false, ofBytes(1, 2, 3)));    assertTrue(tracker.tryReturnRecordAt(false, ofBytes(1, 2, 3)));    assertTrue(tracker.tryReturnRecordAt(true, ofBytes(1, 2, 5)));    assertTrue(tracker.tryReturnRecordAt(false, ofBytes(1, 2, 5)));    assertTrue(tracker.tryReturnRecordAt(true, ofBytes(3, 6, 8, 10)));    assertTrue(tracker.tryReturnRecordAt(true, ofBytes(4, 255, 255, 255, 255)));}
public void beam_f10970_0() throws Exception
{    GroupingShuffleRangeTracker tracker = new GroupingShuffleRangeTracker(ofBytes(3, 0, 0), ofBytes(5, 0, 0));    expected.expect(IllegalStateException.class);    tracker.tryReturnRecordAt(false, ofBytes(3, 4, 5));}
public void beam_f10978_0() throws Exception
{    Combiner<Object, Integer, Long, Long> summingCombineFn = new Combiner<Object, Integer, Long, Long>() {        @Override        public Long createAccumulator(Object key) {            return 0L;        }        @Override        public Long add(Object key, Long accumulator, Integer value) {            return accumulator + value;        }        @Override        public Long merge(Object key, Iterable<Long> accumulators) {            long sum = 0;            for (Long part : accumulators) {                sum += part;            }            return sum;        }        @Override        public Long compact(Object key, Long accumulator) {            return accumulator;        }        @Override        public Long extract(Object key, Long accumulator) {            return accumulator;        }    };    GroupingTableBase<String, Integer, Long> table = (GroupingTableBase<String, Integer, Long>) GroupingTables.combining(new IdentityGroupingKeyCreator(), new KvPairInfo(), summingCombineFn, new StringPowerSizeEstimator(), new IdentitySizeEstimator());    table.setMaxSize(1000);    TestOutputReceiver receiver = new TestOutputReceiver(KvCoder.of(StringUtf8Coder.of(), BigEndianLongCoder.of()), NameContextsForTests.nameContextForTest());    table.put("A", 1, receiver);    table.put("B", 2, receiver);    table.put("B", 3, receiver);    table.put("C", 4, receiver);    assertThat(receiver.outputElems, empty());    table.put("C", 5000, receiver);    assertThat(receiver.outputElems, hasItem((Object) KV.of("C", 5004L)));    table.put("DDDD", 6, receiver);    assertThat(receiver.outputElems, hasItem((Object) KV.of("DDDD", 6L)));    table.flush(receiver);    assertThat(receiver.outputElems, IsIterableContainingInAnyOrder.<Object>containsInAnyOrder(KV.of("A", 1L), KV.of("B", 2L + 3), KV.of("C", 5000L + 4), KV.of("DDDD", 6L)));}
public Long beam_f10979_0(Object key)
{    return 0L;}
public Long beam_f10980_0(Object key, Long accumulator, Integer value)
{    return accumulator + value;}
private static TypeSafeDiagnosingMatcher<T> beam_f10988_0(final T min, final T max)
{    return new TypeSafeDiagnosingMatcher<T>() {        @Override        public void describeTo(Description description) {            description.appendText("is between " + min + " and " + max);        }        @Override        protected boolean matchesSafely(T item, Description mismatchDescription) {            return min.compareTo(item) <= 0 && item.compareTo(max) <= 0;        }    };}
public void beam_f10989_0(Description description)
{    description.appendText("is between " + min + " and " + max);}
protected boolean beam_f10990_0(T item, Description mismatchDescription)
{    return min.compareTo(item) <= 0 && item.compareTo(max) <= 0;}
public NativeReader.Progress beam_f10998_0()
{    return cloudProgressToReaderProgress(progress);}
public NativeReader.DynamicSplitResult beam_f10999_0(NativeReader.DynamicSplitRequest splitRequest)
{        return new NativeReader.DynamicSplitResultWithPosition(cloudPositionToReaderPosition(splitRequestToApproximateSplitRequest(splitRequest).getPosition()));}
public void beam_f11000_0(ApproximateReportedProgress progress)
{    this.progress = progress;}
public void beam_f11013_0() throws Exception
{    ExecutionStateTracker stateTracker = new DataflowExecutionStateTracker(ExecutionStateSampler.newForTest(), new TestDataflowExecutionState(NameContext.forStage("testStage"), "other", null, /* requestingStepName */    null, /* sideInputIndex */    null, /* metricsContainer */    NoopProfileScope.NOOP), new CounterSet(), PipelineOptionsFactory.create(), "test-work-item-id");    final String o1 = "o1";    TestOperationContext context1 = createContext(o1, stateTracker);    final String o2 = "o2";    TestOperationContext context2 = createContext(o2, stateTracker);    final String o3 = "o3";    TestOperationContext context3 = createContext(o3, stateTracker);    List<Operation> operations = Arrays.asList(new Operation(new OutputReceiver[] {}, context1) {        @Override        public void start() throws Exception {            super.start();            try (Closeable scope = context.enterStart()) {                Metrics.counter("TestMetric", "MetricCounter").inc(1L);            }        }    }, new Operation(new OutputReceiver[] {}, context2) {        @Override        public void start() throws Exception {            super.start();            try (Closeable scope = context.enterStart()) {                Metrics.counter("TestMetric", "MetricCounter").inc(2L);            }        }    }, new Operation(new OutputReceiver[] {}, context3) {        @Override        public void start() throws Exception {            super.start();            try (Closeable scope = context.enterStart()) {                Metrics.counter("TestMetric", "MetricCounter").inc(3L);            }        }    });    try (MapTaskExecutor executor = new MapTaskExecutor(operations, counterSet, stateTracker)) {                executor.execute();        assertThat(context1.metricsContainer().getUpdates().counterUpdates(), contains(metricUpdate("TestMetric", "MetricCounter", o1, 1L)));        assertThat(context2.metricsContainer().getUpdates().counterUpdates(), contains(metricUpdate("TestMetric", "MetricCounter", o2, 2L)));        assertThat(context3.metricsContainer().getUpdates().counterUpdates(), contains(metricUpdate("TestMetric", "MetricCounter", o3, 3L)));    }}
public void beam_f11014_0() throws Exception
{    super.start();    try (Closeable scope = context.enterStart()) {        Metrics.counter("TestMetric", "MetricCounter").inc(1L);    }}
public void beam_f11015_0() throws Exception
{    super.start();    try (Closeable scope = context.enterStart()) {        Metrics.counter("TestMetric", "MetricCounter").inc(2L);    }}
public void beam_f11023_0() throws Exception
{    Operation o1 = Mockito.mock(Operation.class);    Operation o2 = Mockito.mock(Operation.class);    Operation o3 = Mockito.mock(Operation.class);    Operation o4 = Mockito.mock(Operation.class);    Mockito.doThrow(new Exception("in finish")).when(o2).finish();    Mockito.doThrow(new Exception("suppressed in abort")).when(o3).abort();    ExecutionStateTracker stateTracker = ExecutionStateTracker.newForTest();    try (MapTaskExecutor executor = new MapTaskExecutor(Arrays.<Operation>asList(o1, o2, o3, o4), counterSet, stateTracker)) {        executor.execute();        fail("Should have thrown");    } catch (Exception e) {        InOrder inOrder = Mockito.inOrder(o1, o2, o3, o4);        inOrder.verify(o4).start();        inOrder.verify(o3).start();        inOrder.verify(o2).start();        inOrder.verify(o1).start();        inOrder.verify(o1).finish();                inOrder.verify(o2).finish();                Mockito.verify(o1).abort();        Mockito.verify(o2).abort();                Mockito.verify(o3).abort();        Mockito.verify(o4).abort();        Mockito.verifyNoMoreInteractions(o1, o2, o3, o4);                assertThat(e.getMessage(), equalTo("in finish"));        assertThat(e.getSuppressed(), arrayWithSize(1));        assertThat(e.getSuppressed()[0].getMessage(), equalTo("suppressed in abort"));    }}
public void beam_f11024_0() throws Exception
{    TestOutputCounter outputCounter = new TestOutputCounter(NameContextsForTests.nameContextForTest());    outputCounter.update("hi");    outputCounter.finishLazyUpdate("hi");    outputCounter.update("bob");    outputCounter.finishLazyUpdate("bob");    CounterMean<Long> meanByteCount = outputCounter.getMeanByteCount().getAggregate();    assertEquals(7, (long) meanByteCount.getAggregate());    assertEquals(2, meanByteCount.getCount());}
public void beam_f11025_0() throws Exception
{    TestOutputCounter outputCounter = new TestOutputCounter(NameContextsForTests.nameContextForTest());    thrown.expect(ClassCastException.class);    outputCounter.update(5);}
public void beam_f11033_0() throws Exception
{    OutputReceiver fanOut = new OutputReceiver();    TestOutputCounter outputCounter = new TestOutputCounter(nameContextForTest());    fanOut.addOutputCounter(outputCounter);    TestOutputReceiver receiver1 = new TestOutputReceiver(counterSet, nameContextForTest());    fanOut.addOutput(receiver1);    TestOutputReceiver receiver2 = new TestOutputReceiver(counterSet, nameContextForTest());    fanOut.addOutput(receiver2);    fanOut.process("hi");    fanOut.process("bob");    CounterMean<Long> meanByteCount = outputCounter.getMeanByteCount().getAggregate();    Assert.assertEquals(7, meanByteCount.getAggregate().longValue());    Assert.assertEquals(2, meanByteCount.getCount());    Assert.assertThat(receiver1.outputElems, CoreMatchers.<Object>hasItems("hi", "bob"));    Assert.assertThat(receiver2.outputElems, CoreMatchers.<Object>hasItems("hi", "bob"));}
public void beam_f11034_0(final Receiver... receivers) throws Exception
{    if (receivers.length != 1) {        throw new AssertionError("unexpected number of receivers for DoFn");    }    outputReceiver.process("x-start");}
public void beam_f11035_0(Object elem) throws Exception
{    outputReceiver.process("y-" + elem);}
private ScheduledFuture<?> beam_f11043_0(final Runnable runnable)
{    return realExecutor.schedule(new Runnable() {        @Override        public void run() {            while (true) {                try {                    beforeRun.await();                    runnable.run();                    afterRun.await();                } catch (InterruptedException | BrokenBarrierException e) {                    break;                }            }        }    }, 0, TimeUnit.MILLISECONDS);}
public void beam_f11044_0()
{    while (true) {        try {            beforeRun.await();            runnable.run();            afterRun.await();        } catch (InterruptedException | BrokenBarrierException e) {            break;        }    }}
public void beam_f11045_0() throws Exception
{    beforeRun.await();    afterRun.await();}
public void beam_f11053_0() throws Exception
{    MockReaderIterator iterator = new MockReaderIterator(0, 10);    MockOutputReceiver receiver = new MockOutputReceiver();    ReadOperation readOperation = ReadOperation.forTest(new MockReader(iterator), receiver, context);    Thread thread = runReadLoopInThread(readOperation);    iterator.offerNext(0);    receiver.unblockProcess();            NativeReader.DynamicSplitResultWithPosition split = (NativeReader.DynamicSplitResultWithPosition) readOperation.requestDynamicSplit(splitRequestAtIndex(5L));    assertNotNull(split);    assertEquals(positionAtIndex(5L), toCloudPosition(split.getAcceptedPosition()));    for (int i = 1; i < 5; ++i) {        iterator.offerNext(i);        receiver.unblockProcess();    }    thread.join();}
public void beam_f11054_0() throws Exception
{    InMemoryReader<String> reader = new InMemoryReader<>(Arrays.asList("a", "b", "c", "d", "e"), 0, 5, StringUtf8Coder.of());    TestOutputReceiver receiver = new TestOutputReceiver(StringUtf8Coder.of(), NameContextsForTests.nameContextForTest());    final ReadOperation readOperation = ReadOperation.forTest(reader, receiver, context);    readOperation.start();            readOperation.requestDynamicSplit(splitRequestAtIndex(5L));    readOperation.finish();        readOperation.requestDynamicSplit(splitRequestAtIndex(5L));}
public void beam_f11055_0() throws Exception
{    MockOutputReceiver receiver = new MockOutputReceiver();    TestReader reader = new TestReader();    ReadOperation op = ReadOperation.forTest(reader, receiver, context);    op.start();    op.abort();    assertTrue(reader.aborted);}
public NativeReader.DynamicSplitResult beam_f11063_0()
{    Preconditions.checkState(!isClosed);    if (!tracker.trySplitAtPosition(current + 1)) {        return null;    }    return new NativeReader.DynamicSplitResultWithPosition(cloudPositionToReaderPosition(positionAtIndex(current + 1L)));}
public NativeReader.DynamicSplitResult beam_f11064_0(NativeReader.DynamicSplitRequest splitRequest)
{    Preconditions.checkState(!isClosed);    ApproximateSplitRequest approximateSplitRequest = splitRequestToApproximateSplitRequest(splitRequest);    int index = approximateSplitRequest.getPosition().getRecordIndex().intValue();    if (!tracker.trySplitAtPosition(index)) {        return null;    }    return new NativeReader.DynamicSplitResultWithPosition(cloudPositionToReaderPosition(approximateSplitRequest.getPosition()));}
public int beam_f11065_0(int next)
{    try {        return exchanger.exchange(next);    } catch (InterruptedException e) {        throw new RuntimeException(e);    }}
public void beam_f11073_0()
{    ShuffleEntry entry = new ShuffleEntry(KEY, SKEY, VALUE);    assertEquals(entry, entry);}
public void beam_f11074_0()
{    ShuffleEntry entry0 = new ShuffleEntry(KEY, SKEY, VALUE);    ShuffleEntry entry1 = new ShuffleEntry(KEY.clone(), SKEY.clone(), VALUE.clone());    assertEquals(entry0, entry1);    assertEquals(entry1, entry0);    assertEquals(entry0.hashCode(), entry1.hashCode());}
public void beam_f11075_0()
{    ShuffleEntry entry0 = new ShuffleEntry(null, null, null);    ShuffleEntry entry1 = new ShuffleEntry(null, null, null);    assertEquals(entry0, entry1);    assertEquals(entry1, entry0);    assertEquals(entry0.hashCode(), entry1.hashCode());}
public void beam_f11083_1()
{    assertNotNull(lastRunnable);        clock.setTime(clock.currentTimeMillis() + lastDelay);    Runnable r = lastRunnable;    lastRunnable = null;    r.run();}
public ScheduledExecutorService beam_f11084_0()
{    return executor;}
public void beam_f11085_0(Object elem) throws Exception
{    super.process(elem);    outputElems.add(elem);}
public void beam_f11093_0() throws Exception
{            initialLeaseExpirationMs = clock.currentTimeMillis() + 60 * 1000L;    when(workExecutor.requestCheckpoint()).thenReturn(null);    when(progressHelper.reportProgress(any(NativeReader.DynamicSplitResult.class))).thenReturn(25 * 1000L);    progressUpdater.startReportingProgress();    executor.runNextRunnable();    assertEquals(checkpointTimeMs, clock.currentTimeMillis());    verify(workExecutor).requestCheckpoint();    verify(progressHelper).reportProgress(null);    progressUpdater.stopReportingProgress();}
public void beam_f11094_0() throws Exception
{            initialLeaseExpirationMs = clock.currentTimeMillis() + 60 * 1000L;    when(workExecutor.requestCheckpoint()).thenReturn(checkpointPos);    when(progressHelper.reportProgress(any(NativeReader.DynamicSplitResult.class))).thenReturn(25 * 1000L);    progressUpdater.startReportingProgress();    executor.runNextRunnable();    assertEquals(checkpointTimeMs, clock.currentTimeMillis());    verify(workExecutor).requestCheckpoint();    verify(progressHelper).reportProgress(checkpointPos);    progressUpdater.stopReportingProgress();}
public void beam_f11095_0() throws Exception
{            initialLeaseExpirationMs = clock.currentTimeMillis() + 20 * 1000L;        when(progressHelper.reportProgress(null)).thenReturn(4 * 1000L);    progressUpdater.startReportingProgress();    executor.runNextRunnable();        assertEquals(startTimeMs + 10 * 1000L, clock.currentTimeMillis());    verify(workExecutor, never()).requestCheckpoint();    verify(progressHelper, times(1)).reportProgress(null);    verify(progressHelper, never()).reportProgress(checkpointPos);        when(progressHelper.reportProgress(null)).thenReturn(4 * 1000L);    executor.runNextRunnable();        assertEquals(startTimeMs + 14 * 1000L, clock.currentTimeMillis());    verify(workExecutor, never()).requestCheckpoint();    verify(progressHelper, times(2)).reportProgress(null);    verify(progressHelper, never()).reportProgress(checkpointPos);        when(progressHelper.reportProgress(null)).thenReturn(4 * 1000L);    executor.runNextRunnable();        assertEquals(startTimeMs + 18 * 1000L, clock.currentTimeMillis());    verify(workExecutor, never()).requestCheckpoint();    verify(progressHelper, times(3)).reportProgress(null);    verify(progressHelper, never()).reportProgress(checkpointPos);    when(workExecutor.requestCheckpoint()).thenReturn(checkpointPos);    when(progressHelper.reportProgress(any(NativeReader.DynamicSplitResult.class))).thenReturn(4 * 1000L);    executor.runNextRunnable();        assertEquals(checkpointTimeMs, clock.currentTimeMillis());    verify(workExecutor).requestCheckpoint();    verify(progressHelper, times(3)).reportProgress(null);    verify(progressHelper).reportProgress(checkpointPos);    progressUpdater.stopReportingProgress();}
public void beam_f11103_0() throws Exception
{    Sink<Object> mockSink = mock(Sink.class);    Sink.SinkWriter<Object> mockWriter = mock(Sink.SinkWriter.class);    when(mockSink.writer()).thenReturn(mockWriter);    WriteOperation writeOperation = WriteOperation.forTest(mockSink, context);    Mockito.doThrow(new IOException("Expected failure to close")).when(mockWriter).close();    writeOperation.start();    try {        writeOperation.finish();        fail("Expected exception from finish");    } catch (Exception e) {        }    assertThat(writeOperation.initializationState, equalTo(InitializationState.FINISHED));    writeOperation.abort();    assertThat(writeOperation.initializationState, equalTo(InitializationState.ABORTED));        verify(mockWriter).close();        verify(mockWriter, never()).abort();}
public void beam_f11104_0() throws Exception
{    Sink<Object> mockSink = mock(Sink.class);    WriteOperation writeOperation = WriteOperation.forTest(mockSink, context);    writeOperation.abort();    assertThat(writeOperation.initializationState, equalTo(InitializationState.ABORTED));}
public void beam_f11105_0() throws Exception
{    OperationContext context = mock(OperationContext.class);    Sink sink = mock(Sink.class);    Sink.SinkWriter sinkWriter = mock(Sink.SinkWriter.class);    when(sink.writer()).thenReturn(sinkWriter);    when(sinkWriter.add("hello")).thenReturn(10L);    when(context.counterFactory()).thenReturn(counterSet);    when(context.nameContext()).thenReturn(NameContextsForTests.nameContextForTest());    WriteOperation operation = WriteOperation.forTest(sink, context);    Closeable startCloseable = mock(Closeable.class);    Closeable processCloseable = mock(Closeable.class);    Closeable finishCloseable = mock(Closeable.class);    when(context.enterStart()).thenReturn(startCloseable);    when(context.enterProcess()).thenReturn(processCloseable);    when(context.enterFinish()).thenReturn(finishCloseable);    operation.start();    operation.process("hello");    operation.finish();    InOrder inOrder = Mockito.inOrder(sink, sinkWriter, context, startCloseable, processCloseable, finishCloseable);    inOrder.verify(context).enterStart();    inOrder.verify(sink).writer();    inOrder.verify(startCloseable).close();    inOrder.verify(context).enterProcess();    inOrder.verify(sinkWriter).add("hello");    inOrder.verify(processCloseable).close();    inOrder.verify(context).enterFinish();    inOrder.verify(sinkWriter).close();    inOrder.verify(finishCloseable).close();}
public boolean beam_f11113_0(CounterUpdate counter)
{    return kind.equals(counter.getNameAndKind().getKind());}
public void beam_f11114_0(Description description)
{    description.appendText("CounterKind " + kind);}
public static TypeSafeMatcher<CounterUpdate> beam_f11115_0(String kind)
{    return new CounterKindMatcher(kind);}
public boolean beam_f11123_0(CounterUpdate counterUpdate)
{    return value.equals(counterUpdate.getBoolean());}
public void beam_f11124_0(Description description)
{    description.appendText("CounterBooleanValue " + value);}
public static TypeSafeMatcher<CounterUpdate> beam_f11125_0(Boolean value)
{    return new CounterUpdateBooleanValueMatcher(value);}
public void beam_f11133_0(Description description)
{    description.appendText("CounterDoubleMeanSum " + value);}
public static TypeSafeMatcher<CounterUpdate> beam_f11134_0(Double value)
{    return new CounterUpdateDoubleSumMatcher(value);}
public boolean beam_f11135_0(CounterUpdate counterUpdate)
{    return value.equals(splitIntToIntValue(counterUpdate.getFloatingPointMean().getCount()));}
public void beam_f11143_0()
{        assertEquals(0x80000000_00000000L, DataflowCounterUpdateExtractor.splitIntToLong(createSplitInt(Integer.MIN_VALUE, 0L)));        assertEquals(0x00000000_00000000L, DataflowCounterUpdateExtractor.splitIntToLong(createSplitInt(0, 0L)));        assertEquals(0x7fffffff_00000000L, DataflowCounterUpdateExtractor.splitIntToLong(createSplitInt(Integer.MAX_VALUE, 0L)));        assertEquals(0x80000000_ffffffffL, DataflowCounterUpdateExtractor.splitIntToLong(createSplitInt(Integer.MIN_VALUE, 0xffffffffL)));        assertEquals(0x00000000_ffffffffL, DataflowCounterUpdateExtractor.splitIntToLong(createSplitInt(0, 0xffffffffL)));        assertEquals(0x7fffffff_ffffffffL, DataflowCounterUpdateExtractor.splitIntToLong(createSplitInt(Integer.MAX_VALUE, 0xffffffffL)));}
public void beam_f11144_0()
{    Counter<Double, ?> c1 = counterFactory.doubleSum(CounterName.named("c1"));    Counter<Integer, ?> c2 = counterFactory.intSum(CounterName.named("c2"));    Counter<Double, ?> c3 = counterFactory.doubleMean(CounterName.named("c3"));    Counter<Integer, ?> c4 = counterFactory.intMean(CounterName.named("c4"));    Counter<Double, ?> c5 = counterFactory.doubleMin(CounterName.named("c5"));    Counter<Integer, ?> c6 = counterFactory.intMin(CounterName.named("c6"));    Counter<Double, ?> c7 = counterFactory.doubleMax(CounterName.named("c7"));    Counter<Integer, ?> c8 = counterFactory.intMax(CounterName.named("c8"));    c1.addValue(-1D).addValue(-1D);    assertThat(((Counter<?, ?>) c1).extractUpdate(true, DataflowCounterUpdateExtractor.INSTANCE), hasDoubleValue(-2D));    c2.addValue(-1).addValue(-1);    assertThat(((Counter<?, ?>) c2).extractUpdate(true, DataflowCounterUpdateExtractor.INSTANCE), hasIntegerValue(-2));    c3.addValue(-1.5D).addValue(-2.5D).addValue(-3D);    assertThat(((Counter<?, ?>) c3).extractUpdate(true, DataflowCounterUpdateExtractor.INSTANCE), allOf(hasDoubleSum(-7D), hasDoubleCount(3)));    c4.addValue(-1).addValue(-2).addValue(-3);    assertThat(((Counter<?, ?>) c4).extractUpdate(true, DataflowCounterUpdateExtractor.INSTANCE), allOf(hasIntegerSum(-6), hasIntegerCount(3)));    c5.addValue(-1D).addValue(-2D);    assertThat(((Counter<?, ?>) c5).extractUpdate(true, DataflowCounterUpdateExtractor.INSTANCE), hasDoubleValue(-2D));    c6.addValue(-1).addValue(-2);    assertThat(((Counter<?, ?>) c6).extractUpdate(true, DataflowCounterUpdateExtractor.INSTANCE), hasIntegerValue(-2));    c7.addValue(-1D).addValue(-2D);    assertThat(((Counter<?, ?>) c7).extractUpdate(true, DataflowCounterUpdateExtractor.INSTANCE), hasDoubleValue(-1D));    c8.addValue(-1).addValue(-2);    assertThat(((Counter<?, ?>) c8).extractUpdate(true, DataflowCounterUpdateExtractor.INSTANCE), hasIntegerValue(-1));}
public void beam_f11145_0()
{    Counter<Integer, ?> c1 = counterFactory.intSum(CounterName.named("c1"));    c1.addValue(1);    assertThat(((Counter<?, ?>) c1).extractUpdate(false, DataflowCounterUpdateExtractor.INSTANCE), hasIntegerValue(1));    assertThat(((Counter<?, ?>) c1).extractUpdate(false, DataflowCounterUpdateExtractor.INSTANCE), hasIntegerValue(1));    c1.addValue(2);    assertThat(((Counter<?, ?>) c1).extractUpdate(false, DataflowCounterUpdateExtractor.INSTANCE), hasIntegerValue(3));    assertThat(((Counter<?, ?>) c1).extractUpdate(true, DataflowCounterUpdateExtractor.INSTANCE), hasIntegerValue(3));    assertThat(((Counter<?, ?>) c1).extractUpdate(true, DataflowCounterUpdateExtractor.INSTANCE), hasIntegerValue(0));}
public static void beam_f11153_0(GroupAlsoByWindowDoFnFactory<String, Long, Long> gabwFactory, CombineFn<Long, ?, Long> combineFn) throws Exception
{    WindowingStrategy<?, IntervalWindow> windowingStrategy = WindowingStrategy.of(SlidingWindows.of(Duration.millis(20)).every(Duration.millis(10))).withTimestampCombiner(TimestampCombiner.EARLIEST);    List<WindowedValue<KV<String, Long>>> result = runGABW(gabwFactory, windowingStrategy, "k", WindowedValue.of(1L, new Instant(5), Arrays.asList(window(-10, 10), window(0, 20)), PaneInfo.NO_FIRING), WindowedValue.of(2L, new Instant(15), Arrays.asList(window(0, 20), window(10, 30)), PaneInfo.NO_FIRING), WindowedValue.of(4L, new Instant(18), Arrays.asList(window(0, 20), window(10, 30)), PaneInfo.NO_FIRING));    assertThat(result, hasSize(3));    TimestampedValue<KV<String, Long>> item0 = getOnlyElementInWindow(result, window(-10, 10));    assertThat(item0.getValue().getKey(), equalTo("k"));    assertThat(item0.getValue().getValue(), equalTo(combineFn.apply(ImmutableList.of(1L))));    assertThat(item0.getTimestamp(), equalTo(new Instant(5L)));    TimestampedValue<KV<String, Long>> item1 = getOnlyElementInWindow(result, window(0, 20));    assertThat(item1.getValue().getKey(), equalTo("k"));    assertThat(item1.getValue().getValue(), equalTo(combineFn.apply(ImmutableList.of(1L, 2L, 4L))));        assertThat(item1.getTimestamp(), equalTo(new Instant(10L)));    TimestampedValue<KV<String, Long>> item2 = getOnlyElementInWindow(result, window(10, 30));    assertThat(item2.getValue().getKey(), equalTo("k"));    assertThat(item2.getValue().getValue(), equalTo(combineFn.apply(ImmutableList.of(2L, 4L))));        assertThat(item2.getTimestamp(), equalTo(new Instant(20L)));}
public static void beam_f11154_0(GroupAlsoByWindowDoFnFactory<String, String, Iterable<String>> gabwFactory) throws Exception
{    WindowingStrategy<?, IntervalWindow> windowingStrategy = WindowingStrategy.of(FixedWindows.of(Duration.millis(10)));    List<WindowedValue<KV<String, Iterable<String>>>> result = runGABW(gabwFactory, windowingStrategy, "key", WindowedValue.of("v1", new Instant(1), Arrays.asList(window(0, 5)), PaneInfo.NO_FIRING), WindowedValue.of("v2", new Instant(4), Arrays.asList(window(1, 5)), PaneInfo.NO_FIRING), WindowedValue.of("v3", new Instant(4), Arrays.asList(window(0, 5)), PaneInfo.NO_FIRING));    assertThat(result, hasSize(2));    TimestampedValue<KV<String, Iterable<String>>> item0 = getOnlyElementInWindow(result, window(0, 5));    assertThat(item0.getValue().getValue(), containsInAnyOrder("v1", "v3"));    assertThat(item0.getTimestamp(), equalTo(window(1, 5).maxTimestamp()));    TimestampedValue<KV<String, Iterable<String>>> item1 = getOnlyElementInWindow(result, window(1, 5));    assertThat(item1.getValue().getValue(), contains("v2"));    assertThat(item1.getTimestamp(), equalTo(window(0, 5).maxTimestamp()));}
public static void beam_f11155_0(GroupAlsoByWindowDoFnFactory<String, String, Iterable<String>> gabwFactory) throws Exception
{    WindowingStrategy<?, IntervalWindow> windowingStrategy = WindowingStrategy.of(Sessions.withGapDuration(Duration.millis(10)));    List<WindowedValue<KV<String, Iterable<String>>>> result = runGABW(gabwFactory, windowingStrategy, "key", WindowedValue.of("v1", new Instant(0), Arrays.asList(window(0, 10)), PaneInfo.NO_FIRING), WindowedValue.of("v2", new Instant(5), Arrays.asList(window(5, 15)), PaneInfo.NO_FIRING), WindowedValue.of("v3", new Instant(15), Arrays.asList(window(15, 25)), PaneInfo.NO_FIRING));    assertThat(result, hasSize(2));    TimestampedValue<KV<String, Iterable<String>>> item0 = getOnlyElementInWindow(result, window(0, 15));    assertThat(item0.getValue().getValue(), containsInAnyOrder("v1", "v2"));    assertThat(item0.getTimestamp(), equalTo(window(0, 15).maxTimestamp()));    TimestampedValue<KV<String, Iterable<String>>> item1 = getOnlyElementInWindow(result, window(15, 25));    assertThat(item1.getValue().getValue(), contains("v3"));    assertThat(item1.getTimestamp(), equalTo(window(15, 25).maxTimestamp()));}
private static List<WindowedValue<KV<K, OutputT>>> beam_f11163_0(GroupAlsoByWindowDoFnFactory<K, InputT, OutputT> gabwFactory, WindowingStrategy<?, W> windowingStrategy, K key, Collection<WindowedValue<InputT>> values) throws Exception
{    final StateInternalsFactory<K> stateInternalsCache = new CachingStateInternalsFactory<>();    List<WindowedValue<KV<K, OutputT>>> output = processElement(gabwFactory.forStrategy(windowingStrategy, stateInternalsCache), KV.<K, Iterable<WindowedValue<InputT>>>of(key, values));        for (WindowedValue<KV<K, OutputT>> value : output) {        assertThat(value.getValue().getKey(), equalTo(key));    }    return output;}
private static BoundedWindow beam_f11164_0(long start, long end)
{    return new IntervalWindow(new Instant(start), new Instant(end));}
public StateInternals beam_f11165_0(K key)
{    try {        return stateInternalsCache.get(key);    } catch (Exception exc) {        throw new RuntimeException(exc);    }}
public void beam_f11173_0() throws Exception
{    GroupAlsoByWindowProperties.emptyInputEmptyOutput(new GABWAndCombineDoFnFactory<>(Sum.ofLongs()));}
public void beam_f11174_0() throws Exception
{    CombineFn<Long, ?, Long> combineFn = Sum.ofLongs();    GroupAlsoByWindowProperties.combinesElementsInSlidingWindows(new GABWAndCombineDoFnFactory<>(combineFn), combineFn);}
public void beam_f11175_0() throws Exception
{    CombineFn<Long, ?, Long> combineFn = Sum.ofLongs();    GroupAlsoByWindowProperties.combinesElementsPerSession(new GABWAndCombineDoFnFactory<>(combineFn), combineFn);}
public void beam_f11183_0() throws Exception
{    thrown.expect(IllegalArgumentException.class);    thrown.expectMessage("TimestampCombiner");    thrown.expectMessage("not support");    GroupAlsoByWindowProperties.groupsElementsIntoFixedWindowsWithLatestTimestamp(new GABWViaIteratorsDoFnFactory<String, String>());}
public void beam_f11184_0() throws Exception
{    thrown.expect(IllegalArgumentException.class);    thrown.expectMessage("merging");    thrown.expectMessage("not support");    GroupAlsoByWindowProperties.groupsElementsInMergedSessions(new GABWViaIteratorsDoFnFactory<String, String>());}
public BatchGroupAlsoByWindowFn<K, InputT, Iterable<InputT>> beam_f11185_0(WindowingStrategy<?, W> windowingStrategy, StateInternalsFactory<K> stateInternalsFactory)
{    return new BatchGroupAlsoByWindowViaOutputBufferFn<K, InputT, Iterable<InputT>, W>(windowingStrategy, stateInternalsFactory, SystemReduceFn.<K, InputT, W>buffering(inputCoder));}
public void beam_f11193_0() throws Exception
{    GroupAlsoByWindowProperties.groupsElementsInMergedSessionsWithEndOfWindowTimestamp(new BufferingGABWViaOutputBufferDoFnFactory<String, String>(StringUtf8Coder.of()));}
public void beam_f11194_0() throws Exception
{    GroupAlsoByWindowProperties.groupsElementsInMergedSessionsWithLatestTimestamp(new BufferingGABWViaOutputBufferDoFnFactory<String, String>(StringUtf8Coder.of()));}
public void beam_f11195_0(TupleTag<T> tag, WindowedValue<T> output)
{    @SuppressWarnings({ "rawtypes", "unchecked" })    List<WindowedValue<T>> outputList = (List) outputLists.get(tag);    if (outputList == null) {        outputList = Lists.newArrayList();        @SuppressWarnings({ "rawtypes", "unchecked" })        List<WindowedValue<?>> untypedList = (List) outputList;        outputLists.put(tag, untypedList);    }    outputList.add(output);}
public void beam_f11203_0() throws Exception
{    monitor = MemoryMonitor.forTest(provider, 10, 0, true, null, localDumpFolder);        monitor.dumpHeap();        assertFalse(monitor.tryUploadHeapDumpIfItExists());}
public void beam_f11204_0() throws Exception
{    Builder builder = ScalableBloomFilter.withMaximumSizeBytes(MAX_SIZE);    assertTrue("Expected Bloom filter to have been modified.", builder.put(BUFFER));        assertFalse("Expected Bloom filter to not have been modified.", builder.put(BUFFER));        int maxValue = insertAndVerifyContents(builder, 31);            verifyCoder(builder.build(), maxValue, MAX_SIZE / 50);}
public void beam_f11205_0() throws Exception
{    Builder builder = ScalableBloomFilter.withMaximumSizeBytes(MAX_SIZE);    int maxValue = insertAndVerifyContents(builder, (int) (MAX_SIZE * 1.1));    ScalableBloomFilter bloomFilter = builder.build();            verifyCoder(bloomFilter, maxValue, MAX_SIZE);}
public void beam_f11213_0() throws Exception
{    String name = "Fake server for " + getClass();    this.server = InProcessServerBuilder.forName(name).fallbackHandlerRegistry(serviceRegistry).executor(Executors.newFixedThreadPool(1)).build().start();    this.client = GrpcWindmillServer.newTestInstance(name, true);}
public void beam_f11214_0() throws Exception
{    server.shutdownNow();}
private void beam_f11215_0(Stream stream)
{    if (remainingErrors > 0 && ThreadLocalRandom.current().nextInt(20) == 0) {        try {            stream.onError(new RuntimeException("oops"));            remainingErrors--;        } catch (IllegalStateException e) {                }    }}
public StreamObserver<StreamingGetDataRequest> beam_f11224_1(StreamObserver<StreamingGetDataResponse> responseObserver)
{    return new StreamObserver<StreamingGetDataRequest>() {        boolean sawHeader = false;        HashSet<Long> seenIds = new HashSet<>();        ResponseErrorInjector injector = new ResponseErrorInjector(responseObserver);        StreamingGetDataResponse.Builder responseBuilder = StreamingGetDataResponse.newBuilder();        @Override        public void onNext(StreamingGetDataRequest chunk) {            maybeInjectError(responseObserver);            try {                if (!sawHeader) {                                        errorCollector.checkThat(chunk.getHeader(), Matchers.equalTo(JobHeader.newBuilder().setJobId("job").setProjectId("project").setWorkerId("worker").build()));                    sawHeader = true;                } else {                                        errorCollector.checkThat(chunk.getSerializedSize(), Matchers.lessThanOrEqualTo(STREAM_CHUNK_SIZE));                    int i = 0;                    for (GlobalDataRequest request : chunk.getGlobalDataRequestList()) {                        long requestId = chunk.getRequestId(i++);                        errorCollector.checkThat(seenIds.add(requestId), Matchers.is(true));                        sendResponse(requestId, processGlobalDataRequest(request));                    }                    for (ComputationGetDataRequest request : chunk.getStateRequestList()) {                        long requestId = chunk.getRequestId(i++);                        errorCollector.checkThat(seenIds.add(requestId), Matchers.is(true));                        sendResponse(requestId, processStateRequest(request));                    }                    flushResponse();                }            } catch (Exception e) {                errorCollector.addError(e);            }        }        @Override        public void onError(Throwable throwable) {        }        @Override        public void onCompleted() {            injector.cancel();            responseObserver.onCompleted();        }        private ByteString processGlobalDataRequest(GlobalDataRequest request) {            errorCollector.checkThat(request.getStateFamily(), Matchers.is("family"));            return GlobalData.newBuilder().setDataId(request.getDataId()).setStateFamily("family").setData(ByteString.copyFromUtf8(request.getDataId().getTag())).build().toByteString();        }        private ByteString processStateRequest(ComputationGetDataRequest compRequest) {            errorCollector.checkThat(compRequest.getRequestsCount(), Matchers.is(1));            errorCollector.checkThat(compRequest.getComputationId(), Matchers.is("computation"));            KeyedGetDataRequest request = compRequest.getRequests(0);            KeyedGetDataResponse response = makeGetDataResponse(request.getKey().toStringUtf8(), request.getValuesToFetch(0).getTag().toStringUtf8());            return response.toByteString();        }        private void sendResponse(long id, ByteString serializedResponse) {            if (ThreadLocalRandom.current().nextInt(4) == 0) {                sendChunkedResponse(id, serializedResponse);            } else {                responseBuilder.addRequestId(id).addSerializedResponse(serializedResponse);                if (responseBuilder.getRequestIdCount() > 10) {                    flushResponse();                }            }        }        private void sendChunkedResponse(long id, ByteString serializedResponse) {                        for (int i = 0; i < serializedResponse.size(); i += 10) {                int end = Math.min(serializedResponse.size(), i + 10);                try {                    responseObserver.onNext(StreamingGetDataResponse.newBuilder().addRequestId(id).addSerializedResponse(serializedResponse.substring(i, end)).setRemainingBytesForResponse(serializedResponse.size() - end).build());                } catch (IllegalStateException e) {                                }            }        }        private void flushResponse() {            if (responseBuilder.getRequestIdCount() > 0) {                                try {                    responseObserver.onNext(responseBuilder.build());                } catch (IllegalStateException e) {                                }                responseBuilder.clear();            }        }    };}
public void beam_f11225_1(StreamingGetDataRequest chunk)
{    maybeInjectError(responseObserver);    try {        if (!sawHeader) {                        errorCollector.checkThat(chunk.getHeader(), Matchers.equalTo(JobHeader.newBuilder().setJobId("job").setProjectId("project").setWorkerId("worker").build()));            sawHeader = true;        } else {                        errorCollector.checkThat(chunk.getSerializedSize(), Matchers.lessThanOrEqualTo(STREAM_CHUNK_SIZE));            int i = 0;            for (GlobalDataRequest request : chunk.getGlobalDataRequestList()) {                long requestId = chunk.getRequestId(i++);                errorCollector.checkThat(seenIds.add(requestId), Matchers.is(true));                sendResponse(requestId, processGlobalDataRequest(request));            }            for (ComputationGetDataRequest request : chunk.getStateRequestList()) {                long requestId = chunk.getRequestId(i++);                errorCollector.checkThat(seenIds.add(requestId), Matchers.is(true));                sendResponse(requestId, processStateRequest(request));            }            flushResponse();        }    } catch (Exception e) {        errorCollector.addError(e);    }}
public void beam_f11227_0()
{    injector.cancel();    responseObserver.onCompleted();}
private KeyedGetDataResponse beam_f11235_0(String key, String tag)
{    return KeyedGetDataResponse.newBuilder().setKey(ByteString.copyFromUtf8("key")).addValues(TagValue.newBuilder().setTag(ByteString.copyFromUtf8("tag")).setValue(Value.newBuilder().setTimestamp(0).setData(ByteString.copyFromUtf8(tag + "-value")))).build();}
private GlobalDataRequest beam_f11236_0(String key)
{    return GlobalDataRequest.newBuilder().setStateFamily("family").setDataId(GlobalDataId.newBuilder().setTag(key).setVersion(ByteString.EMPTY).build()).build();}
private GlobalData beam_f11237_0(String key)
{    return GlobalData.newBuilder().setStateFamily("family").setDataId(GlobalDataId.newBuilder().setTag(key).setVersion(ByteString.EMPTY)).setData(ByteString.copyFromUtf8(key)).build();}
public StreamObserver<StreamingGetDataRequest> beam_f11246_1(StreamObserver<StreamingGetDataResponse> responseObserver)
{    return new StreamObserver<StreamingGetDataRequest>() {        boolean sawHeader = false;        @Override        public void onNext(StreamingGetDataRequest chunk) {            try {                if (!sawHeader) {                                        errorCollector.checkThat(chunk.getHeader(), Matchers.equalTo(JobHeader.newBuilder().setJobId("job").setProjectId("project").setWorkerId("worker").build()));                    sawHeader = true;                } else {                                        errorCollector.checkThat(chunk.getSerializedSize(), Matchers.lessThanOrEqualTo(STREAM_CHUNK_SIZE));                    errorCollector.checkThat(chunk.getRequestIdCount(), Matchers.is(0));                    synchronized (heartbeats) {                        for (ComputationGetDataRequest request : chunk.getStateRequestList()) {                            errorCollector.checkThat(request.getRequestsCount(), Matchers.is(1));                            heartbeats.putIfAbsent(request.getComputationId(), new ArrayList<>());                            heartbeats.get(request.getComputationId()).add(request.getRequestsList().get(0));                        }                    }                }            } catch (Exception e) {                errorCollector.addError(e);            }        }        @Override        public void onError(Throwable throwable) {        }        @Override        public void onCompleted() {            responseObserver.onCompleted();        }    };}
public void beam_f11247_1(StreamingGetDataRequest chunk)
{    try {        if (!sawHeader) {                        errorCollector.checkThat(chunk.getHeader(), Matchers.equalTo(JobHeader.newBuilder().setJobId("job").setProjectId("project").setWorkerId("worker").build()));            sawHeader = true;        } else {                        errorCollector.checkThat(chunk.getSerializedSize(), Matchers.lessThanOrEqualTo(STREAM_CHUNK_SIZE));            errorCollector.checkThat(chunk.getRequestIdCount(), Matchers.is(0));            synchronized (heartbeats) {                for (ComputationGetDataRequest request : chunk.getStateRequestList()) {                    errorCollector.checkThat(request.getRequestsCount(), Matchers.is(1));                    heartbeats.putIfAbsent(request.getComputationId(), new ArrayList<>());                    heartbeats.get(request.getComputationId()).add(request.getRequestsList().get(0));                }            }        }    } catch (Exception e) {        errorCollector.addError(e);    }}
public void beam_f11249_0()
{    responseObserver.onCompleted();}
private PaneInfo beam_f11258_0(int index)
{    return PaneInfo.createPane(false, false, Timing.EARLY, index, -1);}
public void beam_f11259_0() throws Exception
{    Windmill.WorkItem workItem = Windmill.WorkItem.newBuilder().setKey(SERIALIZED_KEY).setWorkToken(17).setTimers(Windmill.TimerBundle.newBuilder().addTimers(makeSerializedTimer(STATE_NAMESPACE_1, 0, Windmill.Timer.Type.REALTIME)).addTimers(makeSerializedTimer(STATE_NAMESPACE_1, 1, Windmill.Timer.Type.WATERMARK)).addTimers(makeSerializedTimer(STATE_NAMESPACE_1, 2, Windmill.Timer.Type.REALTIME)).addTimers(makeSerializedTimer(STATE_NAMESPACE_2, 3, Windmill.Timer.Type.WATERMARK)).build()).build();    KeyedWorkItem<String, String> keyedWorkItem = new WindmillKeyedWorkItem<>(KEY, workItem, WINDOW_CODER, WINDOWS_CODER, VALUE_CODER);    assertThat(keyedWorkItem.timersIterable(), Matchers.contains(makeTimer(STATE_NAMESPACE_1, 1, TimeDomain.EVENT_TIME), makeTimer(STATE_NAMESPACE_2, 3, TimeDomain.EVENT_TIME), makeTimer(STATE_NAMESPACE_1, 0, TimeDomain.PROCESSING_TIME), makeTimer(STATE_NAMESPACE_1, 2, TimeDomain.PROCESSING_TIME)));}
private static Windmill.Timer beam_f11260_0(StateNamespace ns, long timestamp, Windmill.Timer.Type type)
{    return Windmill.Timer.newBuilder().setTag(WindmillTimerInternals.timerTag(WindmillNamespacePrefix.SYSTEM_NAMESPACE_PREFIX, TimerData.of(ns, new Instant(timestamp), WindmillTimerInternals.timerTypeToTimeDomain(type)))).setTimestamp(WindmillTimeUtils.harnessToWindmillTimestamp(new Instant(timestamp))).setType(type).setStateFamily(STATE_FAMILY).build();}
public TestState beam_f11268_0(StateBinder binder)
{    throw new UnsupportedOperationException();}
public StateSpec<TestState> beam_f11269_0()
{    return null;}
public String beam_f11270_0()
{    return "Tag(" + id + ")";}
private static StateNamespace beam_f11278_0(long start, int triggerIdx)
{    return StateNamespaces.windowAndTrigger(IntervalWindow.getCoder(), new IntervalWindow(new Instant(start), new Instant(start + 1)), triggerIdx);}
public void beam_f11279_0()
{    cache = new WindmillStateCache();    assertEquals(0, cache.getWeight());}
public void beam_f11280_0() throws Exception
{    keyCache = cache.forComputation(COMPUTATION).forKey(KEY, STATE_FAMILY, 0L, 1L);    assertNull(keyCache.get(StateNamespaces.global(), new TestStateTag("tag1")));    assertNull(keyCache.get(windowNamespace(0), new TestStateTag("tag2")));    assertNull(keyCache.get(triggerNamespace(0, 0), new TestStateTag("tag3")));    assertNull(keyCache.get(triggerNamespace(0, 0), new TestStateTag("tag2")));    keyCache.put(StateNamespaces.global(), new TestStateTag("tag1"), new TestState("g1"), 2);    assertEquals(129, cache.getWeight());    keyCache.put(windowNamespace(0), new TestStateTag("tag2"), new TestState("w2"), 2);    assertEquals(258, cache.getWeight());    keyCache.put(triggerNamespace(0, 0), new TestStateTag("tag3"), new TestState("t3"), 2);    assertEquals(276, cache.getWeight());    keyCache.put(triggerNamespace(0, 0), new TestStateTag("tag2"), new TestState("t2"), 2);    assertEquals(294, cache.getWeight());    keyCache = cache.forComputation(COMPUTATION).forKey(KEY, STATE_FAMILY, 0L, 2L);    assertEquals(new TestState("g1"), keyCache.get(StateNamespaces.global(), new TestStateTag("tag1")));    assertEquals(new TestState("w2"), keyCache.get(windowNamespace(0), new TestStateTag("tag2")));    assertEquals(new TestState("t3"), keyCache.get(triggerNamespace(0, 0), new TestStateTag("tag3")));    assertEquals(new TestState("t2"), keyCache.get(triggerNamespace(0, 0), new TestStateTag("tag2")));}
public void beam_f11288_0() throws Exception
{    WindmillStateCache.ForKey keyCache1 = cache.forComputation("comp1").forKey(ByteString.copyFromUtf8("key1"), STATE_FAMILY, 0L, 0L);    StateTag<TestState> tag = new TestStateTagWithBadEquality("tag1");    keyCache1.put(StateNamespaces.global(), tag, new TestState("g1"), 1);    keyCache1 = cache.forComputation("comp1").forKey(ByteString.copyFromUtf8("key1"), STATE_FAMILY, 0L, 1L);    assertEquals(new TestState("g1"), keyCache1.get(StateNamespaces.global(), tag));    assertEquals(new TestState("g1"), keyCache1.get(StateNamespaces.global(), new TestStateTagWithBadEquality("tag1")));}
private static ByteString beam_f11289_0(StateNamespace namespace, String addrId)
{    return ByteString.copyFromUtf8(namespace.stringKey() + "+u" + addrId);}
private static ByteString beam_f11290_0(StateNamespace namespace, String addrId)
{    return ByteString.copyFromUtf8(namespace.stringKey() + "+s" + addrId);}
public void beam_f11298_0() throws Exception
{    StateTag<BagState<String>> addr = StateTags.bag("bag", StringUtf8Coder.of());    BagState<String> bag = underTest.state(NAMESPACE, addr);    SettableFuture<Iterable<String>> future = SettableFuture.create();    when(mockReader.bagFuture(key(NAMESPACE, "bag"), STATE_FAMILY, StringUtf8Coder.of())).thenReturn(future);    ReadableState<Boolean> result = bag.isEmpty().readLater();    Mockito.verify(mockReader).bagFuture(key(NAMESPACE, "bag"), STATE_FAMILY, StringUtf8Coder.of());    waitAndSet(future, Arrays.asList("world"), 200);    assertThat(result.read(), Matchers.is(false));}
public void beam_f11299_0() throws Exception
{    StateTag<BagState<String>> addr = StateTags.bag("bag", StringUtf8Coder.of());    BagState<String> bag = underTest.state(NAMESPACE, addr);    SettableFuture<Iterable<String>> future = SettableFuture.create();    when(mockReader.bagFuture(key(NAMESPACE, "bag"), STATE_FAMILY, StringUtf8Coder.of())).thenReturn(future);    ReadableState<Boolean> result = bag.isEmpty().readLater();    Mockito.verify(mockReader).bagFuture(key(NAMESPACE, "bag"), STATE_FAMILY, StringUtf8Coder.of());    waitAndSet(future, Arrays.<String>asList(), 200);    assertThat(result.read(), Matchers.is(true));}
public void beam_f11300_0() throws Exception
{    StateTag<BagState<String>> addr = StateTags.bag("bag", StringUtf8Coder.of());    BagState<String> bag = underTest.state(NAMESPACE, addr);    bag.clear();    ReadableState<Boolean> result = bag.isEmpty();    Mockito.verify(mockReader, never()).bagFuture(key(NAMESPACE, "bag"), STATE_FAMILY, StringUtf8Coder.of());    assertThat(result.read(), Matchers.is(true));    bag.add("hello");    assertThat(result.read(), Matchers.is(false));}
public void beam_f11308_0() throws Exception
{    GroupingState<Integer, Integer> value = underTest.state(NAMESPACE, COMBINING_ADDR);    value.clear();    ReadableState<Boolean> result = value.isEmpty();    Mockito.verify(mockReader, never()).bagFuture(COMBINING_KEY, STATE_FAMILY, accumCoder);    assertThat(result.read(), Matchers.is(true));    value.add(87);    assertThat(result.read(), Matchers.is(false));}
public void beam_f11309_0() throws Exception
{    disableCompactOnWrite();    GroupingState<Integer, Integer> value = underTest.state(NAMESPACE, COMBINING_ADDR);    value.add(5);    value.add(6);    Windmill.WorkItemCommitRequest.Builder commitBuilder = Windmill.WorkItemCommitRequest.newBuilder();    underTest.persist(commitBuilder);    assertEquals(1, commitBuilder.getBagUpdatesCount());    TagBag bagUpdates = commitBuilder.getBagUpdates(0);    assertEquals(COMBINING_KEY, bagUpdates.getTag());    assertEquals(1, bagUpdates.getValuesCount());    assertEquals(11, CoderUtils.decodeFromByteArray(accumCoder, bagUpdates.getValues(0).toByteArray())[0]);    Mockito.verifyNoMoreInteractions(mockReader);}
public void beam_f11310_0() throws Exception
{    forceCompactOnWrite();    Mockito.when(mockReader.bagFuture(org.mockito.Matchers.<ByteString>any(), org.mockito.Matchers.<String>any(), org.mockito.Matchers.<Coder<int[]>>any())).thenReturn(Futures.<Iterable<int[]>>immediateFuture(ImmutableList.of(new int[] { 40 }, new int[] { 60 })));    GroupingState<Integer, Integer> value = underTest.state(NAMESPACE, COMBINING_ADDR);    value.add(5);    value.add(6);    Windmill.WorkItemCommitRequest.Builder commitBuilder = Windmill.WorkItemCommitRequest.newBuilder();    underTest.persist(commitBuilder);    assertEquals(1, commitBuilder.getBagUpdatesCount());    TagBag bagUpdates = commitBuilder.getBagUpdates(0);    assertEquals(COMBINING_KEY, bagUpdates.getTag());    assertEquals(1, bagUpdates.getValuesCount());    assertTrue(bagUpdates.getDeleteAll());    assertEquals(111, CoderUtils.decodeFromByteArray(accumCoder, bagUpdates.getValues(0).toByteArray())[0]);}
public void beam_f11318_0() throws Exception
{    StateTag<WatermarkHoldState> addr = StateTags.watermarkStateInternal("watermark", TimestampCombiner.LATEST);    WatermarkHoldState hold = underTest.state(NAMESPACE, addr);    hold.add(new Instant(1000));    hold.add(new Instant(2000));    when(mockReader.watermarkFuture(key(NAMESPACE, "watermark"), STATE_FAMILY)).thenReturn(Futures.<Instant>immediateFuture(null));    Windmill.WorkItemCommitRequest.Builder commitBuilder = Windmill.WorkItemCommitRequest.newBuilder();    underTest.persist(commitBuilder);    assertEquals(1, commitBuilder.getWatermarkHoldsCount());    Windmill.WatermarkHold watermarkHold = commitBuilder.getWatermarkHolds(0);    assertEquals(key(NAMESPACE, "watermark"), watermarkHold.getTag());    assertEquals(TimeUnit.MILLISECONDS.toMicros(2000), watermarkHold.getTimestamps(0));    Mockito.verify(mockReader).watermarkFuture(key(NAMESPACE, "watermark"), STATE_FAMILY);    Mockito.verifyNoMoreInteractions(mockReader);}
public void beam_f11319_0() throws Exception
{    StateTag<WatermarkHoldState> addr = StateTags.watermarkStateInternal("watermark", TimestampCombiner.LATEST);    WatermarkHoldState hold = underTest.state(NAMESPACE, addr);    hold.add(new Instant(1000));    hold.add(new Instant(2000));    when(mockReader.watermarkFuture(key(NAMESPACE, "watermark"), STATE_FAMILY)).thenReturn(Futures.<Instant>immediateFuture(new Instant(4000)));    Windmill.WorkItemCommitRequest.Builder commitBuilder = Windmill.WorkItemCommitRequest.newBuilder();    underTest.persist(commitBuilder);    assertEquals(1, commitBuilder.getWatermarkHoldsCount());    Windmill.WatermarkHold watermarkHold = commitBuilder.getWatermarkHolds(0);    assertEquals(key(NAMESPACE, "watermark"), watermarkHold.getTag());    assertEquals(TimeUnit.MILLISECONDS.toMicros(4000), watermarkHold.getTimestamps(0));    Mockito.verify(mockReader).watermarkFuture(key(NAMESPACE, "watermark"), STATE_FAMILY);    Mockito.verifyNoMoreInteractions(mockReader);}
public void beam_f11320_0() throws Exception
{    StateTag<WatermarkHoldState> addr = StateTags.watermarkStateInternal("watermark", TimestampCombiner.LATEST);    WatermarkHoldState hold = underTest.state(NAMESPACE, addr);    hold.add(new Instant(1000));    hold.add(new Instant(2000));    when(mockReader.watermarkFuture(key(NAMESPACE, "watermark"), STATE_FAMILY)).thenReturn(Futures.<Instant>immediateFuture(new Instant(500)));    Windmill.WorkItemCommitRequest.Builder commitBuilder = Windmill.WorkItemCommitRequest.newBuilder();    underTest.persist(commitBuilder);    assertEquals(1, commitBuilder.getWatermarkHoldsCount());    Windmill.WatermarkHold watermarkHold = commitBuilder.getWatermarkHolds(0);    assertEquals(key(NAMESPACE, "watermark"), watermarkHold.getTag());    assertEquals(TimeUnit.MILLISECONDS.toMicros(2000), watermarkHold.getTimestamps(0));    Mockito.verify(mockReader).watermarkFuture(key(NAMESPACE, "watermark"), STATE_FAMILY);    Mockito.verifyNoMoreInteractions(mockReader);}
public void beam_f11328_0() throws Exception
{    StateTag<ValueState<String>> addr = StateTags.value("value", StringUtf8Coder.of());    ValueState<String> value = underTest.state(NAMESPACE, addr);    value.write("Hi");    Windmill.WorkItemCommitRequest.Builder commitBuilder = Windmill.WorkItemCommitRequest.newBuilder();    underTest.persist(commitBuilder);    assertEquals(1, commitBuilder.getValueUpdatesCount());    TagValue valueUpdate = commitBuilder.getValueUpdates(0);    assertEquals(key(NAMESPACE, "value"), valueUpdate.getTag());    assertEquals("Hi", valueUpdate.getValue().getData().toStringUtf8());    assertTrue(valueUpdate.isInitialized());    Mockito.verifyNoMoreInteractions(mockReader);}
public void beam_f11329_0() throws Exception
{    StateTag<ValueState<String>> addr = StateTags.value("value", StringUtf8Coder.of());    ValueState<String> value = underTest.state(NAMESPACE, addr);    value.write("Hi");    value.clear();    Windmill.WorkItemCommitRequest.Builder commitBuilder = Windmill.WorkItemCommitRequest.newBuilder();    underTest.persist(commitBuilder);    assertEquals(1, commitBuilder.getValueUpdatesCount());    TagValue valueUpdate = commitBuilder.getValueUpdates(0);    assertEquals(key(NAMESPACE, "value"), valueUpdate.getTag());    assertEquals(0, valueUpdate.getValue().getData().size());    Mockito.verifyNoMoreInteractions(mockReader);}
public void beam_f11330_0() throws Exception
{    StateTag<ValueState<String>> addr = StateTags.value("value", StringUtf8Coder.of());    underTest.state(NAMESPACE, addr);    Windmill.WorkItemCommitRequest.Builder commitBuilder = Windmill.WorkItemCommitRequest.newBuilder();    underTest.persist(commitBuilder);    assertEquals(0, commitBuilder.getValueUpdatesCount());    Mockito.verifyNoMoreInteractions(mockReader);}
private static void beam_f11338_0(Object obj) throws Exception
{    WindmillStateTestUtils.assertNoReference(obj, WindmillStateReader.class);}
public void beam_f11339_0()
{    MockitoAnnotations.initMocks(this);    underTest = new WindmillStateReader(mockWindmill, COMPUTATION, DATA_KEY, SHARDING_KEY, WORK_TOKEN);}
private Windmill.Value beam_f11340_0(int value) throws IOException
{    return Windmill.Value.newBuilder().setData(intData(value)).setTimestamp(WindmillTimeUtils.harnessToWindmillTimestamp(BoundedWindow.TIMESTAMP_MAX_VALUE)).build();}
public void beam_f11348_0() throws Exception
{        Future<Instant> watermarkFuture = underTest.watermarkFuture(STATE_KEY_2, STATE_FAMILY);    Future<Iterable<Integer>> bagFuture = underTest.bagFuture(STATE_KEY_1, STATE_FAMILY, INT_CODER);    Mockito.verifyNoMoreInteractions(mockWindmill);    Windmill.KeyedGetDataResponse.Builder response = Windmill.KeyedGetDataResponse.newBuilder().setKey(DATA_KEY).setFailed(true);    Mockito.when(mockWindmill.getStateData(Mockito.eq(COMPUTATION), Mockito.isA(Windmill.KeyedGetDataRequest.class))).thenReturn(response.build());    try {        watermarkFuture.get();        fail("Expected KeyTokenInvalidException");    } catch (Exception e) {        assertTrue(KeyTokenInvalidException.isKeyTokenInvalidException(e));    }    try {        bagFuture.get();        fail("Expected KeyTokenInvalidException");    } catch (Exception e) {        assertTrue(KeyTokenInvalidException.isKeyTokenInvalidException(e));    }}
public void beam_f11349_0() throws Exception
{    underTest.watermarkFuture(STATE_KEY_1, STATE_FAMILY);    underTest.watermarkFuture(STATE_KEY_1, STATE_FAMILY);    assertEquals(1, underTest.pendingLookups.size());}
public static void beam_f11350_0(Object obj, Class<?> clazz) throws Exception
{    assertNoReference(obj, clazz, new ArrayList<String>(), new HashSet<Object>());}
public void beam_f11358_0() throws Exception
{    DataflowPipelineOptions options = PipelineOptionsFactory.as(DataflowPipelineOptions.class);    com.google.api.services.dataflow.model.Source source = translateIOToCloudSource(CountingSource.upTo(10L), options);    List<WindowedValue<Integer>> elems = readElemsFromSource(options, source);    assertEquals(10L, elems.size());    for (long i = 0; i < 10L; i++) {        assertEquals(valueInGlobalWindow(i), elems.get((int) i));    }    SourceSplitResponse response = performSplit(source, options, 16L, /*desiredBundleSizeBytes for two longs*/    null, /* numBundles limit */    null);    assertEquals("SOURCE_SPLIT_OUTCOME_SPLITTING_HAPPENED", response.getOutcome());    List<DerivedSource> bundles = response.getBundles();    assertEquals(5, bundles.size());    for (int i = 0; i < 5; ++i) {        DerivedSource bundle = bundles.get(i);        assertEquals("SOURCE_DERIVATION_MODE_INDEPENDENT", bundle.getDerivationMode());        com.google.api.services.dataflow.model.Source bundleSource = bundle.getSource();        assertTrue(bundleSource.getDoesNotNeedSplitting());        bundleSource.setCodec(source.getCodec());        List<WindowedValue<Integer>> xs = readElemsFromSource(options, bundleSource);        assertThat("Failed on bundle " + i, xs, contains(valueInGlobalWindow(0L + 2 * i), valueInGlobalWindow(1L + 2 * i)));        assertTrue(bundle.getSource().getMetadata().getEstimatedSizeBytes() > 0);    }}
public void beam_f11359_0() throws Exception
{                DataflowPipelineOptions options = PipelineOptionsFactory.create().as(DataflowPipelineOptions.class);    NativeReader<WindowedValue<Integer>> reader = (NativeReader<WindowedValue<Integer>>) ReaderRegistry.defaultRegistry().create(translateIOToCloudSource(CountingSource.upTo(10), options), options,     null, TestOperationContext.create());    try (NativeReader.NativeReaderIterator<WindowedValue<Integer>> iterator = reader.iterator()) {        assertTrue(iterator.start());        assertEquals(valueInGlobalWindow(0L), iterator.getCurrent());        assertEquals(0.0, readerProgressToCloudProgress(iterator.getProgress()).getFractionConsumed().doubleValue(), 1e-6);        assertTrue(iterator.advance());        assertEquals(valueInGlobalWindow(1L), iterator.getCurrent());        assertEquals(0.1, readerProgressToCloudProgress(iterator.getProgress()).getFractionConsumed().doubleValue(), 1e-6);        assertTrue(iterator.advance());        assertEquals(valueInGlobalWindow(2L), iterator.getCurrent());        assertNull(iterator.requestDynamicSplit(ReaderTestUtils.splitRequestAtFraction(0)));        assertNull(iterator.requestDynamicSplit(ReaderTestUtils.splitRequestAtFraction(0.1f)));        WorkerCustomSources.BoundedSourceSplit<Integer> sourceSplit = (WorkerCustomSources.BoundedSourceSplit<Integer>) iterator.requestDynamicSplit(ReaderTestUtils.splitRequestAtFraction(0.5f));        assertNotNull(sourceSplit);        assertThat(readFromSource(sourceSplit.primary, options), contains(0L, 1L, 2L, 3L, 4L));        assertThat(readFromSource(sourceSplit.residual, options), contains(5L, 6L, 7L, 8L, 9L));        sourceSplit = (WorkerCustomSources.BoundedSourceSplit<Integer>) iterator.requestDynamicSplit(ReaderTestUtils.splitRequestAtFraction(0.8f));        assertNotNull(sourceSplit);        assertThat(readFromSource(sourceSplit.primary, options), contains(0L, 1L, 2L, 3L));        assertThat(readFromSource(sourceSplit.residual, options), contains(4L));        assertTrue(iterator.advance());        assertEquals(valueInGlobalWindow(3L), iterator.getCurrent());        assertFalse(iterator.advance());    }}
public List<? extends BoundedSource<Integer>> beam_f11360_0(long desiredBundleSizeBytes, PipelineOptions options) throws Exception
{    return Arrays.asList(this);}
public void beam_f11369_0() throws Exception
{    DataflowPipelineOptions options = PipelineOptionsFactory.as(DataflowPipelineOptions.class);    com.google.api.services.dataflow.model.Source cloudSource = translateIOToCloudSource(new SourceProducingInvalidSplits("original", null), options);    expectedException.expect(IllegalArgumentException.class);    expectedException.expectMessage(allOf(containsString("Splitting a valid source produced an invalid source"), containsString("original"), containsString("badBundle")));    expectedException.expectCause(hasMessage(containsString("intentionally invalid")));    performSplit(cloudSource, options, null, /*desiredBundleSizeBytes*/    null, /* numBundles limit */    null);}
public BoundedSource<Integer> beam_f11370_0()
{    return source;}
public boolean beam_f11371_0() throws IOException
{    throw new IOException("Intentional error");}
public void beam_f11380_0() throws Exception
{    DataflowPipelineOptions options = PipelineOptionsFactory.as(DataflowPipelineOptions.class);    com.google.api.services.dataflow.model.Source source = translateIOToCloudSource(new SourceProducingFailingReader(), options);        try {        readElemsFromSource(options, source);        fail("Expected to fail");    } catch (Exception e) {        assertThat(getStackTraceAsString(e), allOf(containsString("Intentional error"), containsString("Some description")));    }}
 static com.google.api.services.dataflow.model.Source beam_f11381_0(BoundedSource<?> io, DataflowPipelineOptions options) throws Exception
{    options.setRunner(DataflowRunner.class);    options.setProject("test-project");    options.setTempLocation("gs://test-tmp");    options.setPathValidatorClass(NoopPathValidator.class);    options.setGcpCredential(new TestCredential());    DataflowPipelineTranslator translator = DataflowPipelineTranslator.fromOptions(options);    Pipeline p = Pipeline.create(options);    p.begin().apply(Read.from(io));    DataflowRunner runner = DataflowRunner.fromOptions(options);    Job workflow = translator.translate(p, runner, new ArrayList<DataflowPackage>()).getJob();    Step step = workflow.getSteps().get(0);    return stepToCloudSource(step);}
private static com.google.api.services.dataflow.model.Source beam_f11382_0(Step step) throws Exception
{    com.google.api.services.dataflow.model.Source res = dictionaryToCloudSource(getDictionary(step.getProperties(), PropertyNames.SOURCE_STEP_INPUT));                @SuppressWarnings("unchecked")    List<Map<String, Object>> outputInfo = (List<Map<String, Object>>) step.getProperties().get(PropertyNames.OUTPUT_INFO);    CloudObject encoding = CloudObject.fromSpec(getObject(outputInfo.get(0), PropertyNames.ENCODING));    res.setCodec(encoding);    return res;}
public static List<T> beam_f11390_0(PipelineOptions options, Source source)
{    try {        @SuppressWarnings("unchecked")        NativeReader<T> reader = (NativeReader<T>) ReaderRegistry.defaultRegistry().create(source, options, null, TestOperationContext.create());        return ReaderUtils.readAllFromReader(reader);    } catch (Exception e) {        throw new RuntimeException("Failed to read from source: " + source.toString(), e);    }}
public BoundedSource<Void> beam_f11391_0()
{    throw new UnsupportedOperationException();}
public boolean beam_f11392_0() throws IOException
{    throw new UnsupportedOperationException();}
public void beam_f11400_0()
{    ApproximateReportedProgress progress = getReaderProgress(new TestBoundedReader(0.75, 1, 2));    assertEquals(0.75, progress.getFractionConsumed(), 1e-6);    assertEquals(1.0, progress.getConsumedParallelism().getValue(), 1e-6);    assertEquals(2.0, progress.getRemainingParallelism().getValue(), 1e-6);    progress = getReaderProgress(new TestBoundedReader(null, -1, 4));    assertNull(progress.getFractionConsumed());    assertNull(progress.getConsumedParallelism());    assertEquals(4.0, progress.getRemainingParallelism().getValue(), 1e-6);    progress = getReaderProgress(new TestBoundedReader(null, -1, -2));    assertNull(progress.getFractionConsumed());    assertNull(progress.getConsumedParallelism());    assertNull(progress.getRemainingParallelism());}
public void beam_f11401_0()
{        RuntimeException fractionError = new UnsupportedOperationException("fraction");    ApproximateReportedProgress progress = getReaderProgress(new TestBoundedReader(fractionError, 1, 2));    assertNull(progress.getFractionConsumed());    assertEquals(1.0, progress.getConsumedParallelism().getValue(), 1e-6);    assertEquals(2.0, progress.getRemainingParallelism().getValue(), 1e-6);    logged.verifyWarn("fraction");        RuntimeException consumedError = new UnsupportedOperationException("consumed parallelism");    progress = getReaderProgress(new TestBoundedReader(0.75, consumedError, 3));    assertEquals(0.75, progress.getFractionConsumed(), 1e-6);    assertNull(progress.getConsumedParallelism());    assertEquals(3.0, progress.getRemainingParallelism().getValue(), 1e-6);    logged.verifyWarn("consumed parallelism");        RuntimeException remainingError = new UnsupportedOperationException("remaining parallelism");    progress = getReaderProgress(new TestBoundedReader(0.5, 5, remainingError));    assertEquals(0.5, progress.getFractionConsumed(), 1e-6);    assertEquals(5.0, progress.getConsumedParallelism().getValue(), 1e-6);    assertNull(progress.getRemainingParallelism());    logged.verifyWarn("remaining parallelism");}
public void beam_f11402_0() throws Exception
{    System.getProperties().putAll(ImmutableMap.<String, String>builder().put("worker_id", "test_worker_id").put("job_id", "test_job_id").put("sdk_pipeline_options", "{\"options\":{\"numWorkers\":999}}").build());        @SuppressWarnings("deprecation")    DataflowWorkerHarnessOptions options = WorkerPipelineOptionsFactory.createFromSystemProperties();    assertEquals("test_worker_id", options.getWorkerId());    assertEquals("test_job_id", options.getJobId());    assertEquals(999, options.getNumWorkers());}
public void beam_f11410_0() throws Exception
{    RuntimeException error = new RuntimeException();    error.fillInStackTrace();    statusClient.reportError(error);    thrown.expect(IllegalStateException.class);    thrown.expectMessage("reportUpdate");    statusClient.reportUpdate(null, LEASE_DURATION);}
public void beam_f11411_0() throws IOException
{    thrown.expect(IllegalStateException.class);    thrown.expectMessage("setWorker");    thrown.expectMessage("reportSuccess");    statusClient.reportSuccess();}
public void beam_f11412_0() throws IOException
{    when(worker.extractMetricUpdates()).thenReturn(Collections.emptyList());    statusClient.setWorker(worker, executionContext);    statusClient.reportSuccess();    verify(workUnitClient).reportWorkItemStatus(statusCaptor.capture());    WorkItemStatus workStatus = statusCaptor.getValue();    assertThat(workStatus.getWorkItemId(), equalTo(Long.toString(WORK_ID)));    assertThat(workStatus.getCompleted(), equalTo(true));    assertThat(workStatus.getReportIndex(), equalTo(INITIAL_REPORT_INDEX));    assertThat(workStatus.getErrors(), nullValue());}
public void beam_f11420_0() throws Exception
{        WorkItemStatus status = new WorkItemStatus();    statusClient.setWorker(worker, executionContext);    when(worker.extractMetricUpdates()).thenReturn(Collections.emptyList());    statusClient.populateCounterUpdates(status);    assertThat(status.getCounterUpdates(), hasSize(0));}
public void beam_f11421_0() throws Exception
{    final CounterUpdate counter = new CounterUpdate().setNameAndKind(new NameAndKind().setName("some-counter").setKind("SUM")).setCumulative(true).setInteger(DataflowCounterUpdateExtractor.longToSplitInt(42));    CounterSet counterSet = new CounterSet();    counterSet.intSum(CounterName.named("some-counter")).addValue(42);    WorkItemStatus status = new WorkItemStatus();    when(worker.getOutputCounters()).thenReturn(counterSet);    when(worker.extractMetricUpdates()).thenReturn(Collections.emptyList());    when(worker.extractMetricUpdates()).thenReturn(Collections.emptyList());    statusClient.setWorker(worker, executionContext);    statusClient.populateCounterUpdates(status);    assertThat(status.getCounterUpdates(), containsInAnyOrder(counter));}
public void beam_f11422_0() throws Exception
{    final CounterUpdate expectedCounter = new CounterUpdate().setNameAndKind(new NameAndKind().setName("some-counter").setKind("SUM")).setCumulative(true).setInteger(DataflowCounterUpdateExtractor.longToSplitInt(42));    CounterSet counterSet = new CounterSet();    counterSet.intSum(CounterName.named("some-counter")).addValue(42);    final CounterUpdate expectedMetric = new CounterUpdate().setStructuredNameAndMetadata(new CounterStructuredNameAndMetadata().setName(new CounterStructuredName().setOrigin("USER").setOriginNamespace("namespace").setName("some-counter").setOriginalStepName("step")).setMetadata(new CounterMetadata().setKind("SUM"))).setCumulative(true).setInteger(DataflowCounterUpdateExtractor.longToSplitInt(42));    MetricsContainerImpl metricsContainer = new MetricsContainerImpl("step");    BatchModeExecutionContext context = mock(BatchModeExecutionContext.class);    when(context.extractMetricUpdates(anyBoolean())).thenReturn(ImmutableList.of(expectedMetric));    when(context.extractMsecCounters(anyBoolean())).thenReturn(Collections.emptyList());    CounterCell counter = metricsContainer.getCounter(MetricName.named("namespace", "some-counter"));    counter.inc(1);    counter.inc(41);    counter.inc(1);    counter.inc(-1);    WorkItemStatus status = new WorkItemStatus();    when(worker.getOutputCounters()).thenReturn(counterSet);    when(worker.extractMetricUpdates()).thenReturn(Collections.emptyList());    statusClient.setWorker(worker, context);    statusClient.populateCounterUpdates(status);    assertThat(status.getCounterUpdates(), containsInAnyOrder(expectedCounter, expectedMetric));}
public List<? extends BoundedSource<Integer>> beam_f11430_0(long desiredBundleSizeBytes, PipelineOptions options) throws Exception
{    throw new UnsupportedOperationException();}
public long beam_f11431_0(PipelineOptions options) throws Exception
{    throw new UnsupportedOperationException();}
public BoundedReader<Integer> beam_f11432_0(PipelineOptions options) throws IOException
{    throw new UnsupportedOperationException();}
public ProxyManifest beam_f11441_0(String retrievalToken) throws Exception
{    return loadManifest(retrievalToken);}
 static ProxyManifest beam_f11442_1(String retrievalToken) throws IOException
{            ResourceId manifestResourceId = getManifestLocationFromToken(retrievalToken);    return loadManifest(manifestResourceId);}
 static ProxyManifest beam_f11443_1(ResourceId manifestResourceId) throws IOException
{    ProxyManifest.Builder manifestBuilder = ProxyManifest.newBuilder();    try (InputStream stream = Channels.newInputStream(FileSystems.open(manifestResourceId))) {        String contents = new String(ByteStreams.toByteArray(stream), StandardCharsets.UTF_8);        JsonFormat.parser().merge(contents, manifestBuilder);    }    ProxyManifest proxyManifest = manifestBuilder.build();    checkArgument(proxyManifest.hasManifest(), String.format("Invalid ProxyManifest at %s: doesn't have a Manifest", manifestResourceId));    checkArgument(proxyManifest.getLocationCount() == proxyManifest.getManifest().getArtifactCount(), String.format("Invalid ProxyManifestat %s: %d locations but %d artifacts", manifestResourceId, proxyManifest.getLocationCount(), proxyManifest.getManifest().getArtifactCount()));        return proxyManifest;}
private ResourceId beam_f11452_0(StagingSessionToken stagingSessionToken)
{    return getJobDirResourceId(stagingSessionToken).resolve(MANIFEST, StandardResolveOptions.RESOLVE_FILE);}
private ResourceId beam_f11453_0(StagingSessionToken stagingSessionToken)
{    return getJobDirResourceId(stagingSessionToken).resolve(ARTIFACTS, StandardResolveOptions.RESOLVE_DIRECTORY);}
public void beam_f11454_1(PutArtifactRequest putArtifactRequest)
{        if (metadata == null) {        checkNotNull(putArtifactRequest);        checkNotNull(putArtifactRequest.getMetadata());        metadata = putArtifactRequest.getMetadata();                        try {            ResourceId artifactsDirId = getArtifactDirResourceId(StagingSessionToken.decode(putArtifactRequest.getMetadata().getStagingSessionToken()));            artifactId = artifactsDirId.resolve(encodedFileName(metadata.getMetadata()), StandardResolveOptions.RESOLVE_FILE);                        artifactWritableByteChannel = FileSystems.create(artifactId, MimeTypes.BINARY);            hasher = Hashing.sha256().newHasher();        } catch (Exception e) {            String message = String.format("Failed to begin staging artifact %s", metadata.getMetadata().getName());                        outboundObserver.onError(new StatusRuntimeException(Status.DATA_LOSS.withDescription(message).withCause(e)));        }    } else {        try {            ByteString data = putArtifactRequest.getData().getData();            artifactWritableByteChannel.write(data.asReadOnlyByteBuffer());            hasher.putBytes(data.toByteArray());        } catch (IOException e) {            String message = String.format("Failed to write chunk of artifact %s to %s", metadata.getMetadata().getName(), artifactId);                        outboundObserver.onError(new StatusRuntimeException(Status.DATA_LOSS.withDescription(message).withCause(e)));        }    }}
public static StagingSessionToken beam_f11462_0(String stagingSessionToken) throws Exception
{    try {        return MAPPER.readValue(stagingSessionToken, StagingSessionToken.class);    } catch (JsonProcessingException e) {        String message = String.format("Unable to deserialize staging token %s. Expected format: %s. Error: %s", stagingSessionToken, "{\"sessionId\": \"sessionId\", \"basePath\": \"basePath\"}", e.getMessage());        throw new StatusRuntimeException(Status.INVALID_ARGUMENT.withDescription(message));    }}
public String beam_f11463_0()
{    return "StagingSessionToken{" + "sessionId='" + sessionId + "', " + "basePath='" + basePath + "'" + "}";}
 static BundleProgressHandler beam_f11464_0()
{    return new BundleProgressHandler() {        @Override        public void onProgress(ProcessBundleProgressResponse progress) {        }        @Override        public void onCompleted(ProcessBundleResponse response) {        }    };}
private static int beam_f11474_0(JobInfo jobInfo)
{    PipelineOptions pipelineOptions = PipelineOptionsTranslation.fromProto(jobInfo.pipelineOptions());    return pipelineOptions.as(PortablePipelineOptions.class).getEnvironmentExpirationMillis();}
private LoadingCache<Environment, WrappedSdkHarnessClient> beam_f11475_1(ThrowingFunction<ServerFactory, ServerInfo> serverInfoCreator)
{    CacheBuilder builder = CacheBuilder.newBuilder().removalListener((RemovalNotification<Environment, WrappedSdkHarnessClient> notification) -> {        int refCount = notification.getValue().unref();            });    if (environmentExpirationMillis > 0) {        builder = builder.expireAfterWrite(environmentExpirationMillis, TimeUnit.MILLISECONDS);    }    return builder.build(new CacheLoader<Environment, WrappedSdkHarnessClient>() {        @Override        public WrappedSdkHarnessClient load(Environment environment) throws Exception {            EnvironmentFactory.Provider environmentFactoryProvider = environmentFactoryProviderMap.get(environment.getUrn());            ServerFactory serverFactory = environmentFactoryProvider.getServerFactory();            ServerInfo serverInfo = serverInfoCreator.apply(serverFactory);            EnvironmentFactory environmentFactory = environmentFactoryProvider.createEnvironmentFactory(serverInfo.getControlServer(), serverInfo.getLoggingServer(), serverInfo.getRetrievalServer(), serverInfo.getProvisioningServer(), clientPool, stageIdGenerator);            return WrappedSdkHarnessClient.wrapping(environmentFactory.createEnvironment(environment), serverInfo);        }    });}
public WrappedSdkHarnessClient beam_f11476_0(Environment environment) throws Exception
{    EnvironmentFactory.Provider environmentFactoryProvider = environmentFactoryProviderMap.get(environment.getUrn());    ServerFactory serverFactory = environmentFactoryProvider.getServerFactory();    ServerInfo serverInfo = serverInfoCreator.apply(serverFactory);    EnvironmentFactory environmentFactory = environmentFactoryProvider.createEnvironmentFactory(serverInfo.getControlServer(), serverInfo.getLoggingServer(), serverInfo.getRetrievalServer(), serverInfo.getProvisioningServer(), clientPool, stageIdGenerator);    return WrappedSdkHarnessClient.wrapping(environmentFactory.createEnvironment(environment), serverInfo);}
public ExecutableProcessBundleDescriptor beam_f11484_0()
{    return processBundleDescriptor;}
public void beam_f11485_0() throws Exception
{        wrappedClient = null;}
 static WrappedSdkHarnessClient beam_f11486_0(RemoteEnvironment environment, ServerInfo serverInfo)
{    SdkHarnessClient client = SdkHarnessClient.usingFnApiClient(environment.getInstructionRequestHandler(), serverInfo.getDataServer().getService());    return new WrappedSdkHarnessClient(environment, client, serverInfo);}
public CompletionStage<BeamFnApi.InstructionResponse> beam_f11494_1(BeamFnApi.InstructionRequest request)
{        CompletableFuture<BeamFnApi.InstructionResponse> resultFuture = new CompletableFuture<>();    outstandingRequests.put(request.getInstructionId(), resultFuture);    requestReceiver.onNext(request);    return resultFuture;}
public StreamObserver<BeamFnApi.InstructionResponse> beam_f11495_0()
{    return responseObserver;}
public void beam_f11496_0()
{    closeAndTerminateOutstandingRequests(new IllegalStateException("Runner closed connection"));}
public StreamObserver<BeamFnApi.InstructionResponse> beam_f11504_1(StreamObserver<BeamFnApi.InstructionRequest> requestObserver)
{    final String workerId = headerAccessor.getSdkWorkerId();    if (Strings.isNullOrEmpty(workerId)) {                    }        FnApiControlClient newClient = FnApiControlClient.forRequestObserver(workerId, requestObserver);    try {                synchronized (lock) {            checkState(!closed, "%s already closed", FnApiControlClientPoolService.class.getSimpleName());                                    vendedClients.add(newClient);        }                        clientSink.put(headerAccessor.getSdkWorkerId(), newClient);    } catch (InterruptedException e) {        Thread.currentThread().interrupt();        throw new RuntimeException(e);    } catch (Exception e) {        throw new RuntimeException(e);    }    return newClient.asResponseObserver();}
public void beam_f11505_0()
{    synchronized (lock) {        if (!closed) {            closed = true;            for (FnApiControlClient vended : vendedClients) {                vended.close();            }        }    }}
public static MapControlClientPool beam_f11506_0()
{    return new MapControlClientPool();}
private static ExecutableProcessBundleDescriptor beam_f11514_0(String id, ExecutableStage stage, ApiServiceDescriptor dataEndpoint, @Nullable ApiServiceDescriptor stateEndpoint) throws IOException
{            Map<String, PTransform> stageTransforms = stage.getTransforms().stream().collect(Collectors.toMap(PTransformNode::getId, PTransformNode::getTransform));    Components.Builder components = stage.getComponents().toBuilder().clearTransforms().putAllTransforms(stageTransforms);    ImmutableMap.Builder<String, RemoteInputDestination> inputDestinationsBuilder = ImmutableMap.builder();    ImmutableMap.Builder<String, Coder> remoteOutputCodersBuilder = ImmutableMap.builder();        inputDestinationsBuilder.put(stage.getInputPCollection().getId(), addStageInput(dataEndpoint, stage.getInputPCollection(), components));    remoteOutputCodersBuilder.putAll(addStageOutputs(dataEndpoint, stage.getOutputPCollections(), components));    Map<String, Map<String, SideInputSpec>> sideInputSpecs = addSideInputs(stage, components);    Map<String, Map<String, BagUserStateSpec>> bagUserStateSpecs = forBagUserStates(stage, components.build());    Map<String, Map<String, TimerSpec>> timerSpecs = forTimerSpecs(dataEndpoint, stage, components, inputDestinationsBuilder, remoteOutputCodersBuilder);        ProcessBundleDescriptor.Builder bundleDescriptorBuilder = ProcessBundleDescriptor.newBuilder().setId(id);    if (stateEndpoint != null) {        bundleDescriptorBuilder.setStateApiServiceDescriptor(stateEndpoint);    }    bundleDescriptorBuilder.putAllCoders(components.getCodersMap()).putAllEnvironments(components.getEnvironmentsMap()).putAllPcollections(components.getPcollectionsMap()).putAllWindowingStrategies(components.getWindowingStrategiesMap()).putAllTransforms(components.getTransformsMap());    return ExecutableProcessBundleDescriptor.of(bundleDescriptorBuilder.build(), inputDestinationsBuilder.build(), remoteOutputCodersBuilder.build(), sideInputSpecs, bagUserStateSpecs, timerSpecs);}
private static Map<String, Coder<WindowedValue<?>>> beam_f11515_0(ApiServiceDescriptor dataEndpoint, Collection<PCollectionNode> outputPCollections, Components.Builder components) throws IOException
{    Map<String, Coder<WindowedValue<?>>> remoteOutputCoders = new LinkedHashMap<>();    for (PCollectionNode outputPCollection : outputPCollections) {        OutputEncoding outputEncoding = addStageOutput(dataEndpoint, components, outputPCollection);        remoteOutputCoders.put(outputEncoding.getPTransformId(), outputEncoding.getCoder());    }    return remoteOutputCoders;}
private static RemoteInputDestination<WindowedValue<?>> beam_f11516_0(ApiServiceDescriptor dataEndpoint, PCollectionNode inputPCollection, Components.Builder components) throws IOException
{    String inputWireCoderId = WireCoders.addSdkWireCoder(inputPCollection, components);    @SuppressWarnings("unchecked")    Coder<WindowedValue<?>> wireCoder = (Coder) WireCoders.instantiateRunnerWireCoder(inputPCollection, components.build());    RemoteGrpcPort inputPort = RemoteGrpcPort.newBuilder().setApiServiceDescriptor(dataEndpoint).setCoderId(inputWireCoderId).build();    String inputId = uniqueId(String.format("fn/read/%s", inputPCollection.getId()), components::containsTransforms);    PTransform inputTransform = RemoteGrpcPortRead.readFromPort(inputPort, inputPCollection.getId()).toPTransform();    components.putTransforms(inputId, inputTransform);    return RemoteInputDestination.of(wireCoder, inputId);}
public static SideInputSpec beam_f11524_0(String transformId, String sideInputId, RunnerApi.FunctionSpec accessPattern, Coder<T> elementCoder, Coder<W> windowCoder)
{    return new AutoValue_ProcessBundleDescriptors_SideInputSpec(transformId, sideInputId, accessPattern, elementCoder, windowCoder);}
 static BagUserStateSpec<K, V, W> beam_f11525_0(String transformId, String userStateId, Coder<K> keyCoder, Coder<V> valueCoder, Coder<W> windowCoder)
{    return new AutoValue_ProcessBundleDescriptors_BagUserStateSpec(transformId, userStateId, keyCoder, valueCoder, windowCoder);}
 static TimerSpec<K, V, W> beam_f11526_0(String transformId, String timerId, String inputCollectionId, String outputCollectionId, String outputTransformId, org.apache.beam.sdk.state.TimerSpec timerSpec)
{    return new AutoValue_ProcessBundleDescriptors_TimerSpec(transformId, timerId, inputCollectionId, outputCollectionId, outputTransformId, timerSpec);}
public StageBundleFactory beam_f11534_0(ExecutableStage executableStage)
{    return context.getStageBundleFactory(executableStage);}
public void beam_f11535_0()
{        scheduleRelease(jobInfo);}
private void beam_f11536_0() throws Exception
{    context.close();}
private InboundDataClient beam_f11544_0(String bundleId, String ptransformId, RemoteOutputReceiver<OutputT> receiver)
{    return fnApiDataService.receive(LogicalEndpoint.of(bundleId, ptransformId), receiver.getCoder(), receiver.getReceiver());}
public String beam_f11545_0()
{    return bundleId;}
public Map<String, FnDataReceiver> beam_f11546_0()
{    return (Map) inputReceivers;}
public static JobBundleFactory beam_f11557_0(EnvironmentFactory environmentFactory, GrpcFnServer<GrpcDataService> data, GrpcFnServer<GrpcStateService> state, IdGenerator idGenerator)
{    return new SingleEnvironmentInstanceJobBundleFactory(environmentFactory, data, state, idGenerator);}
public StageBundleFactory beam_f11558_0(ExecutableStage executableStage)
{    return stageBundleFactories.computeIfAbsent(executableStage, this::createBundleFactory);}
private StageBundleFactory beam_f11559_0(ExecutableStage stage)
{    RemoteEnvironment remoteEnv = environments.computeIfAbsent(stage.getEnvironment(), env -> {        try {            return environmentFactory.createEnvironment(env);        } catch (Exception e) {            throw new RuntimeException(e);        }    });    SdkHarnessClient sdkHarnessClient = SdkHarnessClient.usingFnApiClient(remoteEnv.getInstructionRequestHandler(), dataService.getService()).withIdGenerator(idGenerator);    ExecutableProcessBundleDescriptor descriptor;    try {        descriptor = ProcessBundleDescriptors.fromExecutableStage(idGenerator.getId(), stage, dataService.getApiServiceDescriptor(), stateService.getApiServiceDescriptor());    } catch (IOException e) {        throw new RuntimeException(e);    }    SdkHarnessClient.BundleProcessor bundleProcessor = sdkHarnessClient.getProcessor(descriptor.getProcessBundleDescriptor(), descriptor.getRemoteInputDestinations(), stateService.getService());    return new BundleProcessorStageBundleFactory(descriptor, bundleProcessor);}
public InboundDataClient beam_f11568_1(final LogicalEndpoint inputLocation, Coder<T> coder, FnDataReceiver<T> listener)
{        final BeamFnDataInboundObserver<T> observer = BeamFnDataInboundObserver.forConsumer(coder, listener);    if (connectedClient.isDone()) {        try {            connectedClient.get().registerConsumer(inputLocation, observer);        } catch (InterruptedException e) {            Thread.currentThread().interrupt();            throw new RuntimeException(e);        } catch (ExecutionException e) {            throw new RuntimeException(e.getCause());        }    } else {        executor.submit(() -> {            try {                connectedClient.get().registerConsumer(inputLocation, observer);            } catch (InterruptedException e) {                Thread.currentThread().interrupt();                throw new RuntimeException(e);            } catch (ExecutionException e) {                throw new RuntimeException(e.getCause());            }        });    }    return observer;}
public CloseableFnDataReceiver<T> beam_f11569_1(LogicalEndpoint outputLocation, Coder<T> coder)
{        try {        return BeamFnDataBufferingOutboundObserver.forLocation(outputLocation, coder, connectedClient.get(3, TimeUnit.MINUTES).getOutboundObserver());    } catch (InterruptedException e) {        Thread.currentThread().interrupt();        throw new RuntimeException(e);    } catch (TimeoutException e) {        throw new RuntimeException("No client connected within timeout", e);    } catch (ExecutionException e) {        throw new RuntimeException(e);    }}
public static RemoteInputDestination<T> beam_f11570_0(Coder<T> coder, String ptransformId)
{    return new AutoValue_RemoteInputDestination<>(coder, ptransformId);}
private String beam_f11578_0(List<String> invocation) throws IOException, TimeoutException, InterruptedException
{    return runShortCommand(invocation, false, "");}
private String beam_f11579_0(List<String> invocation, boolean redirectErrorStream, CharSequence delimiter) throws IOException, TimeoutException, InterruptedException
{    ProcessBuilder pb = new ProcessBuilder(invocation);    pb.redirectErrorStream(redirectErrorStream);    Process process = pb.start();        CompletableFuture<String> resultString = CompletableFuture.supplyAsync(() -> {                BufferedReader reader = new BufferedReader(new InputStreamReader(process.getInputStream(), StandardCharsets.UTF_8));        return reader.lines().collect(Collectors.joining(delimiter));    });    CompletableFuture<String> errorFuture;    String errorStringName;    if (redirectErrorStream) {                errorStringName = "stdout and stderr";        errorFuture = resultString;    } else {                errorStringName = "stderr";        errorFuture = CompletableFuture.supplyAsync(() -> {            BufferedReader reader = new BufferedReader(new InputStreamReader(process.getErrorStream(), StandardCharsets.UTF_8));            return reader.lines().collect(Collectors.joining(delimiter));        });    }        boolean processDone = process.waitFor(commandTimeout.toMillis(), TimeUnit.MILLISECONDS);    if (!processDone) {        process.destroy();        throw new TimeoutException(String.format("Timed out while waiting for command '%s'", invocation.stream().collect(Collectors.joining(" "))));    }    int exitCode = process.exitValue();    if (exitCode != 0) {        String errorString;        try {            errorString = errorFuture.get(commandTimeout.toMillis(), TimeUnit.MILLISECONDS);        } catch (Exception stderrEx) {            errorString = String.format("Error capturing %s: %s", errorStringName, stderrEx.getMessage());        }        throw new IOException(String.format("Received exit code %d for command '%s'. %s: %s", exitCode, invocation.stream().collect(Collectors.joining(" ")), errorStringName, errorString));    }    try {                return resultString.get(commandTimeout.toMillis(), TimeUnit.MILLISECONDS);    } catch (ExecutionException e) {        Throwable cause = e.getCause();                throw new IOException(cause);    }}
 static DockerContainerEnvironment beam_f11580_0(DockerCommand docker, Environment environment, String containerId, InstructionRequestHandler instructionHandler, boolean retainDockerContainer)
{    return new DockerContainerEnvironment(docker, environment, containerId, instructionHandler, retainDockerContainer);}
public EnvironmentFactory beam_f11588_0(GrpcFnServer<FnApiControlClientPoolService> controlServiceServer, GrpcFnServer<GrpcLoggingService> loggingServiceServer, GrpcFnServer<ArtifactRetrievalService> retrievalServiceServer, GrpcFnServer<StaticGrpcProvisionService> provisioningServiceServer, ControlClientPool clientPool, IdGenerator idGenerator)
{    return DockerEnvironmentFactory.forServicesWithDocker(DockerCommand.getDefault(), controlServiceServer, loggingServiceServer, retrievalServiceServer, provisioningServiceServer, clientPool.getSource(), idGenerator, pipelineOptions);}
public ServerFactory beam_f11589_1()
{    switch(getPlatform()) {        case LINUX:            return ServerFactory.createDefault();        case MAC:            return DockerOnMac.getServerFactory();        default:                        return ServerFactory.createDefault();    }}
private static Platform beam_f11590_0()
{    String osName = System.getProperty("os.name").toLowerCase();        if (osName.startsWith("mac") || DockerOnMac.RUNNING_INSIDE_DOCKER_ON_MAC) {        return Platform.MAC;    } else if (osName.startsWith("linux")) {        return Platform.LINUX;    }    return Platform.OTHER;}
public Environment beam_f11598_0()
{    return environment;}
public InstructionRequestHandler beam_f11599_0()
{    return finalInstructionHandler;}
public void beam_f11600_1() throws Exception
{    finalInstructionHandler.close();    BeamFnApi.StopWorkerRequest stopWorkerRequest = BeamFnApi.StopWorkerRequest.newBuilder().setWorkerId(workerId).build();        BeamFnApi.StopWorkerResponse stopWorkerResponse = BeamFnExternalWorkerPoolGrpc.newBlockingStub(ManagedChannelFactory.createDefault().forDescriptor(externalPayload.getEndpoint())).stopWorker(stopWorkerRequest);    if (!stopWorkerResponse.getError().isEmpty()) {        throw new RuntimeException(stopWorkerResponse.getError());    }}
public RemoteEnvironment beam_f11608_1(Environment environment) throws Exception
{    Preconditions.checkState(environment.getUrn().equals(BeamUrns.getUrn(RunnerApi.StandardEnvironments.Environments.PROCESS)), "The passed environment does not contain a ProcessPayload.");    final RunnerApi.ProcessPayload processPayload = RunnerApi.ProcessPayload.parseFrom(environment.getPayload());    final String workerId = idGenerator.getId();    String executable = processPayload.getCommand();    String loggingEndpoint = loggingServiceServer.getApiServiceDescriptor().getUrl();    String artifactEndpoint = retrievalServiceServer.getApiServiceDescriptor().getUrl();    String provisionEndpoint = provisioningServiceServer.getApiServiceDescriptor().getUrl();    String controlEndpoint = controlServiceServer.getApiServiceDescriptor().getUrl();    String semiPersistDir = pipelineOptions.as(RemoteEnvironmentOptions.class).getSemiPersistDir();    ImmutableList.Builder<String> argsBuilder = ImmutableList.<String>builder().add(String.format("--id=%s", workerId)).add(String.format("--logging_endpoint=%s", loggingEndpoint)).add(String.format("--artifact_endpoint=%s", artifactEndpoint)).add(String.format("--provision_endpoint=%s", provisionEndpoint)).add(String.format("--control_endpoint=%s", controlEndpoint));    if (semiPersistDir != null) {        argsBuilder.add(String.format("--semi_persist_dir=%s", semiPersistDir));    }            InstructionRequestHandler instructionHandler = null;    try {        ProcessManager.RunningProcess process = processManager.startProcess(workerId, executable, argsBuilder.build(), processPayload.getEnvMap());                while (instructionHandler == null) {            try {                                process.isAliveOrThrow();                instructionHandler = clientSource.take(workerId, Duration.ofMinutes(2));            } catch (TimeoutException timeoutEx) {                            } catch (InterruptedException interruptEx) {                Thread.currentThread().interrupt();                throw new RuntimeException(interruptEx);            }        }    } catch (Exception e) {        try {            processManager.stopProcess(workerId);        } catch (Exception processKillException) {            e.addSuppressed(processKillException);        }        throw e;    }    return ProcessEnvironment.create(processManager, environment, workerId, instructionHandler);}
public EnvironmentFactory beam_f11609_0(GrpcFnServer<FnApiControlClientPoolService> controlServiceServer, GrpcFnServer<GrpcLoggingService> loggingServiceServer, GrpcFnServer<ArtifactRetrievalService> retrievalServiceServer, GrpcFnServer<StaticGrpcProvisionService> provisioningServiceServer, ControlClientPool clientPool, IdGenerator idGenerator)
{    return create(ProcessManager.create(), controlServiceServer, loggingServiceServer, retrievalServiceServer, provisioningServiceServer, clientPool.getSource(), idGenerator, pipelineOptions);}
public static ProcessManager beam_f11610_0()
{    synchronized (ALL_PROCESS_MANAGERS) {        ProcessManager processManager = new ProcessManager();        ALL_PROCESS_MANAGERS.add(processManager);        return processManager;    }}
private static ShutdownHook beam_f11618_0()
{    return new ShutdownHook();}
public void beam_f11619_0()
{    synchronized (ALL_PROCESS_MANAGERS) {        ALL_PROCESS_MANAGERS.forEach(ProcessManager::stopAllProcesses);        for (ProcessManager pm : ALL_PROCESS_MANAGERS) {            if (pm.processes.values().stream().anyMatch(Process::isAlive)) {                try {                                        Thread.sleep(200);                    break;                } catch (InterruptedException e) {                    Thread.currentThread().interrupt();                    throw new RuntimeException(e);                }            }        }        ALL_PROCESS_MANAGERS.forEach(ProcessManager::killAllProcesses);    }}
private void beam_f11620_0()
{    processes.forEach((id, process) -> process.destroy());}
public static StaticRemoteEnvironmentFactory beam_f11628_0(InstructionRequestHandler instructionRequestHandler)
{    return new StaticRemoteEnvironmentFactory(instructionRequestHandler);}
public RemoteEnvironment beam_f11629_0(Environment environment)
{    return StaticRemoteEnvironment.create(environment, this.instructionRequestHandler);}
public EnvironmentFactory beam_f11630_0(GrpcFnServer<FnApiControlClientPoolService> controlServiceServer, GrpcFnServer<GrpcLoggingService> loggingServiceServer, GrpcFnServer<ArtifactRetrievalService> retrievalServiceServer, GrpcFnServer<StaticGrpcProvisionService> provisioningServiceServer, ControlClientPool clientPool, IdGenerator idGenerator)
{    return StaticRemoteEnvironmentFactory.forService(this.instructionRequestHandler);}
public ApiServiceDescriptor beam_f11639_0()
{    return apiServiceDescriptor;}
public ServiceT beam_f11640_0()
{    return service;}
public Server beam_f11641_0()
{    return server;}
public void beam_f11649_1(GetJobsRequest request, StreamObserver<GetJobsResponse> responseObserver)
{    LOG.trace("{} {}", GetJobsRequest.class.getSimpleName(), request);    try {        List<JobInfo> result = new ArrayList<>();        for (JobInvocation invocation : invocations.values()) {            result.add(invocation.toProto());        }        GetJobsResponse response = GetJobsResponse.newBuilder().addAllJobInfo(result).build();        responseObserver.onNext(response);        responseObserver.onCompleted();    } catch (Exception e) {                responseObserver.onError(Status.INTERNAL.withCause(e).asException());    }}
public void beam_f11650_1(GetJobStateRequest request, StreamObserver<GetJobStateResponse> responseObserver)
{    LOG.trace("{} {}", GetJobStateRequest.class.getSimpleName(), request);    String invocationId = request.getJobId();    try {        JobInvocation invocation = getInvocation(invocationId);        JobState.Enum state = invocation.getState();        GetJobStateResponse response = GetJobStateResponse.newBuilder().setState(state).build();        responseObserver.onNext(response);        responseObserver.onCompleted();    } catch (StatusRuntimeException | StatusException e) {        responseObserver.onError(e);    } catch (Exception e) {        String errMessage = String.format("Encountered Unexpected Exception for Invocation %s", invocationId);                responseObserver.onError(Status.INTERNAL.withCause(e).asException());    }}
public void beam_f11651_1(GetJobPipelineRequest request, StreamObserver<GetJobPipelineResponse> responseObserver)
{    LOG.trace("{} {}", GetJobPipelineRequest.class.getSimpleName(), request);    String invocationId = request.getJobId();    try {        JobInvocation invocation = getInvocation(invocationId);        RunnerApi.Pipeline pipeline = invocation.getPipeline();        GetJobPipelineResponse response = GetJobPipelineResponse.newBuilder().setPipeline(pipeline).build();        responseObserver.onNext(response);        responseObserver.onCompleted();    } catch (StatusRuntimeException | StatusException e) {        responseObserver.onError(e);    } catch (Exception e) {        String errMessage = String.format("Encountered Unexpected Exception for Invocation %s", invocationId);                responseObserver.onError(Status.INTERNAL.withCause(e).asException());    }}
public synchronized void beam_f11660_1()
{        if (getState() != JobState.Enum.STOPPED) {        throw new IllegalStateException(String.format("Job %s already running.", getId()));    }    setState(JobState.Enum.STARTING);    invocationFuture = executorService.submit(this::runPipeline);        setState(JobState.Enum.RUNNING);    Futures.addCallback(invocationFuture, new FutureCallback<PortablePipelineResult>() {        @Override        public void onSuccess(PortablePipelineResult pipelineResult) {            if (pipelineResult != null) {                PipelineResult.State state = pipelineResult.getState();                if (state.isTerminal()) {                    metrics = pipelineResult.portableMetrics();                } else {                    resultHandle = pipelineResult;                }                switch(state) {                    case DONE:                        setState(Enum.DONE);                        break;                    case RUNNING:                        setState(Enum.RUNNING);                        break;                    case CANCELLED:                        setState(Enum.CANCELLED);                        break;                    case FAILED:                        setState(Enum.FAILED);                        break;                    default:                        setState(JobState.Enum.UNSPECIFIED);                }            } else {                setState(JobState.Enum.UNSPECIFIED);            }        }        @Override        public void onFailure(@Nonnull Throwable throwable) {            if (throwable instanceof CancellationException) {                                setState(JobState.Enum.CANCELLED);                return;            }            String message = String.format("Error during job invocation %s.", getId());                        sendMessage(JobMessage.newBuilder().setMessageText(getStackTraceAsString(throwable)).setImportance(JobMessage.MessageImportance.JOB_MESSAGE_DEBUG).build());            sendMessage(JobMessage.newBuilder().setMessageText(getRootCause(throwable).toString()).setImportance(JobMessage.MessageImportance.JOB_MESSAGE_ERROR).build());            setState(JobState.Enum.FAILED);        }    }, executorService);}
public void beam_f11661_0(PortablePipelineResult pipelineResult)
{    if (pipelineResult != null) {        PipelineResult.State state = pipelineResult.getState();        if (state.isTerminal()) {            metrics = pipelineResult.portableMetrics();        } else {            resultHandle = pipelineResult;        }        switch(state) {            case DONE:                setState(Enum.DONE);                break;            case RUNNING:                setState(Enum.RUNNING);                break;            case CANCELLED:                setState(Enum.CANCELLED);                break;            case FAILED:                setState(Enum.FAILED);                break;            default:                setState(JobState.Enum.UNSPECIFIED);        }    } else {        setState(JobState.Enum.UNSPECIFIED);    }}
public void beam_f11662_1(@Nonnull Throwable throwable)
{    if (throwable instanceof CancellationException) {                setState(JobState.Enum.CANCELLED);        return;    }    String message = String.format("Error during job invocation %s.", getId());        sendMessage(JobMessage.newBuilder().setMessageText(getStackTraceAsString(throwable)).setImportance(JobMessage.MessageImportance.JOB_MESSAGE_DEBUG).build());    sendMessage(JobMessage.newBuilder().setMessageText(getRootCause(throwable).toString()).setImportance(JobMessage.MessageImportance.JOB_MESSAGE_ERROR).build());    setState(JobState.Enum.FAILED);}
public synchronized void beam_f11671_0(Consumer<JobMessage> messageStreamObserver)
{    messageObservers.add(messageStreamObserver);}
public JobApi.JobInfo beam_f11672_0()
{    return JobApi.JobInfo.newBuilder().setJobId(jobInfo.jobId()).setJobName(jobInfo.jobName()).setPipelineOptions(jobInfo.pipelineOptions()).setState(getState()).build();}
private synchronized void beam_f11673_0(JobState.Enum state)
{    this.jobState = state;    for (Consumer<JobState.Enum> observer : stateObservers) {        observer.accept(state);    }}
public int beam_f11681_0()
{    return port;}
public int beam_f11682_0()
{    return artifactPort;}
public int beam_f11683_0()
{    return expansionPort;}
public synchronized void beam_f11691_1()
{    if (jobServer != null) {        try {            jobServer.close();                        jobServer = null;        } catch (Exception e) {                    }    }    if (artifactStagingServer != null) {        try {            artifactStagingServer.close();                        artifactStagingServer = null;        } catch (Exception e) {                    }    }    if (expansionServer != null) {        try {            expansionServer.close();                        expansionServer = null;        } catch (Exception e) {                    }    }}
protected String beam_f11692_0(String session)
{    return BeamFileSystemArtifactStagingService.generateStagingSessionToken(session, configuration.artifactStagingPath);}
private GrpcFnServer<InMemoryJobService> beam_f11693_1() throws IOException
{    InMemoryJobService service = createJobService();    GrpcFnServer<InMemoryJobService> jobServiceGrpcFnServer;    if (configuration.port == 0) {        jobServiceGrpcFnServer = GrpcFnServer.allocatePortAndCreateFor(service, jobServerFactory);    } else {        Endpoints.ApiServiceDescriptor descriptor = Endpoints.ApiServiceDescriptor.newBuilder().setUrl(configuration.host + ":" + configuration.port).build();        jobServiceGrpcFnServer = GrpcFnServer.create(service, descriptor, jobServerFactory);    }        return jobServiceGrpcFnServer;}
private void beam_f11701_0(String retrievalToken) throws Exception
{    try (GrpcFnServer artifactServer = GrpcFnServer.allocatePortAndCreateFor(BeamFileSystemArtifactRetrievalService.create(), InProcessServerFactory.create())) {        ManagedChannel grpcChannel = InProcessManagedChannelFactory.create().forDescriptor(artifactServer.getApiServiceDescriptor());        ArtifactRetrievalServiceBlockingStub retrievalServiceStub = ArtifactRetrievalServiceGrpc.newBlockingStub(grpcChannel);        ProxyManifest proxyManifest = copyStagedArtifacts(retrievalToken, new ArtifactRetriever() {            @Override            public GetManifestResponse getManifest(GetManifestRequest request) {                return retrievalServiceStub.getManifest(request);            }            @Override            public Iterator<ArtifactChunk> getArtifact(GetArtifactRequest request) {                return retrievalServiceStub.getArtifact(request);            }        });        writeAsJson(proxyManifest, PortablePipelineJarUtils.ARTIFACT_MANIFEST_PATH);        grpcChannel.shutdown();    }}
public GetManifestResponse beam_f11702_0(GetManifestRequest request)
{    return retrievalServiceStub.getManifest(request);}
public Iterator<ArtifactChunk> beam_f11703_0(GetArtifactRequest request)
{    return retrievalServiceStub.getArtifact(request);}
private static InputStream beam_f11711_0(String resourcePath) throws IOException
{    InputStream inputStream = PortablePipelineJarUtils.class.getResourceAsStream(resourcePath);    if (inputStream == null) {        throw new FileNotFoundException(String.format("Resource %s not found on classpath.", resourcePath));    }    return inputStream;}
private static void beam_f11712_0(String resourcePath, Builder builder) throws IOException
{    try (InputStream inputStream = getResourceFromClassPath(resourcePath)) {        String contents = new String(ByteStreams.toByteArray(inputStream), StandardCharsets.UTF_8);        JsonFormat.parser().merge(contents, builder);    }}
public static Pipeline beam_f11713_0() throws IOException
{    Pipeline.Builder builder = Pipeline.newBuilder();    parseJsonResource("/" + PIPELINE_PATH, builder);    return builder.build();}
private void beam_f11721_1(StreamObserver<BeamFnApi.LogControl> outboundObserver)
{    if (outboundObserver != null) {        try {            outboundObserver.onCompleted();        } catch (RuntimeException ignored) {                                }    }}
public void beam_f11722_0(BeamFnApi.LogEntry.List value)
{    for (BeamFnApi.LogEntry logEntry : value.getLogEntriesList()) {        logWriter.log(logEntry);    }}
public void beam_f11723_1(Throwable t)
{                    completeIfNotNull(connectedClients.remove(this));}
public static ServerFactory beam_f11732_0()
{    return new InetSocketAddressServerFactory(UrlFactory.createDefault());}
public static ServerFactory beam_f11733_0(UrlFactory urlFactory)
{    return new InetSocketAddressServerFactory(urlFactory);}
public static ServerFactory beam_f11734_0(Supplier<Integer> portSupplier)
{    return new InetSocketAddressServerFactory(UrlFactory.createDefault(), portSupplier);}
public Server beam_f11742_0(List<BindableService> services, Endpoints.ApiServiceDescriptor.Builder apiServiceDescriptor) throws IOException
{    File tmp;    do {        tmp = chooseRandomTmpFile(ThreadLocalRandom.current().nextInt(10000));    } while (tmp.exists());    apiServiceDescriptor.setUrl("unix://" + tmp.getAbsolutePath());    return create(services, apiServiceDescriptor.build());}
public Server beam_f11743_0(List<BindableService> services, Endpoints.ApiServiceDescriptor serviceDescriptor) throws IOException
{    SocketAddress socketAddress = SocketAddressFactory.createFrom(serviceDescriptor.getUrl());    checkArgument(socketAddress instanceof DomainSocketAddress, "%s requires a Unix domain socket address, got %s", EpollDomainSocket.class.getSimpleName(), serviceDescriptor.getUrl());    return createServer(services, (DomainSocketAddress) socketAddress);}
private static Server beam_f11744_0(List<BindableService> services, DomainSocketAddress domainSocket) throws IOException
{    NettyServerBuilder builder = NettyServerBuilder.forAddress(domainSocket).channelType(EpollServerDomainSocketChannel.class).workerEventLoopGroup(new EpollEventLoopGroup()).bossEventLoopGroup(new EpollEventLoopGroup()).maxMessageSize(Integer.MAX_VALUE).permitKeepAliveTime(KEEP_ALIVE_TIME_SEC, TimeUnit.SECONDS);    for (BindableService service : services) {                builder.addService(ServerInterceptors.intercept(service, GrpcContextHeaderAccessorProvider.interceptor()));    }    return builder.build().start();}
public void beam_f11752_0(List<BundleApplication> primaryRoots, List<DelayedBundleApplication> residualRoots)
{    checkState(this.primaryRoots == null, "At most 1 split supported, however got new split (%s, %s) " + "in addition to existing (%s, %s)", primaryRoots, residualRoots, this.primaryRoots, this.residualRoots);    this.primaryRoots = primaryRoots;    this.residualRoots = residualRoots;}
private void beam_f11753_0(StateNamespace ns)
{    stateNamespace = ns;    seedState = stateInternals.state(ns, seedTag);    restrictionState = stateInternals.state(ns, restrictionTag);    holdState = stateInternals.state(ns, watermarkHoldTag);}
public static GrpcStateService beam_f11754_0()
{    return new GrpcStateService();}
public void beam_f11762_0()
{    outboundObserver.onCompleted();}
private CompletionStage<StateResponse.Builder> beam_f11763_0(StateRequest request)
{    CompletableFuture<StateResponse.Builder> result = new CompletableFuture<>();    result.complete(StateResponse.newBuilder().setError(String.format("Unknown process bundle instruction id '%s'", request.getInstructionId())));    return result;}
private StateResponse beam_f11764_0(String id, Throwable t)
{    return StateResponse.newBuilder().setId(id).setError(getStackTraceAsString(t)).build();}
 void beam_f11772_0()
{    stateInternals = null;}
 Iterable<BeamFnApi.ProcessBundleRequest.CacheToken> beam_f11773_0()
{    return Collections.emptyList();}
 static StateRequestHandler beam_f11774_0()
{    return request -> {        throw new UnsupportedOperationException(String.format("Cannot use an empty %s", StateRequestHandler.class.getSimpleName()));    };}
private CompletionStage<StateResponse.Builder> beam_f11782_0(StateRequest request)
{    CompletableFuture<StateResponse.Builder> rval = new CompletableFuture<>();    rval.completeExceptionally(new IllegalStateException());    return rval;}
public static StateRequestHandler beam_f11783_0(Map<String, Map<String, SideInputSpec>> sideInputSpecs, SideInputHandlerFactory sideInputHandlerFactory)
{    return new StateRequestHandlerToSideInputHandlerFactoryAdapter(sideInputSpecs, sideInputHandlerFactory);}
public CompletionStage<StateResponse.Builder> beam_f11784_0(StateRequest request) throws Exception
{    try {        checkState(TypeCase.MULTIMAP_SIDE_INPUT.equals(request.getStateKey().getTypeCase()), "Unsupported %s type %s, expected %s", StateRequest.class.getSimpleName(), request.getStateKey().getTypeCase(), TypeCase.MULTIMAP_SIDE_INPUT);        StateKey.MultimapSideInput stateKey = request.getStateKey().getMultimapSideInput();        SideInputSpec<?, ?, ?> referenceSpec = sideInputSpecs.get(stateKey.getTransformId()).get(stateKey.getSideInputId());        SideInputHandler<?, ?> handler = handlerCache.computeIfAbsent(referenceSpec, this::createHandler);        switch(request.getRequestCase()) {            case GET:                return handleGetRequest(request, handler);            case APPEND:            case CLEAR:            default:                throw new Exception(String.format("Unsupported request type %s for side input.", request.getRequestCase()));        }    } catch (Exception e) {        CompletableFuture f = new CompletableFuture();        f.completeExceptionally(e);        return f;    }}
private static CompletionStage<StateResponse.Builder> beam_f11792_0(StateRequest request, ByteString key, W window, BagUserStateHandler<ByteString, ByteString, W> handler)
{    handler.clear(key, window);    return CompletableFuture.completedFuture(StateResponse.newBuilder().setId(request.getId()).setClear(StateClearResponse.getDefaultInstance()));}
private BagUserStateHandler<K, V, W> beam_f11793_0(BagUserStateSpec cacheKey)
{    return handlerFactory.forUserState(cacheKey.transformId(), cacheKey.userStateId(), cacheKey.keyCoder(), cacheKey.valueCoder(), cacheKey.windowCoder());}
public static BatchSideInputHandlerFactory beam_f11794_0(ExecutableStage stage, SideInputGetter sideInputGetter)
{    ImmutableMap.Builder<SideInputId, PCollectionNode> sideInputBuilder = ImmutableMap.builder();    for (SideInputReference sideInput : stage.getSideInputs()) {        sideInputBuilder.put(SideInputId.newBuilder().setTransformId(sideInput.transform().getId()).setLocalName(sideInput.localName()).build(), sideInput.collection());    }    return new BatchSideInputHandlerFactory(sideInputBuilder.build(), sideInputGetter);}
 static SideInputKey beam_f11802_0(Object key, Object window)
{    return new AutoValue_BatchSideInputHandlerFactory_SideInputKey(key, window);}
public static BiMap<String, Integer> beam_f11803_0(Iterable<String> localOutputs)
{    ImmutableBiMap.Builder<String, Integer> builder = ImmutableBiMap.builder();    int outputIndex = 0;        for (String tag : Sets.newTreeSet(localOutputs)) {        builder.put(tag, outputIndex);        outputIndex++;    }    return builder.build();}
public static Coder<WindowedValue<T>> beam_f11804_0(String collectionId, RunnerApi.Components components)
{    PipelineNode.PCollectionNode collectionNode = PipelineNode.pCollection(collectionId, components.getPcollectionsOrThrow(collectionId));    try {        return WireCoders.instantiateRunnerWireCoder(collectionNode, components);    } catch (IOException e) {        throw new RuntimeException("Could not instantiate Coder", e);    }}
private static String beam_f11812_0(RunnerApi.Components.Builder components)
{        Coder.Builder byteArrayCoder = Coder.newBuilder();    byteArrayCoder.getSpecBuilder().setUrn(ModelCoders.BYTES_CODER_URN);    String byteArrayCoderId = addCoder(byteArrayCoder.build(), components, "byte_array");        Coder.Builder lengthPrefixByteArrayCoder = Coder.newBuilder();    lengthPrefixByteArrayCoder.addComponentCoderIds(byteArrayCoderId).getSpecBuilder().setUrn(ModelCoders.LENGTH_PREFIX_CODER_URN);    return addCoder(lengthPrefixByteArrayCoder.build(), components, "length_prefix_byte_array");}
private static String beam_f11813_0(RunnerApi.Coder coder, RunnerApi.Components.Builder components, String uniqueIdPrefix)
{    for (Entry<String, Coder> entry : components.getCodersMap().entrySet()) {        if (entry.getValue().equals(coder)) {            return entry.getKey();        }    }    String id = generateUniqueId(uniqueIdPrefix, components::containsCoders);    components.putCoders(id, coder);    return id;}
 static String beam_f11814_0(String prefix, Predicate<String> isExistingId)
{    int i = 0;    while (isExistingId.test(prefix + i)) {        i += 1;    }    return prefix + i;}
public void beam_f11823_0(Throwable throwable)
{    throwable.printStackTrace();    Assert.fail("OnError should never be called.");}
public void beam_f11824_0()
{    complete.complete(Boolean.TRUE);}
private String beam_f11825_0(String stagingSessionToken, List<ArtifactMetadata> artifacts)
{    return stagingBlockingStub.commitManifest(CommitManifestRequest.newBuilder().setStagingSessionToken(stagingSessionToken).setManifest(Manifest.newBuilder().addAllArtifact(artifacts).build()).build()).getRetrievalToken();}
private static String beam_f11833_0(String path) throws IOException
{    try (InputStream stream = Channels.newInputStream(FileSystems.open(FileSystems.matchNewResource(path, false)))) {        return new String(ByteStreams.toByteArray(stream), StandardCharsets.UTF_8);    }}
private String beam_f11834_0(String name, String retrievalToken) throws ExecutionException, InterruptedException
{    CompletableFuture<ByteString> result = new CompletableFuture<>();    retrievalStub.getArtifact(GetArtifactRequest.newBuilder().setRetrievalToken(retrievalToken).setName(name).build(), new StreamObserver<ArtifactChunk>() {        private ByteString data = ByteString.EMPTY;        @Override        public void onNext(ArtifactChunk artifactChunk) {            data = data.concat(artifactChunk.getData());        }        @Override        public void onError(Throwable throwable) {            result.completeExceptionally(throwable);        }        @Override        public void onCompleted() {            result.complete(data);        }    });    return result.get().toStringUtf8();}
public void beam_f11835_0(ArtifactChunk artifactChunk)
{    data = data.concat(artifactChunk.getData());}
public void beam_f11843_0() throws Exception
{    ServerFactory serverFactory = ServerFactory.createDefault();    Environment environmentA = Environment.newBuilder().setUrn("env:urn:a").build();    EnvironmentFactory envFactoryA = mock(EnvironmentFactory.class);    when(envFactoryA.createEnvironment(environmentA)).thenReturn(remoteEnvironment);    EnvironmentFactory.Provider environmentProviderFactoryA = mock(EnvironmentFactory.Provider.class);    when(environmentProviderFactoryA.createEnvironmentFactory(any(), any(), any(), any(), any(), any())).thenReturn(envFactoryA);    when(environmentProviderFactoryA.getServerFactory()).thenReturn(serverFactory);    Environment environmentB = Environment.newBuilder().setUrn("env:urn:b").build();    EnvironmentFactory envFactoryB = mock(EnvironmentFactory.class);    when(envFactoryB.createEnvironment(environmentB)).thenReturn(remoteEnvironment);    EnvironmentFactory.Provider environmentProviderFactoryB = mock(EnvironmentFactory.Provider.class);    when(environmentProviderFactoryB.createEnvironmentFactory(any(), any(), any(), any(), any(), any())).thenReturn(envFactoryB);    when(environmentProviderFactoryB.getServerFactory()).thenReturn(serverFactory);    Map<String, Provider> environmentFactoryProviderMap = ImmutableMap.of(environmentA.getUrn(), environmentProviderFactoryA, environmentB.getUrn(), environmentProviderFactoryB);    try (DefaultJobBundleFactory bundleFactory = DefaultJobBundleFactory.create(JobInfo.create("testJob", "testJob", "token", Struct.getDefaultInstance()), environmentFactoryProviderMap)) {        bundleFactory.forStage(getExecutableStage(environmentB));        bundleFactory.forStage(getExecutableStage(environmentA));    }    verify(envFactoryA).createEnvironment(environmentA);    verify(envFactoryB).createEnvironment(environmentB);}
public void beam_f11844_0() throws Exception
{    ServerFactory serverFactory = ServerFactory.createDefault();    Environment environmentA = Environment.newBuilder().setUrn("env:urn:a").build();    EnvironmentFactory envFactoryA = mock(EnvironmentFactory.class);    when(envFactoryA.createEnvironment(environmentA)).thenReturn(remoteEnvironment);    EnvironmentFactory.Provider environmentProviderFactoryA = mock(EnvironmentFactory.Provider.class);    when(environmentProviderFactoryA.createEnvironmentFactory(any(), any(), any(), any(), any(), any())).thenReturn(envFactoryA);    when(environmentProviderFactoryA.getServerFactory()).thenReturn(serverFactory);    Map<String, Provider> environmentFactoryProviderMap = ImmutableMap.of(environmentA.getUrn(), environmentProviderFactoryA);    PortablePipelineOptions portableOptions = PipelineOptionsFactory.as(PortablePipelineOptions.class);    portableOptions.setEnvironmentExpirationMillis(1);    Struct pipelineOptions = PipelineOptionsTranslation.toProto(portableOptions);    try (DefaultJobBundleFactory bundleFactory = new DefaultJobBundleFactory(JobInfo.create("testJob", "testJob", "token", pipelineOptions), environmentFactoryProviderMap, stageIdGenerator, serverInfo)) {        OutputReceiverFactory orf = mock(OutputReceiverFactory.class);        StateRequestHandler srh = mock(StateRequestHandler.class);        when(srh.getCacheTokens()).thenReturn(Collections.emptyList());        StageBundleFactory sbf = bundleFactory.forStage(getExecutableStage(environmentA));                Thread.sleep(10);        sbf.getBundle(orf, srh, BundleProgressHandler.ignored()).close();                Thread.sleep(10);        sbf.getBundle(orf, srh, BundleProgressHandler.ignored()).close();    }    verify(envFactoryA, Mockito.times(3)).createEnvironment(environmentA);    verify(remoteEnvironment, Mockito.times(3)).close();}
public void beam_f11845_0() throws Exception
{    try (DefaultJobBundleFactory bundleFactory = createDefaultJobBundleFactory(envFactoryProviderMap)) {        bundleFactory.forStage(getExecutableStage(environment));    }    verify(remoteEnvironment).close();}
public void beam_f11853_0() throws Exception
{    CountDownLatch latch = new CountDownLatch(1);    AtomicBoolean sawComplete = new AtomicBoolean();    stub.control(new StreamObserver<InstructionRequest>() {        @Override        public void onNext(InstructionRequest value) {            Assert.fail("Should never see a request");        }        @Override        public void onError(Throwable t) {            latch.countDown();        }        @Override        public void onCompleted() {            sawComplete.set(true);            latch.countDown();        }    });        pool.getSource().take("", Duration.ofSeconds(2));    server.close();    latch.await();    assertThat(sawComplete.get(), is(true));}
public void beam_f11854_0(InstructionRequest value)
{    Assert.fail("Should never see a request");}
public void beam_f11855_0(Throwable t)
{    latch.countDown();}
public void beam_f11863_0() throws Exception
{    String id = "errorInstruction";    CompletionStage<InstructionResponse> responseFuture = client.handle(BeamFnApi.InstructionRequest.newBuilder().setInstructionId(id).build());    class FrazzleException extends Exception {    }    client.asResponseObserver().onError(new FrazzleException());    thrown.expect(ExecutionException.class);    thrown.expectCause(isA(FrazzleException.class));    MoreFutures.get(responseFuture);}
public void beam_f11864_0() throws Exception
{    String id = "serverCloseInstruction";    CompletionStage<InstructionResponse> responseFuture = client.handle(BeamFnApi.InstructionRequest.newBuilder().setInstructionId(id).build());    client.close();    thrown.expect(ExecutionException.class);    thrown.expectCause(isA(IllegalStateException.class));    thrown.expectMessage("closed");    MoreFutures.get(responseFuture);}
public void beam_f11865_0() throws Exception
{    Consumer<FnApiControlClient> mockConsumer1 = Mockito.mock(Consumer.class);    Consumer<FnApiControlClient> mockConsumer2 = Mockito.mock(Consumer.class);    client.onClose(mockConsumer1);    client.onClose(mockConsumer2);    client.close();    verify(mockConsumer1).accept(client);    verify(mockConsumer2).accept(client);}
public void beam_f11873_0() throws Exception
{    Pipeline p = Pipeline.create();    p.apply("impulse", Impulse.create()).apply("create", ParDo.of(new DoFn<byte[], KV<String, String>>() {        @ProcessElement        public void process(ProcessContext ctxt) throws Exception {            String element = CoderUtils.decodeFromByteArray(StringUtf8Coder.of(), ctxt.element());            if (element.equals("X")) {                throw new Exception("testBundleExecutionFailure");            }            ctxt.output(KV.of(element, element));        }    })).apply("gbk", GroupByKey.create());    RunnerApi.Pipeline pipelineProto = PipelineTranslation.toProto(p);    FusedPipeline fused = GreedyPipelineFuser.fuse(pipelineProto);    checkState(fused.getFusedStages().size() == 1, "Expected exactly one fused stage");    ExecutableStage stage = fused.getFusedStages().iterator().next();    ExecutableProcessBundleDescriptor descriptor = ProcessBundleDescriptors.fromExecutableStage("my_stage", stage, dataServer.getApiServiceDescriptor());    BundleProcessor processor = controlClient.getProcessor(descriptor.getProcessBundleDescriptor(), descriptor.getRemoteInputDestinations());    Map<String, ? super Coder<WindowedValue<?>>> remoteOutputCoders = descriptor.getRemoteOutputCoders();    Map<String, Collection<? super WindowedValue<?>>> outputValues = new HashMap<>();    Map<String, RemoteOutputReceiver<?>> outputReceivers = new HashMap<>();    for (Entry<String, ? super Coder<WindowedValue<?>>> remoteOutputCoder : remoteOutputCoders.entrySet()) {        List<? super WindowedValue<?>> outputContents = Collections.synchronizedList(new ArrayList<>());        outputValues.put(remoteOutputCoder.getKey(), outputContents);        outputReceivers.put(remoteOutputCoder.getKey(), RemoteOutputReceiver.of((Coder) remoteOutputCoder.getValue(), (FnDataReceiver<? super WindowedValue<?>>) outputContents::add));    }    try (ActiveBundle bundle = processor.newBundle(outputReceivers, BundleProgressHandler.ignored())) {        Iterables.getOnlyElement(bundle.getInputReceivers().values()).accept(WindowedValue.valueInGlobalWindow(CoderUtils.encodeToByteArray(StringUtf8Coder.of(), "Y")));    }    try {        try (ActiveBundle bundle = processor.newBundle(outputReceivers, BundleProgressHandler.ignored())) {            Iterables.getOnlyElement(bundle.getInputReceivers().values()).accept(WindowedValue.valueInGlobalWindow(CoderUtils.encodeToByteArray(StringUtf8Coder.of(), "X")));        }                fail();    } catch (ExecutionException e) {        assertTrue(e.getMessage().contains("testBundleExecutionFailure"));    }    try (ActiveBundle bundle = processor.newBundle(outputReceivers, BundleProgressHandler.ignored())) {        Iterables.getOnlyElement(bundle.getInputReceivers().values()).accept(WindowedValue.valueInGlobalWindow(CoderUtils.encodeToByteArray(StringUtf8Coder.of(), "Z")));    }    for (Collection<? super WindowedValue<?>> windowedValues : outputValues.values()) {        assertThat(windowedValues, containsInAnyOrder(WindowedValue.valueInGlobalWindow(KV.of("Y", "Y")), WindowedValue.valueInGlobalWindow(KV.of("Z", "Z"))));    }}
public void beam_f11874_0(ProcessContext ctxt) throws Exception
{    String element = CoderUtils.decodeFromByteArray(StringUtf8Coder.of(), ctxt.element());    if (element.equals("X")) {        throw new Exception("testBundleExecutionFailure");    }    ctxt.output(KV.of(element, element));}
public void beam_f11875_0() throws Exception
{    Pipeline p = Pipeline.create();    PCollection<String> input = p.apply("impulse", Impulse.create()).apply("create", ParDo.of(new DoFn<byte[], String>() {        @ProcessElement        public void process(ProcessContext ctxt) {            ctxt.output("zero");            ctxt.output("one");            ctxt.output("two");        }    })).setCoder(StringUtf8Coder.of());    PCollectionView<Iterable<String>> view = input.apply("createSideInput", View.asIterable());    input.apply("readSideInput", ParDo.of(new DoFn<String, KV<String, String>>() {        @ProcessElement        public void processElement(ProcessContext context) {            for (String value : context.sideInput(view)) {                context.output(KV.of(context.element(), value));            }        }    }).withSideInputs(view)).setCoder(KvCoder.of(StringUtf8Coder.of(), StringUtf8Coder.of())).apply("gbk", GroupByKey.create());    RunnerApi.Pipeline pipelineProto = PipelineTranslation.toProto(p);    FusedPipeline fused = GreedyPipelineFuser.fuse(pipelineProto);    Optional<ExecutableStage> optionalStage = Iterables.tryFind(fused.getFusedStages(), (ExecutableStage stage) -> !stage.getSideInputs().isEmpty());    checkState(optionalStage.isPresent(), "Expected a stage with side inputs.");    ExecutableStage stage = optionalStage.get();    ExecutableProcessBundleDescriptor descriptor = ProcessBundleDescriptors.fromExecutableStage("test_stage", stage, dataServer.getApiServiceDescriptor(), stateServer.getApiServiceDescriptor());    BundleProcessor processor = controlClient.getProcessor(descriptor.getProcessBundleDescriptor(), descriptor.getRemoteInputDestinations(), stateDelegator);    Map<String, Coder> remoteOutputCoders = descriptor.getRemoteOutputCoders();    Map<String, Collection<WindowedValue<?>>> outputValues = new HashMap<>();    Map<String, RemoteOutputReceiver<?>> outputReceivers = new HashMap<>();    for (Entry<String, Coder> remoteOutputCoder : remoteOutputCoders.entrySet()) {        List<WindowedValue<?>> outputContents = Collections.synchronizedList(new ArrayList<>());        outputValues.put(remoteOutputCoder.getKey(), outputContents);        outputReceivers.put(remoteOutputCoder.getKey(), RemoteOutputReceiver.of((Coder<WindowedValue<?>>) remoteOutputCoder.getValue(), outputContents::add));    }    Iterable<String> sideInputData = Arrays.asList("A", "B", "C");    StateRequestHandler stateRequestHandler = StateRequestHandlers.forSideInputHandlerFactory(descriptor.getSideInputSpecs(), new SideInputHandlerFactory() {        @Override        public <T, V, W extends BoundedWindow> SideInputHandler<V, W> forSideInput(String pTransformId, String sideInputId, RunnerApi.FunctionSpec accessPattern, Coder<T> elementCoder, Coder<W> windowCoder) {            return new SideInputHandler<V, W>() {                @Override                public Iterable<V> get(byte[] key, W window) {                    return (Iterable) sideInputData;                }                @Override                public Coder<V> resultCoder() {                    return ((KvCoder) elementCoder).getValueCoder();                }            };        }    });    BundleProgressHandler progressHandler = BundleProgressHandler.ignored();    try (ActiveBundle bundle = processor.newBundle(outputReceivers, stateRequestHandler, progressHandler)) {        Iterables.getOnlyElement(bundle.getInputReceivers().values()).accept(WindowedValue.valueInGlobalWindow("X"));        Iterables.getOnlyElement(bundle.getInputReceivers().values()).accept(WindowedValue.valueInGlobalWindow("Y"));    }    for (Collection<WindowedValue<?>> windowedValues : outputValues.values()) {        assertThat(windowedValues, containsInAnyOrder(WindowedValue.valueInGlobalWindow(KV.of("X", "A")), WindowedValue.valueInGlobalWindow(KV.of("X", "B")), WindowedValue.valueInGlobalWindow(KV.of("X", "C")), WindowedValue.valueInGlobalWindow(KV.of("Y", "A")), WindowedValue.valueInGlobalWindow(KV.of("Y", "B")), WindowedValue.valueInGlobalWindow(KV.of("Y", "C"))));    }}
public void beam_f11883_0(ProcessContext ctxt) throws InterruptedException
{        if (!emitted) {        ctxt.output("zero");        ctxt.output("one");        ctxt.output("two");        Thread.sleep(1000);        Metrics.counter(RemoteExecutionTest.class, processUserCounterName).inc();        Metrics.distribution(RemoteExecutionTest.class, processUserDistributionName).update(1);    }    emitted = true;}
public void beam_f11884_0() throws InterruptedException
{    Thread.sleep(1000);    Metrics.counter(RemoteExecutionTest.class, finishUserCounterName).inc(100);    Metrics.distribution(RemoteExecutionTest.class, finishUserDistributionName).update(100);}
public void beam_f11885_0(ProcessContext ctxt)
{        ctxt.output(ctxt.element());    ctxt.output(ctxt.element());}
public Iterable<Object> beam_f11895_0(ByteString key, BoundedWindow window)
{    return (Iterable) userStateData.get(userStateId);}
public void beam_f11896_0(ByteString key, BoundedWindow window, Iterator<Object> values)
{    Iterators.addAll(userStateData.get(userStateId), (Iterator) values);}
public void beam_f11897_0(ByteString key, BoundedWindow window)
{    userStateData.get(userStateId).clear();}
public void beam_f11906_0(ProcessContext c)
{    c.output(KV.of(c.element(), ""));}
private KV<String, byte[]> beam_f11907_0(String key, long value) throws CoderException
{    return KV.of(key, CoderUtils.encodeToByteArray(BigEndianLongCoder.of(), value));}
private KV<String, org.apache.beam.runners.core.construction.Timer<byte[]>> beam_f11908_0(String key, long timestampOffset) throws CoderException
{    return KV.of(key, org.apache.beam.runners.core.construction.Timer.of(BoundedWindow.TIMESTAMP_MIN_VALUE.plus(timestampOffset), CoderUtils.encodeToByteArray(VoidCoder.of(), null, Coder.Context.NESTED)));}
public void beam_f11916_0() throws Exception
{    Exception testException = new Exception();    InboundDataClient mockOutputReceiver = mock(InboundDataClient.class);    CloseableFnDataReceiver mockInputSender = mock(CloseableFnDataReceiver.class);    CompletableFuture<InstructionResponse> processBundleResponseFuture = new CompletableFuture<>();    when(fnApiControlClient.handle(any(BeamFnApi.InstructionRequest.class))).thenReturn(new CompletableFuture<>()).thenReturn(processBundleResponseFuture);    FullWindowedValueCoder<String> coder = FullWindowedValueCoder.of(StringUtf8Coder.of(), Coder.INSTANCE);    BundleProcessor processor = sdkHarnessClient.getProcessor(descriptor, Collections.singletonMap("inputPC", RemoteInputDestination.of((FullWindowedValueCoder) coder, SDK_GRPC_READ_TRANSFORM)));    when(dataService.receive(any(), any(), any())).thenReturn(mockOutputReceiver);    when(dataService.send(any(), eq(coder))).thenReturn(mockInputSender);    doThrow(testException).when(mockInputSender).close();    RemoteOutputReceiver mockRemoteOutputReceiver = mock(RemoteOutputReceiver.class);    BundleProgressHandler mockProgressHandler = mock(BundleProgressHandler.class);    try {        try (ActiveBundle activeBundle = processor.newBundle(ImmutableMap.of(SDK_GRPC_WRITE_TRANSFORM, mockRemoteOutputReceiver), mockProgressHandler)) {                }        fail("Exception expected");    } catch (Exception e) {        assertEquals(testException, e);        verify(mockOutputReceiver).cancel();        verifyNoMoreInteractions(mockOutputReceiver);    }}
public void beam_f11917_0() throws Exception
{    Exception testException = new Exception();    InboundDataClient mockOutputReceiver = mock(InboundDataClient.class);    CloseableFnDataReceiver mockInputSender = mock(CloseableFnDataReceiver.class);    StateDelegator mockStateDelegator = mock(StateDelegator.class);    StateDelegator.Registration mockStateRegistration = mock(StateDelegator.Registration.class);    when(mockStateDelegator.registerForProcessBundleInstructionId(any(), any())).thenReturn(mockStateRegistration);    StateRequestHandler mockStateHandler = mock(StateRequestHandler.class);    when(mockStateHandler.getCacheTokens()).thenReturn(Collections.emptyList());    BundleProgressHandler mockProgressHandler = mock(BundleProgressHandler.class);    CompletableFuture<InstructionResponse> processBundleResponseFuture = new CompletableFuture<>();    when(fnApiControlClient.handle(any(BeamFnApi.InstructionRequest.class))).thenReturn(new CompletableFuture<>()).thenReturn(processBundleResponseFuture);    FullWindowedValueCoder<String> coder = FullWindowedValueCoder.of(StringUtf8Coder.of(), Coder.INSTANCE);    BundleProcessor processor = sdkHarnessClient.getProcessor(descriptor, Collections.singletonMap(inputPCollection, RemoteInputDestination.of((FullWindowedValueCoder) coder, SDK_GRPC_READ_TRANSFORM)), mockStateDelegator);    when(dataService.receive(any(), any(), any())).thenReturn(mockOutputReceiver);    when(dataService.send(any(), eq(coder))).thenReturn(mockInputSender);    doThrow(testException).when(mockInputSender).close();    RemoteOutputReceiver mockRemoteOutputReceiver = mock(RemoteOutputReceiver.class);    try {        try (ActiveBundle activeBundle = processor.newBundle(ImmutableMap.of(SDK_GRPC_WRITE_TRANSFORM, mockRemoteOutputReceiver), mockStateHandler, mockProgressHandler)) {                }        fail("Exception expected");    } catch (Exception e) {        assertEquals(testException, e);        verify(mockStateRegistration).abort();        verify(mockOutputReceiver).cancel();        verifyNoMoreInteractions(mockStateRegistration, mockOutputReceiver);    }}
public void beam_f11918_0() throws Exception
{    Exception testException = new Exception();    InboundDataClient mockOutputReceiver = mock(InboundDataClient.class);    CloseableFnDataReceiver mockInputSender = mock(CloseableFnDataReceiver.class);    CompletableFuture<InstructionResponse> processBundleResponseFuture = new CompletableFuture<>();    when(fnApiControlClient.handle(any(BeamFnApi.InstructionRequest.class))).thenReturn(new CompletableFuture<>()).thenReturn(processBundleResponseFuture);    FullWindowedValueCoder<String> coder = FullWindowedValueCoder.of(StringUtf8Coder.of(), Coder.INSTANCE);    BundleProcessor processor = sdkHarnessClient.getProcessor(descriptor, Collections.singletonMap("inputPC", RemoteInputDestination.of((FullWindowedValueCoder) coder, SDK_GRPC_READ_TRANSFORM)));    when(dataService.receive(any(), any(), any())).thenReturn(mockOutputReceiver);    when(dataService.send(any(), eq(coder))).thenReturn(mockInputSender);    RemoteOutputReceiver mockRemoteOutputReceiver = mock(RemoteOutputReceiver.class);    BundleProgressHandler mockProgressHandler = mock(BundleProgressHandler.class);    try {        try (ActiveBundle activeBundle = processor.newBundle(ImmutableMap.of(SDK_GRPC_WRITE_TRANSFORM, mockRemoteOutputReceiver), mockProgressHandler)) {            processBundleResponseFuture.completeExceptionally(testException);        }        fail("Exception expected");    } catch (ExecutionException e) {        assertEquals(testException, e.getCause());        verify(mockOutputReceiver).cancel();        verifyNoMoreInteractions(mockOutputReceiver);    }}
public void beam_f11926_0() throws Exception
{    Pipeline p = Pipeline.create();    p.apply("Create", Create.of(1, 2, 3));    p.replaceAll(Collections.singletonList(JavaReadViaImpulse.boundedOverride()));    ExecutableStage stage = GreedyPipelineFuser.fuse(PipelineTranslation.toProto(p)).getFusedStages().stream().findFirst().get();    RemoteEnvironment remoteEnv = mock(RemoteEnvironment.class);    when(remoteEnv.getInstructionRequestHandler()).thenReturn(instructionRequestHandler);    when(environmentFactory.createEnvironment(stage.getEnvironment())).thenReturn(remoteEnv);    factory.forStage(stage);    factory.close();    verify(remoteEnv).close();}
public void beam_f11927_0() throws Exception
{    Pipeline p = Pipeline.create();    p.apply("Create", Create.of(1, 2, 3));    p.replaceAll(Collections.singletonList(JavaReadViaImpulse.boundedOverride()));    ExecutableStage firstEnvStage = GreedyPipelineFuser.fuse(PipelineTranslation.toProto(p)).getFusedStages().stream().findFirst().get();    ExecutableStagePayload basePayload = ExecutableStagePayload.parseFrom(firstEnvStage.toPTransform("foo").getSpec().getPayload());    Environment secondEnv = Environments.createDockerEnvironment("second_env");    ExecutableStage secondEnvStage = ExecutableStage.fromPayload(basePayload.toBuilder().setEnvironment(secondEnv).build());    Environment thirdEnv = Environments.createDockerEnvironment("third_env");    ExecutableStage thirdEnvStage = ExecutableStage.fromPayload(basePayload.toBuilder().setEnvironment(thirdEnv).build());    RemoteEnvironment firstRemoteEnv = mock(RemoteEnvironment.class, "First Remote Env");    RemoteEnvironment secondRemoteEnv = mock(RemoteEnvironment.class, "Second Remote Env");    RemoteEnvironment thirdRemoteEnv = mock(RemoteEnvironment.class, "Third Remote Env");    when(environmentFactory.createEnvironment(firstEnvStage.getEnvironment())).thenReturn(firstRemoteEnv);    when(environmentFactory.createEnvironment(secondEnvStage.getEnvironment())).thenReturn(secondRemoteEnv);    when(environmentFactory.createEnvironment(thirdEnvStage.getEnvironment())).thenReturn(thirdRemoteEnv);    when(firstRemoteEnv.getInstructionRequestHandler()).thenReturn(instructionRequestHandler);    when(secondRemoteEnv.getInstructionRequestHandler()).thenReturn(instructionRequestHandler);    when(thirdRemoteEnv.getInstructionRequestHandler()).thenReturn(instructionRequestHandler);    factory.forStage(firstEnvStage);    factory.forStage(secondEnvStage);    factory.forStage(thirdEnvStage);    IllegalStateException firstException = new IllegalStateException("first stage");    doThrow(firstException).when(firstRemoteEnv).close();    IllegalStateException thirdException = new IllegalStateException("third stage");    doThrow(thirdException).when(thirdRemoteEnv).close();    try {        factory.close();        fail("Factory close should have thrown");    } catch (IllegalStateException expected) {        if (expected.equals(firstException)) {            assertThat(ImmutableList.copyOf(expected.getSuppressed()), contains(thirdException));        } else if (expected.equals(thirdException)) {            assertThat(ImmutableList.copyOf(expected.getSuppressed()), contains(firstException));        } else {            throw expected;        }        verify(firstRemoteEnv).close();        verify(secondRemoteEnv).close();        verify(thirdRemoteEnv).close();    }}
public void beam_f11928_0() throws Exception
{    final LinkedBlockingQueue<Elements> clientInboundElements = new LinkedBlockingQueue<>();    ExecutorService executorService = Executors.newCachedThreadPool();    final CountDownLatch waitForInboundElements = new CountDownLatch(1);    GrpcDataService service = GrpcDataService.create(Executors.newCachedThreadPool(), OutboundObserverFactory.serverDirect());    try (GrpcFnServer<GrpcDataService> server = GrpcFnServer.allocatePortAndCreateFor(service, InProcessServerFactory.create())) {        Collection<Future<Void>> clientFutures = new ArrayList<>();        for (int i = 0; i < 3; ++i) {            clientFutures.add(executorService.submit(() -> {                ManagedChannel channel = InProcessChannelBuilder.forName(server.getApiServiceDescriptor().getUrl()).directExecutor().build();                StreamObserver<Elements> outboundObserver = BeamFnDataGrpc.newStub(channel).data(TestStreams.withOnNext(clientInboundElements::add).build());                waitForInboundElements.await();                outboundObserver.onCompleted();                return null;            }));        }        for (int i = 0; i < 3; ++i) {            CloseableFnDataReceiver<WindowedValue<String>> consumer = service.send(LogicalEndpoint.of(Integer.toString(i), TRANSFORM_ID), CODER);            consumer.accept(WindowedValue.valueInGlobalWindow("A" + i));            consumer.accept(WindowedValue.valueInGlobalWindow("B" + i));            consumer.accept(WindowedValue.valueInGlobalWindow("C" + i));            consumer.close();        }        waitForInboundElements.countDown();        for (Future<Void> clientFuture : clientFutures) {            clientFuture.get();        }        assertThat(clientInboundElements, containsInAnyOrder(elementsWithData("0"), elementsWithData("1"), elementsWithData("2")));    }}
protected void beam_f11936_0()
{    try (AutoCloseable logs = loggingServer;        AutoCloseable data = dataServer;        AutoCloseable ctl = controlServer;        AutoCloseable c = client) {        executor.shutdownNow();    } catch (Exception e) {        throw new RuntimeException(e);    }}
public void beam_f11937_0() throws Exception
{    DockerCommand docker = DockerCommand.getDefault();    String container = docker.runImage("hello-world", ImmutableList.of(), ImmutableList.of());    System.out.printf("Started container: %s%n", container);}
public void beam_f11938_0() throws Exception
{    DockerCommand docker = DockerCommand.getDefault();    String container = docker.runImage("debian", ImmutableList.of(), ImmutableList.of("/bin/bash", "-c", "sleep 60"));    Stopwatch stopwatch = Stopwatch.createStarted();    assertThat("Container should be running.", docker.isContainerRunning(container), is(true));    docker.killContainer(container);    long elapsedSec = stopwatch.elapsed(TimeUnit.SECONDS);    assertThat("Container termination should complete before image self-exits", elapsedSec, is(lessThan(60L)));    assertThat("Container should be terminated.", docker.isContainerRunning(container), is(false));}
public void beam_f11946_0() throws Exception
{    when(docker.isContainerRunning(anyString())).thenReturn(true);    DockerEnvironmentFactory factory = getFactory((workerId, timeout) -> client);    Environment fooEnv = Environments.createDockerEnvironment("foo");    RemoteEnvironment fooHandle = factory.createEnvironment(fooEnv);    assertThat(fooHandle.getEnvironment(), is(equalTo(fooEnv)));    Environment barEnv = Environments.createDockerEnvironment("bar");    RemoteEnvironment barHandle = factory.createEnvironment(barEnv);    assertThat(barHandle.getEnvironment(), is(equalTo(barEnv)));}
private DockerEnvironmentFactory beam_f11947_0(ControlClientPool.Source clientSource)
{    return DockerEnvironmentFactory.forServicesWithDocker(docker, controlServiceServer, loggingServiceServer, retrievalServiceServer, provisioningServiceServer, clientSource, ID_GENERATOR, PipelineOptionsFactory.as(RemoteEnvironmentOptions.class));}
public void beam_f11948_0() throws IOException
{    MockitoAnnotations.initMocks(this);    when(processManager.startProcess(anyString(), anyString(), anyList(), anyMap())).thenReturn(Mockito.mock(ProcessManager.RunningProcess.class));    when(controlServiceServer.getApiServiceDescriptor()).thenReturn(SERVICE_DESCRIPTOR);    when(loggingServiceServer.getApiServiceDescriptor()).thenReturn(SERVICE_DESCRIPTOR);    when(retrievalServiceServer.getApiServiceDescriptor()).thenReturn(SERVICE_DESCRIPTOR);    when(provisioningServiceServer.getApiServiceDescriptor()).thenReturn(SERVICE_DESCRIPTOR);    factory = ProcessEnvironmentFactory.create(processManager, controlServiceServer, loggingServiceServer, retrievalServiceServer, provisioningServiceServer, (workerId, timeout) -> client, ID_GENERATOR, PipelineOptionsFactory.as(RemoteEnvironmentOptions.class));}
public void beam_f11956_0() throws IOException
{    ProcessManager processManager = ProcessManager.create();    processManager.startProcess("1", "bash", Arrays.asList("-c", "ls"));    try {        processManager.startProcess("1", "bash", Arrays.asList("-c", "ls"));        fail();    } catch (IllegalStateException e) {        } finally {        processManager.stopProcess("1");    }}
public void beam_f11957_0() throws IOException
{    ProcessManager processManager = ProcessManager.create();    ProcessManager.RunningProcess process = processManager.startProcess("1", "bash", Arrays.asList("-c", "sleep", "1000"));    process.isAliveOrThrow();    processManager.stopProcess("1");    try {        process.isAliveOrThrow();        fail();    } catch (IllegalStateException e) {        }}
public void beam_f11958_0() throws IOException, InterruptedException
{    ProcessManager processManager = ProcessManager.create();    ProcessManager.RunningProcess process = processManager.startProcess("1", "bash", Arrays.asList("-c", "sleep $PARAM"), Collections.singletonMap("PARAM", "-h"));    for (int i = 0; i < 10 && process.getUnderlyingProcess().isAlive(); i++) {        Thread.sleep(100);    }    assertThat(process.getUnderlyingProcess().exitValue(), is(1));    processManager.stopProcess("1");}
public void beam_f11966_0(ClientCall.Listener<RespT> responseListener, Metadata headers)
{    headers.put(workerIdKey, worker1);    super.start(responseListener, headers);}
public StreamObserver<BeamFnApi.Elements> beam_f11967_0(StreamObserver<BeamFnApi.Elements> outboundObserver)
{    consumer.accept(outboundObserver);    return inboundObserver;}
public void beam_f11968_0() throws Exception
{    MockitoAnnotations.initMocks(this);    stagingServiceDescriptor = Endpoints.ApiServiceDescriptor.getDefaultInstance();    service = InMemoryJobService.create(stagingServiceDescriptor, session -> "token", null, invoker);    when(invoker.invoke(TEST_PIPELINE, TEST_OPTIONS, TEST_RETRIEVAL_TOKEN)).thenReturn(invocation);    when(invocation.getId()).thenReturn(TEST_JOB_ID);    when(invocation.getPipeline()).thenReturn(TEST_PIPELINE);    when(invocation.toProto()).thenReturn(TEST_JOB_INFO);}
public void beam_f11976_0() throws Exception
{    JobApi.PrepareJobResponse prepareResponse = prepareJob();        JobApi.RunJobRequest runRequest = JobApi.RunJobRequest.newBuilder().setPreparationId(prepareResponse.getPreparationId()).setRetrievalToken(TEST_RETRIEVAL_TOKEN).build();    RecordingObserver<JobApi.RunJobResponse> runRecorder = new RecordingObserver<>();    service.run(runRequest, runRecorder);    verify(invoker, times(1)).invoke(TEST_PIPELINE, TEST_OPTIONS, TEST_RETRIEVAL_TOKEN);    assertThat(runRecorder.isSuccessful(), is(true));    assertThat(runRecorder.values, hasSize(1));    JobApi.RunJobResponse runResponse = runRecorder.values.get(0);    assertThat(runResponse.getJobId(), is(TEST_JOB_ID));    verify(invocation, times(1)).addStateListener(any());    verify(invocation, times(1)).start();}
public void beam_f11977_0(T t)
{    values.add(t);}
public void beam_f11978_0(Throwable throwable)
{    error = throwable;}
public void beam_f11986_0() throws Exception
{    jobInvocation.start();    assertThat(jobInvocation.getState(), is(JobApi.JobState.Enum.RUNNING));    TestPipelineResult pipelineResult = new TestPipelineResult(PipelineResult.State.FAILED);    runner.setResult(pipelineResult);    awaitJobState(jobInvocation, JobApi.JobState.Enum.FAILED);    jobInvocation.cancel();    pipelineResult.cancelLatch.await();}
public void beam_f11987_0() throws Exception
{    jobInvocation.start();    assertThat(jobInvocation.getState(), is(JobApi.JobState.Enum.RUNNING));    TestPipelineResult pipelineResult = new TestPipelineResult(PipelineResult.State.DONE);    runner.setResult(pipelineResult);    awaitJobState(jobInvocation, JobApi.JobState.Enum.DONE);    jobInvocation.cancel();    assertThat(jobInvocation.getState(), is(JobApi.JobState.Enum.DONE));        assertThat(pipelineResult.cancelLatch.getCount(), is(1L));}
public void beam_f11988_0() throws Exception
{    JobApi.MetricResults expectedMonitoringInfos = JobApi.MetricResults.newBuilder().build();    TestPipelineResult result = new TestPipelineResult(PipelineResult.State.DONE, expectedMonitoringInfos);    jobInvocation.start();    runner.setResult(result);    awaitJobState(jobInvocation, JobApi.JobState.Enum.DONE);    assertThat(jobInvocation.getMetrics(), allOf(is(notNullValue()), is(sameInstance(result.portableMetrics()))));}
public MetricResults beam_f11996_0()
{    return null;}
public JobApi.MetricResults beam_f11997_0()
{    return monitoringInfos;}
public void beam_f11998_0() throws IOException
{    initMocks(this);    JarInputStream emptyInputStream = new JarInputStream(new ByteArrayInputStream(new byte[0]));    when(inputJar.getInputStream(any())).thenReturn(emptyInputStream);    when(retrievalServiceStub.getArtifact(any())).thenReturn(Collections.emptyIterator());    jarCreator = new PortablePipelineJarCreator(null);    jarCreator.outputStream = outputStream;    jarCreator.outputChannel = outputChannel;}
public void beam_f12006_0()
{    Manifest manifest = jarCreator.createManifest(EmptyPipelineRunner.class);    assertNull(manifest.getMainAttributes().getValue(Name.MAIN_CLASS));}
public static int beam_f12007_0(String[] args)
{    return 0;}
public void beam_f12008_0()
{    Manifest manifest = jarCreator.createManifest(EvilPipelineRunner.class);    assertNull(manifest.getMainAttributes().getValue(Name.MAIN_CLASS));}
public void beam_f12016_0() throws Exception
{    Struct options = Struct.newBuilder().putFields("foo", Value.newBuilder().setBoolValue(true).build()).putFields("bar", Value.newBuilder().setNumberValue(2.5).build()).putFields("baz", Value.newBuilder().setListValue(ListValue.newBuilder().addValues(Value.newBuilder().setStructValue(Struct.newBuilder().putFields("spam", Value.newBuilder().setNullValue(NullValue.NULL_VALUE).build()))).build()).build()).build();    Resources resourceLimits = Resources.newBuilder().setCpu(Cpu.newBuilder().setShares(0.75F).buildPartial()).setMemory(Memory.newBuilder().setSize(2L * 1024L * 1024L * 1024L).build()).setSemiPersistentDisk(Disk.newBuilder().setSize(1024L * 1024L * 1024L * 1024L).build()).build();    ProvisionInfo info = ProvisionInfo.newBuilder().setJobId("id").setJobName("name").setWorkerId("worker").setPipelineOptions(options).setResourceLimits(resourceLimits).build();    GrpcFnServer<StaticGrpcProvisionService> server = GrpcFnServer.allocatePortAndCreateFor(StaticGrpcProvisionService.create(info), InProcessServerFactory.create());    ProvisionServiceBlockingStub stub = ProvisionServiceGrpc.newBlockingStub(InProcessChannelBuilder.forName(server.getApiServiceDescriptor().getUrl()).build());    GetProvisionInfoResponse provisionResponse = stub.getProvisionInfo(GetProvisionInfoRequest.getDefaultInstance());    assertThat(provisionResponse.getInfo(), equalTo(info));}
public void beam_f12017_0() throws Exception
{    Endpoints.ApiServiceDescriptor apiServiceDescriptor = runTestUsing(ServerFactory.createDefault(), ManagedChannelFactory.createDefault());    HostAndPort hostAndPort = HostAndPort.fromString(apiServiceDescriptor.getUrl());    assertThat(hostAndPort.getHost(), anyOf(equalTo(InetAddress.getLoopbackAddress().getHostName()), equalTo(InetAddress.getLoopbackAddress().getHostAddress())));    assertThat(hostAndPort.getPort(), allOf(greaterThan(0), lessThan(65536)));}
public void beam_f12018_0() throws Exception
{    ServerFactory serverFactory = ServerFactory.createWithUrlFactory((host, port) -> "foo");    CallStreamObserver<Elements> observer = TestStreams.withOnNext((Elements unused) -> {    }).withOnCompleted(() -> {    }).build();    TestDataService service = new TestDataService(observer);    ApiServiceDescriptor.Builder descriptorBuilder = ApiServiceDescriptor.newBuilder();    Server server = serverFactory.allocateAddressAndCreate(ImmutableList.of(service), descriptorBuilder);        server.shutdown();    assertThat(descriptorBuilder.getUrl(), is("foo"));}
public void beam_f12026_0() throws Exception
{        String bundleInstructionId = "bundle_instruction";    stateService.registerForProcessBundleInstructionId(bundleInstructionId, handler);        StreamObserver requestObserver = stateService.state(responseObserver);        BeamFnApi.StateRequest request = BeamFnApi.StateRequest.newBuilder().setInstructionId(bundleInstructionId).build();    requestObserver.onNext(request);        verify(handler).handle(request);}
public void beam_f12027_0() throws Exception
{        ByteString expectedResponseData = ByteString.copyFrom("EXPECTED_RESPONSE_DATA", StandardCharsets.UTF_8);    String bundleInstructionId = "EXPECTED_BUNDLE_INSTRUCTION_ID";    BeamFnApi.StateResponse.Builder expectedBuilder = BeamFnApi.StateResponse.newBuilder().setGet(BeamFnApi.StateGetResponse.newBuilder().setData(expectedResponseData));    StateRequestHandler dummyHandler = request -> {        CompletableFuture<BeamFnApi.StateResponse.Builder> response = new CompletableFuture<>();        response.complete(expectedBuilder);        return response;    };        BlockingDeque<BeamFnApi.StateResponse> responses = new LinkedBlockingDeque<>();    StreamObserver<BeamFnApi.StateResponse> recordingResponseObserver = TestStreams.withOnNext(responses::add).build();    recordingResponseObserver = Mockito.spy(recordingResponseObserver);        stateService.registerForProcessBundleInstructionId(bundleInstructionId, dummyHandler);        StreamObserver<BeamFnApi.StateRequest> requestObserver = stateService.state(recordingResponseObserver);        BeamFnApi.StateRequest request = BeamFnApi.StateRequest.newBuilder().setInstructionId(bundleInstructionId).build();    requestObserver.onNext(request);        BeamFnApi.StateResponse response = responses.poll(TIMEOUT_MS, TimeUnit.MILLISECONDS);        verify(recordingResponseObserver, times(1)).onNext(any());    verify(recordingResponseObserver, never()).onCompleted();    verify(recordingResponseObserver, never()).onError(any());    assertThat(response.getGet().getData(), equalTo(expectedResponseData));}
public void beam_f12028_0() throws Exception
{    StateRequestHandler mockHandler = Mockito.mock(StateRequestHandler.class);    StateRequestHandler mockHandler2 = Mockito.mock(StateRequestHandler.class);    EnumMap<StateKey.TypeCase, StateRequestHandler> handlers = new EnumMap<>(StateKey.TypeCase.class);    handlers.put(StateKey.TypeCase.TYPE_NOT_SET, mockHandler);    handlers.put(TypeCase.MULTIMAP_SIDE_INPUT, mockHandler2);    StateRequest request = StateRequest.getDefaultInstance();    StateRequest request2 = StateRequest.newBuilder().setStateKey(StateKey.newBuilder().setMultimapSideInput(MultimapSideInput.getDefaultInstance())).build();    StateRequestHandlers.delegateBasedUponType(handlers).handle(request);    StateRequestHandlers.delegateBasedUponType(handlers).handle(request2);    verify(mockHandler).handle(request);    verify(mockHandler2).handle(request2);    verifyNoMoreInteractions(mockHandler, mockHandler2);}
public void beam_f12036_0()
{    Instant instantA = new DateTime(2018, 1, 1, 1, 1, DateTimeZone.UTC).toInstant();    Instant instantB = new DateTime(2018, 1, 1, 1, 2, DateTimeZone.UTC).toInstant();    Instant instantC = new DateTime(2018, 1, 1, 1, 3, DateTimeZone.UTC).toInstant();    IntervalWindow windowA = new IntervalWindow(instantA, instantB);    IntervalWindow windowB = new IntervalWindow(instantB, instantC);    when(context.getSideInput(COLLECTION_ID)).thenReturn(Arrays.asList(WindowedValue.of(1, instantA, windowA, PaneInfo.NO_FIRING), WindowedValue.of(2, instantA, windowA, PaneInfo.NO_FIRING), WindowedValue.of(3, instantB, windowB, PaneInfo.NO_FIRING), WindowedValue.of(4, instantB, windowB, PaneInfo.NO_FIRING)));    BatchSideInputHandlerFactory factory = BatchSideInputHandlerFactory.forStage(EXECUTABLE_STAGE, context);    SideInputHandler<Integer, IntervalWindow> handler = factory.forSideInput(TRANSFORM_ID, SIDE_INPUT_NAME, ITERABLE_ACCESS, VarIntCoder.of(), IntervalWindowCoder.of());    Iterable<Integer> resultA = handler.get(null, windowA);    Iterable<Integer> resultB = handler.get(null, windowB);    assertThat(resultA, containsInAnyOrder(1, 2));    assertThat(resultB, containsInAnyOrder(3, 4));}
private static ExecutableStage beam_f12037_0(Collection<SideInputReference> sideInputs)
{    Components components = Components.getDefaultInstance();    Environment environment = Environment.getDefaultInstance();    PCollectionNode inputCollection = PipelineNode.pCollection("collection-id", RunnerApi.PCollection.getDefaultInstance());    return ImmutableExecutableStage.of(components, environment, inputCollection, sideInputs, Collections.emptyList(), Collections.emptyList(), Collections.emptyList(), Collections.emptyList());}
private static byte[] beam_f12038_0(T value, Coder<T> coder)
{    ByteArrayOutputStream out = new ByteArrayOutputStream();    try {        coder.encode(value, out);    } catch (IOException e) {        throw new RuntimeException(e);    }    return out.toByteArray();}
 void beam_f12047_0(WiringListener listener)
{    listeners.add(listener);}
 String beam_f12048_0(String transformName)
{    return vertexId++ + " (" + transformName + ")";}
 void beam_f12049_0(String edgeId, String pCollId)
{    String prevPCollId = pCollsOfEdges.put(edgeId, pCollId);    if (prevPCollId != null) {        throw new RuntimeException("Oops!");    }}
private void beam_f12057_0(Vertex sourceVertex, Vertex destinationVertex, Coder coder, String edgeId, String pCollId, boolean sideInputEdge)
{    try {        Edge edge = Edge.from(sourceVertex, getNextFreeOrdinal(sourceVertex, false)).to(destinationVertex, getNextFreeOrdinal(destinationVertex, true));        edge = edge.distributed();        if (sideInputEdge) {            edge = edge.broadcast();        } else {            edge = edge.partitioned(new PartitionedKeyExtractor(coder));        }        dag.edge(edge);        String sourceVertexName = sourceVertex.getName();        String destinationVertexName = destinationVertex.getName();        for (WiringListener listener : listeners) {            listener.isInboundEdgeOfVertex(edge, edgeId, pCollId, destinationVertexName);            listener.isOutboundEdgeOfVertex(edge, edgeId, pCollId, sourceVertexName);        }    } catch (Exception e) {        throw new RuntimeException(e);    }}
private int beam_f12058_0(Vertex vertex, boolean inbound)
{    Map<Vertex, Integer> ordinals = inbound ? inboundOrdinals : outboundOrdinals;    int nextOrdinal = 1 + ordinals.getOrDefault(vertex, -1);    ordinals.put(vertex, nextOrdinal);    return nextOrdinal;}
public Object beam_f12059_0(byte[] b) throws Exception
{    if (coder == null) {        return "ALL";    } else {        WindowedValue<KV<K, V>> windowedValue =         CoderUtils.decodeFromByteArray(coder, b);        KvCoder<K, V> kvCoder = (KvCoder<K, V>) coder.getValueCoder();        return CoderUtils.encodeToByteArray(kvCoder.getKeyCoder(), windowedValue.getValue().getKey());    }}
public Iterable<MetricResult<Long>> beam_f12067_0()
{    return Collections.emptyList();}
public Iterable<MetricResult<DistributionResult>> beam_f12068_0()
{    return Collections.emptyList();}
public Iterable<MetricResult<GaugeResult>> beam_f12069_0()
{    return Collections.emptyList();}
public State beam_f12078_1()
{    if (terminalState != null) {        return terminalState;    }    JobStatus status = job.getStatus();    switch(status) {        case COMPLETED:            return State.DONE;        case COMPLETING:        case RUNNING:        case STARTING:            return State.RUNNING;        case FAILED:            return State.FAILED;        case NOT_RUNNING:        case SUSPENDED:        case SUSPENDED_EXPORTING_SNAPSHOT:            return State.STOPPED;        default:                        return State.UNKNOWN;    }}
public State beam_f12079_0() throws IOException
{    if (terminalState != null) {        throw new IllegalStateException("Job already completed");    }    try {        job.cancel();        job.join();    } catch (CancellationException ignored) {    } catch (Exception e) {        throw new IOException("Failed to cancel the job: " + e, e);    }    return State.FAILED;}
public State beam_f12080_0(Duration duration)
{    if (terminalState != null) {        return terminalState;    }    try {        completionFuture.get(duration.getMillis(), TimeUnit.MILLISECONDS);        return State.DONE;    } catch (InterruptedException | TimeoutException e) {                return getState();    } catch (ExecutionException e) {        throw new CompletionException(e.getCause());    }}
private DAG beam_f12088_0(Pipeline pipeline)
{    JetGraphVisitor graphVisitor = new JetGraphVisitor(options, translatorProvider);    pipeline.traverseTopologically(graphVisitor);    return graphVisitor.getDAG();}
private JetPipelineResult beam_f12089_0(DAG dag)
{    startClusterIfNeeded(options);    JetInstance jet = getJetInstance(    options);        Job job = jet.newJob(dag, getJobConfig(options));    IMapJet<String, MetricUpdates> metricsAccumulator = jet.getMap(JetMetricsContainer.getMetricsMapName(job.getId()));    JetPipelineResult pipelineResult = new JetPipelineResult(job, metricsAccumulator);    CompletableFuture<Void> completionFuture = job.getFuture().whenCompleteAsync((r, f) -> {        pipelineResult.freeze(f);        metricsAccumulator.destroy();        jet.shutdown();        stopClusterIfNeeded(options);    });    pipelineResult.setCompletionFuture(completionFuture);    return pipelineResult;}
private void beam_f12090_1(JetPipelineOptions options)
{    Integer noOfLocalMembers = options.getJetLocalMode();    if (noOfLocalMembers > 0) {        Collection<JetInstance> jetInstances = new ArrayList<>();        for (int i = 0; i < noOfLocalMembers; i++) {            jetInstances.add(Jet.newJetInstance());        }            }}
 static JetTransformTranslator<?> beam_f12098_0(PTransform<?, ?> transform)
{    String urn = PTransformTranslation.urnForTransformOrNull(transform);    return urn == null ? null : TRANSLATORS.get(urn);}
public Vertex beam_f12099_0(Pipeline pipeline, AppliedPTransform<?, ?, ?> appliedTransform, Node node, JetTranslationContext context)
{    Map.Entry<TupleTag<?>, PValue> output = Utils.getOutput(appliedTransform);    Coder outputCoder = Utils.getCoder((PCollection) Utils.getOutput(appliedTransform).getValue());    String transformName = appliedTransform.getFullName();    DAGBuilder dagBuilder = context.getDagBuilder();    String vertexId = dagBuilder.newVertexId(transformName);    SerializablePipelineOptions pipelineOptions = context.getOptions();    ProcessorMetaSupplier processorSupplier = getProcessorSupplier((AppliedPTransform<PBegin, PCollection<T>, PTransform<PBegin, PCollection<T>>>) appliedTransform, outputCoder, vertexId, pipelineOptions);    Vertex vertex = dagBuilder.addVertex(vertexId, processorSupplier);    String outputEdgeId = Utils.getTupleTagId(output.getValue());    dagBuilder.registerCollectionOfEdge(outputEdgeId, output.getKey().getId());    dagBuilder.registerEdgeStartPoint(outputEdgeId, vertex, outputCoder);    return vertex;}
private ProcessorMetaSupplier beam_f12100_0(AppliedPTransform<PBegin, PCollection<T>, PTransform<PBegin, PCollection<T>>> appliedTransform, Coder outputCoder, String vertexId, SerializablePipelineOptions pipelineOptions)
{    try {        if (Utils.isBounded(appliedTransform)) {            BoundedSource<T> source = ReadTranslation.boundedSourceFromTransform(appliedTransform);            return BoundedSourceP.supplier(source, pipelineOptions, outputCoder, vertexId);        } else {            UnboundedSource<T, ?> source = ReadTranslation.unboundedSourceFromTransform(appliedTransform);            if (source.requiresDeduping()) {                throw new UnsupportedOperationException(                "Sources requiring deduping not supported!");            }            return UnboundedSourceP.supplier(source, pipelineOptions, outputCoder, vertexId);        }    } catch (IOException e) {        throw new RuntimeException(e);    }}
 DAGBuilder beam_f12108_0()
{    return dagBuilder;}
 WindowedValue.FullWindowedValueCoder<T> beam_f12109_0(PCollection<T> collection)
{    return getTypeInfo(collection.getCoder(), collection.getWindowingStrategy());}
 WindowedValue.FullWindowedValueCoder<T> beam_f12110_0(Coder<T> coder, WindowingStrategy<?, ?> windowingStrategy)
{    return WindowedValue.getFullCoder(coder, windowingStrategy.getWindowFn().windowCoder());}
public void beam_f12118_0(long value)
{    update(DistributionData.singleton(value));}
public void beam_f12119_0(long sum, long count, long min, long max)
{    update(DistributionData.create(sum, count, min, max));}
private void beam_f12120_0(DistributionData update)
{    distributionData = distributionData.combine(update);}
public Iterable<MetricResult<DistributionResult>> beam_f12128_0()
{    return distributions;}
public Iterable<MetricResult<GaugeResult>> beam_f12129_0()
{    return gauges;}
 void beam_f12130_0(Iterable<MetricUpdate<Long>> updates)
{    for (MetricUpdate<Long> update : updates) {        MetricKey key = update.getKey();        Long oldValue = counters.getOrDefault(key, 0L);        Long updatedValue = oldValue + update.getUpdate();        counters.put(key, updatedValue);    }}
 void beam_f12138_0(Iterable<MetricUpdate<GaugeData>> updates)
{    for (MetricUpdate<GaugeData> update : updates) {        MetricKey key = update.getKey();        GaugeData oldGauge = gauges.getOrDefault(key, GaugeData.empty());        GaugeData updatedGauge = update.getUpdate().combine(oldGauge);        gauges.put(key, updatedGauge);    }}
 void beam_f12139_0()
{    gauges.clear();}
 Iterable<MetricResult<GaugeResult>> beam_f12140_0(MetricsFilter filter)
{    return FluentIterable.from(gauges.entrySet()).filter(matchesFilter(filter)).transform(this::toUpdateResult).toList();}
public Iterable<MetricUpdate<Long>> beam_f12148_0()
{    return counters;}
public Iterable<MetricUpdate<DistributionData>> beam_f12149_0()
{    return distributions;}
public Iterable<MetricUpdate<GaugeData>> beam_f12150_0()
{    return gauges;}
protected void beam_f12158_0(DoFnRunner<InputT, OutputT> runner, WindowedValue<InputT> windowedValue)
{    runner.processElement(windowedValue);}
protected void beam_f12159_0(DoFnRunner<InputT, OutputT> runner)
{    runner.finishBundle();}
private void beam_f12160_0(Inbox inbox)
{    for (byte[] value; (value = (byte[]) inbox.poll()) != null; ) {        bufferedItems.add(value);    }}
 boolean beam_f12168_0()
{    for (; currentBucket < outputBuckets.length; currentBucket++) {        List<Object> bucket = outputBuckets[currentBucket];        for (; currentItem < bucket.size(); currentItem++) {            if (!outbox.offer(currentBucket, bucket.get(currentItem))) {                return false;            }        }        bucket.clear();        currentItem = 0;    }    currentBucket = 0;    int sum = 0;    for (List<Object> outputBucket : outputBuckets) {        sum += outputBucket.size();    }    return sum == 0;}
public Processor beam_f12169_0()
{    if (ordinalToSideInput.size() != sideInputs.size()) {        throw new RuntimeException("Oops");    }    return getEx(doFn, windowingStrategy, doFnSchemaInformation, outputCollToOrdinals.entrySet().stream().collect(Collectors.toMap(Map.Entry::getKey, e -> e.getValue().stream().mapToInt(i -> i).toArray())), pipelineOptions, mainOutputTag, inputCoder, Collections.unmodifiableMap(sideInputCoders), Collections.unmodifiableMap(outputCoders), inputValueCoder, Collections.unmodifiableMap(outputValueCoders), Collections.unmodifiableMap(ordinalToSideInput), ownerId, stepId);}
public void beam_f12170_0(Edge edge, String edgeId, String pCollId, String vertexId)
{    if (ownerId.equals(vertexId)) {        List<Integer> ordinals = outputCollToOrdinals.get(new TupleTag<>(pCollId));        if (ordinals == null) {                        throw new RuntimeException("Oops");        }        ordinals.add(edge.getSourceOrdinal());    }}
protected boolean beam_f12178_0(int ordinal, @Nonnull Object item)
{    return flatMapper.tryProcess((byte[]) item);}
public void beam_f12179_0(WindowedValue<InputT> value)
{    if (Iterables.size(value.getWindows()) != 1) {        throw new IllegalArgumentException(String.format("%s passed to window assignment must be in a single window, but it was in %s: %s", WindowedValue.class.getSimpleName(), Iterables.size(value.getWindows()), value.getWindows()));    }    this.value = value;}
public InputT beam_f12180_0()
{    return value.getValue();}
public void beam_f12188_0() throws Exception
{    if (currentReader != null) {        currentReader.close();    }}
public static ProcessorMetaSupplier beam_f12189_0(BoundedSource<T> boundedSource, SerializablePipelineOptions options, Coder outputCoder, String ownerId)
{    return new BoundedSourceMetaProcessorSupplier<>(boundedSource, options, outputCoder, ownerId);}
public void beam_f12190_0(@Nonnull ProcessorMetaSupplier.Context context) throws Exception
{    long desiredSizeBytes = Math.max(1, boundedSource.getEstimatedSizeBytes(options.get()) / context.totalParallelism());    shards = boundedSource.split(desiredSizeBytes, options.get());}
public static ProcessorMetaSupplier beam_f12199_0(Coder outputCoder, String ownerId)
{    return new ImpulseMetaProcessorSupplier(outputCoder, ownerId);}
public Function<? super Address, ? extends ProcessorSupplier> beam_f12200_0(@Nonnull List<Address> addresses)
{    return address -> new ImpulseProcessorSupplier(outputCoder, ownerId);}
public void beam_f12201_0(@Nonnull Context context)
{    this.context = context;}
protected void beam_f12209_0(DoFnRunner<KV<?, ?>, OutputT> runner)
{    try {        Instant now = Instant.now();        timerInternals.advanceProcessingTime(now);        timerInternals.advanceSynchronizedProcessingTime(now);    } catch (Exception e) {        throw new RuntimeException("Failed advancing time!");    }    super.startRunnerBundle(runner);}
protected void beam_f12210_0(DoFnRunner<KV<?, ?>, OutputT> runner, WindowedValue<KV<?, ?>> windowedValue)
{    KV<?, ?> kv = windowedValue.getValue();    Object key = kv.getKey();    keyedStepContext.setKey(key);    super.processElementWithRunner(runner, windowedValue);}
public boolean beam_f12211_0(@Nonnull Watermark watermark)
{    return flushTimers(watermark.timestamp()) && super.tryProcessWatermark(watermark);}
protected void beam_f12219_0(@Nonnull Processor.Context context) throws IOException
{    List<? extends UnboundedSource<T, CmT>> myShards = Utils.roundRobinSubList(allShards, context.globalProcessorIndex(), context.totalParallelism());    this.readers = createReaders(myShards, options);    Function<UnboundedSource.UnboundedReader<T>, byte[]> mapFn = (reader) -> Utils.encode(WindowedValue.timestampedValueInGlobalWindow(reader.getCurrent(), reader.getCurrentTimestamp()), outputCoder);    if (myShards.size() == 0) {        traverser = Traversers.empty();    } else if (myShards.size() == 1) {        traverser = new SingleReaderTraverser<>(readers[0], mapFn);    } else {        traverser = new CoalescingTraverser<>(readers, mapFn);    }    for (UnboundedSource.UnboundedReader<T> reader : readers) {        reader.start();    }}
public boolean beam_f12220_0()
{    emitFromTraverser(traverser);    return readers.length == 0;}
public boolean beam_f12221_0()
{    return false;}
public void beam_f12229_0(@Nonnull ProcessorMetaSupplier.Context context) throws Exception
{    shards = unboundedSource.split(context.totalParallelism(), options.get());}
public Function<? super Address, ? extends ProcessorSupplier> beam_f12230_0(@Nonnull List<Address> addresses)
{    return address -> ProcessorSupplier.of(() -> new UnboundedSourceP<>(shards, options.get(), outputCoder, ownerId));}
public Object beam_f12231_0()
{    long wm = reader.getWatermark().getMillis();    if (wm > lastWatermark) {        lastWatermark = wm;        return new Watermark(wm);    }    try {        return reader.advance() ? mapFn.apply(reader) : null;    } catch (IOException e) {        throw new RuntimeException(e);    }}
public boolean beam_f12239_0()
{    return flatMapper.tryProcess(TRY_PROCESS_MARKER);}
protected boolean beam_f12240_0(int ordinal, @Nonnull Object item)
{    return flatMapper.tryProcess(item);}
public boolean beam_f12241_0(@Nonnull Watermark watermark)
{    return flatMapper.tryProcess(watermark);}
 void beam_f12249_0(Instant now)
{    try {        timerInternals.advanceProcessingTime(now);        reduceFnRunner.persist();    } catch (Exception e) {        throw ExceptionUtil.rethrow(e);    }}
private void beam_f12250_0(Instant watermark) throws Exception
{    timerInternals.advanceInputWatermark(watermark);    while (true) {        TimerInternals.TimerData timer;        List<TimerInternals.TimerData> timers = new ArrayList<>();        while ((timer = timerInternals.removeNextEventTimer()) != null) {            timers.add(timer);        }        if (timers.isEmpty()) {            break;        }        reduceFnRunner.onTimers(timers);    }}
private void beam_f12251_0(Instant watermark)
{    Objects.requireNonNull(watermark);    timerInternals.advanceOutputWatermark(watermark);}
 static Map<TupleTag<?>, PValue> beam_f12259_0(TransformHierarchy.Node node)
{    return node.getTransform() != null ? node.getTransform().getAdditionalInputs() : null;}
 static PValue beam_f12260_0(AppliedPTransform<?, ?, ?> appliedTransform)
{    if (appliedTransform.getTransform() == null) {        return null;    }    return Iterables.getOnlyElement(TransformInputs.nonAdditionalInputs(appliedTransform));}
 static Map<TupleTag<?>, PValue> beam_f12261_0(AppliedPTransform<?, ?, ?> appliedTransform)
{    if (appliedTransform.getTransform() == null) {        return null;    }    return appliedTransform.getOutputs();}
 static List<PCollectionView<?>> beam_f12269_0(AppliedPTransform<?, ?, ?> appliedTransform)
{    PTransform<?, ?> transform = appliedTransform.getTransform();    if (transform instanceof ParDo.MultiOutput) {        ParDo.MultiOutput multiParDo = (ParDo.MultiOutput) transform;        return (List) multiParDo.getSideInputs().values().stream().collect(Collectors.toList());    } else if (transform instanceof ParDo.SingleOutput) {        ParDo.SingleOutput singleParDo = (ParDo.SingleOutput) transform;        return (List) singleParDo.getSideInputs().values().stream().collect(Collectors.toList());    }    return Collections.emptyList();}
 static boolean beam_f12270_0(AppliedPTransform<?, ?, ?> appliedTransform)
{    try {        return ParDoTranslation.usesStateOrTimers(appliedTransform);    } catch (IOException e) {        throw new RuntimeException(e);    }}
 static DoFn<?, ?> beam_f12271_0(AppliedPTransform<?, ?, ?> appliedTransform)
{    try {        DoFn<?, ?> doFn = ParDoTranslation.getDoFn(appliedTransform);        if (DoFnSignatures.signatureForDoFn(doFn).processElement().isSplittable()) {            throw new IllegalStateException("Not expected to directly translate splittable DoFn, should have been overridden: " +             doFn);        }        return doFn;    } catch (IOException e) {        throw new RuntimeException(e);    }}
public int beam_f12279_0()
{    if (hash == 0) {        hash = Arrays.hashCode(value);    }    return hash;}
public Iterable<Class<? extends PipelineRunner<?>>> beam_f12280_0()
{    return ImmutableList.of(TestJetRunner.class);}
public Iterable<Class<? extends PipelineOptions>> beam_f12281_0()
{    return ImmutableList.of(JetPipelineOptions.class);}
public static ProcessorMetaSupplier beam_f12289_0(byte[] payload, TestStream.TestStreamCoder payloadCoder, Coder outputCoder)
{    return ProcessorMetaSupplier.forceTotalParallelismOne(ProcessorSupplier.of(() -> new ThrottleWrappedP(new TestStreamP(payload, payloadCoder, outputCoder), 4)));}
private static TestStream beam_f12290_0(byte[] payload, TestStream.TestStreamCoder coder)
{    try {        return (TestStream) CoderUtils.decodeFromByteArray(coder, payload);    } catch (CoderException e) {        throw ExceptionUtil.rethrow(e);    }}
public boolean beam_f12291_0()
{    return emitFromTraverser(traverser);}
public void beam_f12299_0()
{    Assert.assertThat(StructuralKey.of(1234, VarIntCoder.of()).getKey(), Matchers.equalTo(1234));    Assert.assertThat(StructuralKey.of("foobar", StringUtf8Coder.of()).getKey(), Matchers.equalTo("foobar"));    assertArrayEquals(StructuralKey.of(new byte[] { 2, 9, -22 }, ByteArrayCoder.of()).getKey(), new byte[] { 2, 9, -22 });}
public void beam_f12300_0()
{    byte[] original = new byte[] { 1, 4, 9, 127, -22 };    StructuralKey<byte[]> key = StructuralKey.of(original, ByteArrayCoder.of());    Assert.assertThat(key.getKey(), Matchers.not(Matchers.theInstance(original)));}
public void beam_f12301_0()
{    StructuralKey<?> empty = StructuralKey.empty();    Assert.assertThat(empty, Matchers.not(Matchers.equalTo(StructuralKey.empty())));    Assert.assertThat(empty, Matchers.equalTo(empty));}
public void beam_f12309_1(StartWorkerRequest request, StreamObserver<StartWorkerResponse> responseObserver)
{            Thread th = new Thread(() -> {        try {            FnHarness.main(request.getWorkerId(), options, request.getLoggingEndpoint(), request.getControlEndpoint());                    } catch (Exception exn) {                    }    });    th.setName("SDK-worker-" + request.getWorkerId());    th.setDaemon(true);    th.start();    responseObserver.onNext(StartWorkerResponse.newBuilder().build());    responseObserver.onCompleted();}
public GrpcFnServer<ExternalWorkerService> beam_f12311_1() throws Exception
{    GrpcFnServer<ExternalWorkerService> server = GrpcFnServer.allocatePortAndCreateFor(this, serverFactory);        return server;}
public State beam_f12312_0()
{    if (terminationState != null) {        return terminationState;    }    JobServiceBlockingStub stub = jobService.get();    GetJobStateResponse response = stub.getState(GetJobStateRequest.newBuilder().setJobIdBytes(jobId).build());    return getJavaState(response.getState());}
 static PortableRunner beam_f12320_1(PipelineOptions options, ManagedChannelFactory channelFactory)
{    PortablePipelineOptions portableOptions = PipelineOptionsValidator.validate(PortablePipelineOptions.class, options);    String endpoint = portableOptions.getJobEndpoint();        Set<String> pathsToStage = Sets.newHashSet();    if (portableOptions.getFilesToStage() == null) {        pathsToStage.addAll(detectClassPathResourcesToStage(PortableRunner.class.getClassLoader()));        if (pathsToStage.isEmpty()) {            throw new IllegalArgumentException("No classpath elements found.");        }            } else {        pathsToStage.addAll(portableOptions.getFilesToStage());    }    ImmutableList.Builder<StagedFile> filesToStage = ImmutableList.builder();    for (String path : pathsToStage) {        File file = new File(path);        if (new File(path).exists()) {                        if (file.isDirectory()) {                                try {                    filesToStage.add(createStagingFile(zipDirectory(file)));                } catch (IOException e) {                    throw new RuntimeException(e);                }            } else {                filesToStage.add(createStagingFile(file));            }        }    }    return new PortableRunner(options, endpoint, filesToStage.build(), channelFactory);}
public PipelineResult beam_f12321_1(Pipeline pipeline)
{    pipeline.replaceAll(ImmutableList.of(JavaReadViaImpulse.boundedOverride()));    Runnable cleanup;    if (Environments.ENVIRONMENT_LOOPBACK.equals(options.as(PortablePipelineOptions.class).getDefaultEnvironmentType())) {        GrpcFnServer<ExternalWorkerService> workerService;        try {            workerService = new ExternalWorkerService(options).start();        } catch (Exception exn) {            throw new RuntimeException("Failed to start GrpcFnServer for ExternalWorkerService", exn);        }                options.as(PortablePipelineOptions.class).setDefaultEnvironmentConfig(workerService.getApiServiceDescriptor().getUrl());        cleanup = () -> {            try {                                workerService.close();            } catch (Exception exn) {                throw new RuntimeException(exn);            }        };    } else {        cleanup = null;    }        PrepareJobRequest prepareJobRequest = PrepareJobRequest.newBuilder().setJobName(options.getJobName()).setPipeline(PipelineTranslation.toProto(pipeline)).setPipelineOptions(PipelineOptionsTranslation.toProto(options)).build();        ManagedChannel jobServiceChannel = channelFactory.forDescriptor(ApiServiceDescriptor.newBuilder().setUrl(endpoint).build());    JobServiceBlockingStub jobService = JobServiceGrpc.newBlockingStub(jobServiceChannel);    try (CloseableResource<JobServiceBlockingStub> wrappedJobService = CloseableResource.of(jobService, unused -> jobServiceChannel.shutdown())) {        PrepareJobResponse prepareJobResponse = jobService.prepare(prepareJobRequest);                ApiServiceDescriptor artifactStagingEndpoint = prepareJobResponse.getArtifactStagingEndpoint();        String stagingSessionToken = prepareJobResponse.getStagingSessionToken();        String retrievalToken = null;        try (CloseableResource<ManagedChannel> artifactChannel = CloseableResource.of(channelFactory.forDescriptor(artifactStagingEndpoint), ManagedChannel::shutdown)) {            ArtifactServiceStager stager = ArtifactServiceStager.overChannel(artifactChannel.get());                        retrievalToken = stager.stage(stagingSessionToken, filesToStage);        } catch (CloseableResource.CloseException e) {                                    checkState(retrievalToken != null);        } catch (Exception e) {            throw new RuntimeException("Error staging files.", e);        }        RunJobRequest runJobRequest = RunJobRequest.newBuilder().setPreparationId(prepareJobResponse.getPreparationId()).setRetrievalToken(retrievalToken).build();        RunJobResponse runJobResponse = jobService.run(runJobRequest);                ByteString jobId = runJobResponse.getJobIdBytes();        return new JobServicePipelineResult(jobId, wrappedJobService.transfer(), cleanup);    } catch (CloseException e) {        throw new RuntimeException(e);    }}
public String beam_f12322_0()
{    return "PortableRunner#" + hashCode();}
public String[] beam_f12330_0(PipelineOptions options)
{    return new String[0];}
public Iterable<Class<? extends PipelineOptions>> beam_f12331_0()
{    return ImmutableList.of(TestPortablePipelineOptions.class);}
public static TestPortableRunner beam_f12332_0(PipelineOptions options)
{    return new TestPortableRunner(options.as(PortablePipelineOptions.class));}
public void beam_f12340_0()
{    CloseableResource<Foo> foo = CloseableResource.of(new Foo(), unused -> {    });    foo.transfer();    thrown.expect(IllegalStateException.class);    foo.transfer();}
public void beam_f12341_0() throws Exception
{    try (CloseableResource<Server> server = createJobServer(JobState.Enum.DONE)) {        PortableRunner runner = PortableRunner.create(options, InProcessManagedChannelFactory.create());        State state = runner.run(p).waitUntilFinish();        assertThat(state, is(State.DONE));    }}
private static CloseableResource<Server> beam_f12342_0(JobState.Enum jobState) throws IOException
{    CloseableResource<Server> server = CloseableResource.of(InProcessServerBuilder.forName(ENDPOINT_URL).addService(new TestJobService(ENDPOINT_DESCRIPTOR, "prepId", "jobId", jobState)).addService(new InMemoryArtifactStagerService()).build(), Server::shutdown);    server.get().start();    return server;}
public void beam_f12350_0(SystemStreamPartition ssp, String offset)
{    final int partitionId = ssp.getPartition().getPartitionId();    try {        final BoundedReader<T> reader = splits.get(partitionId).createReader(pipelineOptions);        readerToSsp.put(reader, ssp);    } catch (Exception e) {        throw new SamzaException("Error while creating source reader for ssp: " + ssp, e);    }}
public Map<SystemStreamPartition, List<IncomingMessageEnvelope>> beam_f12351_0(Set<SystemStreamPartition> systemStreamPartitions, long timeout) throws InterruptedException
{        assert !readerToSsp.isEmpty();    final Map<SystemStreamPartition, List<IncomingMessageEnvelope>> envelopes = new HashMap<>();    for (SystemStreamPartition ssp : systemStreamPartitions) {        envelopes.put(ssp, readerTask.getNextMessages(ssp, timeout));    }    return envelopes;}
public void beam_f12352_1()
{    readerThread = Thread.currentThread();    final Set<BoundedReader<T>> availableReaders = new HashSet<>(readerToSsp.keySet());    try {        for (BoundedReader<T> reader : readerToSsp.keySet()) {            boolean hasData = invoke(reader::start);            if (hasData) {                enqueueMessage(reader);            } else {                enqueueMaxWatermarkAndEndOfStream(reader);                reader.close();                availableReaders.remove(reader);            }        }        while (!stopInvoked && !availableReaders.isEmpty()) {            final Iterator<BoundedReader<T>> iter = availableReaders.iterator();            while (iter.hasNext()) {                final BoundedReader<T> reader = iter.next();                final boolean hasData = invoke(reader::advance);                if (hasData) {                    enqueueMessage(reader);                } else {                    enqueueMaxWatermarkAndEndOfStream(reader);                    reader.close();                    iter.remove();                }            }        }    } catch (InterruptedException e) {            } catch (Exception e) {        setError(e);    } finally {        availableReaders.forEach(reader -> {            try {                reader.close();            } catch (IOException e) {                            }        });    }}
public SystemConsumer beam_f12360_0(String systemName, Config config, MetricsRegistry registry)
{    final String streamPrefix = "systems." + systemName;    final Config scopedConfig = config.subset(streamPrefix + ".", true);    return new Consumer<T>(getBoundedSource(scopedConfig), getPipelineOptions(config), new SamzaMetricsContainer((MetricsRegistryMap) registry), scopedConfig.get("stepName"));}
public SystemProducer beam_f12361_1(String systemName, Config config, MetricsRegistry registry)
{        return null;}
public SystemAdmin beam_f12362_0(String systemName, Config config)
{    final Config scopedConfig = config.subset("systems." + systemName + ".", true);    return new Admin<T>(getBoundedSource(scopedConfig), getPipelineOptions(config));}
public void beam_f12370_0()
{    if (this.readerToSsp.isEmpty()) {        throw new IllegalArgumentException("Attempted to call start without assigned system stream partitions");    }    final FnWithMetricsWrapper metricsWrapper = pipelineOptions.getEnableMetrics() ? new FnWithMetricsWrapper(metricsContainer, stepName) : null;    readerTask = new ReaderTask<>(readerToSsp, checkpointMarkCoder, pipelineOptions.getSystemBufferSize(), pipelineOptions.getWatermarkInterval(), metricsWrapper);    final Thread thread = new Thread(readerTask, "unbounded-source-system-consumer-" + NEXT_ID.getAndIncrement());    thread.start();}
public void beam_f12371_0()
{        readerTask.stop();}
public void beam_f12372_0(SystemStreamPartition ssp, String offset)
{    CheckpointMarkT checkpoint = null;    if (StringUtils.isNoneEmpty(offset)) {        final byte[] offsetBytes = Base64.getDecoder().decode(offset);        final ByteArrayInputStream bais = new ByteArrayInputStream(offsetBytes);        try {            checkpoint = checkpointMarkCoder.decode(bais);        } catch (Exception e) {            throw new SamzaException("Error in decode offset", e);        }    }        final int partitionId = ssp.getPartition().getPartitionId();    try {        final UnboundedReader reader = splits.get(partitionId).createReader(pipelineOptions, checkpoint);        readerToSsp.put(reader, ssp);    } catch (Exception e) {        throw new SamzaException("Error while creating source reader for ssp: " + ssp, e);    }}
 List<IncomingMessageEnvelope> beam_f12380_0(SystemStreamPartition ssp, long timeoutMillis) throws InterruptedException
{    if (lastException != null) {        throw new RuntimeException(lastException);    }    final List<IncomingMessageEnvelope> envelopes = new ArrayList<>();    final BlockingQueue<IncomingMessageEnvelope> queue = queues.get(ssp);    final IncomingMessageEnvelope envelope = queue.poll(timeoutMillis, TimeUnit.MILLISECONDS);    if (envelope != null) {        envelopes.add(envelope);        queue.drainTo(envelopes);    }    final int numElements = (int) envelopes.stream().filter(ev -> (ev.getMessage() instanceof OpMessage)).count();    available.release(numElements);    if (lastException != null) {        throw new RuntimeException(lastException);    }    return envelopes;}
private String beam_f12381_0(UnboundedReader reader)
{    try {        final ByteArrayOutputStream baos = new ByteArrayOutputStream();        @SuppressWarnings("unchecked")        final CheckpointMarkT checkpointMark = (CheckpointMarkT) invoke(reader::getCheckpointMark);        checkpointMarkCoder.encode(checkpointMark, baos);        return Base64.getEncoder().encodeToString(baos.toByteArray());    } catch (Exception e) {        throw new RuntimeException(e);    }}
public SystemConsumer beam_f12382_0(String systemName, Config config, MetricsRegistry registry)
{    final String streamPrefix = "systems." + systemName;    final Config scopedConfig = config.subset(streamPrefix + ".", true);    return new Consumer<T, CheckpointMarkT>(getUnboundedSource(scopedConfig), getPipelineOptions(config), new SamzaMetricsContainer((MetricsRegistryMap) registry), scopedConfig.get("stepName"));}
public void beam_f12391_1()
{            }
public boolean beam_f12392_1(Duration timeout)
{                return true;}
public Config beam_f12393_1(URI configUri)
{    if (jobModel == null) {        synchronized (LOCK) {            if (jobModel == null) {                String containerId = System.getenv(ShellCommandConfig.ENV_CONTAINER_ID());                                String coordinatorUrl = System.getenv(ShellCommandConfig.ENV_COORDINATOR_URL());                                int delay = new Random().nextInt(SamzaContainer.DEFAULT_READ_JOBMODEL_DELAY_MS()) + 1;                jobModel = SamzaContainer.readJobModel(coordinatorUrl, delay);            }        }    }    final Map<String, String> config = new HashMap<>(jobModel.getConfig());    config.put("app.runner.class", BeamContainerRunner.class.getName());    return new MapConfig(config);}
public T beam_f12401_0(SupplierWithException<T> fn) throws Exception
{    try (Closeable closeable = MetricsEnvironment.scopedMetricsContainer(metricsContainer.getContainer(stepName))) {        T result = fn.get();        metricsContainer.updateMetrics();        return result;    }}
public MetricsContainer beam_f12402_0(String stepName)
{    return this.metricsContainers.getContainer(stepName);}
public MetricsContainerStepMap beam_f12403_0()
{    return this.metricsContainers;}
public void beam_f12411_0(WindowedValue<InT> inputElement, OpEmitter<OutT> emitter)
{    pushbackFnRunner.startBundle();    final Iterable<WindowedValue<InT>> rejectedValues = pushbackFnRunner.processElementInReadyWindows(inputElement);    for (WindowedValue<InT> rejectedValue : rejectedValues) {        if (rejectedValue.getTimestamp().compareTo(pushbackWatermarkHold) < 0) {            pushbackWatermarkHold = rejectedValue.getTimestamp();        }        pushbackValues.add(rejectedValue);    }    pushbackFnRunner.finishBundle();}
public void beam_f12412_0(Instant watermark, OpEmitter<OutT> emitter)
{    this.inputWatermark = watermark;    if (sideInputWatermark.isEqual(BoundedWindow.TIMESTAMP_MAX_VALUE)) {                emitAllPushbackValues();    }    final Instant actualInputWatermark = pushbackWatermarkHold.isBefore(inputWatermark) ? pushbackWatermarkHold : inputWatermark;    timerInternalsFactory.setInputWatermark(actualInputWatermark);    pushbackFnRunner.startBundle();    for (KeyedTimerData<?> keyedTimerData : timerInternalsFactory.removeReadyTimers()) {        fireTimer(keyedTimerData);    }    pushbackFnRunner.finishBundle();    if (timerInternalsFactory.getOutputWatermark() == null || timerInternalsFactory.getOutputWatermark().isBefore(actualInputWatermark)) {        timerInternalsFactory.setOutputWatermark(actualInputWatermark);        emitter.emitWatermark(timerInternalsFactory.getOutputWatermark());    }}
public void beam_f12413_0(String id, WindowedValue<? extends Iterable<?>> elements, OpEmitter<OutT> emitter)
{    @SuppressWarnings("unchecked")    final WindowedValue<Iterable<?>> retypedElements = (WindowedValue<Iterable<?>>) elements;    final PCollectionView<?> view = idToViewMap.get(id);    if (view == null) {        throw new IllegalArgumentException("No mapping of id " + id + " to view.");    }    sideInputHandler.addSideInputValue(view, retypedElements);    final List<WindowedValue<InT>> previousPushbackValues = new ArrayList<>(pushbackValues);    pushbackWatermarkHold = BoundedWindow.TIMESTAMP_MAX_VALUE;    pushbackValues.clear();    for (final WindowedValue<InT> value : previousPushbackValues) {        processElement(value, emitter);    }            processWatermark(this.inputWatermark, emitter);}
public DoFnRunners.OutputManager beam_f12421_0(OpEmitter<RawUnionValue> emitter)
{    return new DoFnRunners.OutputManager() {        @Override        public <T> void output(TupleTag<T> tupleTag, WindowedValue<T> windowedValue) {            final int index = tagToIndexMap.get(tupleTag);            final T rawValue = windowedValue.getValue();            final RawUnionValue rawUnionValue = new RawUnionValue(index, rawValue);            emitter.emitElement(windowedValue.withValue(rawUnionValue));        }    };}
public void beam_f12422_0(TupleTag<T> tupleTag, WindowedValue<T> windowedValue)
{    final int index = tagToIndexMap.get(tupleTag);    final T rawValue = windowedValue.getValue();    final RawUnionValue rawUnionValue = new RawUnionValue(index, rawValue);    emitter.emitElement(windowedValue.withValue(rawUnionValue));}
public void beam_f12423_0()
{    underlying.startBundle();}
public void beam_f12431_0(Config config, Context context, Scheduler<KeyedTimerData<K>> timerRegistry, OpEmitter<KV<K, OutputT>> emitter)
{    this.pipelineOptions = Base64Serializer.deserializeUnchecked(config.get("beamPipelineOptions"), SerializablePipelineOptions.class).get().as(SamzaPipelineOptions.class);    final SamzaStoreStateInternals.Factory<?> nonKeyedStateInternalsFactory = SamzaStoreStateInternals.createStateInternalFactory(stepId, null, context.getTaskContext(), pipelineOptions, null);    final DoFnRunners.OutputManager outputManager = outputManagerFactory.create(emitter);    this.stateInternalsFactory = new SamzaStoreStateInternals.Factory<>(stepId, Collections.singletonMap(SamzaStoreStateInternals.BEAM_STORE, SamzaStoreStateInternals.getBeamStore(context.getTaskContext())), keyCoder, pipelineOptions.getStoreBatchGetSize());    this.timerInternalsFactory = SamzaTimerInternalsFactory.createTimerInternalFactory(keyCoder, timerRegistry, TIMER_STATE_ID, nonKeyedStateInternalsFactory, windowingStrategy, isBounded, pipelineOptions);    final DoFn<KeyedWorkItem<K, InputT>, KV<K, OutputT>> doFn = GroupAlsoByWindowViaWindowSetNewDoFn.create(windowingStrategy, stateInternalsFactory, timerInternalsFactory, NullSideInputReader.of(Collections.emptyList()), reduceFn, outputManager, mainOutputTag);    final KeyedInternals<K> keyedInternals = new KeyedInternals<>(stateInternalsFactory, timerInternalsFactory);    final StepContext stepContext = new StepContext() {        @Override        public StateInternals stateInternals() {            return keyedInternals.stateInternals();        }        @Override        public TimerInternals timerInternals() {            return keyedInternals.timerInternals();        }    };    final DoFnRunner<KeyedWorkItem<K, InputT>, KV<K, OutputT>> doFnRunner = DoFnRunners.simpleRunner(PipelineOptionsFactory.create(), doFn, NullSideInputReader.of(Collections.emptyList()), outputManager, mainOutputTag, Collections.emptyList(), stepContext, null, Collections.emptyMap(), windowingStrategy, DoFnSchemaInformation.create(), Collections.emptyMap());    final SamzaExecutionContext executionContext = (SamzaExecutionContext) context.getApplicationContainerContext();    this.fnRunner = DoFnRunnerWithMetrics.wrap(doFnRunner, executionContext.getMetricsContainer(), stepName);}
public StateInternals beam_f12432_0()
{    return keyedInternals.stateInternals();}
public TimerInternals beam_f12433_0()
{    return keyedInternals.timerInternals();}
 K beam_f12441_0()
{    KeyedStates<K> keyedStates = threadLocalKeyedStates.get();    return keyedStates == null ? null : keyedStates.key;}
 void beam_f12442_0()
{    final List<State> states = threadLocalKeyedStates.get().states;    states.forEach(state -> {        if (state instanceof SamzaStoreStateInternals.KeyValueIteratorState) {            ((SamzaStoreStateInternals.KeyValueIteratorState) state).closeIterators();        }    });    states.clear();    threadLocalKeyedStates.remove();}
public K beam_f12443_0()
{    return KeyedInternals.this.getKey();}
public Instant beam_f12451_0()
{    return getInternals().currentProcessingTime();}
public Instant beam_f12452_0()
{    return getInternals().currentSynchronizedProcessingTime();}
public Instant beam_f12453_0()
{    return getInternals().currentInputWatermarkTime();}
public void beam_f12461_0(KeyedTimerData<K> value, OutputStream outStream) throws CoderException, IOException
{    final TimerData timer = value.getTimerData();        INSTANT_CODER.encode(timer.getTimestamp(), outStream);    STRING_CODER.encode(timer.getTimerId(), outStream);    STRING_CODER.encode(timer.getNamespace().stringKey(), outStream);    STRING_CODER.encode(timer.getDomain().name(), outStream);    if (keyCoder != null) {        keyCoder.encode(value.key, outStream);    }}
public KeyedTimerData<K> beam_f12462_0(InputStream inStream) throws CoderException, IOException
{        final Instant timestamp = INSTANT_CODER.decode(inStream);    final String timerId = STRING_CODER.decode(inStream);    final StateNamespace namespace = StateNamespaces.fromString(STRING_CODER.decode(inStream), windowCoder);    final TimeDomain domain = TimeDomain.valueOf(STRING_CODER.decode(inStream));    final TimerData timer = TimerData.of(timerId, namespace, timestamp, domain);    byte[] keyBytes = null;    K key = null;    if (keyCoder != null) {        key = keyCoder.decode(inStream);        final ByteArrayOutputStream baos = new ByteArrayOutputStream();        try {            keyCoder.encode(key, baos);        } catch (IOException e) {            throw new RuntimeException("Could not encode key: " + key, e);        }        keyBytes = baos.toByteArray();    }    return new KeyedTimerData(keyBytes, key, timer);}
public List<? extends Coder<?>> beam_f12463_0()
{    return Arrays.asList(keyCoder, windowCoder);}
public Collection<OpMessage<OutT>> beam_f12476_1(long time)
{    assert outputList.isEmpty();    try {        op.processWatermark(new Instant(time), emitter);    } catch (Exception e) {                throw UserCodeException.wrap(e);    }    final List<OpMessage<OutT>> results = new ArrayList<>(outputList);    outputList.clear();    return results;}
public Long beam_f12477_0()
{    return outputWatermark != null ? outputWatermark.getMillis() : null;}
public Collection<OpMessage<OutT>> beam_f12478_1(KeyedTimerData<K> keyedTimerData, long time)
{    assert outputList.isEmpty();    try {        op.processTimer(keyedTimerData);    } catch (Exception e) {                throw UserCodeException.wrap(e);    }    final List<OpMessage<OutT>> results = new ArrayList<>(outputList);    outputList.clear();    return results;}
public Type beam_f12486_0()
{    return type;}
public WindowedValue<T> beam_f12487_0()
{    ensureType(Type.ELEMENT, "getElement");    return element;}
public String beam_f12488_0()
{    ensureType(Type.SIDE_INPUT, "getViewId");    return viewId;}
public Instant beam_f12496_0()
{    return value.getTimestamp();}
public BoundedWindow beam_f12497_0()
{    return Iterables.getOnlyElement(value.getWindows());}
public static DoFnRunner<InT, FnOutT> beam_f12498_0(SamzaPipelineOptions pipelineOptions, DoFn<InT, FnOutT> doFn, WindowingStrategy<?, ?> windowingStrategy, String stepName, String stateId, Context context, TupleTag<FnOutT> mainOutputTag, SideInputHandler sideInputHandler, SamzaTimerInternalsFactory<?> timerInternalsFactory, Coder<?> keyCoder, DoFnRunners.OutputManager outputManager, Coder<InT> inputCoder, List<TupleTag<?>> sideOutputTags, Map<TupleTag<?>, Coder<?>> outputCoders, DoFnSchemaInformation doFnSchemaInformation, Map<String, PCollectionView<?>> sideInputMapping)
{    final KeyedInternals keyedInternals;    final TimerInternals timerInternals;    final StateInternals stateInternals;    final DoFnSignature signature = DoFnSignatures.getSignature(doFn.getClass());    final SamzaStoreStateInternals.Factory<?> stateInternalsFactory = SamzaStoreStateInternals.createStateInternalFactory(stateId, keyCoder, context.getTaskContext(), pipelineOptions, signature);    final SamzaExecutionContext executionContext = (SamzaExecutionContext) context.getApplicationContainerContext();    if (signature.usesState()) {        keyedInternals = new KeyedInternals(stateInternalsFactory, timerInternalsFactory);        stateInternals = keyedInternals.stateInternals();        timerInternals = keyedInternals.timerInternals();    } else {        keyedInternals = null;        stateInternals = stateInternalsFactory.stateInternalsForKey(null);        timerInternals = timerInternalsFactory.timerInternalsForKey(null);    }    final DoFnRunner<InT, FnOutT> underlyingRunner = DoFnRunners.simpleRunner(pipelineOptions, doFn, sideInputHandler, outputManager, mainOutputTag, sideOutputTags, createStepContext(stateInternals, timerInternals), inputCoder, outputCoders, windowingStrategy, doFnSchemaInformation, sideInputMapping);    final DoFnRunner<InT, FnOutT> doFnRunnerWithMetrics = pipelineOptions.getEnableMetrics() ? DoFnRunnerWithMetrics.wrap(underlyingRunner, executionContext.getMetricsContainer(), stepName) : underlyingRunner;    if (keyedInternals != null) {        final DoFnRunner<InT, FnOutT> statefulDoFnRunner = DoFnRunners.defaultStatefulDoFnRunner(doFn, doFnRunnerWithMetrics, windowingStrategy, new StatefulDoFnRunner.TimeInternalsCleanupTimer(timerInternals, windowingStrategy), createStateCleaner(doFn, windowingStrategy, keyedInternals.stateInternals()));        return new DoFnRunnerWithKeyedInternals<>(statefulDoFnRunner, keyedInternals);    } else {        return doFnRunnerWithMetrics;    }}
public DoFn<InT, FnOutT> beam_f12509_0()
{    throw new UnsupportedOperationException();}
 static KeyValueStore<byte[], byte[]> beam_f12510_0(TaskContext context)
{    return (KeyValueStore<byte[], byte[]>) context.getStore(SamzaStoreStateInternals.BEAM_STORE);}
 static Factory beam_f12511_0(String id, Coder<?> keyCoder, TaskContext context, SamzaPipelineOptions pipelineOptions, DoFnSignature signature)
{    final int batchGetSize = pipelineOptions.getStoreBatchGetSize();    final Map<String, KeyValueStore<byte[], byte[]>> stores = new HashMap<>();    stores.put(BEAM_STORE, getBeamStore(context));    final Coder stateKeyCoder;    if (keyCoder != null) {        signature.stateDeclarations().keySet().forEach(stateId -> stores.put(stateId, (KeyValueStore<byte[], byte[]>) context.getStore(stateId)));        stateKeyCoder = keyCoder;    } else {        stateKeyCoder = VoidCoder.of();    }    return new Factory<>(Objects.toString(id), stores, stateKeyCoder, batchGetSize);}
public CombiningState<InputT, AccumT, OutputT> beam_f12519_0(StateTag<CombiningState<InputT, AccumT, OutputT>> spec, Coder<AccumT> accumCoder, Combine.CombineFn<InputT, AccumT, OutputT> combineFn)
{    return new SamzaAccumulatorCombiningState<>(namespace, address, accumCoder, combineFn);}
public CombiningState<InputT, AccumT, OutputT> beam_f12520_0(StateTag<CombiningState<InputT, AccumT, OutputT>> spec, Coder<AccumT> accumCoder, CombineWithContext.CombineFnWithContext<InputT, AccumT, OutputT> combineFn)
{    throw new UnsupportedOperationException(String.format("%s is not supported", CombiningState.class.getSimpleName()));}
public WatermarkHoldState beam_f12521_0(StateTag<WatermarkHoldState> spec, TimestampCombiner timestampCombiner)
{    return new SamzaWatermarkHoldState(namespace, address, timestampCombiner);}
public ReadableState<Boolean> beam_f12529_0()
{    return this;}
protected byte[] beam_f12530_0()
{    return encodedStoreKey;}
protected byte[] beam_f12531_0(T value)
{    final ByteArrayOutputStream baos = getThreadLocalBaos();    try {        coder.encode(value, baos);    } catch (IOException e) {        throw new RuntimeException("Could not encode state value: " + value, e);    }    return baos.toByteArray();}
public void beam_f12539_0(T value)
{    synchronized (store) {        final int size = getSize();        final byte[] encodedKey = encodeKey(size);        store.put(encodedKey, encodeValue(value));        store.put(getEncodedStoreKey(), Ints.toByteArray(size + 1));    }}
public ReadableState<Boolean> beam_f12540_0()
{    synchronized (store) {        return isEmptyInternal();    }}
public List<T> beam_f12541_0()
{    synchronized (store) {        final int size = getSize();        if (size == 0) {            return Collections.emptyList();        }        final List<T> values = new ArrayList<>(size);        final List<byte[]> keys = new ArrayList<>(size);        int start = 0;        while (start < size) {            final int end = Math.min(size, start + batchGetSize);            for (int i = start; i < end; i++) {                keys.add(encodeKey(i));            }            store.getAll(keys).values().forEach(value -> values.add(decodeValue(value)));            start += batchGetSize;            keys.clear();        }        return values;    }}
public void beam_f12549_0(T value)
{    mapState.put(value, true);}
public ReadableState<Boolean> beam_f12550_0()
{    return new ReadableState<Boolean>() {        @Override        public Boolean read() {            return Iterables.isEmpty(mapState.entries().read());        }        @Override        public ReadableState<Boolean> readLater() {            return this;        }    };}
public Boolean beam_f12551_0()
{    return Iterables.isEmpty(mapState.entries().read());}
public T beam_f12559_0()
{    return iter.next().getKey();}
public ReadableState<Iterator<T>> beam_f12560_0()
{    return this;}
public void beam_f12561_0()
{    mapState.closeIterators();}
public ReadableState<Iterable<ValueT>> beam_f12569_0()
{    return new ReadableState<Iterable<ValueT>>() {        @Override        public Iterable<ValueT> read() {            return createIterable(entry -> decodeValue(entry.getValue()));        }        @Override        public ReadableState<Iterable<ValueT>> readLater() {            return this;        }    };}
public Iterable<ValueT> beam_f12570_0()
{    return createIterable(entry -> decodeValue(entry.getValue()));}
public ReadableState<Iterable<ValueT>> beam_f12571_0()
{    return this;}
public ReadableState<Iterator<Map.Entry<KeyT, ValueT>>> beam_f12579_0()
{    return this;}
private Iterable<OutputT> beam_f12580_0(SerializableFunction<org.apache.samza.storage.kv.Entry<byte[], byte[]>, OutputT> fn)
{    final byte[] maxKey = createMaxKey();    final KeyValueIterator<byte[], byte[]> kvIter = store.range(getEncodedStoreKey(), maxKey);    final List<Entry<byte[], byte[]>> iterable = ImmutableList.copyOf(kvIter);    kvIter.close();    return new Iterable<OutputT>() {        @Override        public Iterator<OutputT> iterator() {            final Iterator<Entry<byte[], byte[]>> iter = iterable.iterator();            return new Iterator<OutputT>() {                @Override                public boolean hasNext() {                    return iter.hasNext();                }                @Override                public OutputT next() {                    return fn.apply(iter.next());                }            };        }    };}
public Iterator<OutputT> beam_f12581_0()
{    final Iterator<Entry<byte[], byte[]>> iter = iterable.iterator();    return new Iterator<OutputT>() {        @Override        public boolean hasNext() {            return iter.hasNext();        }        @Override        public OutputT next() {            return fn.apply(iter.next());        }    };}
public void beam_f12589_0()
{    clearInternal();}
public void beam_f12590_0(InT value)
{    final AccumT accum = getAccum();    final AccumT current = combineFn.addInput(accum, value);    writeInternal(current);}
public ReadableState<Boolean> beam_f12591_0()
{    return isEmptyInternal();}
public Instant beam_f12599_0()
{    return readInternal();}
public TimestampCombiner beam_f12600_0()
{    return this.timestampCombiner;}
public WatermarkHoldState beam_f12601_0()
{    return this;}
public Instant beam_f12609_0()
{    return inputWatermark;}
public Instant beam_f12610_0()
{    return outputWatermark;}
public void beam_f12611_0(StateNamespace namespace, String timerId, Instant target, TimeDomain timeDomain)
{    setTimer(TimerData.of(timerId, namespace, target, timeDomain));}
public Instant beam_f12619_0()
{    return outputWatermark;}
 void beam_f12620_0(KeyedTimerData<K> keyedTimerData)
{    switch(keyedTimerData.getTimerData().getDomain()) {        case EVENT_TIME:            if (!eventTimeTimers.contains(keyedTimerData)) {                eventTimerTimerState.add(keyedTimerData);            }            break;        case PROCESSING_TIME:            processingTimerTimerState.add(keyedTimerData);            break;        default:            throw new UnsupportedOperationException(String.format("%s currently only supports event time", SamzaRunner.class));    }}
 void beam_f12621_0(KeyedTimerData<K> keyedTimerData)
{    switch(keyedTimerData.getTimerData().getDomain()) {        case EVENT_TIME:            eventTimerTimerState.remove(keyedTimerData);            break;        case PROCESSING_TIME:            processingTimerTimerState.remove(keyedTimerData);            break;        default:            throw new UnsupportedOperationException(String.format("%s currently only supports event time", SamzaRunner.class));    }}
public SamzaPipelineOptions beam_f12629_0()
{    return options;}
public SamzaMetricsContainer beam_f12630_0()
{    return this.metricsContainer;}
 void beam_f12631_0(SamzaMetricsContainer metricsContainer)
{    this.metricsContainer = metricsContainer;}
public static SamzaJobServerDriver beam_f12639_0(SamzaPortablePipelineOptions pipelineOptions)
{    Map<String, String> overrideConfig = pipelineOptions.getConfigOverride() != null ? pipelineOptions.getConfigOverride() : new HashMap<>();    overrideConfig.put(SamzaRunnerOverrideConfigs.IS_PORTABLE_MODE, String.valueOf(true));    overrideConfig.put(SamzaRunnerOverrideConfigs.FN_CONTROL_PORT, String.valueOf(pipelineOptions.getControlPort()));    pipelineOptions.setConfigOverride(overrideConfig);    return new SamzaJobServerDriver(pipelineOptions);}
private static InMemoryJobService beam_f12640_0(SamzaPortablePipelineOptions pipelineOptions) throws IOException
{    JobInvoker jobInvoker = new JobInvoker("samza-job-invoker") {        @Override        protected JobInvocation invokeWithExecutor(RunnerApi.Pipeline pipeline, Struct options, @Nullable String retrievalToken, ListeningExecutorService executorService) throws IOException {            String invocationId = String.format("%s_%s", pipelineOptions.getJobName(), UUID.randomUUID().toString());            SamzaPipelineRunner pipelineRunner = new SamzaPipelineRunner(pipelineOptions);            JobInfo jobInfo = JobInfo.create(invocationId, pipelineOptions.getJobName(), retrievalToken, PipelineOptionsTranslation.toProto(pipelineOptions));            return new JobInvocation(jobInfo, executorService, pipeline, pipelineRunner);        }    };    return InMemoryJobService.create(null, (String session) -> {        try {            return BeamFileSystemArtifactStagingService.generateStagingSessionToken(session, "/tmp/beam-artifact-staging");        } catch (Exception exn) {            throw new RuntimeException(exn);        }    }, stagingSessionToken -> {    }, jobInvoker);}
protected JobInvocation beam_f12641_0(RunnerApi.Pipeline pipeline, Struct options, @Nullable String retrievalToken, ListeningExecutorService executorService) throws IOException
{    String invocationId = String.format("%s_%s", pipelineOptions.getJobName(), UUID.randomUUID().toString());    SamzaPipelineRunner pipelineRunner = new SamzaPipelineRunner(pipelineOptions);    JobInfo jobInfo = JobInfo.create(invocationId, pipelineOptions.getJobName(), retrievalToken, PipelineOptionsTranslation.toProto(pipelineOptions));    return new JobInvocation(jobInfo, executorService, pipeline, pipelineRunner);}
private StateInfo beam_f12649_1()
{    final ApplicationStatus status = runner.status();    switch(status.getStatusCode()) {        case New:            return new StateInfo(State.STOPPED);        case Running:            return new StateInfo(State.RUNNING);        case SuccessfulFinish:            return new StateInfo(State.DONE);        case UnsuccessfulFinish:                        return new StateInfo(State.FAILED, new Pipeline.PipelineExecutionException(getUserCodeException(status.getThrowable())));        default:            return new StateInfo(State.UNKNOWN);    }}
private static Throwable beam_f12650_0(Throwable throwable)
{    Throwable t = throwable;    while (t != null) {        if (t instanceof UserCodeException) {            return t;        }        t = t.getCause();    }    return throwable;}
public PortablePipelineResult beam_f12651_1(final Pipeline pipeline, JobInfo jobInfo)
{        final RunnerApi.Pipeline fusedPipeline = GreedyPipelineFuser.fuse(pipeline).toPipeline();                        options.setRunner(SamzaRunner.class);    try {        final SamzaRunner runner = SamzaRunner.fromOptions(options);        return runner.runPortablePipeline(fusedPipeline);    } catch (Exception e) {        throw new RuntimeException("Failed to invoke samza job", e);    }}
public static boolean beam_f12659_0(SamzaPipelineOptions options)
{    if (containsKey(options, IS_PORTABLE_MODE)) {        return options.getConfigOverride().get(IS_PORTABLE_MODE).equals(String.valueOf(true));    } else {        return false;    }}
public static int beam_f12660_0(SamzaPipelineOptions options)
{    if (containsKey(options, FN_CONTROL_PORT)) {        return Integer.parseInt(options.getConfigOverride().get(FN_CONTROL_PORT));    } else {        return -1;    }}
public static long beam_f12661_0(SamzaPipelineOptions options)
{    if (containsKey(options, CONTROL_CLIENT_MAX_WAIT_TIME_MS)) {        return Long.parseLong(options.getConfigOverride().get(CONTROL_CLIENT_MAX_WAIT_TIME_MS));    } else {        return DEFAULT_CONTROL_CLIENT_MAX_WAIT_TIME_MS;    }}
public void beam_f12669_0(String name, String property)
{    config.put(name, property);}
public void beam_f12670_0(Map<String, String> properties)
{    config.putAll(properties);}
public Config beam_f12671_0()
{    try {                config.putAll(createSystemConfig(options));                config.putAll(createUserConfig(options));        config.put(ApplicationConfig.APP_NAME, options.getJobName());        config.put(ApplicationConfig.APP_ID, options.getJobInstance());        config.put(JobConfig.JOB_NAME(), options.getJobName());        config.put(JobConfig.JOB_ID(), options.getJobInstance());        config.put("beamPipelineOptions", Base64Serializer.serializeUnchecked(new SerializablePipelineOptions(options)));        validateConfigs(options, config);        return new MapConfig(config);    } catch (Exception e) {        throw new RuntimeException(e);    }}
private static void beam_f12679_0(SamzaPipelineOptions options, Map<String, String> config)
{        switch(options.getSamzaExecutionEnvironment()) {        case YARN:            validateYarnRun(config);            break;        case STANDALONE:            validateZKStandAloneRun(config);            break;        default:                        break;    }}
 static String beam_f12680_0(SamzaPipelineOptions options, String storeName)
{    return String.format("%s-%s-%s-changelog", options.getJobName(), options.getJobInstance(), storeName);}
public void beam_f12681_0(AppliedPTransform<?, ?, ?> currentTransform)
{    this.currentTransform = currentTransform;}
public void beam_f12689_0(PipelineNode.PTransformNode transform, QueryablePipeline pipeline, PortableTranslationContext ctx)
{    doTranslatePortable(transform, pipeline, ctx);}
private static void beam_f12690_0(PipelineNode.PTransformNode transform, QueryablePipeline pipeline, PortableTranslationContext ctx)
{    final List<MessageStream<OpMessage<T>>> inputStreams = ctx.getAllInputMessageStreams(transform);    final String outputId = ctx.getOutputId(transform);            checkState(!inputStreams.isEmpty(), "no input streams defined for Flatten: %s", transform.getId());    ctx.registerMessageStream(outputId, mergeInputStreams(inputStreams));}
private static MessageStream<OpMessage<T>> beam_f12691_0(List<MessageStream<OpMessage<T>>> inputStreams)
{    if (inputStreams.size() == 1) {        return Iterables.getOnlyElement(inputStreams);    }    final Set<MessageStream<OpMessage<T>>> streamsToMerge = new HashSet<>();    inputStreams.forEach(stream -> {        if (!streamsToMerge.add(stream)) {                        streamsToMerge.add(stream.map(m -> m));        }    });    return MessageStream.mergeAll(streamsToMerge);}
public void beam_f12699_0(PipelineNode.PTransformNode transform, QueryablePipeline pipeline, PortableTranslationContext ctx)
{    final String outputId = ctx.getOutputId(transform);    final GenericSystemDescriptor systemDescriptor = new GenericSystemDescriptor(outputId, SamzaImpulseSystemFactory.class.getName());        final Serde<KV<?, OpMessage<byte[]>>> kvSerde = KVSerde.of(new NoOpSerde(), new NoOpSerde<>());    final GenericInputDescriptor<KV<?, OpMessage<byte[]>>> inputDescriptor = systemDescriptor.getInputDescriptor(outputId, kvSerde);    ctx.registerInputMessageStream(outputId, inputDescriptor);}
public void beam_f12700_0(ParDo.MultiOutput<InT, OutT> transform, TransformHierarchy.Node node, TranslationContext ctx)
{    doTranslate(transform, node, ctx);}
private static void beam_f12701_0(ParDo.MultiOutput<InT, OutT> transform, TransformHierarchy.Node node, TranslationContext ctx)
{    final PCollection<? extends InT> input = ctx.getInput(transform);    final Map<TupleTag<?>, Coder<?>> outputCoders = ctx.getCurrentTransform().getOutputs().entrySet().stream().filter(e -> e.getValue() instanceof PCollection).collect(Collectors.toMap(e -> e.getKey(), e -> ((PCollection<?>) e.getValue()).getCoder()));    final DoFnSignature signature = DoFnSignatures.getSignature(transform.getFn().getClass());    final Coder<?> keyCoder = signature.usesState() ? ((KvCoder<?, ?>) input.getCoder()).getKeyCoder() : null;    if (signature.processElement().isSplittable()) {        throw new UnsupportedOperationException("Splittable DoFn is not currently supported");    }    final MessageStream<OpMessage<InT>> inputStream = ctx.getMessageStream(input);    final List<MessageStream<OpMessage<InT>>> sideInputStreams = transform.getSideInputs().values().stream().map(ctx::<InT>getViewStream).collect(Collectors.toList());    final ArrayList<Map.Entry<TupleTag<?>, PValue>> outputs = new ArrayList<>(node.getOutputs().entrySet());    final Map<TupleTag<?>, Integer> tagToIndexMap = new HashMap<>();    final Map<Integer, PCollection<?>> indexToPCollectionMap = new HashMap<>();    for (int index = 0; index < outputs.size(); ++index) {        final Map.Entry<TupleTag<?>, PValue> taggedOutput = outputs.get(index);        tagToIndexMap.put(taggedOutput.getKey(), index);        if (!(taggedOutput.getValue() instanceof PCollection)) {            throw new IllegalArgumentException("Expected side output to be PCollection, but was: " + taggedOutput.getValue());        }        final PCollection<?> sideOutputCollection = (PCollection<?>) taggedOutput.getValue();        indexToPCollectionMap.put(index, sideOutputCollection);    }    final HashMap<String, PCollectionView<?>> idToPValueMap = new HashMap<>();    for (PCollectionView<?> view : transform.getSideInputs().values()) {        idToPValueMap.put(ctx.getViewId(view), view);    }    DoFnSchemaInformation doFnSchemaInformation;    doFnSchemaInformation = ParDoTranslation.getSchemaInformation(ctx.getCurrentTransform());    Map<String, PCollectionView<?>> sideInputMapping = ParDoTranslation.getSideInputMapping(ctx.getCurrentTransform());    final DoFnOp<InT, OutT, RawUnionValue> op = new DoFnOp<>(transform.getMainOutputTag(), transform.getFn(), keyCoder, (Coder<InT>) input.getCoder(), outputCoders, transform.getSideInputs().values(), transform.getAdditionalOutputTags().getAll(), input.getWindowingStrategy(), idToPValueMap, new DoFnOp.MultiOutputManagerFactory(tagToIndexMap), node.getFullName(),     String.valueOf(ctx.getCurrentTopologicalId()), input.isBounded(), false, null, Collections.emptyMap(), doFnSchemaInformation, sideInputMapping);    final MessageStream<OpMessage<InT>> mergedStreams;    if (sideInputStreams.isEmpty()) {        mergedStreams = inputStream;    } else {        MessageStream<OpMessage<InT>> mergedSideInputStreams = MessageStream.mergeAll(sideInputStreams).flatMap(new SideInputWatermarkFn());        mergedStreams = inputStream.merge(Collections.singletonList(mergedSideInputStreams));    }    final MessageStream<OpMessage<RawUnionValue>> taggedOutputStream = mergedStreams.flatMap(OpAdapter.adapt(op));    for (int outputIndex : tagToIndexMap.values()) {        @SuppressWarnings("unchecked")        final MessageStream<OpMessage<OutT>> outputStream = taggedOutputStream.filter(message -> message.getType() != OpMessage.Type.ELEMENT || message.getElement().getValue().getUnionTag() == outputIndex).flatMap(OpAdapter.adapt(new RawUnionValueToValue()));        ctx.registerMessageStream(indexToPCollectionMap.get(outputIndex), outputStream);    }}
public SamzaPipelineOptions beam_f12710_0()
{    return this.options;}
public void beam_f12711_0(int id)
{    this.topologicalId = id;}
public int beam_f12712_0()
{    return this.topologicalId;}
public void beam_f12720_0(String id, InputDescriptor<KV<?, OpMessage<T>>, ?> inputDescriptor)
{        final String streamId = inputDescriptor.getStreamId();    if (registeredInputStreams.contains(streamId)) {        return;    }    final MessageStream<OpMessage<T>> stream = appDescriptor.getInputStream(inputDescriptor).map(org.apache.samza.operators.KV::getValue);    registerMessageStream(id, stream);    registeredInputStreams.add(streamId);}
public WindowedValue.WindowedValueCoder beam_f12721_0(String collectionId, RunnerApi.Components components)
{    PipelineNode.PCollectionNode collectionNode = PipelineNode.pCollection(collectionId, components.getPcollectionsOrThrow(collectionId));    try {        return (WindowedValue.WindowedValueCoder) WireCoders.instantiateRunnerWireCoder(collectionNode, components);    } catch (IOException e) {        throw new RuntimeException(e);    }}
public WindowingStrategy<?, BoundedWindow> beam_f12722_0(PipelineNode.PTransformNode transform, QueryablePipeline pipeline)
{    String inputId = Iterables.getOnlyElement(transform.getTransform().getInputsMap().values());    RehydratedComponents rehydratedComponents = RehydratedComponents.forComponents(pipeline.getComponents());    RunnerApi.WindowingStrategy windowingStrategyProto = pipeline.getComponents().getWindowingStrategiesOrThrow(pipeline.getComponents().getPcollectionsOrThrow(inputId).getWindowingStrategyId());    WindowingStrategy<?, ?> windowingStrategy;    try {        windowingStrategy = WindowingStrategyTranslation.fromProto(windowingStrategyProto, rehydratedComponents);    } catch (Exception e) {        throw new IllegalStateException(String.format("Unable to hydrate GroupByKey windowing strategy %s.", windowingStrategyProto), e);    }    @SuppressWarnings("unchecked")    WindowingStrategy<?, BoundedWindow> ret = (WindowingStrategy<?, BoundedWindow>) windowingStrategy;    return ret;}
public SystemAdmin beam_f12730_0(String systemName, Config config)
{    return new SamzaImpulseSystemAdmin();}
public Map<SystemStreamPartition, String> beam_f12731_0(Map<SystemStreamPartition, String> offset)
{    return offset.keySet().stream().collect(Collectors.toMap(Function.identity(), k -> DUMMY_OFFSET));}
public Map<String, SystemStreamMetadata> beam_f12732_0(Set<String> streamNames)
{    return streamNames.stream().collect(Collectors.toMap(Function.identity(), stream -> {                Map<Partition, SystemStreamMetadata.SystemStreamPartitionMetadata> partitionMetadata = Collections.singletonMap(new Partition(0), new SystemStreamMetadata.SystemStreamPartitionMetadata(DUMMY_OFFSET, DUMMY_OFFSET, DUMMY_OFFSET));        return new SystemStreamMetadata(stream, partitionMetadata);    }));}
public void beam_f12743_0(T transform, TransformHierarchy.Node node, Pipeline pipeline, TransformTranslator<T> translator)
{    ctx.setCurrentTransform(node.toAppliedPTransform(pipeline));    if (translator instanceof TransformConfigGenerator) {        TransformConfigGenerator<T> configGenerator = (TransformConfigGenerator<T>) translator;        configBuilder.putAll(configGenerator.createConfig(transform, node, ctx));    }    ctx.clearCurrentTransform();}
public CompositeBehavior beam_f12744_0(TransformHierarchy.Node node)
{    final PTransform<?, ?> transform = node.getTransform();    final String urn = getUrnForTransform(transform);    if (canTranslate(urn, transform)) {        applyTransform(transform, node, TRANSLATORS.get(urn));        return CompositeBehavior.DO_NOT_ENTER_TRANSFORM;    }    return CompositeBehavior.ENTER_TRANSFORM;}
public void beam_f12745_0(TransformHierarchy.Node node)
{    final PTransform<?, ?> transform = node.getTransform();    final String urn = getUrnForTransform(transform);    checkArgument(canTranslate(urn, transform), String.format("Unsupported transform class: %s. Node: %s", transform, node));    applyTransform(transform, node, TRANSLATORS.get(urn));}
public static void beam_f12753_0(RunnerApi.Pipeline pipeline, ConfigBuilder configBuilder, SamzaPipelineOptions options)
{    QueryablePipeline queryablePipeline = QueryablePipeline.forTransforms(pipeline.getRootTransformIdsList(), pipeline.getComponents());    for (PipelineNode.PTransformNode transform : queryablePipeline.getTopologicallyOrderedTransforms()) {        TransformTranslator<?> translator = TRANSLATORS.get(transform.getTransform().getSpec().getUrn());        if (translator instanceof TransformConfigGenerator) {            TransformConfigGenerator configGenerator = (TransformConfigGenerator) translator;            configBuilder.putAll(configGenerator.createPortableConfig(transform, options));        }    }}
public PCollection<List<ElemT>> beam_f12754_0(PCollection<List<ElemT>> input)
{    return PCollection.<List<ElemT>>createPrimitiveOutputInternal(input.getPipeline(), input.getWindowingStrategy(), input.isBounded(), input.getCoder());}
public PCollectionView<ViewT> beam_f12755_0()
{    return view;}
public List<T> beam_f12763_0(List<T> accumulator)
{    return accumulator;}
public Coder<List<T>> beam_f12764_0(CoderRegistry registry, Coder<T> inputCoder)
{    return ListCoder.of(inputCoder);}
public Coder<List<T>> beam_f12765_0(CoderRegistry registry, Coder<T> inputCoder)
{    return ListCoder.of(inputCoder);}
public void beam_f12773_1(PValue pvalue, InputDescriptor<org.apache.samza.operators.KV<?, OpMessage<OutT>>, ?> inputDescriptor)
{        final String streamId = inputDescriptor.getStreamId();    if (registeredInputStreams.containsKey(streamId)) {        MessageStream<OpMessage<OutT>> messageStream = registeredInputStreams.get(streamId);                registerMessageStream(pvalue, messageStream);        return;    }    @SuppressWarnings("unchecked")    final MessageStream<OpMessage<OutT>> typedStream = getValueStream(appDescriptor.getInputStream(inputDescriptor));    registerMessageStream(pvalue, typedStream);    registeredInputStreams.put(streamId, typedStream);}
public void beam_f12774_0(PValue pvalue, MessageStream<OpMessage<OutT>> stream)
{    if (messsageStreams.containsKey(pvalue)) {        throw new IllegalArgumentException("Stream already registered for pvalue: " + pvalue);    }    messsageStreams.put(pvalue, stream);}
public MessageStream<OpMessage<String>> beam_f12775_0()
{    InputDescriptor<OpMessage<String>, ?> dummyInput = createDummyStreamDescriptor(UUID.randomUUID().toString());    return appDescriptor.getInputStream(dummyInput);}
public void beam_f12783_0(int id)
{    this.topologicalId = id;}
public int beam_f12784_0()
{    return this.topologicalId;}
public InT beam_f12785_0(PTransform<InT, ?> transform)
{    return (InT) Iterables.getOnlyElement(TransformInputs.nonAdditionalInputs(this.currentTransform));}
private static InputDescriptor<OpMessage<String>, ?> beam_f12793_0(String id)
{    final GenericSystemDescriptor dummySystem = new GenericSystemDescriptor(id, InMemorySystemFactory.class.getName());    final GenericInputDescriptor<OpMessage<String>> dummyInput = dummySystem.getInputDescriptor(id, new NoOpSerde<>());    dummyInput.withOffsetDefault(SystemStreamMetadata.OffsetType.OLDEST);    final Config config = new MapConfig(dummyInput.toConfig(), dummySystem.toConfig());    final SystemFactory factory = new InMemorySystemFactory();    final StreamSpec dummyStreamSpec = new StreamSpec(id, id, id, 1);    factory.getAdmin(id, config).createStream(dummyStreamSpec);    final SystemProducer producer = factory.getProducer(id, config, null);    final SystemStream sysStream = new SystemStream(id, id);    final Consumer<Object> sendFn = (msg) -> {        producer.send(id, new OutgoingMessageEnvelope(sysStream, 0, null, msg));    };    final WindowedValue<String> windowedValue = WindowedValue.timestampedValueInGlobalWindow("dummy", new Instant());    sendFn.accept(OpMessage.ofElement(windowedValue));    sendFn.accept(new WatermarkMessage(BoundedWindow.TIMESTAMP_MAX_VALUE.getMillis()));    sendFn.accept(new EndOfStreamMessage(null));    return dummyInput;}
public void beam_f12794_0(Window.Assign<T> transform, TransformHierarchy.Node node, TranslationContext ctx)
{    final PCollection<T> output = ctx.getOutput(transform);    @SuppressWarnings("unchecked")    final WindowFn<T, ?> windowFn = (WindowFn<T, ?>) output.getWindowingStrategy().getWindowFn();    final MessageStream<OpMessage<T>> inputStream = ctx.getMessageStream(ctx.getInput(transform));    final MessageStream<OpMessage<T>> outputStream = inputStream.flatMap(OpAdapter.adapt(new WindowAssignOp<>(windowFn)));    ctx.registerMessageStream(output, outputStream);}
public void beam_f12795_0(PipelineNode.PTransformNode transform, QueryablePipeline pipeline, PortableTranslationContext ctx)
{    final RunnerApi.WindowIntoPayload payload;    try {        payload = RunnerApi.WindowIntoPayload.parseFrom(transform.getTransform().getSpec().getPayload());    } catch (InvalidProtocolBufferException e) {        throw new IllegalArgumentException(String.format("failed to parse WindowIntoPayload: %s", transform.getId()), e);    }    @SuppressWarnings("unchecked")    final WindowFn<T, ?> windowFn = (WindowFn<T, ?>) WindowingStrategyTranslation.windowFnFromProto(payload.getWindowFn());    final MessageStream<OpMessage<T>> inputStream = ctx.getOneInputMessageStream(transform);    final MessageStream<OpMessage<T>> outputStream = inputStream.flatMap(OpAdapter.adapt(new WindowAssignOp<>(windowFn)));    ctx.registerMessageStream(ctx.getOutputId(transform), outputStream);}
public static PCollection.IsBounded beam_f12803_0(RunnerApi.PCollection pCollection)
{    return pCollection.getIsBounded() == RunnerApi.IsBounded.Enum.BOUNDED ? PCollection.IsBounded.BOUNDED : PCollection.IsBounded.UNBOUNDED;}
public void beam_f12804_0() throws IOException, InterruptedException
{    final TestBoundedSource<String> source = TestBoundedSource.<String>createBuilder().build();    final BoundedSourceSystem.Consumer<String> consumer = createConsumer(source);    consumer.register(DEFAULT_SSP, "0");    consumer.start();    assertEquals(Arrays.asList(createWatermarkMessage(DEFAULT_SSP, BoundedWindow.TIMESTAMP_MAX_VALUE), createEndOfStreamMessage(DEFAULT_SSP)), consumeUntilTimeoutOrEos(consumer, DEFAULT_SSP, DEFAULT_TIMEOUT_MILLIS));    consumer.stop();}
public void beam_f12805_0() throws IOException, InterruptedException
{    final TestBoundedSource<String> source = TestBoundedSource.<String>createBuilder().addElements("test").build();    final BoundedSourceSystem.Consumer<String> consumer = createConsumer(source);    consumer.register(DEFAULT_SSP, "0");    consumer.start();    assertEquals(Arrays.asList(createElementMessage(DEFAULT_SSP, "0", "test", BoundedWindow.TIMESTAMP_MIN_VALUE), createWatermarkMessage(DEFAULT_SSP, BoundedWindow.TIMESTAMP_MAX_VALUE), createEndOfStreamMessage(DEFAULT_SSP)), consumeUntilTimeoutOrEos(consumer, DEFAULT_SSP, DEFAULT_TIMEOUT_MILLIS));    consumer.stop();}
private static List<IncomingMessageEnvelope> beam_f12813_0(SystemConsumer consumer, SystemStreamPartition ssp, long timeoutMillis) throws InterruptedException
{    final Set<SystemStreamPartition> sspSet = Collections.singleton(ssp);    final Map<SystemStreamPartition, List<IncomingMessageEnvelope>> pollResult = consumer.poll(sspSet, timeoutMillis);    assertEquals(sspSet, pollResult.keySet());    assertNotNull(pollResult.get(ssp));    return pollResult.get(ssp);}
private static BoundedSourceSystem.Consumer<String> beam_f12814_0(BoundedSource<String> source)
{    return createConsumer(source, 1);}
private static BoundedSourceSystem.Consumer<String> beam_f12815_0(BoundedSource<String> source, int splitNum)
{    SamzaPipelineOptions pipelineOptions = PipelineOptionsFactory.as(SamzaPipelineOptions.class);    pipelineOptions.setMaxSourceParallelism(splitNum);    return new BoundedSourceSystem.Consumer<>(source, pipelineOptions, new SamzaMetricsContainer(new MetricsRegistryMap()), "test-step");}
public TestBoundedSource<T> beam_f12824_0()
{    final List<List<Event<T>>> events = new ArrayList<>();    builders.forEach(builder -> events.add(builder.getEvents()));    return new TestBoundedSource<>(events);}
public Builder<T> beam_f12825_0(int split)
{    return builders.get(split);}
public boolean beam_f12826_0() throws IOException
{    if (started) {        throw new IllegalStateException("Start called when reader was already started");    }    started = true;    return advance();}
public SourceBuilder<T, W> beam_f12836_0(CountDownLatch latch)
{    events.add(new LatchEvent<>(latch));    return this;}
public SourceBuilder<T, W> beam_f12837_0(Instant timestamp)
{    assertTrue("Expected " + timestamp + " to be greater than or equal to " + currentTimestamp, timestamp.isEqual(currentTimestamp) || timestamp.isAfter(currentTimestamp));    currentTimestamp = timestamp;    return this;}
public SourceBuilder<T, W> beam_f12838_0(Instant watermark)
{    events.add(new WatermarkEvent<>(watermark));    return this;}
public static SplittableBuilder<T> beam_f12846_0(int numSplits)
{    return new SplittableBuilder<>(numSplits);}
public List<? extends UnboundedSource<T, TestCheckpointMark>> beam_f12847_0(int desiredNumSplits, PipelineOptions options) throws Exception
{    return events.stream().map(ev -> new TestUnboundedSource<>(Collections.singletonList(ev))).collect(Collectors.toList());}
public UnboundedReader<T> beam_f12848_0(PipelineOptions options, @Nullable TestCheckpointMark checkpointMark) throws IOException
{    assert events.size() == 1;    return new Reader(events.get(0), checkpointMark);}
public Instant beam_f12857_0() throws NoSuchElementException
{    return curTime;}
public Instant beam_f12858_0()
{    return watermark;}
public CheckpointMark beam_f12859_0()
{    return TestCheckpointMark.of(offset);}
public void beam_f12868_0() throws Exception
{    final IOException exception = new IOException("Expected exception");    final TestUnboundedSource<String> source = TestUnboundedSource.<String>createBuilder().addException(exception).build();    final UnboundedSourceSystem.Consumer<String, TestCheckpointMark> consumer = createConsumer(source);    consumer.register(DEFAULT_SSP, NULL_STRING);    consumer.start();    expectWrappedException(exception, () -> consumeUntilTimeoutOrWatermark(consumer, DEFAULT_SSP, DEFAULT_TIMEOUT_MILLIS));    consumer.stop();}
public void beam_f12869_0() throws Exception
{    final IOException exception = new IOException("Expected exception");    final TestUnboundedSource<String> source = TestUnboundedSource.<String>createBuilder().addElements("test", "a", "few", "good", "messages", "then", "...").addException(exception).build();    final UnboundedSourceSystem.Consumer<String, TestCheckpointMark> consumer = createConsumer(source);    consumer.register(DEFAULT_SSP, offset(0));    consumer.start();    expectWrappedException(exception, () -> consumeUntilTimeoutOrWatermark(consumer, DEFAULT_SSP, DEFAULT_TIMEOUT_MILLIS));    consumer.stop();}
public void beam_f12870_0() throws Exception
{    final CountDownLatch advanceLatch = new CountDownLatch(1);    final Instant now = Instant.now();    final Instant nowPlusOne = now.plus(1);    final TestUnboundedSource<String> source = TestUnboundedSource.<String>createBuilder().setTimestamp(now).addElements("before").addLatch(advanceLatch).setTimestamp(nowPlusOne).addElements("after").advanceWatermarkTo(nowPlusOne).build();    final UnboundedSourceSystem.Consumer<String, TestCheckpointMark> consumer = createConsumer(source);    consumer.register(DEFAULT_SSP, NULL_STRING);    consumer.start();    assertEquals(Collections.singletonList(createElementMessage(DEFAULT_SSP, offset(0), "before", now)), consumeUntilTimeoutOrWatermark(consumer, DEFAULT_SSP, DEFAULT_TIMEOUT_MILLIS));    advanceLatch.countDown();    assertEquals(Arrays.asList(createElementMessage(DEFAULT_SSP, offset(1), "after", nowPlusOne), createWatermarkMessage(DEFAULT_SSP, nowPlusOne)), consumeUntilTimeoutOrWatermark(consumer, DEFAULT_SSP, DEFAULT_TIMEOUT_MILLIS));    consumer.stop();}
private static SystemStreamPartition beam_f12878_0(int partition)
{    return new SystemStreamPartition("default-system", "default-system", new Partition(partition));}
public void beam_f12879_0() throws Exception
{    final TimerInternals.TimerData td = TimerInternals.TimerData.of("timer", StateNamespaces.global(), new Instant(), TimeDomain.EVENT_TIME);    final String key = "timer-key";    final ByteArrayOutputStream baos = new ByteArrayOutputStream();    STRING_CODER.encode(key, baos);    final byte[] keyBytes = baos.toByteArray();    final KeyedTimerData<String> ktd = new KeyedTimerData<>(keyBytes, key, td);    final KeyedTimerData.KeyedTimerDataCoder<String> ktdCoder = new KeyedTimerData.KeyedTimerDataCoder<>(STRING_CODER, GlobalWindow.Coder.INSTANCE);    CoderProperties.coderDecodeEncodeEqual(ktdCoder, ktd);}
public void beam_f12880_0()
{    final String stateId = "foo";    final String countStateId = "count";    DoFn<KV<String, KV<String, Integer>>, KV<String, Integer>> fn = new DoFn<KV<String, KV<String, Integer>>, KV<String, Integer>>() {        @StateId(stateId)        private final StateSpec<MapState<String, Integer>> mapState = StateSpecs.map(StringUtf8Coder.of(), VarIntCoder.of());        @StateId(countStateId)        private final StateSpec<CombiningState<Integer, int[], Integer>> countState = StateSpecs.combiningFromInputInternal(VarIntCoder.of(), Sum.ofIntegers());        @ProcessElement        public void processElement(ProcessContext c, @StateId(stateId) MapState<String, Integer> mapState, @StateId(countStateId) CombiningState<Integer, int[], Integer> count) {            SamzaMapState<String, Integer> state = (SamzaMapState<String, Integer>) mapState;            KV<String, Integer> value = c.element().getValue();            state.put(value.getKey(), value.getValue());            count.add(1);            if (count.read() >= 4) {                final List<KV<String, Integer>> content = new ArrayList<>();                final Iterator<Map.Entry<String, Integer>> iterator = state.readIterator().read();                while (iterator.hasNext()) {                    Map.Entry<String, Integer> entry = iterator.next();                    content.add(KV.of(entry.getKey(), entry.getValue()));                    c.output(KV.of(entry.getKey(), entry.getValue()));                }                assertEquals(content, ImmutableList.of(KV.of("a", 97), KV.of("b", 42), KV.of("c", 12)));            }        }    };    PCollection<KV<String, Integer>> output = pipeline.apply(Create.of(KV.of("hello", KV.of("a", 97)), KV.of("hello", KV.of("b", 42)), KV.of("hello", KV.of("b", 42)), KV.of("hello", KV.of("c", 12)))).apply(ParDo.of(fn));    PAssert.that(output).containsInAnyOrder(KV.of("a", 97), KV.of("b", 42), KV.of("c", 12));    TestSamzaRunner.fromOptions(PipelineOptionsFactory.fromArgs("--runner=org.apache.beam.runners.samza.TestSamzaRunner").create()).run(pipeline);}
public boolean beam_f12888_0()
{    return iter.hasNext();}
public Entry<byte[], byte[]> beam_f12889_0()
{    return iter.next();}
public void beam_f12890_0()
{    final String stateId = "foo";    DoFn<KV<String, Integer>, Set<Integer>> fn = new DoFn<KV<String, Integer>, Set<Integer>>() {        @StateId(stateId)        private final StateSpec<SetState<Integer>> setState = StateSpecs.set(VarIntCoder.of());        @ProcessElement        public void processElement(ProcessContext c, @StateId(stateId) SetState<Integer> setState) {            SamzaSetState<Integer> state = (SamzaSetState<Integer>) setState;            state.add(c.element().getValue());                        int size = Iterators.size(state.readIterator().read());            if (size > 1) {                final Iterator<Integer> iterator = state.readIterator().read();                assertTrue(iterator.hasNext());                                iterator.next();            }        }    };    pipeline.apply(Create.of(KV.of("hello", 97), KV.of("hello", 42), KV.of("hello", 42), KV.of("hello", 12))).apply(ParDo.of(fn));    SamzaPipelineOptions options = PipelineOptionsFactory.create().as(SamzaPipelineOptions.class);    options.setRunner(TestSamzaRunner.class);    Map<String, String> configs = new HashMap(ConfigBuilder.localRunConfig());    configs.put("stores.foo.factory", TestStorageEngine.class.getName());    options.setConfigOverride(configs);    TestSamzaRunner.fromOptions(options).run(pipeline).waitUntilFinish();            assertEquals(8, TestStore.iterators.size());    TestStore.iterators.forEach(iter -> assertTrue(iter.closed));}
public void beam_f12898_0()
{    final SamzaPipelineOptions pipelineOptions = PipelineOptionsFactory.create().as(SamzaPipelineOptions.class);    pipelineOptions.setTimerBufferSize(1);    RocksDbKeyValueStore store = createStore("store2");    final SamzaTimerInternalsFactory<String> timerInternalsFactory = createTimerInternalsFactory(null, "timer", pipelineOptions, store);    final StateNamespace nameSpace = StateNamespaces.global();    final TimerInternals timerInternals = timerInternalsFactory.timerInternalsForKey("testKey");    final TimerInternals.TimerData timer1 = TimerInternals.TimerData.of("timer1", nameSpace, new Instant(10), TimeDomain.EVENT_TIME);    timerInternals.setTimer(timer1);    final TimerInternals.TimerData timer2 = TimerInternals.TimerData.of("timer2", nameSpace, new Instant(100), TimeDomain.EVENT_TIME);    timerInternals.setTimer(timer2);    store.close();        store = createStore("store2");    final SamzaTimerInternalsFactory<String> restoredFactory = createTimerInternalsFactory(null, "timer", pipelineOptions, store);    restoredFactory.setInputWatermark(new Instant(150));    Collection<KeyedTimerData<String>> readyTimers = restoredFactory.removeReadyTimers();    assertEquals(2, readyTimers.size());    store.close();}
public void beam_f12899_0() throws IOException
{    final SamzaPipelineOptions pipelineOptions = PipelineOptionsFactory.create().as(SamzaPipelineOptions.class);    RocksDbKeyValueStore store = createStore("store3");    TestTimerRegistry timerRegistry = new TestTimerRegistry();    final SamzaTimerInternalsFactory<String> timerInternalsFactory = createTimerInternalsFactory(timerRegistry, "timer", pipelineOptions, store);    final StateNamespace nameSpace = StateNamespaces.global();    final TimerInternals timerInternals = timerInternalsFactory.timerInternalsForKey("testKey");    final TimerInternals.TimerData timer1 = TimerInternals.TimerData.of("timer1", nameSpace, new Instant(10), TimeDomain.PROCESSING_TIME);    timerInternals.setTimer(timer1);    final TimerInternals.TimerData timer2 = TimerInternals.TimerData.of("timer2", nameSpace, new Instant(100), TimeDomain.PROCESSING_TIME);    timerInternals.setTimer(timer2);    assertEquals(2, timerRegistry.timers.size());    store.close();        store = createStore("store3");    TestTimerRegistry restoredRegistry = new TestTimerRegistry();    final SamzaTimerInternalsFactory<String> restoredFactory = createTimerInternalsFactory(restoredRegistry, "timer", pipelineOptions, store);    assertEquals(2, restoredRegistry.timers.size());    final ByteArrayOutputStream baos = new ByteArrayOutputStream();    StringUtf8Coder.of().encode("testKey", baos);    final byte[] keyBytes = baos.toByteArray();    restoredFactory.removeProcessingTimer(new KeyedTimerData(keyBytes, "testKey", timer1));    restoredFactory.removeProcessingTimer(new KeyedTimerData(keyBytes, "testKey", timer2));    store.close();}
public void beam_f12900_0()
{    SamzaPipelineOptions options = PipelineOptionsFactory.create().as(SamzaPipelineOptions.class);    options.setJobName("TestStoreConfig");    options.setRunner(SamzaRunner.class);    Pipeline pipeline = Pipeline.create(options);    pipeline.apply(Create.of(1, 2, 3)).apply(Sum.integersGlobally());    pipeline.replaceAll(SamzaTransformOverrides.getDefaultOverrides());    final Map<PValue, String> idMap = PViewToIdMapper.buildIdMap(pipeline);    final ConfigBuilder configBuilder = new ConfigBuilder(options);    SamzaPipelineTranslator.createConfig(pipeline, options, idMap, configBuilder);    final Config config = configBuilder.build();    assertEquals(RocksDbKeyValueStorageEngineFactory.class.getName(), config.get("stores.beamStore.factory"));    assertEquals("byteSerde", config.get("stores.beamStore.key.serde"));    assertEquals("byteSerde", config.get("stores.beamStore.msg.serde"));    assertNull(config.get("stores.beamStore.changelog"));    options.setStateDurable(true);    SamzaPipelineTranslator.createConfig(pipeline, options, idMap, configBuilder);    final Config config2 = configBuilder.build();    assertEquals("TestStoreConfig-1-beamStore-changelog", config2.get("stores.beamStore.changelog"));}
public static NamedAggregatorsAccumulator beam_f12909_0()
{    if (instance == null) {        throw new IllegalStateException("Aggregrators accumulator has not been instantiated");    } else {        return instance;    }}
private static Optional<NamedAggregators> beam_f12910_1(JavaSparkContext jsc, CheckpointDir checkpointDir)
{    try {        Path beamCheckpointPath = checkpointDir.getBeamCheckpointDir();        checkpointFilePath = new Path(beamCheckpointPath, ACCUMULATOR_CHECKPOINT_FILENAME);        fileSystem = checkpointFilePath.getFileSystem(jsc.hadoopConfiguration());        NamedAggregators recoveredValue = Checkpoint.readObject(fileSystem, checkpointFilePath);        if (recoveredValue != null) {                        return Optional.of(recoveredValue);        } else {                    }    } catch (Exception e) {        throw new RuntimeException("Failure while reading accumulator checkpoint.", e);    }    return Optional.absent();}
private static void beam_f12911_0() throws IOException
{    if (checkpointFilePath != null) {        Checkpoint.writeObject(fileSystem, checkpointFilePath, instance.value());    }}
public boolean beam_f12919_0()
{    return value.equals(empty);}
public NamedAggregatorsAccumulator beam_f12920_0()
{    NamedAggregators newContainer = new NamedAggregators();    newContainer.merge(value);    return new NamedAggregatorsAccumulator(newContainer);}
public void beam_f12921_0()
{    this.value = new NamedAggregators();}
public static Function<T, byte[]> beam_f12929_0(final Coder<T> coder)
{    return t -> toByteArray(t, coder);}
public static Function<byte[], T> beam_f12930_0(final Coder<T> coder)
{    return bytes -> fromByteArray(bytes, coder);}
public static PairFunction<Tuple2<K, V>, ByteArray, byte[]> beam_f12931_0(final Coder<K> keyCoder, final Coder<V> valueCoder)
{    return kv -> new Tuple2<>(new ByteArray(toByteArray(kv._1(), keyCoder)), toByteArray(kv._2(), valueCoder));}
public void beam_f12939_0(ProcessContext c)
{    if (c.element().trim().isEmpty()) {        emptyLines.inc();    }        String[] words = c.element().split("[^\\p{L}]+");        for (String word : words) {        if (!word.isEmpty()) {            c.output(word);        }    }}
public String beam_f12940_0(KV<String, Long> input)
{    return input.getKey() + ": " + input.getValue();}
public PCollection<KV<String, Long>> beam_f12941_0(PCollection<String> lines)
{        PCollection<String> words = lines.apply(ParDo.of(new ExtractWordsFn()));        return words.apply(Count.perElement());}
public final CreateStream<T> beam_f12949_0(TimestampedValue<T>... batchElements)
{        for (final TimestampedValue<T> timestampedValue : batchElements) {        checkArgument(timestampedValue.getTimestamp().isBefore(BoundedWindow.TIMESTAMP_MAX_VALUE), "Elements must have timestamps before %s. Got: %s", BoundedWindow.TIMESTAMP_MAX_VALUE, timestampedValue.getTimestamp());    }    batches.offer(Arrays.asList(batchElements));    return this;}
public final CreateStream<T> beam_f12950_0(T... batchElements)
{    List<TimestampedValue<T>> timestamped = Lists.newArrayListWithCapacity(batchElements.length);        for (T element : batchElements) {        timestamped.add(TimestampedValue.atMinimumTimestamp(element));    }    batches.offer(timestamped);    return this;}
public CreateStream<T> beam_f12951_0()
{    batches.offer(Collections.emptyList());    return this;}
public boolean beam_f12959_0()
{    return forceWatermarkSync;}
public PCollection<T> beam_f12960_0(PBegin input)
{    return PCollection.createPrimitiveOutputInternal(input.getPipeline(), WindowingStrategy.globalDefault(), PCollection.IsBounded.UNBOUNDED, coder);}
public static EmptyCheckpointMark beam_f12961_0()
{    return INSTANCE;}
public Coder<T> beam_f12970_0()
{    return source.getOutputCoder();}
public Coder<CheckpointMarkT> beam_f12971_0()
{    return source.getCheckpointMarkCoder();}
public String beam_f12972_0()
{    return sourceId + "_" + splitId;}
private void beam_f12980_1() throws IOException
{    unboundedReader.getCheckpointMark().finalizeCheckpoint();    }
public T beam_f12981_0() throws NoSuchElementException
{    return unboundedReader.getCurrent();}
public Instant beam_f12982_0() throws NoSuchElementException
{    return unboundedReader.getCurrentTimestamp();}
public scala.Option<RDD<Tuple2<Source<T>, CheckpointMarkT>>> beam_f12990_0(Time validTime)
{    RDD<Tuple2<Source<T>, CheckpointMarkT>> rdd = new SourceRDD.Unbounded<>(ssc().sparkContext(), options, createMicrobatchSource(), numPartitions);    return scala.Option.apply(rdd);}
private MicrobatchSource<T, CheckpointMarkT> beam_f12991_0()
{    return new MicrobatchSource<>(unboundedSource, boundReadDuration, initialParallelism, computeReadMaxRecords(), -1, id(), readerCacheInterval);}
private long beam_f12992_1()
{    if (boundMaxRecords > 0) {                return boundMaxRecords;    } else {        final scala.Option<Long> rateControlledMax = rateControlledMaxRecords();        if (rateControlledMax.isDefined()) {                        return rateControlledMax.get();        } else {                        return Long.MAX_VALUE;        }    }}
public scala.collection.Iterator<WindowedValue<T>> beam_f13003_0(final Partition split, final TaskContext context)
{    final MetricsContainer metricsContainer = metricsAccum.value().getContainer(stepName);    @SuppressWarnings("unchecked")    final BoundedSource.BoundedReader<T> reader = createReader((SourcePartition<T>) split);    final Iterator<WindowedValue<T>> readerIterator = new ReaderToIteratorAdapter<>(metricsContainer, reader);    return new InterruptibleIterator<>(context, JavaConversions.asScalaIterator(readerIterator));}
private boolean beam_f13004_0()
{    try (Closeable ignored = MetricsEnvironment.scopedMetricsContainer(metricsContainer)) {        if (closed) {            return FAILED_TO_OBTAIN_NEXT;        } else {            checkState(next == null, "unexpected non-null value for next");            if (seekNext()) {                next = WindowedValue.timestampedValueInGlobalWindow(reader.getCurrent(), reader.getCurrentTimestamp());                return SUCCESSFULLY_OBTAINED_NEXT;            } else {                close();                return FAILED_TO_OBTAIN_NEXT;            }        }    } catch (final Exception e) {        throw new RuntimeException("Failed to read data.", e);    }}
private void beam_f13005_0()
{    closed = true;    try {        reader.close();    } catch (final IOException e) {        throw new RuntimeException(e);    }}
public int beam_f13013_0()
{    return 41 * (41 + rddId) + index;}
 Source<T> beam_f13014_0()
{    return source;}
public Partition[] beam_f13015_0()
{    try {        final List<? extends Source<T>> partitionedSources = microbatchSource.split(options.get());        final Partition[] partitions = new CheckpointableSourcePartition[partitionedSources.size()];        for (int i = 0; i < partitionedSources.size(); i++) {            partitions[i] = new CheckpointableSourcePartition<>(id(), i, partitionedSources.get(i), EmptyCheckpointMark.get());        }        return partitions;    } catch (Exception e) {        throw new RuntimeException("Failed to create partitions.", e);    }}
public scala.Option<RDD<BoxedUnit>> beam_f13023_0(Time validTime)
{        scala.Option<RDD<Metadata>> parentRDDOpt = parent.getOrCompute(validTime);    final MetricsContainerStepMapAccumulator metricsAccum = MetricsAccumulator.getInstance();    long count = 0;    SparkWatermarks sparkWatermark = null;    Instant globalLowWatermarkForBatch = BoundedWindow.TIMESTAMP_MIN_VALUE;    Instant globalHighWatermarkForBatch = BoundedWindow.TIMESTAMP_MIN_VALUE;    long maxReadDuration = 0;    if (parentRDDOpt.isDefined()) {        JavaRDD<Metadata> parentRDD = parentRDDOpt.get().toJavaRDD();        for (Metadata metadata : parentRDD.collect()) {            count += metadata.getNumRecords();                        Instant partitionLowWatermark = metadata.getLowWatermark();            globalLowWatermarkForBatch = globalLowWatermarkForBatch.isBefore(partitionLowWatermark) ? partitionLowWatermark : globalLowWatermarkForBatch;            Instant partitionHighWatermark = metadata.getHighWatermark();            globalHighWatermarkForBatch = globalHighWatermarkForBatch.isBefore(partitionHighWatermark) ? partitionHighWatermark : globalHighWatermarkForBatch;                        final Gauge gauge = Metrics.gauge(NAMESPACE, READ_DURATION_MILLIS);            final MetricsContainer container = metadata.getMetricsContainers().getContainer(stepName);            try (Closeable ignored = MetricsEnvironment.scopedMetricsContainer(container)) {                final long readDurationMillis = metadata.getReadDurationMillis();                if (readDurationMillis > maxReadDuration) {                    gauge.set(readDurationMillis);                }            } catch (IOException e) {                throw new RuntimeException(e);            }            metricsAccum.value().updateAll(metadata.getMetricsContainers());        }        sparkWatermark = new SparkWatermarks(globalLowWatermarkForBatch, globalHighWatermarkForBatch, new Instant(validTime.milliseconds()));                GlobalWatermarkHolder.add(inputDStreamId, sparkWatermark);    }        report(validTime, count, sparkWatermark);    return scala.Option.empty();}
private void beam_f13024_0(Time batchTime, long count, SparkWatermarks sparkWatermark)
{        scala.collection.immutable.Map<String, Object> metadata = new scala.collection.immutable.Map.Map1<>(StreamInputInfo.METADATA_KEY_DESCRIPTION(), String.format("Read %d records with observed watermarks %s, from %s for batch time: %s", count, sparkWatermark == null ? "N/A" : sparkWatermark, sourceName, batchTime));    StreamInputInfo streamInputInfo = new StreamInputInfo(inputDStreamId, count, metadata);    ssc().scheduler().inputInfoTracker().reportInfo(batchTime, streamInputInfo);}
 long beam_f13025_0()
{    return numRecords;}
 NamedAggregators beam_f13033_0()
{    return namedAggregators;}
public String beam_f13034_0()
{    return name;}
public MetricRegistry beam_f13035_0()
{    return metricRegistry;}
public void beam_f13043_1(JavaStreamingListenerBatchCompleted batchCompleted)
{    try {        checkpoint();    } catch (IOException e) {            }}
public boolean beam_f13044_0()
{    return value.equals(empty);}
public MetricsContainerStepMapAccumulator beam_f13045_0()
{    MetricsContainerStepMap newContainer = new MetricsContainerStepMap();    newContainer.updateAll(value);    return new MetricsContainerStepMapAccumulator(newContainer);}
public MetricRegistry beam_f13053_0()
{    return metricRegistry;}
public String beam_f13054_0()
{    return new SparkBeamMetric().renderAll().toString();}
public boolean beam_f13055_0(Object o)
{    return super.equals(o);}
private Map<String, Gauge> beam_f13063_0(final MetricRegistry metricRegistry, final MetricFilter filter)
{    Map<String, Gauge> gauges = new HashMap<>();        final Optional<Map<String, Gauge>> aggregatorMetrics = FluentIterable.from(metricRegistry.getMetrics().entrySet()).firstMatch(isAggregatorMetric()).transform(aggregatorMetricToGauges());        final Optional<Map<String, Gauge>> beamMetrics = FluentIterable.from(metricRegistry.getMetrics().entrySet()).firstMatch(isSparkBeamMetric()).transform(beamMetricToGauges());    if (aggregatorMetrics.isPresent()) {        gauges.putAll(Maps.filterEntries(aggregatorMetrics.get(), matches(filter)));    }    if (beamMetrics.isPresent()) {        gauges.putAll(Maps.filterEntries(beamMetrics.get(), matches(filter)));    }    return gauges;}
private Function<Map.Entry<String, Metric>, Map<String, Gauge>> beam_f13064_0()
{    return entry -> {        final NamedAggregators agg = ((AggregatorMetric) entry.getValue()).getNamedAggregators();        final String parentName = entry.getKey();        final Map<String, Gauge> gaugeMap = Maps.transformEntries(agg.renderAll(), toGauge());        final Map<String, Gauge> fullNameGaugeMap = Maps.newLinkedHashMap();        for (Map.Entry<String, Gauge> gaugeEntry : gaugeMap.entrySet()) {            fullNameGaugeMap.put(parentName + "." + gaugeEntry.getKey(), gaugeEntry.getValue());        }        return Maps.filterValues(fullNameGaugeMap, Predicates.notNull());    };}
private Function<Map.Entry<String, Metric>, Map<String, Gauge>> beam_f13065_0()
{    return entry -> {        final Map<String, ?> metrics = ((SparkBeamMetric) entry.getValue()).renderAll();        final String parentName = entry.getKey();        final Map<String, Gauge> gaugeMap = Maps.transformEntries(metrics, toGauge());        final Map<String, Gauge> fullNameGaugeMap = Maps.newLinkedHashMap();        for (Map.Entry<String, Gauge> gaugeEntry : gaugeMap.entrySet()) {            fullNameGaugeMap.put(parentName + "." + gaugeEntry.getKey(), gaugeEntry.getValue());        }        return Maps.filterValues(fullNameGaugeMap, Predicates.notNull());    };}
 static JobInvocation beam_f13073_0(String invocationId, String retrievalToken, ListeningExecutorService executorService, Pipeline pipeline, SparkPipelineOptions sparkOptions)
{    JobInfo jobInfo = JobInfo.create(invocationId, sparkOptions.getJobName(), retrievalToken, PipelineOptionsTranslation.toProto(sparkOptions));    SparkPipelineRunner pipelineRunner = new SparkPipelineRunner(sparkOptions);    return new JobInvocation(jobInfo, executorService, pipeline, pipelineRunner);}
protected JobInvoker beam_f13074_0()
{    return SparkJobInvoker.create((SparkServerConfiguration) configuration);}
 String beam_f13075_0()
{    return this.sparkMasterUrl;}
private boolean beam_f13083_0(final TransformHierarchy.Node node)
{    return node == null || (transforms.stream().noneMatch(debugTransform -> debugTransform.getNode().equals(node) && debugTransform.isComposite()) && shouldDebug(node.getEnclosingNode()));}
 void beam_f13084_0(TransformHierarchy.Node node)
{    @SuppressWarnings("unchecked")    TransformT transform = (TransformT) node.getTransform();    TransformEvaluator<TransformT> evaluator = translate(node, transform);    if (shouldDebug(node)) {        transforms.add(new NativeTransform(node, evaluator, transform, false));    }}
 String beam_f13085_0()
{    return Joiner.on("\n").join(transforms);}
private static RuntimeException beam_f13093_0(final Throwable e)
{    if (e instanceof SparkException) {        if (e.getCause() != null && e.getCause() instanceof UserCodeException) {            UserCodeException userException = (UserCodeException) e.getCause();            return new Pipeline.PipelineExecutionException(userException.getCause());        } else if (e.getCause() != null) {            return new Pipeline.PipelineExecutionException(e.getCause());        }    }    return runtimeExceptionFrom(e);}
public PipelineResult.State beam_f13094_0()
{    return state;}
public PipelineResult.State beam_f13095_0()
{    return waitUntilFinish(Duration.millis(-1));}
protected State beam_f13103_0(final Duration duration) throws ExecutionException, InterruptedException
{        pipelineExecution.get();    javaStreamingContext.awaitTerminationOrTimeout(duration.getMillis());    State terminationState;    switch(javaStreamingContext.getState()) {        case ACTIVE:            terminationState = State.RUNNING;            break;        case STOPPED:            terminationState = State.DONE;            break;        default:            terminationState = State.UNKNOWN;            break;    }    return terminationState;}
private void beam_f13104_0(State newState)
{    State oldState = this.state;    this.state = newState;    if (!oldState.isTerminal() && newState.isTerminal()) {        stop();    }}
public PortablePipelineResult beam_f13105_1(RunnerApi.Pipeline pipeline, JobInfo jobInfo)
{    SparkBatchPortablePipelineTranslator translator = new SparkBatchPortablePipelineTranslator();        Pipeline trimmedPipeline = PipelineTrimmer.trim(pipeline, translator.knownUrns());            RunnerApi.Pipeline fusedPipeline = trimmedPipeline.getComponents().getTransformsMap().values().stream().anyMatch(proto -> ExecutableStage.URN.equals(proto.getSpec().getUrn())) ? trimmedPipeline : GreedyPipelineFuser.fuse(trimmedPipeline).toPipeline();    if (pipelineOptions.getFilesToStage() == null) {        pipelineOptions.setFilesToStage(detectClassPathResourcesToStage(SparkPipelineRunner.class.getClassLoader()));            }    prepareFilesToStage(pipelineOptions);            final JavaSparkContext jsc = SparkContextFactory.getSparkContext(pipelineOptions);        AggregatorsAccumulator.init(pipelineOptions, jsc);    MetricsEnvironment.setMetricsSupported(false);    MetricsAccumulator.init(pipelineOptions, jsc);    final SparkTranslationContext context = new SparkTranslationContext(jsc, pipelineOptions, jobInfo);    final ExecutorService executorService = Executors.newSingleThreadExecutor();    final Future<?> submissionFuture = executorService.submit(() -> {        translator.translate(fusedPipeline, context);                context.computeOutputs();            });    PortablePipelineResult result = new SparkPipelineResult.PortableBatchMode(submissionFuture, jsc);    MetricsPusher metricsPusher = new MetricsPusher(MetricsAccumulator.getInstance().value(), pipelineOptions.as(MetricsOptions.class), result);    metricsPusher.start();    result.waitUntilFinish();    executorService.shutdown();    return result;}
public static void beam_f13113_0(Pipeline pipeline, SparkPipelineTranslator translator, EvaluationContext evaluationContext)
{    CacheVisitor cacheVisitor = new CacheVisitor(translator, evaluationContext);    pipeline.traverseTopologically(cacheVisitor);}
 TranslationMode beam_f13114_0()
{    return translationMode;}
public void beam_f13115_1(PValue value, Node producer)
{    if (translationMode.equals(TranslationMode.BATCH)) {        if (value instanceof PCollection && ((PCollection) value).isBounded() == IsBounded.UNBOUNDED) {                        translationMode = TranslationMode.STREAMING;        }    }}
public static SparkRunnerDebugger beam_f13123_0(PipelineOptions options)
{    if (options instanceof TestSparkPipelineOptions) {        TestSparkPipelineOptions testSparkPipelineOptions = PipelineOptionsValidator.validate(TestSparkPipelineOptions.class, options);        return new SparkRunnerDebugger(testSparkPipelineOptions);    } else {        SparkPipelineOptions sparkPipelineOptions = PipelineOptionsValidator.validate(SparkPipelineOptions.class, options);        return new SparkRunnerDebugger(sparkPipelineOptions);    }}
public SparkPipelineResult beam_f13124_1(Pipeline pipeline)
{    JavaSparkContext jsc = new JavaSparkContext("local[1]", "Debug_Pipeline");    JavaStreamingContext jssc = new JavaStreamingContext(jsc, new org.apache.spark.streaming.Duration(1000));    SparkRunner.initAccumulators(options, jsc);    TransformTranslator.Translator translator = new TransformTranslator.Translator();    SparkNativePipelineVisitor visitor;    if (options.isStreaming() || (options instanceof TestSparkPipelineOptions && ((TestSparkPipelineOptions) options).isForceStreaming())) {        SparkPipelineTranslator streamingTranslator = new StreamingTransformTranslator.Translator(translator);        EvaluationContext ctxt = new EvaluationContext(jsc, pipeline, options, jssc);        visitor = new SparkNativePipelineVisitor(streamingTranslator, ctxt);    } else {        EvaluationContext ctxt = new EvaluationContext(jsc, pipeline, options, jssc);        visitor = new SparkNativePipelineVisitor(translator, ctxt);    }    pipeline.traverseTopologically(visitor);    jsc.stop();    String debugString = visitor.getDebugString();        return new DebugSparkPipelineResult(debugString);}
 String beam_f13125_0()
{    return debugString;}
private List<WindowedValue<KV<K, Iterable<V>>>> beam_f13134_0()
{    return windowedValues;}
public void beam_f13135_0(final TupleTag<AdditionalOutputT> tag, final AdditionalOutputT output, final Instant timestamp, final Collection<? extends BoundedWindow> windows, final PaneInfo pane)
{    throw new UnsupportedOperationException("Tagged outputs are not allowed in GroupAlsoByWindow.");}
private SparkStateInternals<K> beam_f13136_0(final Option<Tuple2<StateAndTimers, List<byte[]>>> prevStateAndTimersOpt, final K key, final SparkTimerInternals timerInternals)
{    final SparkStateInternals<K> stateInternals;    if (prevStateAndTimersOpt.isEmpty()) {                stateInternals = SparkStateInternals.forKey(key);    } else {                final StateAndTimers prevStateAndTimers = prevStateAndTimersOpt.get()._1();                stateInternals = SparkStateInternals.forKeyAndState(key, prevStateAndTimers.getState());        timerInternals.addTimers(SparkTimerInternals.deserializeTimers(prevStateAndTimers.getTimers(), timerDataCoder));    }    return stateInternals;}
private static JavaDStream<WindowedValue<KV<K, Iterable<InputT>>>> beam_f13144_0(final DStream<Tuple2<ByteArray, Tuple2<StateAndTimers, List<byte[]>>>> firedStream, final Coder<K> keyCoder, final FullWindowedValueCoder<InputT> wvCoder)
{    return JavaPairDStream.fromPairDStream(firedStream, JavaSparkContext$.MODULE$.fakeClassTag(), JavaSparkContext$.MODULE$.fakeClassTag()).filter(    t2 -> !t2._2()._2().isEmpty()).flatMap(new FlatMapFunction<Tuple2</*K*/    ByteArray, Tuple2<StateAndTimers, /*WV<KV<K, Itr<I>>>*/    List<byte[]>>>, WindowedValue<KV<K, Iterable<InputT>>>>() {        private final FullWindowedValueCoder<KV<K, Iterable<InputT>>> windowedValueKeyValueCoder = windowedValueKeyValueCoderOf(keyCoder, wvCoder.getValueCoder(), wvCoder.getWindowCoder());        @Override        public java.util.Iterator<WindowedValue<KV<K, Iterable<InputT>>>> call(final Tuple2</*K*/        ByteArray, Tuple2<StateAndTimers, /*WV<KV<K, Itr<I>>>*/        List<byte[]>>> t2) throws Exception {                        return CoderHelpers.fromByteArrays(t2._2()._2(), windowedValueKeyValueCoder).iterator();        }    });}
public java.util.Iterator<WindowedValue<KV<K, Iterable<InputT>>>> beam_f13145_0(final Tuple2<ByteArray, Tuple2<StateAndTimers, List<byte[]>>> t2) throws Exception
{        return CoderHelpers.fromByteArrays(t2._2()._2(), windowedValueKeyValueCoder).iterator();}
private static PairDStreamFunctions<ByteArray, byte[]> beam_f13146_0(final JavaDStream<WindowedValue<KV<K, InputT>>> inputDStream, final Coder<K> keyCoder, final Coder<WindowedValue<InputT>> wvCoder)
{                                                final DStream<Tuple2<ByteArray, byte[]>> tupleDStream = inputDStream.map(new ReifyTimestampsAndWindowsFunction<>()).mapToPair(TranslationUtils.toPairFunction()).mapToPair(CoderHelpers.toByteFunction(keyCoder, wvCoder)).dstream();    return DStream.toPairDStreamFunctions(tupleDStream, JavaSparkContext$.MODULE$.fakeClassTag(), JavaSparkContext$.MODULE$.fakeClassTag(), null);}
public BagState<T> beam_f13154_0(StateTag<BagState<T>> address, Coder<T> elemCoder)
{    return new SparkBagState<>(namespace, address, elemCoder);}
public SetState<T> beam_f13155_0(StateTag<SetState<T>> spec, Coder<T> elemCoder)
{    throw new UnsupportedOperationException(String.format("%s is not supported", SetState.class.getSimpleName()));}
public MapState<KeyT, ValueT> beam_f13156_0(StateTag<MapState<KeyT, ValueT>> spec, Coder<KeyT> mapKeyCoder, Coder<ValueT> mapValueCoder)
{    throw new UnsupportedOperationException(String.format("%s is not supported", MapState.class.getSimpleName()));}
public int beam_f13164_0()
{    int result = namespace.hashCode();    result = 31 * result + address.hashCode();    return result;}
public SparkValueState<T> beam_f13165_0()
{    return this;}
public T beam_f13166_0()
{    return readValue();}
public TimestampCombiner beam_f13174_0()
{    return timestampCombiner;}
public SparkCombiningState<K, InputT, AccumT, OutputT> beam_f13175_0()
{    return this;}
public OutputT beam_f13176_0()
{    return combineFn.extractOutput(getAccum());}
public SparkBagState<T> beam_f13184_0()
{    return this;}
public List<T> beam_f13185_0()
{    List<T> value = super.readValue();    if (value == null) {        value = new ArrayList<>();    }    return value;}
public void beam_f13186_0(T input)
{    List<T> value = read();    value.add(input);    writeValue(value);}
public void beam_f13194_0(TimerData timer)
{    this.timers.add(timer);}
public void beam_f13195_0(StateNamespace namespace, String timerId, TimeDomain timeDomain)
{    throw new UnsupportedOperationException("Deleting a timer by ID is not yet supported.");}
public void beam_f13196_0(TimerData timer)
{    this.timers.remove(timer);}
public static Collection<byte[]> beam_f13204_0(Collection<TimerData> timers, TimerDataCoder timerDataCoder)
{    return CoderHelpers.toByteArrays(timers, timerDataCoder);}
public static Iterator<TimerData> beam_f13205_0(Collection<byte[]> serTimers, TimerDataCoder timerDataCoder)
{    return CoderHelpers.fromByteArrays(serTimers, timerDataCoder).iterator();}
public String beam_f13206_0()
{    return "SparkTimerInternals{" + "highWatermark=" + highWatermark + ", synchronizedProcessingTime=" + synchronizedProcessingTime + ", timers=" + timers + ", inputWatermark=" + inputWatermark + '}';}
 List<byte[]> beam_f13214_0(WindowedValue.WindowedValueCoder<T> wvCoder)
{    if (clientBytes == null) {        JavaRDDLike<byte[], ?> bytesRDD = rdd.map(CoderHelpers.toByteFunction(wvCoder));        clientBytes = bytesRDD.collect();    }    return clientBytes;}
 Iterable<WindowedValue<T>> beam_f13215_0(PCollection<T> pcollection)
{    if (windowedValues == null) {        WindowFn<?, ?> windowFn = pcollection.getWindowingStrategy().getWindowFn();        Coder<? extends BoundedWindow> windowCoder = windowFn.windowCoder();        final WindowedValue.WindowedValueCoder<T> windowedValueCoder;        if (windowFn instanceof GlobalWindows) {            windowedValueCoder = WindowedValue.ValueOnlyWindowedValueCoder.of(pcollection.getCoder());        } else {            windowedValueCoder = WindowedValue.FullWindowedValueCoder.of(pcollection.getCoder(), windowCoder);        }        JavaRDDLike<byte[], ?> bytesRDD = rdd.map(CoderHelpers.toByteFunction(windowedValueCoder));        List<byte[]> clientBytes = bytesRDD.collect();        windowedValues = clientBytes.stream().map(bytes -> CoderHelpers.fromByteArray(bytes, windowedValueCoder)).collect(Collectors.toList());    }    return windowedValues;}
public void beam_f13216_0(String storageLevel, Coder<?> coder)
{    StorageLevel level = StorageLevel.fromString(storageLevel);    if (TranslationUtils.canAvoidRddSerialization(level)) {                this.rdd = getRDD().persist(level);    } else {                        Coder<WindowedValue<T>> windowedValueCoder = (Coder<WindowedValue<T>>) coder;        this.rdd = getRDD().map(v -> ValueAndCoderLazySerializable.of(v, windowedValueCoder)).persist(level).map(v -> v.getOrDecode(windowedValueCoder));    }}
private MetricsContainer beam_f13224_0()
{    return metricsAccum.value().getContainer(stepName);}
public JavaSparkContext beam_f13225_0()
{    return jsc;}
public JavaStreamingContext beam_f13226_0()
{    return jssc;}
public T beam_f13234_0(PTransform<?, T> transform)
{    @SuppressWarnings("unchecked")    T output = (T) Iterables.getOnlyElement(getOutputs(transform).values());    return output;}
public Map<TupleTag<?>, PValue> beam_f13235_0(PTransform<?, ?> transform)
{    checkArgument(currentTransform != null, "can only be called with non-null currentTransform");    checkArgument(currentTransform.getTransform() == transform, "can only be called with current transform");    return currentTransform.getOutputs();}
public Map<TupleTag<?>, Coder<?>> beam_f13236_0()
{    return currentTransform.getOutputs().entrySet().stream().filter(e -> e.getValue() instanceof PCollection).collect(Collectors.toMap(Map.Entry::getKey, e -> ((PCollection) e.getValue()).getCoder()));}
public T beam_f13244_0(PValue value)
{    if (pobjects.containsKey(value)) {        return (T) pobjects.get(value);    }    if (pcollections.containsKey(value)) {        JavaRDD<?> rdd = ((BoundedDataset) pcollections.get(value)).getRDD();        T res = (T) Iterables.getOnlyElement(rdd.collect());        pobjects.put(value, res);        return res;    }    throw new IllegalStateException("Cannot resolve un-known PObject: " + value);}
public SparkPCollectionView beam_f13245_0()
{    return pviews;}
public void beam_f13246_0(PCollectionView<?> view, Iterable<WindowedValue<?>> value, Coder<Iterable<WindowedValue<?>>> coder)
{    pviews.putPView(view, value, coder);}
public static JavaRDD<WindowedValue<T>> beam_f13254_0(JavaRDD<WindowedValue<T>> rdd, WindowedValueCoder<T> wvCoder)
{        return rdd.map(CoderHelpers.toByteFunction(wvCoder)).repartition(rdd.getNumPartitions()).map(CoderHelpers.fromByteFunction(wvCoder));}
 static boolean beam_f13255_0(WindowingStrategy<?, ?> windowingStrategy)
{    return windowingStrategy.getWindowFn().isNonMerging() && windowingStrategy.getTimestampCombiner() == TimestampCombiner.END_OF_WINDOW && windowingStrategy.getWindowFn().windowCoder().consistentWithEquals();}
 static JavaRDD<WindowedValue<KV<K, Iterable<V>>>> beam_f13256_0(JavaRDD<WindowedValue<KV<K, V>>> rdd, Coder<K> keyCoder, Coder<V> valueCoder, WindowingStrategy<?, W> windowingStrategy, Partitioner partitioner)
{    final Coder<W> windowCoder = windowingStrategy.getWindowFn().windowCoder();    FullWindowedValueCoder<KV<K, V>> windowedKvCoder = WindowedValue.FullWindowedValueCoder.of(KvCoder.of(keyCoder, valueCoder), windowCoder);    JavaPairRDD<ByteArray, byte[]> windowInKey = bringWindowToKey(rdd, keyCoder, windowCoder, wv -> CoderHelpers.toByteArray(wv, windowedKvCoder));    return windowInKey.repartitionAndSortWithinPartitions(getPartitioner(partitioner, rdd)).mapPartitions(it -> new GroupByKeyIterator<>(it, keyCoder, windowingStrategy, windowedKvCoder)).filter(    Objects::nonNull);}
protected V beam_f13264_0()
{    if (inner.hasNext() && currentKey.equals(inner.peek()._1)) {        return decodeValue(inner.next()._2);    }    return endOfData();}
private V beam_f13265_0(byte[] windowedValueBytes)
{    final WindowedValue<KV<K, V>> windowedValue = CoderHelpers.fromByteArray(windowedValueBytes, windowedValueCoder);    return windowedValue.getValue().getValue();}
private WindowedValue<KV<K, V>> beam_f13266_0(Tuple2<ByteArray, byte[]> item)
{    final K key = CoderHelpers.fromByteArray(item._1.getValue(), keyCoder);    final WindowedValue<KV<K, V>> windowedValue = CoderHelpers.fromByteArray(item._2, windowedValueCoder);    final V value = windowedValue.getValue().getValue();    @SuppressWarnings("unchecked")    final W window = (W) Iterables.getOnlyElement(windowedValue.getWindows());    final Instant timestamp = windowingStrategy.getTimestampCombiner().assign(window, windowingStrategy.getWindowFn().getOutputTime(windowedValue.getTimestamp(), window));        return WindowedValue.of(KV.of(key, value), timestamp, window, PaneInfo.ON_TIME_AND_ONLY_FIRING);}
public Iterator<Tuple2<TupleTag<?>, WindowedValue<?>>> beam_f13274_0()
{    Iterator<Map.Entry<TupleTag<?>, WindowedValue<?>>> entryIter = outputs.entries().iterator();    return Iterators.transform(entryIter, this.entryToTupleFn());}
private Function<Map.Entry<K, V>, Tuple2<K, V>> beam_f13275_0()
{    return en -> new Tuple2<>(en.getKey(), en.getValue());}
public synchronized void beam_f13276_0(TupleTag<T> tag, WindowedValue<T> output)
{    outputs.put(tag, output);}
private static void beam_f13284_0(PTransformNode transformNode, RunnerApi.Pipeline pipeline, SparkTranslationContext context)
{    throw new IllegalArgumentException(String.format("Transform %s has unknown URN %s", transformNode.getId(), transformNode.getTransform().getSpec().getUrn()));}
private static void beam_f13285_0(PTransformNode transformNode, RunnerApi.Pipeline pipeline, SparkTranslationContext context)
{    BoundedDataset<byte[]> output = new BoundedDataset<>(Collections.singletonList(new byte[0]), context.getSparkContext(), ByteArrayCoder.of());    context.pushDataset(getOutputId(transformNode), output);}
private static void beam_f13286_0(PTransformNode transformNode, RunnerApi.Pipeline pipeline, SparkTranslationContext context)
{    RunnerApi.Components components = pipeline.getComponents();    String inputId = getInputId(transformNode);    Dataset inputDataset = context.popDataset(inputId);    JavaRDD<WindowedValue<KV<K, V>>> inputRdd = ((BoundedDataset<KV<K, V>>) inputDataset).getRDD();    WindowedValueCoder<KV<K, V>> inputCoder = getWindowedValueCoder(inputId, components);    KvCoder<K, V> inputKvCoder = (KvCoder<K, V>) inputCoder.getValueCoder();    Coder<K> inputKeyCoder = inputKvCoder.getKeyCoder();    Coder<V> inputValueCoder = inputKvCoder.getValueCoder();    WindowingStrategy windowingStrategy = getWindowingStrategy(inputId, components);    WindowFn<Object, BoundedWindow> windowFn = windowingStrategy.getWindowFn();    WindowedValue.WindowedValueCoder<V> wvCoder = WindowedValue.FullWindowedValueCoder.of(inputValueCoder, windowFn.windowCoder());    JavaRDD<WindowedValue<KV<K, Iterable<V>>>> groupedByKeyAndWindow;    Partitioner partitioner = getPartitioner(context);    if (GroupNonMergingWindowsFunctions.isEligibleForGroupByWindow(windowingStrategy)) {                groupedByKeyAndWindow = GroupNonMergingWindowsFunctions.groupByKeyAndWindow(inputRdd, inputKeyCoder, inputValueCoder, windowingStrategy, partitioner);    } else {        JavaRDD<KV<K, Iterable<WindowedValue<V>>>> groupedByKeyOnly = GroupCombineFunctions.groupByKeyOnly(inputRdd, inputKeyCoder, wvCoder, partitioner);                groupedByKeyAndWindow = groupedByKeyOnly.flatMap(new SparkGroupAlsoByWindowViaOutputBufferFn<>(windowingStrategy, new TranslationUtils.InMemoryStateInternalsFactory<>(), SystemReduceFn.buffering(inputValueCoder), context.serializablePipelineOptions));    }    context.pushDataset(getOutputId(transformNode), new BoundedDataset<>(groupedByKeyAndWindow));}
private static void beam_f13294_0(PTransformNode transformNode, RunnerApi.Pipeline pipeline, SparkTranslationContext context)
{    String inputId = getInputId(transformNode);    WindowedValueCoder<T> coder = getWindowedValueCoder(inputId, pipeline.getComponents());    JavaRDD<WindowedValue<T>> inRDD = ((BoundedDataset<T>) context.popDataset(inputId)).getRDD();    JavaRDD<WindowedValue<T>> reshuffled = GroupCombineFunctions.reshuffle(inRDD, coder);    context.pushDataset(getOutputId(transformNode), new BoundedDataset<>(reshuffled));}
private static Partitioner beam_f13295_0(SparkTranslationContext context)
{    Long bundleSize = context.serializablePipelineOptions.get().as(SparkPipelineOptions.class).getBundleSize();    return (bundleSize > 0) ? null : new HashPartitioner(context.getSparkContext().defaultParallelism());}
private static String beam_f13296_0(PTransformNode transformNode)
{    return Iterables.getOnlyElement(transformNode.getTransform().getInputsMap().values());}
 static WindowedAccumulator<InputT, ValueT, AccumT, ?> beam_f13304_0(Function<InputT, ValueT> toValue, Type type, Iterable<WindowedValue<AccumT>> values, Comparator<BoundedWindow> windowComparator)
{    switch(type) {        case MERGING:            return MergingWindowedAccumulator.from(toValue, values, windowComparator);        case NON_MERGING:            return NonMergingWindowedAccumulator.from(toValue, values);        case SINGLE_WINDOW:        case EXPLODE_WINDOWS:            Iterator<WindowedValue<AccumT>> iter = values.iterator();            if (iter.hasNext()) {                return SingleWindowWindowedAccumulator.create(toValue, iter.next());            }            return SingleWindowWindowedAccumulator.create(toValue);        default:            throw new IllegalArgumentException("Unknown type: " + type);    }}
 static SingleWindowWindowedAccumulator<InputT, ValueT, AccumT> beam_f13305_0(Function<InputT, ValueT> toValue)
{    return new SingleWindowWindowedAccumulator<>(toValue);}
 static WindowedAccumulator<InputT, ValueT, AccumT, ?> beam_f13306_0(Function<InputT, ValueT> toValue, WindowedValue<AccumT> accumulator)
{    return new SingleWindowWindowedAccumulator<>(toValue, accumulator);}
public Collection<WindowedValue<AccumT>> beam_f13314_0()
{    return map.values();}
public boolean beam_f13315_0()
{    return map.isEmpty();}
private ValueT beam_f13317_0(WindowedValue<InputT> value)
{    try {        return toValue.call(value.getValue());    } catch (Exception ex) {        throw UserCodeException.wrap(ex);    }}
public void beam_f13325_0(Collection<BoundedWindow> toBeMerged, BoundedWindow mergeResult)
{    AccumT accumulator = null;    for (BoundedWindow w : toBeMerged) {        WindowedValue<AccumT> windowAccumulator = Objects.requireNonNull(map.get(w));        if (accumulator == null) {            accumulator = windowAccumulator.getValue();        } else {            accumulator = mergeFn.apply(accumulator, windowAccumulator.getValue());        }    }    afterMerge.accept(toBeMerged, KV.of(mergeResult, accumulator));}
public String beam_f13326_0()
{    return "MergingWindowedAccumulator(" + this.map + ")";}
public void beam_f13327_0(WindowedAccumulator<InputT, ValueT, AccumT, ?> value, OutputStream outStream) throws CoderException, IOException
{    if (type.isMapBased()) {        wrap.encode(((MapBasedWindowedAccumulator<?, ?, AccumT, ?>) value).map.values(), outStream);    } else {        SingleWindowWindowedAccumulator<?, ?, AccumT> swwa = (SingleWindowWindowedAccumulator<?, ?, AccumT>) value;        if (swwa.isEmpty()) {            outStream.write(0);        } else {            outStream.write(1);            accumCoder.encode(WindowedValue.of(swwa.windowAccumulator, swwa.accTimestamp, swwa.accWindow, PaneInfo.NO_FIRING), outStream);        }    }}
public static SparkCombineFn<InputT, InputT, AccumT, OutputT> beam_f13336_0(CombineWithContext.CombineFnWithContext<InputT, AccumT, OutputT> combineFn, SerializablePipelineOptions options, Map<TupleTag<?>, KV<WindowingStrategy<?, ?>, SideInputBroadcast<?>>> sideInputs, WindowingStrategy<?, ?> windowingStrategy)
{    return new SparkCombineFn<>(true, e -> e, combineFn, options, sideInputs, windowingStrategy);}
private static Map<BoundedWindow, WindowedValue<T>> beam_f13337_0(Iterable<WindowedValue<T>> values, Map<BoundedWindow, WindowedValue<T>> res)
{    for (WindowedValue<T> v : values) {        res.put(getWindow(v), v);    }    return res;}
private static Comparator<BoundedWindow> beam_f13338_0(@Nullable TypeDescriptor<?> windowType)
{    final Comparator<BoundedWindow> comparator;    if (windowType != null && StreamSupport.stream(windowType.getInterfaces().spliterator(), false).anyMatch(t -> t.isSubtypeOf(TypeDescriptor.of(Comparable.class)))) {        comparator = (Comparator) Comparator.naturalOrder();    } else {        java.util.function.Function<BoundedWindow, Instant> keyExtractor = (java.util.function.Function<BoundedWindow, Instant> & Serializable) BoundedWindow::maxTimestamp;        comparator = Comparator.comparing(keyExtractor);    }    return comparator;}
 CombineWithContext.CombineFnWithContext<ValueT, AccumT, OutputT> beam_f13346_0()
{    return combineFn;}
 boolean beam_f13347_0()
{    return !getType(windowingStrategy).isMapBased();}
private WindowedAccumulator.Type beam_f13348_0(WindowingStrategy<?, ?> windowingStrategy)
{    if (windowingStrategy.getWindowFn().isNonMerging()) {        if (globalCombine) {            /* global combine must use map-based accumulator to incorporate multiple windows */            return WindowedAccumulator.Type.NON_MERGING;        }        if (windowingStrategy.getWindowFn().assignsToOneWindow() && GroupNonMergingWindowsFunctions.isEligibleForGroupByWindow(windowingStrategy)) {            return WindowedAccumulator.Type.SINGLE_WINDOW;        }        return defaultNonMergingCombineStrategy;    }    return WindowedAccumulator.Type.MERGING;}
public ExecutableStageContext beam_f13356_0(JobInfo jobInfo)
{    MultiInstanceFactory jobFactory = jobFactories.computeIfAbsent(jobInfo.jobId(), k -> {        PortablePipelineOptions portableOptions = PipelineOptionsTranslation.fromProto(jobInfo.pipelineOptions()).as(PortablePipelineOptions.class);        return new MultiInstanceFactory(MoreObjects.firstNonNull(portableOptions.getSdkWorkerParallelism(), 1L).intValue(),         (caller) -> false);    });    return jobFactory.get(jobInfo);}
public Iterator<WindowedValue<OutputT>> beam_f13357_0(RawUnionValue rawUnionValue)
{    if (rawUnionValue.getUnionTag() == unionTag) {        WindowedValue<OutputT> output = (WindowedValue<OutputT>) rawUnionValue.getValue();        return Collections.singleton(output).iterator();    }    return Collections.emptyIterator();}
 FlatMapFunction<Tuple2<ByteArray, Iterable<WindowedValue<InputT>>>, RawUnionValue> beam_f13358_0()
{    return (input) -> call(input._2.iterator());}
public FnDataReceiver<OutputT> beam_f13366_0(String pCollectionId)
{    Integer unionTag = outputMap.get(pCollectionId);    if (unionTag != null) {        int tagInt = unionTag;        return receivedElement -> collector.add(new RawUnionValue(tagInt, receivedElement));    } else if (timerReceiverFactory != null) {                return timerReceiverFactory.create(pCollectionId);    } else {        throw new IllegalStateException(String.format(Locale.ENGLISH, "Unknown PCollectionId %s", pCollectionId));    }}
public Iterator<WindowedValue<KV<K, Iterable<InputT>>>> beam_f13367_0(KV<K, Iterable<WindowedValue<InputT>>> kv) throws Exception
{    K key = kv.getKey();    Iterable<WindowedValue<InputT>> values = kv.getValue();                    InMemoryTimerInternals timerInternals = new InMemoryTimerInternals();    timerInternals.advanceProcessingTime(Instant.now());    timerInternals.advanceSynchronizedProcessingTime(Instant.now());    StateInternals stateInternals = stateInternalsFactory.stateInternalsForKey(key);    GABWOutputWindowedValue<K, InputT> outputter = new GABWOutputWindowedValue<>();    ReduceFnRunner<K, InputT, Iterable<InputT>, W> reduceFnRunner = new ReduceFnRunner<>(key, windowingStrategy, ExecutableTriggerStateMachine.create(TriggerStateMachines.stateMachineForTrigger(TriggerTranslation.toProto(windowingStrategy.getTrigger()))), stateInternals, timerInternals, outputter, new UnsupportedSideInputReader("GroupAlsoByWindow"), reduceFn, options.get());        reduceFnRunner.processElements(values);        timerInternals.advanceInputWatermark(BoundedWindow.TIMESTAMP_MAX_VALUE);        timerInternals.advanceProcessingTime(BoundedWindow.TIMESTAMP_MAX_VALUE);    timerInternals.advanceSynchronizedProcessingTime(BoundedWindow.TIMESTAMP_MAX_VALUE);    fireEligibleTimers(timerInternals, reduceFnRunner);    reduceFnRunner.persist();    return outputter.getOutputs().iterator();}
private void beam_f13368_0(InMemoryTimerInternals timerInternals, ReduceFnRunner<K, InputT, Iterable<InputT>, W> reduceFnRunner) throws Exception
{    List<TimerInternals.TimerData> timers = new ArrayList<>();    while (true) {        TimerInternals.TimerData timer;        while ((timer = timerInternals.removeNextEventTimer()) != null) {            timers.add(timer);        }        while ((timer = timerInternals.removeNextProcessingTimer()) != null) {            timers.add(timer);        }        while ((timer = timerInternals.removeNextSynchronizedProcessingTimer()) != null) {            timers.add(timer);        }        if (timers.isEmpty()) {            break;        }        reduceFnRunner.onTimers(timers);        timers.clear();    }}
private void beam_f13376_0()
{    outputManager.clear();}
private Iterator<OutputT> beam_f13377_0()
{    return outputManager.iterator();}
private Iterable<OutputT> beam_f13378_0(final Iterator<WindowedValue<FnInputT>> iter, final DoFnRunner<FnInputT, FnOutputT> doFnRunner)
{    return () -> new ProcCtxtIterator(iter, doFnRunner);}
public void beam_f13386_0()
{    for (Dataset dataset : leaves) {                dataset.action();    }}
 void beam_f13387_0(String pCollectionId, int addend)
{    int count = consumptionCount.getOrDefault(pCollectionId, 0);    consumptionCount.put(pCollectionId, count + addend);}
 void beam_f13388_0(String pCollectionId, Coder coder)
{    coderMap.put(pCollectionId, coder);}
public Path beam_f13396_0()
{    return beamCheckpointDir;}
public JavaStreamingContext beam_f13397_1() throws Exception
{            checkArgument(options.getMinReadTimeMillis() < options.getBatchIntervalMillis(), "Minimum read time has to be less than batch time.");    checkArgument(options.getReadTimePercentage() > 0 && options.getReadTimePercentage() < 1, "Read time percentage is bound to (0, 1).");    SparkPipelineTranslator translator = new StreamingTransformTranslator.Translator(new TransformTranslator.Translator());    Duration batchDuration = new Duration(options.getBatchIntervalMillis());        JavaSparkContext jsc = SparkContextFactory.getSparkContext(options);    JavaStreamingContext jssc = new JavaStreamingContext(jsc, batchDuration);        SparkRunner.initAccumulators(options, jsc);        EvaluationContext ctxt = new EvaluationContext(jsc, pipeline, options, jssc);        SparkRunner.updateCacheCandidates(pipeline, translator, ctxt);    pipeline.traverseTopologically(new SparkRunner.Evaluator(translator, ctxt));    ctxt.computeOutputs();    checkpoint(jssc, checkpointDir);    return jssc;}
private void beam_f13398_0(JavaStreamingContext jssc, CheckpointDir checkpointDir)
{    Path rootCheckpointPath = checkpointDir.getRootCheckpointDir();    Path sparkCheckpointPath = checkpointDir.getSparkCheckpointDir();    Path beamCheckpointPath = checkpointDir.getBeamCheckpointDir();    try {        FileSystem fileSystem = rootCheckpointPath.getFileSystem(jssc.sparkContext().hadoopConfiguration());        if (!fileSystem.exists(rootCheckpointPath)) {            fileSystem.mkdirs(rootCheckpointPath);        }        if (!fileSystem.exists(sparkCheckpointPath)) {            fileSystem.mkdirs(sparkCheckpointPath);        }        if (!fileSystem.exists(beamCheckpointPath)) {            fileSystem.mkdirs(beamCheckpointPath);        }    } catch (IOException e) {        throw new RuntimeException("Failed to create checkpoint dir", e);    }    jssc.checkpoint(sparkCheckpointPath.toString());}
public void beam_f13406_0(CreateStream<T> transform, EvaluationContext context)
{    final Queue<JavaRDD<WindowedValue<T>>> rddQueue = buildRdds(transform.getBatches(), context.getStreamingContext(), context.getOutput(transform).getCoder());    final JavaInputDStream<WindowedValue<T>> javaInputDStream = buildInputStream(rddQueue, transform, context);    final UnboundedDataset<T> unboundedDataset = new UnboundedDataset<>(javaInputDStream, Collections.singletonList(javaInputDStream.inputDStream().id()));        GlobalWatermarkHolder.addAll(ImmutableMap.of(unboundedDataset.getStreamSources().get(0), transform.getTimes()));    context.putDataset(transform, unboundedDataset);}
private Queue<JavaRDD<WindowedValue<T>>> beam_f13407_0(Queue<Iterable<TimestampedValue<T>>> batches, JavaStreamingContext jssc, Coder<T> coder)
{    final WindowedValue.FullWindowedValueCoder<T> windowCoder = WindowedValue.FullWindowedValueCoder.of(coder, GlobalWindow.Coder.INSTANCE);    final Queue<JavaRDD<WindowedValue<T>>> rddQueue = new LinkedBlockingQueue<>();    for (final Iterable<TimestampedValue<T>> timestampedValues : batches) {        final Iterable<WindowedValue<T>> windowedValues = StreamSupport.stream(timestampedValues.spliterator(), false).map(timestampedValue -> WindowedValue.of(timestampedValue.getValue(), timestampedValue.getTimestamp(), GlobalWindow.INSTANCE, PaneInfo.NO_FIRING)).collect(Collectors.toList());        final JavaRDD<WindowedValue<T>> rdd = jssc.sparkContext().parallelize(CoderHelpers.toByteArrays(windowedValues, windowCoder)).map(CoderHelpers.fromByteFunction(windowCoder));        rddQueue.offer(rdd);    }    return rddQueue;}
private JavaInputDStream<WindowedValue<T>> beam_f13408_0(Queue<JavaRDD<WindowedValue<T>>> rddQueue, CreateStream<T> transform, EvaluationContext context)
{    return transform.isForceWatermarkSync() ? new JavaInputDStream<>(new WatermarkSyncedDStream<>(rddQueue, transform.getBatchDuration(), context.getStreamingContext().ssc()), JavaSparkContext$.MODULE$.fakeClassTag()) : context.getStreamingContext().queueStream(rddQueue, true);}
private static TransformEvaluator<GroupByKey<K, V>> beam_f13416_0()
{    return new TransformEvaluator<GroupByKey<K, V>>() {        @Override        public void evaluate(GroupByKey<K, V> transform, EvaluationContext context) {            @SuppressWarnings("unchecked")            UnboundedDataset<KV<K, V>> inputDataset = (UnboundedDataset<KV<K, V>>) context.borrowDataset(transform);            List<Integer> streamSources = inputDataset.getStreamSources();            JavaDStream<WindowedValue<KV<K, V>>> dStream = inputDataset.getDStream();            final KvCoder<K, V> coder = (KvCoder<K, V>) context.getInput(transform).getCoder();            @SuppressWarnings("unchecked")            final WindowingStrategy<?, W> windowingStrategy = (WindowingStrategy<?, W>) context.getInput(transform).getWindowingStrategy();            @SuppressWarnings("unchecked")            final WindowFn<Object, W> windowFn = (WindowFn<Object, W>) windowingStrategy.getWindowFn();                        final WindowedValue.WindowedValueCoder<V> wvCoder = WindowedValue.FullWindowedValueCoder.of(coder.getValueCoder(), windowFn.windowCoder());            JavaDStream<WindowedValue<KV<K, Iterable<V>>>> outStream = SparkGroupAlsoByWindowViaWindowSet.groupByKeyAndWindow(dStream, coder.getKeyCoder(), wvCoder, windowingStrategy, context.getSerializableOptions(), streamSources, context.getCurrentTransform().getFullName());            context.putDataset(transform, new UnboundedDataset<>(outStream, streamSources));        }        @Override        public String toNativeString() {            return "groupByKey()";        }    };}
public void beam_f13417_0(GroupByKey<K, V> transform, EvaluationContext context)
{    @SuppressWarnings("unchecked")    UnboundedDataset<KV<K, V>> inputDataset = (UnboundedDataset<KV<K, V>>) context.borrowDataset(transform);    List<Integer> streamSources = inputDataset.getStreamSources();    JavaDStream<WindowedValue<KV<K, V>>> dStream = inputDataset.getDStream();    final KvCoder<K, V> coder = (KvCoder<K, V>) context.getInput(transform).getCoder();    @SuppressWarnings("unchecked")    final WindowingStrategy<?, W> windowingStrategy = (WindowingStrategy<?, W>) context.getInput(transform).getWindowingStrategy();    @SuppressWarnings("unchecked")    final WindowFn<Object, W> windowFn = (WindowFn<Object, W>) windowingStrategy.getWindowFn();        final WindowedValue.WindowedValueCoder<V> wvCoder = WindowedValue.FullWindowedValueCoder.of(coder.getValueCoder(), windowFn.windowCoder());    JavaDStream<WindowedValue<KV<K, Iterable<V>>>> outStream = SparkGroupAlsoByWindowViaWindowSet.groupByKeyAndWindow(dStream, coder.getKeyCoder(), wvCoder, windowingStrategy, context.getSerializableOptions(), streamSources, context.getCurrentTransform().getFullName());    context.putDataset(transform, new UnboundedDataset<>(outStream, streamSources));}
public String beam_f13418_0()
{    return "groupByKey()";}
public void beam_f13426_0(Reshuffle<K, V> transform, EvaluationContext context)
{    @SuppressWarnings("unchecked")    UnboundedDataset<KV<K, V>> inputDataset = (UnboundedDataset<KV<K, V>>) context.borrowDataset(transform);    List<Integer> streamSources = inputDataset.getStreamSources();    JavaDStream<WindowedValue<KV<K, V>>> dStream = inputDataset.getDStream();    final KvCoder<K, V> coder = (KvCoder<K, V>) context.getInput(transform).getCoder();    @SuppressWarnings("unchecked")    final WindowingStrategy<?, W> windowingStrategy = (WindowingStrategy<?, W>) context.getInput(transform).getWindowingStrategy();    @SuppressWarnings("unchecked")    final WindowFn<Object, W> windowFn = (WindowFn<Object, W>) windowingStrategy.getWindowFn();    final WindowedValue.WindowedValueCoder<KV<K, V>> wvCoder = WindowedValue.FullWindowedValueCoder.of(coder, windowFn.windowCoder());    JavaDStream<WindowedValue<KV<K, V>>> reshuffledStream = dStream.transform(rdd -> GroupCombineFunctions.reshuffle(rdd, wvCoder));    context.putDataset(transform, new UnboundedDataset<>(reshuffledStream, streamSources));}
public String beam_f13427_0()
{    return "repartition(...)";}
private static TransformEvaluator<?> beam_f13428_0(PTransform<?, ?> transform)
{    @Nullable    String urn = PTransformTranslation.urnForTransformOrNull(transform);    return urn == null ? null : EVALUATORS.get(urn);}
 List<Integer> beam_f13436_0()
{    return streamSources;}
public void beam_f13437_1(String storageLevel, Coder<?> coder)
{        if (!StorageLevel.fromString(storageLevel).equals(StorageLevel.MEMORY_ONLY_SER())) {            }            Coder<WindowedValue<T>> wc = (Coder<WindowedValue<T>>) coder;    this.dStream = dStream.map(CoderHelpers.toByteFunction(wc)).cache().map(CoderHelpers.fromByteFunction(wc));}
public void beam_f13438_0()
{        dStream.foreachRDD(rdd -> rdd.foreach(TranslationUtils.<WindowedValue<T>>emptyVoidFunction()));}
public void beam_f13449_0(Flatten.PCollections<T> transform, EvaluationContext context)
{    Collection<PValue> pcs = context.getInputs(transform).values();    JavaRDD<WindowedValue<T>> unionRDD;    if (pcs.isEmpty()) {        unionRDD = context.getSparkContext().emptyRDD();    } else {        JavaRDD<WindowedValue<T>>[] rdds = new JavaRDD[pcs.size()];        int index = 0;        for (PValue pc : pcs) {            checkArgument(pc instanceof PCollection, "Flatten had non-PCollection value in input: %s of type %s", pc, pc.getClass().getSimpleName());            rdds[index] = ((BoundedDataset<T>) context.borrowDataset(pc)).getRDD();            index++;        }        unionRDD = context.getSparkContext().union(rdds);    }    context.putDataset(transform, new BoundedDataset<>(unionRDD));}
public String beam_f13450_0()
{    return "sparkContext.union(...)";}
private static TransformEvaluator<GroupByKey<K, V>> beam_f13451_0()
{    return new TransformEvaluator<GroupByKey<K, V>>() {        @Override        public void evaluate(GroupByKey<K, V> transform, EvaluationContext context) {            @SuppressWarnings("unchecked")            JavaRDD<WindowedValue<KV<K, V>>> inRDD = ((BoundedDataset<KV<K, V>>) context.borrowDataset(transform)).getRDD();            final KvCoder<K, V> coder = (KvCoder<K, V>) context.getInput(transform).getCoder();            @SuppressWarnings("unchecked")            final WindowingStrategy<?, W> windowingStrategy = (WindowingStrategy<?, W>) context.getInput(transform).getWindowingStrategy();            @SuppressWarnings("unchecked")            final WindowFn<Object, W> windowFn = (WindowFn<Object, W>) windowingStrategy.getWindowFn();                        final Coder<K> keyCoder = coder.getKeyCoder();            final WindowedValue.WindowedValueCoder<V> wvCoder = WindowedValue.FullWindowedValueCoder.of(coder.getValueCoder(), windowFn.windowCoder());            JavaRDD<WindowedValue<KV<K, Iterable<V>>>> groupedByKey;            Partitioner partitioner = getPartitioner(context);            if (GroupNonMergingWindowsFunctions.isEligibleForGroupByWindow(windowingStrategy)) {                                groupedByKey = GroupNonMergingWindowsFunctions.groupByKeyAndWindow(inRDD, keyCoder, coder.getValueCoder(), windowingStrategy, partitioner);            } else {                                JavaRDD<KV<K, Iterable<WindowedValue<V>>>> groupedByKeyOnly = GroupCombineFunctions.groupByKeyOnly(inRDD, keyCoder, wvCoder, partitioner);                                                groupedByKey = groupedByKeyOnly.flatMap(new SparkGroupAlsoByWindowViaOutputBufferFn<>(windowingStrategy, new TranslationUtils.InMemoryStateInternalsFactory<>(), SystemReduceFn.buffering(coder.getValueCoder()), context.getSerializableOptions()));            }            context.putDataset(transform, new BoundedDataset<>(groupedByKey));        }        @Override        public String toNativeString() {            return "groupByKey()";        }    };}
public String beam_f13459_0()
{    return "aggregate(..., new <fn>(), ...)";}
private static TransformEvaluator<Combine.PerKey<K, InputT, OutputT>> beam_f13460_0()
{    return new TransformEvaluator<Combine.PerKey<K, InputT, OutputT>>() {        @Override        public void evaluate(Combine.PerKey<K, InputT, OutputT> transform, EvaluationContext context) {            final PCollection<KV<K, InputT>> input = context.getInput(transform);                        final KvCoder<K, InputT> inputCoder = (KvCoder<K, InputT>) context.getInput(transform).getCoder();            @SuppressWarnings("unchecked")            final CombineWithContext.CombineFnWithContext<InputT, AccumT, OutputT> combineFn = (CombineWithContext.CombineFnWithContext<InputT, AccumT, OutputT>) CombineFnUtil.toFnWithContext(transform.getFn());            final WindowingStrategy<?, ?> windowingStrategy = input.getWindowingStrategy();            final Map<TupleTag<?>, KV<WindowingStrategy<?, ?>, SideInputBroadcast<?>>> sideInputs = TranslationUtils.getSideInputs(transform.getSideInputs(), context);            final SparkCombineFn<KV<K, InputT>, InputT, AccumT, OutputT> sparkCombineFn = SparkCombineFn.keyed(combineFn, context.getSerializableOptions(), sideInputs, windowingStrategy);            final Coder<AccumT> vaCoder;            try {                vaCoder = combineFn.getAccumulatorCoder(context.getPipeline().getCoderRegistry(), inputCoder.getValueCoder());            } catch (CannotProvideCoderException e) {                throw new IllegalStateException("Could not determine coder for accumulator", e);            }            @SuppressWarnings("unchecked")            JavaRDD<WindowedValue<KV<K, InputT>>> inRdd = ((BoundedDataset<KV<K, InputT>>) context.borrowDataset(transform)).getRDD();            JavaPairRDD<K, SparkCombineFn.WindowedAccumulator<KV<K, InputT>, InputT, AccumT, ?>> accumulatePerKey;            accumulatePerKey = GroupCombineFunctions.combinePerKey(inRdd, sparkCombineFn, inputCoder.getKeyCoder(), inputCoder.getValueCoder(), vaCoder, windowingStrategy);            JavaPairRDD<K, WindowedValue<OutputT>> kwvs = SparkCompat.extractOutput(accumulatePerKey, sparkCombineFn);            JavaRDD<WindowedValue<KV<K, OutputT>>> outRdd = kwvs.map(new TranslationUtils.FromPairFunction()).map(new TranslationUtils.ToKVByWindowInValueFunction<>());            context.putDataset(transform, new BoundedDataset<>(outRdd));        }        @Override        public String toNativeString() {            return "combineByKey(..., new <fn>(), ...)";        }    };}
public void beam_f13461_0(Combine.PerKey<K, InputT, OutputT> transform, EvaluationContext context)
{    final PCollection<KV<K, InputT>> input = context.getInput(transform);        final KvCoder<K, InputT> inputCoder = (KvCoder<K, InputT>) context.getInput(transform).getCoder();    @SuppressWarnings("unchecked")    final CombineWithContext.CombineFnWithContext<InputT, AccumT, OutputT> combineFn = (CombineWithContext.CombineFnWithContext<InputT, AccumT, OutputT>) CombineFnUtil.toFnWithContext(transform.getFn());    final WindowingStrategy<?, ?> windowingStrategy = input.getWindowingStrategy();    final Map<TupleTag<?>, KV<WindowingStrategy<?, ?>, SideInputBroadcast<?>>> sideInputs = TranslationUtils.getSideInputs(transform.getSideInputs(), context);    final SparkCombineFn<KV<K, InputT>, InputT, AccumT, OutputT> sparkCombineFn = SparkCombineFn.keyed(combineFn, context.getSerializableOptions(), sideInputs, windowingStrategy);    final Coder<AccumT> vaCoder;    try {        vaCoder = combineFn.getAccumulatorCoder(context.getPipeline().getCoderRegistry(), inputCoder.getValueCoder());    } catch (CannotProvideCoderException e) {        throw new IllegalStateException("Could not determine coder for accumulator", e);    }    @SuppressWarnings("unchecked")    JavaRDD<WindowedValue<KV<K, InputT>>> inRdd = ((BoundedDataset<KV<K, InputT>>) context.borrowDataset(transform)).getRDD();    JavaPairRDD<K, SparkCombineFn.WindowedAccumulator<KV<K, InputT>, InputT, AccumT, ?>> accumulatePerKey;    accumulatePerKey = GroupCombineFunctions.combinePerKey(inRdd, sparkCombineFn, inputCoder.getKeyCoder(), inputCoder.getValueCoder(), vaCoder, windowingStrategy);    JavaPairRDD<K, WindowedValue<OutputT>> kwvs = SparkCompat.extractOutput(accumulatePerKey, sparkCombineFn);    JavaRDD<WindowedValue<KV<K, OutputT>>> outRdd = kwvs.map(new TranslationUtils.FromPairFunction()).map(new TranslationUtils.ToKVByWindowInValueFunction<>());    context.putDataset(transform, new BoundedDataset<>(outRdd));}
public String beam_f13469_0()
{    return "sparkContext.<readFrom(<source>)>()";}
private static TransformEvaluator<Window.Assign<T>> beam_f13470_0()
{    return new TransformEvaluator<Window.Assign<T>>() {        @Override        public void evaluate(Window.Assign<T> transform, EvaluationContext context) {            @SuppressWarnings("unchecked")            JavaRDD<WindowedValue<T>> inRDD = ((BoundedDataset<T>) context.borrowDataset(transform)).getRDD();            if (TranslationUtils.skipAssignWindows(transform, context)) {                context.putDataset(transform, new BoundedDataset<>(inRDD));            } else {                context.putDataset(transform, new BoundedDataset<>(inRDD.map(new SparkAssignWindowFn<>(transform.getWindowFn()))));            }        }        @Override        public String toNativeString() {            return "map(new <windowFn>())";        }    };}
public void beam_f13471_0(Window.Assign<T> transform, EvaluationContext context)
{    @SuppressWarnings("unchecked")    JavaRDD<WindowedValue<T>> inRDD = ((BoundedDataset<T>) context.borrowDataset(transform)).getRDD();    if (TranslationUtils.skipAssignWindows(transform, context)) {        context.putDataset(transform, new BoundedDataset<>(inRDD));    } else {        context.putDataset(transform, new BoundedDataset<>(inRDD.map(new SparkAssignWindowFn<>(transform.getWindowFn()))));    }}
private static Partitioner beam_f13479_0(EvaluationContext context)
{    Long bundleSize = context.getSerializableOptions().get().as(SparkPipelineOptions.class).getBundleSize();    return (bundleSize > 0) ? null : new HashPartitioner(context.getSparkContext().defaultParallelism());}
private static TransformEvaluator<?> beam_f13480_0(PTransform<?, ?> transform)
{    @Nullable    String urn = PTransformTranslation.urnForTransformOrNull(transform);    return urn == null ? null : EVALUATORS.get(urn);}
public boolean beam_f13481_0(PTransform<?, ?> transform)
{    return EVALUATORS.containsKey(PTransformTranslation.urnForTransformOrNull(transform));}
public static PairFlatMapFunction<Iterator<KV<K, V>>, K, V> beam_f13489_0()
{    return itr -> Iterators.transform(itr, kv -> new Tuple2<>(kv.getKey(), kv.getValue()));}
public KV<K, V> beam_f13490_0(Tuple2<K, V> t2)
{    return KV.of(t2._1(), t2._2());}
public KV<K, V> beam_f13491_0(@Nonnull Tuple2<K, V> t2)
{    return call(t2);}
public static void beam_f13499_0(DoFn<?, ?> doFn)
{    DoFnSignature signature = DoFnSignatures.getSignature(doFn.getClass());    if (signature.stateDeclarations().size() > 0) {        throw new UnsupportedOperationException(String.format("Found %s annotations on %s, but %s cannot yet be used with state in the %s.", DoFn.StateId.class.getSimpleName(), doFn.getClass().getName(), DoFn.class.getSimpleName(), SparkRunner.class.getSimpleName()));    }    if (signature.timerDeclarations().size() > 0) {        throw new UnsupportedOperationException(String.format("Found %s annotations on %s, but %s cannot yet be used with timers in the %s.", DoFn.TimerId.class.getSimpleName(), doFn.getClass().getName(), DoFn.class.getSimpleName(), SparkRunner.class.getSimpleName()));    }}
public static VoidFunction<T> beam_f13500_0()
{    return t -> {        };}
public static PairFlatMapFunction<Iterator<T>, K, V> beam_f13501_0(final PairFunction<T, K, V> pairFunction)
{    return itr -> Iterators.transform(itr, t -> {        try {            return pairFunction.call(t);        } catch (Exception e) {            throw new RuntimeException(e);        }    });}
public static ValueAndCoderLazySerializable<T> beam_f13509_0(T value, Coder<T> coder)
{    return new ValueAndCoderLazySerializable<>(value, coder);}
public T beam_f13510_0(Coder<T> coder)
{    if (!(coderOrBytes instanceof Coder)) {        ByteArrayInputStream bais = new ByteArrayInputStream((byte[]) this.coderOrBytes);        try {            value = coder.decode(bais);        } catch (IOException e) {            throw new IllegalStateException("Error decoding bytes for coder: " + coder, e);        }        this.coderOrBytes = coder;    }    return value;}
protected void beam_f13511_0(long elementByteSize)
{    observedSize += elementByteSize;}
public int beam_f13519_0(ByteArray other)
{    return UnsignedBytes.lexicographicalComparator().compare(value, other.value);}
public static CachedSideInputReader beam_f13520_0(SideInputReader delegate)
{    return new CachedSideInputReader(delegate);}
public T beam_f13521_1(PCollectionView<T> view, BoundedWindow window)
{    @SuppressWarnings("unchecked")    final Cache<Key<T>, Value<T>> materializedCasted = (Cache) SideInputStorage.getMaterializedSideInputs();    Key<T> sideInputKey = new Key<>(view, window);    try {        Value<T> cachedResult = materializedCasted.get(sideInputKey, () -> {            final T result = delegate.get(view, window);                        return new Value<>(result);        });        return cachedResult.getValue();    } catch (ExecutionException e) {        throw new RuntimeException(e.getCause());    }}
private static synchronized LoadingCache<String, Map<Integer, SparkWatermarks>> beam_f13529_0(final Long batchDuration)
{    return CacheBuilder.newBuilder().expireAfterWrite(batchDuration / 2, TimeUnit.MILLISECONDS).build(new WatermarksLoader());}
private static void beam_f13530_1(final String batchId)
{    synchronized (GlobalWatermarkHolder.class) {        final BlockManager blockManager = SparkEnv.get().blockManager();        final Map<Integer, SparkWatermarks> newWatermarks = computeNewWatermarks(blockManager);        if (!newWatermarks.isEmpty()) {            writeRemoteWatermarkBlock(newWatermarks, blockManager);            writeLocalWatermarkCopy(newWatermarks);        } else {                    }    }}
private static void beam_f13531_0(Map<Integer, SparkWatermarks> newWatermarks)
{    driverNodeWatermarks = newWatermarks;}
public static synchronized void beam_f13539_0()
{    sourceTimes.clear();    lastWatermarkedBatchTime = 0;    writeLocalWatermarkCopy(null);    final SparkEnv sparkEnv = SparkEnv.get();    if (sparkEnv != null) {        final BlockManager blockManager = sparkEnv.blockManager();        blockManager.removeBlock(WATERMARKS_BLOCK_ID, true);    }}
public Instant beam_f13540_0()
{    return lowWatermark;}
public Instant beam_f13541_0()
{    return highWatermark;}
public void beam_f13549_0(JavaSparkContext jsc)
{    this.bcast = jsc.broadcast(bytes);}
public void beam_f13550_0()
{    this.bcast.unpersist();}
private T beam_f13551_1()
{    T val;    try {        val = coder.decode(new ByteArrayInputStream(bcast.value()));    } catch (IOException ioe) {                        val = null;    }    return val;}
public static JavaPairRDD<K, WindowedValue<OutputT>> beam_f13559_0(JavaPairRDD<K, SparkCombineFn.WindowedAccumulator<KV<K, InputT>, InputT, AccumT, ?>> accumulatePerKey, SparkCombineFn<KV<K, InputT>, InputT, AccumT, OutputT> sparkCombineFn)
{    try {        if (accumulatePerKey.context().version().startsWith("3")) {            FlatMapFunction<SparkCombineFn.WindowedAccumulator<KV<K, InputT>, InputT, AccumT, ?>, WindowedValue<OutputT>> flatMapFunction = (FlatMapFunction<SparkCombineFn.WindowedAccumulator<KV<K, InputT>, InputT, AccumT, ?>, WindowedValue<OutputT>>) windowedAccumulator -> sparkCombineFn.extractOutputStream(windowedAccumulator).iterator();                                    Method method = accumulatePerKey.getClass().getDeclaredMethod("flatMapValues", FlatMapFunction.class);            Object result = method.invoke(accumulatePerKey, flatMapFunction);            return (JavaPairRDD<K, WindowedValue<OutputT>>) result;        }        Function<SparkCombineFn.WindowedAccumulator<KV<K, InputT>, InputT, AccumT, ?>, Iterable<WindowedValue<OutputT>>> flatMapFunction = windowedAccumulator -> sparkCombineFn.extractOutputStream(windowedAccumulator).collect(Collectors.toList());                        Method method = accumulatePerKey.getClass().getDeclaredMethod("flatMapValues", Function.class);        Object result = method.invoke(accumulatePerKey, flatMapFunction);        return (JavaPairRDD<K, WindowedValue<OutputT>>) result;    } catch (NoSuchMethodException | IllegalAccessException | InvocationTargetException e) {        throw new RuntimeException("Error invoking Spark flatMapValues", e);    }}
public T beam_f13560_0(PCollectionView<T> view, BoundedWindow window)
{        checkNotNull(view, "The PCollectionView passed to sideInput cannot be null ");    KV<WindowingStrategy<?, ?>, SideInputBroadcast<?>> windowedBroadcastHelper = sideInputs.get(view.getTagInternal());    checkNotNull(windowedBroadcastHelper, "SideInput for view " + view + " is not available.");        final BoundedWindow sideInputWindow = view.getWindowMappingFn().getSideInputWindow(window);                Iterable<WindowedValue<KV<?, ?>>> availableSideInputs = (Iterable<WindowedValue<KV<?, ?>>>) windowedBroadcastHelper.getValue().getValue();    Iterable<KV<?, ?>> sideInputForWindow = StreamSupport.stream(availableSideInputs.spliterator(), false).filter(sideInputCandidate -> {        if (sideInputCandidate == null) {            return false;        }        return Iterables.contains(sideInputCandidate.getWindows(), sideInputWindow);    }).collect(Collectors.toList()).stream().map(WindowedValue::getValue).collect(Collectors.toList());    ViewFn<MultimapView, T> viewFn = (ViewFn<MultimapView, T>) view.getViewFn();    Coder keyCoder = ((KvCoder<?, ?>) view.getCoderInternal()).getKeyCoder();    return viewFn.apply(InMemoryMultimapSideInputView.fromIterable(keyCoder, (Iterable) sideInputForWindow));}
public boolean beam_f13561_0(PCollectionView<T> view)
{    return sideInputs.containsKey(view.getTagInternal());}
public void beam_f13572_0(ProcessContext processContext)
{    if (processContext.sideInput(view).contains(processContext.element())) {        processContext.output(processContext.element());    }}
public void beam_f13573_0()
{    SparkPipelineOptions options = createOptions();    options.setCacheDisabled(true);    Pipeline pipeline = Pipeline.create(options);    Values<String> valuesTransform = Create.of("foo", "bar");    PCollection pCollection = mock(PCollection.class);    JavaSparkContext jsc = SparkContextFactory.getSparkContext(options);    EvaluationContext ctxt = new EvaluationContext(jsc, pipeline, options);    ctxt.getCacheCandidates().put(pCollection, 2L);    assertFalse(ctxt.shouldCache(valuesTransform, pCollection));    options.setCacheDisabled(false);    assertTrue(ctxt.shouldCache(valuesTransform, pCollection));    GroupByKey<String, String> gbkTransform = GroupByKey.create();    assertFalse(ctxt.shouldCache(gbkTransform, pCollection));}
private SparkPipelineOptions beam_f13574_0()
{    SparkPipelineOptions options = PipelineOptionsFactory.create().as(TestSparkPipelineOptions.class);    options.setRunner(TestSparkRunner.class);    return options;}
public void beam_f13582_0()
{    JavaSparkContext jsc = SparkContextFactory.getSparkContext(options);    Instant instant = new Instant(0);        GlobalWatermarkHolder.add(1, new SparkWatermarks(instant.plus(Duration.millis(5)), instant.plus(Duration.millis(5)), instant));    GlobalWatermarkHolder.advance();        GlobalWatermarkHolder.add(1, new SparkWatermarks(instant.plus(Duration.millis(10)), instant.plus(Duration.millis(15)), instant.plus(Duration.millis(100))));    GlobalWatermarkHolder.advance();        SparkWatermarks currentWatermarks = GlobalWatermarkHolder.get(0L).get(1);    assertThat(currentWatermarks.getLowWatermark(), equalTo(instant.plus(Duration.millis(10))));    assertThat(currentWatermarks.getHighWatermark(), equalTo(instant.plus(Duration.millis(15))));    assertThat(currentWatermarks.getSynchronizedProcessingTime(), equalTo(instant.plus(Duration.millis(100))));        thrown.expect(IllegalStateException.class);    thrown.expectMessage(RegexMatcher.matches("Low watermark " + INSTANT_PATTERN + " cannot be later then high watermark " + INSTANT_PATTERN));        GlobalWatermarkHolder.add(1, new SparkWatermarks(instant.plus(Duration.millis(25)), instant.plus(Duration.millis(20)), instant.plus(Duration.millis(200))));    GlobalWatermarkHolder.advance();}
public void beam_f13583_0()
{    JavaSparkContext jsc = SparkContextFactory.getSparkContext(options);    Instant instant = new Instant(0);    GlobalWatermarkHolder.add(1, new SparkWatermarks(instant.plus(Duration.millis(5)), instant.plus(Duration.millis(10)), instant));    GlobalWatermarkHolder.advance();    thrown.expect(IllegalStateException.class);    thrown.expectMessage("Synchronized processing time must advance.");            GlobalWatermarkHolder.add(1, new SparkWatermarks(instant.plus(Duration.millis(5)), instant.plus(Duration.millis(10)), instant));    GlobalWatermarkHolder.advance();}
public void beam_f13584_0()
{    JavaSparkContext jsc = SparkContextFactory.getSparkContext(options);    Instant instant = new Instant(0);    GlobalWatermarkHolder.add(1, new SparkWatermarks(instant.plus(Duration.millis(5)), instant.plus(Duration.millis(10)), instant));    GlobalWatermarkHolder.add(2, new SparkWatermarks(instant.plus(Duration.millis(3)), instant.plus(Duration.millis(6)), instant));    GlobalWatermarkHolder.advance();        SparkWatermarks watermarksForSource1 = GlobalWatermarkHolder.get(0L).get(1);    assertThat(watermarksForSource1.getLowWatermark(), equalTo(instant.plus(Duration.millis(5))));    assertThat(watermarksForSource1.getHighWatermark(), equalTo(instant.plus(Duration.millis(10))));        SparkWatermarks watermarksForSource2 = GlobalWatermarkHolder.get(0L).get(2);    assertThat(watermarksForSource2.getLowWatermark(), equalTo(instant.plus(Duration.millis(3))));    assertThat(watermarksForSource2.getHighWatermark(), equalTo(instant.plus(Duration.millis(6))));}
public boolean beam_f13592_0() throws IOException
{    return advance();}
public boolean beam_f13593_0() throws IOException
{    checkState(!drained && !closed);    drained = ++current >= LIMIT;    return !drained;}
public Integer beam_f13594_0() throws NoSuchElementException
{    checkState(!drained && !closed);    return current;}
public void beam_f13602_0()
{    MetricResult<Object> metricResult = MetricResult.create(MetricKey.create("myStep.one.two(three)", MetricName.named("myNameSpace//", "myName()")), 123, 456);    String renderedName = new SparkBeamMetric().renderName(metricResult);    assertThat("Metric name was not rendered correctly", renderedName, equalTo("myStep_one_two_three.myNameSpace__.myName__"));}
private Duration beam_f13603_0()
{    return Duration.millis((pipeline.getOptions().as(SparkPipelineOptions.class)).getBatchIntervalMillis());}
public void beam_f13604_0()
{    TestMetricsSink.clear();    MetricsOptions options = pipeline.getOptions().as(MetricsOptions.class);    options.setMetricsSink(TestMetricsSink.class);}
private void beam_f13612_0(JavaSparkContext jsc)
{    SparkContextOptions options = getSparkContextOptions(jsc);    Pipeline p = Pipeline.create(options);    PCollection<String> inputWords = p.apply(Create.of(WORDS).withCoder(StringUtf8Coder.of()));    inputWords.apply(new WordCount.CountWords()).apply(MapElements.via(new WordCount.FormatAsTextFn()));    try {        p.run().waitUntilFinish();        fail("Should throw an exception when The provided Spark context is null or stopped");    } catch (RuntimeException e) {        assert e.getMessage().contains(PROVIDED_CONTEXT_EXCEPTION);    }}
private static SparkContextOptions beam_f13613_0(JavaSparkContext jsc)
{    final SparkContextOptions options = PipelineOptionsFactory.as(SparkContextOptions.class);    options.setRunner(TestSparkRunner.class);    options.setUsesProvidedSparkContext(true);    options.setProvidedSparkContext(jsc);    options.setEnableSparkMetricSinks(false);    return options;}
public static ReuseSparkContextRule beam_f13614_0()
{    return new ReuseSparkContextRule(false);}
private Pipeline beam_f13622_0(final SparkPipelineOptions options)
{    final Pipeline pipeline = Pipeline.create(options);    final String name = testName.getMethodName() + "(isStreaming=" + options.isStreaming() + ")";    pipeline.apply(getValues(options)).setCoder(StringUtf8Coder.of()).apply(printParDo(name));    return pipeline;}
private void beam_f13623_0(final SparkPipelineOptions options) throws Exception
{    SparkPipelineResult result = null;    try {        final Pipeline pipeline = Pipeline.create(options);        pipeline.apply(getValues(options)).setCoder(StringUtf8Coder.of()).apply(MapElements.via(new SimpleFunction<String, String>() {            @Override            public String apply(final String input) {                throw new MyCustomException(FAILED_THE_BATCH_INTENTIONALLY);            }        }));        result = (SparkPipelineResult) pipeline.run();        result.waitUntilFinish();    } catch (final Exception e) {        assertThat(e, instanceOf(Pipeline.PipelineExecutionException.class));        assertThat(e.getCause(), instanceOf(MyCustomException.class));        assertThat(e.getCause().getMessage(), is(FAILED_THE_BATCH_INTENTIONALLY));        assertThat(result.getState(), is(PipelineResult.State.FAILED));        result.cancel();        return;    }    fail("An injected failure did not affect the pipeline as expected.");}
public String beam_f13624_0(final String input)
{    throw new MyCustomException(FAILED_THE_BATCH_INTENTIONALLY);}
public void beam_f13632_0() throws Exception
{    testFailedPipeline(getStreamingOptions());}
public void beam_f13633_0() throws Exception
{    testFailedPipeline(getBatchOptions());}
public void beam_f13634_0() throws Exception
{    testTimeoutPipeline(getStreamingOptions());}
public void beam_f13642_0(ProcessContext context)
{    context.output(KV.of("bar", context.sideInput(view)));    for (Long i : context.element().getValue()) {        context.output(KV.of(context.element().getKey(), i));    }}
public void beam_f13643_0() throws Exception
{    PipelineOptions options = PipelineOptionsFactory.create();    options.setRunner(CrashingRunner.class);    options.as(PortablePipelineOptions.class).setDefaultEnvironmentType(Environments.ENVIRONMENT_EMBEDDED);    Pipeline pipeline = Pipeline.create(options);    PCollection<KV<String, String>> a = pipeline.apply("impulse", Impulse.create()).apply("A", ParDo.of(new DoFnWithSideEffect<>("A")));    PCollection<KV<String, String>> b = a.apply("B", ParDo.of(new DoFnWithSideEffect<>("B")));    PCollection<KV<String, String>> c = a.apply("C", ParDo.of(new DoFnWithSideEffect<>("C")));        b.apply(GroupByKey.create());    c.apply(GroupByKey.create());    RunnerApi.Pipeline pipelineProto = PipelineTranslation.toProto(pipeline);    JobInvocation jobInvocation = SparkJobInvoker.createJobInvocation("testExecStageWithMultipleOutputs", "testExecStageWithMultipleOutputsRetrievalToken", sparkJobExecutor, pipelineProto, options.as(SparkPipelineOptions.class));    jobInvocation.start();    Assert.assertEquals(Enum.DONE, jobInvocation.getState());}
public void beam_f13644_0() throws Exception
{    PipelineOptions options = PipelineOptionsFactory.create();    options.setRunner(CrashingRunner.class);    options.as(PortablePipelineOptions.class).setDefaultEnvironmentType(Environments.ENVIRONMENT_EMBEDDED);    Pipeline pipeline = Pipeline.create(options);    PCollection<KV<String, Iterable<String>>> f = pipeline.apply("impulse", Impulse.create()).apply("F", ParDo.of(new DoFnWithSideEffect<>("F"))).apply(GroupByKey.create());    f.apply("G", ParDo.of(new DoFnWithSideEffect<>("G")));    f.apply("H", ParDo.of(new DoFnWithSideEffect<>("H")));    RunnerApi.Pipeline pipelineProto = PipelineTranslation.toProto(pipeline);    JobInvocation jobInvocation = SparkJobInvoker.createJobInvocation("testExecStageWithMultipleConsumers", "testExecStageWithMultipleConsumersRetrievalToken", sparkJobExecutor, pipelineProto, options.as(SparkPipelineOptions.class));    jobInvocation.start();    Assert.assertEquals(Enum.DONE, jobInvocation.getState());}
public void beam_f13652_0()
{    assertEquals(ImmutableList.of(SparkRunner.class, TestSparkRunner.class), new SparkRunnerRegistrar.Runner().getPipelineRunners());}
public void beam_f13653_0()
{    for (PipelineOptionsRegistrar registrar : Lists.newArrayList(ServiceLoader.load(PipelineOptionsRegistrar.class).iterator())) {        if (registrar instanceof SparkRunnerRegistrar.Options) {            return;        }    }    fail("Expected to find " + SparkRunnerRegistrar.Options.class);}
public void beam_f13654_0()
{    for (PipelineRunnerRegistrar registrar : Lists.newArrayList(ServiceLoader.load(PipelineRunnerRegistrar.class).iterator())) {        if (registrar instanceof SparkRunnerRegistrar.Runner) {            return;        }    }    fail("Expected to find " + SparkRunnerRegistrar.Runner.class);}
 static ItemFactory<K, V, GlobalWindow> beam_f13669_0(Coder<K> keyCoder, FullWindowedValueCoder<KV<K, V>> winValCoder)
{    return new ItemFactory<>(keyCoder, winValCoder, GlobalWindow.Coder.INSTANCE, GlobalWindow.INSTANCE);}
 static ItemFactory<K, V, W> beam_f13670_0(Coder<K> keyCoder, FullWindowedValueCoder<KV<K, V>> winValCoder, Coder<W> winCoder, W window)
{    return new ItemFactory<>(keyCoder, winValCoder, winCoder, window);}
private Tuple2<ByteArray, byte[]> beam_f13671_0(K key, V value)
{    ByteArray kaw = new ByteArray(Bytes.concat(CoderHelpers.toByteArray(key, keyCoder), windowBytes));    byte[] windowedValue = CoderHelpers.toByteArray(WindowedValue.of(KV.of(key, value), Instant.now(), window, PaneInfo.ON_TIME_AND_ONLY_FIRING), winValCoder);    return new Tuple2<>(kaw, windowedValue);}
public Long beam_f13679_0()
{    return 0L;}
public Long beam_f13680_0(Long mutableAccumulator, Integer input)
{    return mutableAccumulator + input;}
public Long beam_f13681_0(Iterable<Long> accumulators)
{    return StreamSupport.stream(accumulators.spliterator(), false).mapToLong(e -> e).sum();}
public Instant beam_f13689_0()
{    return timestamp;}
public BoundedWindow beam_f13690_0()
{    return GlobalWindow.INSTANCE;}
private void beam_f13691_0(List<T> actual, List<T> expected)
{    assertEquals(actual.stream().collect(Collectors.toSet()), expected.stream().collect(Collectors.toSet()));}
public void beam_f13699_0() throws Exception
{    if (once) {        return;    }        receiverFactory.create("one").accept(three);    receiverFactory.create("two").accept(four);    receiverFactory.create("three").accept(five);    once = true;}
public ProcessBundleDescriptors.ExecutableProcessBundleDescriptor beam_f13700_0()
{    return Mockito.mock(ProcessBundleDescriptors.ExecutableProcessBundleDescriptor.class);}
public void beam_f13702_0() throws Exception
{    SparkExecutableStageFunction<Integer, ?> function = getFunction(Collections.emptyMap());    function.call(Collections.emptyIterator());    verify(stageBundleFactory).getBundle(any(), any(), any());    verify(stageBundleFactory).getProcessBundleDescriptor();    verify(stageBundleFactory).close();    verifyNoMoreInteractions(stageBundleFactory);}
public void beam_f13710_0() throws IOException
{    Instant instant = new Instant(0);    CreateStream<Integer> source1 = CreateStream.of(VarIntCoder.of(), batchDuration()).emptyBatch().advanceWatermarkForNextBatch(instant.plus(Duration.standardMinutes(5))).nextBatch(TimestampedValue.of(1, instant), TimestampedValue.of(2, instant), TimestampedValue.of(3, instant)).advanceNextBatchWatermarkToInfinity();    PCollection<Integer> inputs = p.apply(source1);    final TupleTag<Integer> mainTag = new TupleTag<>();    final TupleTag<Integer> additionalTag = new TupleTag<>();    PCollectionTuple outputs = inputs.apply(ParDo.of(new DoFn<Integer, Integer>() {        @SuppressWarnings("unused")        @ProcessElement        public void process(ProcessContext context) {            Integer element = context.element();            context.output(element);            context.output(additionalTag, element + 1);        }    }).withOutputTags(mainTag, TupleTagList.of(additionalTag)));    PCollection<Integer> output1 = outputs.get(mainTag).setCoder(VarIntCoder.of());    PCollection<Integer> output2 = outputs.get(additionalTag).setCoder(VarIntCoder.of());    PAssert.that(output1).containsInAnyOrder(1, 2, 3);    PAssert.that(output2).containsInAnyOrder(2, 3, 4);    p.run();}
public void beam_f13711_0(ProcessContext context)
{    Integer element = context.element();    context.output(element);    context.output(additionalTag, element + 1);}
public void beam_f13712_0()
{    Instant instant = new Instant(0);    p.apply(CreateStream.of(VarIntCoder.of(), batchDuration()).emptyBatch().advanceWatermarkForNextBatch(instant.plus(Duration.standardMinutes(5))).nextBatch(TimestampedValue.of(1, instant), TimestampedValue.of(2, instant), TimestampedValue.of(3, instant)).advanceNextBatchWatermarkToInfinity()).apply(ParDo.of(new LifecycleDoFn()));    p.run();    assertThat("Function should have been torn down", LifecycleDoFn.teardownCalls.intValue(), is(equalTo(LifecycleDoFn.setupCalls.intValue())));}
public static void beam_f13720_0() throws IOException
{    EMBEDDED_ZOOKEEPER.startup();    EMBEDDED_KAFKA_CLUSTER.startup();}
public void beam_f13721_0()
{    temporaryFolder = new TemporaryFolder();    try {        temporaryFolder.create();    } catch (IOException e) {        throw new RuntimeException(e);    }}
private static void beam_f13722_0(Map<String, Instant> messages)
{    Properties producerProps = new Properties();    producerProps.putAll(EMBEDDED_KAFKA_CLUSTER.getProps());    producerProps.put("request.required.acks", 1);    producerProps.put("bootstrap.servers", EMBEDDED_KAFKA_CLUSTER.getBrokerList());    Serializer<String> stringSerializer = new StringSerializer();    Serializer<Instant> instantSerializer = new InstantSerializer();    try (KafkaProducer<String, Instant> kafkaProducer = new KafkaProducer(producerProps, stringSerializer, instantSerializer)) {        for (Map.Entry<String, Instant> en : messages.entrySet()) {            kafkaProducer.send(new ProducerRecord<>(TOPIC, en.getKey(), en.getValue()));        }    }}
public void beam_f13730_0(ProcessContext c) throws Exception
{    try {        assertThat(c.element(), containsInAnyOrder(expected));        success.inc();    } catch (Throwable t) {        failure.inc();        throw t;    }}
private Duration beam_f13731_0()
{    return Duration.millis((pipeline.getOptions().as(SparkPipelineOptions.class)).getBatchIntervalMillis());}
public void beam_f13732_0() throws Exception
{    Instant instant = new Instant(0);    CreateStream<KV<Integer, Integer>> source1 = CreateStream.of(KvCoder.of(VarIntCoder.of(), VarIntCoder.of()), batchDuration()).emptyBatch().advanceWatermarkForNextBatch(instant).nextBatch(TimestampedValue.of(KV.of(1, 1), instant), TimestampedValue.of(KV.of(1, 2), instant), TimestampedValue.of(KV.of(1, 3), instant)).advanceWatermarkForNextBatch(instant.plus(Duration.standardSeconds(1L))).nextBatch(TimestampedValue.of(KV.of(2, 4), instant.plus(Duration.standardSeconds(1L))), TimestampedValue.of(KV.of(2, 5), instant.plus(Duration.standardSeconds(1L))), TimestampedValue.of(KV.of(2, 6), instant.plus(Duration.standardSeconds(1L)))).advanceNextBatchWatermarkToInfinity();    CreateStream<KV<Integer, Integer>> source2 = CreateStream.of(KvCoder.of(VarIntCoder.of(), VarIntCoder.of()), batchDuration()).emptyBatch().advanceWatermarkForNextBatch(instant).nextBatch(TimestampedValue.of(KV.of(1, 11), instant), TimestampedValue.of(KV.of(1, 12), instant), TimestampedValue.of(KV.of(1, 13), instant)).advanceWatermarkForNextBatch(instant.plus(Duration.standardSeconds(1L))).nextBatch(TimestampedValue.of(KV.of(2, 14), instant.plus(Duration.standardSeconds(1L))), TimestampedValue.of(KV.of(2, 15), instant.plus(Duration.standardSeconds(1L))), TimestampedValue.of(KV.of(2, 16), instant.plus(Duration.standardSeconds(1L)))).advanceNextBatchWatermarkToInfinity();    PCollection<KV<Integer, Integer>> input1 = pipeline.apply("create source1", source1).apply("window input1", Window.<KV<Integer, Integer>>into(FixedWindows.of(Duration.standardSeconds(3L))).withAllowedLateness(Duration.ZERO));    PCollection<KV<Integer, Integer>> input2 = pipeline.apply("create source2", source2).apply("window input2", Window.<KV<Integer, Integer>>into(FixedWindows.of(Duration.standardSeconds(3L))).withAllowedLateness(Duration.ZERO));    PCollection<KV<Integer, CoGbkResult>> output = KeyedPCollectionTuple.of(INPUT1_TAG, input1).and(INPUT2_TAG, input2).apply(CoGroupByKey.create());    PAssert.that("Wrong output of the join using CoGroupByKey in streaming mode", output).satisfies((SerializableFunction<Iterable<KV<Integer, CoGbkResult>>, Void>) input -> {        assertEquals("Wrong size of the output PCollection", 2, Iterables.size(input));        for (KV<Integer, CoGbkResult> element : input) {            if (element.getKey() == 1) {                Iterable<Integer> input1Elements = element.getValue().getAll(INPUT1_TAG);                assertEquals("Wrong number of values for output elements for tag input1 and key 1", 3, Iterables.size(input1Elements));                assertThat("Elements of PCollection input1 for key \"1\" are not present in the output PCollection", input1Elements, containsInAnyOrder(1, 2, 3));                Iterable<Integer> input2Elements = element.getValue().getAll(INPUT2_TAG);                assertEquals("Wrong number of values for output elements for tag input2 and key 1", 3, Iterables.size(input2Elements));                assertThat("Elements of PCollection input2 for key \"1\" are not present in the output PCollection", input2Elements, containsInAnyOrder(11, 12, 13));            } else if (element.getKey() == 2) {                Iterable<Integer> input1Elements = element.getValue().getAll(INPUT1_TAG);                assertEquals("Wrong number of values for output elements for tag input1 and key 2", 3, Iterables.size(input1Elements));                assertThat("Elements of PCollection input1 for key \"2\" are not present in the output PCollection", input1Elements, containsInAnyOrder(4, 5, 6));                Iterable<Integer> input2Elements = element.getValue().getAll(INPUT2_TAG);                assertEquals("Wrong number of values for output elements for tag input2 and key 2", 3, Iterables.size(input2Elements));                assertThat("Elements of PCollection input2 for key \"2\" are not present in the output PCollection", input2Elements, containsInAnyOrder(14, 15, 16));            } else {                fail("Unknown key in the output PCollection");            }        }        return null;    });    pipeline.run();}
public CompositeBehavior beam_f13740_0(TransformHierarchy.Node node)
{    return evaluator.enterCompositeTransform(node);}
public void beam_f13741_0(TransformHierarchy.Node node)
{    PTransform transform = node.getTransform();    if (transform.getClass() == transformClassToAssert) {        AppliedPTransform<?, ?, ?> appliedTransform = node.toAppliedPTransform(getPipeline());        ctxt.setCurrentTransform(appliedTransform);                Dataset dataset = ctxt.borrowDataset((PTransform<? extends PValue, ?>) transform);        assertSourceIds(((UnboundedDataset<?>) dataset).getStreamSources());        ctxt.setCurrentTransform(null);    } else {        evaluator.visitPrimitiveTransform(node);    }}
public void beam_f13742_0(Pipeline p)
{    super.leavePipeline(p);    evaluator.leavePipeline(p);}
public List<Integer> beam_f13750_0()
{    return ports;}
public String beam_f13751_0()
{    return zkConnection;}
public void beam_f13752_1()
{    for (KafkaServerStartable broker : brokers) {        try {            broker.shutdown();        } catch (Exception e) {                    }    }    for (File logDir : logDirs) {        try {            TestUtils.deleteFile(logDir);        } catch (FileNotFoundException e) {                    }    }}
public int beam_f13760_0()
{    return port;}
public int beam_f13761_0()
{    return tickTime;}
public String beam_f13762_0()
{    return "EmbeddedZookeeper{" + "connection=" + getConnection() + "}";}
public MyClass beam_f13770_0(List<?>... sideInputs)
{    return null;}
public List<? extends Coder<?>> beam_f13772_0()
{    return Collections.emptyList();}
public final List<? extends Coder<?>> beam_f13773_0()
{    return Collections.emptyList();}
public Coder<T> beam_f13781_0(TypeDescriptor<T> typeDescriptor, List<? extends Coder<?>> componentCoders) throws CannotProvideCoderException
{    try {        return AvroCoder.of(typeDescriptor);    } catch (AvroRuntimeException e) {        throw new CannotProvideCoderException(String.format("%s is not compatible with Avro", typeDescriptor), e);    }}
private Object beam_f13782_0() throws IOException, ClassNotFoundException
{    return new SerializableSchemaSupplier(new Schema.Parser().parse(schema));}
private Object beam_f13783_0()
{    return new SerializableSchemaString(schema.toString());}
public void beam_f13791_0() throws NonDeterministicException
{    if (!nonDeterministicReasons.isEmpty()) {        throw new NonDeterministicException(this, nonDeterministicReasons);    }}
public Schema beam_f13792_0()
{    return schemaSupplier.get();}
public TypeDescriptor<T> beam_f13793_0()
{    return typeDescriptor;}
private void beam_f13801_0(TypeDescriptor<?> type, Schema schema)
{        Class<?> clazz = type.getRawType();    for (Schema.Field fieldSchema : schema.getFields()) {        Field field = getField(clazz, fieldSchema.name());        String fieldContext = field.getDeclaringClass().getName() + "#" + field.getName();        if (field.isAnnotationPresent(AvroEncode.class)) {            reportError(fieldContext, "Custom encoders may be non-deterministic -- remove @AvroEncode");            continue;        }        if (!IndexedRecord.class.isAssignableFrom(field.getType()) && field.isAnnotationPresent(AvroSchema.class)) {                                    reportError(fieldContext, "Custom schemas are only supported for subtypes of IndexedRecord.");            continue;        }        TypeDescriptor<?> fieldType = type.resolveType(field.getGenericType());        recurse(fieldContext, fieldType, fieldSchema.schema());    }}
private void beam_f13802_0(String context, Schema schema, @Nullable String specificClassStr)
{    if (!activeSchemas.add(schema)) {        reportError(context, "%s appears recursively", schema.getName());        return;    }    switch(schema.getType()) {        case ARRAY:                                                checkIndexedRecord(context, schema.getElementType(), null);            break;        case ENUM:                        break;        case FIXED:                        break;        case MAP:            reportError(context, "GenericRecord and SpecificRecords use a HashMap to represent MAPs," + " so it is non-deterministic");            break;        case RECORD:            for (Schema.Field field : schema.getFields()) {                checkIndexedRecord(schema.getName() + "." + field.name(), field.schema(), field.getProp(SpecificData.CLASS_PROP));            }            break;        case STRING:                        if (specificClassStr != null) {                Class<?> specificClass;                try {                    specificClass = ClassUtils.forName(specificClassStr);                    if (!DETERMINISTIC_STRINGABLE_CLASSES.contains(specificClass)) {                        reportError(context, "Specific class %s is not known to be deterministic", specificClassStr);                    }                } catch (ClassNotFoundException e) {                    reportError(context, "Specific class %s is not known to be deterministic", specificClassStr);                }            }            break;        case UNION:            for (Schema subschema : schema.getTypes()) {                checkIndexedRecord(subschema.getName(), subschema, null);            }            break;        case BOOLEAN:        case BYTES:        case DOUBLE:        case INT:        case FLOAT:        case LONG:        case NULL:                        break;        default:            reportError(context, "Unknown schema type %s may be non-deterministic", schema.getType());            break;    }    activeSchemas.remove(schema);}
private void beam_f13803_0(String context, TypeDescriptor<?> type, Schema schema)
{    if (!isSubtypeOf(type, SortedMap.class)) {        reportError(context, "%s may not be deterministically ordered", type);    }            Class<?> keyType = type.resolveType(Map.class.getTypeParameters()[0]).getRawType();    if (!String.class.equals(keyType)) {        reportError(context, "map keys should be Strings, but was %s", keyType);    }    recurse(context, type.resolveType(Map.class.getTypeParameters()[1]), schema.getValueType());}
public BigDecimal beam_f13811_0(InputStream inStream) throws IOException, CoderException
{    return decode(inStream, Context.NESTED);}
public BigDecimal beam_f13812_0(InputStream inStream, Context context) throws IOException, CoderException
{    int scale = VAR_INT_CODER.decode(inStream);    BigInteger bigInteger = BIG_INT_CODER.decode(inStream, context);    return new BigDecimal(bigInteger, scale);}
public void beam_f13813_0() throws NonDeterministicException
{    VAR_INT_CODER.verifyDeterministic();    BIG_INT_CODER.verifyDeterministic();}
public boolean beam_f13822_0(Integer value)
{    return true;}
public TypeDescriptor<Integer> beam_f13823_0()
{    return TYPE_DESCRIPTOR;}
protected long beam_f13824_0(Integer value) throws Exception
{    if (value == null) {        throw new CoderException("cannot encode a null Integer");    }    return 4;}
public static BigEndianShortCoder beam_f13833_0()
{    return INSTANCE;}
public void beam_f13834_0(Short value, OutputStream outStream) throws IOException
{    if (value == null) {        throw new CoderException("cannot encode a null Short");    }    new DataOutputStream(outStream).writeShort(value);}
public Short beam_f13835_0(InputStream inStream) throws IOException, CoderException
{    try {        return new DataInputStream(inStream).readShort();    } catch (EOFException | UTFDataFormatException exn) {                throw new CoderException(exn);    }}
public BigInteger beam_f13844_0(InputStream inStream) throws IOException, CoderException
{    return decode(inStream, Context.NESTED);}
public BigInteger beam_f13845_0(InputStream inStream, Context context) throws IOException, CoderException
{    return new BigInteger(BYTE_ARRAY_CODER.decode(inStream, context));}
public void beam_f13846_0()
{    BYTE_ARRAY_CODER.verifyDeterministic();}
public BitSet beam_f13854_0(InputStream inStream, Context context) throws CoderException, IOException
{    return BitSet.valueOf(BYTE_ARRAY_CODER.decode(inStream, context));}
public void beam_f13855_0() throws NonDeterministicException
{    verifyDeterministic(this, "BitSetCoder requires its ByteArrayCoder to be deterministic.", BYTE_ARRAY_CODER);}
public boolean beam_f13856_0()
{    return true;}
public void beam_f13864_0(byte[] value, OutputStream outStream) throws IOException, CoderException
{    encode(value, outStream, Context.NESTED);}
public void beam_f13865_0(byte[] value, OutputStream outStream, Context context) throws IOException, CoderException
{    if (value == null) {        throw new CoderException("cannot encode a null byte[]");    }    if (!context.isWholeStream) {        VarInt.encode(value.length, outStream);        outStream.write(value);    } else {        outStream.write(value);    }}
public void beam_f13866_0(byte[] value, OutputStream outStream, Context context) throws IOException, CoderException
{    if (!context.isWholeStream) {        VarInt.encode(value.length, outStream);        outStream.write(value);    } else {        if (outStream instanceof ExposedByteArrayOutputStream) {            ((ExposedByteArrayOutputStream) outStream).writeAndOwn(value);        } else {            outStream.write(value);        }    }}
public void beam_f13875_0(Byte value, OutputStream outStream) throws IOException, CoderException
{    if (value == null) {        throw new CoderException("cannot encode a null Byte");    }    outStream.write(value);}
public Byte beam_f13876_0(InputStream inStream) throws IOException, CoderException
{    try {                int value = inStream.read();        if (value == -1) {            throw new EOFException("EOF encountered decoding 1 byte from input stream");        }        return (byte) value;    } catch (EOFException | UTFDataFormatException exn) {                throw new CoderException(exn);    }}
public boolean beam_f13878_0()
{    return true;}
public int beam_f13886_0()
{    return Objects.hashCode(isWholeStream);}
public String beam_f13887_0()
{    return MoreObjects.toStringHelper(Context.class).addValue(isWholeStream ? "OUTER" : "NESTED").toString();}
public void beam_f13888_0(T value, OutputStream outStream, Context context) throws CoderException, IOException
{    encode(value, outStream);}
protected long beam_f13896_0(T value) throws Exception
{    try (CountingOutputStream os = new CountingOutputStream(ByteStreams.nullOutputStream())) {        encode(value, os);        return os.getCount();    } catch (Exception exn) {        throw new IllegalArgumentException("Unable to encode element '" + value + "' with coder '" + this + "'.", exn);    }}
public TypeDescriptor<T> beam_f13897_0()
{    return (TypeDescriptor<T>) TypeDescriptor.of(getClass()).resolveType(new TypeDescriptor<T>() {    }.getType());}
public Iterable<String> beam_f13898_0()
{    return reasons;}
public String beam_f13906_0()
{    return MoreObjects.toStringHelper(getClass()).add("type", type).add("coder", coder).toString();}
public Coder<T> beam_f13907_0(TypeDescriptor<T> typeDescriptor, List<? extends Coder<?>> componentCoders) throws CannotProvideCoderException
{    CoderProvider factory = commonTypesToCoderProviders.get(typeDescriptor.getRawType());    if (factory == null) {        throw new CannotProvideCoderException(String.format("%s is not one of the common types.", typeDescriptor));    }    return factory.coderFor(typeDescriptor, componentCoders);}
public static CoderRegistry beam_f13908_0()
{    return new CoderRegistry();}
public Coder<OutputT> beam_f13916_0(Class<? extends T> subClass, Class<T> baseClass, Map<Type, ? extends Coder<?>> knownCoders, TypeVariable<?> param) throws CannotProvideCoderException
{    Map<Type, Coder<?>> inferredCoders = getDefaultCoders(subClass, baseClass, knownCoders);    @SuppressWarnings("unchecked")    Coder<OutputT> paramCoderOrNull = (Coder<OutputT>) inferredCoders.get(param);    if (paramCoderOrNull != null) {        return paramCoderOrNull;    } else {        throw new CannotProvideCoderException("Cannot infer coder for type parameter " + param);    }}
private Map<Type, Coder<?>> beam_f13917_0(Class<? extends T> subClass, Class<T> baseClass, Map<Type, ? extends Coder<?>> knownCoders)
{    TypeVariable<Class<T>>[] typeParams = baseClass.getTypeParameters();    Coder<?>[] knownCodersArray = new Coder<?>[typeParams.length];    for (int i = 0; i < typeParams.length; i++) {        knownCodersArray[i] = knownCoders.get(typeParams[i]);    }    Coder<?>[] resultArray = getDefaultCoders(subClass, baseClass, knownCodersArray);    Map<Type, Coder<?>> result = new HashMap<>();    for (int i = 0; i < typeParams.length; i++) {        if (resultArray[i] != null) {            result.put(typeParams[i], resultArray[i]);        }    }    return result;}
private Coder<?>[] beam_f13918_0(Class<? extends T> subClass, Class<T> baseClass, @Nullable Coder<?>[] knownCoders)
{    Type type = TypeDescriptor.of(subClass).getSupertype(baseClass).getType();    if (!(type instanceof ParameterizedType)) {        throw new IllegalArgumentException(type + " is not a ParameterizedType");    }    ParameterizedType parameterizedType = (ParameterizedType) type;    Type[] typeArgs = parameterizedType.getActualTypeArguments();    if (knownCoders == null) {        knownCoders = new Coder<?>[typeArgs.length];    } else if (typeArgs.length != knownCoders.length) {        throw new IllegalArgumentException(String.format("Class %s has %d parameters, but %d coders are requested.", baseClass.getCanonicalName(), typeArgs.length, knownCoders.length));    }    SetMultimap<Type, Coder<?>> context = HashMultimap.create();    for (int i = 0; i < knownCoders.length; i++) {        if (knownCoders[i] != null) {            try {                verifyCompatible(knownCoders[i], typeArgs[i]);            } catch (IncompatibleCoderException exn) {                throw new IllegalArgumentException(String.format("Provided coders for type arguments of %s contain incompatibilities:" + " Cannot encode elements of type %s with coder %s", baseClass, typeArgs[i], knownCoders[i]), exn);            }            context.putAll(getTypeToCoderBindings(typeArgs[i], knownCoders[i]));        }    }    Coder<?>[] result = new Coder<?>[typeArgs.length];    for (int i = 0; i < knownCoders.length; i++) {        if (knownCoders[i] != null) {            result[i] = knownCoders[i];        } else {            try {                result[i] = getCoderFromTypeDescriptor(TypeDescriptor.of(typeArgs[i]), context);            } catch (CannotProvideCoderException exc) {                result[i] = null;            }        }    }    return result;}
private SetMultimap<Type, Coder<?>> beam_f13926_0(Type type, Coder<?> coder)
{    checkArgument(type != null);    checkArgument(coder != null);    if (type instanceof TypeVariable || type instanceof Class) {        return ImmutableSetMultimap.of(type, coder);    } else if (type instanceof ParameterizedType) {        return getTypeToCoderBindings((ParameterizedType) type, coder);    } else {        return ImmutableSetMultimap.of();    }}
private SetMultimap<Type, Coder<?>> beam_f13927_0(ParameterizedType type, Coder<?> coder)
{    List<Type> typeArguments = Arrays.asList(type.getActualTypeArguments());    List<? extends Coder<?>> coderArguments = coder.getCoderArguments();    if ((coderArguments == null) || (typeArguments.size() != coderArguments.size())) {        return ImmutableSetMultimap.of();    } else {        SetMultimap<Type, Coder<?>> typeToCoder = HashMultimap.create();        typeToCoder.put(type, coder);        for (int i = 0; i < typeArguments.size(); i++) {            Type typeArgument = typeArguments.get(i);            Coder<?> coderArgument = coderArguments.get(i);            if (coderArgument != null) {                typeToCoder.putAll(getTypeToCoderBindings(typeArgument, coderArgument));            }        }        return ImmutableSetMultimap.<Type, Coder<?>>builder().putAll(typeToCoder).build();    }}
public static CollectionCoder<T> beam_f13928_0(Coder<T> elemCoder)
{    return new CollectionCoder<>(elemCoder);}
public static DelegateCoder<T, IntermediateT> beam_f13936_0(Coder<IntermediateT> coder, CodingFunction<T, IntermediateT> toFn, CodingFunction<IntermediateT, T> fromFn, @Nullable TypeDescriptor<T> typeDescriptor)
{    return new DelegateCoder<>(coder, toFn, fromFn, typeDescriptor);}
public void beam_f13937_0(T value, OutputStream outStream) throws CoderException, IOException
{    encode(value, outStream, Context.NESTED);}
public void beam_f13938_0(T value, OutputStream outStream, Context context) throws CoderException, IOException
{    coder.encode(applyAndWrapExceptions(toFn, value), outStream, context);}
public String beam_f13946_0()
{    return MoreObjects.toStringHelper(getClass()).add("coder", coder).add("toFn", toFn).add("fromFn", fromFn).toString();}
public TypeDescriptor<T> beam_f13947_0()
{    if (typeDescriptor == null) {        return super.getEncodedTypeDescriptor();    }    return typeDescriptor;}
private String beam_f13948_0(Class<?> delegateClass, String encodingId)
{    return String.format("%s:%s", delegateClass.getName(), encodingId);}
public TypeDescriptor<Double> beam_f13956_0()
{    return TYPE_DESCRIPTOR;}
protected long beam_f13957_0(Double value) throws Exception
{    if (value == null) {        throw new CoderException("cannot encode a null Double");    }    return 8;}
public static DurationCoder beam_f13958_0()
{    return INSTANCE;}
public void beam_f13966_0(ReadableDuration value, ElementByteSizeObserver observer) throws Exception
{    LONG_CODER.registerByteSizeObserver(toLong(value), observer);}
public TypeDescriptor<ReadableDuration> beam_f13967_0()
{    return TYPE_DESCRIPTOR;}
public static FloatCoder beam_f13968_0()
{    return INSTANCE;}
public static InstantCoder beam_f13976_0()
{    return INSTANCE;}
public void beam_f13977_0(Instant value, OutputStream outStream) throws CoderException, IOException
{    if (value == null) {        throw new CoderException("cannot encode a null Instant");    }                            long shiftedMillis = value.getMillis() - Long.MIN_VALUE;    new DataOutputStream(outStream).writeLong(shiftedMillis);}
public Instant beam_f13978_0(InputStream inStream) throws CoderException, IOException
{    long shiftedMillis;    try {        shiftedMillis = new DataInputStream(inStream).readLong();    } catch (EOFException | UTFDataFormatException exn) {                throw new CoderException(exn);    }        return new Instant(shiftedMillis + Long.MIN_VALUE);}
public TypeDescriptor<Iterable<T>> beam_f13987_0()
{    return new TypeDescriptor<Iterable<T>>() {    }.where(new TypeParameter<T>() {    }, getElemCoder().getEncodedTypeDescriptor());}
public Coder<T> beam_f13988_0()
{    return elementCoder;}
public void beam_f13989_0(IterableT iterable, OutputStream outStream) throws IOException, CoderException
{    if (iterable == null) {        throw new CoderException("cannot encode a null " + iterableName);    }    DataOutputStream dataOutStream = new DataOutputStream(outStream);    if (iterable instanceof Collection) {                        Collection<T> collection = (Collection<T>) iterable;        dataOutStream.writeInt(collection.size());        for (T elem : collection) {            elementCoder.encode(elem, dataOutStream);        }    } else {                                dataOutStream.writeInt(-1);        BufferedElementCountingOutputStream countingOutputStream = new BufferedElementCountingOutputStream(dataOutStream);        for (T elem : iterable) {            countingOutputStream.markElementStart();            elementCoder.encode(elem, countingOutputStream);        }        countingOutputStream.finish();    }        dataOutStream.flush();}
public Coder<K> beam_f13997_0()
{    return keyCoder;}
public Coder<V> beam_f13998_0()
{    return valueCoder;}
public void beam_f13999_0(KV<K, V> kv, OutputStream outStream) throws IOException, CoderException
{    encode(kv, outStream, Context.NESTED);}
public boolean beam_f14007_0(KV<K, V> kv)
{    return keyCoder.isRegisterByteSizeObserverCheap(kv.getKey()) && valueCoder.isRegisterByteSizeObserverCheap(kv.getValue());}
public void beam_f14008_0(KV<K, V> kv, ElementByteSizeObserver observer) throws Exception
{    if (kv == null) {        throw new CoderException("cannot encode a null KV");    }    keyCoder.registerByteSizeObserver(kv.getKey(), observer);    valueCoder.registerByteSizeObserver(kv.getValue(), observer);}
public TypeDescriptor<KV<K, V>> beam_f14009_0()
{    return new TypeDescriptor<KV<K, V>>() {    }.where(new TypeParameter<K>() {    }, keyCoder.getEncodedTypeDescriptor()).where(new TypeParameter<V>() {    }, valueCoder.getEncodedTypeDescriptor());}
protected long beam_f14017_0(T value) throws Exception
{    if (valueCoder instanceof StructuredCoder) {                        long valueSize = ((StructuredCoder<T>) valueCoder).getEncodedElementByteSize(value);        return VarInt.getLength(valueSize) + valueSize;    }        return super.getEncodedElementByteSize(value);}
public boolean beam_f14018_0(T value)
{    return valueCoder.isRegisterByteSizeObserverCheap(value);}
public static ListCoder<T> beam_f14019_0(Coder<T> elemCoder)
{    return new ListCoder<>(elemCoder);}
public Coder<V> beam_f14027_0()
{    return valueCoder;}
public void beam_f14028_0(Map<K, V> map, OutputStream outStream) throws IOException, CoderException
{    encode(map, outStream, Context.NESTED);}
public void beam_f14029_0(Map<K, V> map, OutputStream outStream, Context context) throws IOException, CoderException
{    if (map == null) {        throw new CoderException("cannot encode a null Map");    }    DataOutputStream dataOutStream = new DataOutputStream(outStream);    int size = map.size();    dataOutStream.writeInt(size);    if (size == 0) {        return;    }        Iterator<Entry<K, V>> iterator = map.entrySet().iterator();    Entry<K, V> entry = iterator.next();    while (iterator.hasNext()) {        keyCoder.encode(entry.getKey(), outStream);        valueCoder.encode(entry.getValue(), outStream);        entry = iterator.next();    }    keyCoder.encode(entry.getKey(), outStream);    valueCoder.encode(entry.getValue(), outStream, context);}
public TypeDescriptor<Map<K, V>> beam_f14037_0()
{    return new TypeDescriptor<Map<K, V>>() {    }.where(new TypeParameter<K>() {    }, keyCoder.getEncodedTypeDescriptor()).where(new TypeParameter<V>() {    }, valueCoder.getEncodedTypeDescriptor());}
public static NullableCoder<T> beam_f14038_0(Coder<T> valueCoder)
{    if (valueCoder instanceof NullableCoder) {        return (NullableCoder<T>) valueCoder;    }    return new NullableCoder<>(valueCoder);}
public Coder<T> beam_f14039_0()
{    return valueCoder;}
public Object beam_f14047_0(@Nullable T value)
{    if (value == null) {        return Optional.absent();    }    return Optional.of(valueCoder.structuralValue(value));}
public void beam_f14048_0(@Nullable T value, ElementByteSizeObserver observer) throws Exception
{    observer.update(1);    if (value != null) {        valueCoder.registerByteSizeObserver(value, observer);    }}
protected long beam_f14049_0(@Nullable T value) throws Exception
{    if (value == null) {        return 1;    }    if (valueCoder instanceof StructuredCoder) {                return 1 + ((StructuredCoder<T>) valueCoder).getEncodedElementByteSize(value);    }        return super.getEncodedElementByteSize(value);}
public void beam_f14057_0(Row value, OutputStream outStream) throws IOException
{    getDelegateCoder().encode(value, outStream);}
public Row beam_f14058_0(InputStream inStream) throws IOException
{    return getDelegateCoder().decode(inStream);}
public Schema beam_f14059_0()
{    return schema;}
public static Coder<Row> beam_f14067_0(Schema schema)
{            Coder<Row> rowCoder = generatedCoders.get(schema.getUUID());    if (rowCoder == null) {        TypeDescription.Generic coderType = TypeDescription.Generic.Builder.parameterizedType(Coder.class, Row.class).build();        DynamicType.Builder<Coder> builder = (DynamicType.Builder<Coder>) BYTE_BUDDY.subclass(coderType);        builder = createComponentCoders(schema, builder);        builder = implementMethods(schema, builder);        try {            rowCoder = builder.make().load(Coder.class.getClassLoader(), ClassLoadingStrategy.Default.INJECTION).getLoaded().getDeclaredConstructor().newInstance();        } catch (InstantiationException | IllegalAccessException | NoSuchMethodException | InvocationTargetException e) {            throw new RuntimeException("Unable to generate coder for schema " + schema);        }        generatedCoders.put(schema.getUUID(), rowCoder);    }    return rowCoder;}
private static DynamicType.Builder<Coder> beam_f14068_0(Schema schema, DynamicType.Builder<Coder> builder)
{    boolean hasNullableFields = schema.getFields().stream().map(Field::getType).anyMatch(FieldType::getNullable);    return builder.defineMethod("getSchema", Schema.class, Visibility.PRIVATE, Ownership.STATIC).intercept(FixedValue.reference(schema)).defineMethod("hasNullableFields", boolean.class, Visibility.PRIVATE, Ownership.STATIC).intercept(FixedValue.reference(hasNullableFields)).method(ElementMatchers.named("encode")).intercept(new EncodeInstruction()).method(ElementMatchers.named("decode")).intercept(new DecodeInstruction());}
public ByteCodeAppender beam_f14069_0(Target implementationTarget)
{    return (methodVisitor, implementationContext, instrumentedMethod) -> {        StackManipulation manipulation = new StackManipulation.Compound(        FieldAccess.forField(implementationContext.getInstrumentedType().getDeclaredFields().filter(ElementMatchers.named(CODERS_FIELD_NAME)).getOnly()).read(),         MethodVariableAccess.REFERENCE.loadFrom(1),         MethodVariableAccess.REFERENCE.loadFrom(2),         MethodInvocation.invoke(implementationContext.getInstrumentedType().getDeclaredMethods().filter(ElementMatchers.named("hasNullableFields")).getOnly()),         MethodInvocation.invoke(LOADED_TYPE.getDeclaredMethods().filter(ElementMatchers.isStatic().and(ElementMatchers.named("encodeDelegate"))).getOnly()), MethodReturn.VOID);        StackManipulation.Size size = manipulation.apply(methodVisitor, implementationContext);        return new ByteCodeAppender.Size(size.getMaximalSize(), instrumentedMethod.getStackSize());    };}
private static StackManipulation beam_f14077_0(Schema.FieldType fieldType)
{    if (TypeName.LOGICAL_TYPE.equals(fieldType.getTypeName())) {        return getCoder(fieldType.getLogicalType().getBaseType());    } else if (TypeName.ARRAY.equals(fieldType.getTypeName())) {        return listCoder(fieldType.getCollectionElementType());    } else if (TypeName.MAP.equals(fieldType.getTypeName())) {        return mapCoder(fieldType.getMapKeyType(), fieldType.getMapValueType());    } else if (TypeName.ROW.equals(fieldType.getTypeName())) {        checkState(fieldType.getRowSchema().getUUID() != null);        Coder<Row> nestedCoder = generate(fieldType.getRowSchema());        return rowCoder(nestedCoder.getClass());    } else {        StackManipulation primitiveCoder = coderForPrimitiveType(fieldType.getTypeName());        if (fieldType.getNullable()) {            primitiveCoder = new Compound(primitiveCoder, MethodInvocation.invoke(NULLABLE_CODER.getDeclaredMethods().filter(ElementMatchers.named("of")).getOnly()));        }        return primitiveCoder;    }}
private static StackManipulation beam_f14078_0(Schema.FieldType fieldType)
{    StackManipulation componentCoder = getCoder(fieldType);    return new Compound(componentCoder, MethodInvocation.invoke(LIST_CODER_TYPE.getDeclaredMethods().filter(ElementMatchers.named("of")).getOnly()));}
 static StackManipulation beam_f14079_0(Schema.TypeName typeName)
{    return CODER_MAP.get(typeName);}
public static CoderProvider beam_f14087_0()
{    return new SerializableCoderProvider();}
public List<CoderProvider> beam_f14088_0()
{    return ImmutableList.of(getCoderProvider());}
public Coder<T> beam_f14089_0(TypeDescriptor<T> typeDescriptor, List<? extends Coder<?>> componentCoders) throws CannotProvideCoderException
{    if (Serializable.class.isAssignableFrom(typeDescriptor.getRawType())) {        return SerializableCoder.of((TypeDescriptor) typeDescriptor);    }    throw new CannotProvideCoderException("Cannot provide SerializableCoder because " + typeDescriptor + " does not implement Serializable");}
public static SetCoder<T> beam_f14097_0(Coder<T> elementCoder)
{    return new SetCoder<>(elementCoder);}
public void beam_f14098_0() throws NonDeterministicException
{    throw new NonDeterministicException(this, "Ordering of elements in a set may be non-deterministic.");}
public TypeDescriptor<Set<T>> beam_f14099_0()
{    return new TypeDescriptor<Set<T>>() {    }.where(new TypeParameter<T>() {    }, getElemCoder().getEncodedTypeDescriptor());}
public void beam_f14107_0(T value, OutputStream os) throws IOException
{    ByteArrayCoder.of().encode(Snappy.compress(CoderUtils.encodeToByteArray(innerCoder, value)), os);}
public T beam_f14108_0(InputStream is) throws IOException
{    return CoderUtils.decodeFromByteArray(innerCoder, Snappy.uncompress(ByteArrayCoder.of().decode(is)));}
public List<? extends Coder<?>> beam_f14109_0()
{    return ImmutableList.of(innerCoder);}
public void beam_f14117_0(T value, OutputStream outStream, Context context) throws CoderException, IOException
{    delegateCoder.encode(value, outStream, context);}
public T beam_f14118_0(InputStream inStream) throws CoderException, IOException
{    return decode(inStream, Context.NESTED);}
public T beam_f14119_0(InputStream inStream, Context context) throws CoderException, IOException
{    return delegateCoder.decode(inStream, context);}
public void beam_f14127_0(String value, OutputStream outStream, Context context) throws IOException
{    if (value == null) {        throw new CoderException("cannot encode a null String");    }    if (context.isWholeStream) {        byte[] bytes = value.getBytes(StandardCharsets.UTF_8);        if (outStream instanceof ExposedByteArrayOutputStream) {            ((ExposedByteArrayOutputStream) outStream).writeAndOwn(bytes);        } else {            outStream.write(bytes);        }    } else {        writeString(value, outStream);    }}
public String beam_f14128_0(InputStream inStream) throws IOException
{    return decode(inStream, Context.NESTED);}
public String beam_f14129_0(InputStream inStream, Context context) throws IOException
{    if (context.isWholeStream) {        byte[] bytes = StreamUtils.getBytesWithoutClosing(inStream);        return new String(bytes, StandardCharsets.UTF_8);    } else {        try {            return readString(inStream);        } catch (EOFException | UTFDataFormatException exn) {                        throw new CoderException(exn);        }    }}
public List<? extends Coder<?>> beam_f14138_0()
{    return getCoderArguments();}
public boolean beam_f14139_0(Object o)
{    if (o == null || this.getClass() != o.getClass()) {        return false;    }    StructuredCoder<?> that = (StructuredCoder<?>) o;    return this.getComponents().equals(that.getComponents());}
public int beam_f14140_0()
{    return getClass().hashCode() * 31 + getComponents().hashCode();}
public TypeDescriptor<Integer> beam_f14148_0()
{    return TYPE_DESCRIPTOR;}
protected long beam_f14149_0(Integer value) throws Exception
{    if (value == null) {        throw new CoderException("cannot encode a null Integer");    }    String textualValue = value.toString();    return StringUtf8Coder.of().getEncodedElementByteSize(textualValue);}
public static VarIntCoder beam_f14150_0()
{    return INSTANCE;}
public void beam_f14159_0(Long value, OutputStream outStream) throws IOException, CoderException
{    if (value == null) {        throw new CoderException("cannot encode a null Long");    }    VarInt.encode(value, outStream);}
public Long beam_f14160_0(InputStream inStream) throws IOException, CoderException
{    try {        return VarInt.decodeLong(inStream);    } catch (EOFException | UTFDataFormatException exn) {                throw new CoderException(exn);    }}
public List<? extends Coder<?>> beam_f14161_0()
{    return Collections.emptyList();}
public boolean beam_f14172_0(Void value)
{    return true;}
public TypeDescriptor<Void> beam_f14173_0()
{    return TYPE_DESCRIPTOR;}
protected long beam_f14174_0(Void value) throws Exception
{    return 0;}
public static ReadFiles<GenericRecord> beam_f14184_0(String schema)
{    return readFilesGenericRecords(new Schema.Parser().parse(schema));}
public static ReadAll<GenericRecord> beam_f14185_0(String schema)
{    return readAllGenericRecords(new Schema.Parser().parse(schema));}
public static Parse<T> beam_f14186_0(SerializableFunction<GenericRecord, T> parseFn)
{    return new AutoValue_AvroIO_Parse.Builder<T>().setMatchConfiguration(MatchConfiguration.create(EmptyMatchTreatment.DISALLOW)).setParseFn(parseFn).setHintMatchesManyFiles(false).build();}
private static TypedWrite.Builder<UserT, Void, OutputT> beam_f14194_0()
{    return new AutoValue_AvroIO_TypedWrite.Builder<UserT, Void, OutputT>().setFilenameSuffix(null).setShardTemplate(null).setNumShards(0).setCodec(TypedWrite.DEFAULT_SERIALIZABLE_CODEC).setMetadata(ImmutableMap.of()).setWindowedWrites(false).setNoSpilling(false);}
private static PCollection<T> beam_f14195_0(PCollection<T> pc, Class<T> clazz, @Nullable Schema schema)
{    org.apache.beam.sdk.schemas.Schema beamSchema = org.apache.beam.sdk.schemas.utils.AvroUtils.getSchema(clazz, schema);    if (beamSchema != null) {        pc.setSchema(beamSchema, org.apache.beam.sdk.schemas.utils.AvroUtils.getToRowFunction(clazz, schema), org.apache.beam.sdk.schemas.utils.AvroUtils.getFromRowFunction(clazz));    }    return pc;}
public Read<T> beam_f14196_0(ValueProvider<String> filepattern)
{    return toBuilder().setFilepattern(filepattern).build();}
public void beam_f14204_0(DisplayData.Builder builder)
{    super.populateDisplayData(builder);    builder.add(DisplayData.item("inferBeamSchema", getInferBeamSchema()).withLabel("Infer Beam Schema")).addIfNotNull(DisplayData.item("schema", String.valueOf(getSchema()))).addIfNotNull(DisplayData.item("recordClass", getRecordClass()).withLabel("Record Class")).addIfNotNull(DisplayData.item("filePattern", getFilepattern()).withLabel("Input File Pattern")).include("matchConfiguration", getMatchConfiguration());}
private static AvroSource<T> beam_f14205_0(ValueProvider<String> filepattern, EmptyMatchTreatment emptyMatchTreatment, Class<T> recordClass, Schema schema)
{    AvroSource<?> source = AvroSource.from(filepattern).withEmptyMatchTreatment(emptyMatchTreatment);    return recordClass == GenericRecord.class ? (AvroSource<T>) source.withSchema(schema) : source.withSchema(recordClass);}
 ReadFiles<T> beam_f14206_0(long desiredBundleSizeBytes)
{    return toBuilder().setDesiredBundleSizeBytes(desiredBundleSizeBytes).build();}
public ReadAll<T> beam_f14214_0(boolean withBeamSchemas)
{    return toBuilder().setInferBeamSchema(withBeamSchemas).build();}
public PCollection<T> beam_f14215_0(PCollection<String> input)
{    checkNotNull(getSchema(), "schema");    PCollection<T> read = input.apply(FileIO.matchAll().withConfiguration(getMatchConfiguration())).apply(FileIO.readMatches().withDirectoryTreatment(DirectoryTreatment.PROHIBIT)).apply(readFiles(getRecordClass()));    return getInferBeamSchema() ? setBeamSchema(read, getRecordClass(), getSchema()) : read;}
public void beam_f14216_0(DisplayData.Builder builder)
{    super.populateDisplayData(builder);    builder.add(DisplayData.item("inferBeamSchema", getInferBeamSchema()).withLabel("Infer Beam Schema")).addIfNotNull(DisplayData.item("schema", String.valueOf(getSchema()))).addIfNotNull(DisplayData.item("recordClass", getRecordClass()).withLabel("Record Class")).include("matchConfiguration", getMatchConfiguration());}
public Parse<T> beam_f14224_0(Coder<T> coder)
{    return toBuilder().setCoder(coder).build();}
public Parse<T> beam_f14225_0()
{    return toBuilder().setHintMatchesManyFiles(true).build();}
public PCollection<T> beam_f14226_0(PBegin input)
{    checkNotNull(getFilepattern(), "filepattern");    Coder<T> coder = inferCoder(getCoder(), getParseFn(), input.getPipeline().getCoderRegistry());    if (getMatchConfiguration().getWatchInterval() == null && !getHintMatchesManyFiles()) {        return input.apply(org.apache.beam.sdk.io.Read.from(AvroSource.from(getFilepattern()).withParseFn(getParseFn(), coder)));    }        return input.apply("Create filepattern", Create.ofProvider(getFilepattern(), StringUtf8Coder.of())).apply("Match All", FileIO.matchAll().withConfiguration(getMatchConfiguration())).apply("Read Matches", FileIO.readMatches().withDirectoryTreatment(DirectoryTreatment.PROHIBIT)).apply("Via ParseFiles", parseFilesGenericRecords(getParseFn()).withCoder(coder));}
public ParseAll<T> beam_f14234_0(MatchConfiguration configuration)
{    return toBuilder().setMatchConfiguration(configuration).build();}
public ParseAll<T> beam_f14235_0(EmptyMatchTreatment treatment)
{    return withMatchConfiguration(getMatchConfiguration().withEmptyMatchTreatment(treatment));}
public ParseAll<T> beam_f14236_0(Duration pollInterval, TerminationCondition<String, ?> terminationCondition)
{    return withMatchConfiguration(getMatchConfiguration().continuously(pollInterval, terminationCondition));}
public TypedWrite<UserT, DestinationT, OutputT> beam_f14244_0(ValueProvider<String> outputPrefix)
{    return toResource(NestedValueProvider.of(outputPrefix,     new OutputPrefixToResourceId()));}
public TypedWrite<UserT, DestinationT, OutputT> beam_f14245_0(ValueProvider<ResourceId> outputPrefix)
{    return toBuilder().setFilenamePrefix(outputPrefix).build();}
public TypedWrite<UserT, DestinationT, OutputT> beam_f14246_0(FilenamePolicy filenamePolicy)
{    return toBuilder().setFilenamePolicy(filenamePolicy).build();}
public TypedWrite<UserT, DestinationT, OutputT> beam_f14254_0(int numShards)
{    checkArgument(numShards >= 0);    return toBuilder().setNumShards(numShards).build();}
public TypedWrite<UserT, DestinationT, OutputT> beam_f14255_0()
{    return withNumShards(1).withShardNameTemplate("");}
public TypedWrite<UserT, DestinationT, OutputT> beam_f14256_0()
{    return toBuilder().setWindowedWrites(true).build();}
public Write<T> beam_f14264_0(ResourceId outputPrefix)
{    return new Write<>(inner.to(outputPrefix).withFormatFunction(SerializableFunctions.identity()));}
public Write<T> beam_f14265_0(ValueProvider<String> outputPrefix)
{    return new Write<>(inner.to(outputPrefix).withFormatFunction(SerializableFunctions.identity()));}
public Write<T> beam_f14266_0(ValueProvider<ResourceId> outputPrefix)
{    return new Write<>(inner.toResource(outputPrefix).withFormatFunction(SerializableFunctions.identity()));}
public Write<T> beam_f14274_0(int numShards)
{    return new Write<>(inner.withNumShards(numShards));}
public Write<T> beam_f14275_0()
{    return new Write<>(inner.withoutSharding());}
public Write<T> beam_f14276_0()
{    return new Write<>(inner.withWindowedWrites());}
public static Sink<ElementT> beam_f14284_0(Schema schema)
{    return sink(schema.toString());}
public static Sink<ElementT> beam_f14285_0(String jsonSchema)
{    return new AutoValue_AvroIO_Sink.Builder<ElementT>().setJsonSchema(jsonSchema).setMetadata(ImmutableMap.of()).setCodec(TypedWrite.DEFAULT_SERIALIZABLE_CODEC).build();}
public static Sink<ElementT> beam_f14286_0(Schema schema, RecordFormatter<ElementT> formatter)
{    return new AutoValue_AvroIO_Sink.Builder<ElementT>().setRecordFormatter(formatter).setJsonSchema(schema.toString()).setMetadata(ImmutableMap.of()).setCodec(TypedWrite.DEFAULT_SERIALIZABLE_CODEC).build();}
public Writer<DestinationT, OutputT> beam_f14294_0() throws Exception
{    return new AvroWriter<>(this, dynamicDestinations, genericRecords);}
protected void beam_f14295_0(WritableByteChannel channel) throws Exception
{    DestinationT destination = getDestination();    CodecFactory codec = dynamicDestinations.getCodec(destination);    Schema schema = dynamicDestinations.getSchema(destination);    Map<String, Object> metadata = dynamicDestinations.getMetadata(destination);    DatumWriter<OutputT> datumWriter = genericRecords ? new GenericDatumWriter<>(schema) : new ReflectDatumWriter<>(schema);    dataFileWriter = new DataFileWriter<>(datumWriter).setCodec(codec);    for (Map.Entry<String, Object> entry : metadata.entrySet()) {        Object v = entry.getValue();        if (v instanceof String) {            dataFileWriter.setMeta(entry.getKey(), (String) v);        } else if (v instanceof Long) {            dataFileWriter.setMeta(entry.getKey(), (Long) v);        } else if (v instanceof byte[]) {            dataFileWriter.setMeta(entry.getKey(), (byte[]) v);        } else {            throw new IllegalStateException("Metadata value type must be one of String, Long, or byte[]. Found " + v.getClass().getSimpleName());        }    }    dataFileWriter.create(schema, Channels.newOutputStream(channel));}
public void beam_f14296_0(OutputT value) throws Exception
{    dataFileWriter.append(value);}
public static AvroSource<GenericRecord> beam_f14304_0(ValueProvider<String> fileNameOrPattern)
{    return new AvroSource<>(fileNameOrPattern, EmptyMatchTreatment.DISALLOW, DEFAULT_MIN_BUNDLE_SIZE, readGenericRecordsWithSchema(null));}
public static AvroSource<GenericRecord> beam_f14305_0(Metadata metadata)
{    return new AvroSource<>(metadata, DEFAULT_MIN_BUNDLE_SIZE, 0, metadata.sizeBytes(), readGenericRecordsWithSchema(null));}
public static AvroSource<GenericRecord> beam_f14306_0(String fileNameOrPattern)
{    return from(ValueProvider.StaticValueProvider.of(fileNameOrPattern));}
public BlockBasedSource<T> beam_f14314_0(String fileName, long start, long end) throws IOException
{    return createForSubrangeOfFile(FileSystems.matchSingleFileSpec(fileName), start, end);}
public BlockBasedSource<T> beam_f14315_0(Metadata fileMetadata, long start, long end)
{    return new AvroSource<>(fileMetadata, getMinBundleSize(), start, end, mode);}
protected BlockBasedReader<T> beam_f14316_0(PipelineOptions options)
{    return new AvroReader<>(this);}
private static synchronized Schema beam_f14324_0(String schemaString)
{    Schema schema = schemaLogicalReferenceCache.get(schemaString);    if (schema != null) {        return schema;    }    Schema.Parser parser = new Schema.Parser();    schema = parser.parse(schemaString);    schemaLogicalReferenceCache.put(schemaString, schema);    return schema;}
private Object beam_f14325_0() throws ObjectStreamException
{    switch(getMode()) {        case SINGLE_FILE_OR_SUBRANGE:            return new AvroSource<>(getSingleFileMetadata(), getMinBundleSize(), getStartOffset(), getEndOffset(), mode);        case FILEPATTERN:            return new AvroSource<>(getFileOrPatternSpecProvider(), getEmptyMatchTreatment(), getMinBundleSize(), mode);        default:            throw new InvalidObjectException(String.format("Unknown mode %s for AvroSource %s", getMode(), this));    }}
private static InputStream beam_f14326_0(byte[] data, String codec) throws IOException
{    ByteArrayInputStream byteStream = new ByteArrayInputStream(data);    switch(codec) {        case DataFileConstants.SNAPPY_CODEC:            return new SnappyCompressorInputStream(byteStream, 1 << 16);        case DataFileConstants.DEFLATE_CODEC:                        Inflater inflater = new Inflater(true);            return new InflaterInputStream(byteStream, inflater);        case DataFileConstants.XZ_CODEC:            return new XZCompressorInputStream(byteStream);        case DataFileConstants.BZIP2_CODEC:            return new BZip2CompressorInputStream(byteStream);        case DataFileConstants.NULL_CODEC:            return byteStream;        default:            throw new IllegalArgumentException("Unsupported codec: " + codec);    }}
public long beam_f14334_0()
{    synchronized (progressLock) {        return currentBlockSizeBytes;    }}
public long beam_f14335_0()
{    if (isDone()) {        return 0;    }    synchronized (progressLock) {        if (currentBlockOffset + currentBlockSizeBytes >= getCurrentSource().getEndOffset()) {                        return 1;        }    }    return super.getSplitPointsRemaining();}
private PushbackInputStream beam_f14336_0(ReadableByteChannel channel)
{    return new PushbackInputStream(Channels.newInputStream(channel), metadata.getSyncMarker().length);}
protected long beam_f14344_0()
{    return getCurrentBlockOffset();}
public BoundedReadFromUnboundedSource<T> beam_f14345_0(long maxNumRecords)
{    return new BoundedReadFromUnboundedSource<>(source, maxNumRecords, maxReadTime);}
public BoundedReadFromUnboundedSource<T> beam_f14346_0(Duration maxReadTime)
{    return new BoundedReadFromUnboundedSource<>(source, maxNumRecords, maxReadTime);}
private boolean beam_f14354_0(UnboundedReader<T> reader, Instant endTime) throws IOException
{        BackOff backoff = BACKOFF_FACTORY.backoff();    long nextSleep = backoff.nextBackOffMillis();    while (true) {        if (nextSleep == BackOff.STOP || (endTime != null && Instant.now().isAfter(endTime))) {            return false;        }        if (reader.advance()) {            return true;        }        Uninterruptibles.sleepUninterruptibly(nextSleep, TimeUnit.MILLISECONDS);        nextSleep = backoff.nextBackOffMillis();    }}
public Double beam_f14355_0()
{    return null;}
public long beam_f14356_0()
{    return SPLIT_POINTS_UNKNOWN;}
public static CompressedSource<T> beam_f14364_0(FileBasedSource<T> sourceDelegate)
{    return new CompressedSource<>(sourceDelegate, CompressionMode.AUTO);}
public CompressedSource<T> beam_f14365_0(DecompressingChannelFactory channelFactory)
{    return new CompressedSource<>(this.sourceDelegate, channelFactory);}
public CompressedSource<T> beam_f14366_0(Compression compression)
{    return withDecompression(CompressionMode.fromCanonical(compression));}
public T beam_f14374_0() throws NoSuchElementException
{    return readerDelegate.getCurrent();}
public boolean beam_f14375_0()
{    return false;}
public final long beam_f14376_0()
{    synchronized (progressLock) {        return (isDone() && numRecordsRead > 0) ? 1 : 0;    }}
protected final boolean beam_f14384_0() throws IOException
{    if (!readerDelegate.readNextRecord()) {        return false;    }    synchronized (progressLock) {        ++numRecordsRead;    }    return true;}
protected final long beam_f14385_0() throws NoSuchElementException
{    synchronized (progressLock) {        if (numRecordsRead <= 1) {                        return 0;        }        return channel.getCount();    }}
public Instant beam_f14386_0() throws NoSuchElementException
{    return readerDelegate.getCurrentTimestamp();}
public WritableByteChannel beam_f14394_0(WritableByteChannel channel)
{    throw new UnsupportedOperationException("AUTO is applicable only to reading files");}
public ReadableByteChannel beam_f14395_0(ReadableByteChannel channel)
{    return channel;}
public WritableByteChannel beam_f14396_0(WritableByteChannel channel)
{    return channel;}
public WritableByteChannel beam_f14404_0(WritableByteChannel channel) throws IOException
{    return Channels.newChannel(new ZstdCompressorOutputStream(Channels.newOutputStream(channel)));}
public ReadableByteChannel beam_f14405_0(ReadableByteChannel channel) throws IOException
{    return Channels.newChannel(new DeflateCompressorInputStream(Channels.newInputStream(channel)));}
public WritableByteChannel beam_f14406_0(WritableByteChannel channel) throws IOException
{    return Channels.newChannel(new DeflateCompressorOutputStream(Channels.newOutputStream(channel)));}
public Map<String, Object> beam_f14414_0(Void destination)
{    return metadata;}
public CodecFactory beam_f14415_0(Void destination)
{    return codec.getCodec();}
public void beam_f14416_0(DisplayData.Builder builder)
{    filenamePolicy.populateDisplayData(builder);    builder.add(DisplayData.item("schema", schema.get().toString()).withLabel("Record Schema"));    builder.addIfNotDefault(DisplayData.item("codec", codec.getCodec().toString()).withLabel("Avro Compression Codec"), AvroIO.TypedWrite.DEFAULT_SERIALIZABLE_CODEC.toString());    builder.include("Metadata", new Metadata());}
public int beam_f14424_0()
{    return getClass().hashCode();}
public long beam_f14425_0()
{    return 8;}
public long beam_f14426_0(PipelineOptions options) throws Exception
{    return getEndOffset();}
public synchronized BoundedCountingSource beam_f14434_0()
{    return (BoundedCountingSource) super.getCurrentSource();}
public Long beam_f14435_0() throws NoSuchElementException
{    return current;}
protected boolean beam_f14436_0() throws IOException
{    current = getCurrentSource().getStartOffset();    return true;}
public boolean beam_f14445_0(Object other)
{    if (!(other instanceof UnboundedCountingSource)) {        return false;    }    UnboundedCountingSource that = (UnboundedCountingSource) other;    return this.start == that.start && this.stride == that.stride && this.elementsPerPeriod == that.elementsPerPeriod && Objects.equals(this.period, that.period) && Objects.equals(this.timestampFn, that.timestampFn);}
public int beam_f14446_0()
{    return Objects.hash(start, stride, elementsPerPeriod, period, timestampFn);}
public boolean beam_f14447_0() throws IOException
{    if (firstStarted == null) {        this.firstStarted = Instant.now();    }    return advance();}
public long beam_f14456_0()
{    long expected = expectedValue();    long backlogElements = (expected - current) / source.stride;    return Math.max(0L, 8 * backlogElements);}
public long beam_f14457_0()
{    return lastEmitted;}
public Instant beam_f14458_0()
{    return startTime;}
public String beam_f14467_0()
{    return MoreObjects.toStringHelper(this).add("baseFilename", baseFilename).add("shardTemplate", shardTemplate).add("suffix", suffix).toString();}
public static ParamsCoder beam_f14468_0()
{    return INSTANCE;}
public void beam_f14469_0(Params value, OutputStream outStream) throws IOException
{    if (value == null) {        throw new CoderException("cannot encode a null value");    }    stringCoder.encode(value.baseFilename.get().toString(), outStream);    stringCoder.encode(value.shardTemplate, outStream);    stringCoder.encode(value.suffix, outStream);}
private String beam_f14477_0(PaneInfo paneInfo)
{    String paneString = String.format("pane-%d", paneInfo.getIndex());    if (paneInfo.getTiming() == Timing.LATE) {        paneString = String.format("%s-late", paneString);    }    if (paneInfo.isLast()) {        paneString = String.format("%s-last", paneString);    }    return paneString;}
public void beam_f14478_0(DisplayData.Builder builder)
{    String displayBaseFilename = params.baseFilename.isAccessible() ? params.baseFilename.get().toString() : ("(" + params.baseFilename + ")");    builder.add(DisplayData.item("filenamePattern", String.format("%s%s%s", displayBaseFilename, params.shardTemplate, params.suffix)).withLabel("Filename pattern"));    builder.add(DisplayData.item("filePrefix", params.baseFilename).withLabel("Output File Prefix"));    builder.add(DisplayData.item("shardNameTemplate", params.shardTemplate).withLabel("Output Shard Name Template"));    builder.add(DisplayData.item("fileSuffix", params.suffix).withLabel("Output file Suffix"));}
private static String beam_f14479_0(ResourceId input)
{    if (input.isDirectory()) {        return "";    } else {        return firstNonNull(input.getFilename(), "");    }}
public void beam_f14487_0(DisplayData.Builder builder)
{    checkState(filenamePolicy != null);    filenamePolicy.populateDisplayData(builder);}
public OutputT beam_f14488_0(UserT record)
{    return formatFunction.apply(record);}
public Params beam_f14489_0(UserT element)
{    return destinationFunction.apply(element);}
public String beam_f14497_0()
{    return (canonical == Compression.UNCOMPRESSED) ? null : MimeTypes.BINARY;}
public WritableByteChannel beam_f14498_0(WritableByteChannel channel) throws IOException
{    return canonical.writeCompressed(channel);}
public static CompressionType beam_f14499_0(Compression canonical)
{    switch(canonical) {        case AUTO:            throw new IllegalArgumentException("AUTO is not supported for writing");        case UNCOMPRESSED:            return UNCOMPRESSED;        case GZIP:            return GZIP;        case BZIP2:            return BZIP2;        case ZIP:            throw new IllegalArgumentException("ZIP is unsupported");        case ZSTD:            return ZSTD;        case DEFLATE:            return DEFLATE;        default:            throw new UnsupportedOperationException("Unsupported compression type: " + canonical);    }}
 final Coder<DestinationT> beam_f14508_0(CoderRegistry registry) throws CannotProvideCoderException
{    Coder<DestinationT> destinationCoder = getDestinationCoder();    if (destinationCoder != null) {        return destinationCoder;    }        @Nullable    TypeDescriptor<DestinationT> descriptor = extractFromTypeParameters(this, DynamicDestinations.class, new TypeVariableExtractor<DynamicDestinations<UserT, DestinationT, OutputT>, DestinationT>() {    });    try {        return registry.getCoder(descriptor);    } catch (CannotProvideCoderException e) {        throw new CannotProvideCoderException("Failed to infer coder for DestinationT from type " + descriptor + ", please provide it explicitly by overriding getDestinationCoder()", e);    }}
public ResourceId beam_f14510_0(ResourceId input)
{    return input.getCurrentDirectory();}
public DynamicDestinations<UserT, DestinationT, OutputT> beam_f14511_0()
{    return (DynamicDestinations<UserT, DestinationT, OutputT>) dynamicDestinations;}
private Collection<FileResult<DestinationT>> beam_f14520_1(@Nullable DestinationT dest, @Nullable Integer numShards, Collection<FileResult<DestinationT>> existingResults) throws Exception
{    Collection<FileResult<DestinationT>> completeResults;        if (numShards != null) {        checkArgument(existingResults.size() <= numShards, "Fixed sharding into %s shards was specified, but got %s file results", numShards, existingResults.size());    }            Set<Integer> missingShardNums;    if (numShards == null) {        missingShardNums = existingResults.isEmpty() ? ImmutableSet.of(UNKNOWN_SHARDNUM) : ImmutableSet.of();    } else {        missingShardNums = Sets.newHashSet();        for (int i = 0; i < numShards; ++i) {            missingShardNums.add(i);        }        for (FileResult<DestinationT> res : existingResults) {            checkArgument(res.getShard() != UNKNOWN_SHARDNUM, "Fixed sharding into %s shards was specified, " + "but file result %s does not specify a shard", numShards, res);            missingShardNums.remove(res.getShard());        }    }    completeResults = Lists.newArrayList(existingResults);    if (!missingShardNums.isEmpty()) {                for (int shard : missingShardNums) {            String uuid = UUID.randomUUID().toString();                        Writer<DestinationT, ?> writer = createWriter();            writer.setDestination(dest);                        writer.open(uuid);            writer.close();            completeResults.add(new FileResult<>(writer.getOutputFile(), shard, GlobalWindow.INSTANCE, PaneInfo.ON_TIME_AND_ONLY_FIRING, dest));        }            }    return completeResults;}
 final void beam_f14521_1(List<KV<FileResult<DestinationT>, ResourceId>> resultsToFinalFilenames) throws IOException
{    int numFiles = resultsToFinalFilenames.size();        List<ResourceId> srcFiles = new ArrayList<>();    List<ResourceId> dstFiles = new ArrayList<>();    for (KV<FileResult<DestinationT>, ResourceId> entry : resultsToFinalFilenames) {        srcFiles.add(entry.getKey().getTempFilename());        dstFiles.add(entry.getValue());            }            FileSystems.rename(srcFiles, dstFiles, StandardMoveOptions.IGNORE_MISSING_FILES);    removeTemporaryFiles(srcFiles);}
 final void beam_f14522_1(Collection<ResourceId> knownFiles, boolean shouldRemoveTemporaryDirectory) throws IOException
{    ResourceId tempDir = tempDirectory.get();                                Set<ResourceId> allMatches = new HashSet<>(knownFiles);    for (ResourceId match : allMatches) {            }        if (shouldRemoveTemporaryDirectory) {        try {            MatchResult singleMatch = Iterables.getOnlyElement(FileSystems.match(Collections.singletonList(tempDir.toString() + "*")));            for (Metadata matchResult : singleMatch.metadata()) {                if (allMatches.add(matchResult.resourceId())) {                                    }            }        } catch (Exception e) {                    }    }    FileSystems.delete(allMatches, StandardMoveOptions.IGNORE_MISSING_FILES);    if (shouldRemoveTemporaryDirectory) {                try {            FileSystems.delete(Collections.singletonList(tempDir), StandardMoveOptions.IGNORE_MISSING_FILES);        } catch (Exception e) {                    }    }}
public final void beam_f14533_1() throws Exception
{    checkState(outputFile != null, "FileResult.close cannot be called with a null outputFile");        try {        writeFooter();    } catch (Exception e) {        closeChannelAndThrow(channel, outputFile, e);    }    try {        finishWrite();    } catch (Exception e) {        closeChannelAndThrow(channel, outputFile, e);    }        if (channel.isOpen()) {                try {            channel.close();        } catch (Exception e) {            throw new IOException(String.format("Failed closing channel to %s", outputFile), e);        }    }    }
public WriteOperation<DestinationT, OutputT> beam_f14534_0()
{    return writeOperation;}
 void beam_f14535_0(DestinationT destination)
{    this.destination = destination;}
public ResourceId beam_f14543_0(boolean windowedWrites, DynamicDestinations<?, DestinationT, ?> dynamicDestinations, int numShards, OutputFileHints outputFileHints)
{    checkArgument(getShard() != UNKNOWN_SHARDNUM);    checkArgument(numShards > 0);    FilenamePolicy policy = dynamicDestinations.getFilenamePolicy(destination);    if (windowedWrites) {        return policy.windowedFilename(getShard(), numShards, getWindow(), getPaneInfo(), outputFileHints);    } else {        return policy.unwindowedFilename(getShard(), numShards, outputFileHints);    }}
public String beam_f14544_0()
{    return MoreObjects.toStringHelper(FileResult.class).add("tempFilename", tempFilename).add("shard", shard).add("window", window).add("paneInfo", paneInfo).toString();}
public static FileResultCoder<DestinationT> beam_f14545_0(Coder<BoundedWindow> windowCoder, Coder<DestinationT> destinationCoder)
{    return new FileResultCoder<>(windowCoder, destinationCoder);}
public final EmptyMatchTreatment beam_f14553_0()
{    return emptyMatchTreatment;}
public final Mode beam_f14554_0()
{    return mode;}
public final FileBasedSource<T> beam_f14555_0(long start, long end)
{    checkArgument(mode != Mode.FILEPATTERN, "Cannot split a file pattern based source based on positions");    checkArgument(start >= getStartOffset(), "Start offset value %s of the subrange cannot be smaller than the start offset value %s" + " of the parent source", start, getStartOffset());    checkArgument(end <= getEndOffset(), "End offset value %s of the subrange cannot be larger than the end offset value %s", end, getEndOffset());    checkState(singleFileMetadata != null, "A single file source should not have null metadata: %s", this);    FileBasedSource<T> source = createForSubrangeOfFile(singleFileMetadata, start, end);    if (start > 0 || end != Long.MAX_VALUE) {        checkArgument(source.getMode() == Mode.SINGLE_FILE_OR_SUBRANGE, "Source created for the range [%s,%s) must be a subrange source", start, end);    }    return source;}
public final long beam_f14563_0(PipelineOptions options) throws IOException
{    checkArgument(mode != Mode.FILEPATTERN, "Cannot determine the exact end offset of a file pattern");    Metadata metadata = getSingleFileMetadata();    return metadata.sizeBytes();}
public synchronized FileBasedSource<T> beam_f14564_0()
{    return (FileBasedSource<T>) super.getCurrentSource();}
protected final boolean beam_f14565_0() throws IOException
{    FileBasedSource<T> source = getCurrentSource();    this.channel = FileSystems.open(source.getSingleFileMetadata().resourceId());    if (channel instanceof SeekableByteChannel) {        SeekableByteChannel seekChannel = (SeekableByteChannel) channel;        seekChannel.position(source.getStartOffset());    } else {                checkArgument(source.mode != Mode.SINGLE_FILE_OR_SUBRANGE, "Subrange-based sources must only be defined for file types that support seekable " + " read channels");        checkArgument(source.getStartOffset() == 0, "Start offset %s is not zero but channel for reading the file is not seekable.", source.getStartOffset());    }    startReading(channel);        return advanceImpl();}
public Instant beam_f14573_0() throws NoSuchElementException
{        return currentReader.getCurrentTimestamp();}
public void beam_f14574_0() throws IOException
{        if (currentReader != null) {        currentReader.close();    }    while (fileReadersIterator.hasNext()) {        fileReadersIterator.next().close();    }}
public FileBasedSource<T> beam_f14575_0()
{    return source;}
public MatchResult.Metadata beam_f14583_0()
{    return metadata;}
public Compression beam_f14584_0()
{    return compression;}
public ReadableByteChannel beam_f14585_0() throws IOException
{    return compression.readDecompressed(FileSystems.open(metadata.resourceId()));}
public MatchConfiguration beam_f14593_0(EmptyMatchTreatment treatment)
{    return toBuilder().setEmptyMatchTreatment(treatment).build();}
public MatchConfiguration beam_f14594_0(Duration interval, TerminationCondition<String, ?> condition)
{    return toBuilder().setWatchInterval(interval).setWatchTerminationCondition(condition).build();}
public void beam_f14595_0(DisplayData.Builder builder)
{    builder.add(DisplayData.item("emptyMatchTreatment", getEmptyMatchTreatment().toString()).withLabel("Treatment of filepatterns that match no files")).addIfNotNull(DisplayData.item("watchForNewFilesInterval", getWatchInterval()).withLabel("Interval to watch for new files"));}
public MatchAll beam_f14603_0(MatchConfiguration configuration)
{    return toBuilder().setConfiguration(configuration).build();}
public MatchAll beam_f14604_0(EmptyMatchTreatment treatment)
{    return withConfiguration(getConfiguration().withEmptyMatchTreatment(treatment));}
public MatchAll beam_f14605_0(Duration pollInterval, TerminationCondition<String, ?> terminationCondition)
{    return withConfiguration(getConfiguration().continuously(pollInterval, terminationCondition));}
public PCollection<ReadableFile> beam_f14613_0(PCollection<MatchResult.Metadata> input)
{    return input.apply(ParDo.of(new ToReadableFileFn(this)));}
public void beam_f14614_0(DisplayData.Builder builder)
{    builder.add(DisplayData.item("compression", getCompression().toString()));    builder.add(DisplayData.item("directoryTreatment", getDirectoryTreatment().toString()));}
 static boolean beam_f14615_0(MatchResult.Metadata metadata, DirectoryTreatment directoryTreatment)
{    if (metadata.resourceId().isDirectory()) {        switch(directoryTreatment) {            case SKIP:                return true;            case PROHIBIT:                throw new IllegalArgumentException("Trying to read " + metadata.resourceId() + " which is a directory");            default:                throw new UnsupportedOperationException("Unknown DirectoryTreatment: " + directoryTreatment);        }    }    return false;}
public Write<DestinationT, UserT> beam_f14623_0(Contextful<Fn<UserT, OutputT>> outputFn, Contextful<Fn<DestinationT, Sink<OutputT>>> sinkFn)
{    checkArgument(sinkFn != null, "sinkFn can not be null");    checkArgument(outputFn != null, "outputFn can not be null");    return toBuilder().setSinkFn((Contextful) sinkFn).setOutputFn(outputFn).build();}
public Write<DestinationT, UserT> beam_f14624_0(Contextful<Fn<UserT, OutputT>> outputFn, final Sink<OutputT> sink)
{    checkArgument(sink != null, "sink can not be null");    checkArgument(outputFn != null, "outputFn can not be null");    return via(outputFn, fn(SerializableFunctions.clonesOf(sink)));}
public Write<DestinationT, UserT> beam_f14625_0(Contextful<Fn<DestinationT, Sink<UserT>>> sinkFn)
{    checkArgument(sinkFn != null, "sinkFn can not be null");    return toBuilder().setSinkFn((Contextful) sinkFn).setOutputFn(fn(SerializableFunctions.<UserT>identity())).build();}
public Write<DestinationT, UserT> beam_f14633_0(FileNaming naming)
{    checkArgument(naming != null, "naming can not be null");    return toBuilder().setConstantFileNaming(naming).build();}
public Write<DestinationT, UserT> beam_f14634_0(SerializableFunction<DestinationT, FileNaming> namingFn)
{    checkArgument(namingFn != null, "namingFn can not be null");    return withNaming(fn(namingFn));}
public Write<DestinationT, UserT> beam_f14635_0(Contextful<Fn<DestinationT, FileNaming>> namingFn)
{    checkArgument(namingFn != null, "namingFn can not be null");    return toBuilder().setFileNamingFn(namingFn).build();}
public Write<DestinationT, UserT> beam_f14643_0(PTransform<PCollection<UserT>, PCollectionView<Integer>> sharding)
{    checkArgument(sharding != null, "sharding can not be null");    return toBuilder().setSharding(sharding).build();}
public Write<DestinationT, UserT> beam_f14644_0()
{    return toBuilder().setIgnoreWindowing(true).build();}
public Write<DestinationT, UserT> beam_f14645_0()
{    return toBuilder().setNoSpilling(true).build();}
public T beam_f14653_0(PCollectionView<T> view)
{    return getWriteOperation().getSink().getDynamicDestinations().sideInput(view);}
public void beam_f14654_0(OutputT value) throws Exception
{    sink.write(value);}
protected void beam_f14655_0() throws Exception
{    sink.flush();}
public ResourceId beam_f14663_0(int shardNumber, int numShards, OutputFileHints outputFileHints)
{    return FileSystems.matchNewResource(namingFn.getFilename(GlobalWindow.INSTANCE, PaneInfo.NO_FIRING, numShards, shardNumber, spec.getCompression()), false);}
public List<PCollectionView<?>> beam_f14664_0()
{    return Lists.newArrayList(spec.getAllSideInputs());}
public Coder<DestinationT> beam_f14665_0()
{    return spec.getDestinationCoder();}
public static List<MatchResult> beam_f14673_0(List<ResourceId> resourceIds) throws IOException
{    return match(FluentIterable.from(resourceIds).transform(ResourceId::toString).toList());}
public static WritableByteChannel beam_f14674_0(ResourceId resourceId, String mimeType) throws IOException
{    return create(resourceId, StandardCreateOptions.builder().setMimeType(mimeType).build());}
public static WritableByteChannel beam_f14675_0(ResourceId resourceId, CreateOptions createOptions) throws IOException
{    return getFileSystemInternal(resourceId.getScheme()).create(resourceId, createOptions);}
private static void beam_f14683_0(List<ResourceId> srcResourceIds, List<ResourceId> destResourceIds)
{    checkArgument(srcResourceIds.size() == destResourceIds.size(), "Number of source resource ids %s must equal number of destination resource ids %s", srcResourceIds.size(), destResourceIds.size());    if (srcResourceIds.isEmpty()) {                return;    }    Set<String> schemes = FluentIterable.from(srcResourceIds).append(destResourceIds).transform(ResourceId::getScheme).toSet();    checkArgument(schemes.size() == 1, String.format("Expect srcResourceIds and destResourceIds have the same scheme, but received %s.", Joiner.on(", ").join(schemes)));}
private static String beam_f14684_0(List<String> specs)
{    checkArgument(!specs.isEmpty(), "Expect specs are not empty.");    Set<String> schemes = FluentIterable.from(specs).transform(FileSystems::parseScheme).toSet();    return Iterables.getOnlyElement(schemes);}
private static String beam_f14685_0(String spec)
{                        Matcher matcher = FILE_SCHEME_PATTERN.matcher(spec);    if (!matcher.matches()) {        return DEFAULT_SCHEME;    } else {        return matcher.group("scheme").toLowerCase();    }}
public static MatchResult beam_f14693_0(final Status status, final IOException e)
{    return new AutoValue_MatchResult_Failure(status, e);}
public List<Metadata> beam_f14694_0() throws IOException
{    throw getException();}
public static MatchResult beam_f14695_0()
{    return new AutoValue_MatchResult_Failure(Status.UNKNOWN, new IOException("MatchResult status is UNKNOWN, and metadata is not available."));}
public void beam_f14703_0(Metadata value, OutputStream os) throws IOException
{    V1_CODER.encode(value, os);    LONG_CODER.encode(value.lastModifiedMillis(), os);}
public Metadata beam_f14704_0(InputStream is) throws IOException
{    Builder builder = V1_CODER.decodeBuilder(is);    long lastModifiedMillis = LONG_CODER.decode(is);    return builder.setLastModifiedMillis(lastModifiedMillis).build();}
public boolean beam_f14705_0()
{    return true;}
private static void beam_f14713_0(ResourceId baseDirectory)
{    try {        ResourceId badFile = baseDirectory.resolve("file/", RESOLVE_FILE);        fail(String.format("Resolving badFile %s should have failed", badFile));    } catch (IllegalArgumentException e) {        }    ResourceId file = baseDirectory.resolve("file", RESOLVE_FILE);    try {        file.resolve("file2", RESOLVE_FILE);        fail(String.format("Should not be able to resolve against file resource %s", file));    } catch (IllegalStateException e) {        }}
private static void beam_f14714_0(List<ResourceId> resourceIds)
{    for (ResourceId resourceId : resourceIds) {                assertThat("ResourceId equal to itself", resourceId, equalTo(resourceId));                ResourceId cloned;        if (resourceId.isDirectory()) {            cloned = FileSystems.matchNewResource(resourceId.toString(), true);        } else {            cloned = FileSystems.matchNewResource(resourceId.toString(), false);        }        assertThat("ResourceId equals clone of itself", cloned, equalTo(resourceId));                assertThat("ResourceId toString consistency", cloned.toString(), equalTo(resourceId.toString()));                assertThat("ResourceId isDirectory consistency", cloned.isDirectory(), equalTo(resourceId.isDirectory()));    }}
public GenerateSequence beam_f14715_0(External.ExternalConfiguration config)
{    Preconditions.checkNotNull(config.start, "Parameters 'from' must not be null.");    setFrom(config.start);    setTo(-1);    setElementsPerPeriod(0);    if (config.stop != null) {        setTo(config.stop);    }    if (config.period != null) {        setPeriod(Duration.millis(config.period));    }    if (config.maxReadTime != null) {        setMaxReadTime(Duration.millis(config.maxReadTime));    }    if (config.elementsPerPeriod != null) {        setElementsPerPeriod(config.elementsPerPeriod);    }    return build();}
public GenerateSequence beam_f14723_0(long to)
{    checkArgument(getTo() == -1 || getTo() >= getFrom(), "Degenerate range [%s, %s)", getFrom(), getTo());    return toBuilder().setTo(to).build();}
public GenerateSequence beam_f14724_0(SerializableFunction<Long, Instant> timestampFn)
{    return toBuilder().setTimestampFn(timestampFn).build();}
public GenerateSequence beam_f14725_0(long numElements, Duration periodLength)
{    checkArgument(numElements > 0, "Number of elements in withRate must be positive, but was: %s", numElements);    checkArgument(periodLength != null, "periodLength can not be null");    return toBuilder().setElementsPerPeriod(numElements).setPeriod(periodLength).build();}
protected void beam_f14733_1(List<LocalResourceId> srcResourceIds, List<LocalResourceId> destResourceIds) throws IOException
{    checkArgument(srcResourceIds.size() == destResourceIds.size(), "Number of source files %s must equal number of destination files %s", srcResourceIds.size(), destResourceIds.size());    int numFiles = srcResourceIds.size();    for (int i = 0; i < numFiles; i++) {        LocalResourceId src = srcResourceIds.get(i);        LocalResourceId dst = destResourceIds.get(i);                File parent = dst.getCurrentDirectory().getPath().toFile();        if (!parent.exists()) {            checkArgument(parent.mkdirs() || parent.exists(), "Unable to make output directory %s in order to move into file %s", parent, dst.getPath());        }                Files.move(src.getPath(), dst.getPath(), StandardCopyOption.REPLACE_EXISTING, StandardCopyOption.ATOMIC_MOVE);    }}
protected void beam_f14734_1(Collection<LocalResourceId> resourceIds) throws IOException
{    for (LocalResourceId resourceId : resourceIds) {        try {            Files.delete(resourceId.getPath());        } catch (NoSuchFileException e) {                    }    }}
protected LocalResourceId beam_f14735_0(String singleResourceSpec, boolean isDirectory)
{    Path path = Paths.get(singleResourceSpec);    return LocalResourceId.fromPath(path, isDirectory);}
public LocalResourceId beam_f14743_0(String other, ResolveOptions resolveOptions)
{    checkState(isDirectory, "Expected the path is a directory, but had [%s].", pathString);    checkArgument(resolveOptions.equals(StandardResolveOptions.RESOLVE_FILE) || resolveOptions.equals(StandardResolveOptions.RESOLVE_DIRECTORY), "ResolveOptions: [%s] is not supported.", resolveOptions);    checkArgument(!(resolveOptions.equals(StandardResolveOptions.RESOLVE_FILE) && other.endsWith("/")), "The resolved file: [%s] should not end with '/'.", other);    if (SystemUtils.IS_OS_WINDOWS) {        return resolveLocalPathWindowsOS(other, resolveOptions);    } else {        return resolveLocalPath(other, resolveOptions);    }}
public LocalResourceId beam_f14744_0()
{    if (isDirectory) {        return this;    } else {        Path path = getPath();        Path parent = path.getParent();        if (parent == null && path.getNameCount() == 1) {            parent = Paths.get(".");        }        checkState(parent != null, "Failed to get the current directory for path: [%s].", pathString);        return fromPath(parent, true);    }}
public String beam_f14745_0()
{    Path fileName = getPath().getFileName();    return fileName == null ? null : fileName.toString();}
public int beam_f14753_0()
{    return Objects.hash(pathString, isDirectory);}
public static ResourceId beam_f14754_0(File file, boolean isDirectory)
{    return LocalResourceId.fromPath(file.toPath(), isDirectory);}
public static ResourceId beam_f14755_0(Path path, boolean isDirectory)
{    return LocalResourceId.fromPath(path, isDirectory);}
public void beam_f14763_0()
{    checkArgument(this.startOffset >= 0, "Start offset has value %s, must be non-negative", this.startOffset);    checkArgument(this.endOffset >= 0, "End offset has value %s, must be non-negative", this.endOffset);    checkArgument(this.startOffset <= this.endOffset, "Start offset %s may not be larger than end offset %s", this.startOffset, this.endOffset);    checkArgument(this.minBundleSize >= 0, "minBundleSize has value %s, must be non-negative", this.minBundleSize);}
public String beam_f14764_0()
{    return "[" + startOffset + ", " + endOffset + ")";}
public long beam_f14765_0()
{    return 1L;}
public Double beam_f14773_0()
{    return rangeTracker.getFractionConsumed();}
public long beam_f14774_0()
{    return rangeTracker.getSplitPointsProcessed();}
public long beam_f14775_0()
{    if (isDone()) {        return 0;    } else if (!isStarted()) {                return BoundedReader.SPLIT_POINTS_UNKNOWN;    } else if (!allowsDynamicSplitting()) {                return 1;    } else if (getCurrentOffset() >= rangeTracker.getStopPosition() - 1) {                return 1;    } else {                return super.getSplitPointsRemaining();    }}
public boolean beam_f14783_0()
{    return value.isEmpty();}
public int beam_f14784_0(@Nonnull ByteKey other)
{    checkNotNull(other, "other");    ByteIterator thisIt = value.iterator();    ByteIterator otherIt = other.value.iterator();    while (thisIt.hasNext() && otherIt.hasNext()) {                int cmp = (thisIt.nextByte() & 0xff) - (otherIt.nextByte() & 0xff);        if (cmp != 0) {            return cmp;        }    }        return value.size() - other.value.size();}
public String beam_f14785_0()
{    char[] encoded = new char[2 * value.size() + 2];    encoded[0] = '[';    int cnt = 1;    ByteIterator iterator = value.iterator();    while (iterator.hasNext()) {        byte b = iterator.nextByte();        encoded[cnt] = HEX[(b & 0xF0) >>> 4];        ++cnt;        encoded[cnt] = HEX[b & 0xF];        ++cnt;    }    encoded[cnt] = ']';    return new String(encoded);}
public List<ByteKey> beam_f14793_0(int numSplits)
{    checkArgument(numSplits > 0, "numSplits %s must be a positive integer", numSplits);    try {        ImmutableList.Builder<ByteKey> ret = ImmutableList.builder();        ret.add(startKey);        for (int i = 1; i < numSplits; ++i) {            ret.add(interpolateKey(i / (double) numSplits));        }        ret.add(endKey);        return ret.build();    } catch (IllegalStateException e) {                return ImmutableList.of(startKey, endKey);    }}
public double beam_f14794_1(ByteKey key)
{    checkNotNull(key, "key");    checkArgument(!key.isEmpty(), "Cannot compute fraction for an empty key");    checkArgument(key.compareTo(startKey) >= 0, "Expected key %s >= range start key %s", key, startKey);    if (key.equals(endKey)) {        return 1.0;    }    checkArgument(containsKey(key), "Cannot compute fraction for %s outside this %s", key, this);    byte[] startBytes = startKey.getBytes();    byte[] endBytes = endKey.getBytes();    byte[] keyBytes = key.getBytes();        if (endKey.isEmpty()) {        startBytes = addHeadByte(startBytes, (byte) 0);        endBytes = addHeadByte(endBytes, (byte) 1);        keyBytes = addHeadByte(keyBytes, (byte) 0);    }        int paddedKeyLength = Math.max(Math.max(startBytes.length, endBytes.length), keyBytes.length);    BigInteger rangeStartInt = paddedPositiveInt(startBytes, paddedKeyLength);    BigInteger rangeEndInt = paddedPositiveInt(endBytes, paddedKeyLength);    BigInteger keyInt = paddedPositiveInt(keyBytes, paddedKeyLength);        BigInteger range = rangeEndInt.subtract(rangeStartInt);    if (range.equals(BigInteger.ZERO)) {                return 0.0;    }                BigInteger progressScaled = keyInt.subtract(rangeStartInt).shiftLeft(64);    return progressScaled.divide(range).doubleValue() / Math.pow(2, 64);}
public ByteKey beam_f14795_0(double fraction)
{    checkArgument(fraction >= 0.0 && fraction < 1.0, "Fraction %s must be in the range [0, 1)", fraction);    byte[] startBytes = startKey.getBytes();    byte[] endBytes = endKey.getBytes();        if (endKey.isEmpty()) {        startBytes = addHeadByte(startBytes, (byte) 0);        endBytes = addHeadByte(endBytes, (byte) 1);    }        int paddedKeyLength = Math.max(startBytes.length, endBytes.length);    BigInteger rangeStartInt = paddedPositiveInt(startBytes, paddedKeyLength);    BigInteger rangeEndInt = paddedPositiveInt(endBytes, paddedKeyLength);        BigInteger range = rangeEndInt.subtract(rangeStartInt);    checkState(!range.equals(BigInteger.ZERO), "Refusing to interpolate for near-empty %s where start and end keys differ only by trailing" + " zero bytes.", this);                    int bytesNeeded = ((53 - range.bitLength()) + 7) / 8;    if (bytesNeeded > 0) {        range = range.shiftLeft(bytesNeeded * 8);        rangeStartInt = rangeStartInt.shiftLeft(bytesNeeded * 8);        paddedKeyLength += bytesNeeded;    }    BigInteger interpolatedOffset = new BigDecimal(range).multiply(BigDecimal.valueOf(fraction)).toBigInteger();    int outputKeyLength = endKey.isEmpty() ? (paddedKeyLength - 1) : paddedKeyLength;    return ByteKey.copyFrom(fixupHeadZeros(rangeStartInt.add(interpolatedOffset).toByteArray(), outputKeyLength));}
 boolean beam_f14803_0(ByteKey key)
{    return endKey.isEmpty() || key.compareTo(endKey) < 0;}
private static BigInteger beam_f14804_0(byte[] bytes, int length)
{    int bytePaddingNeeded = length - bytes.length;    checkArgument(bytePaddingNeeded >= 0, "Required bytes.length {} < length {}", bytes.length, length);    BigInteger ret = new BigInteger(1, bytes);    return (bytePaddingNeeded == 0) ? ret : ret.shiftLeft(8 * bytePaddingNeeded);}
public org.apache.beam.sdk.transforms.splittabledofn.ByteKeyRangeTracker beam_f14805_0()
{    return org.apache.beam.sdk.transforms.splittabledofn.ByteKeyRangeTracker.of(this);}
public synchronized double beam_f14813_0()
{    if (position == null) {        return 0;    } else if (done) {        return 1.0;    } else if (position.compareTo(range.getEndKey()) >= 0) {        return 1.0;    }    return range.estimateFractionForKey(position);}
public synchronized long beam_f14814_0()
{    if (position == null) {        return 0;    } else if (isDone()) {        return splitPointsSeen;    } else {                checkState(splitPointsSeen > 0, "A started rangeTracker should have seen > 0 split points (is %s)", splitPointsSeen);        return splitPointsSeen - 1;    }}
public synchronized boolean beam_f14815_0()
{    done = true;    return false;}
public List<OffsetRange> beam_f14823_0(long desiredNumOffsetsPerSplit, long minNumOffsetPerSplit)
{    List<OffsetRange> res = new ArrayList<>();    long start = getFrom();    long maxEnd = getTo();    while (start < maxEnd) {        long end = start + desiredNumOffsetsPerSplit;        end = Math.min(end, maxEnd);                long remaining = maxEnd - end;        if ((remaining < desiredNumOffsetsPerSplit / 4) || (remaining < minNumOffsetPerSplit)) {            end = maxEnd;        }        res.add(new OffsetRange(start, end));        start = end;    }    return res;}
public static Coder beam_f14824_0()
{    return INSTANCE;}
public void beam_f14825_0(OffsetRange value, OutputStream outStream) throws CoderException, IOException
{    VarInt.encode(value.from, outStream);    VarInt.encode(value.to, outStream);}
public synchronized Long beam_f14833_0()
{    return startOffset;}
public synchronized Long beam_f14834_0()
{    return stopOffset;}
public boolean beam_f14835_0(boolean isAtSplitPoint, Long recordStart)
{    return tryReturnRecordAt(isAtSplitPoint, recordStart.longValue());}
public synchronized String beam_f14843_0()
{    String stopString = (stopOffset == OFFSET_INFINITY) ? "infinity" : String.valueOf(stopOffset);    if (lastRecordStart >= 0) {        return String.format("<at [starting at %d] of offset range [%d, %s)>", lastRecordStart, startOffset, stopString);    } else {        return String.format("<unstarted in offset range [%d, %s)>", startOffset, stopString);    }}
 OffsetRangeTracker beam_f14844_0()
{    synchronized (this) {        OffsetRangeTracker res = new OffsetRangeTracker();                synchronized (res) {            res.startOffset = this.startOffset;            res.stopOffset = this.stopOffset;            res.offsetOfLastSplitPoint = this.offsetOfLastSplitPoint;            res.lastRecordStart = this.lastRecordStart;            res.done = this.done;            res.splitPointsSeen = this.splitPointsSeen;        }        return res;    }}
public static Bounded<T> beam_f14845_0(BoundedSource<T> source)
{    return new Bounded<>(null, source);}
public BoundedReadFromUnboundedSource<T> beam_f14853_0(long maxNumRecords)
{    return new BoundedReadFromUnboundedSource<>(source, maxNumRecords, null);}
public BoundedReadFromUnboundedSource<T> beam_f14854_0(Duration maxReadTime)
{    return new BoundedReadFromUnboundedSource<>(source, Long.MAX_VALUE, maxReadTime);}
public final PCollection<T> beam_f14855_0(PBegin input)
{    source.validate();    return PCollection.createPrimitiveOutputInternal(input.getPipeline(), WindowingStrategy.globalDefault(), IsBounded.UNBOUNDED, source.getOutputCoder());}
public void beam_f14863_0(ProcessContext c)
{    Metadata metadata = c.element().getMetadata();    if (!metadata.isReadSeekEfficient()) {        c.output(KV.of(c.element(), new OffsetRange(0, metadata.sizeBytes())));        return;    }    for (OffsetRange range : new OffsetRange(0, metadata.sizeBytes()).split(desiredBundleSizeBytes, 0)) {        c.output(KV.of(c.element(), range));    }}
public void beam_f14864_0(ProcessContext c) throws IOException
{    ReadableFile file = c.element().getKey();    OffsetRange range = c.element().getValue();    FileBasedSource<T> source = CompressedSource.from(createSource.apply(file.getMetadata().resourceId().toString())).withCompression(file.getCompression());    try (BoundedSource.BoundedReader<T> reader = source.createForSubrangeOfFile(file.getMetadata(), range.getFrom(), range.getTo()).createReader(c.getPipelineOptions())) {        for (boolean more = reader.start(); more; more = reader.advance()) {            c.output(reader.getCurrent());        }    }}
private boolean beam_f14865_0(CodecFactory codecFactory)
{    final String codecStr = codecFactory.toString();    return noOptAvroCodecs.contains(codecStr) || deflatePattern.matcher(codecStr).matches() || xzPattern.matcher(codecStr).matches();}
public static ReadAll beam_f14875_0()
{    return new AutoValue_TextIO_ReadAll.Builder().setCompression(Compression.AUTO).setMatchConfiguration(MatchConfiguration.create(EmptyMatchTreatment.ALLOW_IF_WILDCARD)).build();}
public static ReadFiles beam_f14876_0()
{    return new AutoValue_TextIO_ReadFiles.Builder().setDesiredBundleSizeBytes(DEFAULT_BUNDLE_SIZE_BYTES).build();}
public static Write beam_f14877_0()
{    return new TextIO.Write();}
public Read beam_f14885_0()
{    return toBuilder().setHintMatchesManyFiles(true).build();}
public Read beam_f14886_0(EmptyMatchTreatment treatment)
{    return withMatchConfiguration(getMatchConfiguration().withEmptyMatchTreatment(treatment));}
public Read beam_f14887_0(byte[] delimiter)
{    checkArgument(delimiter != null, "delimiter can not be null");    checkArgument(!isSelfOverlapping(delimiter), "delimiter must not self-overlap");    return toBuilder().setDelimiter(delimiter).build();}
public ReadAll beam_f14895_0(EmptyMatchTreatment treatment)
{    return withMatchConfiguration(getMatchConfiguration().withEmptyMatchTreatment(treatment));}
public ReadAll beam_f14896_0(Duration pollInterval, TerminationCondition<String, ?> terminationCondition)
{    return withMatchConfiguration(getMatchConfiguration().continuously(pollInterval, terminationCondition));}
 ReadAll beam_f14897_0(byte[] delimiter)
{    return toBuilder().setDelimiter(delimiter).build();}
public TypedWrite<UserT, DestinationT> beam_f14905_0(String filenamePrefix)
{    return to(FileBasedSink.convertToFileResourceIfPossible(filenamePrefix));}
public TypedWrite<UserT, DestinationT> beam_f14906_0(ResourceId filenamePrefix)
{    return toResource(StaticValueProvider.of(filenamePrefix));}
public TypedWrite<UserT, DestinationT> beam_f14907_0(ValueProvider<String> outputPrefix)
{    return toResource(NestedValueProvider.of(outputPrefix, FileBasedSink::convertToFileResourceIfPossible));}
public TypedWrite<UserT, DestinationT> beam_f14915_0(String shardTemplate)
{    return toBuilder().setShardTemplate(shardTemplate).build();}
public TypedWrite<UserT, DestinationT> beam_f14916_0(String filenameSuffix)
{    return toBuilder().setFilenameSuffix(filenameSuffix).build();}
public TypedWrite<UserT, DestinationT> beam_f14917_0(int numShards)
{    checkArgument(numShards >= 0);    return toBuilder().setNumShards(numShards).build();}
public TypedWrite<UserT, DestinationT> beam_f14925_0()
{    return toBuilder().setNoSpilling(true).build();}
private DynamicDestinations<UserT, DestinationT, String> beam_f14926_0()
{    DynamicDestinations<UserT, DestinationT, String> dynamicDestinations = getDynamicDestinations();    if (dynamicDestinations == null) {        if (getDestinationFunction() != null) {                        dynamicDestinations = (DynamicDestinations) DynamicFileDestinations.toDefaultPolicies(getDestinationFunction(), getEmptyDestination(), getFormatFunction());        } else {                        FilenamePolicy usedFilenamePolicy = getFilenamePolicy();            if (usedFilenamePolicy == null) {                usedFilenamePolicy = DefaultFilenamePolicy.fromStandardParameters(getFilenamePrefix(), getShardTemplate(), getFilenameSuffix(), getWindowedWrites());            }            dynamicDestinations = (DynamicDestinations) DynamicFileDestinations.constant(usedFilenamePolicy, getFormatFunction());        }    }    return dynamicDestinations;}
public WriteFilesResult<DestinationT> beam_f14927_0(PCollection<UserT> input)
{    checkState(getFilenamePrefix() != null || getTempDirectory() != null, "Need to set either the filename prefix or the tempDirectory of a TextIO.Write " + "transform.");    List<?> allToArgs = Lists.newArrayList(getFilenamePolicy(), getDynamicDestinations(), getFilenamePrefix(), getDestinationFunction());    checkArgument(1 == Iterables.size(allToArgs.stream().filter(Predicates.notNull()::apply).collect(Collectors.toList())), "Exactly one of filename policy, dynamic destinations, filename prefix, or destination " + "function must be set");    if (getDynamicDestinations() != null) {        checkArgument(getFormatFunction() == null, "A format function should not be specified " + "with DynamicDestinations. Use DynamicDestinations.formatRecord instead");    }    if (getFilenamePolicy() != null || getDynamicDestinations() != null) {        checkState(getShardTemplate() == null && getFilenameSuffix() == null, "shardTemplate and filenameSuffix should only be used with the default " + "filename policy");    }    ValueProvider<ResourceId> tempDirectory = getTempDirectory();    if (tempDirectory == null) {        tempDirectory = getFilenamePrefix();    }    WriteFiles<UserT, DestinationT, String> write = WriteFiles.to(new TextSink<>(tempDirectory, resolveDynamicDestinations(), getDelimiter(), getHeader(), getFooter(), getWritableByteChannelFactory()));    if (getNumShards() > 0) {        write = write.withNumShards(getNumShards());    }    if (getWindowedWrites()) {        write = write.withWindowedWrites();    }    if (getNoSpilling()) {        write = write.withNoSpilling();    }    return input.apply("WriteFiles", write);}
public Write beam_f14935_0(SerializableFunction<String, Params> destinationFunction, Params emptyDestination)
{    return new Write(inner.to(destinationFunction, emptyDestination).withFormatFunction(SerializableFunctions.identity()));}
public Write beam_f14936_0(ValueProvider<ResourceId> tempDirectory)
{    return new Write(inner.withTempDirectory(tempDirectory));}
public Write beam_f14937_0(ResourceId tempDirectory)
{    return new Write(inner.withTempDirectory(tempDirectory));}
public Write beam_f14945_0(WritableByteChannelFactory writableByteChannelFactory)
{    return new Write(inner.withWritableByteChannelFactory(writableByteChannelFactory));}
public Write beam_f14946_0(Compression compression)
{    return new Write(inner.withCompression(compression));}
public Write beam_f14947_0()
{    return new Write(inner.withWindowedWrites());}
public Sink beam_f14955_0(String footer)
{    checkArgument(footer != null, "footer can not be null");    return toBuilder().setFooter(footer).build();}
public void beam_f14956_0(WritableByteChannel channel) throws IOException
{    writer = new PrintWriter(new BufferedWriter(new OutputStreamWriter(Channels.newOutputStream(channel), UTF_8)));    if (getHeader() != null) {        writer.println(getHeader());    }}
public void beam_f14957_0(String element) throws IOException
{    writer.println(element);}
public Writer<DestinationT, String> beam_f14965_0() throws Exception
{    return new TextWriter<>(this, delimiter, header, footer);}
private void beam_f14966_0(@Nullable String value) throws IOException
{    if (value != null) {        writeLine(value);    }}
private void beam_f14967_0(String value) throws IOException
{    out.write(value);    out.write(delimiter);}
public Coder<String> beam_f14975_0()
{    return StringUtf8Coder.of();}
protected long beam_f14976_0() throws NoSuchElementException
{    if (!elementIsPresent) {        throw new NoSuchElementException();    }    return startOfRecord;}
public long beam_f14977_0()
{    if (isStarted() && startOfNextRecord >= getCurrentSource().getEndOffset()) {        return isDone() ? 0 : 1;    }    return super.getSplitPointsRemaining();}
public static ReadFiles beam_f14985_0()
{    return new AutoValue_TFRecordIO_ReadFiles.Builder().build();}
public static Write beam_f14986_0()
{    return new AutoValue_TFRecordIO_Write.Builder().setShardTemplate(null).setFilenameSuffix(null).setNumShards(0).setCompression(Compression.UNCOMPRESSED).setNoSpilling(false).build();}
public static Sink beam_f14987_0()
{    return new Sink();}
public void beam_f14995_0(DisplayData.Builder builder)
{    super.populateDisplayData(builder);    builder.add(DisplayData.item("compressionType", getCompression().toString()).withLabel("Compression Type")).addIfNotDefault(DisplayData.item("validation", getValidate()).withLabel("Validation Enabled"), true).addIfNotNull(DisplayData.item("filePattern", getFilepattern()).withLabel("File Pattern"));}
public PCollection<byte[]> beam_f14996_0(PCollection<FileIO.ReadableFile> input)
{    return input.apply("Read all via FileBasedSource", new ReadAllViaFileBasedSource<>(Long.MAX_VALUE, new CreateSourceFn(), DEFAULT_BYTE_ARRAY_CODER));}
public FileBasedSource<byte[]> beam_f14997_0(String input)
{    return new TFRecordSource(StaticValueProvider.of(input));}
public Write beam_f15005_0(CompressionType compressionType)
{    return withCompression(compressionType.canonical);}
public Write beam_f15006_0(Compression compression)
{    return toBuilder().setCompression(compression).build();}
public Write beam_f15007_0()
{    return toBuilder().setNoSpilling(true).build();}
public Coder<byte[]> beam_f15016_0()
{    return DEFAULT_BYTE_ARRAY_CODER;}
protected boolean beam_f15017_0()
{        return false;}
public boolean beam_f15018_0()
{    /* TFRecords cannot be dynamically split. */    return false;}
public void beam_f15026_0(byte[] value) throws Exception
{    codec.write(outChannel, value);}
private int beam_f15027_0(int crc)
{    return ((crc >>> 15) | (crc << 17)) + 0xa282ead8;}
private int beam_f15028_0(long x)
{    return mask(crc32c.hashLong(x).asInt());}
public long beam_f15037_0()
{    return BACKLOG_UNKNOWN;}
public static WriteFiles<UserT, DestinationT, OutputT> beam_f15038_0(FileBasedSink<UserT, DestinationT, OutputT> sink)
{    checkArgument(sink != null, "sink can not be null");    return new AutoValue_WriteFiles.Builder<UserT, DestinationT, OutputT>().setSink(sink).setComputeNumShards(null).setNumShardsProvider(null).setWindowedWrites(false).setMaxNumWritersPerBundle(DEFAULT_MAX_NUM_WRITERS_PER_BUNDLE).setSideInputs(sink.getDynamicDestinations().getSideInputs()).build();}
public Map<TupleTag<?>, PValue> beam_f15039_0()
{    return PCollectionViews.toAdditionalInputs(getSideInputs());}
public WriteFiles<UserT, DestinationT, OutputT> beam_f15047_0()
{    return toBuilder().setWindowedWrites(true).build();}
public WriteFiles<UserT, DestinationT, OutputT> beam_f15048_0()
{    return toBuilder().setMaxNumWritersPerBundle(-1).build();}
public void beam_f15049_0(PipelineOptions options)
{    getSink().validate(options);}
public void beam_f15057_1(ProcessContext c, BoundedWindow window) throws Exception
{    getDynamicDestinations().setSideInputAccessorFromProcessContext(c);    PaneInfo paneInfo = c.pane();                        DestinationT destination = getDynamicDestinations().getDestination(c.element());    WriterKey<DestinationT> key = new WriterKey<>(window, c.pane(), destination);    Writer<DestinationT, OutputT> writer = writers.get(key);    if (writer == null) {        if (getMaxNumWritersPerBundle() < 0 || writers.size() <= getMaxNumWritersPerBundle()) {            String uuid = UUID.randomUUID().toString();                        writer = writeOperation.createWriter();            writer.setDestination(destination);            writer.open(uuid);            writers.put(key, writer);                    } else {            if (spilledShardNum == UNKNOWN_SHARDNUM) {                                spilledShardNum = ThreadLocalRandom.current().nextInt(SPILLED_RECORD_SHARDING_FACTOR);            } else {                spilledShardNum = (spilledShardNum + 1) % SPILLED_RECORD_SHARDING_FACTOR;            }            c.output(unwrittenRecordsTag, KV.of(ShardedKey.of(hashDestination(destination, destinationCoder), spilledShardNum), c.element()));            return;        }    }    writeOrClose(writer, getDynamicDestinations().formatRecord(c.element()));}
public void beam_f15058_0(FinishBundleContext c) throws Exception
{    for (Map.Entry<WriterKey<DestinationT>, Writer<DestinationT, OutputT>> entry : writers.entrySet()) {        WriterKey<DestinationT> key = entry.getKey();        Writer<DestinationT, OutputT> writer = entry.getValue();        try {            writer.close();        } catch (Exception e) {                        writer.cleanup();            throw e;        }        BoundedWindow window = key.window;        c.output(new FileResult<>(writer.getOutputFile(), UNKNOWN_SHARDNUM, window, key.paneInfo, key.destination), window.maxTimestamp(), window);    }}
private static void beam_f15059_0(Writer<DestinationT, OutputT> writer, OutputT t) throws Exception
{    try {        writer.write(t);    } catch (Exception e) {        try {            writer.close();                        writer.cleanup();        } catch (Exception closeException) {            if (closeException instanceof InterruptedException) {                                Thread.currentThread().interrupt();            }                        e.addSuppressed(closeException);        }        throw e;    }}
public WriteFilesResult<DestinationT> beam_f15067_0(PCollection<List<FileResult<DestinationT>>> input)
{    List<PCollectionView<?>> finalizeSideInputs = Lists.newArrayList(getSideInputs());    if (numShardsView != null) {        finalizeSideInputs.add(numShardsView);    }    PCollection<KV<DestinationT, String>> outputFilenames = input.apply("Finalize", ParDo.of(new FinalizeFn()).withSideInputs(finalizeSideInputs)).setCoder(KvCoder.of(destinationCoder, StringUtf8Coder.of())).apply(Reshuffle.viaRandomKey());    TupleTag<KV<DestinationT, String>> perDestinationOutputFilenamesTag = new TupleTag<>("perDestinationOutputFilenames");    return WriteFilesResult.in(input.getPipeline(), perDestinationOutputFilenamesTag, outputFilenames);}
public void beam_f15068_1(ProcessContext c) throws Exception
{    getDynamicDestinations().setSideInputAccessorFromProcessContext(c);    @Nullable    Integer fixedNumShards;    if (numShardsView != null) {        fixedNumShards = c.sideInput(numShardsView);    } else if (getNumShardsProvider() != null) {        fixedNumShards = getNumShardsProvider().get();    } else {        fixedNumShards = null;    }    List<FileResult<DestinationT>> fileResults = Lists.newArrayList(c.element());        DestinationT defaultDest = getDynamicDestinations().getDefaultDestination();    List<KV<FileResult<DestinationT>, ResourceId>> resultsToFinalFilenames = fileResults.isEmpty() ? writeOperation.finalizeDestination(defaultDest, GlobalWindow.INSTANCE, fixedNumShards, fileResults) : finalizeAllDestinations(fileResults, fixedNumShards);    writeOperation.moveToOutputFiles(resultsToFinalFilenames);    for (KV<FileResult<DestinationT>, ResourceId> entry : resultsToFinalFilenames) {        FileResult<DestinationT> res = entry.getKey();        c.output(KV.of(res.getDestination(), entry.getValue().toString()));    }}
private List<KV<FileResult<DestinationT>, ResourceId>> beam_f15069_0(List<FileResult<DestinationT>> fileResults, @Nullable Integer fixedNumShards) throws Exception
{    Multimap<KV<DestinationT, BoundedWindow>, FileResult<DestinationT>> res = ArrayListMultimap.create();    for (FileResult<DestinationT> result : fileResults) {        res.put(KV.of(result.getDestination(), result.getWindow()), result);    }    List<KV<FileResult<DestinationT>, ResourceId>> resultsToFinalFilenames = Lists.newArrayList();    for (Map.Entry<KV<DestinationT, BoundedWindow>, Collection<FileResult<DestinationT>>> destEntry : res.asMap().entrySet()) {        KV<DestinationT, BoundedWindow> destWindow = destEntry.getKey();        resultsToFinalFilenames.addAll(writeOperation.finalizeDestination(destWindow.getKey(), destWindow.getValue(), fixedNumShards, destEntry.getValue()));    }    return resultsToFinalFilenames;}
public void beam_f15078_0()
{    inc(1);}
public void beam_f15079_0(long n)
{    MetricsContainer container = MetricsEnvironment.getCurrentContainer();    if (container != null) {        container.getCounter(name).inc(n);    }}
public void beam_f15080_0()
{    inc(-1);}
public Instant beam_f15088_0()
{    return EPOCH;}
public static boolean beam_f15089_0(MetricsFilter filter, MetricKey key)
{    return filter == null || (matchesName(key.metricName(), filter.names()) && matchesScope(key.stepName(), filter.steps()));}
public static boolean beam_f15090_0(String haystack, String needle)
{    int location = haystack.indexOf(needle);    int end = location + needle.length();    if (location == -1) {                return false;    } else if (location != 0 && haystack.charAt(location - 1) != '/') {                return false;    } else if (end != haystack.length() && haystack.charAt(end) != '/') {                return false;    } else {        return true;    }}
public static MetricName beam_f15098_0(String namespace, String name)
{    checkArgument(!Strings.isNullOrEmpty(namespace), "Metric namespace must be non-empty");    checkArgument(!Strings.isNullOrEmpty(name), "Metric name must be non-empty");    return new AutoValue_MetricName(namespace, name);}
public static MetricName beam_f15099_0(Class<?> namespace, String name)
{    checkArgument(namespace != null, "Metric namespace must be non-null");    checkArgument(!Strings.isNullOrEmpty(name), "Metric name must be non-empty");    return new AutoValue_MetricName(namespace.getName(), name);}
public static MetricNameFilter beam_f15100_0(String namespace)
{    return new AutoValue_MetricNameFilter(namespace, null);}
public T beam_f15108_0()
{    T committed = getCommittedOrNull();    if (committed == null) {        throw new UnsupportedOperationException("This runner does not currently support committed" + " metrics results. Please use 'attempted' instead.");    }    return committed;}
public boolean beam_f15109_0()
{    return getCommittedOrNull() != null;}
public MetricResult<V> beam_f15110_0(Function<T, V> fn)
{    T committed = getCommittedOrNull();    return create(getKey(), committed == null ? null : fn.apply(committed), fn.apply(getAttempted()));}
public static Counter beam_f15118_0(String namespace, String name)
{    return new DelegatingCounter(MetricName.named(namespace, name));}
public static Counter beam_f15119_0(Class<?> namespace, String name)
{    return new DelegatingCounter(MetricName.named(namespace, name));}
public static Distribution beam_f15120_0(String namespace, String name)
{    return new DelegatingDistribution(MetricName.named(namespace, name));}
public MetricName beam_f15128_0()
{    return name;}
public static MetricsContainer beam_f15129_0(@Nullable MetricsContainer container)
{    MetricsContainer previous = CONTAINER_FOR_THREAD.get();    if (container == null) {        CONTAINER_FOR_THREAD.remove();    } else {        CONTAINER_FOR_THREAD.set(container);    }    return previous;}
public static void beam_f15130_0(boolean supported)
{    METRICS_SUPPORTED.set(supported);}
public Builder beam_f15138_0(MetricNameFilter nameFilter)
{    immutableNamesBuilder().add(nameFilter);    return this;}
public Builder beam_f15139_0(String step)
{    immutableStepsBuilder().add(step);    return this;}
public Class<? extends MetricsSink> beam_f15140_0(PipelineOptions options)
{    try {        @SuppressWarnings({ "unchecked", "rawtypes" })        Class<? extends MetricsSink> noOpMetricsSinkClass = (Class<? extends MetricsSink>) Class.forName("org.apache.beam.runners.core.metrics.NoOpMetricsSink", true, ReflectHelpers.findClassLoader());        return noOpMetricsSinkClass;    } catch (ClassNotFoundException e) {        throw new IllegalArgumentException(String.format("NoOpMetricsSink was not found on classpath"));    }}
public static Gauge beam_f15148_0(String splitId)
{    return Metrics.gauge(SOURCE_SPLITS_NAMESPACE, renderName(splitId, BACKLOG_BYTES));}
public static Gauge beam_f15149_0()
{    return BACKLOG_ELEMENTS_GAUGE;}
public static Gauge beam_f15150_0(String splitId)
{    return Metrics.gauge(SOURCE_SPLITS_NAMESPACE, renderName(splitId, BACKLOG_ELEMENTS));}
public Long beam_f15158_0(PipelineOptions options)
{    return NEXT_ID.getAndIncrement();}
public String beam_f15159_0(PipelineOptions options)
{    ReleaseInfo info = ReleaseInfo.getReleaseInfo();    return String.format("%s/%s", info.getName(), info.getVersion()).replace(" ", "_");}
public static PipelineOptions beam_f15160_0()
{    return new Builder().as(PipelineOptions.class);}
public T beam_f15168_1(Class<T> klass)
{    Map<String, Object> initialOptions = Maps.newHashMap();        if (args != null) {        ListMultimap<String, String> options = parseCommandLine(args, strictParsing);                printHelpUsageAndExitIfNeeded(options, System.out, true);        initialOptions = parseObjects(klass, options, strictParsing);    }        ProxyInvocationHandler handler = new ProxyInvocationHandler(initialOptions);    T t = handler.as(klass);        ApplicationNameOptions appNameOptions = t.as(ApplicationNameOptions.class);    if (appNameOptions.getAppName() == null) {        appNameOptions.setAppName(defaultAppName);    }            t.getOptionsId();    if (validation) {        if (isCli) {            PipelineOptionsValidator.validateCli(klass, t);        } else {            PipelineOptionsValidator.validate(klass, t);        }    }    return t;}
 static boolean beam_f15169_0(ListMultimap<String, String> options, PrintStream printStream, boolean exit)
{    if (options.containsKey("help")) {        final String helpOption = Iterables.getOnlyElement(options.get("help"));                if (Boolean.TRUE.toString().equals(helpOption)) {            printHelp(printStream);            if (exit) {                System.exit(0);            } else {                return true;            }        }                try {            Class<?> klass = Class.forName(helpOption, true, ReflectHelpers.findClassLoader());            if (!PipelineOptions.class.isAssignableFrom(klass)) {                throw new ClassNotFoundException("PipelineOptions of type " + klass + " not found.");            }            printHelp(printStream, (Class<? extends PipelineOptions>) klass);        } catch (ClassNotFoundException e) {                        Iterable<Class<? extends PipelineOptions>> matches = getRegisteredOptions().stream().filter(input -> {                if (helpOption.contains(".")) {                    return input.getName().endsWith(helpOption);                } else {                    return input.getSimpleName().equals(helpOption);                }            }).collect(Collectors.toList());            try {                printHelp(printStream, Iterables.getOnlyElement(matches));            } catch (NoSuchElementException exception) {                printStream.format("Unable to find option %s.%n", helpOption);                printHelp(printStream);            } catch (IllegalArgumentException exception) {                printStream.format("Multiple matches found for %s: %s.%n", helpOption, StreamSupport.stream(matches.spliterator(), false).map(ReflectHelpers.CLASS_NAME::apply).collect(Collectors.toList()));                printHelp(printStream);            }        }        if (exit) {            System.exit(0);        } else {            return true;        }    }    return false;}
private static String beam_f15170_0()
{    Iterator<StackTraceElement> elements = Iterators.forArray(Thread.currentThread().getStackTrace());        while (elements.hasNext()) {        StackTraceElement next = elements.next();        if (PIPELINE_OPTIONS_FACTORY_CLASSES.contains(next.getClassName())) {            break;        }    }        while (elements.hasNext()) {        StackTraceElement next = elements.next();        if (!PIPELINE_OPTIONS_FACTORY_CLASSES.contains(next.getClassName())) {            try {                return Class.forName(next.getClassName(), true, ReflectHelpers.findClassLoader()).getSimpleName();            } catch (ClassNotFoundException e) {                break;            }        }    }    return "unknown";}
public static List<PipelineOptionDescriptor> beam_f15178_0(Set<Class<? extends PipelineOptions>> ifaces)
{    checkNotNull(ifaces);    List<PipelineOptionDescriptor> result = new ArrayList<>();    Set<Method> seenMethods = Sets.newHashSet();    for (Class<? extends PipelineOptions> iface : ifaces) {        CACHE.get().validateWellFormed(iface);        Set<PipelineOptionSpec> properties = PipelineOptionsReflector.getOptionSpecs(iface, false);        RowSortedTable<Class<?>, String, Method> ifacePropGetterTable = TreeBasedTable.create(ClassNameComparator.INSTANCE, Ordering.natural());        for (PipelineOptionSpec prop : properties) {            ifacePropGetterTable.put(prop.getDefiningInterface(), prop.getName(), prop.getGetterMethod());        }        for (Map.Entry<Class<?>, Map<String, Method>> ifaceToPropertyMap : ifacePropGetterTable.rowMap().entrySet()) {            Class<?> currentIface = ifaceToPropertyMap.getKey();            Map<String, Method> propertyNamesToGetters = ifaceToPropertyMap.getValue();            List<String> lists = Lists.newArrayList(propertyNamesToGetters.keySet());            lists.sort(String.CASE_INSENSITIVE_ORDER);            for (String propertyName : lists) {                Method method = propertyNamesToGetters.get(propertyName);                if (!seenMethods.add(method)) {                    continue;                }                Class<?> returnType = method.getReturnType();                PipelineOptionType.Enum optionType = PipelineOptionType.Enum.STRING;                if (JSON_INTEGER_TYPES.contains(returnType)) {                    optionType = PipelineOptionType.Enum.INTEGER;                } else if (JSON_NUMBER_TYPES.contains(returnType)) {                    optionType = PipelineOptionType.Enum.NUMBER;                } else if (returnType == boolean.class || returnType == Boolean.class) {                    optionType = PipelineOptionType.Enum.BOOLEAN;                } else if (List.class.isAssignableFrom(returnType)) {                    optionType = PipelineOptionType.Enum.ARRAY;                }                String optionName = CaseFormat.LOWER_CAMEL.to(CaseFormat.LOWER_UNDERSCORE, propertyName);                Description description = method.getAnnotation(Description.class);                PipelineOptionDescriptor.Builder builder = PipelineOptionDescriptor.newBuilder().setName(optionName).setType(optionType).setGroup(currentIface.getName());                Optional<String> defaultValue = getDefaultValueFromAnnotation(method);                if (defaultValue.isPresent()) {                    builder.setDefaultValue(defaultValue.get());                }                if (description != null) {                    builder.setDescription(description.value());                }                result.add(builder.build());            }        }    }    return result;}
private static void beam_f15179_0(PrintStream out, Required annotation, SortedSetMultimap<String, String> requiredGroupNameToProperties)
{    if (annotation == null || annotation.groups() == null) {        return;    }    for (String group : annotation.groups()) {        SortedSet<String> groupMembers = requiredGroupNameToProperties.get(group);        String requirement;        if (groupMembers.size() == 1) {            requirement = Iterables.getOnlyElement(groupMembers) + " is required.";        } else {            requirement = "At least one of " + groupMembers + " is required";        }        terminalPrettyPrint(out, requirement.split("\\s+"));    }}
private static void beam_f15180_0(PrintStream out, Description description)
{    if (description == null || description.value() == null) {        return;    }    String[] words = description.value().split("\\s+");    terminalPrettyPrint(out, words);}
private static void beam_f15188_0(Class<? extends PipelineOptions> iface)
{    Iterable<Method> interfaceMethods = FluentIterable.from(ReflectHelpers.getClosureOfMethodsOnInterface(iface)).filter(NOT_SYNTHETIC_PREDICATE).toSortedSet(MethodComparator.INSTANCE);    SortedSetMultimap<Method, Method> methodNameToMethodMap = TreeMultimap.create(MethodNameComparator.INSTANCE, MethodComparator.INSTANCE);    for (Method method : interfaceMethods) {        methodNameToMethodMap.put(method, method);    }    List<MultipleDefinitions> multipleDefinitions = Lists.newArrayList();    for (Map.Entry<Method, Collection<Method>> entry : methodNameToMethodMap.asMap().entrySet()) {        Set<Class<?>> returnTypes = FluentIterable.from(entry.getValue()).transform(ReturnTypeFetchingFunction.INSTANCE).toSet();        SortedSet<Method> collidingMethods = FluentIterable.from(entry.getValue()).toSortedSet(MethodComparator.INSTANCE);        if (returnTypes.size() > 1) {            MultipleDefinitions defs = new MultipleDefinitions();            defs.method = entry.getKey();            defs.collidingMethods = collidingMethods;            multipleDefinitions.add(defs);        }    }    throwForMultipleDefinitions(iface, multipleDefinitions);}
private static void beam_f15189_0(SortedSet<Method> allInterfaceMethods, List<PropertyDescriptor> descriptors)
{    SortedSetMultimap<Method, Method> methodNameToAllMethodMap = TreeMultimap.create(MethodNameComparator.INSTANCE, MethodComparator.INSTANCE);    for (Method method : allInterfaceMethods) {        methodNameToAllMethodMap.put(method, method);    }        validateGettersHaveConsistentAnnotation(methodNameToAllMethodMap, descriptors, AnnotationPredicates.JSON_IGNORE);        validateGettersHaveConsistentAnnotation(methodNameToAllMethodMap, descriptors, AnnotationPredicates.DEFAULT_VALUE);        validateSettersDoNotHaveAnnotation(methodNameToAllMethodMap, descriptors, AnnotationPredicates.JSON_IGNORE);        validateSettersDoNotHaveAnnotation(methodNameToAllMethodMap, descriptors, AnnotationPredicates.DEFAULT_VALUE);}
private static void beam_f15190_0(SortedSetMultimap<Method, Method> methodNameToAllMethodMap, List<PropertyDescriptor> descriptors, final AnnotationPredicates annotationPredicates)
{    List<InconsistentlyAnnotatedGetters> inconsistentlyAnnotatedGetters = new ArrayList<>();    for (final PropertyDescriptor descriptor : descriptors) {        if (descriptor.getReadMethod() == null || IGNORED_METHODS.contains(descriptor.getReadMethod())) {            continue;        }        SortedSet<Method> getters = methodNameToAllMethodMap.get(descriptor.getReadMethod());        SortedSet<Method> gettersWithTheAnnotation = Sets.filter(getters, annotationPredicates.forMethod);        Set<Annotation> distinctAnnotations = Sets.newLinkedHashSet(FluentIterable.from(gettersWithTheAnnotation).transformAndConcat(new Function<Method, Iterable<? extends Annotation>>() {            @SuppressFBWarnings(value = "NP_METHOD_PARAMETER_TIGHTENS_ANNOTATION", justification = "https://github.com/google/guava/issues/920")            @Nonnull            @Override            public Iterable<? extends Annotation> apply(@Nonnull Method method) {                return FluentIterable.from(method.getAnnotations());            }        }).filter(annotationPredicates.forAnnotation));        if (distinctAnnotations.size() > 1) {            throw new IllegalArgumentException(String.format("Property [%s] is marked with contradictory annotations. Found [%s].", descriptor.getName(), FluentIterable.from(gettersWithTheAnnotation).transformAndConcat(new Function<Method, Iterable<String>>() {                @SuppressFBWarnings(value = "NP_METHOD_PARAMETER_TIGHTENS_ANNOTATION", justification = "https://github.com/google/guava/issues/920")                @Nonnull                @Override                public Iterable<String> apply(@Nonnull final Method method) {                    return FluentIterable.from(method.getAnnotations()).filter(annotationPredicates.forAnnotation).transform(new Function<Annotation, String>() {                        @SuppressFBWarnings(value = "NP_METHOD_PARAMETER_TIGHTENS_ANNOTATION", justification = "https://github.com/google/guava/issues/920")                        @Nonnull                        @Override                        public String apply(@Nonnull Annotation annotation) {                            return String.format("[%s on %s]", ReflectHelpers.ANNOTATION_FORMATTER.apply(annotation), ReflectHelpers.CLASS_AND_METHOD_FORMATTER.apply(method));                        }                    });                }            }).join(Joiner.on(", "))));        }        Iterable<String> getterClassNames = FluentIterable.from(getters).transform(MethodToDeclaringClassFunction.INSTANCE).transform(ReflectHelpers.CLASS_NAME);        Iterable<String> gettersWithTheAnnotationClassNames = FluentIterable.from(gettersWithTheAnnotation).transform(MethodToDeclaringClassFunction.INSTANCE).transform(ReflectHelpers.CLASS_NAME);        if (!(gettersWithTheAnnotation.isEmpty() || getters.size() == gettersWithTheAnnotation.size())) {            InconsistentlyAnnotatedGetters err = new InconsistentlyAnnotatedGetters();            err.descriptor = descriptor;            err.getterClassNames = getterClassNames;            err.gettersWithTheAnnotationClassNames = gettersWithTheAnnotationClassNames;            inconsistentlyAnnotatedGetters.add(err);        }    }    throwForGettersWithInconsistentAnnotation(inconsistentlyAnnotatedGetters, annotationPredicates.annotationClass);}
private static void beam_f15198_0(Class<?> klass, Set<Class<?>> nonPipelineOptionsClasses)
{    StringBuilder errorBuilder = new StringBuilder(String.format("All inherited interfaces of [%s] should inherit from the PipelineOptions interface. " + "The following inherited interfaces do not:", klass.getName()));    for (Class<?> invalidKlass : nonPipelineOptionsClasses) {        errorBuilder.append(String.format("%n - %s", invalidKlass.getName()));    }    throw new IllegalArgumentException(errorBuilder.toString());}
private static void beam_f15199_0(Class<?> klass)
{    Set<Class<?>> nonPipelineOptionsClasses = new LinkedHashSet<>();    checkInheritedFrom(klass, PipelineOptions.class, nonPipelineOptionsClasses);    if (!nonPipelineOptionsClasses.isEmpty()) {        throwNonPipelineOptions(klass, nonPipelineOptionsClasses);    }}
private static void beam_f15200_0(Class<? extends PipelineOptions> iface, List<MultipleDefinitions> definitions)
{    if (definitions.size() == 1) {        MultipleDefinitions errDef = definitions.get(0);        throw new IllegalArgumentException(String.format("Method [%s] has multiple definitions %s with different return types for [%s].", errDef.method.getName(), errDef.collidingMethods, iface.getName()));    } else if (definitions.size() > 1) {        StringBuilder errorBuilder = new StringBuilder(String.format("Interface [%s] has Methods with multiple definitions with different return types:", iface.getName()));        for (MultipleDefinitions errDef : definitions) {            errorBuilder.append(String.format("%n  - Method [%s] has multiple definitions %s", errDef.method.getName(), errDef.collidingMethods));        }        throw new IllegalArgumentException(errorBuilder.toString());    }}
public Class<?> beam_f15208_0(@Nonnull Method input)
{    return input.getDeclaringClass();}
private static ListMultimap<String, String> beam_f15209_1(String[] args, boolean strictParsing)
{    ImmutableListMultimap.Builder<String, String> builder = ImmutableListMultimap.builder();    for (String arg : args) {        if (Strings.isNullOrEmpty(arg)) {            continue;        }        try {            checkArgument(arg.startsWith("--"), "Argument '%s' does not begin with '--'", arg);            int index = arg.indexOf('=');                        checkArgument(index != 2, "Argument '%s' starts with '--=', empty argument name not allowed", arg);            if (index > 0) {                builder.put(arg.substring(2, index), arg.substring(index + 1, arg.length()));            } else {                builder.put(arg.substring(2), "true");            }        } catch (IllegalArgumentException e) {            if (strictParsing) {                throw e;            } else {                            }        }    }    return builder.build();}
private static Map<String, Object> beam_f15210_1(Class<T> klass, ListMultimap<String, String> options, boolean strictParsing)
{    Map<String, Method> propertyNamesToGetters = Maps.newHashMap();    Cache cache = CACHE.get();    cache.validateWellFormed(klass);    @SuppressWarnings("unchecked")    Iterable<PropertyDescriptor> propertyDescriptors = cache.getPropertyDescriptors(FluentIterable.from(getRegisteredOptions()).append(klass).toSet());    for (PropertyDescriptor descriptor : propertyDescriptors) {        propertyNamesToGetters.put(descriptor.getName(), descriptor.getReadMethod());    }    Map<String, Object> convertedOptions = Maps.newHashMap();    for (final Map.Entry<String, Collection<String>> entry : options.asMap().entrySet()) {        try {                        if (!propertyNamesToGetters.containsKey(entry.getKey())) {                SortedSet<String> closestMatches = new TreeSet<>(Sets.filter(propertyNamesToGetters.keySet(), input -> StringUtils.getLevenshteinDistance(entry.getKey(), input) <= 2));                switch(closestMatches.size()) {                    case 0:                        throw new IllegalArgumentException(String.format("Class %s missing a property named '%s'.", klass, entry.getKey()));                    case 1:                        throw new IllegalArgumentException(String.format("Class %s missing a property named '%s'. Did you mean '%s'?", klass, entry.getKey(), Iterables.getOnlyElement(closestMatches)));                    default:                        throw new IllegalArgumentException(String.format("Class %s missing a property named '%s'. Did you mean one of %s?", klass, entry.getKey(), closestMatches));                }            }            Method method = propertyNamesToGetters.get(entry.getKey());                        Class<?> returnType = method.getReturnType();            JavaType type = MAPPER.getTypeFactory().constructType(method.getGenericReturnType());            if ("runner".equals(entry.getKey())) {                String runner = Iterables.getOnlyElement(entry.getValue());                final Map<String, Class<? extends PipelineRunner<?>>> pipelineRunners = cache.supportedPipelineRunners;                if (pipelineRunners.containsKey(runner.toLowerCase())) {                    convertedOptions.put("runner", pipelineRunners.get(runner.toLowerCase(ROOT)));                } else {                    try {                        Class<?> runnerClass = Class.forName(runner, true, ReflectHelpers.findClassLoader());                        if (!PipelineRunner.class.isAssignableFrom(runnerClass)) {                            throw new IllegalArgumentException(String.format("Class '%s' does not implement PipelineRunner. " + "Supported pipeline runners %s", runner, cache.getSupportedRunners()));                        }                        convertedOptions.put("runner", runnerClass);                    } catch (ClassNotFoundException e) {                        String msg = String.format("Unknown 'runner' specified '%s', supported pipeline runners %s", runner, cache.getSupportedRunners());                        throw new IllegalArgumentException(msg, e);                    }                }            } else if (isCollectionOrArrayOfAllowedTypes(returnType, type)) {                                List<String> values = FluentIterable.from(entry.getValue()).transformAndConcat(input -> Arrays.asList(input.split(","))).toList();                if (values.contains("")) {                    checkEmptyStringAllowed(returnType, type, method.getGenericReturnType().toString());                }                convertedOptions.put(entry.getKey(), MAPPER.convertValue(values, type));            } else if (isSimpleType(returnType, type)) {                String value = Iterables.getOnlyElement(entry.getValue());                if (value.isEmpty()) {                    checkEmptyStringAllowed(returnType, type, method.getGenericReturnType().toString());                }                convertedOptions.put(entry.getKey(), MAPPER.convertValue(value, type));            } else {                String value = Iterables.getOnlyElement(entry.getValue());                if (value.isEmpty()) {                    checkEmptyStringAllowed(returnType, type, method.getGenericReturnType().toString());                }                try {                    convertedOptions.put(entry.getKey(), MAPPER.readValue(value, type));                } catch (IOException e) {                    throw new IllegalArgumentException("Unable to parse JSON value " + value, e);                }            }        } catch (IllegalArgumentException e) {            if (strictParsing) {                throw e;            } else {                            }        }    }    return convertedOptions;}
 Map<String, Class<? extends PipelineRunner<?>>> beam_f15218_0()
{    return supportedPipelineRunners;}
 synchronized Registration<T> beam_f15219_0(Class<T> iface, Set<Class<? extends PipelineOptions>> validatedPipelineOptionsInterfaces)
{    checkArgument(iface.isInterface(), "Only interface types are supported.");            validateInheritedInterfacesExtendPipelineOptions(iface);    @SuppressWarnings("unchecked")    Set<Class<? extends PipelineOptions>> combinedPipelineOptionsInterfaces = FluentIterable.from(validatedPipelineOptionsInterfaces).append(iface).toSet();        if (!combinedCache.containsKey(combinedPipelineOptionsInterfaces)) {        final Class<?>[] interfaces = combinedPipelineOptionsInterfaces.toArray(EMPTY_CLASS_ARRAY);        @SuppressWarnings("unchecked")        Class<T> allProxyClass = (Class<T>) Proxy.getProxyClass(ReflectHelpers.findClassLoader(interfaces), interfaces);        try {            List<PropertyDescriptor> propertyDescriptors = validateClass(iface, validatedPipelineOptionsInterfaces, allProxyClass);            combinedCache.put(combinedPipelineOptionsInterfaces, new Registration<>(allProxyClass, propertyDescriptors));        } catch (IntrospectionException e) {            throw new RuntimeException(e);        }    }        if (!interfaceCache.containsKey(iface)) {        @SuppressWarnings({ "rawtypes", "unchecked" })        Class<T> proxyClass = (Class<T>) Proxy.getProxyClass(ReflectHelpers.findClassLoader(iface), new Class[] { iface });        try {            List<PropertyDescriptor> propertyDescriptors = validateClass(iface, validatedPipelineOptionsInterfaces, proxyClass);            interfaceCache.put(iface, new Registration<>(proxyClass, propertyDescriptors));        } catch (IntrospectionException e) {            throw new RuntimeException(e);        }    }    @SuppressWarnings("unchecked")    Registration<T> result = (Registration<T>) interfaceCache.get(iface);    return result;}
 List<PropertyDescriptor> beam_f15220_0(Set<Class<? extends PipelineOptions>> interfaces)
{    return combinedCache.get(interfaces).getPropertyDescriptors();}
private static T beam_f15228_0(Class<T> klass, PipelineOptions options, boolean isCli)
{    checkNotNull(klass);    checkNotNull(options);    checkArgument(Proxy.isProxyClass(options.getClass()));    checkArgument(Proxy.getInvocationHandler(options) instanceof ProxyInvocationHandler);        T asClassOptions = options.as(klass);    ProxyInvocationHandler handler = (ProxyInvocationHandler) Proxy.getInvocationHandler(asClassOptions);    SortedSetMultimap<String, Method> requiredGroups = TreeMultimap.create(Ordering.natural(), PipelineOptionsFactory.MethodNameComparator.INSTANCE);    for (Method method : ReflectHelpers.getClosureOfMethodsOnInterface(klass)) {        Required requiredAnnotation = method.getAnnotation(Validation.Required.class);        if (requiredAnnotation != null) {            if (requiredAnnotation.groups().length > 0) {                for (String requiredGroup : requiredAnnotation.groups()) {                    requiredGroups.put(requiredGroup, method);                }            } else {                if (isCli) {                    checkArgument(handler.invoke(asClassOptions, method, null) != null, "Missing required value for [--%s, \"%s\"]. ", handler.getOptionName(method), getDescription(method));                } else {                    checkArgument(handler.invoke(asClassOptions, method, null) != null, "Missing required value for [%s, \"%s\"]. ", method, getDescription(method));                }            }        }    }    for (String requiredGroup : requiredGroups.keySet()) {        if (!verifyGroup(handler, asClassOptions, requiredGroups.get(requiredGroup))) {            throw new IllegalArgumentException("Missing required value for group [" + requiredGroup + "]. At least one of the following properties " + Collections2.transform(requiredGroups.get(requiredGroup), ReflectHelpers.METHOD_FORMATTER) + " required. Run with --help=" + klass.getSimpleName() + " for more information.");        }    }    return asClassOptions;}
private static boolean beam_f15229_0(ProxyInvocationHandler handler, PipelineOptions options, Collection<Method> requiredGroup)
{    for (Method m : requiredGroup) {        if (handler.invoke(options, m, null) != null) {            return true;        }    }    return false;}
private static String beam_f15230_0(Method method)
{    Description description = method.getAnnotation(Description.class);    return description == null ? "" : description.value();}
 synchronized T beam_f15238_0(Class<T> iface)
{    checkNotNull(iface);    checkArgument(iface.isInterface(), "Not an interface: %s", iface);    if (!interfaceToProxyCache.containsKey(iface)) {        Registration<T> registration = PipelineOptionsFactory.CACHE.get().validateWellFormed(iface, knownInterfaces);        List<PropertyDescriptor> propertyDescriptors = registration.getPropertyDescriptors();        Class<T> proxyClass = registration.getProxyClass();        gettersToPropertyNames.putAll(generateGettersToPropertyNames(propertyDescriptors));        settersToPropertyNames.putAll(generateSettersToPropertyNames(propertyDescriptors));        knownInterfaces.add(iface);        interfaceToProxyCache.putInstance(iface, InstanceBuilder.ofType(proxyClass).fromClass(proxyClass).withArg(InvocationHandler.class, this).build());    }    return interfaceToProxyCache.getInstance(iface);}
public boolean beam_f15239_0(Object obj)
{    return obj != null && ((obj instanceof ProxyInvocationHandler && this == obj) || (Proxy.isProxyClass(obj.getClass()) && this == Proxy.getInvocationHandler(obj)));}
public int beam_f15240_0()
{    return hashCode;}
private Object beam_f15248_0(PipelineOptions proxy, Method method)
{    if (method.getReturnType().equals(RuntimeValueProvider.class)) {        throw new RuntimeException(String.format("Method %s should not have return type " + "RuntimeValueProvider, use ValueProvider instead.", method.getName()));    }    if (method.getReturnType().equals(StaticValueProvider.class)) {        throw new RuntimeException(String.format("Method %s should not have return type " + "StaticValueProvider, use ValueProvider instead.", method.getName()));    }    @Nullable    Object defaultObject = null;    for (Annotation annotation : method.getAnnotations()) {        defaultObject = returnDefaultHelper(annotation, proxy, method);        if (defaultObject != null) {            break;        }    }    if (method.getReturnType().equals(ValueProvider.class)) {        String propertyName = gettersToPropertyNames.get(method.getName());        return defaultObject == null ? new RuntimeValueProvider(method.getName(), propertyName, (Class<? extends PipelineOptions>) method.getDeclaringClass(), proxy.getOptionsId()) : new RuntimeValueProvider(method.getName(), propertyName, (Class<? extends PipelineOptions>) method.getDeclaringClass(), defaultObject, proxy.getOptionsId());    } else if (defaultObject != null) {        return defaultObject;    }    /*     * We need to make sure that we return something appropriate for the return type. Thus we return     * a default value as defined by the JLS.     */    return Defaults.defaultValue(method.getReturnType());}
private Object beam_f15249_0(Annotation annotation, PipelineOptions proxy, Method method)
{    if (annotation instanceof Default.Class) {        return ((Default.Class) annotation).value();    } else if (annotation instanceof Default.String) {        return ((Default.String) annotation).value();    } else if (annotation instanceof Default.Boolean) {        return ((Default.Boolean) annotation).value();    } else if (annotation instanceof Default.Character) {        return ((Default.Character) annotation).value();    } else if (annotation instanceof Default.Byte) {        return ((Default.Byte) annotation).value();    } else if (annotation instanceof Default.Short) {        return ((Default.Short) annotation).value();    } else if (annotation instanceof Default.Integer) {        return ((Default.Integer) annotation).value();    } else if (annotation instanceof Default.Long) {        return ((Default.Long) annotation).value();    } else if (annotation instanceof Default.Float) {        return ((Default.Float) annotation).value();    } else if (annotation instanceof Default.Double) {        return ((Default.Double) annotation).value();    } else if (annotation instanceof Default.Enum) {        return Enum.valueOf((Class<Enum>) method.getReturnType(), ((Default.Enum) annotation).value());    } else if (annotation instanceof Default.InstanceFactory) {        return InstanceBuilder.ofType(((Default.InstanceFactory) annotation).value()).build().create(proxy);    }    return null;}
private static Map<String, String> beam_f15250_0(List<PropertyDescriptor> propertyDescriptors)
{    ImmutableMap.Builder<String, String> builder = ImmutableMap.builder();    for (PropertyDescriptor descriptor : propertyDescriptors) {        if (descriptor.getReadMethod() != null) {            builder.put(descriptor.getReadMethod().getName(), descriptor.getName());        }    }    return builder.build();}
public SdkHarnessLogLevelOverrides beam_f15258_0(Package pkg, LogLevel logLevel)
{    checkNotNull(pkg, "Expected package to be not null.");    addOverrideForName(pkg.getName(), logLevel);    return this;}
public SdkHarnessLogLevelOverrides beam_f15259_0(String name, LogLevel logLevel)
{    checkNotNull(name, "Expected name to be not null.");    checkNotNull(logLevel, "Expected logLevel to be one of %s.", Arrays.toString(LogLevel.values()));    put(name, logLevel);    return this;}
public static SdkHarnessLogLevelOverrides beam_f15260_0(Map<String, String> values)
{    checkNotNull(values, "Expected values to be not null.");    SdkHarnessLogLevelOverrides overrides = new SdkHarnessLogLevelOverrides();    for (Map.Entry<String, String> entry : values.entrySet()) {        try {            overrides.addOverrideForName(entry.getKey(), LogLevel.valueOf(entry.getValue()));        } catch (IllegalArgumentException e) {            throw new IllegalArgumentException(String.format("Unsupported log level '%s' requested for %s. Must be one of %s.", entry.getValue(), entry.getKey(), Arrays.toString(LogLevel.values())));        }    }    return overrides;}
public String beam_f15268_0()
{    if (value instanceof RuntimeValueProvider) {        return ((RuntimeValueProvider) value).propertyName();    } else if (value instanceof NestedValueProvider) {        return ((NestedValueProvider) value).propertyName();    } else {        throw new RuntimeException("Only a RuntimeValueProvider or a NestedValueProvider can supply" + " a property name.");    }}
public String beam_f15269_0()
{    if (isAccessible()) {        return String.valueOf(get());    }    return MoreObjects.toStringHelper(this).add("value", value).add("translator", translator.getClass().getSimpleName()).toString();}
 static void beam_f15270_0(PipelineOptions runtimeOptions)
{    optionsMap.put(runtimeOptions.getOptionsId(), runtimeOptions);}
public static String beam_f15278_0(String serializedOptions, Map<String, String> runtimeValues)
{    ObjectNode root, options;    try {        root = PipelineOptionsFactory.MAPPER.readValue(serializedOptions, ObjectNode.class);        options = (ObjectNode) root.get("options");        checkNotNull(options, "Unable to locate 'options' in %s", serializedOptions);    } catch (IOException e) {        throw new RuntimeException(String.format("Unable to parse %s", serializedOptions), e);    }    for (Map.Entry<String, String> entry : runtimeValues.entrySet()) {        options.put(entry.getKey(), entry.getValue());    }    try {        return PipelineOptionsFactory.MAPPER.writeValueAsString(root);    } catch (IOException e) {        throw new RuntimeException("Unable to parse re-serialize options", e);    }}
public static Pipeline beam_f15279_1()
{    Pipeline pipeline = new Pipeline(PipelineOptionsFactory.create());        return pipeline;}
public static Pipeline beam_f15280_1(PipelineOptions options)
{        PipelineRunner.fromOptions(options);    Pipeline pipeline = new Pipeline(options);        return pipeline;}
public CompositeBehavior beam_f15288_0(Node node)
{    if (!node.isRootNode()) {        checkForMatches(node);    }    if (matched.containsKey(node)) {        return CompositeBehavior.DO_NOT_ENTER_TRANSFORM;    } else {        return CompositeBehavior.ENTER_TRANSFORM;    }}
public void beam_f15289_0(Node node)
{    if (node.isRootNode()) {        checkState(matched.isEmpty(), "Found nodes that matched overrides. Matches: %s", matched);    }}
public void beam_f15290_0(Node node)
{    checkForMatches(node);}
public SchemaRegistry beam_f15298_0()
{    if (schemaRegistry == null) {        schemaRegistry = SchemaRegistry.createDefault();    }    return schemaRegistry;}
public void beam_f15299_0(CoderRegistry coderRegistry)
{    this.coderRegistry = coderRegistry;}
protected Pipeline beam_f15300_0()
{    if (pipeline == null) {        throw new IllegalStateException("Illegal access to pipeline after visitor traversal was completed");    }    return pipeline;}
private OutputT beam_f15311_1(String name, InputT input, PTransform<? super InputT, OutputT> transform)
{    String namePrefix = transforms.getCurrent().getFullName();    String uniqueName = uniquifyInternal(namePrefix, name);    final String builtName = buildName(namePrefix, name);    instancePerName.put(builtName, transform);        transforms.pushNode(uniqueName, input, transform);    try {        transforms.finishSpecifyingInput();        OutputT output = transform.expand(input);        transforms.setOutput(output);        return output;    } finally {        transforms.popNode();    }}
private void beam_f15312_1(Node original, PTransformOverrideFactory<InputT, OutputT, TransformT> replacementFactory)
{    PTransformReplacement<InputT, OutputT> replacement = replacementFactory.getReplacementTransform((AppliedPTransform<InputT, OutputT, TransformT>) original.toAppliedPTransform(this));    if (replacement.getTransform() == original.getTransform()) {        return;    }    InputT originalInput = replacement.getInput();        transforms.replaceNode(original, originalInput, replacement.getTransform());    try {        OutputT newOutput = replacement.getTransform().expand(originalInput);        Map<PValue, ReplacementOutput> originalToReplacement = replacementFactory.mapOutputs(original.getOutputs(), newOutput);                transforms.setOutput(newOutput);        transforms.replaceOutputs(originalToReplacement);    } finally {        transforms.popNode();    }}
 void beam_f15313_1(PipelineOptions options)
{    this.traverseTopologically(new ValidateVisitor(options));    final Collection<Map.Entry<String, Collection<PTransform<?, ?>>>> errors = Collections2.filter(instancePerName.asMap().entrySet(), Predicates.not(new IsUnique<>()));    if (!errors.isEmpty()) {        switch(options.getStableUniqueNames()) {            case OFF:                break;            case WARNING:                                break;            case             ERROR:                throw new IllegalStateException(String.format("Pipeline update will not be possible" + " because the following transforms do not have stable unique names: %s.", Joiner.on(", ").join(transform(errors, new KeysExtractor()))) + "\n\n" + "Conflicting instances:\n" + Joiner.on("\n").join(transform(errors, new UnstableNameToMessage(instancePerName))) + "\n\nYou can fix it adding a name when you call apply(): " + "pipeline.apply(<name>, <transform>).");            default:                throw new IllegalArgumentException("Unrecognized value for stable unique names: " + options.getStableUniqueNames());        }    }}
public boolean beam_f15321_0(@Nonnull final Map.Entry<K, Collection<V>> input)
{    return input != null && input.getValue().size() == 1;}
public final boolean beam_f15322_0()
{    return terminal;}
public final boolean beam_f15323_0()
{    return hasReplacement;}
 PTransformMatcher beam_f15331_0(PTransformMatcher matcher)
{    return application -> this.matches(application) || matcher.matches(application);}
public static PTransformOverride beam_f15332_0(PTransformMatcher matcher, PTransformOverrideFactory factory)
{    return new AutoValue_PTransformOverride(matcher, factory);}
public static PTransformReplacement<InputT, OutputT> beam_f15333_0(InputT input, PTransform<InputT, OutputT> transform)
{    return new AutoValue_PTransformOverrideFactory_PTransformReplacement(input, transform);}
public void beam_f15341_0(Map<PValue, ReplacementOutput> originalToReplacement)
{    current.replaceOutputs(originalToReplacement);}
public void beam_f15342_0()
{    current.finishSpecifying();    unexpandedInputs.remove(current);    current = current.getEnclosingNode();    checkState(current != null, "Can't pop the root node of a TransformHierarchy");}
 Node beam_f15343_0(PValue produced)
{    return producers.get(produced);}
public void beam_f15351_0(Node node)
{    parts.add(node);}
public void beam_f15352_1(Node existing, Node replacement)
{    checkNotNull(existing);    checkNotNull(replacement);    int existingIndex = parts.indexOf(existing);    checkArgument(existingIndex >= 0, "Tried to replace a node %s that doesn't exist as a component of node %s", existing.getFullName(), getFullName());        parts.set(existingIndex, replacement);}
public boolean beam_f15353_0()
{    return !parts.isEmpty() || isRootNode() || returnsOthersOutput();}
public AppliedPTransform<?, ?, ?> beam_f15361_0(Pipeline pipeline)
{    return AppliedPTransform.of(getFullName(), inputs, outputs, (PTransform) getTransform(), pipeline);}
private void beam_f15362_1(PipelineVisitor visitor, Set<PValue> visitedValues, Set<Node> visitedNodes, Set<Node> skippedComposites)
{    if (getEnclosingNode() != null && !visitedNodes.contains(getEnclosingNode())) {                getEnclosingNode().visit(visitor, visitedValues, visitedNodes, skippedComposites);    }        if (!visitedNodes.add(this)) {                return;    } else if (childNodeOf(skippedComposites)) {                                return;    }    if (!finishedSpecifying) {        finishSpecifying();    }    if (!isRootNode()) {                for (PValue inputValue : inputs.values()) {            Node valueProducer = maybeGetProducer(inputValue);            if (valueProducer != null) {                if (!visitedNodes.contains(valueProducer)) {                    valueProducer.visit(visitor, visitedValues, visitedNodes, skippedComposites);                }                if (visitedValues.add(inputValue)) {                                        visitor.visitValue(inputValue, valueProducer);                }            }        }    }    if (isCompositeNode()) {                PipelineVisitor.CompositeBehavior recurse = visitor.enterCompositeTransform(this);        if (recurse.equals(CompositeBehavior.ENTER_TRANSFORM)) {            for (Node child : parts) {                child.visit(visitor, visitedValues, visitedNodes, skippedComposites);            }        } else {            skippedComposites.add(this);        }        visitor.leaveCompositeTransform(this);    } else {                visitor.visitPrimitiveTransform(this);    }    if (!isRootNode()) {        checkNotNull(outputs, "Outputs for non-root node %s are null", getFullName());                for (PValue pValue : outputs.values()) {            if (visitedValues.add(pValue)) {                                visitor.visitValue(pValue, this);            }        }    }}
private boolean beam_f15363_0(Set<Node> nodes)
{    if (isRootNode()) {        return false;    }    Node parent = this.getEnclosingNode();    while (!parent.isRootNode() && !nodes.contains(parent)) {        parent = parent.getEnclosingNode();    }    return nodes.contains(parent);}
public List<FieldValueTypeInformation> beam_f15371_0(Class<?> clazz)
{        Class<?> targetClass = AutoValueUtils.getBaseAutoValueClass(clazz);    return ReflectUtils.getMethods(targetClass).stream().filter(ReflectUtils::isGetter).filter(m -> Modifier.isAbstract(m.getModifiers())).filter(m -> !Modifier.isPrivate(m.getModifiers())).filter(m -> !Modifier.isProtected(m.getModifiers())).filter(m -> !m.isAnnotationPresent(SchemaIgnore.class)).map(FieldValueTypeInformation::forGetter).map(t -> {        SchemaFieldName fieldName = t.getMethod().getAnnotation(SchemaFieldName.class);        return (fieldName == null) ? t : t.withName(fieldName.value());    }).collect(Collectors.toList());}
 FieldValueGetterFactory beam_f15372_0()
{    return (Class<?> targetClass, Schema schema) -> JavaBeanUtils.getGetters(targetClass, schema, AbstractGetterTypeSupplier.INSTANCE);}
 FieldValueTypeInformationFactory beam_f15373_0()
{    return (Class<?> targetClass, Schema schema) -> JavaBeanUtils.getFieldTypes(targetClass, schema, AbstractGetterTypeSupplier.INSTANCE);}
public static Qualifier beam_f15381_0(ListQualifier qualifier)
{    return AutoOneOf_FieldAccessDescriptor_FieldDescriptor_Qualifier.list(qualifier);}
public static Qualifier beam_f15382_0(MapQualifier qualifier)
{    return AutoOneOf_FieldAccessDescriptor_FieldDescriptor_Qualifier.map(qualifier);}
public static Builder beam_f15383_0()
{    return new AutoValue_FieldAccessDescriptor_FieldDescriptor.Builder().setQualifiers(Collections.emptyList());}
public static FieldAccessDescriptor beam_f15391_0(Iterable<FieldDescriptor> fields)
{    return builder().setFieldsAccessed(Lists.newArrayList(fields)).build();}
private static FieldAccessDescriptor beam_f15392_0(Iterable<FieldAccessDescriptor> fieldAccessDescriptors)
{    Set<FieldDescriptor> fieldsAccessed = Sets.newLinkedHashSet();    Multimap<FieldDescriptor, FieldAccessDescriptor> nestedFieldsAccessed = ArrayListMultimap.create();    for (FieldAccessDescriptor fieldAccessDescriptor : fieldAccessDescriptors) {        if (fieldAccessDescriptor.getAllFields()) {                        return FieldAccessDescriptor.withAllFields();        }        for (FieldDescriptor field : fieldAccessDescriptor.getFieldsAccessed()) {            fieldsAccessed.add(field);                        nestedFieldsAccessed.removeAll(field);        }        for (Map.Entry<FieldDescriptor, FieldAccessDescriptor> nested : fieldAccessDescriptor.getNestedFieldsAccessed().entrySet()) {            FieldDescriptor field = nested.getKey();            nestedFieldsAccessed.put(field, nested.getValue());        }    }        FieldAccessDescriptor fieldAccessDescriptor = FieldAccessDescriptor.withFields(fieldsAccessed);        for (Map.Entry<FieldDescriptor, Collection<FieldAccessDescriptor>> entry : nestedFieldsAccessed.asMap().entrySet()) {        if (fieldsAccessed.contains(entry.getKey())) {                        continue;        }                        fieldAccessDescriptor = fieldAccessDescriptor.withNestedField(entry.getKey(), union(entry.getValue()));    }    return fieldAccessDescriptor;}
public static FieldAccessDescriptor beam_f15393_0()
{    return builder().build();}
public Map<String, FieldAccessDescriptor> beam_f15401_0()
{    return getNestedFieldsAccessed().entrySet().stream().collect(Collectors.toMap(f -> f.getKey().getFieldName(), f -> f.getValue()));}
public boolean beam_f15402_0()
{    if (getAllFields()) {        return false;    }    if (getFieldsAccessed().size() == 1 && getNestedFieldsAccessed().isEmpty()) {        return true;    }    if (getFieldsAccessed().isEmpty() && getNestedFieldsAccessed().size() == 1) {        return getNestedFieldsAccessed().values().iterator().next().referencesSingleField();    }    return false;}
public FieldAccessDescriptor beam_f15403_0(Schema schema)
{    List<FieldDescriptor> resolvedFieldIdsAccessed = resolveDirectFieldsAccessed(schema);    Map<FieldDescriptor, FieldAccessDescriptor> resolvedNestedFieldsAccessed = resolveNestedFieldsAccessed(schema);    checkState(!getAllFields() || resolvedNestedFieldsAccessed.isEmpty(), "nested fields cannot be set if allFields is also set");            resolvedFieldIdsAccessed.removeAll(resolvedNestedFieldsAccessed.keySet());    return builder().setAllFields(getAllFields()).setFieldsAccessed(resolvedFieldIdsAccessed).setNestedFieldsAccessed(resolvedNestedFieldsAccessed).build();}
public static TypeDescriptor beam_f15411_0(FieldType fieldType)
{    switch(fieldType.getTypeName()) {        case LOGICAL_TYPE:                        return javaTypeForFieldType(fieldType.getLogicalType().getBaseType());        case ARRAY:            return TypeDescriptors.lists(javaTypeForFieldType(fieldType.getCollectionElementType()));        case MAP:            return TypeDescriptors.maps(javaTypeForFieldType(fieldType.getMapKeyType()), javaTypeForFieldType(fieldType.getMapValueType()));        case ROW:            return TypeDescriptors.rows();        default:            return PRIMITIVE_MAPPING.get(fieldType.getTypeName());    }}
public static FieldType beam_f15412_0(TypeDescriptor typeDescriptor)
{        if (typeDescriptor.isArray() || typeDescriptor.isSubtypeOf(TypeDescriptor.of(Collection.class))) {        return getArrayFieldType(typeDescriptor);    } else if (typeDescriptor.isSubtypeOf(TypeDescriptor.of(Map.class))) {        return getMapFieldType(typeDescriptor);    } else if (typeDescriptor.isSubtypeOf(TypeDescriptor.of(Row.class))) {        throw new IllegalArgumentException("Cannot automatically determine a field type from a Row class" + " as we cannot determine the schema. You should set a field type explicitly.");    } else {        TypeName typeName = PRIMITIVE_MAPPING.inverse().get(typeDescriptor);        if (typeName == null) {            throw new RuntimeException("Couldn't find field type for " + typeDescriptor);        }        return FieldType.of(typeName);    }}
private static FieldType beam_f15413_0(TypeDescriptor typeDescriptor)
{    if (typeDescriptor.isArray()) {        if (typeDescriptor.getComponentType().getType().equals(byte.class)) {            return FieldType.BYTES;        } else {            return FieldType.array(fieldTypeForJavaType(typeDescriptor.getComponentType()));        }    }    if (typeDescriptor.isSubtypeOf(TypeDescriptor.of(Collection.class))) {        TypeDescriptor<Collection<?>> collection = typeDescriptor.getSupertype(Collection.class);        if (collection.getType() instanceof ParameterizedType) {            ParameterizedType ptype = (ParameterizedType) collection.getType();            java.lang.reflect.Type[] params = ptype.getActualTypeArguments();            checkArgument(params.length == 1);            return FieldType.array(fieldTypeForJavaType(TypeDescriptor.of(params[0])));        }    }    throw new RuntimeException("Could not determine array parameter type for field.");}
private static FieldValueTypeInformation beam_f15421_0(Field field)
{    return getMapKeyType(TypeDescriptor.of(field.getGenericType()));}
private static FieldValueTypeInformation beam_f15422_0(TypeDescriptor<?> typeDescriptor)
{    return getMapType(typeDescriptor, 0);}
private static FieldValueTypeInformation beam_f15423_0(Field field)
{    return getMapType(TypeDescriptor.of(field.getGenericType()), 1);}
public SerializableFunction<T, Row> beam_f15431_0(TypeDescriptor<T> typeDescriptor)
{                            Schema schema = schemaFor(typeDescriptor);                Factory<List<FieldValueGetter>> getterFactory = new CachingFactory<>(fieldValueGetterFactory());    return o -> Row.withSchema(schema).withFieldValueGetters(getterFactory, o).build();}
public SerializableFunction<Row, T> beam_f15432_0(TypeDescriptor<T> typeDescriptor)
{    Class<T> clazz = (Class<T>) typeDescriptor.getType();    return new FromRowUsingCreator<>(clazz, schemaTypeCreatorFactory(), fieldValueTypeInformationFactory());}
public List<FieldValueTypeInformation> beam_f15433_0(Class<?> clazz)
{    return ReflectUtils.getMethods(clazz).stream().filter(ReflectUtils::isGetter).filter(m -> !m.isAnnotationPresent(SchemaIgnore.class)).map(FieldValueTypeInformation::forGetter).map(t -> {        SchemaFieldName fieldName = t.getMethod().getAnnotation(SchemaFieldName.class);        return (fieldName != null) ? t.withName(fieldName.value()) : t;    }).collect(Collectors.toList());}
public Schema beam_f15441_0(TypeDescriptor<T> typeDescriptor)
{    return POJOUtils.schemaFromPojoClass(typeDescriptor.getRawType(), JavaFieldTypeSupplier.INSTANCE);}
public FieldValueGetterFactory beam_f15442_0()
{    return (Class<?> targetClass, Schema schema) -> POJOUtils.getGetters(targetClass, schema, JavaFieldTypeSupplier.INSTANCE);}
public FieldValueTypeInformationFactory beam_f15443_0()
{    return (Class<?> targetClass, Schema schema) -> POJOUtils.getFieldTypes(targetClass, schema, JavaFieldTypeSupplier.INSTANCE);}
public int beam_f15451_0()
{    return byteArraySize;}
public String beam_f15452_0()
{    return IDENTIFIER;}
public String beam_f15453_0()
{    return Integer.toString(byteArraySize);}
public FieldAccessDescriptor beam_f15461_0(SimpleIdentifierContext ctx)
{    FieldDescriptor field = FieldDescriptor.builder().setFieldName(ctx.IDENTIFIER().getText()).build();    return FieldAccessDescriptor.withFields(field);}
public FieldAccessDescriptor beam_f15462_0(WildcardContext ctx)
{    return FieldAccessDescriptor.withAllFields();}
public FieldAccessDescriptor beam_f15463_0(QualifiedComponentContext ctx)
{    QualifierVisitor qualifierVisitor = new QualifierVisitor();    ctx.qualifierList().accept(qualifierVisitor);    FieldDescriptor field = FieldDescriptor.builder().setFieldName(ctx.IDENTIFIER().getText()).setQualifiers(qualifierVisitor.getQualifiers()).build();    return FieldAccessDescriptor.withFields(field);}
public Builder beam_f15471_0(Field... fields)
{    return addFields(Arrays.asList(fields));}
public Builder beam_f15472_0(Field field)
{    fields.add(field);    return this;}
public Builder beam_f15473_0(String name, FieldType type)
{    fields.add(Field.of(name, type));    return this;}
public Builder beam_f15481_0(String name)
{    fields.add(Field.of(name, FieldType.FLOAT));    return this;}
public Builder beam_f15482_0(String name)
{    fields.add(Field.of(name, FieldType.DOUBLE));    return this;}
public Builder beam_f15483_0(String name)
{    fields.add(Field.of(name, FieldType.STRING));    return this;}
public Schema beam_f15491_0()
{    return new Schema(fields);}
public static Builder beam_f15492_0()
{    return new Builder();}
public static Schema beam_f15493_0(Field... fields)
{    return Schema.builder().addFields(fields).build();}
public boolean beam_f15501_0(Schema other)
{    return equivalent(other, EquivalenceNullablePolicy.WEAKEN);}
public boolean beam_f15502_0(Schema other)
{    return equivalent(other, EquivalenceNullablePolicy.IGNORE);}
private boolean beam_f15503_0(Schema other, EquivalenceNullablePolicy nullablePolicy)
{    if (other.getFieldCount() != getFieldCount()) {        return false;    }    List<Field> otherFields = other.getFields().stream().sorted(Comparator.comparing(Field::getName)).collect(Collectors.toList());    List<Field> actualFields = getFields().stream().sorted(Comparator.comparing(Field::getName)).collect(Collectors.toList());    for (int i = 0; i < otherFields.size(); ++i) {        Field otherField = otherFields.get(i);        Field actualField = actualFields.get(i);        if (!actualField.equivalent(otherField, nullablePolicy)) {            return false;        }    }    return true;}
public boolean beam_f15511_0()
{    return COLLECTION_TYPES.contains(this);}
public boolean beam_f15512_0()
{    return MAP_TYPES.contains(this);}
public boolean beam_f15513_0()
{    return COMPOSITE_TYPES.contains(this);}
public static final FieldType beam_f15521_0(FieldType elementType)
{    return FieldType.forTypeName(TypeName.ARRAY).setCollectionElementType(elementType).build();}
public static final FieldType beam_f15522_0(FieldType elementType, boolean nullable)
{    return FieldType.forTypeName(TypeName.ARRAY).setCollectionElementType(elementType.withNullable(nullable)).build();}
public static final FieldType beam_f15523_0(FieldType keyType, FieldType valueType)
{    return FieldType.forTypeName(TypeName.MAP).setMapKeyType(keyType).setMapValueType(valueType).build();}
public String beam_f15531_0(String key)
{    ByteArrayWrapper metadata = getMetadata().get(key);    if (metadata != null) {        return new String(metadata.array, StandardCharsets.UTF_8);    } else {        return "";    }}
public FieldType beam_f15532_0(boolean nullable)
{    return toBuilder().setNullable(nullable).build();}
public boolean beam_f15533_0(Object o)
{    if (!(o instanceof FieldType)) {        return false;    }                FieldType other = (FieldType) o;    return Objects.equals(getTypeName(), other.getTypeName()) && Objects.equals(getNullable(), other.getNullable()) && Objects.equals(getCollectionElementType(), other.getCollectionElementType()) && Objects.equals(getMapKeyType(), other.getMapKeyType()) && Objects.equals(getMapValueType(), other.getMapValueType()) && Objects.equals(getRowSchema(), other.getRowSchema()) && Objects.equals(getMetadata(), other.getMetadata());}
public Field beam_f15541_0(FieldType fieldType)
{    return toBuilder().setType(fieldType).build();}
public Field beam_f15542_0(boolean isNullable)
{    return toBuilder().setType(getType().withNullable(isNullable)).build();}
public boolean beam_f15543_0(Object o)
{    if (!(o instanceof Field)) {        return false;    }    Field other = (Field) o;    return Objects.equals(getName(), other.getName()) && Objects.equals(getDescription(), other.getDescription()) && Objects.equals(getType(), other.getType());}
public Field beam_f15551_0(String name)
{    return getFields().get(indexOf(name));}
public int beam_f15552_0(String fieldName)
{    Integer index = fieldIndices.get(fieldName);    if (index == null) {        throw new IllegalArgumentException(String.format("Cannot find field %s in schema %s", fieldName, this));    }    return index;}
public boolean beam_f15553_0(String fieldName)
{    return fieldIndices.containsKey(fieldName);}
public void beam_f15561_0(T value, OutputStream outStream) throws IOException
{    rowCoder.encode(toRowFunction.apply(value), outStream);}
public T beam_f15562_0(InputStream inStream) throws IOException
{    return fromRowFunction.apply(rowCoder.decode(inStream));}
public void beam_f15563_0() throws NonDeterministicException
{    rowCoder.verifyDeterministic();}
public void beam_f15571_0(Class<T> clazz, Schema schema, SerializableFunction<T, Row> toRow, SerializableFunction<Row, T> fromRow)
{    registerSchemaForType(TypeDescriptor.of(clazz), schema, toRow, fromRow);}
public void beam_f15572_0(TypeDescriptor<T> type, Schema schema, SerializableFunction<T, Row> toRow, SerializableFunction<Row, T> fromRow)
{    entries.put(type, new SchemaEntry<>(schema, toRow, fromRow));}
public void beam_f15573_0(SchemaProvider schemaProvider)
{    providers.addFirst(schemaProvider);}
private ReturnT beam_f15581_0(Function<SchemaProvider, ReturnT> f) throws NoSuchSchemaException
{    for (SchemaProvider provider : providers) {        ReturnT result = f.apply(provider);        if (result != null) {            return result;        }    }    throw new NoSuchSchemaException();}
public Schema beam_f15582_0(TypeDescriptor<T> typeDescriptor) throws NoSuchSchemaException
{    SchemaEntry entry = entries.get(typeDescriptor);    if (entry != null) {        return entry.schema;    }    return getProviderResult((SchemaProvider p) -> p.schemaFor(typeDescriptor));}
public SerializableFunction<T, Row> beam_f15583_0(Class<T> clazz) throws NoSuchSchemaException
{    return getToRowFunction(TypeDescriptor.of(clazz));}
 static NewField beam_f15591_0(FieldAccessDescriptor fieldAccessDescriptor, Schema.FieldType fieldType, Object defaultValue)
{    return new AutoValue_AddFields_Inner_NewField.Builder().setName(getName(fieldAccessDescriptor)).setDescriptor(fieldAccessDescriptor).setFieldType(fieldType).setDefaultValue(defaultValue).build();}
 NewField beam_f15592_0()
{    FieldAccessDescriptor descriptor = Iterables.getOnlyElement(getDescriptor().getNestedFieldsAccessed().values());    return toBuilder().setDescriptor(descriptor).setName(getName(descriptor)).build();}
 static String beam_f15593_0(FieldAccessDescriptor descriptor)
{    if (!descriptor.getFieldsAccessed().isEmpty()) {        return Iterables.getOnlyElement(descriptor.fieldNamesAccessed());    } else {        return Iterables.getOnlyElement(descriptor.nestedFieldsByName().keySet());    }}
private static Object beam_f15601_0(Object original, Schema.FieldType fieldType, AddFieldsInformation addFieldsInformation)
{    switch(fieldType.getTypeName()) {        case ROW:            if (original == null) {                original = Row.withSchema(fieldType.getRowSchema()).build();            }            return fillNewFields((Row) original, addFieldsInformation);        case ARRAY:            if (original == null) {                return Collections.emptyList();            }            List<Object> list = (List<Object>) original;            List<Object> filledList = new ArrayList<>(list.size());            Schema.FieldType elementType = fieldType.getCollectionElementType();            AddFieldsInformation elementAddFieldInformation = addFieldsInformation.toBuilder().setOutputFieldType(elementType).build();            for (Object element : list) {                filledList.add(fillNewFields(element, elementType, elementAddFieldInformation));            }            return filledList;        case MAP:            if (original == null) {                return Collections.emptyMap();            }            Map<Object, Object> originalMap = (Map<Object, Object>) original;            Map<Object, Object> filledMap = Maps.newHashMapWithExpectedSize(originalMap.size());            Schema.FieldType mapValueType = fieldType.getMapValueType();            AddFieldsInformation mapValueAddFieldInformation = addFieldsInformation.toBuilder().setOutputFieldType(mapValueType).build();            for (Map.Entry<Object, Object> entry : originalMap.entrySet()) {                filledMap.put(entry.getKey(), fillNewFields(entry.getValue(), mapValueType, mapValueAddFieldInformation));            }            return filledMap;        default:            throw new RuntimeException("Unexpected field type");    }}
public PCollection<Row> beam_f15602_0(PCollection<T> input)
{    final AddFieldsInformation addFieldsInformation = getAddFieldsInformation(input.getSchema(), newFields);    Schema outputSchema = checkNotNull(addFieldsInformation.getOutputFieldType().getRowSchema());    return input.apply(ParDo.of(new DoFn<T, Row>() {        @ProcessElement        public void processElement(@Element Row row, OutputReceiver<Row> o) {            o.output(fillNewFields(row, addFieldsInformation));        }    })).setRowSchema(outputSchema);}
public void beam_f15603_0(@Element Row row, OutputReceiver<Row> o)
{    o.output(fillNewFields(row, addFieldsInformation));}
public List<CompatibilityError> beam_f15611_0(List<CompatibilityError> left, List<CompatibilityError> right)
{    return ImmutableList.<CompatibilityError>builder().addAll(left).addAll(right).build();}
public List<CompatibilityError> beam_f15612_0(Context context, Optional<Field> left, Optional<Field> right)
{    if (!left.isPresent() && !right.isPresent()) {        return Collections.emptyList();    } else if (left.isPresent() && !right.isPresent()) {        return Collections.emptyList();    } else if (!left.isPresent() && right.isPresent()) {        return Collections.singletonList(CompatibilityError.create(context.path(), "Field is missing in output schema"));    } else {        if (left.get().getType().getNullable() && !right.get().getType().getNullable()) {            return Collections.singletonList(CompatibilityError.create(context.path(), "Can't cast nullable field to non-nullable field"));        }    }    return Collections.emptyList();}
public List<CompatibilityError> beam_f15613_0(Context context, FieldType input, FieldType output)
{    TypeName inputType = input.getTypeName();    TypeName outputType = output.getTypeName();    boolean supertype = outputType.isSupertypeOf(inputType);    if (isIntegral(inputType) && isDecimal(outputType)) {        return Collections.emptyList();    } else if (!supertype) {        return Collections.singletonList(CompatibilityError.create(context.path(), "Can't cast '" + inputType + "' to '" + outputType + "'"));    }    return Collections.emptyList();}
public static boolean beam_f15621_0(TypeName type)
{    return type == TypeName.FLOAT || type == TypeName.DOUBLE || type == TypeName.DECIMAL;}
public void beam_f15622_0(Schema inputSchema)
{    List<CompatibilityError> errors = validator().apply(inputSchema, outputSchema());    if (!errors.isEmpty()) {        String reason = errors.stream().map(x -> Joiner.on('.').join(x.path()) + ": " + x.message()).collect(Collectors.joining("\n\t"));        throw new IllegalArgumentException("Cast isn't compatible using " + validator() + ":\n\t" + reason);    }}
public PCollection<Row> beam_f15623_0(PCollection<T> input)
{    Schema inputSchema = input.getSchema();    verifyCompatibility(inputSchema);    return input.apply(ParDo.of(new DoFn<T, Row>() {                                @FieldAccess("filterFields")        final FieldAccessDescriptor fieldAccessDescriptor = FieldAccessDescriptor.withAllFields();        @ProcessElement        public void process(@FieldAccess("filterFields") @Element Row input, OutputReceiver<Row> r) {            Row output = castRow(input, inputSchema, outputSchema());            r.output(output);        }    })).setRowSchema(outputSchema());}
public By beam_f15631_0()
{    return toBuilder().setOptionalParticipation(true).build();}
 JoinArguments beam_f15632_0(String tag, By clause)
{    return new JoinArguments(new ImmutableMap.Builder<String, By>().putAll(joinArgsMap).put(tag, clause).build());}
private FieldAccessDescriptor beam_f15633_0(String tag)
{    return (allInputsJoinArgs != null) ? allInputsJoinArgs.getFieldAccessDescriptor() : joinArgsMap.get(tag).getFieldAccessDescriptor();}
public Impl beam_f15641_0(String tag, By clause)
{    if (joinArgs.allInputsJoinArgs != null) {        throw new IllegalStateException("Cannot set both a global and per-tag fields.");    }    return new Impl(joinArgs.with(tag, clause));}
public ExpandCrossProduct beam_f15642_0()
{    return new ExpandCrossProduct(joinArgs);}
private Schema beam_f15643_0(JoinInformation joinInformation)
{            Schema.Builder joinedSchemaBuilder = Schema.builder();    for (Map.Entry<String, Schema> entry : joinInformation.componentSchemas.entrySet()) {        joinedSchemaBuilder.addArrayField(entry.getKey(), FieldType.row(entry.getValue()));    }    return joinedSchemaBuilder.build();}
private void beam_f15651_0(int tagIndex, List<Row> accumulatedRows, List<Iterable> iterables, OutputReceiver<Row> o)
{    if (tagIndex >= sortedTags.size()) {        return;    }    SerializableFunction<Object, Row> toRow = toRows.get(tagIndex);    for (Object item : iterables.get(tagIndex)) {                        Row row = toRow.apply(item);        crossProductHelper(tagIndex, accumulatedRows, row, iterables, o);    }}
private void beam_f15652_0(int tagIndex, List<Row> accumulatedRows, Row newRow, List<Iterable> iterables, OutputReceiver<Row> o)
{    boolean atBottom = tagIndex == sortedTags.size() - 1;    accumulatedRows.add(newRow);    if (atBottom) {                o.output(buildOutputRow(accumulatedRows));    } else {        crossProduct(tagIndex + 1, accumulatedRows, iterables, o);    }    accumulatedRows.remove(accumulatedRows.size() - 1);}
private Row beam_f15653_0(List rows)
{    return Row.withSchema(outputSchema).addValues(rows).build();}
public void beam_f15661_0(@Element Row row, OutputReceiver<OutputT> o)
{        Object input = unbox ? row.getValue(0) : row;        o.output(converted.outputSchemaCoder.getFromRowFunction().apply((Row) input));}
public void beam_f15662_0(@Element Row row, OutputReceiver<OutputT> o)
{    o.output(convertPrimitive.apply(row.getValue(0)));}
public static Inner<T> beam_f15663_0(String... fields)
{    return fields(FieldAccessDescriptor.withFieldNames(fields));}
public Inner<T> beam_f15671_0(List<String> fieldNames, SerializableFunction<Row, Boolean> predicate)
{    filters.add(new AutoValue_Filter_Inner_FilterDescription.Builder<Row>().setFieldAccessDescriptor(FieldAccessDescriptor.withFieldNames(fieldNames)).setPredicate(predicate).setSelectsSingleField(false).build());    return this;}
public Inner<T> beam_f15672_0(List<Integer> fieldIds, SerializableFunction<Row, Boolean> predicate)
{    filters.add(new AutoValue_Filter_Inner_FilterDescription.Builder<Row>().setFieldAccessDescriptor(FieldAccessDescriptor.withFieldIds(fieldIds)).setPredicate(predicate).setSelectsSingleField(false).build());    return this;}
public PCollection<T> beam_f15673_0(PCollection<T> input)
{    Schema inputSchema = input.getSchema();    List<FilterDescription> resolvedFilters = filters.stream().map(f -> f.toBuilder().setFieldAccessDescriptor(f.getFieldAccessDescriptor().resolve(inputSchema)).build()).map(f -> f.toBuilder().setSelectedSchema(SelectHelpers.getOutputSchema(inputSchema, f.getFieldAccessDescriptor())).build()).collect(Collectors.toList());    return input.apply(ParDo.of(new DoFn<T, T>() {        @ProcessElement        public void process(@Element Row row, OutputReceiver<Row> o) {            for (FilterDescription filter : resolvedFilters) {                Row selected = SelectHelpers.selectRow(row, filter.getFieldAccessDescriptor(), inputSchema, filter.getSelectedSchema());                if (filter.getSelectsSingleField()) {                    SerializableFunction<Object, Boolean> predicate = (SerializableFunction<Object, Boolean>) filter.getPredicate();                    if (!predicate.apply(selected.getValue(0))) {                        return;                    }                } else {                    SerializableFunction<Row, Boolean> predicate = (SerializableFunction<Row, Boolean>) filter.getPredicate();                    if (!predicate.apply(selected)) {                        return;                    }                }            }                        o.output(row);        }    }));}
public CombineGlobally<InputT, OutputT> beam_f15681_0(CombineFn<InputT, ?, OutputT> combineFn)
{    return new CombineGlobally<>(combineFn);}
public CombineFieldsGlobally<InputT> beam_f15682_0(String inputFieldName, CombineFn<CombineInputT, AccumT, CombineOutputT> fn, String outputFieldName)
{    return new CombineFieldsGlobally<>(SchemaAggregateFn.<InputT>create().aggregateFields(FieldAccessDescriptor.withFieldNames(inputFieldName), fn, outputFieldName));}
public CombineFieldsGlobally<InputT> beam_f15683_0(int inputFieldId, CombineFn<CombineInputT, AccumT, CombineOutputT> fn, String outputFieldName)
{    return new CombineFieldsGlobally<>(SchemaAggregateFn.<InputT>create().aggregateFields(FieldAccessDescriptor.withFieldIds(inputFieldId), fn, outputFieldName));}
public CombineFieldsGlobally<InputT> beam_f15691_0(FieldAccessDescriptor fieldsToAggregate, CombineFn<CombineInputT, AccumT, CombineOutputT> fn, Field outputField)
{    return new CombineFieldsGlobally<>(SchemaAggregateFn.<InputT>create().aggregateFields(fieldsToAggregate, fn, outputField));}
public PCollection<Iterable<InputT>> beam_f15692_0(PCollection<InputT> input)
{    return input.apply(WithKeys.of((Void) null)).apply(GroupByKey.create()).apply(Values.create());}
public PCollection<OutputT> beam_f15693_0(PCollection<InputT> input)
{    return input.apply(Combine.globally(combineFn));}
public CombineFieldsGlobally<InputT> beam_f15701_0(List<String> inputFieldNames, CombineFn<CombineInputT, AccumT, CombineOutputT> fn, Field outputField)
{    return aggregateFields(FieldAccessDescriptor.withFieldNames(inputFieldNames), fn, outputField);}
public CombineFieldsGlobally<InputT> beam_f15702_0(List<Integer> inputFieldIds, CombineFn<CombineInputT, AccumT, CombineOutputT> fn, Field outputField)
{    return aggregateFields(FieldAccessDescriptor.withFieldIds(inputFieldIds), fn, outputField);}
public CombineFieldsGlobally<InputT> beam_f15703_0(FieldAccessDescriptor fieldAccessDescriptor, CombineFn<CombineInputT, AccumT, CombineOutputT> fn, Field outputField)
{    return new CombineFieldsGlobally<>(schemaAggregateFn.aggregateFields(fieldAccessDescriptor, fn, outputField));}
public CombineFieldsByFields<InputT> beam_f15711_0(List<String> inputFieldNames, CombineFn<CombineInputT, AccumT, CombineOutputT> fn, String outputFieldName)
{    return aggregateFields(FieldAccessDescriptor.withFieldNames(inputFieldNames), fn, outputFieldName);}
public CombineFieldsByFields<InputT> beam_f15712_0(List<Integer> inputFieldIds, CombineFn<CombineInputT, AccumT, CombineOutputT> fn, String outputFieldName)
{    return aggregateFields(FieldAccessDescriptor.withFieldIds(inputFieldIds), fn, outputFieldName);}
public CombineFieldsByFields<InputT> beam_f15713_0(FieldAccessDescriptor fieldsToAggregate, CombineFn<CombineInputT, AccumT, CombineOutputT> fn, String outputFieldName)
{    return new CombineFieldsByFields<>(this, SchemaAggregateFn.<InputT>create().aggregateFields(fieldsToAggregate, fn, outputFieldName));}
public CombineFieldsByFields<InputT> beam_f15721_0(int inputFieldId, CombineFn<CombineInputT, AccumT, CombineOutputT> fn, String outputFieldName)
{    return new CombineFieldsByFields<>(byFields, schemaAggregateFn.aggregateFields(FieldAccessDescriptor.withFieldIds(inputFieldId), fn, outputFieldName));}
public CombineFieldsByFields<InputT> beam_f15722_0(String inputFieldName, CombineFn<CombineInputT, AccumT, CombineOutputT> fn, Field outputField)
{    return new CombineFieldsByFields<>(byFields, schemaAggregateFn.aggregateFields(FieldAccessDescriptor.withFieldNames(inputFieldName), fn, outputField));}
public CombineFieldsByFields<InputT> beam_f15723_0(int inputFieldId, CombineFn<CombineInputT, AccumT, CombineOutputT> fn, Field outputField)
{    return new CombineFieldsByFields<>(byFields, schemaAggregateFn.aggregateFields(FieldAccessDescriptor.withFieldIds(inputFieldId), fn, outputField));}
public static Impl beam_f15731_0(Integer... fieldIds)
{    return new Impl(FieldAccessDescriptor.withFieldIds(fieldIds), FieldAccessDescriptor.create());}
public static Impl beam_f15732_0(FieldAccessDescriptor fieldAccessDescriptor)
{    return new Impl(fieldAccessDescriptor, FieldAccessDescriptor.create());}
public Impl beam_f15733_0(String... fieldNames)
{    return new Impl(FieldAccessDescriptor.create(), FieldAccessDescriptor.withFieldNames(fieldNames));}
public Impl beam_f15741_0(FieldAccessDescriptor fieldAccessDescriptor)
{    return new Impl(lhs, fieldAccessDescriptor);}
private Impl beam_f15742_0(Schema lhsSchema, Schema rhsSchema)
{    return new Impl(lhs.resolve(lhsSchema), rhs.resolve(rhsSchema));}
public static Impl<LhsT, RhsT> beam_f15743_0(PCollection<RhsT> rhs)
{    return new Impl<>(JoinType.INNER, rhs);}
public PCollection<Row> beam_f15751_0(PCollection lhs)
{    FieldsEqual.Impl resolvedPredicate = predicate.resolve(lhs.getSchema(), rhs.getSchema());    PCollectionTuple tuple = PCollectionTuple.of(LHS_TAG, lhs).and(RHS_TAG, rhs);    switch(joinType) {        case INNER:            return tuple.apply(CoGroup.join(LHS_TAG, CoGroup.By.fieldAccessDescriptor(resolvedPredicate.lhs)).join(RHS_TAG, CoGroup.By.fieldAccessDescriptor(resolvedPredicate.rhs)).crossProductJoin());        case OUTER:            return tuple.apply(CoGroup.join(LHS_TAG, CoGroup.By.fieldAccessDescriptor(resolvedPredicate.lhs).withOptionalParticipation()).join(RHS_TAG, CoGroup.By.fieldAccessDescriptor(resolvedPredicate.rhs).withOptionalParticipation()).crossProductJoin());        case LEFT_OUTER:            return tuple.apply(CoGroup.join(LHS_TAG, CoGroup.By.fieldAccessDescriptor(resolvedPredicate.lhs)).join(RHS_TAG, CoGroup.By.fieldAccessDescriptor(resolvedPredicate.rhs).withOptionalParticipation()).crossProductJoin());        case RIGHT_OUTER:            return tuple.apply(CoGroup.join(LHS_TAG, CoGroup.By.fieldAccessDescriptor(resolvedPredicate.lhs).withOptionalParticipation()).join(RHS_TAG, CoGroup.By.fieldAccessDescriptor(resolvedPredicate.rhs)).crossProductJoin());        default:            throw new RuntimeException("Unexpected join type");    }}
public static Inner<T> beam_f15752_0()
{    return new Inner<>();}
 RenamePair beam_f15753_0(Schema schema)
{    FieldAccessDescriptor resolved = fieldAccessDescriptor.resolve(schema);    if (!resolved.referencesSingleField()) {        throw new IllegalArgumentException(resolved + " references multiple fields.");    }    return new RenamePair(resolved, newName);}
 FieldAggregation<FieldT, AccumT, OutputT> beam_f15761_0(Schema schema)
{    return new FieldAggregation<>(fieldsToAggregate, outputField, fn, combineTag, aggregationSchema, schema);}
 Inner<T> beam_f15762_0(Schema inputSchema, SerializableFunction<T, Row> toRowFunction)
{    List<FieldAggregation> fieldAggregations = getFieldAggregations().stream().map(f -> f.resolve(inputSchema)).collect(Collectors.toList());    ComposedCombineFn<T> composedCombineFn = null;    for (int i = 0; i < fieldAggregations.size(); ++i) {        FieldAggregation fieldAggregation = fieldAggregations.get(i);        SimpleFunction<T, ?> extractFunction;        Coder extractOutputCoder;        if (fieldAggregation.unnestedInputSubSchema.getFieldCount() == 1) {            extractFunction = new ExtractSingleFieldFunction<>(fieldAggregation, toRowFunction);            extractOutputCoder = RowCoder.coderForFieldType(fieldAggregation.unnestedInputSubSchema.getField(0).getType());        } else {            extractFunction = new ExtractFieldsFunction<>(fieldAggregation, toRowFunction);            extractOutputCoder = RowCoder.of(fieldAggregation.inputSubSchema);        }        if (i == 0) {            composedCombineFn = CombineFns.compose().with(extractFunction, extractOutputCoder, fieldAggregation.fn, fieldAggregation.combineTag);        } else {            composedCombineFn = composedCombineFn.with(extractFunction, extractOutputCoder, fieldAggregation.fn, fieldAggregation.combineTag);        }    }    return toBuilder().setInputSchema(inputSchema).setComposedCombineFn(composedCombineFn).setFieldAggregations(fieldAggregations).build();}
 Inner<T> beam_f15763_0(FieldAccessDescriptor fieldsToAggregate, CombineFn<CombineInputT, AccumT, CombineOutputT> fn, String outputFieldName)
{    return aggregateFields(fieldsToAggregate, fn, Field.of(outputFieldName, FieldTypeDescriptors.fieldTypeForJavaType(fn.getOutputType())));}
public Coder<Object[]> beam_f15771_0(CoderRegistry registry, Coder<T> inputCoder) throws CannotProvideCoderException
{    return getComposedCombineFn().getAccumulatorCoder(registry, inputCoder);}
public Coder<Row> beam_f15772_0(CoderRegistry registry, Coder<T> inputCoder)
{    return SchemaCoder.of(getOutputSchema(), SerializableFunctions.identity(), SerializableFunctions.identity());}
public Row beam_f15773_0(Object[] accumulator)
{        CoCombineResult coCombineResult = getComposedCombineFn().extractOutput(accumulator);    Row.Builder output = Row.withSchema(getOutputSchema());    for (FieldAggregation fieldAggregation : getFieldAggregations()) {        Object aggregate = coCombineResult.get(fieldAggregation.combineTag);        output.addValue(aggregate);    }    return output.build();}
 static Schema beam_f15781_0(Schema schema, SerializableFunction<List<String>, String> fn)
{    List<String> nameComponents = Lists.newArrayList();    return getUnnestedSchema(schema, nameComponents, fn);}
private static Schema beam_f15782_0(Schema schema, List<String> nameComponents, SerializableFunction<List<String>, String> fn)
{    Schema.Builder builder = Schema.builder();    for (Field field : schema.getFields()) {        nameComponents.add(field.getName());        if (field.getType().getTypeName().isCompositeType()) {            Schema nestedSchema = getUnnestedSchema(field.getType().getRowSchema(), nameComponents, fn);            for (Field nestedField : nestedSchema.getFields()) {                builder.addField(nestedField);            }        } else {            String name = fn.apply(nameComponents);            Field newField = field.toBuilder().setName(name).build();            builder.addField(newField);        }        nameComponents.remove(nameComponents.size() - 1);    }    return builder.build();}
 static Row beam_f15783_0(Row input, Schema unnestedSchema)
{    Row.Builder builder = Row.withSchema(unnestedSchema);    unnestRow(input, builder);    return builder.build();}
private static String beam_f15791_0(String baseClass)
{    int lastDot = baseClass.lastIndexOf('.');    String packageName = baseClass.substring(0, lastDot);    String baseName = baseClass.substring(lastDot + 1, baseClass.length());    baseName = baseName.replace('$', '_');    return packageName + ".AutoValue_" + baseName;}
public static SchemaUserTypeCreator beam_f15792_0(Class<?> clazz, Schema schema, FieldValueTypeSupplier fieldValueTypeSupplier)
{    Class<?> generatedClass = getAutoValueGenerated(clazz);    List<FieldValueTypeInformation> schemaTypes = fieldValueTypeSupplier.get(clazz, schema);    Optional<Constructor<?>> constructor = Arrays.stream(generatedClass.getDeclaredConstructors()).filter(c -> !Modifier.isPrivate(c.getModifiers())).filter(c -> matchConstructor(c, schemaTypes)).findAny();    return constructor.map(c -> JavaBeanUtils.getConstructorCreator(generatedClass, c, schema, fieldValueTypeSupplier)).orElse(null);}
private static boolean beam_f15793_0(Constructor<?> constructor, List<FieldValueTypeInformation> getterTypes)
{    if (constructor.getParameters().length != getterTypes.size()) {        return false;    }    Map<String, FieldValueTypeInformation> typeMap = getterTypes.stream().collect(Collectors.toMap(f -> ReflectUtils.stripGetterPrefix(f.getMethod().getName()), Function.identity()));    for (FieldValueTypeInformation type : getterTypes) {        if (typeMap.get(type.getName()) == null) {            return false;        }    }    return true;}
public static FixedBytesField beam_f15801_0(int size)
{    return new FixedBytesField(size);}
public static FixedBytesField beam_f15802_0(FieldType fieldType)
{    if (fieldType.getTypeName().isLogicalType() && fieldType.getLogicalType().getIdentifier().equals(FixedBytes.IDENTIFIER)) {        int length = fieldType.getLogicalType(FixedBytes.class).getLength();        return new FixedBytesField(length);    } else {        return null;    }}
public static FixedBytesField beam_f15803_0(org.apache.avro.Schema type)
{    if (type.getType().equals(Type.FIXED)) {        return new FixedBytesField(type.getFixedSize());    } else {        return null;    }}
public static org.apache.avro.Schema beam_f15811_0(Schema beamSchema)
{    return toAvroSchema(beamSchema, null, null);}
public static Row beam_f15812_0(GenericRecord record, @Nullable Schema schema)
{    if (schema == null) {        schema = toBeamSchema(record.getSchema());    }    Row.Builder builder = Row.withSchema(schema);    org.apache.avro.Schema avroSchema = record.getSchema();    for (Schema.Field field : schema.getFields()) {        Object value = record.get(field.getName());        org.apache.avro.Schema fieldAvroSchema = avroSchema.getField(field.getName()).schema();        builder.addValue(convertAvroFieldStrict(value, fieldAvroSchema, field.getType()));    }    return builder.build();}
public static GenericRecord beam_f15813_0(Row row, @Nullable org.apache.avro.Schema avroSchema)
{    Schema beamSchema = row.getSchema();        if (avroSchema != null && avroSchema.getFields().size() != beamSchema.getFieldCount()) {        throw new IllegalArgumentException("AVRO schema doesn't match row schema. Row schema " + beamSchema + ". AVRO schema + " + avroSchema);    }    if (avroSchema == null) {        avroSchema = toAvroSchema(beamSchema);    }    GenericRecordBuilder builder = new GenericRecordBuilder(avroSchema);    for (int i = 0; i < beamSchema.getFieldCount(); ++i) {        Schema.Field field = beamSchema.getField(i);        builder.set(field.getName(), genericFromBeamField(field.getType(), avroSchema.getField(field.getName()).schema(), row.getValue(i)));    }    return builder.build();}
public static SchemaCoder<GenericRecord> beam_f15821_0(org.apache.avro.Schema schema)
{    return schemaCoder(GenericRecord.class, schema);}
public static SchemaCoder<T> beam_f15822_0(Class<T> clazz, org.apache.avro.Schema schema)
{    return SchemaCoder.of(getSchema(clazz, schema), getToRowFunction(clazz, schema), getFromRowFunction(clazz));}
public static SchemaCoder<T> beam_f15823_0(AvroCoder<T> avroCoder)
{    return schemaCoder(avroCoder.getType(), avroCoder.getSchema());}
private static Schema.FieldType beam_f15831_0(TypeWithNullability type)
{    Schema.FieldType fieldType = null;    org.apache.avro.Schema avroSchema = type.type;    LogicalType logicalType = LogicalTypes.fromSchema(avroSchema);    if (logicalType != null) {        if (logicalType instanceof LogicalTypes.Decimal) {            fieldType = FieldType.DECIMAL;        } else if (logicalType instanceof LogicalTypes.TimestampMillis) {                                    fieldType = FieldType.DATETIME;        } else if (logicalType instanceof LogicalTypes.Date) {            fieldType = FieldType.DATETIME;        }    }    if (fieldType == null) {        switch(type.type.getType()) {            case RECORD:                fieldType = Schema.FieldType.row(toBeamSchema(avroSchema));                break;            case ENUM:                fieldType = Schema.FieldType.STRING;                break;            case ARRAY:                Schema.FieldType elementType = toFieldType(new TypeWithNullability(avroSchema.getElementType()));                fieldType = Schema.FieldType.array(elementType);                break;            case MAP:                fieldType = Schema.FieldType.map(Schema.FieldType.STRING, toFieldType(new TypeWithNullability(avroSchema.getValueType())));                break;            case FIXED:                fieldType = FixedBytesField.fromAvroType(type.type).toBeamType();                break;            case STRING:                fieldType = Schema.FieldType.STRING;                break;            case BYTES:                fieldType = Schema.FieldType.BYTES;                break;            case INT:                fieldType = Schema.FieldType.INT32;                break;            case LONG:                fieldType = Schema.FieldType.INT64;                break;            case FLOAT:                fieldType = Schema.FieldType.FLOAT;                break;            case DOUBLE:                fieldType = Schema.FieldType.DOUBLE;                break;            case BOOLEAN:                fieldType = Schema.FieldType.BOOLEAN;                break;            case UNION:                throw new RuntimeException("Can't convert 'union' to FieldType");            case NULL:                throw new RuntimeException("Can't convert 'null' to FieldType");            default:                throw new AssertionError("Unexpected AVRO Schema.Type: " + avroSchema.getType());        }    }    fieldType = fieldType.withNullable(type.nullable);    return fieldType;}
private static org.apache.avro.Schema beam_f15832_0(Schema.FieldType fieldType, String fieldName, String namespace)
{    org.apache.avro.Schema baseType;    switch(fieldType.getTypeName()) {        case BYTE:        case INT16:        case INT32:            baseType = org.apache.avro.Schema.create(Type.INT);            break;        case INT64:            baseType = org.apache.avro.Schema.create(Type.LONG);            break;        case DECIMAL:            baseType = LogicalTypes.decimal(Integer.MAX_VALUE).addToSchema(org.apache.avro.Schema.create(Type.BYTES));            break;        case FLOAT:            baseType = org.apache.avro.Schema.create(Type.FLOAT);            break;        case DOUBLE:            baseType = org.apache.avro.Schema.create(Type.DOUBLE);            break;        case STRING:            baseType = org.apache.avro.Schema.create(Type.STRING);            break;        case DATETIME:                                    baseType = LogicalTypes.timestampMillis().addToSchema(org.apache.avro.Schema.create(Type.LONG));            break;        case BOOLEAN:            baseType = org.apache.avro.Schema.create(Type.BOOLEAN);            break;        case BYTES:            baseType = org.apache.avro.Schema.create(Type.BYTES);            break;        case LOGICAL_TYPE:            FixedBytesField fixedBytesField = FixedBytesField.fromBeamFieldType(fieldType);            if (fixedBytesField != null) {                baseType = fixedBytesField.toAvroType("fixed", namespace + "." + fieldName);            } else {                throw new RuntimeException("Unhandled logical type " + fieldType.getLogicalType().getIdentifier());            }            break;        case ARRAY:            baseType = org.apache.avro.Schema.createArray(getFieldSchema(fieldType.getCollectionElementType(), fieldName, namespace));            break;        case MAP:            if (fieldType.getMapKeyType().getTypeName().isStringType()) {                                baseType = org.apache.avro.Schema.createMap(getFieldSchema(fieldType.getMapValueType(), fieldName, namespace));            } else {                throw new IllegalArgumentException("Avro only supports maps with string keys");            }            break;        case ROW:            baseType = toAvroSchema(fieldType.getRowSchema(), fieldName, namespace);            break;        default:            throw new IllegalArgumentException("Unexpected type " + fieldType);    }    return fieldType.getNullable() ? ReflectData.makeNullable(baseType) : baseType;}
private static Object beam_f15833_0(Schema.FieldType fieldType, org.apache.avro.Schema avroSchema, @Nullable Object value)
{    TypeWithNullability typeWithNullability = new TypeWithNullability(avroSchema);    if (!fieldType.getNullable().equals(typeWithNullability.nullable)) {        throw new IllegalArgumentException("FieldType " + fieldType + " and AVRO schema " + avroSchema + " don't have matching nullability");    }    if (value == null) {        return value;    }    switch(fieldType.getTypeName()) {        case BYTE:        case INT16:        case INT32:        case INT64:        case FLOAT:        case DOUBLE:        case BOOLEAN:            return value;        case STRING:            return new Utf8((String) value);        case DECIMAL:            BigDecimal decimal = (BigDecimal) value;            LogicalType logicalType = typeWithNullability.type.getLogicalType();            return new Conversions.DecimalConversion().toBytes(decimal, null, logicalType);        case DATETIME:            if (typeWithNullability.type.getType() == Type.INT) {                ReadableInstant instant = (ReadableInstant) value;                return (int) Days.daysBetween(Instant.EPOCH, instant).getDays();            } else if (typeWithNullability.type.getType() == Type.LONG) {                ReadableInstant instant = (ReadableInstant) value;                return (long) instant.getMillis();            } else {                throw new IllegalArgumentException("Can't represent " + fieldType + " as " + typeWithNullability.type.getType());            }        case BYTES:            return ByteBuffer.wrap((byte[]) value);        case LOGICAL_TYPE:            FixedBytesField fixedBytesField = FixedBytesField.fromBeamFieldType(fieldType);            if (fixedBytesField != null) {                byte[] byteArray = (byte[]) value;                if (byteArray.length != fixedBytesField.getSize()) {                    throw new IllegalArgumentException("Incorrectly sized byte array.");                }                return GenericData.get().createFixed(null, (byte[]) value, typeWithNullability.type);            }            throw new RuntimeException("Unknown logical type " + fieldType.getLogicalType().getIdentifier());        case ARRAY:            List array = (List) value;            List<Object> translatedArray = Lists.newArrayListWithExpectedSize(array.size());            for (Object arrayElement : array) {                translatedArray.add(genericFromBeamField(fieldType.getCollectionElementType(), typeWithNullability.type.getElementType(), arrayElement));            }            return translatedArray;        case MAP:            Map map = Maps.newHashMap();            Map<Object, Object> valueMap = (Map<Object, Object>) value;            for (Map.Entry entry : valueMap.entrySet()) {                Utf8 key = new Utf8((String) entry.getKey());                map.put(key, genericFromBeamField(fieldType.getMapValueType(), typeWithNullability.type.getValueType(), entry.getValue()));            }            return map;        case ROW:            return toGenericRecord((Row) value, typeWithNullability.type);        default:            throw new IllegalArgumentException("Unsupported type " + fieldType);    }}
private static Object beam_f15841_0(BigDecimal value, Schema.FieldType fieldType)
{    checkTypeName(fieldType.getTypeName(), TypeName.DECIMAL, "decimal");    return value;}
private static Object beam_f15842_0(Integer epochDays, Schema.FieldType fieldType)
{    checkTypeName(fieldType.getTypeName(), TypeName.DATETIME, "date");    return Instant.EPOCH.plus(Duration.standardDays(epochDays));}
private static Object beam_f15843_0(Long value, Schema.FieldType fieldType)
{    checkTypeName(fieldType.getTypeName(), TypeName.DATETIME, "dateTime");    return new Instant(value);}
protected String beam_f15851_0(TypeDescription superClass)
{    String baseName = baseNameResolver.resolve(superClass);    int lastDot = baseName.lastIndexOf('.');    String className = baseName.substring(lastDot, baseName.length());    return targetPackage + className + "$" + SUFFIX + "$" + randomString.nextString();}
 static DynamicType.Builder<FieldValueGetter> beam_f15852_0(ByteBuddy byteBuddy, Type objectType, Type fieldType)
{    TypeDescription.Generic getterGenericType = TypeDescription.Generic.Builder.parameterizedType(FieldValueGetter.class, objectType, fieldType).build();    return (DynamicType.Builder<FieldValueGetter>) byteBuddy.with(new InjectPackageStrategy((Class) objectType)).subclass(getterGenericType);}
 static DynamicType.Builder<FieldValueSetter> beam_f15853_0(ByteBuddy byteBuddy, Type objectType, Type fieldType)
{    TypeDescription.Generic setterGenericType = TypeDescription.Generic.Builder.parameterizedType(FieldValueSetter.class, objectType, fieldType).build();    return (DynamicType.Builder<FieldValueSetter>) byteBuddy.with(new InjectPackageStrategy((Class) objectType)).subclass(setterGenericType);}
protected Type beam_f15861_0(TypeDescriptor<?> type)
{    return String.class;}
protected Type beam_f15862_0(TypeDescriptor<?> type)
{    return ClassUtils.primitiveToWrapper(type.getRawType());}
protected Type beam_f15863_0(TypeDescriptor<?> type)
{    return String.class;}
protected StackManipulation beam_f15871_0(TypeDescriptor<?> type)
{    return new Compound(readValue, MethodInvocation.invoke(new ForLoadedType(GenericFixed.class).getDeclaredMethods().filter(ElementMatchers.named("bytes").and(ElementMatchers.returns(BYTE_ARRAY_TYPE))).getOnly()));}
protected StackManipulation beam_f15872_0(TypeDescriptor<?> type)
{        if (type.isSubtypeOf(TypeDescriptor.of(String.class))) {        return readValue;    }        return new Compound(readValue, MethodInvocation.invoke(CHAR_SEQUENCE_TYPE.getDeclaredMethods().filter(ElementMatchers.named("toString").and(ElementMatchers.takesArguments(0))).getOnly()));}
protected StackManipulation beam_f15873_0(TypeDescriptor<?> type)
{    ForLoadedType loadedType = new ForLoadedType(type.getRawType());        return new Compound(readValue, Assigner.DEFAULT.assign(loadedType.asGenericType(), loadedType.asBoxed().asGenericType(), Typing.STATIC));}
protected StackManipulation beam_f15881_0(TypeDescriptor<?> type)
{                ForLoadedType loadedType = new ForLoadedType(type.getRawType());    return new Compound(TypeCreation.of(loadedType), Duplication.SINGLE,     readValue, TypeCasting.to(BYTE_ARRAY_TYPE),     MethodInvocation.invoke(loadedType.getDeclaredMethods().filter(ElementMatchers.isConstructor().and(ElementMatchers.takesArguments(BYTE_ARRAY_TYPE))).getOnly()));}
protected StackManipulation beam_f15882_0(TypeDescriptor<?> type)
{        if (type.getRawType().isAssignableFrom(String.class)) {        return readValue;    }            ForLoadedType loadedType = new ForLoadedType(type.getRawType());    return new StackManipulation.Compound(TypeCreation.of(loadedType), Duplication.SINGLE,     readValue, TypeCasting.to(CHAR_SEQUENCE_TYPE),     MethodInvocation.invoke(loadedType.getDeclaredMethods().filter(ElementMatchers.isConstructor().and(ElementMatchers.takesArguments(CHAR_SEQUENCE_TYPE))).getOnly()));}
protected StackManipulation beam_f15883_0(TypeDescriptor<?> type)
{    ForLoadedType valueType = new ForLoadedType(type.getRawType());        return new StackManipulation.Compound(readValue, Assigner.DEFAULT.assign(valueType.asBoxed().asGenericType(), valueType.asUnboxed().asGenericType(), Typing.STATIC));}
public InstrumentedType beam_f15891_0(InstrumentedType instrumentedType)
{    return instrumentedType;}
public ByteCodeAppender beam_f15892_0(final Target implementationTarget)
{    return (methodVisitor, implementationContext, instrumentedMethod) -> {                int numLocals = 1 + instrumentedMethod.getParameters().size();        StackManipulation stackManipulation = beforePushingParameters();                ConvertType convertType = new ConvertType(true);        for (int i = 0; i < parameters.size(); i++) {            Parameter parameter = parameters.get(i);            ForLoadedType convertedType = new ForLoadedType((Class) convertType.convert(TypeDescriptor.of(parameter.getType())));                                    StackManipulation readParameter = new StackManipulation.Compound(MethodVariableAccess.REFERENCE.loadFrom(1), IntegerConstant.forValue(fieldMapping.get(i)), ArrayAccess.REFERENCE.load(), TypeCasting.to(convertedType));            stackManipulation = new StackManipulation.Compound(stackManipulation, new ConvertValueForSetter(readParameter).convert(TypeDescriptor.of(parameter.getType())));        }        stackManipulation = new StackManipulation.Compound(stackManipulation, afterPushingParameters(), MethodReturn.REFERENCE);        StackManipulation.Size size = stackManipulation.apply(methodVisitor, implementationContext);        return new Size(size.getMaximalSize(), numLocals);    };}
protected StackManipulation beam_f15893_0()
{    return new StackManipulation.Compound();}
public static void beam_f15901_0(List<FieldValueTypeInformation> getters, List<FieldValueTypeInformation> setters)
{    Map<String, FieldValueTypeInformation> setterMap = setters.stream().collect(Collectors.toMap(FieldValueTypeInformation::getName, Function.identity()));    for (FieldValueTypeInformation type : getters) {        FieldValueTypeInformation setterType = setterMap.get(type.getName());        if (setterType == null) {            throw new RuntimeException("JavaBean contained a getter for field " + type.getName() + "but did not contain a matching setter.");        }        if (!type.getType().equals(setterType.getType())) {            throw new RuntimeException("JavaBean contained setter for field " + type.getName() + " that had a mismatching type.");        }        if (!type.isNullable() == setterType.isNullable()) {            throw new RuntimeException("JavaBean contained setter for field " + type.getName() + " that had a mismatching nullable attribute.");        }    }}
public static List<FieldValueTypeInformation> beam_f15902_0(Class<?> clazz, Schema schema, FieldValueTypeSupplier fieldValueTypeSupplier)
{    return CACHED_FIELD_TYPES.computeIfAbsent(new ClassWithSchema(clazz, schema), c -> fieldValueTypeSupplier.get(clazz, schema));}
public static List<FieldValueGetter> beam_f15903_0(Class<?> clazz, Schema schema, FieldValueTypeSupplier fieldValueTypeSupplier)
{    return CACHED_GETTERS.computeIfAbsent(new ClassWithSchema(clazz, schema), c -> {        List<FieldValueTypeInformation> types = fieldValueTypeSupplier.get(clazz, schema);        return types.stream().map(JavaBeanUtils::createGetter).collect(Collectors.toList());    });}
public static SchemaUserTypeCreator beam_f15911_0(Class clazz, Method creator, Schema schema, FieldValueTypeSupplier fieldValueTypeSupplier)
{    return CACHED_CREATORS.computeIfAbsent(new ClassWithSchema(clazz, schema), c -> {        List<FieldValueTypeInformation> types = fieldValueTypeSupplier.get(clazz, schema);        return createStaticCreator(clazz, creator, schema, types);    });}
public static SchemaUserTypeCreator beam_f15912_0(Class<T> clazz, Method creator, Schema schema, List<FieldValueTypeInformation> types)
{    try {        DynamicType.Builder<SchemaUserTypeCreator> builder = BYTE_BUDDY.with(new InjectPackageStrategy(clazz)).subclass(SchemaUserTypeCreator.class).method(ElementMatchers.named("create")).intercept(new StaticFactoryMethodInstruction(types, clazz, creator));        return builder.make().load(ReflectHelpers.findClassLoader(clazz.getClassLoader()), ClassLoadingStrategy.Default.INJECTION).getLoaded().getDeclaredConstructor().newInstance();    } catch (InstantiationException | IllegalAccessException | NoSuchMethodException | InvocationTargetException e) {        throw new RuntimeException("Unable to generate a creator for " + clazz + " with schema " + schema);    }}
public InstrumentedType beam_f15913_0(InstrumentedType instrumentedType)
{    return instrumentedType;}
private static SchemaUserTypeCreator beam_f15921_0(Class<T> clazz, Schema schema, List<FieldValueTypeInformation> types)
{        List<Field> fields = types.stream().map(FieldValueTypeInformation::getField).collect(Collectors.toList());    try {        DynamicType.Builder<SchemaUserTypeCreator> builder = BYTE_BUDDY.with(new InjectPackageStrategy(clazz)).subclass(SchemaUserTypeCreator.class).method(ElementMatchers.named("create")).intercept(new SetFieldCreateInstruction(fields, clazz));        return builder.make().load(ReflectHelpers.findClassLoader(clazz.getClassLoader()), ClassLoadingStrategy.Default.INJECTION).getLoaded().getDeclaredConstructor().newInstance();    } catch (InstantiationException | IllegalAccessException | NoSuchMethodException | InvocationTargetException e) {        throw new RuntimeException("Unable to generate a creator for " + clazz + " with schema " + schema);    }}
public static SchemaUserTypeCreator beam_f15922_0(Class clazz, Constructor constructor, Schema schema, FieldValueTypeSupplier fieldValueTypeSupplier)
{    return CACHED_CREATORS.computeIfAbsent(new ClassWithSchema(clazz, schema), c -> {        List<FieldValueTypeInformation> types = fieldValueTypeSupplier.get(clazz, schema);        return createConstructorCreator(clazz, constructor, schema, types);    });}
public static SchemaUserTypeCreator beam_f15923_0(Class<T> clazz, Constructor<T> constructor, Schema schema, List<FieldValueTypeInformation> types)
{    try {        DynamicType.Builder<SchemaUserTypeCreator> builder = BYTE_BUDDY.with(new InjectPackageStrategy(clazz)).subclass(SchemaUserTypeCreator.class).method(ElementMatchers.named("create")).intercept(new ConstructorCreateInstruction(types, clazz, constructor));        return builder.make().load(ReflectHelpers.findClassLoader(clazz.getClassLoader()), ClassLoadingStrategy.Default.INJECTION).getLoaded().getDeclaredConstructor().newInstance();    } catch (InstantiationException | IllegalAccessException | NoSuchMethodException | InvocationTargetException e) {        throw new RuntimeException("Unable to generate a creator for " + clazz + " with schema " + schema);    }}
public InstrumentedType beam_f15931_0(InstrumentedType instrumentedType)
{    return instrumentedType;}
public ByteCodeAppender beam_f15932_0(final Target implementationTarget)
{    return (methodVisitor, implementationContext, instrumentedMethod) -> {                int numLocals = 1 + instrumentedMethod.getParameters().size();                StackManipulation readValue = new StackManipulation.Compound(        MethodVariableAccess.REFERENCE.loadFrom(1),         FieldAccess.forField(new ForLoadedField(field)).read());        StackManipulation stackManipulation = new StackManipulation.Compound(new ConvertValueForGetter(readValue).convert(TypeDescriptor.of(field.getType())), MethodReturn.REFERENCE);        StackManipulation.Size size = stackManipulation.apply(methodVisitor, implementationContext);        return new Size(size.getMaximalSize(), numLocals);    };}
public InstrumentedType beam_f15933_0(InstrumentedType instrumentedType)
{    return instrumentedType;}
public static Method beam_f15941_0(Class clazz)
{    return ANNOTATED_CONSTRUCTORS.computeIfAbsent(clazz, c -> {        Method method = Arrays.stream(clazz.getDeclaredMethods()).filter(m -> !Modifier.isPrivate(m.getModifiers())).filter(m -> !Modifier.isProtected(m.getModifiers())).filter(m -> Modifier.isStatic(m.getModifiers())).filter(m -> m.getAnnotation(SchemaCreate.class) != null).findFirst().orElse(null);        if (method != null && !clazz.isAssignableFrom(method.getReturnType())) {            throw new InvalidParameterException("A method marked with SchemaCreate in class " + clazz + " does not return a type assignable to " + clazz);        }        return method;    });}
public static List<Field> beam_f15942_0(Class<?> clazz)
{    return DECLARED_FIELDS.computeIfAbsent(clazz, c -> {        Map<String, Field> types = new LinkedHashMap<>();        do {            if (c.getPackage() != null && c.getPackage().getName().startsWith("java.")) {                                break;            }            for (java.lang.reflect.Field field : c.getDeclaredFields()) {                if ((field.getModifiers() & (Modifier.TRANSIENT | Modifier.STATIC)) == 0) {                    if ((field.getModifiers() & (Modifier.PRIVATE | Modifier.PROTECTED)) == 0) {                        checkArgument(types.put(field.getName(), field) == null, c.getSimpleName() + " contains two fields named: " + field);                    }                }            }            c = c.getSuperclass();        } while (c != null);        return Lists.newArrayList(types.values());    });}
public static boolean beam_f15943_0(Method method)
{    if (Void.TYPE.equals(method.getReturnType())) {        return false;    }    if (method.getName().startsWith("get") && method.getName().length() > 3) {        return true;    }    return (method.getName().startsWith("is") && method.getName().length() > 2 && method.getParameterCount() == 0 && (Boolean.TYPE.equals(method.getReturnType()) || Boolean.class.equals(method.getReturnType())));}
public static Context beam_f15951_0(List<String> path, Optional<TypeName> parent)
{    return new AutoValue_SchemaZipFold_Context(path, parent);}
 static T beam_f15952_0(SchemaZipFold<T> zipFold, Context context, FieldType left, FieldType right)
{    if (left.getTypeName() != right.getTypeName()) {        return zipFold.accept(context, left, right);    }    Context newContext = context.withParent(left.getTypeName());    switch(left.getTypeName()) {        case ARRAY:            return zipFold.accumulate(zipFold.accept(context, left, right), visit(zipFold, newContext, left.getCollectionElementType(), right.getCollectionElementType()));        case ROW:            return visitRow(zipFold, newContext, left.getRowSchema(), right.getRowSchema());        case MAP:            return zipFold.accumulate(zipFold.accept(context, left, right), visit(zipFold, newContext, left.getCollectionElementType(), right.getCollectionElementType()));        default:            return zipFold.accept(context, left, right);    }}
 static T beam_f15953_0(SchemaZipFold<T> zipFold, Context context, Schema left, Schema right)
{    T node = zipFold.accept(context, FieldType.row(left), FieldType.row(right));    Stream<String> union = Stream.concat(left.getFields().stream().map(Schema.Field::getName), right.getFields().stream().map(Schema.Field::getName)).distinct();    Stream<String> intersection = left.getFields().stream().map(Schema.Field::getName).filter(right::hasField);    T inner0 = intersection.map(name -> visit(zipFold, context.withPathPart(name).withParent(TypeName.ROW), left.getField(name).getType(), right.getField(name).getType())).reduce(node, zipFold::accumulate);    T inner1 = union.map(name -> {        Optional<Field> field0 = Optional.empty();        Optional<Field> field1 = Optional.empty();        if (left.hasField(name)) {            field0 = Optional.of(left.getField(name));        }        if (right.hasField(name)) {            field1 = Optional.of(right.getField(name));        }        Context newContext = context.withPathPart(name).withParent(TypeName.ROW);        return zipFold.accept(newContext, field0, field1);    }).reduce(node, zipFold::accumulate);    return zipFold.accumulate(zipFold.accumulate(node, inner0), inner1);}
public static List<FieldValueTypeInformation> beam_f15961_0(List<FieldValueTypeInformation> types, Schema schema)
{    Map<String, FieldValueTypeInformation> typeMap = types.stream().collect(Collectors.toMap(FieldValueTypeInformation::getName, Function.identity()));    return schema.getFields().stream().map(f -> typeMap.get(f.getName())).collect(Collectors.toList());}
public static Schema beam_f15962_0(Class<?> clazz, FieldValueTypeSupplier fieldValueTypeSupplier)
{    Schema.Builder builder = Schema.builder();    for (FieldValueTypeInformation type : fieldValueTypeSupplier.get(clazz)) {        Schema.FieldType fieldType = fieldFromType(type.getType(), fieldValueTypeSupplier);        if (type.isNullable()) {            builder.addNullableField(type.getName(), fieldType);        } else {            builder.addField(type.getName(), fieldType);        }    }    return builder.build();}
public static Schema.FieldType beam_f15963_0(TypeDescriptor type, FieldValueTypeSupplier fieldValueTypeSupplier)
{    FieldType primitiveType = PRIMITIVE_TYPES.get(type.getRawType());    if (primitiveType != null) {        return primitiveType;    }    if (type.isArray()) {                TypeDescriptor component = type.getComponentType();        if (component.getRawType().equals(byte.class)) {            return FieldType.BYTES;        } else {                        return FieldType.array(fieldFromType(component, fieldValueTypeSupplier));        }    } else if (type.isSubtypeOf(TypeDescriptor.of(Collection.class))) {        TypeDescriptor<Collection<?>> collection = type.getSupertype(Collection.class);        if (collection.getType() instanceof ParameterizedType) {            ParameterizedType ptype = (ParameterizedType) collection.getType();            java.lang.reflect.Type[] params = ptype.getActualTypeArguments();            checkArgument(params.length == 1);            return FieldType.array(fieldFromType(TypeDescriptor.of(params[0]), fieldValueTypeSupplier));        } else {            throw new RuntimeException("Cannot infer schema from unparameterized collection.");        }    } else if (type.isSubtypeOf(TypeDescriptor.of(Map.class))) {        TypeDescriptor<Collection<?>> map = type.getSupertype(Map.class);        if (map.getType() instanceof ParameterizedType) {            ParameterizedType ptype = (ParameterizedType) map.getType();            java.lang.reflect.Type[] params = ptype.getActualTypeArguments();            checkArgument(params.length == 2);            FieldType keyType = fieldFromType(TypeDescriptor.of(params[0]), fieldValueTypeSupplier);            FieldType valueType = fieldFromType(TypeDescriptor.of(params[1]), fieldValueTypeSupplier);            checkArgument(keyType.getTypeName().isPrimitiveType(), "Only primitive types can be map keys. type: " + keyType.getTypeName());            return FieldType.map(keyType, valueType);        } else {            throw new RuntimeException("Cannot infer schema from unparameterized map.");        }    } else if (type.isSubtypeOf(TypeDescriptor.of(CharSequence.class))) {        return FieldType.STRING;    } else if (type.isSubtypeOf(TypeDescriptor.of(ReadableInstant.class))) {        return FieldType.DATETIME;    } else if (type.isSubtypeOf(TypeDescriptor.of(ByteBuffer.class))) {        return FieldType.BYTES;    } else {        return FieldType.row(schemaFromClass(type.getRawType(), fieldValueTypeSupplier));    }}
public static StateContext<W> beam_f15971_0(W window)
{    return new WindowOnlyContext<>(window);}
public PipelineOptions beam_f15972_0()
{    throw new IllegalArgumentException("cannot call getPipelineOptions() in a window-only context");}
public T beam_f15973_0(PCollectionView<T> view)
{    throw new IllegalArgumentException("cannot call sideInput() in a window-only context");}
public static StateSpec<ValueState<T>> beam_f15981_0(Coder<T> valueCoder)
{    checkArgument(valueCoder != null, "valueCoder should not be null. Consider value() instead");    return new ValueStateSpec<>(valueCoder);}
public static StateSpec<CombiningState<InputT, AccumT, OutputT>> beam_f15982_0(CombineFn<InputT, AccumT, OutputT> combineFn)
{    return new CombiningStateSpec<>(null, combineFn);}
public static StateSpec<CombiningState<InputT, AccumT, OutputT>> beam_f15983_0(CombineFnWithContext<InputT, AccumT, OutputT> combineFn)
{    return new CombiningWithContextStateSpec<>(null, combineFn);}
public static StateSpec<MapState<K, V>> beam_f15991_0(Coder<K> keyCoder, Coder<V> valueCoder)
{    return new MapStateSpec<>(keyCoder, valueCoder);}
public static StateSpec<CombiningState<InputT, AccumT, OutputT>> beam_f15992_0(Coder<InputT> inputCoder, CombineFn<InputT, AccumT, OutputT> combineFn)
{    try {        Coder<AccumT> accumCoder = combineFn.getAccumulatorCoder(STANDARD_REGISTRY, inputCoder);        return combiningInternal(accumCoder, combineFn);    } catch (CannotProvideCoderException e) {        throw new IllegalArgumentException("Unable to determine accumulator coder for " + combineFn.getClass().getSimpleName() + " from " + inputCoder, e);    }}
private static StateSpec<CombiningState<InputT, AccumT, OutputT>> beam_f15993_0(Coder<AccumT> accumCoder, CombineFn<InputT, AccumT, OutputT> combineFn)
{    return new CombiningStateSpec<>(accumCoder, combineFn);}
public boolean beam_f16001_0(Object obj)
{    if (obj == this) {        return true;    }    if (!(obj instanceof ValueStateSpec)) {        return false;    }    ValueStateSpec<?> that = (ValueStateSpec<?>) obj;    return Objects.equals(this.coder, that.coder);}
public int beam_f16002_0()
{    return Objects.hash(getClass(), coder);}
public CombiningState<InputT, AccumT, OutputT> beam_f16003_0(String id, StateBinder visitor)
{    return visitor.bindCombining(id, this, accumCoder, combineFn);}
public ResultT beam_f16011_0(Cases<ResultT> cases)
{    throw new UnsupportedOperationException(String.format("%s is for internal use only and does not support case dispatch", getClass().getSimpleName()));}
public void beam_f16012_0(Coder[] coders)
{    if (this.accumCoder == null && coders[2] != null) {        this.accumCoder = (Coder<AccumT>) coders[2];    }}
public void beam_f16013_0()
{    if (accumCoder == null) {        throw new IllegalStateException("Unable to infer a coder for" + " CombiningWithContextState and no Coder was specified." + " Please set a coder by either invoking" + " StateSpecs.combiningWithcontext(Coder<AccumT> accumCoder," + " CombineFnWithContext<InputT, AccumT, OutputT> combineFn)" + " or by registering the coder in the Pipeline's CoderRegistry.");    }}
public boolean beam_f16021_0(Object obj)
{    if (obj == this) {        return true;    }    if (!(obj instanceof BagStateSpec)) {        return false;    }    BagStateSpec<?> that = (BagStateSpec<?>) obj;    return Objects.equals(this.elemCoder, that.elemCoder);}
public int beam_f16022_0()
{    return Objects.hash(getClass(), elemCoder);}
public MapState<K, V> beam_f16023_0(String id, StateBinder visitor)
{    return visitor.bindMap(id, this, keyCoder, valueCoder);}
public void beam_f16031_0(Coder[] coders)
{    if (this.elemCoder == null && coders[0] != null) {        this.elemCoder = (Coder<T>) coders[0];    }}
public void beam_f16032_0()
{    if (elemCoder == null) {        throw new IllegalStateException("Unable to infer a coder for SetState and no Coder" + " was specified. Please set a coder by either invoking" + " StateSpecs.set(Coder<T> elemCoder) or by registering the coder in the" + " Pipeline's CoderRegistry.");    }}
public boolean beam_f16033_0(Object obj)
{    if (obj == this) {        return true;    }    if (!(obj instanceof SetStateSpec)) {        return false;    }    SetStateSpec<?> that = (SetStateSpec<?>) obj;    return Objects.equals(this.elemCoder, that.elemCoder);}
 static Predicate<Annotation> beam_f16043_0(final Class<?> value, final boolean allowDerived)
{    return category -> FluentIterable.from(Arrays.asList(((Category) category).value())).anyMatch(aClass -> allowDerived ? value.isAssignableFrom(aClass) : value.equals(aClass));}
public static void beam_f16044_0(Coder<T> coder, T value1, T value2) throws Exception
{    for (Coder.Context context : ALL_CONTEXTS) {        coderDeterministicInContext(coder, context, value1, value2);    }}
public static void beam_f16045_0(Coder<T> coder, Coder.Context context, T value1, T value2) throws Exception
{    try {        coder.verifyDeterministic();    } catch (NonDeterministicException e) {        throw new AssertionError("Expected that the coder is deterministic", e);    }    assertThat("Expected that the passed in values are equal()", value1, equalTo(value2));    assertThat(encode(coder, context, value1), equalTo(encode(coder, context, value2)));}
public static void beam_f16053_0(Coder<T> coder, T value1, T value2) throws Exception
{    for (Coder.Context context : ALL_CONTEXTS) {        CoderProperties.coderConsistentWithEqualsInContext(coder, context, value1, value2);    }}
public static void beam_f16054_0(Coder<T> coder, Coder.Context context, T value1, T value2) throws Exception
{    assertEquals(value1.equals(value2), Arrays.equals(encode(coder, context, value1), encode(coder, context, value2)));}
public static void beam_f16055_0(Coder<T> coder, T value1, T value2) throws Exception
{    for (Coder.Context context : ALL_CONTEXTS) {        CoderProperties.structuralValueConsistentWithEqualsInContext(coder, context, value1, value2);    }}
public static void beam_f16063_0(Coder<IterableT> coder, String base64Encoding, IterableT expected) throws Exception
{    IterableT result = CoderUtils.decodeFromBase64(coder, base64Encoding);    if (Iterables.isEmpty(expected)) {        assertThat(ENCODING_WIRE_FORMAT_MESSAGE, result, emptyIterable());    } else {        assertThat(ENCODING_WIRE_FORMAT_MESSAGE, result, containsInAnyOrder((T[]) Iterables.toArray(expected, Object.class)));    }}
public static void beam_f16064_0(Coder<IterableT> coder, List<String> base64Encodings, List<IterableT> expected) throws Exception
{    assertThat("List of base64 encodings has different size than List of values", base64Encodings.size(), equalTo(expected.size()));    for (int i = 0; i < base64Encodings.size(); i++) {        coderDecodesBase64ContentsEqual(coder, base64Encodings.get(i), expected.get(i));    }}
 static byte[] beam_f16065_0(Coder<T> coder, Coder.Context context, T value) throws CoderException, IOException
{    @SuppressWarnings("unchecked")    Coder<T> deserializedCoder = SerializableUtils.clone(coder);    ByteArrayOutputStream os = new ByteArrayOutputStream();    deserializedCoder.encode(value, new UnownedOutputStream(os), context);    return os.toByteArray();}
public void beam_f16073_0()
{    currentSum = 0;    count = 0;}
public long beam_f16074_0()
{    long returnValue = currentSum;    reset();    return returnValue;}
public static void beam_f16075_0(CombineFn<InputT, AccumT, OutputT> fn, List<InputT> input, final OutputT expected)
{    testCombineFn(fn, input, is(expected));    Collections.shuffle(input);    testCombineFn(fn, input, is(expected));}
private static List<List<T>> beam_f16083_0(List<T> input, double base)
{    assert base > 1.0;    List<List<T>> shards = new ArrayList<>();    int end = input.size();    while (end > 0) {        int start = (int) (end / base);        shards.add(input.subList(start, end));        end = start;    }    return shards;}
public static CrashingRunner beam_f16084_0(PipelineOptions opts)
{    return new CrashingRunner();}
public PipelineResult beam_f16085_0(Pipeline pipeline)
{    throw new IllegalArgumentException(String.format("Cannot call #run(Pipeline) on an instance " + "of %s. %s should only be used as the default to construct a Pipeline " + "using %s, and cannot execute Pipelines. Instead, specify a %s " + "by providing PipelineOptions in the system property '%s'.", CrashingRunner.class.getSimpleName(), CrashingRunner.class.getSimpleName(), TestPipeline.class.getSimpleName(), PipelineRunner.class.getSimpleName(), TestPipeline.PROPERTY_BEAM_TEST_PIPELINE_OPTIONS));}
public SerializableMatcher<?> beam_f16093_0(JsonParser jsonParser, DeserializationContext deserializationContext) throws IOException, JsonProcessingException
{    ObjectNode node = jsonParser.readValueAsTree();    String matcher = node.get("matcher").asText();    byte[] in = BaseEncoding.base64().decode(matcher);    return (SerializableMatcher<?>) SerializableUtils.deserializeFromByteArray(in, "SerializableMatcher");}
public void beam_f16094_0(SerializableMatcher<?> matcher, JsonGenerator jsonGenerator, SerializerProvider serializerProvider) throws IOException, JsonProcessingException
{    byte[] out = SerializableUtils.serializeToByteArray(matcher);    String encodedString = BaseEncoding.base64().encode(out);    jsonGenerator.writeStartObject();    jsonGenerator.writeStringField("matcher", encodedString);    jsonGenerator.writeEndObject();}
 static SimpleFunction<Iterable<ValueInSingleWindow<T>>, Iterable<T>> beam_f16095_0(PAssert.PAssertionSite site)
{    return new ExtractOnlyPane<>(site);}
public Iterable<T> beam_f16103_0(Iterable<ValueInSingleWindow<T>> input)
{    List<T> outputs = new ArrayList<>();    for (ValueInSingleWindow<T> value : input) {        if (value.getPane().getTiming().equals(Timing.ON_TIME)) {            outputs.add(value.getValue());        }    }    return outputs;}
public Iterable<T> beam_f16104_0(Iterable<ValueInSingleWindow<T>> input)
{    List<T> outputs = new ArrayList<>();    for (ValueInSingleWindow<T> value : input) {        if (value.getPane().isLast()) {            outputs.add(value.getValue());        }    }    return outputs;}
public Iterable<T> beam_f16105_0(Iterable<ValueInSingleWindow<T>> input)
{    List<T> outputs = new ArrayList<>();    for (ValueInSingleWindow<T> value : input) {        outputs.add(value.getValue());    }    return outputs;}
public AssertionError beam_f16113_0(Throwable t)
{    AssertionError res = new AssertionError(message.isEmpty() ? t.getMessage() : (message + ": " + t.getMessage()), t);    res.setStackTrace(creationStackTrace);    return res;}
public AssertionError beam_f16114_0(String message)
{    String outputMessage = (this.message == null || this.message.isEmpty()) ? message : (this.message + ": " + message);    AssertionError res = new AssertionError(outputMessage);    res.setStackTrace(creationStackTrace);    return res;}
public boolean beam_f16115_0(Object o)
{    if (this == o) {        return true;    }    if (o == null || getClass() != o.getClass()) {        return false;    }    PAssertionSite that = (PAssertionSite) o;    return Objects.equal(message, that.message) && Arrays.equals(creationStackTrace, that.creationStackTrace);}
public static SingletonAssert<Map<K, Iterable<V>>> beam_f16123_0(PCollection<KV<K, V>> actual)
{    return thatMultimap(actual.getName(), actual);}
public static SingletonAssert<Map<K, Iterable<V>>> beam_f16124_0(String reason, PCollection<KV<K, V>> actual)
{    @SuppressWarnings("unchecked")    KvCoder<K, V> kvCoder = (KvCoder<K, V>) actual.getCoder();    return new PCollectionViewAssert<>(actual, View.asMultimap(), MapCoder.of(kvCoder.getKeyCoder(), IterableCoder.of(kvCoder.getValueCoder())), PAssertionSite.capture(reason));}
public static SingletonAssert<Map<K, V>> beam_f16125_0(PCollection<KV<K, V>> actual)
{    return thatMap(actual.getName(), actual);}
public IterableAssert<T> beam_f16133_0()
{    return withPane(GlobalWindow.INSTANCE, PaneExtractors.earlyPanes());}
private PCollectionContentsAssert<T> beam_f16134_0(BoundedWindow window, SimpleFunction<Iterable<ValueInSingleWindow<T>>, Iterable<T>> paneExtractor)
{    @SuppressWarnings({ "unchecked", "rawtypes" })    Coder<BoundedWindow> windowCoder = (Coder) actual.getWindowingStrategy().getWindowFn().windowCoder();    return new PCollectionContentsAssert<>(actual, IntoStaticWindows.of(windowCoder, window), paneExtractor, site);}
public final PCollectionContentsAssert<T> beam_f16135_0(T... expectedElements)
{    return containsInAnyOrder(Arrays.asList(expectedElements));}
public boolean beam_f16143_0(Object o)
{    throw new UnsupportedOperationException("If you meant to test object equality, use .containsInAnyOrder instead.");}
public int beam_f16144_0()
{    throw new UnsupportedOperationException(String.format("%s.hashCode() is not supported.", IterableAssert.class.getSimpleName()));}
public PCollectionSingletonIterableAssert<T> beam_f16145_0(BoundedWindow window)
{    return withPanes(window, PaneExtractors.allPanes());}
public final PCollectionSingletonIterableAssert<T> beam_f16153_0(T... expectedElements)
{    return containsInAnyOrder(Arrays.asList(expectedElements));}
public PCollectionSingletonIterableAssert<T> beam_f16154_0()
{    return containsInAnyOrder(Collections.emptyList());}
public PCollectionSingletonIterableAssert<T> beam_f16155_0(Iterable<T> expectedElements)
{    return satisfies(new AssertContainsInAnyOrderRelation<>(), expectedElements);}
public SingletonAssert<T> beam_f16163_0(T notExpected)
{    return satisfies(new AssertNotEqualToRelation<>(), notExpected);}
public PCollectionSingletonAssert<T> beam_f16164_0(BoundedWindow window)
{    return withPanes(window, PaneExtractors.onlyPane(site));}
private PCollectionSingletonAssert<T> beam_f16165_0(BoundedWindow window, SimpleFunction<Iterable<ValueInSingleWindow<T>>, Iterable<T>> paneExtractor)
{    @SuppressWarnings({ "unchecked", "rawtypes" })    Coder<BoundedWindow> windowCoder = (Coder) actual.getWindowingStrategy().getWindowFn().windowCoder();    return new PCollectionSingletonAssert<>(actual, IntoStaticWindows.of(windowCoder, window), paneExtractor, site);}
public PCollectionViewAssert<ElemT, ViewT> beam_f16173_0(BoundedWindow window)
{    return inPane(window, PaneExtractors.earlyPanes());}
public PCollectionViewAssert<ElemT, ViewT> beam_f16174_0(BoundedWindow window)
{    return inPane(window, PaneExtractors.latePanes());}
private PCollectionViewAssert<ElemT, ViewT> beam_f16175_0(BoundedWindow window, SimpleFunction<Iterable<ValueInSingleWindow<ElemT>>, Iterable<ElemT>> paneExtractor)
{    return new PCollectionViewAssert<>(actual, view, IntoStaticWindows.of((Coder) actual.getWindowingStrategy().getWindowFn().windowCoder(), window), paneExtractor, coder, site);}
public PCollectionView<ActualT> beam_f16183_0(PBegin input)
{    final Coder<T> coder = actual.getCoder();    return actual.apply("FilterActuals", rewindowActuals.prepareActuals()).apply("GatherPanes", GatherAllPanes.globally()).apply("ExtractPane", MapElements.via(extractPane)).setCoder(IterableCoder.of(actual.getCoder())).apply(Flatten.iterables()).apply("RewindowActuals", rewindowActuals.windowActuals()).apply(ParDo.of(new DoFn<T, T>() {        @ProcessElement        public void processElement(ProcessContext context) throws CoderException {            context.output(CoderUtils.clone(coder, context.element()));        }    })).apply(actualView);}
public void beam_f16184_0(ProcessContext context) throws CoderException
{    context.output(CoderUtils.clone(coder, context.element()));}
public Void beam_f16185_0(T actual)
{    try {        T expected = CoderUtils.decodeFromByteArray(coder, encodedExpected);        return relation.assertFor(expected).apply(actual);    } catch (IOException coderException) {        throw new RuntimeException(coderException);    }}
public void beam_f16193_0(ProcessContext c)
{    ActualT actualContents = Iterables.getOnlyElement(c.element());    c.output(doChecks(site, actualContents, checkerFn));}
protected static SuccessOrFailure beam_f16194_0(PAssertionSite site, ActualT actualContents, SerializableFunction<ActualT, Void> checkerFn)
{    try {        checkerFn.apply(actualContents);        return SuccessOrFailure.success();    } catch (Throwable t) {        return SuccessOrFailure.failure(site, t);    }}
public Void beam_f16195_0(T actual)
{    assertThat(actual, equalTo(expected));    return null;}
public PTransform<PCollection<T>, PCollection<T>> beam_f16203_0()
{    return window();}
public PTransform<PCollection<T>, PCollection<T>> beam_f16204_0()
{    return window();}
public PTransform<PCollection<T>, PCollection<T>> beam_f16205_0()
{    return window();}
public CompositeBehavior beam_f16213_0(Node node)
{    if (node.isRootNode()) {        checkState(!pipelineVisited, "Tried to visit a pipeline with an already used %s", AssertionCountingVisitor.class.getSimpleName());    }    if (!node.isRootNode() && (node.getTransform() instanceof PAssert.OneSideInputAssert || node.getTransform() instanceof PAssert.GroupThenAssert || node.getTransform() instanceof PAssert.GroupThenAssertForSingleton)) {        assertCount++;    }    return CompositeBehavior.ENTER_TRANSFORM;}
public void beam_f16214_0(Node node)
{    if (node.isRootNode()) {        pipelineVisited = true;    }}
public void beam_f16215_0(Node node)
{    if (node.getTransform() instanceof PAssert.OneSideInputAssert || node.getTransform() instanceof PAssert.GroupThenAssert || node.getTransform() instanceof PAssert.GroupThenAssertForSingleton) {        assertCount++;    }}
public static SerializableMatcher<T> beam_f16223_0(final SerializableMatcher<T>... matchers)
{    return fromSupplier(() -> Matchers.anyOf(matchers));}
public static SerializableMatcher<Object> beam_f16224_0()
{    return fromSupplier(Matchers::anything);}
public static SerializableMatcher<T[]> beam_f16225_0(final T... items)
{    return fromSupplier(() -> Matchers.arrayContaining(items));}
public static SerializableMatcher<T[]> beam_f16233_0(final int size)
{    return fromSupplier(() -> Matchers.arrayWithSize(size));}
public static SerializableMatcher<T[]> beam_f16234_0(final SerializableMatcher<? super Integer> sizeMatcher)
{    return fromSupplier(() -> Matchers.arrayWithSize(sizeMatcher));}
public static SerializableMatcher<Double> beam_f16235_0(final double target, final double error)
{    return fromSupplier(() -> Matchers.closeTo(target, error));}
public static SerializableMatcher<Iterable<? extends T>> beam_f16243_0(Collection<SerializableMatcher<? super T>> serializableMatchers)
{        @SuppressWarnings({ "rawtypes", "unchecked" })    final Collection<Matcher<? super T>> matchers = (Collection) serializableMatchers;    return fromSupplier(() -> Matchers.containsInAnyOrder(matchers));}
public static SerializableMatcher<String> beam_f16244_0(final String substring)
{    return fromSupplier(() -> Matchers.containsString(substring));}
public static SerializableMatcher<Collection<? extends T>> beam_f16245_0()
{    return fromSupplier(Matchers::empty);}
public static SerializableMatcher<T> beam_f16253_0(final T target)
{    return fromSupplier(() -> Matchers.greaterThanOrEqualTo(target));}
public static SerializableMatcher<T> beam_f16254_0(final Coder<T> coder, T target)
{    final SerializableSupplier<T> targetSupplier = new SerializableViaCoder<>(coder, target);    return fromSupplier(() -> Matchers.greaterThanOrEqualTo(targetSupplier.get()));}
public static SerializableMatcher<Iterable<? super T>> beam_f16255_0(final T target)
{    return fromSupplier(() -> Matchers.hasItem(target));}
public static SerializableMatcher<T> beam_f16263_0(Coder<T> coder, Collection<T> collection)
{    @SuppressWarnings("unchecked")    T[] items = (T[]) collection.toArray();    final SerializableSupplier<T[]> itemsSupplier = new SerializableArrayViaCoder<>(coder, items);    return fromSupplier(() -> is(in(itemsSupplier.get())));}
public static SerializableMatcher<T> beam_f16264_0(final T[] items)
{    return fromSupplier(() -> is(in(items)));}
public static SerializableMatcher<T> beam_f16265_0(Coder<T> coder, T[] items)
{    final SerializableSupplier<T[]> itemsSupplier = new SerializableArrayViaCoder<>(coder, items);    return fromSupplier(() -> is(in(itemsSupplier.get())));}
public static SerializableMatcher<KV<? extends K, ? extends V>> beam_f16273_0(final SerializableMatcher<? super V> valueMatcher)
{    return new KvValueMatcher<>(valueMatcher);}
public static SerializableMatcher<KV<? extends K, ? extends V>> beam_f16274_0(final SerializableMatcher<? super K> keyMatcher, final SerializableMatcher<? super V> valueMatcher)
{    return SerializableMatchers.allOf(SerializableMatchers.kvWithKey(keyMatcher), SerializableMatchers.kvWithValue(valueMatcher));}
public static SerializableMatcher<T> beam_f16275_0(final T target)
{    return fromSupplier(() -> Matchers.lessThan(target));}
public void beam_f16283_0(Object item, Description mismatchDescription)
{    @SuppressWarnings("unchecked")    KV<K, ?> kvItem = (KV<K, ?>) item;    if (!keyMatcher.matches(kvItem.getKey())) {        mismatchDescription.appendText("key did not match: ");        keyMatcher.describeMismatch(kvItem.getKey(), mismatchDescription);    }}
public void beam_f16284_0(Description description)
{    description.appendText("KV with key matching ");    keyMatcher.describeTo(description);}
public String beam_f16285_0()
{    return MoreObjects.toStringHelper(this).addValue(keyMatcher).toString();}
public void beam_f16293_0(Object item, Description mismatchDescription)
{    supplier.get().describeMismatch(item, mismatchDescription);}
public T beam_f16294_0()
{    if (value == null) {        try {            value = CoderUtils.decodeFromByteArray(coder, encodedValue);        } catch (CoderException exc) {            throw new RuntimeException("Error deserializing via Coder", exc);        }    }    return value;}
public T[] beam_f16295_0()
{    if (value == null) {        try {            @SuppressWarnings("unchecked")            T[] decoded = (T[]) CoderUtils.decodeFromByteArray(coder, encodedValue).toArray();            value = decoded;        } catch (CoderException exc) {            throw new RuntimeException("Error deserializing via Coder", exc);        }    }    return value;}
public static List<T> beam_f16303_0(Source.Reader<T> reader) throws IOException
{    return readRemainingFromReader(reader, true);}
public static List<T> beam_f16304_0(Source.Reader<T> reader, int n) throws IOException
{    return readNItemsFromReader(reader, n, false);}
public static List<T> beam_f16305_0(Source.Reader<T> reader, int n) throws IOException
{    return readNItemsFromReader(reader, n, true);}
private static SourceTestUtils.SplitAtFractionResult beam_f16313_0(BoundedSource<T> source, List<T> expectedItems, List<T> currentItems, BoundedSource<T> primary, BoundedSource<T> residual, int numItemsToReadBeforeSplit, double splitFraction, PipelineOptions options) throws Exception
{    List<T> primaryItems = readFromSource(primary, options);    if (residual != null) {        List<T> residualItems = readFromSource(residual, options);        List<T> totalItems = new ArrayList<>();        totalItems.addAll(primaryItems);        totalItems.addAll(residualItems);        String errorMsgForPrimarySourceComp = String.format("Continued reading after split yielded different items than primary source: " + "split at %s after reading %s items, original source: %s, primary source: %s", splitFraction, numItemsToReadBeforeSplit, source, primary);        String errorMsgForTotalSourceComp = String.format("Items in primary and residual sources after split do not add up to items " + "in the original source. Split at %s after reading %s items; " + "original source: %s, primary: %s, residual: %s", splitFraction, numItemsToReadBeforeSplit, source, primary, residual);        Coder<T> coder = primary.getOutputCoder();        List<ReadableStructuralValue<T>> primaryValues = createStructuralValues(coder, primaryItems);        List<ReadableStructuralValue<T>> currentValues = createStructuralValues(coder, currentItems);        List<ReadableStructuralValue<T>> expectedValues = createStructuralValues(coder, expectedItems);        List<ReadableStructuralValue<T>> totalValues = createStructuralValues(coder, totalItems);        assertListsEqualInOrder(errorMsgForPrimarySourceComp, "current", currentValues, "primary", primaryValues);        assertListsEqualInOrder(errorMsgForTotalSourceComp, "total", expectedValues, "primary+residual", totalValues);        return new SplitAtFractionResult(primaryItems.size(), residualItems.size());    }    return new SplitAtFractionResult(primaryItems.size(), -1);}
public static void beam_f16314_0(BoundedSource<T> source, int numItemsToReadBeforeSplit, double splitFraction, PipelineOptions options) throws Exception
{    assertSplitAtFractionBehavior(source, numItemsToReadBeforeSplit, splitFraction, ExpectedSplitOutcome.MUST_SUCCEED_AND_BE_CONSISTENT, options);}
public static void beam_f16315_0(BoundedSource<T> source, int numItemsToReadBeforeSplit, double splitFraction, PipelineOptions options) throws Exception
{    assertSplitAtFractionBehavior(source, numItemsToReadBeforeSplit, splitFraction, ExpectedSplitOutcome.MUST_FAIL, options);}
public BoundedReader<T> beam_f16323_0(PipelineOptions options) throws IOException
{    return new UnsplittableReader<>(boundedSource, boundedSource.createReader(options));}
public void beam_f16324_0()
{    boundedSource.validate();}
public Coder<T> beam_f16325_0()
{    return boundedSource.getOutputCoder();}
public long beam_f16333_0()
{    return boundedReader.getSplitPointsConsumed();}
public long beam_f16334_0()
{    return boundedReader.getSplitPointsRemaining();}
public Instant beam_f16335_0() throws NoSuchElementException
{    return boundedReader.getCurrentTimestamp();}
public Coder<BoundedWindow> beam_f16343_0()
{    return coder;}
public WindowMappingFn<BoundedWindow> beam_f16344_0()
{    return new WindowMappingFn<BoundedWindow>(Duration.millis(Long.MAX_VALUE)) {        @Override        public BoundedWindow getSideInputWindow(BoundedWindow mainWindow) {            checkArgument(windows.get().contains(mainWindow), "%s only supports side input windows for main input windows that it contains", StaticWindows.class.getSimpleName());            return mainWindow;        }    };}
public BoundedWindow beam_f16345_0(BoundedWindow mainWindow)
{    checkArgument(windows.get().contains(mainWindow), "%s only supports side input windows for main input windows that it contains", StaticWindows.class.getSimpleName());    return mainWindow;}
public int beam_f16353_0()
{    return Objects.hashCode(isSuccess, site, throwable);}
protected void beam_f16354_0(final boolean enable)
{    enableAutoRunIfMissing = enable;}
protected void beam_f16355_0()
{    runAttempted = true;}
private boolean beam_f16364_0()
{    return runVisitedNodes != null;}
protected void beam_f16365_0()
{    runVisitedNodes = recordPipelineNodes(pipeline);    super.afterPipelineExecution();}
protected void beam_f16366_0()
{    super.afterUserCodeFinished();    verifyPipelineExecution();}
public PipelineResult beam_f16374_0(PipelineOptions options)
{    checkState(enforcement.isPresent(), "Is your TestPipeline declaration missing a @Rule annotation? Usage: " + "@Rule public final transient TestPipeline pipeline = TestPipeline.create();");    final PipelineResult pipelineResult;    try {        enforcement.get().beforePipelineExecution();        PipelineOptions updatedOptions = MAPPER.convertValue(MAPPER.valueToTree(options), PipelineOptions.class);        updatedOptions.as(TestValueProviderOptions.class).setProviderRuntimeValues(StaticValueProvider.of(providerRuntimeValues));        pipelineResult = super.run(updatedOptions);        verifyPAssertsSucceeded(this, pipelineResult);    } catch (RuntimeException exc) {        Throwable cause = exc.getCause();        if (cause instanceof AssertionError) {            throw (AssertionError) cause;        } else {            throw exc;        }    }            enforcement.get().afterPipelineExecution();    return pipelineResult;}
public ValueProvider<T> beam_f16375_0(T runtimeValue)
{    String uuid = UUID.randomUUID().toString();    providerRuntimeValues.put(uuid, runtimeValue);    return ValueProvider.NestedValueProvider.of(options.as(TestValueProviderOptions.class).getProviderRuntimeValues(), new GetFromRuntimeValues<T>(uuid));}
public T beam_f16376_0(Map<String, Object> input)
{    return (T) input.get(key);}
public void beam_f16384_0(TransformHierarchy.Node node)
{    empty = false;}
public SerializableMatcher<PipelineResult> beam_f16385_0(PipelineOptions options)
{    return new AlwaysPassMatcher();}
public boolean beam_f16386_0(Object o)
{    return true;}
public TestStream<T> beam_f16395_0()
{    ImmutableList<Event<T>> newEvents = ImmutableList.<Event<T>>builder().addAll(events).add(WatermarkEvent.advanceTo(BoundedWindow.TIMESTAMP_MAX_VALUE)).build();    return new TestStream<>(coder, newEvents);}
 static Event<T> beam_f16396_0(TimestampedValue<T> element, TimestampedValue<T>... elements)
{    return add(ImmutableList.<TimestampedValue<T>>builder().add(element).add(elements).build());}
public static Event<T> beam_f16397_0(Iterable<TimestampedValue<T>> elements)
{    return new AutoValue_TestStream_ElementEvent<>(EventType.ELEMENT, elements);}
public int beam_f16405_0()
{    return Objects.hash(TestStream.class, getValueCoder(), getEvents());}
public static TestStreamCoder<T> beam_f16406_0(Coder<T> valueCoder)
{    return new TestStreamCoder<>(valueCoder);}
public void beam_f16407_0(TestStream<T> value, OutputStream outStream) throws IOException
{    List<Event<T>> events = value.getEvents();    VarIntCoder.of().encode(events.size(), outStream);    for (Event event : events) {        if (event instanceof ElementEvent) {            outStream.write(event.getType().ordinal());            Iterable<TimestampedValue<T>> elements = ((ElementEvent) event).getElements();            VarIntCoder.of().encode(Iterables.size(elements), outStream);            for (TimestampedValue<T> element : elements) {                elementCoder.encode(element, outStream);            }        } else if (event instanceof WatermarkEvent) {            outStream.write(event.getType().ordinal());            Instant watermark = ((WatermarkEvent) event).getWatermark();            InstantCoder.of().encode(watermark, outStream);        } else if (event instanceof ProcessingTimeEvent) {            outStream.write(event.getType().ordinal());            Duration processingTimeAdvance = ((ProcessingTimeEvent) event).getProcessingTimeAdvance();            DurationCoder.of().encode(processingTimeAdvance, outStream);        }    }}
private static String beam_f16416_0(long timestamp)
{    return "T" + new Instant(timestamp);}
public T beam_f16417_0()
{    return timestampedValue.getValue();}
public Instant beam_f16418_0()
{    return timestampedValue.getTimestamp();}
public static void beam_f16426_0(WindowFn<T, W> windowFn, long timestamp) throws Exception
{    validateNonInterferingOutputTimesWithValue(windowFn, TimestampedValue.of((T) null, new Instant(timestamp)));}
public static void beam_f16427_0(WindowFn<T, W> windowFn, TimestampedValue<T> timestampedValue) throws Exception
{    Collection<W> windows = assignedWindowsWithValue(windowFn, timestampedValue);    Instant instant = timestampedValue.getTimestamp();    for (W window : windows) {        Instant outputTimestamp = windowFn.getOutputTime(instant, window);        assertFalse("getOutputTime must be greater than or equal to input timestamp", outputTimestamp.isBefore(instant));        assertFalse("getOutputTime must be less than or equal to the max timestamp", outputTimestamp.isAfter(window.maxTimestamp()));    }}
public static void beam_f16428_0(WindowFn<T, W> windowFn, long timestamp) throws Exception
{    validateGetOutputTimestampWithValue(windowFn, TimestampedValue.of((T) null, new Instant(timestamp)));}
public static WindowSupplier beam_f16436_0(Coder<W> coder, Iterable<W> windows)
{    ImmutableSet.Builder<byte[]> windowsBuilder = ImmutableSet.builder();    for (W window : windows) {        try {            windowsBuilder.add(CoderUtils.encodeToByteArray(coder, window));        } catch (CoderException e) {            throw new IllegalArgumentException("Could not encode provided windows with the provided window coder", e);        }    }    return new WindowSupplier(coder, windowsBuilder.build());}
public Collection<BoundedWindow> beam_f16437_0()
{    if (windows == null) {        decodeWindows();    }    return windows;}
private synchronized void beam_f16438_0()
{    if (windows == null) {        ImmutableList.Builder<BoundedWindow> windowsBuilder = ImmutableList.builder();        for (byte[] encoded : encodedWindows) {            try {                windowsBuilder.add(CoderUtils.decodeFromByteArray(coder, encoded));            } catch (CoderException e) {                throw new IllegalArgumentException("Could not decode provided windows with the provided window coder", e);            }        }        this.windows = windowsBuilder.build();    }}
public ApproximateQuantilesCombineFn<T, ComparatorT> beam_f16446_0(long maxNumElements)
{    return create(numQuantiles, compareFn, maxNumElements, maxNumElements);}
public static ApproximateQuantilesCombineFn<T, ComparatorT> beam_f16447_0(int numQuantiles, ComparatorT compareFn, long maxNumElements, double epsilon)
{        int b = 2;    while ((b - 2) * (1 << (b - 2)) < epsilon * maxNumElements) {        b++;    }    b--;    int k = Math.max(2, (int) Math.ceil(maxNumElements / (float) (1 << (b - 1))));    return new ApproximateQuantilesCombineFn<>(numQuantiles, compareFn, k, b, maxNumElements);}
public QuantileState<T, ComparatorT> beam_f16448_0()
{    return QuantileState.empty(compareFn, numQuantiles, numBuffers, bufferSize);}
private void beam_f16456_0(T elem)
{    unbufferedElements.add(elem);    if (unbufferedElements.size() == bufferSize) {        unbufferedElements.sort(compareFn);        buffers.add(new QuantileBuffer<>(unbufferedElements));        unbufferedElements = Lists.newArrayListWithCapacity(bufferSize);        collapseIfNeeded();    }}
public void beam_f16457_0(QuantileState<T, ComparatorT> other)
{    if (other.isEmpty()) {        return;    }    if (min == null || compareFn.compare(other.min, min) < 0) {        min = other.min;    }    if (max == null || compareFn.compare(other.max, max) > 0) {        max = other.max;    }    for (T elem : other.unbufferedElements) {        addUnbuffered(elem);    }    buffers.addAll(other.buffers);    collapseIfNeeded();}
public boolean beam_f16458_0()
{    return unbufferedElements.isEmpty() && buffers.isEmpty();}
public boolean beam_f16466_0()
{    return iter.hasNext();}
public WeightedValue<T> beam_f16467_0()
{    return WeightedValue.of(iter.next(), weight);}
public void beam_f16468_0(QuantileState<T, ComparatorT> state, OutputStream outStream) throws CoderException, IOException
{    intCoder.encode(state.numQuantiles, outStream);    intCoder.encode(state.bufferSize, outStream);    elementCoder.encode(state.min, outStream);    elementCoder.encode(state.max, outStream);    elementListCoder.encode(state.unbufferedElements, outStream);    BigEndianIntegerCoder.of().encode(state.buffers.size(), outStream);    for (QuantileBuffer<T> buffer : state.buffers) {        encodeBuffer(buffer, outStream);    }}
public static Globally<T> beam_f16476_0(int sampleSize)
{    return new Globally<>(sampleSize);}
public static Globally<T> beam_f16477_0(double maximumEstimationError)
{    return new Globally<>(maximumEstimationError);}
public static PerKey<K, V> beam_f16478_0(int sampleSize)
{    return new PerKey<>(sampleSize);}
public boolean beam_f16486_0(Object o)
{    if (this == o) {        return true;    }    if (o == null || getClass() != o.getClass()) {        return false;    }    LargestUnique that = (LargestUnique) o;    return sampleSize == that.sampleSize && Iterables.elementsEqual(heap, that.heap);}
public int beam_f16487_0()
{    return Objects.hashCode(Lists.newArrayList(heap), sampleSize);}
public LargestUnique beam_f16488_0()
{    return new LargestUnique(sampleSize);}
public static Globally<V, V> beam_f16496_0(SerializableFunction<Iterable<V>, V> combiner)
{    return globally(IterableCombineFn.of(combiner), displayDataForFn(combiner));}
public static Globally<V, V> beam_f16497_0(SerializableBiFunction<V, V, V> combiner)
{    return globally(BinaryCombineFn.of(combiner), displayDataForFn(combiner));}
public static Globally<InputT, OutputT> beam_f16498_0(GlobalCombineFn<? super InputT, ?, OutputT> fn)
{    return globally(fn, displayDataForFn(fn));}
public static GroupedValues<K, V, V> beam_f16506_0(SerializableFunction<Iterable<V>, V> fn)
{    return groupedValues(IterableCombineFn.of(fn), displayDataForFn(fn));}
public static GroupedValues<K, V, V> beam_f16507_0(SerializableBiFunction<V, V, V> fn)
{    return groupedValues(BinaryCombineFn.of(fn), displayDataForFn(fn));}
public static GroupedValues<K, InputT, OutputT> beam_f16508_0(GlobalCombineFn<? super InputT, ?, OutputT> fn)
{    return groupedValues(fn, displayDataForFn(fn));}
public V beam_f16516_0(V left, V right)
{    return combiner.apply(left, right);}
public V beam_f16517_0()
{    return null;}
public Holder<V> beam_f16518_0()
{    return new Holder<>();}
public void beam_f16526_0(Holder<V> accumulator, OutputStream outStream) throws CoderException, IOException
{    encode(accumulator, outStream, Context.NESTED);}
public void beam_f16527_0(Holder<V> accumulator, OutputStream outStream, Context context) throws CoderException, IOException
{    if (accumulator.present) {        outStream.write(1);        valueCoder.encode(accumulator.value, outStream, context);    } else {        outStream.write(0);    }}
public Holder<V> beam_f16528_0(InputStream inStream) throws CoderException, IOException
{    return decode(inStream, Context.NESTED);}
public Coder<int[]> beam_f16536_0(CoderRegistry registry, Coder<Integer> inputCoder)
{    return DelegateCoder.of(inputCoder, new ToIntegerCodingFunction(), new FromIntegerCodingFunction());}
public Coder<Integer> beam_f16537_0(CoderRegistry registry, Coder<Integer> inputCoder)
{    return inputCoder;}
private static int[] beam_f16538_0(int value)
{    return new int[] { value };}
public long[] beam_f16546_0(long[] accumulator, Long input)
{    accumulator[0] = apply(accumulator[0], input);    return accumulator;}
public long[] beam_f16547_0(Iterable<long[]> accumulators)
{    Iterator<long[]> iter = accumulators.iterator();    if (!iter.hasNext()) {        return createAccumulator();    } else {        long[] running = iter.next();        while (iter.hasNext()) {            running[0] = apply(running[0], iter.next()[0]);        }        return running;    }}
public Long beam_f16548_0(long[] accumulator)
{    return accumulator[0];}
public boolean beam_f16556_0(Object o)
{    return o instanceof FromLongCodingFunction;}
public int beam_f16557_0()
{    return this.getClass().hashCode();}
public double[] beam_f16558_0()
{    return wrap(identity());}
public boolean beam_f16566_0(Object o)
{    return o instanceof ToDoubleCodingFunction;}
public int beam_f16567_0()
{    return this.getClass().hashCode();}
public double[] beam_f16568_0(Double value)
{    return wrap(value);}
public Globally<InputT, OutputT> beam_f16576_0()
{    return new Globally<>(fn, fnDisplayData, false, fanout, sideInputs);}
public Globally<InputT, OutputT> beam_f16577_0(int fanout)
{    return new Globally<>(fn, fnDisplayData, insertDefault, fanout, sideInputs);}
public Globally<InputT, OutputT> beam_f16578_0(PCollectionView<?>... sideInputs)
{    return withSideInputs(Arrays.asList(sideInputs));}
private PCollection<OutputT> beam_f16586_0(PCollection<OutputT> maybeEmpty)
{    final PCollectionView<Iterable<OutputT>> maybeEmptyView = maybeEmpty.apply(View.asIterable());    final OutputT defaultValue = fn.defaultValue();    PCollection<OutputT> defaultIfEmpty = maybeEmpty.getPipeline().apply("CreateVoid", Create.of((Void) null).withCoder(VoidCoder.of())).apply("ProduceDefault", ParDo.of(new DoFn<Void, OutputT>() {        @ProcessElement        public void processElement(ProcessContext c) {            Iterator<OutputT> combined = c.sideInput(maybeEmptyView).iterator();            if (!combined.hasNext()) {                c.output(defaultValue);            }        }    }).withSideInputs(maybeEmptyView)).setCoder(maybeEmpty.getCoder()).setWindowingStrategyInternal(maybeEmpty.getWindowingStrategy());    return PCollectionList.of(maybeEmpty).and(defaultIfEmpty).apply(Flatten.pCollections());}
public void beam_f16587_0(ProcessContext c)
{    Iterator<OutputT> combined = c.sideInput(maybeEmptyView).iterator();    if (!combined.hasNext()) {        c.output(defaultValue);    }}
private static void beam_f16588_0(DisplayData.Builder builder, HasDisplayData fn, DisplayData.ItemSpec<? extends Class<?>> fnDisplayItem)
{    builder.include("combineFn", fn).add(fnDisplayItem);}
public static IterableCombineFn<V> beam_f16596_0(SerializableFunction<Iterable<V>, V> combiner, int bufferSize)
{    return new IterableCombineFn<>(combiner, bufferSize);}
public List<V> beam_f16597_0()
{    return new ArrayList<>();}
public List<V> beam_f16598_0(List<V> accumulator, V input)
{    accumulator.add(input);    if (accumulator.size() > bufferSize) {        return mergeToSingleton(accumulator);    } else {        return accumulator;    }}
protected String beam_f16606_0()
{    return String.format("Combine.perKey(%s)", NameUtils.approximateSimpleName(fn));}
public PerKey<K, InputT, OutputT> beam_f16607_0(PCollectionView<?>... sideInputs)
{    return withSideInputs(Arrays.asList(sideInputs));}
public PerKey<K, InputT, OutputT> beam_f16608_0(Iterable<? extends PCollectionView<?>> sideInputs)
{    checkState(fn instanceof RequiresContextInternal);    return new PerKey<>(fn, fnDisplayData, fewKeys, ImmutableList.copyOf(sideInputs));}
public PCollection<KV<K, OutputT>> beam_f16616_0(PCollection<KV<K, InputT>> input)
{    return input.apply(fewKeys ? GroupByKey.createWithFewKeys() : GroupByKey.create()).apply(Combine.<K, InputT, OutputT>groupedValues(fn, fnDisplayData).withSideInputs(sideInputs));}
public void beam_f16617_0(DisplayData.Builder builder)
{    super.populateDisplayData(builder);    Combine.populateDisplayData(builder, fn, fnDisplayData);}
protected String beam_f16618_0()
{    return String.format("Combine.perKeyWithFanout(%s)", NameUtils.approximateSimpleName(fn));}
public Coder<AccumT> beam_f16626_0(CoderRegistry registry, Coder<InputT> inputCoder) throws CannotProvideCoderException
{    return accumCoder;}
public void beam_f16627_0(DisplayData.Builder builder)
{    builder.delegate(PerKeyWithHotKeyFanout.this);}
public AccumT beam_f16628_0()
{    return fn.createAccumulator();}
public AccumT beam_f16636_0(Context c)
{    return fnWithContext.createAccumulator(c);}
public AccumT beam_f16637_0(AccumT accumulator, InputT value, Context c)
{    return fnWithContext.addInput(accumulator, value, c);}
public AccumT beam_f16638_0(Iterable<AccumT> accumulators, Context c)
{    return fnWithContext.mergeAccumulators(accumulators, c);}
public AccumT beam_f16646_0(AccumT accumulator, Context c)
{    return fnWithContext.compact(accumulator, c);}
public OutputT beam_f16647_0(AccumT accumulator, Context c)
{    return fnWithContext.extractOutput(accumulator, c);}
public Coder<OutputT> beam_f16648_0(CoderRegistry registry, Coder<InputOrAccum<InputT, AccumT>> accumulatorCoder) throws CannotProvideCoderException
{    return fnWithContext.getDefaultOutputCoder(registry, inputCoder.getValueCoder());}
public static InputOrAccum<InputT, AccumT> beam_f16656_0(InputT input)
{    return new InputOrAccum<>(input, null);}
public static InputOrAccum<InputT, AccumT> beam_f16657_0(AccumT aggr)
{    return new InputOrAccum<>(null, aggr);}
public void beam_f16658_0(InputOrAccum<InputT, AccumT> value, OutputStream outStream) throws CoderException, IOException
{    encode(value, outStream, Coder.Context.NESTED);}
public GlobalCombineFn<? super InputT, ?, OutputT> beam_f16666_0()
{    return fn;}
public List<PCollectionView<?>> beam_f16667_0()
{    return sideInputs;}
public PCollection<KV<K, OutputT>> beam_f16668_0(PCollection<? extends KV<K, ? extends Iterable<InputT>>> input)
{    PCollection<KV<K, OutputT>> output = input.apply(ParDo.of(new DoFn<KV<K, ? extends Iterable<InputT>>, KV<K, OutputT>>() {        @ProcessElement        public void processElement(final ProcessContext c) {            K key = c.element().getKey();            OutputT output;            if (fn instanceof CombineFnWithContext) {                output = ((CombineFnWithContext<? super InputT, ?, OutputT>) fn).apply(c.element().getValue(), new CombineWithContext.Context() {                    @Override                    public PipelineOptions getPipelineOptions() {                        return c.getPipelineOptions();                    }                    @Override                    public <T> T sideInput(PCollectionView<T> view) {                        return c.sideInput(view);                    }                });            } else if (fn instanceof CombineFn) {                output = ((CombineFn<? super InputT, ?, OutputT>) fn).apply(c.element().getValue());            } else {                throw new IllegalStateException(String.format("Unknown type of CombineFn: %s", fn.getClass()));            }            c.output(KV.of(key, output));        }        @Override        public void populateDisplayData(DisplayData.Builder builder) {            builder.delegate(Combine.GroupedValues.this);        }    }).withSideInputs(sideInputs));    try {        KvCoder<K, InputT> kvCoder = getKvCoder(input.getCoder());        @SuppressWarnings("unchecked")        Coder<OutputT> outputValueCoder = ((GlobalCombineFn<InputT, ?, OutputT>) fn).getDefaultOutputCoder(input.getPipeline().getCoderRegistry(), kvCoder.getValueCoder());        output.setCoder(KvCoder.of(kvCoder.getKeyCoder(), outputValueCoder));    } catch (CannotProvideCoderException exc) {        }    return output;}
public Coder<AccumT> beam_f16676_0(CoderRegistry registry, Coder<InputT> inputCoder) throws CannotProvideCoderException
{    return registry.getCoder(getClass(), AbstractGlobalCombineFn.class, ImmutableMap.<Type, Coder<?>>of(getInputTVariable(), inputCoder), getAccumTVariable());}
public Coder<OutputT> beam_f16677_0(CoderRegistry registry, Coder<InputT> inputCoder) throws CannotProvideCoderException
{    return registry.getCoder(getClass(), AbstractGlobalCombineFn.class, ImmutableMap.<Type, Coder<?>>of(getInputTVariable(), inputCoder, getAccumTVariable(), this.getAccumulatorCoder(registry, inputCoder)), getOutputTVariable());}
public String beam_f16678_0()
{    return INCOMPATIBLE_GLOBAL_WINDOW_ERROR_MESSAGE;}
public ComposedCombineFnWithContext<DataT> beam_f16687_0(SimpleFunction<DataT, InputT> extractInputFn, Coder combineInputCoder, CombineFnWithContext<InputT, ?, OutputT> combineFnWithContext, TupleTag<OutputT> outputTag)
{    return new ComposedCombineFnWithContext<DataT>().with(extractInputFn, combineInputCoder, combineFnWithContext, outputTag);}
public V beam_f16688_0(TupleTag<V> tag)
{    checkArgument(valuesMap.keySet().contains(tag), "TupleTag " + tag + " is not in the CoCombineResult");    Object value = valuesMap.get(tag);    if (value == NullValue.INSTANCE) {        return null;    } else {        return (V) value;    }}
public boolean beam_f16689_0(Object o)
{    if (this == o) {        return true;    }    if (o == null || getClass() != o.getClass()) {        return false;    }    CoCombineResult that = (CoCombineResult) o;    return Objects.equal(valuesMap, that.valuesMap);}
public Object[] beam_f16697_0(Iterable<Object[]> accumulators)
{    Iterator<Object[]> iter = accumulators.iterator();    if (!iter.hasNext()) {        return createAccumulator();    } else {                                Object[] accum = iter.next();        for (int i = 0; i < combineFnCount; ++i) {            accum[i] = combineFns.get(i).mergeAccumulators(new ProjectionIterable(accumulators, i));        }        return accum;    }}
public CoCombineResult beam_f16698_0(Object[] accumulator)
{    Map<TupleTag<?>, Object> valuesMap = Maps.newHashMap();    for (int i = 0; i < combineFnCount; ++i) {        valuesMap.put(outputTags.get(i), combineFns.get(i).extractOutput(accumulator[i]));    }    return new CoCombineResult(valuesMap);}
public Object[] beam_f16699_0(Object[] accumulator)
{    for (int i = 0; i < combineFnCount; ++i) {        accumulator[i] = combineFns.get(i).compact(accumulator[i]);    }    return accumulator;}
public CoCombineResult beam_f16707_0(Object[] accumulator, Context c)
{    Map<TupleTag<?>, Object> valuesMap = Maps.newHashMap();    for (int i = 0; i < combineFnCount; ++i) {        valuesMap.put(outputTags.get(i), combineFnWithContexts.get(i).extractOutput(accumulator[i], c));    }    return new CoCombineResult(valuesMap);}
public Object[] beam_f16708_0(Object[] accumulator, Context c)
{    for (int i = 0; i < combineFnCount; ++i) {        accumulator[i] = combineFnWithContexts.get(i).compact(accumulator[i], c);    }    return accumulator;}
public Coder<Object[]> beam_f16709_0(CoderRegistry registry, Coder<DataT> dataCoder) throws CannotProvideCoderException
{    List<Coder<Object>> coders = Lists.newArrayList();    for (int i = 0; i < combineFnCount; ++i) {        Coder<Object> inputCoder = combineInputCoders.get(i).isPresent() ? combineInputCoders.get(i).get() : registry.getOutputCoder(extractInputFns.get(i), dataCoder);        coders.add(combineFnWithContexts.get(i).getAccumulatorCoder(registry, inputCoder));    }    return new ComposedAccumulatorCoder(coders);}
public Object[] beam_f16717_0(InputStream inStream) throws CoderException, IOException
{    return decode(inStream, Context.NESTED);}
public Object[] beam_f16718_0(InputStream inStream, Context context) throws CoderException, IOException
{    Object[] ret = new Object[codersCount];    if (codersCount == 0) {        return ret;    }    int lastIndex = codersCount - 1;    for (int i = 0; i < lastIndex; ++i) {        ret[i] = coders.get(i).decode(inStream);    }    ret[lastIndex] = coders.get(lastIndex).decode(inStream, context);    return ret;}
public List<? extends Coder<?>> beam_f16719_0()
{    return coders;}
public Requirements beam_f16727_0()
{    return requirements;}
public static Contextful<ClosureT> beam_f16728_0(ClosureT closure, Requirements requirements)
{    return new Contextful<>(closure, requirements);}
public String beam_f16729_0()
{    return MoreObjects.toStringHelper(this).add("closure", closure).add("requirements", requirements).toString();}
public static PTransform<PCollection<T>, PCollection<Long>> beam_f16737_0()
{    return Combine.globally(new CountFn<T>());}
public static PTransform<PCollection<KV<K, V>>, PCollection<KV<K, Long>>> beam_f16738_0()
{    return Combine.perKey(new CountFn<V>());}
public static PTransform<PCollection<T>, PCollection<KV<T, Long>>> beam_f16739_0()
{    return new PerElement<>();}
public void beam_f16747_0(long[] value, OutputStream outStream) throws IOException
{    VarInt.encode(value[0], outStream);}
public long[] beam_f16748_0(InputStream inStream) throws IOException, CoderException
{    try {        return new long[] { VarInt.decodeLong(inStream) };    } catch (EOFException | UTFDataFormatException exn) {        throw new CoderException(exn);    }}
public boolean beam_f16749_0(long[] value)
{    return true;}
public static Values<T> beam_f16757_0(Coder<T> coder)
{    return new Values<>(new ArrayList<>(), Optional.of(coder), Optional.absent());}
public static Values<T> beam_f16758_0(TypeDescriptor<T> type)
{    return new Values<>(new ArrayList<>(), Optional.absent(), Optional.of(type));}
public static Values<KV<K, V>> beam_f16759_0(Map<K, V> elems)
{    List<KV<K, V>> kvs = new ArrayList<>(elems.size());    for (Map.Entry<K, V> entry : elems.entrySet()) {        kvs.add(KV.of(entry.getKey(), entry.getValue()));    }    return of(kvs);}
public Values<T> beam_f16767_0(TypeDescriptor<T> type)
{    return new Values<>(elems, coder, Optional.of(type));}
public Iterable<T> beam_f16768_0()
{    return elems;}
public PCollection<T> beam_f16769_0(PBegin input)
{    Coder<T> coder;    try {        CoderRegistry coderRegistry = input.getPipeline().getCoderRegistry();        SchemaRegistry schemaRegistry = input.getPipeline().getSchemaRegistry();        coder = this.coder.isPresent() ? this.coder.get() : null;        if (coder == null) {            if (typeDescriptor.isPresent()) {                try {                    coder = SchemaCoder.of(schemaRegistry.getSchema(typeDescriptor.get()), schemaRegistry.getToRowFunction(typeDescriptor.get()), schemaRegistry.getFromRowFunction(typeDescriptor.get()));                } catch (NoSuchSchemaException e) {                                }                if (coder == null) {                    coder = coderRegistry.getCoder(typeDescriptor.get());                }            } else {                coder = getDefaultCreateCoder(coderRegistry, schemaRegistry, elems);            }        }    } catch (CannotProvideCoderException e) {        throw new IllegalArgumentException("Unable to infer a coder and no Coder was specified. " + "Please set a coder by invoking Create.withCoder() explicitly " + " or a schema by invoking Create.withSchema().", e);    }    try {        CreateSource<T> source = CreateSource.fromIterable(elems, coder);        return input.getPipeline().apply(Read.from(source));    } catch (IOException e) {        throw new RuntimeException(String.format("Unable to apply Create %s using Coder %s.", this, coder), e);    }}
public T beam_f16777_0() throws NoSuchElementException
{    if (next == null) {        throw new NoSuchElementException();    }    return next.orNull();}
protected long beam_f16779_0()
{    return index;}
protected boolean beam_f16780_0() throws IOException
{    return advanceImpl();}
public PCollection<T> beam_f16788_0(PBegin input)
{    try {        Coder<T> coder = null;        CoderRegistry coderRegistry = input.getPipeline().getCoderRegistry();        SchemaRegistry schemaRegistry = input.getPipeline().getSchemaRegistry();        if (elementCoder.isPresent()) {            coder = elementCoder.get();        } else if (typeDescriptor.isPresent()) {            try {                coder = SchemaCoder.of(schemaRegistry.getSchema(typeDescriptor.get()), schemaRegistry.getToRowFunction(typeDescriptor.get()), schemaRegistry.getFromRowFunction(typeDescriptor.get()));            } catch (NoSuchSchemaException e) {                        }            if (coder == null) {                coder = coderRegistry.getCoder(typeDescriptor.get());            }        } else {            Iterable<T> rawElements = Iterables.transform(timestampedElements, TimestampedValue::getValue);            coder = getDefaultCreateCoder(coderRegistry, schemaRegistry, rawElements);        }        PCollection<TimestampedValue<T>> intermediate = Pipeline.applyTransform(input, Create.of(timestampedElements).withCoder(TimestampedValueCoder.of(coder)));        PCollection<T> output = intermediate.apply(ParDo.of(new ConvertTimestamps<>()));        output.setCoder(coder);        return output;    } catch (CannotProvideCoderException e) {        throw new IllegalArgumentException("Unable to infer a coder and no Coder was specified. " + "Please set a coder by invoking CreateTimestamped.withCoder() explicitly.", e);    }}
public void beam_f16789_0(@Element TimestampedValue<T> element, OutputReceiver<T> r)
{    r.outputWithTimestamp(element.getValue(), element.getTimestamp());}
private static Coder<T> beam_f16790_0(CoderRegistry coderRegistry, SchemaRegistry schemaRegistry, Iterable<T> elems) throws CannotProvideCoderException
{    checkArgument(!Iterables.isEmpty(elems), "Can not determine a default Coder for a 'Create' PTransform that " + "has no elements.  Either add elements, call Create.empty(Coder)," + " Create.empty(TypeDescriptor), or call 'withCoder(Coder)' or " + "'withType(TypeDescriptor)' on the PTransform.");        Class<?> elementClazz = Void.class;    for (T elem : elems) {        if (elem == null) {            continue;        }        Class<?> clazz = elem.getClass();        if (elementClazz.equals(Void.class)) {            elementClazz = clazz;        } else if (!elementClazz.equals(clazz)) {                        throw new CannotProvideCoderException(String.format("Cannot provide coder for %s: The elements are not all of the same class.", Create.class.getSimpleName()));        }    }    TypeDescriptor<T> typeDescriptor = (TypeDescriptor<T>) TypeDescriptor.of(elementClazz);    if (elementClazz.getTypeParameters().length == 0) {        try {            Coder<T> coder = SchemaCoder.of(schemaRegistry.getSchema(typeDescriptor), schemaRegistry.getToRowFunction(typeDescriptor), schemaRegistry.getFromRowFunction(typeDescriptor));            return coder;        } catch (NoSuchSchemaException e) {                }        try {                        @SuppressWarnings("unchecked")            Coder<T> coder = (Coder<T>) coderRegistry.getCoder(typeDescriptor);            return coder;        } catch (CannotProvideCoderException exc) {                }    }        return (Coder<T>) inferCoderFromObjects(coderRegistry, schemaRegistry, elems);}
public int beam_f16798_0()
{    return entries.hashCode();}
public boolean beam_f16799_0(Object obj)
{    if (obj instanceof DisplayData) {        DisplayData that = (DisplayData) obj;        return Objects.equals(this.entries, that.entries);    }    return false;}
public String beam_f16800_0()
{    StringBuilder builder = new StringBuilder();    boolean isFirstLine = true;    for (Item entry : entries.values()) {        if (isFirstLine) {            isFirstLine = false;        } else {            builder.append("\n");        }        builder.append(entry);    }    return builder.toString();}
public String beam_f16808_0()
{    return String.format("%s:%s=%s", getNamespace(), getKey(), getValue());}
 static ItemSpec.Builder<T> beam_f16809_0()
{    return new AutoValue_DisplayData_ItemSpec.Builder<>();}
 ItemSpec.Builder<T> beam_f16810_0(@Nullable T value)
{    FormattedItemValue formatted = getType().safeFormat(value);    return this.setValue(formatted.getLongValue()).setShortValue(formatted.getShortValue());}
public String beam_f16818_0()
{    StringBuilder b = new StringBuilder().append("[");    Joiner.on("/").appendTo(b, components);    b.append("]");    return b.toString();}
public boolean beam_f16819_0(Object obj)
{    return obj instanceof Path && Objects.equals(components, ((Path) obj).components);}
public int beam_f16820_0()
{    return components.hashCode();}
 FormattedItemValue beam_f16828_0(Object value)
{    Instant instant = checkType(value, Instant.class, TIMESTAMP);    return new FormattedItemValue(TIMESTAMP_FORMATTER.print(instant));}
 FormattedItemValue beam_f16829_0(Object value)
{    Duration duration = checkType(value, Duration.class, DURATION);    return new FormattedItemValue(duration.getMillis());}
 FormattedItemValue beam_f16830_0(Object value)
{    Class<?> clazz = checkType(value, Class.class, JAVA_CLASS);    return new FormattedItemValue(clazz.getName(), clazz.getSimpleName());}
public Builder beam_f16838_0(ItemSpec<T> item, @Nullable T defaultValue)
{    checkNotNull(item, "Input display item cannot be null");    ItemSpec<T> defaultItem = item.withValue(defaultValue);    return addItemIf(!Objects.equals(item, defaultItem), item);}
private Builder beam_f16839_0(boolean condition, ItemSpec<?> spec)
{    if (!condition) {        return this;    }    checkNotNull(spec, "Input display item cannot be null");    checkNotNull(spec.getValue(), "Input display value cannot be null");    if (spec.getNamespace() == null) {        spec = spec.withNamespace(latestNs);    }    Item item = Item.create(spec, latestPath);    Identifier id = Identifier.of(item.getPath(), item.getNamespace(), item.getKey());    checkArgument(!entries.containsKey(id), "Display data key (%s) is not unique within the specified path and namespace: %s%s.", item.getKey(), item.getPath(), item.getNamespace());    entries.put(id, item);    return this;}
private DisplayData beam_f16840_0()
{    return new DisplayData(this.entries);}
public static ItemSpec<Instant> beam_f16848_0(String key, @Nullable Instant value)
{    return item(key, Type.TIMESTAMP, value);}
public static ItemSpec<Duration> beam_f16849_0(String key, @Nullable Duration value)
{    return item(key, Type.DURATION, value);}
public static ItemSpec<Class<T>> beam_f16850_0(String key, @Nullable Class<T> value)
{    return item(key, Type.JAVA_CLASS, value);}
public void beam_f16858_0(@Element KV<T, Void> element, PaneInfo pane, OutputReceiver<T> receiver)
{    if (pane.isFirst()) {                receiver.output(element.getKey());    }}
public PCollection<T> beam_f16859_0(PCollection<T> in)
{    validateWindowStrategy(in.getWindowingStrategy());    WithKeys<IdT, T> withKeys = WithKeys.of(fn);    if (representativeType != null) {        withKeys = withKeys.withKeyType(representativeType);    }    PCollection<KV<IdT, T>> keyed = in.apply("KeyByRepresentativeValue", withKeys);    KvCoder<IdT, T> keyedCoder = (KvCoder<IdT, T>) keyed.getCoder();    PCollection<KV<IdT, T>> combined = keyed.apply("OneValuePerKey", Combine.perKey(new Combine.BinaryCombineFn<T>() {        @Override        public T apply(T left, T right) {            return left;        }    })).setCoder(KvCoder.of(keyedCoder.getKeyCoder(), NullableCoder.of(keyedCoder.getValueCoder())));    return combined.apply("KeepFirstPane", ParDo.of(new DoFn<KV<IdT, T>, T>() {        @ProcessElement        public void processElement(@Element KV<IdT, T> element, PaneInfo pane, OutputReceiver<T> receiver) {                        if (pane.isFirst()) {                receiver.output(element.getValue());            }        }    }));}
public T beam_f16860_0(T left, T right)
{    return left;}
public ProcessContinuation beam_f16868_0(Duration resumeDelay)
{    return new AutoValue_DoFn_ProcessContinuation(shouldResume(), resumeDelay);}
public void beam_f16871_0(Row output)
{    outputReceiver.output(schemaCoder.getFromRowFunction().apply(output));}
public void beam_f16872_0(Row output, Instant timestamp)
{    outputReceiver.outputWithTimestamp(schemaCoder.getFromRowFunction().apply(output), timestamp);}
public static OutputReceiver<Row> beam_f16880_0(DoFn<?, ?>.WindowedContext context, @Nullable TupleTag<T> outputTag, SchemaCoder<T> schemaCoder)
{    return new RowOutputReceiver<>(context, outputTag, schemaCoder);}
public static DoFnSchemaInformation beam_f16881_0()
{    return new AutoValue_DoFnSchemaInformation.Builder().setElementConverters(Collections.emptyList()).build();}
 DoFnSchemaInformation beam_f16882_0(SchemaCoder<?> inputCoder, FieldAccessDescriptor selectDescriptor, Schema selectOutputSchema, SchemaCoder<?> parameterCoder, boolean unbox)
{    List<SerializableFunction<?, ?>> converters = ImmutableList.<SerializableFunction<?, ?>>builder().addAll(getElementConverters()).add(ConversionFunction.of(inputCoder.getSchema(), inputCoder.getToRowFunction(), parameterCoder.getFromRowFunction(), selectDescriptor, selectOutputSchema, unbox)).build();    return toBuilder().setElementConverters(converters).build();}
public void beam_f16890_0(Map<PCollectionView<?>, Map<BoundedWindow, ?>> sideInputs)
{    checkState(state == State.UNINITIALIZED, "Can't add side inputs: DoFnTester is already initialized, in state %s", state);    this.sideInputs = sideInputs;}
public void beam_f16891_0(PCollectionView<T> sideInput, BoundedWindow window, T value)
{    checkState(state == State.UNINITIALIZED, "Can't add side inputs: DoFnTester is already initialized, in state %s", state);    Map<BoundedWindow, T> windowValues = (Map<BoundedWindow, T>) sideInputs.get(sideInput);    if (windowValues == null) {        windowValues = new HashMap<>();        sideInputs.put(sideInput, windowValues);    }    windowValues.put(window, value);}
public PipelineOptions beam_f16892_0()
{    return options;}
public void beam_f16900_0(TimestampedValue<InputT> element) throws Exception
{    checkNotNull(element, "Timestamped element cannot be null");    processWindowedElement(element.getValue(), element.getTimestamp(), GlobalWindow.INSTANCE);}
public void beam_f16901_0(InputT element, Instant timestamp, final BoundedWindow window) throws Exception
{    if (state != State.BUNDLE_STARTED) {        startBundle();    }    try {        final DoFn<InputT, OutputT>.ProcessContext processContext = createProcessContext(ValueInSingleWindow.of(element, timestamp, window, PaneInfo.NO_FIRING));        fnInvoker.invokeProcessElement(new DoFnInvoker.ArgumentProvider<InputT, OutputT>() {            @Override            public BoundedWindow window() {                return window;            }            @Override            public PaneInfo paneInfo(DoFn<InputT, OutputT> doFn) {                return processContext.pane();            }            @Override            public PipelineOptions pipelineOptions() {                return getPipelineOptions();            }            @Override            public DoFn<InputT, OutputT>.StartBundleContext startBundleContext(DoFn<InputT, OutputT> doFn) {                throw new UnsupportedOperationException("Not expected to access DoFn.StartBundleContext from @ProcessElement");            }            @Override            public DoFn<InputT, OutputT>.FinishBundleContext finishBundleContext(DoFn<InputT, OutputT> doFn) {                throw new UnsupportedOperationException("Not expected to access DoFn.FinishBundleContext from @ProcessElement");            }            @Override            public DoFn<InputT, OutputT>.ProcessContext processContext(DoFn<InputT, OutputT> doFn) {                return processContext;            }            @Override            public InputT element(DoFn<InputT, OutputT> doFn) {                return processContext.element();            }            @Override            public InputT sideInput(String sideInputTag) {                throw new UnsupportedOperationException("SideInputs are not supported by DoFnTester");            }            @Override            public InputT schemaElement(int index) {                throw new UnsupportedOperationException("Schemas are not supported by DoFnTester");            }            @Override            public Instant timestamp(DoFn<InputT, OutputT> doFn) {                return processContext.timestamp();            }            @Override            public TimeDomain timeDomain(DoFn<InputT, OutputT> doFn) {                throw new UnsupportedOperationException("Not expected to access TimeDomain from @ProcessElement");            }            @Override            public OutputReceiver<OutputT> outputReceiver(DoFn<InputT, OutputT> doFn) {                return DoFnOutputReceivers.windowedReceiver(processContext, null);            }            @Override            public OutputReceiver<Row> outputRowReceiver(DoFn<InputT, OutputT> doFn) {                throw new UnsupportedOperationException("Schemas are not supported by DoFnTester");            }            @Override            public MultiOutputReceiver taggedOutputReceiver(DoFn<InputT, OutputT> doFn) {                return DoFnOutputReceivers.windowedMultiReceiver(processContext, null);            }            @Override            public OnTimerContext onTimerContext(DoFn<InputT, OutputT> doFn) {                throw new UnsupportedOperationException("DoFnTester doesn't support timers yet.");            }            @Override            public RestrictionTracker<?, ?> restrictionTracker() {                throw new UnsupportedOperationException("Not expected to access RestrictionTracker from a regular DoFn in DoFnTester");            }            @Override            public org.apache.beam.sdk.state.State state(String stateId) {                throw new UnsupportedOperationException("DoFnTester doesn't support state yet");            }            @Override            public Timer timer(String timerId) {                throw new UnsupportedOperationException("DoFnTester doesn't support timers yet");            }        });    } catch (UserCodeException e) {        unwrapUserCodeException(e);    }}
public BoundedWindow beam_f16902_0()
{    return window;}
public InputT beam_f16910_0(int index)
{    throw new UnsupportedOperationException("Schemas are not supported by DoFnTester");}
public Instant beam_f16911_0(DoFn<InputT, OutputT> doFn)
{    return processContext.timestamp();}
public TimeDomain beam_f16912_0(DoFn<InputT, OutputT> doFn)
{    throw new UnsupportedOperationException("Not expected to access TimeDomain from @ProcessElement");}
public void beam_f16920_0() throws Exception
{    checkState(state == State.BUNDLE_STARTED, "Must be inside bundle to call finishBundle, but was: %s", state);    try {        fnInvoker.invokeFinishBundle(new TestFinishBundleContext());    } catch (UserCodeException e) {        unwrapUserCodeException(e);    }    if (cloningBehavior == CloningBehavior.CLONE_PER_BUNDLE) {        fnInvoker.invokeTeardown();        fn = null;        fnInvoker = null;        state = State.UNINITIALIZED;    } else {        state = State.BUNDLE_FINISHED;    }}
public List<OutputT> beam_f16921_0()
{    return peekOutputElementsWithTimestamp().stream().map(TimestampedValue::getValue).collect(Collectors.toList());}
public List<TimestampedValue<OutputT>> beam_f16922_0()
{        return getImmutableOutput(mainOutputTag).stream().map(input -> TimestampedValue.of(input.getValue(), input.getTimestamp())).collect(Collectors.toList());}
public List<T> beam_f16930_0(TupleTag<T> tag)
{    List<T> resultElems = new ArrayList<>(peekOutputElements(tag));    clearOutputElements(tag);    return resultElems;}
private List<ValueInSingleWindow<T>> beam_f16931_0(TupleTag<T> tag)
{    @SuppressWarnings({ "unchecked", "rawtypes" })    List<ValueInSingleWindow<T>> elems = (List) getOutputs().get(tag);    return ImmutableList.copyOf(MoreObjects.firstNonNull(elems, Collections.emptyList()));}
public List<ValueInSingleWindow<T>> beam_f16932_0(TupleTag<T> tag)
{    List<ValueInSingleWindow<T>> outputList = (List) getOutputs().get(tag);    if (outputList == null) {        outputList = new ArrayList<>();        getOutputs().put(tag, (List) outputList);    }    return outputList;}
public T beam_f16940_0(PCollectionView<T> view)
{    Map<BoundedWindow, ?> viewValues = sideInputs.get(view);    if (viewValues != null) {        BoundedWindow sideInputWindow = view.getWindowMappingFn().getSideInputWindow(element.getWindow());        @SuppressWarnings("unchecked")        T windowValue = (T) viewValues.get(sideInputWindow);        if (windowValue != null) {            return windowValue;        }    }                        checkState(Materializations.MULTIMAP_MATERIALIZATION_URN.equals(view.getViewFn().getMaterialization().getUrn()), "Only materializations of type %s supported, received %s", Materializations.MULTIMAP_MATERIALIZATION_URN, view.getViewFn().getMaterialization().getUrn());    return ((ViewFn<Materializations.MultimapView, T>) view.getViewFn()).apply(o -> Collections.emptyList());}
public Instant beam_f16941_0()
{    return element.getTimestamp();}
public PaneInfo beam_f16942_0()
{    return element.getPane();}
public Void beam_f16950_0(DoFnSignature.Parameter.ProcessContextParameter p)
{        return null;}
public Void beam_f16951_0(DoFnSignature.Parameter.WindowParameter p)
{        return null;}
public Void beam_f16952_0(DoFnSignature.Parameter.ElementParameter p)
{    return null;}
private Map beam_f16960_0()
{    if (outputs == null) {        outputs = new HashMap<>();    }    return outputs;}
public static Filter<T> beam_f16961_0(PredicateT predicate)
{    return new Filter<>(predicate);}
public static Filter<T> beam_f16962_0(PredicateT predicate)
{    return by((ProcessFunction<T, Boolean>) predicate);}
public void beam_f16970_0(@Element T element, OutputReceiver<T> r) throws Exception
{    if (predicate.apply(element)) {        r.output(element);    }}
public void beam_f16971_0(DisplayData.Builder builder)
{    super.populateDisplayData(builder);    builder.add(DisplayData.item("predicate", predicateDescription).withLabel("Filter Predicate"));}
public static FlatMapElements<InputT, OutputT> beam_f16972_0(InferableFunction<? super InputT, ? extends Iterable<OutputT>> fn)
{    Contextful<Fn<InputT, Iterable<OutputT>>> wrapped = (Contextful) Contextful.fn(fn);    TypeDescriptor<OutputT> outputType = TypeDescriptors.extractFromTypeParameters((TypeDescriptor<Iterable<OutputT>>) fn.getOutputTypeDescriptor(), Iterable.class, new TypeDescriptors.TypeVariableExtractor<Iterable<OutputT>, OutputT>() {    });    TypeDescriptor<InputT> inputType = (TypeDescriptor<InputT>) fn.getInputTypeDescriptor();    return new FlatMapElements<>(wrapped, fn, inputType, outputType);}
public TypeDescriptor<InputT> beam_f16980_0()
{    return inputType;}
public TypeDescriptor<OutputT> beam_f16981_0()
{    checkState(outputType != null, "%s output type descriptor was null; " + "this probably means that getOutputTypeDescriptor() was called after " + "serialization/deserialization, but it is only available prior to " + "serialization, for constructing a pipeline and inferring coders", FlatMapElements.class.getSimpleName());    return outputType;}
public void beam_f16982_0(DisplayData.Builder builder)
{    builder.delegate(FlatMapElements.this);}
public void beam_f16990_0(@Element InputT element, MultiOutputReceiver r, ProcessContext c) throws Exception
{    boolean exceptionWasThrown = false;    Iterable<OutputT> res = null;    try {        res = fn.getClosure().apply(c.element(), Fn.Context.wrapProcessContext(c));    } catch (Exception e) {        exceptionWasThrown = true;        ExceptionElement<InputT> exceptionElement = ExceptionElement.of(element, e);        r.get(failureTag).output(exceptionHandler.apply(exceptionElement));    }        if (!exceptionWasThrown) {        for (OutputT output : res) {            r.get(outputTag).output(output);        }    }}
public void beam_f16991_0(DisplayData.Builder builder)
{    builder.delegate(FlatMapWithFailures.this);}
public TypeDescriptor<InputT> beam_f16992_0()
{    return inputType;}
 static GroupByKey<K, V> beam_f17000_0()
{    return new GroupByKey<>(true);}
public boolean beam_f17001_0()
{    return fewKeys;}
public static void beam_f17002_0(PCollection<?> input)
{    WindowingStrategy<?, ?> windowingStrategy = input.getWindowingStrategy();        if (windowingStrategy.getWindowFn() instanceof GlobalWindows && windowingStrategy.getTrigger() instanceof DefaultTrigger && input.isBounded() != IsBounded.BOUNDED) {        throw new IllegalStateException("GroupByKey cannot be applied to non-bounded PCollection in " + "the GlobalWindow without a trigger. Use a Window.into or Window.triggering transform " + "prior to GroupByKey.");    }        if (windowingStrategy.getWindowFn() instanceof InvalidWindows) {        String cause = ((InvalidWindows<?>) windowingStrategy.getWindowFn()).getCause();        throw new IllegalStateException("GroupByKey must have a valid Window merge function.  " + "Invalid because: " + cause);    }}
public void beam_f17010_0(DisplayData.Builder builder)
{    super.populateDisplayData(builder);    if (fewKeys) {        builder.add(DisplayData.item("fewKeys", true).withLabel("Has Few Keys"));    }}
public static GroupIntoBatches<K, InputT> beam_f17011_0(long batchSize)
{    return new GroupIntoBatches<>(batchSize);}
public long beam_f17012_0()
{    return batchSize;}
public PCollection<byte[]> beam_f17020_0(PBegin input)
{    return PCollection.createPrimitiveOutputInternal(input.getPipeline(), WindowingStrategy.globalDefault(), IsBounded.BOUNDED, ByteArrayCoder.of());}
public OutputT beam_f17021_0(InputT input) throws Exception
{    return fn.apply(input);}
public static InferableFunction<InputT, OutputT> beam_f17022_0(ProcessFunction<InputT, OutputT> fn, TypeDescriptor<OutputT> outputType)
{    return new InferableFunctionWithOutputType<>(fn, outputType);}
public Iterable<V> beam_f17031_0(TupleTag<V> tag)
{    int index = schema.getIndex(tag);    if (index < 0) {        throw new IllegalArgumentException("TupleTag " + tag + " is not in the schema");    }    @SuppressWarnings("unchecked")    Iterable<V> unions = (Iterable<V>) valueMap.get(index);    return unions;}
public Iterable<V> beam_f17032_0(String tag)
{    return getAll(new TupleTag<>(tag));}
public V beam_f17033_0(TupleTag<V> tag)
{    return innerGetOnly(tag, null, false);}
public void beam_f17041_0(CoGbkResult value, OutputStream outStream) throws CoderException, IOException
{    if (!schema.equals(value.getSchema())) {        throw new CoderException("input schema does not match coder schema");    }    if (schema.size() == 0) {        return;    }    for (int unionTag = 0; unionTag < schema.size(); unionTag++) {        tagListCoder(unionTag).encode(value.valueMap.get(unionTag), outStream);    }}
public CoGbkResult beam_f17042_0(InputStream inStream) throws CoderException, IOException
{    if (schema.size() == 0) {        return new CoGbkResult(schema, ImmutableList.<Iterable<?>>of());    }    List<Iterable<?>> valueMap = Lists.newArrayListWithExpectedSize(schema.size());    for (int unionTag = 0; unionTag < schema.size(); unionTag++) {        valueMap.add(tagListCoder(unionTag).decode(inStream));    }    return new CoGbkResult(schema, valueMap);}
private IterableCoder beam_f17043_0(int unionTag)
{    return IterableCoder.of(unionCoder.getElementCoders().get(unionTag));}
public boolean beam_f17051_0()
{    if (Boolean.FALSE.equals(containsTag[tag])) {        return false;    }    advance();    if (unions.hasNext()) {        return true;    } else {                for (int i = 0; i < containsTag.length; i++) {            if (containsTag[i] == null) {                containsTag[i] = false;            }        }        return false;    }}
public V beam_f17052_0()
{    advance();    return (V) unions.next().getValue();}
private void beam_f17053_0()
{    while (unions.hasNext()) {        int curTag = unions.peek().getUnionTag();        containsTag[curTag] = true;        if (curTag == tag) {            break;        }        unions.next();    }}
public int beam_f17061_0()
{    return tupleTagList.getAll().hashCode();}
public String beam_f17062_0()
{    return "CoGbkResultSchema: " + tupleTagList.getAll();}
public static CoGroupByKey<K> beam_f17063_0()
{    return new CoGroupByKey<>();}
public static KeyedPCollectionTuple<K> beam_f17071_0(String tag, PCollection<KV<K, InputT>> pc)
{    return of(new TupleTag<>(tag), pc);}
public KeyedPCollectionTuple<K> beam_f17072_0(TupleTag<V> tag, PCollection<KV<K, V>> pc)
{    if (pc.getPipeline() != getPipeline()) {        throw new IllegalArgumentException("PCollections come from different Pipelines");    }    TaggedKeyedPCollection<K, ?> wrapper = new TaggedKeyedPCollection<>(tag, pc);    Coder<K> myKeyCoder = keyCoder == null ? getKeyCoder(pc) : keyCoder;    List<TaggedKeyedPCollection<K, ?>> newKeyedCollections = copyAddLast(keyedCollections, wrapper);    return new KeyedPCollectionTuple<>(getPipeline(), newKeyedCollections, schema.getTupleTagList().and(tag), myKeyCoder);}
public KeyedPCollectionTuple<K> beam_f17073_0(String tag, PCollection<KV<K, V>> pc)
{    return and(new TupleTag<>(tag), pc);}
public Pipeline beam_f17081_0()
{    return pipeline;}
private static Coder<K> beam_f17082_0(PCollection<KV<K, V>> pc)
{                Coder<?> entryCoder = pc.getCoder();    if (!(entryCoder instanceof KvCoder<?, ?>)) {        throw new IllegalArgumentException("PCollection does not use a KvCoder");    }    @SuppressWarnings("unchecked")    KvCoder<K, V> coder = (KvCoder<K, V>) entryCoder;    return coder.getKeyCoder();}
public PCollection<KV<K, V>> beam_f17083_0()
{    return pCollection;}
public static UnionCoder beam_f17091_0(List<Coder<?>> elementCoders)
{    return new UnionCoder(elementCoders);}
private int beam_f17092_0(RawUnionValue union)
{    if (union == null) {        throw new IllegalArgumentException("cannot encode a null tagged union");    }    int index = union.getUnionTag();    if (index < 0 || index >= elementCoders.size()) {        throw new IllegalArgumentException("union value index " + index + " not in range [0.." + (elementCoders.size() - 1) + "]");    }    return index;}
public void beam_f17093_0(RawUnionValue union, OutputStream outStream) throws IOException, CoderException
{    encode(union, outStream, Context.NESTED);}
public void beam_f17101_0(RawUnionValue union, ElementByteSizeObserver observer) throws Exception
{    int index = getIndexForEncoding(union);        observer.update(VarInt.getLength(index));        @SuppressWarnings("unchecked")    Coder<Object> coder = (Coder<Object>) elementCoders.get(index);    coder.registerByteSizeObserver(union.getValue(), observer);}
public void beam_f17102_0() throws NonDeterministicException
{    verifyDeterministic(this, "UnionCoder is only deterministic if all element coders are", elementCoders);}
public static PTransform<PCollection<? extends String>, PCollection<Row>> beam_f17103_0(Schema rowSchema)
{    return JsonToRowFn.forSchema(rowSchema);}
public static KvSwap<K, V> beam_f17111_0()
{    return new KvSwap<>();}
public PCollection<KV<V, K>> beam_f17112_0(PCollection<KV<K, V>> in)
{    return in.apply("KvSwap", MapElements.via(new SimpleFunction<KV<K, V>, KV<V, K>>() {        @Override        public KV<V, K> apply(KV<K, V> kv) {            return KV.of(kv.getValue(), kv.getKey());        }    }));}
public KV<V, K> beam_f17113_0(KV<K, V> kv)
{    return KV.of(kv.getValue(), kv.getKey());}
public TimestampedValue<T> beam_f17121_0(Iterable<TimestampedValue<T>> accumulators)
{    checkNotNull(accumulators, "accumulators must be non-null");    Iterator<TimestampedValue<T>> iter = accumulators.iterator();    if (!iter.hasNext()) {        return createAccumulator();    }    TimestampedValue<T> merged = iter.next();    while (iter.hasNext()) {        merged = addInput(merged, iter.next());    }    return merged;}
public T beam_f17122_0(TimestampedValue<T> accumulator)
{    return accumulator.getValue();}
public PCollection<T> beam_f17123_0(PCollection<T> input)
{    Coder<T> inputCoder = input.getCoder();    return input.apply("Reify Timestamps", Reify.timestamps()).apply("Latest Value", Combine.globally(new LatestFn<>())).setCoder(NullableCoder.of(inputCoder));}
public PCollection<OutputT> beam_f17131_0(PCollection<? extends InputT> input)
{    checkNotNull(fn, "Must specify a function on MapElements using .via()");    return input.apply("Map", ParDo.of(new DoFn<InputT, OutputT>() {        @ProcessElement        public void processElement(@Element InputT element, OutputReceiver<OutputT> receiver, ProcessContext c) throws Exception {            receiver.output(fn.getClosure().apply(element, Fn.Context.wrapProcessContext(c)));        }        @Override        public void populateDisplayData(DisplayData.Builder builder) {            builder.delegate(MapElements.this);        }        @Override        public TypeDescriptor<InputT> getInputTypeDescriptor() {            return inputType;        }        @Override        public TypeDescriptor<OutputT> getOutputTypeDescriptor() {            checkState(outputType != null, "%s output type descriptor was null; " + "this probably means that getOutputTypeDescriptor() was called after " + "serialization/deserialization, but it is only available prior to " + "serialization, for constructing a pipeline and inferring coders", MapElements.class.getSimpleName());            return outputType;        }    }).withSideInputs(fn.getRequirements().getSideInputs()));}
public void beam_f17132_0(@Element InputT element, OutputReceiver<OutputT> receiver, ProcessContext c) throws Exception
{    receiver.output(fn.getClosure().apply(element, Fn.Context.wrapProcessContext(c)));}
public void beam_f17133_0(DisplayData.Builder builder)
{    builder.delegate(MapElements.this);}
public void beam_f17141_0(DisplayData.Builder builder)
{    super.populateDisplayData(builder);    builder.add(DisplayData.item("class", originalFnForDisplayData.getClass()));    if (originalFnForDisplayData instanceof HasDisplayData) {        builder.include("fn", (HasDisplayData) originalFnForDisplayData);    }    builder.add(DisplayData.item("exceptionHandler.class", exceptionHandler.getClass()));    if (exceptionHandler instanceof HasDisplayData) {        builder.include("exceptionHandler", (HasDisplayData) exceptionHandler);    }}
public TypeDescriptor<FailureT> beam_f17142_0()
{    return failureType;}
public void beam_f17143_0(@Element InputT element, MultiOutputReceiver r, ProcessContext c) throws Exception
{    boolean exceptionWasThrown = false;    OutputT result = null;    try {        result = fn.getClosure().apply(c.element(), Fn.Context.wrapProcessContext(c));    } catch (Exception e) {        exceptionWasThrown = true;        ExceptionElement<InputT> exceptionElement = ExceptionElement.of(element, e);        r.get(failureTag).output(exceptionHandler.apply(exceptionElement));    }        if (!exceptionWasThrown) {        r.get(outputTag).output(result);    }}
public static Combine.Globally<Integer, Integer> beam_f17151_0()
{    return Combine.globally(new MaxIntegerFn());}
public static Combine.PerKey<K, Integer, Integer> beam_f17152_0()
{    return Combine.perKey(new MaxIntegerFn());}
public static Combine.Globally<Long, Long> beam_f17153_0()
{    return Combine.globally(new MaxLongFn());}
public static BinaryCombineFn<T> beam_f17161_0(final ComparatorT comparator)
{    return new MaxFn<>(null, comparator);}
public static BinaryCombineFn<T> beam_f17162_0(T identity)
{    return new MaxFn<>(identity, new Top.Natural<>());}
public static BinaryCombineFn<T> beam_f17163_0()
{    return new MaxFn<>(null, new Top.Natural<>());}
public int beam_f17171_0(int left, int right)
{    return left >= right ? left : right;}
public int beam_f17172_0()
{    return Integer.MIN_VALUE;}
public long beam_f17173_0(long left, long right)
{    return left >= right ? left : right;}
public Coder<CountSum<NumT>> beam_f17181_0(CoderRegistry registry, Coder<NumT> inputCoder)
{    return new CountSumCoder<>();}
public void beam_f17182_0(NumT element)
{    count++;    sum += element.doubleValue();}
public void beam_f17183_0(CountSum<NumT> accumulator)
{    count += accumulator.count;    sum += accumulator.sum;}
public static Combine.Globally<Integer, Integer> beam_f17191_0()
{    return Combine.globally(new MinIntegerFn());}
public static Combine.PerKey<K, Integer, Integer> beam_f17192_0()
{    return Combine.perKey(new MinIntegerFn());}
public static Combine.Globally<Long, Long> beam_f17193_0()
{    return Combine.globally(new MinLongFn());}
public static BinaryCombineFn<T> beam_f17201_0(ComparatorT comparator)
{    return new MinFn<>(null, comparator);}
public static BinaryCombineFn<T> beam_f17202_0(T identity)
{    return new MinFn<>(identity, new Top.Natural<>());}
public static BinaryCombineFn<T> beam_f17203_0()
{    return new MinFn<>(null, new Top.Natural<>());}
public int beam_f17211_0(int left, int right)
{    return left <= right ? left : right;}
public int beam_f17212_0()
{    return Integer.MAX_VALUE;}
public long beam_f17213_0(long left, long right)
{    return left <= right ? left : right;}
private static void beam_f17221_0(Map<String, PCollectionView<?>> sideInputs, DoFn<?, ?> fn)
{    DoFnSignature signature = DoFnSignatures.getSignature(fn.getClass());    DoFnSignature.ProcessElementMethod processElementMethod = signature.processElement();    for (SideInputParameter sideInput : processElementMethod.getSideInputParameters()) {        PCollectionView<?> view = sideInputs.get(sideInput.sideInputId());        checkArgument(view != null, "the ProcessElement method expects a side input identified with the tag %s, but no such side input was" + " supplied. Use withSideInput(String, PCollectionView) to supply this side input.", sideInput.sideInputId());        TypeDescriptor<?> viewType = view.getViewFn().getTypeDescriptor();                checkArgument(viewType.equals(sideInput.elementT()), "Side Input with tag %s and type %s cannot be bound to ProcessElement parameter with type %s", sideInput.sideInputId(), viewType, sideInput.elementT());    }}
private static FieldAccessDescriptor beam_f17222_0(@Nullable String fieldAccessString, Schema inputSchema, Map<String, FieldAccessDeclaration> fieldAccessDeclarations, DoFn<?, ?> fn)
{                        FieldAccessDescriptor fieldAccessDescriptor = null;    if (fieldAccessString == null) {                fieldAccessDescriptor = FieldAccessDescriptor.withAllFields();    } else {                FieldAccessDeclaration fieldAccessDeclaration = fieldAccessDeclarations.get(fieldAccessString);        if (fieldAccessDeclaration != null) {            checkArgument(fieldAccessDeclaration.field().getType().equals(FieldAccessDescriptor.class));            try {                fieldAccessDescriptor = (FieldAccessDescriptor) fieldAccessDeclaration.field().get(fn);            } catch (IllegalAccessException e) {                throw new RuntimeException(e);            }        } else {                        fieldAccessDescriptor = FieldAccessDescriptor.withFieldNames(fieldAccessString);        }    }    return fieldAccessDescriptor.resolve(inputSchema);}
private static Coder[] beam_f17223_0(DoFnSignature.StateDeclaration stateDeclaration, CoderRegistry coderRegistry, Coder<InputT> inputCoder)
{    Type stateType = stateDeclaration.stateType().getType();    if (!(stateType instanceof ParameterizedType)) {                return new Coder[0];    }    Type[] typeArguments = ((ParameterizedType) stateType).getActualTypeArguments();    Coder[] coders = new Coder[typeArguments.length];    for (int i = 0; i < typeArguments.length; i++) {        Type typeArgument = typeArguments[i];        TypeDescriptor<?> typeDescriptor = TypeDescriptor.of(typeArgument);        try {            coders[i] = coderRegistry.getCoder(typeDescriptor);        } catch (CannotProvideCoderException e) {            try {                coders[i] = coderRegistry.getCoder(typeDescriptor, inputCoder.getEncodedTypeDescriptor(), inputCoder);            } catch (CannotProvideCoderException ignored) {                        }        }    }    return coders;}
public SingleOutput<InputT, OutputT> beam_f17231_0(String tagId, PCollectionView<?> pCollectionView)
{    return withSideInputs(Collections.singletonMap(tagId, pCollectionView));}
public MultiOutput<InputT, OutputT> beam_f17232_0(TupleTag<OutputT> mainOutputTag, TupleTagList additionalOutputTags)
{    return new MultiOutput<>(fn, sideInputs, mainOutputTag, additionalOutputTags, fnDisplayData);}
public PCollection<OutputT> beam_f17233_0(PCollection<? extends InputT> input)
{    SchemaRegistry schemaRegistry = input.getPipeline().getSchemaRegistry();    CoderRegistry registry = input.getPipeline().getCoderRegistry();    finishSpecifyingStateSpecs(fn, registry, input.getCoder());    TupleTag<OutputT> mainOutput = new TupleTag<>(MAIN_OUTPUT_TAG);    PCollection<OutputT> res = input.apply(withOutputTags(mainOutput, TupleTagList.empty())).get(mainOutput);    try {        res.setSchema(schemaRegistry.getSchema(getFn().getOutputTypeDescriptor()), schemaRegistry.getToRowFunction(getFn().getOutputTypeDescriptor()), schemaRegistry.getFromRowFunction(getFn().getOutputTypeDescriptor()));    } catch (NoSuchSchemaException e) {        try {            res.setCoder(registry.getCoder(getFn().getOutputTypeDescriptor(), getFn().getInputTypeDescriptor(), ((PCollection<InputT>) input).getCoder()));        } catch (CannotProvideCoderException e2) {                }    }    return res;}
public MultiOutput<InputT, OutputT> beam_f17241_0(Iterable<? extends PCollectionView<?>> sideInputs)
{    Map<String, PCollectionView<?>> mappedInputs = StreamSupport.stream(sideInputs.spliterator(), false).collect(Collectors.toMap(v -> v.getTagInternal().getId(), v -> v));    return withSideInputs(mappedInputs);}
public MultiOutput<InputT, OutputT> beam_f17242_0(Map<String, PCollectionView<?>> sideInputs)
{    return new MultiOutput<>(fn, ImmutableMap.<String, PCollectionView<?>>builder().putAll(this.sideInputs).putAll(sideInputs).build(), mainOutputTag, additionalOutputTags, fnDisplayData);}
public MultiOutput<InputT, OutputT> beam_f17243_0(String tagId, PCollectionView<?> pCollectionView)
{    return withSideInputs(Collections.singletonMap(tagId, pCollectionView));}
public Map<TupleTag<?>, PValue> beam_f17251_0()
{    return PCollectionViews.toAdditionalInputs(sideInputs.values());}
public String beam_f17252_0()
{    return fn.toString();}
private static void beam_f17253_0(DisplayData.Builder builder, HasDisplayData fn, DisplayData.ItemSpec<? extends Class<?>> fnDisplayData)
{    builder.include("fn", fn).add(fnDisplayData);}
public Map<TupleTag<?>, PValue> beam_f17262_0()
{    return Collections.emptyMap();}
public String beam_f17263_0()
{    return name != null ? name : getKindString();}
public String beam_f17264_0()
{    if (name == null) {        return getKindString();    } else {        return getName() + " [" + getKindString() + "]";    }}
public OutputT beam_f17275_0(InputT input)
{    return fn.apply(input);}
public static ByteBuddyDoFnInvokerFactory beam_f17276_0()
{    return INSTANCE;}
public DoFnInvoker<InputT, OutputT> beam_f17277_0(DoFn<InputT, OutputT> fn)
{    return newByteBuddyInvoker(fn);}
public Coder<RestrictionT> beam_f17285_0(CoderRegistry registry) throws CannotProvideCoderException
{    return (Coder) registry.getCoder(restrictionType);}
public static RestrictionTracker beam_f17286_0(Object restriction)
{    return ((HasDefaultTracker) restriction).newTracker();}
private static Class<? extends DoFnInvoker<?, ?>> beam_f17287_0(DoFnSignature signature)
{    Class<? extends DoFn<?, ?>> fnClass = signature.fnClass();    final TypeDescription clazzDescription = new TypeDescription.ForLoadedType(fnClass);    DynamicType.Builder<?> builder = new ByteBuddy().with(StableInvokerNamingStrategy.forDoFnClass(fnClass).withSuffix(DoFnInvoker.class.getSimpleName())).subclass(DoFnInvokerBase.class, ConstructorStrategy.Default.NO_CONSTRUCTORS).defineConstructor(Visibility.PUBLIC).withParameter(fnClass).intercept(new InvokerConstructor()).method(ElementMatchers.named("invokeProcessElement")).intercept(new ProcessElementDelegation(clazzDescription, signature.processElement())).method(ElementMatchers.named("invokeStartBundle")).intercept(delegateOrNoop(clazzDescription, signature.startBundle())).method(ElementMatchers.named("invokeFinishBundle")).intercept(delegateOrNoop(clazzDescription, signature.finishBundle())).method(ElementMatchers.named("invokeSetup")).intercept(delegateOrNoop(clazzDescription, signature.setup())).method(ElementMatchers.named("invokeTeardown")).intercept(delegateOrNoop(clazzDescription, signature.teardown())).method(ElementMatchers.named("invokeOnWindowExpiration")).intercept(delegateMethodWithExtraParametersOrNoop(clazzDescription, signature.onWindowExpiration())).method(ElementMatchers.named("invokeGetInitialRestriction")).intercept(delegateWithDowncastOrThrow(clazzDescription, signature.getInitialRestriction())).method(ElementMatchers.named("invokeSplitRestriction")).intercept(splitRestrictionDelegation(clazzDescription, signature)).method(ElementMatchers.named("invokeGetRestrictionCoder")).intercept(getRestrictionCoderDelegation(clazzDescription, signature)).method(ElementMatchers.named("invokeNewTracker")).intercept(newTrackerDelegation(clazzDescription, signature.newTracker()));    DynamicType.Unloaded<?> unloaded = builder.make();    @SuppressWarnings("unchecked")    Class<? extends DoFnInvoker<?, ?>> res = (Class<? extends DoFnInvoker<?, ?>>) unloaded.load(findClassLoader(fnClass.getClassLoader()), ClassLoadingStrategy.Default.INJECTION).getLoaded();    return res;}
public ByteCodeAppender beam_f17295_0(final Target implementationTarget)
{    return new ByteCodeAppender() {        /**         * @param instrumentedMethod The {@link DoFnInvoker} method for which we're generating code.         */        @Override        public Size apply(MethodVisitor methodVisitor, Context implementationContext, MethodDescription instrumentedMethod) {                                                int numLocals = 1 + instrumentedMethod.getParameters().size() + (targetHasReturn ? 1 : 0);            Integer returnVarIndex = null;            if (targetHasReturn) {                                                returnVarIndex = 1;                for (Type param : Type.getArgumentTypes(instrumentedMethod.getDescriptor())) {                    returnVarIndex += param.getSize();                }            }            StackManipulation manipulation = new StackManipulation.Compound(            MethodVariableAccess.REFERENCE.loadFrom(0),             FieldAccess.forField(delegateField).read(),             TypeCasting.to(doFnType),             beforeDelegation(instrumentedMethod),             new UserCodeMethodInvocation(returnVarIndex, targetMethod, instrumentedMethod),             afterDelegation(instrumentedMethod));            StackManipulation.Size size = manipulation.apply(methodVisitor, implementationContext);            return new Size(size.getMaximalSize(), numLocals);        }    };}
public Size beam_f17296_0(MethodVisitor methodVisitor, Context implementationContext, MethodDescription instrumentedMethod)
{                int numLocals = 1 + instrumentedMethod.getParameters().size() + (targetHasReturn ? 1 : 0);    Integer returnVarIndex = null;    if (targetHasReturn) {                        returnVarIndex = 1;        for (Type param : Type.getArgumentTypes(instrumentedMethod.getDescriptor())) {            returnVarIndex += param.getSize();        }    }    StackManipulation manipulation = new StackManipulation.Compound(    MethodVariableAccess.REFERENCE.loadFrom(0),     FieldAccess.forField(delegateField).read(),     TypeCasting.to(doFnType),     beforeDelegation(instrumentedMethod),     new UserCodeMethodInvocation(returnVarIndex, targetMethod, instrumentedMethod),     afterDelegation(instrumentedMethod));    StackManipulation.Size size = manipulation.apply(methodVisitor, implementationContext);    return new Size(size.getMaximalSize(), numLocals);}
protected StackManipulation beam_f17297_0(MethodDescription instrumentedMethod)
{    return MethodVariableAccess.allArgumentsOf(targetMethod);}
public StackManipulation beam_f17305_0(FinishBundleContextParameter p)
{    return new StackManipulation.Compound(pushDelegate, MethodInvocation.invoke(getExtraContextFactoryMethodDescription(FINISH_BUNDLE_CONTEXT_PARAMETER_METHOD, DoFn.class)));}
public StackManipulation beam_f17306_0(ProcessContextParameter p)
{    return new StackManipulation.Compound(pushDelegate, MethodInvocation.invoke(getExtraContextFactoryMethodDescription(PROCESS_CONTEXT_PARAMETER_METHOD, DoFn.class)));}
public StackManipulation beam_f17307_0(ElementParameter p)
{    return new StackManipulation.Compound(pushDelegate, MethodInvocation.invoke(getExtraContextFactoryMethodDescription(ELEMENT_PARAMETER_METHOD, DoFn.class)), TypeCasting.to(new TypeDescription.ForLoadedType(p.elementT().getRawType())));}
public StackManipulation beam_f17315_0(PaneInfoParameter p)
{    return new StackManipulation.Compound(pushDelegate, MethodInvocation.invoke(getExtraContextFactoryMethodDescription(PANE_INFO_PARAMETER_METHOD, DoFn.class)));}
public StackManipulation beam_f17316_0(RestrictionTrackerParameter p)
{        return new StackManipulation.Compound(simpleExtraContextParameter(RESTRICTION_TRACKER_PARAMETER_METHOD), TypeCasting.to(new TypeDescription.ForLoadedType(p.trackerT().getRawType())));}
public StackManipulation beam_f17317_0(StateParameter p)
{    return new StackManipulation.Compound(new TextConstant(p.referent().id()), MethodInvocation.invoke(getExtraContextFactoryMethodDescription(STATE_PARAMETER_METHOD, String.class)), TypeCasting.to(new TypeDescription.ForLoadedType(p.referent().stateType().getRawType())));}
public Size beam_f17325_0(MethodVisitor mv, Context context)
{    Size size = new Size(0, 0);    mv.visitLabel(wrapStart);    String throwableName = new TypeDescription.ForLoadedType(Throwable.class).getInternalName();    mv.visitTryCatchBlock(tryBlockStart, tryBlockEnd, catchBlockStart, throwableName);        mv.visitLabel(tryBlockStart);    size = size.aggregate(MethodInvocation.invoke(targetMethod).apply(mv, context));    if (returnVarIndex != null) {        mv.visitVarInsn(Opcodes.ASTORE, returnVarIndex);                size = size.aggregate(new Size(-1, 0));    }    mv.visitJumpInsn(Opcodes.GOTO, catchBlockEnd);    mv.visitLabel(tryBlockEnd);        mv.visitLabel(catchBlockStart);        visitFrame(mv, false, throwableName);        size = size.aggregate(new Compound(MethodInvocation.invoke(createUserCodeException), Throw.INSTANCE).apply(mv, context));    mv.visitLabel(catchBlockEnd);        visitFrame(mv, true, null);        if (returnVarIndex != null) {        mv.visitVarInsn(Opcodes.ALOAD, returnVarIndex);                size = size.aggregate(new Size(1, 0));    }    mv.visitLabel(wrapEnd);    if (returnVarIndex != null) {                mv.visitLocalVariable("res", returnType.getDescriptor(), returnType.getGenericSignature(), wrapStart, wrapEnd, returnVarIndex);    }    return size;}
public InstrumentedType beam_f17326_0(InstrumentedType instrumentedType)
{    return instrumentedType;}
public ByteCodeAppender beam_f17327_0(final Target implementationTarget)
{    return (methodVisitor, implementationContext, instrumentedMethod) -> {        StackManipulation.Size size = new StackManipulation.Compound(        MethodVariableAccess.REFERENCE.loadFrom(0),         MethodVariableAccess.REFERENCE.loadFrom(1),         MethodInvocation.invoke(new TypeDescription.ForLoadedType(DoFnInvokerBase.class).getDeclaredMethods().filter(ElementMatchers.isConstructor().and(ElementMatchers.takesArguments(DoFn.class))).getOnly()),         MethodReturn.VOID).apply(methodVisitor, implementationContext);        return new ByteCodeAppender.Size(size.getMaximalSize(), instrumentedMethod.getStackSize());    };}
public DoFn<InputT, OutputT>.ProcessContext beam_f17335_0(DoFn<InputT, OutputT> doFn)
{    throw new UnsupportedOperationException(String.format("Should never call non-overridden methods of %s", FakeArgumentProvider.class.getSimpleName()));}
public InputT beam_f17336_0(DoFn<InputT, OutputT> doFn)
{    throw new UnsupportedOperationException(String.format("Should never call non-overridden methods of %s", FakeArgumentProvider.class.getSimpleName()));}
public InputT beam_f17337_0(String tagId)
{    throw new UnsupportedOperationException(String.format("Should never call non-overridden methods of %s", FakeArgumentProvider.class.getSimpleName()));}
public PaneInfo beam_f17345_0(DoFn<InputT, OutputT> doFn)
{    throw new UnsupportedOperationException(String.format("Should never call non-overridden methods of %s", FakeArgumentProvider.class.getSimpleName()));}
public PipelineOptions beam_f17346_0()
{    throw new UnsupportedOperationException(String.format("Should never call non-overridden methods of %s", FakeArgumentProvider.class.getSimpleName()));}
public DoFn<InputT, OutputT>.StartBundleContext beam_f17347_0(DoFn<InputT, OutputT> doFn)
{    throw new UnsupportedOperationException(String.format("Should never call non-overridden methods of %s", FakeArgumentProvider.class.getSimpleName()));}
public boolean beam_f17355_0()
{    return stateDeclarations().size() > 0;}
public boolean beam_f17356_0()
{    return stateDeclarations().size() > 0;}
public boolean beam_f17357_0()
{    return timerDeclarations().size() > 0;}
public ResultT beam_f17365_0(TaggedOutputReceiverParameter p)
{    return dispatchDefault(p);}
public ResultT beam_f17366_0(OutputReceiverParameter p)
{    return dispatchDefault(p);}
public ResultT beam_f17367_0(TimestampParameter p)
{    return dispatchDefault(p);}
public ResultT beam_f17375_0(PipelineOptionsParameter p)
{    return dispatchDefault(p);}
public ResultT beam_f17376_0(SideInputParameter p)
{    return dispatchDefault(p);}
public static ProcessContextParameter beam_f17377_0()
{    return PROCESS_CONTEXT_PARAMETER;}
public static OnTimerContextParameter beam_f17385_0()
{    return ON_TIMER_CONTEXT_PARAMETER;}
public static PaneInfoParameter beam_f17386_0()
{    return PANE_INFO_PARAMETER;}
public static WindowParameter beam_f17387_0(TypeDescriptor<? extends BoundedWindow> windowT)
{    return new AutoValue_DoFnSignature_Parameter_WindowParameter(windowT);}
public List<SideInputParameter> beam_f17395_0()
{    return extraParameters().stream().filter(Predicates.instanceOf(SideInputParameter.class)::apply).map(SideInputParameter.class::cast).collect(Collectors.toList());}
public OutputReceiverParameter beam_f17396_0()
{    Optional<Parameter> parameter = extraParameters().stream().filter(Predicates.instanceOf(OutputReceiverParameter.class)::apply).findFirst();    return parameter.isPresent() ? ((OutputReceiverParameter) parameter.get()) : null;}
public boolean beam_f17397_0()
{    return extraParameters().stream().anyMatch(Predicates.instanceOf(RestrictionTrackerParameter.class)::apply);}
 static GetInitialRestrictionMethod beam_f17405_0(Method targetMethod, TypeDescriptor<?> restrictionT)
{    return new AutoValue_DoFnSignature_GetInitialRestrictionMethod(targetMethod, restrictionT);}
 static SplitRestrictionMethod beam_f17406_0(Method targetMethod, TypeDescriptor<?> restrictionT)
{    return new AutoValue_DoFnSignature_SplitRestrictionMethod(targetMethod, restrictionT);}
 static NewTrackerMethod beam_f17407_0(Method targetMethod, TypeDescriptor<?> restrictionT, TypeDescriptor<?> trackerT)
{    return new AutoValue_DoFnSignature_NewTrackerMethod(targetMethod, restrictionT, trackerT);}
public void beam_f17415_0(StateDeclaration decl)
{    stateDeclarations.put(decl.id(), decl);}
public void beam_f17416_0(Iterable<StateDeclaration> decls)
{    for (StateDeclaration decl : decls) {        addStateDeclaration(decl);    }}
public void beam_f17417_0(TimerDeclaration decl)
{    timerDeclarations.put(decl.id(), decl);}
public Map<String, StateParameter> beam_f17425_0()
{    return Collections.unmodifiableMap(stateParameters);}
public Map<String, TimerParameter> beam_f17426_0()
{    return Collections.unmodifiableMap(timerParameters);}
public List<Parameter> beam_f17427_0()
{    return Collections.unmodifiableList(extraParameters);}
private static PCollection.IsBounded beam_f17435_0(TypeDescriptor<? extends DoFn> fnT, DoFnSignature.ProcessElementMethod processElement, ErrorReporter errors)
{    PCollection.IsBounded isBounded = null;    for (TypeDescriptor<?> supertype : fnT.getTypes()) {        if (supertype.getRawType().isAnnotationPresent(DoFn.BoundedPerElement.class) || supertype.getRawType().isAnnotationPresent(DoFn.UnboundedPerElement.class)) {            errors.checkArgument(isBounded == null, "Both @%s and @%s specified", DoFn.BoundedPerElement.class.getSimpleName(), DoFn.UnboundedPerElement.class.getSimpleName());            isBounded = supertype.getRawType().isAnnotationPresent(DoFn.BoundedPerElement.class) ? PCollection.IsBounded.BOUNDED : PCollection.IsBounded.UNBOUNDED;        }    }    if (processElement.isSplittable()) {        if (isBounded == null) {            isBounded = processElement.hasReturnValue() ? PCollection.IsBounded.UNBOUNDED : PCollection.IsBounded.BOUNDED;        }    } else {        errors.checkArgument(isBounded == null, "Non-splittable, but annotated as @" + ((isBounded == PCollection.IsBounded.BOUNDED) ? DoFn.BoundedPerElement.class.getSimpleName() : DoFn.UnboundedPerElement.class.getSimpleName()));        checkState(!processElement.hasReturnValue(), "Should have been inferred splittable");        isBounded = PCollection.IsBounded.BOUNDED;    }    return isBounded;}
private static void beam_f17436_0(DoFnSignature signature, ErrorReporter errors)
{    DoFnSignature.ProcessElementMethod processElement = signature.processElement();    DoFnSignature.GetInitialRestrictionMethod getInitialRestriction = signature.getInitialRestriction();    DoFnSignature.NewTrackerMethod newTracker = signature.newTracker();    DoFnSignature.GetRestrictionCoderMethod getRestrictionCoder = signature.getRestrictionCoder();    DoFnSignature.SplitRestrictionMethod splitRestriction = signature.splitRestriction();    ErrorReporter processElementErrors = errors.forMethod(DoFn.ProcessElement.class, processElement.targetMethod());    List<String> missingRequiredMethods = new ArrayList<>();    if (getInitialRestriction == null) {        missingRequiredMethods.add("@" + DoFn.GetInitialRestriction.class.getSimpleName());    }    if (newTracker == null) {        if (getInitialRestriction != null && getInitialRestriction.restrictionT().isSubtypeOf(TypeDescriptor.of(HasDefaultTracker.class))) {                } else {            missingRequiredMethods.add("@" + DoFn.NewTracker.class.getSimpleName());        }    } else {        ErrorReporter getInitialRestrictionErrors = errors.forMethod(DoFn.GetInitialRestriction.class, getInitialRestriction.targetMethod());        TypeDescriptor<?> restrictionT = getInitialRestriction.restrictionT();        getInitialRestrictionErrors.checkArgument(restrictionT.equals(newTracker.restrictionT()), "Uses restriction type %s, but @%s method %s uses restriction type %s", formatType(restrictionT), DoFn.NewTracker.class.getSimpleName(), format(newTracker.targetMethod()), formatType(newTracker.restrictionT()));    }    if (!missingRequiredMethods.isEmpty()) {        processElementErrors.throwIllegalArgument("Splittable, but does not define the following required methods: %s", missingRequiredMethods);    }    ErrorReporter getInitialRestrictionErrors = errors.forMethod(DoFn.GetInitialRestriction.class, getInitialRestriction.targetMethod());    TypeDescriptor<?> restrictionT = getInitialRestriction.restrictionT();    processElementErrors.checkArgument(processElement.trackerT().getRawType().equals(RestrictionTracker.class), "Has tracker type %s, but the DoFn's tracker type must be of type RestrictionTracker.", formatType(processElement.trackerT()));    if (getRestrictionCoder != null) {        getInitialRestrictionErrors.checkArgument(getRestrictionCoder.coderT().isSubtypeOf(coderTypeOf(restrictionT)), "Uses restriction type %s, but @%s method %s returns %s " + "which is not a subtype of %s", formatType(restrictionT), DoFn.GetRestrictionCoder.class.getSimpleName(), format(getRestrictionCoder.targetMethod()), formatType(getRestrictionCoder.coderT()), formatType(coderTypeOf(restrictionT)));    }    if (splitRestriction != null) {        getInitialRestrictionErrors.checkArgument(splitRestriction.restrictionT().equals(restrictionT), "Uses restriction type %s, but @%s method %s uses restriction type %s", formatType(restrictionT), DoFn.SplitRestriction.class.getSimpleName(), format(splitRestriction.targetMethod()), formatType(splitRestriction.restrictionT()));    }}
private static void beam_f17437_0(ErrorReporter errors, DoFnSignature signature)
{    List<String> forbiddenMethods = new ArrayList<>();    if (signature.getInitialRestriction() != null) {        forbiddenMethods.add("@" + DoFn.GetInitialRestriction.class.getSimpleName());    }    if (signature.splitRestriction() != null) {        forbiddenMethods.add("@" + DoFn.SplitRestriction.class.getSimpleName());    }    if (signature.newTracker() != null) {        forbiddenMethods.add("@" + DoFn.NewTracker.class.getSimpleName());    }    if (signature.getRestrictionCoder() != null) {        forbiddenMethods.add("@" + DoFn.GetRestrictionCoder.class.getSimpleName());    }    errors.checkArgument(forbiddenMethods.isEmpty(), "Non-splittable, but defines methods: %s", forbiddenMethods);}
private static void beam_f17445_0(ErrorReporter errors, Parameter parameter, Collection<Class<? extends Parameter>> allowedParameterClasses)
{    for (Class<? extends Parameter> paramClass : allowedParameterClasses) {        if (paramClass.isAssignableFrom(parameter.getClass())) {            return;        }    }        errors.throwIllegalArgument("Illegal parameter type: %s", parameter);}
private static Parameter beam_f17446_0(ErrorReporter methodErrors, FnAnalysisContext fnContext, MethodAnalysisContext methodContext, TypeDescriptor<? extends DoFn<?, ?>> fnClass, ParameterDescription param, TypeDescriptor<?> inputT, TypeDescriptor<?> outputT)
{    TypeDescriptor<?> expectedProcessContextT = doFnProcessContextTypeOf(inputT, outputT);    TypeDescriptor<?> expectedOnTimerContextT = doFnOnTimerContextTypeOf(inputT, outputT);    TypeDescriptor<?> paramT = param.getType();    Class<?> rawType = paramT.getRawType();    ErrorReporter paramErrors = methodErrors.forParameter(param);    String fieldAccessString = getFieldAccessId(param.getAnnotations());    if (fieldAccessString != null) {        return Parameter.schemaElementParameter(paramT, fieldAccessString, param.getIndex());    } else if (hasElementAnnotation(param.getAnnotations())) {        return (paramT.equals(inputT)) ? Parameter.elementParameter(paramT) : Parameter.schemaElementParameter(paramT, null, param.getIndex());    } else if (hasTimestampAnnotation(param.getAnnotations())) {        methodErrors.checkArgument(rawType.equals(Instant.class), "@Timestamp argument must have type org.joda.time.Instant.");        return Parameter.timestampParameter();    } else if (rawType.equals(TimeDomain.class)) {        return Parameter.timeDomainParameter();    } else if (hasSideInputAnnotation(param.getAnnotations())) {        String sideInputId = getSideInputId(param.getAnnotations());        paramErrors.checkArgument(sideInputId != null, "%s missing %s annotation", SideInput.class.getSimpleName());        return Parameter.sideInputParameter(paramT, sideInputId);    } else if (rawType.equals(PaneInfo.class)) {        return Parameter.paneInfoParameter();    } else if (rawType.equals(DoFn.ProcessContext.class)) {        paramErrors.checkArgument(paramT.equals(expectedProcessContextT), "ProcessContext argument must have type %s", formatType(expectedProcessContextT));        return Parameter.processContext();    } else if (rawType.equals(DoFn.OnTimerContext.class)) {        paramErrors.checkArgument(paramT.equals(expectedOnTimerContextT), "OnTimerContext argument must have type %s", formatType(expectedOnTimerContextT));        return Parameter.onTimerContext();    } else if (BoundedWindow.class.isAssignableFrom(rawType)) {        methodErrors.checkArgument(!methodContext.hasWindowParameter(), "Multiple %s parameters", BoundedWindow.class.getSimpleName());        return Parameter.boundedWindow((TypeDescriptor<? extends BoundedWindow>) paramT);    } else if (rawType.equals(OutputReceiver.class)) {                        boolean schemaRowReceiver = paramT.equals(outputReceiverTypeOf(TypeDescriptor.of(Row.class))) && !outputT.equals(TypeDescriptor.of(Row.class));        if (!schemaRowReceiver) {            TypeDescriptor<?> expectedReceiverT = outputReceiverTypeOf(outputT);            paramErrors.checkArgument(paramT.equals(expectedReceiverT), "OutputReceiver should be parameterized by %s", outputT);        }        return Parameter.outputReceiverParameter(schemaRowReceiver);    } else if (rawType.equals(MultiOutputReceiver.class)) {        return Parameter.taggedOutputReceiverParameter();    } else if (PipelineOptions.class.equals(rawType)) {        methodErrors.checkArgument(!methodContext.hasPipelineOptionsParamter(), "Multiple %s parameters", PipelineOptions.class.getSimpleName());        return Parameter.pipelineOptions();    } else if (RestrictionTracker.class.isAssignableFrom(rawType)) {        methodErrors.checkArgument(!methodContext.hasRestrictionTrackerParameter(), "Multiple %s parameters", RestrictionTracker.class.getSimpleName());        return Parameter.restrictionTracker(paramT);    } else if (rawType.equals(Timer.class)) {                String id = getTimerId(param.getAnnotations());        paramErrors.checkArgument(id != null, "%s missing %s annotation", Timer.class.getSimpleName(), TimerId.class.getSimpleName());        paramErrors.checkArgument(!methodContext.getTimerParameters().containsKey(id), "duplicate %s: \"%s\"", TimerId.class.getSimpleName(), id);        TimerDeclaration timerDecl = fnContext.getTimerDeclarations().get(id);        paramErrors.checkArgument(timerDecl != null, "reference to undeclared %s: \"%s\"", TimerId.class.getSimpleName(), id);        paramErrors.checkArgument(timerDecl.field().getDeclaringClass().equals(getDeclaringClass(param.getMethod())), "%s %s declared in a different class %s." + " Timers may be referenced only in the lexical scope where they are declared.", TimerId.class.getSimpleName(), id, timerDecl.field().getDeclaringClass().getName());        return Parameter.timerParameter(timerDecl);    } else if (State.class.isAssignableFrom(rawType)) {                String id = getStateId(param.getAnnotations());        paramErrors.checkArgument(id != null, "missing %s annotation", DoFn.StateId.class.getSimpleName());        paramErrors.checkArgument(!methodContext.getStateParameters().containsKey(id), "duplicate %s: \"%s\"", DoFn.StateId.class.getSimpleName(), id);                TypeDescriptor<? extends State> stateType = (TypeDescriptor<? extends State>) param.getType();        StateDeclaration stateDecl = fnContext.getStateDeclarations().get(id);        paramErrors.checkArgument(stateDecl != null, "reference to undeclared %s: \"%s\"", DoFn.StateId.class.getSimpleName(), id);        paramErrors.checkArgument(stateDecl.stateType().isSubtypeOf(stateType), "data type of reference to %s %s must be a supertype of %s", StateId.class.getSimpleName(), id, formatType(stateDecl.stateType()));        paramErrors.checkArgument(stateDecl.field().getDeclaringClass().equals(getDeclaringClass(param.getMethod())), "%s %s declared in a different class %s." + " State may be referenced only in the class where it is declared.", StateId.class.getSimpleName(), id, stateDecl.field().getDeclaringClass().getName());        return Parameter.stateParameter(stateDecl);    } else {        List<String> allowedParamTypes = Arrays.asList(formatType(new TypeDescriptor<BoundedWindow>() {        }), formatType(new TypeDescriptor<RestrictionTracker<?, ?>>() {        }));        paramErrors.throwIllegalArgument("%s is not a valid context parameter. Should be one of %s", formatType(paramT), allowedParamTypes);                return null;    }}
private static String beam_f17447_0(List<Annotation> annotations)
{    DoFn.TimerId stateId = findFirstOfType(annotations, DoFn.TimerId.class);    return stateId != null ? stateId.value() : null;}
private static TypeDescriptor<?> beam_f17455_0(TypeDescriptor<?> fnClass, Method method)
{    Type[] params = method.getGenericParameterTypes();    for (Type param : params) {        TypeDescriptor<?> paramT = fnClass.resolveType(param);        if (RestrictionTracker.class.isAssignableFrom(paramT.getRawType())) {            return paramT;        }    }    return null;}
private static TypeDescriptor<? extends BoundedWindow> beam_f17456_0(TypeDescriptor<?> fnClass, Method method)
{    Type[] params = method.getGenericParameterTypes();    for (Type param : params) {        TypeDescriptor<?> paramT = fnClass.resolveType(param);        if (BoundedWindow.class.isAssignableFrom(paramT.getRawType())) {            return (TypeDescriptor<? extends BoundedWindow>) paramT;        }    }    return null;}
 static DoFnSignature.BundleMethod beam_f17457_0(ErrorReporter errors, TypeDescriptor<? extends DoFn<?, ?>> fnT, Method m, TypeDescriptor<?> inputT, TypeDescriptor<?> outputT)
{    errors.checkArgument(void.class.equals(m.getReturnType()), "Must return void");    TypeDescriptor<?> expectedContextT = doFnStartBundleContextTypeOf(inputT, outputT);    Type[] params = m.getGenericParameterTypes();    errors.checkArgument(params.length == 0 || (params.length == 1 && fnT.resolveType(params[0]).equals(expectedContextT)), "Must take a single argument of type %s", formatType(expectedContextT));    return DoFnSignature.BundleMethod.create(m);}
private static TypeDescriptor<Coder<T>> beam_f17465_0(TypeDescriptor<T> elementT)
{    return new TypeDescriptor<Coder<T>>() {    }.where(new TypeParameter<T>() {    }, elementT);}
 static DoFnSignature.GetRestrictionCoderMethod beam_f17466_0(ErrorReporter errors, TypeDescriptor<? extends DoFn> fnT, Method m)
{    errors.checkArgument(m.getParameterTypes().length == 0, "Must have zero arguments");    TypeDescriptor<?> resT = fnT.resolveType(m.getGenericReturnType());    errors.checkArgument(resT.isSubtypeOf(TypeDescriptor.of(Coder.class)), "Must return a Coder, but returns %s", formatType(resT));    return DoFnSignature.GetRestrictionCoderMethod.create(m, resT);}
private static TypeDescriptor<RestrictionTracker<RestrictionT, ?>> beam_f17467_0(TypeDescriptor<RestrictionT> restrictionT)
{    return new TypeDescriptor<RestrictionTracker<RestrictionT, ?>>() {    }.where(new TypeParameter<RestrictionT>() {    }, restrictionT);}
private static String beam_f17475_0(Method method)
{    return ReflectHelpers.METHOD_FORMATTER.apply(method);}
private static String beam_f17476_0(TypeDescriptor<?> t)
{    return ReflectHelpers.TYPE_SIMPLE_DESCRIPTION.apply(t.getType());}
 ErrorReporter beam_f17477_0(Class<? extends Annotation> annotation, Method method)
{    return new ErrorReporter(this, String.format("@%s %s", annotation.getSimpleName(), (method == null) ? "(absent)" : format(method)));}
public static OnTimerMethodSpecifier beam_f17485_0(Class<? extends DoFn<?, ?>> fnClass, String timerId)
{    return new AutoValue_OnTimerMethodSpecifier(fnClass, timerId);}
public static StableInvokerNamingStrategy beam_f17486_0(Class<? extends DoFn<?, ?>> fnClass)
{    return new AutoValue_StableInvokerNamingStrategy(fnClass, null);}
public StableInvokerNamingStrategy beam_f17487_0(String newSuffix)
{    return new AutoValue_StableInvokerNamingStrategy(getFnClass(), newSuffix);}
public static AllMatches beam_f17495_0(String regex)
{    return allMatches(Pattern.compile(regex));}
public static AllMatches beam_f17496_0(Pattern pattern)
{    return new AllMatches(pattern);}
public static MatchesKV beam_f17497_0(String regex, int keyGroup, int valueGroup)
{    return matchesKV(Pattern.compile(regex), keyGroup, valueGroup);}
public static FindName beam_f17505_0(String regex, String groupName)
{    return find(Pattern.compile(regex), groupName);}
public static FindName beam_f17506_0(Pattern pattern, String groupName)
{    return new FindName(pattern, groupName);}
public static FindAll beam_f17507_0(String regex)
{    return findAll(Pattern.compile(regex));}
public static ReplaceFirst beam_f17515_0(String regex, String replacement)
{    return replaceFirst(Pattern.compile(regex), replacement);}
public static ReplaceFirst beam_f17516_0(Pattern pattern, String replacement)
{    return new ReplaceFirst(pattern, replacement);}
public static Split beam_f17517_0(String regex)
{    return split(Pattern.compile(regex), false);}
public PCollection<List<String>> beam_f17525_0(PCollection<String> in)
{    return in.apply(ParDo.of(new DoFn<String, List<String>>() {        @ProcessElement        public void processElement(@Element String element, OutputReceiver<List<String>> r) throws Exception {            Matcher m = pattern.matcher(element);            if (m.matches()) {                ArrayList list = new ArrayList(m.groupCount());                                for (int i = 0; i < m.groupCount() + 1; i++) {                    list.add(m.group(i));                }                r.output(list);            }        }    }));}
public void beam_f17526_0(@Element String element, OutputReceiver<List<String>> r) throws Exception
{    Matcher m = pattern.matcher(element);    if (m.matches()) {        ArrayList list = new ArrayList(m.groupCount());                for (int i = 0; i < m.groupCount() + 1; i++) {            list.add(m.group(i));        }        r.output(list);    }}
public PCollection<KV<String, String>> beam_f17527_0(PCollection<String> in)
{    return in.apply(ParDo.of(new DoFn<String, KV<String, String>>() {        @ProcessElement        public void processElement(@Element String element, OutputReceiver<KV<String, String>> r) throws Exception {            Matcher m = pattern.matcher(element);            if (m.find()) {                r.output(KV.of(m.group(keyGroup), m.group(valueGroup)));            }        }    }));}
public PCollection<List<String>> beam_f17535_0(PCollection<String> in)
{    return in.apply(ParDo.of(new DoFn<String, List<String>>() {        @ProcessElement        public void processElement(@Element String element, OutputReceiver<List<String>> r) throws Exception {            Matcher m = pattern.matcher(element);            if (m.find()) {                ArrayList list = new ArrayList(m.groupCount());                                for (int i = 0; i < m.groupCount() + 1; i++) {                    list.add(m.group(i));                }                r.output(list);            }        }    }));}
public void beam_f17536_0(@Element String element, OutputReceiver<List<String>> r) throws Exception
{    Matcher m = pattern.matcher(element);    if (m.find()) {        ArrayList list = new ArrayList(m.groupCount());                for (int i = 0; i < m.groupCount() + 1; i++) {            list.add(m.group(i));        }        r.output(list);    }}
public PCollection<KV<String, String>> beam_f17537_0(PCollection<String> in)
{    return in.apply(ParDo.of(new DoFn<String, KV<String, String>>() {        @ProcessElement        public void processElement(@Element String element, OutputReceiver<KV<String, String>> r) throws Exception {            Matcher m = pattern.matcher(element);            if (m.find()) {                r.output(KV.of(m.group(keyGroup), m.group(valueGroup)));            }        }    }));}
public PCollection<String> beam_f17545_0(PCollection<String> in)
{    return in.apply(ParDo.of(new DoFn<String, String>() {        @ProcessElement        public void processElement(@Element String element, OutputReceiver<String> r) throws Exception {            String[] items = pattern.split(element);            for (String item : items) {                if (outputEmpty || !item.isEmpty()) {                    r.output(item);                }            }        }    }));}
public void beam_f17546_0(@Element String element, OutputReceiver<String> r) throws Exception
{    String[] items = pattern.split(element);    for (String item : items) {        if (outputEmpty || !item.isEmpty()) {            r.output(item);        }    }}
public PCollection<KV<K, V>> beam_f17547_0(PCollection<K> input)
{    return input.apply(ParDo.of(new DoFn<K, KV<K, V>>() {        @ProcessElement        public void process(ProcessContext c) {            c.output(KV.of(c.element(), c.sideInput(view)));        }    }).withSideInputs(view)).setCoder(KvCoder.of(input.getCoder(), coder));}
public void beam_f17555_0(@Element KV<K, V> element, @Timestamp Instant timestamp, BoundedWindow window, PaneInfo pane, OutputReceiver<KV<K, ValueInSingleWindow<V>>> r)
{    r.output(KV.of(element.getKey(), ValueInSingleWindow.of(element.getValue(), timestamp, window, pane)));}
public PCollection<KV<K, TimestampedValue<V>>> beam_f17556_0(PCollection<KV<K, V>> input)
{    KvCoder<K, V> coder = (KvCoder<K, V>) input.getCoder();    return input.apply(ParDo.of(new DoFn<KV<K, V>, KV<K, TimestampedValue<V>>>() {        @ProcessElement        public void processElement(@Element KV<K, V> element, @Timestamp Instant timestamp, OutputReceiver<KV<K, TimestampedValue<V>>> r) {            r.output(KV.of(element.getKey(), TimestampedValue.of(element.getValue(), timestamp)));        }    })).setCoder(KvCoder.of(coder.getKeyCoder(), TimestampedValueCoder.of(coder.getValueCoder())));}
public void beam_f17557_0(@Element KV<K, V> element, @Timestamp Instant timestamp, OutputReceiver<KV<K, TimestampedValue<V>>> r)
{    r.output(KV.of(element.getKey(), TimestampedValue.of(element.getValue(), timestamp)));}
public static PTransform<PCollection<KV<K, TimestampedValue<V>>>, PCollection<KV<K, V>>> beam_f17565_0()
{    return new ExtractTimestampsFromValues<>();}
public static PTransform<PCollection<K>, PCollection<KV<K, V>>> beam_f17566_0(PCollectionView<V> view, Coder<V> coder)
{    return new ReifyView<>(view, coder);}
public static PTransform<PBegin, PCollection<V>> beam_f17567_0(PCollectionView<V> view, Coder<V> coder)
{    return new ReifyViewInGlobalWindow<>(view, coder);}
public static Requirements beam_f17575_0(Collection<PCollectionView<?>> sideInputs)
{    return new Requirements(sideInputs);}
public static Requirements beam_f17576_0(PCollectionView<?>... sideInputs)
{    return requiresSideInputs(Arrays.asList(sideInputs));}
public static Requirements beam_f17577_0()
{    return new Requirements(Collections.emptyList());}
public void beam_f17585_0()
{    shard = ThreadLocalRandom.current().nextInt();}
public void beam_f17586_0(@Element T element, OutputReceiver<KV<Integer, T>> r)
{    ++shard;                                    int hashOfShard = 0x1b873593 * Integer.rotateLeft(shard * 0xcc9e2d51, 15);    r.output(KV.of(hashOfShard, element));}
public static CombineFn<T, ?, Iterable<T>> beam_f17587_0(int sampleSize)
{    return new FixedSizedSampleFn<>(sampleSize);}
public void beam_f17595_0(DisplayData.Builder builder)
{    super.populateDisplayData(builder);    builder.add(DisplayData.item("sampleSize", sampleSize).withLabel("Sample Size"));}
public PCollection<KV<K, Iterable<V>>> beam_f17596_0(PCollection<KV<K, V>> input)
{    return input.apply(Combine.perKey(new FixedSizedSampleFn<>(sampleSize)));}
public void beam_f17597_0(DisplayData.Builder builder)
{    super.populateDisplayData(builder);    builder.add(DisplayData.item("sampleSize", sampleSize).withLabel("Sample Size"));}
public Iterable<T> beam_f17605_0(Top.BoundedHeap<KV<Integer, T>, SerializableComparator<KV<Integer, T>>> accumulator)
{    List<T> out = new ArrayList<>();    for (KV<Integer, T> element : accumulator.extractOutput()) {        out.add(element.getValue());    }    return out;}
public Coder<Top.BoundedHeap<KV<Integer, T>, SerializableComparator<KV<Integer, T>>>> beam_f17606_0(CoderRegistry registry, Coder<T> inputCoder)
{    return topCombineFn.getAccumulatorCoder(registry, KvCoder.of(BigEndianIntegerCoder.of(), inputCoder));}
public Coder<Iterable<T>> beam_f17607_0(CoderRegistry registry, Coder<T> inputCoder)
{    return IterableCoder.of(inputCoder);}
public static SimpleFunction<InputT, OutputT> beam_f17615_0(SerializableFunction<InputT, OutputT> fn, TypeDescriptor<OutputT> outputType)
{    return new SimpleFunctionWithOutputType<>(fn, outputType);}
public TypeDescriptor<OutputT> beam_f17616_0()
{    return outputType;}
public static ByteKeyRangeTracker beam_f17617_0(ByteKeyRange range)
{    return new ByteKeyRangeTracker(ByteKeyRange.of(range.getStartKey(), range.getEndKey()));}
public OffsetRange beam_f17625_0()
{    return range;}
public OffsetRange beam_f17626_0()
{    checkState(lastClaimedOffset != null, "Can't checkpoint before any offset was successfully claimed");    OffsetRange res = new OffsetRange(lastClaimedOffset + 1, range.getTo());    this.range = new OffsetRange(range.getFrom(), lastClaimedOffset + 1);    return res;}
public boolean beam_f17627_0(Long i)
{    checkArgument(lastAttemptedOffset == null || i > lastAttemptedOffset, "Trying to claim offset %s while last attempted was %s", i, lastAttemptedOffset);    checkArgument(i >= range.getFrom(), "Trying to claim offset %s before start of the range %s", i, range);    lastAttemptedOffset = i;        if (i >= range.getTo()) {        return false;    }    lastClaimedOffset = i;    return true;}
public static Combine.Globally<Double, Double> beam_f17635_0()
{    return Combine.globally(Sum.ofDoubles());}
public static Combine.PerKey<K, Double, Double> beam_f17636_0()
{    return Combine.perKey(Sum.ofDoubles());}
public static Combine.BinaryCombineIntegerFn beam_f17637_0()
{    return new SumIntegerFn();}
public long beam_f17645_0()
{    return 0;}
public boolean beam_f17646_0(Object other)
{    return other != null && other.getClass().equals(this.getClass());}
public int beam_f17647_0()
{    return getClass().hashCode();}
public static TopCombineFn<T, Natural<T>> beam_f17655_0(int count)
{    return new TopCombineFn<T, Natural<T>>(count, new Natural<T>()) {    };}
public static TopCombineFn<Long, Natural<Long>> beam_f17656_0(int count)
{    return new TopCombineFn<Long, Natural<Long>>(count, new Natural<Long>()) {    };}
public static TopCombineFn<Integer, Natural<Integer>> beam_f17657_0(int count)
{    return new TopCombineFn<Integer, Natural<Integer>>(count, new Natural<>()) {    };}
public static PerKey<K, V, List<V>> beam_f17665_0(int count)
{    return Combine.perKey(largestFn(count));}
public int beam_f17666_0(T a, T b)
{    return a.compareTo(b);}
public int beam_f17667_0(T a, T b)
{    return a.compareTo(b);}
public void beam_f17675_0(T value)
{    maybeAddInput(value);}
private boolean beam_f17676_0(T value)
{    if (maximumSize == 0) {                return false;    }        if (asQueue == null) {        asQueue = new PriorityQueue<>(maximumSize, compareFn);        for (T item : asList) {            asQueue.add(item);        }        asList = null;    }    if (asQueue.size() < maximumSize) {        asQueue.add(value);        return true;    } else if (compareFn.compare(value, asQueue.peek()) > 0) {        asQueue.poll();        asQueue.add(value);        return true;    } else {        return false;    }}
public void beam_f17677_0(BoundedHeap<T, ComparatorT> accumulator)
{    for (T value : accumulator.asList()) {        if (!maybeAddInput(value)) {                        break;        }    }}
public boolean beam_f17685_0(Object other)
{    if (other == this) {        return true;    }    if (!(other instanceof BoundedHeapCoder)) {        return false;    }    BoundedHeapCoder<?, ?> that = (BoundedHeapCoder<?, ?>) other;    return Objects.equals(this.compareFn, that.compareFn) && Objects.equals(this.listCoder, that.listCoder) && this.maximumSize == that.maximumSize;}
public int beam_f17686_0()
{    return Objects.hash(compareFn, listCoder, maximumSize);}
public static PTransform<PCollection<?>, PCollection<String>> beam_f17687_0()
{    return new Elements();}
public String beam_f17695_0(KV<?, ?> input)
{    return input.getKey().toString() + delimiter + input.getValue().toString();}
public PCollection<String> beam_f17696_0(PCollection<? extends Iterable<?>> input)
{    return input.apply(MapElements.via(new SimpleFunction<Iterable<?>, String>() {        @Override        public String apply(Iterable<?> input) {            return Joiner.on(delimiter).join(input);        }    }));}
public String beam_f17697_0(Iterable<?> input)
{    return Joiner.on(delimiter).join(input);}
public static AsMultimap<K, V> beam_f17705_0()
{    return new AsMultimap<>();}
public PCollectionView<List<T>> beam_f17706_0(PCollection<T> input)
{    try {        GroupByKey.applicableTo(input);    } catch (IllegalStateException e) {        throw new IllegalStateException("Unable to create a side-input view from input", e);    }    PCollection<KV<Void, T>> materializationInput = input.apply(new VoidKeyToMultimapMaterialization<>());    Coder<T> inputCoder = input.getCoder();    PCollectionView<List<T>> view = PCollectionViews.listView(materializationInput, (TypeDescriptorSupplier<T>) inputCoder::getEncodedTypeDescriptor, materializationInput.getWindowingStrategy());    materializationInput.apply(CreatePCollectionView.of(view));    return view;}
public PCollectionView<Iterable<T>> beam_f17707_0(PCollection<T> input)
{    try {        GroupByKey.applicableTo(input);    } catch (IllegalStateException e) {        throw new IllegalStateException("Unable to create a side-input view from input", e);    }    PCollection<KV<Void, T>> materializationInput = input.apply(new VoidKeyToMultimapMaterialization<>());    Coder<T> inputCoder = input.getCoder();    PCollectionView<Iterable<T>> view = PCollectionViews.iterableView(materializationInput, (TypeDescriptorSupplier<T>) inputCoder::getEncodedTypeDescriptor, materializationInput.getWindowingStrategy());    materializationInput.apply(CreatePCollectionView.of(view));    return view;}
public AsMap<K, V> beam_f17715_0()
{    return this;}
public PCollectionView<Map<K, V>> beam_f17716_0(PCollection<KV<K, V>> input)
{    try {        GroupByKey.applicableTo(input);    } catch (IllegalStateException e) {        throw new IllegalStateException("Unable to create a side-input view from input", e);    }    KvCoder<K, V> kvCoder = (KvCoder<K, V>) input.getCoder();    Coder<K> keyCoder = kvCoder.getKeyCoder();    Coder<V> valueCoder = kvCoder.getValueCoder();    PCollection<KV<Void, KV<K, V>>> materializationInput = input.apply(new VoidKeyToMultimapMaterialization<>());    PCollectionView<Map<K, V>> view = PCollectionViews.mapView(materializationInput, (TypeDescriptorSupplier<K>) keyCoder::getEncodedTypeDescriptor, (TypeDescriptorSupplier<V>) valueCoder::getEncodedTypeDescriptor, materializationInput.getWindowingStrategy());    materializationInput.apply(CreatePCollectionView.of(view));    return view;}
public void beam_f17717_0(@Element T element, OutputReceiver<KV<Void, T>> r)
{    r.output(KV.of((Void) null, element));}
public PCollectionView<?> beam_f17725_0(PCollection<?> input)
{    return expandTyped(input);}
private PCollectionView<?> beam_f17726_0(PCollection<SignalT> input)
{    return input.apply(Window.<SignalT>configure().triggering(Never.ever()).discardingFiredPanes()).apply(ParDo.of(new CollectWindowsFn<>())).apply(Sample.any(1)).apply(View.asList());}
public void beam_f17727_0()
{    windows = Sets.newHashSetWithExpectedSize(1);}
public PollResult<OutputT> beam_f17735_0(Instant watermark)
{    checkNotNull(watermark, "watermark");    return new PollResult<>(outputs, watermark);}
public PollResult<OutputT> beam_f17736_0(List<TimestampedValue<OutputT>> outputs)
{    checkNotNull(outputs);    return new PollResult<>(outputs, watermark);}
public String beam_f17737_0()
{    return MoreObjects.toStringHelper(this).add("watermark", watermark).add("outputs", outputs).toString();}
public static Never<InputT> beam_f17745_0()
{    return new Never<>();}
public static TerminationCondition<InputT, StateT> beam_f17746_0(TerminationCondition<?, StateT> condition)
{    return new IgnoreInput<>(condition);}
public static AfterTotalOf<InputT> beam_f17747_0(ReadableDuration timeSinceInput)
{    return afterTotalOf(SerializableFunctions.<InputT, ReadableDuration>constant(timeSinceInput));}
public Integer beam_f17755_0(Instant now, Integer state)
{    return state;}
public boolean beam_f17756_0(Instant now, Integer state)
{    return false;}
public String beam_f17757_0(Integer state)
{    return "Never";}
public KV<Instant, ReadableDuration> beam_f17765_0(Instant now, KV<Instant, ReadableDuration> state)
{    return state;}
public boolean beam_f17766_0(Instant now, KV<Instant, ReadableDuration> state)
{    return new Duration(state.getKey(), now).isLongerThan(state.getValue());}
public String beam_f17767_0(KV<Instant, ReadableDuration> state)
{    return "AfterTotalOf{" + "timeStarted=" + state.getKey() + ", maxTimeSinceInput=" + state.getValue() + '}';}
public KV<FirstStateT, SecondStateT> beam_f17775_0(Instant now, KV<FirstStateT, SecondStateT> state)
{    return KV.of(first.onSeenNewOutput(now, state.getKey()), second.onSeenNewOutput(now, state.getValue()));}
public boolean beam_f17776_0(Instant now, KV<FirstStateT, SecondStateT> state)
{    switch(operation) {        case OR:            return first.canStopPolling(now, state.getKey()) || second.canStopPolling(now, state.getValue());        case AND:            return first.canStopPolling(now, state.getKey()) && second.canStopPolling(now, state.getValue());        default:            throw new UnsupportedOperationException("Unexpected operation " + operation);    }}
public String beam_f17777_0(KV<FirstStateT, SecondStateT> state)
{    return operation + "{first=" + first.toString(state.getKey()) + ", second=" + second.toString(state.getValue()) + '}';}
public OffsetRangeTracker beam_f17785_0(OffsetRange restriction)
{    return restriction.newTracker();}
public Coder<OffsetRange> beam_f17786_0()
{    return OffsetRange.Coder.of();}
public ProcessContinuation beam_f17787_1(ProcessContext c, RestrictionTracker<GrowthState, KV<Growth.PollResult<OutputT>, TerminationStateT>> tracker) throws Exception
{    GrowthState currentRestriction = tracker.currentRestriction();    if (currentRestriction instanceof NonPollingGrowthState) {        Growth.PollResult<OutputT> priorPoll = ((NonPollingGrowthState<OutputT>) currentRestriction).getPending();        if (tracker.tryClaim(KV.of(priorPoll, null))) {            if (!priorPoll.getOutputs().isEmpty()) {                                c.output(KV.of(c.element(), priorPoll.getOutputs()));            }            if (priorPoll.getWatermark() != null) {                c.updateWatermark(priorPoll.getWatermark());            }        }        return stop();    }        Instant now = Instant.now();    Growth.PollResult<OutputT> res = spec.getPollFn().getClosure().apply(c.element(), wrapProcessContext(c));    PollingGrowthState<TerminationStateT> pollingRestriction = (PollingGrowthState<TerminationStateT>) currentRestriction;        Growth.PollResult<OutputT> newResults = computeNeverSeenBeforeResults(pollingRestriction, res);                TerminationStateT terminationState = pollingRestriction.getTerminationState();    if (!newResults.getOutputs().isEmpty()) {        terminationState = getTerminationCondition().onSeenNewOutput(Instant.now(), terminationState);    }    if (!tracker.tryClaim(KV.of(newResults, terminationState))) {                return stop();    }    if (!newResults.getOutputs().isEmpty()) {        c.output(KV.of(c.element(), newResults.getOutputs()));    }    if (newResults.getWatermark() != null) {        c.updateWatermark(newResults.getWatermark());    }    Instant currentTime = Instant.now();    if (getTerminationCondition().canStopPolling(currentTime, terminationState)) {                return stop();    }    if (BoundedWindow.TIMESTAMP_MAX_VALUE.equals(newResults.getWatermark())) {                return stop();    }        return resume().withResumeDelay(spec.getPollInterval());}
public static PollingGrowthState<TerminationStateT> beam_f17795_0(TerminationStateT terminationState)
{    return new AutoValue_Watch_PollingGrowthState(ImmutableMap.of(), null, terminationState);}
public static PollingGrowthState<TerminationStateT> beam_f17796_0(ImmutableMap<HashCode, Instant> completed, Instant pollWatermark, TerminationStateT terminationState)
{    return new AutoValue_Watch_PollingGrowthState(completed, pollWatermark, terminationState);}
public GrowthState beam_f17797_0()
{    return state;}
public HashCode beam_f17805_0(InputStream is) throws IOException
{    byte[] res = new byte[16];    int numRead = is.read(res, 0, 16);    checkArgument(numRead == 16, "Expected to read 16 bytes, but read %s", numRead);    return HashCode.fromBytes(res);}
public static GrowthStateCoder<OutputT, TerminationStateT> beam_f17806_0(Coder<OutputT> outputCoder, Coder<TerminationStateT> terminationStateCoder)
{    return new GrowthStateCoder<>(outputCoder, terminationStateCoder);}
public void beam_f17807_0(GrowthState value, OutputStream os) throws IOException
{    if (value instanceof PollingGrowthState) {        VarInt.encode(POLLING_GROWTH_STATE, os);        encodePollingGrowthState((PollingGrowthState<TerminationStateT>) value, os);    } else if (value instanceof NonPollingGrowthState) {        VarInt.encode(NON_POLLING_GROWTH_STATE, os);        encodeNonPollingGrowthState((NonPollingGrowthState<OutputT>) value, os);    } else {        throw new IOException("Unknown growth state: " + value);    }}
public static AfterAll beam_f17815_0(OnceTrigger... triggers)
{    return new AfterAll(Arrays.asList(triggers));}
public static AfterAll beam_f17816_0(List<Trigger> triggers)
{    return new AfterAll(triggers);}
public Instant beam_f17817_0(BoundedWindow window)
{        Instant deadline = BoundedWindow.TIMESTAMP_MIN_VALUE;    for (Trigger subTrigger : subTriggers) {        Instant subDeadline = subTrigger.getWatermarkThatGuaranteesFiring(window);        if (deadline.isBefore(subDeadline)) {            deadline = subDeadline;        }    }    return deadline;}
public static AfterFirst beam_f17825_0(OnceTrigger... triggers)
{    return new AfterFirst(Arrays.asList(triggers));}
public static AfterFirst beam_f17826_0(List<Trigger> triggers)
{    return new AfterFirst(triggers);}
public Instant beam_f17827_0(BoundedWindow window)
{        Instant deadline = BoundedWindow.TIMESTAMP_MAX_VALUE;    for (Trigger subTrigger : subTriggers) {        Instant subDeadline = subTrigger.getWatermarkThatGuaranteesFiring(window);        if (deadline.isAfter(subDeadline)) {            deadline = subDeadline;        }    }    return deadline;}
public String beam_f17835_0()
{    return "AfterPane.elementCountAtLeast(" + countElems + ")";}
public boolean beam_f17836_0(Object obj)
{    if (this == obj) {        return true;    }    if (!(obj instanceof AfterPane)) {        return false;    }    AfterPane that = (AfterPane) obj;    return this.countElems == that.countElems;}
public int beam_f17837_0()
{    return Objects.hash(countElems);}
protected Trigger beam_f17845_0(List<Trigger> continuationTriggers)
{    return AfterSynchronizedProcessingTime.ofFirstElement();}
public String beam_f17846_0()
{    StringBuilder builder = new StringBuilder("AfterProcessingTime.pastFirstElementInPane()");    for (TimestampTransform transform : getTimestampTransforms()) {        if (transform instanceof TimestampTransform.Delay) {            TimestampTransform.Delay delay = (TimestampTransform.Delay) transform;            builder.append(".plusDelayOf(").append(DURATION_FORMATTER.print(delay.getDelay().toPeriod())).append(")");        } else if (transform instanceof TimestampTransform.AlignTo) {            TimestampTransform.AlignTo alignTo = (TimestampTransform.AlignTo) transform;            builder.append(".alignedTo(").append(DURATION_FORMATTER.print(alignTo.getPeriod().toPeriod())).append(", ").append(alignTo.getOffset()).append(")");        }    }    return builder.toString();}
public boolean beam_f17847_0(Object obj)
{    if (this == obj) {        return true;    }    if (!(obj instanceof AfterProcessingTime)) {        return false;    }    AfterProcessingTime that = (AfterProcessingTime) obj;    return getTimestampTransforms().equals(that.getTimestampTransforms());}
public static FromEndOfWindow beam_f17855_0()
{    return new FromEndOfWindow();}
public OnceTrigger beam_f17856_0()
{    return earlyTrigger;}
public OnceTrigger beam_f17857_0()
{    return lateTrigger;}
public AfterWatermarkEarlyAndLate beam_f17865_0(OnceTrigger lateFirings)
{    checkNotNull(lateFirings, "Must specify the trigger to use for late firings");    return new AfterWatermarkEarlyAndLate(Never.ever(), lateFirings);}
public Instant beam_f17866_0(BoundedWindow window)
{    return window.maxTimestamp();}
protected FromEndOfWindow beam_f17867_0(List<Trigger> continuationTriggers)
{    return this;}
public static MonthsWindows beam_f17875_0(int number)
{    return new MonthsWindows(number, 1, DEFAULT_START_DATE, DateTimeZone.UTC);}
public static YearsWindows beam_f17876_0(int number)
{    return new YearsWindows(number, 1, 1, DEFAULT_START_DATE, DateTimeZone.UTC);}
public DaysWindows beam_f17877_0(int year, int month, int day)
{    return new DaysWindows(number, new DateTime(year, month, day, 0, 0, timeZone), timeZone);}
public DateTime beam_f17885_0()
{    return startDate;}
public DateTimeZone beam_f17886_0()
{    return timeZone;}
public MonthsWindows beam_f17887_0(int dayOfMonth)
{    return new MonthsWindows(number, dayOfMonth, startDate, timeZone);}
public int beam_f17895_0()
{    return number;}
public int beam_f17896_0()
{    return dayOfMonth;}
public DateTime beam_f17897_0()
{    return startDate;}
public void beam_f17905_0(WindowFn<?, ?> other) throws IncompatibleWindowException
{    if (!this.isCompatible(other)) {        throw new IncompatibleWindowException(other, String.format("Only %s objects with the same number of years, month of year, " + "day of month, start date and time zone are compatible.", YearsWindows.class.getSimpleName()));    }}
public void beam_f17906_0(DisplayData.Builder builder)
{    super.populateDisplayData(builder);    builder.add(DisplayData.item("numYears", number).withLabel("Window Years")).addIfNotDefault(DisplayData.item("startDate", new DateTime(startDate, timeZone).toInstant()).withLabel("Window Start Date"), new DateTime(DEFAULT_START_DATE, DateTimeZone.UTC).toInstant());}
public DateTimeZone beam_f17907_0()
{    return timeZone;}
protected Trigger beam_f17915_0(List<Trigger> continuationTriggers)
{    return this;}
public static FixedWindows beam_f17916_0(Duration size)
{    return new FixedWindows(size, Duration.ZERO);}
public FixedWindows beam_f17917_0(Duration offset)
{    return new FixedWindows(size, offset);}
public boolean beam_f17925_0(Object object)
{    if (!(object instanceof FixedWindows)) {        return false;    }    FixedWindows other = (FixedWindows) object;    return getOffset().equals(other.getOffset()) && getSize().equals(other.getSize());}
public int beam_f17926_0()
{    return Objects.hash(size, offset);}
public Instant beam_f17927_0()
{    return END_OF_GLOBAL_WINDOW;}
public boolean beam_f17937_0(WindowFn<?, ?> o)
{    return o instanceof GlobalWindows;}
public void beam_f17938_0(WindowFn<?, ?> other) throws IncompatibleWindowException
{    if (!this.isCompatible(other)) {        throw new IncompatibleWindowException(other, String.format("%s is only compatible with %s.", GlobalWindows.class.getSimpleName(), GlobalWindows.class.getSimpleName()));    }}
public Coder<GlobalWindow> beam_f17939_0()
{    return GlobalWindow.Coder.INSTANCE;}
public String beam_f17947_0()
{    String windowFn = givenWindowFn.getClass().getSimpleName();    return String.format("The given WindowFn is %s. %s", windowFn, reason);}
public Instant beam_f17948_0()
{    return start;}
public Instant beam_f17949_0()
{    return end;}
private int beam_f17957_0(int x)
{        int inverse = x * x * x;        inverse *= 2 - x * inverse;    inverse *= 2 - x * inverse;    inverse *= 2 - x * inverse;    return inverse;}
public String beam_f17958_0()
{    return "[" + start + ".." + end + ")";}
public int beam_f17959_0(IntervalWindow o)
{    if (start.isEqual(o.start)) {        return end.compareTo(o.end);    }    return start.compareTo(o.start);}
public String beam_f17967_0()
{    return cause;}
public WindowFn<?, W> beam_f17968_0()
{    return originalWindowFn;}
public Collection<W> beam_f17969_0(AssignContext c)
{    throw new UnsupportedOperationException();}
public boolean beam_f17977_0(IntervalWindow window)
{    return union == null || union.intersects(window);}
public void beam_f17978_0(IntervalWindow window)
{    union = union == null ? window : union.span(window);    parts.add(window);}
public void beam_f17979_0(WindowFn<?, IntervalWindow>.MergeContext c) throws Exception
{    if (parts.size() > 1) {        c.merge(parts, union);    }}
public Instant beam_f17988_0(BoundedWindow window)
{        Instant actualDeadline = subTriggers.get(ACTUAL).getWatermarkThatGuaranteesFiring(window);    Instant untilDeadline = subTriggers.get(UNTIL).getWatermarkThatGuaranteesFiring(window);    return actualDeadline.isBefore(untilDeadline) ? actualDeadline : untilDeadline;}
protected Trigger beam_f17989_0(List<Trigger> continuationTriggers)
{        return Repeatedly.forever(new OrFinallyTrigger(continuationTriggers.get(ACTUAL), (Trigger.OnceTrigger) continuationTriggers.get(UNTIL)));}
public String beam_f17990_0()
{    return String.format("%s.orFinally(%s)", subTriggers.get(ACTUAL), subTriggers.get(UNTIL));}
public boolean beam_f17998_0()
{    return isLast;}
public Timing beam_f17999_0()
{    return timing;}
public long beam_f18000_0()
{    return index;}
public static PaneInfoCoder beam_f18008_0()
{    return INSTANCE;}
public void beam_f18009_0(PaneInfo value, final OutputStream outStream) throws CoderException, IOException
{    Encoding encoding = chooseEncoding(value);    switch(chooseEncoding(value)) {        case FIRST:            outStream.write(value.encodedByte);            break;        case ONE_INDEX:            outStream.write(value.encodedByte | encoding.tag);            VarInt.encode(value.index, outStream);            break;        case TWO_INDICES:            outStream.write(value.encodedByte | encoding.tag);            VarInt.encode(value.index, outStream);            VarInt.encode(value.nonSpeculativeIndex, outStream);            break;        default:            throw new CoderException("Unknown encoding " + encoding);    }}
public PaneInfo beam_f18010_0(final InputStream inStream) throws CoderException, IOException
{    byte keyAndTag = (byte) inStream.read();    PaneInfo base = BYTE_TO_PANE_INFO.get((byte) (keyAndTag & 0x0F));    long index, onTimeIndex;    switch(Encoding.fromTag(keyAndTag)) {        case FIRST:            return base;        case ONE_INDEX:            index = VarInt.decodeLong(inStream);            onTimeIndex = base.timing == Timing.EARLY ? -1 : index;            break;        case TWO_INDICES:            index = VarInt.decodeLong(inStream);            onTimeIndex = VarInt.decodeLong(inStream);            break;        default:            throw new CoderException("Unknown encoding " + (keyAndTag & 0xF0));    }    return new PaneInfo(base.isFirst, base.isLast, base.timing, index, onTimeIndex);}
public Instant beam_f18019_0(BoundedWindow window)
{        return subTriggers.get(REPEATED).getWatermarkThatGuaranteesFiring(window);}
protected Trigger beam_f18020_0(List<Trigger> continuationTriggers)
{    return new Repeatedly(continuationTriggers.get(REPEATED));}
public String beam_f18021_0()
{    return String.format("Repeatedly.forever(%s)", subTriggers.get(REPEATED));}
public boolean beam_f18029_0(WindowFn<?, ?> other)
{    return other instanceof Sessions;}
public void beam_f18030_0(WindowFn<?, ?> other) throws IncompatibleWindowException
{    if (!this.isCompatible(other)) {        throw new IncompatibleWindowException(other, String.format("%s is only compatible with %s.", Sessions.class.getSimpleName(), Sessions.class.getSimpleName()));    }}
public TypeDescriptor<IntervalWindow> beam_f18031_0()
{    return TypeDescriptor.of(IntervalWindow.class);}
public SlidingWindows beam_f18039_0(Duration offset)
{    return new SlidingWindows(period, size, offset);}
public Coder<IntervalWindow> beam_f18040_0()
{    return IntervalWindow.getCoder();}
public Collection<IntervalWindow> beam_f18041_0(AssignContext c)
{    return assignWindows(c.timestamp());}
private long beam_f18049_0(Instant timestamp)
{    return timestamp.getMillis() - timestamp.plus(period).minus(offset).getMillis() % period.getMillis();}
 static Duration beam_f18050_0(Duration size)
{    if (size.isLongerThan(Duration.standardHours(1))) {        return Duration.standardHours(1);    }    if (size.isLongerThan(Duration.standardMinutes(1))) {        return Duration.standardMinutes(1);    }    if (size.isLongerThan(Duration.standardSeconds(1))) {        return Duration.standardSeconds(1);    }    return Duration.millis(1);}
public Duration beam_f18051_0()
{    return period;}
public final Instant beam_f18059_0(BoundedWindow intoWindow, Instant... timestamps)
{    return merge(intoWindow, Arrays.asList(timestamps));}
public Instant beam_f18060_0(Iterable<? extends Instant> timestamps)
{    return Ordering.natural().min(timestamps);}
public Instant beam_f18061_0(BoundedWindow intoWindow, Iterable<? extends Instant> mergingTimestamps)
{    return combine(mergingTimestamps);}
public Instant beam_f18069_0(BoundedWindow intoWindow, Iterable<? extends Instant> mergingTimestamps)
{    return intoWindow.maxTimestamp();}
public boolean beam_f18070_0()
{    return false;}
public boolean beam_f18071_0()
{    return true;}
public boolean beam_f18079_0(Object obj)
{    if (this == obj) {        return true;    }    if (!(obj instanceof Trigger)) {        return false;    }    Trigger that = (Trigger) obj;    return Objects.equals(getClass(), that.getClass()) && Objects.equals(subTriggers, that.subTriggers);}
public int beam_f18080_0()
{    return Objects.hash(getClass(), subTriggers);}
public OrFinallyTrigger beam_f18081_0(OnceTrigger until)
{    return new OrFinallyTrigger(this, until);}
public Window<T> beam_f18089_0(Duration allowedLateness)
{    return toBuilder().setAllowedLateness(allowedLateness).build();}
public Window<T> beam_f18090_0(TimestampCombiner timestampCombiner)
{    return toBuilder().setTimestampCombiner(timestampCombiner).build();}
public Window<T> beam_f18091_0(Duration allowedLateness, ClosingBehavior behavior)
{    return toBuilder().setAllowedLateness(allowedLateness).setClosingBehavior(behavior).build();}
public PCollection<T> beam_f18099_0(PCollection<T> input)
{    return PCollection.createPrimitiveOutputInternal(input.getPipeline(), updatedStrategy, input.isBounded(), input.getCoder());}
public void beam_f18100_0(DisplayData.Builder builder)
{    original.populateDisplayData(builder);}
public WindowFn<T, ?> beam_f18101_0()
{    return updatedStrategy.getWindowFn();}
public boolean beam_f18109_0()
{    return false;}
public TypeDescriptor<W> beam_f18110_0()
{    return new TypeDescriptor<W>(this) {    };}
public final Duration beam_f18112_0()
{    return maximumLookback;}
public Map<TupleTag<?>, PValue> beam_f18120_0()
{    Map<TupleTag<?>, PValue> values = new HashMap<>();    values.put(failuresTag(), failures());    if (outputTag() != null && output() instanceof PValue) {        values.put(outputTag(), (PValue) output());    }    return values;}
public static WithKeys<K, V> beam_f18122_0(SerializableFunction<V, K> fn)
{    checkNotNull(fn, "WithKeys constructed with null function. Did you mean WithKeys.of((Void) null)?");    return new WithKeys<>(fn, null);}
public static WithKeys<K, V> beam_f18123_0(@Nullable final K key)
{    return new WithKeys<>(value -> key, (Class<K>) (key == null ? Void.class : key.getClass()));}
public void beam_f18131_0(@Element T element, OutputReceiver<T> r)
{    Instant timestamp = fn.apply(element);    checkNotNull(timestamp, "Timestamps for WithTimestamps cannot be null. Timestamp provided by %s.", fn);    r.outputWithTimestamp(element, timestamp);}
public Duration beam_f18132_0()
{    return allowedTimestampSkew;}
public static Matcher<Class<?>> beam_f18133_0(final String packageName)
{    return new Matchers.ClassInPackage(packageName);}
private boolean beam_f18141_0(final ApiSurface checkedApiSurface, final Set<Matcher<Class<?>>> allowedClasses, final Description mismatchDescription)
{    /* <helper_lambdas> */    final Function<Class<?>, List<Class<?>>> toExposure = checkedApiSurface::getAnyExposurePath;    final Maps.EntryTransformer<Class<?>, List<Class<?>>, String> toMessage = (aClass, exposure) -> aClass + " exposed via:\n\t\t" + Joiner.on("\n\t\t").join(exposure);    final Predicate<Class<?>> disallowed = aClass -> !classIsAllowed(aClass, allowedClasses);    /* </helper_lambdas> */    final FluentIterable<Class<?>> disallowedClasses = FluentIterable.from(checkedApiSurface.getExposedClasses()).filter(disallowed);    final ImmutableMap<Class<?>, List<Class<?>>> exposures = Maps.toMap(disallowedClasses, toExposure);    final ImmutableList<String> messages = FluentIterable.from(Maps.transformEntries(exposures, toMessage).values()).toSortedList(Ordering.natural());    if (!messages.isEmpty()) {        mismatchDescription.appendText("The following disallowed classes appeared on the API surface:\n\t" + Joiner.on("\n\t").join(messages));    }    return messages.isEmpty();}
private boolean beam_f18142_0(final Class<?> clazz, final Set<Matcher<Class<?>>> allowedClasses)
{        return anyOf((Iterable) allowedClasses).matches(clazz);}
protected boolean beam_f18143_0(final ApiSurface apiSurface, final Description mismatchDescription)
{    final boolean noDisallowed = verifyNoDisallowed(apiSurface, classMatchers, mismatchDescription);    final boolean noAbandoned = verifyNoAbandoned(apiSurface, classMatchers, mismatchDescription);    return noDisallowed && noAbandoned;}
public ApiSurface beam_f18151_0(String prefix)
{    return pruningPattern(Pattern.compile(Pattern.quote(prefix) + ".*"));}
public ApiSurface beam_f18152_0(String className)
{    return pruningPattern(Pattern.compile(Pattern.quote(className)));}
public ApiSurface beam_f18153_0(Class<?> clazz)
{    return pruningClassName(clazz.getName());}
private void beam_f18161_0()
{    visited = Sets.newHashSet();    exposedToExposers = Multimaps.newSetMultimap(Maps.<Class<?>, Collection<Class<?>>>newHashMap(), Sets::newHashSet);    for (Class<?> clazz : rootClasses) {        addExposedTypes(clazz, null);    }}
private Pattern beam_f18162_0()
{    if (prunedPattern == null) {        constructPrunedPattern();    }    return prunedPattern;}
private void beam_f18163_0()
{    Set<String> prunedPatternStrings = Sets.newHashSet();    for (Pattern patternToPrune : patternsToPrune) {        prunedPatternStrings.add(patternToPrune.pattern());    }    prunedPattern = Pattern.compile("(" + Joiner.on(")|(").join(prunedPatternStrings) + ")");}
private void beam_f18171_1(Type type, Class<?> cause)
{    if (type instanceof TypeVariable) {                addExposedTypes((TypeVariable) type, cause);    } else if (type instanceof WildcardType) {                addExposedTypes((WildcardType) type, cause);    } else if (type instanceof GenericArrayType) {                addExposedTypes((GenericArrayType) type, cause);    } else if (type instanceof ParameterizedType) {                addExposedTypes((ParameterizedType) type, cause);    } else if (type instanceof Class) {                addExposedTypes((Class) type, cause);    } else {        throw new IllegalArgumentException("Unknown implementation of Type");    }}
private void beam_f18172_1(TypeVariable type, Class<?> cause)
{    if (done(type)) {        return;    }    visit(type);    for (Type bound : type.getBounds()) {                addExposedTypes(bound, cause);    }}
private void beam_f18173_1(WildcardType type, Class<?> cause)
{    visit(type);    for (Type lowerBound : type.getLowerBounds()) {                addExposedTypes(lowerBound, cause);    }    for (Type upperBound : type.getUpperBounds()) {                addExposedTypes(upperBound, cause);    }}
private boolean beam_f18181_0(int modifiers)
{    return 0 != (modifiers & (Modifier.PUBLIC | Modifier.PROTECTED));}
public static AppliedCombineFn<K, InputT, AccumT, OutputT> beam_f18182_0(GlobalCombineFn<? super InputT, AccumT, OutputT> fn, Coder<AccumT> accumCoder)
{    return withAccumulatorCoder(fn, accumCoder, null, null, null);}
public static AppliedCombineFn<K, InputT, AccumT, OutputT> beam_f18183_0(GlobalCombineFn<? super InputT, AccumT, OutputT> fn, Coder<AccumT> accumCoder, Iterable<PCollectionView<?>> sideInputViews, KvCoder<K, InputT> kvCoder, WindowingStrategy<?, ?> windowingStrategy)
{        @SuppressWarnings("unchecked")    GlobalCombineFn<InputT, AccumT, OutputT> clonedFn = (GlobalCombineFn<InputT, AccumT, OutputT>) SerializableUtils.clone(fn);    return create(clonedFn, accumCoder, sideInputViews, kvCoder, windowingStrategy);}
public WindowingStrategy<?, ?> beam_f18191_0()
{    return windowingStrategy;}
public long beam_f18193_0() throws IOException
{    return 0;}
public long beam_f18195_0() throws IOException
{    return STOP;}
public void beam_f18203_0(BucketingFunction outer, long value)
{    combinedValue = outer.function.apply(combinedValue, value);    numSamples++;}
public boolean beam_f18204_0()
{    numSamples--;    checkState(numSamples >= 0, "Lost count of samples");    return numSamples == 0;}
public long beam_f18205_0()
{    return combinedValue;}
public void beam_f18213_0(int b) throws IOException
{    if (finished) {        throw new IOException("Stream has been finished. Can not write any more data.");    }    if (count == 0) {        os.write(b);        return;    }    if (buffer.hasRemaining()) {        buffer.put((byte) b);    } else {        outputBuffer();        os.write(b);    }}
public void beam_f18214_0(byte[] b, int off, int len) throws IOException
{    if (finished) {        throw new IOException("Stream has been finished. Can not write any more data.");    }    if (count == 0) {        os.write(b, off, len);        return;    }    if (buffer.remaining() >= len) {        buffer.put(b, off, len);    } else {        outputBuffer();        os.write(b, off, len);    }}
public void beam_f18215_0() throws IOException
{    if (finished) {        return;    }    outputBuffer();    os.flush();}
public ImmutableSet<ClassInfo> beam_f18223_0(String packageName)
{    checkNotNull(packageName);    String packagePrefix = packageName + '.';    ImmutableSet.Builder<ClassInfo> builder = ImmutableSet.builder();    for (ClassInfo classInfo : getTopLevelClasses()) {        if (classInfo.getName().startsWith(packagePrefix)) {            builder.add(classInfo);        }    }    return builder.build();}
 static ResourceInfo beam_f18224_0(String resourceName, ClassLoader loader)
{    if (resourceName.endsWith(CLASS_FILE_NAME_EXTENSION)) {        return new ClassInfo(resourceName, loader);    } else {        return new ResourceInfo(resourceName, loader);    }}
public final URL beam_f18225_0()
{    URL url = loader.getResource(resourceName);    if (url == null) {        throw new NoSuchElementException(resourceName);    }    return url;}
public String beam_f18233_0()
{    int lastDollarSign = className.lastIndexOf('$');    if (lastDollarSign != -1) {        String innerClassName = className.substring(lastDollarSign + 1);                return CharMatcher.digit().trimLeadingFrom(innerClassName);    }    String packageName = getPackageName();    if (packageName.isEmpty()) {        return className;    }        return className.substring(packageName.length() + 1);}
public String beam_f18234_0()
{    return className;}
public Class<?> beam_f18235_0()
{    try {        return loader.loadClass(className);    } catch (ClassNotFoundException e) {                throw new IllegalStateException(e);    }}
 static URL beam_f18243_0(File jarFile, String path) throws MalformedURLException
{    return new URL(jarFile.toURI().toURL(), path);}
 ImmutableSet<ResourceInfo> beam_f18244_0()
{    ImmutableSet.Builder<ResourceInfo> builder = ImmutableSet.builder();    for (Map.Entry<ClassLoader, String> entry : resources.entries()) {        builder.add(ResourceInfo.of(entry.getValue(), entry.getKey()));    }    return builder.build();}
protected void beam_f18245_0(ClassLoader classloader, JarFile file)
{    Enumeration<JarEntry> entries = file.entries();    while (entries.hasMoreElements()) {        JarEntry entry = entries.nextElement();        if (entry.isDirectory() || entry.getName().equals(JarFile.MANIFEST_NAME)) {            continue;        }        resources.get(classloader).add(entry.getName());    }}
public static T beam_f18253_0(Coder<T> coder, byte[] encodedValue) throws CoderException
{    return decodeFromByteArray(coder, encodedValue, Coder.Context.OUTER);}
public static T beam_f18254_0(Coder<T> coder, byte[] encodedValue, Coder.Context context) throws CoderException
{    try (ExposedByteArrayInputStream stream = new ExposedByteArrayInputStream(encodedValue)) {        T result = decodeFromSafeStream(coder, stream, context);        if (stream.available() != 0) {            throw new CoderException(stream.available() + " unexpected extra bytes after decoding " + result);        }        return result;    }}
private static T beam_f18255_0(Coder<T> coder, InputStream stream, Coder.Context context) throws CoderException
{    try {        return coder.decode(new UnownedInputStream(stream), context);    } catch (IOException exn) {        Throwables.propagateIfPossible(exn, CoderException.class);        throw new IllegalArgumentException("Forbidden IOException when reading from InputStream", exn);    }}
public static Context beam_f18263_0()
{    return NULL_CONTEXT;}
public static Context beam_f18264_0(final StateContext<?> c)
{    return new Context() {        @Override        public PipelineOptions getPipelineOptions() {            return c.getPipelineOptions();        }        @Override        public <T> T sideInput(PCollectionView<T> view) {            return c.sideInput(view);        }    };}
public PipelineOptions beam_f18265_0()
{    return c.getPipelineOptions();}
public AccumT beam_f18273_0(AccumT accumulator, Context c)
{    return combineFn.compact(accumulator);}
public OutputT beam_f18274_0()
{    return combineFn.defaultValue();}
public Coder<AccumT> beam_f18275_0(CoderRegistry registry, Coder<InputT> inputCoder) throws CannotProvideCoderException
{    return combineFn.getAccumulatorCoder(registry, inputCoder);}
public Coder<AccumT> beam_f18283_0(CoderRegistry registry, Coder<InputT> inputCoder) throws CannotProvideCoderException
{    return combineFn.getAccumulatorCoder(registry, inputCoder);}
public Coder<OutputT> beam_f18284_0(CoderRegistry registry, Coder<InputT> inputCoder) throws CannotProvideCoderException
{    return combineFn.getDefaultOutputCoder(registry, inputCoder);}
public void beam_f18285_0(DisplayData.Builder builder)
{    combineFn.populateDisplayData(builder);}
public void beam_f18293_0(double scalingFactor)
{    this.scalingFactor = scalingFactor;}
public void beam_f18294_0(Observable obs, Object obj)
{    if (obj instanceof Long) {        totalSize += (long) (scalingFactor * (Long) obj);    } else if (obj instanceof Integer) {        totalSize += (long) (scalingFactor * (Integer) obj);    } else {        throw new AssertionError("unexpected parameter object");    }}
public void beam_f18295_0()
{    reportElementSize(totalSize);    totalSize = 0;    isLazy = false;}
private void beam_f18303_0(StringBuilder builder, ParameterizedType t)
{    if (t.getOwnerType() != null) {        format(builder, t.getOwnerType());        builder.append('.');    }    format(builder, t.getRawType());    if (t.getActualTypeArguments().length > 0) {        builder.append('<');        COMMA_SEPARATOR.appendTo(builder, FluentIterable.from(asList(t.getActualTypeArguments())).transform(TYPE_SIMPLE_DESCRIPTION));        builder.append('>');    }}
private void beam_f18304_0(StringBuilder builder, GenericArrayType t)
{    format(builder, t.getGenericComponentType());    builder.append("[]");}
public int beam_f18305_0(Object o1, Object o2)
{    return o1.getClass().getCanonicalName().compareTo(o2.getClass().getCanonicalName());}
private static boolean beam_f18313_0(final ClassLoader current, final ClassLoader proposed)
{    final Collection<ClassLoader> visited = new ArrayList<>();    ClassLoader it = proposed.getParent();    while (it != null) {        if (it == current) {            return true;        }        if (visited.contains(it)) {                        return false;        }        visited.add(it);        it = it.getParent();    }    return false;}
public static DoFnInfo<InputT, OutputT> beam_f18314_0(DoFn<InputT, OutputT> doFn, WindowingStrategy<?, ?> windowingStrategy, Iterable<PCollectionView<?>> sideInputViews, Coder<InputT> inputCoder, TupleTag<OutputT> mainOutput, DoFnSchemaInformation doFnSchemaInformation, Map<String, PCollectionView<?>> sideInputMapping)
{    return new DoFnInfo<>(doFn, windowingStrategy, sideInputViews, inputCoder, Collections.emptyMap(), mainOutput, doFnSchemaInformation, sideInputMapping);}
public static DoFnInfo<InputT, OutputT> beam_f18315_0(DoFn<InputT, OutputT> doFn, WindowingStrategy<?, ?> windowingStrategy, Iterable<PCollectionView<?>> sideInputViews, Coder<InputT> inputCoder, Map<TupleTag<?>, Coder<?>> outputCoders, TupleTag<OutputT> mainOutput, DoFnSchemaInformation doFnSchemaInformation, Map<String, PCollectionView<?>> sideInputMapping)
{    return new DoFnInfo<>(doFn, windowingStrategy, sideInputViews, inputCoder, outputCoders, mainOutput, doFnSchemaInformation, sideInputMapping);}
public DoFnSchemaInformation beam_f18323_0()
{    return doFnSchemaInformation;}
public Map<String, PCollectionView<?>> beam_f18324_0()
{    return sideInputMapping;}
public static DoFnWithExecutionInformation beam_f18325_0(DoFn<?, ?> fn, TupleTag<?> tag, Map<String, PCollectionView<?>> sideInputMapping, DoFnSchemaInformation doFnSchemaInformation)
{    return new AutoValue_DoFnWithExecutionInformation(fn, tag, sideInputMapping, doFnSchemaInformation);}
public synchronized void beam_f18336_0(byte[] b) throws IOException
{    if (b.length == 0) {        return;    }    if (count == 0) {                        swappedBuffer = buf;        buf = b;        count = b.length;    } else {        fallback();        super.write(b);    }}
public synchronized void beam_f18337_0(byte[] b, int off, int len)
{    fallback();    super.write(b, off, len);}
public synchronized void beam_f18338_0(int b)
{    fallback();    super.write(b);}
public FluentBackoff beam_f18346_0(double exponent)
{    checkArgument(exponent > 0, "exponent %s must be greater than 0", exponent);    return new FluentBackoff(exponent, initialBackoff, maxBackoff, maxCumulativeBackoff, maxRetries);}
public FluentBackoff beam_f18347_0(Duration initialBackoff)
{    checkArgument(initialBackoff.isLongerThan(Duration.ZERO), "initialBackoff %s must be at least 1 millisecond", initialBackoff);    return new FluentBackoff(exponent, initialBackoff, maxBackoff, maxCumulativeBackoff, maxRetries);}
public FluentBackoff beam_f18348_0(Duration maxBackoff)
{    checkArgument(maxBackoff.getMillis() > 0, "maxBackoff %s must be at least 1 millisecond", maxBackoff);    return new FluentBackoff(exponent, initialBackoff, maxBackoff, maxCumulativeBackoff, maxRetries);}
public boolean beam_f18356_0(WindowFn<?, ?> other)
{    throw new UnsupportedOperationException(String.format("%s.isCompatible() should never be called." + " It is a private implementation detail of sdk utilities." + " This message indicates a bug in the Beam SDK.", getClass().getCanonicalName()));}
public void beam_f18357_0(WindowFn<?, ?> other) throws IncompatibleWindowException
{    throw new UnsupportedOperationException(String.format("%s.verifyCompatibility() should never be called." + " It is a private implementation detail of sdk utilities." + " This message indicates a bug in the Beam SDK.", getClass().getCanonicalName()));}
public Coder<BoundedWindow> beam_f18358_0()
{        return coder;}
public InstanceBuilder<T> beam_f18366_0(String name) throws ClassNotFoundException
{    checkArgument(factoryClass == null, "Class name may only be specified once");    if (name.indexOf('.') == -1) {        name = type.getPackage().getName() + "." + name;    }    try {        factoryClass = Class.forName(name);    } catch (ClassNotFoundException e) {        throw new ClassNotFoundException(String.format("Could not find class: %s", name), e);    }    return this;}
public InstanceBuilder<T> beam_f18367_0(Class<?> factoryClass)
{    this.factoryClass = factoryClass;    return this;}
public InstanceBuilder<T> beam_f18368_0(String methodName)
{    checkArgument(this.methodName == null, "Factory method name may only be specified once");    this.methodName = methodName;    return this;}
public static T beam_f18376_0(CompletionStage<T> future, long duration, TimeUnit unit) throws InterruptedException, ExecutionException, TimeoutException
{    return future.toCompletableFuture().get(duration, unit);}
public static boolean beam_f18377_0(CompletionStage<?> future)
{    return future.toCompletableFuture().isDone();}
public static boolean beam_f18378_0(CompletionStage<?> future)
{    return future.toCompletableFuture().isCancelled();}
public static CompletionStage<List<ExceptionOrResult<T>>> beam_f18386_0(Collection<? extends CompletionStage<? extends T>> futures)
{            CompletionStage<Void> blockAndDiscard = CompletableFuture.allOf(futuresToCompletableFutures(futures)).whenComplete((ignoredValues, arbitraryException) -> {    });    return blockAndDiscard.thenApply(nothing -> futures.stream().map(future -> {                try {            return ExceptionOrResult.<T>result(future.toCompletableFuture().join());        } catch (CompletionException exc) {            return ExceptionOrResult.<T>exception(exc);        }    }).collect(Collectors.toList()));}
private static CompletableFuture<? extends T>[] beam_f18387_0(Collection<? extends CompletionStage<? extends T>> futures)
{    CompletableFuture<? extends T>[] completableFutures = new CompletableFuture[futures.size()];    int i = 0;    for (CompletionStage<? extends T> future : futures) {        completableFutures[i] = future.toCompletableFuture();        ++i;    }    return completableFutures;}
private void beam_f18388_0(long nowMsSinceEpoch)
{    checkArgument(nowMsSinceEpoch >= 0, "Only positive timestamps supported");    checkArgument(nowMsSinceEpoch >= currentMsSinceEpoch, "Attempting to move backwards");    int newBuckets = Math.min((int) ((nowMsSinceEpoch - currentMsSinceEpoch) / sampleUpdateMs), buckets.length);    currentMsSinceEpoch = nowMsSinceEpoch - (nowMsSinceEpoch % sampleUpdateMs);    while (newBuckets > 0) {        currentIndex = (currentIndex + 1) % buckets.length;        buckets[currentIndex] = function.identity();        numSamples[currentIndex] = 0;        newBuckets--;    }}
private void beam_f18398_0(T previousValue, T newValue) throws CoderException
{    throw new IllegalMutationException(String.format("Value %s mutated illegally, new value was %s." + " Encoding was %s, now %s.", previousValue, newValue, CoderUtils.encodeToBase64(coder, previousValue), CoderUtils.encodeToBase64(coder, newValue)), previousValue, newValue);}
public void beam_f18399_0()
{    verifyUnmodified();}
private static String beam_f18400_0(Class<?> clazz, boolean dropOuterClassNames)
{    checkArgument(!clazz.isAnonymousClass(), "Attempted to get simple name of anonymous class");    return approximateSimpleName(clazz.getName(), dropOuterClassNames);}
public List<String> beam_f18408_0() throws IOException, InterruptedException
{    return readFilesWithRetries(Sleeper.DEFAULT, BACK_OFF_FACTORY.backoff());}
public String beam_f18409_0()
{    return String.format("%s with shard template '%s'", filePattern, shardTemplate);}
 List<String> beam_f18410_1(Collection<Metadata> files) throws IOException
{    List<String> allLines = Lists.newArrayList();    int i = 1;    for (Metadata file : files) {        try (Reader reader = Channels.newReader(FileSystems.open(file.resourceId()), StandardCharsets.UTF_8.name())) {            List<String> lines = CharStreams.readLines(reader);            allLines.addAll(lines);                    }        i++;    }    return allLines;}
private static Row beam_f18418_0(FieldValue rowFieldValue)
{    if (!rowFieldValue.isJsonObject()) {        throw new UnsupportedRowJsonException("Expected JSON object for field '" + rowFieldValue.name() + "'. " + "Unable to convert '" + rowFieldValue.jsonValue().asText() + "'" + " to Beam Row, it is not a JSON object. Currently only JSON objects " + "can be parsed to Beam Rows");    }    return rowFieldValue.rowSchema().getFields().stream().map(schemaField -> extractJsonNodeValue(FieldValue.of(schemaField.getName(), schemaField.getType(), rowFieldValue.jsonFieldValue(schemaField.getName())))).collect(toRow(rowFieldValue.rowSchema()));}
private static Object beam_f18419_0(FieldValue arrayFieldValue)
{    if (!arrayFieldValue.isJsonArray()) {        throw new UnsupportedRowJsonException("Expected JSON array for field '" + arrayFieldValue.name() + "'. " + "Instead got " + arrayFieldValue.jsonNodeType().name());    }    return arrayFieldValue.jsonArrayElements().map(jsonArrayElement -> extractJsonNodeValue(FieldValue.of(arrayFieldValue.name() + "[]", arrayFieldValue.arrayElementType(), jsonArrayElement))).collect(toList());}
private static Object beam_f18420_0(FieldValue fieldValue)
{    try {        return JSON_VALUE_GETTERS.get(fieldValue.typeName()).extractValue(fieldValue.jsonValue());    } catch (RuntimeException e) {        throw new UnsupportedRowJsonException("Unable to get value from field '" + fieldValue.name() + "'. " + "Schema type '" + fieldValue.typeName() + "'. " + "JSON node type " + fieldValue.jsonNodeType().name(), e);    }}
 FieldType beam_f18428_0()
{    return type().getCollectionElementType();}
 boolean beam_f18429_0()
{    return jsonValue().isObject();}
 JsonNode beam_f18430_0(String fieldName)
{    return jsonValue().get(fieldName);}
 static ValueExtractor<Integer> beam_f18438_0()
{    return ValidatingValueExtractor.<Integer>builder().setExtractor(JsonNode::intValue).setValidator(jsonNode -> jsonNode.isIntegralNumber() && jsonNode.canConvertToInt()).build();}
 static ValueExtractor<Long> beam_f18439_0()
{    return ValidatingValueExtractor.<Long>builder().setExtractor(JsonNode::longValue).setValidator(jsonNode -> jsonNode.isIntegralNumber() && jsonNode.canConvertToLong()).build();}
 static ValueExtractor<Float> beam_f18440_0()
{    return ValidatingValueExtractor.<Float>builder().setExtractor(JsonNode::floatValue).setValidator(jsonNode -> jsonNode.isFloat() ||     (jsonNode.isFloatingPointNumber() && jsonNode.doubleValue() == (double) (float) jsonNode.doubleValue()) ||     (jsonNode.isIntegralNumber() && jsonNode.canConvertToInt() && jsonNode.asInt() == (int) (float) jsonNode.asInt())).build();}
private void beam_f18448_0(ObjectInputStream is) throws IOException, ClassNotFoundException
{    is.defaultReadObject();    if (throwable != null) {        throwable.setStackTrace(stackTrace);    }}
public boolean beam_f18449_0(Object o)
{    if (this == o) {        return true;    }    if (o == null || getClass() != o.getClass()) {        return false;    }    SerializableThrowable that = (SerializableThrowable) o;    return Arrays.equals(stackTrace, that.stackTrace);}
public int beam_f18450_0()
{    return Arrays.hashCode(stackTrace);}
protected Class beam_f18458_0(final String[] interfaces) throws IOException, ClassNotFoundException
{    final ClassLoader classloader = ReflectHelpers.findClassLoader();    final Class[] cinterfaces = new Class[interfaces.length];    for (int i = 0; i < interfaces.length; i++) {        cinterfaces[i] = classloader.loadClass(interfaces[i]);    }    try {        return Proxy.getProxyClass(classloader, cinterfaces);    } catch (final IllegalArgumentException e) {        throw new ClassNotFoundException(null, e);    }}
public int beam_f18459_0()
{    return writers.size();}
public void beam_f18460_0(WritableByteChannel writer)
{    writers.add(writer);}
public static byte[] beam_f18468_0(String string)
{    List<Byte> bytes = new ArrayList<>();    for (int i = 0; i < string.length(); ) {        char c = string.charAt(i);        Byte b;        if (c == '%') {                        try {                b = (byte) Integer.parseInt(string.substring(i + 1, i + 3), 16);            } catch (IndexOutOfBoundsException | NumberFormatException exn) {                throw new IllegalArgumentException("not in legal encoded format; " + "substring [" + i + ".." + (i + 2) + "] not in format \"%xx\"", exn);            }            i += 3;        } else {                        b = (byte) c;            i++;        }        bytes.add(b);    }    byte[] byteArray = new byte[bytes.size()];    int i = 0;    for (Byte b : bytes) {        byteArray[i++] = b;    }    return byteArray;}
public static int beam_f18469_0(final String s, final String t)
{    checkNotNull(s);    checkNotNull(t);        if (s.equals(t)) {        return 0;    }    if (s.length() == 0) {        return t.length();    }    if (t.length() == 0) {        return s.length();    }        final int[] v0 = new int[t.length() + 1];    final int[] v1 = new int[t.length() + 1];        for (int i = 0; i < v0.length; i++) {        v0[i] = i;    }    for (int i = 0; i < s.length(); i++) {                                v1[0] = i + 1;                for (int j = 0; j < t.length(); j++) {            int cost = (s.charAt(i) == t.charAt(j)) ? 0 : 1;            v1[j + 1] = Math.min(Math.min(v1[j] + 1, v0[j + 1] + 1), v0[j] + cost);        }                System.arraycopy(v1, 0, v0, 0, v0.length);    }    return v1[t.length()];}
public void beam_f18470_0() throws IOException
{    throw new UnsupportedOperationException("Caller does not own the underlying input stream " + " and should not call close().");}
public boolean beam_f18478_0(Object obj)
{    return obj instanceof UnownedOutputStream && ((UnownedOutputStream) obj).out.equals(out);}
public int beam_f18479_0()
{    return out.hashCode();}
public String beam_f18480_0()
{    return MoreObjects.toStringHelper(UnownedOutputStream.class).add("out", out).toString();}
public static int beam_f18488_0(InputStream stream) throws IOException
{    long r = decodeLong(stream);    if (r < 0 || r >= 1L << 32) {        throw new IOException("varint overflow " + r);    }    return (int) r;}
public static long beam_f18489_0(InputStream stream) throws IOException
{    long result = 0;    int shift = 0;    int b;    do {                b = stream.read();        if (b < 0) {            if (shift == 0) {                throw new EOFException();            } else {                throw new IOException("varint not terminated");            }        }        long bits = b & 0x7F;        if (shift >= 64 || (shift == 63 && bits > 1)) {                        throw new IOException("varint too long");        }        result |= bits << shift;        shift += 7;    } while ((b & 0x80) != 0);    return result;}
public static int beam_f18490_0(int v)
{    return getLength(convertIntToLongNoSignExtend(v));}
public static WindowedValue<T> beam_f18498_0(T value)
{    return new ValueInGlobalWindow<>(value, PaneInfo.NO_FIRING);}
public static WindowedValue<T> beam_f18499_0(T value, PaneInfo pane)
{    return new ValueInGlobalWindow<>(value, pane);}
public static WindowedValue<T> beam_f18500_0(T value, Instant timestamp)
{    if (BoundedWindow.TIMESTAMP_MIN_VALUE.equals(timestamp)) {        return valueInGlobalWindow(value);    } else {        return new TimestampedValueInGlobalWindow<>(value, timestamp, PaneInfo.NO_FIRING);    }}
public WindowedValue<NewT> beam_f18508_0(NewT newValue)
{    return new ValueInGlobalWindow<>(newValue, getPane());}
public Collection<? extends BoundedWindow> beam_f18509_0()
{    return GLOBAL_WINDOWS;}
public boolean beam_f18510_0()
{    return true;}
public boolean beam_f18518_0()
{    return true;}
public BoundedWindow beam_f18519_0()
{    return GlobalWindow.INSTANCE;}
public boolean beam_f18520_0(Object o)
{    if (o instanceof TimestampedValueInGlobalWindow) {        TimestampedValueInGlobalWindow<?> that = (TimestampedValueInGlobalWindow<?>) o;                return this.getTimestamp().isEqual(that.getTimestamp()) && Objects.equals(that.getPane(), this.getPane()) && Objects.equals(that.getValue(), this.getValue());    } else {        return super.equals(o);    }}
public int beam_f18528_0()
{        return Objects.hash(getValue(), getTimestamp().getMillis(), getPane(), window);}
public String beam_f18529_0()
{    return MoreObjects.toStringHelper(getClass()).add("value", getValue()).add("timestamp", getTimestamp()).add("window", window).add("pane", getPane()).toString();}
public WindowedValue<NewT> beam_f18530_0(NewT newValue)
{    return new TimestampedValueInMultipleWindows<>(newValue, getTimestamp(), windows, getPane());}
public Coder<T> beam_f18538_0()
{    return valueCoder;}
public static FullWindowedValueCoder<T> beam_f18539_0(Coder<T> valueCoder, Coder<? extends BoundedWindow> windowCoder)
{    return new FullWindowedValueCoder<>(valueCoder, windowCoder);}
public Coder<? extends BoundedWindow> beam_f18540_0()
{    return windowCoder;}
public void beam_f18548_0(WindowedValue<T> value, ElementByteSizeObserver observer) throws Exception
{    InstantCoder.of().registerByteSizeObserver(value.getTimestamp(), observer);    windowsCoder.registerByteSizeObserver(value.getWindows(), observer);    PaneInfoCoder.INSTANCE.registerByteSizeObserver(value.getPane(), observer);    valueCoder.registerByteSizeObserver(value.getValue(), observer);}
public List<? extends Coder<?>> beam_f18549_0()
{        return Collections.singletonList(valueCoder);}
public List<? extends Coder<?>> beam_f18550_0()
{    return Arrays.asList(valueCoder, windowCoder);}
public void beam_f18558_0(WindowedValue<T> value, ElementByteSizeObserver observer) throws Exception
{    valueCoder.registerByteSizeObserver(value.getValue(), observer);}
public List<? extends Coder<?>> beam_f18559_0()
{    return Collections.singletonList(valueCoder);}
public static void beam_f18560_1(String format, Object... args)
{    }
 static void beam_f18568_0(File zipFile, File targetDirectory) throws IOException
{    checkNotNull(zipFile);    checkNotNull(targetDirectory);    checkArgument(targetDirectory.isDirectory(), "%s is not a valid directory", targetDirectory.getAbsolutePath());    try (ZipFile zipFileObj = new ZipFile(zipFile)) {        for (ZipEntry entry : entries(zipFileObj)) {            checkName(entry.getName());            File targetFile = new File(targetDirectory, entry.getName());            if (entry.isDirectory()) {                if (!targetFile.isDirectory() && !targetFile.mkdirs()) {                    throw new IOException("Failed to create directory: " + targetFile.getAbsolutePath());                }            } else {                File parentFile = targetFile.getParentFile();                if (!parentFile.isDirectory() && !parentFile.mkdirs()) {                    throw new IOException("Failed to create directory: " + parentFile.getAbsolutePath());                }                                asByteSource(zipFileObj, entry).copyTo(Files.asByteSink(targetFile));            }        }    }}
private static void beam_f18569_0(String name) throws IOException
{        if (name.contains("..")) {                                File file = new File(name);        while (file != null) {            if ("..".equals(file.getName())) {                throw new IOException("Cannot unzip file containing an entry with " + "\"..\" in the name: " + name);            }            file = file.getParentFile();        }    }}
 static void beam_f18570_0(File sourceDirectory, File zipFile) throws IOException
{    checkNotNull(sourceDirectory);    checkNotNull(zipFile);    checkArgument(sourceDirectory.isDirectory(), "%s is not a valid directory", sourceDirectory.getAbsolutePath());    checkArgument(!zipFile.exists(), "%s does already exist, files are not being overwritten", zipFile.getAbsolutePath());    Closer closer = Closer.create();    try {        OutputStream outputStream = closer.register(new BufferedOutputStream(new FileOutputStream(zipFile)));        zipDirectory(sourceDirectory, outputStream);    } catch (Throwable t) {        throw closer.rethrow(t);    } finally {        closer.close();    }}
public int beam_f18578_0(KV<K, V> a, KV<K, V> b)
{    if (a.value == null) {        return b.value == null ? 0 : -1;    } else if (b.value == null) {        return 1;    } else {        return a.value.compareTo(b.value);    }}
public int beam_f18579_0()
{        return Arrays.deepHashCode(new Object[] { key, value });}
public String beam_f18580_0()
{    return MoreObjects.toStringHelper(this).addValue(key).addValue(value).toString();}
public TypeDescriptor<T> beam_f18588_0()
{    return typeDescriptor;}
private CoderOrFailure<T> beam_f18589_0(PInput input, PTransform<?, ?> transform, CoderRegistry coderRegistry, SchemaRegistry schemaRegistry)
{        if (coderOrFailure.coder != null) {        return coderOrFailure;    }        CannotProvideCoderException inputCoderException;    try {        return new CoderOrFailure<>(((PTransform) transform).getDefaultOutputCoder(input, this), null);    } catch (CannotProvideCoderException exc) {        inputCoderException = exc;    }    TypeDescriptor<T> token = getTypeDescriptor();        if (token != null) {        try {            SchemaCoder<T> schemaCoder = SchemaCoder.of(schemaRegistry.getSchema(token), schemaRegistry.getToRowFunction(token), schemaRegistry.getFromRowFunction(token));            return new CoderOrFailure<>(schemaCoder, null);        } catch (NoSuchSchemaException esc) {                }    }        CannotProvideCoderException inferFromTokenException = null;    if (token != null) {        try {            return new CoderOrFailure<>(coderRegistry.getCoder(token), null);        } catch (CannotProvideCoderException exc) {            inferFromTokenException = exc;                        if (transform instanceof ParDo.MultiOutput && exc.getReason() == ReasonCode.TYPE_ERASURE) {                inferFromTokenException = new CannotProvideCoderException(exc.getMessage() + " If this error occurs for an output of the producing ParDo, verify that the " + "TupleTag for this output is constructed with proper type information (see " + "TupleTag Javadoc) or explicitly set the Coder to use if this is not possible.");            }        }    }        StringBuilder messageBuilder = new StringBuilder().append("Unable to return a default Coder for ").append(this).append(". Correct one of the following root causes:");        messageBuilder.append("\n  No Coder has been manually specified; ").append(" you may do so using .setCoder().");    if (inferFromTokenException != null) {        messageBuilder.append("\n  Inferring a Coder from the CoderRegistry failed: ").append(inferFromTokenException.getMessage());    }    if (inputCoderException != null) {        messageBuilder.append("\n  Using the default output Coder from the producing PTransform failed: ").append(inputCoderException.getMessage());    }        return new CoderOrFailure<>(null, messageBuilder.toString());}
public IsBounded beam_f18590_0(IsBounded that)
{    if (this == BOUNDED && that == BOUNDED) {        return BOUNDED;    } else {        return UNBOUNDED;    }}
public boolean beam_f18598_0()
{    return coderOrFailure.coder != null && coderOrFailure.coder instanceof SchemaCoder;}
public Schema beam_f18599_0()
{    if (!hasSchema()) {        throw new IllegalStateException("Cannot call getSchema when there is no schema");    }    return ((SchemaCoder) getCoder()).getSchema();}
public SerializableFunction<T, Row> beam_f18600_0()
{    if (!hasSchema()) {        throw new IllegalStateException("Cannot call getToRowFunction when there is no schema");    }    return ((SchemaCoder<T>) getCoder()).getToRowFunction();}
public PCollection<T> beam_f18608_0(IsBounded isBounded)
{    this.isBounded = isBounded;    return this;}
public static PCollection<T> beam_f18609_0(Pipeline pipeline, WindowingStrategy<?, ?> windowingStrategy, IsBounded isBounded, @Nullable Coder<T> coder)
{    PCollection<T> res = new PCollection<>(pipeline, windowingStrategy, isBounded);    if (coder != null) {        res.setCoder(coder);    }    return res;}
public static PCollection<T> beam_f18610_0(Pipeline pipeline, WindowingStrategy<?, ?> windowingStrategy, IsBounded isBounded, @Nullable Coder<T> coder, TupleTag<?> tag)
{    PCollection<T> res = new PCollection<>(pipeline, windowingStrategy, isBounded, tag);    if (coder != null) {        res.setCoder(coder);    }    return res;}
public List<PCollection<T>> beam_f18618_0()
{    ImmutableList.Builder<PCollection<T>> res = ImmutableList.builder();    for (TaggedPValue value : pcollections) {                @SuppressWarnings("unchecked")        PCollection<T> typedValue = (PCollection<T>) value.getValue();        res.add(typedValue);    }    return res.build();}
public OutputT beam_f18619_0(PTransform<PCollectionList<T>, OutputT> t)
{    return Pipeline.applyTransform(this, t);}
public OutputT beam_f18620_0(String name, PTransform<PCollectionList<T>, OutputT> t)
{    return Pipeline.applyTransform(name, this, t);}
public static PCollectionTuple beam_f18628_0(String tag, PCollection<T> pc)
{    return of(new TupleTag<>(tag), pc);}
public static PCollectionTuple beam_f18629_0(String tag1, PCollection<T> pc1, String tag2, PCollection<T> pc2)
{    return of(tag1, pc1).and(tag2, pc2);}
public static PCollectionTuple beam_f18630_0(String tag1, PCollection<T> pc1, String tag2, PCollection<T> pc2, String tag3, PCollection<T> pc3)
{    return of(tag1, pc1, tag2, pc2).and(tag3, pc3);}
public PCollection<T> beam_f18638_0(String tag)
{    return get(new TupleTag<>(tag));}
public Map<TupleTag<?>, PCollection<?>> beam_f18639_0()
{    return pcollectionMap;}
public OutputT beam_f18640_0(PTransform<? super PCollectionTuple, OutputT> t)
{    return Pipeline.applyTransform(this, t);}
public static PCollectionView<T> beam_f18648_0(PCollection<KV<Void, T>> pCollection, TypeDescriptorSupplier<T> typeDescriptorSupplier, WindowingStrategy<?, W> windowingStrategy, boolean hasDefault, @Nullable T defaultValue, Coder<T> defaultValueCoder)
{    return new SimplePCollectionView<>(pCollection, new SingletonViewFn<T>(hasDefault, defaultValue, defaultValueCoder, typeDescriptorSupplier), windowingStrategy.getWindowFn().getDefaultWindowMappingFn(), windowingStrategy);}
public static PCollectionView<Iterable<T>> beam_f18649_0(PCollection<KV<Void, T>> pCollection, TypeDescriptorSupplier<T> typeDescriptorSupplier, WindowingStrategy<?, W> windowingStrategy)
{    return new SimplePCollectionView<>(pCollection, new IterableViewFn<T>(typeDescriptorSupplier), windowingStrategy.getWindowFn().getDefaultWindowMappingFn(), windowingStrategy);}
public static PCollectionView<List<T>> beam_f18650_0(PCollection<KV<Void, T>> pCollection, TypeDescriptorSupplier<T> typeDescriptorSupplier, WindowingStrategy<?, W> windowingStrategy)
{    return new SimplePCollectionView<>(pCollection, new ListViewFn<>(typeDescriptorSupplier), windowingStrategy.getWindowFn().getDefaultWindowMappingFn(), windowingStrategy);}
public TypeDescriptor<T> beam_f18658_0()
{    return typeDescriptorSupplier.get();}
public Materialization<MultimapView<Void, T>> beam_f18659_0()
{    return Materializations.multimap();}
public Iterable<T> beam_f18660_0(MultimapView<Void, T> primitiveViewT)
{    return Iterables.unmodifiableIterable(primitiveViewT.get(null));}
public Map<K, Iterable<V>> beam_f18668_0(MultimapView<Void, KV<K, V>> primitiveViewT)
{            Multimap<K, V> multimap = ArrayListMultimap.create();    for (KV<K, V> elem : primitiveViewT.get(null)) {        multimap.put(elem.getKey(), elem.getValue());    }        @SuppressWarnings({ "unchecked", "rawtypes" })    Map<K, Iterable<V>> resultMap = (Map) multimap.asMap();    return Collections.unmodifiableMap(resultMap);}
public TypeDescriptor<Map<K, Iterable<V>>> beam_f18669_0()
{    return TypeDescriptors.maps(keyTypeDescriptorSupplier.get(), TypeDescriptors.iterables(valueTypeDescriptorSupplier.get()));}
public Materialization<MultimapView<Void, KV<K, V>>> beam_f18670_0()
{    return Materializations.multimap();}
public Coder<?> beam_f18678_0()
{    return coder;}
public int beam_f18679_0()
{    return Objects.hash(tag);}
public boolean beam_f18680_0(Object other)
{    if (!(other instanceof PCollectionView)) {        return false;    }    @SuppressWarnings("unchecked")    PCollectionView<?> otherView = (PCollectionView<?>) other;    return tag.equals(otherView.getTagInternal());}
 boolean beam_f18689_0()
{    return finishedSpecifying;}
public void beam_f18690_0(PInput input, PTransform<?, ?> transform)
{    finishedSpecifying = true;}
public String beam_f18691_0()
{    return (name == null ? "<unnamed>" : getName()) + " [" + getKindString() + "]";}
public Short beam_f18699_0(String fieldName)
{    return getInt16(getSchema().indexOf(fieldName));}
public Integer beam_f18700_0(String fieldName)
{    return getInt32(getSchema().indexOf(fieldName));}
public Long beam_f18701_0(String fieldName)
{    return getInt64(getSchema().indexOf(fieldName));}
public Map<T1, T2> beam_f18709_0(String fieldName)
{    return getMap(getSchema().indexOf(fieldName));}
public Row beam_f18710_0(String fieldName)
{    return getRow(getSchema().indexOf(fieldName));}
public Byte beam_f18711_0(int idx)
{    return getValue(idx);}
public ReadableDateTime beam_f18719_0(int idx)
{    ReadableInstant instant = getValue(idx);    return instant == null ? null : new DateTime(instant).withZone(instant.getZone());}
public BigDecimal beam_f18720_0(int idx)
{    return getValue(idx);}
public Boolean beam_f18721_0(int idx)
{    return getValue(idx);}
 static int beam_f18729_0(Object a, Schema.FieldType fieldType)
{    if (a == null) {        return 0;    } else if (fieldType.getTypeName() == TypeName.LOGICAL_TYPE) {        return deepHashCode(a, fieldType.getLogicalType().getBaseType());    } else if (fieldType.getTypeName() == Schema.TypeName.BYTES) {        return Arrays.hashCode((byte[]) a);    } else if (fieldType.getTypeName() == Schema.TypeName.ARRAY) {        return deepHashCodeForList((List<Object>) a, fieldType.getCollectionElementType());    } else if (fieldType.getTypeName() == Schema.TypeName.MAP) {        return deepHashCodeForMap((Map<Object, Object>) a, fieldType.getMapKeyType(), fieldType.getMapValueType());    } else {        return Objects.hashCode(a);    }}
 static boolean beam_f18730_0(Map<K, V> a, Map<K, V> b, Schema.FieldType valueType)
{    if (a == b) {        return true;    }    if (a.size() != b.size()) {        return false;    }    for (Map.Entry<K, V> e : a.entrySet()) {        K key = e.getKey();        V value = e.getValue();        V otherValue = b.get(key);        if (value == null) {            if (otherValue != null || !b.containsKey(key)) {                return false;            }        } else {            if (!deepEquals(value, otherValue, valueType)) {                return false;            }        }    }    return true;}
 static int beam_f18731_0(Map<Object, Object> a, Schema.FieldType keyType, Schema.FieldType valueType)
{    int h = 0;    for (Map.Entry<Object, Object> e : a.entrySet()) {        Object key = e.getKey();        Object value = e.getValue();        h += deepHashCode(key, keyType) ^ deepHashCode(value, valueType);    }    return h;}
public Builder beam_f18739_0(List<Object> values)
{    this.values.addAll(values);    return this;}
public Builder beam_f18740_0(Object... values)
{    return addValues(Arrays.asList(values));}
public Builder beam_f18741_0(List<T> values)
{    this.values.add(values);    return this;}
private Map<Object, Object> beam_f18749_0(Object value, FieldType keyType, FieldType valueType, String fieldName)
{    boolean valueTypeNullable = valueType.getNullable();    if (!(value instanceof Map)) {        throw new IllegalArgumentException(String.format("For field name %s and map type expected Map class. Instead " + "class type was %s.", fieldName, value.getClass()));    }    Map<Object, Object> valueMap = (Map<Object, Object>) value;    Map<Object, Object> verifiedMap = Maps.newHashMapWithExpectedSize(valueMap.size());    for (Entry<Object, Object> kv : valueMap.entrySet()) {        if (kv.getValue() == null) {            if (!valueTypeNullable) {                throw new IllegalArgumentException(String.format("%s is not nullable in Map field %s", valueType, fieldName));            }            verifiedMap.put(verify(kv.getKey(), keyType, fieldName), null);        } else {            verifiedMap.put(verify(kv.getKey(), keyType, fieldName), verify(kv.getValue(), valueType, fieldName));        }    }    return verifiedMap;}
private Row beam_f18750_0(Object value, String fieldName)
{    if (!(value instanceof Row)) {        throw new IllegalArgumentException(String.format("For field name %s expected Row type. " + "Instead class type was %s.", fieldName, value.getClass()));    }        return (Row) value;}
private Object beam_f18751_0(Object value, TypeName type, String fieldName)
{    if (type.isDateType()) {        return verifyDateTime(value, fieldName);    } else {        switch(type) {            case BYTE:                if (value instanceof Byte) {                    return value;                }                break;            case BYTES:                if (value instanceof ByteBuffer) {                    return ((ByteBuffer) value).array();                } else if (value instanceof byte[]) {                    return (byte[]) value;                }                break;            case INT16:                if (value instanceof Short) {                    return value;                }                break;            case INT32:                if (value instanceof Integer) {                    return value;                }                break;            case INT64:                if (value instanceof Long) {                    return value;                }                break;            case DECIMAL:                if (value instanceof BigDecimal) {                    return value;                }                break;            case FLOAT:                if (value instanceof Float) {                    return value;                }                break;            case DOUBLE:                if (value instanceof Double) {                    return value;                }                break;            case STRING:                if (value instanceof String) {                    return value;                }                break;            case BOOLEAN:                if (value instanceof Boolean) {                    return value;                }                break;            default:                                throw new IllegalArgumentException(String.format("Not a primitive type for field name %s: %s", fieldName, type));        }        throw new IllegalArgumentException(String.format("For field name %s and type %s found incorrect class type %s", fieldName, type, value.getClass()));    }}
private T beam_f18759_0(FieldType type, Object fieldValue, @Nullable Integer cacheKey)
{    if (type.getTypeName().equals(TypeName.ROW)) {        return (T) new RowWithGetters(type.getRowSchema(), fieldValueGetterFactory, fieldValue);    } else if (type.getTypeName().equals(TypeName.ARRAY)) {        return cacheKey != null ? (T) cachedLists.computeIfAbsent(cacheKey, i -> getListValue(type.getCollectionElementType(), fieldValue)) : (T) getListValue(type.getCollectionElementType(), fieldValue);    } else if (type.getTypeName().equals(TypeName.MAP)) {        Map map = (Map) fieldValue;        return cacheKey != null ? (T) cachedMaps.computeIfAbsent(cacheKey, i -> getMapValue(type.getMapKeyType(), type.getMapValueType(), map)) : (T) getMapValue(type.getMapKeyType(), type.getMapValueType(), map);    } else {        return (T) fieldValue;    }}
public int beam_f18760_0()
{    return getters.size();}
public List<Object> beam_f18761_0()
{    return getters.stream().map(g -> g.get(getterTarget)).collect(Collectors.toList());}
public static ShardedKey<K> beam_f18769_0(K key, int shardNumber)
{    return new ShardedKey<>(key, shardNumber);}
public K beam_f18770_0()
{    return key;}
public int beam_f18771_0()
{    return shardNumber;}
public V beam_f18779_0()
{    return value;}
public Instant beam_f18780_0()
{    return timestamp;}
public boolean beam_f18781_0(Object other)
{    if (!(other instanceof TimestampedValue)) {        return false;    }    TimestampedValue<?> that = (TimestampedValue<?>) other;    return Objects.equals(value, that.value) && Objects.equals(timestamp, that.timestamp);}
public List<? extends Coder<?>> beam_f18789_0()
{    return Arrays.<Coder<?>>asList(valueCoder);}
public Coder<T> beam_f18790_0()
{    return valueCoder;}
public TypeDescriptor<TimestampedValue<T>> beam_f18791_0()
{    return new TypeDescriptor<TimestampedValue<T>>() {    }.where(new TypeParameter<T>() {    }, valueCoder.getEncodedTypeDescriptor());}
public String beam_f18799_0()
{    return "Tag<" + id + ">";}
public static TupleTagList beam_f18800_0()
{    return new TupleTagList();}
public static TupleTagList beam_f18801_0(TupleTag<?> tag)
{    return empty().and(tag);}
private boolean beam_f18809_0(Type type)
{    if (type instanceof TypeVariable) {        return true;    } else if (type instanceof ParameterizedType) {        ParameterizedType param = (ParameterizedType) type;        for (Type arg : param.getActualTypeArguments()) {            if (hasUnresolvedParameters(arg)) {                return true;            }        }    }    return false;}
private Object beam_f18810_0(Field field, Object instance)
{    if (!field.isSynthetic()) {        return null;    }    boolean accessible = field.isAccessible();    try {        field.setAccessible(true);        return field.get(instance);    } catch (IllegalArgumentException | IllegalAccessException e) {                return null;    } finally {        field.setAccessible(accessible);    }}
public static TypeDescriptor<T> beam_f18811_0(Class<T> type)
{    return new SimpleTypeDescriptor<>(TypeToken.of(type));}
public final boolean beam_f18819_0(TypeDescriptor<?> source)
{    return token.isSupertypeOf(source.token);}
public final boolean beam_f18820_0(TypeDescriptor<?> parent)
{    return token.isSubtypeOf(parent.token);}
public List<TypeDescriptor<?>> beam_f18821_0(Method method)
{    Invokable<?, ?> typedMethod = token.method(method);    List<TypeDescriptor<?>> argTypes = Lists.newArrayList();    for (Parameter parameter : typedMethod.getParameters()) {        argTypes.add(new SimpleTypeDescriptor<>(parameter.getType()));    }    return argTypes;}
public String beam_f18829_0()
{    return token.toString();}
public boolean beam_f18830_0(Object other)
{    if (!(other instanceof TypeDescriptor)) {        return false;    } else {        @SuppressWarnings("unchecked")        TypeDescriptor<?> descriptor = (TypeDescriptor<?>) other;        return token.equals(descriptor.token);    }}
public int beam_f18831_0()
{    return token.hashCode();}
public static TypeDescriptor<BigInteger> beam_f18839_0()
{    return new TypeDescriptor<BigInteger>() {    };}
public static TypeDescriptor<Row> beam_f18840_0()
{    return new TypeDescriptor<Row>() {    };}
public static TypeDescriptor<String> beam_f18841_0()
{    return new TypeDescriptor<String>() {    };}
public static TypeDescriptor<Iterable<T>> beam_f18849_0(TypeDescriptor<T> iterable)
{    return new TypeDescriptor<Iterable<T>>() {    }.where(new TypeParameter<T>() {    }, iterable);}
public static TypeDescriptor<Void> beam_f18850_0()
{    return new TypeDescriptor<Void>() {    };}
public static TypeDescriptor<V> beam_f18851_0(T instance, Class<? super T> supertype, TypeVariableExtractor<T, V> extractor)
{    return extractFromTypeParameters((TypeDescriptor<T>) TypeDescriptor.of(instance.getClass()), supertype, extractor);}
public int beam_f18859_0()
{    return typeVariable.hashCode();}
public boolean beam_f18860_0(Object obj)
{    if (!(obj instanceof TypeParameter)) {        return false;    }    TypeParameter<?> that = (TypeParameter<?>) obj;    return typeVariable.equals(that.typeVariable);}
public String beam_f18861_0()
{    return typeVariable.toString();}
public List<? extends org.apache.beam.sdk.coders.Coder<?>> beam_f18869_0()
{        return ImmutableList.of(valueCoder, windowCoder);}
public void beam_f18870_0() throws NonDeterministicException
{    valueCoder.verifyDeterministic();    windowCoder.verifyDeterministic();}
public ValueT beam_f18871_0()
{    return value;}
public void beam_f18879_0(ValueWithRecordId<ValueT> value, OutputStream outStream, Context context) throws IOException
{    valueCoder.encode(value.value, outStream);    idCoder.encode(value.id, outStream, context);}
public ValueWithRecordId<ValueT> beam_f18880_0(InputStream inStream) throws IOException
{    return decode(inStream, Context.NESTED);}
public ValueWithRecordId<ValueT> beam_f18881_0(InputStream inStream, Context context) throws IOException
{    return new ValueWithRecordId<>(valueCoder.decode(inStream), idCoder.decode(inStream, context));}
public boolean beam_f18889_0()
{    return triggerSpecified;}
public Duration beam_f18890_0()
{    return allowedLateness;}
public boolean beam_f18891_0()
{    return allowedLatenessSpecified;}
public WindowingStrategy<T, W> beam_f18899_0(AccumulationMode mode)
{    return new WindowingStrategy<>(windowFn, trigger, triggerSpecified, mode, true, allowedLateness, allowedLatenessSpecified, timestampCombiner, timestampCombinerSpecified, closingBehavior, onTimeBehavior);}
public WindowingStrategy<T, W> beam_f18900_0(WindowFn<?, ?> wildcardWindowFn)
{    @SuppressWarnings("unchecked")    WindowFn<T, W> typedWindowFn = (WindowFn<T, W>) wildcardWindowFn;    return new WindowingStrategy<>(typedWindowFn, trigger, triggerSpecified, mode, modeSpecified, allowedLateness, allowedLatenessSpecified, timestampCombiner, timestampCombinerSpecified, closingBehavior, onTimeBehavior);}
public WindowingStrategy<T, W> beam_f18901_0(Duration allowedLateness)
{    return new WindowingStrategy<>(windowFn, trigger, triggerSpecified, mode, modeSpecified, allowedLateness, true, timestampCombiner, timestampCombinerSpecified, closingBehavior, onTimeBehavior);}
public boolean beam_f18909_0(Object o)
{    if (this == o) {        return true;    }    if (o == null || getClass() != o.getClass()) {        return false;    }    Pojo pojo = (Pojo) o;    if (count != pojo.count) {        return false;    }    if (text != null ? !text.equals(pojo.text) : pojo.text != null) {        return false;    }    return true;}
public int beam_f18910_0()
{    return 0;}
public String beam_f18911_0()
{    return "Pojo{" + "text='" + text + '\'' + ", count=" + count + '}';}
public void beam_f18919_0() throws Exception
{                Pojo before = new Pojo("Hello", 42);    AvroCoder<Pojo> coder = AvroCoder.of(Pojo.class);    SerializableCoder<Integer> intCoder = SerializableCoder.of(Integer.class);    ByteArrayOutputStream outStream = new ByteArrayOutputStream();    Context context = Context.NESTED;    coder.encode(before, outStream, context);    intCoder.encode(10, outStream, context);    ByteArrayInputStream inStream = new ByteArrayInputStream(outStream.toByteArray());    Pojo after = coder.decode(inStream, context);    Assert.assertEquals(before, after);    Integer intAfter = intCoder.decode(inStream, context);    Assert.assertEquals(Integer.valueOf(10), intAfter);}
public void beam_f18920_0() throws Exception
{            PCollection<String> output = pipeline.apply(Create.of(new Pojo("hello", 1), new Pojo("world", 2))).apply(ParDo.of(new GetTextFn()));    PAssert.that(output).containsInAnyOrder("hello", "world");    pipeline.run();}
public void beam_f18921_0() throws Exception
{    AvroCoder<Pojo> coder = AvroCoder.of(Pojo.class);        SerializableUtils.ensureSerializable(coder);}
protected boolean beam_f18929_0(String item)
{    return item.startsWith(prefix + ":") && item.contains(messagePart);}
private Matcher<String> beam_f18930_0(Class<?> clazz, String message)
{    return reason(clazz.getName(), message);}
private Matcher<String> beam_f18931_0(Class<?> clazz, String field, String message)
{    return reason(clazz.getName() + "#" + field, message);}
public void beam_f18939_0()
{        assertNonDeterministic(AvroCoder.of(LinkedHashMapField.class), reasonField(LinkedHashMapField.class, "nonDeterministicMap", "java.util.LinkedHashMap<java.lang.String, java.lang.String> " + "may not be deterministically ordered"));}
public void beam_f18940_0()
{    assertNonDeterministic(AvroCoder.of(StringCollection.class), reasonField(StringCollection.class, "stringCollection", "java.util.Collection<java.lang.String> may not be deterministically ordered"));}
public void beam_f18941_0()
{    assertDeterministic(AvroCoder.of(StringList.class));    assertDeterministic(AvroCoder.of(StringArrayList.class));}
public void beam_f18949_0() throws Exception, NonDeterministicException
{    TreeMapField size1 = new TreeMapField();    TreeMapField size2 = new TreeMapField();        size1.field.put("hello", "world");    size1.field.put("another", "entry");    size2.field.put("another", "entry");    size2.field.put("hello", "world");    AvroCoder<TreeMapField> coder = AvroCoder.of(TreeMapField.class);    coder.verifyDeterministic();    ByteArrayOutputStream outStream1 = new ByteArrayOutputStream();    ByteArrayOutputStream outStream2 = new ByteArrayOutputStream();    Context context = Context.NESTED;    coder.encode(size1, outStream1, context);    coder.encode(size2, outStream2, context);    assertArrayEquals(outStream1.toByteArray(), outStream2.toByteArray());}
public void beam_f18950_0()
{    assertDeterministic(AvroCoder.of(SchemaBuilder.record("someRecord").fields().endRecord()));    assertDeterministic(AvroCoder.of(SchemaBuilder.record("someRecord").fields().name("int").type().intType().noDefault().endRecord()));    assertDeterministic(AvroCoder.of(SchemaBuilder.record("someRecord").fields().name("string").type().stringType().noDefault().endRecord()));    assertNonDeterministic(AvroCoder.of(SchemaBuilder.record("someRecord").fields().name("map").type().map().values().stringType().noDefault().endRecord()), reason("someRecord.map", "HashMap to represent MAPs"));    assertDeterministic(AvroCoder.of(SchemaBuilder.record("someRecord").fields().name("array").type().array().items().stringType().noDefault().endRecord()));    assertDeterministic(AvroCoder.of(SchemaBuilder.record("someRecord").fields().name("enum").type().enumeration("anEnum").symbols("s1", "s2").enumDefault("s1").endRecord()));    assertDeterministic(AvroCoder.of(SchemaBuilder.unionOf().intType().and().record("someRecord").fields().nullableString("someField", "").endRecord().endUnion()));}
public void beam_f18951_0()
{        assertDeterministic(AvroCoder.of(SchemaBuilder.record("someRecord").fields().name("string").prop(SpecificData.CLASS_PROP, "java.lang.String").type().stringType().noDefault().endRecord()));    assertNonDeterministic(AvroCoder.of(SchemaBuilder.record("someRecord").fields().name("string").prop(SpecificData.CLASS_PROP, "unknownString").type().stringType().noDefault().endRecord()), reason("someRecord.string", "unknownString is not known to be deterministic"));        assertNonDeterministic(AvroCoder.of(SchemaBuilder.unionOf().intType().and().record("someRecord").fields().name("someField").prop(SpecificData.CLASS_PROP, "unknownString").type().stringType().noDefault().endRecord().endUnion()), reason("someRecord.someField", "unknownString is not known to be deterministic"));}
public void beam_f18959_0() throws Exception
{    Schema fooSchema = AvroCoder.of(Foo.class).getSchema();    Schema schema = new Schema.Parser().parse("{" + "\"type\":\"record\"," + "\"name\":\"SomeGeneric\"," + "\"namespace\":\"ns\"," + "\"fields\":[" + "  {\"name\":\"foo\", \"type\":" + fooSchema.toString() + "}" + "]}");    @SuppressWarnings("rawtypes")    AvroCoder<SomeGeneric> coder = AvroCoder.of(SomeGeneric.class, schema);    assertNonDeterministic(coder, reasonField(SomeGeneric.class, "foo", "erasure"));}
public void beam_f18960_0() throws Exception
{    AvroCoder<Pojo> coder = AvroCoder.of(Pojo.class);    assertThat(coder.getEncodedTypeDescriptor(), equalTo(TypeDescriptor.of(Pojo.class)));}
public boolean beam_f18961_0(Object other)
{    return (other instanceof AvroCoderTestPojo) && ((AvroCoderTestPojo) other).text.equals(text);}
public void beam_f18969_0() throws Exception
{    CoderProperties.coderEncodesBase64(TEST_CODER, TEST_VALUES, TEST_ENCODINGS);}
public void beam_f18970_0() throws Exception
{    thrown.expect(CoderException.class);    thrown.expectMessage("cannot encode a null Integer");    CoderUtils.encodeToBase64(TEST_CODER, null);}
public void beam_f18971_0() throws Exception
{    assertThat(TEST_CODER.getEncodedTypeDescriptor(), equalTo(TypeDescriptor.of(Integer.class)));}
public void beam_f18979_0() throws Exception
{    thrown.expect(NullPointerException.class);    thrown.expectMessage("cannot encode a null BigInteger");    CoderUtils.encodeToBase64(TEST_CODER, null);}
public void beam_f18980_0() throws Exception
{    for (BitSet value : TEST_VALUES) {        CoderProperties.coderDecodeEncodeEqual(TEST_CODER, value);    }}
public void beam_f18981_0() throws Exception
{    CoderProperties.testByteCount(ByteArrayCoder.of(), Coder.Context.OUTER, new byte[][] { { 0xa, 0xb, 0xc } });    CoderProperties.testByteCount(ByteArrayCoder.of(), Coder.Context.NESTED, new byte[][] { { 0xa, 0xb, 0xc }, {}, {}, { 0xd, 0xe }, {} });}
public void beam_f18989_0() throws Exception
{    byte[] input = { 0x7, 0x3, 0xA, 0xf };    byte[] encoded = CoderUtils.encodeToByteArray(TEST_CODER, input);    input[1] = 0x9;    byte[] decoded = CoderUtils.decodeFromByteArray(TEST_CODER, encoded);        assertThat(input, not(equalTo(decoded)));}
public void beam_f18990_0() throws Exception
{    for (byte[] value : TEST_VALUES) {        byte[] encodedSlow = CoderUtils.encodeToByteArray(TEST_CODER, value);        byte[] encodedFast = encodeToByteArrayAndOwn(TEST_CODER, value);        assertThat(encodedSlow, equalTo(encodedFast));    }}
private static byte[] beam_f18991_0(ByteArrayCoder coder, byte[] value) throws IOException
{    return encodeToByteArrayAndOwn(coder, value, Coder.Context.OUTER);}
public void beam_f18999_0() throws Exception
{    assertThat(TEST_CODER.getEncodedTypeDescriptor(), equalTo(TypeDescriptor.of(Byte.class)));}
public void beam_f19000_0() throws Exception
{    CoderProvider factory = CoderProviders.fromStaticMethods(String.class, StringUtf8Coder.class);    assertEquals(StringUtf8Coder.of(), factory.coderFor(TypeDescriptors.strings(), Collections.emptyList()));    factory = CoderProviders.fromStaticMethods(Double.class, DoubleCoder.class);    assertEquals(DoubleCoder.of(), factory.coderFor(TypeDescriptors.doubles(), Collections.emptyList()));    factory = CoderProviders.fromStaticMethods(byte[].class, ByteArrayCoder.class);    assertEquals(ByteArrayCoder.of(), factory.coderFor(TypeDescriptor.of(byte[].class), Collections.emptyList()));}
public void beam_f19001_0() throws Exception
{    TypeDescriptor<KV<Double, Double>> type = TypeDescriptors.kvs(TypeDescriptors.doubles(), TypeDescriptors.doubles());    CoderProvider kvCoderProvider = CoderProviders.fromStaticMethods(KV.class, KvCoder.class);    assertEquals(KvCoder.of(DoubleCoder.of(), DoubleCoder.of()), kvCoderProvider.coderFor(type, Arrays.asList(DoubleCoder.of(), DoubleCoder.of())));}
public void beam_f19009_0() throws Exception
{    CoderRegistry registry = CoderRegistry.createDefault();    TypeDescriptor<Map<Integer, Map<String, Double>>> mapToken = new TypeDescriptor<Map<Integer, Map<String, Double>>>() {    };    assertEquals(MapCoder.of(VarIntCoder.of(), MapCoder.of(StringUtf8Coder.of(), DoubleCoder.of())), registry.getCoder(mapToken));}
public void beam_f19010_0() throws Exception
{    CoderRegistry registry = CoderRegistry.createDefault();    TypeDescriptor<Set<Integer>> setToken = new TypeDescriptor<Set<Integer>>() {    };    assertEquals(SetCoder.of(VarIntCoder.of()), registry.getCoder(setToken));}
public void beam_f19011_0() throws Exception
{    CoderRegistry registry = CoderRegistry.createDefault();    TypeDescriptor<Set<Set<Integer>>> setToken = new TypeDescriptor<Set<Set<Integer>>>() {    };    assertEquals(SetCoder.of(SetCoder.of(VarIntCoder.of())), registry.getCoder(setToken));}
public List<? extends Coder<?>> beam_f19019_0()
{    return ImmutableList.<Coder<?>>builder().addAll(super.getCoderArguments()).add(BigEndianLongCoder.of()).build();}
public void beam_f19020_0() throws Exception
{    thrown.expect(IncompatibleCoderException.class);    thrown.expectMessage("type parameters");    thrown.expectMessage("less than the number of coder arguments");    CoderRegistry.verifyCompatible(new TooManyComponentCoders<>(BigEndianIntegerCoder.of()), List.class);}
public void beam_f19021_0() throws Exception
{    thrown.expect(IncompatibleCoderException.class);    thrown.expectMessage("component coder is incompatible");    CoderRegistry.verifyCompatible(ListCoder.of(BigEndianIntegerCoder.of()), new TypeDescriptor<List<String>>() {    }.getType());}
public void beam_f19031_0() throws Exception
{    pipeline.apply(Create.of("hello", "goodbye")).apply(new GenericOutputMySerializedGeneric<String>());    pipeline.run();}
public static MyValueCoder beam_f19032_0()
{    return INSTANCE;}
public MyValue beam_f19034_0(InputStream inStream) throws CoderException, IOException
{    return new MyValue();}
public List<CoderProvider> beam_f19044_0()
{    return ImmutableList.of(CoderProviders.forCoder(TypeDescriptor.of(AutoRegistrationClass.class), AutoRegistrationClassCoder.INSTANCE));}
public void beam_f19045_0() throws Exception
{    CoderRegistry registry = CoderRegistry.createDefault();        assertEquals(AvroCoder.of(MyValueA.class), registry.getCoder(MyValueA.class));        assertEquals(MyValueBCoder.INSTANCE, registry.getCoder(MyValueB.class));        assertEquals(SerializableCoder.of(MyValueC.class), registry.getCoder(MyValueC.class));}
public MyValueA beam_f19047_0(InputStream inStream) throws CoderException, IOException
{    return null;}
public void beam_f19056_0()
{    NonDeterministicException rootCause = new NonDeterministicException(VoidCoder.of(), "Root Cause");    NonDeterministicException exception = new NonDeterministicException(StringUtf8Coder.of(), Arrays.asList("Problem1", "Problem2"), rootCause);    String expectedMessage = "StringUtf8Coder is not deterministic because:\n\tProblem1\n\tProblem2";    assertThat(exception.getMessage(), equalTo(expectedMessage));}
public void beam_f19057_0() throws Exception
{    assertThat(VoidCoder.of().getEncodedTypeDescriptor(), equalTo(TypeDescriptor.of(Void.class)));}
public void beam_f19058_0() throws Exception
{    for (Collection<Integer> value : TEST_VALUES) {        CoderProperties.coderDecodeEncodeContentsEqual(TEST_CODER, value);    }}
public int beam_f19066_0()
{    return key.hashCode();}
public void beam_f19067_0() throws Exception
{    MyCustomCoder coder = new MyCustomCoder("key");    CoderProperties.coderDecodeEncodeEqual(coder, KV.of("key", 3L));    byte[] encoded2 = CoderUtils.encodeToByteArray(coder, KV.of("ignored", 3L));    Assert.assertEquals(KV.of("key", 3L), CoderUtils.decodeFromByteArray(coder, encoded2));}
public void beam_f19068_0() throws Exception
{    SerializableUtils.ensureSerializable(new MyCustomCoder("key"));}
public void beam_f19076_0() throws Exception
{    CoderRegistry registry = CoderRegistry.createDefault();    registry.registerCoderProvider(new DefaultCoderProvider());    Coder<List<AvroRecord>> avroRecordCoder = registry.getCoder(new TypeDescriptor<List<AvroRecord>>() {    });    assertThat(avroRecordCoder, instanceOf(ListCoder.class));    assertThat(((ListCoder) avroRecordCoder).getElemCoder(), instanceOf(AvroCoder.class));    assertThat(registry.getCoder(new TypeDescriptor<List<SerializableRecord>>() {    }), Matchers.equalTo(ListCoder.of(SerializableCoder.of(SerializableRecord.class))));}
public void beam_f19077_0() throws Exception
{    thrown.expect(CannotProvideCoderException.class);    new DefaultCoderProvider().coderFor(TypeDescriptor.of(Unknown.class), Collections.emptyList());}
public List<Integer> beam_f19078_0(Set<Integer> input)
{    return Lists.newArrayList(input);}
public void beam_f19086_0() throws Exception
{    CoderProperties.coderSerializable(TEST_CODER);}
public void beam_f19087_0() throws Exception
{    DelegateCoder.CodingFunction<Integer, Integer> identityFn = input -> input;    Coder<Integer> varIntCoder1 = DelegateCoder.of(VarIntCoder.of(), identityFn, identityFn);    Coder<Integer> varIntCoder2 = DelegateCoder.of(VarIntCoder.of(), identityFn, identityFn);    Coder<Integer> bigEndianIntegerCoder = DelegateCoder.of(BigEndianIntegerCoder.of(), identityFn, identityFn);    assertEquals(varIntCoder1, varIntCoder2);    assertEquals(varIntCoder1.hashCode(), varIntCoder2.hashCode());    assertNotEquals(varIntCoder1, bigEndianIntegerCoder);    assertNotEquals(varIntCoder1.hashCode(), bigEndianIntegerCoder.hashCode());}
public void beam_f19088_0() throws Exception
{    assertThat(DelegateCoder.of(StringUtf8Coder.of(), String::valueOf, Integer::valueOf, new TypeDescriptor<Integer>() {    }).getEncodedTypeDescriptor(), equalTo(TypeDescriptor.of(Integer.class)));}
public void beam_f19096_0() throws Exception
{    thrown.expect(CoderException.class);    thrown.expectMessage("cannot encode a null ReadableDuration");    CoderUtils.encodeToBase64(TEST_CODER, null);}
public void beam_f19097_0() throws Exception
{    assertThat(TEST_CODER.getEncodedTypeDescriptor(), equalTo(TypeDescriptor.of(ReadableDuration.class)));}
public void beam_f19098_0() throws Exception
{    for (Float value : TEST_VALUES) {        CoderProperties.coderDecodeEncodeEqual(TEST_CODER, value);    }}
public void beam_f19106_0() throws Exception
{    assertThat(TEST_CODER.getEncodedTypeDescriptor(), equalTo(TypeDescriptor.of(Instant.class)));}
public void beam_f19107_0() throws Exception
{    CoderProperties.coderSerializable(ListCoder.of(GlobalWindow.Coder.INSTANCE));}
public void beam_f19108_0() throws Exception
{    for (Iterable<Integer> value : TEST_VALUES) {        CoderProperties.coderDecodeEncodeContentsInSameOrder(TEST_CODER, value);    }}
public void beam_f19116_0() throws Exception
{    CoderProperties.coderEncodesBase64(TEST_CODER, TEST_VALUES, TEST_ENCODINGS);}
public void beam_f19117_0() throws Exception
{    thrown.expect(CoderException.class);    thrown.expectMessage("cannot encode a null KV");    CoderUtils.encodeToBase64(TEST_CODER, null);}
public void beam_f19118_0() throws Exception
{    TypeDescriptor<KV<String, Integer>> typeDescriptor = new TypeDescriptor<KV<String, Integer>>() {    };    assertThat(TEST_CODER.getEncodedTypeDescriptor(), equalTo(typeDescriptor));}
public void beam_f19126_0() throws Exception
{    for (byte[] value1 : TEST_VALUES) {        for (byte[] value2 : TEST_VALUES) {            CoderProperties.structuralValueConsistentWithEquals(TEST_CODER, value1, value2);        }    }}
public void beam_f19127_0() throws Exception
{    CoderProperties.coderEncodesBase64(TEST_CODER, TEST_VALUES, TEST_ENCODINGS);}
public void beam_f19128_0() throws Exception
{    LengthPrefixCoder<Long> lengthPrefixedValueCoder = LengthPrefixCoder.of(BigEndianLongCoder.of());    LengthPrefixCoder<byte[]> lengthPrefixedBytesCoder = LengthPrefixCoder.of(ByteArrayCoder.of());        byte[] userEncoded = CoderUtils.encodeToByteArray(lengthPrefixedValueCoder, 22L);        byte[] decodedToBytes = CoderUtils.decodeFromByteArray(lengthPrefixedBytesCoder, userEncoded);        byte[] reencodedBytes = CoderUtils.encodeToByteArray(lengthPrefixedBytesCoder, decodedToBytes);    long userDecoded = CoderUtils.decodeFromByteArray(lengthPrefixedValueCoder, reencodedBytes);    assertFalse("Length-prefix decoding to bytes should drop the length", Arrays.equals(userEncoded, decodedToBytes));    assertArrayEquals(userEncoded, reencodedBytes);    assertEquals(22L, userDecoded);}
public void beam_f19136_0() throws Exception
{    List<Integer> list = Arrays.asList(1, 2, 3, null, 4);    Coder<List<Integer>> coder = ListCoder.of(SerializableCoder.of(Integer.class));    CoderProperties.coderDecodeEncodeEqual(coder, list);}
public void beam_f19137_0() throws Exception
{    ListCoder<byte[]> coder = ListCoder.of(ByteArrayCoder.of());    List<byte[]> value = Collections.singletonList(new byte[] { 1, 2, 3, 4 });    CoderProperties.structuralValueDecodeEncodeEqual(coder, value);}
public void beam_f19138_0()
{    ListCoder<byte[]> coder = ListCoder.of(ByteArrayCoder.of());    assertFalse(coder.consistentWithEquals());}
public void beam_f19146_0()
{    MapCoder<Integer, byte[]> coder = MapCoder.of(VarIntCoder.of(), ByteArrayCoder.of());    assertFalse(coder.consistentWithEquals());}
public void beam_f19147_0()
{    MapCoder<Integer, Integer> coder = MapCoder.of(VarIntCoder.of(), VarIntCoder.of());    assertTrue(coder.consistentWithEquals());}
public void beam_f19148_0() throws Exception
{    TypeDescriptor<Map<Integer, String>> typeDescriptor = new TypeDescriptor<Map<Integer, String>>() {    };    assertThat(TEST_CODER.getEncodedTypeDescriptor(), equalTo(typeDescriptor));}
public void beam_f19156_0() throws Exception
{    NullableCoder<List<String>> coder = NullableCoder.of(ListCoder.of(StringUtf8Coder.of()));    assertFalse(coder.isRegisterByteSizeObserverCheap(ImmutableList.of("hi", "test")));}
public void beam_f19157_0() throws Exception
{    NullableCoder<List<String>> coder = NullableCoder.of(ListCoder.of(StringUtf8Coder.of()));    assertTrue(coder.isRegisterByteSizeObserverCheap(null));}
public void beam_f19158_0() throws Exception
{    CoderProperties.structuralValueConsistentWithEquals(TEST_CODER, null, null);}
public String beam_f19166_0(InputStream inStream, Context context) throws CoderException, IOException
{    checkArgument(context.isWholeStream, "Expected to get entire stream");    return StringUtf8Coder.of().decode(inStream, context);}
public List<? extends Coder<?>> beam_f19167_0()
{    return Collections.emptyList();}
public void beam_f19169_0(String value, OutputStream outStream) throws CoderException, IOException
{    throwIfPresent(encodingException);    StringUtf8Coder.of().encode(value, outStream);}
public void beam_f19177_0() throws Exception
{    Pipeline p = pipelineWith(new CustomTestCoder(null, NULL_POINTER_EXCEPTION, null, null, EXCEPTION_MESSAGE));    thrown.expect(Exception.class);    thrown.expect(new ExceptionMatcher("java.lang.NullPointerException: Super Unique Message!!!"));    p.run().waitUntilFinish();}
public void beam_f19178_0() throws Exception
{    Pipeline p = pipelineWith(new CustomTestCoder(null, null, IO_EXCEPTION, null, EXCEPTION_MESSAGE));    thrown.expect(Exception.class);    thrown.expect(new ExceptionMatcher("java.io.IOException: Super Unique Message!!!"));    p.run().waitUntilFinish();}
public void beam_f19179_0() throws Exception
{    Pipeline p = pipelineWith(new CustomTestCoder(null, null, NULL_POINTER_EXCEPTION, null, EXCEPTION_MESSAGE));    thrown.expect(Exception.class);    thrown.expect(new ExceptionMatcher("java.lang.NullPointerException: Super Unique Message!!!"));    p.run().waitUntilFinish();}
public boolean beam_f19187_0(Throwable result)
{    if (result.toString().contains(expectedError)) {        return true;    }    Throwable cause = result.getCause();    while (null != cause) {        String causeString = cause.toString();        if (causeString.contains(expectedError)) {            return true;        }        cause = cause.getCause();    }    return false;}
public void beam_f19188_0(Description descr)
{    descr.appendText("exception with text matching: " + expectedError);}
private static Field beam_f19189_0(Class<?> clazz, String fieldName) throws Exception
{    for (Field field : clazz.getDeclaredFields()) {        if (field.getName().equals(fieldName)) {            if (!Modifier.isPublic(field.getModifiers())) {                field.setAccessible(true);            }            return field;        }    }    throw new NoSuchFieldException(clazz.getCanonicalName() + "." + fieldName);}
public void beam_f19197_0() throws NonDeterministicException
{    Schema schema = Schema.builder().addField("f1", FieldType.DOUBLE).addField("f2", FieldType.FLOAT).addField("f3", FieldType.INT32).build();    RowCoder coder = RowCoder.of(schema);    coder.verifyDeterministic();}
public void beam_f19198_0() throws NonDeterministicException
{    Schema schema = Schema.builder().addField("f1", FieldType.row(Schema.builder().addField("a1", FieldType.DOUBLE).addField("a2", FieldType.INT64).build())).build();    RowCoder coder = RowCoder.of(schema);    coder.verifyDeterministic();}
public void beam_f19199_0() throws Exception
{    Schema schema = Schema.of(Schema.Field.of("f1", FieldType.BYTES));    Row row1 = Row.withSchema(schema).addValue(new byte[] { 1, 2, 3, 4 }).build();    Row row2 = Row.withSchema(schema).addValue(new byte[] { 1, 2, 3, 4 }).build();    RowCoder coder = RowCoder.of(schema);    Assume.assumeTrue(coder.consistentWithEquals());    CoderProperties.coderConsistentWithEquals(coder, row1, row2);}
public void beam_f19207_0(ProcessContext c)
{    c.output(new MyRecord(c.element()));}
public void beam_f19208_0(ProcessContext c)
{    c.output(c.element().value);}
public void beam_f19209_0() throws Exception
{    IterableCoder<MyRecord> coder = IterableCoder.of(SerializableCoder.of(MyRecord.class));    List<MyRecord> records = new ArrayList<>();    for (String l : LINES) {        records.add(new MyRecord(l));    }    byte[] encoded = CoderUtils.encodeToByteArray(coder, records);    Iterable<MyRecord> decoded = CoderUtils.decodeFromByteArray(coder, encoded);    assertEquals(records, decoded);}
public void beam_f19217_0() throws Exception
{    assertThat(SerializableCoder.of(MyRecord.class).getEncodedTypeDescriptor(), Matchers.equalTo(TypeDescriptor.of(MyRecord.class)));}
public void beam_f19218_0() throws Exception
{    assertThat(CoderRegistry.createDefault().getCoder(AutoRegistration.class), instanceOf(SerializableCoder.class));}
public void beam_f19219_0() throws Exception
{    String expectedLogMessage = "Can't verify serialized elements of type TestInterface " + "have well defined equals method.";        SerializableCoder.of(TestInterface.class);    SerializableCoder.of(TestInterface.class);    SerializableCoder.of(TestInterface.class);    expectedLogs.verifyLogRecords(new TypeSafeMatcher<Iterable<LogRecord>>() {        @Override        public void describeTo(Description description) {            description.appendText(String.format("single warn log message containing [%s]", expectedLogMessage));        }        @Override        protected boolean matchesSafely(Iterable<LogRecord> item) {            int count = 0;            for (LogRecord logRecord : item) {                if (logRecord.getLevel().equals(Level.WARNING) && logRecord.getMessage().contains(expectedLogMessage)) {                    count += 1;                }            }            return count == 1;        }    });}
public void beam_f19227_0() throws Exception
{    SerializableCoder.of(ProperEquals.class);    expectedLogs.verifyNotLogged("Can't verify serialized elements of type");}
public void beam_f19228_0() throws Exception
{    final SerializableCoder<String> coder = SerializableCoder.of(String.class);    final OutputStream outputStream = mock(OutputStream.class, (Answer) invocationOnMock -> {        throw new IOException();    });    coder.encode("", outputStream);}
public void beam_f19229_0() throws Exception
{    CoderProperties.coderSerializable(SetCoder.of(GlobalWindow.Coder.INSTANCE));}
public void beam_f19237_0() throws Exception
{    assertThat(StringDelegateCoder.of(URI.class).getEncodedTypeDescriptor(), equalTo(TypeDescriptor.of(URI.class)));}
public void beam_f19238_0() throws Exception
{    for (String value : TEST_VALUES) {        CoderProperties.coderDecodeEncodeEqual(TEST_CODER, value);    }}
public void beam_f19239_0() throws Exception
{    CoderProperties.coderEncodesBase64(TEST_CODER, TEST_VALUES, TEST_ENCODINGS);}
public boolean beam_f19248_0()
{    return value;}
public void beam_f19249_0(@Nullable ObjectIdentityBoolean value, OutputStream outStream) throws CoderException, IOException
{    if (value == null) {        outStream.write(2);    } else if (value.getValue()) {        outStream.write(1);    } else {        outStream.write(0);    }}
public ObjectIdentityBoolean beam_f19250_0(InputStream inStream) throws CoderException, IOException
{    int value = inStream.read();    if (value == 0) {        return new ObjectIdentityBoolean(false);    } else if (value == 1) {        return new ObjectIdentityBoolean(true);    } else if (value == 2) {        return null;    }    throw new CoderException("Invalid value for nullable Boolean: " + value);}
public void beam_f19259_0(T value, OutputStream outStream) throws CoderException, IOException
{    throw new UnsupportedOperationException();}
public T beam_f19260_0(InputStream inStream) throws CoderException, IOException
{    throw new UnsupportedOperationException();}
public List<? extends Coder<?>> beam_f19261_0()
{    throw new UnsupportedOperationException();}
public void beam_f19270_0() throws Exception
{    assertThat(TEST_CODER.getEncodedTypeDescriptor(), equalTo(TypeDescriptor.of(Integer.class)));}
public void beam_f19271_0() throws Exception
{    for (Long value : TEST_VALUES) {        CoderProperties.coderDecodeEncodeEqual(TEST_CODER, value);    }}
public void beam_f19272_0() throws Exception
{    CoderProperties.coderEncodesBase64(TEST_CODER, TEST_VALUES, TEST_ENCODINGS);}
public void beam_f19280_0()
{    AvroIO.Write<String> write = AvroIO.write(String.class).to("/tmp/foo/baz").withCodec(CodecFactory.deflateCodec(9));    assertEquals(CodecFactory.deflateCodec(9).toString(), SerializableUtils.clone(write.inner.getCodec()).getCodec().toString());}
public void beam_f19281_0()
{    AvroIO.Write<String> write = AvroIO.write(String.class).to("/tmp/foo/baz").withCodec(CodecFactory.xzCodec(9));    assertEquals(CodecFactory.xzCodec(9).toString(), SerializableUtils.clone(write.inner.getCodec()).getCodec().toString());}
public void beam_f19282_0()
{    AvroIO.Read<String> read = AvroIO.read(String.class).from("/foo.*");    DisplayData displayData = DisplayData.from(read);    assertThat(displayData, hasDisplayItem("filePattern", "/foo.*"));}
public void beam_f19290_0() throws Throwable
{    List<Long> values = Arrays.asList(0L, 1L, 2L);    File outputFile = tmpFolder.newFile("output.avro");    writePipeline.apply(Create.of(values)).apply(AvroIO.<Long, GenericClass>writeCustomType().to(writePipeline.newProvider(outputFile.getAbsolutePath())).withFormatFunction(new CreateGenericClass()).withSchema(ReflectData.get().getSchema(GenericClass.class)).withoutSharding());    writePipeline.run();    PAssert.that(readPipeline.apply("Read", AvroIO.read(GenericClass.class).withBeamSchemas(withBeamSchemas).from(readPipeline.newProvider(outputFile.getAbsolutePath()))).apply(MapElements.via(new SimpleFunction<GenericClass, Long>() {        @Override        public Long apply(GenericClass input) {            return (long) input.intField;        }    }))).containsInAnyOrder(values);    readPipeline.run();}
public Long beam_f19291_0(GenericClass input)
{    return (long) input.intField;}
private void beam_f19292_0(AvroIO.Write<T> writeTransform, AvroIO.Read<T> readTransform) throws Exception
{    File outputFile = tmpFolder.newFile("output.avro");    List<T> values = ImmutableList.of((T) new AvroGeneratedUser("Bob", 256, null), (T) new AvroGeneratedUser("Alice", 128, null), (T) new AvroGeneratedUser("Ted", null, "white"));    writePipeline.apply(Create.of(values)).apply(writeTransform.to(writePipeline.newProvider(outputFile.getAbsolutePath())).withoutSharding());    writePipeline.run();    PAssert.that(readPipeline.apply("Read", readTransform.from(readPipeline.newProvider(outputFile.getAbsolutePath())))).containsInAnyOrder(values);    readPipeline.run();}
public void beam_f19300_0() throws Throwable
{    List<GenericClass> values = ImmutableList.of(new GenericClass(3, "hi"), new GenericClass(5, "bar"));    File outputFile = tmpFolder.newFile("output.avro");    writePipeline.apply(Create.of(values)).apply(AvroIO.write(GenericClass.class).to(outputFile.getAbsolutePath()).withoutSharding().withCodec(CodecFactory.deflateCodec(9)));    writePipeline.run();    PAssert.that(readPipeline.apply(AvroIO.read(GenericClass.class).withBeamSchemas(withBeamSchemas).from(outputFile.getAbsolutePath()))).containsInAnyOrder(values);    readPipeline.run();    try (DataFileStream dataFileStream = new DataFileStream(new FileInputStream(outputFile), new GenericDatumReader())) {        assertEquals("deflate", dataFileStream.getMetaString("avro.codec"));    }}
public void beam_f19301_0() throws Throwable
{    List<GenericClass> values = ImmutableList.of(new GenericClass(3, "hi"), new GenericClass(5, "bar"));    File outputFile = tmpFolder.newFile("output.avro");    writePipeline.apply(Create.of(values)).apply(AvroIO.write(GenericClass.class).to(outputFile.getAbsolutePath()).withoutSharding().withCodec(CodecFactory.nullCodec()));    writePipeline.run();    PAssert.that(readPipeline.apply(AvroIO.read(GenericClass.class).withBeamSchemas(withBeamSchemas).from(outputFile.getAbsolutePath()))).containsInAnyOrder(values);    readPipeline.run();    try (DataFileStream dataFileStream = new DataFileStream(new FileInputStream(outputFile), new GenericDatumReader())) {        assertEquals("null", dataFileStream.getMetaString("avro.codec"));    }}
public String beam_f19302_0()
{    return MoreObjects.toStringHelper(getClass()).add("intField", intField).add("stringField", stringField).add("nullableField", nullableField).toString();}
public void beam_f19310_0() throws Throwable
{    testWindowedAvroIOWriteUsingMethod(WriteMethod.AVROIO_SINK_WITH_CLASS);}
 void beam_f19311_0(WriteMethod method) throws IOException
{    Path baseDir = Files.createTempDirectory(tmpFolder.getRoot().toPath(), "testwrite");    final String baseFilename = baseDir.resolve("prefix").toString();    Instant base = new Instant(0);    ArrayList<GenericClass> allElements = new ArrayList<>();    ArrayList<TimestampedValue<GenericClass>> firstWindowElements = new ArrayList<>();    ArrayList<Instant> firstWindowTimestamps = Lists.newArrayList(base.plus(Duration.ZERO), base.plus(Duration.standardSeconds(10)), base.plus(Duration.standardSeconds(20)), base.plus(Duration.standardSeconds(30)));    Random random = new Random();    for (int i = 0; i < 100; ++i) {        GenericClass item = new GenericClass(i, String.valueOf(i));        allElements.add(item);        firstWindowElements.add(TimestampedValue.of(item, firstWindowTimestamps.get(random.nextInt(firstWindowTimestamps.size()))));    }    ArrayList<TimestampedValue<GenericClass>> secondWindowElements = new ArrayList<>();    ArrayList<Instant> secondWindowTimestamps = Lists.newArrayList(base.plus(Duration.standardSeconds(60)), base.plus(Duration.standardSeconds(70)), base.plus(Duration.standardSeconds(80)), base.plus(Duration.standardSeconds(90)));    for (int i = 100; i < 200; ++i) {        GenericClass item = new GenericClass(i, String.valueOf(i));        allElements.add(new GenericClass(i, String.valueOf(i)));        secondWindowElements.add(TimestampedValue.of(item, secondWindowTimestamps.get(random.nextInt(secondWindowTimestamps.size()))));    }    TimestampedValue<GenericClass>[] firstWindowArray = firstWindowElements.toArray(new TimestampedValue[100]);    TimestampedValue<GenericClass>[] secondWindowArray = secondWindowElements.toArray(new TimestampedValue[100]);    TestStream<GenericClass> values = TestStream.create(AvroCoder.of(GenericClass.class)).advanceWatermarkTo(new Instant(0)).addElements(firstWindowArray[0], Arrays.copyOfRange(firstWindowArray, 1, firstWindowArray.length)).advanceWatermarkTo(new Instant(0).plus(Duration.standardMinutes(1))).addElements(secondWindowArray[0], Arrays.copyOfRange(secondWindowArray, 1, secondWindowArray.length)).advanceWatermarkToInfinity();    final PTransform<PCollection<GenericClass>, WriteFilesResult<Void>> write;    switch(method) {        case AVROIO_WRITE:            {                FilenamePolicy policy = new WindowedFilenamePolicy(FileBasedSink.convertToFileResourceIfPossible(baseFilename));                write = AvroIO.write(GenericClass.class).to(policy).withTempDirectory(StaticValueProvider.of(FileSystems.matchNewResource(baseDir.toString(), true))).withWindowedWrites().withNumShards(2).withOutputFilenames();                break;            }        case AVROIO_SINK_WITH_CLASS:            {                write = FileIO.<GenericClass>write().via(AvroIO.sink(GenericClass.class)).to(baseDir.toString()).withPrefix("prefix").withSuffix(".avro").withTempDirectory(baseDir.toString()).withNumShards(2);                break;            }        default:            throw new UnsupportedOperationException();    }    windowedAvroWritePipeline.apply(values).apply(Window.into(FixedWindows.of(Duration.standardMinutes(1)))).apply(write);    windowedAvroWritePipeline.run();        List<File> expectedFiles = new ArrayList<>();    for (int shard = 0; shard < 2; shard++) {        for (int window = 0; window < 2; window++) {            Instant windowStart = new Instant(0).plus(Duration.standardMinutes(window));            IntervalWindow iw = new IntervalWindow(windowStart, Duration.standardMinutes(1));            String baseAndWindow = baseFilename + "-" + iw.start() + "-" + iw.end();            switch(method) {                case AVROIO_WRITE:                    expectedFiles.add(new File(baseAndWindow + "-" + shard + "-of-2-pane-0-last.avro"));                    break;                case AVROIO_SINK_WITH_CLASS:                    expectedFiles.add(new File(baseAndWindow + "-0000" + shard + "-of-00002.avro"));                    break;                default:                    throw new UnsupportedOperationException("Unknown write method " + method);            }        }    }    List<GenericClass> actualElements = new ArrayList<>();    for (File outputFile : expectedFiles) {        assertTrue("Expected output file " + outputFile.getAbsolutePath(), outputFile.exists());        try (DataFileReader<GenericClass> reader = new DataFileReader<>(outputFile, new ReflectDatumReader<>(ReflectData.get().getSchema(GenericClass.class)))) {            Iterators.addAll(actualElements, reader);        }        outputFile.delete();    }    assertThat(actualElements, containsInAnyOrder(allElements.toArray()));}
private static String beam_f19312_0(String prefix)
{    return SCHEMA_TEMPLATE_STRING.replace("$$", prefix);}
public void beam_f19320_0(GenericRecord value, OutputStream outStream) throws IOException
{    char prefix = value.getSchema().getName().charAt(0);        outStream.write(prefix);    coderMap.get(prefix).encode(value, outStream);}
public GenericRecord beam_f19321_0(InputStream inStream) throws CoderException, IOException
{    char prefix = (char) inStream.read();    return coderMap.get(prefix).decode(inStream);}
public List<? extends Coder<?>> beam_f19322_0()
{    return Collections.emptyList();}
public void beam_f19330_0() throws Exception
{    testDynamicDestinationsUnwindowedWithSharding(WriteMethod.AVROIO_SINK_WITH_SCHEMA, Sharding.WITHOUT_SHARDING);}
public void beam_f19331_0() throws Exception
{    testDynamicDestinationsUnwindowedWithSharding(WriteMethod.AVROIO_SINK_WITH_SCHEMA, Sharding.FIXED_3_SHARDS);}
public void beam_f19332_0() throws Exception
{    testDynamicDestinationsUnwindowedWithSharding(WriteMethod.AVROIO_SINK_WITH_FORMATTER, Sharding.RUNNER_DETERMINED);}
public void beam_f19340_0()
{    DisplayDataEvaluator evaluator = DisplayDataEvaluator.create();    AvroIO.Read<GenericRecord> read = AvroIO.readGenericRecords(Schema.create(Schema.Type.STRING)).withBeamSchemas(withBeamSchemas).from("/foo.*");    Set<DisplayData> displayData = evaluator.displayDataForPrimitiveSourceTransforms(read);    assertThat("AvroIO.Read should include the file pattern in its primitive transform", displayData, hasItem(hasDisplayItem("filePattern")));}
private String beam_f19341_0(String filename, List<T> elems, SyncBehavior syncBehavior, int syncInterval, AvroCoder<T> coder, String codec) throws IOException
{    Random random = new Random(0);    File tmpFile = tmpFolder.newFile(filename);    String path = tmpFile.toString();    FileOutputStream os = new FileOutputStream(tmpFile);    DatumWriter<T> datumWriter = coder.getType().equals(GenericRecord.class) ? new GenericDatumWriter<>(coder.getSchema()) : new ReflectDatumWriter<>(coder.getSchema());    try (DataFileWriter<T> writer = new DataFileWriter<>(datumWriter)) {        writer.setCodec(CodecFactory.fromString(codec));        writer.create(coder.getSchema(), os);        int recordIndex = 0;        int syncIndex = syncBehavior == SyncBehavior.SYNC_RANDOM ? random.nextInt(syncInterval) : 0;        for (T elem : elems) {            writer.append(elem);            recordIndex++;            switch(syncBehavior) {                case SYNC_REGULAR:                    if (recordIndex == syncInterval) {                        recordIndex = 0;                        writer.sync();                    }                    break;                case SYNC_RANDOM:                    if (recordIndex == syncIndex) {                        recordIndex = 0;                        writer.sync();                        syncIndex = random.nextInt(syncInterval);                    }                    break;                case SYNC_DEFAULT:                default:            }        }    }    return path;}
public void beam_f19342_0() throws Exception
{        String[] codecs = { DataFileConstants.NULL_CODEC, DataFileConstants.BZIP2_CODEC, DataFileConstants.DEFLATE_CODEC, DataFileConstants.SNAPPY_CODEC, DataFileConstants.XZ_CODEC };                List<Bird> expected = createRandomRecords(1 << 16);    for (String codec : codecs) {        String filename = generateTestFile(codec, expected, SyncBehavior.SYNC_DEFAULT, 0, AvroCoder.of(Bird.class), codec);        AvroSource<Bird> source = AvroSource.from(filename).withSchema(Bird.class);        List<Bird> actual = SourceTestUtils.readFromSource(source, null);        assertThat(expected, containsInAnyOrder(actual.toArray()));    }}
public void beam_f19350_0() throws Exception
{    String baseName = "tmp-";    List<Bird> expected = new ArrayList<>();    for (int i = 0; i < 10; i++) {        List<Bird> contents = createRandomRecords(DEFAULT_RECORD_COUNT / 10);        expected.addAll(contents);        generateTestFile(baseName + i, contents, SyncBehavior.SYNC_DEFAULT, 0, AvroCoder.of(Bird.class), DataFileConstants.NULL_CODEC);    }    AvroSource<Bird> source = AvroSource.from(new File(tmpFolder.getRoot().toString(), baseName + "*").toString()).withSchema(Bird.class);    List<Bird> actual = SourceTestUtils.readFromSource(source, null);    assertThat(actual, containsInAnyOrder(expected.toArray()));}
public void beam_f19351_0() throws Exception
{    List<Bird> expected = createRandomRecords(100);    String filename = generateTestFile("tmp.avro", expected, SyncBehavior.SYNC_DEFAULT, 0, AvroCoder.of(Bird.class), DataFileConstants.NULL_CODEC);        Schema schema = ReflectData.get().getSchema(Bird.class);    AvroSource<GenericRecord> source = AvroSource.from(filename).withSchema(schema);    List<GenericRecord> records = SourceTestUtils.readFromSource(source, null);    assertEqualsWithGeneric(expected, records);        String schemaString = ReflectData.get().getSchema(Bird.class).toString();    source = AvroSource.from(filename).withSchema(schemaString);    records = SourceTestUtils.readFromSource(source, null);    assertEqualsWithGeneric(expected, records);}
public void beam_f19352_0() throws Exception
{    List<Bird> birds = createRandomRecords(100);    String filename = generateTestFile("tmp.avro", birds, SyncBehavior.SYNC_DEFAULT, 0, AvroCoder.of(Bird.class), DataFileConstants.NULL_CODEC);    AvroSource<FancyBird> source = AvroSource.from(filename).withSchema(FancyBird.class);    List<FancyBird> actual = SourceTestUtils.readFromSource(source, null);    List<FancyBird> expected = new ArrayList<>();    for (Bird bird : birds) {        expected.add(new FancyBird(bird.number, bird.species, bird.quality, bird.quantity, null, "MAXIMUM OVERDRIVE"));    }    assertThat(actual, containsInAnyOrder(expected.toArray()));}
public void beam_f19360_0()
{    byte[] marker = { 0, 1, 2, 3 };    byte[] buffer;    Seeker s;    s = new Seeker(marker);    buffer = new byte[] { 0, 0, 0, 0, 0, 0, 0, 0 };    assertEquals(-1, s.find(buffer, buffer.length));    buffer = new byte[] { 1, 2, 3, 0, 0, 0, 0, 0 };    assertEquals(2, s.find(buffer, buffer.length));    buffer = new byte[] { 0, 0, 0, 0, 0, 0, 1, 2 };    assertEquals(-1, s.find(buffer, buffer.length));    buffer = new byte[] { 3, 0, 1, 2, 3, 0, 1, 2 };    assertEquals(0, s.find(buffer, buffer.length));    buffer = new byte[] { 0 };    assertEquals(-1, s.find(buffer, buffer.length));    buffer = new byte[] { 1 };    assertEquals(-1, s.find(buffer, buffer.length));    buffer = new byte[] { 2 };    assertEquals(-1, s.find(buffer, buffer.length));    buffer = new byte[] { 3 };    assertEquals(0, s.find(buffer, buffer.length));}
public void beam_f19361_0()
{    byte[] marker = { 0, 0, 1 };    byte[] buffer;    Seeker s;    s = new Seeker(marker);    buffer = new byte[] { 0, 0, 0, 1 };    assertEquals(-1, s.find(buffer, 3));    s = new Seeker(marker);    buffer = new byte[] { 0, 0 };    assertEquals(-1, s.find(buffer, 1));    buffer = new byte[] { 1, 0 };    assertEquals(-1, s.find(buffer, 1));    s = new Seeker(marker);    buffer = new byte[] { 0, 2 };    assertEquals(-1, s.find(buffer, 1));    buffer = new byte[] { 0, 2 };    assertEquals(-1, s.find(buffer, 1));    buffer = new byte[] { 1, 2 };    assertEquals(0, s.find(buffer, 1));}
public void beam_f19362_0()
{    byte[] marker = { 0, 0, 1 };    byte[] buffer;    Seeker s;    s = new Seeker(marker);    buffer = new byte[] { 0, 0, 0, 1 };    assertEquals(3, s.find(buffer, buffer.length));    marker = new byte[] { 1, 1, 1, 2 };    s = new Seeker(marker);    buffer = new byte[] { 1, 1, 1, 1, 1 };    assertEquals(-1, s.find(buffer, buffer.length));    buffer = new byte[] { 1, 1, 2 };    assertEquals(2, s.find(buffer, buffer.length));    buffer = new byte[] { 1, 1, 1, 1, 1 };    assertEquals(-1, s.find(buffer, buffer.length));    buffer = new byte[] { 2, 1, 1, 1, 2 };    assertEquals(0, s.find(buffer, buffer.length));}
public int beam_f19370_0()
{    return toString().hashCode();}
public String beam_f19371_0()
{    return Integer.toString(this.asInt());}
private static List<FixedRecord> beam_f19372_0(int count)
{    List<FixedRecord> records = new ArrayList<>();    for (int i = 0; i < count; i++) {        records.add(new FixedRecord(i));    }    return records;}
public void beam_f19380_0() throws Exception
{    test(false, false);}
public void beam_f19381_0() throws Exception
{    test(true, false);}
public void beam_f19382_0() throws Exception
{    test(false, true);}
public TestCountingSource beam_f19390_0(boolean throwOnFirstSnapshot)
{    return new TestCountingSource(numMessagesPerShard, shardNumber, dedup, throwOnFirstSnapshot, true);}
public TestCountingSource beam_f19391_0()
{    return new TestCountingSource(numMessagesPerShard, shardNumber, dedup, throwOnFirstSnapshot, false);}
public int beam_f19392_0()
{    return shardNumber;}
public Instant beam_f19400_0()
{    return new Instant(current);}
public byte[] beam_f19401_0()
{    try {        return encodeToByteArray(KvCoder.of(VarIntCoder.of(), VarIntCoder.of()), getCurrent());    } catch (IOException e) {        throw new RuntimeException(e);    }}
public TestCountingSource beam_f19403_0()
{    return TestCountingSource.this;}
public void beam_f19411_0() throws Exception
{    byte[] input = generateInput(5000);    runReadTest(input, CompressionMode.GZIP);}
public void beam_f19412_0() throws Exception
{    CompressedSource<Byte> source;        source = CompressedSource.from(new ByteSource("input.gz", 1));    assertFalse(source.isSplittable());    source = CompressedSource.from(new ByteSource("input.GZ", 1));    assertFalse(source.isSplittable());        source = CompressedSource.from(new ByteSource("input.bz2", 1));    assertFalse(source.isSplittable());    source = CompressedSource.from(new ByteSource("input.BZ2", 1));    assertFalse(source.isSplittable());        source = CompressedSource.from(new ByteSource("input.zip", 1));    assertFalse(source.isSplittable());    source = CompressedSource.from(new ByteSource("input.ZIP", 1));    assertFalse(source.isSplittable());        source = CompressedSource.from(new ByteSource("input.zst", 1));    assertFalse(source.isSplittable());    source = CompressedSource.from(new ByteSource("input.ZST", 1));    assertFalse(source.isSplittable());    source = CompressedSource.from(new ByteSource("input.zstd", 1));    assertFalse(source.isSplittable());        source = CompressedSource.from(new ByteSource("input.deflate", 1));    assertFalse(source.isSplittable());    source = CompressedSource.from(new ByteSource("input.DEFLATE", 1));    assertFalse(source.isSplittable());        source = CompressedSource.from(new ByteSource("input.txt", 1));    assertTrue(source.isSplittable());    source = CompressedSource.from(new ByteSource("input.csv", 1));    assertTrue(source.isSplittable());}
public void beam_f19413_0() throws Exception
{    CompressedSource<Byte> source;        source = CompressedSource.from(new ByteSource("input.gz", 1)).withDecompression(CompressionMode.GZIP);    assertFalse(source.isSplittable());    source = CompressedSource.from(new ByteSource("input.GZ", 1)).withDecompression(CompressionMode.GZIP);    assertFalse(source.isSplittable());        source = CompressedSource.from(new ByteSource("input.txt", 1)).withDecompression(CompressionMode.GZIP);    assertFalse(source.isSplittable());    source = CompressedSource.from(new ByteSource("input.csv", 1)).withDecompression(CompressionMode.GZIP);    assertFalse(source.isSplittable());}
public void beam_f19421_0() throws IOException
{    byte[] header = "a,b,c\n".getBytes(StandardCharsets.UTF_8);    byte[] body = "1,2,3\n4,5,6\n7,8,9\n".getBytes(StandardCharsets.UTF_8);    byte[] expected = concat(header, body);    byte[] totalGz = concat(compressGzip(header), compressGzip(body));    File tmpFile = tmpFolder.newFile();    try (FileOutputStream os = new FileOutputStream(tmpFile)) {        os.write(totalGz);    }    CompressedSource<Byte> source = CompressedSource.from(new ByteSource(tmpFile.getAbsolutePath(), 1)).withDecompression(CompressionMode.GZIP);    List<Byte> actual = SourceTestUtils.readFromSource(source, PipelineOptionsFactory.create());    assertEquals(Bytes.asList(expected), actual);}
public void beam_f19422_0() throws IOException
{    CompressionMode mode = CompressionMode.BZIP2;    byte[] input1 = generateInput(5, 587973);    byte[] input2 = generateInput(5, 387374);    ByteArrayOutputStream stream1 = new ByteArrayOutputStream();    try (OutputStream os = getOutputStreamForMode(mode, stream1)) {        os.write(input1);    }    ByteArrayOutputStream stream2 = new ByteArrayOutputStream();    try (OutputStream os = getOutputStreamForMode(mode, stream2)) {        os.write(input2);    }    File tmpFile = tmpFolder.newFile();    try (OutputStream os = new FileOutputStream(tmpFile)) {        os.write(stream1.toByteArray());        os.write(stream2.toByteArray());    }    byte[] output = Bytes.concat(input1, input2);    verifyReadContents(output, tmpFile, mode);}
public void beam_f19423_0() throws Exception
{    byte[] input = generateInput(0);    runReadTest(input, CompressionMode.BZIP2);}
public void beam_f19431_0() throws Exception
{    String baseName = "test-input";    File compressedFile = tmpFolder.newFile(baseName + ".gz");    writeFile(compressedFile, generateInput(10), CompressionMode.GZIP);    CompressedSource<Byte> source = CompressedSource.from(new ByteSource(compressedFile.getPath(), 1));    assertFalse(source.isSplittable());}
public void beam_f19432_0() throws Exception
{    String baseName = "test-input";    File compressedFile = tmpFolder.newFile(baseName + ".bz2");    writeFile(compressedFile, generateInput(10), CompressionMode.BZIP2);    CompressedSource<Byte> source = CompressedSource.from(new ByteSource(compressedFile.getPath(), 1));    assertFalse(source.isSplittable());}
public void beam_f19433_0() throws Exception
{    String baseName = "test-input";    File compressedFile = tmpFolder.newFile(baseName + ".zst");    writeFile(compressedFile, generateInput(10), CompressionMode.ZSTD);    CompressedSource<Byte> source = CompressedSource.from(new ByteSource(compressedFile.getPath(), 1));    assertFalse(source.isSplittable());}
public void beam_f19441_0(DisplayData.Builder builder)
{    builder.add(DisplayData.item("foo", "bar"));}
private byte[] beam_f19442_0(int size)
{        return generateInput(size, 285930);}
private byte[] beam_f19443_0(int size, int seed)
{        Random random = new Random(seed);    byte[] buff = new byte[size];    random.nextBytes(buff);    return buff;}
protected ByteSource beam_f19451_0(Metadata metadata, long start, long end)
{    return new ByteSource(metadata, getMinBundleSize(), start, end);}
protected FileBasedReader<Byte> beam_f19452_0(PipelineOptions options)
{    return new ByteReader(this);}
public Coder<Byte> beam_f19453_0()
{    return SerializableCoder.of(Byte.class);}
public void beam_f19461_0() throws IOException
{    File tmpFile = tmpFolder.newFile("empty.gz");    String filename = tmpFile.toPath().toString();    writeFile(tmpFile, new byte[0], CompressionMode.GZIP);    PipelineOptions options = PipelineOptionsFactory.create();    CompressedSource<Byte> source = CompressedSource.from(new ByteSource(filename, 1));    try (BoundedReader<Byte> readerOrig = source.createReader(options)) {        assertThat(readerOrig, instanceOf(CompressedReader.class));        CompressedReader<Byte> reader = (CompressedReader<Byte>) readerOrig;                assertEquals(0.0, reader.getFractionConsumed(), 1e-6);        assertEquals(0, reader.getSplitPointsConsumed());        assertEquals(1, reader.getSplitPointsRemaining());                assertFalse(reader.start());                assertEquals(1.0, reader.getFractionConsumed(), 1e-6);        assertEquals(0, reader.getSplitPointsConsumed());        assertEquals(0, reader.getSplitPointsRemaining());    }}
public void beam_f19462_0() throws IOException
{    int numRecords = 3;    File tmpFile = tmpFolder.newFile("nonempty.gz");    String filename = tmpFile.toPath().toString();    writeFile(tmpFile, new byte[numRecords], CompressionMode.GZIP);    PipelineOptions options = PipelineOptionsFactory.create();    CompressedSource<Byte> source = CompressedSource.from(new ByteSource(filename, 1));    try (BoundedReader<Byte> readerOrig = source.createReader(options)) {        assertThat(readerOrig, instanceOf(CompressedReader.class));        CompressedReader<Byte> reader = (CompressedReader<Byte>) readerOrig;                assertEquals(0.0, reader.getFractionConsumed(), 1e-6);        assertEquals(0, reader.getSplitPointsConsumed());        assertEquals(1, reader.getSplitPointsRemaining());                for (int i = 0; i < numRecords; ++i) {            if (i == 0) {                assertTrue(reader.start());            } else {                assertTrue(reader.advance());            }            assertEquals(0, reader.getSplitPointsConsumed());            assertEquals(1, reader.getSplitPointsRemaining());        }        assertFalse(reader.advance());                assertEquals(1.0, reader.getFractionConsumed(), 1e-6);        assertEquals(1, reader.getSplitPointsConsumed());        assertEquals(0, reader.getSplitPointsRemaining());    }}
public void beam_f19463_0() throws IOException
{    String baseName = "test-input";    File compressedFile = tmpFolder.newFile(baseName + ".gz");    byte[] input = generateInput(10000);    writeFile(compressedFile, input, CompressionMode.GZIP);    CompressedSource<Byte> source = CompressedSource.from(new ByteSource(compressedFile.getPath(), 1));    List<Byte> expected = Lists.newArrayList();    for (byte i : input) {        expected.add(i);    }    PipelineOptions options = PipelineOptionsFactory.create();    BoundedReader<Byte> reader = source.createReader(options);    List<Byte> actual = Lists.newArrayList();    for (boolean hasNext = reader.start(); hasNext; hasNext = reader.advance()) {        actual.add(reader.getCurrent());                if (actual.size() % 9 == 0) {            Double fractionConsumed = reader.getFractionConsumed();            assertNotNull(fractionConsumed);            assertNull(reader.splitAtFraction(fractionConsumed));        }    }    assertEquals(expected.size(), actual.size());    assertEquals(Sets.newHashSet(expected), Sets.newHashSet(actual));}
public void beam_f19471_0(ProcessContext c) throws Exception
{    c.output(c.element() - c.timestamp().getMillis());}
public void beam_f19472_0()
{    long numElements = 1000;    PCollection<Long> input = p.apply(Read.from(CountingSource.unboundedWithTimestampFn(new ValueAsTimestampFn())).withMaxNumRecords(numElements));    addCountingAsserts(input, numElements);    PCollection<Long> diffs = input.apply("TimestampDiff", ParDo.of(new ElementValueDiff())).apply("DistinctTimestamps", Distinct.create());        PAssert.thatSingleton(diffs).isEqualTo(0L);    p.run();}
public void beam_f19473_0()
{    Duration period = Duration.millis(5);    long numElements = 1000L;    PCollection<Long> input = p.apply(Read.from(CountingSource.createUnboundedFrom(0).withTimestampFn(new ValueAsTimestampFn()).withRate(1, period)).withMaxNumRecords(numElements));    addCountingAsserts(input, numElements);    PCollection<Long> diffs = input.apply("TimestampDiff", ParDo.of(new ElementValueDiff())).apply("DistinctTimestamps", Distinct.create());        PAssert.thatSingleton(diffs).isEqualTo(0L);    Instant started = Instant.now();    p.run();    Instant finished = Instant.now();    Duration expectedDuration = period.multipliedBy((int) numElements);    assertThat(started.plus(expectedDuration).isBefore(finished), is(true));}
public void beam_f19481_0()
{    assertEquals("/path/to/output-001-of-123.txt", constructName("/path/to/output", "-SSS-of-NNN", ".txt", 1, 123, null, null));    assertEquals("/path/to/output-001-of-123-PPP-W.txt", constructName("/path/to/output", "-SSS-of-NNN-PPP-W", ".txt", 1, 123, null, null));    assertEquals("/path/to/out" + ".txt/part-00042-myPaneStr-myWindowStr", constructName("/path/to/out.txt", "/part-SSSSS-P-W", "", 42, 100, "myPaneStr", "myWindowStr"));    assertEquals("/path/to/out.txt", constructName("/path/to/ou", "t.t", "xt", 1, 1, "myPaneStr2", "anotherWindowStr"));    assertEquals("/path/to/out0102shard-oneMoreWindowStr-anotherPaneStr.txt", constructName("/path/to/out", "SSNNshard-W-P", ".txt", 1, 2, "anotherPaneStr", "oneMoreWindowStr"));    assertEquals("/out-2/1.part-1-of-2-slidingWindow1-myPaneStr3-windowslidingWindow1-" + "panemyPaneStr3.txt", constructName("/out", "-N/S.part-S-of-N-W-P-windowW-paneP", ".txt", 1, 2, "myPaneStr3", "slidingWindow1"));        assertEquals("/out.txt/part-00042-myWindowStr-pane-11-true-false", constructName("/out.txt", "/part-SSSSS-W-P", "", 42, 100, "pane-11-true-false", "myWindowStr"));    assertEquals("/path/to/out.txt", constructName("/path/to/ou", "t.t", "xt", 1, 1, "pane", "anotherWindowStr"));    assertEquals("/out0102shard-oneMoreWindowStr-pane--1-false-false-pane--1-false-false.txt", constructName("/out", "SSNNshard-W-P-P", ".txt", 1, 2, "pane--1-false-false", "oneMoreWindowStr"));    assertEquals("/path/to/out-2/1.part-1-of-2-sWindow1-winsWindow1-ppaneL.txt", constructName("/path/to/out", "-N/S.part-S-of-N-W-winW-pP", ".txt", 1, 2, "paneL", "sWindow1"));}
public WritableByteChannel beam_f19482_0(WritableByteChannel channel) throws IOException
{    return new DrunkWritableByteChannel(channel);}
public String beam_f19483_0()
{    return MimeTypes.TEXT;}
private ResourceId beam_f19491_0()
{    return getTemporaryFolder().resolve(tempDirectoryName, StandardResolveOptions.RESOLVE_DIRECTORY);}
public void beam_f19492_0() throws Exception
{    String testUid = "testId";    ResourceId expectedTempFile = getBaseTempDirectory().resolve(testUid, StandardResolveOptions.RESOLVE_FILE);    List<String> values = Arrays.asList("sympathetic vulture", "boresome hummingbird");    List<String> expected = new ArrayList<>();    expected.add(SimpleSink.SimpleWriter.HEADER);    expected.addAll(values);    expected.add(SimpleSink.SimpleWriter.FOOTER);    SimpleSink.SimpleWriter<Void> writer = buildWriteOperationWithTempDir(getBaseTempDirectory()).createWriter();    writer.open(testUid);    for (String value : values) {        writer.write(value);    }    writer.close();    assertEquals(expectedTempFile, writer.getOutputFile());    assertFileContains(expected, expectedTempFile);}
private void beam_f19493_0(List<String> expected, ResourceId file) throws Exception
{    try (BufferedReader reader = Files.newBufferedReader(Paths.get(file.toString()), UTF_8)) {        List<String> actual = new ArrayList<>();        for (; ; ) {            String line = reader.readLine();            if (line == null) {                break;            }            actual.add(line);        }        assertEquals("contents for " + file, expected, actual);    }}
private void beam_f19501_0(SimpleSink.SimpleWriteOperation<Void> writeOp, List<File> temporaryFiles) throws Exception
{    int numFiles = temporaryFiles.size();    List<FileResult<Void>> fileResults = new ArrayList<>();        for (File temporaryFile : temporaryFiles) {        fileResults.add(new FileResult<>(LocalResources.fromFile(temporaryFile, false), UNKNOWN_SHARDNUM, GlobalWindow.INSTANCE, PaneInfo.ON_TIME_AND_ONLY_FIRING, null));    }        List<KV<FileResult<Void>, ResourceId>> resultsToFinalFilenames = writeOp.finalizeDestination(null, GlobalWindow.INSTANCE, null, fileResults);    writeOp.moveToOutputFiles(resultsToFinalFilenames);    for (int i = 0; i < numFiles; i++) {        ResourceId outputFilename = writeOp.getSink().getDynamicDestinations().getFilenamePolicy(null).unwindowedFilename(i, numFiles, CompressionType.UNCOMPRESSED);        assertTrue(outputFilename.toString(), new File(outputFilename.toString()).exists());        assertFalse(temporaryFiles.get(i).exists());    }    assertFalse(new File(writeOp.tempDirectory.get().toString()).exists());        assertEquals(writeOp.tempDirectory.get(), writeOp.tempDirectory.get());}
private void beam_f19502_0(int numFiles, ResourceId tempDirectory) throws Exception
{    String prefix = "file";    SimpleSink<Void> sink = SimpleSink.makeSimpleSink(getBaseOutputDirectory(), prefix, "", "", Compression.UNCOMPRESSED);    WriteOperation<Void, String> writeOp = new SimpleSink.SimpleWriteOperation<>(sink, tempDirectory);    List<File> temporaryFiles = new ArrayList<>();    List<File> outputFiles = new ArrayList<>();    for (int i = 0; i < numFiles; i++) {        ResourceId tempResource = WriteOperation.buildTemporaryFilename(tempDirectory, prefix + i);        File tmpFile = new File(tempResource.toString());        tmpFile.getParentFile().mkdirs();        assertTrue("not able to create new temp file", tmpFile.createNewFile());        temporaryFiles.add(tmpFile);        ResourceId outputFileId = getBaseOutputDirectory().resolve(prefix + i, StandardResolveOptions.RESOLVE_FILE);        File outputFile = new File(outputFileId.toString());        outputFile.getParentFile().mkdirs();        assertTrue("not able to create new output file", outputFile.createNewFile());        outputFiles.add(outputFile);    }    writeOp.removeTemporaryFiles(Collections.emptySet(), true);    for (int i = 0; i < numFiles; i++) {        File temporaryFile = temporaryFiles.get(i);        assertThat(String.format("temp file %s exists", temporaryFile), temporaryFile.exists(), is(false));        File outputFile = outputFiles.get(i);        assertThat(String.format("output file %s exists", outputFile), outputFile.exists(), is(true));    }}
public void beam_f19503_0() throws Exception
{    SimpleSink.SimpleWriteOperation<Void> writeOp = buildWriteOperation();    List<String> inputFilenames = Arrays.asList("input-1", "input-2", "input-3");    List<String> inputContents = Arrays.asList("1", "2", "3");    List<String> expectedOutputFilenames = Arrays.asList("file-00-of-03.test", "file-01-of-03.test", "file-02-of-03.test");    List<KV<FileResult<Void>, ResourceId>> resultsToFinalFilenames = Lists.newArrayList();    List<ResourceId> expectedOutputPaths = Lists.newArrayList();    for (int i = 0; i < inputFilenames.size(); i++) {                expectedOutputPaths.add(getBaseOutputDirectory().resolve(expectedOutputFilenames.get(i), StandardResolveOptions.RESOLVE_FILE));                File inputTmpFile = tmpFolder.newFile(inputFilenames.get(i));        List<String> lines = Collections.singletonList(inputContents.get(i));        writeFile(lines, inputTmpFile);        ResourceId finalFilename = writeOp.getSink().getDynamicDestinations().getFilenamePolicy(null).unwindowedFilename(i, inputFilenames.size(), CompressionType.UNCOMPRESSED);        resultsToFinalFilenames.add(KV.of(new FileResult<>(LocalResources.fromFile(inputTmpFile, false), UNKNOWN_SHARDNUM, GlobalWindow.INSTANCE, PaneInfo.ON_TIME_AND_ONLY_FIRING, null), finalFilename));    }        writeOp.moveToOutputFiles(resultsToFinalFilenames);        for (int i = 0; i < expectedOutputPaths.size(); i++) {        assertFileContains(Collections.singletonList(inputContents.get(i)), expectedOutputPaths.get(i));    }}
public void beam_f19511_0() throws FileNotFoundException, IOException
{    final File file = writeValuesWithCompression(Compression.UNCOMPRESSED, "abc", "123");        assertReadValues(new BufferedReader(new InputStreamReader(new FileInputStream(file), StandardCharsets.UTF_8)), "abc", "123");}
private void beam_f19512_0(final BufferedReader br, String... values) throws IOException
{    try (final BufferedReader lbr = br) {        for (String value : values) {            assertEquals(String.format("Line should read '%s'", value), value, lbr.readLine());        }    }}
private File beam_f19513_0(Compression compression, String... values) throws IOException
{    final File file = tmpFolder.newFile("test.gz");    final WritableByteChannel channel = compression.writeCompressed(Channels.newChannel(new FileOutputStream(file)));    for (String value : values) {        channel.write(ByteBuffer.wrap((value + "\n").getBytes(StandardCharsets.UTF_8)));    }    channel.close();    return file;}
private int beam_f19521_0(ByteArrayOutputStream out) throws IOException
{    int byteCount = 0;    while (true) {        if (!buf.hasRemaining()) {            buf.clear();            int read = channel.read(buf);            if (read < 0) {                break;            }            buf.flip();        }        byte b = buf.get();        byteCount++;        if (b == '\n') {            break;        }        out.write(b);    }    return byteCount;}
public boolean beam_f19522_0() throws IOException
{    currentLineStart = nextLineStart;    ByteArrayOutputStream buf = new ByteArrayOutputStream();    int offsetAdjustment = readNextLine(buf);    if (offsetAdjustment == 0) {                return false;    }    nextLineStart += offsetAdjustment;            currentValue = CoderUtils.decodeFromByteArray(StringUtf8Coder.of(), buf.toByteArray()).trim();    return true;}
public String beam_f19523_0()
{    return currentValue;}
protected boolean beam_f19531_0() throws IOException
{    if (!foundFirstSplitPoint) {        while (!isAtSplitPoint) {            if (!readNextRecordInternal()) {                return false;            }        }        foundFirstSplitPoint = true;        return true;    }    return readNextRecordInternal();}
private boolean beam_f19532_0() throws IOException
{    isAtSplitPoint = false;    if (!lineReader.readNextLine()) {        return false;    }    currentOffset = lineReader.getCurrentLineStart();    while (getCurrent().equals(splitHeader)) {        currentOffset = lineReader.getCurrentLineStart();        if (!lineReader.readNextLine()) {            return false;        }        isAtSplitPoint = true;    }    return true;}
protected boolean beam_f19533_0()
{    return isAtSplitPoint;}
public void beam_f19541_0() throws IOException
{    PipelineOptions options = PipelineOptionsFactory.create();    TestFileBasedSource source = new TestFileBasedSource(new File(tempFolder.getRoot(), "doesNotExist").getPath(), 64, null);    thrown.expect(FileNotFoundException.class);    readFromSource(source, options);}
public void beam_f19542_0() throws IOException
{    PipelineOptions options = PipelineOptionsFactory.create();    TestFileBasedSource source = new TestFileBasedSource(new File(tempFolder.getRoot(), "doesNotExist").getPath(), EmptyMatchTreatment.ALLOW, 64, null);    TestFileBasedSource sourceWithWildcard = new TestFileBasedSource(new File(tempFolder.getRoot(), "doesNotExist*").getPath(), EmptyMatchTreatment.ALLOW_IF_WILDCARD, 64, null);    assertEquals(0, readFromSource(source, options).size());    assertEquals(0, readFromSource(sourceWithWildcard, options).size());}
public void beam_f19543_0() throws IOException
{    PipelineOptions options = PipelineOptionsFactory.create();    TestFileBasedSource source = new TestFileBasedSource(new File(tempFolder.getRoot(), "doesNotExist").getPath(), EmptyMatchTreatment.ALLOW_IF_WILDCARD, 64, null);    thrown.expect(FileNotFoundException.class);    readFromSource(source, options);}
public void beam_f19551_0() throws IOException
{    PipelineOptions options = PipelineOptionsFactory.create();    String header = "<h>";    List<String> data = new ArrayList<>();    for (int i = 0; i < 10; i++) {        data.add(header);        data.addAll(createStringDataset(3, 9));    }    String fileName = "file";    File file = createFileWithData(fileName, data);    Metadata metadata = FileSystems.matchSingleFileSpec(file.getPath());    TestFileBasedSource source1 = new TestFileBasedSource(metadata, 64, 0, 42, header);    TestFileBasedSource source2 = new TestFileBasedSource(metadata, 64, 42, 112, header);    TestFileBasedSource source3 = new TestFileBasedSource(metadata, 64, 112, Long.MAX_VALUE, header);    List<String> expectedResults = new ArrayList<>();    expectedResults.addAll(data);        expectedResults.removeAll(Collections.singletonList(header));    List<String> results = new ArrayList<>();    results.addAll(readFromSource(source1, options));    results.addAll(readFromSource(source2, options));    results.addAll(readFromSource(source3, options));    assertThat(expectedResults, containsInAnyOrder(results.toArray()));}
public void beam_f19552_0() throws IOException
{    PipelineOptions options = PipelineOptionsFactory.create();    String header = "<h>";    List<String> data = new ArrayList<>();    for (int i = 0; i < 5; i++) {        data.add(header);        data.addAll(createStringDataset(3, 9));    }    String fileName = "file";    File file = createFileWithData(fileName, data);    Metadata metadata = FileSystems.matchSingleFileSpec(file.getPath());    TestFileBasedSource source1 = new TestFileBasedSource(metadata, 64, 0, 42, header);    TestFileBasedSource source2 = new TestFileBasedSource(metadata, 64, 42, 62, header);    TestFileBasedSource source3 = new TestFileBasedSource(metadata, 64, 62, Long.MAX_VALUE, header);    List<String> expectedResults = new ArrayList<>();    expectedResults.addAll(data);        expectedResults.removeAll(Collections.singletonList(header));    List<String> results = new ArrayList<>();    results.addAll(readFromSource(source1, options));    results.addAll(readFromSource(source2, options));    results.addAll(readFromSource(source3, options));    assertThat(expectedResults, containsInAnyOrder(results.toArray()));}
public void beam_f19553_0() throws IOException
{    PipelineOptions options = PipelineOptionsFactory.create();    String header = "<h>";    List<String> data = new ArrayList<>();    for (int i = 0; i < 10; i++) {        data.add(header);        data.addAll(createStringDataset(3, 9));    }    String fileName = "file";    File file = createFileWithData(fileName, data);    List<String> expectedResults = new ArrayList<>();    expectedResults.addAll(data.subList(10, data.size()));        expectedResults.removeAll(Collections.singletonList(header));    Metadata metadata = FileSystems.matchSingleFileSpec(file.getPath());        TestFileBasedSource source = new TestFileBasedSource(metadata, 64, 1, Long.MAX_VALUE, header);    assertThat(expectedResults, containsInAnyOrder(readFromSource(source, options).toArray()));        source = new TestFileBasedSource(metadata, 64, 2, Long.MAX_VALUE, header);    assertThat(expectedResults, containsInAnyOrder(readFromSource(source, options).toArray()));        source = new TestFileBasedSource(metadata, 64, 3, Long.MAX_VALUE, header);    assertThat(expectedResults, containsInAnyOrder(readFromSource(source, options).toArray()));}
public void beam_f19561_0() throws Exception
{    PipelineOptions options = PipelineOptionsFactory.create();    List<String> data1 = createStringDataset(3, 50);    File file1 = createFileWithData("file1", data1);    List<String> data2 = createStringDataset(3, 50);    createFileWithData("file2", data2);    List<String> data3 = createStringDataset(3, 50);    createFileWithData("file3", data3);    List<String> data4 = createStringDataset(3, 50);    createFileWithData("otherfile", data4);    TestFileBasedSource source = new TestFileBasedSource(new File(file1.getParent(), "file*").getPath(), 64, null);    List<? extends BoundedSource<String>> sources = source.split(512, null);        assertTrue(sources.size() > 1);    List<String> results = new ArrayList<>();    for (BoundedSource<String> split : sources) {        results.addAll(readFromSource(split, options));    }    List<String> expectedResults = new ArrayList<>();    expectedResults.addAll(data1);    expectedResults.addAll(data2);    expectedResults.addAll(data3);    assertThat(expectedResults, containsInAnyOrder(results.toArray()));}
public void beam_f19562_0() throws Exception
{    PipelineOptions options = PipelineOptionsFactory.create();    File file = createFileWithData("file", createStringDataset(3, 100));    Metadata metadata = FileSystems.matchSingleFileSpec(file.getPath());    TestFileBasedSource source = new TestFileBasedSource(metadata, 1, 0, file.length(), null);        assertSplitAtFractionFails(source, 0, 0.7, options);    assertSplitAtFractionSucceedsAndConsistent(source, 1, 0.7, options);    assertSplitAtFractionSucceedsAndConsistent(source, 30, 0.7, options);    assertSplitAtFractionFails(source, 0, 0.0, options);    assertSplitAtFractionFails(source, 70, 0.3, options);    assertSplitAtFractionFails(source, 100, 1.0, options);    assertSplitAtFractionFails(source, 100, 0.99, options);    assertSplitAtFractionSucceedsAndConsistent(source, 100, 0.995, options);}
public void beam_f19563_0() throws Exception
{    PipelineOptions options = PipelineOptionsFactory.create();        File file = createFileWithData("file", createStringDataset(3, 20));    Metadata metadata = FileSystems.matchSingleFileSpec(file.getPath());    TestFileBasedSource source = new TestFileBasedSource(metadata, 1, 0, file.length(), null);    assertSplitAtFractionExhaustive(source, options);}
public void beam_f19571_0() throws IOException, InterruptedException
{        final Path sourcePath = tmpFolder.getRoot().toPath().resolve("source");    sourcePath.toFile().mkdir();    Files.write(sourcePath.resolve("first"), new byte[42]);    Files.write(sourcePath.resolve("second"), new byte[37]);    Files.write(sourcePath.resolve("third"), new byte[99]);        final Path watchPath = tmpFolder.getRoot().toPath().resolve("watch");    watchPath.toFile().mkdir();    PCollection<MatchResult.Metadata> matchMetadata = p.apply(FileIO.match().filepattern(watchPath.resolve("*").toString()).continuously(Duration.millis(100), Watch.Growth.afterTimeSinceNewOutput(Duration.standardSeconds(3))));    PCollection<MatchResult.Metadata> matchAllMetadata = p.apply(Create.of(watchPath.resolve("*").toString())).apply(FileIO.matchAll().continuously(Duration.millis(100), Watch.Growth.afterTimeSinceNewOutput(Duration.standardSeconds(3))));    assertEquals(PCollection.IsBounded.UNBOUNDED, matchMetadata.isBounded());    assertEquals(PCollection.IsBounded.UNBOUNDED, matchAllMetadata.isBounded());            CopyOption[] copyOptions = { StandardCopyOption.COPY_ATTRIBUTES };    Thread writer = new Thread(() -> {        try {            Thread.sleep(1000);            Files.copy(sourcePath.resolve("first"), watchPath.resolve("first"), copyOptions);            Thread.sleep(300);            Files.copy(sourcePath.resolve("second"), watchPath.resolve("second"), copyOptions);            Thread.sleep(300);            Files.copy(sourcePath.resolve("third"), watchPath.resolve("third"), copyOptions);        } catch (IOException | InterruptedException e) {            throw new RuntimeException(e);        }    });    writer.start();            List<MatchResult.Metadata> expected = Arrays.asList(metadata(watchPath.resolve("first"), 42, lastModifiedMillis(sourcePath.resolve("first"))), metadata(watchPath.resolve("second"), 37, lastModifiedMillis(sourcePath.resolve("second"))), metadata(watchPath.resolve("third"), 99, lastModifiedMillis(sourcePath.resolve("third"))));    PAssert.that(matchMetadata).containsInAnyOrder(expected);    PAssert.that(matchAllMetadata).containsInAnyOrder(expected);    p.run();    writer.join();}
public void beam_f19572_0() throws IOException
{    final String path = tmpFolder.newFile("file").getAbsolutePath();    final String pathGZ = tmpFolder.newFile("file.gz").getAbsolutePath();    Files.write(new File(path).toPath(), "Hello world".getBytes(Charsets.UTF_8));    try (Writer writer = new OutputStreamWriter(new GZIPOutputStream(new FileOutputStream(pathGZ)), Charsets.UTF_8)) {        writer.write("Hello world");    }    PCollection<MatchResult.Metadata> matches = p.apply("Match", FileIO.match().filepattern(path));    PCollection<FileIO.ReadableFile> decompressedAuto = matches.apply("Read AUTO", FileIO.readMatches().withCompression(Compression.AUTO));    PCollection<FileIO.ReadableFile> decompressedDefault = matches.apply("Read default", FileIO.readMatches());    PCollection<FileIO.ReadableFile> decompressedUncompressed = matches.apply("Read UNCOMPRESSED", FileIO.readMatches().withCompression(Compression.UNCOMPRESSED));    for (PCollection<FileIO.ReadableFile> c : Arrays.asList(decompressedAuto, decompressedDefault, decompressedUncompressed)) {        PAssert.thatSingleton(c).satisfies(input -> {            assertEquals(path, input.getMetadata().resourceId().toString());            assertEquals("Hello world".length(), input.getMetadata().sizeBytes());            assertEquals(Compression.UNCOMPRESSED, input.getCompression());            assertTrue(input.getMetadata().isReadSeekEfficient());            try {                assertEquals("Hello world", input.readFullyAsUTF8String());            } catch (IOException e) {                throw new RuntimeException(e);            }            return null;        });    }    PCollection<MatchResult.Metadata> matchesGZ = p.apply("Match GZ", FileIO.match().filepattern(pathGZ));    PCollection<FileIO.ReadableFile> compressionAuto = matchesGZ.apply("Read GZ AUTO", FileIO.readMatches().withCompression(Compression.AUTO));    PCollection<FileIO.ReadableFile> compressionDefault = matchesGZ.apply("Read GZ default", FileIO.readMatches());    PCollection<FileIO.ReadableFile> compressionGzip = matchesGZ.apply("Read GZ GZIP", FileIO.readMatches().withCompression(Compression.GZIP));    for (PCollection<FileIO.ReadableFile> c : Arrays.asList(compressionAuto, compressionDefault, compressionGzip)) {        PAssert.thatSingleton(c).satisfies(input -> {            assertEquals(pathGZ, input.getMetadata().resourceId().toString());            assertFalse(input.getMetadata().sizeBytes() == "Hello world".length());            assertEquals(Compression.GZIP, input.getCompression());            assertFalse(input.getMetadata().isReadSeekEfficient());            try {                assertEquals("Hello world", input.readFullyAsUTF8String());            } catch (IOException e) {                throw new RuntimeException(e);            }            return null;        });    }    p.run();}
private static MatchResult.Metadata beam_f19573_0(Path path, int size, long lastModifiedMillis)
{    return MatchResult.Metadata.builder().setResourceId(FileSystems.matchNewResource(path.toString(), false)).setIsReadSeekEfficient(true).setSizeBytes(size).setLastModifiedMillis(lastModifiedMillis).build();}
public void beam_f19581_0() throws Exception
{    Path existingPath = temporaryFolder.newFile().toPath();    Path nonExistentPath = existingPath.resolveSibling("non-existent");    createFileWithContent(existingPath, "content1");    FileSystems.delete(toResourceIds(ImmutableList.of(existingPath, nonExistentPath), false));}
public void beam_f19582_0() throws Exception
{    Path existingPath = temporaryFolder.newFile().toPath();    Path nonExistentPath = existingPath.resolveSibling("non-existent");    Path destPath1 = existingPath.resolveSibling("dest1");    Path destPath2 = nonExistentPath.resolveSibling("dest2");    createFileWithContent(existingPath, "content1");    thrown.expect(NoSuchFileException.class);    FileSystems.copy(toResourceIds(ImmutableList.of(existingPath, nonExistentPath), false), toResourceIds(ImmutableList.of(destPath1, destPath2), false));}
public void beam_f19583_0() throws Exception
{    Path srcPath1 = temporaryFolder.newFile().toPath();    Path nonExistentPath = srcPath1.resolveSibling("non-existent");    Path srcPath3 = temporaryFolder.newFile().toPath();    Path destPath1 = srcPath1.resolveSibling("dest1");    Path destPath2 = nonExistentPath.resolveSibling("dest2");    Path destPath3 = srcPath1.resolveSibling("dest3");    createFileWithContent(srcPath1, "content1");    createFileWithContent(srcPath3, "content3");    FileSystems.copy(toResourceIds(ImmutableList.of(srcPath1, nonExistentPath, srcPath3), false), toResourceIds(ImmutableList.of(destPath1, destPath2, destPath3), false), MoveOptions.StandardMoveOptions.IGNORE_MISSING_FILES);    assertTrue(srcPath1.toFile().exists());    assertTrue(srcPath3.toFile().exists());    assertThat(Files.readLines(srcPath1.toFile(), StandardCharsets.UTF_8), containsInAnyOrder("content1"));    assertFalse(destPath2.toFile().exists());    assertThat(Files.readLines(srcPath3.toFile(), StandardCharsets.UTF_8), containsInAnyOrder("content3"));}
public void beam_f19591_0() throws Exception
{    Path filePath = tmpFolder.newFile("somefile").toPath();    Metadata metadata = Metadata.builder().setResourceId(FileSystems.matchNewResource(filePath.toString(), false)).setIsReadSeekEfficient(true).setSizeBytes(1024).build();    CoderProperties.coderDecodeEncodeEqual(MetadataCoder.of(), metadata);}
public void beam_f19592_0() throws Exception
{    Path filePath = tmpFolder.newFile("somefile").toPath();    Metadata metadata = Metadata.builder().setResourceId(FileSystems.matchNewResource(filePath.toString(), false)).setIsReadSeekEfficient(true).setSizeBytes(1024).setLastModifiedMillis(1541097000L).build();        CoderProperties.coderDecodeEncodeEqual(MetadataCoder.of(), metadata);}
public void beam_f19593_0()
{    CoderProperties.coderSerializable(MetadataCoder.of());}
public void beam_f19601_0()
{    long start = 10;    long end = 1000;    PCollection<Long> input = p.apply(GenerateSequence.from(start).to(end));    addCountingAsserts(input, start, end);    p.run();}
public void beam_f19602_0()
{    PTransform<?, ?> input = GenerateSequence.from(0).to(1234);    DisplayData displayData = DisplayData.from(input);    assertThat(displayData, hasDisplayItem("from", 0));    assertThat(displayData, hasDisplayItem("to", 1234));}
public void beam_f19603_0()
{    PTransform<?, ?> input = GenerateSequence.from(12).to(1234);    DisplayData displayData = DisplayData.from(input);    assertThat(displayData, hasDisplayItem("from", 12));    assertThat(displayData, hasDisplayItem("to", 1234));}
public void beam_f19611_0() throws Exception
{    File existingFile = temporaryFolder.newFile();    testCreate(existingFile.toPath());}
public void beam_f19612_0() throws Exception
{    testCreate(temporaryFolder.getRoot().toPath().resolve("file.txt"));}
public void beam_f19613_0() throws Exception
{    testCreate(temporaryFolder.getRoot().toPath().resolve("non-existent-dir").resolve("file.txt"));}
public void beam_f19621_0() throws Exception
{    String globPattern = "/A/a=[0-9][0-9][0-9]/*/*";    File baseFolder = temporaryFolder.newFolder("A");    File folder1 = new File(baseFolder, "a=100");    File folder2 = new File(baseFolder, "a=233");    File dataFolder1 = new File(folder1, "data1");    File dataFolder2 = new File(folder2, "data_dir");    File expectedFile1 = new File(dataFolder1, "file1");    File expectedFile2 = new File(dataFolder2, "data_file2");    createEmptyFile(expectedFile1);    createEmptyFile(expectedFile2);    List<String> expected = ImmutableList.of(expectedFile1.getAbsolutePath(), expectedFile2.getAbsolutePath());    List<MatchResult> matchResults = matchGlobWithPathPrefix(temporaryFolder.getRoot().toPath(), globPattern);    assertThat(toFilenames(matchResults), containsInAnyOrder(expected.toArray(new String[expected.size()])));}
public void beam_f19622_0() throws Exception
{    List<String> expected = ImmutableList.of(temporaryFolder.newFile("a").toString());    temporaryFolder.newFile("aa");    temporaryFolder.newFile("ab");    List<MatchResult> matchResults = localFileSystem.match(ImmutableList.of(temporaryFolder.getRoot().toPath().resolve("a").toString()));    assertThat(toFilenames(matchResults), containsInAnyOrder(expected.toArray(new String[expected.size()])));}
public void beam_f19623_0() throws Exception
{    final Path dir = temporaryFolder.newFolder("dir").toPath();    final MatchResult matchResult = Iterables.getOnlyElement(localFileSystem.match(Collections.singletonList(dir.toString())));    assertThat(matchResult, equalTo(MatchResult.create(MatchResult.Status.OK, ImmutableList.of(MatchResult.Metadata.builder().setResourceId(LocalResourceId.fromPath(dir, true)).setIsReadSeekEfficient(true).setSizeBytes(dir.toFile().length()).setLastModifiedMillis(dir.toFile().lastModified()).build()))));}
public void beam_f19631_0() throws Exception
{    File matchedSubDir = temporaryFolder.newFolder("a");    File matchedSubDirFile = File.createTempFile("sub-dir-file", "", matchedSubDir);    matchedSubDirFile.deleteOnExit();    File unmatchedSubDir = temporaryFolder.newFolder("b");    File unmatchedSubDirFile = File.createTempFile("sub-dir-file", "", unmatchedSubDir);    unmatchedSubDirFile.deleteOnExit();    List<String> expected = ImmutableList.of(matchedSubDirFile.toString(), temporaryFolder.newFile("aa").toString(), temporaryFolder.newFile("ab").toString());    temporaryFolder.newFile("ba");    temporaryFolder.newFile("bb");    List<MatchResult> matchResults = matchGlobWithPathPrefix(temporaryFolder.getRoot().toPath().resolve("a"), "**");    assertThat(toFilenames(matchResults), Matchers.hasItems(expected.toArray(new String[expected.size()])));}
public void beam_f19632_0() throws Exception
{    List<String> expected = ImmutableList.of(temporaryFolder.newFile("a").toString());    temporaryFolder.newFolder("a_dir_that_should_not_be_matched");    List<MatchResult> matchResults = matchGlobWithPathPrefix(temporaryFolder.getRoot().toPath().resolve("a"), "*");    assertThat(toFilenames(matchResults), containsInAnyOrder(expected.toArray(new String[expected.size()])));}
public void beam_f19633_0() throws Exception
{    Path pattern = LocalResourceId.fromPath(temporaryFolder.getRoot().toPath(), true).resolve("non_existing_dir", StandardResolveOptions.RESOLVE_DIRECTORY).resolve("*", StandardResolveOptions.RESOLVE_FILE).getPath();    assertTrue(toFilenames(localFileSystem.match(ImmutableList.of(pattern.toString()))).isEmpty());}
public void beam_f19641_0()
{    if (SystemUtils.IS_OS_WINDOWS) {                return;    }                    assertEquals(toResourceIdentifier("file://home/bb"), toResourceIdentifier("file://root/../home/output/../").resolve("aa", StandardResolveOptions.RESOLVE_DIRECTORY).resolve("..", StandardResolveOptions.RESOLVE_DIRECTORY).resolve("bb", StandardResolveOptions.RESOLVE_FILE));    assertEquals(toResourceIdentifier("file://root/aa/bb"), toResourceIdentifier("file://root/./").resolve("aa", StandardResolveOptions.RESOLVE_DIRECTORY).resolve(".", StandardResolveOptions.RESOLVE_DIRECTORY).resolve("bb", StandardResolveOptions.RESOLVE_FILE));    assertEquals(toResourceIdentifier("aa/bb"), toResourceIdentifier("a/../").resolve("aa", StandardResolveOptions.RESOLVE_DIRECTORY).resolve(".", StandardResolveOptions.RESOLVE_DIRECTORY).resolve("bb", StandardResolveOptions.RESOLVE_FILE));    assertEquals(toResourceIdentifier("/aa/bb"), toResourceIdentifier("/a/../").resolve("aa", StandardResolveOptions.RESOLVE_DIRECTORY).resolve(".", StandardResolveOptions.RESOLVE_DIRECTORY).resolve("bb", StandardResolveOptions.RESOLVE_FILE));        assertEquals(toResourceIdentifier("aa/bb"), toResourceIdentifier("./").resolve("aa", StandardResolveOptions.RESOLVE_DIRECTORY).resolve(".", StandardResolveOptions.RESOLVE_DIRECTORY).resolve("bb", StandardResolveOptions.RESOLVE_FILE));    assertEquals(toResourceIdentifier("../aa/bb"), toResourceIdentifier("../").resolve("aa", StandardResolveOptions.RESOLVE_DIRECTORY).resolve(".", StandardResolveOptions.RESOLVE_DIRECTORY).resolve("bb", StandardResolveOptions.RESOLVE_FILE));    assertEquals(toResourceIdentifier("~/aa/bb/"), toResourceIdentifier("~/").resolve("aa", StandardResolveOptions.RESOLVE_DIRECTORY).resolve(".", StandardResolveOptions.RESOLVE_DIRECTORY).resolve("bb", StandardResolveOptions.RESOLVE_DIRECTORY));}
public void beam_f19642_0()
{    if (SystemUtils.IS_OS_WINDOWS) {                return;    }    assertEquals(toResourceIdentifier("/root/tmp/"), toResourceIdentifier("/root/").resolve("tmp/", StandardResolveOptions.RESOLVE_DIRECTORY));}
public void beam_f19643_0()
{    thrown.expect(IllegalArgumentException.class);    thrown.expectMessage("The resolved file: [tmp/] should not end with '/'.");    toResourceIdentifier("/root/").resolve("tmp/", StandardResolveOptions.RESOLVE_FILE);}
public void beam_f19651_0()
{    assertNull(toResourceIdentifier("/").getFilename());    assertEquals("tmp", toResourceIdentifier("/root/tmp").getFilename());    assertEquals("tmp", toResourceIdentifier("/root/tmp/").getFilename());    assertEquals("xyz.txt", toResourceIdentifier("/root/tmp/xyz.txt").getFilename());}
public void beam_f19652_0()
{    ResourceIdTester.runResourceIdBattery(toResourceIdentifier("/tmp/foo/"));}
private LocalResourceId beam_f19653_0(String str)
{    boolean isDirectory;    if (SystemUtils.IS_OS_WINDOWS) {        isDirectory = str.endsWith("\\");    } else {        isDirectory = str.endsWith("/");    }    return LocalResourceId.fromPath(Paths.get(str), isDirectory);}
public boolean beam_f19661_0()
{    ++current;    return true;}
public Integer beam_f19662_0() throws NoSuchElementException
{    return (int) current;}
public boolean beam_f19663_0()
{    return current % granularity == 0;}
public void beam_f19672_0() throws IOException
{    PipelineOptions options = PipelineOptionsFactory.create();    CoarseRangeSource source = new CoarseRangeSource(13, 17, 1, 2);    try (OffsetBasedReader<Integer> reader = source.createReader(options)) {                assertEquals(0.0, reader.getFractionConsumed(), 1e-6);        assertEquals(0, reader.getSplitPointsConsumed());        assertEquals(BoundedReader.SPLIT_POINTS_UNKNOWN, reader.getSplitPointsRemaining());                assertTrue(reader.start());        assertTrue(reader.isAtSplitPoint());        assertEquals(14, reader.getCurrent().intValue());        assertEquals(0, reader.getSplitPointsConsumed());        assertEquals(BoundedReader.SPLIT_POINTS_UNKNOWN, reader.getSplitPointsRemaining());                assertTrue(reader.advance());        assertEquals(15, reader.getCurrent().intValue());        assertEquals(0, reader.getSplitPointsConsumed());        assertEquals(BoundedReader.SPLIT_POINTS_UNKNOWN, reader.getSplitPointsRemaining());                        assertTrue(reader.advance());        assertTrue(reader.isAtSplitPoint());        assertEquals(16, reader.getCurrent().intValue());        assertEquals(1, reader.getSplitPointsConsumed());                assertEquals(1, reader.getSplitPointsRemaining());                assertTrue(reader.advance());        assertEquals(17, reader.getCurrent().intValue());        assertEquals(1, reader.getSplitPointsConsumed());        assertEquals(1, reader.getSplitPointsRemaining());                assertFalse(reader.advance());        assertEquals(1.0, reader.getFractionConsumed(), 1e-6);        assertEquals(2, reader.getSplitPointsConsumed());        assertEquals(0, reader.getSplitPointsRemaining());    }}
public void beam_f19673_0() throws IOException
{    PipelineOptions options = PipelineOptionsFactory.create();    CoarseRangeSource source = new CoarseRangeSource(13, 17, 1, 100);    try (OffsetBasedReader<Integer> reader = source.createReader(options)) {                assertEquals(0.0, reader.getFractionConsumed(), 1e-6);        assertEquals(0, reader.getSplitPointsConsumed());        assertEquals(BoundedReader.SPLIT_POINTS_UNKNOWN, reader.getSplitPointsRemaining());                assertFalse(reader.start());                assertEquals(1.0, reader.getFractionConsumed(), 1e-6);        assertEquals(0, reader.getSplitPointsConsumed());        assertEquals(0, reader.getSplitPointsRemaining());    }}
public void beam_f19674_0() throws IOException
{    PipelineOptions options = PipelineOptionsFactory.create();    CoarseRangeSource source = new CoarseRangeSource(13, 35, 1, 10);    try (CoarseRangeReader reader = source.createReader(options)) {        List<Integer> originalItems = new ArrayList<>();        assertTrue(reader.start());        originalItems.add(reader.getCurrent());        assertTrue(reader.advance());        originalItems.add(reader.getCurrent());        assertTrue(reader.advance());        originalItems.add(reader.getCurrent());        assertTrue(reader.advance());        originalItems.add(reader.getCurrent());        assertNull(reader.splitAtFraction(0.0));        assertNull(reader.splitAtFraction(reader.getFractionConsumed() - 0.1));        BoundedSource<Integer> residual = reader.splitAtFraction(reader.getFractionConsumed() + 0.1);        BoundedSource<Integer> primary = reader.getCurrentSource();        List<Integer> primaryItems = readFromSource(primary, options);        List<Integer> residualItems = readFromSource(residual, options);        for (Integer item : residualItems) {            assertTrue(item > reader.getCurrentOffset());        }        assertFalse(primaryItems.isEmpty());        assertFalse(residualItems.isEmpty());        assertTrue(primaryItems.get(primaryItems.size() - 1) <= residualItems.get(0));        while (reader.advance()) {            originalItems.add(reader.getCurrent());        }        assertEquals(originalItems, primaryItems);    }}
private static void beam_f19682_0(ByteKeyRange left, ByteKeyRange right)
{    bidirectionalOverlapHelper(left, right, true);}
private static void beam_f19683_0(ByteKeyRange left, ByteKeyRange right, boolean result)
{    assertEquals(String.format("%s overlaps %s", left, right), result, left.overlaps(right));    assertEquals(String.format("%s overlaps %s", right, left), result, right.overlaps(left));}
public void beam_f19684_0()
{    bidirectionalOverlap(ByteKeyRange.ALL_KEYS, ByteKeyRange.ALL_KEYS);    bidirectionalOverlap(ByteKeyRange.ALL_KEYS, RANGE_1_10);    bidirectionalOverlap(UP_TO_1, UP_TO_1);    bidirectionalOverlap(UP_TO_1, UP_TO_5);    bidirectionalOverlap(UP_TO_50, AFTER_10);    bidirectionalOverlap(UP_TO_50, RANGE_1_10);    bidirectionalOverlap(UP_TO_10, UP_TO_50);    bidirectionalOverlap(RANGE_1_10, RANGE_5_50);    bidirectionalOverlap(AFTER_1, AFTER_5);    bidirectionalOverlap(RANGE_5_10, RANGE_1_10);    bidirectionalOverlap(RANGE_5_10, RANGE_5_50);}
public void beam_f19692_0()
{    String fmt = "Interpolating %s at fraction 0.0 should not return the empty key";    for (ByteKeyRange range : TEST_RANGES) {        range = ByteKeyRange.ALL_KEYS;        assertFalse(String.format(fmt, range), range.interpolateKey(0.0).isEmpty());    }}
public void beam_f19693_0()
{        assertEquals(AFTER_1.getStartKey(), ByteKey.of(1));    assertEquals(AFTER_1.getEndKey(), ByteKey.EMPTY);        assertEquals(RANGE_1_10.getStartKey(), ByteKey.of(1));    assertEquals(RANGE_1_10.getEndKey(), ByteKey.of(10));        assertEquals(UP_TO_10.getStartKey(), ByteKey.EMPTY);    assertEquals(UP_TO_10.getEndKey(), ByteKey.of(10));}
public void beam_f19694_0()
{    assertEquals("ByteKeyRange{startKey=[], endKey=[0a]}", UP_TO_10.toString());}
public void beam_f19702_0()
{    ByteKeyRangeTracker tracker = ByteKeyRangeTracker.of(INITIAL_RANGE);    double delta = 0.00001;    assertEquals(0.0, tracker.getFractionConsumed(), delta);    tracker.tryReturnRecordAt(true, INITIAL_START_KEY);    assertEquals(0.0, tracker.getFractionConsumed(), delta);    tracker.tryReturnRecordAt(true, INITIAL_MIDDLE_KEY);    assertEquals(0.5, tracker.getFractionConsumed(), delta);    tracker.tryReturnRecordAt(true, BEFORE_END_KEY);    assertEquals(1 - 1 / INITIAL_RANGE_SIZE, tracker.getFractionConsumed(), delta);}
public void beam_f19703_0()
{    ByteKeyRangeTracker tracker = ByteKeyRangeTracker.of(INITIAL_RANGE);    double delta = 0.00001;    assertTrue(tracker.tryReturnRecordAt(true, INITIAL_START_KEY));    tracker.markDone();    assertEquals(1.0, tracker.getFractionConsumed(), delta);}
public void beam_f19704_0()
{    ByteKeyRangeTracker tracker = ByteKeyRangeTracker.of(INITIAL_RANGE);    double delta = 0.00001;    assertTrue(tracker.tryReturnRecordAt(true, INITIAL_START_KEY));    assertTrue(tracker.tryReturnRecordAt(false, KEY_LARGER_THAN_END));    assertEquals(1.0, tracker.getFractionConsumed(), delta);}
public void beam_f19712_0()
{        for (int i = 0; i < TEST_KEYS.length; ++i) {        for (int j = 0; j < TEST_KEYS.length; ++j) {            ByteKey left = TEST_KEYS[i];            ByteKey right = TEST_KEYS[j];            int cmp = left.compareTo(right);            if (i < j && !(cmp < 0)) {                fail(String.format("Expected that cmp(%s, %s) < 0, got %d [i=%d, j=%d]", left, right, cmp, i, j));            } else if (i == j && !(cmp == 0)) {                fail(String.format("Expected that cmp(%s, %s) == 0, got %d [i=%d, j=%d]", left, right, cmp, i, j));            } else if (i > j && !(cmp > 0)) {                fail(String.format("Expected that cmp(%s, %s) > 0, got %d [i=%d, j=%d]", left, right, cmp, i, j));            }        }    }}
public void beam_f19713_0()
{        for (int i = 0; i < TEST_KEYS.length; ++i) {        for (int j = 0; j < TEST_KEYS.length; ++j) {            ByteKey left = TEST_KEYS[i];            ByteKey right = TEST_KEYS[j];            boolean eq = left.equals(right);            if (i == j) {                assertTrue(String.format("Expected that %s is equal to itself.", left), eq);                assertTrue(String.format("Expected that %s is equal to a copy of itself.", left), left.equals(ByteKey.copyFrom(right.getValue())));            } else {                assertFalse(String.format("Expected that %s is not equal to %s", left, right), eq);            }        }    }}
public void beam_f19714_0()
{        int collisions = 0;    for (int i = 0; i < TEST_KEYS.length; ++i) {        int left = TEST_KEYS[i].hashCode();        int leftClone = ByteKey.copyFrom(TEST_KEYS[i].getValue()).hashCode();        assertEquals(String.format("Expected same hash code for %s and a copy of itself", TEST_KEYS[i]), left, leftClone);        for (int j = i + 1; j < TEST_KEYS.length; ++j) {            int right = TEST_KEYS[j].hashCode();            if (left == right) {                ++collisions;            }        }    }    int totalUnequalTests = TEST_KEYS.length * (TEST_KEYS.length - 1) / 2;    assertThat("Too many hash collisions", collisions, lessThan(totalUnequalTests / 2));}
public void beam_f19722_0() throws Exception
{    OffsetRangeTracker tracker = new OffsetRangeTracker(100, 200);    assertFalse(tracker.trySplitAtPosition(150));}
public void beam_f19723_0() throws Exception
{    OffsetRangeTracker tracker = new OffsetRangeTracker(100, 200);    assertTrue(tracker.tryReturnRecordAt(true, 110));        assertFalse(tracker.trySplitAtPosition(109));    assertFalse(tracker.trySplitAtPosition(110));    assertFalse(tracker.trySplitAtPosition(200));    assertFalse(tracker.trySplitAtPosition(210));        assertTrue(tracker.copy().trySplitAtPosition(111));    assertTrue(tracker.copy().trySplitAtPosition(129));    assertTrue(tracker.copy().trySplitAtPosition(130));    assertTrue(tracker.copy().trySplitAtPosition(131));    assertTrue(tracker.copy().trySplitAtPosition(150));    assertTrue(tracker.copy().trySplitAtPosition(199));        assertTrue(tracker.trySplitAtPosition(170));    assertTrue(tracker.trySplitAtPosition(150));            assertTrue(tracker.copy().tryReturnRecordAt(true, 135));    assertTrue(tracker.copy().tryReturnRecordAt(true, 135));        assertTrue(tracker.copy().tryReturnRecordAt(true, 149));        assertFalse(tracker.tryReturnRecordAt(true, 150));    assertFalse(tracker.tryReturnRecordAt(true, 151));        assertTrue(tracker.tryReturnRecordAt(false, 152));    assertTrue(tracker.tryReturnRecordAt(false, 160));    assertFalse(tracker.tryReturnRecordAt(true, 171));}
public void beam_f19724_0() throws Exception
{        OffsetRangeTracker tracker = new OffsetRangeTracker(3, 6);        assertEquals(3, tracker.getPositionForFractionConsumed(0.0));    assertEquals(3, tracker.getPositionForFractionConsumed(1.0 / 6));    assertEquals(3, tracker.getPositionForFractionConsumed(0.333));        assertEquals(4, tracker.getPositionForFractionConsumed(0.334));    assertEquals(4, tracker.getPositionForFractionConsumed(0.666));        assertEquals(5, tracker.getPositionForFractionConsumed(0.667));    assertEquals(5, tracker.getPositionForFractionConsumed(0.999));        assertEquals(6, tracker.getPositionForFractionConsumed(1.0));}
public void beam_f19732_0()
{    thrown.expect(IllegalArgumentException.class);    Read.from(new NotSerializableBoundedSource());}
public void beam_f19733_0()
{    Read.from(new SerializableBoundedSource());}
public void beam_f19734_0()
{    thrown.expect(IllegalArgumentException.class);    Read.from(new NotSerializableUnboundedSource());}
public Coder<String> beam_f19742_0()
{    return StringUtf8Coder.of();}
public List<? extends UnboundedSource<String, NoOpCheckpointMark>> beam_f19743_0(int desiredNumSplits, PipelineOptions options) throws Exception
{    return null;}
public UnboundedReader<String> beam_f19744_0(PipelineOptions options, NoOpCheckpointMark checkpointMark)
{    return null;}
public void beam_f19753_0() throws Exception
{        SerializableAvroCodecFactory codec = new SerializableAvroCodecFactory();    codec.toString();}
public static SimpleSink<Void> beam_f19754_0(ResourceId tempDirectory, FilenamePolicy filenamePolicy)
{    return new SimpleSink<>(tempDirectory, DynamicFileDestinations.constant(filenamePolicy), Compression.UNCOMPRESSED);}
public static SimpleSink<Void> beam_f19755_0(ResourceId baseDirectory, String prefix, String shardTemplate, String suffix, WritableByteChannelFactory writableByteChannelFactory)
{    DynamicDestinations<String, Void, String> dynamicDestinations = DynamicFileDestinations.constant(DefaultFilenamePolicy.fromParams(new Params().withBaseFilename(baseDirectory.resolve(prefix, StandardResolveOptions.RESOLVE_FILE)).withShardTemplate(shardTemplate).withSuffix(suffix)));    return new SimpleSink<>(baseDirectory, dynamicDestinations, writableByteChannelFactory);}
protected void beam_f19763_0() throws Exception
{    channel.write(wrap(FOOTER));}
public void beam_f19764_0(String value) throws Exception
{    channel.write(wrap(value));}
private static File beam_f19765_0(List<String> lines, TemporaryFolder folder, String fileName, Compression compression) throws IOException
{    File file = folder.getRoot().toPath().resolve(fileName).toFile();    OutputStream output = new FileOutputStream(file);    switch(compression) {        case UNCOMPRESSED:            break;        case GZIP:            output = new GZIPOutputStream(output);            break;        case BZIP2:            output = new BZip2CompressorOutputStream(output);            break;        case ZIP:            ZipOutputStream zipOutput = new ZipOutputStream(output);            zipOutput.putNextEntry(new ZipEntry("entry"));            output = zipOutput;            break;        case DEFLATE:            output = new DeflateCompressorOutputStream(output);            break;        default:            throw new UnsupportedOperationException(compression.toString());    }    writeToStreamAndClose(lines, output);    return file;}
public void beam_f19773_0() throws Exception
{    String fileName = lines.size() + "_" + compression + "_no_extension";    File fileWithNoExtension = writeToFile(lines, tempFolder, fileName, compression);    assertReadingCompressedFileMatchesExpected(fileWithNoExtension, compression, lines, p);    p.run();}
public void beam_f19774_0() throws Exception
{    String fileName = lines.size() + "_" + compression + "_no_extension" + getFileSuffix(compression);    File fileWithExtension = writeToFile(lines, tempFolder, fileName, compression);        if (lines.size() == LINES_NUMBER_FOR_LARGE && !compression.equals(UNCOMPRESSED)) {        File uncompressedFile = writeToFile(lines, tempFolder, "large.txt", UNCOMPRESSED);        assertThat(uncompressedFile.length(), greaterThan(fileWithExtension.length()));    }    assertReadingCompressedFileMatchesExpected(fileWithExtension, compression, lines, p);    p.run();}
public void beam_f19775_0() throws Exception
{        String fileName = lines.size() + "_" + compression + "_no_extension" + getFileSuffix(compression);    File fileWithExtension = writeToFile(lines, tempFolder, fileName, compression);    assertReadingCompressedFileMatchesExpected(fileWithExtension, AUTO, lines, p);    p.run();}
public void beam_f19783_0() throws Exception
{    final String[] inputStrings = new String[] {     "To be, or not to be: that |is the question: ",     "To be, or not to be: that *is the question: ",     "Whether 'tis nobler in the mind to suffer |*",     "The slings and arrows of outrageous fortune,|" };    File tmpFile = tempFolder.newFile("tmpfile.txt");    String filename = tmpFile.getPath();    try (Writer writer = Files.newBufferedWriter(tmpFile.toPath(), UTF_8)) {        writer.write(Joiner.on("").join(inputStrings));    }    PAssert.that(p.apply(TextIO.read().from(filename).withDelimiter(new byte[] { '|', '*' }))).containsInAnyOrder("To be, or not to be: that |is the question: To be, or not to be: " + "that *is the question: Whether 'tis nobler in the mind to suffer ", "The slings and arrows of outrageous fortune,|");    p.run();}
public void beam_f19784_0() throws Exception
{    List<String> testCases = Lists.newArrayList();    String infix = "first|*second|*|*third";    String[] affixes = new String[] { "", "|", "*", "|*" };    for (String prefix : affixes) {        for (String suffix : affixes) {            testCases.add(prefix + infix + suffix);        }    }    for (String testCase : testCases) {        SourceTestUtils.assertSplitAtFractionExhaustive(TextIOReadTest.prepareSource(tempFolder, testCase.getBytes(UTF_8), new byte[] { '|', '*' }), PipelineOptionsFactory.create());    }}
public void beam_f19785_0() throws Exception
{    runTestRead(LINES_ARRAY);}
public void beam_f19793_0() throws Exception
{    File smallGzNotCompressed = writeToFile(TINY, tempFolder, "tiny_uncompressed.gz", UNCOMPRESSED);        assertReadingCompressedFileMatchesExpected(smallGzNotCompressed, AUTO, TINY, p);    p.run();}
public void beam_f19794_0() throws Exception
{    File file = createZipFile(new ArrayList<>(), tempFolder, "empty zip file");    assertReadingCompressedFileMatchesExpected(file, ZIP, EMPTY, p);    p.run();}
public void beam_f19795_0() throws Exception
{    String[] entry0 = new String[] { "first", "second", "three" };    String[] entry1 = new String[] { "four", "five", "six" };    String[] entry2 = new String[] { "seven", "eight", "nine" };    List<String> expected = new ArrayList<>();    File file = createZipFile(expected, tempFolder, "multiple entries", entry0, entry1, entry2);    assertReadingCompressedFileMatchesExpected(file, ZIP, expected, p);    p.run();}
public void beam_f19803_0() throws Exception
{    PipelineOptions options = TestPipeline.testingPipelineOptions();    long desiredBundleSize = 1000;    File largeGz = writeToFile(LARGE, tempFolder, "large.gz", GZIP);        assertThat(largeGz.length(), greaterThan(2 * desiredBundleSize));    FileBasedSource<String> source = TextIO.read().from(largeGz.getPath()).getSource();    List<? extends FileBasedSource<String>> splits = source.split(desiredBundleSize, options);        assertThat(splits, hasSize(equalTo(1)));    SourceTestUtils.assertSourcesEqualReferenceSource(source, splits, options);}
public void beam_f19804_0() throws Exception
{    PipelineOptions options = TestPipeline.testingPipelineOptions();    long desiredBundleSize = 1000;    File largeTxt = writeToFile(LARGE, tempFolder, "large.txt", UNCOMPRESSED);        assertThat(largeTxt.length(), greaterThan(2 * desiredBundleSize));    FileBasedSource<String> source = TextIO.read().from(largeTxt.getPath()).withCompression(GZIP).getSource();    List<? extends FileBasedSource<String>> splits = source.split(desiredBundleSize, options);        assertThat(splits, hasSize(equalTo(1)));    SourceTestUtils.assertSourcesEqualReferenceSource(source, splits, options);}
public void beam_f19805_0() throws IOException
{    Path tempFolderPath = tempFolder.getRoot().toPath();    writeToFile(TINY, tempFolder, "readAllTiny1.zip", ZIP);    writeToFile(TINY, tempFolder, "readAllTiny2.txt", UNCOMPRESSED);    writeToFile(LARGE, tempFolder, "readAllLarge1.zip", ZIP);    writeToFile(LARGE, tempFolder, "readAllLarge2.txt", UNCOMPRESSED);    PCollection<String> lines = p.apply(Create.of(tempFolderPath.resolve("readAllTiny*").toString(), tempFolderPath.resolve("readAllLarge*").toString())).apply(TextIO.readAll().withCompression(AUTO));    PAssert.that(lines).containsInAnyOrder(Iterables.concat(TINY, TINY, LARGE, LARGE));    p.run();}
public boolean beam_f19813_0(@Nullable String input)
{    return input.startsWith(prefix);}
public void beam_f19814_0() throws Exception
{    testDynamicDestinations(false);}
public void beam_f19815_0() throws Exception
{    testDynamicDestinations(true);}
private void beam_f19823_0(String[] elems, int numShards) throws Exception
{    runTestWrite(elems, null, null, numShards);}
private void beam_f19824_0(String[] elems, String header, String footer) throws Exception
{    runTestWrite(elems, header, footer, 1);}
private void beam_f19825_0(String[] elems, String header, String footer, int numShards) throws Exception
{    String outputName = "file.txt";    Path baseDir = Files.createTempDirectory(tempFolder.getRoot().toPath(), "testwrite");    ResourceId baseFilename = FileBasedSink.convertToFileResourceIfPossible(baseDir.resolve(outputName).toString());    PCollection<String> input = p.apply("CreateInput", Create.of(Arrays.asList(elems)).withCoder(StringUtf8Coder.of()));    TextIO.TypedWrite<String, Void> write = TextIO.write().to(baseFilename).withHeader(header).withFooter(footer).withOutputFilenames();    if (numShards == 1) {        write = write.withoutSharding();    } else if (numShards > 0) {        write = write.withNumShards(numShards).withShardNameTemplate(ShardNameTemplate.INDEX_OF_MAX);    }    input.apply(write);    p.run();    assertOutputFiles(elems, header, footer, numShards, baseFilename, firstNonNull(write.getShardTemplate(), DefaultFilenamePolicy.DEFAULT_UNWINDOWED_SHARD_TEMPLATE));}
public void beam_f19833_0() throws Exception
{    runTestWrite(NO_LINES_ARRAY);}
public void beam_f19834_0() throws Exception
{    runTestWrite(LINES_ARRAY, 5);}
public void beam_f19835_0() throws Exception
{    runTestWrite(LINES_ARRAY, MY_HEADER, null);}
public void beam_f19843_0() throws Exception
{    p.enableAbandonedNodeEnforcement(false);    RuntimeTestOptions options = PipelineOptionsFactory.as(RuntimeTestOptions.class);    p.apply(Create.of("")).apply(TextIO.write().to(options.getOutput()));}
public void beam_f19844_0() throws Throwable
{        PCollection<String> data = p.apply(Create.of("0", "1", "2")).apply(Window.<String>into(FixedWindows.of(Duration.standardSeconds(1))).triggering(AfterPane.elementCountAtLeast(3)).withAllowedLateness(Duration.standardMinutes(1)).discardingFiredPanes());    PCollection<String> filenames = data.apply(TextIO.write().to(new File(tempFolder.getRoot(), "windowed-writes").getAbsolutePath()).withNumShards(2).withWindowedWrites().<Void>withOutputFilenames()).getPerDestinationOutputFilenames().apply(Values.create());    PAssert.that(filenames.apply(FileIO.matchAll()).apply(FileIO.readMatches()).apply(TextIO.readFiles())).containsInAnyOrder("0", "1", "2");    PAssert.that(filenames.apply(TextIO.readAll())).containsInAnyOrder("0", "1", "2");    p.run();}
public void beam_f19845_0() throws Exception
{    List<String> data = ImmutableList.of("a", "b", "c", "d", "e", "f");    PAssert.that(p.apply("Create Data ReadFiles", Create.of(data)).apply("Write ReadFiles", FileIO.<String>write().to(tempFolder.getRoot().toString()).withSuffix(".txt").via(TextIO.sink()).withIgnoreWindowing()).getPerDestinationOutputFilenames().apply("Extract Values ReadFiles", Values.create()).apply("Match All", FileIO.matchAll()).apply("Read Matches", FileIO.readMatches()).apply("Read Files", TextIO.readFiles())).containsInAnyOrder(data);    PAssert.that(p.apply("Create Data ReadAll", Create.of(data)).apply("Write ReadAll", FileIO.<String>write().to(tempFolder.getRoot().toString()).withSuffix(".txt").via(TextIO.sink()).withIgnoreWindowing()).getPerDestinationOutputFilenames().apply("Extract Values ReadAll", Values.create()).apply("Read All", TextIO.readAll())).containsInAnyOrder(data);    p.run();}
public void beam_f19853_0()
{    readPipeline.enableAbandonedNodeEnforcement(false);    Metadata metadata = Metadata.builder().setResourceId(FileSystems.matchNewResource("file", false)).setIsReadSeekEfficient(true).setSizeBytes(1024).build();    Create.Values<ReadableFile> create = Create.of(new ReadableFile(metadata, Compression.AUTO));    assertEquals("TFRecordIO.ReadFiles/Read all via FileBasedSource/Read ranges/ParMultiDo(ReadFileRanges).output", readPipeline.apply(create).apply(TFRecordIO.readFiles()).getName());    assertEquals("MyRead/Read all via FileBasedSource/Read ranges/ParMultiDo(ReadFileRanges).output", readPipeline.apply(create).apply("MyRead", TFRecordIO.readFiles()).getName());}
public void beam_f19854_0()
{    TFRecordIO.Read read = TFRecordIO.read().from("foo.*").withCompression(GZIP).withoutValidation();    DisplayData displayData = DisplayData.from(read);    assertThat(displayData, hasDisplayItem("filePattern", "foo.*"));    assertThat(displayData, hasDisplayItem("compressionType", GZIP.toString()));    assertThat(displayData, hasDisplayItem("validation", false));}
public void beam_f19855_0()
{    TFRecordIO.Write write = TFRecordIO.write().to("/foo").withSuffix("bar").withShardNameTemplate("-SS-of-NN-").withNumShards(100).withCompression(GZIP);    DisplayData displayData = DisplayData.from(write);    assertThat(displayData, hasDisplayItem("filePrefix", "/foo"));    assertThat(displayData, hasDisplayItem("fileSuffix", "bar"));    assertThat(displayData, hasDisplayItem("shardNameTemplate", "-SS-of-NN-"));    assertThat(displayData, hasDisplayItem("numShards", 100));    assertThat(displayData, hasDisplayItem("compressionType", GZIP.toString()));}
private void beam_f19863_0(String base64, String[] expected) throws IOException
{    runTestRead(BaseEncoding.base64().decode(base64), expected);}
private void beam_f19864_0(byte[] data, String[] expected) throws IOException
{    File tmpFile = Files.createTempFile(tempFolder.getRoot().toPath(), "file", ".tfrecords").toFile();    String filename = tmpFile.getPath();    try (FileOutputStream fos = new FileOutputStream(tmpFile)) {        fos.write(data);    }    TFRecordIO.Read read = TFRecordIO.read().from(filename);    PCollection<String> output = readPipeline.apply(read).apply(ParDo.of(new ByteArrayToString()));    PAssert.that(output).containsInAnyOrder(expected);    Compression compression = AUTO;    PAssert.that(readPipeline.apply("Create_Paths_ReadFiles_" + tmpFile, Create.of(tmpFile.getPath())).apply("Match_" + tmpFile, FileIO.matchAll()).apply("ReadMatches_" + tmpFile, FileIO.readMatches().withCompression(compression)).apply("ReadFiles_" + compression.toString(), TFRecordIO.readFiles()).apply("ToString", ParDo.of(new ByteArrayToString()))).containsInAnyOrder(expected);    readPipeline.run();}
private void beam_f19865_0(String[] elems, String... base64) throws IOException
{    File tmpFile = Files.createTempFile(tempFolder.getRoot().toPath(), "file", ".tfrecords").toFile();    String filename = tmpFile.getPath();    PCollection<byte[]> input = writePipeline.apply(Create.of(Arrays.asList(elems))).apply(ParDo.of(new StringToByteArray()));    TFRecordIO.Write write = TFRecordIO.write().to(filename).withoutSharding();    input.apply(write);    writePipeline.run();    FileInputStream fis = new FileInputStream(tmpFile);    String written = BaseEncoding.base64().encode(ByteStreams.toByteArray(fis));        assertThat(written, is(in(base64)));}
public void beam_f19873_0() throws IOException
{    runTestRoundTrip(LARGE, 10, ".tfrecords", GZIP, AUTO);}
public void beam_f19874_0() throws IOException
{    runTestRoundTrip(LARGE, 10, ".tfrecords", DEFLATE, AUTO);}
public void beam_f19875_0() throws IOException
{    runTestRoundTrip(LARGE_RECORDS, 10, ".tfrecords", UNCOMPRESSED, UNCOMPRESSED);}
public void beam_f19883_0(ProcessContext c)
{    c.output(KV.of(ThreadLocalRandom.current().nextInt(), c.element()));}
public void beam_f19884_0(ProcessContext c)
{    for (T s : c.element().getValue()) {        c.output(s);    }}
public PCollection<T> beam_f19885_0(PCollection<T> input)
{    return input.apply(window).apply(ParDo.of(new AddArbitraryKey<>())).apply(GroupByKey.create()).apply(ParDo.of(new RemoveArbitraryKey<>()));}
public void beam_f19893_0() throws IOException
{        WriteOptions options = TestPipeline.testingPipelineOptions().as(WriteOptions.class);    options.setTestFlag("test_value");    Pipeline p = TestPipeline.create(options);    List<String> inputs = new ArrayList<>();        List<Long> timestamps = new ArrayList<>();    for (long i = 0; i < 1000; i++) {        inputs.add(Integer.toString(3));        timestamps.add(i + 1);    }    SimpleSink<Void> sink = makeSimpleSink();    WriteFiles<String, ?, String> write = WriteFiles.to(sink).withSharding(new LargestInt());    p.apply(Create.timestamped(inputs, timestamps).withCoder(StringUtf8Coder.of())).apply(IDENTITY_MAP).apply(write).getPerDestinationOutputFilenames().apply(new VerifyFilesExist<>());    p.run();    checkFileContents(getBaseOutputFilename(), inputs, Optional.of(3), true);}
public void beam_f19894_0() throws IOException
{    runShardedWrite(Arrays.asList("one", "two", "three", "four", "five", "six"), IDENTITY_MAP, getBaseOutputFilename(), WriteFiles.to(makeSimpleSink()).withNumShards(20));}
public void beam_f19895_0() throws IOException
{    runShardedWrite(Arrays.asList("1", "2", "3", "1", "2", "3"), IDENTITY_MAP, getBaseOutputFilename(), WriteFiles.to(makeSimpleSink()).withNumShards(4).withShardingFunction(new TestShardingFunction()), new BiFunction<Integer, List<String>, Void>() {        @Override        public Void apply(Integer shardNumber, List<String> shardContent) {                        if (shardNumber == 0) {                assertTrue(shardContent.isEmpty());            } else {                assertEquals(Lists.newArrayList(shardNumber.toString(), shardNumber.toString()), shardContent);            }            return null;        }    });}
public void beam_f19903_0()
{    DynamicDestinations<String, Void, String> dynamicDestinations = DynamicFileDestinations.constant(DefaultFilenamePolicy.fromParams(new Params().withBaseFilename(getBaseOutputDirectory().resolve("file", StandardResolveOptions.RESOLVE_FILE)).withShardTemplate("-SS-of-NN")));    SimpleSink<Void> sink = new SimpleSink<Void>(getBaseOutputDirectory(), dynamicDestinations, Compression.UNCOMPRESSED) {        @Override        public void populateDisplayData(DisplayData.Builder builder) {            builder.add(DisplayData.item("foo", "bar"));        }    };    WriteFiles<String, ?, String> write = WriteFiles.to(sink);    DisplayData displayData = DisplayData.from(write);    assertThat(displayData, hasDisplayItem("sink", sink.getClass()));    assertThat(displayData, includesDisplayDataFor("sink", sink));}
public void beam_f19904_0(DisplayData.Builder builder)
{    builder.add(DisplayData.item("foo", "bar"));}
public void beam_f19905_0()
{    thrown.expect(IllegalArgumentException.class);    thrown.expectMessage("Must use windowed writes when applying WriteFiles to an unbounded PCollection");    SimpleSink<Void> sink = makeSimpleSink();    p.apply(Create.of("foo")).setIsBoundedInternal(IsBounded.UNBOUNDED).apply(WriteFiles.to(sink));    p.run();}
public void beam_f19913_0() throws Exception
{    testDynamicDestinationsHelper(true, true);}
private void beam_f19914_0(boolean bounded, boolean emptyShards) throws IOException
{    TestDestinations dynamicDestinations = new TestDestinations(getBaseOutputDirectory());    SimpleSink<Integer> sink = new SimpleSink<>(getBaseOutputDirectory(), dynamicDestinations, Compression.UNCOMPRESSED);        WriteOptions options = TestPipeline.testingPipelineOptions().as(WriteOptions.class);    options.setTestFlag("test_value");    Pipeline p = TestPipeline.create(options);    final int numInputs = 100;    List<String> inputs = Lists.newArrayList();    for (int i = 0; i < numInputs; ++i) {        inputs.add(Integer.toString(i));    }        List<Long> timestamps = new ArrayList<>();    for (long i = 0; i < inputs.size(); i++) {        timestamps.add(i + 1);    }            int numShards = emptyShards ? 2 * numInputs / 5 : 2;    WriteFiles<String, Integer, String> writeFiles = WriteFiles.to(sink).withNumShards(numShards);    PCollection<String> input = p.apply(Create.timestamped(inputs, timestamps));    WriteFilesResult<Integer> res;    if (!bounded) {        input.setIsBoundedInternal(IsBounded.UNBOUNDED);        input = input.apply(Window.into(FixedWindows.of(Duration.standardDays(1))));        res = input.apply(writeFiles.withWindowedWrites());    } else {        res = input.apply(writeFiles);    }    res.getPerDestinationOutputFilenames().apply(new VerifyFilesExist<>());    p.run();    for (int i = 0; i < 5; ++i) {        ResourceId base = getBaseOutputDirectory().resolve("file_" + i, StandardResolveOptions.RESOLVE_FILE);        List<String> expected = Lists.newArrayList();        for (int j = i; j < numInputs; j += 5) {            expected.add("record_" + j);        }        checkFileContents(base.toString(), expected, Optional.of(numShards), bounded);    }}
public void beam_f19915_0()
{    DynamicDestinations<String, Void, String> dynamicDestinations = DynamicFileDestinations.constant(DefaultFilenamePolicy.fromParams(new Params().withBaseFilename(getBaseOutputDirectory().resolve("file", StandardResolveOptions.RESOLVE_FILE)).withShardTemplate("-SS-of-NN")));    SimpleSink<Void> sink = new SimpleSink<Void>(getBaseOutputDirectory(), dynamicDestinations, Compression.UNCOMPRESSED) {        @Override        public void populateDisplayData(DisplayData.Builder builder) {            builder.add(DisplayData.item("foo", "bar"));        }    };    WriteFiles<String, ?, String> write = WriteFiles.to(sink).withNumShards(1);    DisplayData displayData = DisplayData.from(write);    assertThat(displayData, hasDisplayItem("sink", sink.getClass()));    assertThat(displayData, includesDisplayDataFor("sink", sink));    assertThat(displayData, hasDisplayItem("numShards", 1));}
public ResourceId beam_f19923_0(int shardNumber, int numShards, BoundedWindow window, PaneInfo paneInfo, OutputFileHints outputFileHints)
{    DecimalFormat df = new DecimalFormat("0000");    IntervalWindow intervalWindow = (IntervalWindow) window;    String filename = String.format("%s-%s-of-%s%s%s", filenamePrefixForWindow(intervalWindow), df.format(shardNumber), df.format(numShards), outputFileHints.getSuggestedFilenameSuffix(), suffix);    return baseFilename.getCurrentDirectory().resolve(filename, StandardResolveOptions.RESOLVE_FILE);}
public ResourceId beam_f19924_0(int shardNumber, int numShards, OutputFileHints outputFileHints)
{    DecimalFormat df = new DecimalFormat("0000");    String prefix = baseFilename.isDirectory() ? "" : firstNonNull(baseFilename.getFilename(), "");    String filename = String.format("%s-%s-of-%s%s%s", prefix, df.format(shardNumber), df.format(numShards), outputFileHints.getSuggestedFilenameSuffix(), suffix);    return baseFilename.getCurrentDirectory().resolve(filename, StandardResolveOptions.RESOLVE_FILE);}
private void beam_f19925_0(List<String> inputs, PTransform<PCollection<String>, PCollection<String>> transform, String baseName, WriteFiles<String, ?, String> write) throws IOException
{    runShardedWrite(inputs, transform, baseName, write, null);}
public void beam_f19933_0()
{            assertTrue(MetricFiltering.matches(MetricsFilter.builder().addNameFilter(MetricNameFilter.named(MetricFilteringTest.class, "myMetricName")).addStep("myStep").build(), MetricKey.create("myBigStep/myStep", MetricName.named(MetricFilteringTest.class, "myMetricName"))));        assertFalse(MetricFiltering.matches(MetricsFilter.builder().addNameFilter(MetricNameFilter.named(MetricFilteringTest.class, "myMetricName")).addStep("myOtherStep").build(), MetricKey.create("myOtherStepNoMatch/myStep", MetricName.named(MetricFilteringTest.class, "myMetricName"))));}
public void beam_f19934_0()
{            assertTrue(MetricFiltering.matches(MetricsFilter.builder().addNameFilter(MetricNameFilter.named(MetricFilteringTest.class, "myMetricName")).addStep("myStep").build(), MetricKey.create("myStep", MetricName.named(MetricFilteringTest.class, "myMetricName"))));        assertFalse(MetricFiltering.matches(MetricsFilter.builder().addNameFilter(MetricNameFilter.named(MetricFilteringTest.class, "myMetricName")).addStep("myOtherStep").build(), MetricKey.create("myStep", MetricName.named(MetricFilteringTest.class, "myMetricName"))));}
public void beam_f19935_0()
{            assertTrue(MetricFiltering.matches(MetricsFilter.builder().addNameFilter(MetricNameFilter.named(MetricFilteringTest.class, "myMetricName")).build(), MetricKey.create("anyStep", MetricName.named(MetricFilteringTest.class, "myMetricName"))));        assertFalse(MetricFiltering.matches(MetricsFilter.builder().addNameFilter(MetricNameFilter.named(MetricFilteringTest.class, "myMetricName")).build(), MetricKey.create("anyStep", MetricName.named(MetricFiltering.class, "myMetricName"))));}
protected boolean beam_f19943_0(MetricResult<T> item)
{    final T metricValue = isCommitted ? item.getCommitted() : item.getAttempted();    return super.matchesSafely(item) && valueMatcher.matches(metricValue);}
public void beam_f19944_0(Description description)
{    super.describeTo(description);    description.appendText(String.format(", %s=", metricState));    valueMatcher.describeTo(description);    description.appendText("}");}
protected void beam_f19945_0(MetricResult<T> item, Description mismatchDescription)
{    final T metricValue = isCommitted ? item.getCommitted() : item.getAttempted();    super.describeMismatchSafely(item, mismatchDescription);    mismatchDescription.appendText(String.format("%s: ", metricState));    valueMatcher.describeMismatch(metricValue, mismatchDescription);    mismatchDescription.appendText("}");}
public static Matcher<MetricResult<DistributionResult>> beam_f19953_0(final String namespace, final String name, final String step, final Long min, final Long max, final boolean isCommitted)
{    final String metricState = isCommitted ? "committed" : "attempted";    return new MatchNameAndKey<DistributionResult>(namespace, name, step) {        @Override        protected boolean matchesSafely(MetricResult<DistributionResult> item) {            final DistributionResult metricValue = isCommitted ? item.getCommitted() : item.getAttempted();            return super.matchesSafely(item) && Objects.equals(min, metricValue.getMin()) && Objects.equals(max, metricValue.getMax());        }        @Override        public void describeTo(Description description) {            super.describeTo(description);            description.appendText(String.format(", %sMin=", metricState)).appendValue(min).appendText(String.format(", %sMax=", metricState)).appendValue(max).appendText("}");        }        @Override        protected void describeMismatchSafely(MetricResult<DistributionResult> item, Description mismatchDescription) {            final DistributionResult metricValue = isCommitted ? item.getCommitted() : item.getAttempted();            super.describeMismatchSafely(item, mismatchDescription);            if (!Objects.equals(min, metricValue.getMin())) {                mismatchDescription.appendText(String.format("%sMin: ", metricState)).appendValue(min).appendText(" != ").appendValue(metricValue.getMin());            }            if (!Objects.equals(max, metricValue.getMax())) {                mismatchDescription.appendText(String.format("%sMax: ", metricState)).appendValue(max).appendText(" != ").appendValue(metricValue.getMax());            }            mismatchDescription.appendText("}");        }    };}
protected boolean beam_f19954_0(MetricResult<DistributionResult> item)
{    final DistributionResult metricValue = isCommitted ? item.getCommitted() : item.getAttempted();    return super.matchesSafely(item) && Objects.equals(min, metricValue.getMin()) && Objects.equals(max, metricValue.getMax());}
public void beam_f19955_0(Description description)
{    super.describeTo(description);    description.appendText(String.format(", %sMin=", metricState)).appendValue(min).appendText(String.format(", %sMax=", metricState)).appendValue(max).appendText("}");}
private static MetricQueryResults beam_f19963_0(PipelineResult result)
{    return result.metrics().queryMetrics(MetricsFilter.builder().addNameFilter(MetricNameFilter.inNamespace(MetricsTest.class)).build());}
public void beam_f19964_0()
{    MetricsEnvironment.setCurrentContainer(null);}
protected PipelineResult beam_f19965_0()
{    final Counter count = Metrics.counter(MetricsTest.class, "count");    final TupleTag<Integer> output1 = new TupleTag<Integer>() {    };    final TupleTag<Integer> output2 = new TupleTag<Integer>() {    };    pipeline.apply(Create.of(5, 8, 13)).apply("MyStep1", ParDo.of(new DoFn<Integer, Integer>() {        Distribution bundleDist = Metrics.distribution(MetricsTest.class, "bundle");        @StartBundle        public void startBundle() {            bundleDist.update(10L);        }        @SuppressWarnings("unused")        @ProcessElement        public void processElement(ProcessContext c) {            Distribution values = Metrics.distribution(MetricsTest.class, "input");            count.inc();            values.update(c.element());            c.output(c.element());            c.output(c.element());        }        @DoFn.FinishBundle        public void finishBundle() {            bundleDist.update(40L);        }    })).apply("MyStep2", ParDo.of(new DoFn<Integer, Integer>() {        @SuppressWarnings("unused")        @ProcessElement        public void processElement(ProcessContext c) {            Distribution values = Metrics.distribution(MetricsTest.class, "input");            Gauge gauge = Metrics.gauge(MetricsTest.class, "my-gauge");            Integer element = c.element();            count.inc();            values.update(element);            gauge.set(12L);            c.output(element);            c.output(output2, element);        }    }).withOutputTags(output1, TupleTagList.of(output2)));    PipelineResult result = pipeline.run();    result.waitUntilFinish();    return result;}
public void beam_f19973_0()
{    thrown.expect(IllegalArgumentException.class);    Metrics.counter("", NAME);}
public void beam_f19974_0()
{    thrown.expect(IllegalArgumentException.class);    Metrics.distribution(NS, "");}
public void beam_f19975_0()
{    thrown.expect(IllegalArgumentException.class);    Metrics.distribution("", NAME);}
public void beam_f19983_0()
{    long numElements = 1000;        pipeline.apply(GenerateSequence.from(0).to(numElements).withMaxReadTime(Duration.standardDays(1)));    PipelineResult pipelineResult = pipeline.run();    MetricQueryResults metrics = pipelineResult.metrics().queryMetrics(MetricsFilter.builder().addNameFilter(MetricNameFilter.named(ELEMENTS_READ.getNamespace(), ELEMENTS_READ.getName())).build());    assertThat(metrics.getCounters(), hasItem(attemptedMetricsResult(ELEMENTS_READ.getNamespace(), ELEMENTS_READ.getName(), "Read(UnboundedCountingSource)", 1000L)));}
public void beam_f19984_0()
{    PipelineResult result = runPipelineWithMetrics();    MetricQueryResults metrics = queryTestMetrics(result);        assertAllMetrics(metrics, false);}
public void beam_f19985_0()
{    PipelineResult result = runPipelineWithMetrics();    MetricQueryResults metrics = queryTestMetrics(result);    assertCounterMetrics(metrics, false);}
public void beam_f19993_0()
{    assertTrue(PipelineOptionsFactory.getRegisteredOptions().contains(RegisteredTestOptions.class));}
public void beam_f19994_0()
{    assertEquals(REGISTERED_RUNNER, PipelineOptionsFactory.getRegisteredRunners().get(REGISTERED_RUNNER.getSimpleName().toLowerCase()));}
public void beam_f19995_0()
{        assertEquals("RegisteredTest", REGISTERED_RUNNER.getSimpleName().substring(0, REGISTERED_RUNNER.getSimpleName().length() - "Runner".length()));    Map<String, Class<? extends PipelineRunner<?>>> registered = PipelineOptionsFactory.CACHE.get().getSupportedPipelineRunners();    assertEquals(REGISTERED_RUNNER, registered.get(REGISTERED_RUNNER.getSimpleName().toLowerCase().substring(0, REGISTERED_RUNNER.getSimpleName().length() - "Runner".length())));}
public void beam_f20003_0() throws Exception
{    expectedException.expect(IllegalArgumentException.class);    expectedException.expectMessage("Expected setter for property [object] of type [java.lang.Object] on " + "[org.apache.beam.sdk.options.PipelineOptionsFactoryTest$MissingSetter].");    PipelineOptionsFactory.as(MissingSetter.class);}
public void beam_f20004_0()
{    expectedException.expect(IllegalArgumentException.class);    expectedException.expectMessage("missing property methods on [org.apache.beam.sdk.options." + "PipelineOptionsFactoryTest$MissingMultipleSetters]");    expectedException.expectMessage("setter for property [object] of type [java.lang.Object]");    expectedException.expectMessage("setter for property [otherObject] of type [java.lang.Object]");    PipelineOptionsFactory.as(MissingMultipleSetters.class);}
public void beam_f20005_0()
{    expectedException.expect(IllegalArgumentException.class);    expectedException.expectMessage("missing property methods on [org.apache.beam.sdk.options." + "PipelineOptionsFactoryTest$MissingGettersAndSetters]");    expectedException.expectMessage("getter for property [object] of type [java.lang.Object]");    expectedException.expectMessage("setter for property [otherObject] of type [java.lang.Object]");    PipelineOptionsFactory.as(MissingGettersAndSetters.class);}
public void beam_f20013_0() throws Exception
{    expectedException.expect(IllegalArgumentException.class);    expectedException.expectMessage("Found setters marked with @JsonIgnore:");    expectedException.expectMessage("property [other] should not be marked with @JsonIgnore on [" + "org.apache.beam.sdk.options.PipelineOptionsFactoryTest$MultiSetterWithJsonIgnore]");    expectedException.expectMessage("property [value] should not be marked with @JsonIgnore on [" + "org.apache.beam.sdk.options.PipelineOptionsFactoryTest$SetterWithJsonIgnore]");    PipelineOptionsFactory.as(MultiSetterWithJsonIgnore.class);}
public void beam_f20014_0() throws Exception
{        GetterWithJsonIgnore options = PipelineOptionsFactory.as(GetterWithJsonIgnore.class);    expectedException.expect(IllegalArgumentException.class);    expectedException.expectMessage("Expected getter for property [object] to be marked with @JsonIgnore on all [" + "org.apache.beam.sdk.options.PipelineOptionsFactoryTest$GetterWithJsonIgnore, " + "org.apache.beam.sdk.options.PipelineOptionsFactoryTest$MissingSetter], " + "found only on [org.apache.beam.sdk.options." + "PipelineOptionsFactoryTest$GetterWithJsonIgnore]");        options.as(CombinedObject.class);}
public void beam_f20015_0()
{        MultiGetters options = PipelineOptionsFactory.as(MultiGetters.class);    expectedException.expect(IllegalArgumentException.class);    expectedException.expectMessage("Property getters are inconsistently marked with @JsonIgnore:");    expectedException.expectMessage("property [object] to be marked on all");    expectedException.expectMessage("found only on [org.apache.beam.sdk.options." + "PipelineOptionsFactoryTest$MultiGetters]");    expectedException.expectMessage("property [other] to be marked on all");    expectedException.expectMessage("found only on [org.apache.beam.sdk.options." + "PipelineOptionsFactoryTest$MultipleGettersWithInconsistentJsonIgnore]");    expectedException.expectMessage(Matchers.anyOf(containsString(java.util.Arrays.toString(new String[] { "org.apache.beam.sdk.options." + "PipelineOptionsFactoryTest$MultipleGettersWithInconsistentJsonIgnore", "org.apache.beam.sdk.options.PipelineOptionsFactoryTest$MultiGetters" })), containsString(java.util.Arrays.toString(new String[] { "org.apache.beam.sdk.options.PipelineOptionsFactoryTest$MultiGetters", "org.apache.beam.sdk.options." + "PipelineOptionsFactoryTest$MultipleGettersWithInconsistentJsonIgnore" }))));    expectedException.expectMessage(not(containsString("property [consistent]")));        options.as(MultipleGettersWithInconsistentJsonIgnore.class);}
public void beam_f20023_0() throws Exception
{    expectedException.expect(IllegalArgumentException.class);    expectedException.expectMessage("Property [object] is marked with contradictory annotations. Found [" + "[Default.String(value=abc) on org.apache.beam.sdk.options.PipelineOptionsFactoryTest" + "$GettersWithMultipleDefault#getObject()], " + "[Default.Integer(value=0) on org.apache.beam.sdk.options.PipelineOptionsFactoryTest" + "$GettersWithMultipleDefault#getObject()]].");        PipelineOptionsFactory.as(GettersWithMultipleDefault.class);}
public void beam_f20024_0()
{        MultiGettersWithDefault options = PipelineOptionsFactory.as(MultiGettersWithDefault.class);    expectedException.expect(IllegalArgumentException.class);    expectedException.expectMessage("Property getters are inconsistently marked with @Default:");    expectedException.expectMessage("property [object] to be marked on all");    expectedException.expectMessage("found only on [org.apache.beam.sdk.options." + "PipelineOptionsFactoryTest$MultiGettersWithDefault]");    expectedException.expectMessage("property [other] to be marked on all");    expectedException.expectMessage("found only on [org.apache.beam.sdk.options." + "PipelineOptionsFactoryTest$MultipleGettersWithInconsistentDefault]");    expectedException.expectMessage(Matchers.anyOf(containsString(java.util.Arrays.toString(new String[] { "org.apache.beam.sdk.options." + "PipelineOptionsFactoryTest$MultipleGettersWithInconsistentDefault", "org.apache.beam.sdk.options.PipelineOptionsFactoryTest$MultiGettersWithDefault" })), containsString(java.util.Arrays.toString(new String[] { "org.apache.beam.sdk.options.PipelineOptionsFactoryTest$MultiGettersWithDefault", "org.apache.beam.sdk.options." + "PipelineOptionsFactoryTest$MultipleGettersWithInconsistentDefault" }))));    expectedException.expectMessage(not(containsString("property [consistent]")));        options.as(MultipleGettersWithInconsistentDefault.class);}
public void beam_f20025_0()
{    ApplicationNameOptions options = PipelineOptionsFactory.fromArgs("--appName=testAppName").as(ApplicationNameOptions.class);    assertEquals("testAppName", options.getAppName());}
public void beam_f20033_0()
{    String[] args = new String[] { "--enumValue=" + TestEnum.Value };    String[] emptyArgs = new String[] { "--enumValue=" };    Objects options = PipelineOptionsFactory.fromArgs(args).as(Objects.class);    assertEquals(TestEnum.Value, options.getEnumValue().get());    expectedException.expect(IllegalArgumentException.class);    expectedException.expectMessage(emptyStringErrorMessage());    PipelineOptionsFactory.fromArgs(emptyArgs).as(Objects.class);}
public void beam_f20034_0()
{    String[] args = new String[] { "--map={\"key\":\"value\",\"key2\":\"value2\"}", "--object={\"key\":\"value\",\"key2\":\"value2\"}", "--objectValue={\"key\":\"value\",\"key2\":\"value2\"}" };    ComplexTypes options = PipelineOptionsFactory.fromArgs(args).as(ComplexTypes.class);    assertEquals(ImmutableMap.of("key", "value", "key2", "value2"), options.getMap());    assertEquals("value", options.getObject().value);    assertEquals("value2", options.getObject().value2);    assertEquals("value", options.getObjectValue().get().value);    assertEquals("value2", options.getObjectValue().get().value2);}
public void beam_f20035_0()
{    String[] args = new String[] {};    Objects options = PipelineOptionsFactory.fromArgs(args).as(Objects.class);    assertNull(options.getString());}
public void beam_f20043_0()
{    String[] args = new String[] { "--enumValue=" + TestEnum.Value, "--enumValue=" + TestEnum.Value2 };    String[] commaArgs = new String[] { "--enumValue=" + TestEnum.Value + "," + TestEnum.Value2 };    String[] emptyArgs = new String[] { "--enumValue=" };    Arrays options = PipelineOptionsFactory.fromArgs(args).as(Arrays.class);    assertArrayEquals(new TestEnum[] { TestEnum.Value, TestEnum.Value2 }, options.getEnumValue().get());    options = PipelineOptionsFactory.fromArgs(commaArgs).as(Arrays.class);    assertArrayEquals(new TestEnum[] { TestEnum.Value, TestEnum.Value2 }, options.getEnumValue().get());    expectedException.expect(IllegalArgumentException.class);    expectedException.expectMessage(emptyStringErrorMessage());    PipelineOptionsFactory.fromArgs(emptyArgs).as(Arrays.class);}
public void beam_f20044_0()
{    String[] args = new String[] { "--char=d", "--boolean=true", "--boolean=true", "--char=e", "--char=f", "--boolean=false" };    Arrays options = PipelineOptionsFactory.fromArgs(args).as(Arrays.class);    boolean[] bools = options.getBoolean();    assertTrue(bools[0] && bools[1] && !bools[2]);    assertArrayEquals(new char[] { 'd', 'e', 'f' }, options.getChar());}
public void beam_f20045_0()
{    String[] manyArgs = new String[] { "--list=stringValue1", "--list=stringValue2", "--list=stringValue3" };    String[] manyArgsWithEmptyString = new String[] { "--list=stringValue1", "--list=", "--list=stringValue3" };    Lists options = PipelineOptionsFactory.fromArgs(manyArgs).as(Lists.class);    assertEquals(ImmutableList.of("stringValue1", "stringValue2", "stringValue3"), options.getList());    options = PipelineOptionsFactory.fromArgs(manyArgsWithEmptyString).as(Lists.class);    assertEquals(ImmutableList.of("stringValue1", "", "stringValue3"), options.getList());}
public void beam_f20053_0()
{    String[] args = new String[] { "--string=100", "--string=200" };    expectedException.expect(IllegalArgumentException.class);    expectedException.expectMessage("expected one element but was");    PipelineOptionsFactory.fromArgs(args).as(Objects.class);}
public void beam_f20054_0()
{    String[] args = new String[] { "--diskSizeGb=100", "--diskSizeGb=200" };    PipelineOptionsFactory.fromArgs(args).withoutStrictParsing().create();    expectedLogs.verifyWarn("Strict parsing is disabled, ignoring option");}
public void beam_f20055_0() throws Exception
{    expectedException.expect(IllegalArgumentException.class);    expectedException.expectMessage("Please mark non-public interface " + NonPublicPipelineOptions.class.getName() + " as public.");    PipelineOptionsFactory.as(NonPublicPipelineOptions.class);}
public void beam_f20063_0()
{    String[] args = new String[] { "--runner=java.lang.String" };    expectedException.expect(IllegalArgumentException.class);    expectedException.expectMessage("does not implement PipelineRunner");    expectedException.expectMessage("java.lang.String");    PipelineOptionsFactory.fromArgs(args).create();}
public void beam_f20064_0()
{    String[] args = new String[] { "--unknownProperty=value" };    expectedException.expect(IllegalArgumentException.class);    expectedException.expectMessage("missing a property named 'unknownProperty'");    PipelineOptionsFactory.fromArgs(args).create();}
public void beam_f20065_0()
{    String[] args = new String[] { "--ab=value" };    expectedException.expect(IllegalArgumentException.class);    expectedException.expectMessage("missing a property named 'ab'. Did you mean 'abc'?");    PipelineOptionsFactory.fromArgs(args).as(SuggestedOptions.class);}
public void beam_f20073_0()
{    String[] args = new String[] { "--=100" };    PipelineOptionsFactory.fromArgs(args).withoutStrictParsing().create();    expectedLogs.verifyWarn("Strict parsing is disabled, ignoring option");}
public void beam_f20074_0()
{    ByteArrayOutputStream baos = new ByteArrayOutputStream();    ListMultimap<String, String> arguments = ArrayListMultimap.create();    assertFalse(PipelineOptionsFactory.printHelpUsageAndExitIfNeeded(arguments, new PrintStream(baos), false));    String output = new String(baos.toByteArray(), Charsets.UTF_8);    assertEquals("", output);}
public void beam_f20075_0()
{    ByteArrayOutputStream baos = new ByteArrayOutputStream();    ListMultimap<String, String> arguments = ArrayListMultimap.create();    arguments.put("help", "true");    assertTrue(PipelineOptionsFactory.printHelpUsageAndExitIfNeeded(arguments, new PrintStream(baos), false));    String output = new String(baos.toByteArray(), Charsets.UTF_8);    assertThat(output, containsString("The set of registered options are:"));    assertThat(output, containsString("org.apache.beam.sdk.options.PipelineOptions"));    assertThat(output, containsString("Use --help=<OptionsName> for detailed help."));}
public void beam_f20083_0()
{    ByteArrayOutputStream baos = new ByteArrayOutputStream();    PipelineOptionsFactory.printHelp(new PrintStream(baos));    String output = new String(baos.toByteArray(), Charsets.UTF_8);    assertThat(output, containsString("The set of registered options are:"));    assertThat(output, containsString("org.apache.beam.sdk.options.PipelineOptions"));}
public void beam_f20084_0()
{    ByteArrayOutputStream baos = new ByteArrayOutputStream();    PipelineOptionsFactory.printHelp(new PrintStream(baos), PipelineOptions.class);    String output = new String(baos.toByteArray(), Charsets.UTF_8);    assertThat(output, containsString("org.apache.beam.sdk.options.PipelineOptions"));    assertThat(output, containsString("--runner"));    assertThat(output, containsString("Default: " + DEFAULT_RUNNER_NAME));    assertThat(output, containsString("The pipeline runner that will be used to execute the pipeline."));}
public void beam_f20085_0()
{    expectedException.expect(IllegalArgumentException.class);    expectedException.expectMessage("All inherited interfaces of [org.apache.beam.sdk.options.PipelineOptionsFactoryTest" + "$PipelineOptionsInheritedInvalid] should inherit from the PipelineOptions interface. " + "The following inherited interfaces do not:\n" + " - org.apache.beam.sdk.options.PipelineOptionsFactoryTest" + "$InvalidPipelineOptions1\n" + " - org.apache.beam.sdk.options.PipelineOptionsFactoryTest" + "$InvalidPipelineOptions2");    PipelineOptionsFactory.as(PipelineOptionsInheritedInvalid.class);}
public JacksonIncompatible beam_f20093_0(JsonParser jsonParser, DeserializationContext deserializationContext) throws IOException, JsonProcessingException
{    return new JacksonIncompatible(jsonParser.readValueAs(String.class));}
public void beam_f20094_0(JacksonIncompatible jacksonIncompatible, JsonGenerator jsonGenerator, SerializerProvider serializerProvider) throws IOException, JsonProcessingException
{    jsonGenerator.writeString(jacksonIncompatible.value);}
public void beam_f20095_0() throws Exception
{    final Thread thread = Thread.currentThread();    final ClassLoader testClassLoader = thread.getContextClassLoader();    final ClassLoader caseLoader = new InterceptingUrlClassLoader(testClassLoader, name -> name.toLowerCase(ROOT).contains("test"));    thread.setContextClassLoader(caseLoader);    PipelineOptionsFactory.resetCache();    try {        final PipelineOptions pipelineOptions = PipelineOptionsFactory.create();        final Class optionType = caseLoader.loadClass("org.apache.beam.sdk.options.PipelineOptionsFactoryTest$ClassLoaderTestOptions");        final Object options = pipelineOptions.as(optionType);        assertSame(caseLoader, options.getClass().getClassLoader());        assertSame(optionType.getClassLoader(), options.getClass().getClassLoader());        assertSame(testClassLoader, optionType.getInterfaces()[0].getClassLoader());        assertTrue(Boolean.class.cast(optionType.getMethod("isOption").invoke(options)));    } finally {        thread.setContextClassLoader(testClassLoader);        PipelineOptionsFactory.resetCache();    }}
public void beam_f20103_0()
{    Set<PipelineOptionSpec> properties = PipelineOptionsReflector.getOptionSpecs(OnlyTwoValidGetters.class, true);    assertThat(properties, not(hasItem(hasName(isOneOf("misspelled", "hasParameter", "prefix")))));}
public void beam_f20104_0()
{    Set<PipelineOptionSpec> props = PipelineOptionsReflector.getOptionSpecs(ExtendsSimpleOptions.class, true);    assertThat(props, hasItem(allOf(hasName("foo"), hasClass(SimpleOptions.class))));    assertThat(props, hasItem(allOf(hasName("foo"), hasClass(ExtendsSimpleOptions.class))));    assertThat(props, hasItem(allOf(hasName("bar"), hasClass(ExtendsSimpleOptions.class))));}
public void beam_f20105_0()
{    Set<PipelineOptionSpec> properties = PipelineOptionsReflector.getOptionSpecs(ExtendsNonPipelineOptions.class, true);    assertThat(properties, not(hasItem(hasName("foo"))));}
private static Matcher<PipelineOptionSpec> beam_f20113_0(Class<?> clazz)
{    return new FeatureMatcher<PipelineOptionSpec, Class<?>>(Matchers.<Class<?>>is(clazz), "defining class", "class") {        @Override        protected Class<?> featureValueOf(PipelineOptionSpec actual) {            return actual.getDefiningInterface();        }    };}
protected Class<?> beam_f20114_0(PipelineOptionSpec actual)
{    return actual.getDefiningInterface();}
private static Matcher<PipelineOptionSpec> beam_f20115_0(String methodName)
{    return new FeatureMatcher<PipelineOptionSpec, String>(is(methodName), "getter method", "name") {        @Override        protected String featureValueOf(PipelineOptionSpec actual) {            return actual.getGetterMethod().getName();        }    };}
public void beam_f20123_0()
{    Required required = PipelineOptionsFactory.as(Required.class);    required.setRunner(CrashingRunner.class);    required.setObject("blah");    PipelineOptionsValidator.validate(Required.class, required);}
public void beam_f20124_0()
{    expectedException.expect(IllegalArgumentException.class);    expectedException.expectMessage("Missing required value for " + "[public abstract java.lang.String org.apache.beam." + "sdk.options.PipelineOptionsValidatorTest$Required.getObject(), \"Fake Description\"].");    Required required = PipelineOptionsFactory.as(Required.class);    required.setObject("blah");    required.setObject(null);    PipelineOptionsValidator.validate(Required.class, required);}
public void beam_f20125_0()
{    expectedException.expect(IllegalArgumentException.class);    expectedException.expectMessage("Missing required value for " + "[--object, \"Fake Description\"].");    Required required = PipelineOptionsFactory.fromArgs(new String[] { "--object=blah" }).as(Required.class);    required.setObject(null);    PipelineOptionsValidator.validateCli(Required.class, required);}
public void beam_f20133_0()
{    GroupRequired groupRequired = PipelineOptionsFactory.as(GroupRequired.class);    groupRequired.setRunner(CrashingRunner.class);    expectedException.expect(IllegalArgumentException.class);    expectedException.expectMessage("Missing required value for group [ham]");    expectedException.expectMessage("properties");    expectedException.expectMessage("getFoo");    expectedException.expectMessage("getBar");    PipelineOptionsValidator.validate(GroupRequired.class, groupRequired);}
public void beam_f20134_0()
{    MultiGroupRequired multiGroupRequired = PipelineOptionsFactory.as(MultiGroupRequired.class);    multiGroupRequired.setRunner(CrashingRunner.class);    multiGroupRequired.setFoo("eggs");    PipelineOptionsValidator.validate(MultiGroupRequired.class, multiGroupRequired);}
public void beam_f20135_0()
{    RightOptions rightOptions = PipelineOptionsFactory.as(RightOptions.class);    rightOptions.setBoth("foo");    rightOptions.setRunner(CrashingRunner.class);    expectedException.expect(IllegalArgumentException.class);    expectedException.expectMessage("Missing required value for group");    expectedException.expectMessage("getFoo");    PipelineOptionsValidator.validate(JoinedOptions.class, rightOptions);}
public void beam_f20143_0() throws Exception
{    ProxyInvocationHandler handler = new ProxyInvocationHandler(Maps.newHashMap());    Simple proxy = handler.as(Simple.class);    proxy.setString("OBJECT");    proxy.setOptionEnabled(true);    proxy.setPrimitive(4);    assertEquals("OBJECT", proxy.getString());    assertTrue(proxy.isOptionEnabled());    assertEquals(4, proxy.getPrimitive());}
public void beam_f20144_0() throws Exception
{    ProxyInvocationHandler handler = new ProxyInvocationHandler(Maps.newHashMap());    JLSDefaults proxy = handler.as(JLSDefaults.class);    assertFalse(proxy.getBoolean());    assertEquals('\0', proxy.getChar());    assertEquals((byte) 0, proxy.getByte());    assertEquals((short) 0, proxy.getShort());    assertEquals(0, proxy.getInt());    assertEquals(0L, proxy.getLong());    assertEquals(0f, proxy.getFloat(), 0f);    assertEquals(0d, proxy.getDouble(), 0d);    assertNull(proxy.getObject());}
public String beam_f20145_0(PipelineOptions options)
{    return "testOptionFactory";}
public void beam_f20153_0() throws Exception
{    ProxyInvocationHandler handler = new ProxyInvocationHandler(Maps.newHashMap());    SubClass extended = handler.as(SubClass.class);    extended.setString("parentValue");    assertEquals("parentValue", extended.getString());}
public void beam_f20154_0() throws Exception
{    ProxyInvocationHandler handler = new ProxyInvocationHandler(Maps.newHashMap());    SubClass extended = handler.as(SubClass.class);    extended.setString("parentValue");    Simple simple = extended.as(Simple.class);    assertEquals("parentValue", simple.getString());}
public void beam_f20155_0() throws Exception
{    ProxyInvocationHandler handler = new ProxyInvocationHandler(Maps.newHashMap());    SubClass extended = handler.as(SubClass.class);    extended.setExtended("subClassValue");    SubClass extended2 = extended.as(Simple.class).as(SubClass.class);    assertEquals("subClassValue", extended2.getExtended());}
public void beam_f20163_0() throws Exception
{    PipelineOptions options = PipelineOptionsFactory.create();    assertNotNull(serializeDeserialize(PipelineOptions.class, options));}
public void beam_f20164_0() throws Exception
{    SimpleTypes options = PipelineOptionsFactory.as(SimpleTypes.class);    options.setString("TestValue");    options.setInteger(5);    SimpleTypes options2 = serializeDeserialize(SimpleTypes.class, options);    assertEquals(5, options2.getInteger());    assertEquals("TestValue", options2.getString());}
public void beam_f20165_0() throws Exception
{    SimpleTypes options = PipelineOptionsFactory.as(SimpleTypes.class);    options.setString("TestValue");    options.setInteger(5);                    SimpleTypes options2 = serializeDeserialize(SimpleTypes.class, serializeDeserialize(PipelineOptions.class, options));    assertEquals(5, options2.getInteger());    assertEquals("TestValue", options2.getString());}
public boolean beam_f20173_0(Object obj)
{    return obj != null && getClass().equals(obj.getClass()) && Objects.equals(stringField, ((ComplexType) obj).stringField) && Objects.equals(intField, ((ComplexType) obj).intField) && Objects.equals(genericType, ((ComplexType) obj).genericType) && Objects.equals(innerType, ((ComplexType) obj).innerType);}
public void beam_f20174_0() throws Exception
{    ComplexType complexType = new ComplexType();    complexType.stringField = "stringField";    complexType.intField = 12;    complexType.innerType = InnerType.of(12);    complexType.genericType = ImmutableList.of(InnerType.of(16234), InnerType.of(24));    ComplexTypes options = PipelineOptionsFactory.as(ComplexTypes.class);    options.setComplexType(complexType);    ComplexTypes options2 = serializeDeserialize(ComplexTypes.class, options);    assertEquals(complexType, options2.getComplexType());}
public void beam_f20175_0() throws Exception
{    IgnoredProperty options = PipelineOptionsFactory.as(IgnoredProperty.class);    options.setValue("TestValue");    IgnoredProperty options2 = serializeDeserialize(IgnoredProperty.class, options);    assertNull(options2.getValue());}
public String beam_f20183_0()
{    return "foobar";}
public void beam_f20184_0()
{    Object brokenValueType = new Object() {        @JsonValue        public int getValue() {            return 42;        }        @Override        public String toString() {            throw new RuntimeException("oh noes!!");        }    };    p.getOptions().as(ObjectPipelineOptions.class).setValue(brokenValueType);    p.apply(Create.of(1, 2, 3));    expectedException.expectMessage(ProxyInvocationHandler.PipelineOptionsDisplayData.class.getName());    expectedException.expectMessage("oh noes!!");    p.run();}
public int beam_f20185_0()
{    return 42;}
public void beam_f20193_0() throws Exception
{    FooOptions options = PipelineOptionsFactory.as(FooOptions.class);    options.setFoo(null);    DisplayData data = DisplayData.from(options);    assertThat(data, hasDisplayItem("foo", ""));    FooOptions deserializedOptions = serializeDeserialize(FooOptions.class, options);    DisplayData deserializedData = DisplayData.from(deserializedOptions);    assertThat(deserializedData, hasDisplayItem("foo", ""));}
public void beam_f20194_0() throws Exception
{    ArrayOptions options = PipelineOptionsFactory.as(ArrayOptions.class);    options.setDeepArray(new String[][] { new String[] { "a", "b" }, new String[] { "c" } });    options.setDeepPrimitiveArray(new int[][] { new int[] { 1, 2 }, new int[] { 3 } });    DisplayData data = DisplayData.from(options);    assertThat(data, hasDisplayItem("deepArray", "[[a, b], [c]]"));    assertThat(data, hasDisplayItem("deepPrimitiveArray", "[[1, 2], [3]]"));    ArrayOptions deserializedOptions = serializeDeserialize(ArrayOptions.class, options);    DisplayData deserializedData = DisplayData.from(deserializedOptions);    assertThat(deserializedData, hasDisplayItem("deepPrimitiveArray", "[[1, 2], [3]]"));}
public void beam_f20195_0() throws IOException
{    FooOptions options = PipelineOptionsFactory.as(FooOptions.class);    options.setFoo("bar");    @SuppressWarnings("unchecked")    Map<String, Object> map = MAPPER.readValue(MAPPER.writeValueAsBytes(options), Map.class);    assertThat("main pipeline options data keyed as 'options'", map, Matchers.hasKey("options"));    assertThat("display data keyed as 'display_data'", map, Matchers.hasKey("display_data"));    Map<?, ?> expectedDisplayItem = ImmutableMap.<String, String>builder().put("namespace", FooOptions.class.getName()).put("key", "foo").put("value", "bar").put("type", "STRING").build();    @SuppressWarnings("unchecked")    List<Map<?, ?>> deserializedDisplayData = (List<Map<?, ?>>) map.get("display_data");    assertThat(deserializedDisplayData, hasItem(expectedDisplayItem));}
public void beam_f20203_0() throws NoSuchMethodException
{    ProxyInvocationHandler handler = new ProxyInvocationHandler(Maps.newHashMap());    handler.as(BaseOptions.class);    assertEquals("foo", handler.getOptionName(BaseOptions.class.getMethod("getFoo")));}
public void beam_f20204_0()
{    RemoteEnvironmentOptions options = PipelineOptionsFactory.as(RemoteEnvironmentOptions.class);    assertEquals(null, options.getSemiPersistDir());    String semiDir = "/ab/cd";    options.setSemiPersistDir(semiDir);    assertEquals(semiDir, options.getSemiPersistDir());}
public void beam_f20205_0()
{    expectedException.expect(IllegalArgumentException.class);    expectedException.expectMessage("Unsupported log level");    SdkHarnessLogLevelOverrides.from(ImmutableMap.of("Name", "FakeLevel"));}
public void beam_f20213_0()
{    TestOptions options = PipelineOptionsFactory.fromArgs("--foo=baz").as(TestOptions.class);    ValueProvider<String> provider = options.getFoo();    assertEquals("baz", provider.get());    assertTrue(provider.isAccessible());}
public void beam_f20214_0()
{    TestOptions options = PipelineOptionsFactory.fromArgs("--list=1,2,3").as(TestOptions.class);    ValueProvider<List<Integer>> provider = options.getList();    assertEquals(ImmutableList.of(1, 2, 3), provider.get());    assertTrue(provider.isAccessible());}
public void beam_f20215_0()
{    TestOptions options = PipelineOptionsFactory.fromArgs("--bar=baz").as(TestOptions.class);    ValueProvider<String> provider = options.getBar();    assertEquals("baz", provider.get());    assertTrue(provider.isAccessible());}
public void beam_f20223_0()
{    BadOptionsRuntime options = PipelineOptionsFactory.as(BadOptionsRuntime.class);    expectedException.expect(RuntimeException.class);    expectedException.expectMessage("Method getBar should not have return type " + "RuntimeValueProvider, use ValueProvider instead.");    options.getBar();}
public void beam_f20224_0()
{    BadOptionsStatic options = PipelineOptionsFactory.as(BadOptionsStatic.class);    expectedException.expect(RuntimeException.class);    expectedException.expectMessage("Method getBar should not have return type " + "StaticValueProvider, use ValueProvider instead.");    options.getBar();}
public void beam_f20225_0() throws Exception
{    TestOptions submitOptions = PipelineOptionsFactory.as(TestOptions.class);    assertFalse(submitOptions.getFoo().isAccessible());    ObjectNode root = MAPPER.valueToTree(submitOptions);    ((ObjectNode) root.get("options")).put("foo", "quux");    TestOptions runtime = MAPPER.convertValue(root, PipelineOptions.class).as(TestOptions.class);    ValueProvider<String> vp = runtime.getFoo();    assertTrue(vp.isAccessible());    assertEquals("quux", vp.get());    assertEquals(vp.getClass(), StaticValueProvider.class);}
public static TestPipelineRunnerThrowingUserException beam_f20233_0(PipelineOptions options)
{    return new TestPipelineRunnerThrowingUserException();}
public PipelineResult beam_f20234_0(Pipeline pipeline)
{    Throwable t = new IllegalStateException("user code exception");    throw UserCodeException.wrap(t);}
public static TestPipelineRunnerThrowingSdkException beam_f20235_0(PipelineOptions options)
{    return new TestPipelineRunnerThrowingSdkException();}
public void beam_f20244_0()
{    PTransform<PCollection<? extends String>, PCollection<String>> myTransform = addSuffix("+");    PCollection<String> input = pipeline.apply(Create.of(ImmutableList.of("a", "b")));    PCollection<String> left = input.apply("Left1", myTransform).apply("Left2", myTransform);    PCollection<String> right = input.apply("Right", myTransform);    PCollection<String> both = PCollectionList.of(left).and(right).apply(Flatten.pCollections());    PAssert.that(both).containsInAnyOrder("a++", "b++", "a+", "b+");    pipeline.run();}
private static PTransform<PCollection<? extends String>, PCollection<String>> beam_f20245_0(final String suffix)
{    return MapElements.via(new SimpleFunction<String, String>() {        @Override        public String apply(String input) {            return input + suffix;        }    });}
public String beam_f20246_0(String input)
{    return input + suffix;}
public PCollection<T> beam_f20254_0(PCollectionTuple input)
{    return input.get(tag);}
public void beam_f20255_0() throws Exception
{    PCollection<Integer> input = pipeline.apply(Create.of(1, 2, 3, 4));    TupleTag<Integer> tag = new TupleTag<>();    PCollectionTuple output = input.apply("ProjectTag", new TupleInjectionTransform<>(tag));    PAssert.that(output.get(tag)).containsInAnyOrder(1, 2, 3, 4);    pipeline.run();}
public PCollectionTuple beam_f20256_0(PCollection<T> input)
{    return PCollectionTuple.of(tag, input);}
public PTransformReplacement<PCollection<Integer>, PCollection<Integer>> beam_f20264_0(AppliedPTransform<PCollection<Integer>, PCollection<Integer>, OriginalTransform> transform)
{    return PTransformReplacement.of(originalInput, new ReplacementTransform());}
public Map<PValue, ReplacementOutput> beam_f20265_0(Map<TupleTag<?>, PValue> outputs, PCollection<Integer> newOutput)
{    return Collections.singletonMap(newOutput, ReplacementOutput.of(TaggedPValue.ofExpandedValue(Iterables.getOnlyElement(outputs.values())), TaggedPValue.ofExpandedValue(newOutput)));}
public boolean beam_f20266_0(AppliedPTransform<?, ?, ?> application)
{    return application.getTransform() instanceof OriginalTransform;}
public KV<String, String> beam_f20274_0(String value)
{    String key = UUID.randomUUID().toString();    return KV.of(key, value);}
public void beam_f20275_0(ProcessContext c) throws Exception
{    MatchResult matchResult = FileSystems.match(outputPrefix + "*");    boolean firstTime = matchResult.metadata().isEmpty();    KV<String, String> kv = c.element();    writeTextToFileSideEffect(kv.getValue(), outputPrefix + kv.getKey());    if (firstTime) {        firstTimeCallback.apply(null);    }}
public static void beam_f20276_0(String text, String filename) throws IOException
{    ResourceId rid = FileSystems.matchNewResource(filename, false);    WritableByteChannel chan = FileSystems.create(rid, "text/plain");    chan.write(ByteBuffer.wrap(text.getBytes(StandardCharsets.UTF_8)));    chan.close();}
public void beam_f20284_0()
{    assertThat(hierarchy.getCurrent().isRootNode(), is(true));}
public void beam_f20285_0()
{    thrown.expect(IllegalStateException.class);    hierarchy.popNode();}
public void beam_f20286_0()
{    TransformHierarchy.Node root = hierarchy.getCurrent();    TransformHierarchy.Node node = hierarchy.pushNode("Create", PBegin.in(pipeline), Create.of(1));    assertThat(hierarchy.getCurrent(), equalTo(node));    hierarchy.popNode();    assertThat(node.finishedSpecifying, is(true));    assertThat(hierarchy.getCurrent(), equalTo(root));}
public POutput beam_f20294_0(PInput input)
{    return PDone.in(input.getPipeline());}
public void beam_f20295_0()
{    final SingleOutput<Long, Long> originalParDo = ParDo.of(new DoFn<Long, Long>() {        @ProcessElement        public void processElement(ProcessContext ctxt) {            ctxt.output(ctxt.element() + 1L);        }    });    GenerateSequence genUpstream = GenerateSequence.from(0);    PCollection<Long> upstream = pipeline.apply(genUpstream);    PCollection<Long> output = upstream.apply("Original", originalParDo);    hierarchy.pushNode("Upstream", pipeline.begin(), genUpstream);    hierarchy.finishSpecifyingInput();    hierarchy.setOutput(upstream);    hierarchy.popNode();    TransformHierarchy.Node original = hierarchy.pushNode("Original", upstream, originalParDo);    hierarchy.finishSpecifyingInput();    hierarchy.setOutput(output);    hierarchy.popNode();    final TupleTag<Long> longs = new TupleTag<>();    final MultiOutput<Long, Long> replacementParDo = ParDo.of(new DoFn<Long, Long>() {        @ProcessElement        public void processElement(ProcessContext ctxt) {            ctxt.output(ctxt.element() + 1L);        }    }).withOutputTags(longs, TupleTagList.empty());    PTransform<PCollection<Long>, PCollection<Long>> replacementComposite = new PTransform<PCollection<Long>, PCollection<Long>>() {        @Override        public PCollection<Long> expand(PCollection<Long> input) {            return input.apply("Contained", replacementParDo).get(longs);        }    };    PCollectionTuple replacementOutput = upstream.apply("Contained", replacementParDo);    Node compositeNode = hierarchy.replaceNode(original, upstream, replacementComposite);    Node replacementParNode = hierarchy.pushNode("Original/Contained", upstream, replacementParDo);    hierarchy.finishSpecifyingInput();    hierarchy.setOutput(replacementOutput);    hierarchy.popNode();    hierarchy.setOutput(replacementOutput.get(longs));    Entry<TupleTag<?>, PValue> replacementLongs = Iterables.getOnlyElement(replacementOutput.expand().entrySet());    hierarchy.replaceOutputs(Collections.singletonMap(replacementOutput.get(longs), ReplacementOutput.of(TaggedPValue.ofExpandedValue(output), TaggedPValue.of(replacementLongs.getKey(), replacementLongs.getValue()))));    assertThat(replacementParNode.getOutputs().keySet(), Matchers.contains(replacementLongs.getKey()));    assertThat(replacementParNode.getOutputs().values(), Matchers.contains(output));    assertThat(compositeNode.getOutputs().keySet(), equalTo(replacementOutput.get(longs).expand().keySet()));    assertThat(compositeNode.getOutputs().values(), Matchers.contains(output));    hierarchy.popNode();}
public void beam_f20296_0(ProcessContext ctxt)
{    ctxt.output(ctxt.element() + 1L);}
public void beam_f20304_0()
{    Node root = hierarchy.getCurrent();    final SingleOutput<Long, Long> originalParDo = ParDo.of(new DoFn<Long, Long>() {        @ProcessElement        public void processElement(ProcessContext ctxt) {            ctxt.output(ctxt.element() + 1L);        }    });    GenerateSequence genUpstream = GenerateSequence.from(0);    PCollection<Long> upstream = pipeline.apply(genUpstream);    PCollection<Long> output = upstream.apply("Original", originalParDo);    Node upstreamNode = hierarchy.pushNode("Upstream", pipeline.begin(), genUpstream);    hierarchy.finishSpecifyingInput();    hierarchy.setOutput(upstream);    hierarchy.popNode();    Node original = hierarchy.pushNode("Original", upstream, originalParDo);    hierarchy.finishSpecifyingInput();    hierarchy.setOutput(output);    hierarchy.popNode();    final TupleTag<Long> longs = new TupleTag<>();    final MultiOutput<Long, Long> replacementParDo = ParDo.of(new DoFn<Long, Long>() {        @ProcessElement        public void processElement(ProcessContext ctxt) {            ctxt.output(ctxt.element() + 1L);        }    }).withOutputTags(longs, TupleTagList.empty());    PTransform<PCollection<Long>, PCollection<Long>> replacementComposite = new PTransform<PCollection<Long>, PCollection<Long>>() {        @Override        public PCollection<Long> expand(PCollection<Long> input) {            return input.apply("Contained", replacementParDo).get(longs);        }    };    PCollectionTuple replacementOutput = upstream.apply("Contained", replacementParDo);    Node compositeNode = hierarchy.replaceNode(original, upstream, replacementComposite);    Node replacementParNode = hierarchy.pushNode("Original/Contained", upstream, replacementParDo);    hierarchy.finishSpecifyingInput();    hierarchy.setOutput(replacementOutput);    hierarchy.popNode();    hierarchy.setOutput(replacementOutput.get(longs));    Entry<TupleTag<?>, PValue> replacementLongs = Iterables.getOnlyElement(replacementOutput.expand().entrySet());    hierarchy.replaceOutputs(Collections.singletonMap(replacementOutput.get(longs), ReplacementOutput.of(TaggedPValue.ofExpandedValue(output), TaggedPValue.of(replacementLongs.getKey(), replacementLongs.getValue()))));    hierarchy.popNode();    final Set<Node> visitedCompositeNodes = new HashSet<>();    final Set<Node> visitedPrimitiveNodes = new HashSet<>();    Set<PValue> visitedValues = hierarchy.visit(new Defaults() {        @Override        public CompositeBehavior enterCompositeTransform(Node node) {            visitedCompositeNodes.add(node);            return CompositeBehavior.ENTER_TRANSFORM;        }        @Override        public void visitPrimitiveTransform(Node node) {            visitedPrimitiveNodes.add(node);        }    });    /*    Final Graph:    Upstream -> Upstream.out -> Composite -> (ReplacementParDo -> OriginalParDo.out)    */    assertThat(visitedCompositeNodes, containsInAnyOrder(root, compositeNode));    assertThat(visitedPrimitiveNodes, containsInAnyOrder(upstreamNode, replacementParNode));    assertThat(visitedValues, containsInAnyOrder(upstream, output));}
public void beam_f20305_0(ProcessContext ctxt)
{    ctxt.output(ctxt.element() + 1L);}
public void beam_f20306_0(ProcessContext ctxt)
{    ctxt.output(ctxt.element() + 1L);}
public PCollectionTuple beam_f20314_0(PBegin input)
{    return input.apply(producer);}
public CompositeBehavior beam_f20315_0(Node node)
{    for (PValue input : node.getInputs().values()) {        assertThat(visitedValues, hasItem(input));    }    assertThat("Nodes should not be visited more than once", visitedNodes, not(hasItem(node)));    if (!node.isRootNode()) {        assertThat("Nodes should always be visited after their enclosing nodes", visitedNodes, hasItem(node.getEnclosingNode()));    }    visitedNodes.add(node);    return CompositeBehavior.ENTER_TRANSFORM;}
public void beam_f20316_0(Node node)
{    assertThat(visitedNodes, hasItem(node));    if (!node.isRootNode()) {        assertThat("Nodes should always be left before their enclosing nodes are left", exitedNodes, not(hasItem(node.getEnclosingNode())));    }    assertThat(exitedNodes, not(hasItem(node)));    exitedNodes.add(node);}
public CompositeBehavior beam_f20324_0(Node node)
{    visitedNodes.add(node);    return node.equals(enclosing) ? CompositeBehavior.DO_NOT_ENTER_TRANSFORM : CompositeBehavior.ENTER_TRANSFORM;}
public void beam_f20325_0(Node node)
{    visitedNodes.add(node);}
public PCollectionList<String> beam_f20326_0(PBegin b)
{            PCollection<String> result = b.apply(Create.of("hello", "world"));        return PCollectionList.of(Arrays.asList(result, PCollection.createPrimitiveOutputInternal(b.getPipeline(), WindowingStrategy.globalDefault(), result.isBounded(), StringUtf8Coder.of())));}
private Row beam_f20334_0(String name)
{    return Row.withSchema(SIMPLE_SCHEMA).addValues(name, (byte) 1, (short) 2, 3, 4L, true, DATE, DATE, BYTE_ARRAY, BYTE_ARRAY, BigDecimal.ONE, new StringBuilder(name).append("builder").toString()).build();}
private void beam_f20335_0(Row row)
{    assertEquals("string", row.getString("str"));    assertEquals((byte) 1, (Object) row.getByte("aByte"));    assertEquals((short) 2, (Object) row.getInt16("aShort"));    assertEquals((int) 3, (Object) row.getInt32("anInt"));    assertEquals((long) 4, (Object) row.getInt64("aLong"));    assertTrue(row.getBoolean("aBoolean"));    assertEquals(DATE.toInstant(), row.getDateTime("dateTime"));    assertEquals(DATE.toInstant(), row.getDateTime("instant"));    assertArrayEquals(BYTE_ARRAY, row.getBytes("bytes"));    assertArrayEquals(BYTE_ARRAY, row.getBytes("byteBuffer"));    assertEquals(BigDecimal.ONE, row.getDecimal("bigDecimal"));    assertEquals("stringbuilder", row.getString("stringBuilder"));}
private void beam_f20336_0(SimpleSchema value)
{    assertEquals("string", value.getStr());    assertEquals((byte) 1, value.getaByte());    assertEquals((short) 2, value.getaShort());    assertEquals((int) 3, value.getAnInt());    assertEquals((long) 4, value.getaLong());    assertTrue(value.isaBoolean());    assertEquals(DATE, value.getDateTime());    assertEquals(DATE.toInstant(), value.getInstant());    assertArrayEquals("not equal", BYTE_ARRAY, value.getBytes());    assertArrayEquals("not equal", BYTE_ARRAY, value.getByteBuffer().array());    assertEquals(BigDecimal.ONE, value.getBigDecimal());    assertEquals("stringbuilder", value.getStringBuilder().toString());}
public void beam_f20344_0() throws NoSuchSchemaException
{    SchemaRegistry registry = SchemaRegistry.createDefault();    Row inner = createSimpleRow("string");    Row outer = Row.withSchema(OUTER_SCHEMA).addValue(inner).build();    AutoValueOuter value = registry.getFromRowFunction(AutoValueOuter.class).apply(outer);    verifyAutoValue(value.getInner());}
public void beam_f20345_0() throws NoSuchSchemaException
{    SchemaRegistry registry = SchemaRegistry.createDefault();    Row inner = createSimpleRow("string");    Row outer = Row.withSchema(OUTER_SCHEMA).addValue(inner).build();    AutoValueOuterWithBuilder value = registry.getFromRowFunction(AutoValueOuterWithBuilder.class).apply(outer);    verifyAutoValue(value.getInner());}
 static SimpleAutoValueWithStaticFactory beam_f20346_0(String str, byte aByte, short aShort, int anInt, long aLong, boolean aBoolean, DateTime dateTime, byte[] bytes, ByteBuffer byteBuffer, Instant instant, BigDecimal bigDecimal, StringBuilder stringBuilder)
{    return new AutoValue_AutoValueSchemaTest_SimpleAutoValueWithStaticFactory(str, aByte, aShort, anInt, aLong, aBoolean, dateTime, bytes, byteBuffer, instant, bigDecimal, stringBuilder);}
public void beam_f20354_0()
{    assertEquals(SCHEMA, new AvroRecordSchema().schemaFor(TypeDescriptor.of(TestAvro.class)));}
public void beam_f20355_0()
{    assertEquals(POJO_SCHEMA, new AvroRecordSchema().schemaFor(TypeDescriptor.of(AvroPojo.class)));}
public void beam_f20356_0()
{    SerializableFunction<TestAvro, Row> toRow = new AvroRecordSchema().toRowFunction(TypeDescriptor.of(TestAvro.class));    assertEquals(ROW, toRow.apply(AVRO_SPECIFIC_RECORD));}
public void beam_f20364_0()
{    FieldAccessDescriptor fieldAccessDescriptor = FieldAccessDescriptor.withAllFields();    assertTrue(fieldAccessDescriptor.resolve(SIMPLE_SCHEMA).getAllFields());}
public void beam_f20365_0()
{    FieldAccessDescriptor fieldAccessDescriptor = FieldAccessDescriptor.withFieldNames("field0", "field2").resolve(SIMPLE_SCHEMA);    assertEquals(ImmutableList.of(0, 2), fieldAccessDescriptor.fieldIdsAccessed());}
public void beam_f20366_0()
{    FieldAccessDescriptor fieldAccessDescriptor = FieldAccessDescriptor.withFieldIds(1, 3).resolve(SIMPLE_SCHEMA);    assertEquals(ImmutableList.of(1, 3), fieldAccessDescriptor.fieldIdsAccessed());}
public void beam_f20374_0()
{    FieldAccessDescriptor fieldAccessDescriptor = FieldAccessDescriptor.withFieldNames("field1.field0", "field1.field1").resolve(NESTED_SCHEMA1);    assertEquals(0, fieldAccessDescriptor.getFieldsAccessed().size());    assertEquals(1, fieldAccessDescriptor.nestedFieldsById().size());    FieldAccessDescriptor nestedAccess = fieldAccessDescriptor.nestedFieldsById().get(1);    assertEquals(ImmutableSet.of(0, 1), nestedAccess.getFieldsAccessed().stream().map(FieldDescriptor::getFieldId).collect(Collectors.toSet()));}
public void beam_f20375_0()
{    FieldAccessDescriptor fieldAccessDescriptor = FieldAccessDescriptor.withFieldNames("field0", "field1.field0", "field1").resolve(NESTED_SCHEMA2);    assertEquals(2, fieldAccessDescriptor.getFieldsAccessed().size());    assertEquals(ImmutableSet.of(0, 1), fieldAccessDescriptor.getFieldsAccessed().stream().map(FieldDescriptor::getFieldId).collect(Collectors.toSet()));    assertTrue(fieldAccessDescriptor.nestedFieldsById().isEmpty());}
public void beam_f20376_0()
{    FieldAccessDescriptor fieldAccessDescriptor = FieldAccessDescriptor.withFieldNames("field0", "field1.field0", "field1.*").resolve(NESTED_SCHEMA2);    assertEquals(1, fieldAccessDescriptor.getFieldsAccessed().size());    assertEquals("field0", fieldAccessDescriptor.getFieldsAccessed().iterator().next().getFieldName());    assertEquals(1, fieldAccessDescriptor.nestedFieldsById().size());    FieldAccessDescriptor nestedAccess = fieldAccessDescriptor.nestedFieldsById().get(1);    assertTrue(nestedAccess.getAllFields());}
public void beam_f20384_0()
{    FieldAccessDescriptor fieldAccessDescriptor = FieldAccessDescriptor.withFieldNames("field3", "field2", "field1", "field0").withOrderByFieldInsertionOrder().resolve(SIMPLE_SCHEMA);    assertEquals(ImmutableList.of(3, 2, 1, 0), fieldAccessDescriptor.fieldIdsAccessed());}
public void beam_f20385_0()
{    assertEquals(TypeDescriptors.bytes(), FieldTypeDescriptors.javaTypeForFieldType(FieldType.BYTE));    assertEquals(TypeDescriptors.shorts(), FieldTypeDescriptors.javaTypeForFieldType(FieldType.INT16));    assertEquals(TypeDescriptors.integers(), FieldTypeDescriptors.javaTypeForFieldType(FieldType.INT32));    assertEquals(TypeDescriptors.longs(), FieldTypeDescriptors.javaTypeForFieldType(FieldType.INT64));    assertEquals(TypeDescriptors.bigdecimals(), FieldTypeDescriptors.javaTypeForFieldType(FieldType.DECIMAL));    assertEquals(TypeDescriptors.floats(), FieldTypeDescriptors.javaTypeForFieldType(FieldType.FLOAT));    assertEquals(TypeDescriptors.doubles(), FieldTypeDescriptors.javaTypeForFieldType(FieldType.DOUBLE));    assertEquals(TypeDescriptors.strings(), FieldTypeDescriptors.javaTypeForFieldType(FieldType.STRING));    assertEquals(TypeDescriptor.of(Instant.class), FieldTypeDescriptors.javaTypeForFieldType(FieldType.DATETIME));    assertEquals(TypeDescriptors.booleans(), FieldTypeDescriptors.javaTypeForFieldType(FieldType.BOOLEAN));    assertEquals(TypeDescriptor.of(byte[].class), FieldTypeDescriptors.javaTypeForFieldType(FieldType.BYTES));}
public void beam_f20386_0()
{    assertEquals(TypeDescriptors.lists(TypeDescriptors.rows()), FieldTypeDescriptors.javaTypeForFieldType(FieldType.array(FieldType.row(Schema.builder().build()))));}
private SimpleBeanWithAnnotations beam_f20394_0(String name)
{    return new SimpleBeanWithAnnotations(name, (byte) 1, (short) 2, 3, 4L, true, DATE, DATE.toInstant(), BYTE_ARRAY, BigDecimal.ONE, new StringBuilder(name).append("builder"));}
private Row beam_f20395_0(String name)
{    return Row.withSchema(SIMPLE_BEAN_SCHEMA).addValues(name, (byte) 1, (short) 2, 3, 4L, true, DATE, DATE, BYTE_ARRAY, BYTE_ARRAY, BigDecimal.ONE, new StringBuilder(name).append("builder").toString()).build();}
public void beam_f20396_0() throws NoSuchSchemaException
{    SchemaRegistry registry = SchemaRegistry.createDefault();    Schema schema = registry.getSchema(SimpleBean.class);    SchemaTestUtils.assertSchemaEquivalent(SIMPLE_BEAN_SCHEMA, schema);}
public void beam_f20404_0() throws NoSuchSchemaException
{    SchemaRegistry registry = SchemaRegistry.createDefault();    SchemaTestUtils.assertSchemaEquivalent(NESTED_ARRAY_BEAN_SCHEMA, registry.getSchema(NestedArrayBean.class));    SimpleBean simple1 = createSimple("string1");    SimpleBean simple2 = createSimple("string2");    SimpleBean simple3 = createSimple("string3");    NestedArrayBean bean = new NestedArrayBean(simple1, simple2, simple3);    Row row = registry.getToRowFunction(NestedArrayBean.class).apply(bean);    List<Row> rows = row.getArray("beans");    assertSame(simple1, registry.getFromRowFunction(SimpleBean.class).apply(rows.get(0)));    assertSame(simple2, registry.getFromRowFunction(SimpleBean.class).apply(rows.get(1)));    assertSame(simple3, registry.getFromRowFunction(SimpleBean.class).apply(rows.get(2)));}
public void beam_f20405_0() throws NoSuchSchemaException
{    SchemaRegistry registry = SchemaRegistry.createDefault();    Row row1 = createSimpleRow("string1");    Row row2 = createSimpleRow("string2");    Row row3 = createSimpleRow("string3");    Row row = Row.withSchema(NESTED_ARRAY_BEAN_SCHEMA).addArray(row1, row2, row3).build();    NestedArrayBean bean = registry.getFromRowFunction(NestedArrayBean.class).apply(row);    assertEquals(3, bean.getBeans().length);    assertEquals("string1", bean.getBeans()[0].getStr());    assertEquals("string2", bean.getBeans()[1].getStr());    assertEquals("string3", bean.getBeans()[2].getStr());}
public void beam_f20406_0() throws NoSuchSchemaException
{    SchemaRegistry registry = SchemaRegistry.createDefault();    SchemaTestUtils.assertSchemaEquivalent(NESTED_ARRAYS_BEAM_SCHEMA, registry.getSchema(NestedArraysBean.class));    List<List<String>> listOfLists = Lists.newArrayList(Lists.newArrayList("a", "b", "c"), Lists.newArrayList("d", "e", "f"), Lists.newArrayList("g", "h", "i"));    NestedArraysBean bean = new NestedArraysBean(listOfLists);    Row row = registry.getToRowFunction(NestedArraysBean.class).apply(bean);    assertEquals(listOfLists, row.getArray("lists"));}
private StaticCreationSimplePojo beam_f20414_0(String name)
{    return StaticCreationSimplePojo.of(name, 4L, (byte) 1, (short) 2, 3, true, DATE, BYTE_BUFFER, INSTANT, BYTE_ARRAY, BigDecimal.ONE, new StringBuilder(name).append("builder"));}
private Row beam_f20415_0(String name)
{    return Row.withSchema(SIMPLE_POJO_SCHEMA).addValues(name, (byte) 1, (short) 2, 3, 4L, true, DATE, INSTANT, BYTE_ARRAY, BYTE_BUFFER.array(), BigDecimal.ONE, new StringBuilder(name).append("builder").toString()).build();}
public void beam_f20416_0() throws NoSuchSchemaException
{    SchemaRegistry registry = SchemaRegistry.createDefault();    Schema schema = registry.getSchema(SimplePOJO.class);    SchemaTestUtils.assertSchemaEquivalent(SIMPLE_POJO_SCHEMA, schema);}
public void beam_f20424_0() throws NoSuchSchemaException
{    SchemaRegistry registry = SchemaRegistry.createDefault();    SchemaTestUtils.assertSchemaEquivalent(NESTED_ARRAY_POJO_SCHEMA, registry.getSchema(NestedArrayPOJO.class));    SimplePOJO simple1 = createSimple("string1");    SimplePOJO simple2 = createSimple("string2");    SimplePOJO simple3 = createSimple("string3");    NestedArrayPOJO pojo = new NestedArrayPOJO(simple1, simple2, simple3);    Row row = registry.getToRowFunction(NestedArrayPOJO.class).apply(pojo);    List<Row> rows = row.getArray("pojos");    assertSame(simple1, registry.getFromRowFunction(SimplePOJO.class).apply(rows.get(0)));    assertSame(simple2, registry.getFromRowFunction(SimplePOJO.class).apply(rows.get(1)));    assertSame(simple3, registry.getFromRowFunction(SimplePOJO.class).apply(rows.get(2)));}
public void beam_f20425_0() throws NoSuchSchemaException
{    SchemaRegistry registry = SchemaRegistry.createDefault();    Row row1 = createSimpleRow("string1");    Row row2 = createSimpleRow("string2");    Row row3 = createSimpleRow("string3");    ;    Row row = Row.withSchema(NESTED_ARRAY_POJO_SCHEMA).addArray(row1, row2, row3).build();    NestedArrayPOJO pojo = registry.getFromRowFunction(NestedArrayPOJO.class).apply(row);    assertEquals(3, pojo.pojos.length);    assertEquals("string1", pojo.pojos[0].str);    assertEquals("string2", pojo.pojos[1].str);    assertEquals("string3", pojo.pojos[2].str);}
public void beam_f20426_0() throws NoSuchSchemaException
{    SchemaRegistry registry = SchemaRegistry.createDefault();    SchemaTestUtils.assertSchemaEquivalent(NESTED_ARRAYS_POJO_SCHEMA, registry.getSchema(NestedArraysPOJO.class));    List<List<String>> listOfLists = Lists.newArrayList(Lists.newArrayList("a", "b", "c"), Lists.newArrayList("d", "e", "f"), Lists.newArrayList("g", "h", "i"));    NestedArraysPOJO pojo = new NestedArraysPOJO(listOfLists);    Row row = registry.getToRowFunction(NestedArraysPOJO.class).apply(pojo);    assertEquals(listOfLists, row.getArray("lists"));}
public void beam_f20434_0() throws NoSuchSchemaException
{    SchemaRegistry registry = SchemaRegistry.createDefault();    Schema schema = registry.getSchema(AnnotatedSimplePojo.class);    SchemaTestUtils.assertSchemaEquivalent(SIMPLE_POJO_SCHEMA, schema);    Row simpleRow = createSimpleRow("string");    AnnotatedSimplePojo pojo = createAnnotated("string");    assertEquals(simpleRow, registry.getToRowFunction(AnnotatedSimplePojo.class).apply(pojo));    AnnotatedSimplePojo pojo2 = registry.getFromRowFunction(AnnotatedSimplePojo.class).apply(simpleRow);    assertEquals(pojo, pojo2);}
public void beam_f20435_0() throws NoSuchSchemaException
{    SchemaRegistry registry = SchemaRegistry.createDefault();    Schema schema = registry.getSchema(StaticCreationSimplePojo.class);    SchemaTestUtils.assertSchemaEquivalent(SIMPLE_POJO_SCHEMA, schema);    Row simpleRow = createSimpleRow("string");    StaticCreationSimplePojo pojo = createStaticCreation("string");    assertEquals(simpleRow, registry.getToRowFunction(StaticCreationSimplePojo.class).apply(pojo));    StaticCreationSimplePojo pojo2 = registry.getFromRowFunction(StaticCreationSimplePojo.class).apply(simpleRow);    assertEquals(pojo, pojo2);}
public void beam_f20436_0() throws NoSuchSchemaException
{    SchemaRegistry registry = SchemaRegistry.createDefault();    Schema schema = registry.getSchema(PojoWithNestedArray.class);    SchemaTestUtils.assertSchemaEquivalent(POJO_WITH_NESTED_ARRAY_SCHEMA, schema);    Row simpleRow = createSimpleRow("string");    List<Row> list = ImmutableList.of(simpleRow, simpleRow);    List<List<Row>> listOfList = ImmutableList.of(list, list);    Row nestedRow = Row.withSchema(POJO_WITH_NESTED_ARRAY_SCHEMA).addValue(listOfList).build();    SimplePOJO simplePojo = createSimple("string");    List<SimplePOJO> simplePojoList = ImmutableList.of(simplePojo, simplePojo);    List<List<SimplePOJO>> simplePojoListOfList = ImmutableList.of(simplePojoList, simplePojoList);    PojoWithNestedArray converted = registry.getFromRowFunction(PojoWithNestedArray.class).apply(nestedRow);    assertEquals(simplePojoListOfList, converted.pojos);}
public void beam_f20444_0() throws NoSuchSchemaException
{    SchemaRegistry registry = SchemaRegistry.createDefault();    registry.registerSchemaProvider(new Provider());    tryGetters(registry);}
public Schema beam_f20445_0(TypeDescriptor<T> typeDescriptor)
{    if (typeDescriptor.equals(TypeDescriptor.of(TestSchemaClass.class))) {        return EMPTY_SCHEMA;    }    return null;}
public SerializableFunction<T, Row> beam_f20446_0(TypeDescriptor<T> typeDescriptor)
{    if (typeDescriptor.equals(TypeDescriptor.of(TestSchemaClass.class))) {        return v -> Row.withSchema(EMPTY_SCHEMA).build();    }    return null;}
public void beam_f20454_0() throws NoSuchSchemaException
{    SchemaRegistry registry = SchemaRegistry.createDefault();    registry.registerPOJO(SimplePOJO.class);    Schema schema = registry.getSchema(SimplePOJO.class);    assertTrue(SIMPLE_POJO_SCHEMA.equivalent(schema));}
public void beam_f20455_0() throws NoSuchSchemaException
{    SchemaRegistry registry = SchemaRegistry.createDefault();    registry.registerJavaBean(SimpleBean.class);    Schema schema = registry.getSchema(SimpleBean.class);    assertTrue(SIMPLE_BEAN_SCHEMA.equivalent(schema));}
public void beam_f20456_0()
{    Schema schema = Schema.builder().addByteField("f_byte").addInt16Field("f_int16").addInt32Field("f_int32").addInt64Field("f_int64").addDecimalField("f_decimal").addFloatField("f_float").addDoubleField("f_double").addStringField("f_string").addDateTimeField("f_datetime").addBooleanField("f_boolean").build();    assertEquals(10, schema.getFieldCount());    assertEquals(0, schema.indexOf("f_byte"));    assertEquals("f_byte", schema.getField(0).getName());    assertEquals(FieldType.BYTE, schema.getField(0).getType());    assertEquals(1, schema.indexOf("f_int16"));    assertEquals("f_int16", schema.getField(1).getName());    assertEquals(FieldType.INT16, schema.getField(1).getType());    assertEquals(2, schema.indexOf("f_int32"));    assertEquals("f_int32", schema.getField(2).getName());    assertEquals(FieldType.INT32, schema.getField(2).getType());    assertEquals(3, schema.indexOf("f_int64"));    assertEquals("f_int64", schema.getField(3).getName());    assertEquals(FieldType.INT64, schema.getField(3).getType());    assertEquals(4, schema.indexOf("f_decimal"));    assertEquals("f_decimal", schema.getField(4).getName());    assertEquals(FieldType.DECIMAL, schema.getField(4).getType());    assertEquals(5, schema.indexOf("f_float"));    assertEquals("f_float", schema.getField(5).getName());    assertEquals(FieldType.FLOAT, schema.getField(5).getType());    assertEquals(6, schema.indexOf("f_double"));    assertEquals("f_double", schema.getField(6).getName());    assertEquals(FieldType.DOUBLE, schema.getField(6).getType());    assertEquals(7, schema.indexOf("f_string"));    assertEquals("f_string", schema.getField(7).getName());    assertEquals(FieldType.STRING, schema.getField(7).getType());    assertEquals(8, schema.indexOf("f_datetime"));    assertEquals("f_datetime", schema.getField(8).getName());    assertEquals(FieldType.DATETIME, schema.getField(8).getType());    assertEquals(9, schema.indexOf("f_boolean"));    assertEquals("f_boolean", schema.getField(9).getName());    assertEquals(FieldType.BOOLEAN, schema.getField(9).getType());}
public void beam_f20464_0()
{    final Schema expectedNested1 = Schema.builder().addStringField("yard1").addInt64Field("yard2").build();    final Schema expectedSchema1 = Schema.builder().addStringField("field1").addInt64Field("field2").addRowField("field3", expectedNested1).addArrayField("field4", FieldType.row(expectedNested1)).addMapField("field5", FieldType.STRING, FieldType.row(expectedNested1)).build();    final Schema expectedNested2 = Schema.builder().addInt64Field("yard2").addStringField("yard1").build();    final Schema expectedSchema2 = Schema.builder().addMapField("field5", FieldType.STRING, FieldType.row(expectedNested2)).addArrayField("field4", FieldType.row(expectedNested2)).addRowField("field3", expectedNested2).addInt64Field("field2").addStringField("field1").build();    assertNotEquals(expectedSchema1, expectedSchema2);    assertTrue(expectedSchema1.equivalent(expectedSchema2));}
public void beam_f20465_0()
{    Schema schema1 = Schema.builder().addInt64Field("foo").build();    Schema schema2 = Schema.builder().addStringField("foo").build();    assertNotEquals(schema1, schema2);    assertFalse(schema1.equivalent(schema2));    schema1 = Schema.builder().addInt64Field("foo").build();    schema2 = Schema.builder().addInt64Field("bar").build();    assertNotEquals(schema1, schema2);    assertFalse(schema1.equivalent(schema2));    schema1 = Schema.builder().addInt64Field("foo").build();    schema2 = Schema.builder().addNullableField("foo", FieldType.INT64).build();    assertNotEquals(schema1, schema2);    assertFalse(schema1.equivalent(schema2));}
public void beam_f20466_0()
{    Schema nestedSchema1 = Schema.builder().addInt64Field("foo").build();    Schema nestedSchema2 = Schema.builder().addStringField("foo").build();    Schema schema1 = Schema.builder().addRowField("foo", nestedSchema1).build();    Schema schema2 = Schema.builder().addRowField("foo", nestedSchema2).build();    assertNotEquals(schema1, schema2);    assertFalse(schema1.equivalent(schema2));}
public void beam_f20474_0()
{    Schema nested = Schema.builder().addStringField("field1").build();    Schema schema = Schema.builder().addRowField("nested", nested).build();    Row subRow = Row.withSchema(nested).addValue("value").build();    Row row = Row.withSchema(schema).addValue(subRow).build();    PCollection<Row> added = pipeline.apply(Create.of(row).withRowSchema(schema)).apply(AddFields.<Row>create().field("nested.field2", Schema.FieldType.INT32).field("nested.field3", Schema.FieldType.array(Schema.FieldType.STRING)));    Schema expectedNestedSchema = Schema.builder().addStringField("field1").addNullableField("field2", Schema.FieldType.INT32).addNullableField("field3", Schema.FieldType.array(Schema.FieldType.STRING)).build();    Schema expectedSchema = Schema.builder().addRowField("nested", expectedNestedSchema).build();    assertEquals(expectedSchema, added.getSchema());    Row expectedNested = Row.withSchema(expectedNestedSchema).addValues("value", null, null).build();    Row expected = Row.withSchema(expectedSchema).addValue(expectedNested).build();    PAssert.that(added).containsInAnyOrder(expected);    pipeline.run();}
public void beam_f20475_0()
{    Schema nested = Schema.builder().addStringField("field1").build();    Schema schema = Schema.builder().addRowField("nested", nested).build();    Row subRow = Row.withSchema(nested).addValue("value").build();    Row row = Row.withSchema(schema).addValue(subRow).build();    List<String> list = ImmutableList.of("one", "two", "three");    PCollection<Row> added = pipeline.apply(Create.of(row).withRowSchema(schema)).apply(AddFields.<Row>create().field("nested.field2", Schema.FieldType.INT32, 42).field("nested.field3", Schema.FieldType.array(Schema.FieldType.STRING), list));    Schema expectedNestedSchema = Schema.builder().addStringField("field1").addField("field2", Schema.FieldType.INT32).addField("field3", Schema.FieldType.array(Schema.FieldType.STRING)).build();    Schema expectedSchema = Schema.builder().addRowField("nested", expectedNestedSchema).build();    assertEquals(expectedSchema, added.getSchema());    Row expectedNested = Row.withSchema(expectedNestedSchema).addValues("value", 42, list).build();    Row expected = Row.withSchema(expectedSchema).addValue(expectedNested).build();    PAssert.that(added).containsInAnyOrder(expected);    pipeline.run();}
public void beam_f20476_0()
{    Schema nested = Schema.builder().addStringField("field1").build();    Schema schema = Schema.builder().addRowField("nested", nested).build();    Row subRow = Row.withSchema(nested).addValue("value").build();    Row row = Row.withSchema(schema).addValue(subRow).build();    PCollection<Row> added = pipeline.apply(Create.of(row).withRowSchema(schema)).apply(AddFields.<Row>create().field("field2", Schema.FieldType.INT32).field("nested.field2", Schema.FieldType.INT32).field("nested.field3", Schema.FieldType.array(Schema.FieldType.STRING)));    Schema expectedNestedSchema = Schema.builder().addStringField("field1").addNullableField("field2", Schema.FieldType.INT32).addNullableField("field3", Schema.FieldType.array(Schema.FieldType.STRING)).build();    Schema expectedSchema = Schema.builder().addRowField("nested", expectedNestedSchema).addNullableField("field2", Schema.FieldType.INT32).build();    assertEquals(expectedSchema, added.getSchema());    Row expectedNested = Row.withSchema(expectedNestedSchema).addValues("value", null, null).build();    Row expected = Row.withSchema(expectedSchema).addValues(expectedNested, null).build();    PAssert.that(added).containsInAnyOrder(expected);    pipeline.run();}
public void beam_f20484_0()
{    Schema inputSchema = Schema.of(Schema.Field.of("f0", Schema.FieldType.INT16), Schema.Field.of("f1", Schema.FieldType.INT32));    Schema outputSchema = Schema.of(Schema.Field.of("f0", Schema.FieldType.INT32), Schema.Field.of("f1", Schema.FieldType.INT64));    Row input = Row.withSchema(inputSchema).addValues((short) 1, 2).build();    Row expected = Row.withSchema(outputSchema).addValues(1, 2L).build();    PCollection<Row> output = pipeline.apply(Create.of(input).withRowSchema(inputSchema)).apply(Cast.widening(outputSchema));    PAssert.that(output).containsInAnyOrder(expected);    pipeline.run();}
public void beam_f20485_0()
{    Schema inputSchema = Schema.of(Schema.Field.of("f0", Schema.FieldType.INT16), Schema.Field.of("f1", Schema.FieldType.INT64));    Schema outputSchema = Schema.of(Schema.Field.of("f0", Schema.FieldType.INT32), Schema.Field.of("f1", Schema.FieldType.INT32));    expectedException.expect(IllegalArgumentException.class);    expectedException.expectMessage(containsString("f1: Can't cast 'INT64' to 'INT32'"));    Cast.widening(outputSchema).verifyCompatibility(inputSchema);}
public void beam_f20486_0()
{        Schema inputSchema = Schema.of(Schema.Field.of("f0", Schema.FieldType.INT32), Schema.Field.of("f1", Schema.FieldType.INT64));    Schema outputSchema = Schema.of(Schema.Field.of("f0", Schema.FieldType.INT16), Schema.Field.of("f1", Schema.FieldType.INT32));    Row input = Row.withSchema(inputSchema).addValues(1, 2L).build();    Row expected = Row.withSchema(outputSchema).addValues((short) 1, 2).build();    PCollection<Row> output = pipeline.apply(Create.of(input).withRowSchema(inputSchema)).apply(Cast.narrowing(outputSchema));    PAssert.that(output).containsInAnyOrder(expected);    pipeline.run();}
public void beam_f20494_0()
{    NUMERICS.keySet().forEach(input -> NUMERICS.keySet().forEach(output -> testWideningOrder(input, output)));}
public void beam_f20495_0()
{    NUMERICS.keySet().forEach(input -> NUMERICS.keySet().forEach(output -> testCasting(input, output)));}
private void beam_f20496_0(TypeName inputType, TypeName outputType)
{    Object output = Cast.castValue(NUMERICS.get(inputType), FieldType.of(inputType), FieldType.of(outputType));    assertEquals(NUMERICS.get(outputType), output);}
public void beam_f20504_0()
{    PCollection<Row> pc1 = pipeline.apply("Create1", Create.of(Row.withSchema(CG_SCHEMA_1).addValues("user1", 1, "us").build())).setRowSchema(CG_SCHEMA_1);    PCollection<Row> pc2 = pipeline.apply("Create2", Create.of(Row.withSchema(CG_SCHEMA_1).addValues("user1", 9, "us").build())).setRowSchema(CG_SCHEMA_1);    thrown.expect(IllegalStateException.class);    PCollection<KV<Row, Row>> joined = PCollectionTuple.of("pc1", pc1, "pc2", pc2).apply("CoGroup", CoGroup.join("pc1", By.fieldNames("user")).join("pc2", By.fieldNames("count")));    pipeline.run();}
public void beam_f20505_0()
{    List<Row> pc1Rows = Lists.newArrayList(Row.withSchema(CG_SCHEMA_1).addValues("user1", 1, "us").build(), Row.withSchema(CG_SCHEMA_1).addValues("user1", 2, "us").build(), Row.withSchema(CG_SCHEMA_1).addValues("user1", 3, "il").build(), Row.withSchema(CG_SCHEMA_1).addValues("user1", 4, "il").build(), Row.withSchema(CG_SCHEMA_1).addValues("user2", 5, "fr").build(), Row.withSchema(CG_SCHEMA_1).addValues("user2", 6, "fr").build(), Row.withSchema(CG_SCHEMA_1).addValues("user2", 7, "ar").build(), Row.withSchema(CG_SCHEMA_1).addValues("user2", 8, "ar").build());    List<Row> pc2Rows = Lists.newArrayList(Row.withSchema(CG_SCHEMA_2).addValues("user1", 9, "us").build(), Row.withSchema(CG_SCHEMA_2).addValues("user1", 10, "us").build(), Row.withSchema(CG_SCHEMA_2).addValues("user1", 11, "il").build(), Row.withSchema(CG_SCHEMA_2).addValues("user1", 12, "il").build(), Row.withSchema(CG_SCHEMA_2).addValues("user2", 13, "fr").build(), Row.withSchema(CG_SCHEMA_2).addValues("user2", 14, "fr").build(), Row.withSchema(CG_SCHEMA_2).addValues("user2", 15, "ar").build(), Row.withSchema(CG_SCHEMA_2).addValues("user2", 16, "ar").build());    List<Row> pc3Rows = Lists.newArrayList(Row.withSchema(CG_SCHEMA_3).addValues("user1", 17, "us").build(), Row.withSchema(CG_SCHEMA_3).addValues("user1", 18, "us").build(), Row.withSchema(CG_SCHEMA_3).addValues("user1", 19, "il").build(), Row.withSchema(CG_SCHEMA_3).addValues("user1", 20, "il").build(), Row.withSchema(CG_SCHEMA_3).addValues("user2", 21, "fr").build(), Row.withSchema(CG_SCHEMA_3).addValues("user2", 22, "fr").build(), Row.withSchema(CG_SCHEMA_3).addValues("user2", 23, "ar").build(), Row.withSchema(CG_SCHEMA_3).addValues("user2", 24, "ar").build());    PCollection<Row> pc1 = pipeline.apply("Create1", Create.of(pc1Rows)).setRowSchema(CG_SCHEMA_1);    PCollection<Row> pc2 = pipeline.apply("Create2", Create.of(pc2Rows)).setRowSchema(CG_SCHEMA_2);    PCollection<Row> pc3 = pipeline.apply("Create3", Create.of(pc3Rows)).setRowSchema(CG_SCHEMA_3);    Schema expectedSchema = Schema.builder().addRowField("pc1", CG_SCHEMA_1).addRowField("pc2", CG_SCHEMA_2).addRowField("pc3", CG_SCHEMA_3).build();    PCollection<Row> joined = PCollectionTuple.of("pc1", pc1, "pc2", pc2, "pc3", pc3).apply("CoGroup", CoGroup.join("pc1", By.fieldNames("user", "country")).join("pc2", By.fieldNames("user2", "country2")).join("pc3", By.fieldNames("user3", "country3")).crossProductJoin());    assertEquals(expectedSchema, joined.getSchema());    List<Row> expectedJoinedRows = JoinTestUtils.innerJoin(pc1Rows, pc2Rows, pc3Rows, new String[] { "user", "country" }, new String[] { "user2", "country2" }, new String[] { "user3", "country3" }, expectedSchema);    PAssert.that(joined).containsInAnyOrder(expectedJoinedRows);    pipeline.run();}
public void beam_f20506_0()
{    List<Row> pc1Rows = Lists.newArrayList(Row.withSchema(CG_SCHEMA_1).addValues("user1", 1, "us").build(), Row.withSchema(CG_SCHEMA_1).addValues("user1", 2, "us").build(), Row.withSchema(CG_SCHEMA_1).addValues("user1", 3, "il").build(), Row.withSchema(CG_SCHEMA_1).addValues("user1", 4, "il").build(), Row.withSchema(CG_SCHEMA_1).addValues("user2", 5, "fr").build(), Row.withSchema(CG_SCHEMA_1).addValues("user2", 6, "fr").build(), Row.withSchema(CG_SCHEMA_1).addValues("user2", 7, "ar").build(), Row.withSchema(CG_SCHEMA_1).addValues("user2", 8, "ar").build(), Row.withSchema(CG_SCHEMA_1).addValues("user3", 7, "ar").build());    List<Row> pc2Rows = Lists.newArrayList(Row.withSchema(CG_SCHEMA_2).addValues("user1", 9, "us").build(), Row.withSchema(CG_SCHEMA_2).addValues("user1", 10, "us").build(), Row.withSchema(CG_SCHEMA_2).addValues("user1", 11, "il").build(), Row.withSchema(CG_SCHEMA_2).addValues("user1", 12, "il").build(), Row.withSchema(CG_SCHEMA_2).addValues("user2", 13, "fr").build(), Row.withSchema(CG_SCHEMA_2).addValues("user2", 14, "fr").build(), Row.withSchema(CG_SCHEMA_2).addValues("user2", 15, "ar").build(), Row.withSchema(CG_SCHEMA_2).addValues("user2", 16, "ar").build(), Row.withSchema(CG_SCHEMA_2).addValues("user2", 16, "es").build());    List<Row> pc3Rows = Lists.newArrayList(Row.withSchema(CG_SCHEMA_3).addValues("user1", 17, "us").build(), Row.withSchema(CG_SCHEMA_3).addValues("user1", 18, "us").build(), Row.withSchema(CG_SCHEMA_3).addValues("user1", 19, "il").build(), Row.withSchema(CG_SCHEMA_3).addValues("user1", 20, "il").build(), Row.withSchema(CG_SCHEMA_3).addValues("user2", 21, "fr").build(), Row.withSchema(CG_SCHEMA_3).addValues("user2", 22, "fr").build(), Row.withSchema(CG_SCHEMA_3).addValues("user2", 23, "ar").build(), Row.withSchema(CG_SCHEMA_3).addValues("user2", 24, "ar").build(), Row.withSchema(CG_SCHEMA_3).addValues("user27", 24, "se").build());    PCollection<Row> pc1 = pipeline.apply("Create1", Create.of(pc1Rows)).setRowSchema(CG_SCHEMA_1);    PCollection<Row> pc2 = pipeline.apply("Create2", Create.of(pc2Rows)).setRowSchema(CG_SCHEMA_2);    PCollection<Row> pc3 = pipeline.apply("Create3", Create.of(pc3Rows)).setRowSchema(CG_SCHEMA_3);        Schema expectedSchema = Schema.builder().addNullableField("pc1", FieldType.row(CG_SCHEMA_1)).addNullableField("pc2", FieldType.row(CG_SCHEMA_2)).addNullableField("pc3", FieldType.row(CG_SCHEMA_3)).build();    PCollection<Row> joined = PCollectionTuple.of("pc1", pc1, "pc2", pc2, "pc3", pc3).apply("CoGroup", CoGroup.join("pc1", By.fieldNames("user", "country").withOptionalParticipation()).join("pc2", By.fieldNames("user2", "country2").withOptionalParticipation()).join("pc3", By.fieldNames("user3", "country3").withOptionalParticipation()).crossProductJoin());    assertEquals(expectedSchema, joined.getSchema());    List<Row> expectedJoinedRows = JoinTestUtils.innerJoin(pc1Rows, pc2Rows, pc3Rows, new String[] { "user", "country" }, new String[] { "user2", "country2" }, new String[] { "user3", "country3" }, expectedSchema);        expectedJoinedRows.add(Row.withSchema(expectedSchema).addValues(Row.withSchema(CG_SCHEMA_1).addValues("user3", 7, "ar").build(), null, null).build());    expectedJoinedRows.add(Row.withSchema(expectedSchema).addValues(null, Row.withSchema(CG_SCHEMA_2).addValues("user2", 16, "es").build(), null).build());    expectedJoinedRows.add(Row.withSchema(expectedSchema).addValues(null, null, Row.withSchema(CG_SCHEMA_3).addValues("user27", 24, "se").build()).build());    PAssert.that(joined).containsInAnyOrder(expectedJoinedRows);    pipeline.run();}
public boolean beam_f20514_0(Object o)
{    if (this == o) {        return true;    }    if (o == null || getClass() != o.getClass()) {        return false;    }    POJO1Nested that = (POJO1Nested) o;    return yard2 == that.yard2 && Objects.equals(yard1, that.yard1);}
public int beam_f20515_0()
{    return Objects.hash(yard1, yard2);}
public boolean beam_f20516_0(Object o)
{    if (this == o) {        return true;    }    if (o == null || getClass() != o.getClass()) {        return false;    }    POJO2 pojo2 = (POJO2) o;    return field2 == pojo2.field2 && Objects.equals(field5, pojo2.field5) && Arrays.equals(field4, pojo2.field4) && Objects.equals(field3, pojo2.field3) && Objects.equals(field1, pojo2.field1);}
public void beam_f20524_0()
{    PCollection<Long> longs = pipeline.apply(Create.of(new POJO1())).apply(Select.fieldNames("field2")).apply(Convert.to(TypeDescriptors.longs()));    PAssert.that(longs).containsInAnyOrder((Long) EXPECTED_ROW1.getValue("field2"));    pipeline.run();}
private static Row beam_f20525_0(int field1, String field2)
{    return Row.withSchema(SIMPLE_SCHEMA).addValues(field1, field2).build();}
private static Row beam_f20526_0(Row nested)
{    return Row.withSchema(NESTED_SCHEMA).addValues(nested, "foo").build();}
public void beam_f20534_0()
{        PCollection<AutoValue_FilterTest_Simple> filtered = pipeline.apply(Create.of(new AutoValue_FilterTest_Simple("pass", 52, 2), new AutoValue_FilterTest_Simple("pass", 2, 2), new AutoValue_FilterTest_Simple("fail", 100, 100))).apply(Filter.<AutoValue_FilterTest_Simple>create().whereFieldName("field1", s -> "pass".equals(s)).whereFieldName("field2", (Integer i) -> i > 50));    PAssert.that(filtered).containsInAnyOrder(new AutoValue_FilterTest_Simple("pass", 52, 2));    pipeline.run();}
public void beam_f20535_0()
{        PCollection<AutoValue_FilterTest_Nested> filtered = pipeline.apply(Create.of(new AutoValue_FilterTest_Nested(new AutoValue_FilterTest_Simple("pass", 52, 2)), new AutoValue_FilterTest_Nested(new AutoValue_FilterTest_Simple("pass", 2, 2)), new AutoValue_FilterTest_Nested(new AutoValue_FilterTest_Simple("fail", 100, 100)))).apply(Filter.<AutoValue_FilterTest_Nested>create().whereFieldName("nested.field1", s -> "pass".equals(s)).whereFieldName("nested.field2", (Integer i) -> i > 50));    PAssert.that(filtered).containsInAnyOrder(new AutoValue_FilterTest_Nested(new AutoValue_FilterTest_Simple("pass", 52, 2)));    pipeline.run();}
public void beam_f20536_0()
{        PCollection<AutoValue_FilterTest_Simple> filtered = pipeline.apply(Create.of(new AutoValue_FilterTest_Simple("", 52, 48), new AutoValue_FilterTest_Simple("", 52, 2), new AutoValue_FilterTest_Simple("", 70, 33))).apply(Filter.<AutoValue_FilterTest_Simple>create().whereFieldNames(Lists.newArrayList("field2", "field3"), r -> r.getInt32("field2") + r.getInt32("field3") >= 100));    PAssert.that(filtered).containsInAnyOrder(new AutoValue_FilterTest_Simple("", 52, 48), new AutoValue_FilterTest_Simple("", 70, 33));    pipeline.run();}
public String beam_f20544_0()
{    return "OuterPOJO{" + "inner=" + inner + '}';}
public void beam_f20545_0()
{    PCollection<KV<Row, Iterable<OuterPOJO>>> grouped = pipeline.apply(Create.of(new OuterPOJO(new POJO("key1", 1L, "value1")), new OuterPOJO(new POJO("key1", 1L, "value2")), new OuterPOJO(new POJO("key2", 2L, "value3")), new OuterPOJO(new POJO("key2", 2L, "value4")))).apply(Group.byFieldNames("inner.field1", "inner.field2"));    Schema keySchema = Schema.builder().addStringField("field1").addInt64Field("field2").build();    List<KV<Row, Collection<OuterPOJO>>> expected = ImmutableList.of(KV.of(Row.withSchema(keySchema).addValues("key1", 1L).build(), ImmutableList.of(new OuterPOJO(new POJO("key1", 1L, "value1")), new OuterPOJO(new POJO("key1", 1L, "value2")))), KV.of(Row.withSchema(keySchema).addValues("key2", 2L).build(), ImmutableList.of(new OuterPOJO(new POJO("key2", 2L, "value3")), new OuterPOJO(new POJO("key2", 2L, "value4")))));    PAssert.that(grouped).satisfies(actual -> containsKIterableVs(expected, actual, new OuterPOJO[0]));    pipeline.run();}
public void beam_f20546_0()
{    Collection<POJO> elements = ImmutableList.of(new POJO("key1", 1, "value1"), new POJO("key1", 1, "value2"), new POJO("key2", 2, "value3"), new POJO("key2", 2, "value4"));    PCollection<Iterable<POJO>> grouped = pipeline.apply(Create.of(elements)).apply(Group.globally());    PAssert.that(grouped).satisfies(actual -> containsSingleIterable(elements, actual));    pipeline.run();}
public long[] beam_f20554_0()
{    return new long[] { 0 };}
public long[] beam_f20555_0(long[] accumulator, Row input)
{    for (Object o : input.getValues()) {        if (o instanceof Long) {            accumulator[0] += (Long) o;        }    }    return accumulator;}
public long[] beam_f20556_0(Iterable<long[]> accumulators)
{    Iterator<long[]> iter = accumulators.iterator();    if (!iter.hasNext()) {        return createAccumulator();    } else {        long[] running = iter.next();        while (iter.hasNext()) {            running[0] += iter.next()[0];        }        return running;    }}
private static Void beam_f20564_0(List<KV<Row, Row>> expectedKvs, Iterable<KV<Row, Row>> actualKvs)
{    List<Matcher<? super KV<Row, Iterable<POJO>>>> matchers = new ArrayList<>();    for (KV<Row, Row> expected : expectedKvs) {        matchers.add(isKv(equalTo(expected.getKey()), equalTo(expected.getValue())));    }    assertThat(actualKvs, containsInAnyOrder(matchers.toArray(new Matcher[0])));    return null;}
private static Void beam_f20565_0(Collection<POJO> expected, Iterable<Iterable<POJO>> actual)
{    POJO[] values = expected.toArray(new POJO[0]);    assertThat(actual, containsInAnyOrder(containsInAnyOrder(values)));    return null;}
public void beam_f20566_0()
{    List<Row> pc1Rows = Lists.newArrayList(Row.withSchema(CG_SCHEMA_1).addValues("user1", 1, "us").build(), Row.withSchema(CG_SCHEMA_1).addValues("user1", 2, "us").build(), Row.withSchema(CG_SCHEMA_1).addValues("user1", 3, "il").build(), Row.withSchema(CG_SCHEMA_1).addValues("user1", 4, "il").build(), Row.withSchema(CG_SCHEMA_1).addValues("user2", 5, "fr").build(), Row.withSchema(CG_SCHEMA_1).addValues("user2", 6, "fr").build(), Row.withSchema(CG_SCHEMA_1).addValues("user2", 7, "ar").build(), Row.withSchema(CG_SCHEMA_1).addValues("user2", 8, "ar").build(), Row.withSchema(CG_SCHEMA_1).addValues("user3", 8, "ar").build());    List<Row> pc2Rows = Lists.newArrayList(Row.withSchema(CG_SCHEMA_1).addValues("user1", 9, "us").build(), Row.withSchema(CG_SCHEMA_1).addValues("user1", 10, "us").build(), Row.withSchema(CG_SCHEMA_1).addValues("user1", 11, "il").build(), Row.withSchema(CG_SCHEMA_1).addValues("user1", 12, "il").build(), Row.withSchema(CG_SCHEMA_1).addValues("user2", 13, "fr").build(), Row.withSchema(CG_SCHEMA_1).addValues("user2", 14, "fr").build(), Row.withSchema(CG_SCHEMA_1).addValues("user2", 15, "ar").build(), Row.withSchema(CG_SCHEMA_1).addValues("user2", 16, "ar").build(), Row.withSchema(CG_SCHEMA_1).addValues("user4", 8, "ar").build());    PCollection<Row> pc1 = pipeline.apply("Create1", Create.of(pc1Rows)).setRowSchema(CG_SCHEMA_1);    PCollection<Row> pc2 = pipeline.apply("Create2", Create.of(pc2Rows)).setRowSchema(CG_SCHEMA_1);    Schema expectedSchema = Schema.builder().addRowField(Join.LHS_TAG, CG_SCHEMA_1).addRowField(Join.RHS_TAG, CG_SCHEMA_1).build();    PCollection<Row> joined = pc1.apply(Join.<Row, Row>innerJoin(pc2).using("user", "country"));    assertEquals(expectedSchema, joined.getSchema());    List<Row> expectedJoinedRows = innerJoin(pc1Rows, pc2Rows, new String[] { "user", "country" }, new String[] { "user", "country" }, expectedSchema);    PAssert.that(joined).containsInAnyOrder(expectedJoinedRows);    pipeline.run();}
public void beam_f20574_0()
{    Schema schema = Schema.builder().addStringField("field1").addInt32Field("field2").build();    PCollection<Row> renamed = pipeline.apply(Create.of(Row.withSchema(schema).addValues("one", 1).build(), Row.withSchema(schema).addValues("two", 2).build()).withRowSchema(schema)).apply(RenameFields.<Row>create().rename("field1", "new1").rename("field2", "new2"));    Schema expectedSchema = Schema.builder().addStringField("new1").addInt32Field("new2").build();    assertEquals(expectedSchema, renamed.getSchema());    List<Row> expectedRows = ImmutableList.of(Row.withSchema(expectedSchema).addValues("one", 1).build(), Row.withSchema(expectedSchema).addValues("two", 2).build());    PAssert.that(renamed).containsInAnyOrder(expectedRows);    pipeline.run();}
public void beam_f20575_0()
{    Schema nestedSchema = Schema.builder().addStringField("field1").addInt32Field("field2").build();    Schema schema = Schema.builder().addStringField("field1").addRowField("nested", nestedSchema).build();    PCollection<Row> renamed = pipeline.apply(Create.of(Row.withSchema(schema).addValues("one", Row.withSchema(nestedSchema).addValues("one", 1).build()).build(), Row.withSchema(schema).addValues("two", Row.withSchema(nestedSchema).addValues("two", 1).build()).build()).withRowSchema(schema)).apply(RenameFields.<Row>create().rename("nested.field1", "new1").rename("nested.field2", "new2"));    Schema expectedNestedSchema = Schema.builder().addStringField("new1").addInt32Field("new2").build();    Schema expectedSchema = Schema.builder().addStringField("field1").addRowField("nested", expectedNestedSchema).build();    assertEquals(expectedSchema, renamed.getSchema());    List<Row> expectedRows = ImmutableList.of(Row.withSchema(expectedSchema).addValues("one", Row.withSchema(expectedNestedSchema).addValues("one", 1).build()).build(), Row.withSchema(expectedSchema).addValues("two", Row.withSchema(expectedNestedSchema).addValues("two", 1).build()).build());    PAssert.that(renamed).containsInAnyOrder(expectedRows);    pipeline.run();}
public void beam_f20576_0()
{    Schema nestedSchema = Schema.builder().addStringField("field1").addInt32Field("field2").build();    Schema schema = Schema.builder().addStringField("field1").addRowField("nested", nestedSchema).build();    PCollection<Row> renamed = pipeline.apply(Create.of(Row.withSchema(schema).addValues("one", Row.withSchema(nestedSchema).addValues("one", 1).build()).build(), Row.withSchema(schema).addValues("two", Row.withSchema(nestedSchema).addValues("two", 1).build()).build()).withRowSchema(schema)).apply(RenameFields.<Row>create().rename("field1", "top1").rename("nested", "newnested").rename("nested.field1", "new1").rename("nested.field2", "new2"));    Schema expectedNestedSchema = Schema.builder().addStringField("new1").addInt32Field("new2").build();    Schema expectedSchema = Schema.builder().addStringField("top1").addRowField("newnested", expectedNestedSchema).build();    assertEquals(expectedSchema, renamed.getSchema());    List<Row> expectedRows = ImmutableList.of(Row.withSchema(expectedSchema).addValues("one", Row.withSchema(expectedNestedSchema).addValues("one", 1).build()).build(), Row.withSchema(expectedSchema).addValues("two", Row.withSchema(expectedNestedSchema).addValues("two", 1).build()).build());    PAssert.that(renamed).containsInAnyOrder(expectedRows);    pipeline.run();}
public boolean beam_f20584_0(Object o)
{    if (this == o) {        return true;    }    if (!(o instanceof POJO2)) {        return false;    }    POJO2 pojo2 = (POJO2) o;    return Objects.equals(field1, pojo2.field1) && Objects.equals(field2, pojo2.field2);}
public int beam_f20585_0()
{    return Objects.hash(field1, field2);}
public void beam_f20586_0()
{    thrown.expect(IllegalArgumentException.class);    pipeline.apply(Create.of(new POJO1())).apply(Select.fieldNames("missing"));    pipeline.run();}
public int beam_f20594_0()
{    return Objects.hash(field1);}
public void beam_f20595_0()
{    PCollection<PrimitiveArray> selected = pipeline.apply(Create.of(new PrimitiveArray())).apply(Select.fieldNames("field1")).apply(Convert.to(PrimitiveArray.class));    PAssert.that(selected).containsInAnyOrder(new PrimitiveArray());    pipeline.run();}
public boolean beam_f20596_0(Object o)
{    if (this == o) {        return true;    }    if (!(o instanceof RowSingleArray)) {        return false;    }    RowSingleArray that = (RowSingleArray) o;    return Objects.equals(field1, that.field1);}
public int beam_f20604_0()
{    return Objects.hash(field1, field3);}
public void beam_f20605_0()
{    PCollection<PartialRowSingleMap> selected = pipeline.apply(Create.of(new RowSingleMap())).apply(Select.fieldNames("field1.field1", "field1.field3")).apply(Convert.to(PartialRowSingleMap.class));    PAssert.that(selected).containsInAnyOrder(new PartialRowSingleMap());    pipeline.run();}
public boolean beam_f20606_0(Object o)
{    if (this == o) {        return true;    }    if (!(o instanceof RowMultipleArray)) {        return false;    }    RowMultipleArray that = (RowMultipleArray) o;    return Objects.equals(field1, that.field1);}
public int beam_f20614_0()
{    return Objects.hash(field1, field3);}
public void beam_f20615_0()
{    PCollection<RowMultipleMaps> input = pipeline.apply(Create.of(new RowMultipleMaps()));    PCollection<PartialRowMultipleMaps> selected = input.apply("select1", Select.fieldNames("field1.field1", "field1.field3")).apply("convert1", Convert.to(PartialRowMultipleMaps.class));    PCollection<PartialRowMultipleMaps> selected2 = input.apply("select2", Select.fieldNames("field1{}{}{}.field1", "field1{}{}{}.field3")).apply("convert2", Convert.to(PartialRowMultipleMaps.class));    PAssert.that(selected).containsInAnyOrder(new PartialRowMultipleMaps());    PAssert.that(selected2).containsInAnyOrder(new PartialRowMultipleMaps());    pipeline.run();}
public boolean beam_f20616_0(Object o)
{    if (this == o) {        return true;    }    if (!(o instanceof RowNestedArraysAndMaps)) {        return false;    }    RowNestedArraysAndMaps that = (RowNestedArraysAndMaps) o;    return Objects.equals(field1, that.field1);}
public void beam_f20624_0()
{    List<Row> bottomRow = IntStream.rangeClosed(0, 2).mapToObj(i -> Row.withSchema(SIMPLE_SCHEMA).addValues(i, Integer.toString(i)).build()).collect(Collectors.toList());    List<Row> rows = bottomRow.stream().map(r -> Row.withSchema(NESTED_SCHEMA2).addValues(r).build()).collect(Collectors.toList());    PCollection<Row> unnested = pipeline.apply(Create.of(rows).withRowSchema(NESTED_SCHEMA2)).apply(Unnest.<Row>create().withFieldNameFunction(Unnest.KEEP_NESTED_NAME));    assertEquals(UNNESTED2_SCHEMA_ALTERNATE, unnested.getSchema());    List<Row> expected = bottomRow.stream().map(r -> Row.withSchema(UNNESTED2_SCHEMA_ALTERNATE).addValues(r.getValue(0), r.getValue(1)).build()).collect(Collectors.toList());    ;    PAssert.that(unnested).containsInAnyOrder(expected);    pipeline.run();}
public void beam_f20625_0()
{    List<Row> bottomRow = IntStream.rangeClosed(0, 2).mapToObj(i -> Row.withSchema(SIMPLE_SCHEMA).addValues(i, Integer.toString(i)).build()).collect(Collectors.toList());    thrown.expect(IllegalArgumentException.class);    List<Row> rows = bottomRow.stream().map(r -> Row.withSchema(NESTED_SCHEMA).addValues(r, r).build()).collect(Collectors.toList());    PCollection<Row> unnested = pipeline.apply(Create.of(rows).withRowSchema(NESTED_SCHEMA)).apply(Unnest.<Row>create().withFieldNameFunction(Unnest.KEEP_NESTED_NAME));    pipeline.run();}
public Schema beam_f20626_0(SourceOfRandomness random, GenerationStatus status)
{    Schema.Type type;    if (nesting(status) >= MAX_NESTING) {        type = random.choose(PRIMITIVE_TYPES);    } else {        type = random.choose(ALL_TYPES);    }    if (PRIMITIVE_TYPES.contains(type)) {        return Schema.create(type);    } else {        nestingInc(status);        if (type == Schema.Type.FIXED) {            int size = random.choose(Arrays.asList(1, 5, 12));            return Schema.createFixed("fixed_" + branch(status), "", "", size);        } else if (type == Schema.Type.UNION) {                        return UnionSchemaGenerator.INSTANCE.generate(random, status);        } else if (type == Schema.Type.ENUM) {            return EnumSchemaGenerator.INSTANCE.generate(random, status);        } else if (type == Schema.Type.RECORD) {            return RecordSchemaGenerator.INSTANCE.generate(random, status);        } else if (type == Schema.Type.MAP) {            return Schema.createMap(generate(random, status));        } else if (type == Schema.Type.ARRAY) {            return Schema.createArray(generate(random, status));        } else {            throw new AssertionError("Unexpected AVRO type: " + type);        }    }}
 int beam_f20634_0(GenerationStatus status)
{    return status.valueOf(NESTING_KEY).orElse(0);}
 void beam_f20635_0(GenerationStatus status)
{    status.setValue(NESTING_KEY, nesting(status) + 1);}
public void beam_f20636_0(@From(RecordSchemaGenerator.class) org.apache.avro.Schema avroSchema)
{        assumeThat(avroSchema, not(containsField(AvroUtilsTest::hasNonNullUnion)));    Schema schema = AvroUtils.toBeamSchema(avroSchema);    Iterable iterable = new RandomData(avroSchema, 10);    List<GenericRecord> records = Lists.newArrayList((Iterable<GenericRecord>) iterable);    for (GenericRecord record : records) {        AvroUtils.toBeamRowStrict(record, schema);    }}
private static org.apache.avro.Schema beam_f20644_0(String name)
{    return org.apache.avro.Schema.createRecord(name, null, "topLevelRecord", false, getAvroSubSchemaFields());}
private static org.apache.avro.Schema beam_f20645_0()
{    List<org.apache.avro.Schema.Field> fields = Lists.newArrayList();    fields.add(new org.apache.avro.Schema.Field("bool", org.apache.avro.Schema.create(Type.BOOLEAN), "", (Object) null));    fields.add(new org.apache.avro.Schema.Field("int", org.apache.avro.Schema.create(Type.INT), "", (Object) null));    fields.add(new org.apache.avro.Schema.Field("long", org.apache.avro.Schema.create(Type.LONG), "", (Object) null));    fields.add(new org.apache.avro.Schema.Field("float", org.apache.avro.Schema.create(Type.FLOAT), "", (Object) null));    fields.add(new org.apache.avro.Schema.Field("double", org.apache.avro.Schema.create(Type.DOUBLE), "", (Object) null));    fields.add(new org.apache.avro.Schema.Field("string", org.apache.avro.Schema.create(Type.STRING), "", (Object) null));    fields.add(new org.apache.avro.Schema.Field("bytes", org.apache.avro.Schema.create(Type.BYTES), "", (Object) null));    fields.add(new org.apache.avro.Schema.Field("decimal", LogicalTypes.decimal(Integer.MAX_VALUE).addToSchema(org.apache.avro.Schema.create(Type.BYTES)), "", (Object) null));    fields.add(new org.apache.avro.Schema.Field("timestampMillis", LogicalTypes.timestampMillis().addToSchema(org.apache.avro.Schema.create(Type.LONG)), "", (Object) null));    fields.add(new org.apache.avro.Schema.Field("row", getAvroSubSchema("row"), "", (Object) null));    fields.add(new org.apache.avro.Schema.Field("array", org.apache.avro.Schema.createArray(getAvroSubSchema("array")), "", (Object) null));    fields.add(new org.apache.avro.Schema.Field("map", org.apache.avro.Schema.createMap(getAvroSubSchema("map")), "", (Object) null));    return org.apache.avro.Schema.createRecord("topLevelRecord", null, null, false, fields);}
private static Schema beam_f20646_0()
{    return new Schema.Builder().addField(Field.of("bool", FieldType.BOOLEAN)).addField(Field.of("int", FieldType.INT32)).build();}
public void beam_f20654_0()
{        Schema contact = new Schema.Builder().addField(Field.of("name", FieldType.STRING)).addField(Field.of("address", FieldType.row(new Schema.Builder().addField(Field.of("street", FieldType.STRING)).addField(Field.of("city", FieldType.STRING)).build()))).build();    Schema contactMultiline = new Schema.Builder().addField(Field.of("name", FieldType.STRING)).addField(Field.of("address", FieldType.row(new Schema.Builder().addField(Field.of("street", FieldType.array(FieldType.STRING))).addField(Field.of("city", FieldType.STRING)).build()))).build();                Schema beamSchema = new Schema.Builder().addField(Field.of("home", FieldType.row(contact))).addField(Field.of("work", FieldType.row(contactMultiline))).addField(Field.of("address", FieldType.row(contact))).addField(Field.of("topLevelRecord", FieldType.row(contactMultiline))).build();    org.apache.avro.Schema convertedSchema = AvroUtils.toAvroSchema(beamSchema);    org.apache.avro.Schema validatedSchema = new org.apache.avro.Schema.Parser().parse(convertedSchema.toString());    assertEquals(convertedSchema, validatedSchema);}
public void beam_f20655_0()
{    List<org.apache.avro.Schema.Field> fields = Lists.newArrayList();    fields.add(new org.apache.avro.Schema.Field("int", ReflectData.makeNullable(org.apache.avro.Schema.create(Type.INT)), "", null));    fields.add(new org.apache.avro.Schema.Field("array", org.apache.avro.Schema.createArray(ReflectData.makeNullable(org.apache.avro.Schema.create(Type.BYTES))), "", null));    fields.add(new org.apache.avro.Schema.Field("map", org.apache.avro.Schema.createMap(ReflectData.makeNullable(org.apache.avro.Schema.create(Type.INT))), "", null));    org.apache.avro.Schema avroSchema = org.apache.avro.Schema.createRecord("topLevelRecord", null, null, false, fields);    Schema expectedSchema = Schema.builder().addNullableField("int", FieldType.INT32).addArrayField("array", FieldType.BYTES.withNullable(true)).addMapField("map", FieldType.STRING, FieldType.INT32.withNullable(true)).build();    assertEquals(expectedSchema, AvroUtils.toBeamSchema(avroSchema));    Map<String, Object> nullMap = Maps.newHashMap();    nullMap.put("k1", null);    GenericRecord genericRecord = new GenericRecordBuilder(avroSchema).set("int", null).set("array", Lists.newArrayList((Object) null)).set("map", nullMap).build();    Row expectedRow = Row.withSchema(expectedSchema).addValue(null).addValue(Lists.newArrayList((Object) null)).addValue(nullMap).build();    assertEquals(expectedRow, AvroUtils.toBeamRowStrict(genericRecord, expectedSchema));}
public void beam_f20656_0()
{    Schema beamSchema = Schema.builder().addNullableField("int", FieldType.INT32).addArrayField("array", FieldType.INT32.withNullable(true)).addMapField("map", FieldType.STRING, FieldType.INT32.withNullable(true)).build();    List<org.apache.avro.Schema.Field> fields = Lists.newArrayList();    fields.add(new org.apache.avro.Schema.Field("int", ReflectData.makeNullable(org.apache.avro.Schema.create(Type.INT)), "", null));    fields.add(new org.apache.avro.Schema.Field("array", org.apache.avro.Schema.createArray(ReflectData.makeNullable(org.apache.avro.Schema.create(Type.INT))), "", null));    fields.add(new org.apache.avro.Schema.Field("map", org.apache.avro.Schema.createMap(ReflectData.makeNullable(org.apache.avro.Schema.create(Type.INT))), "", null));    org.apache.avro.Schema avroSchema = org.apache.avro.Schema.createRecord("topLevelRecord", null, null, false, fields);    assertEquals(avroSchema, AvroUtils.toAvroSchema(beamSchema));    Map<Utf8, Object> nullMapUtf8 = Maps.newHashMap();    nullMapUtf8.put(new Utf8("k1"), null);    Map<String, Object> nullMapString = Maps.newHashMap();    nullMapString.put("k1", null);    GenericRecord expectedGenericRecord = new GenericRecordBuilder(avroSchema).set("int", null).set("array", Lists.newArrayList((Object) null)).set("map", nullMapUtf8).build();    Row row = Row.withSchema(beamSchema).addValue(null).addValue(Lists.newArrayList((Object) null)).addValue(nullMapString).build();    assertEquals(expectedGenericRecord, AvroUtils.toGenericRecord(row, avroSchema));}
public void beam_f20665_0()
{    Schema schema = JavaBeanUtils.schemaFromJavaBeanClass(SimpleBean.class, GetterTypeSupplier.INSTANCE);    SchemaTestUtils.assertSchemaEquivalent(SIMPLE_BEAN_SCHEMA, schema);}
public void beam_f20666_0()
{    Schema schema = JavaBeanUtils.schemaFromJavaBeanClass(NestedBean.class, GetterTypeSupplier.INSTANCE);    SchemaTestUtils.assertSchemaEquivalent(NESTED_BEAN_SCHEMA, schema);}
public void beam_f20667_0()
{    Schema schema = JavaBeanUtils.schemaFromJavaBeanClass(PrimitiveArrayBean.class, GetterTypeSupplier.INSTANCE);    SchemaTestUtils.assertSchemaEquivalent(PRIMITIVE_ARRAY_BEAN_SCHEMA, schema);}
public void beam_f20675_0()
{    BeanWithBoxedFields bean = new BeanWithBoxedFields();    List<FieldValueSetter> setters = JavaBeanUtils.getSetters(BeanWithBoxedFields.class, BEAN_WITH_BOXED_FIELDS_SCHEMA, new SetterTypeSupplier());    setters.get(0).set(bean, (byte) 41);    setters.get(1).set(bean, (short) 42);    setters.get(2).set(bean, (int) 43);    setters.get(3).set(bean, (long) 44);    setters.get(4).set(bean, true);    assertEquals((byte) 41, bean.getaByte().byteValue());    assertEquals((short) 42, bean.getaShort().shortValue());    assertEquals((int) 43, bean.getAnInt().intValue());    assertEquals((long) 44, bean.getaLong().longValue());    assertTrue(bean.getaBoolean().booleanValue());}
public void beam_f20676_0()
{    BeanWithByteArray bean = new BeanWithByteArray();    List<FieldValueSetter> setters = JavaBeanUtils.getSetters(BeanWithByteArray.class, BEAN_WITH_BYTE_ARRAY_SCHEMA, new SetterTypeSupplier());    setters.get(0).set(bean, "field1".getBytes(Charset.defaultCharset()));    setters.get(1).set(bean, "field2".getBytes(Charset.defaultCharset()));    assertArrayEquals("not equal", "field1".getBytes(Charset.defaultCharset()), bean.getBytes1());    assertEquals(ByteBuffer.wrap("field2".getBytes(Charset.defaultCharset())), bean.getBytes2());}
public void beam_f20677_0()
{    Schema schema = POJOUtils.schemaFromPojoClass(POJOWithNullables.class, JavaFieldTypeSupplier.INSTANCE);    assertTrue(schema.getField("str").getType().getNullable());    assertFalse(schema.getField("anInt").getType().getNullable());}
public void beam_f20685_0()
{    SimplePOJO simplePojo = new SimplePOJO("field1", (byte) 41, (short) 42, 43, 44L, true, DATE, INSTANT, BYTE_ARRAY, BYTE_BUFFER, new BigDecimal(42), new StringBuilder("stringBuilder"));    List<FieldValueGetter> getters = POJOUtils.getGetters(SimplePOJO.class, SIMPLE_POJO_SCHEMA, JavaFieldTypeSupplier.INSTANCE);    assertEquals(12, getters.size());    assertEquals("str", getters.get(0).name());    assertEquals("field1", getters.get(0).get(simplePojo));    assertEquals((byte) 41, getters.get(1).get(simplePojo));    assertEquals((short) 42, getters.get(2).get(simplePojo));    assertEquals((int) 43, getters.get(3).get(simplePojo));    assertEquals((long) 44, getters.get(4).get(simplePojo));    assertTrue((Boolean) getters.get(5).get(simplePojo));    assertEquals(DATE.toInstant(), getters.get(6).get(simplePojo));    assertEquals(INSTANT, getters.get(7).get(simplePojo));    assertArrayEquals("Unexpected bytes", BYTE_ARRAY, (byte[]) getters.get(8).get(simplePojo));    assertArrayEquals("Unexpected bytes", BYTE_BUFFER.array(), (byte[]) getters.get(9).get(simplePojo));    assertEquals(new BigDecimal(42), getters.get(10).get(simplePojo));    assertEquals("stringBuilder", getters.get(11).get(simplePojo));}
public void beam_f20686_0()
{    SimplePOJO simplePojo = new SimplePOJO();    List<FieldValueSetter> setters = POJOUtils.getSetters(SimplePOJO.class, SIMPLE_POJO_SCHEMA, JavaFieldTypeSupplier.INSTANCE);    assertEquals(12, setters.size());    setters.get(0).set(simplePojo, "field1");    setters.get(1).set(simplePojo, (byte) 41);    setters.get(2).set(simplePojo, (short) 42);    setters.get(3).set(simplePojo, (int) 43);    setters.get(4).set(simplePojo, (long) 44);    setters.get(5).set(simplePojo, true);    setters.get(6).set(simplePojo, DATE.toInstant());    setters.get(7).set(simplePojo, INSTANT);    setters.get(8).set(simplePojo, BYTE_ARRAY);    setters.get(9).set(simplePojo, BYTE_BUFFER.array());    setters.get(10).set(simplePojo, new BigDecimal(42));    setters.get(11).set(simplePojo, "stringBuilder");    assertEquals("field1", simplePojo.str);    assertEquals((byte) 41, simplePojo.aByte);    assertEquals((short) 42, simplePojo.aShort);    assertEquals((int) 43, simplePojo.anInt);    assertEquals((long) 44, simplePojo.aLong);    assertTrue(simplePojo.aBoolean);    assertEquals(DATE, simplePojo.dateTime);    assertEquals(INSTANT, simplePojo.instant);    assertArrayEquals("Unexpected bytes", BYTE_ARRAY, simplePojo.bytes);    assertEquals(BYTE_BUFFER, simplePojo.byteBuffer);    assertEquals(new BigDecimal(42), simplePojo.bigDecimal);    assertEquals("stringBuilder", simplePojo.stringBuilder.toString());}
public void beam_f20687_0()
{    POJOWithBoxedFields pojo = new POJOWithBoxedFields((byte) 41, (short) 42, 43, 44L, true);    List<FieldValueGetter> getters = POJOUtils.getGetters(POJOWithBoxedFields.class, POJO_WITH_BOXED_FIELDS_SCHEMA, JavaFieldTypeSupplier.INSTANCE);    assertEquals((byte) 41, getters.get(0).get(pojo));    assertEquals((short) 42, getters.get(1).get(pojo));    assertEquals((int) 43, getters.get(2).get(pojo));    assertEquals((long) 44, getters.get(3).get(pojo));    assertTrue((Boolean) getters.get(4).get(pojo));}
public Integer beam_f20695_0(Integer left, Integer right)
{    return left + right;}
public Integer beam_f20696_0(Context context, Schema.FieldType left, Schema.FieldType right)
{    if (left.getTypeName() != right.getTypeName()) {        return 0;    }    if (left.getTypeName() == Schema.TypeName.ROW) {        return 0;    }    if (left.getTypeName() == Schema.TypeName.ARRAY) {        return 0;    }    if (left.getTypeName() == Schema.TypeName.MAP) {        return 0;    }    return 1;}
public Integer beam_f20697_0(Context context, Optional<Schema.Field> left, Optional<Schema.Field> right)
{    return 0;}
public List<String> beam_f20705_0(Context context, Schema.FieldType left, Schema.FieldType right)
{    return Collections.emptyList();}
public List<String> beam_f20706_0(Context context, Optional<Schema.Field> left, Optional<Schema.Field> right)
{    if (left.isPresent() && right.isPresent()) {        String pathStr = Joiner.on('.').join(context.path());        return Collections.singletonList(pathStr);    } else {        return Collections.emptyList();    }}
public void beam_f20707_0()
{    FieldAccessDescriptor fieldAccessDescriptor = FieldAccessDescriptor.withFieldNames("*").resolve(FLAT_SCHEMA);    Schema outputSchema = SelectHelpers.getOutputSchema(FLAT_SCHEMA, fieldAccessDescriptor);    assertEquals(FLAT_SCHEMA, outputSchema);    Row row = SelectHelpers.selectRow(FLAT_ROW, fieldAccessDescriptor, FLAT_SCHEMA, outputSchema);    assertEquals(FLAT_ROW, row);}
public void beam_f20715_0()
{    FieldAccessDescriptor fieldAccessDescriptor = FieldAccessDescriptor.withFieldNames("primitiveArray").resolve(ARRAY_SCHEMA);    Schema outputSchema = SelectHelpers.getOutputSchema(ARRAY_SCHEMA, fieldAccessDescriptor);    Schema expectedSchema = Schema.builder().addArrayField("primitiveArray", FieldType.INT32).build();    assertEquals(expectedSchema, outputSchema);    Row row = SelectHelpers.selectRow(ARRAY_ROW, fieldAccessDescriptor, ARRAY_SCHEMA, outputSchema);    Row expectedRow = Row.withSchema(expectedSchema).addArray(1, 2).build();    assertEquals(expectedRow, row);}
public void beam_f20716_0()
{    FieldAccessDescriptor fieldAccessDescriptor = FieldAccessDescriptor.withFieldNames("rowArray").resolve(ARRAY_SCHEMA);    Schema outputSchema = SelectHelpers.getOutputSchema(ARRAY_SCHEMA, fieldAccessDescriptor);    Schema expectedSchema = Schema.builder().addArrayField("rowArray", FieldType.row(FLAT_SCHEMA)).build();    assertEquals(expectedSchema, outputSchema);    Row row = SelectHelpers.selectRow(ARRAY_ROW, fieldAccessDescriptor, ARRAY_SCHEMA, outputSchema);    Row expectedRow = Row.withSchema(expectedSchema).addArray(FLAT_ROW, FLAT_ROW).build();    assertEquals(expectedRow, row);}
public void beam_f20717_0()
{    FieldAccessDescriptor fieldAccessDescriptor = FieldAccessDescriptor.withFieldNames("rowArray[].field1").resolve(ARRAY_SCHEMA);    Schema outputSchema = SelectHelpers.getOutputSchema(ARRAY_SCHEMA, fieldAccessDescriptor);    Schema expectedSchema = Schema.builder().addArrayField("field1", FieldType.STRING).build();    assertEquals(expectedSchema, outputSchema);    Row row = SelectHelpers.selectRow(ARRAY_ROW, fieldAccessDescriptor, ARRAY_SCHEMA, outputSchema);    Row expectedRow = Row.withSchema(expectedSchema).addArray("first", "first").build();    assertEquals(expectedRow, row);}
public void beam_f20725_0()
{    Schema f1 = Schema.builder().addStringField("f0").build();    Schema f2 = Schema.builder().addArrayField("f1", FieldType.row(f1)).build();    Schema f3 = Schema.builder().addRowField("f2", f2).build();    Schema f4 = Schema.builder().addArrayField("f3", FieldType.row(f3)).build();    Row r1 = Row.withSchema(f1).addValue("first").build();    Row r2 = Row.withSchema(f2).addArray(r1, r1).build();    Row r3 = Row.withSchema(f3).addValue(r2).build();    Row r4 = Row.withSchema(f4).addArray(r3, r3).build();    FieldAccessDescriptor fieldAccessDescriptor = FieldAccessDescriptor.withFieldNames("f3.f2.f1.f0").resolve(f4);    Schema outputSchema = SelectHelpers.getOutputSchema(f4, fieldAccessDescriptor);    Schema expectedSchema = Schema.builder().addArrayField("f0", FieldType.array(FieldType.STRING)).build();    assertEquals(expectedSchema, outputSchema);    Row out = SelectHelpers.selectRow(r4, fieldAccessDescriptor, r4.getSchema(), outputSchema);    Row expected = Row.withSchema(outputSchema).addArray(Lists.newArrayList("first", "first"), Lists.newArrayList("first", "first")).build();    assertEquals(expected, out);}
public String beam_f20726_0()
{    return str;}
public void beam_f20727_0(@Nullable String str)
{    this.str = str;}
public int beam_f20735_0()
{    return Objects.hash(str);}
public String beam_f20736_0()
{    return str;}
public void beam_f20737_0(String str)
{    this.str = str;}
public void beam_f20745_0(long aLong)
{    this.aLong = aLong;}
public boolean beam_f20746_0()
{    return aBoolean;}
public void beam_f20747_0(boolean aBoolean)
{    this.aBoolean = aBoolean;}
public void beam_f20755_0(Instant instant)
{    this.instant = instant;}
public BigDecimal beam_f20756_0()
{    return bigDecimal;}
public void beam_f20757_0(BigDecimal bigDecimal)
{    this.bigDecimal = bigDecimal;}
public short beam_f20765_0()
{    return aShort;}
public int beam_f20766_0()
{    return anInt;}
public long beam_f20767_0()
{    return aLong;}
public boolean beam_f20775_0(Object o)
{    if (this == o) {        return true;    }    if (o == null || getClass() != o.getClass()) {        return false;    }    SimpleBeanWithAnnotations that = (SimpleBeanWithAnnotations) o;    return aByte == that.aByte && aShort == that.aShort && anInt == that.anInt && aLong == that.aLong && aBoolean == that.aBoolean && Objects.equals(str, that.str) && Objects.equals(dateTime, that.dateTime) && Objects.equals(instant, that.instant) && Arrays.equals(bytes, that.bytes) && Objects.equals(byteBuffer, that.byteBuffer) && Objects.equals(bigDecimal, that.bigDecimal) && Objects.equals(stringBuilder.toString(), that.stringBuilder.toString());}
public int beam_f20776_0()
{    int result = Objects.hash(str, aByte, aShort, anInt, aLong, aBoolean, dateTime, instant, byteBuffer, bigDecimal, stringBuilder.toString());    result = 31 * result + Arrays.hashCode(bytes);    return result;}
public SimpleBean beam_f20777_0()
{    return nested;}
public Long[] beam_f20785_0()
{    return longs;}
public void beam_f20786_0(Long[] longs)
{    this.longs = longs;}
public boolean beam_f20787_0(Object o)
{    if (this == o) {        return true;    }    if (o == null || getClass() != o.getClass()) {        return false;    }    PrimitiveArrayBean that = (PrimitiveArrayBean) o;    return Objects.equals(strings, that.strings) && Arrays.equals(integers, that.integers) && Arrays.equals(longs, that.longs);}
public boolean beam_f20795_0(Object o)
{    if (this == o) {        return true;    }    if (o == null || getClass() != o.getClass()) {        return false;    }    NestedArraysBean that = (NestedArraysBean) o;    return Objects.equals(lists, that.lists);}
public int beam_f20796_0()
{    return Objects.hash(lists);}
public List<SimpleBean> beam_f20797_0()
{    return simples;}
public Map<String, SimpleBean> beam_f20805_0()
{    return map;}
public void beam_f20806_0(Map<String, SimpleBean> map)
{    this.map = map;}
public boolean beam_f20807_0(Object o)
{    if (this == o) {        return true;    }    if (o == null || getClass() != o.getClass()) {        return false;    }    NestedMapBean that = (NestedMapBean) o;    return Objects.equals(map, that.map);}
public Long beam_f20815_0()
{    return aLong;}
public void beam_f20816_0(Long aLong)
{    this.aLong = aLong;}
public Boolean beam_f20817_0()
{    return aBoolean;}
public boolean beam_f20825_0(Object o)
{    if (this == o) {        return true;    }    if (o == null || getClass() != o.getClass()) {        return false;    }    BeanWithByteArray that = (BeanWithByteArray) o;    return Arrays.equals(bytes1, that.bytes1) && Objects.equals(bytes2, that.bytes2);}
public int beam_f20826_0()
{    int result = Objects.hash(bytes2);    result = 31 * result + Arrays.hashCode(bytes1);    return result;}
public boolean beam_f20827_0(Object o)
{    if (this == o) {        return true;    }    if (o == null || getClass() != o.getClass()) {        return false;    }    POJOWithNullables that = (POJOWithNullables) o;    return anInt == that.anInt && Objects.equals(str, that.str);}
public int beam_f20835_0()
{    int result = Objects.hash(str, theByte, theShort, anInt, aLong, aBoolean, dateTime, instant, byteBuffer, bigDecimal, stringBuilder.toString());    result = 31 * result + Arrays.hashCode(bytes);    return result;}
public String beam_f20836_0()
{    return "AnnotatedSimplePojo{" + "str='" + str + '\'' + ", theByte=" + theByte + ", theShort=" + theShort + ", anInt=" + anInt + ", aLong=" + aLong + ", aBoolean=" + aBoolean + ", dateTime=" + dateTime + ", instant=" + instant + ", bytes=" + Arrays.toString(bytes) + ", byteBuffer=" + byteBuffer + ", bigDecimal=" + bigDecimal + ", stringBuilder=" + stringBuilder + ", pleaseIgnore=" + pleaseIgnore + '}';}
public boolean beam_f20837_0(Object o)
{    if (this == o) {        return true;    }    if (o == null || getClass() != o.getClass()) {        return false;    }    SimplePOJO that = (SimplePOJO) o;    return aByte == that.aByte && aShort == that.aShort && anInt == that.anInt && aLong == that.aLong && aBoolean == that.aBoolean && Objects.equals(str, that.str) && Objects.equals(dateTime, that.dateTime) && Objects.equals(instant, that.instant) && Arrays.equals(bytes, that.bytes) && Objects.equals(byteBuffer, that.byteBuffer) && Objects.equals(bigDecimal, that.bigDecimal) && Objects.equals(stringBuilder.toString(), that.stringBuilder.toString());}
public boolean beam_f20845_0(Object o)
{    if (this == o) {        return true;    }    if (o == null || getClass() != o.getClass()) {        return false;    }    NestedArraysPOJO that = (NestedArraysPOJO) o;    return Objects.equals(lists, that.lists);}
public int beam_f20846_0()
{    return Objects.hash(lists);}
public boolean beam_f20847_0(Object o)
{    if (this == o) {        return true;    }    if (o == null || getClass() != o.getClass()) {        return false;    }    NestedCollectionPOJO that = (NestedCollectionPOJO) o;    return Objects.equals(simples, that.simples);}
public boolean beam_f20855_0(Object o)
{    if (this == o) {        return true;    }    if (o == null || getClass() != o.getClass()) {        return false;    }    POJOWithByteArray that = (POJOWithByteArray) o;    return Arrays.equals(bytes1, that.bytes1) && Objects.equals(bytes2, that.bytes2);}
public int beam_f20856_0()
{    int result = Objects.hash(bytes2);    result = 31 * result + Arrays.hashCode(bytes1);    return result;}
public boolean beam_f20857_0(Object o)
{    if (this == o) {        return true;    }    if (!(o instanceof PojoWithNestedArray)) {        return false;    }    PojoWithNestedArray that = (PojoWithNestedArray) o;    return Objects.equals(pojos, that.pojos);}
public void beam_f20865_0()
{    BoundedWindow window = new IntervalWindow(new Instant(-137), Duration.millis(21L));    StateContext<BoundedWindow> context = StateContexts.windowOnlyContext(window);    assertThat(context.window(), equalTo(window));}
public void beam_f20866_0() throws Exception
{    CoderProperties.coderDeterministic(StringUtf8Coder.of(), "TestData", "TestData");}
public void beam_f20867_0(String value, OutputStream outStream) throws CoderException, IOException
{    StringUtf8Coder.of().encode(value, outStream);}
public void beam_f20876_0() throws Exception
{    CoderProperties.coderDecodeEncodeEqual(StringUtf8Coder.of(), "TestData");}
public void beam_f20877_0(String value, OutputStream outStream) throws CoderException, IOException
{    changedState += 1;    StringUtf8Coder.of().encode(value + Strings.repeat("A", changedState), outStream);}
public String beam_f20878_0(InputStream inStream) throws CoderException, IOException
{    String decodedValue = StringUtf8Coder.of().decode(inStream);    return decodedValue.substring(0, decodedValue.length() - changedState);}
public void beam_f20886_0() throws Exception
{    expectedException.expect(RuntimeException.class);    expectedException.expectMessage("I forgot something...");    CoderProperties.coderDecodeEncodeEqual(new ForgetfulSerializingCoder(1), "TestData");}
public void beam_f20887_0(String value, OutputStream outStream) throws IOException
{    outStream.close();}
public String beam_f20888_0(InputStream inStream) throws IOException
{    inStream.close();    return null;}
public Integer beam_f20896_0(Iterable<Integer> accumulators)
{    int result = 0;    for (int accum : accumulators) {        if (accum == 0) {            sawEmpty.set(true);        }        result += accum;    }    return result;}
public Integer beam_f20897_0(Integer accumulator)
{    return accumulator;}
public void beam_f20898_0()
{    final AtomicBoolean sawSingleShard = new AtomicBoolean();    CombineFn<Integer, Integer, Integer> combineFn = new CombineFn<Integer, Integer, Integer>() {        int accumCount = 0;        @Override        public Integer createAccumulator() {            accumCount++;            return 0;        }        @Override        public Integer addInput(Integer accumulator, Integer input) {            return accumulator + input;        }        @Override        public Integer mergeAccumulators(Iterable<Integer> accumulators) {            int result = 0;            for (int accum : accumulators) {                result += accum;            }            return result;        }        @Override        public Integer extractOutput(Integer accumulator) {            if (accumCount == 1) {                sawSingleShard.set(true);            }            accumCount = 0;            return accumulator;        }    };    CombineFnTester.testCombineFn(combineFn, Arrays.asList(1, 2, 3, 4, 5), 15);    assertThat(sawSingleShard.get(), is(true));}
public Integer beam_f20906_0(Iterable<Integer> accumulators)
{    if (Iterables.size(accumulators) > 2) {        sawManyShards.set(true);    }    int result = 0;    for (int accum : accumulators) {        result += accum;    }    return result;}
public Integer beam_f20907_0(Integer accumulator)
{    return accumulator;}
public void beam_f20908_0()
{    final AtomicBoolean sawMultipleMerges = new AtomicBoolean();    CombineFn<Integer, KV<Integer, Integer>, Integer> combineFn = new CombineFn<Integer, KV<Integer, Integer>, Integer>() {        @Override        public KV<Integer, Integer> createAccumulator() {            return KV.of(0, 0);        }        @Override        public KV<Integer, Integer> addInput(KV<Integer, Integer> accumulator, Integer input) {            return KV.of(accumulator.getKey() + input, accumulator.getValue());        }        @Override        public KV<Integer, Integer> mergeAccumulators(Iterable<KV<Integer, Integer>> accumulators) {            int result = 0;            int numMerges = 0;            for (KV<Integer, Integer> accum : accumulators) {                result += accum.getKey();                numMerges += accum.getValue();            }            return KV.of(result, numMerges + 1);        }        @Override        public Integer extractOutput(KV<Integer, Integer> accumulator) {            if (accumulator.getValue() > 1) {                sawMultipleMerges.set(true);            }            return accumulator.getKey();        }    };    CombineFnTester.testCombineFn(combineFn, Arrays.asList(1, 1, 2, 2, 3, 3, 4, 4, 5, 5), 30);    assertThat(sawMultipleMerges.get(), is(true));}
public List<Integer> beam_f20916_0(Iterable<List<Integer>> accumulators)
{    List<Integer> result = new ArrayList<>();    for (List<Integer> accum : accumulators) {        result.addAll(accum);    }    return result;}
public Integer beam_f20917_0(List<Integer> accumulator)
{    int value = 0;    for (int i : accumulator) {        value += i;    }    return value;}
public void beam_f20918_0()
{    final AtomicBoolean matcherUsed = new AtomicBoolean();    Matcher<Integer> matcher = new TypeSafeMatcher<Integer>() {        @Override        public void describeTo(Description description) {        }        @Override        protected boolean matchesSafely(Integer item) {            matcherUsed.set(true);            return item == 30;        }    };    CombineFnTester.testCombineFn(Sum.ofIntegers(), Arrays.asList(1, 1, 2, 2, 3, 3, 4, 4, 5, 5), matcher);    assertThat(matcherUsed.get(), is(true));    try {        CombineFnTester.testCombineFn(Sum.ofIntegers(), Arrays.asList(1, 2, 3, 4, 5), Matchers.not(Matchers.equalTo(15)));    } catch (AssertionError ignored) {                return;    }    fail("The matcher should have failed, throwing an error");}
public void beam_f20927_0(String substring, Throwable t)
{    verify(Level.FINEST, substring, t);}
public void beam_f20928_0(String substring)
{    verify(Level.FINE, substring);}
public void beam_f20929_0(String message, Throwable t)
{    verify(Level.FINE, message, t);}
public void beam_f20937_0(String substring, Throwable t)
{    verifyNo(Level.SEVERE, substring, t);}
public void beam_f20938_0(Matcher<Iterable<LogRecord>> matcher)
{    if (!matcher.matches(logSaver.getLogs())) {        fail(String.format("Missing match for [%s]", matcher));    }}
private void beam_f20939_0(final Level level, final String substring)
{    verifyLogged(matcher(level, substring));}
private void beam_f20947_0(final Level level, final String substring, final Throwable throwable)
{    verifyNotLogged(matcher(level, substring, throwable));}
private TypeSafeMatcher<LogRecord> beam_f20948_0(final Level level, final String substring, final Throwable throwable)
{    return new TypeSafeMatcher<LogRecord>() {        @Override        public void describeTo(Description description) {            description.appendText(String.format("log message of level [%s] containg message [%s] with exception [%s] " + "containing message [%s]", level, substring, throwable.getClass(), throwable.getMessage()));        }        @Override        protected boolean matchesSafely(LogRecord item) {            return level.equals(item.getLevel()) && item.getMessage().contains(substring) && item.getThrown().getClass().equals(throwable.getClass()) && item.getThrown().getMessage().contains(throwable.getMessage());        }    };}
public void beam_f20949_0(Description description)
{    description.appendText(String.format("log message of level [%s] containg message [%s] with exception [%s] " + "containing message [%s]", level, substring, throwable.getClass(), throwable.getMessage()));}
private void beam_f20959_0()
{    logRecords.clear();}
public void beam_f20960_1() throws Throwable
{    }
public void beam_f20961_1() throws Throwable
{    String expected = generateRandomString();        expectedLogs.verifyError(expected);}
public void beam_f20970_1() throws Throwable
{    String expected = generateRandomString();        expectedLogs.verifyNotLogged(expected);}
public void beam_f20971_0() throws Throwable
{    String expected = generateRandomString();    expectedLogs.verifyNotLogged(expected);}
public void beam_f20972_0() throws Throwable
{    String expected = generateRandomString();    LOG.trace(expected);    expectedLogs.verifyTrace(expected);}
public void beam_f20980_0() throws IOException
{    String tmpPath = tmpFolder.newFile().getPath();    thrown.expect(NullPointerException.class);    thrown.expectMessage(containsString("Expected non-null shard pattern. " + "Please call the other constructor to use default pattern:"));    new FileChecksumMatcher("checksumString", tmpPath, null);}
public void beam_f20981_0() throws IOException
{    File tmpFile = tmpFolder.newFile("result-000-of-001");    Files.write("Test for file checksum verifier.", tmpFile, StandardCharsets.UTF_8);    FileChecksumMatcher matcher = new FileChecksumMatcher("a8772322f5d7b851777f820fc79d050f9d302915", tmpFile.getPath());    assertThat(pResult, matcher);}
public void beam_f20982_0() throws IOException
{    File tmpFile1 = tmpFolder.newFile("result-000-of-002");    File tmpFile2 = tmpFolder.newFile("result-001-of-002");    File tmpFile3 = tmpFolder.newFile("tmp");    Files.write("To be or not to be, ", tmpFile1, StandardCharsets.UTF_8);    Files.write("it is not a question.", tmpFile2, StandardCharsets.UTF_8);    Files.write("tmp", tmpFile3, StandardCharsets.UTF_8);    FileChecksumMatcher matcher = new FileChecksumMatcher("90552392c28396935fe4f123bd0b5c2d0f6260c8", tmpFolder.getRoot().toPath().resolve("result-*").toString());    assertThat(pResult, matcher);}
public void beam_f20990_0()
{    SerializableFunction<Iterable<ValueInSingleWindow<Integer>>, Iterable<Integer>> extractor = PaneExtractors.onlyPane(PAssert.PAssertionSite.capture(""));    Iterable<ValueInSingleWindow<Integer>> multipleFiring = ImmutableList.of(ValueInSingleWindow.of(4, new Instant(0L), GlobalWindow.INSTANCE, PaneInfo.createPane(true, false, Timing.EARLY)), ValueInSingleWindow.of(2, new Instant(0L), GlobalWindow.INSTANCE, PaneInfo.createPane(false, false, Timing.ON_TIME, 1L, 0L)), ValueInSingleWindow.of(1, new Instant(0L), GlobalWindow.INSTANCE, PaneInfo.createPane(false, false, Timing.LATE, 2L, 1L)));    thrown.expectMessage("trigger that fires at most once");    extractor.apply(multipleFiring);}
public void beam_f20991_0()
{    SerializableFunction<Iterable<ValueInSingleWindow<Integer>>, Iterable<Integer>> extractor = PaneExtractors.onTimePane();    Iterable<ValueInSingleWindow<Integer>> onlyOnTime = ImmutableList.of(ValueInSingleWindow.of(4, new Instant(0L), GlobalWindow.INSTANCE, PaneInfo.createPane(false, false, Timing.ON_TIME, 1L, 0L)), ValueInSingleWindow.of(2, new Instant(0L), GlobalWindow.INSTANCE, PaneInfo.createPane(false, false, Timing.ON_TIME, 1L, 0L)));    assertThat(extractor.apply(onlyOnTime), containsInAnyOrder(2, 4));}
public void beam_f20992_0()
{    SerializableFunction<Iterable<ValueInSingleWindow<Integer>>, Iterable<Integer>> extractor = PaneExtractors.onTimePane();    Iterable<ValueInSingleWindow<Integer>> onlyOnTime = ImmutableList.of(ValueInSingleWindow.of(8, new Instant(0L), GlobalWindow.INSTANCE, PaneInfo.createPane(false, false, Timing.LATE, 2L, 1L)), ValueInSingleWindow.of(4, new Instant(0L), GlobalWindow.INSTANCE, PaneInfo.createPane(false, false, Timing.ON_TIME, 1L, 0L)), ValueInSingleWindow.of(2, new Instant(0L), GlobalWindow.INSTANCE, PaneInfo.createPane(false, false, Timing.ON_TIME, 1L, 0L)), ValueInSingleWindow.of(1, new Instant(0L), GlobalWindow.INSTANCE, PaneInfo.createPane(true, false, Timing.EARLY)));    assertThat(extractor.apply(onlyOnTime), containsInAnyOrder(2, 4));}
public void beam_f21000_0()
{    SerializableFunction<Iterable<ValueInSingleWindow<Integer>>, Iterable<Integer>> extractor = PaneExtractors.allPanes();    Iterable<ValueInSingleWindow<Integer>> onlyOnTime = ImmutableList.of(ValueInSingleWindow.of(8, new Instant(0L), GlobalWindow.INSTANCE, PaneInfo.ON_TIME_AND_ONLY_FIRING), ValueInSingleWindow.of(4, new Instant(0L), GlobalWindow.INSTANCE, PaneInfo.ON_TIME_AND_ONLY_FIRING), ValueInSingleWindow.of(2, new Instant(0L), GlobalWindow.INSTANCE, PaneInfo.ON_TIME_AND_ONLY_FIRING));    assertThat(extractor.apply(onlyOnTime), containsInAnyOrder(2, 4, 8));}
public void beam_f21001_0()
{    SerializableFunction<Iterable<ValueInSingleWindow<Integer>>, Iterable<Integer>> extractor = PaneExtractors.allPanes();    Iterable<ValueInSingleWindow<Integer>> onlyOnTime = ImmutableList.of(ValueInSingleWindow.of(8, new Instant(0L), GlobalWindow.INSTANCE, PaneInfo.createPane(false, false, Timing.LATE, 2L, 1L)), ValueInSingleWindow.of(4, new Instant(0L), GlobalWindow.INSTANCE, PaneInfo.createPane(false, false, Timing.ON_TIME, 1L, 0L)), ValueInSingleWindow.of(1, new Instant(0L), GlobalWindow.INSTANCE, PaneInfo.createPane(true, false, Timing.EARLY)));    assertThat(extractor.apply(onlyOnTime), containsInAnyOrder(4, 8, 1));}
public void beam_f21002_0()
{    SerializableFunction<Iterable<ValueInSingleWindow<Integer>>, Iterable<Integer>> extractor = PaneExtractors.allPanes();    Iterable<ValueInSingleWindow<Integer>> noPanes = ImmutableList.of();    assertThat(extractor.apply(noPanes), emptyIterable());}
private void beam_f21011_0()
{    try {        throwNestedError();    } catch (Exception e) {        throw new RuntimeException("Wrapped error", e);    }}
public void beam_f21012_0() throws IOException
{    Throwable error;    try {        throwWrappedError();        throw new IllegalStateException("Should have failed");    } catch (Throwable e) {        error = e;    }    SuccessOrFailure failure = SuccessOrFailure.failure(PAssert.PAssertionSite.capture("here"), error);    SuccessOrFailure res = CoderUtils.clone(SerializableCoder.of(SuccessOrFailure.class), failure);    assertEquals("Encode-decode failed SuccessOrFailure", Throwables.getStackTraceAsString(failure.assertionError()), Throwables.getStackTraceAsString(res.assertionError()));}
public void beam_f21013_0() throws IOException
{    SuccessOrFailure success = SuccessOrFailure.success();    SerializableCoder<SuccessOrFailure> coder = SerializableCoder.of(SuccessOrFailure.class);    byte[] encoded = CoderUtils.encodeToByteArray(coder, success);    SuccessOrFailure res = CoderUtils.decodeFromByteArray(coder, encoded);    assertEquals("Encode-decode successful SuccessOrFailure", success.isSuccess(), res.isSuccess());    assertEquals("Encode-decode successful SuccessOrFailure", success.assertionError(), res.assertionError());}
public void beam_f21021_0() throws Exception
{    PCollection<Integer> pcollection = pipeline.apply(Create.of(43));    PAssert.thatSingleton(pcollection).isEqualTo(43);    pipeline.run();}
public void beam_f21022_0() throws Exception
{    PCollection<Integer> pcollection = pipeline.apply(Create.timestamped(TimestampedValue.of(43, new Instant(250L)), TimestampedValue.of(22, new Instant(-250L)))).apply(Window.into(FixedWindows.of(Duration.millis(500L)))).apply(WithKeys.of(0)).apply(GroupByKey.create()).apply(ParDo.of(new DoFn<KV<Integer, Iterable<Integer>>, Integer>() {        @ProcessElement        public void processElement(ProcessContext ctxt) {            for (Integer integer : ctxt.element().getValue()) {                ctxt.output(integer);            }        }    }));    PAssert.thatSingleton(pcollection).inOnlyPane(new IntervalWindow(new Instant(0L), new Instant(500L))).isEqualTo(43);    PAssert.thatSingleton(pcollection).inOnlyPane(new IntervalWindow(new Instant(-500L), new Instant(0L))).isEqualTo(22);    pipeline.run();}
public void beam_f21023_0(ProcessContext ctxt)
{    for (Integer integer : ctxt.element().getValue()) {        ctxt.output(integer);    }}
public void beam_f21031_0() throws Exception
{    PCollection<Integer> pcollection = pipeline.apply(Create.of(1, 2, 3, 4));    PAssert.that(pcollection).containsInAnyOrder(2, 1, 4, 3, 7);    Throwable exc = runExpectingAssertionFailure(pipeline);    Pattern expectedPattern = Pattern.compile("Expected: iterable with items \\[((<4>|<7>|<3>|<2>|<1>)(, )?){5}\\] in any order");        assertTrue("Expected error message from PAssert with substring matching " + expectedPattern + " but the message was \"" + exc.getMessage() + "\"", expectedPattern.matcher(exc.getMessage()).find());}
public void beam_f21032_0() throws Exception
{    PCollection<Long> vals = pipeline.apply(GenerateSequence.from(0).to(5));    PAssert.that("Vals should have been empty", vals).empty();    Throwable thrown = runExpectingAssertionFailure(pipeline);    String message = thrown.getMessage();    assertThat(message, containsString("Vals should have been empty"));    assertThat(message, containsString("Expected: iterable with items [] in any order"));}
public void beam_f21033_0() throws Exception
{    PCollection<Long> vals = pipeline.apply(GenerateSequence.from(0).to(5));    PAssert.that(vals).empty();    Throwable thrown = runExpectingAssertionFailure(pipeline);    String message = thrown.getMessage();    assertThat(message, containsString("GenerateSequence/Read(BoundedCountingSource).out"));    assertThat(message, containsString("Expected: iterable with items [] in any order"));}
public void beam_f21041_0()
{    PCollection<Integer> create = pipeline.apply("FirstCreate", Create.of(1, 2, 3));    PAssert.that(create).containsInAnyOrder(1, 2, 3);    PAssert.thatSingleton(create.apply(Sum.integersGlobally())).isEqualTo(6);    assertThat(PAssert.countAsserts(pipeline), equalTo(2));    PAssert.thatMap(pipeline.apply("CreateMap", Create.of(KV.of(1, 2)))).isEqualTo(Collections.singletonMap(1, 2));    assertThat(PAssert.countAsserts(pipeline), equalTo(3));}
public static List<Object> beam_f21042_0(PTransform<?, ? extends PCollectionView<?>> viewTransformClass, Object... values)
{    List<Object> rval = new ArrayList<>();        if (View.AsSingleton.class.equals(viewTransformClass.getClass())) {        for (Object value : values) {            rval.add(KV.of(null, value));        }    } else if (View.AsIterable.class.equals(viewTransformClass.getClass())) {        for (Object value : values) {            rval.add(KV.of(null, value));        }    } else if (View.AsList.class.equals(viewTransformClass.getClass())) {        for (Object value : values) {            rval.add(KV.of(null, value));        }    } else if (View.AsMap.class.equals(viewTransformClass.getClass())) {        for (Object value : values) {            rval.add(KV.of(null, value));        }    } else if (View.AsMultimap.class.equals(viewTransformClass.getClass())) {        for (Object value : values) {            rval.add(KV.of(null, value));        }    } else {        throw new IllegalArgumentException(String.format("Unknown type of view %s. Supported views are %s.", viewTransformClass.getClass(), ImmutableSet.of(View.AsSingleton.class, View.AsIterable.class, View.AsList.class, View.AsMap.class, View.AsMultimap.class)));    }    return Collections.unmodifiableList(rval);}
public void beam_f21043_0(String iso8601)
{    setDateTimeFixed(ISODateTimeFormat.dateTime().parseMillis(iso8601));}
public void beam_f21051_0()
{    System.getProperties().put("TestB", "TestB");    assertNotNull(System.getProperty("TestB"));    assertNull(System.getProperty("TestA"));}
public void beam_f21052_0() throws Exception
{    SerializableUtils.ensureSerializable(anything());}
public void beam_f21053_0() throws Exception
{    SerializableUtils.ensureSerializable(allOf(anything()));}
public void beam_f21061_0() throws Exception
{    AssertionError exc = assertionShouldFail(() -> assertThat(KV.of("key", ImmutableList.of(1, 2, 3)), SerializableMatchers.<String, Iterable<Integer>>kv(anything(), containsInAnyOrder(1, 2, 3, 4))));    assertThat(exc.getMessage(), Matchers.containsString("value did not match"));}
private static AssertionError beam_f21062_0(Runnable runnable)
{    try {        runnable.run();    } catch (AssertionError expected) {        return expected;    }    throw new AssertionError("Should have failed.");}
public boolean beam_f21063_0(Object other)
{    return other instanceof NotSerializableClass;}
public void beam_f21072_0()
{    thrown.expect(IllegalArgumentException.class);    thrown.expectMessage("may not be empty");    StaticWindows.of(GlobalWindow.Coder.INSTANCE, ImmutableList.of());}
public void beam_f21073_0() throws IncompatibleWindowException
{    StaticWindows staticWindows = StaticWindows.of(IntervalWindow.getCoder(), ImmutableList.of(first, second));    staticWindows.verifyCompatibility(StaticWindows.of(IntervalWindow.getCoder(), ImmutableList.of(first, second)));    thrown.expect(IncompatibleWindowException.class);    staticWindows.verifyCompatibility(StaticWindows.of(IntervalWindow.getCoder(), ImmutableList.of(first)));}
public void beam_f21074_0(long millis) throws InterruptedException
{    long currentTime;    long endTime = System.nanoTime() + TimeUnit.NANOSECONDS.convert(millis, TimeUnit.MILLISECONDS);    while ((currentTime = System.nanoTime()) < endTime) {        if (Thread.interrupted()) {            throw new InterruptedException();        }        LockSupport.parkNanos(endTime - currentTime);    }    if (Thread.interrupted()) {        throw new InterruptedException();    }}
public void beam_f21082_0() throws Exception
{    String stringOptions = MAPPER.writeValueAsString(new String[] { "--runner=org.apache.beam.sdk.testing.CrashingRunner" });    System.getProperties().put("beamTestPipelineOptions", stringOptions);    PipelineOptions options = TestPipeline.testingPipelineOptions();    assertEquals(CrashingRunner.class, options.getRunner());}
public void beam_f21083_0() throws Exception
{    PipelineOptions options = pipeline.getOptions();    assertThat(options.as(ApplicationNameOptions.class).getAppName(), startsWith("TestPipelineTest$TestPipelineCreationTest" + "-testCreationOfPipelineOptionsFromReallyVerboselyNamedTestCase"));}
public void beam_f21084_0()
{    assertEquals("TestPipeline#TestPipelineTest$TestPipelineCreationTest-testToString", pipeline.toString());}
private static PCollection<String> beam_f21092_0(final Pipeline pipeline)
{    return pipeline.apply("Create", Create.of(WORDS).withCoder(StringUtf8Coder.of())).apply("Map1", MapElements.via(new SimpleFunction<String, String>() {        @Override        public String apply(final String input) {            return WHATEVER;        }    }));}
public String beam_f21093_0(final String input)
{    return WHATEVER;}
public void beam_f21094_0() throws Exception
{    addTransform(pCollection(pipeline));    pipeline.run();}
public void beam_f21105_0()
{    ValueProvider<String> foo = pipeline.newProvider("foo");    ValueProvider<String> foobar = ValueProvider.NestedValueProvider.of(foo, input -> input + "bar");    assertFalse(foo.isAccessible());    assertFalse(foobar.isAccessible());    PAssert.that(pipeline.apply("create foo", Create.ofProvider(foo, StringUtf8Coder.of()))).containsInAnyOrder("foo");    PAssert.that(pipeline.apply("create foobar", Create.ofProvider(foobar, StringUtf8Coder.of()))).containsInAnyOrder("foobar");    pipeline.run();}
public void beam_f21106_0()
{    Instant instant = new Instant(0);    TestStream<Integer> source = TestStream.create(VarIntCoder.of()).addElements(TimestampedValue.of(1, instant), TimestampedValue.of(2, instant), TimestampedValue.of(3, instant)).advanceWatermarkTo(instant.plus(Duration.standardMinutes(6))).addElements(TimestampedValue.of(4, instant), TimestampedValue.of(5, instant)).advanceWatermarkTo(instant.plus(Duration.standardMinutes(20))).addElements(TimestampedValue.of(-1, instant), TimestampedValue.of(-2, instant), TimestampedValue.of(-3, instant)).advanceWatermarkToInfinity();    PCollection<Integer> windowed = p.apply(source).apply(Window.<Integer>into(FixedWindows.of(Duration.standardMinutes(5))).triggering(AfterWatermark.pastEndOfWindow().withEarlyFirings(AfterProcessingTime.pastFirstElementInPane().plusDelayOf(Duration.standardMinutes(2))).withLateFirings(AfterPane.elementCountAtLeast(1))).accumulatingFiredPanes().withAllowedLateness(Duration.standardMinutes(5), ClosingBehavior.FIRE_ALWAYS));    PCollection<Integer> triggered = windowed.apply(WithKeys.of(1)).apply(GroupByKey.create()).apply(Values.create()).apply(Flatten.iterables());    PCollection<Long> count = windowed.apply(Combine.globally(Count.<Integer>combineFn()).withoutDefaults());    PCollection<Integer> sum = windowed.apply(Sum.integersGlobally().withoutDefaults());    IntervalWindow window = new IntervalWindow(instant, instant.plus(Duration.standardMinutes(5L)));    PAssert.that(triggered).inFinalPane(window).containsInAnyOrder(1, 2, 3, 4, 5);    PAssert.that(triggered).inOnTimePane(window).containsInAnyOrder(1, 2, 3);    PAssert.that(count).inWindow(window).satisfies(input -> {        for (Long count1 : input) {            assertThat(count1, allOf(greaterThanOrEqualTo(3L), lessThanOrEqualTo(5L)));        }        return null;    });    PAssert.that(sum).inWindow(window).satisfies(input -> {        for (Integer sum1 : input) {            assertThat(sum1, allOf(greaterThanOrEqualTo(6), lessThanOrEqualTo(15)));        }        return null;    });    p.run();}
public void beam_f21107_0()
{    TestStream<Long> source = TestStream.create(VarLongCoder.of()).addElements(TimestampedValue.of(1L, new Instant(1000L)), TimestampedValue.of(2L, new Instant(2000L))).advanceProcessingTime(Duration.standardMinutes(12)).addElements(TimestampedValue.of(3L, new Instant(3000L))).advanceProcessingTime(Duration.standardMinutes(6)).advanceWatermarkToInfinity();    PCollection<Long> sum = p.apply(source).apply(Window.<Long>configure().triggering(AfterWatermark.pastEndOfWindow().withEarlyFirings(AfterProcessingTime.pastFirstElementInPane().plusDelayOf(Duration.standardMinutes(5)))).accumulatingFiredPanes().withAllowedLateness(Duration.ZERO)).apply(Sum.longsGlobally());    PAssert.that(sum).inEarlyGlobalWindowPanes().containsInAnyOrder(3L, 6L);    p.run();}
public void beam_f21115_0()
{    TestStream<Long> source = TestStream.create(VarLongCoder.of()).addElements(TimestampedValue.of(1L, new Instant(1000L))).advanceProcessingTime(    Duration.standardMinutes(6)).addElements(TimestampedValue.of(2L, new Instant(2000L))).advanceProcessingTime(    Duration.standardMinutes(6)).addElements(TimestampedValue.of(3L, new Instant(3000L))).advanceProcessingTime(    Duration.standardMinutes(6)).advanceWatermarkToInfinity();    PCollection<KV<String, Long>> sum = p.apply(source).apply(Window.<Long>into(FixedWindows.of(Duration.standardMinutes(30))).triggering(AfterWatermark.pastEndOfWindow().withEarlyFirings(AfterProcessingTime.pastFirstElementInPane().plusDelayOf(Duration.standardMinutes(5)))).accumulatingFiredPanes().withAllowedLateness(Duration.ZERO)).apply(MapElements.into(TypeDescriptors.kvs(TypeDescriptors.strings(), TypeDescriptors.longs())).via(v -> KV.of("key", v))).apply(Sum.longsPerKey());    IntervalWindow window = new IntervalWindow(new Instant(0L), new Instant(0L).plus(Duration.standardMinutes(30)));    PAssert.that(sum).inEarlyPane(window).satisfies(input -> {        assertThat(StreamSupport.stream(input.spliterator(), false).count(), is(3L));        return null;    }).containsInAnyOrder(KV.of("key", 1L), KV.of("key", 3L), KV.of("key", 6L)).inOnTimePane(window).satisfies(input -> {        assertThat(StreamSupport.stream(input.spliterator(), false).count(), is(1L));        return null;    }).containsInAnyOrder(KV.of("key", 6L));    p.run().waitUntilFinish();}
public void beam_f21116_0() throws Exception
{    TestStream<String> testStream = TestStream.create(StringUtf8Coder.of()).addElements("hey").advanceWatermarkTo(Instant.ofEpochMilli(22521600)).advanceProcessingTime(Duration.millis(42)).addElements("hey", "joe").advanceWatermarkToInfinity();    TestStream.TestStreamCoder<String> coder = TestStream.TestStreamCoder.of(StringUtf8Coder.of());    byte[] bytes = CoderUtils.encodeToByteArray(coder, testStream);    TestStream<String> recoveredStream = CoderUtils.decodeFromByteArray(coder, bytes);    assertThat(recoveredStream, is(testStream));}
public Statement beam_f21117_0(final Statement base, final Description description)
{    return new Statement() {        @Override        public void evaluate() throws Throwable {            final Thread thread = Thread.currentThread();            final ThreadGroup threadGroup = thread.getThreadGroup();            final ThreadGroup testGroup = new ThreadGroup(threadGroup, Long.toString(thread.getId()));            groupField.set(thread, testGroup);            try {                base.evaluate();            } finally {                groupField.set(thread, threadGroup);                final Thread[] threads = listThreads();                final List<Thread> leaked = Stream.of(threads).filter(t -> t.getThreadGroup() == testGroup).collect(toList());                if (!leaked.isEmpty()) {                    fail("Some threads leaked: " + leaked.stream().map(Thread::getName).collect(joining("\n- ", "\n- ", "")));                }            }        }    };}
public void beam_f21125_0()
{    thrown.expect(IllegalArgumentException.class);    thrown.expectMessage("Could not encode");    WindowSupplier.of(new FailingCoder(), Collections.singleton(window));}
public void beam_f21126_0(BoundedWindow value, OutputStream outStream) throws CoderException, IOException
{    throw new CoderException("Test Encode Exception");}
public BoundedWindow beam_f21127_0(InputStream inStream) throws CoderException, IOException
{    throw new CoderException("Test Decode Exception");}
public void beam_f21135_0()
{    PCollection<KV<String, Integer>> input = createInputTable(p);    PCollection<KV<String, List<Integer>>> quantiles = input.apply(ApproximateQuantiles.perKey(2, new DescendingIntComparator()));    PAssert.that(quantiles).containsInAnyOrder(KV.of("a", Arrays.asList(3, 1)), KV.of("b", Arrays.asList(100, 1)));    p.run();}
public void beam_f21136_0()
{    testCombineFn(ApproximateQuantilesCombineFn.create(5), Arrays.asList(389), Arrays.asList(389, 389, 389, 389, 389));}
public void beam_f21137_0()
{    testCombineFn(ApproximateQuantilesCombineFn.create(5), intRange(101), Arrays.asList(0, 25, 50, 75, 100));}
public void beam_f21145_0()
{    List<String> inputs = Arrays.asList("aa", "aaa", "aaaa", "b", "ccccc", "dddd", "zz");    testCombineFn(ApproximateQuantilesCombineFn.create(3), inputs, Arrays.asList("aa", "b", "zz"));    testCombineFn(ApproximateQuantilesCombineFn.create(3, new OrderByLength()), inputs, Arrays.asList("b", "aaa", "ccccc"));}
public void beam_f21146_0()
{    Top.Natural<Integer> comparer = new Top.Natural<>();    PTransform<?, ?> approxQuanitiles = ApproximateQuantiles.globally(20, comparer);    DisplayData displayData = DisplayData.from(approxQuanitiles);    assertThat(displayData, hasDisplayItem("numQuantiles", 20));    assertThat(displayData, hasDisplayItem("comparer", comparer.getClass()));}
private Matcher<Iterable<? extends Integer>> beam_f21147_0(int size, int numQuantiles, int absoluteError)
{    List<Matcher<? super Integer>> quantiles = new ArrayList<>();    quantiles.add(CoreMatchers.is(0));    for (int k = 1; k < numQuantiles - 1; k++) {        int expected = (int) (((double) (size - 1)) * k / (numQuantiles - 1));        quantiles.add(new Between<>(expected - absoluteError, expected + absoluteError));    }    quantiles.add(CoreMatchers.is(size - 1));    return contains(quantiles);}
public void beam_f21155_0()
{    assertEquals("Number of buffers", expectedNumBuffers, combineFn.getNumBuffers());    assertEquals("Buffer size", expectedBufferSize, combineFn.getBufferSize());}
public void beam_f21156_0()
{    int b = combineFn.getNumBuffers();    int k = combineFn.getBufferSize();    long n = this.maxInputSize;    assertThat("(b-2)2^(b-2) + 1/2 <= eN", (b - 2) * (1 << (b - 2)) + 0.5, Matchers.lessThanOrEqualTo(this.epsilon * n));    assertThat("k2^(b-1) >= N", Math.pow(k * 2, b - 1), Matchers.greaterThanOrEqualTo((double) n));}
public Void beam_f21157_0(final Long estimate)
{    verifyEstimate(uniqueCount, sampleSize, estimate);    return null;}
private void beam_f21165_0(final int sampleSize)
{    final PCollection<String> input = p.apply(Create.of(TEST_LINES));    final PCollection<Long> approximate = input.apply(ApproximateUnique.globally(sampleSize));    final PCollectionView<Long> exact = input.apply(Distinct.create()).apply(Count.globally()).apply(View.asSingleton());    final PCollection<KV<Long, Long>> approximateAndExact = approximate.apply(ParDo.of(new DoFn<Long, KV<Long, Long>>() {        @ProcessElement        public void processElement(final ProcessContext c) {            c.output(KV.of(c.element(), c.sideInput(exact)));        }    }).withSideInputs(exact));    PAssert.that(approximateAndExact).satisfies(new VerifyEstimatePerKeyFn(sampleSize));    p.run();}
public void beam_f21166_0(final ProcessContext c)
{    c.output(KV.of(c.element(), c.sideInput(exact)));}
public void beam_f21167_0()
{    if (sampleSize > 16) {        runApproximateUniquePipeline(sampleSize);    } else {        try {            p.enableAbandonedNodeEnforcement(false);            runApproximateUniquePipeline(15);            fail("Accepted sampleSize < 16");        } catch (final IllegalArgumentException e) {            assertTrue("Expected an exception due to sampleSize < 16", e.getMessage().startsWith("ApproximateUnique needs a sampleSize >= 16"));        }    }}
private void beam_f21175_0(final int elementCount, final int uniqueCount, final int sampleSize)
{    final List<Integer> elements = Lists.newArrayList();        final double s = 1 - 1.0 * uniqueCount / elementCount;    final double maxCount = Math.pow(uniqueCount, s);    for (int k = 0; k < uniqueCount; k++) {        final int count = Math.max(1, (int) Math.round(maxCount * Math.pow(k, -s)));                for (int c = 0; c < count; c++) {            elements.add(k);        }    }    final PCollection<Integer> input = p.apply(Create.of(elements));    final PCollection<Long> estimate = input.apply(ApproximateUnique.globally(sampleSize));    PAssert.thatSingleton(estimate).satisfies(new VerifyEstimateFn(uniqueCount, sampleSize));    p.run();}
public void beam_f21176_0()
{    final List<KV<Long, Long>> elements = Lists.newArrayList();    final List<Long> keys = ImmutableList.of(20L, 50L, 100L);    final int elementCount = 1000;    final int sampleSize = 100;        for (final long uniqueCount : keys) {        for (long value = 0; value < elementCount; value++) {            elements.add(KV.of(uniqueCount, value % uniqueCount));        }    }    final PCollection<KV<Long, Long>> input = p.apply(Create.of(elements));    final PCollection<KV<Long, Long>> counts = input.apply(ApproximateUnique.perKey(sampleSize));    PAssert.that(counts).satisfies(new VerifyEstimatePerKeyFn(sampleSize));    p.run();}
public void beam_f21177_0()
{    assertEquals("ApproximateUnique.PerKey", ApproximateUnique.<Long, Long>perKey(16).getName());    assertEquals("ApproximateUnique.Globally", ApproximateUnique.<Integer>globally(16).getName());}
public void beam_f21185_0()
{    SimpleFunction<String, String> extractFn = new SimpleFunction<String, String>() {        @Override        public String apply(String input) {            return input;        }    };    DisplayDataCombineFn combineFn1 = new DisplayDataCombineFn("value1");    DisplayDataCombineFn combineFn2 = new DisplayDataCombineFn("value2");    CombineFns.ComposedCombineFn<String> composedCombine = CombineFns.compose().with(extractFn, combineFn1, new TupleTag<>()).with(extractFn, combineFn2, new TupleTag<>());    DisplayData displayData = DisplayData.from(composedCombine);    assertThat(displayData, hasDisplayItem("combineFn1", combineFn1.getClass()));    assertThat(displayData, hasDisplayItem("combineFn2", combineFn2.getClass()));    assertThat(displayData, includesDisplayDataFor("combineFn1", combineFn1));    assertThat(displayData, includesDisplayDataFor("combineFn2", combineFn2));}
public String beam_f21186_0(String input)
{    return input;}
public String beam_f21187_0()
{    return null;}
public static UserStringCoder beam_f21195_0()
{    return INSTANCE;}
public void beam_f21196_0(UserString value, OutputStream outStream) throws CoderException, IOException
{    encode(value, outStream, Context.NESTED);}
public void beam_f21197_0(UserString value, OutputStream outStream, Context context) throws CoderException, IOException
{    StringUtf8Coder.of().encode(value.strValue, outStream, context);}
public UserString beam_f21206_0(CombineWithContext.Context c)
{    return UserString.of(c.sideInput(view));}
public UserString beam_f21207_0(UserString accumulator, UserString input, CombineWithContext.Context c)
{    assertThat(accumulator.strValue, Matchers.startsWith(c.sideInput(view)));    accumulator.strValue += input.strValue;    return accumulator;}
public UserString beam_f21208_0(Iterable<UserString> accumulators, CombineWithContext.Context c)
{    String keyPrefix = c.sideInput(view);    String all = keyPrefix;    for (UserString accumulator : accumulators) {        assertThat(accumulator.strValue, Matchers.startsWith(keyPrefix));        all += accumulator.strValue.substring(keyPrefix.length());        accumulator.strValue = "cleared in mergeAccumulators";    }    return UserString.of(all);}
public void beam_f21216_0(Counter accumulator)
{    checkState(outputs == 0);    assertEquals(0, accumulator.outputs);    merges += accumulator.merges + 1;    inputs += accumulator.inputs;    sum += accumulator.sum;}
public Iterable<Long> beam_f21217_0()
{    checkState(outputs == 0);    return Arrays.asList(sum, inputs, merges, outputs);}
public int beam_f21218_0()
{    return (int) (sum * 17 + inputs * 31 + merges * 43 + outputs * 181);}
public Coder<Accumulator> beam_f21226_0(CoderRegistry registry, Coder<Integer> inputCoder)
{    return Accumulator.getCoder();}
public Accumulator beam_f21227_0()
{    return new Accumulator("", "");}
public Accumulator beam_f21228_0(Accumulator accumulator, Integer value)
{    try {        return new Accumulator(accumulator.seed, accumulator.value + String.valueOf(value));    } finally {        accumulator.value = "cleared in addInput";    }}
public void beam_f21236_0(ProcessContext c)
{    c.output(c.element() + ": " + c.pane().isLast());}
public void beam_f21237_0(ProcessContext c)
{    if (c.pane().isLast()) {        c.output(c.element());    }}
public int beam_f21238_0(int left, int right)
{    return left * right;}
public Set<Integer> beam_f21246_0(Set<Integer> accumulator)
{    return accumulator;}
public void beam_f21247_0(Integer element)
{    count++;    sum += element.doubleValue();}
public void beam_f21248_0(CountSum accumulator)
{    count += accumulator.count;    sum += accumulator.sum;}
public CountSum beam_f21256_0(InputStream inStream) throws IOException
{    long count = LONG_CODER.decode(inStream);    double sum = DOUBLE_CODER.decode(inStream);    return new CountSum(count, sum);}
public boolean beam_f21258_0(CountSum value)
{    return true;}
public void beam_f21259_0(CountSum value, ElementByteSizeObserver observer) throws Exception
{    LONG_CODER.registerByteSizeObserver(value.count, observer);    DOUBLE_CODER.registerByteSizeObserver(value.sum, observer);}
public void beam_f21267_0()
{    runTestBasicCombine(Arrays.asList(KV.of("a", 1), KV.of("a", 1), KV.of("a", 4), KV.of("b", 1), KV.of("b", 13)), ImmutableSet.of(1, 13, 4), Arrays.asList(KV.of("a", (Set<Integer>) ImmutableSet.of(1, 4)), KV.of("b", (Set<Integer>) ImmutableSet.of(1, 13))));}
public void beam_f21268_0()
{    runTestBasicCombine(EMPTY_TABLE, ImmutableSet.of(), Collections.emptyList());}
public void beam_f21269_0()
{    Combine.PerKey<String, Integer, Integer> min = Min.integersPerKey();    Combine.PerKey<String, Integer, Integer> max = Max.integersPerKey();    Combine.PerKey<String, Integer, Double> mean = Mean.perKey();    Combine.PerKey<String, Integer, Integer> sum = Sum.integersPerKey();    assertThat(min.getName(), equalTo("Combine.perKey(MinInteger)"));    assertThat(max.getName(), equalTo("Combine.perKey(MaxInteger)"));    assertThat(mean.getName(), equalTo("Combine.perKey(Mean)"));    assertThat(sum.getName(), equalTo("Combine.perKey(SumInteger)"));}
public void beam_f21277_0()
{    UniqueInts combineFn = new UniqueInts() {        @Override        public void populateDisplayData(DisplayData.Builder builder) {            builder.add(DisplayData.item("foo", "bar"));        }    };    Combine.PerKey<?, ?, ?> combine = Combine.perKey(combineFn);    DisplayData displayData = DisplayData.from(combine);    assertThat(displayData, hasDisplayItem("combineFn", combineFn.getClass()));    assertThat(displayData, hasDisplayItem(hasNamespace(combineFn.getClass())));}
public void beam_f21278_0(DisplayData.Builder builder)
{    builder.add(DisplayData.item("foo", "bar"));}
public void beam_f21279_0()
{    DisplayDataEvaluator evaluator = DisplayDataEvaluator.create();    UniqueInts combineFn = new UniqueInts();    PTransform<PCollection<KV<Integer, Integer>>, ? extends POutput> combine = Combine.perKey(combineFn);    Set<DisplayData> displayData = evaluator.displayDataForPrimitiveTransforms(combine, KvCoder.of(VarIntCoder.of(), VarIntCoder.of()));    assertThat("Combine.perKey should include the combineFn in its primitive transform", displayData, hasItem(hasDisplayItem("combineFn", combineFn.getClass())));}
public void beam_f21287_0()
{    runTestSimpleCombineWithContext(Arrays.asList(KV.of("a", 1), KV.of("a", 1), KV.of("a", 4), KV.of("b", 1), KV.of("b", 13)), 20, Arrays.asList(KV.of("a", "20:114"), KV.of("b", "20:113")), new String[] { "20:111134" });}
public void beam_f21288_0()
{    runTestSimpleCombineWithContext(EMPTY_TABLE, 0, Collections.emptyList(), new String[] {});}
public void beam_f21289_0()
{    final PCollectionView<Integer> view = pipeline.apply(Create.of(1)).apply(Sum.integersGlobally().asSingletonView());    Combine.Globally<Integer, String> combine = Combine.globally(new TestCombineFnWithContext(view)).withSideInputs(view).withoutDefaults();    assertEquals(Collections.singletonList(view), combine.getSideInputs());}
public List<String> beam_f21297_0(List<String> accumulator)
{    List<String> result = new ArrayList<>(accumulator);    Collections.sort(result);    return result;}
public void beam_f21298_0()
{        PCollection<KV<String, Integer>> perKeyInput = pipeline.apply(Create.timestamped(TimestampedValue.of(KV.of("a", 1), new Instant(2L)), TimestampedValue.of(KV.of("a", 1), new Instant(3L)), TimestampedValue.of(KV.of("a", 4), new Instant(8L)), TimestampedValue.of(KV.of("b", 1), new Instant(9L)), TimestampedValue.of(KV.of("b", 13), new Instant(10L))).withCoder(KvCoder.of(StringUtf8Coder.of(), BigEndianIntegerCoder.of()))).apply(Window.into(SlidingWindows.of(Duration.millis(2))));    PCollection<Integer> globallyInput = perKeyInput.apply(Values.create());    PCollection<Integer> sum = globallyInput.apply("Sum", Sum.integersGlobally().withoutDefaults());    PCollectionView<Integer> globallySumView = sum.apply(View.asSingleton());    PCollection<KV<String, String>> combinePerKeyWithContext = perKeyInput.apply(Combine.<String, Integer, String>perKey(new TestCombineFnWithContext(globallySumView)).withSideInputs(globallySumView));    PCollection<String> combineGloballyWithContext = globallyInput.apply(Combine.globally(new TestCombineFnWithContext(globallySumView)).withoutDefaults().withSideInputs(globallySumView));    PAssert.that(sum).containsInAnyOrder(1, 2, 1, 4, 5, 14, 13);    PAssert.that(combinePerKeyWithContext).containsInAnyOrder(Arrays.asList(KV.of("a", "1:1"), KV.of("a", "2:11"), KV.of("a", "1:1"), KV.of("a", "4:4"), KV.of("a", "5:4"), KV.of("b", "5:1"), KV.of("b", "14:113"), KV.of("b", "13:13")));    PAssert.that(combineGloballyWithContext).containsInAnyOrder("1:1", "2:11", "1:1", "4:4", "5:14", "14:113", "13:13");    pipeline.run();}
public void beam_f21299_0()
{    PCollection<Integer> input = pipeline.apply(Create.of(1, 1));    PCollection<String> output = input.apply(Window.<Integer>into(new GlobalWindows()).triggering(Repeatedly.forever(AfterPane.elementCountAtLeast(1))).accumulatingFiredPanes().withAllowedLateness(new Duration(0), ClosingBehavior.FIRE_ALWAYS)).apply(Sum.integersGlobally()).apply(ParDo.of(new FormatPaneInfo()));            PAssert.that(output).satisfies(input1 -> {        assertThat(input1, hasItem("2: true"));        return null;    });    pipeline.run();}
public void beam_f21307_0()
{    PCollection<Integer> output = pipeline.apply(Create.of(1, 2, 3, 4)).apply(Combine.globally(integers -> {        int sum = 0;        for (int i : integers) {            sum += i;        }        return sum;    }));    PAssert.that(output).containsInAnyOrder(10);    pipeline.run();}
public void beam_f21308_0()
{    PCollection<Integer> output = pipeline.apply(Create.of(1, 2, 3, 4)).apply(Combine.globally(new Summer()::sum));    PAssert.that(output).containsInAnyOrder(10);    pipeline.run();}
public void beam_f21309_0()
{    runTestAccumulatingCombine(Arrays.asList(KV.of("a", 1), KV.of("a", 1), KV.of("a", 4), KV.of("b", 1), KV.of("b", 13)), 4.0, Arrays.asList(KV.of("a", 2.0), KV.of("b", 7.0)));}
public void beam_f21317_0()
{    PCollection<String> output = p.apply(Create.of(LINES));    PAssert.that(output).containsInAnyOrder(LINES_ARRAY);    p.run();}
public void beam_f21318_0()
{    PCollection<String> output = p.apply(Create.empty(StringUtf8Coder.of()));    PAssert.that(output).containsInAnyOrder(NO_LINES_ARRAY);    assertEquals(StringUtf8Coder.of(), output.getCoder());    p.run();}
public void beam_f21319_0()
{    p.enableAbandonedNodeEnforcement(false);    thrown.expect(IllegalArgumentException.class);    thrown.expectMessage("determine a default Coder");    thrown.expectMessage("Create.empty(Coder)");    thrown.expectMessage("Create.empty(TypeDescriptor)");    thrown.expectMessage("withCoder(Coder)");    thrown.expectMessage("withType(TypeDescriptor)");    p.apply(Create.of(Collections.emptyList()));}
public void beam_f21328_0(UnserializableRecord value, OutputStream outStream) throws CoderException, IOException
{    stringCoder.encode(value.myString, outStream);}
public UnserializableRecord beam_f21329_0(InputStream inStream) throws CoderException, IOException
{    return new UnserializableRecord(stringCoder.decode(inStream));}
public void beam_f21330_0() throws Exception
{    List<UnserializableRecord> elements = ImmutableList.of(new UnserializableRecord("foo"), new UnserializableRecord("bar"), new UnserializableRecord("baz"));    Create.Values<UnserializableRecord> create = Create.of(elements).withCoder(new UnserializableRecord.UnserializableRecordCoder());    PAssert.that(p.apply(create)).containsInAnyOrder(new UnserializableRecord("foo"), new UnserializableRecord("bar"), new UnserializableRecord("baz"));    p.run();}
public void beam_f21338_0() throws Exception
{    PCollection<Void> output = p.apply(Create.of((Void) null, (Void) null));    PAssert.that(output).containsInAnyOrder((Void) null, (Void) null);    p.run();}
public void beam_f21339_0() throws Exception
{    PCollection<KV<Void, Void>> output = p.apply(Create.of(KV.of((Void) null, (Void) null), KV.of((Void) null, (Void) null)));    PAssert.that(output).containsInAnyOrder(KV.of((Void) null, (Void) null), KV.of((Void) null, (Void) null));    p.run();}
public void beam_f21340_0() throws Exception
{    PAssert.that(p.apply("Static", Create.ofProvider(StaticValueProvider.of("foo"), StringUtf8Coder.of()))).containsInAnyOrder("foo");    PAssert.that(p.apply("Static nested", Create.ofProvider(NestedValueProvider.of(StaticValueProvider.of("foo"), input -> input + "bar"), StringUtf8Coder.of()))).containsInAnyOrder("foobar");    PAssert.that(p.apply("Runtime", Create.ofProvider(p.newProvider("runtimeFoo"), StringUtf8Coder.of()))).containsInAnyOrder("runtimeFoo");    p.run();}
public void beam_f21348_0() throws Exception
{    CreateSource<Integer> source = CreateSource.fromIterable(ImmutableList.of(1, 2, 3, 4, 5, 6, 7, 8), BigEndianIntegerCoder.of());    PipelineOptions options = PipelineOptionsFactory.create();    List<? extends BoundedSource<Integer>> splitSources = source.split(12, options);    assertThat(splitSources, hasSize(3));    SourceTestUtils.assertSourcesEqualReferenceSource(source, splitSources, options);}
public void beam_f21349_0() throws Exception
{    CreateSource<Void> source = CreateSource.fromIterable(Lists.newArrayList(null, null, null, null, null), VoidCoder.of());    PipelineOptions options = PipelineOptionsFactory.create();    List<? extends BoundedSource<Void>> splitSources = source.split(3, options);    SourceTestUtils.assertSourcesEqualReferenceSource(source, splitSources, options);}
public void beam_f21350_0() throws Exception
{    CreateSource<Integer> source = CreateSource.fromIterable(ImmutableList.of(), BigEndianIntegerCoder.of());    PipelineOptions options = PipelineOptionsFactory.create();    List<? extends BoundedSource<Integer>> splitSources = source.split(12, options);    SourceTestUtils.assertSourcesEqualReferenceSource(source, splitSources, options);}
public Set<DisplayData> beam_f21358_0(final PTransform<? super PBegin, ? extends POutput> root)
{    Pipeline pipeline = Pipeline.create(options);    pipeline.apply("SourceTransform", root);    return displayDataForPipeline(pipeline, root);}
private static Set<DisplayData> beam_f21359_0(Pipeline pipeline, PTransform<?, ?> root)
{    PrimitiveDisplayDataPTransformVisitor visitor = new PrimitiveDisplayDataPTransformVisitor(root);    pipeline.traverseTopologically(visitor);    return visitor.getPrimitivesDisplayData();}
 Set<DisplayData> beam_f21360_0()
{    return displayData;}
public void beam_f21368_0(DisplayData.Builder builder)
{    builder.add(DisplayData.item("compositeKey", "compositeValue"));}
public void beam_f21369_0()
{    PTransform<? super PCollection<Integer>, ? super PCollection<Integer>> myTransform = ParDo.of(new DoFn<Integer, Integer>() {        @ProcessElement        public void processElement(ProcessContext c) throws Exception {        }        @Override        public void populateDisplayData(DisplayData.Builder builder) {            builder.add(DisplayData.item("foo", "bar"));        }    });    DisplayDataEvaluator evaluator = DisplayDataEvaluator.create();    Set<DisplayData> displayData = evaluator.displayDataForPrimitiveTransforms(myTransform);    assertThat(displayData, hasItem(hasDisplayItem("foo")));}
public void beam_f21371_0(DisplayData.Builder builder)
{    builder.add(DisplayData.item("foo", "bar"));}
public static Matcher<DisplayData> beam_f21379_0(String key, double value)
{    return hasDisplayItem(key, DisplayData.Type.FLOAT, value);}
public static Matcher<DisplayData> beam_f21380_0(String key, long value)
{    return hasDisplayItem(key, DisplayData.Type.INTEGER, value);}
public static Matcher<DisplayData> beam_f21381_0(String key, Class<?> value)
{    return hasDisplayItem(key, DisplayData.Type.JAVA_CLASS, value);}
protected void beam_f21389_0(DisplayData displayData, Description mismatchDescription)
{    DisplayData subComponentDisplayData = subComponentData(path);    DisplayDataComparison comparison = checkSubset(displayData, subComponentDisplayData, path);    mismatchDescription.appendText("did not include:\n").appendValue(comparison.missingItems).appendText("\nNon-matching items:\n").appendValue(comparison.unmatchedItems);}
private DisplayData beam_f21390_0(final String path)
{    return DisplayData.from(builder -> builder.include(path, subComponent));}
private DisplayDataComparison beam_f21391_0(DisplayData displayData, DisplayData included, String path)
{    DisplayDataComparison comparison = new DisplayDataComparison(displayData.items());    for (Item item : included.items()) {        Item matchedItem = displayData.asMap().get(DisplayData.Identifier.of(DisplayData.Path.absolute(path), item.getNamespace(), item.getKey()));        if (matchedItem != null) {            comparison.matched(matchedItem);        } else {            comparison.missing(item);        }    }    return comparison;}
public static Matcher<DisplayData.Item> beam_f21399_0(Class<?> namespace)
{    return hasNamespace(Matchers.<Class<?>>is(namespace));}
public static Matcher<DisplayData.Item> beam_f21400_0(Matcher<Class<?>> namespaceMatcher)
{    return new FeatureMatcher<DisplayData.Item, Class<?>>(namespaceMatcher, " with namespace", "namespace") {        @Override        protected Class<?> featureValueOf(DisplayData.Item actual) {            return actual.getNamespace();        }    };}
protected Class<?> beam_f21401_0(DisplayData.Item actual)
{    return actual.getNamespace();}
public static Matcher<DisplayData.Item> beam_f21409_0(Matcher<String> labelMatcher)
{    return new FeatureMatcher<DisplayData.Item, String>(labelMatcher, "display item with label", "label") {        @Override        protected String featureValueOf(DisplayData.Item actual) {            return actual.getLabel();        }    };}
protected String beam_f21410_0(DisplayData.Item actual)
{    return actual.getLabel();}
public void beam_f21411_0()
{    Matcher<DisplayData> matcher = hasDisplayItem();    assertFalse(matcher.matches(DisplayData.none()));    assertThat(createDisplayDataWithItem("foo", "bar"), matcher);}
public void beam_f21419_0()
{    Matcher<DisplayData> matcher = hasDisplayItem(hasNamespace(SampleTransform.class));    assertFalse(matcher.matches(DisplayData.from(new PTransform<PCollection<String>, PCollection<String>>() {        @Override        public PCollection<String> expand(PCollection<String> input) {            throw new IllegalArgumentException("Should never be applied");        }    })));    assertThat(createDisplayDataWithItem("foo", "bar"), matcher);}
public PCollection<String> beam_f21420_0(PCollection<String> input)
{    throw new IllegalArgumentException("Should never be applied");}
public void beam_f21421_0()
{    final HasDisplayData subComponent = builder -> builder.add(DisplayData.item("foo", "bar"));    HasDisplayData hasSubcomponent = builder -> builder.include("p", subComponent);    HasDisplayData wrongPath = builder -> builder.include("q", subComponent);    HasDisplayData deeplyNested = builder -> builder.include("p", builder1 -> builder1.include("p", subComponent));    HasDisplayData sameDisplayItemDifferentComponent = builder -> builder.add(DisplayData.item("foo", "bar"));    Matcher<DisplayData> matcher = includesDisplayDataFor("p", subComponent);    assertFalse("should not match sub-component at different path", matcher.matches(DisplayData.from(wrongPath)));    assertFalse("should not match deeply nested sub-component", matcher.matches(DisplayData.from(deeplyNested)));    assertFalse("should not match identical display data from different component", matcher.matches(DisplayData.from(sameDisplayItemDifferentComponent)));    assertThat(DisplayData.from(hasSubcomponent), matcher);}
public void beam_f21429_0(DisplayData.Builder builder)
{    builder.include("p1", subComponent1).include("p2", subComponent2).add(DisplayData.item("minSproggles", 200).withLabel("Minimum Required Sproggles")).add(DisplayData.item("fireLasers", true)).addIfNotDefault(DisplayData.item("startTime", startTime), defaultStartTime).add(DisplayData.item("timeBomb", Instant.now().plus(Duration.standardDays(1)))).add(DisplayData.item("filterLogic", subComponent1.getClass())).add(DisplayData.item("serviceUrl", "google.com/fizzbang").withLinkUrl("http://www.google.com/fizzbang"));}
public void beam_f21430_0()
{    DisplayData none = DisplayData.none();    assertThat(none.items(), empty());}
public void beam_f21431_0()
{    DisplayData data = DisplayData.from(new HasDisplayData() {        @Override        public void populateDisplayData(DisplayData.Builder builder) {            builder.add(DisplayData.item("foo", "bar"));        }    });    assertThat(data.items(), hasSize(1));    assertThat(data, hasDisplayItem("foo", "bar"));}
public void beam_f21439_0()
{    DisplayData data = DisplayData.from(new HasDisplayData() {        @Override        public void populateDisplayData(DisplayData.Builder builder) {            builder.add(DisplayData.item("foo", new ValueProvider<String>() {                @Override                public boolean isAccessible() {                    return false;                }                @Override                public String get() {                    return "bar";                }                @Override                public String toString() {                    return "toString";                }            }));        }    });    assertThat(data.items(), hasSize(1));    assertThat(data, hasDisplayItem("foo", "toString"));}
public void beam_f21440_0(DisplayData.Builder builder)
{    builder.add(DisplayData.item("foo", new ValueProvider<String>() {        @Override        public boolean isAccessible() {            return false;        }        @Override        public String get() {            return "bar";        }        @Override        public String toString() {            return "toString";        }    }));}
public boolean beam_f21441_0()
{    return false;}
public void beam_f21449_0(DisplayData.Builder builder)
{    builder.add(DisplayData.item("foo", "bar"));}
public void beam_f21450_0()
{    DisplayData data = DisplayData.from(new HasDisplayData() {        @Override        public void populateDisplayData(Builder builder) {            builder.addIfNotDefault(DisplayData.item("defaultString", "foo"), "foo").addIfNotDefault(DisplayData.item("notDefaultString", "foo"), "notFoo").addIfNotDefault(DisplayData.item("defaultInteger", 1), 1).addIfNotDefault(DisplayData.item("notDefaultInteger", 1), 2).addIfNotDefault(DisplayData.item("defaultDouble", 123.4), 123.4).addIfNotDefault(DisplayData.item("notDefaultDouble", 123.4), 234.5).addIfNotDefault(DisplayData.item("defaultBoolean", true), true).addIfNotDefault(DisplayData.item("notDefaultBoolean", true), false).addIfNotDefault(DisplayData.item("defaultInstant", new Instant(0)), new Instant(0)).addIfNotDefault(DisplayData.item("notDefaultInstant", new Instant(0)), Instant.now()).addIfNotDefault(DisplayData.item("defaultDuration", Duration.ZERO), Duration.ZERO).addIfNotDefault(DisplayData.item("notDefaultDuration", Duration.millis(1234)), Duration.ZERO).addIfNotDefault(DisplayData.item("defaultClass", DisplayDataTest.class), DisplayDataTest.class).addIfNotDefault(DisplayData.item("notDefaultClass", DisplayDataTest.class), null);        }    });    assertThat(data.items(), hasSize(7));    assertThat(data.items(), everyItem(hasKey(startsWith("notDefault"))));}
public void beam_f21451_0(Builder builder)
{    builder.addIfNotDefault(DisplayData.item("defaultString", "foo"), "foo").addIfNotDefault(DisplayData.item("notDefaultString", "foo"), "notFoo").addIfNotDefault(DisplayData.item("defaultInteger", 1), 1).addIfNotDefault(DisplayData.item("notDefaultInteger", 1), 2).addIfNotDefault(DisplayData.item("defaultDouble", 123.4), 123.4).addIfNotDefault(DisplayData.item("notDefaultDouble", 123.4), 234.5).addIfNotDefault(DisplayData.item("defaultBoolean", true), true).addIfNotDefault(DisplayData.item("notDefaultBoolean", true), false).addIfNotDefault(DisplayData.item("defaultInstant", new Instant(0)), new Instant(0)).addIfNotDefault(DisplayData.item("notDefaultInstant", new Instant(0)), Instant.now()).addIfNotDefault(DisplayData.item("defaultDuration", Duration.ZERO), Duration.ZERO).addIfNotDefault(DisplayData.item("notDefaultDuration", Duration.millis(1234)), Duration.ZERO).addIfNotDefault(DisplayData.item("defaultClass", DisplayDataTest.class), DisplayDataTest.class).addIfNotDefault(DisplayData.item("notDefaultClass", DisplayDataTest.class), null);}
public void beam_f21459_0()
{    DisplayData.Path a = DisplayData.Path.root().extend("a");    assertThat(a.getComponents(), hasItems("a"));    DisplayData.Path b = a.extend("b");    assertThat(b.getComponents(), hasItems("a", "b"));}
public void beam_f21460_0()
{    DisplayData.Path root = DisplayData.Path.root();    thrown.expect(NullPointerException.class);    root.extend(null);}
public void beam_f21461_0()
{    DisplayData.Path root = DisplayData.Path.root();    thrown.expect(IllegalArgumentException.class);    root.extend("");}
public void beam_f21469_0()
{    final HasDisplayData subComponent = new HasDisplayData() {        @Override        public void populateDisplayData(DisplayData.Builder builder) {            builder.add(DisplayData.item("foo", "bar"));        }    };    DisplayData data = DisplayData.from(new HasDisplayData() {        @Override        public void populateDisplayData(DisplayData.Builder builder) {            builder.include("p", subComponent);        }    });    assertThat(data, includesDisplayDataFor("p", subComponent));}
public void beam_f21470_0(DisplayData.Builder builder)
{    builder.add(DisplayData.item("foo", "bar"));}
public void beam_f21471_0(DisplayData.Builder builder)
{    builder.include("p", subComponent);}
public void beam_f21479_0(Builder builder)
{    builder.add(DisplayData.item("foo", "bar").withNamespace(null));}
public void beam_f21480_0()
{    new EqualsTester().addEqualityGroup(DisplayData.Identifier.of(DisplayData.Path.absolute("a"), DisplayDataTest.class, "1"), DisplayData.Identifier.of(DisplayData.Path.absolute("a"), DisplayDataTest.class, "1")).addEqualityGroup(DisplayData.Identifier.of(DisplayData.Path.absolute("b"), DisplayDataTest.class, "1")).addEqualityGroup(DisplayData.Identifier.of(DisplayData.Path.absolute("a"), Object.class, "1")).addEqualityGroup(DisplayData.Identifier.of(DisplayData.Path.absolute("a"), DisplayDataTest.class, "2")).testEquals();}
public void beam_f21481_0()
{    new EqualsTester().addEqualityGroup(DisplayData.item("foo", "bar"), DisplayData.item("foo", "bar")).addEqualityGroup(DisplayData.item("foo", "barz")).testEquals();}
public void beam_f21489_0(DisplayData.Builder builder)
{    builder.add(DisplayData.item("foo", "bar")).add(DisplayData.item("foo", "baz"));}
public void beam_f21490_0()
{    DisplayData displayData = DisplayData.from(new HasDisplayData() {        @Override        public void populateDisplayData(DisplayData.Builder builder) {            builder.add(DisplayData.item("foo", "bar")).add(DisplayData.item("foo", "baz").withNamespace(DisplayDataTest.class));        }    });    assertThat(displayData.items(), hasSize(2));}
public void beam_f21491_0(DisplayData.Builder builder)
{    builder.add(DisplayData.item("foo", "bar")).add(DisplayData.item("foo", "baz").withNamespace(DisplayDataTest.class));}
public void beam_f21499_0(Builder builder)
{    builder.add(DisplayData.item("subComponent", subComponent.getClass())).include("p", subComponent);}
public void beam_f21500_0()
{    DisplayData data = DisplayData.from(new HasDisplayData() {        @Override        public void populateDisplayData(DisplayData.Builder builder) {            builder.include("p1", new EqualsEverything("foo1", "bar1")).include("p2", new EqualsEverything("foo2", "bar2"));        }    });    assertThat(data.items(), hasSize(2));}
public void beam_f21501_0(DisplayData.Builder builder)
{    builder.include("p1", new EqualsEverything("foo1", "bar1")).include("p2", new EqualsEverything("foo2", "bar2"));}
public void beam_f21509_0(DisplayData.Builder builder)
{    builder.add(DisplayData.item("id", getId())).include(getId(), subComponent);}
public void beam_f21510_0()
{    DisplayData data = DisplayData.from(new HasDisplayData() {        @Override        public void populateDisplayData(DisplayData.Builder builder) {            builder.add(DisplayData.item("string", "foobar")).add(DisplayData.item("integer", 123)).add(DisplayData.item("float", 2.34)).add(DisplayData.item("boolean", true)).add(DisplayData.item("java_class", DisplayDataTest.class)).add(DisplayData.item("timestamp", Instant.now())).add(DisplayData.item("duration", Duration.standardHours(1)));        }    });    Collection<Item> items = data.items();    assertThat(items, hasItem(allOf(hasKey("string"), hasType(DisplayData.Type.STRING))));    assertThat(items, hasItem(allOf(hasKey("integer"), hasType(DisplayData.Type.INTEGER))));    assertThat(items, hasItem(allOf(hasKey("float"), hasType(DisplayData.Type.FLOAT))));    assertThat(items, hasItem(allOf(hasKey("boolean"), hasType(DisplayData.Type.BOOLEAN))));    assertThat(items, hasItem(allOf(hasKey("java_class"), hasType(DisplayData.Type.JAVA_CLASS))));    assertThat(items, hasItem(allOf(hasKey("timestamp"), hasType(DisplayData.Type.TIMESTAMP))));    assertThat(items, hasItem(allOf(hasKey("duration"), hasType(DisplayData.Type.DURATION))));}
public void beam_f21511_0(DisplayData.Builder builder)
{    builder.add(DisplayData.item("string", "foobar")).add(DisplayData.item("integer", 123)).add(DisplayData.item("float", 2.34)).add(DisplayData.item("boolean", true)).add(DisplayData.item("java_class", DisplayDataTest.class)).add(DisplayData.item("timestamp", Instant.now())).add(DisplayData.item("duration", Duration.standardHours(1)));}
public void beam_f21519_0() throws IOException
{    final Instant now = Instant.now();    final Duration oneHour = Duration.standardHours(1);    HasDisplayData component = new HasDisplayData() {        @Override        public void populateDisplayData(DisplayData.Builder builder) {            builder.add(DisplayData.item("string", "foobar")).add(DisplayData.item("integer", 123)).add(DisplayData.item("float", 2.34)).add(DisplayData.item("boolean", true)).add(DisplayData.item("java_class", DisplayDataTest.class)).add(DisplayData.item("timestamp", now)).add(DisplayData.item("duration", oneHour));        }    };    DisplayData data = DisplayData.from(component);    assertThat(data, hasDisplayItem("string", "foobar"));    assertThat(data, hasDisplayItem("integer", 123));    assertThat(data, hasDisplayItem("float", 2.34));    assertThat(data, hasDisplayItem("boolean", true));    assertThat(data, hasDisplayItem("java_class", DisplayDataTest.class));    assertThat(data, hasDisplayItem("timestamp", now));    assertThat(data, hasDisplayItem("duration", oneHour));}
public void beam_f21520_0(DisplayData.Builder builder)
{    builder.add(DisplayData.item("string", "foobar")).add(DisplayData.item("integer", 123)).add(DisplayData.item("float", 2.34)).add(DisplayData.item("boolean", true)).add(DisplayData.item("java_class", DisplayDataTest.class)).add(DisplayData.item("timestamp", now)).add(DisplayData.item("duration", oneHour));}
public void beam_f21521_0()
{    final HasDisplayData subComponent = new HasDisplayData() {        @Override        public void populateDisplayData(DisplayData.Builder builder) {            builder.add(DisplayData.item("foo", "bar"));        }    };    HasDisplayData component = new HasDisplayData() {        @Override        public void populateDisplayData(DisplayData.Builder builder) {            builder.include("p", subComponent).add(DisplayData.item("alpha", "bravo"));        }    };    DisplayData data = DisplayData.from(component);    assertThat(data.items(), hasItem(allOf(hasKey("alpha"), hasNamespace(component.getClass()))));}
public void beam_f21529_0()
{    thrown.expectCause(isA(IllegalArgumentException.class));    DisplayData.from(new HasDisplayData() {        @Override        public void populateDisplayData(Builder builder) {            builder.include("", new NoopDisplayData());        }    });}
public void beam_f21530_0(Builder builder)
{    builder.include("", new NoopDisplayData());}
public void beam_f21531_0()
{    thrown.expectCause(isA(NullPointerException.class));    DisplayData.from(new HasDisplayData() {        @Override        public void populateDisplayData(Builder builder) {            builder.add(DisplayData.item(null, "foo"));        }    });}
public void beam_f21539_0() throws IOException
{    HasDisplayData component = new HasDisplayData() {        @Override        public void populateDisplayData(Builder builder) {            builder.add(DisplayData.item("foo", "bar"));        }    };    DisplayData data = DisplayData.from(component);    JsonNode json = MAPPER.readTree(MAPPER.writeValueAsBytes(data));    String namespace = json.elements().next().get("namespace").asText();    final Pattern anonClassRegex = Pattern.compile(Pattern.quote(DisplayDataTest.class.getName()) + "\\$\\d+$");    assertThat(namespace, new CustomTypeSafeMatcher<String>("anonymous class regex: " + anonClassRegex) {        @Override        protected boolean matchesSafely(String item) {            java.util.regex.Matcher m = anonClassRegex.matcher(item);            return m.matches();        }    });}
public void beam_f21540_0(Builder builder)
{    builder.add(DisplayData.item("foo", "bar"));}
protected boolean beam_f21541_0(String item)
{    java.util.regex.Matcher m = anonClassRegex.matcher(item);    return m.matches();}
public void beam_f21549_0()
{    final RuntimeException cause = new RuntimeException("oh noes!");    HasDisplayData component = new HasDisplayData() {        @Override        public void populateDisplayData(Builder builder) {            throw cause;        }    };    thrown.expectMessage(component.getClass().getName());    thrown.expectCause(is(cause));    DisplayData.from(component);}
public void beam_f21550_0(Builder builder)
{    throw cause;}
public void beam_f21551_0()
{    final RuntimeException cause = new RuntimeException("oh noes!");    HasDisplayData component = new HasDisplayData() {        @Override        public void populateDisplayData(Builder builder) {            builder.include("p", new HasDisplayData() {                @Override                public void populateDisplayData(Builder builder) {                    throw cause;                }            });        }    };    thrown.expectCause(is(cause));    DisplayData.from(component);}
private static Matcher<DisplayData.Item> beam_f21560_0(Matcher<String> urlMatcher)
{    return new FeatureMatcher<DisplayData.Item, String>(urlMatcher, "display item with url", "URL") {        @Override        protected String featureValueOf(DisplayData.Item actual) {            return actual.getLinkUrl();        }    };}
protected String beam_f21561_0(DisplayData.Item actual)
{    return actual.getLinkUrl();}
private static Matcher<DisplayData.Item> beam_f21562_0(Matcher<T> valueStringMatcher)
{    return new FeatureMatcher<DisplayData.Item, T>(valueStringMatcher, "display item with short value", "short value") {        @Override        protected T featureValueOf(DisplayData.Item actual) {            @SuppressWarnings("unchecked")            T shortValue = (T) actual.getShortValue();            return shortValue;        }    };}
public void beam_f21570_0()
{    Instant base = new Instant(0);    TestStream<String> values = TestStream.create(StringUtf8Coder.of()).advanceWatermarkTo(base).addElements(TimestampedValue.of("k1", base), TimestampedValue.of("k2", base.plus(Duration.standardSeconds(10))), TimestampedValue.of("k3", base.plus(Duration.standardSeconds(20)))).advanceProcessingTime(Duration.standardMinutes(1)).addElements(TimestampedValue.of("k1", base.plus(Duration.standardSeconds(30))), TimestampedValue.of("k2", base.plus(Duration.standardSeconds(40))), TimestampedValue.of("k3", base.plus(Duration.standardSeconds(50)))).advanceWatermarkToInfinity();    PCollection<String> distinctValues = triggeredDistinctPipeline.apply(values).apply(Window.<String>into(FixedWindows.of(Duration.standardMinutes(1))).triggering(Repeatedly.forever(AfterProcessingTime.pastFirstElementInPane().plusDelayOf(Duration.standardSeconds(30)))).withAllowedLateness(Duration.ZERO).accumulatingFiredPanes()).apply(Distinct.create());    PAssert.that(distinctValues).containsInAnyOrder("k1", "k2", "k3");    triggeredDistinctPipeline.run();}
public void beam_f21571_0()
{    Instant base = new Instant(0);    TestStream<KV<Integer, String>> values = TestStream.create(KvCoder.of(VarIntCoder.of(), StringUtf8Coder.of())).advanceWatermarkTo(base).addElements(TimestampedValue.of(KV.of(1, "k1"), base), TimestampedValue.of(KV.of(2, "k2"), base.plus(Duration.standardSeconds(10))), TimestampedValue.of(KV.of(3, "k3"), base.plus(Duration.standardSeconds(20)))).advanceProcessingTime(Duration.standardMinutes(1)).addElements(TimestampedValue.of(KV.of(1, "k1"), base.plus(Duration.standardSeconds(30))), TimestampedValue.of(KV.of(2, "k2"), base.plus(Duration.standardSeconds(40))), TimestampedValue.of(KV.of(3, "k3"), base.plus(Duration.standardSeconds(50)))).advanceWatermarkToInfinity();    PCollection<KV<Integer, String>> distinctValues = triggeredDistinctRepresentativePipeline.apply(values).apply(Window.<KV<Integer, String>>into(FixedWindows.of(Duration.standardMinutes(1))).triggering(Repeatedly.forever(AfterProcessingTime.pastFirstElementInPane().plusDelayOf(Duration.standardSeconds(30)))).withAllowedLateness(Duration.ZERO).accumulatingFiredPanes()).apply(Distinct.withRepresentativeValueFn(new Keys<Integer>()).withRepresentativeType(TypeDescriptor.of(Integer.class)));    PAssert.that(distinctValues).containsInAnyOrder(KV.of(1, "k1"), KV.of(2, "k2"), KV.of(3, "k3"));    triggeredDistinctRepresentativePipeline.run();}
public void beam_f21572_0()
{    Instant base = new Instant(0);    TestStream<KV<Integer, String>> values = TestStream.create(KvCoder.of(VarIntCoder.of(), StringUtf8Coder.of())).advanceWatermarkTo(base).addElements(TimestampedValue.of(KV.of(1, "k1"), base)).advanceProcessingTime(Duration.standardMinutes(1)).advanceWatermarkToInfinity();    PCollection<KV<Integer, String>> distinctValues = triggeredDistinctRepresentativePipeline.apply(values).apply(Window.<KV<Integer, String>>into(FixedWindows.of(Duration.standardMinutes(1))).triggering(AfterWatermark.pastEndOfWindow().withEarlyFirings(AfterProcessingTime.pastFirstElementInPane().plusDelayOf(Duration.standardSeconds(30)))).withAllowedLateness(Duration.ZERO).discardingFiredPanes()).apply(Distinct.withRepresentativeValueFn(new Keys<Integer>()).withRepresentativeType(TypeDescriptor.of(Integer.class)));    PAssert.that(distinctValues).containsInAnyOrder(KV.of(1, "k1"));    triggeredDistinctRepresentativePipeline.run();}
public void beam_f21580_0() throws Exception
{    final AtomicInteger numSetupCalls = new AtomicInteger();    final AtomicInteger numTeardownCalls = new AtomicInteger();    DoFn<Long, String> fn = new DoFn<Long, String>() {        @ProcessElement        public void process(ProcessContext context) {        }        @Setup        public void setup() {            numSetupCalls.addAndGet(1);        }        @Teardown        public void teardown() {            numTeardownCalls.addAndGet(1);        }    };    try (DoFnTester<Long, String> tester = DoFnTester.of(fn)) {        tester.setCloningBehavior(DoFnTester.CloningBehavior.DO_NOT_CLONE);        tester.processBundle(1L, 2L, 3L);        tester.processBundle(4L, 5L);        tester.processBundle(6L);    }    assertEquals(1, numSetupCalls.get());    assertEquals(1, numTeardownCalls.get());}
public void beam_f21582_0()
{    numSetupCalls.addAndGet(1);}
public void beam_f21583_0()
{    numTeardownCalls.addAndGet(1);}
public void beam_f21591_0() throws Exception
{    try (DoFnTester<Long, String> tester = DoFnTester.of(new CounterDoFn())) {        tester.processElement(1L);        tester.processElement(2L);        List<TimestampedValue<String>> peek = tester.peekOutputElementsWithTimestamp();        TimestampedValue<String> one = TimestampedValue.of("1", new Instant(1000L));        TimestampedValue<String> two = TimestampedValue.of("2", new Instant(2000L));        assertThat(peek, hasItems(one, two));        tester.processElement(3L);        tester.processElement(4L);        TimestampedValue<String> three = TimestampedValue.of("3", new Instant(3000L));        TimestampedValue<String> four = TimestampedValue.of("4", new Instant(4000L));        peek = tester.peekOutputElementsWithTimestamp();        assertThat(peek, hasItems(one, two, three, four));        List<TimestampedValue<String>> take = tester.takeOutputElementsWithTimestamp();        assertThat(take, hasItems(one, two, three, four));                        assertTrue(tester.takeOutputElementsWithTimestamp().isEmpty());        assertTrue(tester.peekOutputElementsWithTimestamp().isEmpty());                assertTrue(tester.peekOutputElements().isEmpty());        assertTrue(tester.takeOutputElements().isEmpty());    }}
public void beam_f21592_0() throws Exception
{    try (DoFnTester<Long, String> tester = DoFnTester.of(new CounterDoFn())) {        tester.startBundle();        tester.processElement(1L);        tester.processElement(2L);        tester.finishBundle();        assertThat(tester.peekOutputElementsInWindow(GlobalWindow.INSTANCE), containsInAnyOrder(TimestampedValue.of("1", new Instant(1000L)), TimestampedValue.of("2", new Instant(2000L))));        assertThat(tester.peekOutputElementsInWindow(new IntervalWindow(new Instant(0L), new Instant(10L))), Matchers.emptyIterable());    }}
public void beam_f21593_0() throws Exception
{    PCollection<Integer> pCollection = p.apply(Create.empty(VarIntCoder.of()));    final PCollectionView<Integer> value = pCollection.apply(View.<Integer>asSingleton().withDefaultValue(0));    try (DoFnTester<Integer, Integer> tester = DoFnTester.of(new SideInputDoFn(value))) {        tester.processElement(1);        tester.processElement(2);        tester.processElement(4);        tester.processElement(8);        assertThat(tester.peekOutputElements(), containsInAnyOrder(0, 0, 0, 0));    }}
public void beam_f21601_0(ProcessContext c) throws Exception
{    c.output(c.sideInput(value));}
public void beam_f21602_0()
{    checkState(state == LifecycleState.UNINITIALIZED, "Wrong state: %s", state);    state = LifecycleState.SET_UP;}
public void beam_f21603_0()
{    checkState(state == LifecycleState.SET_UP, "Wrong state: %s", state);    state = LifecycleState.INSIDE_BUNDLE;    startBundleCalls.inc();}
public void beam_f21611_0()
{    PCollection<Integer> output = p.apply(Create.of(1, 2, 4, 5)).apply(Filter.by(new TrivialFn(false)));    PAssert.that(output).empty();    p.run();}
public void beam_f21612_0()
{    PCollection<Integer> output = p.apply(Create.of(1, 2, 3, 4, 5, 6, 7)).apply(Filter.by(new EvenFn()));    PAssert.that(output).containsInAnyOrder(2, 4, 6);    p.run();}
public void beam_f21613_0()
{    PCollection<Integer> output = p.apply(Create.of(1, 2, 3, 4, 5, 6, 7)).apply(Filter.by(new EvenProcessFn()));    PAssert.that(output).containsInAnyOrder(2, 4, 6);    p.run();}
public void beam_f21621_0()
{    PCollection<Integer> output = p.apply(Create.of(1, 2, 4, 5)).apply(Filter.by(i -> false));    PAssert.that(output).empty();    p.run();}
public void beam_f21622_0()
{    PCollection<Integer> output = p.apply(Create.of(1, 2, 3, 4, 5, 6, 7)).apply(Filter.by(i -> i % 2 == 0));    PAssert.that(output).containsInAnyOrder(2, 4, 6);    p.run();}
public void beam_f21623_0() throws Exception
{    @SuppressWarnings({ "unchecked", "rawtypes" })    PCollection<String> output = p.apply(Create.of("hello")).apply(Filter.by(s -> true));    thrown.expect(CannotProvideCoderException.class);    p.getCoderRegistry().getCoder(output.getTypeDescriptor());}
public void beam_f21631_0() throws Exception
{    PCollection<String> output = pipeline.apply(Create.of("hello")).apply(FlatMapElements.via(new SimpleFunction<String, Set<String>>() {        @Override        public Set<String> apply(String input) {            return ImmutableSet.copyOf(input.split(""));        }    }));    assertThat(output.getTypeDescriptor(), equalTo((TypeDescriptor<String>) new TypeDescriptor<String>() {    }));    assertThat(pipeline.getCoderRegistry().getCoder(output.getTypeDescriptor()), equalTo(pipeline.getCoderRegistry().getCoder(new TypeDescriptor<String>() {    })));        pipeline.run();}
public Set<String> beam_f21632_0(String input)
{    return ImmutableSet.copyOf(input.split(""));}
public Iterable<T> beam_f21633_0(T input)
{    return Collections.emptyList();}
public List<Integer> beam_f21641_0(Integer input)
{    return Collections.emptyList();}
public void beam_f21642_0(DisplayData.Builder builder)
{    builder.add(DisplayData.item("foo", "baz"));}
public void beam_f21643_0()
{    InferableFunction<Integer, List<Integer>> inferableFn = new InferableFunction<Integer, List<Integer>>() {        @Override        public List<Integer> apply(Integer input) {            return Collections.emptyList();        }        @Override        public void populateDisplayData(DisplayData.Builder builder) {            builder.add(DisplayData.item("foo", "baz"));        }    };    FlatMapElements<?, ?> inferableFlatMap = FlatMapElements.via(inferableFn);    assertThat(DisplayData.from(inferableFlatMap), hasDisplayItem("class", inferableFn.getClass()));    assertThat(DisplayData.from(inferableFlatMap), hasDisplayItem("foo", "baz"));}
public List<Integer> beam_f21651_0(int input)
{    return ImmutableList.of(input, -input);}
public void beam_f21652_0() throws Exception
{    Result<PCollection<Integer>, KV<Integer, Map<String, String>>> result = pipeline.apply(Create.of(0, 2, 3)).apply(FlatMapElements.into(TypeDescriptors.integers()).via((Integer i) -> ImmutableList.of(i + 1 / i, -i - 1 / i)).exceptionsVia(new ExceptionAsMapHandler<Integer>() {    }));    PAssert.that(result.output()).containsInAnyOrder(2, -2, 3, -3);    pipeline.run();}
public void beam_f21653_0()
{    Result<PCollection<Integer>, KV<Integer, String>> result = pipeline.apply(Create.of(0, 2, 3)).apply(FlatMapElements.into(TypeDescriptors.integers()).via((Integer i) -> ImmutableList.of(i + 1 / i, -i - 1 / i)).exceptionsInto(TypeDescriptors.kvs(TypeDescriptors.integers(), TypeDescriptors.strings())).exceptionsVia(f -> KV.of(f.element(), f.exception().getMessage())));    PAssert.that(result.output()).containsInAnyOrder(2, -2, 3, -3);    PAssert.that(result.failures()).containsInAnyOrder(KV.of(0, "/ by zero"));    pipeline.run();}
public void beam_f21661_0()
{    List<List<String>> inputs = Arrays.asList(LINES, NO_LINES, LINES2, NO_LINES, LINES, NO_LINES);    PCollection<String> output = makePCollectionListOfStrings(p, inputs).apply(Flatten.pCollections());    PAssert.that(output).containsInAnyOrder(flattenLists(inputs));    p.run();}
public void beam_f21662_0()
{    PCollection<String> input = p.apply(Create.of(LINES));    PCollection<String> output = PCollectionList.of(input).apply(Flatten.pCollections());    assertThat(output, not(equalTo(input)));    PAssert.that(output).containsInAnyOrder(LINES);    p.run();}
public void beam_f21663_0()
{    List<List<String>> inputs = Arrays.asList(LINES, NO_LINES, LINES2, NO_LINES, LINES, NO_LINES);    PCollection<String> output = makePCollectionListOfStrings(p, inputs).apply(Flatten.pCollections()).apply(ParDo.of(new IdentityFn<>()));    PAssert.that(output).containsInAnyOrder(flattenLists(inputs));    p.run();}
public void beam_f21671_0()
{        thrown.expect(IllegalStateException.class);    thrown.expectMessage("Unable to return a default Coder");    PCollectionList.<ClassWithoutCoder>empty(p).apply(Flatten.pCollections());    p.run();}
public void beam_f21672_0()
{    PCollection<Iterable<String>> input = p.apply(Create.<Iterable<String>>of(LINES).withCoder(IterableCoder.of(StringUtf8Coder.of())));    PCollection<String> output = input.apply(Flatten.iterables());    PAssert.that(output).containsInAnyOrder(LINES_ARRAY);    p.run();}
public void beam_f21673_0()
{    PCollection<List<String>> input = p.apply(Create.<List<String>>of(LINES).withCoder(ListCoder.of(StringUtf8Coder.of())));    PCollection<String> output = input.apply(Flatten.iterables());    PAssert.that(output).containsInAnyOrder(LINES_ARRAY);    p.run();}
public void beam_f21681_0()
{    p.enableAbandonedNodeEnforcement(false);    PCollection<String> input1 = p.apply("CreateInput1", Create.of("Input1")).apply("Window1", Window.into(FixedWindows.of(Duration.standardMinutes(1))));    PCollection<String> input2 = p.apply("CreateInput2", Create.of("Input2")).apply("Window2", Window.into(FixedWindows.of(Duration.standardMinutes(2))));    try {        PCollectionList.of(input1).and(input2).apply(Flatten.pCollections());        Assert.fail("Exception should have been thrown");    } catch (IllegalStateException e) {        Assert.assertTrue(e.getMessage().startsWith("Inputs to Flatten had incompatible window windowFns"));    }}
public void beam_f21682_0()
{    Assert.assertEquals("Flatten.Iterables", Flatten.<String>iterables().getName());    Assert.assertEquals("Flatten.PCollections", Flatten.<String>pCollections().getName());}
public void beam_f21683_0(ProcessContext c)
{    c.output(c.element());}
public void beam_f21691_0()
{    List<KV<String, Integer>> ungroupedPairs = Arrays.asList();    PCollection<KV<String, Integer>> input = p.apply(Create.of(ungroupedPairs).withCoder(KvCoder.of(StringUtf8Coder.of(), BigEndianIntegerCoder.of()))).apply(Window.into(Sessions.withGapDuration(Duration.standardMinutes(1))));    PCollection<KV<String, Iterable<Iterable<Integer>>>> middle = input.apply("GroupByKey", GroupByKey.create()).apply("Remerge", Window.remerge()).apply("GroupByKeyAgain", GroupByKey.create()).apply("RemergeAgain", Window.remerge());    p.run();    Assert.assertTrue(middle.getWindowingStrategy().getWindowFn().isCompatible(Sessions.withGapDuration(Duration.standardMinutes(1))));}
public void beam_f21692_0()
{    PCollection<KV<String, Integer>> input = p.apply(new PTransform<PBegin, PCollection<KV<String, Integer>>>() {        @Override        public PCollection<KV<String, Integer>> expand(PBegin input) {            return PCollection.createPrimitiveOutputInternal(input.getPipeline(), WindowingStrategy.globalDefault(), PCollection.IsBounded.UNBOUNDED, KvCoder.of(StringUtf8Coder.of(), VarIntCoder.of()));        }    });    thrown.expect(IllegalStateException.class);    thrown.expectMessage("GroupByKey cannot be applied to non-bounded PCollection in the GlobalWindow without " + "a trigger. Use a Window.into or Window.triggering transform prior to GroupByKey.");    input.apply("GroupByKey", GroupByKey.create());}
public PCollection<KV<String, Integer>> beam_f21693_0(PBegin input)
{    return PCollection.createPrimitiveOutputInternal(input.getPipeline(), WindowingStrategy.globalDefault(), PCollection.IsBounded.UNBOUNDED, KvCoder.of(StringUtf8Coder.of(), VarIntCoder.of()));}
public void beam_f21701_0() throws Exception
{    runLargeKeysTest(p, 1 << 20);}
public void beam_f21702_0() throws Exception
{    runLargeKeysTest(p, 10 << 20);}
public void beam_f21703_0() throws Exception
{    runLargeKeysTest(p, 100 << 20);}
private static SerializableFunction<Iterable<KV<String, Iterable<Integer>>>, Void> beam_f21711_0(KV<String, Collection<Integer>>... kvs)
{    return new ContainsKVs(ImmutableList.copyOf(kvs));}
public Void beam_f21712_0(Iterable<KV<String, Iterable<Integer>>> input)
{    List<Matcher<? super KV<String, Iterable<Integer>>>> matchers = new ArrayList<>();    for (KV<String, Collection<Integer>> expected : expectedKvs) {        Integer[] values = expected.getValue().toArray(new Integer[0]);        matchers.add(isKv(equalTo(expected.getKey()), containsInAnyOrder(values)));    }    assertThat(input, containsInAnyOrder(matchers.toArray(new Matcher[0])));    return null;}
public void beam_f21713_0(ProcessContext c) throws Exception
{    assertThat(c.timestamp(), equalTo(timestamp));}
public void beam_f21721_0(BadEqualityKey value, OutputStream outStream) throws IOException
{    new DataOutputStream(outStream).writeLong(value.key);}
public BadEqualityKey beam_f21722_0(InputStream inStream) throws IOException
{    return new BadEqualityKey(new DataInputStream(inStream).readLong());}
public void beam_f21724_0(ProcessContext c) throws Exception
{    c.output(KV.of(ThreadLocalRandom.current().nextLong(), c.element()));}
public Void beam_f21732_0(Iterable<KV<String, Iterable<String>>> input)
{    assertTrue(checkBatchSizes(input));    return null;}
public void beam_f21733_1()
{    int timestampInterval = 1;    Instant startInstant = new Instant(0L);    TestStream.Builder<KV<String, String>> streamBuilder = TestStream.create(KvCoder.of(StringUtf8Coder.of(), StringUtf8Coder.of())).advanceWatermarkTo(startInstant);    long offset = 0L;    for (KV<String, String> element : data) {        streamBuilder = streamBuilder.addElements(TimestampedValue.of(element, startInstant.plus(Duration.standardSeconds(offset * timestampInterval))));        offset++;    }    final long windowDuration = 6;    TestStream<KV<String, String>> stream = streamBuilder.advanceWatermarkTo(startInstant.plus(Duration.standardSeconds(windowDuration - 1))).advanceWatermarkTo(startInstant.plus(Duration.standardSeconds(windowDuration + 1))).advanceWatermarkTo(startInstant.plus(Duration.standardSeconds(NUM_ELEMENTS))).advanceWatermarkToInfinity();    PCollection<KV<String, String>> inputCollection = pipeline.apply(stream).apply(Window.<KV<String, String>>into(FixedWindows.of(Duration.standardSeconds(windowDuration))).withAllowedLateness(Duration.millis(ALLOWED_LATENESS)));    inputCollection.apply(ParDo.of(new DoFn<KV<String, String>, Void>() {        @ProcessElement        public void processElement(ProcessContext c, BoundedWindow window) {                    }    }));    PCollection<KV<String, Iterable<String>>> outputCollection = inputCollection.apply(GroupIntoBatches.ofSize(BATCH_SIZE)).setCoder(KvCoder.of(StringUtf8Coder.of(), IterableCoder.of(StringUtf8Coder.of())));            PCollection<KV<String, Long>> countOutput = outputCollection.apply("Count elements in windows after applying GroupIntoBatches", Count.perKey());    PAssert.that("Wrong number of elements in windows after GroupIntoBatches", countOutput).satisfies(input -> {        Iterator<KV<String, Long>> inputIterator = input.iterator();                long count0 = inputIterator.next().getValue();                        assertEquals("Wrong number of elements in first window", 2, count0);                long count1 = inputIterator.next().getValue();                        assertEquals("Wrong number of elements in second window", 1, count1);                return null;    });    PAssert.that("Incorrect output collection after GroupIntoBatches", outputCollection).satisfies(input -> {        Iterator<KV<String, Iterable<String>>> inputIterator = input.iterator();                int size0 = Iterables.size(inputIterator.next().getValue());                        assertEquals("Wrong first element batch Size", 5, size0);                int size1 = Iterables.size(inputIterator.next().getValue());                        assertEquals("Wrong second element batch Size", 1, size1);                int size2 = Iterables.size(inputIterator.next().getValue());                        assertEquals("Wrong third element batch Size", 4, size2);        return null;    });    pipeline.run().waitUntilFinish();}
public void beam_f21734_1(ProcessContext c, BoundedWindow window)
{    }
public int beam_f21742_0()
{    return maxPos;}
public Reiterator<RawUnionValue> beam_f21743_0()
{    return iterator(0);}
public Reiterator<RawUnionValue> beam_f21744_0(final int start)
{    return new Reiterator<RawUnionValue>() {        int pos = start;        @Override        public boolean hasNext() {            return pos < tags.length;        }        @Override        public RawUnionValue next() {            maxPos = Math.max(pos + 1, maxPos);            return new RawUnionValue(tags[pos], pos++);        }        @Override        public void remove() {            throw new UnsupportedOperationException();        }        @Override        public Reiterator<RawUnionValue> copy() {            return iterator(pos);        }    };}
private PCollection<KV<Integer, CoGbkResult>> beam_f21752_0(Pipeline p, TupleTag<String> tag1, TupleTag<String> tag2)
{    List<KV<Integer, String>> list1 = Arrays.asList(KV.of(1, "collection1-1"), KV.of(2, "collection1-2"));    List<KV<Integer, String>> list2 = Arrays.asList(KV.of(2, "collection2-2"), KV.of(3, "collection2-3"));    PCollection<KV<Integer, String>> collection1 = createInput("CreateList1", p, list1);    PCollection<KV<Integer, String>> collection2 = createInput("CreateList2", p, list2);    PCollection<KV<Integer, CoGbkResult>> coGbkResults = KeyedPCollectionTuple.of(tag1, collection1).and(tag2, collection2).apply(CoGroupByKey.create());    return coGbkResults;}
public void beam_f21753_0()
{    final TupleTag<String> tag1 = new TupleTag<>();    final TupleTag<String> tag2 = new TupleTag<>();    PCollection<KV<Integer, CoGbkResult>> coGbkResults = buildGetOnlyGbk(p, tag1, tag2);    PAssert.thatMap(coGbkResults).satisfies(results -> {        assertEquals("collection1-1", results.get(1).getOnly(tag1));        assertEquals("collection1-2", results.get(2).getOnly(tag1));        assertEquals("collection2-2", results.get(2).getOnly(tag2));        assertEquals("collection2-3", results.get(3).getOnly(tag2));        return null;    });    p.run();}
private PCollection<KV<Integer, CoGbkResult>> beam_f21754_0(Pipeline p, TupleTag<String> purchasesTag, TupleTag<String> addressesTag, TupleTag<String> namesTag)
{    List<KV<Integer, String>> idToPurchases = Arrays.asList(KV.of(2, "Boat"), KV.of(1, "Shoes"), KV.of(3, "Car"), KV.of(1, "Book"), KV.of(10, "Pens"), KV.of(8, "House"), KV.of(4, "Suit"), KV.of(11, "House"), KV.of(14, "Shoes"), KV.of(2, "Suit"), KV.of(8, "Suit Case"), KV.of(3, "House"));    List<KV<Integer, String>> idToAddress = Arrays.asList(KV.of(2, "53 S. 3rd"), KV.of(10, "383 Jackson Street"), KV.of(20, "3 W. Arizona"), KV.of(3, "29 School Rd"), KV.of(8, "6 Watling Rd"));    List<KV<Integer, String>> idToName = Arrays.asList(KV.of(1, "John Smith"), KV.of(2, "Sally James"), KV.of(8, "Jeffery Spalding"), KV.of(20, "Joan Lichtfield"));    PCollection<KV<Integer, String>> purchasesTable = createInput("CreateIdToPurchases", p, idToPurchases);    PCollection<KV<Integer, String>> addressTable = createInput("CreateIdToAddress", p, idToAddress);    PCollection<KV<Integer, String>> nameTable = createInput("CreateIdToName", p, idToName);    PCollection<KV<Integer, CoGbkResult>> coGbkResults = KeyedPCollectionTuple.of(namesTag, nameTable).and(addressesTag, addressTable).and(purchasesTag, purchasesTable).apply(CoGroupByKey.create());    return coGbkResults;}
public void beam_f21762_0()
{    UnionCoder unionCoder = UnionCoder.of(ImmutableList.of(StringUtf8Coder.of(), DoubleCoder.of()));    assertThat(unionCoder.getElementCoders().get(0), Matchers.equalTo(StringUtf8Coder.of()));    assertThat(unionCoder.getElementCoders().get(1), Matchers.equalTo(DoubleCoder.of()));}
public void beam_f21763_0()
{    CoderProperties.coderSerializable(UnionCoder.of(ImmutableList.of(StringUtf8Coder.of(), DoubleCoder.of())));}
public void beam_f21764_0() throws Exception
{    Schema personSchema = Schema.builder().addStringField("name").addInt32Field("height").addBooleanField("knowsJavascript").build();    PCollection<String> jsonPersons = pipeline.apply("jsonPersons", Create.of(jsonPerson("person1", "80", "true"), jsonPerson("person2", "70", "false"), jsonPerson("person3", "60", "true"), jsonPerson("person4", "50", "false"), jsonPerson("person5", "40", "true")));    PCollection<Row> personRows = jsonPersons.apply(JsonToRow.withSchema(personSchema)).setRowSchema(personSchema);    PAssert.that(personRows).containsInAnyOrder(row(personSchema, "person1", 80, true), row(personSchema, "person2", 70, false), row(personSchema, "person3", 60, true), row(personSchema, "person4", 50, false), row(personSchema, "person5", 40, true));    pipeline.run();}
public void beam_f21772_0()
{    assertEquals(TimestampedValue.atMinimumTimestamp(null), fn.createAccumulator());}
public void beam_f21773_0()
{    TimestampedValue<Long> input = TV;    assertEquals(input, fn.addInput(fn.createAccumulator(), input));}
public void beam_f21774_0()
{    TimestampedValue<Long> input = TimestampedValue.atMinimumTimestamp(1234L);    assertEquals(input, fn.addInput(fn.createAccumulator(), input));}
public void beam_f21782_0()
{    assertEquals(TV, fn.mergeAccumulators(Lists.newArrayList(TV)));}
public void beam_f21783_0()
{    ArrayList<TimestampedValue<Long>> emptyAccums = Lists.newArrayList();    assertEquals(TimestampedValue.atMinimumTimestamp(null), fn.mergeAccumulators(emptyAccums));}
public void beam_f21784_0()
{    TimestampedValue<Long> defaultAccum = fn.createAccumulator();    assertEquals(TV, fn.mergeAccumulators(Lists.newArrayList(TV, defaultAccum)));}
public void beam_f21792_0()
{    p.enableAbandonedNodeEnforcement(false);    BigEndianLongCoder inputCoder = BigEndianLongCoder.of();    PCollection<Long> output = p.apply(Create.of(1L, 2L).withCoder(inputCoder)).apply(Latest.globally());    Coder<Long> outputCoder = output.getCoder();    assertThat(outputCoder, instanceOf(NullableCoder.class));    assertEquals(inputCoder, ((NullableCoder<?>) outputCoder).getValueCoder());}
public void beam_f21793_0()
{    PCollection<Long> emptyInput = p.apply(Create.empty(VarLongCoder.of()));    PCollection<Long> output = emptyInput.apply(Latest.globally());    PAssert.that(output).containsInAnyOrder((Long) null);    p.run();}
public void beam_f21794_0()
{    PCollection<KV<String, String>> output = p.apply(Create.timestamped(TimestampedValue.of(KV.of("A", "foo"), new Instant(100)), TimestampedValue.of(KV.of("B", "bar"), new Instant(300)), TimestampedValue.of(KV.of("A", "baz"), new Instant(200)))).apply(Latest.perKey());    PAssert.that(output).containsInAnyOrder(KV.of("B", "bar"), KV.of("A", "baz"));    p.run();}
public void beam_f21802_0() throws Exception
{    PCollection<Integer> output = pipeline.apply(Create.of(1, 2, 3)).apply(MapElements.via(new SimpleFunction<Integer, Integer>() {        @Override        public Integer apply(Integer input) {            return -input;        }    }));    PAssert.that(output).containsInAnyOrder(-2, -1, -3);    pipeline.run();}
public Integer beam_f21803_0(Integer input)
{    return -input;}
public void beam_f21804_0() throws Exception
{    PCollection<Integer> output = pipeline.apply(Create.of(1, 2, 3)).apply(MapElements.via(new InferableFunction<Integer, Integer>() {        @Override        public Integer apply(Integer input) throws Exception {            return -input;        }    }));    PAssert.that(output).containsInAnyOrder(-2, -1, -3);    pipeline.run();}
public Integer beam_f21812_0(KV<Integer, String> input)
{    return 42;}
public void beam_f21813_0() throws Exception
{    pipeline.enableAbandonedNodeEnforcement(false);    pipeline.apply(Create.of(1, 2, 3)).apply("Polymorphic Identity", MapElements.via(new NestedPolymorphicInferableFunction<>())).apply("Test Consumer", MapElements.via(new InferableFunction<KV<Integer, String>, Integer>() {        @Override        public Integer apply(KV<Integer, String> input) throws Exception {            return 42;        }    }));}
public Integer beam_f21814_0(KV<Integer, String> input) throws Exception
{    return 42;}
public void beam_f21822_0()
{    ProcessFunction<Integer, Integer> processFn = input -> input;    MapElements<?, ?> processMap = MapElements.into(integers()).via(processFn);    assertThat(DisplayData.from(processMap), hasDisplayItem("class", processFn.getClass()));}
public void beam_f21823_0()
{    SimpleFunction<?, ?> simpleFn = new SimpleFunction<Integer, Integer>() {        @Override        public Integer apply(Integer input) {            return input;        }    };    MapElements<?, ?> simpleMap = MapElements.via(simpleFn);    assertThat(DisplayData.from(simpleMap), hasDisplayItem("class", simpleFn.getClass()));}
public Integer beam_f21824_0(Integer input)
{    return input;}
public void beam_f21832_0(DisplayData.Builder builder)
{    builder.add(DisplayData.item("foo", "baz"));}
public void beam_f21833_0()
{    SimpleFunction<Integer, ?> mapFn = new SimpleFunction<Integer, Integer>() {        @Override        public Integer apply(Integer input) {            return input;        }    };    MapElements<Integer, ?> map = MapElements.via(mapFn);    DisplayDataEvaluator evaluator = DisplayDataEvaluator.create();    Set<DisplayData> displayData = evaluator.displayDataForPrimitiveTransforms(map);    assertThat("MapElements should include the mapFn in its primitive display data", displayData, hasItem(hasDisplayItem("class", mapFn.getClass())));}
public Integer beam_f21834_0(Integer input)
{    return input;}
public void beam_f21842_0()
{    Result<PCollection<Integer>, KV<Integer, String>> result = pipeline.apply(Create.of(0, 1)).apply(MapElements.into(TypeDescriptors.integers()).via((Integer i) -> 1 / i).exceptionsInto(TypeDescriptors.kvs(TypeDescriptors.integers(), TypeDescriptors.strings())).exceptionsVia(f -> KV.of(f.element(), f.exception().getMessage())));    PAssert.that(result.output()).containsInAnyOrder(1);    PAssert.that(result.failures()).containsInAnyOrder(KV.of(0, "/ by zero"));    pipeline.run();}
public void beam_f21843_0()
{    Result<PCollection<Integer>, KV<Integer, String>> result = pipeline.apply(Create.of(0, 1)).apply(MapElements.into(TypeDescriptors.integers()).via((Integer i) -> 1 / i).exceptionsVia(new SimpleFunction<ExceptionElement<Integer>, KV<Integer, String>>() {        @Override        public KV<Integer, String> apply(ExceptionElement<Integer> failure) {            return KV.of(failure.element(), failure.exception().getMessage());        }    }));    PAssert.that(result.output()).containsInAnyOrder(1);    PAssert.that(result.failures()).containsInAnyOrder(KV.of(0, "/ by zero"));    pipeline.run();}
public KV<Integer, String> beam_f21844_0(ExceptionElement<Integer> failure)
{    return KV.of(failure.element(), failure.exception().getMessage());}
public void beam_f21852_0()
{    testCombineFn(Max.ofLongs(), Lists.newArrayList(1L, 2L, 3L, 4L), 4L);}
public void beam_f21853_0()
{    testCombineFn(Max.ofDoubles(), Lists.newArrayList(1.0, 2.0, 3.0, 4.0), 4.0);}
public void beam_f21854_0()
{    Top.Natural<Integer> comparer = new Top.Natural<>();    Combine.Globally<Integer, Integer> max = Max.globally(comparer);    assertThat(DisplayData.from(max), hasDisplayItem("comparer", comparer.getClass()));}
public void beam_f21862_0()
{    testCombineFn(Min.ofDoubles(), Lists.newArrayList(1.0, 2.0, 3.0, 4.0), 1.0);}
public void beam_f21863_0()
{    Top.Reversed<Integer> comparer = new Top.Reversed<>();    Combine.Globally<Integer, Integer> min = Min.globally(comparer);    assertThat(DisplayData.from(min), hasDisplayItem("comparer", comparer.getClass()));}
public void beam_f21864_0()
{    PCollectionList.of(p.apply("Impolite", Create.of(1, 2, 4))).and(p.apply("Polite", Create.of(3, 5, 6, 7))).apply(Flatten.pCollections()).apply(ParDo.of(new CallSequenceEnforcingFn<>()));    p.run();}
public void beam_f21872_0()
{    ExceptionThrowingFn fn = new ExceptionThrowingFn(MethodForException.SETUP);    p.apply(Create.of(1, 2, 3)).apply(ParDo.of(fn));    try {        p.run();        fail("Pipeline should have failed with an exception");    } catch (Exception e) {        validate();    }}
public void beam_f21873_0()
{    ExceptionThrowingFn fn = new ExceptionThrowingFn(MethodForException.START_BUNDLE);    p.apply(Create.of(1, 2, 3)).apply(ParDo.of(fn));    try {        p.run();        fail("Pipeline should have failed with an exception");    } catch (Exception e) {        validate();    }}
public void beam_f21874_0()
{    ExceptionThrowingFn fn = new ExceptionThrowingFn(MethodForException.PROCESS_ELEMENT);    p.apply(Create.of(1, 2, 3)).apply(ParDo.of(fn));    try {        p.run();        fail("Pipeline should have failed with an exception");    } catch (Exception e) {        validate();    }}
 DelayedCallStateTracker beam_f21882_0(CallState val)
{    CallState previous = callState.getAndSet(val);    if (previous == CallState.TEARDOWN && val != CallState.TEARDOWN) {        fail("illegal state change from " + callState + " to " + val);    }    if (CallState.TEARDOWN == val) {        latch.countDown();    }    return this;}
public String beam_f21883_0()
{    return "DelayedCallStateTracker{" + "latch=" + latch + ", callState=" + callState + '}';}
 CallState beam_f21884_0()
{    return callState.get();}
private void beam_f21892_0()
{    DelayedCallStateTracker previousTracker = callStateMap.put(id(), new DelayedCallStateTracker(CallState.SETUP));    if (previousTracker != null) {        fail(CallState.SETUP + " method called multiple times");    }}
private int beam_f21893_0()
{    return System.identityHashCode(this);}
private void beam_f21894_0(CallState processElement)
{    callStateMap.get(id()).update(processElement);}
public void beam_f21902_0(@Element Row row, MultiOutputReceiver r)
{    r.getRowReceiver(firstOutput).output(Row.withSchema(schema2).addValues(row.getString(0), row.getInt32(1)).build());    r.getRowReceiver(secondOutput).output(Row.withSchema(schema3).addValues(row.getString(0), row.getInt32(1)).build());}
public void beam_f21903_0(@Element Row row, OutputReceiver<String> r)
{    r.output(row.getString("string2_field") + ":" + row.getInt32("integer2_field"));}
public void beam_f21904_0(@Element Row row, OutputReceiver<String> r)
{    r.output(row.getString("string3_field") + ":" + row.getInt32("integer3_field"));}
public void beam_f21914_0()
{    List<Inferred> pojoList = Lists.newArrayList(new AutoValue_ParDoSchemaTest_Inferred("a", 1), new AutoValue_ParDoSchemaTest_Inferred("b", 2), new AutoValue_ParDoSchemaTest_Inferred("c", 3));    PCollection<String> output = pipeline.apply(Create.of(pojoList)).apply(ParDo.of(new DoFn<Inferred, String>() {        @ProcessElement        public void process(@Element Row row, OutputReceiver<String> r) {            r.output(row.getString("stringField") + ":" + row.getInt32("integerField"));        }    }));    PAssert.that(output).containsInAnyOrder("a:1", "b:2", "c:3");    pipeline.run();}
public void beam_f21915_0(@Element Row row, OutputReceiver<String> r)
{    r.output(row.getString("stringField") + ":" + row.getInt32("integerField"));}
public void beam_f21916_0()
{    List<Inferred> pojoList = Lists.newArrayList(new AutoValue_ParDoSchemaTest_Inferred("a", 1), new AutoValue_ParDoSchemaTest_Inferred("b", 2), new AutoValue_ParDoSchemaTest_Inferred("c", 3));    PCollection<Inferred> out = pipeline.apply(Create.of(pojoList)).apply(Filter.by(e -> true));    assertTrue(out.hasSchema());    pipeline.run();}
public void beam_f21924_0(@FieldAccess("stringSelector") String stringField, @FieldAccess("intSelector") int integerField, @FieldAccess("intsSelector") int[] intArray, OutputReceiver<String> r)
{    r.output(stringField + ":" + integerField + ":" + Arrays.toString(intArray));}
public void beam_f21925_0()
{    List<ForExtraction> pojoList = Lists.newArrayList(new AutoValue_ParDoSchemaTest_ForExtraction(1, "a", Lists.newArrayList(1, 2)), new AutoValue_ParDoSchemaTest_ForExtraction(2, "b", Lists.newArrayList(2, 3)), new AutoValue_ParDoSchemaTest_ForExtraction(3, "c", Lists.newArrayList(3, 4)));    List<NestedForExtraction> outerList = pojoList.stream().map(AutoValue_ParDoSchemaTest_NestedForExtraction::new).collect(Collectors.toList());    PCollection<String> output = pipeline.apply(Create.of(outerList)).apply(ParDo.of(new DoFn<NestedForExtraction, String>() {        @ProcessElement        public void process(@FieldAccess("inner.*") ForExtraction extracted, @FieldAccess("inner") ForExtraction extracted1, @FieldAccess("inner.stringField") String stringField, @FieldAccess("inner.integerField") int integerField, @FieldAccess("inner.ints") List<Integer> intArray, OutputReceiver<String> r) {            assertEquals(extracted, extracted1);            assertEquals(stringField, extracted.getStringField());            assertEquals(integerField, (int) extracted.getIntegerField());            assertEquals(intArray, extracted.getInts());            r.output(extracted.getStringField() + ":" + extracted.getIntegerField() + ":" + extracted.getInts().toString());        }    }));    PAssert.that(output).containsInAnyOrder("a:1:[1, 2]", "b:2:[2, 3]", "c:3:[3, 4]");    pipeline.run();}
public void beam_f21926_0(@FieldAccess("inner.*") ForExtraction extracted, @FieldAccess("inner") ForExtraction extracted1, @FieldAccess("inner.stringField") String stringField, @FieldAccess("inner.integerField") int integerField, @FieldAccess("inner.ints") List<Integer> intArray, OutputReceiver<String> r)
{    assertEquals(extracted, extracted1);    assertEquals(stringField, extracted.getStringField());    assertEquals(integerField, (int) extracted.getIntegerField());    assertEquals(intArray, extracted.getInts());    r.output(extracted.getStringField() + ":" + extracted.getIntegerField() + ":" + extracted.getInts().toString());}
public void beam_f21936_0(ProcessContext c)
{    throw new RuntimeException("test error in process");}
public void beam_f21938_0(FinishBundleContext c)
{    throw new RuntimeException("test error in finalize");}
public void beam_f21940_0(@Element T value, OutputReceiver<T> r)
{    r.outputWithTimestamp(value, new Instant(value.longValue()));}
public void beam_f21948_0()
{    List<Integer> inputs = Arrays.asList();    PCollection<String> output = pipeline.apply(Create.of(inputs).withCoder(VarIntCoder.of())).apply("TestDoFn", ParDo.of(new TestNoOutputDoFn()));    PAssert.that(output).empty();    pipeline.run();}
public void beam_f21949_0()
{    assertThat(ParDo.of(new PrintingDoFn()).getName(), containsString("ParDo(Printing)"));}
public void beam_f21950_0()
{    List<Integer> inputs = Arrays.asList(3, -42, 666);    PCollection<String> output = pipeline.apply(Create.of(inputs)).apply("CustomTransform", new PTransform<PCollection<Integer>, PCollection<String>>() {        @Override        public PCollection<String> expand(PCollection<Integer> input) {            return input.apply(ParDo.of(new TestDoFn()));        }    });            PAssert.that(output).satisfies(ParDoTest.HasExpectedOutput.forInput(inputs));    pipeline.run();}
public void beam_f21960_0()
{    thrown.expect(IllegalArgumentException.class);    thrown.expectMessage(GlobalWindow.class.getSimpleName());    thrown.expectMessage(IntervalWindow.class.getSimpleName());    thrown.expectMessage("window type");    thrown.expectMessage("not a supertype");    pipeline.apply(Create.of(1, 2, 3)).apply(ParDo.of(new DoFn<Integer, Integer>() {        @ProcessElement        public void process(ProcessContext c, IntervalWindow w) {        }    }));}
public void beam_f21962_0()
{    final String timerId = "gobbledegook";    pipeline.apply(Create.of(1, 2, 3)).apply(Window.into(FixedWindows.of(Duration.standardSeconds(10)))).apply(ParDo.of(new DoFn<Integer, Integer>() {        @TimerId(timerId)        private final TimerSpec spec = TimerSpecs.timer(TimeDomain.EVENT_TIME);        @ProcessElement        public void process(ProcessContext c, IntervalWindow w) {        }        @OnTimer(timerId)        public void onTimer(BoundedWindow w) {        }    }));}
public void beam_f21965_0()
{    PCollection<String> results = pipeline.apply(Create.of(1)).apply(ParDo.of(new DoFn<Integer, String>() {        @ProcessElement        public void process(OutputReceiver<String> r, PipelineOptions options) {            r.output(options.as(MyOptions.class).getFakeOption());        }    }));    String testOptionValue = "not fake anymore";    pipeline.getOptions().as(MyOptions.class).setFakeOption(testOptionValue);    PAssert.that(results).containsInAnyOrder("not fake anymore");    pipeline.run();}
public void beam_f21973_0()
{    List<Integer> inputs = Arrays.asList(3, -42, 666);    PCollectionView<Integer> sideInput1 = pipeline.apply("CreateSideInput1", Create.of(11)).apply("ViewSideInput1", View.asSingleton());    PCollectionView<Integer> sideInputUnread = pipeline.apply("CreateSideInputUnread", Create.of(-3333)).apply("ViewSideInputUnread", View.asSingleton());    PCollectionView<Integer> sideInput2 = pipeline.apply("CreateSideInput2", Create.of(222)).apply("ViewSideInput2", View.asSingleton());    PCollection<String> output = pipeline.apply(Create.of(inputs)).apply(ParDo.of(new TestDoFn(Arrays.asList(sideInput1, sideInput2), Arrays.asList())).withSideInputs(sideInput1, sideInputUnread, sideInput2));    PAssert.that(output).satisfies(ParDoTest.HasExpectedOutput.forInput(inputs).andSideInputs(11, 222));    pipeline.run();}
public void beam_f21974_0()
{        final String sideInputTag1 = "tag1";    DoFn<Integer, List<Integer>> fn = new DoFn<Integer, List<Integer>>() {        @ProcessElement        public void processElement(@SideInput(sideInputTag1) String tag1) {        }    };    thrown.expect(IllegalArgumentException.class);    PCollection<List<Integer>> output = pipeline.apply("Create main input", Create.of(2)).apply(ParDo.of(fn));    pipeline.run();}
public void beam_f21976_0()
{    final PCollectionView<Integer> sideInput1 = pipeline.apply("CreateSideInput1", Create.of(2)).apply("ViewSideInput1", View.asSingleton());        final String sideInputTag1 = "tag1";    DoFn<Integer, List<Integer>> fn = new DoFn<Integer, List<Integer>>() {        @ProcessElement        public void processElement(@SideInput(sideInputTag1) String tag1) {        }    };    thrown.expect(IllegalArgumentException.class);    PCollection<List<Integer>> output = pipeline.apply("Create main input", Create.of(2)).apply(ParDo.of(fn).withSideInput(sideInputTag1, sideInput1));    pipeline.run();}
public void beam_f21989_0(MultiOutputReceiver r, @SideInput(sideInputTag1) List<Integer> side1, @SideInput(sideInputTag2) Integer side2, @SideInput(sideInputTag3) Iterable<Integer> side3, @SideInput(sideInputTag4) Map<Integer, Integer> side4, @SideInput(sideInputTag5) Map<Integer, Iterable<Integer>> side5)
{    side1.forEach(i -> r.get(outputTag1).output(i));    r.get(outputTag2).output(side2);    side3.forEach(i -> r.get(outputTag3).output(i));    side4.forEach((k, v) -> r.get(outputTag4).output(KV.of(k, v)));    side5.forEach((k, v) -> v.forEach(v2 -> r.get(outputTag5).output(KV.of(k, v2))));}
public void beam_f21990_0()
{    List<Integer> inputs = Arrays.asList(3, -42, 666);    PCollectionView<Integer> sideInput1 = pipeline.apply("CreateSideInput1", Create.of(11)).apply("ViewSideInput1", View.asSingleton());    PCollectionView<Integer> sideInputUnread = pipeline.apply("CreateSideInputUnread", Create.of(-3333)).apply("ViewSideInputUnread", View.asSingleton());    PCollectionView<Integer> sideInput2 = pipeline.apply("CreateSideInput2", Create.of(222)).apply("ViewSideInput2", View.asSingleton());    PCollection<String> output = pipeline.apply(Create.of(inputs)).apply(ParDo.of(new TestDoFn(Arrays.asList(sideInput1, sideInput2), Arrays.asList())).withSideInputs(sideInput1).withSideInputs(sideInputUnread).withSideInputs(sideInput2));    PAssert.that(output).satisfies(ParDoTest.HasExpectedOutput.forInput(inputs).andSideInputs(11, 222));    pipeline.run();}
public void beam_f21991_0()
{    List<Integer> inputs = Arrays.asList(3, -42, 666);    final TupleTag<String> mainOutputTag = new TupleTag<String>("main") {    };    final TupleTag<Void> additionalOutputTag = new TupleTag<Void>("output") {    };    PCollectionView<Integer> sideInput1 = pipeline.apply("CreateSideInput1", Create.of(11)).apply("ViewSideInput1", View.asSingleton());    PCollectionView<Integer> sideInputUnread = pipeline.apply("CreateSideInputUnread", Create.of(-3333)).apply("ViewSideInputUnread", View.asSingleton());    PCollectionView<Integer> sideInput2 = pipeline.apply("CreateSideInput2", Create.of(222)).apply("ViewSideInput2", View.asSingleton());    PCollectionTuple outputs = pipeline.apply(Create.of(inputs)).apply(ParDo.of(new TestDoFn(Arrays.asList(sideInput1, sideInput2), Arrays.asList())).withSideInputs(sideInput1).withSideInputs(sideInputUnread).withSideInputs(sideInput2).withOutputTags(mainOutputTag, TupleTagList.of(additionalOutputTag)));    PAssert.that(outputs.get(mainOutputTag)).satisfies(ParDoTest.HasExpectedOutput.forInput(inputs).andSideInputs(11, 222));    pipeline.run();}
public void beam_f21999_0()
{    pipeline.enableAbandonedNodeEnforcement(false);    TupleTag<String> mainOutputTag = new TupleTag<String>("main") {    };    TupleTag<String> additionalOutputTag1 = new TupleTag<String>("output1") {    };    TupleTag<String> additionalOutputTag2 = new TupleTag<String>("output2") {    };    TupleTag<String> additionalOutputTag3 = new TupleTag<String>("output3") {    };    TupleTag<String> additionalOutputTagUnwritten = new TupleTag<String>("unwrittenOutput") {    };    PCollectionTuple outputs = pipeline.apply(Create.of(Arrays.asList(3, -42, 666))).setName("MyInput").apply("MyParDo", ParDo.of(new TestDoFn(Arrays.asList(), Arrays.asList(additionalOutputTag1, additionalOutputTag2, additionalOutputTag3))).withOutputTags(mainOutputTag, TupleTagList.of(additionalOutputTag3).and(additionalOutputTag1).and(additionalOutputTagUnwritten).and(additionalOutputTag2)));    assertEquals("MyParDo.main", outputs.get(mainOutputTag).getName());    assertEquals("MyParDo.output1", outputs.get(additionalOutputTag1).getName());    assertEquals("MyParDo.output2", outputs.get(additionalOutputTag2).getName());    assertEquals("MyParDo.output3", outputs.get(additionalOutputTag3).getName());    assertEquals("MyParDo.unwrittenOutput", outputs.get(additionalOutputTagUnwritten).getName());}
public void beam_f22000_0()
{    pipeline.enableAbandonedNodeEnforcement(false);    PCollection<Long> longs = pipeline.apply(GenerateSequence.from(0));    TupleTag<Long> mainOut = new TupleTag<>();    final TupleTag<String> valueAsString = new TupleTag<>();    final TupleTag<Integer> valueAsInt = new TupleTag<>();    DoFn<Long, Long> fn = new DoFn<Long, Long>() {        @ProcessElement        public void processElement(ProcessContext cxt, @Element Long element) {            cxt.output(cxt.element());            cxt.output(valueAsString, Long.toString(cxt.element()));            cxt.output(valueAsInt, element.intValue());        }    };    ParDo.MultiOutput<Long, Long> parDo = ParDo.of(fn).withOutputTags(mainOut, TupleTagList.of(valueAsString).and(valueAsInt));    PCollectionTuple firstApplication = longs.apply("first", parDo);    PCollectionTuple secondApplication = longs.apply("second", parDo);    assertThat(firstApplication, not(equalTo(secondApplication)));    assertThat(firstApplication.getAll().keySet(), Matchers.containsInAnyOrder(mainOut, valueAsString, valueAsInt));    assertThat(secondApplication.getAll().keySet(), Matchers.containsInAnyOrder(mainOut, valueAsString, valueAsInt));}
public void beam_f22001_0(ProcessContext cxt, @Element Long element)
{    cxt.output(cxt.element());    cxt.output(valueAsString, Long.toString(cxt.element()));    cxt.output(valueAsInt, element.intValue());}
public void beam_f22009_0()
{    DoFn<String, String> fn = new DoFn<String, String>() {        @ProcessElement        public void proccessElement(ProcessContext c) {        }        @Override        public void populateDisplayData(Builder builder) {            builder.add(DisplayData.item("fnMetadata", "foobar"));        }    };    ParDo.MultiOutput<String, String> parDo = ParDo.of(fn).withOutputTags(new TupleTag<>(), TupleTagList.empty());    DisplayData displayData = DisplayData.from(parDo);    assertThat(displayData, includesDisplayDataFor("fn", fn));    assertThat(displayData, hasDisplayItem("fn", fn.getClass()));}
public void beam_f22011_0(Builder builder)
{    builder.add(DisplayData.item("fnMetadata", "foobar"));}
public void beam_f22012_0()
{    List<Integer> inputs = Arrays.asList(3, -42, 666);    pipeline.apply(Create.of(inputs)).apply(ParDo.of(new TestStartBatchErrorDoFn()));    thrown.expect(RuntimeException.class);    thrown.expectMessage("test error in initialize");    pipeline.run();}
public void beam_f22020_0(@Element Integer element, MultiOutputReceiver r)
{    r.get(additionalOutputTag).outputWithTimestamp(element, new Instant(element.longValue()));}
public void beam_f22021_0()
{    PCollection<Integer> input = pipeline.apply(Create.of(Arrays.asList(3, 42, 6)));    PCollection<String> output = input.apply(ParDo.of(new TestOutputTimestampDoFn<>())).apply(ParDo.of(new TestShiftTimestampDoFn<>(Duration.millis(1000), Duration.millis(-1000)))).apply(ParDo.of(new TestFormatTimestampDoFn<>()));    PAssert.that(output).containsInAnyOrder("processing: 3, timestamp: -997", "processing: 42, timestamp: -958", "processing: 6, timestamp: -994");    pipeline.run();}
public void beam_f22022_0()
{    pipeline.apply(Create.of(Arrays.asList(3, 42, 6))).apply(ParDo.of(new TestOutputTimestampDoFn<>())).apply(ParDo.of(new TestShiftTimestampDoFn<>(    Duration.millis(1000), Duration.millis(-1001)))).apply(ParDo.of(new TestFormatTimestampDoFn<>()));    thrown.expect(RuntimeException.class);    thrown.expectMessage("Cannot output with timestamp");    thrown.expectMessage("Output timestamps must be no earlier than the timestamp of the current input");    thrown.expectMessage("minus the allowed skew (1 second).");    pipeline.run();}
public void beam_f22030_0(@Element KV<Integer, Integer> element, @StateId(stateId) ValueState<Integer> seenState, OutputReceiver<Integer> r)
{    Integer seen = MoreObjects.firstNonNull(seenState.read(), 0);    if (seen == 0) {        seenState.write(seen + 1);        r.output(element.getValue());    }}
public void beam_f22031_0()
{    final String stateId = "foo";    DoFn<String, Integer> fn = new DoFn<String, Integer>() {        @StateId(stateId)        private final StateSpec<ValueState<Integer>> intState = StateSpecs.value();        @ProcessElement        public void processElement(ProcessContext c, @StateId(stateId) ValueState<Integer> state) {        }    };    thrown.expect(IllegalArgumentException.class);    thrown.expectMessage("state");    thrown.expectMessage("KvCoder");    pipeline.apply(Create.of("hello", "goodbye", "hello again")).apply(ParDo.of(fn));}
public void beam_f22033_0()
{    final String stateId = "foo";        DoFn<KV<Double, String>, Integer> fn = new DoFn<KV<Double, String>, Integer>() {        @StateId(stateId)        private final StateSpec<ValueState<Integer>> intState = StateSpecs.value();        @ProcessElement        public void processElement(ProcessContext c, @StateId(stateId) ValueState<Integer> state) {        }    };    thrown.expect(IllegalArgumentException.class);    thrown.expectMessage("state");    thrown.expectMessage("deterministic");    pipeline.apply(Create.of(KV.of(1.0, "hello"), KV.of(5.4, "goodbye"), KV.of(7.2, "hello again"))).apply(ParDo.of(fn));}
public void beam_f22042_0()
{    final String stateId = "foo";    final TupleTag<Integer> evenTag = new TupleTag<Integer>() {    };    final TupleTag<Integer> oddTag = new TupleTag<Integer>() {    };    DoFn<KV<String, Integer>, Integer> fn = new DoFn<KV<String, Integer>, Integer>() {        @StateId(stateId)        private final StateSpec<ValueState<Integer>> intState = StateSpecs.value(VarIntCoder.of());        @ProcessElement        public void processElement(@StateId(stateId) ValueState<Integer> state, MultiOutputReceiver r) {            Integer currentValue = MoreObjects.firstNonNull(state.read(), 0);            if (currentValue % 2 == 0) {                r.get(evenTag).output(currentValue);            } else {                r.get(oddTag).output(currentValue);            }            state.write(currentValue + 1);        }    };    PCollectionTuple output = pipeline.apply(Create.of(KV.of("hello", 42), KV.of("hello", 97), KV.of("hello", 84), KV.of("goodbye", 33), KV.of("hello", 859), KV.of("goodbye", 83945))).apply(ParDo.of(fn).withOutputTags(evenTag, TupleTagList.of(oddTag)));    PCollection<Integer> evens = output.get(evenTag);    PCollection<Integer> odds = output.get(oddTag);        PAssert.that(evens).containsInAnyOrder(0, 2, 0);        PAssert.that(odds).containsInAnyOrder(1, 3, 1);    pipeline.run();}
public void beam_f22043_0(@StateId(stateId) ValueState<Integer> state, MultiOutputReceiver r)
{    Integer currentValue = MoreObjects.firstNonNull(state.read(), 0);    if (currentValue % 2 == 0) {        r.get(evenTag).output(currentValue);    } else {        r.get(oddTag).output(currentValue);    }    state.write(currentValue + 1);}
public void beam_f22044_0()
{    final String stateId = "foo";    DoFn<KV<String, Integer>, List<Integer>> fn = new DoFn<KV<String, Integer>, List<Integer>>() {        @StateId(stateId)        private final StateSpec<BagState<Integer>> bufferState = StateSpecs.bag(VarIntCoder.of());        @ProcessElement        public void processElement(@Element KV<String, Integer> element, @StateId(stateId) BagState<Integer> state, OutputReceiver<List<Integer>> r) {            ReadableState<Boolean> isEmpty = state.isEmpty();            state.add(element.getValue());            assertFalse(isEmpty.read());            Iterable<Integer> currentValue = state.read();            if (Iterables.size(currentValue) >= 4) {                                state.add(-1);                assertEquals(4, Iterables.size(currentValue));                assertEquals(5, Iterables.size(state.read()));                List<Integer> sorted = Lists.newArrayList(currentValue);                Collections.sort(sorted);                r.output(sorted);            }        }    };    PCollection<List<Integer>> output = pipeline.apply(Create.of(KV.of("hello", 97), KV.of("hello", 42), KV.of("hello", 84), KV.of("hello", 12))).apply(ParDo.of(fn));    PAssert.that(output).containsInAnyOrder(Lists.newArrayList(12, 42, 84, 97));    pipeline.run();}
public void beam_f22052_0()
{    final String stateId = "foo";    DoFn<KV<Integer, Integer>, String> fn = new DoFn<KV<Integer, Integer>, String>() {        private static final int EXPECTED_SUM = 8;        @StateId(stateId)        private final StateSpec<CombiningState<Integer, int[], Integer>> state = StateSpecs.combining(Sum.ofIntegers());        @ProcessElement        public void processElement(@Element KV<Integer, Integer> element, @StateId(stateId) GroupingState<Integer, Integer> state, OutputReceiver<String> r) {            state.add(element.getValue());            Integer currentValue = state.read();            if (currentValue == EXPECTED_SUM) {                r.output("right on");            }        }    };    PCollection<String> output = pipeline.apply(Create.of(KV.of(123, 4), KV.of(123, 7), KV.of(123, -3))).apply(ParDo.of(fn));        PAssert.that(output).containsInAnyOrder("right on");    pipeline.run();}
public void beam_f22053_0(@Element KV<Integer, Integer> element, @StateId(stateId) GroupingState<Integer, Integer> state, OutputReceiver<String> r)
{    state.add(element.getValue());    Integer currentValue = state.read();    if (currentValue == EXPECTED_SUM) {        r.output("right on");    }}
public void beam_f22054_0()
{    final PCollectionView<List<Integer>> listView = pipeline.apply("Create list for side input", Create.of(2, 1, 0)).apply(View.asList());    final String stateId = "foo";    DoFn<KV<String, Integer>, List<Integer>> fn = new DoFn<KV<String, Integer>, List<Integer>>() {        @StateId(stateId)        private final StateSpec<BagState<Integer>> bufferState = StateSpecs.bag(VarIntCoder.of());        @ProcessElement        public void processElement(ProcessContext c, @Element KV<String, Integer> element, @StateId(stateId) BagState<Integer> state, OutputReceiver<List<Integer>> r) {            state.add(element.getValue());            Iterable<Integer> currentValue = state.read();            if (Iterables.size(currentValue) >= 4) {                List<Integer> sorted = Lists.newArrayList(currentValue);                Collections.sort(sorted);                r.output(sorted);                List<Integer> sideSorted = Lists.newArrayList(c.sideInput(listView));                Collections.sort(sideSorted);                r.output(sideSorted);            }        }    };    PCollection<List<Integer>> output = pipeline.apply("Create main input", Create.of(KV.of("hello", 97), KV.of("hello", 42), KV.of("hello", 84), KV.of("hello", 12))).apply(ParDo.of(fn).withSideInputs(listView));    PAssert.that(output).containsInAnyOrder(Lists.newArrayList(12, 42, 84, 97), Lists.newArrayList(0, 1, 2));    pipeline.run();}
public void beam_f22062_0() throws Exception
{    final String stateId = "foo";    final String countStateId = "count";    Coder<MyInteger> myIntegerCoder = MyIntegerCoder.of();    DoFn<KV<String, Integer>, Set<MyInteger>> fn = new DoFn<KV<String, Integer>, Set<MyInteger>>() {        @StateId(stateId)        private final StateSpec<SetState<MyInteger>> setState = StateSpecs.set();        @StateId(countStateId)        private final StateSpec<CombiningState<Integer, int[], Integer>> countState = StateSpecs.combiningFromInputInternal(VarIntCoder.of(), Sum.ofIntegers());        @ProcessElement        public void processElement(@Element KV<String, Integer> element, @StateId(stateId) SetState<MyInteger> state, @StateId(countStateId) CombiningState<Integer, int[], Integer> count, OutputReceiver<Set<MyInteger>> r) {            state.add(new MyInteger(element.getValue()));            count.add(1);            if (count.read() >= 4) {                Set<MyInteger> set = Sets.newHashSet(state.read());                r.output(set);            }        }    };    thrown.expect(RuntimeException.class);    thrown.expectMessage("Unable to infer a coder for SetState and no Coder was specified.");    pipeline.apply(Create.of(KV.of("hello", 97), KV.of("hello", 42), KV.of("hello", 42), KV.of("hello", 12))).apply(ParDo.of(fn)).setCoder(SetCoder.of(myIntegerCoder));    pipeline.run();}
public void beam_f22063_0(@Element KV<String, Integer> element, @StateId(stateId) SetState<MyInteger> state, @StateId(countStateId) CombiningState<Integer, int[], Integer> count, OutputReceiver<Set<MyInteger>> r)
{    state.add(new MyInteger(element.getValue()));    count.add(1);    if (count.read() >= 4) {        Set<MyInteger> set = Sets.newHashSet(state.read());        r.output(set);    }}
public void beam_f22064_0()
{    final String stateId = "foo";    final String countStateId = "count";    Coder<MyInteger> myIntegerCoder = MyIntegerCoder.of();    pipeline.getCoderRegistry().registerCoderForClass(MyInteger.class, myIntegerCoder);    DoFn<KV<String, KV<String, Integer>>, KV<String, MyInteger>> fn = new DoFn<KV<String, KV<String, Integer>>, KV<String, MyInteger>>() {        @StateId(stateId)        private final StateSpec<MapState<String, MyInteger>> mapState = StateSpecs.map();        @StateId(countStateId)        private final StateSpec<CombiningState<Integer, int[], Integer>> countState = StateSpecs.combiningFromInputInternal(VarIntCoder.of(), Sum.ofIntegers());        @ProcessElement        public void processElement(@Element KV<String, KV<String, Integer>> element, @StateId(stateId) MapState<String, MyInteger> state, @StateId(countStateId) CombiningState<Integer, int[], Integer> count, OutputReceiver<KV<String, MyInteger>> r) {            KV<String, Integer> value = element.getValue();            state.put(value.getKey(), new MyInteger(value.getValue()));            count.add(1);            if (count.read() >= 4) {                Iterable<Map.Entry<String, MyInteger>> iterate = state.entries().read();                for (Map.Entry<String, MyInteger> entry : iterate) {                    r.output(KV.of(entry.getKey(), entry.getValue()));                }            }        }    };    PCollection<KV<String, MyInteger>> output = pipeline.apply(Create.of(KV.of("hello", KV.of("a", 97)), KV.of("hello", KV.of("b", 42)), KV.of("hello", KV.of("b", 42)), KV.of("hello", KV.of("c", 12)))).apply(ParDo.of(fn)).setCoder(KvCoder.of(StringUtf8Coder.of(), myIntegerCoder));    PAssert.that(output).containsInAnyOrder(KV.of("a", new MyInteger(97)), KV.of("b", new MyInteger(42)), KV.of("c", new MyInteger(12)));    pipeline.run();}
public Integer beam_f22072_0(MyInteger accumulator)
{    return accumulator.getValue();}
public void beam_f22073_0(@Element KV<String, Integer> element, @StateId(stateId) CombiningState<Integer, MyInteger, Integer> state, OutputReceiver<String> r)
{    state.add(element.getValue());    Integer currentValue = state.read();    if (currentValue == EXPECTED_SUM) {        r.output("right on");    }}
public void beam_f22074_0() throws Exception
{    final String stateId = "foo";    DoFn<KV<String, Integer>, String> fn = new DoFn<KV<String, Integer>, String>() {        private static final int EXPECTED_SUM = 16;        @StateId(stateId)        private final StateSpec<CombiningState<Integer, MyInteger, Integer>> combiningState = StateSpecs.combining(new Combine.CombineFn<Integer, MyInteger, Integer>() {            @Override            public MyInteger createAccumulator() {                return new MyInteger(0);            }            @Override            public MyInteger addInput(MyInteger accumulator, Integer input) {                return new MyInteger(accumulator.getValue() + input);            }            @Override            public MyInteger mergeAccumulators(Iterable<MyInteger> accumulators) {                int newValue = 0;                for (MyInteger myInteger : accumulators) {                    newValue += myInteger.getValue();                }                return new MyInteger(newValue);            }            @Override            public Integer extractOutput(MyInteger accumulator) {                return accumulator.getValue();            }        });        @ProcessElement        public void processElement(@Element KV<String, Integer> element, @StateId(stateId) CombiningState<Integer, MyInteger, Integer> state, OutputReceiver<String> r) {            state.add(element.getValue());            Integer currentValue = state.read();            if (currentValue == EXPECTED_SUM) {                r.output("right on");            }        }    };    thrown.expect(RuntimeException.class);    thrown.expectMessage("Unable to infer a coder for CombiningState and no Coder was specified.");    pipeline.apply(Create.of(KV.of("hello", 3), KV.of("hello", 6), KV.of("hello", 7))).apply(ParDo.of(fn));    pipeline.run();}
public void beam_f22086_0() throws Exception
{    final String timerId = "foo";    DoFn<KV<String, Integer>, Integer> fn = new DoFn<KV<String, Integer>, Integer>() {        @TimerId(timerId)        private final TimerSpec spec = TimerSpecs.timer(TimeDomain.EVENT_TIME);        @ProcessElement        public void processElement(@TimerId(timerId) Timer timer, OutputReceiver<Integer> r) {            timer.offset(Duration.standardSeconds(1)).setRelative();            r.output(3);        }        @OnTimer(timerId)        public void onTimer(TimeDomain timeDomain, OutputReceiver<Integer> r) {            if (timeDomain.equals(TimeDomain.EVENT_TIME)) {                r.output(42);            }        }    };    PCollection<Integer> output = pipeline.apply(Create.of(KV.of("hello", 37))).apply(ParDo.of(fn));    PAssert.that(output).containsInAnyOrder(3, 42);    pipeline.run();}
public void beam_f22087_0(@TimerId(timerId) Timer timer, OutputReceiver<Integer> r)
{    timer.offset(Duration.standardSeconds(1)).setRelative();    r.output(3);}
public void beam_f22088_0(TimeDomain timeDomain, OutputReceiver<Integer> r)
{    if (timeDomain.equals(TimeDomain.EVENT_TIME)) {        r.output(42);    }}
public void beam_f22096_0(@TimerId(timerId) Timer timer)
{    timer.offset(Duration.standardSeconds(1)).setRelative();}
public void beam_f22097_0(BoundedWindow window, OutputReceiver<BoundedWindow> r)
{    r.output(window);}
public TypeDescriptor<BoundedWindow> beam_f22098_0()
{    return (TypeDescriptor) TypeDescriptor.of(IntervalWindow.class);}
public void beam_f22106_0(ProcessContext context, @TimerId(timerId) Timer timer, @StateId(stateId) ValueState<String> state, BoundedWindow window)
{    timer.set(window.maxTimestamp());    state.write(context.element().getKey());    context.output(KV.of(context.element().getKey(), context.element().getValue() + offset));}
public void beam_f22107_0(@StateId(stateId) ValueState<String> state, OutputReceiver<KV<String, Integer>> r)
{    r.output(KV.of(state.read(), timerOutput));}
public void beam_f22108_0() throws Exception
{    final String timerId = "foo";    DoFn<KV<String, Integer>, Integer> fn = new DoFn<KV<String, Integer>, Integer>() {        @TimerId(timerId)        private final TimerSpec spec = TimerSpecs.timer(TimeDomain.PROCESSING_TIME);        @ProcessElement        public void processElement(@TimerId(timerId) Timer timer) {            try {                timer.set(new Instant(0));                fail("Should have failed due to processing time with absolute timer.");            } catch (RuntimeException e) {                String message = e.getMessage();                List<String> expectedSubstrings = Arrays.asList("relative timers", "processing time");                expectedSubstrings.forEach(str -> Preconditions.checkState(message.contains(str), "Pipeline didn't fail with the expected strings: %s", expectedSubstrings));            }        }        @OnTimer(timerId)        public void onTimer() {        }    };    pipeline.apply(Create.of(KV.of("hello", 37))).apply(ParDo.of(fn));    pipeline.run();}
public void beam_f22118_0(@TimerId(timerId) Timer timer, OutputReceiver<Integer> r)
{    timer.offset(Duration.standardSeconds(1)).setRelative();    r.output(3);}
public void beam_f22119_0(OutputReceiver<Integer> r)
{    r.output(42);}
public void beam_f22120_0() throws Exception
{    final String timerId = "foo";    DoFn<KV<String, Integer>, KV<Integer, Instant>> fn = new DoFn<KV<String, Integer>, KV<Integer, Instant>>() {        @TimerId(timerId)        private final TimerSpec spec = TimerSpecs.timer(TimeDomain.EVENT_TIME);        @ProcessElement        public void processElement(@TimerId(timerId) Timer timer, @Timestamp Instant timestamp, OutputReceiver<KV<Integer, Instant>> r) {            timer.align(Duration.standardSeconds(1)).offset(Duration.millis(1)).setRelative();            r.output(KV.of(3, timestamp));        }        @OnTimer(timerId)        public void onTimer(@Timestamp Instant timestamp, OutputReceiver<KV<Integer, Instant>> r) {            r.output(KV.of(42, timestamp));        }    };    TestStream<KV<String, Integer>> stream = TestStream.create(KvCoder.of(StringUtf8Coder.of(), VarIntCoder.of())).advanceWatermarkTo(new Instant(5)).addElements(KV.of("hello", 37)).advanceWatermarkTo(new Instant(0).plus(Duration.standardSeconds(1).plus(1))).advanceWatermarkToInfinity();    PCollection<KV<Integer, Instant>> output = pipeline.apply(stream).apply(ParDo.of(fn));    PAssert.that(output).containsInAnyOrder(KV.of(3, new Instant(5)), KV.of(42, new Instant(Duration.standardSeconds(1).minus(1).getMillis())));    pipeline.run();}
public void beam_f22128_0(OutputReceiver<String> r)
{    r.output("timer_output");}
public void beam_f22129_0() throws Exception
{    final String timerId = "foo";    DoFn<KV<String, String>, String> fn = new DoFn<KV<String, String>, String>() {        @TimerId(timerId)        private final TimerSpec spec = TimerSpecs.timer(TimeDomain.EVENT_TIME);        @ProcessElement        public void processElement(ProcessContext context, @TimerId(timerId) Timer timer) {            timer.offset(Duration.standardSeconds(1)).setRelative();            context.output(context.element().getValue());        }        @OnTimer(timerId)        public void onTimer(OutputReceiver<String> r) {            r.output("timer_output");        }    };    TestStream<KV<String, String>> stream = TestStream.create(KvCoder.of(StringUtf8Coder.of(), StringUtf8Coder.of())).advanceWatermarkTo(new Instant(0)).addElements(KV.of("hello", "input1")).addElements(KV.of("hello", "input2")).advanceWatermarkToInfinity();    PCollection<String> output = pipeline.apply(stream).apply(ParDo.of(fn));            PAssert.that(output).containsInAnyOrder("input1", "input2", "timer_output");    pipeline.run();}
public void beam_f22130_0(ProcessContext context, @TimerId(timerId) Timer timer)
{    timer.offset(Duration.standardSeconds(1)).setRelative();    context.output(context.element().getValue());}
public void beam_f22138_0() throws Exception
{    final String stateId = "foo";    MyIntegerCoder myIntegerCoder = MyIntegerCoder.of();    DoFn<KV<String, Integer>, MyInteger> fn = new DoFn<KV<String, Integer>, MyInteger>() {        @StateId(stateId)        private final StateSpec<ValueState<MyInteger>> intState = StateSpecs.value();        @ProcessElement        public void processElement(@StateId(stateId) ValueState<MyInteger> state, OutputReceiver<MyInteger> r) {            MyInteger currentValue = MoreObjects.firstNonNull(state.read(), new MyInteger(0));            r.output(currentValue);            state.write(new MyInteger(currentValue.getValue() + 1));        }    };    thrown.expect(RuntimeException.class);    thrown.expectMessage("Unable to infer a coder for ValueState and no Coder was specified.");    pipeline.apply(Create.of(KV.of("hello", 42), KV.of("hello", 97), KV.of("hello", 84))).apply(ParDo.of(fn)).setCoder(myIntegerCoder);    pipeline.run();}
public void beam_f22139_0(@StateId(stateId) ValueState<MyInteger> state, OutputReceiver<MyInteger> r)
{    MyInteger currentValue = MoreObjects.firstNonNull(state.read(), new MyInteger(0));    r.output(currentValue);    state.write(new MyInteger(currentValue.getValue() + 1));}
public void beam_f22140_0()
{    final String stateId = "foo";    MyIntegerCoder myIntegerCoder = MyIntegerCoder.of();    DoFn<KV<String, MyInteger>, MyInteger> fn = new DoFn<KV<String, MyInteger>, MyInteger>() {        @StateId(stateId)        private final StateSpec<ValueState<MyInteger>> intState = StateSpecs.value();        @ProcessElement        public void processElement(@StateId(stateId) ValueState<MyInteger> state, OutputReceiver<MyInteger> r) {            MyInteger currentValue = MoreObjects.firstNonNull(state.read(), new MyInteger(0));            r.output(currentValue);            state.write(new MyInteger(currentValue.getValue() + 1));        }    };    pipeline.apply(Create.of(KV.of("hello", new MyInteger(42)), KV.of("hello", new MyInteger(97)), KV.of("hello", new MyInteger(84))).withCoder(KvCoder.of(StringUtf8Coder.of(), myIntegerCoder))).apply(ParDo.of(fn)).setCoder(myIntegerCoder);    pipeline.run();}
public void beam_f22150_0(MultiOutputReceiver r)
{    r.get(mainOutputTag).output(new TestDummy());    r.get(intOutputTag).output(1);}
public int beam_f22151_0()
{    return value;}
public boolean beam_f22152_0(Object o)
{    if (this == o) {        return true;    }    if (!(o instanceof MyInteger)) {        return false;    }    MyInteger myInteger = (MyInteger) o;    return value == myInteger.value;}
public HasExpectedOutput beam_f22160_0(Integer... sideInputValues)
{    return new HasExpectedOutput(inputs, Arrays.asList(sideInputValues), additionalOutput);}
public HasExpectedOutput beam_f22161_0(TupleTag<String> outputTag)
{    return fromOutput(outputTag.getId());}
public HasExpectedOutput beam_f22162_0(String outputId)
{    return new HasExpectedOutput(inputs, sideInputs, outputId);}
public void beam_f22170_0()
{    PCollectionList<Integer> outputs = pipeline.apply(Create.of(1, 2, 4, 5)).apply(Partition.of(3, new ModFn()));    assertTrue(outputs.size() == 3);    PAssert.that(outputs.get(0)).empty();    PAssert.that(outputs.get(1)).containsInAnyOrder(1, 4);    PAssert.that(outputs.get(2)).containsInAnyOrder(2, 5);    pipeline.run();}
public void beam_f22171_0()
{    pipeline.apply(Create.of(-1)).apply(Partition.of(5, new IdentityFn()));    thrown.expect(RuntimeException.class);    thrown.expectMessage("Partition function returned out of bounds index: -1 not in [0..5)");    pipeline.run();}
public void beam_f22172_0()
{    PCollection<Integer> input = pipeline.apply(Create.of(591));    thrown.expect(IllegalArgumentException.class);    thrown.expectMessage("numPartitions must be > 0");    input.apply(Partition.of(0, new IdentityFn()));}
public void beam_f22180_0()
{    PTransform<PCollection<Integer>, PCollection<Integer>> composed = PTransform.compose("MyName", (PCollection<Integer> numbers) -> numbers);    assertEquals("MyName", composed.name);}
public void beam_f22181_0() throws Exception
{    PCollection<Integer> output = pipeline.apply(Create.of(1, 2, 3)).apply(PTransform.compose((PCollection<Integer> numbers) -> {        PCollection<Integer> inverted = numbers.apply(MapElements.into(integers()).via(input -> -input));        return PCollectionList.of(numbers).and(inverted).apply(Flatten.pCollections());    }));    PAssert.that(output).containsInAnyOrder(-2, -1, -3, 2, 1, 3);    pipeline.run();}
public void beam_f22182_0()
{    mockElement = new String("element");    mockTimestamp = new Instant(0);    MockitoAnnotations.initMocks(this);    when(mockArgumentProvider.window()).thenReturn(mockWindow);            when(mockArgumentProvider.element(Matchers.<DoFn>any())).thenReturn(mockElement);    when(mockArgumentProvider.timestamp(Matchers.<DoFn>any())).thenReturn(mockTimestamp);    when(mockArgumentProvider.outputReceiver(Matchers.<DoFn>any())).thenReturn(mockOutputReceiver);    when(mockArgumentProvider.taggedOutputReceiver(Matchers.<DoFn>any())).thenReturn(mockMultiOutputReceiver);    when(mockArgumentProvider.startBundleContext(Matchers.<DoFn>any())).thenReturn(mockStartBundleContext);    when(mockArgumentProvider.finishBundleContext(Matchers.<DoFn>any())).thenReturn(mockFinishBundleContext);    when(mockArgumentProvider.processContext(Matchers.<DoFn>any())).thenReturn(mockProcessContext);}
public void beam_f22193_0() throws Exception
{    IdentityChildWithOverride fn = mock(IdentityChildWithOverride.class);    assertEquals(stop(), invokeProcessElement(fn));    verify(fn).process(mockProcessContext);}
public void beam_f22194_0() throws Exception
{    class MockFn extends DoFn<String, String> {        @DoFn.ProcessElement        public void processElement(ProcessContext c, IntervalWindow w) throws Exception {        }    }    MockFn fn = mock(MockFn.class);    assertEquals(stop(), invokeProcessElement(fn));    verify(fn).processElement(mockProcessContext, mockWindow);}
public void beam_f22196_0() throws Exception
{    class MockFn extends DoFn<String, String> {        @DoFn.ProcessElement        public void processElement(ProcessContext c, @Element String element, @Timestamp Instant timestamp, IntervalWindow w,         OutputReceiver<String> receiver, MultiOutputReceiver multiReceiver) throws Exception {        }    }    MockFn fn = mock(MockFn.class);    assertEquals(stop(), invokeProcessElement(fn));    verify(fn).processElement(mockProcessContext, mockElement, mockTimestamp, mockWindow, mockOutputReceiver, mockMultiOutputReceiver);}
public SomeRestrictionTracker beam_f22212_0(SomeRestriction restriction)
{    return null;}
public void beam_f22213_0() throws Exception
{    class MockFn extends DoFn<String, String> {        @ProcessElement        public void processElement(ProcessContext c) {        }        @StartBundle        public void startBundle(StartBundleContext c) {        }        @FinishBundle        public void finishBundle(FinishBundleContext c) {        }        @Setup        public void before() {        }        @Teardown        public void after() {        }    }    MockFn fn = mock(MockFn.class);    DoFnInvoker<String, String> invoker = DoFnInvokers.invokerFor(fn);    invoker.invokeSetup();    invoker.invokeStartBundle(mockStartBundleContext);    invoker.invokeFinishBundle(mockFinishBundleContext);    invoker.invokeTeardown();    verify(fn).before();    verify(fn).startBundle(mockStartBundleContext);    verify(fn).finishBundle(mockFinishBundleContext);    verify(fn).after();}
public static SomeRestrictionCoder beam_f22219_0()
{    return new SomeRestrictionCoder();}
public void beam_f22229_0(SomeRestriction output)
{    outputs.add(output);}
public void beam_f22230_0(SomeRestriction output, Instant timestamp)
{    outputs.add(output);}
public DoFn<String, String>.ProcessContext beam_f22231_0(DoFn<String, String> fn)
{    return mockProcessContext;}
public void beam_f22241_0() throws Exception
{    class MockFn extends DoFn<String, String> {        @ProcessElement        public void processElement(ProcessContext c, RestrictionTracker<RestrictionWithDefaultTracker, Void> tracker) {        }        @GetInitialRestriction        public RestrictionWithDefaultTracker getInitialRestriction(String element) {            return null;        }    }    MockFn fn = mock(MockFn.class);    DoFnInvoker<String, String> invoker = DoFnInvokers.invokerFor(fn);    CoderRegistry coderRegistry = CoderRegistry.createDefault();    coderRegistry.registerCoderProvider(CoderProviders.fromStaticMethods(RestrictionWithDefaultTracker.class, CoderForDefaultTracker.class));    assertThat(invoker.<RestrictionWithDefaultTracker>invokeGetRestrictionCoder(coderRegistry), instanceOf(CoderForDefaultTracker.class));    invoker.invokeSplitRestriction("blah", "foo", new DoFn.OutputReceiver<String>() {        private boolean invoked;        @Override        public void output(String output) {            assertFalse(invoked);            invoked = true;            assertEquals("foo", output);        }        @Override        public void outputWithTimestamp(String output, Instant instant) {            assertFalse(invoked);            invoked = true;            assertEquals("foo", output);        }    });    assertEquals(stop(), invoker.invokeProcessElement(mockArgumentProvider));    assertThat(invoker.invokeNewTracker(new RestrictionWithDefaultTracker()), instanceOf(DefaultTracker.class));}
public RestrictionWithDefaultTracker beam_f22243_0(String element)
{    return null;}
public void beam_f22244_0(String output)
{    assertFalse(invoked);    invoked = true;    assertEquals("foo", output);}
public void beam_f22254_0() throws Exception
{        DoFn<String, String> fn = DoFnInvokersTestHelper.newStaticAnonymousDoFnWithTimers();    invokeOnTimer(TIMER_ID, fn);    DoFnInvokersTestHelper.verifyStaticAnonymousDoFnWithTimersInvoked(fn, mockWindow);}
public void beam_f22256_0() throws Exception
{    PrivateDoFnClass fn = mock(PrivateDoFnClass.class);    assertEquals(stop(), invokeProcessElement(fn));    verify(fn).processThis(mockProcessContext);}
public void beam_f22257_0() throws Exception
{    DoFn<String, String> fn = mock(DoFnInvokersTestHelper.newStaticPackagePrivateDoFn().getClass());    assertEquals(stop(), invokeProcessElement(fn));    DoFnInvokersTestHelper.verifyStaticPackagePrivateDoFn(fn, mockProcessContext);}
public DoFn<Integer, Integer>.ProcessContext beam_f22265_0(DoFn<Integer, Integer> fn)
{    return null;}
public void beam_f22266_0() throws Exception
{    thrown.expect(UserCodeException.class);    thrown.expectMessage("bogus");    DoFnInvokers.invokerFor(new DoFn<Integer, Integer>() {        @ProcessElement        public ProcessContinuation processElement(@SuppressWarnings("unused") ProcessContext c, RestrictionTracker<SomeRestriction, Void> tracker) {            throw new IllegalArgumentException("bogus");        }        @GetInitialRestriction        public SomeRestriction getInitialRestriction(Integer element) {            return null;        }        @NewTracker        public SomeRestrictionTracker newTracker(SomeRestriction restriction) {            return null;        }    }).invokeProcessElement(new FakeArgumentProvider<Integer, Integer>() {        @Override        public DoFn.ProcessContext processContext(DoFn<Integer, Integer> doFn) {                        return null;        }        @Override        public RestrictionTracker<?, ?> restrictionTracker() {                        return null;        }    });}
public ProcessContinuation beam_f22267_0(@SuppressWarnings("unused") ProcessContext c, RestrictionTracker<SomeRestriction, Void> tracker)
{    throw new IllegalArgumentException("bogus");}
public void beam_f22276_0(@SuppressWarnings("unused") FinishBundleContext c)
{    throw new IllegalArgumentException("bogus");}
public void beam_f22278_0() throws Exception
{    final String timerId = "my-timer-id";    class SimpleTimerDoFn extends DoFn<String, String> {        public String status = "not yet";        @TimerId(timerId)        private final TimerSpec myTimer = TimerSpecs.timer(TimeDomain.PROCESSING_TIME);        @ProcessElement        public void process(ProcessContext c) {        }        @OnTimer(timerId)        public void onMyTimer() {            status = "OK now";        }    }    SimpleTimerDoFn fn = new SimpleTimerDoFn();    DoFnInvoker<String, String> invoker = DoFnInvokers.invokerFor(fn);    invoker.invokeOnTimer(timerId, mockArgumentProvider);    assertThat(fn.status, equalTo("OK now"));}
public void beam_f22280_0()
{    status = "OK now";}
public void beam_f22292_0() throws Exception
{    thrown.expect(IllegalArgumentException.class);    thrown.expectMessage("DoFn<Integer, Integer>.ProcessContext");    thrown.expectMessage("must have type");    thrown.expectMessage("DoFn<Integer, String>.ProcessContext");    analyzeProcessElementMethod(new AnonymousMethod() {        private void method(DoFn<Integer, Integer>.ProcessContext c) {        }    });}
public void beam_f22294_0() throws Exception
{    thrown.expect(IllegalArgumentException.class);    thrown.expectMessage("DoFn<Integer, ? super Integer>.ProcessContext");    thrown.expectMessage("must have type");    thrown.expectMessage("DoFn<Integer, String>.ProcessContext");    analyzeProcessElementMethod(new AnonymousMethod() {        private void method(DoFn<Integer, ? super Integer>.ProcessContext c) {        }    });}
public void beam_f22297_0() throws Exception
{    thrown.expect(IllegalArgumentException.class);    thrown.expectMessage("DoFn<InputT, InputT>.ProcessContext");    thrown.expectMessage("must have type");    thrown.expectMessage("DoFn<InputT, OutputT>.ProcessContext");    DoFnSignatures.getSignature(BadTypeVariables.class);}
private DoFn.ProcessContinuation beam_f22309_0(DoFn<Integer, String>.ProcessContext context)
{    return null;}
public void beam_f22310_0() throws Exception
{    DoFnSignature.ProcessElementMethod signature = analyzeProcessElementMethod(new AnonymousMethod() {        private void method(DoFn<Integer, String>.ProcessContext context, SomeRestrictionTracker tracker) {        }    });    assertTrue(signature.isSplittable());    assertTrue(signature.extraParameters().stream().anyMatch(Predicates.instanceOf(DoFnSignature.Parameter.RestrictionTrackerParameter.class)::apply));    assertEquals(SomeRestrictionTracker.class, signature.trackerT().getRawType());}
public void beam_f22312_0() throws Exception
{    thrown.expect(IllegalArgumentException.class);    thrown.expectMessage("Illegal parameter");    thrown.expectMessage("BoundedWindow");    DoFnSignature.ProcessElementMethod signature = analyzeProcessElementMethod(new AnonymousMethod() {        private void method(DoFn<Integer, String>.ProcessContext context, SomeRestrictionTracker tracker, BoundedWindow window) {        }    });}
public void beam_f22323_0() throws Exception
{    class UnsplittableFn extends DoFn<Integer, String> {        @ProcessElement        public void process(ProcessContext context) {        }    }    assertEquals(PCollection.IsBounded.BOUNDED, DoFnSignatures.getSignature(UnsplittableFn.class).isBoundedPerElement());}
public void beam_f22325_0() throws Exception
{    @BoundedPerElement    class SomeFn extends DoFn<Integer, String> {        @ProcessElement        public void process(ProcessContext context) {        }    }    thrown.expectMessage("Non-splittable, but annotated as @Bounded");    DoFnSignatures.getSignature(SomeFn.class);}
public void beam_f22327_0() throws Exception
{    @UnboundedPerElement    class SomeFn extends DoFn<Integer, String> {        @ProcessElement        public void process(ProcessContext context) {        }    }    thrown.expectMessage("Non-splittable, but annotated as @Unbounded");    DoFnSignatures.getSignature(SomeFn.class);}
public RestrictionT beam_f22337_0(Integer element)
{    return null;}
public TrackerT beam_f22339_0(RestrictionT restriction)
{    return null;}
public CoderT beam_f22340_0()
{    return null;}
public void beam_f22353_0() throws Exception
{    class BadFn extends DoFn<Integer, String> {        @ProcessElement        public void process(ProcessContext context, RestrictionTracker<SomeRestriction, Void> tracker) {        }        @NewTracker        public SomeRestrictionTracker newTracker(SomeRestriction restriction) {            return null;        }        @GetInitialRestriction        public String getInitialRestriction(Integer element) {            return null;        }    }    thrown.expectMessage("getInitialRestriction(Integer): Uses restriction type String, but @NewTracker method");    thrown.expectMessage("newTracker(SomeRestriction) uses restriction type SomeRestriction");    DoFnSignatures.getSignature(BadFn.class);}
public SomeRestrictionTracker beam_f22355_0(SomeRestriction restriction)
{    return null;}
public String beam_f22356_0(Integer element)
{    return null;}
public void beam_f22367_0() throws Exception
{    thrown.expectMessage("Must have exactly 3 arguments");    DoFnSignatures.analyzeSplitRestrictionMethod(errors(), TypeDescriptor.of(FakeDoFn.class), new AnonymousMethod() {        private void method(Integer element, SomeRestriction restriction, DoFn.OutputReceiver<SomeRestriction> receiver, Object extra) {        }    }.getMethod(), TypeDescriptor.of(Integer.class));}
public void beam_f22369_0() throws Exception
{    class OtherRestriction {    }    class BadFn extends DoFn<Integer, String> {        @ProcessElement        public void process(ProcessContext context, RestrictionTracker<SomeRestriction, Void> tracker) {        }        @NewTracker        public SomeRestrictionTracker newTracker(SomeRestriction restriction) {            return null;        }        @GetInitialRestriction        public SomeRestriction getInitialRestriction(Integer element) {            return null;        }        @DoFn.SplitRestriction        public void splitRestriction(Integer element, OtherRestriction restriction, OutputReceiver<OtherRestriction> receiver) {        }    }    thrown.expectMessage("getInitialRestriction(Integer): Uses restriction type SomeRestriction, " + "but @SplitRestriction method ");    thrown.expectMessage("splitRestriction(Integer, OtherRestriction, OutputReceiver) " + "uses restriction type OtherRestriction");    DoFnSignatures.getSignature(BadFn.class);}
public SomeRestrictionTracker beam_f22371_0(SomeRestriction restriction)
{    return null;}
public void beam_f22382_0() throws Exception
{    thrown.expectMessage("Returns SomeRestrictionTracker, " + "but must return a subtype of RestrictionTracker<String, ?>");    DoFnSignatures.analyzeNewTrackerMethod(errors(), TypeDescriptor.of(FakeDoFn.class), new AnonymousMethod() {        private SomeRestrictionTracker method(String restriction) {            return null;        }    }.getMethod());}
private SomeRestrictionTracker beam_f22383_0(String restriction)
{    return null;}
public void beam_f22384_0() throws Exception
{    DoFnSignature sig = DoFnSignatures.getSignature(new DoFn<String, String>() {        @ProcessElement        public void process(ProcessContext c) {        }    }.getClass());    assertThat(sig.processElement().extraParameters().size(), equalTo(1));    assertThat(sig.processElement().extraParameters().get(0), instanceOf(ProcessContextParameter.class));}
public void beam_f22400_0() throws IllegalAccessException
{    FieldAccessDescriptor descriptor = FieldAccessDescriptor.withFieldNames("foo", "bar");    DoFn<String, String> doFn = new DoFn<String, String>() {        @FieldAccess("foo")        final FieldAccessDescriptor fieldAccess = descriptor;        @ProcessElement        public void process(@FieldAccess("foo") @Element Row row) {        }    };    DoFnSignature sig = DoFnSignatures.getSignature(doFn.getClass());    assertThat(sig.fieldAccessDeclarations().get("foo"), notNullValue());    Field field = sig.fieldAccessDeclarations().get("foo").field();    assertThat(field.getName(), equalTo("fieldAccess"));    assertThat(field.get(doFn), equalTo(descriptor));    assertFalse(sig.processElement().getSchemaElementParameters().isEmpty());}
public void beam_f22402_0()
{    DoFnSignature sig = DoFnSignatures.getSignature(new DoFn<String, String>() {        @ProcessElement        public void process(OutputReceiver<Row> rowReceiver) {        }    }.getClass());    assertThat(sig.processElement().getMainOutputReceiver().isRowReceiver(), is(true));}
public void beam_f22404_0() throws Exception
{    DoFnSignature sig = DoFnSignatures.getSignature(new DoFn<String, String>() {        @ProcessElement        @RequiresStableInput        public void process(ProcessContext c) {        }    }.getClass());    assertThat(sig.processElement().requiresStableInput(), is(true));}
public void beam_f22426_0() throws Exception
{    thrown.expect(IllegalArgumentException.class);    thrown.expectMessage("Callback");    thrown.expectMessage("undeclared timer");    thrown.expectMessage("onFoo");    thrown.expectMessage("my-id");    thrown.expectMessage(not(mentionsState()));    thrown.expectMessage(mentionsTimers());    DoFnSignatures.getSignature(new DoFn<KV<String, Integer>, Long>() {        @OnTimer("my-id")        public void onFoo() {        }        @ProcessElement        public void foo(ProcessContext context) {        }    }.getClass());}
public void beam_f22429_0() throws Exception
{    class DoFnDeclaringTimerAndProcessElement extends DoFn<KV<String, Integer>, Long> {        public static final String TIMER_ID = "my-timer-id";        @TimerId(TIMER_ID)        private final TimerSpec bizzle = TimerSpecs.timer(TimeDomain.EVENT_TIME);        @ProcessElement        public void foo(ProcessContext context) {        }    }    DoFnDeclaringTimerAndProcessElement fn = new DoFnDeclaringTimerAndProcessElement() {        @OnTimer(DoFnDeclaringTimerAndProcessElement.TIMER_ID)        public void onTimerFoo() {        }    };    thrown.expect(IllegalArgumentException.class);    thrown.expectMessage("Callback");    thrown.expectMessage("declared in a different class");    thrown.expectMessage(DoFnDeclaringTimerAndProcessElement.TIMER_ID);    thrown.expectMessage(fn.getClass().getSimpleName());    thrown.expectMessage(not(mentionsState()));    thrown.expectMessage(mentionsTimers());    DoFnSignatures.getSignature(fn.getClass());}
public void beam_f22432_0() throws Exception
{    thrown.expect(IllegalArgumentException.class);    thrown.expectMessage("process");    thrown.expectMessage("declared in a different class");    thrown.expectMessage(DoFnDeclaringTimerAndCallback.TIMER_ID);    thrown.expectMessage(not(mentionsState()));    thrown.expectMessage(mentionsTimers());    DoFnSignatures.getSignature(new DoFnDeclaringTimerAndCallback() {        @ProcessElement        public void process(ProcessContext context, @TimerId(DoFnDeclaringTimerAndCallback.TIMER_ID) Timer timer) {        }    }.getClass());}
public void beam_f22449_0() throws Exception
{    thrown.expect(IllegalArgumentException.class);    thrown.expectMessage("Duplicate");    thrown.expectMessage("TimerId");    thrown.expectMessage("my-id");    thrown.expectMessage("myfield1");    thrown.expectMessage("myfield2");        thrown.expectMessage(not(containsString("State")));    thrown.expectMessage(mentionsTimers());    DoFnSignatures.getSignature(new DoFn<KV<String, Integer>, Long>() {        @TimerId("my-id")        private final TimerSpec myfield1 = TimerSpecs.timer(TimeDomain.EVENT_TIME);        @TimerId("my-id")        private final TimerSpec myfield2 = TimerSpecs.timer(TimeDomain.EVENT_TIME);        @ProcessElement        public void foo(ProcessContext context) {        }    }.getClass());}
public void beam_f22451_0() throws Exception
{    thrown.expect(IllegalArgumentException.class);    thrown.expectMessage("Timer declarations must be final");    thrown.expectMessage("Non-final field");    thrown.expectMessage("myfield");        thrown.expectMessage(not(containsString("State")));    thrown.expectMessage(mentionsTimers());    DoFnSignatures.getSignature(new DoFn<KV<String, Integer>, Long>() {        @TimerId("my-timer-id")        private TimerSpec myfield = TimerSpecs.timer(TimeDomain.PROCESSING_TIME);        @ProcessElement        public void foo(ProcessContext context) {        }    }.getClass());}
public void beam_f22453_0() throws Exception
{    DoFnSignature sig = DoFnSignatures.getSignature(new DoFn<KV<String, Integer>, Long>() {        @TimerId("foo")        private final TimerSpec bizzle = TimerSpecs.timer(TimeDomain.EVENT_TIME);        @ProcessElement        public void foo(ProcessContext context) {        }        @OnTimer("foo")        public void onFoo() {        }    }.getClass());    assertThat(sig.timerDeclarations().size(), equalTo(1));    DoFnSignature.TimerDeclaration decl = sig.timerDeclarations().get("foo");    assertThat(decl.id(), equalTo("foo"));    assertThat(decl.field().getName(), equalTo("bizzle"));}
public void beam_f22473_0() throws Exception
{    thrown.expect(IllegalArgumentException.class);    thrown.expectMessage("undeclared");    thrown.expectMessage("my-id");    thrown.expectMessage("myProcessElement");    thrown.expectMessage("index 1");    thrown.expectMessage(not(mentionsTimers()));    DoFnSignatures.getSignature(new DoFn<KV<String, Integer>, Long>() {        @ProcessElement        public void myProcessElement(ProcessContext context, @StateId("my-id") ValueState<Integer> undeclared) {        }    }.getClass());}
public void beam_f22475_0() throws Exception
{    thrown.expect(IllegalArgumentException.class);    thrown.expectMessage("duplicate");    thrown.expectMessage("my-id");    thrown.expectMessage("myProcessElement");    thrown.expectMessage("index 2");    thrown.expectMessage(not(mentionsTimers()));    DoFnSignatures.getSignature(new DoFn<KV<String, Integer>, Long>() {        @StateId("my-id")        private final StateSpec<ValueState<Integer>> myfield = StateSpecs.value(VarIntCoder.of());        @ProcessElement        public void myProcessElement(ProcessContext context, @StateId("my-id") ValueState<Integer> one, @StateId("my-id") ValueState<Integer> two) {        }    }.getClass());}
public void beam_f22477_0() throws Exception
{    thrown.expect(IllegalArgumentException.class);    thrown.expectMessage("WatermarkHoldState");    thrown.expectMessage("reference to");    thrown.expectMessage("supertype");    thrown.expectMessage("ValueState");    thrown.expectMessage("my-id");    thrown.expectMessage("myProcessElement");    thrown.expectMessage("index 1");    thrown.expectMessage(not(mentionsTimers()));    DoFnSignatures.getSignature(new DoFn<KV<String, Integer>, Long>() {        @StateId("my-id")        private final StateSpec<ValueState<Integer>> myfield = StateSpecs.value(VarIntCoder.of());        @ProcessElement        public void myProcessElement(ProcessContext context, @StateId("my-id") WatermarkHoldState watermark) {        }    }.getClass());}
protected Void beam_f22492_0(Parameter p)
{    fail(String.format("Expected a state parameter but got %s", p));    return null;}
public Void beam_f22493_0(StateParameter stateParam)
{    assertThat(stateParam.referent(), equalTo(decl));    return null;}
public void beam_f22494_0() throws Exception
{    class DoFnForTestSimpleStateIdNamedDoFn extends DoFn<KV<String, Integer>, Long> {        @StateId("foo")        private final StateSpec<ValueState<Integer>> bizzle = StateSpecs.value(VarIntCoder.of());        @ProcessElement        public void foo(ProcessContext context) {        }    }        DoFnSignature sig = DoFnSignatures.signatureForDoFn(new DoFnForTestSimpleStateIdNamedDoFn());    assertThat(sig.stateDeclarations().size(), equalTo(1));    DoFnSignature.StateDeclaration decl = sig.stateDeclarations().get("foo");    assertThat(decl.id(), equalTo("foo"));    assertThat(decl.field(), equalTo(DoFnForTestSimpleStateIdNamedDoFn.class.getDeclaredField("bizzle")));    assertThat(decl.stateType(), Matchers.<TypeDescriptor<?>>equalTo(new TypeDescriptor<ValueState<Integer>>() {    }));}
private Matcher<String> beam_f22516_0()
{    return anyOf(containsString("state"), containsString("State"));}
 static DoFnSignatures.ErrorReporter beam_f22527_0()
{    return new DoFnSignatures.ErrorReporter(null, "[test]");}
 final Method beam_f22528_0() throws Exception
{    for (Method method : getClass().getDeclaredMethods()) {        if ("method".equals(method.getName())) {            return method;        }    }    throw new NoSuchElementException("No method named 'method' defined on " + getClass());}
public void beam_f22540_0()
{    OnTimerInvoker<Void, Void> invoker = OnTimerInvokers.forTimer(new StableNameTestDoFn(), StableNameTestDoFn.TIMER_ID);    assertThat(invoker.getClass().getName(), equalTo(String.format("%s$%s$%s$%s", StableNameTestDoFn.class.getName(), OnTimerInvoker.class.getSimpleName(), "timeridwithspecialChars", /* alphanum only; human readable but not unique */    "dGltZXItaWQud2l0aCBzcGVjaWFsQ2hhcnN7fQ")));}
public static DoFn<String, String> beam_f22545_0()
{    return new StaticPackagePrivateDoFn();}
public static void beam_f22546_0(DoFn<String, String> fn, DoFn<String, String>.ProcessContext context)
{    verify((StaticPackagePrivateDoFn) fn).process(context);}
public static void beam_f22555_0(DoFn<String, String> fn, DoFn<String, String>.ProcessContext context) throws Exception
{    DoFn<String, String> verifier = verify(fn);    verifier.getClass().getMethod("process", DoFn.ProcessContext.class).invoke(verifier, context);}
public static DoFn<String, String> beam_f22556_0()
{    return new DoFn<String, String>() {        private DoFn<String, String>.ProcessContext invokedContext;        @ProcessElement        public void process(ProcessContext c) {            assertNull("Should have been invoked just once", invokedContext);            invokedContext = c;        }        @SuppressWarnings("unused")        public void verify(DoFn<String, String>.ProcessContext context) {            assertEquals(context, invokedContext);        }    };}
public void beam_f22557_0(ProcessContext c)
{    assertNull("Should have been invoked just once", invokedContext);    invokedContext = c;}
public static void beam_f22573_0(DoFn<String, String> fn, BoundedWindow window)
{    verify((StaticPrivateDoFnWithTimers) fn).onTimer(window);}
public DoFn<String, String> beam_f22574_0()
{    return new InnerPrivateDoFnWithTimers();}
public static void beam_f22575_0(DoFn<String, String> fn, BoundedWindow window)
{    verify((InnerPrivateDoFnWithTimers) fn).onTimer(window);}
public void beam_f22586_0()
{    PCollection<String> output = p.apply(Create.of("aj", "xj", "yj", "zj")).apply(Regex.find("([xyz])", 1));    PAssert.that(output).containsInAnyOrder("x", "y", "z");    p.run();}
public void beam_f22587_0()
{    PCollection<String> output = p.apply(Create.of("a", "b", "c", "d")).apply(Regex.find("[xyz]"));    PAssert.that(output).empty();    p.run();}
public void beam_f22588_0()
{    PCollection<String> output = p.apply(Create.of("aj", "xj", "yj", "zj")).apply(Regex.find("(?<namedgroup>[xyz])", "namedgroup"));    PAssert.that(output).containsInAnyOrder("x", "y", "z");    p.run();}
public void beam_f22596_0()
{    PCollection<String> output = p.apply(Create.of("a", "b", "c", "d")).apply(Regex.matches("[xyz]"));    PAssert.that(output).empty();    p.run();}
public void beam_f22597_0()
{    PCollection<String> output = p.apply(Create.of("a", "x xxx", "x yyy", "x zzz")).apply(Regex.matches("x ([xyz]*)", 1));    PAssert.that(output).containsInAnyOrder("xxx", "yyy", "zzz");    p.run();}
public void beam_f22598_0()
{    PCollection<String> output = p.apply(Create.of("a", "x xxx", "x yyy", "x zzz")).apply(Regex.matches("x (?<namedgroup>[xyz]*)", "namedgroup"));    PAssert.that(output).containsInAnyOrder("xxx", "yyy", "zzz");    p.run();}
public void beam_f22606_0()
{    PCollection<String> output = p.apply(Create.of("abc", "xj", "yj", "zj", "def")).apply(Regex.replaceAll("[xyz]", "new"));    PAssert.that(output).containsInAnyOrder("abc", "newj", "newj", "newj", "def");    p.run();}
public void beam_f22607_0()
{    PCollection<String> output = p.apply(Create.of("xjx", "yjy", "zjz")).apply(Regex.replaceFirst("[xyz]", "new"));    PAssert.that(output).containsInAnyOrder("newjx", "newjy", "newjz");    p.run();}
public void beam_f22608_0()
{    PCollection<String> output = p.apply(Create.of("abc", "xjx", "yjy", "zjz", "def")).apply(Regex.replaceFirst("[xyz]", "new"));    PAssert.that(output).containsInAnyOrder("abc", "newjx", "newjy", "newjz", "def");    p.run();}
public void beam_f22616_0()
{    PCollection<ValueInSingleWindow<String>> result = pipeline.apply(TestStream.create(StringUtf8Coder.of()).addElements(TimestampedValue.of("dei", new Instant(123L))).advanceWatermarkToInfinity()).apply(Reify.windows());    PAssert.that(result).containsInAnyOrder(ValueInSingleWindow.of("dei", new Instant(123L), GlobalWindow.INSTANCE, PaneInfo.NO_FIRING));    pipeline.run();}
public void beam_f22617_0()
{    PCollection<KV<String, Integer>> timestamped = pipeline.apply(Create.of(KV.of("foo", 0), KV.of("foo", 1), KV.of("bar", 2), KV.of("baz", 3))).apply(TIMESTAMP_FROM_V);    PCollection<KV<String, TimestampedValue<Integer>>> reified = timestamped.apply(Reify.timestampsInValue());    PAssert.that(reified).containsInAnyOrder(KV.of("foo", TimestampedValue.of(0, new Instant(0))), KV.of("foo", TimestampedValue.of(1, new Instant(1))), KV.of("bar", TimestampedValue.of(2, new Instant(2))), KV.of("baz", TimestampedValue.of(3, new Instant(3))));    pipeline.run();}
public void beam_f22618_0()
{    PCollection<String> timestamped = pipeline.apply(Create.timestamped(TimestampedValue.of("foo", new Instant(0L)), TimestampedValue.of("bar", new Instant(1L))));    PCollection<TimestampedValue<String>> reified = timestamped.apply(Reify.timestamps());    PAssert.that(reified).containsInAnyOrder(TimestampedValue.of("foo", new Instant(0)), TimestampedValue.of("bar", new Instant(1)));    pipeline.run();}
public void beam_f22626_0()
{    PCollection<KV<String, Integer>> input = pipeline.apply(Create.of(ARBITRARY_KVS).withCoder(KvCoder.of(StringUtf8Coder.of(), VarIntCoder.of())));    PCollection<KV<String, Integer>> output = input.apply(Reshuffle.of());    PAssert.that(output).containsInAnyOrder(ARBITRARY_KVS);    assertEquals(input.getWindowingStrategy(), output.getWindowingStrategy());    pipeline.run();}
public void beam_f22627_0()
{    PCollection<KV<String, TimestampedValue<String>>> input = pipeline.apply(Create.timestamped(TimestampedValue.of("foo", BoundedWindow.TIMESTAMP_MIN_VALUE), TimestampedValue.of("foo", new Instant(0)), TimestampedValue.of("bar", new Instant(33)), TimestampedValue.of("bar", GlobalWindow.INSTANCE.maxTimestamp())).withCoder(StringUtf8Coder.of())).apply(WithKeys.<String, String>of(input12 -> input12).withKeyType(TypeDescriptors.strings())).apply("ReifyOriginalTimestamps", Reify.timestampsInValue());            PCollection<TimestampedValue<TimestampedValue<String>>> output = input.apply(Reshuffle.of()).apply("ReifyReshuffledTimestamps", Reify.timestampsInValue()).apply(Values.create());    PAssert.that(output).satisfies(input1 -> {        for (TimestampedValue<TimestampedValue<String>> elem : input1) {            Instant originalTimestamp = elem.getValue().getTimestamp();            Instant afterReshuffleTimestamp = elem.getTimestamp();            assertThat("Reshuffle must preserve element timestamps", afterReshuffleTimestamp, equalTo(originalTimestamp));        }        return null;    });    pipeline.run();}
public void beam_f22628_0()
{    PCollection<KV<String, Iterable<Integer>>> input = pipeline.apply(Create.of(GBK_TESTABLE_KVS).withCoder(KvCoder.of(StringUtf8Coder.of(), VarIntCoder.of()))).apply(Window.into(Sessions.withGapDuration(Duration.standardMinutes(10)))).apply(GroupByKey.create());    PCollection<KV<String, Iterable<Integer>>> output = input.apply(Reshuffle.of());    PAssert.that(output).satisfies(new AssertThatHasExpectedContents());    assertEquals(input.getWindowingStrategy(), output.getWindowingStrategy());    pipeline.run();}
 void beam_f22636_0(final List<String> lines, int limit)
{    checkArgument(new HashSet<>(lines).size() == lines.size(), "Duplicates are unsupported.");    PCollection<String> input = p.apply(Create.of(lines).withCoder(StringUtf8Coder.of()));    PCollection<String> output = input.apply(Sample.any(limit));    PAssert.that(output).satisfies(new VerifyAnySample(lines, limit));    p.run();}
public void beam_f22637_0()
{    runPickAnyTest(lines, limit);}
public void beam_f22638_0()
{    CombineFnTester.testCombineFn(Sample.combineFn(limit), lines, allOf(Matchers.<String>iterableWithSize(Math.min(lines.size(), limit)), everyItem(is(in(lines)))));}
public void beam_f22646_0()
{    PCollection<Integer> input = pipeline.apply(Create.of(ImmutableList.copyOf(DATA)).withCoder(BigEndianIntegerCoder.of()));    PCollection<Iterable<Integer>> output = input.apply(Sample.fixedSizeGlobally(3));    PAssert.thatSingletonIterable(output).satisfies(new VerifyCorrectSample<>(3, DATA));    pipeline.run();}
public void beam_f22647_0()
{    PCollection<Integer> input = pipeline.apply(Create.empty(BigEndianIntegerCoder.of()));    PCollection<Iterable<Integer>> output = input.apply(Sample.fixedSizeGlobally(3));    PAssert.thatSingletonIterable(output).satisfies(new VerifyCorrectSample<>(0, EMPTY));    pipeline.run();}
public void beam_f22648_0()
{    PCollection<Integer> input = pipeline.apply(Create.of(ImmutableList.copyOf(DATA)).withCoder(BigEndianIntegerCoder.of()));    PCollection<Iterable<Integer>> output = input.apply(Sample.fixedSizeGlobally(0));    PAssert.thatSingletonIterable(output).satisfies(new VerifyCorrectSample<>(0, DATA));    pipeline.run();}
public void beam_f22656_0() throws Exception
{    SimpleFunction<Integer, String> fn = new SimpleFunction<Integer, String>(SimpleFunctionTest::toStringThisThing) {    };    assertThat(fn.getInputTypeDescriptor(), equalTo(TypeDescriptors.integers()));    assertThat(fn.getOutputTypeDescriptor(), equalTo(TypeDescriptors.strings()));}
private static String beam_f22657_0(Integer i)
{    return i.toString();}
public void beam_f22658_0()
{    assertEquals(new Instant(1000), Min.<Instant>naturalOrder().apply(Arrays.asList(new Instant(1000), new Instant(2000))));    assertEquals(null, Min.<Instant>naturalOrder().apply(Collections.emptyList()));    assertEquals(new Instant(5000), Min.naturalOrder(new Instant(5000)).apply(Collections.emptyList()));    assertEquals(new Instant(2000), Max.<Instant>naturalOrder().apply(Arrays.asList(new Instant(1000), new Instant(2000))));    assertEquals(null, Max.<Instant>naturalOrder().apply(Collections.emptyList()));    assertEquals(new Instant(5000), Max.naturalOrder(new Instant(5000)).apply(Collections.emptyList()));}
public void beam_f22666_0() throws Exception
{    ByteKeyRangeTracker tracker = ByteKeyRangeTracker.of(ByteKeyRange.of(ByteKey.of(0x10), ByteKey.of(0xc0)));    assertTrue(tracker.tryClaim(ByteKey.of(0x10)));    ByteKeyRange checkpoint = tracker.checkpoint();    assertEquals(ByteKeyRange.of(ByteKey.of(0x10), ByteKey.of(0x10, 0x00)), tracker.currentRestriction());    assertEquals(ByteKeyRange.of(ByteKey.of(0x10, 0x00), ByteKey.of(0xc0)), checkpoint);}
public void beam_f22667_0() throws Exception
{    ByteKeyRangeTracker tracker = ByteKeyRangeTracker.of(ByteKeyRange.of(ByteKey.of(0x10), ByteKey.of(0xc0)));    assertTrue(tracker.tryClaim(ByteKey.of(0x50)));    assertTrue(tracker.tryClaim(ByteKey.of(0x90)));    ByteKeyRange checkpoint = tracker.checkpoint();    assertEquals(ByteKeyRange.of(ByteKey.of(0x10), ByteKey.of(0x90, 0x00)), tracker.currentRestriction());    assertEquals(ByteKeyRange.of(ByteKey.of(0x90, 0x00), ByteKey.of(0xc0)), checkpoint);}
public void beam_f22668_0() throws Exception
{    ByteKeyRangeTracker tracker = ByteKeyRangeTracker.of(ByteKeyRange.of(ByteKey.of(0x10), ByteKey.of(0xc0)));    assertTrue(tracker.tryClaim(ByteKey.of(0x50)));    assertTrue(tracker.tryClaim(ByteKey.of(0x90)));    assertFalse(tracker.tryClaim(ByteKey.of(0xc0)));    ByteKeyRange checkpoint = tracker.checkpoint();    assertEquals(ByteKeyRange.of(ByteKey.of(0x10), ByteKey.of(0xc0)), tracker.currentRestriction());    assertEquals(ByteKeyRangeTracker.NO_KEYS, checkpoint);}
public void beam_f22676_0()
{    ByteKeyRangeTracker tracker = ByteKeyRangeTracker.of(ByteKeyRange.of(ByteKey.of(0x10), ByteKey.EMPTY));    assertTrue(tracker.tryClaim(ByteKey.of(0x50)));    assertTrue(tracker.tryClaim(ByteKey.of(0x90)));    assertFalse(tracker.tryClaim(ByteKey.EMPTY));    tracker.checkDone();}
public void beam_f22677_0()
{    ByteKeyRangeTracker tracker = ByteKeyRangeTracker.of(ByteKeyRange.of(ByteKey.of(0x10), ByteKey.of(0xc0)));    assertTrue(tracker.tryClaim(ByteKey.of(0x50)));    assertTrue(tracker.tryClaim(ByteKey.of(0x90)));    assertTrue(tracker.tryClaim(ByteKey.of(0xbf)));    expected.expectMessage("Last attempted key was [bf] in range ByteKeyRange{startKey=[10], endKey=[c0]}, " + "claiming work in [[bf00], [c0]) was not attempted");    tracker.checkDone();}
public void beam_f22678_0()
{    ByteKeyRangeTracker tracker = ByteKeyRangeTracker.of(ByteKeyRangeTracker.NO_KEYS);    tracker.checkDone();}
public void beam_f22686_0() throws Exception
{    OffsetRangeTracker tracker = new OffsetRangeTracker(new OffsetRange(100, 200));    expected.expect(IllegalStateException.class);    tracker.checkpoint();}
public void beam_f22687_0() throws Exception
{    OffsetRangeTracker tracker = new OffsetRangeTracker(new OffsetRange(100, 200));    assertFalse(tracker.tryClaim(250L));    expected.expect(IllegalStateException.class);    OffsetRange checkpoint = tracker.checkpoint();}
public void beam_f22688_0() throws Exception
{    OffsetRangeTracker tracker = new OffsetRangeTracker(new OffsetRange(100, 200));    assertTrue(tracker.tryClaim(100L));    OffsetRange checkpoint = tracker.checkpoint();    assertEquals(new OffsetRange(100, 101), tracker.currentRestriction());    assertEquals(new OffsetRange(101, 200), checkpoint);}
public void beam_f22696_0()
{    OffsetRangeTracker tracker = new OffsetRangeTracker(new OffsetRange(100, 200));    assertTrue(tracker.tryClaim(150L));    assertTrue(tracker.tryClaim(175L));    assertTrue(tracker.tryClaim(199L));    tracker.checkDone();}
public void beam_f22697_0()
{    OffsetRangeTracker tracker = new OffsetRangeTracker(new OffsetRange(100, 200));    assertTrue(tracker.tryClaim(150L));    assertTrue(tracker.tryClaim(175L));    expected.expectMessage("Last attempted offset was 175 in range [100, 200), " + "claiming work in [176, 200) was not attempted");    tracker.checkDone();}
public void beam_f22698_0()
{    OffsetRangeTracker tracker = new OffsetRangeTracker(new OffsetRange(0, 200));    assertEquals(200, tracker.getSize(), 0.001);    tracker = new OffsetRangeTracker(new OffsetRange(100, 200));    assertEquals(100, tracker.getSize(), 0.001);}
public void beam_f22706_0()
{    testPairWithIndexBasic(IsBounded.UNBOUNDED);}
private void beam_f22707_0(IsBounded bounded)
{    PCollection<KV<String, Integer>> res = p.apply(Create.of("a", "bb", "ccccc")).apply(ParDo.of(pairStringWithIndexToLengthFn(bounded))).setCoder(KvCoder.of(StringUtf8Coder.of(), BigEndianIntegerCoder.of()));    PAssert.that(res).containsInAnyOrder(Arrays.asList(KV.of("a", 0), KV.of("bb", 0), KV.of("bb", 1), KV.of("ccccc", 0), KV.of("ccccc", 1), KV.of("ccccc", 2), KV.of("ccccc", 3), KV.of("ccccc", 4)));    p.run();}
public void beam_f22708_0()
{    testPairWithIndexWindowedTimestamped(IsBounded.BOUNDED);}
public void beam_f22716_0()
{    testOutputAfterCheckpoint(IsBounded.UNBOUNDED);}
private void beam_f22717_0(IsBounded bounded)
{    PCollection<Integer> outputs = p.apply(Create.of("foo")).apply(ParDo.of(sdfWithMultipleOutputsPerBlock(bounded, 3))).apply(Window.<Integer>configure().triggering(Never.ever()).discardingFiredPanes());    PAssert.thatSingleton(outputs.apply(Count.globally())).isEqualTo((long) SDFWithMultipleOutputsPerBlockBase.MAX_INDEX);    p.run();}
public void beam_f22718_0(ProcessContext c, RestrictionTracker<OffsetRange, Long> tracker)
{    checkState(tracker.tryClaim(tracker.currentRestriction().getFrom()));    String side = c.sideInput(sideInput);    c.output(side + ":" + c.element());}
private void beam_f22726_0(IsBounded bounded)
{    PCollection<Integer> mainInput = p.apply("main", Create.timestamped(TimestampedValue.of(0, new Instant(0)), TimestampedValue.of(1, new Instant(1)), TimestampedValue.of(2, new Instant(2)), TimestampedValue.of(3, new Instant(3)), TimestampedValue.of(4, new Instant(4)), TimestampedValue.of(5, new Instant(5)), TimestampedValue.of(6, new Instant(6)), TimestampedValue.of(7, new Instant(7)))).apply("window 2", Window.into(FixedWindows.of(Duration.millis(2))));    PCollectionView<String> sideInput = p.apply("side", Create.timestamped(TimestampedValue.of("a", new Instant(0)), TimestampedValue.of("b", new Instant(4)))).apply("window 4", Window.into(FixedWindows.of(Duration.millis(4)))).apply("singleton", View.asSingleton());    PCollection<String> res = mainInput.apply(ParDo.of(sdfWithSideInput(bounded, sideInput)).withSideInputs(sideInput));    PAssert.that(res).containsInAnyOrder("a:0", "a:1", "a:2", "a:3", "b:4", "b:5", "b:6", "b:7");    p.run();}
private static int beam_f22727_0(int index, int[] blockStarts)
{    for (int i = 1; i < blockStarts.length; ++i) {        if (index > blockStarts[i - 1] && index <= blockStarts[i]) {            return i;        }    }    throw new IllegalStateException("Shouldn't get here");}
public ProcessContinuation beam_f22728_0(ProcessContext c, RestrictionTracker<OffsetRange, Long> tracker)
{    int[] blockStarts = { -1, 0, 12, 123, 1234, 12345, 34567, MAX_INDEX };    int trueStart = snapToNextBlock((int) tracker.currentRestriction().getFrom(), blockStarts);    for (int i = trueStart, numIterations = 1; tracker.tryClaim((long) blockStarts[i]); ++i, ++numIterations) {        for (int index = blockStarts[i]; index < blockStarts[i + 1]; ++index) {            c.output(KV.of(c.sideInput(sideInput) + ":" + c.element(), index));        }        if (numIterations == numClaimsPerCall) {            return resume();        }    }    return stop();}
private static SDFWithAdditionalOutputBase beam_f22736_0(IsBounded bounded, TupleTag<String> additionalOutput)
{    return (bounded == IsBounded.BOUNDED) ? new SDFWithAdditionalOutputBounded(additionalOutput) : new SDFWithAdditionalOutputUnbounded(additionalOutput);}
public void beam_f22737_0()
{    testAdditionalOutput(IsBounded.BOUNDED);}
public void beam_f22738_0()
{    testAdditionalOutput(IsBounded.UNBOUNDED);}
public void beam_f22746_0()
{    assertEquals(State.INSIDE_BUNDLE, state);    state = State.OUTSIDE_BUNDLE;}
public void beam_f22747_0()
{    assertEquals(State.OUTSIDE_BUNDLE, state);    state = State.TORN_DOWN;}
private static SDFWithLifecycleBase beam_f22748_0(IsBounded bounded)
{    return (bounded == IsBounded.BOUNDED) ? new SDFWithLifecycleBounded() : new SDFWithLifecycleUnbounded();}
public void beam_f22757_0()
{    assertEquals("Combine.globally(SumInteger)", Sum.integersGlobally().getName());    assertEquals("Combine.globally(SumDouble)", Sum.doublesGlobally().getName());    assertEquals("Combine.globally(SumLong)", Sum.longsGlobally().getName());    assertEquals("Combine.perKey(SumInteger)", Sum.integersPerKey().getName());    assertEquals("Combine.perKey(SumDouble)", Sum.doublesPerKey().getName());    assertEquals("Combine.perKey(SumLong)", Sum.longsPerKey().getName());}
public void beam_f22758_0()
{    testCombineFn(Sum.ofIntegers(), Lists.newArrayList(1, 2, 3, 4), 10);}
public void beam_f22759_0()
{    testCombineFn(Sum.ofLongs(), Lists.newArrayList(1L, 2L, 3L, 4L), 10L);}
public void beam_f22767_0()
{    PCollection<String> input = p.apply(Create.of(Arrays.asList(COLLECTION)).withCoder(StringUtf8Coder.of()));    PCollection<List<String>> top1 = input.apply(Top.of(0, new OrderByLength()));    PCollection<List<String>> top2 = input.apply(Top.largest(0));    PCollection<List<String>> top3 = input.apply(Top.smallest(0));    PCollection<KV<String, Integer>> inputTable = createInputTable(p);    PCollection<KV<String, List<Integer>>> largestPerKey = inputTable.apply(Top.largestPerKey(0));    PCollection<KV<String, List<Integer>>> smallestPerKey = inputTable.apply(Top.smallestPerKey(0));    PAssert.thatSingletonIterable(top1).empty();    PAssert.thatSingletonIterable(top2).empty();    PAssert.thatSingletonIterable(top3).empty();    PAssert.that(largestPerKey).containsInAnyOrder(KV.of("a", Arrays.asList()), KV.of("b", Arrays.asList()));    PAssert.that(smallestPerKey).containsInAnyOrder(KV.of("a", Arrays.asList()), KV.of("b", Arrays.asList()));    p.run();}
public void beam_f22768_0()
{    p.enableAbandonedNodeEnforcement(false);    p.apply("CreateCollection", Create.of(Arrays.asList(COLLECTION)).withCoder(StringUtf8Coder.of()));    PCollection<KV<String, Integer>> inputTable = createInputTable(p);    inputTable.apply(Top.perKey(1, new IntegerComparator()));    inputTable.apply("PerKey2", Top.perKey(1, new IntegerComparator2()));}
public void beam_f22769_0()
{    p.enableAbandonedNodeEnforcement(false);    PCollection<String> input = p.apply(Create.of(Arrays.asList(COLLECTION)).withCoder(StringUtf8Coder.of()));    expectedEx.expect(IllegalArgumentException.class);    expectedEx.expectMessage(Matchers.containsString(">= 0"));    input.apply(Top.of(-1, new OrderByLength()));}
public void beam_f22777_0()
{    ArrayList<KV<String, Integer>> kvs = new ArrayList<>();    kvs.add(KV.of("one", 1));    kvs.add(KV.of("two", 2));    ArrayList<String> expected = new ArrayList<>();    expected.add("one\t1");    expected.add("two\t2");    PCollection<KV<String, Integer>> input = p.apply(Create.of(kvs));    PCollection<String> output = input.apply(ToString.kvs("\t"));    PAssert.that(output).containsInAnyOrder(expected);    p.run();}
public void beam_f22778_0()
{    ArrayList<Iterable<String>> iterables = new ArrayList<>();    iterables.add(Arrays.asList(new String[] { "one", "two", "three" }));    iterables.add(Arrays.asList(new String[] { "four", "five", "six" }));    ArrayList<String> expected = new ArrayList<>();    expected.add("one,two,three");    expected.add("four,five,six");    PCollection<Iterable<String>> input = p.apply(Create.of(iterables).withCoder(IterableCoder.of(StringUtf8Coder.of())));    PCollection<String> output = input.apply(ToString.iterables());    PAssert.that(output).containsInAnyOrder(expected);    p.run();}
public void beam_f22779_0()
{    ArrayList<Iterable<String>> iterables = new ArrayList<>();    iterables.add(Arrays.asList(new String[] { "one", "two", "three" }));    iterables.add(Arrays.asList(new String[] { "four", "five", "six" }));    ArrayList<String> expected = new ArrayList<>();    expected.add("one\ttwo\tthree");    expected.add("four\tfive\tsix");    PCollection<Iterable<String>> input = p.apply(Create.of(iterables).withCoder(IterableCoder.of(StringUtf8Coder.of())));    PCollection<String> output = input.apply(ToString.iterables("\t"));    PAssert.that(output).containsInAnyOrder(expected);    p.run();}
public void beam_f22787_0() throws Exception
{    final PCollectionView<Integer> view = pipeline.apply("CreateEmptyIntegers", Create.empty(VarIntCoder.of())).apply(View.asSingleton());    pipeline.apply("Create123", Create.of(1, 2, 3)).apply("OutputSideInputs", ParDo.of(new DoFn<Integer, Integer>() {        @ProcessElement        public void processElement(ProcessContext c) {            c.output(c.sideInput(view));        }    }).withSideInputs(view));    thrown.expect(PipelineExecutionException.class);    thrown.expectCause(isA(NoSuchElementException.class));    thrown.expectMessage("Empty");    thrown.expectMessage("PCollection");    thrown.expectMessage("singleton");    pipeline.run();}
public void beam_f22788_0(ProcessContext c)
{    c.output(c.sideInput(view));}
public void beam_f22789_0() throws Exception
{    PCollection<Integer> oneTwoThree = pipeline.apply(Create.of(1, 2, 3));    final PCollectionView<Integer> view = oneTwoThree.apply(View.asSingleton());    oneTwoThree.apply("OutputSideInputs", ParDo.of(new DoFn<Integer, Integer>() {        @ProcessElement        public void processElement(ProcessContext c) {            c.output(c.sideInput(view));        }    }).withSideInputs(view));    thrown.expect(PipelineExecutionException.class);    thrown.expectCause(isA(IllegalArgumentException.class));    thrown.expectMessage("PCollection");    thrown.expectMessage("more than one");    thrown.expectMessage("singleton");    pipeline.run();}
public void beam_f22797_0()
{    final PCollectionView<List<Integer>> view = pipeline.apply("CreateSideInput", Create.of(11)).apply(View.asList());    PCollection<Integer> output = pipeline.apply("CreateMainInput", Create.of(29)).apply("OutputSideInputs", ParDo.of(new DoFn<Integer, Integer>() {        @ProcessElement        public void processElement(ProcessContext c) {            try {                c.sideInput(view).clear();                fail("Expected UnsupportedOperationException on clear()");            } catch (UnsupportedOperationException expected) {            }            try {                c.sideInput(view).add(4);                fail("Expected UnsupportedOperationException on add()");            } catch (UnsupportedOperationException expected) {            }            try {                c.sideInput(view).addAll(new ArrayList<>());                fail("Expected UnsupportedOperationException on addAll()");            } catch (UnsupportedOperationException expected) {            }            try {                c.sideInput(view).remove(0);                fail("Expected UnsupportedOperationException on remove()");            } catch (UnsupportedOperationException expected) {            }            for (Integer i : c.sideInput(view)) {                c.output(i);            }        }    }).withSideInputs(view));        PAssert.that(output).containsInAnyOrder(11);    pipeline.run();}
public void beam_f22798_0(ProcessContext c)
{    try {        c.sideInput(view).clear();        fail("Expected UnsupportedOperationException on clear()");    } catch (UnsupportedOperationException expected) {    }    try {        c.sideInput(view).add(4);        fail("Expected UnsupportedOperationException on add()");    } catch (UnsupportedOperationException expected) {    }    try {        c.sideInput(view).addAll(new ArrayList<>());        fail("Expected UnsupportedOperationException on addAll()");    } catch (UnsupportedOperationException expected) {    }    try {        c.sideInput(view).remove(0);        fail("Expected UnsupportedOperationException on remove()");    } catch (UnsupportedOperationException expected) {    }    for (Integer i : c.sideInput(view)) {        c.output(i);    }}
public void beam_f22799_0()
{    final PCollectionView<Iterable<Integer>> view = pipeline.apply("CreateSideInput", Create.of(11, 13, 17, 23)).apply(View.asIterable());    PCollection<Integer> output = pipeline.apply("CreateMainInput", Create.of(29, 31)).apply("OutputSideInputs", ParDo.of(new DoFn<Integer, Integer>() {        @ProcessElement        public void processElement(ProcessContext c) {            for (Integer i : c.sideInput(view)) {                c.output(i);            }        }    }).withSideInputs(view));    PAssert.that(output).containsInAnyOrder(11, 13, 17, 23, 11, 13, 17, 23);    pipeline.run();}
public void beam_f22807_0()
{    final PCollectionView<Map<String, Iterable<Integer>>> view = pipeline.apply("CreateSideInput", Create.of(KV.of("a", 1), KV.of("a", 1), KV.of("a", 2), KV.of("b", 3))).apply(View.asMultimap());    PCollection<KV<String, Integer>> output = pipeline.apply("CreateMainInput", Create.of("apple", "banana", "blackberry")).apply("OutputSideInputs", ParDo.of(new DoFn<String, KV<String, Integer>>() {        @ProcessElement        public void processElement(ProcessContext c) {            for (Integer v : c.sideInput(view).get(c.element().substring(0, 1))) {                c.output(KV.of(c.element(), v));            }        }    }).withSideInputs(view));    PAssert.that(output).containsInAnyOrder(KV.of("apple", 1), KV.of("apple", 1), KV.of("apple", 2), KV.of("banana", 3), KV.of("blackberry", 3));    pipeline.run();}
public void beam_f22808_0(ProcessContext c)
{    for (Integer v : c.sideInput(view).get(c.element().substring(0, 1))) {        c.output(KV.of(c.element(), v));    }}
public void beam_f22809_0()
{    final PCollectionView<Map<String, Iterable<Integer>>> view = pipeline.apply("CreateSideInput", Create.of(KV.of("a", 1), KV.of("a", 1), KV.of("a", 2), KV.of("b", 3))).apply(View.asMultimap());    PCollection<KV<String, Integer>> output = pipeline.apply("CreateMainInput", Create.of(2)).apply("OutputSideInputs", ParDo.of(new DoFn<Integer, KV<String, Integer>>() {        @ProcessElement        public void processElement(ProcessContext c) {            assertEquals((int) c.element(), c.sideInput(view).size());            assertEquals((int) c.element(), c.sideInput(view).entrySet().size());            for (Entry<String, Iterable<Integer>> entry : c.sideInput(view).entrySet()) {                for (Integer value : entry.getValue()) {                    c.output(KV.of(entry.getKey(), value));                }            }        }    }).withSideInputs(view));    PAssert.that(output).containsInAnyOrder(KV.of("a", 1), KV.of("a", 1), KV.of("a", 2), KV.of("b", 3));    pipeline.run();}
public void beam_f22817_0(ProcessContext c)
{    for (Integer v : c.sideInput(view).get(c.element().substring(0, 1))) {        c.output(KV.of(c.element(), v));    }}
public void beam_f22818_0()
{    final PCollectionView<Map<String, Iterable<Integer>>> view = pipeline.apply("CreateSideInput", Create.timestamped(TimestampedValue.of(KV.of("a", 1), new Instant(1)), TimestampedValue.of(KV.of("a", 1), new Instant(2)), TimestampedValue.of(KV.of("a", 2), new Instant(7)), TimestampedValue.of(KV.of("b", 3), new Instant(14)))).apply("SideWindowInto", Window.into(FixedWindows.of(Duration.millis(10)))).apply(View.asMultimap());    PCollection<KV<String, Integer>> output = pipeline.apply("CreateMainInput", Create.timestamped(TimestampedValue.of("apple", new Instant(5)), TimestampedValue.of("banana", new Instant(13)), TimestampedValue.of("blackberry", new Instant(16)))).apply("MainWindowInto", Window.into(FixedWindows.of(Duration.millis(10)))).apply("OutputSideInputs", ParDo.of(new DoFn<String, KV<String, Integer>>() {        @ProcessElement        public void processElement(ProcessContext c) {            for (Integer v : c.sideInput(view).get(c.element().substring(0, 1))) {                c.output(KV.of(c.element(), v));            }        }    }).withSideInputs(view));    PAssert.that(output).containsInAnyOrder(KV.of("apple", 1), KV.of("apple", 1), KV.of("apple", 2), KV.of("banana", 3), KV.of("blackberry", 3));    pipeline.run();}
public void beam_f22819_0(ProcessContext c)
{    for (Integer v : c.sideInput(view).get(c.element().substring(0, 1))) {        c.output(KV.of(c.element(), v));    }}
public void beam_f22827_0(ProcessContext c)
{    assertTrue(c.sideInput(view).isEmpty());    assertTrue(c.sideInput(view).entrySet().isEmpty());    assertFalse(c.sideInput(view).entrySet().iterator().hasNext());    c.output(c.element());}
public void beam_f22828_0()
{    final PCollectionView<Map<String, Iterable<Integer>>> view = pipeline.apply("CreateSideInput", Create.of(KV.of("a", 1))).apply(View.asMultimap());    PCollection<KV<String, Integer>> output = pipeline.apply("CreateMainInput", Create.of("apple")).apply("OutputSideInputs", ParDo.of(new DoFn<String, KV<String, Integer>>() {        @ProcessElement        public void processElement(ProcessContext c) {            try {                c.sideInput(view).clear();                fail("Expected UnsupportedOperationException on clear()");            } catch (UnsupportedOperationException expected) {            }            try {                c.sideInput(view).put("c", ImmutableList.of(3));                fail("Expected UnsupportedOperationException on put()");            } catch (UnsupportedOperationException expected) {            }            try {                c.sideInput(view).remove("c");                fail("Expected UnsupportedOperationException on remove()");            } catch (UnsupportedOperationException expected) {            }            try {                c.sideInput(view).putAll(new HashMap<>());                fail("Expected UnsupportedOperationException on putAll()");            } catch (UnsupportedOperationException expected) {            }            for (Integer v : c.sideInput(view).get(c.element().substring(0, 1))) {                c.output(KV.of(c.element(), v));            }        }    }).withSideInputs(view));        PAssert.that(output).containsInAnyOrder(KV.of("apple", 1));    pipeline.run();}
public void beam_f22829_0(ProcessContext c)
{    try {        c.sideInput(view).clear();        fail("Expected UnsupportedOperationException on clear()");    } catch (UnsupportedOperationException expected) {    }    try {        c.sideInput(view).put("c", ImmutableList.of(3));        fail("Expected UnsupportedOperationException on put()");    } catch (UnsupportedOperationException expected) {    }    try {        c.sideInput(view).remove("c");        fail("Expected UnsupportedOperationException on remove()");    } catch (UnsupportedOperationException expected) {    }    try {        c.sideInput(view).putAll(new HashMap<>());        fail("Expected UnsupportedOperationException on putAll()");    } catch (UnsupportedOperationException expected) {    }    for (Integer v : c.sideInput(view).get(c.element().substring(0, 1))) {        c.output(KV.of(c.element(), v));    }}
public void beam_f22837_0(ProcessContext c)
{    c.output(KV.of(c.element(), c.sideInput(view).get(c.element().substring(0, 1))));}
public void beam_f22838_0()
{    final PCollectionView<Map<String, Integer>> view = pipeline.apply("CreateSideInput", Create.timestamped(TimestampedValue.of(KV.of("a", 1), new Instant(1)), TimestampedValue.of(KV.of("b", 2), new Instant(4)), TimestampedValue.of(KV.of("b", 3), new Instant(18)))).apply("SideWindowInto", Window.into(FixedWindows.of(Duration.millis(10)))).apply(View.asMap());    PCollection<KV<String, Integer>> output = pipeline.apply("CreateMainInput", Create.timestamped(TimestampedValue.of(2, /* size */    new Instant(5)), TimestampedValue.of(1, /* size */    new Instant(16)))).apply("MainWindowInto", Window.into(FixedWindows.of(Duration.millis(10)))).apply("OutputSideInputs", ParDo.of(new DoFn<Integer, KV<String, Integer>>() {        @ProcessElement        public void processElement(ProcessContext c) {            assertEquals((int) c.element(), c.sideInput(view).size());            assertEquals((int) c.element(), c.sideInput(view).entrySet().size());            for (Entry<String, Integer> entry : c.sideInput(view).entrySet()) {                c.output(KV.of(entry.getKey(), entry.getValue()));            }        }    }).withSideInputs(view));    PAssert.that(output).containsInAnyOrder(KV.of("a", 1), KV.of("b", 2), KV.of("b", 3));    pipeline.run();}
public void beam_f22839_0(ProcessContext c)
{    assertEquals((int) c.element(), c.sideInput(view).size());    assertEquals((int) c.element(), c.sideInput(view).entrySet().size());    for (Entry<String, Integer> entry : c.sideInput(view).entrySet()) {        c.output(KV.of(entry.getKey(), entry.getValue()));    }}
public void beam_f22847_0(ProcessContext c)
{    c.output(KV.of(c.element(), c.sideInput(view).get(c.element().substring(0, 1))));}
public void beam_f22848_0()
{    final PCollectionView<Map<String, Integer>> view = pipeline.apply("CreateSideInput", Create.of(KV.of("a", 1))).apply(View.asMap());    PCollection<KV<String, Integer>> output = pipeline.apply("CreateMainInput", Create.of("apple")).apply("OutputSideInputs", ParDo.of(new DoFn<String, KV<String, Integer>>() {        @ProcessElement        public void processElement(ProcessContext c) {            try {                c.sideInput(view).clear();                fail("Expected UnsupportedOperationException on clear()");            } catch (UnsupportedOperationException expected) {            }            try {                c.sideInput(view).put("c", 3);                fail("Expected UnsupportedOperationException on put()");            } catch (UnsupportedOperationException expected) {            }            try {                c.sideInput(view).remove("c");                fail("Expected UnsupportedOperationException on remove()");            } catch (UnsupportedOperationException expected) {            }            try {                c.sideInput(view).putAll(new HashMap<>());                fail("Expected UnsupportedOperationException on putAll()");            } catch (UnsupportedOperationException expected) {            }            c.output(KV.of(c.element(), c.sideInput(view).get(c.element().substring(0, 1))));        }    }).withSideInputs(view));        PAssert.that(output).containsInAnyOrder(KV.of("apple", 1));    pipeline.run();}
public void beam_f22849_0(ProcessContext c)
{    try {        c.sideInput(view).clear();        fail("Expected UnsupportedOperationException on clear()");    } catch (UnsupportedOperationException expected) {    }    try {        c.sideInput(view).put("c", 3);        fail("Expected UnsupportedOperationException on put()");    } catch (UnsupportedOperationException expected) {    }    try {        c.sideInput(view).remove("c");        fail("Expected UnsupportedOperationException on remove()");    } catch (UnsupportedOperationException expected) {    }    try {        c.sideInput(view).putAll(new HashMap<>());        fail("Expected UnsupportedOperationException on putAll()");    } catch (UnsupportedOperationException expected) {    }    c.output(KV.of(c.element(), c.sideInput(view).get(c.element().substring(0, 1))));}
public void beam_f22857_0(ProcessContext c)
{    c.output(c.element() + c.sideInput(view));}
public void beam_f22858_0()
{    final PCollectionView<Void> view = pipeline.apply("CreateSideInput", Create.of((Void) null).withCoder(VoidCoder.of())).apply(Combine.globally((Iterable<Void> input) -> null).asSingletonView());    @SuppressWarnings("ObjectToString")    PCollection<String> output = pipeline.apply("CreateMainInput", Create.of("")).apply("OutputMainAndSideInputs", ParDo.of(new DoFn<String, String>() {        @ProcessElement        public void processElement(ProcessContext c) {            c.output(c.element() + c.sideInput(view));        }    }).withSideInputs(view));    PAssert.that(output).containsInAnyOrder("null");    pipeline.run();}
public void beam_f22859_0(ProcessContext c)
{    c.output(c.element() + c.sideInput(view));}
private void beam_f22867_0(Pipeline pipeline, PTransform<PCollection<KV<String, Integer>>, ? extends PCollectionView<?>> view)
{    thrown.expect(IllegalStateException.class);    thrown.expectMessage("Unable to create a side-input view from input");    thrown.expectCause(ThrowableMessageMatcher.hasMessage(Matchers.containsString("Consumed by GroupByKey")));    pipeline.apply(Create.of(KV.of("hello", 5))).apply(Window.into(new InvalidWindows<>("Consumed by GroupByKey", FixedWindows.of(Duration.standardHours(1))))).apply(view);}
public void beam_f22868_0()
{    testViewUnbounded(pipeline, View.asSingleton());}
public void beam_f22869_0()
{    testViewUnbounded(pipeline, View.asIterable());}
public void beam_f22877_0()
{    testViewNonmerging(pipeline, View.asMultimap());}
public String beam_f22878_0()
{    return MoreObjects.toStringHelper(this).add("processingTime", processingTime).add("element", element).add("watermarkUpdate", watermarkUpdate).toString();}
private PCollection<Long> beam_f22879_0(String name, Instant base, Duration totalDuration, int numElements, Duration allowedLateness)
{    TestStream.Builder<Long> stream = TestStream.create(VarLongCoder.of());            List<Instant> watermarks = Lists.newArrayList();    for (int i = 0; i < numElements; ++i) {        watermarks.add(base.plus(new Duration((long) (totalDuration.getMillis() * Math.random()))));    }    Collections.sort(watermarks);    List<Event<Long>> events = Lists.newArrayList();    for (int i = 0; i < numElements; ++i) {        Instant processingTimestamp = base.plus((long) (1.0 * i * totalDuration.getMillis() / (numElements + 1)));        Instant watermark = watermarks.get(i);        Instant elementTimestamp = watermark.minus((long) (Math.random() * allowedLateness.getMillis()));        events.add(new Event<>(processingTimestamp, watermark));        events.add(new Event<>(processingTimestamp, TimestampedValue.of((long) i, elementTimestamp)));    }    Instant lastProcessingTime = base;    for (Event<Long> event : events) {        Duration processingTimeDelta = new Duration(lastProcessingTime, event.processingTime);        if (processingTimeDelta.getMillis() > 0) {            stream = stream.advanceProcessingTime(processingTimeDelta);        }        lastProcessingTime = event.processingTime;        if (event.element != null) {            stream = stream.addElements(event.element);        } else {            stream = stream.advanceWatermarkTo(event.watermarkUpdate);        }    }    return p.apply(name, stream.advanceWatermarkToInfinity());}
public void beam_f22887_0(ProcessContext c)
{    Instant maxMainTimestamp = TEST_WAIT_MAX_MAIN_TIMESTAMP.get();    if (maxMainTimestamp != null) {        assertFalse("Signal at timestamp " + c.timestamp() + " generated after main timestamp progressed to " + maxMainTimestamp, c.timestamp().isBefore(maxMainTimestamp));    }    c.output(c.element());}
public void beam_f22888_0(ProcessContext c, BoundedWindow w)
{    while (true) {        Instant maxMainTimestamp = TEST_WAIT_MAX_MAIN_TIMESTAMP.get();        Instant newMaxTimestamp = (maxMainTimestamp == null || c.timestamp().isAfter(maxMainTimestamp)) ? c.timestamp() : maxMainTimestamp;        if (TEST_WAIT_MAX_MAIN_TIMESTAMP.compareAndSet(maxMainTimestamp, newMaxTimestamp)) {            break;        }    }    c.output(c.element());}
public PCollection<T> beam_f22889_0(PCollection<T> input)
{    return input.apply(WithKeys.of("")).apply(GroupByKey.create()).apply(Values.create()).apply(Flatten.iterables());}
public void beam_f22897_0()
{    List<KV<Integer, String>> polls = Arrays.asList(KV.of(0, "0"), KV.of(10, "10"), KV.of(20, "20"), KV.of(30, "30"), KV.of(40, "40"), KV.of(40, "40.1"), KV.of(20, "20.1"), KV.of(50, "50"), KV.of(10, "10.1"), KV.of(10, "10.2"), KV.of(60, "60"), KV.of(70, "70"), KV.of(60, "60.1"), KV.of(80, "80"), KV.of(40, "40.2"), KV.of(90, "90"), KV.of(90, "90.1"));    List<Integer> expected = Arrays.asList(0, 10, 20, 30, 40, 50, 60, 70, 80, 90);    PCollection<Integer> res = p.apply(Create.of("a")).apply(Watch.growthOf(Contextful.of(new TimedPollFn<String, KV<Integer, String>>(polls, standardSeconds(1), /* timeToOutputEverything */    standardSeconds(3), /* timeToDeclareOutputFinal */    standardSeconds(30)), Requirements.empty()), KV::getKey).withTerminationPerInput(Growth.afterTotalOf(standardSeconds(5))).withPollInterval(Duration.millis(100)).withOutputCoder(KvCoder.of(VarIntCoder.of(), StringUtf8Coder.of())).withOutputKeyCoder(VarIntCoder.of())).apply("Drop input", Values.create()).apply("Drop auxiliary string", Keys.create());    PAssert.that(res).containsInAnyOrder(expected);    p.run();}
public void beam_f22898_0()
{    List<Integer> all = Arrays.asList(0, 1, 2, 3, 4, 5, 6, 7, 8, 9);    PCollection<Integer> res = p.apply(Create.of("a")).apply(Watch.growthOf(new TimedPollFn<String, Integer>(all, standardSeconds(1),     standardSeconds(1000), /* timeToDeclareOutputFinal */    standardSeconds(30))).withTerminationPerInput(afterTimeSinceNewOutput(standardSeconds(3))).withPollInterval(Duration.millis(300)).withOutputCoder(VarIntCoder.of())).apply("Drop input", Values.create());    PAssert.that(res).containsInAnyOrder(all);    p.run();}
public void beam_f22899_0()
{        final long numResults = 3000;    PCollection<KV<String, Integer>> res = p.apply(Create.of("a")).apply(Watch.growthOf(new PollFn<String, KV<String, Integer>>() {        @Override        public PollResult<KV<String, Integer>> apply(String element, Context c) throws Exception {            String pollId = UUID.randomUUID().toString();            List<KV<String, Integer>> output = Lists.newArrayList();            for (int i = 0; i < numResults; ++i) {                output.add(KV.of(pollId, i));            }            return PollResult.complete(Instant.now(), output);        }    }).withTerminationPerInput(Growth.afterTotalOf(standardSeconds(1))).withPollInterval(Duration.millis(1)).withOutputCoder(KvCoder.of(StringUtf8Coder.of(), VarIntCoder.of()))).apply("Drop input", Values.create());    PAssert.that("Poll called only once", res.apply(Keys.create())).satisfies(pollIds -> {        assertEquals(1, Sets.newHashSet(pollIds).size());        return null;    });    PAssert.that("Yields all expected results", res.apply("Drop poll id", Values.create())).satisfies(input -> {        assertEquals("Total number of results mismatches", numResults, Lists.newArrayList(input).size());        assertEquals("Results are not unique", numResults, Sets.newHashSet(input).size());        return null;    });    p.run();}
public void beam_f22907_0()
{    Instant now = Instant.now();    Watch.Growth.AfterTotalOf<Object> c = Growth.afterTotalOf(standardSeconds(5));    KV<Instant, ReadableDuration> state = c.forNewInput(now, null);    assertFalse(c.canStopPolling(now, state));    assertFalse(c.canStopPolling(now.plus(standardSeconds(3)), state));    assertTrue(c.canStopPolling(now.plus(standardSeconds(6)), state));}
public void beam_f22908_0()
{    Instant now = Instant.now();    Watch.Growth.AfterTimeSinceNewOutput<Object> c = afterTimeSinceNewOutput(standardSeconds(5));    KV<Instant, ReadableDuration> state = c.forNewInput(now, null);    assertFalse(c.canStopPolling(now, state));    assertFalse(c.canStopPolling(now.plus(standardSeconds(3)), state));    assertFalse(c.canStopPolling(now.plus(standardSeconds(6)), state));    state = c.onSeenNewOutput(now.plus(standardSeconds(3)), state);    assertFalse(c.canStopPolling(now.plus(standardSeconds(3)), state));    assertFalse(c.canStopPolling(now.plus(standardSeconds(6)), state));    assertTrue(c.canStopPolling(now.plus(standardSeconds(9)), state));    state = c.onSeenNewOutput(now.plus(standardSeconds(5)), state);    assertFalse(c.canStopPolling(now.plus(standardSeconds(3)), state));    assertFalse(c.canStopPolling(now.plus(standardSeconds(6)), state));    assertFalse(c.canStopPolling(now.plus(standardSeconds(9)), state));    assertTrue(c.canStopPolling(now.plus(standardSeconds(11)), state));}
public void beam_f22909_0()
{    Instant now = Instant.now();    Watch.Growth.AfterTotalOf<Object> a = Growth.afterTotalOf(standardSeconds(5));    Watch.Growth.AfterTotalOf<Object> b = Growth.afterTotalOf(standardSeconds(10));    Watch.Growth.BinaryCombined<Object, KV<Instant, ReadableDuration>, KV<Instant, ReadableDuration>> c = eitherOf(a, b);    KV<KV<Instant, ReadableDuration>, KV<Instant, ReadableDuration>> state = c.forNewInput(now, null);    assertFalse(c.canStopPolling(now.plus(standardSeconds(3)), state));    assertTrue(c.canStopPolling(now.plus(standardSeconds(7)), state));    assertTrue(c.canStopPolling(now.plus(standardSeconds(12)), state));}
public void beam_f22917_0()
{    Instant now = Instant.now();    PollResult<String> claim = PollResult.incomplete(Arrays.asList(TimestampedValue.of("d", now.plus(standardSeconds(4))), TimestampedValue.of("c", now.plus(standardSeconds(3))), TimestampedValue.of("a", now.plus(standardSeconds(1))), TimestampedValue.of("b", now.plus(standardSeconds(2))))).withWatermark(now.plus(standardSeconds(7)));    GrowthTracker<String, Integer> tracker = newTracker(NonPollingGrowthState.of(claim));    assertTrue(tracker.tryClaim(KV.of(claim, 1)));    GrowthState residual = tracker.checkpoint();    NonPollingGrowthState<String> primary = (NonPollingGrowthState<String>) tracker.currentRestriction();    tracker.checkDone();        assertEquals(claim, primary.getPending());        assertEquals(GrowthTracker.EMPTY_STATE, residual);}
public void beam_f22918_0()
{    Instant now = Instant.now();    PollResult<String> claim = PollResult.incomplete(Arrays.asList(TimestampedValue.of("d", now.plus(standardSeconds(4))), TimestampedValue.of("c", now.plus(standardSeconds(3))), TimestampedValue.of("a", now.plus(standardSeconds(1))), TimestampedValue.of("b", now.plus(standardSeconds(2))))).withWatermark(now.plus(standardSeconds(7)));    GrowthTracker<String, Integer> tracker = newTracker(NonPollingGrowthState.of(claim));    NonPollingGrowthState<String> residual = (NonPollingGrowthState<String>) tracker.checkpoint();    GrowthState primary = tracker.currentRestriction();    tracker.checkDone();        assertEquals(GrowthTracker.EMPTY_STATE, primary);        assertEquals(claim, residual.getPending());}
public void beam_f22919_0()
{    Instant now = Instant.now();    PollResult<String> claim = PollResult.incomplete(Arrays.asList(TimestampedValue.of("d", now.plus(standardSeconds(4))), TimestampedValue.of("c", now.plus(standardSeconds(3))), TimestampedValue.of("a", now.plus(standardSeconds(1))), TimestampedValue.of("b", now.plus(standardSeconds(2))))).withWatermark(now.plus(standardSeconds(7)));    GrowthTracker<String, Integer> tracker = newTracker(NonPollingGrowthState.of(claim));    PollResult<String> otherClaim = PollResult.incomplete(Arrays.asList(TimestampedValue.of("x", now.plus(standardSeconds(14))), TimestampedValue.of("y", now.plus(standardSeconds(13))), TimestampedValue.of("z", now.plus(standardSeconds(12))))).withWatermark(now.plus(standardSeconds(17)));    assertFalse(tracker.tryClaim(KV.of(otherClaim, 1)));}
public void beam_f22927_0() throws Exception
{    OnceTrigger trigger1 = AfterProcessingTime.pastFirstElementInPane();    OnceTrigger trigger2 = AfterWatermark.pastEndOfWindow();    Trigger afterFirst = AfterFirst.of(trigger1, trigger2);    assertEquals(AfterFirst.of(trigger1.getContinuationTrigger(), trigger2.getContinuationTrigger()), afterFirst.getContinuationTrigger());}
public void beam_f22928_0()
{    Trigger trigger = AfterFirst.of(StubTrigger.named("t1"), StubTrigger.named("t2"));    assertEquals("AfterFirst.of(t1, t2)", trigger.toString());}
public void beam_f22929_0() throws Exception
{    assertEquals(BoundedWindow.TIMESTAMP_MAX_VALUE, AfterPane.elementCountAtLeast(1).getWatermarkThatGuaranteesFiring(new IntervalWindow(new Instant(0), new Instant(10))));}
public void beam_f22937_0()
{    Trigger trigger = AfterWatermark.pastEndOfWindow().withLateFirings(AfterProcessingTime.pastFirstElementInPane().plusDelayOf(Duration.standardMinutes(10)));    String expected = "AfterWatermark.pastEndOfWindow()" + ".withLateFirings(AfterProcessingTime" + ".pastFirstElementInPane()" + ".plusDelayOf(10 minutes))";    assertEquals(expected, trigger.toString());}
public void beam_f22938_0() throws Exception
{    assertEquals(BoundedWindow.TIMESTAMP_MAX_VALUE, underTest.getWatermarkThatGuaranteesFiring(new IntervalWindow(new Instant(0), new Instant(10))));}
public void beam_f22939_0() throws Exception
{    assertEquals(underTest, underTest.getContinuationTrigger());}
public void beam_f22947_0() throws IncompatibleWindowException
{    CalendarWindows.DaysWindows daysWindows = CalendarWindows.days(10);    daysWindows.verifyCompatibility(CalendarWindows.days(10));    thrown.expect(IncompatibleWindowException.class);    daysWindows.verifyCompatibility(CalendarWindows.days(9));}
public void beam_f22948_0() throws Exception
{    Map<IntervalWindow, Set<String>> expected = new HashMap<>();    final List<Long> timestamps = Arrays.asList(makeTimestamp(2014, 1, 1, 0, 0).getMillis(), makeTimestamp(2014, 1, 5, 5, 5).getMillis(), makeTimestamp(2014, 1, 8, 0, 0).getMillis(), makeTimestamp(2014, 1, 12, 5, 5).getMillis(), makeTimestamp(2015, 1, 1, 0, 0).getMillis(), makeTimestamp(2015, 1, 6, 5, 5).getMillis());    expected.put(new IntervalWindow(makeTimestamp(2014, 1, 1, 0, 0), makeTimestamp(2014, 1, 8, 0, 0)), set(timestamps.get(0), timestamps.get(1)));    expected.put(new IntervalWindow(makeTimestamp(2014, 1, 8, 0, 0), makeTimestamp(2014, 1, 15, 0, 0)), set(timestamps.get(2), timestamps.get(3)));    expected.put(new IntervalWindow(makeTimestamp(2014, 12, 31, 0, 0), makeTimestamp(2015, 1, 7, 0, 0)), set(timestamps.get(4), timestamps.get(5)));    assertEquals(expected, runWindowFn(CalendarWindows.weeks(1, DateTimeConstants.WEDNESDAY), timestamps));}
public void beam_f22949_0() throws Exception
{    Map<IntervalWindow, Set<String>> expected = new HashMap<>();    final List<Long> timestamps = Arrays.asList(makeTimestamp(2014, 1, 1, 0, 0).getMillis(), makeTimestamp(2014, 1, 31, 5, 5).getMillis(), makeTimestamp(2014, 2, 1, 0, 0).getMillis(), makeTimestamp(2014, 2, 15, 5, 5).getMillis(), makeTimestamp(2015, 1, 1, 0, 0).getMillis(), makeTimestamp(2015, 1, 31, 5, 5).getMillis());    expected.put(new IntervalWindow(makeTimestamp(2014, 1, 1, 0, 0), makeTimestamp(2014, 2, 1, 0, 0)), set(timestamps.get(0), timestamps.get(1)));    expected.put(new IntervalWindow(makeTimestamp(2014, 2, 1, 0, 0), makeTimestamp(2014, 3, 1, 0, 0)), set(timestamps.get(2), timestamps.get(3)));    expected.put(new IntervalWindow(makeTimestamp(2015, 1, 1, 0, 0), makeTimestamp(2015, 2, 1, 0, 0)), set(timestamps.get(4), timestamps.get(5)));    assertEquals(expected, runWindowFn(CalendarWindows.months(1), timestamps));}
public void beam_f22957_0()
{    MonthsWindows windowFn = CalendarWindows.months(2);    WindowMappingFn<?> mapping = windowFn.getDefaultWindowMappingFn();    thrown.expect(IllegalArgumentException.class);    mapping.getSideInputWindow(GlobalWindow.INSTANCE);}
public void beam_f22958_0()
{    DateTimeZone timeZone = DateTimeZone.forID("America/Los_Angeles");    Instant jan1 = new DateTime(1990, 1, 1, 0, 0, timeZone).toInstant();    CalendarWindows.DaysWindows daysWindow = CalendarWindows.days(5).withStartingDay(1990, 1, 1).withTimeZone(timeZone);    DisplayData daysDisplayData = DisplayData.from(daysWindow);    assertThat(daysDisplayData, hasDisplayItem("numDays", 5));    assertThat(daysDisplayData, hasDisplayItem("startDate", jan1));    CalendarWindows.MonthsWindows monthsWindow = CalendarWindows.months(2).withStartingMonth(1990, 1).withTimeZone(timeZone);    DisplayData monthsDisplayData = DisplayData.from(monthsWindow);    assertThat(monthsDisplayData, hasDisplayItem("numMonths", 2));    assertThat(monthsDisplayData, hasDisplayItem("startDate", jan1));    CalendarWindows.YearsWindows yearsWindow = CalendarWindows.years(4).withStartingYear(1990).withTimeZone(timeZone);    DisplayData yearsDisplayData = DisplayData.from(yearsWindow);    assertThat(yearsDisplayData, hasDisplayItem("numYears", 4));    assertThat(yearsDisplayData, hasDisplayItem("startDate", jan1));}
public void beam_f22959_0() throws Exception
{    assertEquals(new Instant(9), DefaultTrigger.of().getWatermarkThatGuaranteesFiring(new IntervalWindow(new Instant(0), new Instant(10))));    assertEquals(GlobalWindow.INSTANCE.maxTimestamp(), DefaultTrigger.of().getWatermarkThatGuaranteesFiring(GlobalWindow.INSTANCE));}
public void beam_f22967_0()
{    PartitioningWindowFn<?, ?> windowFn = FixedWindows.of(Duration.standardMinutes(20L));    WindowMappingFn<?> mapping = windowFn.getDefaultWindowMappingFn();    thrown.expect(IllegalArgumentException.class);    mapping.getSideInputWindow(GlobalWindow.INSTANCE);}
 void beam_f22968_0(int size, int offset)
{    try {        FixedWindows.of(Duration.standardSeconds(size)).withOffset(Duration.standardSeconds(offset));        fail("should have failed");    } catch (IllegalArgumentException e) {        assertThat(e.toString(), containsString("FixedWindows WindowingStrategies must have 0 <= offset < size"));    }}
public void beam_f22969_0() throws Exception
{    checkConstructionFailure(-1, 0);    checkConstructionFailure(1, 2);    checkConstructionFailure(1, -1);}
public void beam_f22977_0() throws Exception
{    CoderProperties.coderDeterministic(GlobalWindow.Coder.INSTANCE, GlobalWindow.INSTANCE, GlobalWindow.INSTANCE);}
public void beam_f22978_0() throws Exception
{    for (IntervalWindow window : TEST_VALUES) {        CoderProperties.coderDecodeEncodeEqual(TEST_CODER, window);    }}
public void beam_f22979_0() throws Exception
{    Instant start = Instant.parse("2015-04-01T00:00:00Z");    Instant minuteEnd = Instant.parse("2015-04-01T00:01:00Z");    Instant hourEnd = Instant.parse("2015-04-01T01:00:00Z");    Instant dayEnd = Instant.parse("2015-04-02T00:00:00Z");    Coder<Instant> instantCoder = InstantCoder.of();    byte[] encodedStart = CoderUtils.encodeToByteArray(instantCoder, start);    byte[] encodedMinuteEnd = CoderUtils.encodeToByteArray(instantCoder, minuteEnd);    byte[] encodedHourEnd = CoderUtils.encodeToByteArray(instantCoder, hourEnd);    byte[] encodedDayEnd = CoderUtils.encodeToByteArray(instantCoder, dayEnd);    byte[] encodedMinuteWindow = CoderUtils.encodeToByteArray(TEST_CODER, new IntervalWindow(start, minuteEnd));    byte[] encodedHourWindow = CoderUtils.encodeToByteArray(TEST_CODER, new IntervalWindow(start, hourEnd));    byte[] encodedDayWindow = CoderUtils.encodeToByteArray(TEST_CODER, new IntervalWindow(start, dayEnd));    assertThat(encodedMinuteWindow.length, equalTo(encodedStart.length + encodedMinuteEnd.length - 5));    assertThat(encodedHourWindow.length, equalTo(encodedStart.length + encodedHourEnd.length - 4));    assertThat(encodedDayWindow.length, equalTo(encodedStart.length + encodedDayEnd.length - 4));}
public void beam_f22987_0()
{    assertEquals("PaneInfo encoding assumes that there are only 4 Timing values.", 4, Timing.values().length);    assertEquals("PaneInfo encoding should remain the same.", 0x0, PaneInfo.createPane(false, false, Timing.EARLY, 1, -1).getEncodedByte());    assertEquals("PaneInfo encoding should remain the same.", 0x1, PaneInfo.createPane(true, false, Timing.EARLY).getEncodedByte());    assertEquals("PaneInfo encoding should remain the same.", 0x3, PaneInfo.createPane(true, true, Timing.EARLY).getEncodedByte());    assertEquals("PaneInfo encoding should remain the same.", 0x7, PaneInfo.createPane(true, true, Timing.ON_TIME).getEncodedByte());    assertEquals("PaneInfo encoding should remain the same.", 0xB, PaneInfo.createPane(true, true, Timing.LATE).getEncodedByte());    assertEquals("PaneInfo encoding should remain the same.", 0xF, PaneInfo.createPane(true, true, Timing.UNKNOWN).getEncodedByte());}
public void beam_f22988_0(WindowFn<Object, IntervalWindow> windowFn) throws Exception
{    MockitoAnnotations.initMocks(this);}
public void beam_f22989_0() throws Exception
{    setUp(FixedWindows.of(Duration.millis(10)));    IntervalWindow window = new IntervalWindow(new Instant(0), new Instant(10));    Instant arbitraryInstant = new Instant(34957849);    when(mockTrigger.getWatermarkThatGuaranteesFiring(Mockito.<IntervalWindow>any())).thenReturn(arbitraryInstant);    assertThat(Repeatedly.forever(mockTrigger).getWatermarkThatGuaranteesFiring(window), equalTo(arbitraryInstant));}
public void beam_f22997_0() throws Exception
{    Map<IntervalWindow, Set<String>> expected = new HashMap<>();    expected.put(new IntervalWindow(new Instant(1), new Instant(40)), set(1, 10, 15, 22, 30));    expected.put(new IntervalWindow(new Instant(95), new Instant(111)), set(95, 100, 101));    assertEquals(expected, runWindowFn(Sessions.withGapDuration(new Duration(10)), Arrays.asList(1L, 15L, 30L, 100L, 101L, 95L, 22L, 10L)));}
public void beam_f22998_0() throws Exception
{    Map<IntervalWindow, Set<String>> expected = new HashMap<>();    expected.put(new IntervalWindow(new Instant(1), new Instant(2000)), set(1, 2, 1000));    expected.put(new IntervalWindow(new Instant(5000), new Instant(6001)), set(5000, 5001));    expected.put(new IntervalWindow(new Instant(10000), new Instant(11000)), set(10000));    assertEquals(expected, runWindowFn(Sessions.withGapDuration(Duration.standardSeconds(1)), Arrays.asList(1L, 2L, 1000L, 5000L, 5001L, 10000L)));}
public void beam_f22999_0()
{    assertTrue(Sessions.withGapDuration(new Duration(10)).isCompatible(Sessions.withGapDuration(new Duration(10))));    assertTrue(Sessions.withGapDuration(new Duration(10)).isCompatible(Sessions.withGapDuration(new Duration(20))));}
public void beam_f23007_0() throws Exception
{    Map<IntervalWindow, Set<String>> expected = new HashMap<>();    expected.put(new IntervalWindow(new Instant(0), new Instant(3)), set(1, 2));    expected.put(new IntervalWindow(new Instant(3), new Instant(6)), set(3, 4, 5));    expected.put(new IntervalWindow(new Instant(6), new Instant(9)), set(6, 7));    SlidingWindows windowFn = SlidingWindows.of(new Duration(3)).every(new Duration(3));    assertEquals(expected, runWindowFn(windowFn, Arrays.asList(1L, 2L, 3L, 4L, 5L, 6L, 7L)));    assertThat(windowFn.assignsToOneWindow(), is(true));}
public void beam_f23008_0() throws Exception
{    Map<IntervalWindow, Set<String>> expected = new HashMap<>();    expected.put(new IntervalWindow(new Instant(0), new Instant(3)), set(1, 2));    expected.put(new IntervalWindow(new Instant(10), new Instant(13)), set(10, 11));    expected.put(new IntervalWindow(new Instant(100), new Instant(103)), set(100));    SlidingWindows windowFn = SlidingWindows.of(new Duration(3)).every(new Duration(10));    assertEquals(expected, runWindowFn(    windowFn, Arrays.asList(1L, 2L, 3L, 5L, 9L, 10L, 11L, 100L)));    assertThat(windowFn.assignsToOneWindow(), is(true));}
public void beam_f23009_0() throws Exception
{    Map<IntervalWindow, Set<String>> expected = new HashMap<>();    expected.put(new IntervalWindow(new Instant(-8), new Instant(2)), set(1));    expected.put(new IntervalWindow(new Instant(-3), new Instant(7)), set(1, 2, 5));    expected.put(new IntervalWindow(new Instant(2), new Instant(12)), set(2, 5, 9, 10, 11));    expected.put(new IntervalWindow(new Instant(7), new Instant(17)), set(9, 10, 11));    assertEquals(expected, runWindowFn(SlidingWindows.of(new Duration(10)).every(new Duration(5)).withOffset(new Duration(2)), Arrays.asList(1L, 2L, 5L, 9L, 10L, 11L)));}
public void beam_f23017_0()
{    Duration windowSize = Duration.standardSeconds(1234);    Duration offset = Duration.standardSeconds(2345);    Duration period = Duration.standardSeconds(3456);    SlidingWindows slidingWindowFn = SlidingWindows.of(windowSize).every(period).withOffset(offset);    DisplayData displayData = DisplayData.from(slidingWindowFn);    assertThat(displayData, hasDisplayItem("size", windowSize));    assertThat(displayData, hasDisplayItem("period", period));    assertThat(displayData, hasDisplayItem("offset", offset));}
 static StubTrigger beam_f23018_0(final String name)
{    return new StubTrigger() {        @Override        public String toString() {            return name;        }    };}
public String beam_f23019_0()
{    return name;}
public Instant beam_f23027_0(BoundedWindow window)
{    return null;}
public void beam_f23028_0(ProcessContext c, BoundedWindow window)
{    c.output(c.element().getKey() + ":" + c.element().getValue() + ":" + c.timestamp().getMillis() + ":" + window);}
public PCollection<String> beam_f23029_0(PCollection<String> in)
{    return in.apply("Window", Window.<String>into(windowFn).withTimestampCombiner(TimestampCombiner.EARLIEST)).apply(Count.perElement()).apply("FormatCounts", ParDo.of(new FormatCountsDoFn())).setCoder(StringUtf8Coder.of());}
public void beam_f23037_0(ProcessContext c)
{    List<String> words = Splitter.onPattern("[^a-zA-Z0-9']+").splitToList(c.element());    if (words.size() == 2) {        c.outputWithTimestamp(words.get(0), new Instant(Long.parseLong(words.get(1))));    }}
public void beam_f23038_0()
{    WindowingStrategy<?, ?> strategy = pipeline.apply(Create.of("hello", "world").withCoder(StringUtf8Coder.of())).apply(Window.into(FixedWindows.of(Duration.standardMinutes(10)))).getWindowingStrategy();    assertTrue(strategy.getWindowFn() instanceof FixedWindows);    assertTrue(strategy.getTrigger() instanceof DefaultTrigger);    assertEquals(AccumulationMode.DISCARDING_FIRED_PANES, strategy.getMode());}
public void beam_f23039_0()
{    FixedWindows fixed10 = FixedWindows.of(Duration.standardMinutes(10));    Repeatedly trigger = Repeatedly.forever(AfterPane.elementCountAtLeast(5));    WindowingStrategy<?, ?> strategy = pipeline.apply(Create.of("hello", "world").withCoder(StringUtf8Coder.of())).apply(Window.<String>into(fixed10).triggering(trigger).accumulatingFiredPanes().withAllowedLateness(Duration.ZERO)).getWindowingStrategy();    assertEquals(fixed10, strategy.getWindowFn());    assertEquals(trigger, strategy.getTrigger());    assertEquals(AccumulationMode.ACCUMULATING_FIRED_PANES, strategy.getMode());}
public void beam_f23047_0(TransformHierarchy.Node node)
{    assertThat(node.getTransform(), not(instanceOf(Window.Assign.class)));}
public void beam_f23048_0()
{    assertEquals("Window.Into()", Window.<String>into(FixedWindows.of(Duration.standardMinutes(10))).getName());}
public void beam_f23049_0() throws NonDeterministicException
{    FixedWindows mockWindowFn = Mockito.mock(FixedWindows.class);    @SuppressWarnings({ "unchecked", "rawtypes" })    Class<Coder<IntervalWindow>> coderClazz = (Class) Coder.class;    Coder<IntervalWindow> mockCoder = Mockito.mock(coderClazz);    when(mockWindowFn.windowCoder()).thenReturn(mockCoder);    NonDeterministicException toBeThrown = new NonDeterministicException(mockCoder, "Its just not deterministic.");    Mockito.doThrow(toBeThrown).when(mockCoder).verifyDeterministic();    thrown.expect(IllegalArgumentException.class);    thrown.expectCause(Matchers.sameInstance(toBeThrown));    thrown.expectMessage("Window coders must be deterministic");    Window.into(mockWindowFn);}
public WindowMappingFn<IntervalWindow> beam_f23057_0()
{    throw new UnsupportedOperationException(String.format("Can't use %s for side inputs", getClass().getSimpleName()));}
public void beam_f23058_0()
{    pipeline.enableAbandonedNodeEnforcement(true);    final PCollection<Long> initialWindows = pipeline.apply(GenerateSequence.from(0).to(10)).apply("AssignWindows", Window.into(new WindowOddEvenBuckets()));        PAssert.that(initialWindows).inWindow(WindowOddEvenBuckets.EVEN_WINDOW).containsInAnyOrder(0L, 2L, 4L, 6L, 8L);    PAssert.that(initialWindows).inWindow(WindowOddEvenBuckets.ODD_WINDOW).containsInAnyOrder(1L, 3L, 5L, 7L, 9L);    PCollection<Boolean> upOne = initialWindows.apply("ModifyTypes", MapElements.via(new SimpleFunction<Long, Boolean>() {        @Override        public Boolean apply(Long input) {            return input % 2 == 0;        }    }));    PAssert.that(upOne).inWindow(WindowOddEvenBuckets.EVEN_WINDOW).containsInAnyOrder(true, true, true, true, true);    PAssert.that(upOne).inWindow(WindowOddEvenBuckets.ODD_WINDOW).containsInAnyOrder(false, false, false, false, false);                    upOne.apply("UpdateWindowingStrategy", Window.<Boolean>configure().triggering(Never.ever()).withAllowedLateness(Duration.ZERO).accumulatingFiredPanes());    pipeline.run();}
public Boolean beam_f23059_0(Long input)
{    return input % 2 == 0;}
public void beam_f23067_0()
{    Window<?> onlyHasAccumulationMode = Window.configure().discardingFiredPanes();    assertThat(DisplayData.from(onlyHasAccumulationMode), not(hasDisplayItem(hasKey(isOneOf("windowFn", "trigger", "timestampCombiner", "allowedLateness", "closingBehavior")))));    Window<?> noAccumulationMode = Window.into(new GlobalWindows());    assertThat(DisplayData.from(noAccumulationMode), not(hasDisplayItem(hasKey("accumulationMode"))));}
public void beam_f23068_0()
{    Window<?> window = Window.into(new GlobalWindows()).triggering(DefaultTrigger.of()).withAllowedLateness(Duration.millis(BoundedWindow.TIMESTAMP_MAX_VALUE.getMillis()));    DisplayData data = DisplayData.from(window);    assertThat(data, not(hasDisplayItem("trigger")));    assertThat(data, not(hasDisplayItem("allowedLateness")));}
public void beam_f23069_0()
{    Instant startInstant = new Instant(0L);    PCollection<String> inputCollection = pipeline.apply(Create.timestamped(TimestampedValue.of("big", startInstant.plus(Duration.standardSeconds(10))), TimestampedValue.of("small1", startInstant.plus(Duration.standardSeconds(20))),     TimestampedValue.of("small2", startInstant.plus(Duration.standardSeconds(39)))));    PCollection<String> windowedCollection = inputCollection.apply(Window.into(new CustomWindowFn<>()));    PCollection<Long> count = windowedCollection.apply(Combine.globally(Count.<String>combineFn()).withoutDefaults());            PAssert.that("Wrong number of elements in output collection", count).containsInAnyOrder(2L, 1L);    pipeline.run();}
public Collection<CustomWindow> beam_f23077_0(AssignContext c) throws Exception
{    String element;        if (c.element() instanceof KV) {        element = ((KV<Integer, String>) c.element()).getValue();    } else {        element = (String) c.element();    }        if ("big".equals(element)) {        return Collections.singletonList(new CustomWindow(c.timestamp(), c.timestamp().plus(Duration.standardSeconds(30)), true));    } else {        return Collections.singletonList(new CustomWindow(c.timestamp(), c.timestamp().plus(Duration.standardSeconds(5)), false));    }}
public void beam_f23078_0(MergeContext c) throws Exception
{    Map<CustomWindow, Set<CustomWindow>> windowsToMerge = new HashMap<>();    for (CustomWindow window : c.windows()) {        if (window.isBig) {            HashSet<CustomWindow> windows = new HashSet<>();            windows.add(window);            windowsToMerge.put(window, windows);        }    }    for (CustomWindow window : c.windows()) {        for (Map.Entry<CustomWindow, Set<CustomWindow>> bigWindow : windowsToMerge.entrySet()) {            if (bigWindow.getKey().contains(window)) {                bigWindow.getValue().add(window);            }        }    }    for (Map.Entry<CustomWindow, Set<CustomWindow>> mergeEntry : windowsToMerge.entrySet()) {        c.merge(mergeEntry.getValue(), mergeEntry.getKey());    }}
public boolean beam_f23079_0(WindowFn<?, ?> other)
{    return other instanceof CustomWindowFn;}
public void beam_f23087_0()
{    PCollection<String> input = p.apply(Create.of(Arrays.asList(COLLECTION)).withCoder(StringUtf8Coder.of()));    PCollection<KV<Integer, String>> output = input.apply(WithKeys.of(new LengthAsKey()).withKeyType(TypeDescriptor.of(Integer.class)));    PAssert.that(output).containsInAnyOrder(WITH_KEYS);    p.run();}
public Integer beam_f23088_0(String value)
{    return value.length();}
public void beam_f23089_0()
{    PCollection<String> values = p.apply(Create.of("1234", "3210", "0", "-12"));    PCollection<KV<Integer, String>> kvs = values.apply(WithKeys.of((SerializableFunction<String, Integer>) Integer::valueOf).withKeyType(TypeDescriptor.of(Integer.class)));    PAssert.that(kvs).containsInAnyOrder(KV.of(1234, "1234"), KV.of(0, "0"), KV.of(-12, "-12"), KV.of(3210, "3210"));    p.run();}
public void beam_f23097_0()
{    SerializableFunction<String, Instant> timestampFn = null;    thrown.expect(NullPointerException.class);    thrown.expectMessage("WithTimestamps fn cannot be null");    p.apply(Create.of("1234", "0", Integer.toString(Integer.MAX_VALUE))).apply(WithTimestamps.of(timestampFn));    p.run();}
public void beam_f23098_0()
{    final String yearTwoThousand = "946684800000";    PCollection<String> timestamped = p.apply(Create.of("1234", "0", Integer.toString(Integer.MAX_VALUE), yearTwoThousand)).apply(WithTimestamps.of((String input) -> new Instant(Long.valueOf(input))));    PCollection<KV<String, Instant>> timestampedVals = timestamped.apply(ParDo.of(new DoFn<String, KV<String, Instant>>() {        @ProcessElement        public void processElement(ProcessContext c) throws Exception {            c.output(KV.of(c.element(), c.timestamp()));        }    }));    PAssert.that(timestamped).containsInAnyOrder(yearTwoThousand, "0", "1234", Integer.toString(Integer.MAX_VALUE));    PAssert.that(timestampedVals).containsInAnyOrder(KV.of("0", new Instant(0)), KV.of("1234", new Instant(Long.valueOf("1234"))), KV.of(Integer.toString(Integer.MAX_VALUE), new Instant(Integer.MAX_VALUE)), KV.of(yearTwoThousand, new Instant(Long.valueOf(yearTwoThousand))));    p.run();}
public void beam_f23099_0(ProcessContext c) throws Exception
{    c.output(KV.of(c.element(), c.timestamp()));}
public void beam_f23107_0() throws Exception
{    assertExposed(ExposedTwice.class, Exposed.class);}
public void beam_f23108_0() throws Exception
{    assertExposed(ExposedCycle.class, Exposed.class);}
public void beam_f23109_0() throws Exception
{    assertExposed(ExposedGenericCycle.class, Exposed.class);}
public void beam_f23117_0()
{    BucketingFunction f = newFunc();    int lost = 0;    for (int i = 0; i < 200; i++) {        f.add(i, 1);        if (i >= 100) {            f.remove(i - 100);            if (i % BUCKET_WIDTH == BUCKET_WIDTH - 1) {                lost += BUCKET_WIDTH;            }        }        assertEquals(i + 1 - lost, f.get());    }}
public void beam_f23118_0() throws Exception
{    testValues(Collections.emptyList());}
public void beam_f23119_0() throws Exception
{    testValues(toBytes("abc"));}
public void beam_f23127_0() throws Exception
{    BufferedElementCountingOutputStream os = testValues(toBytes("a"));    os.finish();    os.finish();    os.finish();}
public void beam_f23128_0() throws Exception
{    ByteArrayOutputStream baos = new ByteArrayOutputStream();    BufferedElementCountingOutputStream os = createAndWriteValues(toBytes("abcdefghij"), baos);    os.close();    verifyValues(toBytes("abcdefghij"), new ByteArrayInputStream(baos.toByteArray()));}
public void beam_f23129_0() throws Exception
{    expectedException.expect(IOException.class);    expectedException.expectMessage("Stream has been finished.");    testValues(toBytes("a")).markElementStart();}
private void beam_f23137_0(List<byte[]> expectedValues, InputStream is) throws Exception
{    List<byte[]> values = new ArrayList<>();    long count;    do {        count = VarInt.decodeLong(is);        for (int i = 0; i < count; ++i) {            values.add(ByteArrayCoder.of().decode(is));        }    } while (count > 0);    if (expectedValues.isEmpty()) {        assertTrue(values.isEmpty());    } else {        assertThat(values, IsIterableContainingInOrder.contains(expectedValues.toArray()));    }}
private BufferedElementCountingOutputStream beam_f23138_0(List<byte[]> values, OutputStream output) throws Exception
{    BufferedElementCountingOutputStream os = new BufferedElementCountingOutputStream(output, BUFFER_SIZE);    for (byte[] value : values) {        os.markElementStart();        ByteArrayCoder.of().encode(value, os);    }    return os;}
public static TestCoder beam_f23139_0()
{    return new TestCoder();}
public void beam_f23147_0() throws Exception
{    expectedException.expect(UnsupportedOperationException.class);    expectedException.expectMessage("Caller does not own the underlying");    CoderUtils.encodeToBase64(new ClosingCoder(), "test-value");}
public void beam_f23148_0() throws Exception
{    expectedException.expect(UnsupportedOperationException.class);    expectedException.expectMessage("Caller does not own the underlying");    CoderUtils.encodeToByteArray(new ClosingCoder(), "test-value");}
public void beam_f23149_0() throws Exception
{    expectedException.expect(UnsupportedOperationException.class);    expectedException.expectMessage("Caller does not own the underlying");    CoderUtils.encodeToByteArray(new ClosingCoder(), "test-value", Context.NESTED);}
public void beam_f23157_0() throws Exception
{    assertEquals(getClass().getName() + "#testMethodFormatter()", ReflectHelpers.CLASS_AND_METHOD_FORMATTER.apply(getClass().getMethod("testMethodFormatter")));    assertEquals(getClass().getName() + "#oneArg(int)", ReflectHelpers.CLASS_AND_METHOD_FORMATTER.apply(getClass().getDeclaredMethod("oneArg", int.class)));    assertEquals(getClass().getName() + "#twoArg(String, List)", ReflectHelpers.CLASS_AND_METHOD_FORMATTER.apply(getClass().getDeclaredMethod("twoArg", String.class, List.class)));}
public void beam_f23160_0() throws Exception
{    assertEquals("Integer", ReflectHelpers.TYPE_SIMPLE_DESCRIPTION.apply(Integer.class));    assertEquals("int", ReflectHelpers.TYPE_SIMPLE_DESCRIPTION.apply(int.class));    assertEquals("Map", ReflectHelpers.TYPE_SIMPLE_DESCRIPTION.apply(Map.class));    assertEquals(getClass().getSimpleName(), ReflectHelpers.TYPE_SIMPLE_DESCRIPTION.apply(getClass()));}
public void beam_f23161_0() throws Exception
{    assertEquals("Integer[]", ReflectHelpers.TYPE_SIMPLE_DESCRIPTION.apply(Integer[].class));    assertEquals("int[]", ReflectHelpers.TYPE_SIMPLE_DESCRIPTION.apply(int[].class));}
public String beam_f23169_0()
{    return "Zeta";}
public void beam_f23170_0()
{    List<String> names = new ArrayList<>();    for (FakeService service : ReflectHelpers.loadServicesOrdered(FakeService.class)) {        names.add(service.getName());    }    assertThat(names, contains("Alpha", "Zeta"));}
public void beam_f23171_0() throws IOException
{    try (ExposedByteArrayInputStream s = new ExposedByteArrayInputStream(new byte[0])) {        assertEquals(0, s.available());        byte[] data = s.readAll();        assertEquals(0, data.length);    }}
public void beam_f23179_0()
{    writeToBoth(32);    writeToBoth(32);    assertStreamContentsEquals(stream, exposedStream);}
public void beam_f23180_0() throws IOException
{    writeToBoth(TEST_DATA);    assertStreamContentsEquals(stream, exposedStream);    assertNotSame(TEST_DATA, exposedStream.toByteArray());}
public void beam_f23181_0() throws IOException
{    writeToBothFast(TEST_DATA);    assertStreamContentsEquals(stream, exposedStream);    assertSame(TEST_DATA, exposedStream.toByteArray());}
public void beam_f23189_0() throws IOException
{    writeToBothFast(TEST_DATA);    writeToBoth(32);    assertStreamContentsEquals(stream, exposedStream);}
public void beam_f23190_0() throws IOException
{    writeToBoth(32);    writeToBothFast(TEST_DATA);    assertStreamContentsEquals(stream, exposedStream);}
public void beam_f23191_0() throws IOException
{    writeToBothFast(TEST_DATA);    resetBoth();    assertStreamContentsEquals(stream, exposedStream);}
private void beam_f23199_0(ByteArrayOutputStream stream1, ByteArrayOutputStream stream2)
{    assertArrayEquals(stream1.toByteArray(), stream2.toByteArray());    assertEquals(stream1.toString(), stream2.toString());    assertEquals(stream1.size(), stream2.size());}
private void beam_f23200_0(int b)
{    exposedStream.write(b);    stream.write(b);}
private void beam_f23201_0(byte[] b) throws IOException
{    exposedStream.write(b);    stream.write(b);}
public void beam_f23209_0() throws Exception
{    String contents1 = "To be or not to be, ";    String contents2 = "it is not a question.";    File tmpFile1 = tmpFolder.newFile("result");    File tmpFile2 = tmpFolder.newFile("tmp");    Files.write(contents1, tmpFile1, StandardCharsets.UTF_8);    Files.write(contents2, tmpFile2, StandardCharsets.UTF_8);    FilePatternMatchingShardedFile shardedFile = new FilePatternMatchingShardedFile(filePattern);    assertThat(shardedFile.readFilesWithRetries(), containsInAnyOrder(contents1, contents2));}
public void beam_f23210_0() throws Exception
{    File emptyFile = tmpFolder.newFile("result-000-of-001");    Files.write("", emptyFile, StandardCharsets.UTF_8);    FilePatternMatchingShardedFile shardedFile = new FilePatternMatchingShardedFile(filePattern);    assertThat(shardedFile.readFilesWithRetries(), empty());}
public void beam_f23211_0() throws Exception
{    File tmpFile = tmpFolder.newFile();    Files.write("Test for file checksum verifier.", tmpFile, StandardCharsets.UTF_8);    FilePatternMatchingShardedFile shardedFile = spy(new FilePatternMatchingShardedFile(filePattern));    doThrow(IOException.class).when(shardedFile).readLines(anyCollection());    thrown.expect(IOException.class);    thrown.expectMessage(containsString("Unable to read file(s) after retrying"));    shardedFile.readFilesWithRetries(fastClock, backOff);}
public void beam_f23219_0() throws Exception
{    BackOff backOff = FluentBackoff.DEFAULT.withInitialBackoff(Duration.millis(500)).withMaxRetries(1).backoff();    assertThat(backOff.nextBackOffMillis(), allOf(greaterThanOrEqualTo(249L), lessThan(751L)));    assertThat(backOff.nextBackOffMillis(), equalTo(BackOff.STOP));    assertThat(backOff.nextBackOffMillis(), equalTo(BackOff.STOP));    assertThat(backOff.nextBackOffMillis(), equalTo(BackOff.STOP));    assertThat(backOff.nextBackOffMillis(), equalTo(BackOff.STOP));    backOff.reset();    assertThat(backOff.nextBackOffMillis(), allOf(greaterThanOrEqualTo(249L), lessThan(751L)));    assertThat(backOff.nextBackOffMillis(), equalTo(BackOff.STOP));}
private static long beam_f23220_0(BackOff backOff) throws IOException
{    long cumulativeBackoffMillis = 0;    long currentBackoffMillis = backOff.nextBackOffMillis();    while (currentBackoffMillis != BackOff.STOP) {        cumulativeBackoffMillis += currentBackoffMillis;        currentBackoffMillis = backOff.nextBackOffMillis();    }    return cumulativeBackoffMillis;}
public void beam_f23221_0() throws Exception
{    BackOff backOff = FluentBackoff.DEFAULT.withInitialBackoff(Duration.millis(500)).withMaxBackoff(Duration.standardSeconds(1)).withMaxCumulativeBackoff(Duration.standardMinutes(1)).backoff();    assertThat(countMaximumBackoff(backOff), equalTo(Duration.standardMinutes(1).getMillis()));    backOff.reset();    assertThat(countMaximumBackoff(backOff), equalTo(Duration.standardMinutes(1).getMillis()));        assertThat(countMaximumBackoff(backOff), equalTo(0L));    backOff.reset();    assertThat(countMaximumBackoff(backOff), equalTo(Duration.standardMinutes(1).getMillis()));}
public BoundedWindow beam_f23230_0(BoundedWindow window)
{    return window;}
private static TupleTag beam_f23231_0(String id)
{    return new TupleTag(id);}
public void beam_f23232_0() throws Exception
{    TupleTag tag = InstanceBuilder.ofType(TupleTag.class).fromClassName(InstanceBuilderTest.class.getName()).fromFactoryMethod("createTag").withArg(String.class, "hello world!").build();    Assert.assertEquals("hello world!", tag.getId());}
public void beam_f23240_0() throws Exception
{    AtomicInteger result = new AtomicInteger(0);    CompletionStage<Void> sideEffectFuture = MoreFutures.runAsync(() -> {        result.set(42);    });    MoreFutures.get(sideEffectFuture);    assertThat(result.get(), equalTo(42));}
public void beam_f23241_0() throws Exception
{    final String testMessage = "this is just a test";    CompletionStage<Void> sideEffectFuture = MoreFutures.runAsync(() -> {        throw new IllegalStateException(testMessage);    });    thrown.expect(ExecutionException.class);    thrown.expectCause(isA(IllegalStateException.class));    thrown.expectMessage(testMessage);    MoreFutures.get(sideEffectFuture);}
public long beam_f23242_0(long left, long right)
{    return left + right;}
public void beam_f23250_0()
{    MovingFunction f = newFunc();    f.add(0, 1);    f.add(SAMPLE_PERIOD * 3, 1);    assertEquals(1, f.get(SAMPLE_PERIOD * 3));}
public Object beam_f23252_0(InputStream inStream) throws IOException
{    return new AtomicInteger();}
public Object beam_f23253_0(Object value)
{    return uniqueInstance;}
public void beam_f23261_0() throws Exception
{    List<Integer> value = Lists.newLinkedList(Arrays.asList(1, 2, 3, 4));    MutationDetector detector = MutationDetectors.forValueWithCoder(value, IterableCoder.of(VarIntCoder.of()));    detector.verifyUnmodified();}
public void beam_f23262_0() throws Exception
{    Set<Integer> value = Sets.newHashSet(Arrays.asList(1, 2, 3, 4));    MutationDetector detector = MutationDetectors.forValueWithCoder(value, IterableCoder.of(VarIntCoder.of()));    detector.verifyUnmodified();}
public void beam_f23263_0() throws Exception
{    Set<Integer> value = Sets.newHashSet(Arrays.asList(1, 2, 3, 4));    MutationDetector detector = MutationDetectors.forValueWithCoder(value, IterableCoder.of(VarIntCoder.of()));    detector.verifyUnmodified();}
public void beam_f23271_0()
{    assertEquals("Foo.Bar", NameUtils.approximateSimpleName("Foo$1$Bar", false));    assertEquals("Foo.1", NameUtils.approximateSimpleName("Foo$1", false));    assertEquals("Foo.2", NameUtils.approximateSimpleName("Foo$1$2", false));}
private EmbeddedDoFn beam_f23272_0()
{    return new DeeperEmbeddedDoFn();}
public PDone beam_f23273_0(PBegin begin)
{    throw new IllegalArgumentException("Should never be applied");}
public void beam_f23281_0() throws Exception
{    AnonymousClass anonymousClassObj = new AnonymousClass() {        class NamedInnerClass extends PTransform<PBegin, PDone> {            @Override            public PDone expand(PBegin begin) {                throw new IllegalArgumentException("Should never be applied");            }        }        @Override        public Object getInnerClassInstance() {            return new NamedInnerClass();        }    };    assertEquals("NamedInnerClass", NameUtils.approximateSimpleName(anonymousClassObj.getInnerClassInstance()));    assertEquals("NameUtilsTest.NamedInnerClass", NameUtils.approximatePTransformName(anonymousClassObj.getInnerClassInstance().getClass()));}
public PDone beam_f23282_0(PBegin begin)
{    throw new IllegalArgumentException("Should never be applied");}
public Object beam_f23283_0()
{    return new NamedInnerClass();}
public void beam_f23291_0() throws Exception
{    String contents1 = "To be or not to be, ", contents2 = "it is not a question.";        File tmpFile1 = tmpFolder.newFile("result0-total2");    File tmpFile2 = tmpFolder.newFile("result1-total2");    Files.write(contents1, tmpFile1, StandardCharsets.UTF_8);    Files.write(contents2, tmpFile2, StandardCharsets.UTF_8);    Pattern customizedTemplate = Pattern.compile("(?x) result (?<shardnum>\\d+) - total (?<numshards>\\d+)");    NumberedShardedFile shardedFile = new NumberedShardedFile(filePattern, customizedTemplate);    assertThat(shardedFile.readFilesWithRetries(), containsInAnyOrder(contents1, contents2));}
public void beam_f23292_0() throws Exception
{    File tmpFile = tmpFolder.newFile();    Files.write("Test for file checksum verifier.", tmpFile, StandardCharsets.UTF_8);    NumberedShardedFile shardedFile = new NumberedShardedFile(filePattern, Pattern.compile("incorrect-template"));    thrown.expect(IOException.class);    thrown.expectMessage(containsString("Unable to read file(s) after retrying " + NumberedShardedFile.MAX_READ_RETRIES));    shardedFile.readFilesWithRetries(fastClock, backOff);}
public void beam_f23293_0() throws Exception
{    File tmpFile = tmpFolder.newFile();    Files.write("Test for file checksum verifier.", tmpFile, StandardCharsets.UTF_8);    NumberedShardedFile shardedFile = spy(new NumberedShardedFile(filePattern));    doThrow(IOException.class).when(shardedFile).readLines(anyCollection());    thrown.expect(IOException.class);    thrown.expectMessage(containsString("Unable to read file(s) after retrying " + NumberedShardedFile.MAX_READ_RETRIES));    shardedFile.readFilesWithRetries(fastClock, backOff);}
public void beam_f23301_0() throws Exception
{    Schema nestedRowSchema = Schema.builder().addInt32Field("f_nestedInt32").addStringField("f_nestedString").build();    Schema schema = Schema.builder().addInt32Field("f_int32").addRowField("f_row", nestedRowSchema).build();    String rowString = "{\n" + "\"f_int32\" : 32,\n" + "\"f_row\" : {\n" + "             \"f_nestedInt32\" : 54,\n" + "             \"f_nestedString\" : \"foo\"\n" + "            }\n" + "}";    RowJsonDeserializer deserializer = RowJsonDeserializer.forSchema(schema);    Row parsedRow = newObjectMapperWith(deserializer).readValue(rowString, Row.class);    Row expectedRow = Row.withSchema(schema).addValues(32, Row.withSchema(nestedRowSchema).addValues(54, "foo").build()).build();    assertEquals(expectedRow, parsedRow);}
public void beam_f23302_0() throws Exception
{    Schema nestedRowSchema = Schema.builder().addInt32Field("f_nestedInt32").addStringField("f_nestedString").build();    Schema schema = Schema.builder().addInt32Field("f_int32").addRowField("f_row", nestedRowSchema).build();    String rowString = "{\n" + "\"f_int32\" : 32,\n" +     "\"f_row\" : []\n" + "}";    RowJsonDeserializer deserializer = RowJsonDeserializer.forSchema(schema);    thrown.expect(UnsupportedRowJsonException.class);    thrown.expectMessage("Expected JSON object");    newObjectMapperWith(deserializer).readValue(rowString, Row.class);}
public void beam_f23303_0() throws Exception
{    Schema doubleNestedRowSchema = Schema.builder().addStringField("f_doubleNestedString").build();    Schema nestedRowSchema = Schema.builder().addRowField("f_nestedRow", doubleNestedRowSchema).build();    Schema schema = Schema.builder().addRowField("f_row", nestedRowSchema).build();    String rowString = "{\n" + "\"f_row\" : {\n" + "             \"f_nestedRow\" : {\n" + "                                \"f_doubleNestedString\":\"foo\"\n" + "                               }\n" + "            }\n" + "}";    RowJsonDeserializer deserializer = RowJsonDeserializer.forSchema(schema);    Row parsedRow = newObjectMapperWith(deserializer).readValue(rowString, Row.class);    Row expectedRow = Row.withSchema(schema).addValues(Row.withSchema(nestedRowSchema).addValues(Row.withSchema(doubleNestedRowSchema).addValues("foo").build()).build()).build();    assertEquals(expectedRow, parsedRow);}
public void beam_f23311_0() throws Exception
{    testSupportedConversion(FieldType.BYTE, BYTE_STRING, BYTE_VALUE);}
public void beam_f23312_0() throws Exception
{    testSupportedConversion(FieldType.INT16, BYTE_STRING, (short) BYTE_VALUE);    testSupportedConversion(FieldType.INT16, SHORT_STRING, SHORT_VALUE);}
public void beam_f23313_0() throws Exception
{    testSupportedConversion(FieldType.INT32, BYTE_STRING, (int) BYTE_VALUE);    testSupportedConversion(FieldType.INT32, SHORT_STRING, (int) SHORT_VALUE);    testSupportedConversion(FieldType.INT32, INT_STRING, INT_VALUE);}
public void beam_f23321_0() throws Exception
{    testUnsupportedConversion(FieldType.INT16, BOOLEAN_TRUE_STRING);    testUnsupportedConversion(FieldType.INT16, quoted(SHORT_STRING));    testUnsupportedConversion(FieldType.INT16, INT_STRING);    testUnsupportedConversion(FieldType.INT16, LONG_STRING);    testUnsupportedConversion(FieldType.INT16, FLOAT_STRING);    testUnsupportedConversion(FieldType.INT16, DOUBLE_STRING);}
public void beam_f23322_0() throws Exception
{    testUnsupportedConversion(FieldType.INT32, quoted(INT_STRING));    testUnsupportedConversion(FieldType.INT32, BOOLEAN_TRUE_STRING);    testUnsupportedConversion(FieldType.INT32, LONG_STRING);    testUnsupportedConversion(FieldType.INT32, FLOAT_STRING);    testUnsupportedConversion(FieldType.INT32, DOUBLE_STRING);}
public void beam_f23323_0() throws Exception
{    testUnsupportedConversion(FieldType.INT64, quoted(LONG_STRING));    testUnsupportedConversion(FieldType.INT64, BOOLEAN_TRUE_STRING);    testUnsupportedConversion(FieldType.INT64, FLOAT_STRING);    testUnsupportedConversion(FieldType.INT64, DOUBLE_STRING);}
private ObjectMapper beam_f23331_0(RowJsonDeserializer deserializer)
{    SimpleModule simpleModule = new SimpleModule("rowSerializationTesModule");    simpleModule.addDeserializer(Row.class, deserializer);    ObjectMapper objectMapper = new ObjectMapper();    objectMapper.registerModule(simpleModule);    return objectMapper;}
public void beam_f23332_0() throws Exception
{        final ClassLoader testLoader = Thread.currentThread().getContextClassLoader();    final ClassLoader loader = new InterceptingUrlClassLoader(testLoader, Foo.class.getName());    final Class<?> source = loader.loadClass(Foo.class.getName());    assertNotSame(source.getClassLoader(), Foo.class.getClassLoader());        final Serializable customLoaderSourceInstance = Serializable.class.cast(source.getConstructor().newInstance());    final Thread thread = Thread.currentThread();    thread.setContextClassLoader(loader);    try {        assertSerializationClassLoader(loader, customLoaderSourceInstance);    } finally {        thread.setContextClassLoader(testLoader);    }        assertSerializationClassLoader(loader, customLoaderSourceInstance);}
public void beam_f23333_0()
{    String stringValue = "hi bob";    int intValue = 42;    SerializableByJava testObject = new SerializableByJava(stringValue, intValue);    SerializableByJava testCopy = SerializableUtils.ensureSerializable(testObject);    assertEquals(stringValue, testCopy.stringValue);    assertEquals(intValue, testCopy.intValue);}
public void beam_f23343_0() throws IOException
{    InputStream stream = new ExposedByteArrayInputStream(testData);    byte[] bytes = StreamUtils.getBytesWithoutClosing(stream);    assertArrayEquals(testData, bytes);    assertSame(testData, bytes);    assertEquals(0, stream.available());}
public void beam_f23344_0() throws IOException
{    InputStream stream = new ByteArrayInputStream(testData);    byte[] bytes = StreamUtils.getBytesWithoutClosing(stream);    assertArrayEquals(testData, bytes);    assertEquals(0, stream.available());}
public void beam_f23345_0() throws IOException
{        InputStream stream = new BufferedInputStream(new ByteArrayInputStream(testData));    byte[] bytes = StreamUtils.getBytesWithoutClosing(stream);    assertArrayEquals(testData, bytes);    assertEquals(0, stream.available());}
public void beam_f23353_0() throws Exception
{    expectedException.expect(UnsupportedOperationException.class);    expectedException.expectMessage("Caller does not own the underlying");    expectedException.expectMessage("reset()");    os.reset();}
public void beam_f23354_0()
{    baos = new ByteArrayOutputStream();    os = new UnownedOutputStream(baos);}
public void beam_f23355_0() throws Exception
{    assertEquals(baos.hashCode(), os.hashCode());    assertEquals("UnownedOutputStream{out=" + baos + "}", os.toString());    assertEquals(new UnownedOutputStream(baos), os);}
public void beam_f23363_0()
{    RuntimeException runtimeException = new RuntimeException("oh noes!");    RuntimeException wrapped = UserCodeException.wrapIf(false, runtimeException);    assertEquals(runtimeException, wrapped);}
public void beam_f23364_0()
{    RuntimeException runtimeException = new RuntimeException("empty stack");    runtimeException.setStackTrace(new StackTraceElement[0]);    RuntimeException wrapped = UserCodeException.wrap(runtimeException);    assertEquals(runtimeException, wrapped.getCause());}
private void beam_f23365_0()
{    try {        userCode();    } catch (Exception ex) {        throw UserCodeException.wrap(ex);    }}
protected StackTraceElement beam_f23373_0(Throwable actual)
{    StackTraceElement[] stackTrace = actual.getStackTrace();    return stackTrace[stackTrace.length - 1];}
public void beam_f23374_0(Description description)
{    description.appendText("stack frame where method name ");    methodNameMatcher.describeTo(description);}
protected boolean beam_f23375_0(StackTraceElement item)
{    return methodNameMatcher.matches(item.getMethodName());}
public void beam_f23383_0() throws IOException
{    byte[] encoded = encodeLong(1L << 32);    thrown.expect(IOException.class);    decodeInt(encoded);}
public void beam_f23384_0() throws IOException
{    byte[] encoded = encodeLong(-1);    thrown.expect(IOException.class);    decodeInt(encoded);}
public void beam_f23385_0() throws IOException
{    final byte[] nonTerminatedNumber = { (byte) 0xff, (byte) 0xff };    thrown.expect(IOException.class);    decodeLong(nonTerminatedNumber);}
public void beam_f23393_0()
{    Instant now = Instant.now();    BoundedWindow window = new IntervalWindow(now.minus(1000L), now.plus(1000L));    WindowedValue<String> value = WindowedValue.of("foo", now, window, PaneInfo.ON_TIME_AND_ONLY_FIRING);    assertThat(Iterables.getOnlyElement(value.explodeWindows()), equalTo(value));}
public void beam_f23394_0()
{    Instant now = Instant.now();    BoundedWindow centerWindow = new IntervalWindow(now.minus(1000L), now.plus(1000L));    BoundedWindow pastWindow = new IntervalWindow(now.minus(1500L), now.plus(500L));    BoundedWindow futureWindow = new IntervalWindow(now.minus(500L), now.plus(1500L));    BoundedWindow futureFutureWindow = new IntervalWindow(now, now.plus(2000L));    PaneInfo pane = PaneInfo.createPane(false, false, Timing.ON_TIME, 3L, 0L);    WindowedValue<String> value = WindowedValue.of("foo", now, ImmutableList.of(pastWindow, centerWindow, futureWindow, futureFutureWindow), pane);    assertThat(value.explodeWindows(), containsInAnyOrder(WindowedValue.of("foo", now, futureFutureWindow, pane), WindowedValue.of("foo", now, futureWindow, pane), WindowedValue.of("foo", now, centerWindow, pane), WindowedValue.of("foo", now, pastWindow, pane)));    assertThat(value.isSingleWindowedValue(), equalTo(false));}
public void beam_f23395_0()
{    WindowedValue<Integer> value = WindowedValue.of(1, Instant.now(), GlobalWindow.INSTANCE, PaneInfo.NO_FIRING);    assertThat(value.isSingleWindowedValue(), equalTo(true));    assertThat(((WindowedValue.SingleWindowedValue) value).getWindow(), equalTo(GlobalWindow.INSTANCE));}
public void beam_f23403_0() throws Exception
{    File zipDir = new File(tmpDir, "zip");    assertTrue(zipDir.mkdirs());    createFileWithContents(zipDir, "myTextFile.txt", "Simple Text");    ZipFiles.zipDirectory(tmpDir, zipFile);    try (ZipFile zip = new ZipFile(zipFile)) {        ZipEntry entry = zip.getEntry("zip/myTextFile.txt");        ByteSource byteSource = ZipFiles.asByteSource(zip, entry);        if (entry.getSize() != -1) {            assertEquals(entry.getSize(), byteSource.size());        }        assertArrayEquals("Simple Text".getBytes(StandardCharsets.UTF_8), byteSource.read());    }}
public void beam_f23404_0() throws Exception
{    File zipDir = new File(tmpDir, "zip");    assertTrue(zipDir.mkdirs());    createFileWithContents(zipDir, "myTextFile.txt", "Simple Text");    ZipFiles.zipDirectory(tmpDir, zipFile);    try (ZipFile zip = new ZipFile(zipFile)) {        ZipEntry entry = zip.getEntry("zip/myTextFile.txt");        CharSource charSource = ZipFiles.asCharSource(zip, entry, StandardCharsets.UTF_8);        assertEquals("Simple Text", charSource.read());    }}
private void beam_f23405_0(String zipFileEntry) throws IOException
{    try (ZipFile zippedFile = new ZipFile(zipFile)) {        assertEquals(1, zippedFile.size());        ZipEntry entry = zippedFile.entries().nextElement();        assertEquals(zipFileEntry, entry.getName());    }}
private int beam_f23413_0(Integer a, Integer b)
{    if (a == null) {        return b == null ? 0 : -1;    } else {        return b == null ? 1 : a.compareTo(b);    }}
public void beam_f23414_0()
{        assertThat(KV.of(1, 2), equalTo(KV.of(1, 2)));        assertThat(KV.of(new int[] { 1, 2 }, 3), equalTo(KV.of(new int[] { 1, 2 }, 3)));        assertThat(KV.of(1, new int[] { 2, 3 }), equalTo(KV.of(1, new int[] { 2, 3 })));        assertThat(KV.of(new int[] { 1, 2 }, new int[] { 3, 4 }), equalTo(KV.of(new int[] { 1, 2 }, new int[] { 3, 4 })));        assertThat(KV.of(ImmutableList.of(new int[] { 1, 2 }), 3), not(equalTo(KV.of(ImmutableList.of(new int[] { 1, 2 }), 3))));    assertThat(KV.of(1, ImmutableList.of(new int[] { 2, 3 })), not(equalTo(KV.of(1, ImmutableList.of(new int[] { 2, 3 })))));        assertThat(KV.of(new int[] { 1, 2 }, 3), not(equalTo(KV.of(new int[] { 1, 37 }, 3))));        assertThat(KV.of(1, new int[] { 2, 3 }), not(equalTo(KV.of(37, new int[] { 1, 2 }))));        assertThat(KV.of(1, new int[] { 2, 3 }), not(equalTo(KV.of(1, new int[] { 37, 3 }))));        assertThat(KV.of(new byte[] { 1, 2 }, 3), not(equalTo(KV.of(new byte[] { 1, 2 }, 37))));}
public void beam_f23415_0()
{    Comparator<KV<Integer, Integer>> orderByKey = new KV.OrderByKey<>();    for (Integer key1 : TEST_VALUES) {        for (Integer val1 : TEST_VALUES) {            for (Integer key2 : TEST_VALUES) {                for (Integer val2 : TEST_VALUES) {                    assertEquals(compareInt(key1, key2), orderByKey.compare(KV.of(key1, val1), KV.of(key2, val2)));                }            }        }    }}
public void beam_f23423_0()
{    pipeline.enableAbandonedNodeEnforcement(true);    List<Integer> inputs = Arrays.asList(3, -42, 666);    TupleTag<Integer> mainOutputTag = new TupleTag<Integer>("main") {    };    TupleTag<Integer> emptyOutputTag = new TupleTag<Integer>("empty") {    };    final TupleTag<Integer> additionalOutputTag = new TupleTag<Integer>("extra") {    };    PCollection<Integer> mainInput = pipeline.apply(Create.of(inputs));    PCollectionTuple outputs = mainInput.apply(ParDo.of(new DoFn<Integer, Integer>() {        @ProcessElement        public void processElement(ProcessContext c) {            c.output(additionalOutputTag, c.element());        }    }).withOutputTags(emptyOutputTag, TupleTagList.of(additionalOutputTag)));    assertNotNull("outputs.getPipeline()", outputs.getPipeline());    outputs = outputs.and(mainOutputTag, mainInput);    PAssert.that(outputs.get(mainOutputTag)).containsInAnyOrder(inputs);    PAssert.that(outputs.get(additionalOutputTag)).containsInAnyOrder(inputs);    PAssert.that(outputs.get(emptyOutputTag)).empty();    pipeline.run();}
public void beam_f23424_0(ProcessContext c)
{    c.output(additionalOutputTag, c.element());}
public void beam_f23425_0()
{    TestPipeline p = TestPipeline.create();    TupleTag<Long> longTag = new TupleTag<>();    PCollection<Long> longs = p.apply(GenerateSequence.from(0));    TupleTag<String> strTag = new TupleTag<>();    PCollection<String> strs = p.apply(Create.of("foo", "bar"));    EqualsTester tester = new EqualsTester();        tester.addEqualityGroup(PCollectionTuple.empty(p), PCollectionTuple.empty(p));    tester.addEqualityGroup(PCollectionTuple.of(longTag, longs).and(strTag, strs), PCollectionTuple.of(longTag, longs).and(strTag, strs));    tester.addEqualityGroup(PCollectionTuple.of(longTag, longs));    tester.addEqualityGroup(PCollectionTuple.of(strTag, strs));    TestPipeline otherPipeline = TestPipeline.create();        tester.addEqualityGroup(PCollectionTuple.empty(otherPipeline));    tester.testEquals();}
public void beam_f23433_0()
{    Schema type = Stream.of(Schema.Field.of("f_int", Schema.FieldType.INT32)).collect(toSchema());    thrown.expect(IllegalArgumentException.class);    Row.nullRow(type);}
public void beam_f23434_0()
{    Schema schema = Schema.builder().addByteField("f_byte").addInt16Field("f_int16").addInt32Field("f_int32").addInt64Field("f_int64").addDecimalField("f_decimal").addFloatField("f_float").addDoubleField("f_double").addStringField("f_string").addDateTimeField("f_datetime").addBooleanField("f_boolean").build();    DateTime dateTime = new DateTime().withDate(1979, 03, 14).withTime(1, 2, 3, 4).withZone(DateTimeZone.UTC);    Row row = Row.withSchema(schema).addValues((byte) 0, (short) 1, 2, 3L, new BigDecimal(2.3), 1.2f, 3.0d, "str", dateTime, false).build();    assertEquals((byte) 0, (Object) row.getByte("f_byte"));    assertEquals((byte) 0, (Object) row.getByte(0));    assertEquals((short) 1, (Object) row.getInt16("f_int16"));    assertEquals((short) 1, (Object) row.getInt16(1));    assertEquals((int) 2, (Object) row.getInt32("f_int32"));    assertEquals((int) 2, (Object) row.getInt32(2));    assertEquals((long) 3, (Object) row.getInt64("f_int64"));    assertEquals((long) 3, (Object) row.getInt64(3));    assertEquals(new BigDecimal(2.3), row.getDecimal("f_decimal"));    assertEquals(new BigDecimal(2.3), row.getDecimal(4));    assertEquals(1.2f, row.getFloat("f_float"), 0);    assertEquals(1.2f, row.getFloat(5), 0);    assertEquals(3.0d, row.getDouble("f_double"), 0);    assertEquals(3.0d, row.getDouble(6), 0);    assertEquals("str", row.getString("f_string"));    assertEquals("str", row.getString(7));    assertEquals(dateTime, row.getDateTime("f_datetime"));    assertEquals(dateTime, row.getDateTime(8));    assertFalse(row.getBoolean("f_boolean"));    assertFalse(row.getBoolean(9));}
public void beam_f23435_0()
{    Schema nestedType = Stream.of(Schema.Field.of("f1_str", Schema.FieldType.STRING)).collect(toSchema());    Schema type = Stream.of(Schema.Field.of("f_int", Schema.FieldType.INT32), Schema.Field.of("nested", Schema.FieldType.row(nestedType))).collect(toSchema());    Row nestedRow = Row.withSchema(nestedType).addValues("foobar").build();    Row row = Row.withSchema(type).addValues(42, nestedRow).build();    assertEquals((int) 42, (Object) row.getInt32("f_int"));    assertEquals("foobar", row.getRow("nested").getString("f1_str"));}
public void beam_f23443_0()
{    Map<Integer, String> data = ImmutableMap.<Integer, String>builder().put(1, "value1").put(2, "value2").put(3, "value3").put(4, "value4").build();    Schema type = Stream.of(Schema.Field.of("map", FieldType.map(FieldType.INT32, FieldType.STRING))).collect(toSchema());    Row row = Row.withSchema(type).addValue(data).build();    assertEquals(data, row.getMap("map"));}
public void beam_f23444_0()
{    List<Integer> data = null;    Schema type = Stream.of(Schema.Field.nullable("map", FieldType.map(FieldType.INT32, FieldType.STRING))).collect(toSchema());    Row row = Row.withSchema(type).addValue(data).build();    assertEquals(data, row.getArray("map"));    Row otherNonNull = Row.withSchema(type).addValue(ImmutableMap.of(1, "value1")).build();    Row otherNull = Row.withSchema(type).addValue(null).build();    assertNotEquals(otherNonNull, row);    assertEquals(otherNull, row);}
public void beam_f23445_0()
{    Map<Integer, String> data = new HashMap();    data.put(1, "value1");    data.put(2, "value2");    data.put(3, null);    data.put(4, null);    Schema type = Stream.of(Schema.Field.of("map", FieldType.map(FieldType.INT32, FieldType.STRING, true))).collect(toSchema());    Row row = Row.withSchema(type).addValue(data).build();    assertEquals(data, row.getMap("map"));}
public void beam_f23453_0()
{    byte[] a0 = new byte[] { 1, 2, 3, 4 };    byte[] b0 = new byte[] { 1, 2, 3, 4 };    Schema schema = Schema.of(Schema.Field.of("bytes", Schema.FieldType.BYTES));    Row a = Row.withSchema(schema).addValue(ByteBuffer.wrap(a0)).build();    Row b = Row.withSchema(schema).addValue(ByteBuffer.wrap(b0)).build();    Assert.assertEquals(a, b);}
public void beam_f23454_0()
{    Instant now = Instant.now();    TimestampedValue<String> tsv = TimestampedValue.of("foobar", now);    assertEquals(now, tsv.getTimestamp());    assertEquals("foobar", tsv.getValue());}
public void beam_f23455_0()
{    TimestampedValue<String> tsv = TimestampedValue.atMinimumTimestamp("foobar");    assertEquals(BoundedWindow.TIMESTAMP_MIN_VALUE, tsv.getTimestamp());}
public void beam_f23463_0()
{    assertEquals("org.apache.beam.sdk.values.TupleTagTest#0", staticTag.getId());    assertEquals("org.apache.beam.sdk.values.TupleTagTest#3", staticBlockTag.getId());    assertEquals("org.apache.beam.sdk.values.TupleTagTest#1", staticMethodTag.getId());    assertEquals("org.apache.beam.sdk.values.TupleTagTest#2", instanceMethodTag.getId());    assertEquals("org.apache.beam.sdk.values.TupleTagTest$AnotherClass#0", AnotherClass.anotherTag.getId());}
private TupleTag<Object> beam_f23464_0()
{    return new TupleTag<>();}
public void beam_f23465_0()
{    assertNotEquals(new TupleTag<>().getId(), new TupleTag<>().getId());    assertNotEquals(createNonstaticTupleTag(), createNonstaticTupleTag());    TupleTag<Object> tag = createNonstaticTupleTag();        assertThat(Iterables.get(Splitter.on('#').split(tag.getId()), 0), startsWith("org.apache.beam.sdk.values.TupleTagTest.createNonstaticTupleTag"));                assertThat(Integer.parseInt(Iterables.get(Splitter.on(':').split(Iterables.get(Splitter.on('#').split(tag.getId()), 0)), 1)), greaterThan(15));}
private static TypeDescriptor<ActualBarT> beam_f23473_0(Generic<ActualFooT, ActualBarT> instance)
{    return TypeDescriptors.extractFromTypeParameters(instance, Generic.class, new TypeDescriptors.TypeVariableExtractor<Generic<ActualFooT, ActualBarT>, ActualBarT>() {    });}
private static TypeDescriptor<KV<ActualFooT, ActualBarT>> beam_f23474_0(Generic<ActualFooT, ActualBarT> instance)
{    return TypeDescriptors.extractFromTypeParameters(instance, Generic.class, new TypeDescriptors.TypeVariableExtractor<Generic<ActualFooT, ActualBarT>, KV<ActualFooT, ActualBarT>>() {    });}
public void beam_f23475_0() throws Exception
{    assertEquals(strings(), extractFooT(new Generic<String, Integer>() {    }));    assertEquals(integers(), extractBarT(new Generic<String, Integer>() {    }));    assertEquals(kvs(strings(), integers()), extractKV(new Generic<String, Integer>() {    }));}
public void beam_f23483_0() throws Exception
{    TypeRemembererer<String, Integer> remembererer = new TypeRemembererer<String, Integer>() {    };    assertEquals(new TypeToken<String>() {    }.getType(), remembererer.descriptor1.getType());    assertEquals(new TypeToken<Integer>() {    }.getType(), remembererer.descriptor2.getType());    TypeRemembererer<List<String>, Set<Integer>> genericRemembererer = new TypeRemembererer<List<String>, Set<Integer>>() {    };    assertEquals(new TypeToken<List<String>>() {    }.getType(), genericRemembererer.descriptor1.getType());    assertEquals(new TypeToken<Set<Integer>>() {    }.getType(), genericRemembererer.descriptor2.getType());}
public void beam_f23484_0() throws Exception
{    @SuppressWarnings("rawtypes")    TypeVariable<Class<? super GenericClass>> bizzleT = TypeDescriptor.of(GenericClass.class).getTypeParameter("BizzleT");    assertEquals(GenericClass.class.getTypeParameters()[0], bizzleT);}
public void beam_f23485_0() throws Exception
{    thrown.expect(IllegalArgumentException.class);        thrown.expectMessage("MerpleT");    TypeDescriptor.of(GenericClass.class).getTypeParameter("MerpleT");}
public void beam_f23493_0(ProcessContext c) throws Exception
{    c.output(c.element());}
private PCollectionTuple beam_f23494_0(TupleTag<Integer> mainOutputTag, TupleTag<Integer> additionalOutputTag)
{    PCollection<Integer> input = p.apply(Create.of(1, 2, 3));    PCollectionTuple tuple = input.apply(ParDo.of(new IdentityDoFn()).withOutputTags(mainOutputTag, TupleTagList.of(additionalOutputTag)));    return tuple;}
private static TupleTag<T> beam_f23495_0()
{    return new TupleTag<T>() {    };}
public void beam_f23503_0()
{    CoderProperties.coderSerializable(ValueWithRecordIdCoder.of(GlobalWindow.Coder.INSTANCE));}
 static AccumulatorProvider.Factory beam_f23504_0(Pipeline pipeline)
{    return pipeline.getOptions().as(EuphoriaOptions.class).getAccumulatorProviderFactory();}
 void beam_f23505_0(long duration, TimeUnit unit)
{    add(Duration.ofMillis(unit.toMillis(duration)));}
public AccumulatorProvider beam_f23513_1()
{    if (isLogged.compareAndSet(false, true)) {            }    return PROVIDER;}
public static CompositeUnaryFunction<InputT, OutputT, X> beam_f23519_0(UnaryFunction<InputT, X> first, UnaryFunction<X, OutputT> second)
{    return new CompositeUnaryFunction<>(first, second);}
public OutputT beam_f23520_0(InputT what)
{    return second.apply(first.apply(what));}
public Split.UsingBuilder<InputT> beam_f23528_0(PCollection<InputT> input)
{    return new Split.UsingBuilder<>(name, input);}
public Split.OutputBuilder<InputT> beam_f23529_0(UnaryPredicate<InputT> predicate)
{    return new Split.OutputBuilder<>(name, input, predicate);}
public Output<InputT> beam_f23530_0()
{    PCollection<InputT> positiveOutput = Filter.named(name + POSITIVE_FILTER_SUFFIX).of(input).by(predicate).output();    PCollection<InputT> negativeOutput = Filter.named(name + NEGATIVE_FILTER_SUFFIX).of(input).by((UnaryPredicate<InputT>) what -> !predicate.apply(what)).output();    return new Output<>(positiveOutput, negativeOutput);}
public PCollection<InputT> beam_f23538_0(OutputHint... outputHints)
{    return OperatorTransform.apply(new AssignEventTime<>(name, eventTimeExtractor, allowedTimestampSkew, input.getTypeDescriptor()), PCollectionList.of(input));}
public ExtractEventTime<InputT> beam_f23539_0()
{    return eventTimeExtractor;}
public PCollection<InputT> beam_f23540_0(PCollectionList<InputT> inputs)
{    final PCollection<InputT> input = PCollectionLists.getOnlyElement(inputs);    return FlatMap.named(getName().orElse(null)).of(input).using((InputT element, Collector<InputT> coll) -> coll.collect(element), input.getTypeDescriptor()).eventTimeBy(getEventTimeExtractor(), allowedTimestampSkew).output();}
public UnaryFunction<InputT, KeyT> beam_f23548_0()
{    return keyExtractor;}
public Optional<TypeDescriptor<KeyT>> beam_f23549_0()
{    return Optional.ofNullable(keyType);}
public Optional<Window<InputT>> beam_f23550_0()
{    return Optional.ofNullable(window);}
public AccumulationModeBuilder<KeyT> beam_f23558_0(Trigger trigger)
{    windowBuilder.triggeredBy(trigger);    return this;}
public WindowedOutputBuilder<KeyT> beam_f23559_0(WindowingStrategy.AccumulationMode accumulationMode)
{    windowBuilder.accumulationMode(accumulationMode);    return this;}
public WindowedOutputBuilder<KeyT> beam_f23560_0(Duration allowedLateness)
{    windowBuilder.withAllowedLateness(allowedLateness);    return this;}
 WindowByBuilder<InputT, KeyT> beam_f23568_0(UnaryFunction<InputT, KeyT> transform)
{    return projected(transform, SelectionPolicy.ANY, null);}
 WindowByBuilder<InputT, KeyT> beam_f23569_0(UnaryFunction<InputT, KeyT> transform, TypeDescriptor<KeyT> projectedType)
{    return projected(transform, SelectionPolicy.ANY, requireNonNull(projectedType));}
 WindowByBuilder<InputT, KeyT> beam_f23570_0(UnaryFunction<InputT, KeyT> transform, SelectionPolicy policy)
{    return projected(transform, policy, null);}
public WindowedOutputBuilder<InputT> beam_f23578_0(Duration allowedLateness, Window.ClosingBehavior closingBehavior)
{    windowBuilder.withAllowedLateness(allowedLateness, closingBehavior);    return this;}
public WindowedOutputBuilder<InputT> beam_f23579_0(TimestampCombiner timestampCombiner)
{    windowBuilder.withTimestampCombiner(timestampCombiner);    return this;}
public WindowedOutputBuilder<InputT> beam_f23580_0(Window.OnTimeBehavior behavior)
{    windowBuilder.withOnTimeBehavior(behavior);    return this;}
public static ByBuilder<InputT> beam_f23588_0(PCollection<InputT> input)
{    return named(null).of(input);}
public static OfBuilder beam_f23589_0(@Nullable String name)
{    return new Builder(name);}
public ByBuilder<T> beam_f23590_0(PCollection<T> input)
{    @SuppressWarnings("unchecked")    final Builder<T> cast = (Builder) this;    cast.input = requireNonNull(input);    return cast;}
public UsingBuilder<InputLocalT> beam_f23598_0(PCollection<InputLocalT> input)
{    @SuppressWarnings("unchecked")    Builder<InputLocalT, ?> cast = (Builder) this;    cast.input = requireNonNull(input);    return cast;}
public EventTimeBuilder<InputT, OutputLocalT> beam_f23599_0(UnaryFunctor<InputT, OutputLocalT> functor)
{    return using(functor, null);}
public EventTimeBuilder<InputT, OutputLocalT> beam_f23600_0(UnaryFunctor<InputT, OutputLocalT> functor, TypeDescriptor<OutputLocalT> outputType)
{    @SuppressWarnings("unchecked")    Builder<InputT, OutputLocalT> cast = (Builder) this;    cast.functor = requireNonNull(functor);    cast.outputType = outputType;    return cast;}
 UsingBuilder<LeftT, RightT, KeyT> beam_f23608_0(UnaryFunction<LeftT, KeyT> leftKeyExtractor, UnaryFunction<RightT, KeyT> rightKeyExtractor)
{    return by(leftKeyExtractor, rightKeyExtractor, null);}
 Join.WindowByBuilder<KeyT, OutputT> beam_f23609_0(BinaryFunctor<Optional<LeftT>, Optional<RightT>, OutputT> joinFunc)
{    return using(joinFunc, null);}
public ByBuilder<FirstT, SecondT> beam_f23610_0(PCollection<FirstT> left, PCollection<SecondT> right)
{    @SuppressWarnings("unchecked")    final Builder<FirstT, SecondT, ?> cast = (Builder) this;    cast.left = requireNonNull(left);    cast.right = requireNonNull(right);    return cast;}
public ByBuilder<FirstT, SecondT> beam_f23618_0(PCollection<FirstT> left, PCollection<SecondT> right)
{    @SuppressWarnings("unchecked")    final Builder<FirstT, SecondT, ?, ?> cast = (Builder) this;    cast.left = requireNonNull(left);    cast.right = requireNonNull(right);    return cast;}
public UsingBuilder<LeftT, RightT, T> beam_f23619_0(UnaryFunction<LeftT, T> leftKeyExtractor, UnaryFunction<RightT, T> rightKeyExtractor, @Nullable TypeDescriptor<T> keyType)
{    @SuppressWarnings("unchecked")    final Builder<LeftT, RightT, T, ?> cast = (Builder) this;    cast.leftKeyExtractor = leftKeyExtractor;    cast.rightKeyExtractor = rightKeyExtractor;    cast.keyType = keyType;    return cast;}
public WindowByBuilder<KeyT, T> beam_f23620_0(BinaryFunctor<LeftT, RightT, T> joinFunc, @Nullable TypeDescriptor<T> outputType)
{    @SuppressWarnings("unchecked")    final Builder<LeftT, RightT, KeyT, T> cast = (Builder) this;    cast.joinFunc = requireNonNull(joinFunc);    cast.outputType = outputType;    return cast;}
public PCollection<KV<KeyT, OutputT>> beam_f23628_0(OutputHint... outputHints)
{    @SuppressWarnings("unchecked")    final PCollectionList<Object> inputs = PCollectionList.of(Arrays.asList((PCollection) left, (PCollection) right));    return OperatorTransform.apply(createOperator(), inputs);}
public PCollection<OutputT> beam_f23629_0(OutputHint... outputHints)
{    @SuppressWarnings("unchecked")    final PCollectionList<Object> inputs = PCollectionList.of(Arrays.asList((PCollection) left, (PCollection) right));    return OperatorTransform.apply(new OutputValues<>(name, outputType, createOperator()), inputs);}
private Join<LeftT, RightT, KeyT, OutputT> beam_f23630_0()
{    return new Join<>(name, type, leftKeyExtractor, rightKeyExtractor, keyType, joinFunc, TypeDescriptors.kvs(TypeAwareness.orObjects(Optional.ofNullable(keyType)), TypeAwareness.orObjects(Optional.ofNullable(outputType))), windowBuilder.getWindow().orElse(null));}
 Join.WindowByBuilder<KeyT, OutputT> beam_f23638_0(BinaryFunctor<LeftT, Optional<RightT>, OutputT> joinFunc)
{    return using(joinFunc, null);}
public ByBuilder<FirstT, SecondT> beam_f23639_0(PCollection<FirstT> left, PCollection<SecondT> right)
{    @SuppressWarnings("unchecked")    final Builder<FirstT, SecondT, ?> cast = (Builder) this;    cast.left = requireNonNull(left);    cast.right = requireNonNull(right);    return cast;}
public UsingBuilder<LeftT, RightT, T> beam_f23640_0(UnaryFunction<LeftT, T> leftKeyExtractor, UnaryFunction<RightT, T> rightKeyExtractor, @Nullable TypeDescriptor<T> keyType)
{    @SuppressWarnings("unchecked")    final Builder<LeftT, RightT, T> cast = (Builder) this;    cast.leftKeyExtractor = requireNonNull(leftKeyExtractor);    cast.rightKeyExtractor = requireNonNull(rightKeyExtractor);    cast.keyType = keyType;    return cast;}
public Builders.Output<T> beam_f23648_0(UnaryFunctionEnv<InputT, T> mapper, @Nullable TypeDescriptor<T> outputType)
{    @SuppressWarnings("unchecked")    final Builder<InputT, T> cast = (Builder) this;    cast.mapper = mapper;    cast.outputType = outputType;    return cast;}
public PCollection<OutputT> beam_f23649_0(OutputHint... outputHints)
{    final MapElements<InputT, OutputT> operator = new MapElements<>(name, mapper, outputType);    return OperatorTransform.apply(operator, PCollectionList.of(input));}
public PCollection<OutputT> beam_f23650_0(PCollectionList<InputT> inputs)
{    return FlatMap.named(getName().orElse(null)).of(PCollectionLists.getOnlyElement(inputs)).using((InputT elem, Collector<OutputT> coll) -> coll.collect(getMapper().apply(elem, coll.asContext())), getOutputType().orElse(null)).output();}
 WithSortedValuesBuilder<KeyT, ValueT, OutputT> beam_f23658_0(ReduceFunction<ValueT, OutputT> reducer, TypeDescriptor<OutputT> outputType)
{    return reduceBy((Stream<ValueT> in, Collector<OutputT> ctx) -> ctx.collect(reducer.apply(in)), outputType);}
 WithSortedValuesBuilder<KeyT, ValueT, OutputT> beam_f23659_0(ReduceFunctor<ValueT, OutputT> reducer)
{    return reduceBy(reducer, null);}
 WindowByBuilder<KeyT, ValueT> beam_f23660_0(CombinableReduceFunction<ValueT> reducer)
{    return reduceBy(ReduceFunctor.of(reducer));}
 OutputBuilder<KeyT, OutputT> beam_f23668_0(boolean cond, UnaryFunction<WindowByBuilder<KeyT, OutputT>, OutputBuilder<KeyT, OutputT>> fn)
{    requireNonNull(fn);    return cond ? fn.apply(this) : this;}
public KeyByBuilder<T> beam_f23669_0(PCollection<T> input)
{    @SuppressWarnings("unchecked")    final Builder<T, ?, ?, ?, ?> cast = (Builder) this;    cast.input = input;    return cast;}
public ValueByReduceByBuilder<InputT, T, InputT> beam_f23670_0(UnaryFunction<InputT, T> keyExtractor, @Nullable TypeDescriptor<T> keyType)
{    @SuppressWarnings("unchecked")    final Builder<InputT, T, InputT, ?, ?> cast = (Builder) this;    cast.keyExtractor = requireNonNull(keyExtractor);    cast.keyType = keyType;    return cast;}
public WindowedOutputBuilder<KeyT, OutputT> beam_f23678_0(WindowingStrategy.AccumulationMode accumulationMode)
{    windowBuilder.accumulationMode(accumulationMode);    return this;}
public WindowedOutputBuilder<KeyT, OutputT> beam_f23679_0(Duration allowedLateness)
{    windowBuilder.withAllowedLateness(allowedLateness);    return this;}
public WindowedOutputBuilder<KeyT, OutputT> beam_f23680_0(Duration allowedLateness, Window.ClosingBehavior closingBehavior)
{    windowBuilder.withAllowedLateness(allowedLateness, closingBehavior);    return this;}
public ReduceFunctor<ValueT, OutputT> beam_f23688_0()
{    return requireNonNull(reducer, "Don't call #getReducer when #isCombinableFnStyle() == true");}
public VoidFunction<AccT> beam_f23689_0()
{    return requireNonNull(accumulatorFactory, "Don't vall #getAccumulatorFactory when #isCombinableFnStyle() == false");}
public BinaryFunction<AccT, ValueT, AccT> beam_f23690_0()
{    return requireNonNull(accumulate, "Don't vall #getAccumulate when #isCombinableFnStyle() == false");}
public static ValueByReduceByBuilder<InputT, InputT> beam_f23698_0(PCollection<InputT> input)
{    return named(null).of(input);}
public static OfBuilder beam_f23699_0(@Nullable String name)
{    return new Builder(name);}
 WithSortedValuesBuilder<ValueT, OutputT> beam_f23700_0(ReduceFunction<ValueT, OutputT> reducer)
{    return reduceBy((Stream<ValueT> in, Collector<OutputT> ctx) -> ctx.collect(reducer.apply(in)));}
 WindowByBuilder<ValueT> beam_f23708_0(ValueT identity, CombinableBinaryFunction<ValueT> reducer, @Nullable TypeDescriptor<ValueT> valueType)
{    return (WindowByBuilder<ValueT>) this.<ValueT, ValueT>combineBy(() -> identity, (BinaryFunction) reducer, reducer, e -> e, valueType, valueType);}
 WindowByBuilder<ValueT> beam_f23709_0(VoidFunction<AccT> accumulatorFactory, BinaryFunction<AccT, ValueT, AccT> accumulate, CombinableBinaryFunction<AccT> mergeAccumulators, UnaryFunction<AccT, ValueT> outputFn)
{    return combineBy(accumulatorFactory, accumulate, mergeAccumulators, outputFn, null, null);}
 ReduceByBuilder<T> beam_f23710_0(UnaryFunction<InputT, T> valueExtractor)
{    return valueBy(valueExtractor, null);}
public AccumulationModeBuilder<OutputT> beam_f23718_0(Trigger trigger)
{    windowBuilder.triggeredBy(trigger);    return this;}
public WindowedOutputBuilder<OutputT> beam_f23719_0(WindowingStrategy.AccumulationMode accumulationMode)
{    windowBuilder.accumulationMode(accumulationMode);    return this;}
public WindowedOutputBuilder<OutputT> beam_f23720_0(Duration allowedLateness)
{    windowBuilder.withAllowedLateness(allowedLateness);    return this;}
public boolean beam_f23728_0()
{    return isCombineFnStyle() || reducer.isCombinable();}
public UnaryFunction<InputT, ValueT> beam_f23729_0()
{    return valueExtractor;}
public Optional<BinaryFunction<ValueT, ValueT, Integer>> beam_f23730_0()
{    return Optional.ofNullable(valueComparator);}
public UsingBuilder<LeftT, RightT, T> beam_f23738_0(UnaryFunction<LeftT, T> leftKeyExtractor, UnaryFunction<RightT, T> rightKeyExtractor, @Nullable TypeDescriptor<T> keyType)
{    @SuppressWarnings("unchecked")    final Builder<LeftT, RightT, T> cast = (Builder) this;    cast.leftKeyExtractor = requireNonNull(leftKeyExtractor);    cast.rightKeyExtractor = requireNonNull(rightKeyExtractor);    cast.keyType = keyType;    return cast;}
public Join.WindowByBuilder<KeyT, OutputT> beam_f23739_0(BinaryFunctor<Optional<LeftT>, RightT, OutputT> joinFunc, @Nullable TypeDescriptor<OutputT> outputType)
{    return new Join.Builder<>(name, Type.RIGHT).of(left, right).by(leftKeyExtractor, rightKeyExtractor, keyType).using((LeftT l, RightT r, Collector<OutputT> c) -> joinFunc.apply(Optional.ofNullable(l), r, c), outputType);}
public static KeyByBuilder<InputT> beam_f23740_0(PCollection<InputT> input)
{    return named(null).of(input);}
public AccumulationModeBuilder<KeyT> beam_f23748_0(Trigger trigger)
{    windowBuilder.triggeredBy(trigger);    return this;}
public WindowedOutputBuilder<KeyT> beam_f23749_0(WindowingStrategy.AccumulationMode accumulationMode)
{    windowBuilder.accumulationMode(accumulationMode);    return this;}
public WindowedOutputBuilder<KeyT> beam_f23750_0(Duration allowedLateness)
{    windowBuilder.withAllowedLateness(allowedLateness);    return this;}
public static OfBuilder beam_f23758_0(@Nullable String name)
{    return new Builder<>(name);}
 ValueByBuilder<InputT, T> beam_f23759_0(UnaryFunction<InputT, T> keyExtractor)
{    return keyBy(keyExtractor, null);}
 ScoreBy<InputT, KeyT, ValueT> beam_f23760_0(UnaryFunction<InputT, ValueT> valueExtractor)
{    return valueBy(valueExtractor, null);}
public AccumulationModeBuilder<KeyT, ValueT, ScoreT> beam_f23768_0(Trigger trigger)
{    windowBuilder.triggeredBy(trigger);    return this;}
public WindowedOutputBuilder<KeyT, ValueT, ScoreT> beam_f23769_0(WindowingStrategy.AccumulationMode accumulationMode)
{    windowBuilder.accumulationMode(accumulationMode);    return this;}
public WindowedOutputBuilder<KeyT, ValueT, ScoreT> beam_f23770_0(Duration allowedLateness)
{    windowBuilder.withAllowedLateness(allowedLateness);    return this;}
public Optional<TypeDescriptor<ScoreT>> beam_f23778_0()
{    return Optional.ofNullable(scoreType);}
public PCollection<Triple<KeyT, ValueT, ScoreT>> beam_f23779_0(PCollectionList<InputT> inputs)
{    final PCollection<Triple<KeyT, ValueT, ScoreT>> extracted = MapElements.named("extract-key-value-score").of(PCollectionLists.getOnlyElement(inputs)).using(elem -> Triple.of(getKeyExtractor().apply(elem), getValueExtractor().apply(elem), getScoreExtractor().apply(elem)), getOutputType().orElse(null)).output();    return ReduceByKey.named("combine-by-key").of(extracted).keyBy(Triple::getFirst, getKeyType().orElse(null)).combineBy((Stream<Triple<KeyT, ValueT, ScoreT>> triplets) -> triplets.reduce((a, b) -> a.getThird().compareTo(b.getThird()) > 0 ? a : b).orElseThrow(IllegalStateException::new)).applyIf(getWindow().isPresent(), builder -> {        @SuppressWarnings("unchecked")        final ReduceByKey.WindowByInternalBuilder<InputT, KeyT, Triple<KeyT, ValueT, ScoreT>> cast = (ReduceByKey.WindowByInternalBuilder) builder;        return cast.windowBy(getWindow().orElseThrow(() -> new IllegalStateException("Unable to resolve windowing for TopPerKey expansion.")));    }).outputValues();}
public static Builders.Output<InputT> beam_f23780_0(PCollection<InputT>... pCollections)
{    return of(Arrays.asList(pCollections));}
public WindowBuilder<T> beam_f23788_0(WindowFn<Object, W> windowFn)
{    checkState(window == null, "Window is already set.");    window = Window.into(windowFn);    return this;}
public WindowBuilder<T> beam_f23789_0(Trigger trigger)
{    window = requireNonNull(window).triggering(trigger);    return this;}
public WindowBuilder<T> beam_f23790_0(WindowingStrategy.AccumulationMode accumulationMode)
{    switch(requireNonNull(accumulationMode)) {        case DISCARDING_FIRED_PANES:            window = requireNonNull(window).discardingFiredPanes();            break;        case ACCUMULATING_FIRED_PANES:            window = requireNonNull(window).accumulatingFiredPanes();            break;        default:            throw new IllegalArgumentException("Unknown accumulation mode [" + accumulationMode + "]");    }    return this;}
public static TypeDescriptor<Triple<K, V, ScoreT>> beam_f23798_0(TypeDescriptor<K> key, TypeDescriptor<V> value, TypeDescriptor<ScoreT> score)
{    if (Objects.isNull(key) || Objects.isNull(value) || Objects.isNull(score)) {        return null;    }    return new TypeDescriptor<Triple<K, V, ScoreT>>() {    }.where(new TypeParameter<K>() {    }, key).where(new TypeParameter<V>() {    }, value).where(new TypeParameter<ScoreT>() {    }, score);}
public static CombinableReduceFunction<T> beam_f23799_0(BinaryFunction<T, T, T> fold)
{    return s -> s.reduce(fold::apply).orElseThrow(() -> new IllegalStateException("Received empty stream on input!"));}
public static CombinableReduceFunction<T> beam_f23800_0(T identity, BinaryFunction<T, T, T> fold)
{    return s -> s.reduce(identity, fold::apply);}
public static ReduceByKey.CombineFunctionWithIdentity<Integer> beam_f23808_0()
{    return SUMS_OF_INT;}
public static ReduceByKey.CombineFunctionWithIdentity<Float> beam_f23809_0()
{    return SUMS_OF_FLOAT;}
public static ReduceByKey.CombineFunctionWithIdentity<Double> beam_f23810_0()
{    return SUMS_OF_DOUBLE;}
public PCollection<KV<KeyT, OutputT>> beam_f23818_0(Join<LeftT, RightT, KeyT, OutputT> operator, PCollectionList<Object> inputs)
{    checkArgument(inputs.size() == 2, "Join expects exactly two inputs.");    @SuppressWarnings("unchecked")    final PCollection<LeftT> left = (PCollection) inputs.get(0);    @SuppressWarnings("unchecked")    final PCollection<RightT> right = (PCollection) inputs.get(1);    PCollection<KV<KeyT, LeftT>> leftKeyed = left.apply("extract-keys-left", new ExtractKey<>(operator.getLeftKeyExtractor(), TypeAwareness.orObjects(operator.getKeyType())));    PCollection<KV<KeyT, RightT>> rightKeyed = right.apply("extract-keys-right", new ExtractKey<>(operator.getRightKeyExtractor(), TypeAwareness.orObjects(operator.getKeyType())));        if (operator.getWindow().isPresent()) {        @SuppressWarnings("unchecked")        final Window<KV<KeyT, LeftT>> leftWindow = (Window) operator.getWindow().get();        leftKeyed = leftKeyed.apply("window-left", leftWindow);        @SuppressWarnings("unchecked")        final Window<KV<KeyT, RightT>> rightWindow = (Window) operator.getWindow().get();        rightKeyed = rightKeyed.apply("window-right", rightWindow);    }    return translate(operator, left, leftKeyed, right, rightKeyed).setTypeDescriptor(operator.getOutputType().orElseThrow(() -> new IllegalStateException("Unable to infer output type descriptor.")));}
public static Factory beam_f23819_0()
{    return Factory.get();}
public Counter beam_f23820_0(String name)
{    throw new UnsupportedOperationException("BeamAccumulatorProvider doesn't support" + " getCounter(String name). Please specify namespace and name.");}
public void beam_f23828_0(long value)
{    Metrics.counter(getNamespace(), getName()).inc(value);}
public void beam_f23829_0()
{    Metrics.counter(getNamespace(), getName()).inc();}
public void beam_f23830_0(long value)
{    Metrics.distribution(getNamespace(), getName()).update(value);}
public void beam_f23838_0(ElemT elem)
{    adapter.collect(requireNonNull(context), elem);}
public Context beam_f23839_0()
{    return this;}
public Counter beam_f23840_0(String name)
{    return accumulators.getCounter(requireNonNull(operatorName, UNSUPPORTED), name);}
public Histogram beam_f23848_0(String name)
{    return accumulators.getHistogram(requireNonNull(operatorName, UNSUPPORTED), name);}
public Timer beam_f23849_0(String name)
{    return accumulators.getTimer(name);}
public PCollection<OutputT> beam_f23850_0(OperatorT operator, PCollectionList<InputT> inputs)
{    checkState(operator instanceof CompositeOperator, "Operator is not composite.");    return ((CompositeOperator<InputT, OutputT>) operator).expand(inputs);}
public Duration beam_f23858_0()
{    return timestampSkew;}
public void beam_f23859_0(DoFn<InputT, OutputT>.ProcessContext ctx, OutputT out)
{    if (eventTimeExtractor != null) {        InputT element = ctx.element();        ctx.outputWithTimestamp(out, new Instant(eventTimeExtractor.extractTimestamp(element)));    } else {        ctx.output(out);    }}
public final void beam_f23860_0(@Element KV<KeyT, CoGbkResult> element, ProcessContext ctx)
{    getCollector().setProcessContext(ctx);    doJoin(requireNonNull(element.getValue()).getAll(leftTag), requireNonNull(element.getValue()).getAll(rightTag));}
public String beam_f23868_0()
{    return "left-outer-join";}
 void beam_f23869_0(Iterable<LeftT> left, Iterable<RightT> right)
{    for (RightT rightValue : right) {        if (left.iterator().hasNext()) {            for (LeftT leftValue : left) {                getJoiner().apply(leftValue, rightValue, getCollector());            }        } else {            getJoiner().apply(null, rightValue, getCollector());        }    }}
public String beam_f23870_0()
{    return "::right-outer-join";}
private AccumulatorProvider beam_f23878_0()
{    if (accumulators == null) {        accumulators = factory.create();    }    return accumulators;}
public static PCollection<OutputT> beam_f23879_0(OperatorT operator, PCollectionList<InputT> inputs)
{    final Optional<OperatorTranslator<InputT, OutputT, OperatorT>> maybeTranslator = TranslatorProvider.of(inputs.getPipeline()).findTranslator(operator);    if (maybeTranslator.isPresent()) {        final PCollection<OutputT> output = inputs.apply(operator.getName().orElseGet(() -> operator.getClass().getName()), new OperatorTransform<>(operator, maybeTranslator.orElse(null)));        Preconditions.checkState(output.getTypeDescriptor() != null, "Translator should always return a typed PCollection.");        return output;    }    throw new IllegalStateException("Unable to find translator for basic operator [" + operator.getClass() + "] with name [" + operator.getName().orElse(null) + "]");}
public PCollection<OutputT> beam_f23880_0(PCollectionList<InputT> inputs)
{    return translator.translate(operator, inputs);}
public Builder beam_f23888_0(Class<OperatorT> clazz, OperatorTranslator<?, ?, ? extends OperatorT> operatorTranslator)
{    possibleTranslators.add(TranslationDescriptor.of(clazz, operatorTranslator));    return this;}
public Builder beam_f23889_0(Class<OperatorT> clazz, Predicate<OperatorT> predicate, OperatorTranslator<?, ?, ? extends OperatorT> operatorTranslator)
{    possibleTranslators.add(TranslationDescriptor.of(clazz, predicate, operatorTranslator));    return this;}
public Builder beam_f23890_0(Predicate<Operator> predicate, OperatorTranslator<?, ?, Operator> operatorTranslator)
{    possibleTranslators.add(TranslationDescriptor.of(predicate, operatorTranslator));    return this;}
public PCollection<KV<KeyT, OutputT>> beam_f23898_0(ReduceByKey<InputT, KeyT, ValueT, ?, OutputT> operator, PCollectionList<InputT> inputs)
{        checkState(!operator.getValueComparator().isPresent(), "Values sorting is not supported.");    final UnaryFunction<InputT, KeyT> keyExtractor = operator.getKeyExtractor();    final UnaryFunction<InputT, ValueT> valueExtractor = operator.getValueExtractor();    final PCollection<InputT> input = operator.getWindow().map(window -> PCollectionLists.getOnlyElement(inputs).apply(window)).orElseGet(() -> PCollectionLists.getOnlyElement(inputs));        final MapElements<InputT, KV<KeyT, ValueT>> extractor = MapElements.via(new KeyValueExtractor<>(keyExtractor, valueExtractor));    final PCollection<KV<KeyT, ValueT>> extracted = input.apply("extract-keys", extractor).setTypeDescriptor(TypeDescriptors.kvs(TypeAwareness.orObjects(operator.getKeyType()), TypeAwareness.orObjects(operator.getValueType())));    final AccumulatorProvider accumulators = new LazyAccumulatorProvider(AccumulatorProvider.of(inputs.getPipeline()));    if (operator.isCombinable()) {                @SuppressWarnings("unchecked")        final PCollection combined;        if (operator.isCombineFnStyle()) {            combined = extracted.apply("combine", Combine.perKey(asCombineFn(operator)));        } else {            combined = extracted.apply("combine", Combine.perKey(asCombiner(operator.getReducer(), accumulators, operator.getName().orElse(null))));        }        @SuppressWarnings("unchecked")        final PCollection<KV<KeyT, OutputT>> cast = (PCollection) combined;        return cast.setTypeDescriptor(operator.getOutputType().orElseThrow(() -> new IllegalStateException("Unable to infer output type descriptor.")));    }    return extracted.apply("group", GroupByKey.create()).setTypeDescriptor(TypeDescriptors.kvs(TypeAwareness.orObjects(operator.getKeyType()), TypeDescriptors.iterables(TypeAwareness.orObjects(operator.getValueType())))).apply("reduce", ParDo.of(new ReduceDoFn<>(operator.getReducer(), accumulators, operator.getName().orElse(null)))).setTypeDescriptor(operator.getOutputType().orElseThrow(() -> new IllegalStateException("Unable to infer output type descriptor.")));}
public boolean beam_f23899_0(ReduceByKey operator)
{        return !operator.getValueComparator().isPresent();}
private static Combine.CombineFn<ValueT, AccT, OutputT> beam_f23900_0(ReduceByKey<InputT, KeyT, ValueT, AccT, OutputT> operator)
{    @SuppressWarnings("unchecked")    ReduceByKey<InputT, KeyT, ValueT, AccT, OutputT> cast = (ReduceByKey) operator;    VoidFunction<AccT> accumulatorFactory = cast.getAccumulatorFactory();    BinaryFunction<AccT, ValueT, AccT> accumulate = cast.getAccumulate();    CombinableBinaryFunction<AccT> mergeAccumulators = cast.getMergeAccumulators();    UnaryFunction<AccT, OutputT> outputFn = cast.getOutputFn();    TypeDescriptor<AccT> accumulatorType = cast.getAccumulatorType();    return new Combine.CombineFn<ValueT, AccT, OutputT>() {        @Override        public AccT createAccumulator() {            return accumulatorFactory.apply();        }        @Override        public Coder<AccT> getAccumulatorCoder(CoderRegistry registry, Coder<ValueT> inputCoder) throws CannotProvideCoderException {            return registry.getCoder(accumulatorType);        }        @Override        public AccT addInput(AccT mutableAccumulator, ValueT input) {            return accumulate.apply(mutableAccumulator, input);        }        @Override        public AccT mergeAccumulators(Iterable<AccT> accumulators) {            AccT accumulated = null;            for (AccT o : accumulators) {                if (accumulated == null) {                    accumulated = o;                } else {                    accumulated = mergeAccumulators.apply(accumulated, o);                }            }            return accumulated;        }        @Override        public OutputT extractOutput(AccT accumulator) {            return outputFn.apply(accumulator);        }    };}
public void beam_f23908_0(ProcessContext ctx)
{    collector.setProcessContext(ctx);    reducer.apply(StreamSupport.stream(requireNonNull(ctx.element().getValue()).spliterator(), false), collector);}
public void beam_f23909_0(T elem)
{    value = elem;}
public Context beam_f23910_0()
{    return this;}
public void beam_f23918_0(ProcessContext ctx)
{    ctx.output(KV.of(ctx.timestamp().getMillis(), ctx.element()));}
public void beam_f23919_0(ProcessContext ctx)
{    ctx.output(ctx.element().getValue());}
public PCollection<OutputT> beam_f23920_0(PCollection<InputT> input)
{    PCollection<KV<Long, InputT>> in;    in = input.apply(getName("wrap"), ParDo.of(new Wrap<>()));    if (input.getTypeDescriptor() != null) {        in = in.setTypeDescriptor(TypeDescriptors.kvs(TypeDescriptors.longs(), input.getTypeDescriptor()));    }    return timestampedTransform.apply(in);}
public static void beam_f23928_0(Stream<T> stream, IOConsumer<T> consumer) throws IOException
{    forEach(stream::iterator, consumer);}
public void beam_f23929_0()
{    final String opName = "split";    final PCollection<String> dataset = TestUtils.createMockDataset(TypeDescriptors.strings());    final Split.Output<String> split = Split.named(opName).of(dataset).using((UnaryPredicate<String>) what -> true).output();    final Filter positive = (Filter) TestUtils.getProducer(split.positive());    assertNotNull(positive.getPredicate());    final Filter negative = (Filter) TestUtils.getProducer(split.negative());    assertNotNull(negative.getPredicate());}
public void beam_f23930_0()
{    final PCollection<String> dataset = TestUtils.createMockDataset(TypeDescriptors.strings());    final Split.Output<String> split = Split.of(dataset).using((UnaryPredicate<String>) what -> true).output();    final Filter positive = (Filter) TestUtils.getProducer(split.positive());    assertTrue(positive.getName().isPresent());    assertEquals(Split.DEFAULT_NAME + Split.POSITIVE_FILTER_SUFFIX, positive.getName().get());    final Filter negative = (Filter) TestUtils.getProducer(split.negative());    assertTrue(negative.getName().isPresent());    assertEquals(Split.DEFAULT_NAME + Split.NEGATIVE_FILTER_SUFFIX, negative.getName().get());}
public void beam_f23938_0()
{    final PCollection<String> dataset = TestUtils.createMockDataset(TypeDescriptors.strings());    final PCollection<String> uniq = Distinct.of(dataset).output();    final Distinct distinct = (Distinct) TestUtils.getProducer(uniq);    assertFalse(distinct.getName().isPresent());}
public void beam_f23939_0()
{    final PCollection<String> dataset = TestUtils.createMockDataset(TypeDescriptors.strings());    final PCollection<String> uniq = Distinct.of(dataset).windowBy(FixedWindows.of(org.joda.time.Duration.standardHours(1))).triggeredBy(DefaultTrigger.of()).accumulationMode(AccumulationMode.DISCARDING_FIRED_PANES).output();    final Distinct distinct = (Distinct) TestUtils.getProducer(uniq);    assertTrue(distinct.getWindow().isPresent());    @SuppressWarnings("unchecked")    final WindowDesc<?> windowDesc = WindowDesc.of((Window) distinct.getWindow().get());    assertEquals(FixedWindows.of(org.joda.time.Duration.standardHours(1)), windowDesc.getWindowFn());    assertEquals(DefaultTrigger.of(), windowDesc.getTrigger());}
public void beam_f23940_0()
{    final PCollection<String> dataset = TestUtils.createMockDataset(TypeDescriptors.strings());    final PCollection<String> uniq = Distinct.of(dataset).applyIf(true, b -> b.windowBy(FixedWindows.of(Duration.standardHours(1))).triggeredBy(DefaultTrigger.of()).discardingFiredPanes()).output();    final Distinct distinct = (Distinct) TestUtils.getProducer(uniq);    assertTrue(distinct.getWindow().isPresent());    @SuppressWarnings("unchecked")    final WindowDesc<?> windowDesc = WindowDesc.of((Window) distinct.getWindow().get());    assertEquals(FixedWindows.of(org.joda.time.Duration.standardHours(1)), windowDesc.getWindowFn());    assertEquals(DefaultTrigger.of(), windowDesc.getTrigger());    assertEquals(AccumulationMode.DISCARDING_FIRED_PANES, windowDesc.getAccumulationMode());}
public void beam_f23948_0()
{    final PCollection<String> dataset = TestUtils.createMockDataset(TypeDescriptors.strings());    final PCollection<String> mapped = FlatMap.of(dataset).using((String s, Collector<String> c) -> c.collect(s)).eventTimeBy(in -> System.currentTimeMillis()).output();    final FlatMap map = (FlatMap) TestUtils.getProducer(mapped);    assertEquals(Long.MAX_VALUE, map.getAllowedTimestampSkew().getMillis());}
public void beam_f23949_0()
{    final Pipeline pipeline = TestUtils.createTestPipeline();    final PCollection<String> left = TestUtils.createMockDataset(pipeline, TypeDescriptors.strings());    final PCollection<String> right = TestUtils.createMockDataset(pipeline, TypeDescriptors.strings());    final PCollection<KV<Integer, String>> joined = Join.named("Join1").of(left, right).by(String::length, String::length).using((String l, String r, Collector<String> c) -> {        }).output();    final Join join = (Join) TestUtils.getProducer(joined);    assertTrue(join.getName().isPresent());    assertEquals("Join1", join.getName().get());    assertNotNull(join.getLeftKeyExtractor());    assertNotNull(join.getRightKeyExtractor());    assertFalse(join.getWindow().isPresent());    assertEquals(Join.Type.INNER, join.getType());}
public void beam_f23950_0()
{    final Pipeline pipeline = TestUtils.createTestPipeline();    final PCollection<String> left = TestUtils.createMockDataset(pipeline, TypeDescriptors.strings());    final PCollection<String> right = TestUtils.createMockDataset(pipeline, TypeDescriptors.strings());    final PCollection<String> joined = Join.named("JoinValues").of(left, right).by(String::length, String::length).using((String l, String r, Collector<String> c) -> {        }).outputValues();    final OutputValues outputValues = (OutputValues) TestUtils.getProducer(joined);    assertTrue(outputValues.getName().isPresent());    assertEquals("JoinValues", outputValues.getName().get());}
public void beam_f23958_0()
{    final Pipeline pipeline = TestUtils.createTestPipeline();    final PCollection<String> left = TestUtils.createMockDataset(pipeline, TypeDescriptors.strings());    final PCollection<String> right = TestUtils.createMockDataset(pipeline, TypeDescriptors.strings());    final TypeDescriptor<Integer> keyType = TypeDescriptors.integers();    final TypeDescriptor<String> outputType = TypeDescriptors.strings();    final PCollection<KV<Integer, String>> joined = Join.named("Join1").of(left, right).by(String::length, String::length, keyType).using((String l, String r, Collector<String> c) -> {        }, outputType).output();    final Join join = (Join) TestUtils.getProducer(joined);    TypePropagationAssert.assertOperatorTypeAwareness(join, keyType, outputType);}
public void beam_f23959_0()
{    final Pipeline pipeline = TestUtils.createTestPipeline();    final PCollection<String> left = TestUtils.createMockDataset(pipeline, TypeDescriptors.strings());    final PCollection<String> right = TestUtils.createMockDataset(pipeline, TypeDescriptors.strings());    TypeDescriptor<Integer> keyType = TypeDescriptors.integers();    TypeDescriptor<String> outputType = TypeDescriptors.strings();    final PCollection<KV<Integer, String>> joined = LeftJoin.named("Join1").of(left, right).by(String::length, String::length, keyType).using((String l, Optional<String> r, Collector<String> c) -> {        }, outputType).output();    final Join join = (Join) TestUtils.getProducer(joined);    TypePropagationAssert.assertOperatorTypeAwareness(join, keyType, outputType);}
public void beam_f23960_0()
{    final Pipeline pipeline = TestUtils.createTestPipeline();    final PCollection<String> left = TestUtils.createMockDataset(pipeline, TypeDescriptors.strings());    final PCollection<String> right = TestUtils.createMockDataset(pipeline, TypeDescriptors.strings());    final TypeDescriptor<Integer> keyType = TypeDescriptors.integers();    final TypeDescriptor<String> outputType = TypeDescriptors.strings();    final PCollection<KV<Integer, String>> joined = RightJoin.named("Join1").of(left, right).by(String::length, String::length, keyType).using((Optional<String> l, String r, Collector<String> c) -> {        }, outputType).output();    final Join join = (Join) TestUtils.getProducer(joined);    TypePropagationAssert.assertOperatorTypeAwareness(join, keyType, outputType);}
public void beam_f23968_0()
{    final PCollection<String> dataset = TestUtils.createMockDataset(TypeDescriptors.strings());    final PCollection<KV<String, Long>> reduced = ReduceByKey.of(dataset).keyBy(s -> s).valueBy(s -> 1L).reduceBy(s -> s.mapToLong(e -> e).sum()).output();    final ReduceByKey reduce = (ReduceByKey) TestUtils.getProducer(reduced);    assertNotNull(reduce.getReducer());    assertFalse(reduce.isCombineFnStyle());}
public void beam_f23969_0()
{    final PCollection<String> dataset = TestUtils.createMockDataset(TypeDescriptors.strings());    final PCollection<KV<String, Long>> reduced = ReduceByKey.of(dataset).keyBy(s -> s).valueBy(s -> 1L).combineBy(s -> s.mapToLong(e -> e).sum()).output();    final ReduceByKey reduce = (ReduceByKey) TestUtils.getProducer(reduced);    assertNotNull(reduce.getReducer());    assertFalse(reduce.isCombineFnStyle());}
public void beam_f23970_0()
{    final PCollection<String> dataset = TestUtils.createMockDataset(TypeDescriptors.strings());    final PCollection<KV<String, Integer>> reduced = ReduceByKey.of(dataset).keyBy(s -> s).valueBy(s -> 1L).combineBy(() -> new ArrayList<>(), (acc, e) -> {        acc.add(e);        return acc;    }, (l, r) -> Lists.newArrayList(Iterables.concat(l, r)), List::size, TypeDescriptors.lists(TypeDescriptors.longs()), TypeDescriptors.integers()).output();    final ReduceByKey reduce = (ReduceByKey) TestUtils.getProducer(reduced);    assertTrue(reduce.isCombineFnStyle());    assertNotNull(reduce.getAccumulatorFactory());    assertNotNull(reduce.getAccumulatorType());    assertNotNull(reduce.getAccumulate());    assertNotNull(reduce.getMergeAccumulators());    assertNotNull(reduce.getOutputFn());    assertTrue(reduce.getOutputType().isPresent());}
public void beam_f23978_0()
{    final PCollection<String> dataset = TestUtils.createMockDataset(TypeDescriptors.strings());    final PCollection<Long> output = ReduceWindow.of(dataset).valueBy(e -> "").reduceBy(e -> 1L).output();    final ReduceWindow rw = (ReduceWindow) TestUtils.getProducer(output);    assertEquals(1L, (long) collectSingle(rw.getReducer(), Stream.of("blah")));    assertEquals("", rw.getValueExtractor().apply("blah"));}
public void beam_f23979_0()
{    final PCollection<String> dataset = TestUtils.createMockDataset(TypeDescriptors.strings());    final PCollection<Long> output = ReduceWindow.of(dataset).reduceBy(e -> 1L).windowBy(FixedWindows.of(org.joda.time.Duration.standardHours(1))).triggeredBy(DefaultTrigger.of()).discardingFiredPanes().withAllowedLateness(Duration.millis(1000)).output();    final ReduceWindow rw = (ReduceWindow) TestUtils.getProducer(output);    assertEquals(1L, (long) collectSingle(rw.getReducer(), Stream.of("blah")));    assertEquals("blah", rw.getValueExtractor().apply("blah"));    assertTrue(rw.getWindow().isPresent());    @SuppressWarnings("unchecked")    final WindowDesc<?> windowDesc = WindowDesc.of((Window) rw.getWindow().get());    assertNotNull(windowDesc);    assertEquals(FixedWindows.of(org.joda.time.Duration.standardHours(1)), windowDesc.getWindowFn());    assertEquals(DefaultTrigger.of(), windowDesc.getTrigger());    assertEquals(Duration.millis(1000), windowDesc.getAllowedLateness());}
public void beam_f23980_0()
{    final PCollection<String> dataset = TestUtils.createMockDataset(TypeDescriptors.strings());    final PCollection<Long> output = ReduceWindow.of(dataset).reduceBy(e -> 1L).withSortedValues(String::compareTo).windowBy(FixedWindows.of(org.joda.time.Duration.standardHours(1))).triggeredBy(DefaultTrigger.of()).accumulationMode(AccumulationMode.DISCARDING_FIRED_PANES).output();    final ReduceWindow rw = (ReduceWindow) TestUtils.getProducer(output);    assertTrue(rw.getValueComparator().isPresent());}
public static TestPipeline beam_f23988_0()
{    final PipelineOptions pipelineOptions = PipelineOptionsFactory.create();    pipelineOptions.as(EuphoriaOptions.class).setTranslatorProvider(new PrimitiveOutputTranslatorProvider());    final TestPipeline testPipeline = TestPipeline.fromOptions(pipelineOptions);    testPipeline.getCoderRegistry().registerCoderForClass(Object.class, KryoCoder.of(pipelineOptions));    return testPipeline;}
public static PCollection<T> beam_f23989_0(TypeDescriptor<T> typeDescriptor)
{    return createMockDataset(createTestPipeline(), typeDescriptor);}
public static PCollection<T> beam_f23990_0(Pipeline pipeline, TypeDescriptor<T> typeDescriptor)
{    return pipeline.apply(Create.empty(typeDescriptor));}
public void beam_f24002_0()
{    final TestPipeline pipeline = TestUtils.createTestPipeline();    final PCollection<String> left = TestUtils.createMockDataset(pipeline, TypeDescriptors.strings());    final PCollection<String> right = TestUtils.createMockDataset(pipeline, TypeDescriptors.strings());    final PCollection<String> unioned = Union.named("Union1").of(left, right).output();    final Union union = (Union) TestUtils.getProducer(unioned);    assertTrue(union.getName().isPresent());    assertEquals("Union1", union.getName().get());}
public void beam_f24003_0()
{    final PCollection<String> first = TestUtils.createMockDataset(TypeDescriptors.strings());    Union.named("Union1").of(first).output();}
public void beam_f24004_0()
{    final TestPipeline pipeline = TestUtils.createTestPipeline();    final PCollection<String> first = TestUtils.createMockDataset(pipeline, TypeDescriptors.strings());    final PCollection<String> second = TestUtils.createMockDataset(pipeline, TypeDescriptors.strings());    final PCollection<String> third = TestUtils.createMockDataset(pipeline, TypeDescriptors.strings());    final PCollection<String> unioned = Union.named("Union1").of(first, second, third).output();    final Union union = (Union) TestUtils.getProducer(unioned);    assertTrue(union.getName().isPresent());    assertEquals("Union1", union.getName().get());}
public void beam_f24012_0()
{    assertEquals(6, (int) apply(Stream.of(1, 2, 3), Sums.ofInts()));}
public void beam_f24013_0()
{    assertEquals(6L, (long) apply(Stream.of(1L, 2L, 3L), Sums.ofLongs()));}
public void beam_f24014_0()
{    assertEquals(6f, (float) apply(Stream.of(1f, 2f, 3f), Sums.ofFloats()), 0.001);}
public void beam_f24022_0()
{    final PipelineOptions options = PipelineOptionsFactory.create();    Pipeline pipeline = Pipeline.create(options);        KryoCoderProvider.of().registerTo(pipeline);        options.as(KryoOptions.class).setKryoRegistrationRequired(true);    KryoCoderProvider.of(kryo -> {                        kryo.register(KryoSerializedElementType.class);    }).registerTo(pipeline);    PCollection<Integer> input = pipeline.apply(Create.of(1, 2, 3, 4)).setTypeDescriptor(TypeDescriptors.integers());    MapElements.named("Int2Str").of(input).using(String::valueOf, TypeDescriptors.strings()).output();}
public void beam_f24023_0()
{    PCollection<Integer> input = pipeline.apply(Create.of(1, 2, 3, 4)).setTypeDescriptor(TypeDescriptors.integers());    PCollection<KV<Integer, Long>> countedElements = CountByKey.of(input).keyBy(e -> e).windowBy(FixedWindows.of(Duration.standardSeconds(1))).triggeredBy(DefaultTrigger.of()).discardingFiredPanes().withAllowedLateness(Duration.standardSeconds(5)).withOnTimeBehavior(OnTimeBehavior.FIRE_IF_NON_EMPTY).withTimestampCombiner(TimestampCombiner.EARLIEST).output();    pipeline.run();}
public void beam_f24024_0()
{    PCollection<Integer> input = pipeline.apply(Create.of(1, 2, 4, 1, 1, 3));        PCollection<KV<Integer, Long>> output = CountByKey.of(input).keyBy(e -> e).output();        PAssert.that(output).containsInAnyOrder(asList(KV.of(1, 3L), KV.of(2, 1L), KV.of(3, 1L), KV.of(4, 1L)));    pipeline.run();}
public void beam_f24032_0()
{    PCollection<SomeEventObject> events = pipeline.apply(Create.of(new SomeEventObject(0), new SomeEventObject(1), new SomeEventObject(2), new SomeEventObject(3), new SomeEventObject(4)));            PCollection<SomeEventObject> timeStampedEvents = FlatMap.named("extract-event-time").of(events).using((SomeEventObject e, Collector<SomeEventObject> c) -> c.collect(e)).eventTimeBy(SomeEventObject::getEventTimeInMillis).output();        pipeline.run();}
public void beam_f24033_0()
{    PCollection<Integer> nums = pipeline.apply(Create.of(asList(0, 1, 2, 3, 4, 5, 6, 7, 8, 9)));        PCollection<Integer> divisibleBythree = Filter.named("divisibleByFive").of(nums).by(e -> e % 3 == 0).output();        PAssert.that(divisibleBythree).containsInAnyOrder(0, 3, 6, 9);    pipeline.run();}
public void beam_f24034_0()
{    PCollection<String> animals = pipeline.apply(Create.of("mouse", "rat", "elephant", "cat", "X", "duck"));        PCollection<KV<Integer, Long>> countOfAnimalNamesByLength = ReduceByKey.named("to-letters-couts").of(animals).keyBy(    String::length).valueBy(e -> 1).reduceBy(Stream::count).output();            PAssert.that(countOfAnimalNamesByLength).containsInAnyOrder(asList(KV.of(1, 1L), KV.of(3, 2L), KV.of(4, 1L), KV.of(5, 1L), KV.of(8, 1L)));    pipeline.run();}
 long beam_f24042_0()
{    return timestamp;}
public void beam_f24043_0()
{    PCollection<Integer> input = pipeline.apply(Create.of(asList(1, 2, 3, 4, 5, 6, 7, 8)));            PCollection<Integer> withEventTime = AssignEventTime.of(input).using(i -> 1000L * i).output();    PCollection<Integer> output = ReduceWindow.of(withEventTime).combineBy(Fold.of((i1, i2) -> i1 + i2)).windowBy(FixedWindows.of(Duration.millis(5000))).triggeredBy(DefaultTrigger.of()).discardingFiredPanes().output();        PAssert.that(output).containsInAnyOrder(10, 26);    pipeline.run();}
public void beam_f24044_0()
{    PCollection<String> animals = pipeline.apply(Create.of("mouse", "elk", "rat", "mule", "elephant", "dinosaur", "cat", "duck", "caterpillar"));            PCollection<Triple<Character, String, Integer>> longestNamesByLetter = TopPerKey.named("longest-animal-names").of(animals).keyBy(    name -> name.charAt(0)).valueBy(    UnaryFunction.identity()).scoreBy(    String::length).output();            PAssert.that(longestNamesByLetter).containsInAnyOrder(Triple.of('m', "mouse", 5), Triple.of('r', "rat", 3), Triple.of('e', "elephant", 8), Triple.of('d', "dinosaur", 8), Triple.of('c', "caterpillar", 11));    pipeline.run();}
public final PCollection<OutputT> beam_f24053_0(Pipeline pipeline)
{    final List<InputT> inputData = getInput();    final PCollection<InputT> inputDataset = pipeline.apply("input", Create.of(inputData)).setTypeDescriptor(getInputType());    return getOutput(inputDataset);}
public void beam_f24054_0(long value)
{    this.value.addAndGet(value);}
public void beam_f24055_0()
{    increment(1);}
public Map<Duration, Long> beam_f24063_0()
{    Map<Duration, Long> m = Maps.newHashMapWithExpectedSize(hist.buckets.size());    hist.buckets.forEach((key, count) -> m.put(Duration.ofNanos(key), count));    return m;}
private static T beam_f24064_0(String name, Class<T> expectedType, Accumulator actualAcc)
{    if (actualAcc.getClass() != expectedType) {                throw new IllegalStateException("Ambiguously named accumulators! Got " + actualAcc.getClass() + " for " + name + " but expected " + expectedType + "!");    }    return (T) actualAcc;}
public Counter beam_f24065_0(String name)
{    return assertType(name, LongCounter.class, accs.computeIfAbsent(name, s -> new LongCounter()));}
public void beam_f24073_0()
{    providerInstance().clear();}
public Map<String, Long> beam_f24074_0()
{    return providerInstance().getSnapshots(LongCounter.class);}
public Map<String, Map<Long, Long>> beam_f24075_0()
{    return providerInstance().getSnapshots(LongHistogram.class);}
protected List<Integer> beam_f24083_0()
{    return Arrays.asList(1, 2, 3, 0, 4, 3, 2, 1);}
protected TypeDescriptor<Integer> beam_f24084_0()
{    return TypeDescriptors.integers();}
protected TypeDescriptor<Long> beam_f24085_0()
{    return TypeDescriptors.longs();}
protected TypeDescriptor<Long> beam_f24093_0()
{    return TypeDescriptors.longs();}
public List<KV<Integer, String>> beam_f24094_0()
{    return Arrays.asList(KV.of(2, "2+12"), KV.of(2, "2+12"), KV.of(4, "4+14"), KV.of(1, "1+11"), KV.of(1, "1+11"), KV.of(3, "3+13"), KV.of(3, "3+13"), KV.of(5, "null+15"));}
public void beam_f24095_0()
{    final String sameHashCodeKey1 = "FB";    final String sameHashCodeKey2 = "Ea";    execute(new TestCase<String, Integer, KV<String, String>>() {        @Override        protected PCollection<KV<String, String>> getOutput(PCollection<String> left, PCollection<Integer> right) {            return LeftJoin.named("Broadcast-leftJoin").of(left, MapElements.of(right).using(i -> i).output()).by(e -> e, e -> e % 2 == 0 ? sameHashCodeKey2 : sameHashCodeKey1).using((String l, Optional<Integer> r, Collector<String> c) -> c.collect(l + "+" + r.orElse(null))).output();        }        @Override        protected List<String> getLeftInput() {            return Arrays.asList(sameHashCodeKey1, sameHashCodeKey2, "keyWithoutRightSide");        }        @Override        protected TypeDescriptor<String> getLeftInputType() {            return TypeDescriptors.strings();        }        @Override        protected List<Integer> getRightInput() {            return Arrays.asList(1, 2);        }        @Override        protected TypeDescriptor<Integer> getRightInputType() {            return TypeDescriptors.integers();        }        @Override        public List<KV<String, String>> getUnorderedOutput() {            return Arrays.asList(KV.of(sameHashCodeKey1, "FB+1"), KV.of(sameHashCodeKey2, "Ea+2"), KV.of("keyWithoutRightSide", "keyWithoutRightSide+null"));        }    });}
protected PCollection<KV<Integer, Long>> beam_f24103_0(PCollection<Integer> input)
{        input = AssignEventTime.of(input).using(e -> 0).output();    return CountByKey.of(input).keyBy(e -> e).windowBy(FixedWindows.of(Duration.standardSeconds(1))).triggeredBy(DefaultTrigger.of()).discardingFiredPanes().output();}
protected List<Integer> beam_f24104_0()
{    return Arrays.asList(1, 2, 3, 4, 5, 6, 7, 10, 9, 8, 7, 6, 5, 4);}
protected TypeDescriptor<Integer> beam_f24105_0()
{    return TypeDescriptors.integers();}
public List<Integer> beam_f24113_0()
{    return Arrays.asList(1, 2, 3);}
protected PCollection<Integer> beam_f24114_0(PCollection<Integer> input)
{    return Distinct.of(input).output();}
protected List<Integer> beam_f24115_0()
{    return Arrays.asList(1, 2, 3, 3, 2, 1);}
public List<Integer> beam_f24123_0()
{    return Arrays.asList(2, 1, 3);}
protected PCollection<Integer> beam_f24124_0(PCollection<KV<Integer, Long>> input)
{    input = AssignEventTime.of(input).using(KV::getValue).output();    PCollection<KV<Integer, Long>> distinct = Distinct.of(input).projected(KV::getKey).windowBy(FixedWindows.of(org.joda.time.Duration.standardSeconds(1))).triggeredBy(DefaultTrigger.of()).discardingFiredPanes().output();    return MapElements.of(distinct).using(KV::getKey).output();}
protected List<KV<Integer, Long>> beam_f24125_0()
{    List<KV<Integer, Long>> first = asTimedList(100, 1, 2, 3, 3, 2, 1);    first.addAll(asTimedList(100, 1, 2, 3, 3, 2, 1));    return first;}
public List<String> beam_f24133_0()
{    return Arrays.asList("2.", "1.", "3.");}
protected PCollection<String> beam_f24134_0(PCollection<KV<String, Long>> input)
{    input = AssignEventTime.of(input).using(KV::getValue).output();    PCollection<KV<String, Long>> distinct = Distinct.of(input).projected(in -> in.getKey().substring(0, 1), Distinct.SelectionPolicy.NEWEST, TypeDescriptors.strings()).windowBy(FixedWindows.of(org.joda.time.Duration.standardSeconds(1))).triggeredBy(DefaultTrigger.of()).discardingFiredPanes().output();    return MapElements.of(distinct).using(KV::getKey).output();}
protected List<KV<String, Long>> beam_f24135_0()
{    return asTimedList(100, "1", "2", "3", "3.", "2.", "1.");}
public void beam_f24143_0()
{    execute(new AbstractTestCase<Integer, Integer>() {        @Override        protected PCollection<Integer> getOutput(PCollection<Integer> input) {            return FlatMap.of(input).using((Integer e, Collector<Integer> c) -> {                for (int i = 1; i <= e; i++) {                    c.collect(i);                }            }).output();        }        @Override        protected List<Integer> getInput() {            return Arrays.asList(1, 2, 3, 4, 3, 2, 1);        }        @Override        protected TypeDescriptor<Integer> getInputType() {            return TypeDescriptors.integers();        }        @Override        public List<Integer> getUnorderedOutput() {            return Arrays.asList(1, 1, 2, 1, 2, 3, 1, 2, 3, 4, 1, 2, 3, 1, 2, 1);        }    });}
protected PCollection<Integer> beam_f24144_0(PCollection<Integer> input)
{    return FlatMap.of(input).using((Integer e, Collector<Integer> c) -> {        for (int i = 1; i <= e; i++) {            c.collect(i);        }    }).output();}
protected List<Integer> beam_f24145_0()
{    return Arrays.asList(1, 2, 3, 4, 3, 2, 1);}
public void beam_f24153_0(SnapshotProvider snapshots)
{    Map<String, Long> counters = snapshots.getCounterSnapshots();    assertEquals(Long.valueOf(9L), counters.get("input"));    assertEquals(Long.valueOf(51L), counters.get("sum"));}
public void beam_f24154_0()
{    execute(new JoinTestCase<Integer, Long, KV<Integer, String>>() {        @Override        protected PCollection<KV<Integer, String>> getOutput(PCollection<Integer> left, PCollection<Long> right) {            return FullJoin.of(left, right).by(e -> e, e -> (int) (e % 10)).using((Optional<Integer> l, Optional<Long> r, Collector<String> c) -> c.collect(l.orElse(null) + "+" + r.orElse(null))).output();        }        @Override        protected List<Integer> getLeftInput() {            return Arrays.asList(1, 2, 3, 0, 4, 3, 2, 1);        }        @Override        protected TypeDescriptor<Integer> getLeftInputType() {            return TypeDescriptors.integers();        }        @Override        protected List<Long> getRightInput() {            return Arrays.asList(11L, 12L, 13L, 14L, 15L);        }        @Override        protected TypeDescriptor<Long> getRightInputType() {            return TypeDescriptors.longs();        }        @Override        public List<KV<Integer, String>> getUnorderedOutput() {            return Arrays.asList(KV.of(0, "0+null"), KV.of(2, "2+12"), KV.of(2, "2+12"), KV.of(4, "4+14"), KV.of(1, "1+11"), KV.of(1, "1+11"), KV.of(3, "3+13"), KV.of(3, "3+13"), KV.of(5, "null+15"));        }    });}
protected PCollection<KV<Integer, String>> beam_f24155_0(PCollection<Integer> left, PCollection<Long> right)
{    return FullJoin.of(left, right).by(e -> e, e -> (int) (e % 10)).using((Optional<Integer> l, Optional<Long> r, Collector<String> c) -> c.collect(l.orElse(null) + "+" + r.orElse(null))).output();}
protected List<Integer> beam_f24163_0()
{    return Arrays.asList(1, 2, 3, 0, 4, 3, 1);}
protected TypeDescriptor<Integer> beam_f24164_0()
{    return TypeDescriptors.integers();}
protected List<String> beam_f24165_0()
{    return Arrays.asList("mouse", "rat", "cat", "X", "duck");}
protected TypeDescriptor<Long> beam_f24173_0()
{    return TypeDescriptors.longs();}
public List<String> beam_f24174_0()
{    return Arrays.asList("0+null", "2+12", "2+12", "4+14", "1+11", "1+11", "3+13", "3+13", "null+15");}
public void beam_f24175_0()
{    execute(new JoinTestCase<Integer, Long, KV<Integer, String>>() {        @Override        protected PCollection<KV<Integer, String>> getOutput(PCollection<Integer> left, PCollection<Long> right) {            return LeftJoin.of(left, right).by(e -> e, e -> (int) (e % 10)).using((Integer l, Optional<Long> r, Collector<String> c) -> c.collect(l + "+" + r.orElse(null))).output();        }        @Override        protected List<Integer> getLeftInput() {            return Arrays.asList(1, 2, 3, 0, 4, 3, 2, 1);        }        @Override        protected TypeDescriptor<Integer> getLeftInputType() {            return TypeDescriptors.integers();        }        @Override        protected List<Long> getRightInput() {            return Arrays.asList(11L, 12L, 13L, 14L, 15L);        }        @Override        protected TypeDescriptor<Long> getRightInputType() {            return TypeDescriptors.longs();        }        @Override        public List<KV<Integer, String>> getUnorderedOutput() {            return Arrays.asList(KV.of(0, "0+null"), KV.of(2, "2+12"), KV.of(2, "2+12"), KV.of(4, "4+14"), KV.of(1, "1+11"), KV.of(1, "1+11"), KV.of(3, "3+13"), KV.of(3, "3+13"));        }    });}
protected PCollection<KV<Integer, String>> beam_f24183_0(PCollection<Integer> left, PCollection<Long> right)
{    return RightJoin.of(left, right).by(e -> e, e -> (int) (e % 10)).using((Optional<Integer> l, Long r, Collector<String> c) -> c.collect(l.orElse(null) + "+" + r)).output();}
protected List<Integer> beam_f24184_0()
{    return Arrays.asList(1, 2, 3, 0, 4, 3, 2, 1);}
protected TypeDescriptor<Integer> beam_f24185_0()
{    return TypeDescriptors.integers();}
protected List<Long> beam_f24193_0()
{    return Arrays.asList(11L, 12L, 13L, 14L, 15L);}
protected TypeDescriptor<Long> beam_f24194_0()
{    return TypeDescriptors.longs();}
public List<KV<Integer, String>> beam_f24195_0()
{    return Arrays.asList(KV.of(2, "2+12"), KV.of(2, "2+12"), KV.of(4, "4+14"), KV.of(1, "1+11"), KV.of(1, "1+11"), KV.of(3, "3+13"), KV.of(3, "3+13"));}
public void beam_f24203_0()
{    execute(new JoinTestCase<Integer, Long, KV<Integer, String>>() {        @Override        protected PCollection<KV<Integer, String>> getOutput(PCollection<Integer> left, PCollection<Long> right) {            @SuppressWarnings("unchecked")            final WindowFn<Object, BoundedWindow> evenOddWindowFn = (WindowFn) new EvenOddWindowFn();            return LeftJoin.of(left, right).by(e -> e, e -> (int) (e % 10)).using((Integer l, Optional<Long> r, Collector<String> c) -> {                c.collect(l + "+" + r.orElse(null));            }).windowBy(evenOddWindowFn).triggeredBy(AfterWatermark.pastEndOfWindow()).discardingFiredPanes().withAllowedLateness(Duration.ZERO).output();        }        @Override        protected List<Integer> getLeftInput() {            return Arrays.asList(1, 2, 3, 0, 4, 3, 2, 1, 5, 6);        }        @Override        protected TypeDescriptor<Integer> getLeftInputType() {            return TypeDescriptors.integers();        }        @Override        protected List<Long> getRightInput() {            return Arrays.asList(11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L);        }        @Override        protected TypeDescriptor<Long> getRightInputType() {            return TypeDescriptors.longs();        }        @Override        public List<KV<Integer, String>> getUnorderedOutput() {            return Arrays.asList(KV.of(0, "0+null"), KV.of(2, "2+12"), KV.of(2, "2+12"), KV.of(4, "4+14"), KV.of(6, "6+16"), KV.of(1, "1+null"), KV.of(1, "1+null"), KV.of(3, "3+null"), KV.of(3, "3+null"), KV.of(5, "5+null"));        }    });}
protected PCollection<KV<Integer, String>> beam_f24204_0(PCollection<Integer> left, PCollection<Long> right)
{    @SuppressWarnings("unchecked")    final WindowFn<Object, BoundedWindow> evenOddWindowFn = (WindowFn) new EvenOddWindowFn();    return LeftJoin.of(left, right).by(e -> e, e -> (int) (e % 10)).using((Integer l, Optional<Long> r, Collector<String> c) -> {        c.collect(l + "+" + r.orElse(null));    }).windowBy(evenOddWindowFn).triggeredBy(AfterWatermark.pastEndOfWindow()).discardingFiredPanes().withAllowedLateness(Duration.ZERO).output();}
protected List<Integer> beam_f24205_0()
{    return Arrays.asList(1, 2, 3, 0, 4, 3, 2, 1, 5, 6);}
protected TypeDescriptor<Integer> beam_f24213_0()
{    return TypeDescriptors.integers();}
protected List<Long> beam_f24214_0()
{    return Arrays.asList(11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L);}
protected TypeDescriptor<Long> beam_f24215_0()
{    return TypeDescriptors.longs();}
public List<KV<String, String>> beam_f24223_0()
{    return Arrays.asList(KV.of("fi", "ha"), KV.of("fi", "ho"), KV.of("fa", "ha"), KV.of("fa", "ho"));}
public void beam_f24224_0()
{    execute(new JoinTestCase<Integer, Long, KV<Integer, String>>() {        @Override        protected PCollection<KV<Integer, String>> getOutput(PCollection<Integer> left, PCollection<Long> right) {            return FullJoin.of(left, right).by(e -> e, e -> (int) (e % 10)).using((Optional<Integer> l, Optional<Long> r, Collector<String> c) -> {                                c.collect(l.orElse(null) + "+" + r.orElse(null));                c.collect(l.orElse(null) + "+" + r.orElse(null));            }).output();        }        @Override        protected List<Integer> getLeftInput() {            return Arrays.asList(0, 1, 2, 3);        }        @Override        protected TypeDescriptor<Integer> getLeftInputType() {            return TypeDescriptors.integers();        }        @Override        protected List<Long> getRightInput() {            return Arrays.asList(11L, 12L, 13L, 14L);        }        @Override        protected TypeDescriptor<Long> getRightInputType() {            return TypeDescriptors.longs();        }        @Override        public List<KV<Integer, String>> getUnorderedOutput() {            return Arrays.asList(KV.of(0, "0+null"), KV.of(0, "0+null"), KV.of(1, "1+11"), KV.of(1, "1+11"), KV.of(2, "2+12"), KV.of(2, "2+12"), KV.of(3, "3+13"), KV.of(3, "3+13"), KV.of(4, "null+14"), KV.of(4, "null+14"));        }    });}
protected PCollection<KV<Integer, String>> beam_f24225_0(PCollection<Integer> left, PCollection<Long> right)
{    return FullJoin.of(left, right).by(e -> e, e -> (int) (e % 10)).using((Optional<Integer> l, Optional<Long> r, Collector<String> c) -> {                c.collect(l.orElse(null) + "+" + r.orElse(null));        c.collect(l.orElse(null) + "+" + r.orElse(null));    }).output();}
public boolean beam_f24234_0(WindowFn<?, ?> other)
{    return other instanceof EvenOddWindowFn;}
public Coder<BoundedWindow> beam_f24235_0()
{    return KryoCoder.of(PipelineOptionsFactory.create());}
public WindowMappingFn<BoundedWindow> beam_f24236_0()
{    return null;}
protected TypeDescriptor<Integer> beam_f24244_0()
{    return TypeDescriptors.integers();}
public List<String> beam_f24245_0()
{    return Arrays.asList("1", "2", "3", "4", "5", "6", "7");}
public void beam_f24246_0()
{    execute(new AbstractTestCase<Integer, Integer>() {        @Override        protected PCollection<Integer> getOutput(PCollection<Integer> input) {            return MapElements.named("test").of(input).using((UnaryFunctionEnv<Integer, Integer>) (x, context) -> {                context.getHistogram("dist").add(x, 1);                return x;            }).output();        }        @Override        protected List<Integer> getInput() {            return Arrays.asList(1, 2, 3, 1, 2, 2, 10, 20, 10);        }        @Override        protected TypeDescriptor<Integer> getInputType() {            return TypeDescriptors.integers();        }        @Override        public List<Integer> getUnorderedOutput() {            return Arrays.asList(1, 2, 3, 1, 2, 2, 10, 20, 10);        }        @Override        public void validateAccumulators(SnapshotProvider snapshots) {            Map<Long, Long> hists = snapshots.getHistogramSnapshots().get("dist");            assertEquals(5, hists.size());            assertEquals(Long.valueOf(2), hists.get(1L));            assertEquals(Long.valueOf(3), hists.get(2L));            assertEquals(Long.valueOf(1), hists.get(3L));            assertEquals(Long.valueOf(2), hists.get(10L));            assertEquals(Long.valueOf(1), hists.get(20L));        }    });}
protected TypeDescriptor<Integer> beam_f24254_0()
{    return TypeDescriptors.integers();}
protected PCollection<KV<Integer, Set<Integer>>> beam_f24255_0(PCollection<Integer> input)
{    return ReduceByKey.of(input).keyBy(e -> e % 2).valueBy(e -> e).reduceBy(s -> s.collect(Collectors.toSet())).windowBy(new GlobalWindows()).triggeredBy(AfterWatermark.pastEndOfWindow()).discardingFiredPanes().output();}
public List<KV<Integer, Set<Integer>>> beam_f24256_0()
{    return Arrays.asList(KV.of(0, Sets.newHashSet(2, 4, 6)), KV.of(1, Sets.newHashSet(1, 3, 5, 7, 9)));}
protected List<KV<Integer, Long>> beam_f24264_0()
{    return Arrays.asList(KV.of(1, 300L), KV.of(2, 600L), KV.of(3, 900L), KV.of(2, 1300L), KV.of(3, 1600L), KV.of(1, 1900L), KV.of(3, 2300L), KV.of(2, 2600L), KV.of(1, 2900L), KV.of(2, 3300L), KV.of(2, 300L), KV.of(4, 600L), KV.of(3, 900L), KV.of(4, 1300L), KV.of(2, 1600L), KV.of(3, 1900L), KV.of(4, 2300L), KV.of(1, 2600L), KV.of(3, 2900L), KV.of(4, 3300L), KV.of(3, 3600L));}
protected TypeDescriptor<KV<Integer, Long>> beam_f24265_0()
{    return TypeDescriptors.kvs(TypeDescriptors.integers(), TypeDescriptors.longs());}
public List<KV<Integer, Long>> beam_f24266_0()
{    return Arrays.asList(KV.of(2, 2L),     KV.of(4, 1L), KV.of(2, 2L),     KV.of(4, 1L), KV.of(2, 1L),     KV.of(4, 1L), KV.of(2, 1L),     KV.of(4, 1L), KV.of(1, 1L),     KV.of(3, 2L), KV.of(1, 1L),     KV.of(3, 2L), KV.of(1, 2L),     KV.of(3, 2L),     KV.of(3, 1L));}
protected TypeDescriptor<String> beam_f24274_0()
{    return TypeDescriptors.strings();}
public List<KV<String, Long>> beam_f24275_0()
{    return Arrays.asList(KV.of("one", 5L), KV.of("two", 4L), KV.of("three", 3L), KV.of("four", 2L));}
protected PCollection<KV<String, Long>> beam_f24276_0(PCollection<String> input)
{    return ReduceByKey.of(input).keyBy(e -> e, TypeDescriptor.of(String.class)).valueBy(e -> 1L, TypeDescriptor.of(Long.class)).combineBy(Sums.ofLongs()).output();}
protected TypeDescriptor<KV<String, Long>> beam_f24284_0()
{    return TypeDescriptors.kvs(TypeDescriptors.strings(), TypeDescriptors.longs());}
protected PCollection<KV<String, Long>> beam_f24285_0(PCollection<KV<String, Long>> input)
{    return ReduceByKey.of(input).keyBy(KV::getKey).valueBy(KV::getValue).combineBy(Sums.ofLongs()).windowBy(new MergingByBucketSizeWindowFn<>(3)).triggeredBy(AfterWatermark.pastEndOfWindow()).discardingFiredPanes().withAllowedLateness(Duration.ZERO).output();}
public List<KV<String, Long>> beam_f24286_0()
{    return Arrays.asList(KV.of("a", 880L), KV.of("a", 104_020L), KV.of("b", 1_110L), KV.of("b", 50_000L), KV.of("c", 3_300L));}
protected TypeDescriptor<Integer> beam_f24294_0()
{    return TypeDescriptors.integers();}
protected PCollection<KV<Integer, Integer>> beam_f24295_0(PCollection<Integer> input)
{    return ReduceByKey.named("test").of(input).keyBy(e -> e % 2).valueBy(e -> e).reduceBy(Fold.of(0, (Integer a, Integer b, Collector<Integer> ctx) -> {        if (b % 2 == 0) {            ctx.getCounter("evens").increment();        } else {            ctx.getCounter("odds").increment();        }        ctx.collect(a + b);    })).windowBy(new GlobalWindows()).triggeredBy(AfterWatermark.pastEndOfWindow()).discardingFiredPanes().output();}
public List<KV<Integer, Integer>> beam_f24296_0()
{    return Arrays.asList(KV.of(1, 9), KV.of(0, 6));}
public boolean beam_f24305_0()
{    return true;}
public boolean beam_f24306_0(WindowFn<?, ?> other)
{    return false;}
public Coder<CountWindow> beam_f24307_0()
{    return KryoCoder.of(PipelineOptionsFactory.create());}
public String beam_f24315_0()
{    return "UniqueWindow{id=" + id + "}";}
public Collection<UniqueWindow> beam_f24316_0(AssignContext c) throws Exception
{    return Collections.singleton(new UniqueWindow());}
public void beam_f24317_0(MergeContext c) throws Exception
{        Collection<UniqueWindow> windows = c.windows();    List<UniqueWindow> merges = new ArrayList<>();    for (UniqueWindow w : windows) {        merges.add(w);        if (merges.size() == bucketSize) {                        c.merge(merges, w);            merges.clear();        }    }    if (merges.size() > 1) {        c.merge(merges, merges.get(merges.size() - 1));    }}
protected PCollection<Integer> beam_f24325_0(PCollection<Integer> input)
{    PCollection<Integer> withEventTime = AssignEventTime.of(input).using(i -> 1000L * i).output();    return ReduceWindow.of(withEventTime).combineBy(Sums.ofInts()).windowBy(FixedWindows.of(org.joda.time.Duration.standardHours(1))).triggeredBy(DefaultTrigger.of()).discardingFiredPanes().output();}
protected List<Integer> beam_f24326_0()
{    return Arrays.asList(1, 2, 3, 4, 5, 6, 7, 8, 9, 10);}
protected TypeDescriptor<Integer> beam_f24327_0()
{    return TypeDescriptors.integers();}
protected PCollection<Integer> beam_f24335_0(PCollection<Integer> input)
{    PCollection<Integer> withEventTime = AssignEventTime.of(input).using(i -> 1000L * i).output();    PCollection<Integer> first = ReduceWindow.named("first-reduce").of(withEventTime).combineBy(Sums.ofInts()).windowBy(FixedWindows.of(org.joda.time.Duration.standardSeconds(5))).triggeredBy(DefaultTrigger.of()).discardingFiredPanes().output();    return ReduceWindow.named("second-reduce").of(first).combineBy(Sums.ofInts()).output();}
protected List<Integer> beam_f24336_0()
{    return Arrays.asList(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 100);}
protected TypeDescriptor<Integer> beam_f24337_0()
{    return TypeDescriptors.integers();}
protected PCollection<Triple<String, String, Integer>> beam_f24345_0(PCollection<Item> input)
{    final PCollection<Item> timestampedElements = AssignEventTime.of(input).using(Item::getTimestamp).output();    return TopPerKey.of(timestampedElements).keyBy(Item::getKey).valueBy(Item::getValue).scoreBy(Item::getScore).windowBy(FixedWindows.of(Duration.millis(10))).triggeredBy(DefaultTrigger.of()).discardingFiredPanes().withAllowedLateness(Duration.ZERO).output();}
public List<Triple<String, String, Integer>> beam_f24346_0()
{    return asList(Triple.of("one", "one-999", 999), Triple.of("two", "two", 10), Triple.of("three", "3-three", 2));}
protected List<Item> beam_f24347_0()
{    return asList(new Item("one", "one-ZZZ-1", 1, 0L), new Item("one", "one-ZZZ-2", 2, 1L), new Item("one", "one-3", 3, 2L), new Item("one", "one-999", 999, 3L), new Item("two", "two", 10, 4L), new Item("three", "1-three", 1, 5L), new Item("three", "2-three", 0, 6L), new Item("one", "one-XXX-100", 100, 7L), new Item("three", "3-three", 2, 8L));}
 String beam_f24355_0()
{    return value;}
 int beam_f24356_0()
{    return score;}
 long beam_f24357_0()
{    return timestamp;}
public PCollection<Integer> beam_f24365_0(Pipeline pipeline)
{    final PCollection<Integer> first = createDataset(pipeline, 1, 2, 3, 4, 5, 6);    final PCollection<Integer> second = createDataset(pipeline, 7, 8, 9, 10, 11, 12);    final PCollection<Integer> third = createDataset(pipeline, 13, 14, 15, 16, 17, 18);    return Union.of(first, second, third).output();}
public List<Integer> beam_f24366_0()
{    return Arrays.asList(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18);}
public void beam_f24367_0()
{    execute(new TestCase<Integer>() {        @Override        public PCollection<Integer> getOutput(Pipeline pipeline) {            final PCollection<Integer> first = createDataset(pipeline, 1, 2, 3);            final PCollection<Integer> second = createDataset(pipeline, 4, 5, 6);            final PCollection<Integer> third = createDataset(pipeline, 7, 8, 9);            final PCollection<Integer> fourth = createDataset(pipeline, 10, 11, 12);            final PCollection<Integer> fifth = createDataset(pipeline, 13, 14, 15);            return Union.of(first, second, third, fourth, fifth).output();        }        @Override        public List<Integer> getUnorderedOutput() {            return Arrays.asList(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15);        }    });}
public void beam_f24375_0()
{    BroadcastHashJoinTranslator<?, ?, ?, ?> translatorUnderTest = new BroadcastHashJoinTranslator<>();    EuphoriaOptions options = p.getOptions().as(EuphoriaOptions.class);        options.setTranslatorProvider(CompositeProvider.of(GenericTranslatorProvider.newBuilder().register(Join.class, (op) -> true, translatorUnderTest).build(), GenericTranslatorProvider.createWithDefaultTranslators()));        PCollection<KV<Integer, String>> lengthStrings = p.apply("names", Create.of(KV.of(1, "one"), KV.of(2, "two"), KV.of(3, "three"))).setTypeDescriptor(TypeDescriptors.kvs(TypeDescriptors.integers(), TypeDescriptors.strings()));    UnaryFunction<KV<Integer, String>, Integer> sharedKeyExtractor = KV::getKey;        PCollection<String> letters = p.apply("letters", Create.of("a", "b", "c", "d")).setTypeDescriptor(TypeDescriptors.strings());    PCollection<String> acronyms = p.apply("acronyms", Create.of("B2K", "DIY", "FKA", "EOBD")).setTypeDescriptor(TypeDescriptors.strings());    PCollection<KV<Integer, String>> lettersJoined = LeftJoin.named("join-letters-with-lengths").of(letters, lengthStrings).by(String::length, sharedKeyExtractor, TypeDescriptors.integers()).using((letter, maybeLength, ctx) -> ctx.collect(letter + "-" + maybeLength.orElse(KV.of(-1, "null")).getValue()), TypeDescriptors.strings()).output();    PCollection<KV<Integer, String>> acronymsJoined = RightJoin.named("join-acronyms-with-lengths").of(lengthStrings, acronyms).by(sharedKeyExtractor, String::length, TypeDescriptors.integers()).using((maybeLength, acronym, ctx) -> ctx.collect(maybeLength.orElse(KV.of(-1, "null")).getValue() + "-" + acronym), TypeDescriptors.strings()).output();    PAssert.that(lettersJoined).containsInAnyOrder(KV.of(1, "a-one"), KV.of(1, "b-one"), KV.of(1, "c-one"), KV.of(1, "d-one"));    PAssert.that(acronymsJoined).containsInAnyOrder(KV.of(3, "three-B2K"), KV.of(3, "three-DIY"), KV.of(3, "three-FKA"), KV.of(4, "null-EOBD"));    p.run();    Assert.assertEquals(1, translatorUnderTest.pViews.size());}
public void beam_f24376_0()
{    final AccumulatorProvider accumulators = accumulatorFactory.create();    SingleValueCollector collector = new SingleValueCollector(accumulators, "test-no_op_name");    Counter counter = collector.getCounter(TEST_COUNTER_NAME);    Assert.assertNotNull(counter);    Histogram histogram = collector.getHistogram(TEST_HISTOGRAM_NAME);    Assert.assertNotNull(histogram);}
public void beam_f24377_0()
{    final AccumulatorProvider accumulators = accumulatorFactory.create();    SingleValueCollector collector = new SingleValueCollector(accumulators, "test-no_op_name");    Counter counter = collector.getCounter(TEST_COUNTER_NAME);    Assert.assertNotNull(counter);    counter.increment();    counter.increment(2);    Map<String, Long> counterSnapshots = accumulatorFactory.getCounterSnapshots();    long counteValue = counterSnapshots.get(TEST_COUNTER_NAME);    Assert.assertEquals(3L, counteValue);    Histogram histogram = collector.getHistogram(TEST_HISTOGRAM_NAME);    Assert.assertNotNull(histogram);    histogram.add(1);    histogram.add(2, 2);    Map<String, Map<Long, Long>> histogramSnapshots = accumulatorFactory.getHistogramSnapshots();    Map<Long, Long> histogramValue = histogramSnapshots.get(TEST_HISTOGRAM_NAME);    long numOfValuesOfOne = histogramValue.get(1L);    Assert.assertEquals(1L, numOfValuesOfOne);    long numOfValuesOfTwo = histogramValue.get(2L);    Assert.assertEquals(2L, numOfValuesOfTwo);}
public void beam_f24385_0()
{    GenericTranslatorProvider builded = GenericTranslatorProvider.newBuilder().build();    Assert.assertNotNull(builded);}
public void beam_f24386_0()
{    String translatorName = "translator";    GenericTranslatorProvider provider = GenericTranslatorProvider.newBuilder().register(TestOperator.class, TestOpTranslator.ofName(translatorName)).build();    Optional<OperatorTranslator<Void, Void, TestOperator>> maybeTranslator = provider.findTranslator(TestOperator.of());    ProviderTestUtils.assertTranslator(translatorName, maybeTranslator, TestOpTranslator.class);}
public void beam_f24387_0()
{    String translatorName = "translator";    GenericTranslatorProvider provider = GenericTranslatorProvider.newBuilder().register(TestOperator.class, op -> true, TestOpTranslator.ofName(translatorName)).build();    Optional<OperatorTranslator<Void, Void, TestOperator>> maybeTranslator = provider.findTranslator(TestOperator.of());    ProviderTestUtils.assertTranslator(translatorName, maybeTranslator, TestOpTranslator.class);}
public String beam_f24395_0()
{    return name;}
 static AnyOpTranslator beam_f24396_0(String name)
{    return new AnyOpTranslator(name, true);}
 static AnyOpTranslator beam_f24397_0(String name, boolean canTranslate)
{    return new AnyOpTranslator(name, canTranslate);}
public void beam_f24405_0()
{    accFactory.clear();}
 static PipelineOptions beam_f24406_0()
{    String[] args = { "--runner=DirectRunner" };    return PipelineOptionsFactory.fromArgs(args).as(PipelineOptions.class);}
public void beam_f24407_0()
{    Pipeline p = Pipeline.create();    PCollection<Integer> input = p.apply(Create.of(1, 2, 3));    PCollection<KV<Integer, Long>> result = input.apply(TimestampExtractTransform.of(in -> CountByKey.of(in).keyBy(KV::getValue, TypeDescriptors.integers()).output()));    PAssert.that(result).containsInAnyOrder(KV.of(1, 1L), KV.of(2, 1L), KV.of(3, 1L));    p.run().waitUntilFinish();}
public static NoopCredentialFactory beam_f24415_0(PipelineOptions options)
{    return INSTANCE;}
public Credentials beam_f24416_0() throws IOException
{    return NOOP_CREDENTIALS;}
public String beam_f24417_0()
{    return null;}
private static boolean beam_f24426_0()
{    return System.getProperty("os.name").toLowerCase(Locale.ENGLISH).contains("windows");}
 Map<String, String> beam_f24427_0()
{    return System.getenv();}
public Credentials beam_f24428_0(PipelineOptions options)
{    GcpOptions gcpOptions = options.as(GcpOptions.class);    try {        CredentialFactory factory = InstanceBuilder.ofType(CredentialFactory.class).fromClass(gcpOptions.getCredentialFactoryClass()).fromFactoryMethod("fromOptions").withArg(PipelineOptions.class, options).build();        return factory.getCredential();    } catch (IOException | GeneralSecurityException e) {        throw new RuntimeException("Unable to obtain credential", e);    }}
private static HttpRequestInitializer beam_f24436_0(Credentials credential, HttpRequestInitializer httpRequestInitializer)
{    if (credential == null) {        return new ChainingHttpRequestInitializer(new NullCredentialInitializer(), httpRequestInitializer);    } else {        return new ChainingHttpRequestInitializer(new HttpCredentialsAdapter(credential), httpRequestInitializer);    }}
public Iterable<Class<? extends PipelineOptions>> beam_f24437_0()
{    return ImmutableList.<Class<? extends PipelineOptions>>builder().add(GcpOptions.class).add(GcsOptions.class).add(GoogleApiDebugOptions.class).build();}
public ExecutorService beam_f24438_0(PipelineOptions options)
{    ThreadFactoryBuilder threadFactoryBuilder = new ThreadFactoryBuilder();    threadFactoryBuilder.setThreadFactory(MoreExecutors.platformThreadFactory());    threadFactoryBuilder.setDaemon(true);    /* The SDK requires an unbounded thread pool because a step may create X writers       * each requiring their own thread to perform the writes otherwise a writer may       * block causing deadlock for the step because the writers buffer is full.       * Also, the MapTaskExecutor launches the steps in reverse order and completes       * them in forward order thus requiring enough threads so that each step's writers       * can be active.       */    return new ThreadPoolExecutor(0,     Integer.MAX_VALUE, Long.MAX_VALUE,     TimeUnit.NANOSECONDS, new SynchronousQueue<>(), threadFactoryBuilder.build());}
protected ReadableByteChannel beam_f24446_0(GcsResourceId resourceId) throws IOException
{    return options.getGcsUtil().open(resourceId.getGcsPath());}
protected void beam_f24447_0(List<GcsResourceId> srcResourceIds, List<GcsResourceId> destResourceIds) throws IOException
{    copy(srcResourceIds, destResourceIds);    delete(srcResourceIds);}
protected void beam_f24448_0(Collection<GcsResourceId> resourceIds) throws IOException
{    options.getGcsUtil().remove(toFilenames(resourceIds));}
private Metadata beam_f24456_0(StorageObject storageObject)
{            Metadata.Builder ret = Metadata.builder().setIsReadSeekEfficient(true).setResourceId(GcsResourceId.fromGcsPath(GcsPath.fromObject(storageObject)));    BigInteger size = firstNonNull(storageObject.getSize(), BigInteger.ZERO);    ret.setSizeBytes(size.longValue());    DateTime lastModified = firstNonNull(storageObject.getUpdated(), new DateTime(0L));    ret.setLastModifiedMillis(lastModified.getValue());    return ret.build();}
private List<String> beam_f24457_0(Collection<GcsResourceId> resources)
{    return FluentIterable.from(resources).transform(resource -> resource.getGcsPath().toString()).toList();}
private List<GcsPath> beam_f24458_0(Collection<String> specs)
{    return FluentIterable.from(specs).transform(GcsPath::fromUri).toList();}
private GcsPath beam_f24466_0(String path)
{    try {        return GcsPath.fromUri(path);    } catch (IllegalArgumentException e) {        throw new IllegalArgumentException(String.format("Expected a valid 'gs://' path but was given '%s'", path), e);    }}
 static GcsResourceId beam_f24467_0(GcsPath gcsPath)
{    checkNotNull(gcsPath, "gcsPath");    return new GcsResourceId(gcsPath);}
public GcsResourceId beam_f24468_0(String other, ResolveOptions resolveOptions)
{    checkState(isDirectory(), String.format("Expected the gcsPath is a directory, but had [%s].", gcsPath));    checkArgument(resolveOptions.equals(StandardResolveOptions.RESOLVE_FILE) || resolveOptions.equals(StandardResolveOptions.RESOLVE_DIRECTORY), String.format("ResolveOptions: [%s] is not supported.", resolveOptions));    if (resolveOptions.equals(StandardResolveOptions.RESOLVE_FILE)) {        checkArgument(!other.endsWith("/"), "The resolved file: [%s] should not end with '/'.", other);        return fromGcsPath(gcsPath.resolve(other));    } else {                if (other.endsWith("/")) {                        return fromGcsPath(gcsPath.resolve(other));        } else {            return fromGcsPath(gcsPath.resolve(other + "/"));        }    }}
public int beam_f24476_0()
{    return gcsPath.hashCode();}
public static PathValidator beam_f24477_0(@SuppressWarnings("unused") PipelineOptions options)
{    return new NoopPathValidator();}
public String beam_f24481_0(String path)
{    return path;}
private static HttpCallCustomError beam_f24489_0(String errorMessage)
{    return (request, response) -> {        return errorMessage;    };}
public String beam_f24490_0(HttpRequestWrapper req, HttpResponseWrapper res)
{    for (MatcherAndError m : matchersAndLogs) {        if (m.getMatcher().matchResponse(req, res)) {            return m.getCustomError().customError(req, res);        }    }    return null;}
public static GcsPath beam_f24491_0(URI uri)
{    checkArgument(uri.getScheme().equalsIgnoreCase(SCHEME), "URI: %s is not a GCS URI", uri);    checkArgument(uri.getPort() == -1, "GCS URI may not specify port: %s (%i)", uri, uri.getPort());    checkArgument(isNullOrEmpty(uri.getUserInfo()), "GCS URI may not specify userInfo: %s (%s)", uri, uri.getUserInfo());    checkArgument(isNullOrEmpty(uri.getQuery()), "GCS URI may not specify query: %s (%s)", uri, uri.getQuery());    checkArgument(isNullOrEmpty(uri.getFragment()), "GCS URI may not specify fragment: %s (%s)", uri, uri.getFragment());    return fromUri(uri.toString());}
public FileSystem beam_f24499_0()
{    return fs;}
public boolean beam_f24500_0()
{    return !bucket.isEmpty() || object.isEmpty();}
public GcsPath beam_f24501_0()
{    return new GcsPath(fs, "", "");}
public boolean beam_f24509_0(Path other)
{    if (other instanceof GcsPath) {        GcsPath gcsPath = (GcsPath) other;        return endsWith(gcsPath.bucketAndObject());    } else {        return endsWith(other.toString());    }}
public boolean beam_f24510_0(String suffix)
{    return bucketAndObject().endsWith(suffix);}
public GcsPath beam_f24511_0()
{    return this;}
public File beam_f24519_0()
{    throw new UnsupportedOperationException();}
public WatchKey beam_f24520_0(WatchService watcher, WatchEvent.Kind<?>[] events, WatchEvent.Modifier... modifiers) throws IOException
{    throw new UnsupportedOperationException();}
public WatchKey beam_f24521_0(WatchService watcher, WatchEvent.Kind<?>... events) throws IOException
{    throw new UnsupportedOperationException();}
public String beam_f24529_0()
{    if (!isAbsolute()) {        return object;    }    StringBuilder sb = new StringBuilder();    sb.append(SCHEME).append("://");    if (!bucket.isEmpty()) {        sb.append(bucket).append('/');    }    sb.append(object);    return sb.toString();}
public String beam_f24530_0()
{    StringBuilder sb = new StringBuilder();    sb.append("storage.googleapis.com/");    if (!bucket.isEmpty()) {        sb.append(bucket).append('/');    }    sb.append(object);    return sb.toString();}
public URI beam_f24531_0()
{    try {        return new URI(SCHEME, "//" + bucketAndObject(), null);    } catch (URISyntaxException e) {        throw new RuntimeException("Unable to create URI for GCS path " + this);    }}
public List<GcsPath> beam_f24539_1(GcsPath gcsPattern) throws IOException
{    Pattern p = null;    String prefix = null;    if (isWildcard(gcsPattern)) {                prefix = getNonWildcardPrefix(gcsPattern.getObject());        p = Pattern.compile(wildcardToRegexp(gcsPattern.getObject()));    } else {                try {                                    getObject(gcsPattern);            return ImmutableList.of(gcsPattern);        } catch (FileNotFoundException e) {                        return ImmutableList.of();        }    }        String pageToken = null;    List<GcsPath> results = new ArrayList<>();    do {        Objects objects = listObjects(gcsPattern.getBucket(), prefix, pageToken);        if (objects.getItems() == null) {            break;        }                for (StorageObject o : objects.getItems()) {            String name = o.getName();                        if (p.matcher(name).matches() && !name.endsWith("/")) {                                results.add(GcsPath.fromObject(o));            }        }        pageToken = objects.getNextPageToken();    } while (pageToken != null);    return results;}
 Integer beam_f24540_0()
{    return uploadBufferSizeBytes;}
private static BackOff beam_f24541_0()
{    return BackOffAdapter.toGcpBackOff(BACKOFF_FACTORY.backoff());}
public SeekableByteChannel beam_f24549_0(GcsPath path) throws IOException
{    return new GoogleCloudStorageReadChannel(storageClient, path.getBucket(), path.getObject(), errorExtractor, new ClientRequestHelper<>());}
public WritableByteChannel beam_f24550_0(GcsPath path, String type) throws IOException
{    return create(path, type, uploadBufferSizeBytes);}
public WritableByteChannel beam_f24551_0(GcsPath path, String type, Integer uploadBufferSizeBytes) throws IOException
{    GoogleCloudStorageWriteChannel channel = new GoogleCloudStorageWriteChannel(executorService, storageClient, new ClientRequestHelper<>(), path.getBucket(), path.getObject(), type, /* kmsKeyName= */    null, AsyncWriteChannelOptions.newBuilder().build(), new ObjectWriteConditions(), Collections.emptyMap());    if (uploadBufferSizeBytes != null) {        channel.setUploadBufferSize(uploadBufferSizeBytes);    }    channel.initialize();    return channel;}
public boolean beam_f24559_0(IOException e)
{    if (errorExtractor.itemAlreadyExists(e) || errorExtractor.accessDenied(e)) {        return false;    }    return RetryDeterminer.SOCKET_ERRORS.shouldRetry(e);}
private static void beam_f24560_0(List<BatchRequest> batches) throws IOException
{    ExecutorService executor = MoreExecutors.listeningDecorator(new ThreadPoolExecutor(MAX_CONCURRENT_BATCHES, MAX_CONCURRENT_BATCHES, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue<>()));    List<CompletionStage<Void>> futures = new ArrayList<>();    for (final BatchRequest batch : batches) {        futures.add(MoreFutures.runAsync(() -> batch.execute(), executor));    }    try {        MoreFutures.get(MoreFutures.allAsList(futures));    } catch (InterruptedException e) {        Thread.currentThread().interrupt();        throw new IOException("Interrupted while executing batch GCS request", e);    } catch (ExecutionException e) {        if (e.getCause() instanceof FileNotFoundException) {            throw (FileNotFoundException) e.getCause();        }        throw new IOException("Error executing batch GCS request", e);    } finally {        executor.shutdown();    }}
 List<BatchRequest> beam_f24561_0(Collection<GcsPath> paths, List<StorageObjectOrIOException[]> results) throws IOException
{    List<BatchRequest> batches = new ArrayList<>();    for (List<GcsPath> filesToGet : Lists.partition(Lists.newArrayList(paths), MAX_REQUESTS_PER_BATCH)) {        BatchRequest batch = createBatchRequest();        for (GcsPath path : filesToGet) {            results.add(enqueueGetFileSize(path, batch));        }        batches.add(batch);    }    return batches;}
 List<BatchRequest> beam_f24569_0(Collection<String> filenames) throws IOException
{    List<BatchRequest> batches = new ArrayList<>();    for (List<String> filesToDelete : Lists.partition(Lists.newArrayList(filenames), MAX_REQUESTS_PER_BATCH)) {        BatchRequest batch = createBatchRequest();        for (String file : filesToDelete) {            enqueueDelete(GcsPath.fromUri(file), batch);        }        batches.add(batch);    }    return batches;}
public void beam_f24570_0(Collection<String> filenames) throws IOException
{    executeBatches(makeRemoveBatches(filenames));}
private StorageObjectOrIOException[] beam_f24571_0(final GcsPath path, BatchRequest batch) throws IOException
{    final StorageObjectOrIOException[] ret = new StorageObjectOrIOException[1];    Storage.Objects.Get getRequest = storageClient.objects().get(path.getBucket(), path.getObject());    getRequest.queue(batch, new JsonBatchCallback<StorageObject>() {        @Override        public void onSuccess(StorageObject response, HttpHeaders httpHeaders) throws IOException {            ret[0] = StorageObjectOrIOException.create(response);        }        @Override        public void onFailure(GoogleJsonError e, HttpHeaders httpHeaders) throws IOException {            IOException ioException;            if (errorExtractor.itemNotFound(e)) {                ioException = new FileNotFoundException(path.toString());            } else {                ioException = new IOException(String.format("Error trying to get %s: %s", path, e));            }            ret[0] = StorageObjectOrIOException.create(ioException);        }    });    return ret;}
private BatchRequest beam_f24579_0()
{    return storageClient.batch(httpRequestInitializer);}
private static int beam_f24580_0(StringBuilder dst, char[] src, int i)
{        dst.append('\\');    if ((i - 1) != src.length) {        dst.append(src[i]);        i++;    } else {                dst.append('\\');    }    return i;}
public GenericUrl beam_f24581_0()
{    return request.getUrl();}
public void beam_f24589_0(int writeTimeout)
{    this.writeTimeout = writeTimeout;}
public static HttpTransport beam_f24590_0()
{    return SingletonHelper.HTTP_TRANSPORT;}
public static JsonFactory beam_f24591_0()
{    return SingletonHelper.JSON_FACTORY;}
public boolean beam_f24599_0()
{    return false;}
public boolean beam_f24600_0()
{    return true;}
public void beam_f24602_0() throws Exception
{    final Package thisPackage = getClass().getPackage();    final ClassLoader thisClassLoader = getClass().getClassLoader();    final ApiSurface apiSurface = ApiSurface.ofPackage(thisPackage, thisClassLoader).pruningPattern("org[.]apache[.]beam[.].*Test.*").pruningPattern("org[.]apache[.]beam[.].*IT").pruningPattern("java[.]lang.*").pruningPattern("java[.]util.*");    @SuppressWarnings("unchecked")    final Set<Matcher<Class<?>>> allowedClasses = ImmutableSet.of(classesInPackage("com.google.api.client.googleapis"), classesInPackage("com.google.api.client.http"), classesInPackage("com.google.api.client.json"), classesInPackage("com.google.api.client.util"), classesInPackage("com.google.api.services.storage"), classesInPackage("com.google.auth"), classesInPackage("com.fasterxml.jackson.annotation"), classesInPackage("java"), classesInPackage("javax"), classesInPackage("org.apache.beam.sdk"), classesInPackage("org.joda.time"), classesInPackage("org.junit"));    assertThat(apiSurface, containsOnlyClassesMatching(allowedClasses));}
public void beam_f24610_0() throws Exception
{    GcpOptions options = PipelineOptionsFactory.as(GcpOptions.class);    String tempLocation = "gs://bucket";    options.setTempLocation(tempLocation);    options.as(GcsOptions.class).setPathValidatorClass(NoopPathValidator.class);    assertEquals(tempLocation, options.getGcpTempLocation());}
public void beam_f24611_0() throws Exception
{    GcpOptions options = PipelineOptionsFactory.as(GcpOptions.class);    options.setTempLocation("file://");    thrown.expect(IllegalArgumentException.class);    thrown.expectMessage("Error constructing default value for gcpTempLocation: tempLocation is not" + " a valid GCS path");    options.getGcpTempLocation();}
public void beam_f24612_0()
{    GcpOptions options = PipelineOptionsFactory.as(GcpOptions.class);    String tempLocation = "gs://does/not/exist";    options.setTempLocation(tempLocation);    thrown.expect(IllegalArgumentException.class);    thrown.expectMessage("Error constructing default value for gcpTempLocation: tempLocation is not" + " a valid GCS path");    thrown.expectCause(hasMessage(containsString("Output path does not exist or is not writeable")));    options.getGcpTempLocation();}
public void beam_f24620_0() throws Exception
{    doReturn(fakeProject).when(mockGet).execute();    when(mockGcsUtil.bucketOwner(any(GcsPath.class))).thenReturn(5L);    thrown.expect(IllegalArgumentException.class);    thrown.expectMessage("Bucket owner does not match the project");    GcpTempLocationFactory.tryCreateDefaultBucket(options, mockCrmClient);}
public void beam_f24621_0() throws Exception
{    doReturn(fakeProject).when(mockGet).execute();    options.as(GcpOptions.class).setDataflowKmsKey("kms_key");    thrown.expect(RuntimeException.class);    thrown.expectMessage("dataflowKmsKey");    GcpTempLocationFactory.tryCreateDefaultBucket(options, mockCrmClient);}
public void beam_f24622_0() throws Exception
{    assertEquals("us-central1", GcpTempLocationFactory.getRegionFromZone("us-central1-a"));    assertEquals("asia-east", GcpTempLocationFactory.getRegionFromZone("asia-east-a"));}
public void beam_f24630_0()
{    for (FileSystemRegistrar registrar : Lists.newArrayList(ServiceLoader.load(FileSystemRegistrar.class).iterator())) {        if (registrar instanceof GcsFileSystemRegistrar) {            Iterable<FileSystem> fileSystems = registrar.fromOptions(PipelineOptionsFactory.create());            assertThat(fileSystems, contains(instanceOf(GcsFileSystem.class)));            return;        }    }    fail("Expected to find " + GcsFileSystemRegistrar.class);}
public void beam_f24631_0()
{    MockitoAnnotations.initMocks(this);    GcsOptions gcsOptions = PipelineOptionsFactory.as(GcsOptions.class);    gcsOptions.setGcsUtil(mockGcsUtil);    gcsFileSystem = new GcsFileSystem(gcsOptions);}
public void beam_f24632_0() throws Exception
{    Objects modelObjects = new Objects();    List<StorageObject> items = new ArrayList<>();        items.add(new StorageObject().setBucket("testbucket").setName("testdirectory/"));        items.add(createStorageObject("gs://testbucket/testdirectory/file1name", 1L));    items.add(createStorageObject("gs://testbucket/testdirectory/file2name", 2L));    items.add(createStorageObject("gs://testbucket/testdirectory/file3name", 3L));    items.add(createStorageObject("gs://testbucket/testdirectory/file4name", 4L));    items.add(createStorageObject("gs://testbucket/testdirectory/otherfile", 5L));    items.add(createStorageObject("gs://testbucket/testdirectory/anotherfile", 6L));    modelObjects.setItems(items);    when(mockGcsUtil.listObjects(eq("testbucket"), anyString(), isNull(String.class))).thenReturn(modelObjects);    List<GcsPath> gcsPaths = ImmutableList.of(GcsPath.fromUri("gs://testbucket/testdirectory/non-exist-file"), GcsPath.fromUri("gs://testbucket/testdirectory/otherfile"));    when(mockGcsUtil.getObjects(eq(gcsPaths))).thenReturn(ImmutableList.of(StorageObjectOrIOException.create(new FileNotFoundException()), StorageObjectOrIOException.create(createStorageObject("gs://testbucket/testdirectory/otherfile", 4L))));    List<String> specs = ImmutableList.of("gs://testbucket/testdirectory/file[1-3]*", "gs://testbucket/testdirectory/non-exist-file", "gs://testbucket/testdirectory/otherfile");    List<MatchResult> matchResults = gcsFileSystem.match(specs);    assertEquals(3, matchResults.size());    assertEquals(Status.OK, matchResults.get(0).status());    assertThat(ImmutableList.of("gs://testbucket/testdirectory/file1name", "gs://testbucket/testdirectory/file2name", "gs://testbucket/testdirectory/file3name"), contains(toFilenames(matchResults.get(0)).toArray()));    assertEquals(Status.NOT_FOUND, matchResults.get(1).status());    assertEquals(Status.OK, matchResults.get(2).status());    assertThat(ImmutableList.of("gs://testbucket/testdirectory/otherfile"), contains(toFilenames(matchResults.get(2)).toArray()));}
public void beam_f24640_0()
{    expectedException.expect(IllegalArgumentException.class);    expectedException.expectMessage("Expected a valid 'gs://' path but was given '/local/path'");    validator.validateInputFilePatternSupported("/local/path");}
public void beam_f24641_0()
{    expectedException.expect(IllegalArgumentException.class);    expectedException.expectMessage("Missing object or bucket in path: 'gs://input/', " + "did you mean: 'gs://some-bucket/input'?");    validator.validateInputFilePatternSupported("gs://input");}
public void beam_f24642_0() throws Exception
{    when(mockGcsUtil.bucketAccessible(any(GcsPath.class))).thenReturn(false);    expectedException.expect(IllegalArgumentException.class);    expectedException.expectMessage("Could not find file gs://non-existent-bucket/location");    validator.validateInputFilePatternSupported("gs://non-existent-bucket/location");}
public void beam_f24650_0()
{        assertEquals(toResourceIdentifier("gs://my_bucket/tmp dir/"), toResourceIdentifier("gs://my_bucket/tmp dir/").getCurrentDirectory());        assertEquals(toResourceIdentifier("gs://my_bucket/输出 目录/"), toResourceIdentifier("gs://my_bucket/输出 目录/文件01.txt").getCurrentDirectory());        assertEquals(toResourceIdentifier("gs://my_bucket/"), toResourceIdentifier("gs://my_bucket").getCurrentDirectory());}
public void beam_f24651_0()
{    assertTrue(toResourceIdentifier("gs://my_bucket/tmp dir/").isDirectory());    assertTrue(toResourceIdentifier("gs://my_bucket/").isDirectory());    assertTrue(toResourceIdentifier("gs://my_bucket").isDirectory());    assertFalse(toResourceIdentifier("gs://my_bucket/file").isDirectory());}
public void beam_f24652_0()
{    thrown.expect(IllegalArgumentException.class);    thrown.expectMessage("Invalid GCS URI: gs://");    toResourceIdentifier("gs://");}
private HttpRequestWrapper beam_f24660_0(String url) throws MalformedURLException
{    HttpRequestWrapper request = mock(HttpRequestWrapper.class);    GenericUrl genericUrl = new GenericUrl(new URL(url));    when(request.getUrl()).thenReturn(genericUrl);    return request;}
private HttpResponseWrapper beam_f24661_0(int statusCode)
{    HttpResponseWrapper response = mock(HttpResponseWrapper.class);    when(response.getStatusCode()).thenReturn(statusCode);    return response;}
public void beam_f24662_0() throws IOException
{    HttpRequestWrapper request = createHttpRequest(BQ_TABLES_LIST_URL);    HttpResponseWrapper response = createHttpResponse(403);    HttpCallCustomError mockCustomError = mock(HttpCallCustomError.class);    CustomHttpErrors.Builder builder = new CustomHttpErrors.Builder();    builder.addErrorForCode(403, "Custom Error Msg");    CustomHttpErrors customErrors = builder.build();    String errorMessage = customErrors.getCustomError(request, response);    assertEquals("Custom Error Msg", errorMessage);}
public void beam_f24670_0() throws Exception
{    for (TestCase testCase : PATH_TEST_CASES) {        String uriString = testCase.uri;        GcsPath path = GcsPath.fromUri(URI.create(uriString));                assertEquals(testCase.expectedBucket, path.getBucket());        assertEquals(testCase.expectedObject, path.getObject());        assertEquals(testCase.uri, testCase.namedComponents.length, path.getNameCount());                GcsPath built = GcsPath.fromComponents(null, null);        for (String component : testCase.namedComponents) {            built = built.resolve(component);        }        assertEquals(testCase.uri, built.toString());    }}
public void beam_f24671_0() throws Exception
{    GcsPath path = GcsPath.fromComponents("bucket", "then/object");    assertEquals("bucket", path.getBucket());    assertEquals("then/object", path.getObject());    assertEquals(3, path.getNameCount());    assertTrue(path.endsWith("object"));    assertTrue(path.startsWith("bucket/then"));        GcsPath parent = path.getParent();    assertEquals("bucket", parent.getBucket());    assertEquals("then/", parent.getObject());    assertEquals(2, parent.getNameCount());    assertThat(path, Matchers.not(Matchers.equalTo(parent)));    assertTrue(path.startsWith(parent));    assertFalse(parent.startsWith(path));    assertTrue(parent.endsWith("then/"));    assertTrue(parent.startsWith("bucket/then"));    assertTrue(parent.isAbsolute());    GcsPath root = path.getRoot();    assertEquals(0, root.getNameCount());    assertEquals("gs://", root.toString());    assertEquals("", root.getBucket());    assertEquals("", root.getObject());    assertTrue(root.isAbsolute());    assertThat(root, Matchers.equalTo(parent.getRoot()));        GcsPath grandParent = parent.getParent();    assertEquals(1, grandParent.getNameCount());    assertEquals("gs://bucket/", grandParent.toString());    assertTrue(grandParent.isAbsolute());    assertThat(root, Matchers.equalTo(grandParent.getParent()));    assertThat(root.getParent(), Matchers.nullValue());    assertTrue(path.startsWith(path.getRoot()));    assertTrue(parent.startsWith(path.getRoot()));}
public void beam_f24672_0() throws Exception
{    GcsPath path = GcsPath.fromComponents(null, "a/b");    GcsPath parent = path.getParent();    assertEquals("a/", parent.toString());    GcsPath grandParent = parent.getParent();    assertNull(grandParent);}
public void beam_f24680_0()
{    GcsPath.fromComponents(null, "a\rb");}
public void beam_f24681_0()
{    GcsPath path = GcsPath.fromComponents("bucket", "a/b/c");    GcsPath d = path.resolve("gs://bucket2/d");    assertEquals("gs://bucket2/d", d.toString());}
public void beam_f24682_0()
{    GcsPath a = GcsPath.fromComponents("bucket", "a");    GcsPath b = a.resolve(Paths.get("b"));    assertEquals("a/b", b.getObject());}
public void beam_f24690_0()
{    GcsPath a = GcsPath.fromComponents("bucket", "a/b/c/d");    assertEquals(5, a.getNameCount());    assertThat(a.getName(0).toString(), Matchers.equalTo("gs://bucket/"));    assertThat(a.getName(1).toString(), Matchers.equalTo("a"));    assertThat(a.getName(2).toString(), Matchers.equalTo("b"));    assertThat(a.getName(3).toString(), Matchers.equalTo("c"));    assertThat(a.getName(4).toString(), Matchers.equalTo("d"));}
public void beam_f24691_0()
{    GcsPath a = GcsPath.fromComponents("bucket", "a/b/c/d");        a.subpath(1, 1);    Assert.fail();}
public void beam_f24692_0() throws IOException
{    TestPipelineOptions options = TestPipeline.testingPipelineOptions().as(TestPipelineOptions.class);            assertNotNull(options.getTempRoot());    options.setTempLocation(options.getTempRoot() + "/testRewriteMultiPart");    GcsOptions gcsOptions = options.as(GcsOptions.class);    GcsUtil gcsUtil = gcsOptions.getGcsUtil();    String srcFilename = "gs://dataflow-samples/wikipedia_edits/wiki_data-000000000000.json";    String dstFilename = gcsOptions.getGcpTempLocation() + String.format("/GcsUtilIT-%tF-%<tH-%<tM-%<tS-%<tL.testRewriteMultiPart.copy", new Date());    gcsUtil.maxBytesRewrittenPerCall = 50L * 1024 * 1024;    gcsUtil.numRewriteTokensUsed = new AtomicInteger();    gcsUtil.copy(Lists.newArrayList(srcFilename), Lists.newArrayList(dstFilename));    assertThat(gcsUtil.numRewriteTokensUsed.get(), equalTo(3));    assertThat(gcsUtil.getObject(GcsPath.fromUri(srcFilename)).getMd5Hash(), equalTo(gcsUtil.getObject(GcsPath.fromUri(dstFilename)).getMd5Hash()));    gcsUtil.remove(Lists.newArrayList(dstFilename));}
public void beam_f24700_0() throws Exception
{    GcsOptions pipelineOptions = PipelineOptionsFactory.as(GcsOptions.class);    ExecutorService executorService = pipelineOptions.getExecutorService();    int numThreads = 100;    final CountDownLatch[] countDownLatches = new CountDownLatch[numThreads];    for (int i = 0; i < numThreads; i++) {        final int currentLatch = i;        countDownLatches[i] = new CountDownLatch(1);        executorService.execute(() -> {                        try {                countDownLatches[currentLatch].await();                if (currentLatch > 0) {                    countDownLatches[currentLatch - 1].countDown();                }            } catch (InterruptedException e) {                Thread.currentThread().interrupt();                throw new RuntimeException(e);            }        });    }        countDownLatches[countDownLatches.length - 1].countDown();    executorService.shutdown();    assertTrue("Expected tasks to complete", executorService.awaitTermination(10, TimeUnit.SECONDS));}
public void beam_f24701_0() throws IOException
{    GcsOptions pipelineOptions = gcsOptionsWithTestCredential();    GcsUtil gcsUtil = pipelineOptions.getGcsUtil();    Storage mockStorage = Mockito.mock(Storage.class);    gcsUtil.setStorageClient(mockStorage);    Storage.Objects mockStorageObjects = Mockito.mock(Storage.Objects.class);    Storage.Objects.Get mockStorageGet = Mockito.mock(Storage.Objects.Get.class);    Storage.Objects.List mockStorageList = Mockito.mock(Storage.Objects.List.class);    Objects modelObjects = new Objects();    List<StorageObject> items = new ArrayList<>();        items.add(new StorageObject().setBucket("testbucket").setName("testdirectory/"));        items.add(new StorageObject().setBucket("testbucket").setName("testdirectory/file1name"));    items.add(new StorageObject().setBucket("testbucket").setName("testdirectory/file2name"));    items.add(new StorageObject().setBucket("testbucket").setName("testdirectory/file3name"));    items.add(new StorageObject().setBucket("testbucket").setName("testdirectory/otherfile"));    items.add(new StorageObject().setBucket("testbucket").setName("testdirectory/anotherfile"));    modelObjects.setItems(items);    when(mockStorage.objects()).thenReturn(mockStorageObjects);    when(mockStorageObjects.get("testbucket", "testdirectory/otherfile")).thenReturn(mockStorageGet);    when(mockStorageObjects.list("testbucket")).thenReturn(mockStorageList);    when(mockStorageGet.execute()).thenReturn(new StorageObject().setBucket("testbucket").setName("testdirectory/otherfile"));    when(mockStorageList.execute()).thenReturn(modelObjects);        {        GcsPath pattern = GcsPath.fromUri("gs://testbucket/testdirectory/otherfile");        List<GcsPath> expectedFiles = ImmutableList.of(GcsPath.fromUri("gs://testbucket/testdirectory/otherfile"));        assertThat(expectedFiles, contains(gcsUtil.expand(pattern).toArray()));    }        {        GcsPath pattern = GcsPath.fromUri("gs://testbucket/testdirectory/file*");        List<GcsPath> expectedFiles = ImmutableList.of(GcsPath.fromUri("gs://testbucket/testdirectory/file1name"), GcsPath.fromUri("gs://testbucket/testdirectory/file2name"), GcsPath.fromUri("gs://testbucket/testdirectory/file3name"));        assertThat(expectedFiles, contains(gcsUtil.expand(pattern).toArray()));    }    {        GcsPath pattern = GcsPath.fromUri("gs://testbucket/testdirectory/file[1-3]*");        List<GcsPath> expectedFiles = ImmutableList.of(GcsPath.fromUri("gs://testbucket/testdirectory/file1name"), GcsPath.fromUri("gs://testbucket/testdirectory/file2name"), GcsPath.fromUri("gs://testbucket/testdirectory/file3name"));        assertThat(expectedFiles, contains(gcsUtil.expand(pattern).toArray()));    }    {        GcsPath pattern = GcsPath.fromUri("gs://testbucket/testdirectory/file?name");        List<GcsPath> expectedFiles = ImmutableList.of(GcsPath.fromUri("gs://testbucket/testdirectory/file1name"), GcsPath.fromUri("gs://testbucket/testdirectory/file2name"), GcsPath.fromUri("gs://testbucket/testdirectory/file3name"));        assertThat(expectedFiles, contains(gcsUtil.expand(pattern).toArray()));    }    {        GcsPath pattern = GcsPath.fromUri("gs://testbucket/test*ectory/fi*name");        List<GcsPath> expectedFiles = ImmutableList.of(GcsPath.fromUri("gs://testbucket/testdirectory/file1name"), GcsPath.fromUri("gs://testbucket/testdirectory/file2name"), GcsPath.fromUri("gs://testbucket/testdirectory/file3name"));        assertThat(expectedFiles, contains(gcsUtil.expand(pattern).toArray()));    }}
public void beam_f24702_0() throws IOException
{    GcsOptions pipelineOptions = gcsOptionsWithTestCredential();    GcsUtil gcsUtil = pipelineOptions.getGcsUtil();    Storage mockStorage = Mockito.mock(Storage.class);    gcsUtil.setStorageClient(mockStorage);    Storage.Objects mockStorageObjects = Mockito.mock(Storage.Objects.class);    Storage.Objects.Get mockStorageGet = Mockito.mock(Storage.Objects.Get.class);    Storage.Objects.List mockStorageList = Mockito.mock(Storage.Objects.List.class);    Objects modelObjects = new Objects();    List<StorageObject> items = new ArrayList<>();        items.add(new StorageObject().setBucket("testbucket").setName("testdirectory/"));        items.add(new StorageObject().setBucket("testbucket").setName("test/directory/file1.txt"));    items.add(new StorageObject().setBucket("testbucket").setName("test/directory/file2.txt"));    items.add(new StorageObject().setBucket("testbucket").setName("test/directory/file3.txt"));    items.add(new StorageObject().setBucket("testbucket").setName("test/directory/otherfile"));    items.add(new StorageObject().setBucket("testbucket").setName("test/directory/anotherfile"));    items.add(new StorageObject().setBucket("testbucket").setName("test/file4.txt"));    modelObjects.setItems(items);    when(mockStorage.objects()).thenReturn(mockStorageObjects);    when(mockStorageObjects.get("testbucket", "test/directory/otherfile")).thenReturn(mockStorageGet);    when(mockStorageObjects.list("testbucket")).thenReturn(mockStorageList);    when(mockStorageGet.execute()).thenReturn(new StorageObject().setBucket("testbucket").setName("test/directory/otherfile"));    when(mockStorageList.execute()).thenReturn(modelObjects);    {        GcsPath pattern = GcsPath.fromUri("gs://testbucket/test/**/*.txt");        List<GcsPath> expectedFiles = ImmutableList.of(GcsPath.fromUri("gs://testbucket/test/directory/file1.txt"), GcsPath.fromUri("gs://testbucket/test/directory/file2.txt"), GcsPath.fromUri("gs://testbucket/test/directory/file3.txt"), GcsPath.fromUri("gs://testbucket/test/file4.txt"));        assertThat(expectedFiles, contains(gcsUtil.expand(pattern).toArray()));    }}
public LowLevelHttpResponse beam_f24710_0() throws IOException
{    return mockResponse;}
public void beam_f24711_0() throws Exception
{    JsonFactory jsonFactory = new JacksonFactory();    String contentBoundary = "batch_foobarbaz";    String contentBoundaryLine = "--" + contentBoundary;    String endOfContentBoundaryLine = "--" + contentBoundary + "--";    GenericJson error = new GenericJson().set("error", new GenericJson().set("code", 404));    error.setFactory(jsonFactory);    String content = contentBoundaryLine + "\n" + "Content-Type: application/http\n" + "\n" + "HTTP/1.1 404 Not Found\n" + "Content-Length: -1\n" + "\n" + error.toString() + "\n" + "\n" + endOfContentBoundaryLine + "\n";    final LowLevelHttpResponse mockResponse = Mockito.mock(LowLevelHttpResponse.class);    when(mockResponse.getContentType()).thenReturn("multipart/mixed; boundary=" + contentBoundary);    when(mockResponse.getStatusCode()).thenReturn(200);    when(mockResponse.getContent()).thenReturn(toStream(content));        MockLowLevelHttpRequest request = new MockLowLevelHttpRequest() {        @Override        public LowLevelHttpResponse execute() throws IOException {            return mockResponse;        }    };    MockHttpTransport mockTransport = new MockHttpTransport.Builder().setLowLevelHttpRequest(request).build();    GcsUtil gcsUtil = gcsOptionsWithTestCredential().getGcsUtil();    gcsUtil.setStorageClient(new Storage(mockTransport, Transport.getJsonFactory(), new RetryHttpRequestInitializer()));    gcsUtil.remove(Arrays.asList("gs://some-bucket/already-deleted"));}
public LowLevelHttpResponse beam_f24712_0() throws IOException
{    return mockResponse;}
public void beam_f24720_0() throws IOException
{    GoogleCloudStorageReadOptions readOptions = GoogleCloudStorageReadOptions.builder().setFastFailOnNotFound(false).build();    SeekableByteChannel channel = new GoogleCloudStorageReadChannel(null, "dummybucket", "dummyobject", null, new ClientRequestHelper<>(), readOptions);    channel.close();    channel.close();}
private static GoogleJsonResponseException beam_f24721_0(final int status, final String reason, final String message) throws IOException
{    final JsonFactory jsonFactory = new JacksonFactory();    HttpTransport transport = new MockHttpTransport() {        @Override        public LowLevelHttpRequest buildRequest(String method, String url) throws IOException {            ErrorInfo errorInfo = new ErrorInfo();            errorInfo.setReason(reason);            errorInfo.setMessage(message);            errorInfo.setFactory(jsonFactory);            GenericJson error = new GenericJson();            error.set("code", status);            error.set("errors", Arrays.asList(errorInfo));            error.setFactory(jsonFactory);            GenericJson errorResponse = new GenericJson();            errorResponse.set("error", error);            errorResponse.setFactory(jsonFactory);            return new MockLowLevelHttpRequest().setResponse(new MockLowLevelHttpResponse().setContent(errorResponse.toPrettyString()).setContentType(Json.MEDIA_TYPE).setStatusCode(status));        }    };    HttpRequest request = transport.createRequestFactory().buildGetRequest(HttpTesting.SIMPLE_GENERIC_URL);    request.setThrowExceptionOnExecuteError(false);    HttpResponse response = request.execute();    return GoogleJsonResponseException.from(jsonFactory, response);}
public LowLevelHttpRequest beam_f24722_0(String method, String url) throws IOException
{    ErrorInfo errorInfo = new ErrorInfo();    errorInfo.setReason(reason);    errorInfo.setMessage(message);    errorInfo.setFactory(jsonFactory);    GenericJson error = new GenericJson();    error.set("code", status);    error.set("errors", Arrays.asList(errorInfo));    error.setFactory(jsonFactory);    GenericJson errorResponse = new GenericJson();    errorResponse.set("error", error);    errorResponse.setFactory(jsonFactory);    return new MockLowLevelHttpRequest().setResponse(new MockLowLevelHttpResponse().setContent(errorResponse.toPrettyString()).setContentType(Json.MEDIA_TYPE).setStatusCode(status));}
public void beam_f24730_0() throws IOException
{    GcsUtil gcsUtil = gcsOptionsWithTestCredential().getGcsUtil();        List<BatchRequest> batches = gcsUtil.makeRemoveBatches(makeStrings("s", 3));    assertThat(batches.size(), equalTo(1));    assertThat(sumBatchSizes(batches), equalTo(3));        batches = gcsUtil.makeRemoveBatches(makeStrings("s", 100));    assertThat(batches.size(), equalTo(1));    assertThat(sumBatchSizes(batches), equalTo(100));        batches = gcsUtil.makeRemoveBatches(makeStrings("s", 501));    assertThat(batches.size(), equalTo(6));    assertThat(sumBatchSizes(batches), equalTo(501));}
public void beam_f24731_0() throws IOException
{    GcsUtil gcsUtil = gcsOptionsWithTestCredential().getGcsUtil();        List<StorageObjectOrIOException[]> results = Lists.newArrayList();    List<BatchRequest> batches = gcsUtil.makeGetBatches(makeGcsPaths("s", 3), results);    assertThat(batches.size(), equalTo(1));    assertThat(sumBatchSizes(batches), equalTo(3));    assertEquals(3, results.size());        results = Lists.newArrayList();    batches = gcsUtil.makeGetBatches(makeGcsPaths("s", 100), results);    assertThat(batches.size(), equalTo(1));    assertThat(sumBatchSizes(batches), equalTo(100));    assertEquals(100, results.size());        results = Lists.newArrayList();    batches = gcsUtil.makeGetBatches(makeGcsPaths("s", 501), results);    assertThat(batches.size(), equalTo(6));    assertThat(sumBatchSizes(batches), equalTo(501));    assertEquals(501, results.size());}
private static InputStream beam_f24732_0(String content) throws IOException
{    return new ByteArrayInputStream(content.getBytes(StandardCharsets.UTF_8));}
public void beam_f24740_0() throws IOException
{    when(mockLowLevelRequest.execute()).thenThrow(new IOException("Fake Error")).thenReturn(mockLowLevelResponse);    when(mockLowLevelResponse.getStatusCode()).thenReturn(200);    Storage.Buckets.Get result = storage.buckets().get("test");    HttpResponse response = result.executeUnparsed();    assertNotNull(response);    verify(mockHttpResponseInterceptor).interceptResponse(any(HttpResponse.class));    verify(mockLowLevelRequest, atLeastOnce()).addHeader(anyString(), anyString());    verify(mockLowLevelRequest, times(2)).setTimeout(anyInt(), anyInt());    verify(mockLowLevelRequest, times(2)).setWriteTimeout(anyInt());    verify(mockLowLevelRequest, times(2)).execute();    verify(mockLowLevelResponse).getStatusCode();    expectedLogs.verifyDebug("Request failed with IOException");}
public void beam_f24741_0() throws IOException
{    when(mockLowLevelRequest.execute()).thenReturn(mockLowLevelResponse);    final int retries = 10;    when(mockLowLevelResponse.getStatusCode()).thenAnswer(new Answer<Integer>() {        int n = 0;        @Override        public Integer answer(InvocationOnMock invocation) {            return n++ < retries ? 503 : 9999;        }    });    Storage.Buckets.Get result = storage.buckets().get("test");    try {        result.executeUnparsed();        fail();    } catch (IOException e) {    }    verify(mockHttpResponseInterceptor).interceptResponse(any(HttpResponse.class));    verify(mockLowLevelRequest, atLeastOnce()).addHeader(anyString(), anyString());    verify(mockLowLevelRequest, times(retries + 1)).setTimeout(anyInt(), anyInt());    verify(mockLowLevelRequest, times(retries + 1)).setWriteTimeout(anyInt());    verify(mockLowLevelRequest, times(retries + 1)).execute();    verify(mockLowLevelResponse, times(retries + 1)).getStatusCode();    expectedLogs.verifyWarn("performed 10 retries due to unsuccessful status codes");}
public Integer beam_f24742_0(InvocationOnMock invocation)
{    return n++ < retries ? 503 : 9999;}
public AsJsonsWithFailures<NewFailureT> beam_f24750_0(TypeDescriptor<NewFailureT> failureTypeDescriptor)
{    return new AsJsonsWithFailures<>(null, failureTypeDescriptor);}
public AsJsonsWithFailures<FailureT> beam_f24751_0(InferableFunction<WithFailures.ExceptionElement<InputT>, FailureT> exceptionHandler)
{    return new AsJsonsWithFailures<>(exceptionHandler, exceptionHandler.getOutputTypeDescriptor());}
public AsJsonsWithFailures<KV<InputT, Map<String, String>>> beam_f24752_0()
{    DefaultExceptionAsMapHandler<InputT> exceptionHandler = new DefaultExceptionAsMapHandler<InputT>() {    };    return new AsJsonsWithFailures<>(exceptionHandler, exceptionHandler.getOutputTypeDescriptor());}
public ParseJsons<OutputT> beam_f24760_0(ObjectMapper mapper)
{    ParseJsons<OutputT> newTransform = new ParseJsons<>(outputClass);    newTransform.customMapper = mapper;    return newTransform;}
public ParseJsonsWithFailures<NewFailureT> beam_f24761_0(TypeDescriptor<NewFailureT> failureTypeDescriptor)
{    return new ParseJsonsWithFailures<>(null, failureTypeDescriptor);}
public ParseJsonsWithFailures<FailureT> beam_f24762_0(InferableFunction<WithFailures.ExceptionElement<String>, FailureT> exceptionHandler)
{    return new ParseJsonsWithFailures<>(exceptionHandler, exceptionHandler.getOutputTypeDescriptor());}
public void beam_f24770_0()
{    PCollection<MyPojo> output = pipeline.apply(Create.of(VALID_JSONS)).apply(ParseJsons.of(MyPojo.class)).setCoder(SerializableCoder.of(MyPojo.class));    PAssert.that(output).containsInAnyOrder(POJOS);    pipeline.run();}
public void beam_f24771_0()
{    PCollection<MyPojo> output = pipeline.apply(Create.of(Iterables.concat(VALID_JSONS, INVALID_JSONS))).apply(ParseJsons.of(MyPojo.class)).setCoder(SerializableCoder.of(MyPojo.class));    PAssert.that(output).containsInAnyOrder(POJOS);    pipeline.run();}
public void beam_f24772_0()
{    WithFailures.Result<PCollection<MyPojo>, KV<String, Map<String, String>>> result = pipeline.apply(Create.of(Iterables.concat(VALID_JSONS, INVALID_JSONS))).apply(ParseJsons.of(MyPojo.class).exceptionsVia());    result.output().setCoder(SerializableCoder.of(MyPojo.class));    PAssert.that(result.output()).containsInAnyOrder(POJOS);    assertParsingWithErrorMapHandler(result);    pipeline.run();}
public void beam_f24780_0()
{    pipeline.apply(Create.of(EMPTY_BEANS)).apply(AsJsons.of(MyEmptyBean.class)).setCoder(StringUtf8Coder.of());    pipeline.run();}
public void beam_f24781_0()
{    ObjectMapper customMapper = new ObjectMapper().configure(SerializationFeature.FAIL_ON_EMPTY_BEANS, false);    PCollection<String> output = pipeline.apply(Create.of(EMPTY_BEANS)).apply(AsJsons.of(MyEmptyBean.class).withMapper(customMapper)).setCoder(StringUtf8Coder.of());    PAssert.that(output).containsInAnyOrder(EMPTY_JSONS);    pipeline.run();}
public void beam_f24782_0()
{    WithFailures.Result<PCollection<String>, KV<MyPojo, Map<String, String>>> result = pipeline.apply(Create.of(Iterables.concat(POJOS, INVALID_POJOS)).withCoder(SerializableCoder.of(MyPojo.class))).apply(AsJsons.of(MyPojo.class).exceptionsVia());    result.output().setCoder(StringUtf8Coder.of());    result.failures().setCoder(KvCoder.of(SerializableCoder.of(MyPojo.class), MapCoder.of(StringUtf8Coder.of(), StringUtf8Coder.of())));    PAssert.that(result.output()).containsInAnyOrder(VALID_JSONS);    assertWritingWithErrorMapHandler(result);    pipeline.run();}
public void beam_f24790_0(int myInt)
{    this.myInt = myInt;}
public boolean beam_f24791_0(Object o)
{    if (this == o) {        return true;    }    if (!(o instanceof MyPojo)) {        return false;    }    MyPojo myPojo = (MyPojo) o;    return myInt == myPojo.myInt && (myString != null ? myString.equals(myPojo.myString) : myPojo.myString == null);}
public int beam_f24792_0()
{    int result = myString != null ? myString.hashCode() : 0;    result = 31 * result + myInt;    return result;}
public static InnerJoin<K, V1, V2> beam_f24800_0(PCollection<KV<K, V2>> rightCollection)
{    return new InnerJoin<>(rightCollection);}
public PCollection<KV<K, KV<V1, V2>>> beam_f24801_0(PCollection<KV<K, V1>> leftCollection)
{    checkNotNull(leftCollection);    checkNotNull(rightCollection);    final TupleTag<V1> v1Tuple = new TupleTag<>();    final TupleTag<V2> v2Tuple = new TupleTag<>();    PCollection<KV<K, CoGbkResult>> coGbkResultCollection = KeyedPCollectionTuple.of(v1Tuple, leftCollection).and(v2Tuple, rightCollection).apply("CoGBK", CoGroupByKey.create());    return coGbkResultCollection.apply("Join", ParDo.of(new DoFn<KV<K, CoGbkResult>, KV<K, KV<V1, V2>>>() {        @ProcessElement        public void processElement(ProcessContext c) {            KV<K, CoGbkResult> e = c.element();            Iterable<V1> leftValuesIterable = e.getValue().getAll(v1Tuple);            Iterable<V2> rightValuesIterable = e.getValue().getAll(v2Tuple);            for (V1 leftValue : leftValuesIterable) {                for (V2 rightValue : rightValuesIterable) {                    c.output(KV.of(e.getKey(), KV.of(leftValue, rightValue)));                }            }        }    })).setCoder(KvCoder.of(((KvCoder) leftCollection.getCoder()).getKeyCoder(), KvCoder.of(((KvCoder) leftCollection.getCoder()).getValueCoder(), ((KvCoder) rightCollection.getCoder()).getValueCoder())));}
public void beam_f24802_0(ProcessContext c)
{    KV<K, CoGbkResult> e = c.element();    Iterable<V1> leftValuesIterable = e.getValue().getAll(v1Tuple);    Iterable<V2> rightValuesIterable = e.getValue().getAll(v2Tuple);    for (V1 leftValue : leftValuesIterable) {        for (V2 rightValue : rightValuesIterable) {            c.output(KV.of(e.getKey(), KV.of(leftValue, rightValue)));        }    }}
public PCollection<KV<K, KV<V1, V2>>> beam_f24810_0(PCollection<KV<K, V1>> leftCollection)
{    checkNotNull(leftCollection);    checkNotNull(rightCollection);    checkNotNull(leftNullValue);    checkNotNull(rightNullValue);    final TupleTag<V1> v1Tuple = new TupleTag<>();    final TupleTag<V2> v2Tuple = new TupleTag<>();    PCollection<KV<K, CoGbkResult>> coGbkResultCollection = KeyedPCollectionTuple.of(v1Tuple, leftCollection).and(v2Tuple, rightCollection).apply("CoGBK", CoGroupByKey.create());    return coGbkResultCollection.apply("Join", ParDo.of(new DoFn<KV<K, CoGbkResult>, KV<K, KV<V1, V2>>>() {        @ProcessElement        public void processElement(ProcessContext c) {            KV<K, CoGbkResult> e = c.element();            Iterable<V1> leftValuesIterable = e.getValue().getAll(v1Tuple);            Iterable<V2> rightValuesIterable = e.getValue().getAll(v2Tuple);            if (leftValuesIterable.iterator().hasNext() && rightValuesIterable.iterator().hasNext()) {                for (V2 rightValue : rightValuesIterable) {                    for (V1 leftValue : leftValuesIterable) {                        c.output(KV.of(e.getKey(), KV.of(leftValue, rightValue)));                    }                }            } else if (leftValuesIterable.iterator().hasNext() && !rightValuesIterable.iterator().hasNext()) {                for (V1 leftValue : leftValuesIterable) {                    c.output(KV.of(e.getKey(), KV.of(leftValue, rightNullValue)));                }            } else if (!leftValuesIterable.iterator().hasNext() && rightValuesIterable.iterator().hasNext()) {                for (V2 rightValue : rightValuesIterable) {                    c.output(KV.of(e.getKey(), KV.of(leftNullValue, rightValue)));                }            }        }    })).setCoder(KvCoder.of(((KvCoder) leftCollection.getCoder()).getKeyCoder(), KvCoder.of(((KvCoder) leftCollection.getCoder()).getValueCoder(), ((KvCoder) rightCollection.getCoder()).getValueCoder())));}
public void beam_f24811_0(ProcessContext c)
{    KV<K, CoGbkResult> e = c.element();    Iterable<V1> leftValuesIterable = e.getValue().getAll(v1Tuple);    Iterable<V2> rightValuesIterable = e.getValue().getAll(v2Tuple);    if (leftValuesIterable.iterator().hasNext() && rightValuesIterable.iterator().hasNext()) {        for (V2 rightValue : rightValuesIterable) {            for (V1 leftValue : leftValuesIterable) {                c.output(KV.of(e.getKey(), KV.of(leftValue, rightValue)));            }        }    } else if (leftValuesIterable.iterator().hasNext() && !rightValuesIterable.iterator().hasNext()) {        for (V1 leftValue : leftValuesIterable) {            c.output(KV.of(e.getKey(), KV.of(leftValue, rightNullValue)));        }    } else if (!leftValuesIterable.iterator().hasNext() && rightValuesIterable.iterator().hasNext()) {        for (V2 rightValue : rightValuesIterable) {            c.output(KV.of(e.getKey(), KV.of(leftNullValue, rightValue)));        }    }}
public static PCollection<KV<K, KV<V1, V2>>> beam_f24812_0(final PCollection<KV<K, V1>> leftCollection, final PCollection<KV<K, V2>> rightCollection)
{    return innerJoin("InnerJoin", leftCollection, rightCollection);}
public void beam_f24820_0()
{    leftListOfKv = new ArrayList<>();    rightListOfKv = new ArrayList<>();    expectedResult = new ArrayList<>();}
public void beam_f24821_0()
{    leftListOfKv.add(KV.of("Key1", 5L));    leftListOfKv.add(KV.of("Key2", 4L));    PCollection<KV<String, Long>> leftCollection = p.apply("CreateLeft", Create.of(leftListOfKv));    rightListOfKv.add(KV.of("Key1", "foo"));    rightListOfKv.add(KV.of("Key2", "bar"));    PCollection<KV<String, String>> rightCollection = p.apply("CreateRight", Create.of(rightListOfKv));    PCollection<KV<String, KV<Long, String>>> output = Join.innerJoin(leftCollection, rightCollection);    expectedResult.add(KV.of("Key1", KV.of(5L, "foo")));    expectedResult.add(KV.of("Key2", KV.of(4L, "bar")));    PAssert.that(output).containsInAnyOrder(expectedResult);    p.run();}
public void beam_f24822_0()
{    leftListOfKv.add(KV.of("Key2", 4L));    PCollection<KV<String, Long>> leftCollection = p.apply("CreateLeft", Create.of(leftListOfKv));    rightListOfKv.add(KV.of("Key2", "bar"));    rightListOfKv.add(KV.of("Key2", "gazonk"));    PCollection<KV<String, String>> rightCollection = p.apply("CreateRight", Create.of(rightListOfKv));    PCollection<KV<String, KV<Long, String>>> output = Join.innerJoin(leftCollection, rightCollection);    expectedResult.add(KV.of("Key2", KV.of(4L, "bar")));    expectedResult.add(KV.of("Key2", KV.of(4L, "gazonk")));    PAssert.that(output).containsInAnyOrder(expectedResult);    p.run();}
public void beam_f24830_0()
{    leftListOfKv.add(KV.of("Key2", 4L));    PCollection<KV<String, Long>> leftCollection = p.apply("CreateLeft", Create.of(leftListOfKv));    rightListOfKv.add(KV.of("Key2", "bar"));    rightListOfKv.add(KV.of("Key2", "gazonk"));    PCollection<KV<String, String>> rightCollection = p.apply("CreateRight", Create.of(rightListOfKv));    PCollection<KV<String, KV<Long, String>>> output = Join.fullOuterJoin(leftCollection, rightCollection, -1L, "");    expectedResult.add(KV.of("Key2", KV.of(4L, "bar")));    expectedResult.add(KV.of("Key2", KV.of(4L, "gazonk")));    PAssert.that(output).containsInAnyOrder(expectedResult);    p.run();}
public void beam_f24831_0()
{    leftListOfKv.add(KV.of("Key2", 4L));    leftListOfKv.add(KV.of("Key2", 6L));    PCollection<KV<String, Long>> leftCollection = p.apply("CreateLeft", Create.of(leftListOfKv));    rightListOfKv.add(KV.of("Key2", "bar"));    PCollection<KV<String, String>> rightCollection = p.apply("CreateRight", Create.of(rightListOfKv));    PCollection<KV<String, KV<Long, String>>> output = Join.fullOuterJoin(leftCollection, rightCollection, -1L, "");    expectedResult.add(KV.of("Key2", KV.of(4L, "bar")));    expectedResult.add(KV.of("Key2", KV.of(6L, "bar")));    PAssert.that(output).containsInAnyOrder(expectedResult);    p.run();}
public void beam_f24832_0()
{    leftListOfKv.add(KV.of("Key2", 4L));    PCollection<KV<String, Long>> leftCollection = p.apply("CreateLeft", Create.of(leftListOfKv));    rightListOfKv.add(KV.of("Key3", "bar"));    PCollection<KV<String, String>> rightCollection = p.apply("CreateRight", Create.of(rightListOfKv));    PCollection<KV<String, KV<Long, String>>> output = Join.fullOuterJoin(leftCollection, rightCollection, -1L, "");    expectedResult.add(KV.of("Key2", KV.of(4L, "")));    expectedResult.add(KV.of("Key3", KV.of(-1L, "bar")));    PAssert.that(output).containsInAnyOrder(expectedResult);    p.run();}
public void beam_f24840_0()
{    leftListOfKv.add(KV.of("Key2", 4L));    PCollection<KV<String, Long>> leftCollection = p.apply("CreateLeft", Create.of(leftListOfKv));    rightListOfKv.add(KV.of("Key2", "bar"));    rightListOfKv.add(KV.of("Key2", "gazonk"));    PCollection<KV<String, String>> rightCollection = p.apply("CreateRight", Create.of(rightListOfKv));    PCollection<KV<String, KV<Long, String>>> output = Join.leftOuterJoin(leftCollection, rightCollection, "");    expectedResult.add(KV.of("Key2", KV.of(4L, "bar")));    expectedResult.add(KV.of("Key2", KV.of(4L, "gazonk")));    PAssert.that(output).containsInAnyOrder(expectedResult);    p.run();}
public void beam_f24841_0()
{    leftListOfKv.add(KV.of("Key2", 4L));    leftListOfKv.add(KV.of("Key2", 6L));    PCollection<KV<String, Long>> leftCollection = p.apply("CreateLeft", Create.of(leftListOfKv));    rightListOfKv.add(KV.of("Key2", "bar"));    PCollection<KV<String, String>> rightCollection = p.apply("CreateRight", Create.of(rightListOfKv));    PCollection<KV<String, KV<Long, String>>> output = Join.leftOuterJoin(leftCollection, rightCollection, "");    expectedResult.add(KV.of("Key2", KV.of(4L, "bar")));    expectedResult.add(KV.of("Key2", KV.of(6L, "bar")));    PAssert.that(output).containsInAnyOrder(expectedResult);    p.run();}
public void beam_f24842_0()
{    leftListOfKv.add(KV.of("Key2", 4L));    PCollection<KV<String, Long>> leftCollection = p.apply("CreateLeft", Create.of(leftListOfKv));    rightListOfKv.add(KV.of("Key3", "bar"));    PCollection<KV<String, String>> rightCollection = p.apply("CreateRight", Create.of(rightListOfKv));    PCollection<KV<String, KV<Long, String>>> output = Join.leftOuterJoin(leftCollection, rightCollection, "");    expectedResult.add(KV.of("Key2", KV.of(4L, "")));    PAssert.that(output).containsInAnyOrder(expectedResult);    p.run();}
public void beam_f24850_0()
{    leftListOfKv.add(KV.of("Key2", 4L));    leftListOfKv.add(KV.of("Key2", 6L));    PCollection<KV<String, Long>> leftCollection = p.apply("CreateLeft", Create.of(leftListOfKv));    rightListOfKv.add(KV.of("Key2", "bar"));    PCollection<KV<String, String>> rightCollection = p.apply("CreateRight", Create.of(rightListOfKv));    PCollection<KV<String, KV<Long, String>>> output = Join.rightOuterJoin(leftCollection, rightCollection, -1L);    expectedResult.add(KV.of("Key2", KV.of(4L, "bar")));    expectedResult.add(KV.of("Key2", KV.of(6L, "bar")));    PAssert.that(output).containsInAnyOrder(expectedResult);    p.run();}
public void beam_f24851_0()
{    leftListOfKv.add(KV.of("Key2", 4L));    PCollection<KV<String, Long>> leftCollection = p.apply("CreateLeft", Create.of(leftListOfKv));    rightListOfKv.add(KV.of("Key3", "bar"));    PCollection<KV<String, String>> rightCollection = p.apply("CreateRight", Create.of(rightListOfKv));    PCollection<KV<String, KV<Long, String>>> output = Join.rightOuterJoin(leftCollection, rightCollection, -1L);    expectedResult.add(KV.of("Key3", KV.of(-1L, "bar")));    PAssert.that(output).containsInAnyOrder(expectedResult);    p.run();}
public void beam_f24852_0()
{    leftListOfKv.add(KV.of("Key2", 4L));    PCollection<KV<String, Long>> leftCollection = p.apply("CreateLeft", Create.of(leftListOfKv));    rightListOfKv.add(KV.of("Key2", "bar"));    PCollection<KV<String, String>> rightCollection = p.apply("CreateRight", Create.of(rightListOfKv));    expectedResult.add(KV.of("Key2", KV.of(4L, "bar")));    PCollection<KV<String, KV<Long, String>>> output1 = Join.rightOuterJoin("Join1", leftCollection, rightCollection, -1L);    PCollection<KV<String, KV<Long, String>>> output2 = Join.rightOuterJoin("Join2", leftCollection, rightCollection, -1L);    PAssert.that(output1).containsInAnyOrder(expectedResult);    PAssert.that(output2).containsInAnyOrder(expectedResult);    p.run();}
public static KryoCoder<T> beam_f24860_0(PipelineOptions pipelineOptions, KryoRegistrar... registrars)
{    return of(pipelineOptions, Arrays.asList(registrars));}
public static KryoCoder<T> beam_f24861_0(PipelineOptions pipelineOptions, List<KryoRegistrar> registrars)
{    final KryoOptions kryoOptions = pipelineOptions.as(KryoOptions.class);    return new KryoCoder<>(new SerializableOptions(kryoOptions.getKryoBufferSize(), kryoOptions.getKryoReferences(), kryoOptions.getKryoRegistrationRequired()), registrars);}
 int beam_f24862_0()
{    return bufferSize;}
 List<KryoRegistrar> beam_f24871_0()
{    return registrars;}
public int beam_f24872_0()
{    return instanceId.hashCode();}
public boolean beam_f24873_0(Object other)
{    if (other != null && getClass().equals(other.getClass())) {        return instanceId.equals(((KryoCoder) other).instanceId);    }    return false;}
private boolean beam_f24881_0(TypeDescriptor<T> typeDescriptor)
{    final KryoState kryoState = KryoState.get(coder);    final Class<? super T> rawType = typeDescriptor.getRawType();    final Kryo kryo = kryoState.getKryo();    final ClassResolver classResolver = kryo.getClassResolver();    final Registration registration = classResolver.getRegistration(rawType);    return registration != null && registration.getId() >= kryoState.getFirstRegistrationId();}
public KryoCoderProvider beam_f24882_0(KryoRegistrar registrar)
{    return new KryoCoderProvider(coder.withRegistrar(registrar));}
public void beam_f24883_0(Pipeline pipeline)
{    pipeline.getCoderRegistry().registerCoderProvider(this);}
public void beam_f24891_0()
{    KryoCoderProvider provider = KryoCoderProvider.of(pipeline.getOptions());    assertNotNull(provider);}
public void beam_f24892_0()
{    KryoCoderProvider.of(pipeline.getOptions()).registerTo(pipeline);}
public void beam_f24893_0() throws CannotProvideCoderException
{    final KryoCoderProvider provider = KryoCoderProvider.of(pipeline.getOptions(), kryo -> {        kryo.register(FirstTestClass.class);        kryo.register(SecondTestClass.class);        kryo.register(ThirdTestClass.class);    });    assertProviderReturnsKryoCoderForClass(provider, FirstTestClass.class);    assertProviderReturnsKryoCoderForClass(provider, SecondTestClass.class);    assertProviderReturnsKryoCoderForClass(provider, ThirdTestClass.class);}
public void beam_f24901_0() throws IOException
{    final KryoRegistrar registrar = k -> {        k.register(ClassToBeEncoded.class);        k.register(TestClass.class);    };    final KryoCoder<ClassToBeEncoded> coder = KryoCoder.of(OPTIONS, registrar);    final KryoCoder<TestClass> secondCoder = KryoCoder.of(OPTIONS, registrar);    final ClassToBeEncoded originalValue = new ClassToBeEncoded("XyZ", 42, Double.NaN);    final TestClass secondOriginalValue = new TestClass("just a parameter");    final ByteArrayOutputStream outputStream = new ByteArrayOutputStream();    coder.encode(originalValue, outputStream);    secondCoder.encode(secondOriginalValue, outputStream);    final byte[] buf = outputStream.toByteArray();    final ByteArrayInputStream inputStream = new ByteArrayInputStream(buf);    final ClassToBeEncoded decodedValue = coder.decode(inputStream);    final TestClass secondDecodedValue = secondCoder.decode(inputStream);    assertNotNull(decodedValue);    assertEquals(originalValue, decodedValue);    assertNotNull(secondDecodedValue);    assertNotNull(secondDecodedValue.param);    assertEquals("just a parameter", secondDecodedValue.param);}
public void beam_f24902_0() throws IOException, ClassNotFoundException
{    final KryoRegistrar registrar = k -> k.register(ClassToBeEncoded.class);    final KryoCoder<ClassToBeEncoded> coder = KryoCoder.of(OPTIONS, registrar);    final ByteArrayOutputStream outStr = new ByteArrayOutputStream();    final ObjectOutputStream oss = new ObjectOutputStream(outStr);    oss.writeObject(coder);    oss.flush();    oss.close();    ObjectInputStream ois = new ObjectInputStream(new ByteArrayInputStream(outStr.toByteArray()));    @SuppressWarnings("unchecked")    KryoCoder<ClassToBeEncoded> coderDeserialized = (KryoCoder<ClassToBeEncoded>) ois.readObject();    assertEncoding(coder, coderDeserialized);}
public void beam_f24903_0() throws IOException
{    final KryoRegistrar registrar = k -> k.register(TestClass.class);    final ListCoder<Void> listCoder = ListCoder.of(VoidCoder.of());    final KvCoder<TestClass, List<Void>> kvCoder = KvCoder.of(KryoCoder.of(OPTIONS, registrar), listCoder);    final List<Void> inputValue = new ArrayList<>();    inputValue.add(null);    inputValue.add(null);    final ByteArrayOutputStream byteArrayOutputStream = new ByteArrayOutputStream();    final TestClass inputKey = new TestClass("something");    kvCoder.encode(KV.of(inputKey, inputValue), byteArrayOutputStream);    final KV<TestClass, List<Void>> decoded = kvCoder.decode(new ByteArrayInputStream(byteArrayOutputStream.toByteArray()));    assertNotNull(decoded);    assertNotNull(decoded.getKey());    assertEquals(inputKey, decoded.getKey());    assertNotNull(decoded.getValue());    assertEquals(inputValue, decoded.getValue());}
public int beam_f24911_0()
{    return Objects.hash(param);}
public void beam_f24912_0() throws IOException, ClassNotFoundException
{    final KryoCoder<?> coder = KryoCoder.of(k -> k.register(TestClass.class));    final KryoState firstKryo = KryoState.get(coder);    final ByteArrayOutputStream outStr = new ByteArrayOutputStream();    final ObjectOutputStream oss = new ObjectOutputStream(outStr);    oss.writeObject(coder);    oss.flush();    oss.close();    final ObjectInputStream ois = new ObjectInputStream(new ByteArrayInputStream(outStr.toByteArray()));    @SuppressWarnings("unchecked")    final KryoCoder<?> deserializedCoder = (KryoCoder) ois.readObject();    final KryoState secondKryo = KryoState.get(deserializedCoder);    assertSame(firstKryo, secondKryo);}
public static ByteStringCoder beam_f24913_0()
{    return INSTANCE;}
public TypeDescriptor<ByteString> beam_f24922_0()
{    return TYPE_DESCRIPTOR;}
public List<CoderProvider> beam_f24923_0()
{    return ImmutableList.of(CoderProviders.forCoder(TypeDescriptor.of(ByteString.class), ByteStringCoder.of()), ProtoCoder.getCoderProvider());}
 static Descriptor beam_f24924_0(Class<? extends Message> clazz)
{    try {        return (Descriptor) clazz.getMethod("getDescriptor").invoke(null);    } catch (IllegalAccessException | InvocationTargetException | NoSuchMethodException e) {        throw new IllegalArgumentException(e);    }}
public ProtoCoder<T> beam_f24932_0(Iterable<Class<?>> moreExtensionHosts)
{    for (Class<?> extensionHost : moreExtensionHosts) {                try {            Method registerAllExtensions = extensionHost.getDeclaredMethod("registerAllExtensions", ExtensionRegistry.class);            checkArgument(Modifier.isStatic(registerAllExtensions.getModifiers()), "Method registerAllExtensions() must be static");        } catch (NoSuchMethodException | SecurityException e) {            throw new IllegalArgumentException(String.format("Unable to register extensions for %s", extensionHost.getCanonicalName()), e);        }    }    return new ProtoCoder<>(protoMessageClass, new ImmutableSet.Builder<Class<?>>().addAll(extensionHostClasses).addAll(moreExtensionHosts).build());}
public ProtoCoder<T> beam_f24933_0(Class<?>... moreExtensionHosts)
{    return withExtensionsFrom(Arrays.asList(moreExtensionHosts));}
public void beam_f24934_0(T value, OutputStream outStream) throws IOException
{    encode(value, outStream, Context.NESTED);}
public Set<Class<?>> beam_f24942_0()
{    return extensionHostClasses;}
public ExtensionRegistry beam_f24943_0()
{    if (memoizedExtensionRegistry == null) {        ExtensionRegistry registry = ExtensionRegistry.newInstance();        for (Class<?> extensionHost : extensionHostClasses) {            try {                extensionHost.getDeclaredMethod("registerAllExtensions", ExtensionRegistry.class).invoke(null, registry);            } catch (IllegalAccessException | InvocationTargetException | NoSuchMethodException e) {                throw new IllegalStateException(e);            }        }        memoizedExtensionRegistry = registry.getUnmodifiable();    }    return memoizedExtensionRegistry;}
private Parser<T> beam_f24944_0()
{    if (memoizedParser == null) {        try {            @SuppressWarnings("unchecked")            T protoMessageInstance = (T) protoMessageClass.getMethod("getDefaultInstance").invoke(null);            @SuppressWarnings("unchecked")            Parser<T> tParser = (Parser<T>) protoMessageInstance.getParserForType();            memoizedParser = tParser;        } catch (IllegalAccessException | InvocationTargetException | NoSuchMethodException e) {            throw new IllegalArgumentException(e);        }    }    return memoizedParser;}
public void beam_f24952_0() throws Exception
{    thrown.expect(CoderException.class);    thrown.expectMessage("cannot encode a null ByteString");    CoderUtils.encodeToBase64(TEST_CODER, null);}
public void beam_f24953_0() throws Throwable
{    Coder<List<ByteString>> listCoder = ListCoder.of(TEST_CODER);    CoderProperties.coderDecodeEncodeContentsEqual(listCoder, TEST_VALUES);    CoderProperties.coderDecodeEncodeContentsInSameOrder(listCoder, TEST_VALUES);}
public void beam_f24954_0() throws Throwable
{    for (ByteString value : TEST_VALUES) {        byte[] encoded = CoderUtils.encodeToByteArray(TEST_CODER, value, Context.NESTED);        assertEquals(encoded.length, TEST_CODER.getEncodedElementByteSize(value));    }}
public void beam_f24962_0()
{    checkProto2Syntax(MessageA.class, ExtensionRegistry.getEmptyRegistry());    checkProto2Syntax(MessageB.class, ExtensionRegistry.getEmptyRegistry());    checkProto2Syntax(MessageC.class, ExtensionRegistry.getEmptyRegistry());    checkProto2Syntax(MessageWithMap.class, ExtensionRegistry.getEmptyRegistry());    checkProto2Syntax(ReferencesMessageWithMap.class, ExtensionRegistry.getEmptyRegistry());}
public void beam_f24963_0()
{        thrown.expect(IllegalArgumentException.class);    thrown.expectMessage(Any.class.getCanonicalName());    thrown.expectMessage("in file " + Any.getDescriptor().getFile().getName());    checkProto2Syntax(Any.class, ExtensionRegistry.getEmptyRegistry());}
public void beam_f24964_0()
{        thrown.expect(IllegalArgumentException.class);    thrown.expectMessage(Duration.class.getCanonicalName());    thrown.expectMessage("in file " + Duration.getDescriptor().getFile().getName());    checkProto2Syntax(Duration.class, ExtensionRegistry.getEmptyRegistry());}
public void beam_f24972_0() throws Exception
{    MessageA value = MessageA.newBuilder().setField1("hello").addField2(MessageB.newBuilder().setField1(true).build()).addField2(MessageB.newBuilder().setField1(false).build()).build();    CoderProperties.coderDecodeEncodeEqual(ProtoCoder.of(MessageA.class), value);}
public void beam_f24973_0() throws Exception
{    MessageA value1 = MessageA.newBuilder().setField1("hello").addField2(MessageB.newBuilder().setField1(true).build()).addField2(MessageB.newBuilder().setField1(false).build()).build();    MessageA value2 = MessageA.newBuilder().setField1("world").addField2(MessageB.newBuilder().setField1(false).build()).addField2(MessageB.newBuilder().setField1(true).build()).build();    CoderProperties.coderDecodeEncodeEqual(ListCoder.of(ProtoCoder.of(MessageA.class)), ImmutableList.of(value1, value2));}
public void beam_f24974_0() throws Exception
{    MessageC value = MessageC.newBuilder().setExtension(Proto2CoderTestMessages.field1, MessageA.newBuilder().setField1("hello").addField2(MessageB.newBuilder().setField1(true).build()).build()).setExtension(Proto2CoderTestMessages.field2, MessageB.newBuilder().setField1(false).build()).build();    CoderProperties.coderDecodeEncodeEqual(ProtoCoder.of(MessageC.class).withExtensionsFrom(Proto2CoderTestMessages.class), value);}
public static PerKeyDistinct<K, V> beam_f24982_0()
{    return PerKeyDistinct.<K, V>builder().build();}
 static Builder<InputT> beam_f24983_0()
{    return new AutoValue_ApproximateDistinct_GloballyDistinct.Builder<InputT>().setPrecision(12).setSparsePrecision(0);}
public GloballyDistinct<InputT> beam_f24984_0(int p)
{    return toBuilder().setPrecision(p).build();}
public ApproximateDistinctFn<InputT> beam_f24992_0(int p)
{    checkArgument(p >= 4, "Expected: p >= 4. Actual: p = %s", p);    return new ApproximateDistinctFn<>(p, this.sp, this.inputCoder);}
public ApproximateDistinctFn<InputT> beam_f24993_0(int sp)
{    checkArgument((sp > this.p && sp < 32) || (sp == 0), "Expected: p <= sp <= 32." + "Actual: p = %s, sp = %s", this.p, sp);    return new ApproximateDistinctFn<>(this.p, sp, this.inputCoder);}
public HyperLogLogPlus beam_f24994_0()
{    return new HyperLogLogPlus(p, sp);}
public boolean beam_f25002_0(HyperLogLogPlus value)
{    return true;}
protected long beam_f25003_0(HyperLogLogPlus value) throws IOException
{    if (value == null) {        throw new CoderException("cannot encode a null HyperLogLogPlus sketch");    }    return value.sizeof();}
private static DoFn<KV<K, HyperLogLogPlus>, KV<K, Long>> beam_f25004_0()
{    return new DoFn<KV<K, HyperLogLogPlus>, KV<K, Long>>() {        @ProcessElement        public void processElement(ProcessContext c) {            KV<K, HyperLogLogPlus> kv = c.element();            c.output(KV.of(kv.getKey(), kv.getValue().cardinality()));        }    };}
 static Builder<InputT> beam_f25012_0()
{    return new AutoValue_SketchFrequencies_GlobalSketch.Builder<InputT>().setRelativeError(0.01).setConfidence(0.999);}
public GlobalSketch<InputT> beam_f25013_0(double eps)
{    return toBuilder().setRelativeError(eps).build();}
public GlobalSketch<InputT> beam_f25014_0(double conf)
{    return toBuilder().setConfidence(conf).build();}
public Sketch<InputT> beam_f25022_0()
{    return Sketch.create(epsilon, confidence);}
public Sketch<InputT> beam_f25023_0(Sketch<InputT> accumulator, InputT element)
{    accumulator.add(element, inputCoder);    return accumulator;}
public Sketch<InputT> beam_f25024_0(Iterable<Sketch<InputT>> accumulators)
{    Iterator<Sketch<InputT>> it = accumulators.iterator();    Sketch<InputT> first = it.next();    CountMinSketch mergedSketches = first.sketch();    try {        while (it.hasNext()) {            mergedSketches = CountMinSketch.merge(mergedSketches, it.next().sketch());        }    } catch (FrequencyMergeException e) {                throw new IllegalStateException("The accumulators cannot be merged:" + e.getMessage());    }    return Sketch.create(mergedSketches);}
private long beam_f25032_0(T element, Coder<T> coder)
{    try {        byte[] elemBytes = CoderUtils.encodeToByteArray(coder, element);        return Hashing.murmur3_128().hashBytes(elemBytes).asLong();    } catch (CoderException e) {        throw new IllegalStateException("The input value cannot be encoded: " + e.getMessage(), e);    }}
public long beam_f25033_0(T element, Coder<T> coder)
{    return sketch().estimateCount(hashElement(element, coder));}
public void beam_f25034_0(Sketch<T> value, OutputStream outStream) throws IOException
{    if (value == null) {        throw new CoderException("cannot encode a null Count-min Sketch");    }    BYTE_ARRAY_CODER.encode(CountMinSketch.serialize(value.sketch()), outStream);}
public PCollection<MergingDigest> beam_f25042_0(PCollection<Double> input)
{    return input.apply("Compute T-Digest Structure", Combine.globally(TDigestQuantilesFn.create(this.compression())));}
 static Builder<K> beam_f25043_0()
{    return new AutoValue_TDigestQuantiles_PerKeyDigest.Builder<K>().setCompression(100);}
public PerKeyDigest<K> beam_f25044_0(double cf)
{    return toBuilder().setCompression(cf).build();}
public Coder<MergingDigest> beam_f25052_0(CoderRegistry registry, Coder inputCoder)
{    return new MergingDigestCoder();}
public void beam_f25053_0(DisplayData.Builder builder)
{    super.populateDisplayData(builder);    builder.add(DisplayData.item("compression", compression).withLabel("Compression factor"));}
public void beam_f25054_0(MergingDigest value, OutputStream outStream) throws IOException
{    if (value == null) {        throw new CoderException("cannot encode a null T-Digest sketch");    }    ByteBuffer buf = ByteBuffer.allocate(value.byteSize());    value.asBytes(buf);    BYTE_ARRAY_CODER.encode(buf.array(), outStream);}
public void beam_f25062_0() throws Exception
{    HyperLogLogPlus hllp = new HyperLogLogPlus(12, 18);    for (int i = 0; i < 10; i++) {        hllp.offer(i);    }    CoderProperties.coderDecodeEncodeEqual(ApproximateDistinct.HyperLogLogPlusCoder.of(), hllp);}
public void beam_f25063_0()
{    final ApproximateDistinctFn<Integer> fnWithPrecision = ApproximateDistinctFn.create(BigEndianIntegerCoder.of()).withPrecision(23);    assertThat(DisplayData.from(fnWithPrecision), hasDisplayItem("p", 23));    assertThat(DisplayData.from(fnWithPrecision), hasDisplayItem("sp", 0));}
public Void beam_f25064_0(Iterable<Long> input)
{    for (Long estimate : input) {        boolean isAccurate = Math.abs(estimate - expectedCard) / expectedCard < expectedError;        Assert.assertTrue("not accurate enough : \nExpected Cardinality : " + expectedCard + "\nComputed Cardinality : " + estimate, isAccurate);    }    return null;}
private static List<Double> beam_f25072_0()
{    List<Double> li = new ArrayList<>();    for (double i = 1D; i <= size; i++) {        li.add(i);    }    Collections.shuffle(li);    return li;}
public void beam_f25073_0()
{    PCollection<KV<Double, Double>> col = tp.apply(Create.of(stream)).apply(TDigestQuantiles.globally().withCompression(compression)).apply(ParDo.of(new RetrieveQuantiles(quantiles)));    PAssert.that("Verify Accuracy", col).satisfies(new VerifyAccuracy());    tp.run();}
public void beam_f25074_0()
{    PCollection<KV<Double, Double>> col = tp.apply(Create.of(stream)).apply(WithKeys.of(1)).apply(TDigestQuantiles.<Integer>perKey().withCompression(compression)).apply(Values.create()).apply(ParDo.of(new RetrieveQuantiles(quantiles)));    PAssert.that("Verify Accuracy", col).satisfies(new VerifyAccuracy());    tp.run();}
public Options beam_f25082_0(String tempLocation)
{    checkArgument(!tempLocation.startsWith("gs://"), "BufferedExternalSorter does not support GCS temporary location");    return new Options(tempLocation, memoryMB, sorterType);}
public String beam_f25083_0()
{    return tempLocation;}
public Options beam_f25084_0(int memoryMB)
{    checkArgument(memoryMB > 0, "memoryMB must be greater than zero");            checkArgument(memoryMB < 2048, "memoryMB must be less than 2048");    return new Options(tempLocation, memoryMB, sorterType);}
public Options beam_f25092_0(String tempLocation)
{    if (tempLocation.startsWith("gs://")) {        throw new IllegalArgumentException("Sorter doesn't support GCS temporary location.");    }    this.tempLocation = tempLocation;    return this;}
public String beam_f25093_0()
{    return tempLocation;}
public Options beam_f25094_0(int memoryMB)
{    checkArgument(memoryMB > 0, "memoryMB must be greater than zero");            checkArgument(memoryMB < 2048, "memoryMB must be less than 2048");    this.memoryMB = memoryMB;    return this;}
private void beam_f25102_0() throws IOException
{    if (!initialized) {        tempDir = new Path(options.getTempLocation(), "tmp" + UUID.randomUUID().toString());        paths = new Path[] { new Path(tempDir, "test.seq") };        JobConf conf = new JobConf();                conf.set("io.seqfile.local.dir", tempDir.toUri().getPath());        writer = SequenceFile.createWriter(conf, Writer.valueClass(BytesWritable.class), Writer.keyClass(BytesWritable.class), Writer.file(paths[0]), Writer.compression(CompressionType.NONE));        FileSystem fs = FileSystem.getLocal(conf);                fs.mkdirs(tempDir);        fs.deleteOnExit(tempDir);        sorter = new SequenceFile.Sorter(fs, new BytesWritable.Comparator(), BytesWritable.class, BytesWritable.class, conf);        sorter.setMemory(options.getMemoryMB() * 1024 * 1024);        initialized = true;    }}
public Iterator<KV<byte[], byte[]>> beam_f25103_0()
{    return new SortedRecordsIterator();}
public boolean beam_f25104_0()
{    return nextKV != null;}
public Iterable<KV<byte[], byte[]>> beam_f25112_0()
{    checkState(!sortCalled, "sort() can only be called once.");    sortCalled = true;    Comparator<KV<byte[], byte[]>> kvComparator = (o1, o2) -> COMPARATOR.compare(o1.getKey(), o2.getKey());    records.sort(kvComparator);    return Collections.unmodifiableList(records);}
private long beam_f25113_0(KV<byte[], byte[]> record)
{    return RECORD_MEMORY_OVERHEAD_ESTIMATE + record.getKey().length + record.getValue().length;}
private boolean beam_f25114_0(long numBytes, long numRecords)
{        return (numBytes + (numRecords * NUM_BYTES_PER_WORD * 2)) < maxBufferSize;}
private List<File> beam_f25122_1() throws IOException
{    final long fileSize = Files.size(dataFile.toPath());    final long memory = maxMemory > 0 ? maxMemory : estimateAvailableMemory();        final long blockSize = estimateBestBlockSize(fileSize, memory);        final List<File> files = new ArrayList<>();    InputStream inputStream = new BufferedInputStream(new FileInputStream(dataFile));    try {        final List<KV<byte[], byte[]>> tempList = new ArrayList<>();        KV<byte[], byte[]> kv = KV.of(null, null);        while (kv != null) {            long currentBlockSize = 0;            while ((currentBlockSize < blockSize) && (kv = readKeyValue(inputStream)) != null) {                                tempList.add(kv);                currentBlockSize += estimateSizeOf(kv);            }            files.add(sortAndSave(tempList));            tempList.clear();        }    } finally {        inputStream.close();    }    return files;}
private File beam_f25123_1(List<KV<byte[], byte[]>> tempList) throws IOException
{    final File tempFile = Files.createTempFile(tempDir, "sort", "seq").toFile();    tempFile.deleteOnExit();        tempList.sort(KV_COMPARATOR);    OutputStream outputStream = new BufferedOutputStream(new FileOutputStream(tempFile));    try {        for (KV<byte[], byte[]> kv : tempList) {            CODER.encode(kv.getKey(), outputStream);            CODER.encode(kv.getValue(), outputStream);        }    } finally {        outputStream.close();    }    return tempFile;}
private Iterable<KV<byte[], byte[]>> beam_f25124_0(List<File> files)
{    return () -> {        final List<Iterator<KV<byte[], byte[]>>> iterators = new ArrayList<>();        for (File file : files) {            try {                iterators.add(iterateFile(file));            } catch (FileNotFoundException e) {                throw new IllegalStateException(e);            }        }        return Iterators.mergeSorted(iterators, KV_COMPARATOR);    };}
private static long beam_f25132_0(final long sizeOfFile, final long maxMemory)
{            long blockSize = sizeOfFile / MAX_TEMP_FILES + (sizeOfFile % MAX_TEMP_FILES == 0 ? 0 : 1);        if (blockSize < maxMemory / 2) {        blockSize = maxMemory / 2;    }    return blockSize;}
private static long beam_f25133_0()
{            boolean is64BitJvm = true;                        String arch = System.getProperty("sun.arch.data.model");    if (arch != null && arch.contains("32")) {                is64BitJvm = false;    }                    long objectHeader = is64BitJvm ? 16 : 8;    long arrayHeader = is64BitJvm ? 24 : 12;    long objectRef = is64BitJvm ? 8 : 4;    return objectHeader + (objectRef + arrayHeader) * 2;}
private static long beam_f25134_0(KV<byte[], byte[]> kv)
{    return kv.getKey().length + kv.getValue().length + OBJECT_OVERHEAD;}
public boolean beam_f25142_0()
{    return iterator.hasNext();}
public KV<SecondaryKeyT, ValueT> beam_f25143_0()
{    KV<byte[], byte[]> next = iterator.next();    try {        return KV.of(CoderUtils.decodeFromByteArray(keyCoder, next.getKey()), CoderUtils.decodeFromByteArray(valueCoder, next.getValue()));    } catch (IOException e) {        throw new RuntimeException(e);    }}
public void beam_f25144_0()
{    throw new UnsupportedOperationException("Iterator does not support remove");}
public void beam_f25152_0() throws Exception
{    SorterTestUtils.testSingleElement(BufferedExternalSorter.create(BufferedExternalSorter.options().withTempLocation(tmpLocation.toString())));}
public void beam_f25153_0() throws Exception
{    SorterTestUtils.testEmptyKeyValueElement(BufferedExternalSorter.create(BufferedExternalSorter.options().withTempLocation(tmpLocation.toString())));}
public void beam_f25154_0() throws Exception
{    SorterTestUtils.testMultipleIterations(BufferedExternalSorter.create(BufferedExternalSorter.options().withTempLocation(tmpLocation.toString())));}
public static void beam_f25162_0(String[] args) throws IOException
{    File tempDirectory = Files.createTempDirectory("sorter").toFile();    tempDirectory.deleteOnExit();    ExternalSorter.Options options = new ExternalSorter.Options().setMemoryMB(32).setTempLocation(tempDirectory.toString());    options.setSorterType(SorterType.HADOOP);    benchmark(ExternalSorter.create(options));    options.setSorterType(SorterType.NATIVE);    benchmark(ExternalSorter.create(options));}
private static void beam_f25163_0(Sorter sorter) throws IOException
{    long start = System.currentTimeMillis();    for (int i = 0; i < N; i++) {        sorter.add(KV.of(UUID.randomUUID().toString().getBytes(Charset.defaultCharset()), UUID.randomUUID().toString().getBytes(Charset.defaultCharset())));    }    int i = 0;    for (KV<byte[], byte[]> ignored : sorter.sort()) {        i++;    }    long end = System.currentTimeMillis();    System.out.println(String.format("%s: %fs", sorter.getClass().getSimpleName(), (end - start) / 1000.0));}
public static void beam_f25164_0() throws IOException
{    tmpLocation = Files.createTempDirectory("tmp");}
public void beam_f25172_0() throws Exception
{    SorterTestUtils.testMultipleIterations(ExternalSorter.create(new ExternalSorter.Options().setTempLocation(tmpLocation.toString()).setSorterType(sorterType)));}
public void beam_f25173_0() throws Exception
{    SorterTestUtils.testRandom(() -> ExternalSorter.create(new ExternalSorter.Options().setTempLocation(tmpLocation.toString()).setSorterType(sorterType)), 1, 1000000);}
public void beam_f25174_0() throws Exception
{    SorterTestUtils.testAddAfterSort(ExternalSorter.create(new ExternalSorter.Options().setTempLocation(tmpLocation.toString()).setSorterType(sorterType)), thrown);    fail();}
public void beam_f25182_0() throws Exception
{    SorterTestUtils.testMultipleIterations(InMemorySorter.create(new InMemorySorter.Options()));}
public void beam_f25183_0() throws Exception
{    SorterTestUtils.testRandom(() -> InMemorySorter.create(new InMemorySorter.Options()), 1000000, 10);}
public void beam_f25184_0() throws Exception
{    SorterTestUtils.testAddAfterSort(InMemorySorter.create(new InMemorySorter.Options()), thrown);    fail();}
public static void beam_f25192_0(Sorter sorter) throws Exception
{    KV<byte[], byte[]> kv = KV.of(new byte[] { 4, 7 }, new byte[] { 1, 2 });    sorter.add(kv);    assertThat(sorter.sort(), contains(kv));}
public static void beam_f25193_0(Sorter sorter) throws Exception
{    KV<byte[], byte[]> kv = KV.of(new byte[] {}, new byte[] {});    sorter.add(kv);    assertThat(sorter.sort(), contains(kv));}
public static void beam_f25194_0(Sorter sorter) throws Exception
{    KV<byte[], byte[]>[] kvs = new KV[] { KV.of(new byte[] { 0 }, new byte[] {}), KV.of(new byte[] { 0, 1 }, new byte[] {}), KV.of(new byte[] { 1 }, new byte[] {}) };    sorter.add(kvs[1]);    sorter.add(kvs[2]);    sorter.add(kvs[0]);    Iterable<KV<byte[], byte[]>> sorted = sorter.sort();    assertThat(sorted, contains(kvs[0], kvs[1], kvs[2]));        assertThat(sorted, contains(kvs[0], kvs[1], kvs[2]));}
public void beam_f25202_0(Description description)
{    description.appendText("a KV(").appendValue(keyMatcher).appendText(", ").appendValue(valueMatcher).appendText(")");}
public static void beam_f25203_1(String[] args) throws Exception
{        DCExamplePipelineOptions options = PipelineOptionsFactory.fromArgs(args).as(DCExamplePipelineOptions.class);        Pipeline pipeline = Pipeline.create(options);    validateArgs(options);    pipeline.apply("SQL Query", SqlTransform.query(options.getQueryString()).withDefaultTableProvider("datacatalog", DataCatalogTableProvider.create(options.as(DataCatalogPipelineOptions.class)))).apply("Convert to Strings", rowsToStrings()).apply("Write output", TextIO.write().to(options.getOutputFilePrefix()));    pipeline.run().waitUntilFinish();}
private static MapElements<Row, String> beam_f25204_0()
{    return MapElements.into(TypeDescriptor.of(String.class)).via(row -> row.getValues().stream().map(String::valueOf).collect(Collectors.joining(", ")));}
public void beam_f25212_0(Table table)
{    throw new UnsupportedOperationException("Creating tables is not supported with DataCatalog table provider.");}
public void beam_f25213_0(String tableName)
{    throw new UnsupportedOperationException("Dropping tables is not supported with DataCatalog table provider");}
public Map<String, Table> beam_f25214_0()
{    throw new UnsupportedOperationException("Loading all tables from DataCatalog is not supported");}
 static Schema beam_f25222_0(com.google.cloud.datacatalog.Schema dcSchema)
{    return fromColumnsList(dcSchema.getColumnsList());}
private static Schema beam_f25223_0(List<ColumnSchema> columnsMap)
{    return columnsMap.stream().map(SchemaUtils::toBeamField).collect(toSchema());}
private static Field beam_f25224_0(ColumnSchema column)
{    String name = column.getColumn();        FieldType fieldType = getBeamFieldType(column);    Field field = Field.of(name, fieldType);        if (Strings.isNullOrEmpty(column.getMode()) || "NULLABLE".equals(column.getMode())) {        field = field.withNullable(true);    } else if ("REQUIRED".equals(column.getMode())) {        field = field.withNullable(false);    } else if ("REPEATED".equals(column.getMode())) {        field = Field.of(name, FieldType.array(fieldType));    } else {        throw new UnsupportedOperationException("Field mode '" + column.getMode() + "' is not supported (field '" + name + "')");    }    return field;}
private Row beam_f25232_0(long id, String name)
{    return Row.withSchema(ID_NAME_SCHEMA).addValues(id, name).build();}
private void beam_f25233_0(TableRow r1, TableRow r2, TableRow r3)
{    writeToBQPipeline.apply(Create.of(r1, r2, r3).withCoder(TableRowJsonCoder.of())).apply(BigQueryIO.writeTableRows().to(bigQuery.tableSpec()).withSchema(new TableSchema().setFields(ImmutableList.of(new TableFieldSchema().setName("id").setType("INTEGER"), new TableFieldSchema().setName("name").setType("STRING")))).withoutValidation());    writeToBQPipeline.run().waitUntilFinish(Duration.standardMinutes(2));}
public void beam_f25234_0()
{    List<String> id = Arrays.asList("aaa", "BbB", "zAzzz00");    assertEquals("aaa.BbB.zAzzz00", ZetaSqlIdUtils.escapeAndJoin(id));}
public void beam_f25242_0(Table table)
{    throw new UnsupportedOperationException("Creating tables is not supported in HCatalog");}
public void beam_f25243_0(String tableName)
{    throw new UnsupportedOperationException("Deleting tables is not supported in HCatalog");}
public Map<String, Table> beam_f25244_0()
{    throw new UnsupportedOperationException("Listing tables is not supported in HCatalog");}
 static Builder beam_f25252_0()
{    return new AutoValue_HCatalogTable.Builder();}
public static HCatalogTableProvider beam_f25253_0(Map<String, String> configuration)
{    HCatalogBeamSchema metastoreSchema = HCatalogBeamSchema.create(configuration);    return new HCatalogTableProvider(new HashMap<>(configuration), metastoreSchema, new DatabaseProvider("default", metastoreSchema, configuration));}
public String beam_f25254_0()
{    return "hcatalog";}
public static void beam_f25262_0() throws IOException
{    service = new EmbeddedMetastoreService(TMP_FOLDER.getRoot().getAbsolutePath());}
public static void beam_f25263_0() throws Exception
{    if (service != null) {        service.executeQuery("drop table " + TEST_TABLE);        service.close();    }}
public void beam_f25264_0() throws Exception
{    initializeHCatalog();    PCollection<KV<String, Integer>> output = readAfterWritePipeline.apply(SqlTransform.query(String.format("SELECT f_str, f_int FROM `hive`.`%s`.`%s`", TEST_DATABASE, TEST_TABLE)).withTableProvider("hive", hiveTableProvider())).apply(ParDo.of(new ToKV()));    PAssert.that(output).containsInAnyOrder(getExpectedRecordsAsKV(TEST_RECORDS_COUNT));    readAfterWritePipeline.run();}
private void beam_f25272_0() throws Exception
{    reCreateTestTable();    insertTestData(service.getHiveConfAsMap());}
private TableProvider beam_f25273_0()
{    return HCatalogTableProvider.create(service.getHiveConfAsMap());}
private Row beam_f25274_0(int fIntValue, String fStringValue)
{    return Row.withSchema(ROW_SCHEMA).addValues(fIntValue, fStringValue).build();}
public void beam_f25282_0()
{    pool = Executors.newFixedThreadPool(1);}
public void beam_f25283_0()
{    pool.shutdown();}
public void beam_f25284_0() throws Exception
{    String[] args = buildArgs(String.format(createPubsubTableStatement, eventsTopic.topicPath()), setProject, "SELECT event_timestamp, taxi_rides.payload.ride_status, taxi_rides.payload.latitude, " + "taxi_rides.payload.longitude from taxi_rides LIMIT 3;");    Future<List<List<String>>> expectedResult = runQueryInBackground(args);    eventsTopic.checkIfAnySubscriptionExists(project, Duration.standardMinutes(1));    List<PubsubMessage> messages = ImmutableList.of(message(convertTimestampToMillis("2018-07-01 21:25:20"), taxiRideJSON("id1", 1, 40.702, -74.001, 1000, 10, "enroute", 2)), message(convertTimestampToMillis("2018-07-01 21:26:06"), taxiRideJSON("id2", 2, 40.703, -74.002, 1000, 10, "enroute", 4)), message(convertTimestampToMillis("2018-07-02 13:26:06"), taxiRideJSON("id3", 3, 30.0, -72.32324, 2000, 20, "enroute", 7)));    eventsTopic.publish(messages);    assertThat(Arrays.asList(Arrays.asList("2018-07-01 21:25:20", "enroute", "40.702", "-74.001"), Arrays.asList("2018-07-01 21:26:06", "enroute", "40.703", "-74.002"), Arrays.asList("2018-07-02 13:26:06", "enroute", "30.0", "-72.32324")), everyItem(IsIn.isOneOf(expectedResult.get(30, TimeUnit.SECONDS).toArray())));}
public void beam_f25292_0() throws Exception
{    BeamSqlLine.main(new String[] { "-e", "SELECT 1;" });}
public void beam_f25293_0() throws Exception
{    BeamSqlLine.main(new String[] { "-e", "SELECT 'beam';" });}
public void beam_f25294_0() throws Exception
{    BeamSqlLine.main(new String[] { "-e", "CREATE EXTERNAL TABLE test (id INTEGER) TYPE 'text';", "-e", "DROP TABLE test;" });}
public static String[] beam_f25302_0(String... strs)
{    List<String> argsList = new ArrayList();    for (String str : strs) {        argsList.add(QUERY_ARG);        argsList.add(str);    }    return argsList.toArray(new String[argsList.size()]);}
public static List<List<String>> beam_f25303_0(ByteArrayOutputStream outputStream)
{    List<String> outputLines = Arrays.asList(outputStream.toString().split("\n"));    return outputLines.stream().map(BeamSqlLineTestingUtils::splitFields).collect(toList());}
private static List<String> beam_f25304_0(String outputLine)
{    return Arrays.stream(outputLine.split("\\|")).map(field -> field.trim()).filter(field -> field.length() != 0).collect(toList());}
public MetaStore beam_f25312_0()
{    return metaStore;}
public String beam_f25313_0(String sqlString) throws ParseException
{    return env.explain(sqlString);}
public void beam_f25314_0(String sqlString) throws ParseException
{    if (env.isDdl(sqlString)) {        env.executeDdl(sqlString);    } else {        PipelineOptions options = BeamEnumerableConverter.createPipelineOptions(env.getPipelineOptions());        options.setJobName("BeamPlanCreator");        Pipeline pipeline = Pipeline.create(options);        BeamSqlRelUtils.toPCollection(pipeline, env.parseQuery(sqlString));        pipeline.run();    }}
private static PCollection<Order> beam_f25324_0(Pipeline pipeline)
{    return pipeline.apply(Create.of(new Order(1, 5), new Order(2, 2), new Order(3, 1), new Order(4, 3), new Order(5, 1), new Order(6, 5), new Order(7, 4), new Order(8, 4), new Order(9, 1)));}
public String beam_f25325_0()
{    return name;}
public int beam_f25326_0()
{    return id;}
public int beam_f25334_0()
{    return customerId;}
public void beam_f25335_0(int id)
{    this.id = id;}
public void beam_f25336_0(int customerId)
{    this.customerId = customerId;}
public boolean beam_f25344_0()
{    return true;}
public Schema beam_f25345_0(SchemaVersion version)
{    return this;}
public Expression beam_f25346_0(SchemaPlus parentSchema, String name)
{    return Schemas.subSchemaExpression(parentSchema, name, getClass());}
public Schema beam_f25354_0(String name)
{    if (!subSchemas.containsKey(name)) {        TableProvider subProvider = tableProvider.getSubProvider(name);        BeamCalciteSchema subSchema = subProvider == null ? null : new BeamCalciteSchema(connection, subProvider);        subSchemas.put(name, subSchema);    }    return subSchemas.get(name);}
 static TableProvider beam_f25355_0(JdbcConnection jdbcConnection)
{    InitialEmptySchema initialEmptySchema = jdbcConnection.getCurrentBeamSchema();    return initialEmptySchema.getTableProvider();}
public TableProvider beam_f25356_0()
{    MetaStore metaStore = new InMemoryMetaStore();    for (TableProvider provider : ServiceLoader.load(TableProvider.class, getClass().getClassLoader())) {        metaStore.registerProvider(provider);    }    return metaStore;}
public Collection<Function> beam_f25364_0(String name)
{    return illegal();}
public Set<String> beam_f25365_0()
{    return illegal();}
public Schema beam_f25366_0(String name)
{    return illegal();}
private PipelineOptions beam_f25374_0()
{    if (pipelineOptions != null) {        return pipelineOptions;    }    pipelineOptions = BeamEnumerableConverter.createPipelineOptions(pipelineOptionsMap);    return pipelineOptions;}
public BeamTableStatistics beam_f25375_0()
{    /*     Changing class loader is required for the JDBC path. It is similar to what done in     {@link BeamEnumerableConverter#toRowList} and {@link BeamEnumerableConverter#toEnumerable }.    */    final ClassLoader originalClassLoader = Thread.currentThread().getContextClassLoader();    try {        Thread.currentThread().setContextClassLoader(BeamEnumerableConverter.class.getClassLoader());        return beamTable.getTableStatistics(getPipelineOptions());    } finally {        Thread.currentThread().setContextClassLoader(originalClassLoader);    }}
public RelNode beam_f25376_0(RelOptTable.ToRelContext context, RelOptTable relOptTable)
{    return new BeamIOSourceRel(context.getCluster(), relOptTable, beamTable, pipelineOptionsMap, this);}
public BeamRelNode beam_f25384_0(String query) throws ParseException
{    return planner.convertToBeamRel(query);}
public boolean beam_f25385_0(String sqlStatement) throws ParseException
{    return planner.parse(sqlStatement) instanceof SqlExecutableStatement;}
public void beam_f25386_0(String sqlStatement) throws ParseException
{    SqlExecutableStatement ddl = (SqlExecutableStatement) planner.parse(sqlStatement);    ddl.execute(getContext());}
public BeamSqlEnvBuilder beam_f25394_0(String functionName, Class<? extends BeamSqlUdf> clazz)
{    return addUdf(functionName, clazz, BeamSqlUdf.UDF_METHOD);}
public BeamSqlEnvBuilder beam_f25395_0(String functionName, SerializableFunction sfn)
{    return addUdf(functionName, sfn.getClass(), "apply");}
public BeamSqlEnvBuilder beam_f25396_0(String functionName, CombineFn combineFn)
{    functionSet.add(new SimpleEntry<>(functionName, new UdafImpl(combineFn)));    return this;}
private void beam_f25404_0(Map<String, List<Method>> methods)
{    for (Map.Entry<String, List<Method>> entry : methods.entrySet()) {        for (Method method : entry.getValue()) {            functionSet.add(new SimpleEntry<>(entry.getKey(), UdfImpl.create(method)));        }    }}
private void beam_f25405_0()
{    if (!autoLoadUdfs) {        return;    }    ServiceLoader.load(UdfUdafProvider.class).forEach(ins -> {        ins.getBeamSqlUdfs().forEach(this::addUdf);        ins.getSerializableFunctionUdfs().forEach(this::addUdf);        ins.getUdafs().forEach(this::addUdaf);    });}
private void beam_f25406_0(JdbcConnection connection)
{    for (Map.Entry<String, Function> functionEntry : functionSet) {        connection.getCurrentSchemaPlus().add(functionEntry.getKey(), functionEntry.getValue());    }}
public boolean beam_f25414_0(ImmutableBitSet columns)
{    return false;}
public List<RelReferentialConstraint> beam_f25415_0()
{    return ImmutableList.of();}
public List<RelCollation> beam_f25416_0()
{    return ImmutableList.of();}
public CallableStatement beam_f25424_0(String sql) throws SQLException
{    return connection.prepareCall(sql);}
public String beam_f25425_0(String sql) throws SQLException
{    return connection.nativeSQL(sql);}
public void beam_f25426_0(boolean autoCommit) throws SQLException
{    connection.setAutoCommit(autoCommit);}
public boolean beam_f25434_0() throws SQLException
{    return connection.isReadOnly();}
public void beam_f25435_0(String catalog) throws SQLException
{    connection.setCatalog(catalog);}
public String beam_f25436_0() throws SQLException
{    return connection.getCatalog();}
public Map<String, Class<?>> beam_f25444_0() throws SQLException
{    return connection.getTypeMap();}
public void beam_f25445_0(Map<String, Class<?>> map) throws SQLException
{    connection.setTypeMap(map);}
public void beam_f25446_0(int holdability) throws SQLException
{    connection.setHoldability(holdability);}
public CallableStatement beam_f25454_0(String sql, int resultSetType, int resultSetConcurrency, int resultSetHoldability) throws SQLException
{    return connection.prepareCall(sql, resultSetType, resultSetConcurrency, resultSetHoldability);}
public PreparedStatement beam_f25455_0(String sql, int autoGeneratedKeys) throws SQLException
{    return connection.prepareStatement(sql, autoGeneratedKeys);}
public PreparedStatement beam_f25456_0(String sql, int[] columnIndexes) throws SQLException
{    return connection.prepareStatement(sql, columnIndexes);}
public void beam_f25464_0(Properties properties) throws SQLClientInfoException
{    connection.setClientInfo(properties);}
public String beam_f25465_0(String name) throws SQLException
{    return connection.getClientInfo(name);}
public Properties beam_f25466_0() throws SQLException
{    return connection.getClientInfo();}
public CalciteConnectionConfig beam_f25474_0()
{    return connection.config();}
public CalcitePrepare.Context beam_f25475_0()
{    return connection.createPrepareContext();}
public T beam_f25476_0(Class<T> iface) throws SQLException
{    return connection.unwrap(iface);}
public AvaticaStatement beam_f25484_0(AvaticaConnection connection, Meta.StatementHandle h, int resultSetType, int resultSetConcurrency, int resultSetHoldability) throws SQLException
{    return this.factory.newStatement(connection, h, resultSetType, resultSetConcurrency, resultSetHoldability);}
public AvaticaPreparedStatement beam_f25485_0(AvaticaConnection connection, Meta.StatementHandle h, Meta.Signature signature, int resultSetType, int resultSetConcurrency, int resultSetHoldability) throws SQLException
{    return this.factory.newPreparedStatement(connection, h, signature, resultSetType, resultSetConcurrency, resultSetHoldability);}
public AvaticaResultSet beam_f25486_0(AvaticaStatement statement, QueryState state, Meta.Signature signature, TimeZone timeZone, Meta.Frame firstFrame) throws SQLException
{    return this.factory.newResultSet(statement, state, signature, timeZone, firstFrame);}
 static JdbcConnection beam_f25494_0(CalciteConnection connection)
{    try {        if (connection == null) {            return null;        }        JdbcConnection jdbcConnection = new JdbcConnection(connection);        jdbcConnection.setPipelineOptionsMap(extractPipelineOptions(connection));        jdbcConnection.setSchema(connection.getSchema(), BeamCalciteSchemaFactory.fromInitialEmptySchema(jdbcConnection));        return jdbcConnection;    } catch (SQLException e) {        throw new RuntimeException(e);    }}
private static Map<String, String> beam_f25495_0(CalciteConnection calciteConnection)
{    return calciteConnection.getProperties().entrySet().stream().map(entry -> KV.of(entry.getKey().toString(), entry.getValue().toString())).filter(kv -> kv.getKey().startsWith(PIPELINE_OPTION_PREFIX)).map(kv -> KV.of(kv.getKey().substring(PIPELINE_OPTION_PREFIX.length()), kv.getValue())).collect(Collectors.toMap(KV::getKey, KV::getValue));}
 Map<String, String> beam_f25496_0()
{    return pipelineOptionsMap;}
protected String beam_f25504_0()
{    return CONNECT_STRING_PREFIX;}
public Connection beam_f25505_0(String url, Properties info) throws SQLException
{        return JdbcConnection.initialize((CalciteConnection) super.connect(url, info));}
public static JdbcConnection beam_f25506_0(TableProvider tableProvider, PipelineOptions options)
{    try {        Properties properties = new Properties();        properties.setProperty(SCHEMA_FACTORY.camelName(), BeamCalciteSchemaFactory.Empty.class.getName());        JdbcConnection connection = (JdbcConnection) INSTANCE.connect(CONNECT_STRING_PREFIX, properties);        connection.setSchema(TOP_LEVEL_BEAM_SCHEMA, tableProvider);        connection.setPipelineOptions(options);        return connection;    } catch (SQLException e) {        throw new RuntimeException(e);    }}
public void beam_f25514_0(SqlWriter writer, int leftPrec, int rightPrec)
{    if (name != null) {        writer.keyword("CONSTRAINT");        name.unparse(writer, 0, 0);    }    writer.keyword("CHECK");    if (writer.isAlwaysUseParentheses()) {        expression.unparse(writer, 0, 0);    } else {        writer.sep("(");        expression.unparse(writer, 0, 0);        writer.sep(")");    }}
public SqlOperator beam_f25515_0()
{    return OPERATOR;}
public List<SqlNode> beam_f25516_0()
{    return ImmutableList.of(name, dataType);}
public static SqlNode beam_f25524_0(SqlParserPos pos, SqlIdentifier name, SqlDataTypeSpec dataType, SqlNode comment)
{    return new SqlColumnDeclaration(pos, name, dataType, comment);}
 static Pair<CalciteSchema, String> beam_f25525_0(CalcitePrepare.Context context, boolean mutable, SqlIdentifier id)
{    final List<String> path;    if (id.isSimple()) {        path = context.getDefaultSchemaPath();    } else {        path = Util.skipLast(id.names);    }    CalciteSchema schema = mutable ? context.getMutableRootSchema() : context.getRootSchema();    for (String p : path) {        schema = schema.getSubSchema(p, true);    }    return Pair.of(schema, name(id));}
 static String beam_f25526_0(SqlIdentifier id)
{    if (id.isSimple()) {        return id.getSimple();    } else {        return Util.last(id.names);    }}
public String beam_f25534_0()
{    return "{0}";}
public String beam_f25535_0()
{    return "{tiny}";}
public double beam_f25536_0()
{    return cpu;}
public int beam_f25544_0()
{    return Objects.hash(cpu, cpuRate);}
public boolean beam_f25545_0(RelOptCost other)
{    return other instanceof BeamCostModel && (this.cpu == ((BeamCostModel) other).cpu) && (this.cpuRate == ((BeamCostModel) other).cpuRate);}
public boolean beam_f25546_0(Object obj)
{    if (obj instanceof BeamCostModel) {        return equals((BeamCostModel) obj);    }    return false;}
public BeamCostModel beam_f25554_0(double dRows, double dCpu, double dIo)
{    return BeamCostModel.INFINITY;}
public BeamCostModel beam_f25555_0(double dCpu, double dCpuRate)
{    return new BeamCostModel(dCpu, dCpuRate);}
public BeamCostModel beam_f25556_0()
{    return BeamCostModel.HUGE;}
public static RuleSet[] beam_f25564_0()
{    return new RuleSet[] { RuleSets.ofList(ImmutableList.<RelOptRule>builder().addAll(BEAM_CONVERTERS).addAll(BEAM_TO_ENUMERABLE).addAll(LOGICAL_OPTIMIZATIONS).build()) };}
public static NodeStats beam_f25565_0(double rowCount, double rate, double window)
{    if (window < 0 || rate < 0 || rowCount < 0) {        throw new IllegalArgumentException("All the estimates in NodeStats should be positive");    }    return new AutoValue_NodeStats(rowCount, rate, window);}
public static NodeStats beam_f25566_0(double rowCount)
{    return create(rowCount, 0d, rowCount);}
public BeamCostModel beam_f25574_0(RelOptPlanner planner, RelMetadataQuery mq)
{    NodeStats inputStat = BeamSqlRelUtils.getNodeStats(this.input, mq);    inputStat = computeWindowingCostEffect(inputStat);        float multiplier = 1f + (float) aggCalls.size() * 0.125f;    for (AggregateCall aggCall : aggCalls) {        if (aggCall.getAggregation().getName().equals("SUM")) {                                    multiplier += 0.0125f;        }    }    return BeamCostModel.FACTORY.makeCost(inputStat.getRowCount() * multiplier, inputStat.getRate() * multiplier);}
public NodeStats beam_f25575_0(RelMetadataQuery mq)
{    NodeStats inputEstimate = BeamSqlRelUtils.getNodeStats(this.input, mq);    inputEstimate = computeWindowingCostEffect(inputEstimate);    NodeStats estimate;        int groupCount = groupSet.cardinality() - (windowFn == null ? 0 : 1);        return (groupCount == 0) ? NodeStats.create(Math.min(inputEstimate.getRowCount(), 1d), inputEstimate.getRate() / inputEstimate.getWindow(), 1d) : inputEstimate.multiply(1.0 - Math.pow(.5, groupCount));}
private NodeStats beam_f25576_0(NodeStats inputStat)
{    if (windowFn == null) {        return inputStat;    }    WindowFn w = windowFn;    double multiplicationFactor = 1;        if (w instanceof SlidingWindows) {        multiplicationFactor = ((double) ((SlidingWindows) w).getSize().getStandardSeconds()) / ((SlidingWindows) w).getPeriod().getStandardSeconds();    }    return NodeStats.create(inputStat.getRowCount() * multiplicationFactor, inputStat.getRate() * multiplicationFactor, BeamIOSourceRel.CONSTANT_WINDOW_SIZE);}
public Aggregate beam_f25584_0(RelTraitSet traitSet, RelNode input, ImmutableBitSet groupSet, List<ImmutableBitSet> groupSets, List<AggregateCall> aggCalls)
{    return new BeamAggregationRel(getCluster(), traitSet, input, indicator, groupSet, groupSets, aggCalls, windowFn, windowFieldIndex);}
public Calc beam_f25585_0(RelTraitSet traitSet, RelNode input, RexProgram program)
{    return new BeamCalcRel(getCluster(), traitSet, input, program);}
public PTransform<PCollectionList<Row>, PCollection<Row>> beam_f25586_0()
{    return new Transform();}
public void beam_f25594_0()
{    this.se = compile();}
public void beam_f25595_0(ProcessContext c)
{    assert se != null;    try {        se.evaluate(new Object[] { outputSchema, c, CONTEXT_INSTANCE });    } catch (InvocationTargetException e) {        throw new RuntimeException("CalcFn failed to evaluate: " + processElementBlock, e.getCause());    }}
private Expression beam_f25596_0(Expression value, FieldType toType)
{    if (value.getType() == Object.class || !(value.getType() instanceof Class)) {                return value;    } else if (CalciteUtils.isDateTimeType(toType) && !Types.isAssignableFrom(ReadableInstant.class, (Class) value.getType())) {        return castOutputTime(value, toType);    } else if (toType.getTypeName() == TypeName.DECIMAL && !Types.isAssignableFrom(BigDecimal.class, (Class) value.getType())) {        return Expressions.new_(BigDecimal.class, value);    } else if (toType.getTypeName() == TypeName.BYTES && Types.isAssignableFrom(ByteString.class, (Class) value.getType())) {        return Expressions.condition(Expressions.equal(value, Expressions.constant(null)), Expressions.constant(null), Expressions.call(value, "getBytes"));    } else if (((Class) value.getType()).isPrimitive() || Types.isAssignableFrom(Number.class, (Class) value.getType())) {        Type rawType = rawTypeMap.get(toType.getTypeName());        if (rawType != null) {            return Types.castIfNecessary(rawType, value);        }    }    return value;}
private static Expression beam_f25604_0(Expression field, Expression ifNotNull)
{    return Expressions.condition(Expressions.equal(field, Expressions.constant(null)), Expressions.constant(null), Expressions.box(ifNotNull));}
public SchemaPlus beam_f25605_0()
{    return null;}
public JavaTypeFactory beam_f25606_0()
{    return null;}
public int beam_f25614_0()
{    return values.size();}
public PTransform<PCollectionList<Row>, PCollection<Row>> beam_f25615_0()
{    return new StandardJoin();}
public PCollection<Row> beam_f25616_0(PCollectionList<Row> pinput)
{    Schema leftSchema = CalciteUtils.toSchema(left.getRowType());    Schema rightSchema = CalciteUtils.toSchema(right.getRowType());    PCollectionList<KV<Row, Row>> keyedInputs = pinput.apply(new ExtractJoinKeys());    PCollection<KV<Row, Row>> extractedLeftRows = keyedInputs.get(0);    PCollection<KV<Row, Row>> extractedRightRows = keyedInputs.get(1);    WindowFn leftWinFn = extractedLeftRows.getWindowingStrategy().getWindowFn();    WindowFn rightWinFn = extractedRightRows.getWindowingStrategy().getWindowFn();    try {        leftWinFn.verifyCompatibility(rightWinFn);    } catch (IncompatibleWindowException e) {        throw new IllegalArgumentException("WindowFns must match for a bounded-vs-bounded/unbounded-vs-unbounded join.", e);    }    verifySupportedTrigger(extractedLeftRows);    verifySupportedTrigger(extractedRightRows);    return standardJoin(extractedLeftRows, extractedRightRows, leftSchema, rightSchema);}
public static Enumerable<Object> beam_f25624_0(BeamRelNode node)
{    final ClassLoader originalClassLoader = Thread.currentThread().getContextClassLoader();    try {        Thread.currentThread().setContextClassLoader(BeamEnumerableConverter.class.getClassLoader());        final PipelineOptions options = createPipelineOptions(node.getPipelineOptions());        return toEnumerable(options, node);    } finally {        Thread.currentThread().setContextClassLoader(originalClassLoader);    }}
public static List<Row> beam_f25625_0(BeamRelNode node)
{    final ClassLoader originalClassLoader = Thread.currentThread().getContextClassLoader();    try {        Thread.currentThread().setContextClassLoader(BeamEnumerableConverter.class.getClassLoader());        final PipelineOptions options = createPipelineOptions(node.getPipelineOptions());        return toRowList(options, node);    } finally {        Thread.currentThread().setContextClassLoader(originalClassLoader);    }}
public static PipelineOptions beam_f25626_0(Map<String, String> map)
{    final String[] args = new String[map.size()];    int i = 0;    for (Map.Entry<String, String> entry : map.entrySet()) {        args[i++] = "--" + entry.getKey() + "=" + entry.getValue();    }    PipelineOptions options = PipelineOptionsFactory.fromArgs(args).withValidation().create();    options.as(ApplicationNameOptions.class).setAppName("BeamSql");    return options;}
public void beam_f25634_0(ProcessContext context)
{    values.add(context.element());}
private static List<Object> beam_f25635_0(Queue<Row> values)
{    return values.stream().map(row -> {        Object[] objects = rowToAvatica(row);        if (objects.length == 1) {                        return objects[0];        } else {            return objects;        }    }).collect(Collectors.toList());}
private static Object[] beam_f25636_0(Row row)
{    Schema schema = row.getSchema();    Object[] convertedColumns = new Object[schema.getFields().size()];    int i = 0;    for (Schema.Field field : schema.getFields()) {        convertedColumns[i] = fieldToAvatica(field.getType(), row.getValue(i));        ++i;    }    return convertedColumns;}
public SetOp beam_f25644_0(RelTraitSet traitSet, List<RelNode> inputs, boolean all)
{    return new BeamIntersectRel(getCluster(), traitSet, inputs, all);}
public PTransform<PCollectionList<Row>, PCollection<Row>> beam_f25645_0()
{    return new BeamSetOperatorRelBase(this, BeamSetOperatorRelBase.OpType.INTERSECT, all);}
public NodeStats beam_f25646_0(RelMetadataQuery mq)
{        double minimumRows = Double.POSITIVE_INFINITY;    double minimumWindowSize = Double.POSITIVE_INFINITY;    double minimumRate = Double.POSITIVE_INFINITY;    for (RelNode input : inputs) {        NodeStats inputEstimates = BeamSqlRelUtils.getNodeStats(input, mq);        minimumRows = Math.min(minimumRows, inputEstimates.getRowCount());        minimumRate = Math.min(minimumRate, inputEstimates.getRate());        minimumWindowSize = Math.min(minimumWindowSize, inputEstimates.getWindow());    }    return NodeStats.create(minimumRows, minimumRate, minimumWindowSize).multiply(0.5);}
public PCollection<Row> beam_f25654_0(PCollectionList<Row> pinput)
{    checkArgument(pinput.size() == 1, "Wrong number of inputs for %s: %s", BeamIOSinkRel.class.getSimpleName(), pinput);    PCollection<Row> input = pinput.get(0);    sqlTable.buildIOWriter(input);    return input;}
public Map<String, String> beam_f25655_0()
{    return pipelineOptions;}
public double beam_f25656_0(RelMetadataQuery mq)
{    BeamTableStatistics rowCountStatistics = calciteTable.getStatistic();    if (beamTable.isBounded() == PCollection.IsBounded.BOUNDED) {        return rowCountStatistics.getRowCount();    } else {        return rowCountStatistics.getRate();    }}
public Map<String, String> beam_f25664_0()
{    return pipelineOptions;}
public List<RelNode> beam_f25665_0()
{    if (isSideInputLookupJoin()) {        return ImmutableList.of(BeamSqlRelUtils.getBeamRelInput(getInputs().get(nonSeekableInputIndex().get())));    } else {        return BeamRelNode.super.getPCollectionInputs();    }}
protected boolean beam_f25666_0()
{    return seekableInputIndex().isPresent() && nonSeekableInputIndex().isPresent();}
protected Schema beam_f25674_0(Schema schema)
{    Schema.Builder builder = Schema.builder();    builder.addFields(schema.getFields().stream().map(f -> f.withNullable(true)).collect(Collectors.toList()));    return builder.build();}
protected static PCollection<KV<K, V>> beam_f25675_0(PCollection<KV<K, V>> kvs, Coder<V> valueCoder)
{        KvCoder<K, V> coder = (KvCoder<K, V>) kvs.getCoder();    return kvs.setCoder(KvCoder.of(coder.getKeyCoder(), valueCoder));}
private static Field beam_f25676_0(Schema schema, RexNode rexNode, int leftRowColumnCount)
{    if (rexNode instanceof RexInputRef) {        return schema.getField(((RexInputRef) rexNode).getIndex() - leftRowColumnCount);    } else if (rexNode instanceof RexFieldAccess) {                return getFieldBasedOnRexFieldAccess(schema, (RexFieldAccess) rexNode, leftRowColumnCount);    }    throw new UnsupportedOperationException("Does not support " + rexNode.getType() + " in JOIN.");}
public Class beam_f25684_0()
{    return BeamRelNode.class;}
public String beam_f25685_0()
{    return "BEAM_LOGICAL";}
public RelTraitDef beam_f25686_0()
{    return ConventionTraitDef.INSTANCE;}
public NodeStats beam_f25695_0(RelMetadataQuery mq)
{    NodeStats firstInputEstimates = BeamSqlRelUtils.getNodeStats(inputs.get(0), mq);        for (int i = 1; i < inputs.size(); i++) {        NodeStats inputEstimate = BeamSqlRelUtils.getNodeStats(inputs.get(i), mq);        firstInputEstimates = firstInputEstimates.minus(inputEstimate.multiply(0.5));    }    return firstInputEstimates;}
 PCollection.IsBounded beam_f25696_0()
{    return getPCollectionInputs().stream().allMatch(rel -> BeamSqlRelUtils.getBeamRelInput(rel).isBounded() == PCollection.IsBounded.BOUNDED) ? PCollection.IsBounded.BOUNDED : PCollection.IsBounded.UNBOUNDED;}
 List<RelNode> beam_f25697_0()
{    return getInputs();}
public PTransform<PCollectionList<Row>, PCollection<Row>> beam_f25705_0()
{        if (joinType == JoinRelType.FULL) {        throw new UnsupportedOperationException("FULL OUTER JOIN is not supported when join " + "a Seekable table with a non Seekable table.");    }    if ((joinType == JoinRelType.LEFT && seekableInputIndex().get() == 0) || (joinType == JoinRelType.RIGHT && seekableInputIndex().get() == 1)) {        throw new UnsupportedOperationException(String.format("%s side of an OUTER JOIN must be a non Seekable table.", joinType.name()));    }    return new SideInputLookupJoin();}
public PCollection<Row> beam_f25706_0(PCollectionList<Row> pinput)
{    Schema schema = CalciteUtils.toSchema(getRowType());    BeamRelNode seekableRel = BeamSqlRelUtils.getBeamRelInput(getInput(seekableInputIndex().get()));    BeamRelNode nonSeekableRel = BeamSqlRelUtils.getBeamRelInput(getInput(nonSeekableInputIndex().get()));        int factColOffset = nonSeekableInputIndex().get() == 0 ? 0 : CalciteUtils.toSchema(seekableRel.getRowType()).getFieldCount();    int lkpColOffset = seekableInputIndex().get() == 0 ? 0 : CalciteUtils.toSchema(nonSeekableRel.getRowType()).getFieldCount();            BeamIOSourceRel seekableInput = (BeamIOSourceRel) seekableRel;    BeamSqlSeekableTable seekableTable = (BeamSqlSeekableTable) seekableInput.getBeamSqlTable();        PCollection<Row> nonSeekableInput = pinput.get(0);    return nonSeekableInput.apply("join_as_lookup", new BeamJoinTransforms.JoinAsLookup(condition, seekableTable, CalciteUtils.toSchema(seekableInput.getRowType()), schema, factColOffset, lkpColOffset)).setRowSchema(schema);}
public Join beam_f25707_0(RelTraitSet traitSet, RexNode conditionExpr, RelNode left, RelNode right, JoinRelType joinType, boolean semiJoinDone)
{    return new BeamSideInputLookupJoinRel(getCluster(), traitSet, left, right, conditionExpr, variablesSet, joinType);}
public void beam_f25715_0(ProcessContext context, @StateId("counter") ValueState<Integer> counterState, @StateId("skipped_rows") ValueState<Integer> skippedRowsState)
{    Integer toSkipRows = firstNonNull(skippedRowsState.read(), startIndex);    if (toSkipRows == 0) {        int current = firstNonNull(counterState.read(), 0);        if (current < limitCount) {            counterState.write(current + 1);            context.output(context.element().getValue());        }    } else {        skippedRowsState.write(toSkipRows - 1);    }}
public void beam_f25716_0(ProcessContext ctx)
{    ctx.output(ctx.element().subList(startIndex, endIndex));}
public Sort beam_f25717_0(RelTraitSet traitSet, RelNode newInput, RelCollation newCollation, RexNode offset, RexNode fetch)
{    return new BeamSortRel(getCluster(), traitSet, newInput, newCollation, offset, fetch);}
public static NodeStats beam_f25725_0(RelNode input, RelMetadataQuery mq)
{    input = getInput(input);    return input.metadata(NodeStatsMetadata.class, mq).getNodeStats();}
public RelNode beam_f25726_0(RelTraitSet traitSet, RelNode input)
{    return new BeamUncollectRel(getCluster(), traitSet, input, withOrdinality);}
public PTransform<PCollectionList<Row>, PCollection<Row>> beam_f25727_0()
{    return new Transform();}
public BeamCostModel beam_f25735_0(RelOptPlanner planner, RelMetadataQuery mq)
{    NodeStats summationOfEstimates = inputs.stream().map(input -> BeamSqlRelUtils.getNodeStats(input, mq)).reduce(NodeStats.create(0, 0, 0), NodeStats::plus);    return BeamCostModel.FACTORY.makeCost(summationOfEstimates.getRowCount(), summationOfEstimates.getRate());}
public Uncollect beam_f25736_0(RelTraitSet traitSet, RelNode input)
{    return new BeamUnnestRel(getCluster(), traitSet, input, unnestType, unnestIndex);}
protected RelDataType beam_f25737_0()
{    return SqlValidatorUtil.deriveJoinRowType(input.getRowType(), unnestType, JoinRelType.INNER, getCluster().getTypeFactory(), null, ImmutableList.of());}
public PTransform<PCollectionList<Row>, PCollection<Row>> beam_f25745_0()
{    return new Transform();}
public PCollection<Row> beam_f25746_0(PCollectionList<Row> pinput)
{    checkArgument(pinput.size() == 0, "Should not have received input for %s: %s", BeamValuesRel.class.getSimpleName(), pinput);    Schema schema = CalciteUtils.toSchema(getRowType());    List<Row> rows = tuples.stream().map(tuple -> tupleToRow(schema, tuple)).collect(toList());    return pinput.getPipeline().begin().apply(Create.of(rows).withRowSchema(schema));}
private Row beam_f25747_0(Schema schema, ImmutableList<RexLiteral> tuple)
{    return IntStream.range(0, tuple.size()).mapToObj(i -> autoCastField(schema.getField(i), tuple.get(i).getValue())).collect(toRow(schema));}
public void beam_f25755_0(RelOptRuleCall call)
{    Aggregate aggregate = call.rel(0);    TableScan tableScan = call.rel(1);    RelNode newTableScan = tableScan.copy(tableScan.getTraitSet(), tableScan.getInputs());    call.transformTo(new BeamAggregationRel(aggregate.getCluster(), aggregate.getTraitSet().replace(BeamLogicalConvention.INSTANCE), convert(newTableScan, newTableScan.getTraitSet().replace(BeamLogicalConvention.INSTANCE)), aggregate.indicator, aggregate.getGroupSet(), aggregate.getGroupSets(), aggregate.getAggCallList(), null, -1));}
public boolean beam_f25756_0(RelOptRuleCall x)
{    return true;}
public RelNode beam_f25757_0(RelNode rel)
{    final Calc calc = (Calc) rel;    final RelNode input = calc.getInput();    return new BeamCalcRel(calc.getCluster(), calc.getTraitSet().replace(BeamLogicalConvention.INSTANCE), RelOptRule.convert(input, input.getTraitSet().replace(BeamLogicalConvention.INSTANCE)), calc.getProgram());}
public RelNode beam_f25765_0(RelNode rel)
{    Minus minus = (Minus) rel;    final List<RelNode> inputs = minus.getInputs();    return new BeamMinusRel(minus.getCluster(), minus.getTraitSet().replace(BeamLogicalConvention.INSTANCE), convertList(inputs, BeamLogicalConvention.INSTANCE), minus.all);}
public boolean beam_f25766_0(RelOptRuleCall call)
{        if (BeamJoinRel.containsSeekableInput(call.rel(0))) {        return false;    }    PCollection.IsBounded boundednessOfLeftRel = BeamJoinRel.getBoundednessOfRelNode(call.rel(1));    PCollection.IsBounded boundednessOfRightRel = BeamJoinRel.getBoundednessOfRelNode(call.rel(2));    return (boundednessOfLeftRel == PCollection.IsBounded.BOUNDED ? boundednessOfRightRel == PCollection.IsBounded.UNBOUNDED : boundednessOfRightRel == PCollection.IsBounded.BOUNDED);}
public void beam_f25767_0(RelOptRuleCall call)
{    Join join = (Join) call.rel(0);    BeamSideInputJoinRel rel = new BeamSideInputJoinRel(join.getCluster(), join.getTraitSet().replace(BeamLogicalConvention.INSTANCE), convert(join.getLeft(), join.getLeft().getTraitSet().replace(BeamLogicalConvention.INSTANCE)), convert(join.getRight(), join.getRight().getTraitSet().replace(BeamLogicalConvention.INSTANCE)), join.getCondition(), join.getVariablesSet(), join.getJoinType());    call.transformTo(rel);}
public void beam_f25775_0(RelNode rel, Map<RelNode, RelNode> equiv)
{    if (checker.check(rel)) {        originalCall.transformTo(rel, equiv);    }}
public RelOptRuleOperand beam_f25776_0()
{    return originalCall.getOperand0();}
public RelOptRule beam_f25777_0()
{    return originalCall.getRule();}
public static ImmutableMultimap<String, Function> beam_f25785_0(Class<?> clazz)
{    final ImmutableMultimap.Builder<String, Function> builder = ImmutableMultimap.builder();    for (Method method : clazz.getMethods()) {        if (method.getDeclaringClass() == Object.class) {            continue;        }        if (!Modifier.isStatic(method.getModifiers()) && !classHasPublicZeroArgsConstructor(clazz)) {            continue;        }        final Function function = create(method);        builder.put(method.getName(), function);    }    return builder.build();}
public static Function beam_f25786_0(Class<?> clazz, String methodName)
{    final Method method = findMethod(clazz, methodName);    if (method == null) {        return null;    }    return create(method);}
public static Function beam_f25787_0(Method method)
{    if (!Modifier.isStatic(method.getModifiers())) {        Class clazz = method.getDeclaringClass();        if (!classHasPublicZeroArgsConstructor(clazz)) {            throw RESOURCE.requireDefaultConstructor(clazz.getName()).ex();        }    }    if (method.getExceptionTypes().length != 0) {        throw new RuntimeException(method.getName() + " must not throw checked exception");    }    CallImplementor implementor = createImplementor(method);    return new ScalarFunctionImpl(method, implementor);}
public RelDataType beam_f25795_0(RelDataTypeFactory typeFactory, SqlOperatorBinding opBinding)
{                final RelDataType returnType = getReturnType(typeFactory);    switch(getNullPolicy(method)) {        case STRICT:            for (RelDataType type : opBinding.collectOperandTypes()) {                if (type.isNullable()) {                    return typeFactory.createTypeWithNullability(returnType, true);                }            }            break;        case SEMI_STRICT:            return typeFactory.createTypeWithNullability(returnType, true);        default:            break;    }    return returnType;}
 static boolean beam_f25796_0(Class<?> clazz)
{    for (Constructor<?> constructor : clazz.getConstructors()) {        if (constructor.getParameterTypes().length == 0 && Modifier.isPublic(constructor.getModifiers())) {            return true;        }    }    return false;}
 static Method beam_f25797_0(Class<?> clazz, String name)
{    for (Method method : clazz.getMethods()) {        if (method.getName().equals(name) && !method.isBridge()) {            return method;        }    }    return null;}
public static Object beam_f25805_0(Schema.Field field, Object rawObj)
{    if (rawObj == null) {        if (!field.getType().getNullable()) {            throw new IllegalArgumentException(String.format("Field %s not nullable", field.getName()));        }        return null;    }    FieldType type = field.getType();    if (CalciteUtils.isStringType(type)) {        if (rawObj instanceof NlsString) {            return ((NlsString) rawObj).getValue();        } else {            return rawObj;        }    } else if (CalciteUtils.isDateTimeType(type)) {                return new DateTime(rawObj);    } else if (type.getTypeName().isNumericType() && ((rawObj instanceof String) || (rawObj instanceof BigDecimal && type.getTypeName() != TypeName.DECIMAL))) {        String raw = rawObj.toString();        switch(type.getTypeName()) {            case BYTE:                return Byte.valueOf(raw);            case INT16:                return Short.valueOf(raw);            case INT32:                return Integer.valueOf(raw);            case INT64:                return Long.valueOf(raw);            case FLOAT:                return Float.valueOf(raw);            case DOUBLE:                return Double.valueOf(raw);            default:                throw new UnsupportedOperationException(String.format("Column type %s is not supported yet!", type));        }    }    return rawObj;}
public static TableName beam_f25806_0(List<String> fullPath)
{    checkNotNull(fullPath, "Full table path cannot be null");    checkArgument(fullPath.size() > 0, "Full table path has to have at least one element");    return create(fullPath.subList(0, fullPath.size() - 1), fullPath.get(fullPath.size() - 1));}
public static TableName beam_f25807_0(List<String> path, String tableName)
{    checkNotNull(tableName, "Table name cannot be null");    return new AutoValue_TableName(path == null ? Collections.emptyList() : path, tableName);}
private static void beam_f25815_0(JdbcConnection connection, List<TableName> tableNames)
{    Map<String, CustomTableResolver> topLevelResolvers = getCustomTopLevelResolvers(connection);    topLevelResolvers.forEach((topLevelSchemaName, resolver) -> resolver.registerKnownTableNames(tablesForSchema(tableNames, topLevelSchemaName)));}
private static Map<String, CustomTableResolver> beam_f25816_0(JdbcConnection connection)
{    return connection.getRootSchema().getSubSchemaNames().stream().map(topLevelSchemaName -> SchemaWithName.create(connection, topLevelSchemaName)).filter(schema -> !schema.getName().equals(getCurrentSchemaName(connection))).filter(SchemaWithName::supportsCustomResolution).collect(toMap(SchemaWithName::getName, SchemaWithName::getCustomTableResolver));}
private static List<TableName> beam_f25817_0(List<TableName> tableNames, String topLevelSchema)
{    return tableNames.stream().filter(TableName::isCompound).filter(t -> t.getPrefix().equals(topLevelSchema)).map(TableName::removePrefix).collect(toList());}
public Object beam_f25825_0()
{    return combineFn.createAccumulator();}
public Object beam_f25826_0(Object accumulator, T input)
{    T processedInput = getInput(input);    return (processedInput == null) ? accumulator : combineFn.addInput(accumulator, getInput(input));}
public Object beam_f25827_0(Iterable<Object> accumulators)
{    return combineFn.mergeAccumulators(accumulators);}
public Row beam_f25835_0(Row accumulator)
{    return EMPTY_ROW;}
public Coder<Row> beam_f25836_0(CoderRegistry registry, Coder<Row> inputCoder) throws CannotProvideCoderException
{    return SchemaCoder.of(EMPTY_SCHEMA);}
public Coder<Row> beam_f25837_0(CoderRegistry registry, Coder<Row> inputCoder)
{    return SchemaCoder.of(EMPTY_SCHEMA);}
private BigDecimal beam_f25845_0(CovarianceAccumulator covarA, CovarianceAccumulator covarB)
{    BigDecimal countA = covarA.count();    BigDecimal countB = covarB.count();    BigDecimal totalCount = countA.add(countB);    BigDecimal avgXA = covarA.xavg();    BigDecimal avgYA = covarA.yavg();    BigDecimal avgXB = covarB.xavg();    BigDecimal avgYB = covarB.yavg();    return avgXA.subtract(avgXB).multiply(avgYA.subtract(avgYB)).multiply(countA).multiply(countB).divide(totalCount, CovarianceFn.MATH_CTX);}
private BigDecimal beam_f25846_0(CovarianceAccumulator covarA, CovarianceAccumulator covarB)
{    BigDecimal countA = covarA.count();    BigDecimal countB = covarB.count();    BigDecimal totalCount = countA.add(countB);    BigDecimal avgXA = covarA.xavg();    BigDecimal avgXB = covarB.xavg();    return avgXA.multiply(countA).add(avgXB.multiply(countB)).divide(totalCount, CovarianceFn.MATH_CTX);}
private BigDecimal beam_f25847_0(CovarianceAccumulator covarA, CovarianceAccumulator covarB)
{    BigDecimal countA = covarA.count();    BigDecimal countB = covarB.count();    BigDecimal totalCount = countA.add(countB);    BigDecimal avgYA = covarA.yavg();    BigDecimal avgYB = covarB.yavg();    return avgYA.multiply(countA).add(avgYB.multiply(countB)).divide(totalCount, CovarianceFn.MATH_CTX);}
public Coder<CovarianceAccumulator> beam_f25855_0(CoderRegistry registry, Coder<Row> inputCoder)
{    return SerializableCoder.of(CovarianceAccumulator.class);}
public T beam_f25856_0(CovarianceAccumulator accumulator)
{    return decimalConverter.apply(getCovariance(accumulator));}
private BigDecimal beam_f25857_0(CovarianceAccumulator covariance)
{    BigDecimal adjustedCount = this.isSample ? covariance.count().subtract(BigDecimal.ONE) : covariance.count();    return covariance.covariance().divide(adjustedCount, MATH_CTX);}
public static VarianceFn beam_f25865_0(Schema.TypeName typeName)
{    return newSample(BigDecimalConverter.forSqlType(typeName));}
public static VarianceFn beam_f25866_0(SerializableFunction<BigDecimal, V> decimalConverter)
{    return new VarianceFn<>(SAMPLE, decimalConverter);}
public VarianceAccumulator beam_f25867_0()
{    return VarianceAccumulator.ofZeroElements();}
 static CombineFn beam_f25875_0(Schema.FieldType fieldType)
{    if (CalciteUtils.isDateTimeType(fieldType)) {        return new CustMin();    }    switch(fieldType.getTypeName()) {        case BOOLEAN:        case BYTE:        case INT16:        case FLOAT:        case DATETIME:        case DECIMAL:        case STRING:            return new CustMin();        case INT32:            return Min.ofIntegers();        case INT64:            return Min.ofLongs();        case DOUBLE:            return Min.ofDoubles();        default:            throw new UnsupportedOperationException(String.format("[%s] is not support in MIN", fieldType));    }}
 static CombineFn beam_f25876_0(Schema.FieldType fieldType)
{    switch(fieldType.getTypeName()) {        case INT32:            return Sum.ofIntegers();        case INT16:            return new ShortSum();        case BYTE:            return new ByteSum();        case INT64:            return Sum.ofLongs();        case FLOAT:            return new FloatSum();        case DOUBLE:            return Sum.ofDoubles();        case DECIMAL:            return new BigDecimalSum();        default:            throw new UnsupportedOperationException(String.format("[%s] is not support in SUM", fieldType));    }}
 static CombineFn beam_f25877_0(Schema.FieldType fieldType)
{    switch(fieldType.getTypeName()) {        case INT32:            return new IntegerAvg();        case INT16:            return new ShortAvg();        case BYTE:            return new ByteAvg();        case INT64:            return new LongAvg();        case FLOAT:            return new FloatAvg();        case DOUBLE:            return new DoubleAvg();        case DECIMAL:            return new BigDecimalAvg();        default:            throw new UnsupportedOperationException(String.format("[%s] is not support in AVG", fieldType));    }}
public KV<Integer, BigDecimal> beam_f25885_0(KV<Integer, BigDecimal> accumulator, T input)
{    return KV.of(accumulator.getKey() + 1, accumulator.getValue().add(toBigDecimal(input)));}
public KV<Integer, BigDecimal> beam_f25886_0(Iterable<KV<Integer, BigDecimal>> accumulators)
{    int size = 0;    BigDecimal acc = BigDecimal.ZERO;    for (KV<Integer, BigDecimal> ele : accumulators) {        size += ele.getKey();        acc = acc.add(ele.getValue());    }    return KV.of(size, acc);}
public Coder<KV<Integer, BigDecimal>> beam_f25887_0(CoderRegistry registry, Coder<T> inputCoder)
{    return KvCoder.of(BigEndianIntegerCoder.of(), BigDecimalCoder.of());}
public Byte beam_f25895_0(KV<Integer, BigDecimal> accumulator)
{    return accumulator.getKey() == 0 ? null : prepareOutput(accumulator).byteValue();}
public BigDecimal beam_f25896_0(Byte record)
{    return new BigDecimal(record);}
public Float beam_f25897_0(KV<Integer, BigDecimal> accumulator)
{    return accumulator.getKey() == 0 ? null : prepareOutput(accumulator).floatValue();}
private Object beam_f25905_0(SerializableRexNode serializableRexNode, Row input, int leftRowColumnCount)
{    if (serializableRexNode instanceof SerializableRexInputRef) {        return input.getValue(((SerializableRexInputRef) serializableRexNode).getIndex() - leftRowColumnCount);    } else {                List<Integer> indexes = ((SerializableRexFieldAccess) serializableRexNode).getIndexes();                Row rowField = input.getValue(indexes.get(0) - leftRowColumnCount);        for (int i = 1; i < indexes.size() - 1; i++) {            rowField = rowField.getRow(indexes.get(i));        }        return rowField.getValue(indexes.get(indexes.size() - 1));    }}
public void beam_f25906_0(ProcessContext context)
{    Row key = context.element().getKey();    Row leftRow = context.element().getValue();    Map<Row, Iterable<Row>> key2Rows = context.sideInput(sideInputView);    Iterable<Row> rightRowsIterable = key2Rows.get(key);    if (rightRowsIterable != null && rightRowsIterable.iterator().hasNext()) {        for (Row aRightRowsIterable : rightRowsIterable) {            context.output(combineTwoRowsIntoOne(leftRow, aRightRowsIterable, swap, schema));        }    } else {        if (joinType == JoinRelType.LEFT) {            context.output(combineTwoRowsIntoOne(leftRow, rightNullRow, swap, schema));        }    }}
public Row beam_f25907_0(KV<Row, KV<Row, Row>> input)
{    KV<Row, Row> parts = input.getValue();    Row leftRow = parts.getKey();    Row rightRow = parts.getValue();    return combineTwoRowsIntoOne(leftRow, rightRow, false, schema);}
private Row beam_f25915_0(Row factRow)
{    List<Object> joinSubsetValues = factJoinIdx.stream().map(factRow::getValue).collect(toList());    return Row.withSchema(joinSubsetType).addValues(joinSubsetValues).build();}
public KV<Row, Row> beam_f25916_0(Row input)
{    return KV.of(input, input);}
public void beam_f25917_0(ProcessContext ctx)
{    CoGbkResult coGbkResult = ctx.element().getValue();    Iterable<Row> leftRows = coGbkResult.getAll(leftTag);    Iterable<Row> rightRows = coGbkResult.getAll(rightTag);    switch(opType) {        case UNION:            if (all) {                                Iterator<Row> iter = leftRows.iterator();                while (iter.hasNext()) {                    ctx.output(iter.next());                }                iter = rightRows.iterator();                while (iter.hasNext()) {                    ctx.output(iter.next());                }            } else {                                ctx.output(ctx.element().getKey());            }            break;        case INTERSECT:            if (leftRows.iterator().hasNext() && rightRows.iterator().hasNext()) {                if (all) {                    int leftCount = Iterators.size(leftRows.iterator());                    int rightCount = Iterators.size(rightRows.iterator());                                                            Iterator<Row> iter = (leftCount <= rightCount) ? leftRows.iterator() : rightRows.iterator();                    while (iter.hasNext()) {                        ctx.output(iter.next());                    }                } else {                    ctx.output(ctx.element().getKey());                }            }            break;        case MINUS:                        if (leftRows.iterator().hasNext() && !rightRows.iterator().hasNext()) {                Iterator<Row> iter = leftRows.iterator();                if (all) {                                        while (iter.hasNext()) {                        ctx.output(iter.next());                    }                } else {                                        ctx.output(iter.next());                }            } else if (leftRows.iterator().hasNext() && rightRows.iterator().hasNext()) {                int leftCount = Iterators.size(leftRows.iterator());                int rightCount = Iterators.size(rightRows.iterator());                int outputCount = leftCount - rightCount;                if (outputCount > 0) {                    if (all) {                        while (outputCount > 0) {                            outputCount--;                            ctx.output(ctx.element().getKey());                        }                    }                                }            }    }}
public boolean beam_f25925_0()
{        return false;}
public AggImplementor beam_f25926_0(boolean windowContext)
{        return null;}
public RelDataType beam_f25927_0(RelDataTypeFactory typeFactory)
{    return CalciteUtils.sqlTypeWithAutoCast(typeFactory, combineFn.getOutputType().getType());}
public byte[] beam_f25935_0(byte[] bytes)
{    byte[] ret = Arrays.copyOf(bytes, bytes.length);    ArrayUtils.reverse(ret);    return ret;}
public byte[] beam_f25936_0(String str)
{    try {        return Hex.decodeHex(str.toCharArray());    } catch (DecoderException e) {        throw new RuntimeException(e);    }}
public String beam_f25937_0(byte[] bytes)
{    return Hex.encodeHexString(bytes);}
public byte[] beam_f25945_0(byte[] originalValue, Long returnLength, byte[] pattern)
{    if (returnLength < -1 || pattern.length == 0) {        throw new IllegalArgumentException("returnLength cannot be 0 or pattern cannot be empty.");    }    int returnLengthInt = Math.toIntExact(returnLength);    if (originalValue.length == returnLengthInt) {        return originalValue;    } else if (originalValue.length < returnLengthInt) {                byte[] ret = new byte[returnLengthInt];                System.arraycopy(originalValue, 0, ret, 0, originalValue.length);                int paddingOff = originalValue.length;        int paddingLeftBytes = returnLengthInt - originalValue.length;        byteArrayPadding(ret, pattern, paddingOff, paddingLeftBytes);        return ret;    } else {                        byte[] ret = new byte[returnLengthInt];        System.arraycopy(originalValue, 0, ret, 0, returnLengthInt);        return ret;    }}
private void beam_f25946_0(byte[] dest, byte[] pattern, int paddingOff, int paddingLeftBytes)
{    while (paddingLeftBytes > 0) {        if (paddingLeftBytes >= pattern.length) {                        System.arraycopy(pattern, 0, dest, paddingOff, pattern.length);            paddingLeftBytes -= pattern.length;            paddingOff += pattern.length;        } else {            System.arraycopy(pattern, 0, dest, paddingOff, paddingLeftBytes);            paddingLeftBytes = 0;        }    }}
public Double beam_f25947_0(Double o)
{    if (o == null) {        return null;    }    return Math.cosh(o);}
public static Function beam_f25955_0(Method method)
{    if (TranslatableTable.class.isAssignableFrom(method.getReturnType())) {        return TableMacroImpl.create(method);    } else {        return ScalarFunctionImpl.create(method);    }}
 static Method beam_f25956_0(Class<?> clazz, String name)
{    for (Method method : clazz.getMethods()) {        if (method.getName().equals(name) && !method.isBridge()) {            return method;        }    }    return null;}
public List<FunctionParameter> beam_f25957_0()
{    return parameters;}
public String beam_f25965_0()
{    return name;}
public RelDataType beam_f25966_0(RelDataTypeFactory typeFactory)
{    return CalciteUtils.sqlTypeWithAutoCast(typeFactory, type);}
public boolean beam_f25967_0()
{    return optional;}
public static Schema.Field beam_f25975_0(RelDataTypeField calciteField)
{    return toField(calciteField.getName(), calciteField.getType());}
public static Schema.Field beam_f25976_0(String name, RelDataType calciteType)
{    return Schema.Field.of(name, toFieldType(calciteType)).withNullable(calciteType.isNullable());}
public static FieldType beam_f25977_0(RelDataType calciteType)
{    switch(calciteType.getSqlTypeName()) {        case ARRAY:        case MULTISET:            return FieldType.array(toFieldType(calciteType.getComponentType()));        case MAP:            return FieldType.map(toFieldType(calciteType.getKeyType()), toFieldType(calciteType.getValueType()));        case ROW:            return FieldType.row(toSchema(calciteType));        default:            return toFieldType(calciteType.getSqlTypeName());    }}
public SerializableRexNode beam_f25985_0()
{    if (rexNode instanceof RexInputRef) {        return new SerializableRexInputRef((RexInputRef) rexNode);    } else if (rexNode instanceof RexFieldAccess) {        return new SerializableRexFieldAccess((RexFieldAccess) rexNode);    }    throw new UnsupportedOperationException("Does not support to convert " + rexNode.getType() + " to SerializableRexNode.");}
public PCollection<Row> beam_f25986_0(PBegin begin)
{    return begin.apply("AvroIORead", AvroIO.readGenericRecords(AvroUtils.toAvroSchema(schema, tableName, null)).withBeamSchemas(true).from(filePattern)).apply("GenericRecordToRow", Convert.toRows());}
public PDone beam_f25987_0(PCollection<Row> input)
{    PTransform<PCollection<Row>, PCollection<GenericRecord>> writeConverter = GenericRecordWriteConverter.builder().beamSchema(schema).build();    return input.apply("GenericRecordToRow", writeConverter).apply("AvroIOWrite", AvroIO.writeGenericRecords(AvroUtils.toAvroSchema(schema, tableName, null)).to(filePattern).withoutSharding());}
public BeamTableStatistics beam_f25995_0(PipelineOptions options)
{    if (rowCountStatistics == null) {        rowCountStatistics = getRowCountFromBQ(options, bqLocation);    }    return rowCountStatistics;}
public PCollection.IsBounded beam_f25996_0()
{    return PCollection.IsBounded.BOUNDED;}
public PCollection<Row> beam_f25997_0(PBegin begin)
{    return begin.apply("Read Input BQ Rows", BigQueryIO.read(record -> BigQueryUtils.toBeamRow(record.getRecord(), getSchema(), conversionOptions)).from(bqLocation).withCoder(SchemaCoder.of(getSchema()))).setRowSchema(getSchema());}
public String beam_f26005_0()
{    return "google.cloud.datacatalog.subprovider";}
public Table beam_f26006_0(String name)
{                Optional<TableName> matchingTable = tableNames.stream().filter(tableName -> tableName.getTableName().equals(name)).findFirst();    TableName fullTableName = matchingTable.orElseThrow(() -> new IllegalStateException("Unexpected table '" + name + "' requested. Current schema level is " + schemaLevel + ". Current known table names: " + tableNames.toString()));    return FullNameTableProvider.this.getTableByFullName(fullTableName);}
public synchronized BeamSqlTable beam_f26007_0(Table table)
{    return FullNameTableProvider.this.buildBeamSqlTable(table);}
public BeamKafkaTable beam_f26017_0(Map<String, Object> configUpdates)
{    this.configUpdates = configUpdates;    return this;}
public PCollection.IsBounded beam_f26018_0()
{    return PCollection.IsBounded.UNBOUNDED;}
public PCollection<Row> beam_f26019_0(PBegin begin)
{    return begin.apply("read", createKafkaRead().withoutMetadata()).apply("in_format", getPTransformForInput()).setRowSchema(getSchema());}
 double beam_f26027_0(Consumer<T, T> consumer, int numberOfRecordsToCheck) throws NoEstimationException
{    Stream<TopicPartition> c = getTopics().stream().map(consumer::partitionsFor).flatMap(Collection::stream).map(parInf -> new TopicPartition(parInf.topic(), parInf.partition()));    List<TopicPartition> topicPartitions = c.collect(Collectors.toList());    consumer.assign(topicPartitions);                        Map<TopicPartition, Long> offsets = consumer.endOffsets(topicPartitions);    long nParsSeen = 0;    for (TopicPartition par : topicPartitions) {        long offset = offsets.get(par);        nParsSeen = (offset == 0) ? nParsSeen : nParsSeen + 1;        consumer.seek(par, Math.max(0L, offset - numberOfRecordsToCheck));    }    if (nParsSeen == 0) {        throw new NoEstimationException("There is no partition with messages in it.");    }    ConsumerRecords<T, T> records = consumer.poll(1000);                        Map<Integer, Long> minTimeStamps = new HashMap<>();    long maxMinTimeStamp = 0;    for (ConsumerRecord<T, T> record : records) {        if (!minTimeStamps.containsKey(record.partition())) {            minTimeStamps.put(record.partition(), record.timestamp());            nParsSeen--;            maxMinTimeStamp = Math.max(record.timestamp(), maxMinTimeStamp);            if (nParsSeen == 0) {                break;            }        }    }    int numberOfRecords = 0;    long maxTimeStamp = 0;    for (ConsumerRecord<T, T> record : records) {        maxTimeStamp = Math.max(maxTimeStamp, record.timestamp());        numberOfRecords = record.timestamp() > maxMinTimeStamp ? numberOfRecords + 1 : numberOfRecords;    }    if (maxTimeStamp == maxMinTimeStamp) {        throw new NoEstimationException("Arrival time of all records are the same.");    }    return (numberOfRecords * 1000.) / ((double) maxTimeStamp - maxMinTimeStamp);}
public BeamSqlTable beam_f26028_0(Table table)
{    Schema schema = table.getSchema();    JSONObject properties = table.getProperties();    String bootstrapServers = properties.getString("bootstrap.servers");    JSONArray topicsArr = properties.getJSONArray("topics");    List<String> topics = new ArrayList<>(topicsArr.size());    for (Object topic : topicsArr) {        topics.add(topic.toString());    }    return new BeamKafkaCSVTable(schema, bootstrapServers, topics);}
public String beam_f26029_0()
{    return "kafka";}
public String beam_f26037_0()
{    return "parquet";}
public BeamSqlTable beam_f26038_0(Table table)
{    return new ParquetTable(table.getSchema(), table.getLocation());}
private boolean beam_f26039_0()
{    return getDeadLetterQueue() != null;}
public BeamTableStatistics beam_f26047_0(PipelineOptions options)
{    return BeamTableStatistics.UNBOUNDED_UNKNOWN;}
public String beam_f26048_0()
{    return "pubsub";}
public BeamSqlTable beam_f26049_0(Table tableDefintion)
{    validatePubsubMessageSchema(tableDefintion);    JSONObject tableProperties = tableDefintion.getProperties();    String timestampAttributeKey = tableProperties.getString("timestampAttributeKey");    String deadLetterQueue = tableProperties.getString("deadLetterQueue");    validateDlq(deadLetterQueue);    return PubsubIOJsonTable.builder().setSchema(tableDefintion.getSchema()).setTimestampAttribute(timestampAttributeKey).setDeadLetterQueue(deadLetterQueue).setTopic(tableDefintion.getLocation()).build();}
private Object beam_f26057_0(Schema.Field field, Instant timestamp, PubsubMessage pubsubMessage)
{    switch(field.getName()) {        case TIMESTAMP_FIELD:            return timestamp;        case ATTRIBUTES_FIELD:            return pubsubMessage.getAttributeMap();        case PAYLOAD_FIELD:            return parsePayloadJsonRow(pubsubMessage);        default:            throw new IllegalArgumentException("Unexpected field '" + field.getName() + "' in top level schema" + " for Pubsub message. Top level schema should only contain " + "'timestamp', 'attributes', and 'payload' fields");    }}
private Row beam_f26058_0(PubsubMessage pubsubMessage)
{    String payloadJson = new String(pubsubMessage.getPayload(), StandardCharsets.UTF_8);    if (objectMapper == null) {        objectMapper = newObjectMapperWith(RowJsonDeserializer.forSchema(payloadSchema()));    }    return JsonToRowUtils.jsonToRow(objectMapper, payloadJson);}
public String beam_f26059_0()
{    return typeName;}
public POutput beam_f26067_0(PCollection<Row> input)
{    throw new UnsupportedOperationException("buildIOWriter unsupported!");}
public String beam_f26068_0()
{    return "sequence";}
public BeamSqlTable beam_f26069_0(Table table)
{    return new GenerateSequenceTable(table);}
public TestBoundedTable beam_f26077_0(Object... args)
{    List<Row> rows = TestTableUtils.buildRows(getSchema(), Arrays.asList(args));    this.rows.addAll(rows);    return this;}
public PCollection<Row> beam_f26078_0(PBegin begin)
{    return begin.apply("MockedBoundedTable_Reader_" + COUNTER.incrementAndGet(), Create.of(rows).withRowSchema(schema)).setRowSchema(getSchema());}
public POutput beam_f26079_0(PCollection<Row> input)
{    input.apply(ParDo.of(new DoFn<Row, Void>() {        @ProcessElement        public void processElement(ProcessContext c) {            CONTENT.add(c.element());        }        @Teardown        public void close() {            CONTENT.clear();        }    }));    return PDone.in(input.getPipeline());}
public Map<String, Table> beam_f26087_0()
{    return tables().entrySet().stream().collect(Collectors.toMap(Map.Entry::getKey, e -> e.getValue().table));}
public synchronized BeamSqlTable beam_f26088_0(Table table)
{    return new InMemoryTable(tables().get(table.getName()));}
public void beam_f26089_0(String tableName, Row... rows)
{    checkArgument(tables().containsKey(tableName), "Table not found: " + tableName);    tables().get(tableName).rows.addAll(Arrays.asList(rows));}
public Schema beam_f26097_0()
{    return tableWithRows.table.getSchema();}
public void beam_f26098_0(ProcessContext context)
{    long instanceId = tableWithRows.tableProviderInstanceId;    String tableName = tableWithRows.table.getName();    GLOBAL_TABLES.get(instanceId).get(tableName).rows.add(context.element());    context.output(context.element());}
public static Schema beam_f26099_0(Object... args)
{    return Stream.iterate(0, i -> i + 2).limit(args.length / 2).map(i -> toRecordField(args, i)).collect(toSchema());}
public TestUnboundedTable beam_f26107_0(int idx)
{    this.timestampField = idx;    return this;}
public PCollection.IsBounded beam_f26108_0()
{    return PCollection.IsBounded.UNBOUNDED;}
public TestUnboundedTable beam_f26109_0(Duration duration, Object... args)
{    List<Row> rows = TestTableUtils.buildRows(getSchema(), Arrays.asList(args));        this.timestampedRows.add(Pair.of(duration, rows));    return this;}
public String beam_f26117_0()
{    return "text";}
public BeamSqlTable beam_f26118_0(Table table)
{    Schema schema = table.getSchema();    String filePattern = table.getLocation();    JSONObject properties = table.getProperties();    String format = MoreObjects.firstNonNull(properties.getString("format"), "csv");            @Nullable    String legacyCsvFormat = null;    if (!ImmutableSet.of("csv", "lines").contains(format)) {        legacyCsvFormat = format;        format = "csv";    }    switch(format) {        case "csv":            String specifiedCsvFormat = properties.getString("csvformat");            CSVFormat csvFormat = specifiedCsvFormat != null ? CSVFormat.valueOf(specifiedCsvFormat) : (legacyCsvFormat != null ? CSVFormat.valueOf(legacyCsvFormat) : CSVFormat.DEFAULT);            return new TextTable(schema, filePattern, new CsvToRow(schema, csvFormat), new RowToCsv(csvFormat));        case "lines":            checkArgument(schema.getFieldCount() == 1 && schema.getField(0).getType().getTypeName().equals(TypeName.STRING), "Table with type 'text' and format 'lines' " + "must have exactly one STRING/VARCHAR/CHAR column ");            return new TextTable(schema, filePattern, new LinesReadConverter(), new LinesWriteConverter());        default:            throw new IllegalArgumentException("Table with type 'text' must have format 'csv' or 'lines'");    }}
public PCollection<String> beam_f26119_0(PCollection<Row> input)
{    return input.apply("rowsToLines", MapElements.into(TypeDescriptors.strings()).via((Row row) -> row.getString(0) + "\n"));}
 Map<String, Combine.CombineFn> beam_f26127_0()
{    return Collections.emptyMap();}
public String beam_f26128_0()
{    return "store";}
public void beam_f26129_0(Table table)
{    validateTableType(table);        if (tables.containsKey(table.getName())) {        throw new IllegalArgumentException("Duplicate table name: " + table.getName());    }        providers.get(table.getType()).createTable(table);        tables.put(table.getName(), table);}
public static Builder beam_f26137_0()
{    return new AutoValue_Table.Builder().properties(new JSONObject());}
public PCollection<Row> beam_f26138_0(PInput input)
{    BeamSqlEnvBuilder sqlEnvBuilder = BeamSqlEnv.builder(new ReadOnlyTableProvider(PCOLLECTION_NAME, toTableMap(input)));    tableProviderMap().forEach(sqlEnvBuilder::addSchema);    if (defaultTableProvider() != null) {        sqlEnvBuilder.setCurrentSchema(defaultTableProvider());    }        sqlEnvBuilder.autoLoadBuiltinFunctions();    registerFunctions(sqlEnvBuilder);    if (autoUdfUdafLoad()) {        sqlEnvBuilder.autoLoadUserDefinedFunctions();    }    sqlEnvBuilder.setQueryPlannerClassName(input.getPipeline().getOptions().as(BeamSqlPipelineOptions.class).getPlannerName());    sqlEnvBuilder.setPipelineOptions(input.getPipeline().getOptions());    BeamSqlEnv sqlEnv = sqlEnvBuilder.build();    return BeamSqlRelUtils.toPCollection(input.getPipeline(), sqlEnv.parseQuery(queryString()));}
private Map<String, BeamSqlTable> beam_f26139_0(PInput inputs)
{    /**     * A single PCollection is transformed to a table named PCOLLECTION, other input types are     * expanded and converted to tables using the tags as names.     */    if (inputs instanceof PCollection) {        PCollection<?> pCollection = (PCollection<?>) inputs;        return ImmutableMap.of(PCOLLECTION_NAME, new BeamPCollectionTable(pCollection));    }    ImmutableMap.Builder<String, BeamSqlTable> tables = ImmutableMap.builder();    for (Map.Entry<TupleTag<?>, PValue> input : inputs.expand().entrySet()) {        PCollection<?> pCollection = (PCollection<?>) input.getValue();        tables.put(input.getKey().getId(), new BeamPCollectionTable(pCollection));    }    return tables.build();}
private SqlTransform beam_f26147_0(String functionName, Class<?> clazz, String method)
{    ImmutableList<UdfDefinition> newUdfDefinitions = ImmutableList.<UdfDefinition>builder().addAll(udfDefinitions()).add(UdfDefinition.of(functionName, clazz, method)).build();    return toBuilder().setUdfDefinitions(newUdfDefinitions).build();}
public SqlTransform beam_f26148_0(String functionName, Combine.CombineFn combineFn)
{    ImmutableList<UdafDefinition> newUdafs = ImmutableList.<UdafDefinition>builder().addAll(udafDefinitions()).add(UdafDefinition.of(functionName, combineFn)).build();    return toBuilder().setUdafDefinitions(newUdafs).build();}
 static Builder beam_f26149_0()
{    return new AutoValue_SqlTransform.Builder();}
public static String beam_f26157_0(byte[] bytes)
{    try {        return new String(bytes, "UTF8");    } catch (UnsupportedEncodingException e) {        throw new RuntimeException(e);    }}
public static String beam_f26158_0(long timestamp)
{    return DateTimeUtils.formatTimestampWithTimeZone(new DateTime(timestamp));}
public DateTime beam_f26159_0(Integer year, Integer month, Integer day)
{    return DateTimeUtils.parseDate(String.join("-", year.toString(), month.toString(), day.toString()));}
public static String beam_f26167_0(DateTime dt)
{    String resultWithoutZone;    if (dt.getMillisOfSecond() == 0) {        resultWithoutZone = dt.toString(DateTimeFormat.forPattern("yyyy-MM-dd HH:mm:ss"));    } else {        resultWithoutZone = dt.toString(DateTimeFormat.forPattern("yyyy-MM-dd HH:mm:ss.SSS"));    }                String zone = dt.toString(DateTimeFormat.forPattern("ZZ"));    List<String> zoneParts = Lists.newArrayList(Splitter.on(':').limit(2).split(zone));    if (zoneParts.size() == 2 && zoneParts.get(1).equals("00")) {        zone = zoneParts.get(0);    }    return resultWithoutZone + zone;}
public static DateTime beam_f26168_0(String str)
{    return DateTimeFormat.forPattern("yyyy-MM-dd HH:mm:ss").parseDateTime(str);}
public static DateTime beam_f26169_0(String str)
{    return DateTimeFormat.forPattern("yyyy-MM-dd").withZoneUTC().parseDateTime(str);}
public static long beam_f26177_0(long micros)
{    safeCheckSubMillisPrecision(micros);    return micros / 1000L;}
public static Long beam_f26178_0(Long ts)
{    if (ts == null) {        return null;    }    if ((ts < MIN_UNIX_MILLIS) || (ts > MAX_UNIX_MILLIS)) {        throw Status.OUT_OF_RANGE.withDescription("Timestamp is out of valid range.").asRuntimeException();    }    return ts;}
public static Long beam_f26179_0(Long arg, TimeUnit unit)
{    if (arg == null) {        return null;    }        long multiplier = unit.multiplier.longValue();    switch(unit) {        case SECOND:        case MILLISECOND:                        multiplier *= 1000L;            break;        default:            break;    }    if ((arg > Long.MAX_VALUE / multiplier) || (arg < Long.MIN_VALUE / multiplier)) {        throw Status.OUT_OF_RANGE.withDescription("Interval is out of valid range").asRuntimeException();    }    return arg;}
 ResolvedStatement beam_f26187_0(String sql)
{    AnalyzerOptions options = initAnalyzerOptions(builder.queryParams);    List<List<String>> tables = Analyzer.extractTableNamesFromStatement(sql);    SimpleCatalog catalog = createPopulatedCatalog(builder.topLevelSchema.getName(), options, tables);    return Analyzer.analyzeStatement(sql, options, catalog);}
private AnalyzerOptions beam_f26188_0(Map<String, Value> queryParams)
{    AnalyzerOptions options = new AnalyzerOptions();    options.setErrorMessageMode(ErrorMessageMode.ERROR_MESSAGE_MULTI_LINE_WITH_CARET);        options.setDefaultTimezone("UTC");    options.getLanguageOptions().setProductMode(ProductMode.PRODUCT_EXTERNAL);    options.getLanguageOptions().setEnabledLanguageFeatures(new HashSet<>(Arrays.asList(LanguageFeature.FEATURE_DISALLOW_GROUP_BY_FLOAT, LanguageFeature.FEATURE_V_1_2_CIVIL_TIME, LanguageFeature.FEATURE_V_1_1_SELECT_STAR_EXCEPT_REPLACE)));    options.getLanguageOptions().setSupportedStatementKinds(ImmutableSet.of(RESOLVED_QUERY_STMT, RESOLVED_CREATE_FUNCTION_STMT));    for (Map.Entry<String, Value> entry : queryParams.entrySet()) {        options.addQueryParameter(entry.getKey(), entry.getValue().getType());    }    return options;}
private SimpleCatalog beam_f26189_0(String catalogName, AnalyzerOptions options, List<List<String>> tables)
{    SimpleCatalog catalog = new SimpleCatalog(catalogName);    addBuiltinFunctionsToCatalog(catalog, options);    tables.forEach(table -> addTableToLeafCatalog(builder.queryTrait, catalog, table));    return catalog;}
 Builder beam_f26197_0(QueryTrait trait)
{    this.queryTrait = trait;    return this;}
 Builder beam_f26198_0(SchemaPlus schema)
{    this.topLevelSchema = schema;    return this;}
 Builder beam_f26199_0(Context context)
{    this.calciteContext = context;    return this;}
public static SqlUserDefinedFunction beam_f26207_0(String name, Class<?> methodClass, String methodName, SqlReturnTypeInference returnTypeInference, List<RelDataType> paramTypes)
{    return new SqlUserDefinedFunction(new SqlIdentifier(name, SqlParserPos.ZERO), returnTypeInference, null, null, paramTypes, ScalarFunctionImpl.create(methodClass, methodName));}
public static SqlUserDefinedFunction beam_f26208_0(String name, Method method)
{    Function function = ScalarFunctionImpl.create(method);    final RelDataTypeFactory typeFactory = createTypeFactory();    List<RelDataType> argTypes = new ArrayList<>();    List<SqlTypeFamily> typeFamilies = new ArrayList<>();    for (FunctionParameter o : function.getParameters()) {        final RelDataType type = o.getType(typeFactory);        argTypes.add(type);        typeFamilies.add(Util.first(type.getSqlTypeName().getFamily(), SqlTypeFamily.ANY));    }    final FamilyOperandTypeChecker typeChecker = OperandTypes.family(typeFamilies, i -> function.getParameters().get(i).isOptional());    final List<RelDataType> paramTypes = toSql(typeFactory, argTypes);    return new SqlUserDefinedFunction(new SqlIdentifier(name, SqlParserPos.ZERO), infer((ScalarFunction) function), InferTypes.explicit(argTypes), typeChecker, paramTypes, function);}
private static RelDataType beam_f26209_0(SqlTypeName typeName, boolean withNullability)
{    final RelDataTypeFactory typeFactory = createTypeFactory();    RelDataType type = typeFactory.createSqlType(typeName);    if (withNullability) {        type = typeFactory.createTypeWithNullability(type, true);    }    return type;}
public static String beam_f26217_0(String arg1, String arg2)
{    return concatIfNotIncludeNull(arg1, arg2);}
public static String beam_f26218_0(String arg1, String arg2, String arg3)
{    return concatIfNotIncludeNull(arg1, arg2, arg3);}
public static String beam_f26219_0(String arg1, String arg2, String arg3, String arg4)
{    return concatIfNotIncludeNull(arg1, arg2, arg3, arg4);}
public static String beam_f26227_0(String str)
{    return rtrim(str, " ");}
public static String beam_f26228_0(String str, String seek)
{    return SqlFunctions.trim(false, true, seek, str, false);}
public static String beam_f26229_0(String str, long from, long len)
{    if (from > Integer.MAX_VALUE || len > Integer.MAX_VALUE || from < Integer.MIN_VALUE || len < Integer.MIN_VALUE) {        throw new RuntimeException(SUBSTR_PARAMETER_EXCEED_INTEGER);    }    return SqlFunctions.substring(str, (int) from, (int) len);}
 SimpleTable beam_f26237_0()
{    return table;}
 List<String> beam_f26238_0()
{    return path;}
 String beam_f26239_0()
{    return topLevelSchema;}
 static Table beam_f26247_0(Schema schema, List<String> tablePath)
{    return schema.getTable(ZetaSqlIdUtils.escapeAndJoin(tablePath));}
public static byte[] beam_f26248_0(String s)
{    return s.getBytes(StandardCharsets.UTF_8);}
public static DateTime beam_f26249_0(String timestampStr)
{    return timestamp(timestampStr, "UTC");}
public boolean beam_f26257_0(ResolvedArrayScan zetaNode)
{    return zetaNode.getInputScan() != null && zetaNode.getJoinExpr() != null;}
public List<ResolvedNode> beam_f26258_0(ResolvedArrayScan zetaNode)
{    return Collections.singletonList(zetaNode.getInputScan());}
public RelNode beam_f26259_0(ResolvedArrayScan zetaNode, List<RelNode> inputs)
{    List<RexNode> projects = new ArrayList<>();    RelNode leftInput = inputs.get(0);    ResolvedColumnRef columnRef = (ResolvedColumnRef) zetaNode.getArrayExpr();    CorrelationId correlationId = getCluster().createCorrel();    getCluster().getQuery().mapCorrel(correlationId.getName(), leftInput);    String columnName = String.format("%s%s", zetaNode.getElementColumn().getTableName(), zetaNode.getElementColumn().getName());    projects.add(getCluster().getRexBuilder().makeFieldAccess(getCluster().getRexBuilder().makeCorrel(leftInput.getRowType(), correlationId), getExpressionConverter().indexOfProjectionColumnRef(columnRef.getColumn().getId(), zetaNode.getInputScan().getColumnList())));    RelNode projectNode = LogicalProject.create(LogicalValues.createOneRow(getCluster()), projects, ImmutableList.of(columnName));            Uncollect uncollectNode = Uncollect.create(projectNode.getTraitSet(), projectNode, false);        RelNode rightInput = LogicalProject.create(uncollectNode, ImmutableList.of(getCluster().getRexBuilder().makeInputRef(uncollectNode, 0)), ImmutableList.of(columnName));        RexNode condition = getExpressionConverter().convertRexNodeFromResolvedExpr(zetaNode.getJoinExpr());    JoinRelType joinRelType = zetaNode.getIsOuter() ? JoinRelType.LEFT : JoinRelType.INNER;    return LogicalJoin.create(leftInput, rightInput, condition, ImmutableSet.of(), joinRelType);}
public List<RexNode> beam_f26267_0(ResolvedProjectScan node, List<RelDataTypeField> fieldList)
{    List<RexNode> ret = new ArrayList<>();    for (ResolvedColumn column : node.getColumnList()) {        int index = -1;        if ((index = indexOfResolvedColumnInExprList(node.getExprList(), column)) != -1) {            ResolvedComputedColumn computedColumn = node.getExprList().get(index);            int windowFieldIndex = -1;            if (computedColumn.getExpr().nodeKind() == RESOLVED_FUNCTION_CALL) {                String functionName = ((ResolvedFunctionCall) computedColumn.getExpr()).getFunction().getName();                if (WINDOW_START_END_FUNCTION_SET.contains(functionName)) {                    ResolvedAggregateScan resolvedAggregateScan = (ResolvedAggregateScan) node.getInputScan();                    windowFieldIndex = indexOfWindowField(resolvedAggregateScan.getGroupByList(), resolvedAggregateScan.getColumnList(), WINDOW_START_END_TO_WINDOW_MAP.get(functionName));                }            }            ret.add(convertRexNodeFromComputedColumnWithFieldList(computedColumn, node.getInputScan().getColumnList(), fieldList, windowFieldIndex));        } else {                        index = indexOfProjectionColumnRef(column.getId(), node.getInputScan().getColumnList());            if (index < 0 || index >= node.getInputScan().getColumnList().size()) {                throw new RuntimeException(String.format("Cannot find %s in fieldList %s", column, fieldList));            }            ret.add(rexBuilder().makeInputRef(fieldList.get(index).getType(), index));        }    }    return ret;}
public List<RexNode> beam_f26268_0(RelOptCluster cluster, ResolvedOrderByScan node, List<RelDataTypeField> fieldList)
{    final RexBuilder rexBuilder = cluster.getRexBuilder();    List<RexNode> ret = new ArrayList<>();    for (ResolvedColumn column : node.getColumnList()) {        int index = indexOfProjectionColumnRef(column.getId(), node.getInputScan().getColumnList());        ret.add(rexBuilder.makeInputRef(fieldList.get(index).getType(), index));    }    return ret;}
private static int beam_f26269_0(ImmutableList<ResolvedComputedColumn> exprList, ResolvedColumn column)
{    if (exprList == null || exprList.isEmpty()) {        return -1;    }    for (int i = 0; i < exprList.size(); i++) {        ResolvedComputedColumn computedColumn = exprList.get(i);        if (computedColumn.getColumn().equals(column)) {            return i;        }    }    return -1;}
private RexNode beam_f26277_0(Type type, Value value)
{    RexNode ret;    switch(type.getKind()) {        case TYPE_BOOL:        case TYPE_INT32:        case TYPE_INT64:        case TYPE_FLOAT:        case TYPE_DOUBLE:        case TYPE_STRING:        case TYPE_TIMESTAMP:        case TYPE_DATE:        case TYPE_TIME:                case TYPE_BYTES:            ret = convertSimpleValueToRexNode(type.getKind(), value);            break;        case TYPE_ARRAY:            ret = convertArrayValueToRexNode(type.asArray(), value);            break;        case TYPE_ENUM:            ret = convertEnumToRexNode(type, value);            break;        default:                        throw new RuntimeException("Unsupported ResolvedLiteral kind: " + type.getKind() + " type: " + type.typeName());    }    return ret;}
private RexNode beam_f26278_0(TypeKind kind, Value value)
{    if (value.isNull()) {        return rexBuilder().makeNullLiteral(TypeUtils.toSimpleRelDataType(kind, rexBuilder()));    }    RexNode ret;    switch(kind) {        case TYPE_BOOL:            ret = rexBuilder().makeLiteral(value.getBoolValue());            break;        case TYPE_INT32:            ret = rexBuilder().makeExactLiteral(new BigDecimal(value.getInt32Value()), TypeUtils.toSimpleRelDataType(kind, rexBuilder()));            break;        case TYPE_INT64:            ret = rexBuilder().makeExactLiteral(new BigDecimal(value.getInt64Value()), TypeUtils.toSimpleRelDataType(kind, rexBuilder()));            break;        case TYPE_FLOAT:            ret = rexBuilder().makeApproxLiteral(new BigDecimal(value.getFloatValue()), TypeUtils.toSimpleRelDataType(kind, rexBuilder()));            break;        case TYPE_DOUBLE:            ret = rexBuilder().makeApproxLiteral(new BigDecimal(value.getDoubleValue()), TypeUtils.toSimpleRelDataType(kind, rexBuilder()));            break;        case TYPE_STRING:                                    ret = rexBuilder().makeLiteral(value.getStringValue(), typeFactory().createSqlType(SqlTypeName.VARCHAR), true);            break;        case TYPE_TIMESTAMP:            ret = rexBuilder().makeTimestampLiteral(TimestampString.fromMillisSinceEpoch(safeMicrosToMillis(value.getTimestampUnixMicros())), typeFactory().getTypeSystem().getMaxPrecision(SqlTypeName.TIMESTAMP));            break;        case TYPE_DATE:            ret = rexBuilder().makeDateLiteral(convertDateValueToDateString(value));            break;        case TYPE_TIME:            RelDataType timeType = typeFactory().createSqlType(SqlTypeName.TIME, typeFactory().getTypeSystem().getMaxPrecision(SqlTypeName.TIME));                        ret = rexBuilder().makeLiteral(convertTimeValueToTimeString(value), timeType, false);            break;        case TYPE_BYTES:            ret = rexBuilder().makeBinaryLiteral(new ByteString(value.getBytesValue().toByteArray()));            break;        default:            throw new RuntimeException("Unsupported column type: " + kind);    }    return ret;}
private RexNode beam_f26279_0(ArrayType arrayType, Value value)
{    if (value.isNull()) {                return rexBuilder().makeNullLiteral(TypeUtils.toArrayRelDataType(rexBuilder(), arrayType, false));    }    List<RexNode> operands = new ArrayList<>();    for (Value v : value.getElementList()) {        operands.add(convertValueToRexNode(arrayType.getElementType(), v));    }    return rexBuilder().makeCall(SqlStdOperatorTable.ARRAY_VALUE_CONSTRUCTOR, operands);}
private RexNode beam_f26287_0(ResolvedLiteral resolvedLiteral)
{    if (resolvedLiteral.getType().getKind() != TYPE_STRING) {        throw new IllegalArgumentException(INTERVAL_FORMAT_MSG);    }    String valStr = resolvedLiteral.getValue().getStringValue();    List<String> stringList = Arrays.stream(valStr.split(" ")).filter(s -> !s.isEmpty()).collect(Collectors.toList());    if (stringList.size() != 3) {        throw new IllegalArgumentException(INTERVAL_FORMAT_MSG);    }    if (!Ascii.toUpperCase(stringList.get(0)).equals("INTERVAL")) {        throw new IllegalArgumentException(INTERVAL_FORMAT_MSG);    }    long intervalValue;    try {        intervalValue = Long.parseLong(stringList.get(1));    } catch (NumberFormatException e) {        throw new IllegalArgumentException(INTERVAL_FORMAT_MSG, e);    }    String intervalDatepart = Ascii.toUpperCase(stringList.get(2));    return createCalciteIntervalRexLiteral(intervalValue, intervalDatepart);}
private RexLiteral beam_f26288_0(long intervalValue, String intervalTimeUnit)
{    SqlIntervalQualifier sqlIntervalQualifier = convertIntervalDatepartToSqlIntervalQualifier(intervalTimeUnit);    BigDecimal decimalValue;    if (DATE_PART_UNITS_TO_MILLIS.contains(intervalTimeUnit)) {        decimalValue = convertIntervalValueToMillis(sqlIntervalQualifier, intervalValue);    } else if (DATE_PART_UNITS_TO_MONTHS.contains(intervalTimeUnit)) {        decimalValue = new BigDecimal(intervalValue * 12);    } else {        decimalValue = new BigDecimal(intervalValue);    }    return rexBuilder().makeIntervalLiteral(decimalValue, sqlIntervalQualifier);}
private static BigDecimal beam_f26289_0(SqlIntervalQualifier qualifier, long value)
{    switch(qualifier.typeName()) {        case INTERVAL_DAY:            return new BigDecimal(value * ONE_DAY_IN_MILLIS);        case INTERVAL_HOUR:            return new BigDecimal(value * ONE_HOUR_IN_MILLIS);        case INTERVAL_MINUTE:            return new BigDecimal(value * ONE_MINUTE_IN_MILLIS);        case INTERVAL_SECOND:            return new BigDecimal(value * ONE_SECOND_IN_MILLIS);        default:            throw new IllegalArgumentException(qualifier.typeName().toString());    }}
private RexNode beam_f26297_0(ResolvedGetStructField resolvedGetStructField, List<ResolvedColumn> columnList, List<RelDataTypeField> fieldList)
{    return rexBuilder().makeFieldAccess(convertRexNodeFromResolvedExpr(resolvedGetStructField.getExpr(), columnList, fieldList), (int) resolvedGetStructField.getFieldIdx());}
private RexBuilder beam_f26298_0()
{    return cluster.getRexBuilder();}
private RelDataTypeFactory beam_f26299_0()
{    return cluster.getTypeFactory();}
public List<ResolvedNode> beam_f26307_0(ResolvedJoinScan zetaNode)
{    return ImmutableList.of(zetaNode.getLeftScan(), zetaNode.getRightScan());}
public RelNode beam_f26308_0(ResolvedJoinScan zetaNode, List<RelNode> inputs)
{    RelNode calciteLeftInput = inputs.get(0);    RelNode calciteRightInput = inputs.get(1);    List<ResolvedColumn> zetaLeftColumnList = getColumnsForScan(zetaNode.getLeftScan());    List<ResolvedColumn> zetaRightColumnList = getColumnsForScan(zetaNode.getRightScan());    RexNode condition = getExpressionConverter().convertRexNodeFromResolvedExprWithRefScan(zetaNode.getJoinExpr(), zetaNode.getLeftScan().getColumnList(), calciteLeftInput.getRowType().getFieldList(), zetaLeftColumnList, zetaNode.getRightScan().getColumnList(), calciteRightInput.getRowType().getFieldList(), zetaRightColumnList);    return LogicalJoin.create(calciteLeftInput, calciteRightInput, condition, ImmutableSet.of(), convertResolvedJoinType(zetaNode.getJoinType()));}
private List<ResolvedColumn> beam_f26309_0(ResolvedScan resolvedScan)
{    return resolvedScan instanceof ResolvedWithRefScan ? getTrait().withEntries.get(((ResolvedWithRefScan) resolvedScan).getWithQueryName()).getWithSubquery().getColumnList() : resolvedScan.getColumnList();}
private static RelFieldCollation beam_f26317_0(ResolvedOrderByItem item)
{        Direction sortDirection = item.getIsDescending() ? DESCENDING : ASCENDING;    int fieldIndex = (int) item.getColumnRef().getColumn().getId();    return new RelFieldCollation(fieldIndex, sortDirection);}
private RelNode beam_f26318_0(ResolvedOrderByScan node, RelNode input)
{    List<RexNode> projects = getExpressionConverter().retrieveRexNodeFromOrderByScan(getCluster(), node, input.getRowType().getFieldList());    List<String> fieldNames = getTrait().retrieveFieldNames(node.getColumnList());    return LogicalProject.create(input, projects, fieldNames);}
public RelNode beam_f26319_0(ResolvedOrderByScan zetaNode, List<RelNode> inputs)
{    throw new UnsupportedOperationException("ORDER BY without a LIMIT is not supported.");}
public List<ResolvedNode> beam_f26327_0(T zetaNode)
{    return ImmutableList.of();}
protected RelOptCluster beam_f26328_0()
{    return context.cluster();}
protected FrameworkConfig beam_f26329_0()
{    return context.getConfig();}
public boolean beam_f26337_0(ResolvedSingleRowScan zetaNode)
{    return zetaNode.getColumnList() == null || zetaNode.getColumnList().isEmpty();}
public RelNode beam_f26338_0(ResolvedSingleRowScan zetaNode, List<RelNode> inputs)
{    return LogicalValues.createOneRow(getCluster());}
public RelNode beam_f26339_0(ResolvedTableScan zetaNode, List<RelNode> inputs)
{    checkTableScanSchema(zetaNode.getColumnList());    List<String> tablePath = getTablePath(zetaNode.getTable());    SchemaPlus defaultSchemaPlus = getConfig().getDefaultSchema();        Table calciteTable = TableResolution.resolveCalciteTable(getConfig().getContext(), defaultSchemaPlus, tablePath);        checkNotNull(calciteTable, "Unable to resolve the table path %s in schema %s", tablePath, defaultSchemaPlus.getName());    String defaultSchemaName = defaultSchemaPlus.getName();    final CalciteCatalogReader catalogReader = new CalciteCatalogReader(CalciteSchema.from(defaultSchemaPlus), ImmutableList.of(defaultSchemaName), getCluster().getTypeFactory(), new CalciteConnectionConfigImpl(new Properties()));    RelOptTableImpl relOptTable = RelOptTableImpl.create(catalogReader, calciteTable.getRowType(getCluster().getTypeFactory()), calciteTable, ImmutableList.<String>builder().add(defaultSchemaName).addAll(tablePath).build());    if (calciteTable instanceof TranslatableTable) {        return ((TranslatableTable) calciteTable).toRel(createToRelContext(), relOptTable);    } else {        throw new RuntimeException("Does not support non TranslatableTable type table!");    }}
public List<ResolvedNode> beam_f26347_0(ResolvedWithScan zetaNode)
{                        zetaNode.getWithEntryList().forEach(withEntry -> getTrait().withEntries.put(withEntry.getWithQueryName(), withEntry));        return Collections.singletonList(zetaNode.getQuery());}
public RelNode beam_f26348_0(ResolvedWithScan zetaNode, List<RelNode> inputs)
{        return inputs.get(0);}
 static Type beam_f26349_0(RelDataType calciteType)
{    if (CALCITE_TO_ZETA_SIMPLE_TYPES.containsKey(calciteType.getSqlTypeName())) {        return CALCITE_TO_ZETA_SIMPLE_TYPES.get(calciteType.getSqlTypeName());    }    switch(calciteType.getSqlTypeName()) {        case ARRAY:            return TypeFactory.createArrayType(toZetaType(calciteType.getComponentType()));        case MAP:                        return TypeFactory.createSimpleType(TypeKind.TYPE_STRING);        case ROW:            List<StructField> structFields = calciteType.getFieldList().stream().map(f -> new StructField(f.getName(), toZetaType(f.getType()))).collect(toList());            return TypeFactory.createStructType(structFields);        default:            throw new RuntimeException("Unsupported RelDataType: " + calciteType);    }}
public CallImplementor beam_f26357_0()
{    return createImplementor(new ZetaSQLCastCallNotNullImplementor(), NullPolicy.STRICT, false);}
public List<FunctionParameter> beam_f26358_0()
{    return null;}
public Expression beam_f26359_0(RexToLixTranslator rexToLixTranslator, RexCall rexCall, List<Expression> list)
{    if (rexCall.getOperands().size() != 1 || list.size() != 1) {        throw new RuntimeException("CAST should have one operand.");    }    SqlTypeName toType = rexCall.getType().getSqlTypeName();    SqlTypeName fromType = rexCall.getOperands().get(0).getType().getSqlTypeName();    Expression translatedOperand = list.get(0);    Expression convertedOperand;        if (fromType == SqlTypeName.BINARY && toType == SqlTypeName.VARCHAR) {                                convertedOperand = Expressions.call(BeamCodegenUtils.class, "toStringUTF8", Expressions.call(translatedOperand, "getBytes"));    } else if (fromType == SqlTypeName.VARBINARY && toType == SqlTypeName.VARCHAR) {                                convertedOperand = Expressions.call(BeamCodegenUtils.class, "toStringUTF8", translatedOperand);    } else if (fromType == SqlTypeName.BOOLEAN && toType == SqlTypeName.BIGINT) {        convertedOperand = Expressions.condition(translatedOperand, Expressions.constant(1L, Long.class), Expressions.constant(0L, Long.class));    } else if (fromType == SqlTypeName.BIGINT && toType == SqlTypeName.BOOLEAN) {        convertedOperand = Expressions.notEqual(translatedOperand, Expressions.constant(0));    } else if (fromType == SqlTypeName.TIMESTAMP && toType == SqlTypeName.VARCHAR) {        convertedOperand = Expressions.call(BeamCodegenUtils.class, "toStringTimestamp", translatedOperand);    } else {        throw new RuntimeException("Unsupported CAST: " + fromType.name() + " to " + toType.name());    }        if (rexCall.getOperands().get(0).getType().isNullable()) {        convertedOperand = Expressions.condition(Expressions.equal(translatedOperand, RexImpTable.NULL_EXPR), RexImpTable.NULL_EXPR, convertedOperand);    }    return convertedOperand;}
public Pair<SqlNode, RelDataType> beam_f26367_0(SqlNode sqlNode) throws ValidationException
{    throw new RuntimeException("validateAndGetType(SqlNode) is not implemented.");}
public RelRoot beam_f26368_0(SqlNode sqlNode) throws RelConversionException
{    return null;}
public RelRoot beam_f26369_0(String sql, Map<String, Value> params)
{    this.cluster = RelOptCluster.create(planner, new RexBuilder(typeFactory));    this.expressionConverter = new ExpressionConverter(cluster, params);    QueryTrait trait = new QueryTrait();    ResolvedStatement statement = SqlAnalyzer.withQueryParams(params).withQueryTrait(trait).withCalciteContext(config.getContext()).withTopLevelSchema(defaultSchemaPlus).withTypeFactory((JavaTypeFactory) cluster.getTypeFactory()).analyze(sql);    if (!(statement instanceof ResolvedQueryStmt)) {        throw new UnsupportedOperationException("Unsupported query statement type: " + sql.getClass().getSimpleName());    }    ConversionContext context = ConversionContext.of(config, expressionConverter, cluster, trait);    RelNode convertedNode = QueryStatementConverter.convertRootQuery(context, (ResolvedQueryStmt) statement);    return RelRoot.of(convertedNode, SqlKind.ALL);}
public BeamRelNode beam_f26378_0(String sqlStatement, Map<String, Value> queryParams) throws ParseException, SqlConversionException
{    try {        return parseQuery(sqlStatement, queryParams);    } catch (RelConversionException e) {        throw new SqlConversionException(e.getCause());    }}
public BeamRelNode beam_f26379_0(String sql) throws RelConversionException
{    return parseQuery(sql, Collections.emptyMap());}
public BeamRelNode beam_f26380_0(String sql, Map<String, Value> queryParams) throws RelConversionException
{    RelRoot root = plannerImpl.rel(sql, queryParams);    RelTraitSet desiredTraits = root.rel.getTraitSet().replace(BeamLogicalConvention.INSTANCE).replace(root.collation).simplify();    BeamRelNode beamRelNode = (BeamRelNode) plannerImpl.transform(0, desiredTraits, root.rel);    return beamRelNode;}
public void beam_f26388_0()
{    BeamSqlEnv sqlEnv = BeamSqlEnv.inMemory(readOnlyTableProvider);    PCollection<Row> stream = BeamSqlRelUtils.toPCollection(pipeline, sqlEnv.parseQuery("SELECT nestedRowTestTable.col.RowField.string_field, nestedRowTestTable.col.RowFieldTwo.long_field FROM nestedRowTestTable"));    PAssert.that(stream).containsInAnyOrder(Row.withSchema(Schema.builder().addStringField("field1").addInt64Field("field2").build()).addValues("inner_str_one", 3L).build());    pipeline.run().waitUntilFinish(Duration.standardMinutes(2));}
public void beam_f26389_0()
{    BeamSqlEnv sqlEnv = BeamSqlEnv.inMemory(readOnlyTableProvider);    PCollection<Row> stream = BeamSqlRelUtils.toPCollection(pipeline, sqlEnv.parseQuery("SELECT nestedRowTestTable.col.RowField FROM nestedRowTestTable"));    PAssert.that(stream).containsInAnyOrder(Row.withSchema(Schema.builder().addStringField("field1").addInt64Field("field2").build()).addValues("inner_str_one", 1L).build());    pipeline.run().waitUntilFinish(Duration.standardMinutes(2));}
public void beam_f26390_0()
{    byte[] bytes = new byte[] { -70, -83, -54, -2 };    Schema nestedInputSchema = Schema.of(Schema.Field.of("c_bytes", Schema.FieldType.BYTES));    Schema inputSchema = Schema.of(Schema.Field.of("nested", Schema.FieldType.row(nestedInputSchema)));    Schema outputSchema = Schema.of(Schema.Field.of("f0", Schema.FieldType.BYTES));    Row nestedRow = Row.withSchema(nestedInputSchema).addValue(bytes).build();    Row row = Row.withSchema(inputSchema).addValue(nestedRow).build();    Row expected = Row.withSchema(outputSchema).addValue(bytes).build();    PCollection<Row> result = pipeline.apply(Create.of(row).withRowSchema(inputSchema)).apply(SqlTransform.query("SELECT t.nested.c_bytes AS f0 FROM PCOLLECTION t"));    PAssert.that(result).containsInAnyOrder(expected);    pipeline.run();}
public Long beam_f26398_0(Instant base)
{    return base.getMillis();}
public String beam_f26399_0()
{    return "SqlDateType";}
public Schema.FieldType beam_f26400_0()
{    return Schema.FieldType.DATETIME;}
public void beam_f26408_0() throws Exception
{    InMemoryMetaStore metaStore = new InMemoryMetaStore();    metaStore.registerProvider(new TextTableProvider());    BeamSqlCli cli = new BeamSqlCli().metaStore(metaStore);    cli.execute("CREATE EXTERNAL TABLE person (\n" + "id int COMMENT 'id', \n" + "name varchar COMMENT 'name', \n" + "age int COMMENT 'age') \n" + "TYPE 'text' \n" + "COMMENT '' LOCATION '/home/admin/orders'");    Table table = metaStore.getTables().get("person");    assertNotNull(table);    assertEquals(Stream.of(Field.of("id", INTEGER).withDescription("id").withNullable(true), Field.of("name", VARCHAR).withDescription("name").withNullable(true), Field.of("age", INTEGER).withDescription("age").withNullable(true)).collect(toSchema()), table.getSchema());}
public void beam_f26409_0() throws Exception
{    InMemoryMetaStore metaStore = new InMemoryMetaStore();    metaStore.registerProvider(new TextTableProvider());    BeamSqlCli cli = new BeamSqlCli().metaStore(metaStore);    cli.execute("CREATE EXTERNAL TABLE person (\n" + "id int COMMENT 'id', \n" + "name varchar COMMENT 'name', \n" + "age int COMMENT 'age', \n" + "tags ARRAY<VARCHAR>, \n" + "matrix ARRAY<ARRAY<INTEGER>> \n" + ") \n" + "TYPE 'text' \n" + "COMMENT '' LOCATION '/home/admin/orders'");    Table table = metaStore.getTables().get("person");    assertNotNull(table);    assertEquals(Stream.of(Field.of("id", INTEGER).withDescription("id").withNullable(true), Field.of("name", VARCHAR).withDescription("name").withNullable(true), Field.of("age", INTEGER).withDescription("age").withNullable(true), Field.of("tags", Schema.FieldType.array(VARCHAR)).withNullable(true), Field.of("matrix", Schema.FieldType.array(Schema.FieldType.array(INTEGER))).withNullable(true)).collect(toSchema()), table.getSchema());}
public void beam_f26410_0() throws Exception
{    InMemoryMetaStore metaStore = new InMemoryMetaStore();    metaStore.registerProvider(new TextTableProvider());    BeamSqlCli cli = new BeamSqlCli().metaStore(metaStore);    cli.execute("CREATE EXTERNAL TABLE person (\n" + "id int COMMENT 'id', \n" + "name varchar COMMENT 'name', \n" + "age int COMMENT 'age', \n" + "tags MAP<VARCHAR, VARCHAR>, \n" + "nestedMap MAP<INTEGER, MAP<VARCHAR, INTEGER>> \n" + ") \n" + "TYPE 'text' \n" + "COMMENT '' LOCATION '/home/admin/orders'");    Table table = metaStore.getTables().get("person");    assertNotNull(table);    assertEquals(Stream.of(Field.of("id", INTEGER).withDescription("id").withNullable(true), Field.of("name", VARCHAR).withDescription("name").withNullable(true), Field.of("age", INTEGER).withDescription("age").withNullable(true), Field.of("tags", Schema.FieldType.map(VARCHAR, VARCHAR)).withNullable(true), Field.of("nestedMap", Schema.FieldType.map(INTEGER, Schema.FieldType.map(VARCHAR, INTEGER))).withNullable(true)).collect(toSchema()), table.getSchema());}
public void beam_f26418_0()
{    String sql = "SELECT COVAR_POP(f_int1, f_int2) FROM PCOLLECTION GROUP BY f_int3";    PAssert.that(boundedInput.apply(SqlTransform.query(sql))).satisfies(matchesScalar(1));    pipeline.run().waitUntilFinish();}
public void beam_f26419_0()
{    String sql = "SELECT COVAR_SAMP(f_double1, f_double2) FROM PCOLLECTION GROUP BY f_int3";    PAssert.that(boundedInput.apply(SqlTransform.query(sql))).satisfies(matchesScalar(2.3, PRECISION));    pipeline.run().waitUntilFinish();}
public void beam_f26420_0()
{    String sql = "SELECT COVAR_SAMP(f_int1, f_int2) FROM PCOLLECTION GROUP BY f_int3";    PAssert.that(boundedInput.apply(SqlTransform.query(sql))).satisfies(matchesScalar(2));    pipeline.run().waitUntilFinish();}
public void beam_f26428_0()
{    String sql = "SELECT AVG(f_int1), f_int2 FROM PCOLLECTION GROUP BY f_int2";    PCollection<Row> out = boundedInput.apply(SqlTransform.query(sql));    Schema schema = out.getSchema();    PAssert.that(out).containsInAnyOrder(Row.withSchema(schema).addValues(null, null).build(), Row.withSchema(schema).addValues(2, 1).build(), Row.withSchema(schema).addValues(1, 5).build(), Row.withSchema(schema).addValues(3, 2).build());    pipeline.run();}
public void beam_f26429_0()
{    String sql = "SELECT COUNT(f_int1) as c, f_int2 FROM PCOLLECTION GROUP BY f_int2";    PCollection<Row> out = boundedInput.apply(SqlTransform.query(sql));    Schema schema = out.getSchema();    PAssert.that(out).containsInAnyOrder(Row.withSchema(schema).addValues(0L, null).build(), Row.withSchema(schema).addValues(1L, 1).build(), Row.withSchema(schema).addValues(1L, 5).build(), Row.withSchema(schema).addValues(1L, 2).build());    assertEquals(Schema.builder().addInt64Field("c").addNullableField("f_int2", Schema.FieldType.INT32).build(), schema);    pipeline.run();}
public void beam_f26430_0()
{        String sql = "SELECT COVAR_SAMP(f_int1, f_int2) FROM PCOLLECTION GROUP BY f_int3";                    PAssert.that(boundedInput.apply(SqlTransform.query(sql))).satisfies(matchesScalar(-1));    pipeline.run();}
private void beam_f26438_0(PCollection<Row> input) throws Exception
{    String sql = "select f_int2, count(*) as getFieldCount, " + "sum(f_long) as sum1, avg(f_long) as avg1, " + "max(f_long) as max1, min(f_long) as min1, " + "sum(f_short) as sum2, avg(f_short) as avg2, " + "max(f_short) as max2, min(f_short) as min2, " + "sum(f_byte) as sum3, avg(f_byte) as avg3, " + "max(f_byte) as max3, min(f_byte) as min3, " + "sum(f_float) as sum4, avg(f_float) as avg4, " + "max(f_float) as max4, min(f_float) as min4, " + "sum(f_double) as sum5, avg(f_double) as avg5, " + "max(f_double) as max5, min(f_double) as min5, " + "max(f_timestamp) as max6, min(f_timestamp) as min6, " + "max(f_string) as max7, min(f_string) as min7, " + "var_pop(f_double) as varpop1, var_samp(f_double) as varsamp1, " + "var_pop(f_int) as varpop2, var_samp(f_int) as varsamp2 " + "FROM TABLE_A group by f_int2";    PCollection<Row> result = PCollectionTuple.of(new TupleTag<>("TABLE_A"), input).apply("testAggregationFunctions", SqlTransform.query(sql));    Schema resultType = Schema.builder().addInt32Field("f_int2").addInt64Field("size").addInt64Field("sum1").addInt64Field("avg1").addInt64Field("max1").addInt64Field("min1").addInt16Field("sum2").addInt16Field("avg2").addInt16Field("max2").addInt16Field("min2").addByteField("sum3").addByteField("avg3").addByteField("max3").addByteField("min3").addFloatField("sum4").addFloatField("avg4").addFloatField("max4").addFloatField("min4").addDoubleField("sum5").addDoubleField("avg5").addDoubleField("max5").addDoubleField("min5").addDateTimeField("max6").addDateTimeField("min6").addStringField("max7").addStringField("min7").addDoubleField("varpop1").addDoubleField("varsamp1").addInt32Field("varpop2").addInt32Field("varsamp2").build();    Row row = Row.withSchema(resultType).addValues(0, 4L, 10000L, 2500L, 4000L, 1000L, (short) 10, (short) 2, (short) 4, (short) 1, (byte) 10, (byte) 2, (byte) 4, (byte) 1, 10.0F, 2.5F, 4.0F, 1.0F, 10.0, 2.5, 4.0, 1.0, parseTimestampWithoutTimeZone("2017-01-01 02:04:03"), parseTimestampWithoutTimeZone("2017-01-01 01:01:03"), "第四行", "string_row1", 1.25, 1.666666667, 1, 1).build();    PAssert.that(result).containsInAnyOrder(row);    pipeline.run().waitUntilFinish();}
public Void beam_f26439_0(Iterable<Row> input)
{    Iterator<Row> iter = input.iterator();    assertTrue(iter.hasNext());    Row row = iter.next();    assertEquals(row.getDouble("avg1"), 8.142857143, 1e-7);    assertTrue(row.getInt32("avg2") == 8);    assertEquals(row.getDouble("varpop1"), 26.40816326, 1e-7);    assertTrue(row.getInt32("varpop2") == 26);    assertEquals(row.getDouble("varsamp1"), 30.80952381, 1e-7);    assertTrue(row.getInt32("varsamp2") == 30);    assertFalse(iter.hasNext());    return null;}
public void beam_f26440_0() throws Exception
{    String sql = "SELECT AVG(f_double) as avg1, AVG(f_int) as avg2, " + "VAR_POP(f_double) as varpop1, VAR_POP(f_int) as varpop2, " + "VAR_SAMP(f_double) as varsamp1, VAR_SAMP(f_int) as varsamp2 " + "FROM PCOLLECTION GROUP BY f_int2";    PCollection<Row> result = boundedInput3.apply("testAggregationWithDecimalValue", SqlTransform.query(sql));    PAssert.that(result).satisfies(new CheckerBigDecimalDivide());    pipeline.run().waitUntilFinish();}
private void beam_f26448_0(PCollection<Row> input) throws Exception
{    String sql = "SELECT f_int2, COUNT(*) AS `getFieldCount`," + " TUMBLE_START(f_timestamp, INTERVAL '1' HOUR) AS `window_start`, " + " TUMBLE_END(f_timestamp, INTERVAL '1' HOUR) AS `window_end` " + " FROM TABLE_A" + " GROUP BY f_int2, TUMBLE(f_timestamp, INTERVAL '1' HOUR)";    PCollection<Row> result = PCollectionTuple.of(new TupleTag<>("TABLE_A"), input).apply("testTumbleWindow", SqlTransform.query(sql));    Schema resultType = Schema.builder().addInt32Field("f_int2").addInt64Field("size").addDateTimeField("window_start").addDateTimeField("window_end").build();    List<Row> expectedRows = TestUtils.RowsBuilder.of(resultType).addRows(0, 3L, parseTimestampWithoutTimeZone("2017-01-01 01:00:00"), parseTimestampWithoutTimeZone("2017-01-01 02:00:00"), 0, 1L, parseTimestampWithoutTimeZone("2017-01-01 02:00:00"), parseTimestampWithoutTimeZone("2017-01-01 03:00:00")).getRows();    PAssert.that(result).containsInAnyOrder(expectedRows);    pipeline.run().waitUntilFinish();}
public void beam_f26449_0() throws Exception
{    Schema inputSchema = Schema.builder().addInt32Field("f_int").addDateTimeField("f_timestamp").build();    PCollection<Row> input = pipeline.apply(TestStream.create(inputSchema).addElements(Row.withSchema(inputSchema).addValues(1, parseTimestampWithoutTimeZone("2017-01-01 01:01:01")).build(), Row.withSchema(inputSchema).addValues(2, parseTimestampWithoutTimeZone("2017-01-01 01:01:01")).build()).addElements(Row.withSchema(inputSchema).addValues(3, parseTimestampWithoutTimeZone("2017-01-01 01:01:01")).build()).addElements(Row.withSchema(inputSchema).addValues(4, parseTimestampWithoutTimeZone("2017-01-01 01:01:01")).build()).advanceWatermarkToInfinity());    String sql = "SELECT SUM(f_int) AS f_int_sum FROM PCOLLECTION" + " GROUP BY TUMBLE(f_timestamp, INTERVAL '1' HOUR)";    Schema outputSchema = Schema.builder().addInt32Field("fn_int_sum").build();    PCollection<Row> result = input.apply("Triggering", Window.<Row>configure().triggering(Repeatedly.forever(AfterPane.elementCountAtLeast(1))).withAllowedLateness(Duration.ZERO).withOnTimeBehavior(Window.OnTimeBehavior.FIRE_IF_NON_EMPTY).accumulatingFiredPanes()).apply("Windowed Query", SqlTransform.query(sql));    PAssert.that(result).containsInAnyOrder(TestUtils.RowsBuilder.of(outputSchema).addRows(    3).addRows(    6).addRows(    10).getRows());    pipeline.run().waitUntilFinish();}
public void beam_f26450_0() throws Exception
{    runHopWindow(boundedInput1);}
public void beam_f26458_0() throws Exception
{    exceptions.expect(IllegalArgumentException.class);    exceptions.expectMessage(containsString("COUNT DISTINCT"));    pipeline.enableAbandonedNodeEnforcement(false);    String sql = "SELECT f_int2, COUNT(DISTINCT f_long)" + "FROM PCOLLECTION GROUP BY f_int2";    boundedInput1.apply("testUnsupportedDistinct", SqlTransform.query(sql));    pipeline.run().waitUntilFinish();}
public void beam_f26459_0()
{    exceptions.expect(UnsupportedOperationException.class);    pipeline.enableAbandonedNodeEnforcement(false);    PCollection<Row> input = unboundedInput1.apply("unboundedInput1.globalWindow", Window.<Row>into(new GlobalWindows()).triggering(DefaultTrigger.of()));    String sql = "SELECT f_int2, COUNT(*) AS `size` FROM PCOLLECTION GROUP BY f_int2";    input.apply("testUnsupportedGlobalWindows", SqlTransform.query(sql));}
public void beam_f26460_0() throws Exception
{    pipeline.enableAbandonedNodeEnforcement(false);    DateTime startTime = parseTimestampWithoutTimeZone("2017-1-1 0:0:0");    Schema type = Schema.builder().addInt32Field("f_intGroupingKey").addInt32Field("f_intValue").addDateTimeField("f_timestamp").build();    Object[] rows = new Object[] { 0, 1, startTime.plusSeconds(0), 0, 2, startTime.plusSeconds(1), 0, 3, startTime.plusSeconds(2), 0, 4, startTime.plusSeconds(3), 0, 5, startTime.plusSeconds(4), 0, 6, startTime.plusSeconds(6) };    PCollection<Row> input = createTestPCollection(type, rows, "f_timestamp").apply(Window.<Row>into(new GlobalWindows()).triggering(Repeatedly.forever(AfterPane.elementCountAtLeast(2))).discardingFiredPanes().withOnTimeBehavior(Window.OnTimeBehavior.FIRE_IF_NON_EMPTY));    String sql = "SELECT SUM(f_intValue) AS `sum` FROM PCOLLECTION GROUP BY f_intGroupingKey";    PCollection<Row> result = input.apply("sql", SqlTransform.query(sql));    assertEquals(new GlobalWindows(), result.getWindowingStrategy().getWindowFn());    PAssert.that(result).containsInAnyOrder(rowsWithSingleIntField("sum", Arrays.asList(3, 7, 11)));    pipeline.run();}
public void beam_f26468_0()
{    String sql = "SELECT VAR_POP(f_int) FROM PCOLLECTION GROUP BY f_int2";    PAssert.that(boundedInput.apply(SqlTransform.query(sql))).satisfies(matchesScalar(26));    pipeline.run().waitUntilFinish();}
public void beam_f26469_0()
{    String sql = "SELECT VAR_SAMP(f_double) FROM PCOLLECTION GROUP BY f_int2";    PAssert.that(boundedInput.apply(SqlTransform.query(sql))).satisfies(matchesScalar(30.80952381, PRECISION));    pipeline.run().waitUntilFinish();}
public void beam_f26470_0()
{    String sql = "SELECT VAR_SAMP(f_int) FROM PCOLLECTION GROUP BY f_int2";    PAssert.that(boundedInput.apply(SqlTransform.query(sql))).satisfies(matchesScalar(30));    pipeline.run().waitUntilFinish();}
public void beam_f26478_0()
{    Row row1 = Row.withSchema(INPUT_SCHEMA).addValues(42).addArray(Arrays.asList("111", "222", "333")).build();    Row row2 = Row.withSchema(INPUT_SCHEMA).addValues(13).addArray(Arrays.asList("444", "555")).build();    PCollection<Row> input = pipeline.apply("boundedInput1", Create.of(row1, row2).withRowSchema(INPUT_SCHEMA));        TupleTag<Row> mainTag = new TupleTag<Row>("main") {    };    PCollectionTuple inputTuple = PCollectionTuple.of(mainTag, input);    Schema resultType = Schema.builder().addInt32Field("f_int").addStringField("f_string").build();    PCollection<Row> result = inputTuple.apply("sqlQuery", SqlTransform.query("SELECT f_int, arrElems.f_string FROM main " + " CROSS JOIN UNNEST (main.f_stringArr) AS arrElems(f_string)"));    PAssert.that(result).containsInAnyOrder(Row.withSchema(resultType).addValues(42, "111").build(), Row.withSchema(resultType).addValues(42, "222").build(), Row.withSchema(resultType).addValues(42, "333").build(), Row.withSchema(resultType).addValues(13, "444").build(), Row.withSchema(resultType).addValues(13, "555").build());    pipeline.run();}
public void beam_f26479_0()
{    Schema elementSchema = Schema.builder().addStringField("f_rowString").addInt32Field("f_rowInt").build();    Schema resultSchema = Schema.builder().addArrayField("f_resultArray", Schema.FieldType.row(elementSchema)).build();    Schema inputType = Schema.builder().addInt32Field("f_int").addArrayField("f_arrayOfRows", Schema.FieldType.row(elementSchema)).build();    PCollection<Row> input = pipeline.apply(Create.of(Row.withSchema(inputType).addValues(1, Arrays.asList(Row.withSchema(elementSchema).addValues("AA", 11).build(), Row.withSchema(elementSchema).addValues("BB", 22).build())).build(), Row.withSchema(inputType).addValues(2, Arrays.asList(Row.withSchema(elementSchema).addValues("CC", 33).build(), Row.withSchema(elementSchema).addValues("DD", 44).build())).build()).withRowSchema(inputType));    PCollection<Row> result = input.apply(SqlTransform.query("SELECT f_arrayOfRows FROM PCOLLECTION")).setRowSchema(resultSchema);    PAssert.that(result).containsInAnyOrder(Row.withSchema(resultSchema).addArray(Arrays.asList(Row.withSchema(elementSchema).addValues("AA", 11).build(), Row.withSchema(elementSchema).addValues("BB", 22).build())).build(), Row.withSchema(resultSchema).addArray(Arrays.asList(Row.withSchema(elementSchema).addValues("CC", 33).build(), Row.withSchema(elementSchema).addValues("DD", 44).build())).build());    pipeline.run();}
public void beam_f26480_0()
{    Schema elementSchema = Schema.builder().addStringField("f_rowString").addInt32Field("f_rowInt").build();    Schema resultSchema = elementSchema;    Schema inputType = Schema.builder().addInt32Field("f_int").addArrayField("f_arrayOfRows", Schema.FieldType.row(elementSchema)).build();    PCollection<Row> input = pipeline.apply(Create.of(Row.withSchema(inputType).addValues(1, Arrays.asList(Row.withSchema(elementSchema).addValues("AA", 11).build(), Row.withSchema(elementSchema).addValues("BB", 22).build())).build(), Row.withSchema(inputType).addValues(2, Arrays.asList(Row.withSchema(elementSchema).addValues("CC", 33).build(), Row.withSchema(elementSchema).addValues("DD", 44).build())).build()).withRowSchema(inputType));    PCollection<Row> result = input.apply(SqlTransform.query("SELECT f_arrayOfRows[2] FROM PCOLLECTION")).setRowSchema(resultSchema);    PAssert.that(result).containsInAnyOrder(Row.withSchema(elementSchema).addValues("BB", 22).build(), Row.withSchema(elementSchema).addValues("DD", 44).build());    pipeline.run();}
public void beam_f26488_0()
{    String sql = "select * from CUSTOMER " + " where exists ( " + " select * from ORDERS " + " where o_custkey = c_custkey )";    PCollection<Row> rows = compilePipeline(sql, pipeline);    PAssert.that(rows).containsInAnyOrder(TestUtils.RowsBuilder.of(Schema.FieldType.INT32, "c_custkey", Schema.FieldType.DOUBLE, "c_acctbal", Schema.FieldType.STRING, "c_city").addRows(1, 1.0, "Seattle").getRows());    pipeline.run().waitUntilFinish();}
public void beam_f26489_0()
{    String sql = "select * from CUSTOMER " + " where not exists ( " + " select * from ORDERS " + " where o_custkey = c_custkey )";    PCollection<Row> rows = compilePipeline(sql, pipeline);    PAssert.that(rows).containsInAnyOrder(TestUtils.RowsBuilder.of(Schema.FieldType.INT32, "c_custkey", Schema.FieldType.DOUBLE, "c_acctbal", Schema.FieldType.STRING, "c_city").addRows(4, 4.0, "Portland", 5, 1.0, "Mountain View").getRows());    pipeline.run().waitUntilFinish();}
public void beam_f26490_0() throws Exception
{    runSingleFilter(boundedInput1);}
private void beam_f26498_0(PCollection<Row> input) throws Exception
{    String sql = "SELECT * FROM TABLE_A WHERE f_int < 1";    PCollection<Row> result = PCollectionTuple.of(new TupleTag<>("TABLE_A"), input).apply("testNoReturnFilter", SqlTransform.query(sql));    PAssert.that(result).empty();    pipeline.run().waitUntilFinish();}
public void beam_f26499_0() throws Exception
{    exceptions.expect(ParseException.class);    exceptions.expectCause(hasMessage(containsString("Object 'TABLE_B' not found")));    pipeline.enableAbandonedNodeEnforcement(false);    String sql = "SELECT * FROM TABLE_B WHERE f_int < 1";    PCollectionTuple.of(new TupleTag<>("TABLE_A"), boundedInput1).apply("testFromInvalidTableName1", SqlTransform.query(sql));    pipeline.run().waitUntilFinish();}
public void beam_f26500_0() throws Exception
{    exceptions.expect(ParseException.class);    exceptions.expectCause(hasMessage(containsString("Column 'f_int_na' not found in any table")));    pipeline.enableAbandonedNodeEnforcement(false);    String sql = "SELECT * FROM PCOLLECTION WHERE f_int_na = 0";    boundedInput1.apply(SqlTransform.query(sql));    pipeline.run().waitUntilFinish();}
public void beam_f26508_0() throws Exception
{    String sql = "SELECT o1.order_id, o1.price, o1.site_id, o2.order_id, o2.price, o2.site_id  " + "FROM ORDER_DETAILS1 o1" + " JOIN ORDER_DETAILS2 o2" + " on " + " o1.order_id=o2.site_id AND o2.price=o1.site_id";    PCollection<Row> orders = ordersUnbounded().apply("window", Window.into(FixedWindows.of(Duration.standardSeconds(50))));    PCollectionTuple inputs = tuple("ORDER_DETAILS1", orders, "ORDER_DETAILS2", orders);    PAssert.that(inputs.apply("sql", SqlTransform.query(sql))).containsInAnyOrder(TestUtils.RowsBuilder.of(RESULT_ROW_TYPE).addRows(1, 2, 2, 2, 2, 1, 1, 4, 3, 3, 3, 1).getRows());    pipeline.run();}
public void beam_f26509_0() throws Exception
{    String sql = "SELECT o1.order_id, o1.price, o1.site_id, o2.order_id, o2.price, o2.site_id  " + "FROM ORDER_DETAILS1 o1" + " JOIN ORDER_DETAILS2 o2" + " on " + " o1.order_id=o2.site_id AND o2.price=o1.site_id";    PCollection<Row> orders = ordersUnbounded().apply("window", Window.<Row>into(FixedWindows.of(Duration.standardSeconds(50))).triggering(AfterWatermark.pastEndOfWindow()).withAllowedLateness(Duration.ZERO).accumulatingFiredPanes());    PCollectionTuple inputs = tuple("ORDER_DETAILS1", orders, "ORDER_DETAILS2", orders);    thrown.expect(UnsupportedOperationException.class);    thrown.expectMessage(stringContainsInOrder(Arrays.asList("once per window", "default trigger")));    inputs.apply("sql", SqlTransform.query(sql));    pipeline.run();}
public void beam_f26510_0() throws Exception
{    String sql = "SELECT *  " + "FROM ORDER_DETAILS1 o1" + " JOIN ORDER_DETAILS2 o2" + " on " + " o1.order_id=o2.site_id AND o2.price=o1.site_id";    PCollection<Row> orders = ordersUnbounded();    PCollectionTuple inputs = tuple("ORDER_DETAILS1", orders, "ORDER_DETAILS2", orders);    thrown.expect(UnsupportedOperationException.class);    thrown.expectMessage(stringContainsInOrder(Arrays.asList("once per window", "default trigger")));    inputs.apply("sql", SqlTransform.query(sql));    pipeline.run();}
public void beam_f26518_0()
{    Schema resultSchema = Schema.builder().addArrayField("f_nestedArray", Schema.FieldType.STRING).build();    Schema nestedSchema = Schema.builder().addInt32Field("f_nestedInt").addStringField("f_nestedString").addInt32Field("f_nestedIntPlusOne").addArrayField("f_nestedArray", Schema.FieldType.STRING).build();    Schema inputType = Schema.builder().addInt32Field("f_int").addRowField("f_nestedRow", nestedSchema).build();    PCollection<Row> input = pipeline.apply(Create.of(Row.withSchema(inputType).addValues(1, Row.withSchema(nestedSchema).addValues(312, "CC", 313, Arrays.asList("one", "two")).build()).build(), Row.withSchema(inputType).addValues(2, Row.withSchema(nestedSchema).addValues(412, "DD", 413, Arrays.asList("three", "four")).build()).build()).withRowSchema(inputType));    PCollection<Row> result = input.apply(SqlTransform.query("SELECT `PCOLLECTION`.`f_nestedRow`.`f_nestedArray` FROM PCOLLECTION")).setRowSchema(resultSchema);    PAssert.that(result).containsInAnyOrder(Row.withSchema(resultSchema).addArray(Arrays.asList("one", "two")).build(), Row.withSchema(resultSchema).addArray(Arrays.asList("three", "four")).build());    pipeline.run();}
public void beam_f26519_0()
{    Schema resultSchema = Schema.builder().addStringField("f_nestedArrayStringField").build();    Schema nestedSchema = Schema.builder().addInt32Field("f_nestedInt").addStringField("f_nestedString").addInt32Field("f_nestedIntPlusOne").addArrayField("f_nestedArray", Schema.FieldType.STRING).build();    Schema inputType = Schema.builder().addInt32Field("f_int").addRowField("f_nestedRow", nestedSchema).build();    PCollection<Row> input = pipeline.apply(Create.of(Row.withSchema(inputType).addValues(1, Row.withSchema(nestedSchema).addValues(312, "CC", 313, Arrays.asList("one", "two")).build()).build(), Row.withSchema(inputType).addValues(2, Row.withSchema(nestedSchema).addValues(412, "DD", 413, Arrays.asList("three", "four")).build()).build()).withRowSchema(inputType));    PCollection<Row> result = input.apply(SqlTransform.query("SELECT `PCOLLECTION`.`f_nestedRow`.`f_nestedArray`[2] FROM PCOLLECTION")).setRowSchema(resultSchema);    PAssert.that(result).containsInAnyOrder(Row.withSchema(resultSchema).addValues("two").build(), Row.withSchema(resultSchema).addValues("four").build());    pipeline.run();}
public void beam_f26520_0() throws Exception
{    runSelectAll(boundedInput2);}
private void beam_f26528_0(PCollection<Row> input) throws Exception
{    String sql = "SELECT f_int, f_long FROM TABLE_A";    PCollection<Row> result = PCollectionTuple.of(new TupleTag<>("TABLE_A"), input).apply("testPartialFieldsInMultipleRow", SqlTransform.query(sql));    Schema resultType = Schema.builder().addInt32Field("f_int").addInt64Field("f_long").build();    List<Row> expectedRows = IntStream.range(0, 4).mapToObj(i -> rowAtIndex(resultType, i)).collect(toList());    PAssert.that(result).containsInAnyOrder(expectedRows);    pipeline.run();}
private Row beam_f26529_0(Schema schema, int index)
{    return Row.withSchema(schema).addValues(rowsInTableA.get(index).getValue(0), rowsInTableA.get(index).getValue(1)).build();}
public void beam_f26530_0() throws Exception
{    runPartialFieldsInRows(boundedInput1);}
public void beam_f26538_0()
{    Schema outputSchema = Schema.of(Schema.Field.of("c_bytes", Schema.FieldType.BYTES));    PCollection<Row> result = PCollectionTuple.empty(pipeline).apply(SqlTransform.query("SELECT x'baadcafe' as c_bytes"));    PAssert.that(result).containsInAnyOrder(Row.withSchema(outputSchema).addValue(new byte[] { -70, -83, -54, -2 }).build());    pipeline.run();}
private static SqlOperatorId beam_f26539_0(String nameAndKind)
{    return sqlOperatorId(nameAndKind, SqlKind.valueOf(nameAndKind));}
private static SqlOperatorId beam_f26540_0(String name, SqlKind kind)
{    return new AutoValue_BeamSqlDslSqlStdOperatorsTest_SqlOperatorId(name, kind);}
public void beam_f26548_0()
{    ExpressionChecker checker = new ExpressionChecker().addExpr("1 = 1 AND 1 = 1", true).addExpr("1 = 1 OR 1 = 2", true).addExpr("NOT 1 = 2", true).addExpr("(NOT 1 = 2) AND (2 = 1 OR 3 = 3)", true).addExpr("2 = 2 AND 2 = 1", false).addExpr("1 = 2 OR 3 = 2", false).addExpr("NOT 1 = 1", false).addExpr("(NOT 2 = 2) AND (1 = 2 OR 2 = 3)", false).addExpr("'a' = 'a' AND 'a' = 'a'", true).addExpr("'a' = 'a' OR 'a' = 'b'", true).addExpr("NOT 'a' = 'b'", true).addExpr("(NOT 'a' = 'b') AND ('b' = 'a' OR 'c' = 'c')", true).addExpr("'b' = 'b' AND 'b' = 'a'", false).addExpr("'a' = 'b' OR 'c' = 'b'", false).addExpr("NOT 'a' = 'a'", false).addExpr("(NOT 'b' = 'b') AND ('a' = 'b' OR 'b' = 'c')", false).addExpr("1.0 = 1.0 AND 1.0 = 1.0", true).addExpr("1.0 = 1.0 OR 1.0 = 2.0", true).addExpr("NOT 1.0 = 2.0", true).addExpr("(NOT 1.0 = 2.0) AND (2.0 = 1.0 OR 3.0 = 3.0)", true).addExpr("2.0 = 2.0 AND 2.0 = 1.0", false).addExpr("1.0 = 2.0 OR 3.0 = 2.0", false).addExpr("NOT 1.0 = 1.0", false).addExpr("(NOT 2.0 = 2.0) AND (1.0 = 2.0 OR 2.0 = 3.0)", false).addExpr("NOT true", false).addExpr("NOT false", true).addExpr("true AND true", true).addExpr("true AND false", false).addExpr("false AND false", false).addExpr("true OR true", true).addExpr("true OR false", true).addExpr("false OR false", false).addExpr("(NOT false) AND (true OR false)", true).addExpr("(NOT true) AND (true OR false)", false).addExpr("(NOT false) OR (true and false)", true).addExpr("(NOT true) OR (true and false)", false);    checker.buildRunAndCheck();}
public void beam_f26549_0()
{    ExpressionChecker checker = new ExpressionChecker().addExpr("1 + 1", 2).addExpr("1.0 + 1", TWO).addExpr("1 + 1.0", TWO).addExpr("1.0 + 1.0", TWO).addExpr("c_tinyint + c_tinyint", (byte) 2).addExpr("c_smallint + c_smallint", (short) 2).addExpr("c_bigint + c_bigint", 2L).addExpr("c_decimal + c_decimal", TWO0).addExpr("c_tinyint + c_decimal", TWO0).addExpr("c_float + c_decimal", 2.0).addExpr("c_double + c_decimal", 2.0).addExpr("c_float + c_float", 2.0f).addExpr("c_double + c_float", 2.0).addExpr("c_double + c_double", 2.0).addExpr("c_float + c_bigint", 2.0f).addExpr("c_double + c_bigint", 2.0).addExpr("1 - 1", 0).addExpr("1.0 - 1", ZERO).addExpr("1 - 0.0", ONE).addExpr("1.0 - 1.0", ZERO).addExpr("c_tinyint - c_tinyint", (byte) 0).addExpr("c_smallint - c_smallint", (short) 0).addExpr("c_bigint - c_bigint", 0L).addExpr("c_decimal - c_decimal", BigDecimal.ZERO).addExpr("c_tinyint - c_decimal", BigDecimal.ZERO).addExpr("c_float - c_decimal", 0.0).addExpr("c_double - c_decimal", 0.0).addExpr("c_float - c_float", 0.0f).addExpr("c_double - c_float", 0.0).addExpr("c_double - c_double", 0.0).addExpr("c_float - c_bigint", 0.0f).addExpr("c_double - c_bigint", 0.0).addExpr("1 * 1", 1).addExpr("1.0 * 1", ONE).addExpr("1 * 1.0", ONE).addExpr("1.0 * 1.0", ONE2).addExpr("c_tinyint * c_tinyint", (byte) 1).addExpr("c_smallint * c_smallint", (short) 1).addExpr("c_bigint * c_bigint", 1L).addExpr("c_decimal * c_decimal", BigDecimal.ONE).addExpr("c_tinyint * c_decimal", BigDecimal.ONE).addExpr("c_float * c_decimal", 1.0).addExpr("c_double * c_decimal", 1.0).addExpr("c_float * c_float", 1.0f).addExpr("c_double * c_float", 1.0).addExpr("c_double * c_double", 1.0).addExpr("c_float * c_bigint", 1.0f).addExpr("c_double * c_bigint", 1.0).addExpr("1 / 1", 1).addExpr("1.0 / 1", ONE).addExpr("1 / 1.0", BigDecimal.ONE).addExpr("1.0 / 1.0", BigDecimal.ONE).addExpr("c_tinyint / c_tinyint", (byte) 1).addExpr("c_smallint / c_smallint", (short) 1).addExpr("c_bigint / c_bigint", 1L).addExpr("c_decimal / c_decimal", BigDecimal.ONE).addExpr("c_tinyint / c_decimal", BigDecimal.ONE).addExpr("c_float / c_decimal", 1.0).addExpr("c_double / c_decimal", 1.0).addExpr("c_float / c_float", 1.0f).addExpr("c_double / c_float", 1.0).addExpr("c_double / c_double", 1.0).addExpr("c_float / c_bigint", 1.0f).addExpr("c_double / c_bigint", 1.0).addExpr("mod(1, 1)", 0).addExpr("mod(1.0, 1)", 0).addExpr("mod(1, 1.0)", BigDecimal.ZERO).addExpr("mod(1.0, 1.0)", ZERO).addExpr("mod(c_tinyint, c_tinyint)", (byte) 0).addExpr("mod(c_smallint, c_smallint)", (short) 0).addExpr("mod(c_bigint, c_bigint)", 0L).addExpr("mod(c_decimal, c_decimal)", BigDecimal.ZERO).addExpr("mod(c_tinyint, c_decimal)", BigDecimal.ZERO).addExpr("c_tinyint_max + c_tinyint_max", (byte) -2).addExpr("c_smallint_max + c_smallint_max", (short) -2).addExpr("c_integer_max + c_integer_max", -2).addExpr("c_bigint_max + c_bigint_max", -2L);    checker.buildRunAndCheck();}
public void beam_f26550_0()
{    ExpressionChecker checker = new ExpressionChecker().addExpr("'string_true_test' LIKE 'string_true_test'", true).addExpr("'string_true_test' LIKE 'string_false_test'", false).addExpr("'string_false_test' LIKE 'string_false_test'", true).addExpr("'string_false_test' LIKE 'string_true_test'", false).addExpr("'string_true_test' LIKE 'string_true_test%'", true).addExpr("'string_true_test' LIKE 'string_false_test%'", false).addExpr("'string_true_test' LIKE 'string_true%'", true).addExpr("'string_true_test' LIKE 'string_false%'", false).addExpr("'string_true_test' LIKE 'string%test'", true).addExpr("'string_true_test' LIKE '%test'", true).addExpr("'string_true_test' LIKE '%string_true_test'", true).addExpr("'string_true_test' LIKE '%string_false_test'", false).addExpr("'string_true_test' LIKE '%false_test'", false).addExpr("'string_false_test' LIKE '%false_test'", true).addExpr("'string_true_test' LIKE 'string_tr_e_test'", true).addExpr("'string_true_test' LIKE 'string______test'", true).addExpr("'string_false_test' LIKE 'string______test'", false).addExpr("'string_false_test' LIKE 'string_______test'", true).addExpr("'string_false_test' LIKE 'string_false_te__'", true).addExpr("'string_false_test' LIKE 'string_false_te___'", false).addExpr("'string_false_test' LIKE 'string_false_te_'", false).addExpr("'string_true_test' LIKE 'string_true_te__'", true).addExpr("'string_true_test' LIKE '_ring_true_te__'", false).addExpr("'string_false_test' LIKE '__ring_false_te__'", true).addExpr("'string_true_test' LIKE '_%ring_true_te__'", true).addExpr("'string_true_test' LIKE '_%tring_true_te__'", true).addExpr("'string_false_test' LIKE 'string_false_te%__'", true).addExpr("'string_false_test' LIKE 'string_false_te__%'", true).addExpr("'string_false_test' LIKE 'string_false_t%__'", true).addExpr("'string_false_test' LIKE 'string_false_t__%'", true).addExpr("'string_false_test' LIKE 'string_false_te_%'", true).addExpr("'string_false_test' LIKE 'string_false_te%_'", true).addExpr("'string_true_test' LIKE 'string_%test'", true).addExpr("'string_true_test' LIKE 'string%_test'", true).addExpr("'string_true_test' LIKE 'string_%_test'", true).addExpr("'string_true_test' NOT LIKE 'string_true_test'", false).addExpr("'string_true_test' NOT LIKE 'string_false_test'", true).addExpr("'string_false_test' NOT LIKE 'string_false_test'", false).addExpr("'string_false_test' NOT LIKE 'string_true_test'", true).addExpr("'string_true_test' NOT LIKE 'string_true_test%'", false).addExpr("'string_true_test' NOT LIKE 'string_false_test%'", true).addExpr("'string_true_test' NOT LIKE 'string_true%'", false).addExpr("'string_true_test' NOT LIKE 'string_false%'", true).addExpr("'string_true_test' NOT LIKE 'string%test'", false).addExpr("'string_true_test' NOT LIKE '%test'", false).addExpr("'string_true_test' NOT LIKE '%string_true_test'", false).addExpr("'string_true_test' NOT LIKE '%string_false_test'", true).addExpr("'string_true_test' NOT LIKE '%false_test'", true).addExpr("'string_false_test' NOT LIKE '%false_test'", false).addExpr("'string_true_test' NOT LIKE 'string_tr_e_test'", false).addExpr("'string_true_test' NOT LIKE 'string______test'", false).addExpr("'string_false_test' NOT LIKE 'string______test'", true).addExpr("'string_false_test' NOT LIKE 'string_______test'", false).addExpr("'string_false_test' NOT LIKE 'string_false_te__'", false).addExpr("'string_false_test' NOT LIKE 'string_false_te___'", true).addExpr("'string_false_test' NOT LIKE 'string_false_te_'", true).addExpr("'string_true_test' NOT LIKE 'string_true_te__'", false).addExpr("'string_true_test' NOT LIKE '_ring_true_te__'", true).addExpr("'string_false_test' NOT LIKE '__ring_false_te__'", false).addExpr("'string_true_test' NOT LIKE '_%ring_true_te__'", false).addExpr("'string_true_test' NOT LIKE '_%tring_true_te__'", false).addExpr("'string_false_test' NOT LIKE 'string_false_te%__'", false).addExpr("'string_false_test' NOT LIKE 'string_false_te__%'", false).addExpr("'string_false_test' NOT LIKE 'string_false_t%__'", false).addExpr("'string_false_test' NOT LIKE 'string_false_t__%'", false).addExpr("'string_false_test' NOT LIKE 'string_false_te_%'", false).addExpr("'string_false_test' NOT LIKE 'string_false_te%_'", false).addExpr("'string_true_test' NOT LIKE 'string_%test'", false).addExpr("'string_true_test' NOT LIKE 'string%_test'", false).addExpr("'string_true_test' NOT LIKE 'string_%_test'", false);    checker.buildRunAndCheck();}
public void beam_f26558_0()
{    ExpressionChecker checker = new ExpressionChecker().addExpr("VAR_POP(c_integer)", 0).addExpr("VAR_POP(c_double)", 0.6666666);    checker.buildRunAndCheck(getAggregationTestPCollection());}
public void beam_f26559_0()
{    ExpressionChecker checker = new ExpressionChecker().addExpr("VAR_SAMP(c_integer)", 1).addExpr("VAR_SAMP(c_double)", 1.0);    checker.buildRunAndCheck(getAggregationTestPCollection());}
public void beam_f26560_0()
{    ExpressionChecker checker = new ExpressionChecker().addExpr("COVAR_POP(c_integer, c_integer_two)", 0).addExpr("COVAR_POP(c_double, c_double_two)", 0.6666666);    checker.buildRunAndCheck(getAggregationTestPCollection());}
public void beam_f26568_0()
{    ExpressionChecker checker = new ExpressionChecker().addExpr("EXP(c_integer)", Math.exp(INTEGER_VALUE)).addExpr("EXP(c_bigint)", Math.exp(LONG_VALUE)).addExpr("EXP(c_smallint)", Math.exp(SHORT_VALUE)).addExpr("EXP(c_tinyint)", Math.exp(BYTE_VALUE)).addExpr("EXP(c_double)", Math.exp(DOUBLE_VALUE)).addExpr("EXP(c_float)", Math.exp(FLOAT_VALUE)).addExpr("EXP(c_decimal)", Math.exp(ONE.doubleValue()));    checker.buildRunAndCheck();}
public void beam_f26569_0()
{    ExpressionChecker checker = new ExpressionChecker().addExpr("ACOS(c_integer)", Math.acos(INTEGER_VALUE)).addExpr("ACOS(c_bigint)", Math.acos(LONG_VALUE)).addExpr("ACOS(c_smallint)", Math.acos(SHORT_VALUE)).addExpr("ACOS(c_tinyint)", Math.acos(BYTE_VALUE)).addExpr("ACOS(c_double)", Math.acos(DOUBLE_VALUE)).addExpr("ACOS(c_float)", Math.acos(FLOAT_VALUE)).addExpr("ACOS(c_decimal)", Math.acos(ONE.doubleValue()));    checker.buildRunAndCheck();}
public void beam_f26570_0()
{    ExpressionChecker checker = new ExpressionChecker().addExpr("ASIN(c_integer)", Math.asin(INTEGER_VALUE)).addExpr("ASIN(c_bigint)", Math.asin(LONG_VALUE)).addExpr("ASIN(c_smallint)", Math.asin(SHORT_VALUE)).addExpr("ASIN(c_tinyint)", Math.asin(BYTE_VALUE)).addExpr("ASIN(c_double)", Math.asin(DOUBLE_VALUE)).addExpr("ASIN(c_float)", Math.asin(FLOAT_VALUE)).addExpr("ASIN(c_decimal)", Math.asin(ONE.doubleValue()));    checker.buildRunAndCheck();}
public void beam_f26578_0()
{    ExpressionChecker checker = new ExpressionChecker().addExpr("SIGN(c_integer)", Integer.signum(INTEGER_VALUE)).addExpr("SIGN(c_bigint)", (long) (Long.signum(LONG_VALUE))).addExpr("SIGN(c_smallint)", (short) (Integer.signum(SHORT_VALUE))).addExpr("SIGN(c_tinyint)", (byte) Integer.signum(BYTE_VALUE)).addExpr("SIGN(c_double)", Math.signum(DOUBLE_VALUE)).addExpr("SIGN(c_float)", Math.signum(FLOAT_VALUE)).addExpr("SIGN(c_decimal)", BigDecimal.valueOf(ONE.signum()));    checker.buildRunAndCheck();}
public void beam_f26579_0()
{    ExpressionChecker checker = new ExpressionChecker().addExpr("POWER(c_integer, 2)", Math.pow(INTEGER_VALUE, 2)).addExpr("POWER(c_bigint, 2)", Math.pow(LONG_VALUE, 2)).addExpr("POWER(c_smallint, 2)", Math.pow(SHORT_VALUE, 2)).addExpr("POWER(c_tinyint, 2)", Math.pow(BYTE_VALUE, 2)).addExpr("POWER(c_double, 2)", Math.pow(DOUBLE_VALUE, 2)).addExpr("POWER(c_float, 2)", Math.pow(FLOAT_VALUE, 2)).addExpr("POWER(c_decimal, 2)", Math.pow(ONE.doubleValue(), 2));    checker.buildRunAndCheck();}
public void beam_f26580_0() throws Exception
{    ExpressionChecker checker = new ExpressionChecker().addExpr("PI", Math.PI);    checker.buildRunAndCheck();}
public void beam_f26588_0()
{    ExpressionChecker checker = new ExpressionChecker().addExpr("CEIL(ts TO SECOND)", parseTimestampWithUTCTimeZone("1986-02-15 11:35:26")).addExpr("CEIL(ts TO MINUTE)", parseTimestampWithUTCTimeZone("1986-02-15 11:36:00")).addExpr("CEIL(ts TO HOUR)", parseTimestampWithUTCTimeZone("1986-02-15 12:00:00")).addExpr("CEIL(ts TO DAY)", parseTimestampWithUTCTimeZone("1986-02-16 00:00:00")).addExpr("CEIL(ts TO MONTH)", parseTimestampWithUTCTimeZone("1986-03-01 00:00:00")).addExpr("CEIL(ts TO YEAR)", parseTimestampWithUTCTimeZone("1987-01-01 00:00:00")).addExpr("CEIL(c_double)", 2.0);    checker.buildRunAndCheck(getFloorCeilingTestPCollection());}
public void beam_f26589_0()
{    ExpressionChecker checker = new ExpressionChecker().addExpr("TIMESTAMPADD(SECOND, 3, TIMESTAMP '1984-04-19 01:02:03')", parseTimestampWithUTCTimeZone("1984-04-19 01:02:06")).addExpr("TIMESTAMPADD(MINUTE, 3, TIMESTAMP '1984-04-19 01:02:03')", parseTimestampWithUTCTimeZone("1984-04-19 01:05:03")).addExpr("TIMESTAMPADD(HOUR, 3, TIMESTAMP '1984-04-19 01:02:03')", parseTimestampWithUTCTimeZone("1984-04-19 04:02:03")).addExpr("TIMESTAMPADD(DAY, 3, TIMESTAMP '1984-04-19 01:02:03')", parseTimestampWithUTCTimeZone("1984-04-22 01:02:03")).addExpr("TIMESTAMPADD(MONTH, 2, TIMESTAMP '1984-01-19 01:02:03')", parseTimestampWithUTCTimeZone("1984-03-19 01:02:03")).addExpr("TIMESTAMPADD(YEAR, 2, TIMESTAMP '1985-01-19 01:02:03')", parseTimestampWithUTCTimeZone("1987-01-19 01:02:03"));    checker.buildRunAndCheck();}
public void beam_f26590_0()
{    ExpressionChecker checker = new ExpressionChecker().addExpr("TIMESTAMP '1984-01-19 01:02:03' + INTERVAL '3' SECOND", parseTimestampWithUTCTimeZone("1984-01-19 01:02:06")).addExpr("TIMESTAMP '1984-01-19 01:02:03' + INTERVAL '2' MINUTE", parseTimestampWithUTCTimeZone("1984-01-19 01:04:03")).addExpr("TIMESTAMP '1984-01-19 01:02:03' + INTERVAL '2' HOUR", parseTimestampWithUTCTimeZone("1984-01-19 03:02:03")).addExpr("TIMESTAMP '1984-01-19 01:02:03' + INTERVAL '2' DAY", parseTimestampWithUTCTimeZone("1984-01-21 01:02:03")).addExpr("TIMESTAMP '1984-01-19 01:02:03' + INTERVAL '2' MONTH", parseTimestampWithUTCTimeZone("1984-03-19 01:02:03")).addExpr("TIMESTAMP '1984-01-19 01:02:03' + INTERVAL '2' YEAR", parseTimestampWithUTCTimeZone("1986-01-19 01:02:03")).addExpr("DATE '1984-04-19' + INTERVAL '2' DAY", parseDate("1984-04-21")).addExpr("DATE '1984-04-19' + INTERVAL '1' MONTH", parseDate("1984-05-19")).addExpr("DATE '1984-04-19' + INTERVAL '3' YEAR", parseDate("1987-04-19")).addExpr("TIME '14:28:30' + INTERVAL '15' SECOND", parseTime("14:28:45")).addExpr("TIME '14:28:30.239' + INTERVAL '4' MINUTE", parseTime("14:32:30.239")).addExpr("TIME '14:28:30.2' + INTERVAL '4' HOUR", parseTime("18:28:30.2"));    checker.buildRunAndCheck();}
public void beam_f26598_0() throws Exception
{    Schema resultType = Schema.builder().addInt32Field("f_int").addInt32Field("cubicvalue").build();    Row row = Row.withSchema(resultType).addValues(2, 8).build();    String sql1 = "SELECT f_int, cubic1(f_int) as cubicvalue FROM PCOLLECTION WHERE f_int = 2";    PCollection<Row> result1 = boundedInput1.apply("testUdf1", SqlTransform.query(sql1).registerUdf("cubic1", CubicInteger.class));    PAssert.that(result1).containsInAnyOrder(row);    String sql2 = "SELECT f_int, cubic2(f_int) as cubicvalue FROM PCOLLECTION WHERE f_int = 2";    PCollection<Row> result2 = PCollectionTuple.of(new TupleTag<>("PCOLLECTION"), boundedInput1).apply("testUdf2", SqlTransform.query(sql2).registerUdf("cubic2", new CubicIntegerFn()));    PAssert.that(result2).containsInAnyOrder(row);    String sql3 = "SELECT f_int, substr(f_string) as sub_string FROM PCOLLECTION WHERE f_int = 2";    PCollection<Row> result3 = PCollectionTuple.of(new TupleTag<>("PCOLLECTION"), boundedInput1).apply("testUdf3", SqlTransform.query(sql3).registerUdf("substr", UdfFnWithDefault.class));    Schema subStrSchema = Schema.builder().addInt32Field("f_int").addStringField("sub_string").build();    Row subStrRow = Row.withSchema(subStrSchema).addValues(2, "s").build();    PAssert.that(result3).containsInAnyOrder(subStrRow);    pipeline.run().waitUntilFinish();}
public void beam_f26599_0() throws Exception
{    String sql1 = "SELECT * FROM table(range_udf(0, 3))";    Schema schema = Schema.of(Schema.Field.of("f0", Schema.FieldType.INT32));    PCollection<Row> rows = pipeline.apply(SqlTransform.query(sql1).registerUdf("range_udf", RangeUdf.class));    PAssert.that(rows).containsInAnyOrder(Row.withSchema(schema).addValue(0).build(), Row.withSchema(schema).addValue(1).build(), Row.withSchema(schema).addValue(2).build());    pipeline.run();}
public void beam_f26600_0() throws Exception
{    Schema resultType = Schema.builder().addInt32Field("f_int2").addInt32Field("autoload_squarecubicsum").build();    Row row = Row.withSchema(resultType).addValues(0, 4890).build();    String sql = "SELECT f_int2, autoload_squaresum(autoload_cubic(f_int)) AS `autoload_squarecubicsum`" + " FROM PCOLLECTION GROUP BY f_int2";    PCollection<Row> result = boundedInput1.apply("testUdaf", SqlTransform.query(sql).withAutoUdfUdafLoad(true));    PAssert.that(result).containsInAnyOrder(row);    pipeline.run().waitUntilFinish();}
public Instant beam_f26608_0(Instant accumulator, Instant input)
{    return accumulator.isBefore(input) ? input : accumulator;}
public Instant beam_f26609_0(Iterable<Instant> accumulators)
{    Instant v = new Instant(0L);    for (Instant accumulator : accumulators) {        v = accumulator.isBefore(v) ? v : accumulator;    }    return v;}
public Instant beam_f26610_0(Instant accumulator)
{    return accumulator;}
public static String beam_f26618_0(@Parameter(name = "s") String s, @Parameter(name = "n", optional = true) Integer n)
{    return s.substring(0, n == null ? 1 : n);}
public static Timestamp beam_f26619_0(Timestamp time)
{    return new Timestamp(time.getTime() - 24 * 3600 * 1000L);}
public static TranslatableTable beam_f26620_0(int startInclusive, int endExclusive)
{    Schema schema = Schema.of(Schema.Field.of("f0", Schema.FieldType.INT32));    Object[] values = IntStream.range(startInclusive, endExclusive).boxed().toArray();    return BeamCalciteTable.of(new TestBoundedTable(schema).addRows(values));}
public void beam_f26628_0()
{    PCollection<Row> input = pipeline.apply(create(row(1, "strstr")));    PCollection<Row> result = input.apply(SqlTransform.query("SELECT f_int, f_string FROM beam.PCOLLECTION"));    PAssert.that(result).containsInAnyOrder(row(1, "strstr"));    pipeline.run();}
public void beam_f26629_0()
{    PCollection<Row> input = pipeline.apply(create(row(1, "strstr")));    PCollection<Row> result = input.apply(SqlTransform.query("SELECT f_int, f_string FROM PCOLLECTION"));    PAssert.that(result).containsInAnyOrder(row(1, "strstr"));    pipeline.run();}
public void beam_f26630_0()
{    PCollection<Row> inputMain = pipeline.apply("mainInput", create(row(1, "pcollection_1"), row(2, "pcollection_2")));    PCollection<Row> inputExtra = pipeline.apply("extraInput", create(row(1, "_extra_table_1"), row(2, "_extra_table_2")));    TableProvider extraInputProvider = extraTableProvider("extraTable", inputExtra);    PCollection<Row> result = inputMain.apply(SqlTransform.query("SELECT f_int, f_string FROM extraSchema.extraTable").withTableProvider("extraSchema", extraInputProvider));    PAssert.that(result).containsInAnyOrder(row(1, "_extra_table_1"), row(2, "_extra_table_2"));    pipeline.run();}
private TableProvider beam_f26638_0(String tableName, PCollection<Row> rows)
{    return new ReadOnlyTableProvider("testExtraTableProvider", ImmutableMap.of(tableName, new BeamPCollectionTable<>(rows)));}
private Row beam_f26639_0(int fIntValue, String fStringValue)
{    return Row.withSchema(ROW_SCHEMA).addValues(fIntValue, fStringValue).build();}
private PTransform<PBegin, PCollection<Row>> beam_f26640_0(Row... rows)
{    return Create.of(Arrays.asList(rows)).withRowSchema(ROW_SCHEMA);}
private PCollection<Row> beam_f26648_0(String alias)
{    return pipeline.apply(SqlTransform.query(String.format("SELECT 321 AS %s", alias)));}
private Matcher<PCollection<Row>> beam_f26649_0(String expected)
{    return new BaseMatcher<PCollection<Row>>() {        @Override        public boolean matches(Object actual) {            PCollection<Row> result = (PCollection<Row>) actual;            PAssert.thatSingleton(result).satisfies(assertFieldNameIs(expected));            pipeline.run();            return true;        }        @Override        public void describeTo(Description description) {            description.appendText("field alias matches");        }    };}
public boolean beam_f26650_0(Object actual)
{    PCollection<Row> result = (PCollection<Row>) actual;    PAssert.thatSingleton(result).satisfies(assertFieldNameIs(expected));    pipeline.run();    return true;}
public void beam_f26658_0() throws Exception
{    Connection connection = DriverManager.getConnection(JdbcDriver.CONNECT_STRING_PREFIX);    Statement statement = connection.createStatement();        assertTrue(statement.execute("SELECT 1"));}
public void beam_f26659_0() throws Exception
{    Connection connection = DriverManager.getConnection(JdbcDriver.CONNECT_STRING_PREFIX);    SchemaPlus rootSchema = ((CalciteConnection) connection).getRootSchema();    BeamCalciteSchema beamSchema = (BeamCalciteSchema) CalciteSchema.from(rootSchema.getSubSchema("beam")).schema;    Map<String, String> pipelineOptions = beamSchema.getPipelineOptions();    assertThat(pipelineOptions.get("userAgent"), containsString("BeamSQL"));}
public void beam_f26660_0() throws Exception
{    JdbcConnection connection = (JdbcConnection) DriverManager.getConnection(JdbcDriver.CONNECT_STRING_PREFIX);    BeamCalciteSchema schema = connection.getCurrentBeamSchema();    assertThat(schema.getPipelineOptions().get("userAgent"), equalTo("BeamSQL/" + ReleaseInfo.getReleaseInfo().getVersion()));}
public void beam_f26668_0() throws Exception
{    Calendar cal = Calendar.getInstance(TimeZone.getTimeZone("UTC"), Locale.ROOT);    TestTableProvider tableProvider = new TestTableProvider();    Connection connection = JdbcDriver.connect(tableProvider, PipelineOptionsFactory.create());        Schema schema = Schema.builder().addDateTimeField("ts").build();    connection.createStatement().executeUpdate("CREATE EXTERNAL TABLE test (ts TIMESTAMP) TYPE 'test'");    ReadableInstant july1 = ISODateTimeFormat.dateTimeParser().parseDateTime("2018-07-01T01:02:03Z");    tableProvider.addRows("test", Row.withSchema(schema).addValue(july1).build());    ResultSet selectResult = connection.createStatement().executeQuery(String.format("SELECT ts FROM test"));    selectResult.next();    Timestamp ts = selectResult.getTimestamp(1, cal);    assertThat(String.format("Wrote %s to a table, but got back %s", ISODateTimeFormat.basicDateTime().print(july1), ISODateTimeFormat.basicDateTime().print(ts.getTime())), ts.getTime(), equalTo(july1.getMillis()));}
public void beam_f26669_0() throws Exception
{    TestTableProvider tableProvider = new TestTableProvider();    Connection connection = JdbcDriver.connect(tableProvider, PipelineOptionsFactory.create());    connection.createStatement().executeUpdate("CREATE EXTERNAL TABLE person ( \n" + "description VARCHAR, \n" + "nestedRow ROW< \n" + "              id BIGINT, \n" + "              name VARCHAR> \n" + ") \n" + "TYPE 'test'");    tableProvider.addRows("person", row(COMPLEX_SCHEMA, "description1", row(1L, "aaa")), row(COMPLEX_SCHEMA, "description2", row(2L, "bbb")));    ResultSet selectResult = connection.createStatement().executeQuery("SELECT person.nestedRow.id, person.nestedRow.name FROM person");    List<Row> resultRows = readResultSet(selectResult).stream().map(values -> values.stream().collect(toRow(BASIC_SCHEMA))).collect(Collectors.toList());    assertThat(resultRows, containsInAnyOrder(row(1L, "aaa"), row(2L, "bbb")));}
public void beam_f26670_0() throws Exception
{    TestTableProvider tableProvider = new TestTableProvider();    Connection connection = JdbcDriver.connect(tableProvider, PipelineOptionsFactory.create());    connection.createStatement().executeUpdate("CREATE EXTERNAL TABLE person (id BIGINT, name VARCHAR) TYPE 'test'");    connection.createStatement().executeUpdate("CREATE EXTERNAL TABLE person_src (id BIGINT, name VARCHAR) TYPE 'test'");    tableProvider.addRows("person_src", row(1L, "aaa"), row(2L, "bbb"));    connection.createStatement().execute("INSERT INTO person SELECT id, name FROM person_src");    ResultSet selectResult = connection.createStatement().executeQuery("SELECT id, name FROM person");    List<Row> resultRows = readResultSet(selectResult).stream().map(resultValues -> resultValues.stream().collect(toRow(BASIC_SCHEMA))).collect(Collectors.toList());    assertThat(resultRows, containsInAnyOrder(row(1L, "aaa"), row(2L, "bbb")));}
public void beam_f26678_0() throws Exception
{    thrown.expectMessage("Unknown 'runner' specified 'bogus'");    CalciteConnection connection = JdbcDriver.connect(BOUNDED_TABLE, PipelineOptionsFactory.create());    Statement statement = connection.createStatement();    assertEquals(0, statement.executeUpdate("SET runner = bogus"));    assertTrue(statement.execute("SELECT * FROM test"));}
public void beam_f26679_0() throws Exception
{    CalciteConnection connection = JdbcDriver.connect(BOUNDED_TABLE, PipelineOptionsFactory.create());    Statement statement = connection.createStatement();    assertEquals(0, statement.executeUpdate("SET runner = bogus"));    assertEquals(0, statement.executeUpdate("RESET ALL"));    assertTrue(statement.execute("SELECT * FROM test"));}
public void beam_f26680_0() throws Exception
{    thrown.expect(SQLException.class);    thrown.expectMessage("No suitable driver found");    DriverManager.getConnection("jdbc:baaaaaad");}
private String beam_f26688_0(FieldType fieldType)
{    return "MAP<" + unparse(fieldType.getMapKeyType()) + ", " + unparse(fieldType.getMapValueType()) + ">";}
private String beam_f26689_0(FieldType fieldType)
{    return "ROW<" + fieldType.getRowSchema().getFields().stream().map(field -> field.getName() + " " + unparse(field.getType())).collect(joining(",")) + ">";}
public void beam_f26690_0() throws Exception
{    TestTableProvider tableProvider = new TestTableProvider();    BeamSqlEnv env = BeamSqlEnv.withTableProvider(tableProvider);    JSONObject properties = new JSONObject();    JSONArray hello = new JSONArray();    hello.add("james");    hello.add("bond");    properties.put("hello", hello);    env.executeDdl("CREATE EXTERNAL TABLE person (\n" + "id int COMMENT 'id', \n" + "name varchar COMMENT 'name') \n" + "TYPE 'text' \n" + "COMMENT 'person table' \n" + "LOCATION '/home/admin/person'\n" + "TBLPROPERTIES '{\"hello\": [\"james\", \"bond\"]}'");    assertEquals(mockTable("person", "text", "person table", properties), tableProvider.getTables().get("person"));}
public void beam_f26698_0() throws Exception
{    TestTableProvider tableProvider = new TestTableProvider();    BeamSqlEnv env = BeamSqlEnv.withTableProvider(tableProvider);    assertNull(tableProvider.getTables().get("person"));    env.executeDdl("CREATE EXTERNAL TABLE person (\n" + "id int COMMENT 'id', \n" + "name varchar COMMENT 'name') \n" + "TYPE 'text' \n" + "COMMENT 'person table' \n");    assertNotNull(tableProvider.getTables().get("person"));    env.executeDdl("drop table person");    assertNull(tableProvider.getTables().get("person"));}
private static Table beam_f26699_0(String name, String type, String comment, JSONObject properties)
{    return mockTable(name, type, comment, properties, "/home/admin/" + name);}
private static Table beam_f26700_0(String name, String type, String comment, JSONObject properties, String location)
{    return Table.builder().name(name).type(type).comment(comment).location(location).schema(Stream.of(Schema.Field.of("id", CalciteUtils.INTEGER).withNullable(true).withDescription("id"), Schema.Field.of("name", CalciteUtils.VARCHAR).withNullable(true).withDescription("name")).collect(toSchema())).properties(properties).build();}
public void beam_f26708_0()
{    BeamCostModel cost = BeamCostModel.FACTORY.makeHugeCost().plus(BeamCostModel.FACTORY.makeHugeCost());    Assert.assertTrue(cost.isInfinite());}
public void beam_f26709_0()
{    registerTable("medium_table", TestBoundedTable.of(Schema.FieldType.INT32, "unbounded_key", Schema.FieldType.INT32, "large_key", Schema.FieldType.INT32, "id").addRows(1, 1, 1, 1, 1, 2, 1, 1, 3, 1, 1, 4, 1, 1, 5));}
public void beam_f26710_0()
{    String sql = "select * from medium_table";    BeamRelNode root = env.parseQuery(sql);    Assert.assertTrue(root.getCluster().getPlanner().getCost(root, root.getCluster().getMetadataQuery()) instanceof BeamCostModel);}
protected static void beam_f26718_0(String tableName, BeamSqlTable table)
{    tables.put(tableName, table);}
protected static BeamSqlTable beam_f26719_0(String tableName)
{    return tables.get(tableName);}
public static void beam_f26720_0()
{    registerTable("ORDER_DETAILS_BOUNDED", TestBoundedTable.of(Schema.FieldType.INT64, "order_id", Schema.FieldType.INT32, "site_id", Schema.FieldType.DECIMAL, "price").addRows(1L, 1, new BigDecimal(1.0), 1L, 1, new BigDecimal(1.0), 2L, 2, new BigDecimal(2.0), 4L, 4, new BigDecimal(4.0), 4L, 4, new BigDecimal(4.0)));    registerTable("ORDER_DETAILS_UNBOUNDED", TestUnboundedTable.of(Schema.FieldType.INT32, "order_id", Schema.FieldType.INT32, "site_id", Schema.FieldType.INT32, "price", Schema.FieldType.DATETIME, "order_time").timestampColumnIndex(3).addRows(Duration.ZERO, 1, 1, 1, FIRST_DATE, 1, 2, 6, FIRST_DATE).addRows(WINDOW_SIZE.plus(Duration.standardMinutes(1)), 2, 2, 7, SECOND_DATE, 2, 3, 8, SECOND_DATE,     1, 3, 3, FIRST_DATE).addRows(    WINDOW_SIZE.plus(WINDOW_SIZE).plus(Duration.standardMinutes(1)), 2, 3, 3, SECOND_DATE).setStatistics(BeamTableStatistics.createUnboundedTableStatistics(2d)));}
public void beam_f26728_0()
{    String sql = "SELECT * FROM ORDER_DETAILS_BOUNDED where order_id=1";    RelNode root = env.parseQuery(sql);    Assert.assertTrue(root instanceof BeamCalcRel);    NodeStats estimate = BeamSqlRelUtils.getNodeStats(root, root.getCluster().getMetadataQuery());    Assert.assertTrue(5d > estimate.getRowCount());    Assert.assertTrue(5d > estimate.getWindow());    Assert.assertEquals(0., estimate.getRate(), 0.001);}
public void beam_f26729_0()
{    String equalSql = "SELECT * FROM ORDER_DETAILS_BOUNDED where order_id=1";    String geqSql = "SELECT * FROM ORDER_DETAILS_BOUNDED where order_id>=1";    RelNode equalRoot = env.parseQuery(equalSql);    RelNode geqRoot = env.parseQuery(geqSql);    NodeStats equalEstimate = BeamSqlRelUtils.getNodeStats(equalRoot, equalRoot.getCluster().getMetadataQuery());    NodeStats geqEstimate = BeamSqlRelUtils.getNodeStats(geqRoot, geqRoot.getCluster().getMetadataQuery());    Assert.assertTrue(geqEstimate.getRowCount() > equalEstimate.getRowCount());    Assert.assertTrue(geqEstimate.getWindow() > equalEstimate.getWindow());}
public void beam_f26730_0()
{    String equalSql = "SELECT * FROM ORDER_DETAILS_BOUNDED where order_id=1";    String doubleEqualSql = "SELECT * FROM ORDER_DETAILS_BOUNDED WHERE order_id=1 AND site_id=2 ";    RelNode equalRoot = env.parseQuery(equalSql);    RelNode doubleEqualRoot = env.parseQuery(doubleEqualSql);    NodeStats equalEstimate = BeamSqlRelUtils.getNodeStats(equalRoot, equalRoot.getCluster().getMetadataQuery());    NodeStats doubleEqualEstimate = BeamSqlRelUtils.getNodeStats(doubleEqualRoot, doubleEqualRoot.getCluster().getMetadataQuery());    Assert.assertTrue(doubleEqualEstimate.getRowCount() < equalEstimate.getRowCount());    Assert.assertTrue(doubleEqualEstimate.getWindow() < equalEstimate.getWindow());}
public void beam_f26738_0() throws Exception
{    String sql = "SELECT *  " + "FROM ORDER_DETAILS1 o1" + " RIGHT OUTER JOIN ORDER_DETAILS2 o2" + " on " + " o1.order_id=o2.site_id AND o2.price=o1.site_id";    PCollection<Row> rows = compilePipeline(sql, pipeline);    PAssert.that(rows).containsInAnyOrder(TestUtils.RowsBuilder.of(Schema.builder().addNullableField("order_id", Schema.FieldType.INT32).addNullableField("site_id", Schema.FieldType.INT32).addNullableField("price", Schema.FieldType.INT32).addField("order_id0", Schema.FieldType.INT32).addField("site_id0", Schema.FieldType.INT32).addField("price0", Schema.FieldType.INT32).build()).addRows(2, 3, 3, 1, 2, 3, null, null, null, 2, 3, 3, null, null, null, 3, 4, 5).getRows());    pipeline.run();}
public void beam_f26739_0() throws Exception
{    String sql = "SELECT *  " + "FROM ORDER_DETAILS1 o1" + " FULL OUTER JOIN ORDER_DETAILS2 o2" + " on " + " o1.order_id=o2.site_id AND o2.price=o1.site_id";    PCollection<Row> rows = compilePipeline(sql, pipeline);    PAssert.that(rows).containsInAnyOrder(TestUtils.RowsBuilder.of(Schema.builder().addNullableField("order_id", Schema.FieldType.INT32).addNullableField("site_id", Schema.FieldType.INT32).addNullableField("price", Schema.FieldType.INT32).addNullableField("order_id0", Schema.FieldType.INT32).addNullableField("site_id0", Schema.FieldType.INT32).addNullableField("price0", Schema.FieldType.INT32).build()).addRows(2, 3, 3, 1, 2, 3, 1, 2, 3, null, null, null, 3, 4, 5, null, null, null, null, null, null, 2, 3, 3, null, null, null, 3, 4, 5).getRows());    pipeline.run();}
public void beam_f26740_0() throws Exception
{    String sql = "SELECT *  " + "FROM ORDER_DETAILS1 o1" + " JOIN ORDER_DETAILS2 o2" + " on " + " o1.order_id>o2.site_id";    pipeline.enableAbandonedNodeEnforcement(false);    compilePipeline(sql, pipeline);    pipeline.run();}
public void beam_f26748_0()
{    String sql = "SELECT * FROM " + "(select order_id, sum(site_id) as sum_site_id FROM ORDER_DETAILS " + "          GROUP BY order_id, TUMBLE(order_time, INTERVAL '1' HOUR)) o1 " + " JOIN " + "(select order_id, sum(site_id) as sum_site_id FROM ORDER_DETAILS " + "          GROUP BY order_id, TUMBLE(order_time, INTERVAL '1' HOUR)) o2 " + " on " + " o1.order_id=o2.order_id";    RelNode root = env.parseQuery(sql);    while (!(root instanceof BeamCoGBKJoinRel)) {        root = root.getInput(0);    }    NodeStats estimate = BeamSqlRelUtils.getNodeStats(root, root.getCluster().getMetadataQuery());    NodeStats leftEstimate = BeamSqlRelUtils.getNodeStats(((BeamCoGBKJoinRel) root).getLeft(), root.getCluster().getMetadataQuery());    NodeStats rightEstimate = BeamSqlRelUtils.getNodeStats(((BeamCoGBKJoinRel) root).getRight(), root.getCluster().getMetadataQuery());    Assert.assertFalse(estimate.isUnknown());    Assert.assertEquals(0d, estimate.getRowCount(), 0.01);    Assert.assertNotEquals(0d, estimate.getRate(), 0.001);    Assert.assertTrue(estimate.getRate() < leftEstimate.getRate() * rightEstimate.getWindow() + rightEstimate.getRate() * leftEstimate.getWindow());    Assert.assertNotEquals(0d, estimate.getWindow(), 0.001);    Assert.assertTrue(estimate.getWindow() < leftEstimate.getWindow() * rightEstimate.getWindow());}
public void beam_f26749_0() throws Exception
{    String sql = "SELECT * FROM " + "(select site_id as order_id, sum(site_id) as sum_site_id FROM ORDER_DETAILS " + "          GROUP BY site_id, TUMBLE(order_time, INTERVAL '1' HOUR)) o1 " + " LEFT OUTER JOIN " + "(select order_id, sum(site_id) as sum_site_id FROM ORDER_DETAILS " + "          GROUP BY order_id, TUMBLE(order_time, INTERVAL '1' HOUR)) o2 " + " on " + " o1.order_id=o2.order_id";                        PCollection<Row> rows = compilePipeline(sql, pipeline);    PAssert.that(rows.apply(ParDo.of(new TestUtils.BeamSqlRow2StringDoFn()))).containsInAnyOrder(TestUtils.RowsBuilder.of(Schema.builder().addField("order_id1", Schema.FieldType.INT32).addField("sum_site_id", Schema.FieldType.INT32).addNullableField("order_id", Schema.FieldType.INT32).addNullableField("sum_site_id0", Schema.FieldType.INT32).build()).addRows(1, 1, 1, 3, 2, 2, null, null, 2, 2, 2, 5, 3, 3, null, null).getStringRows());    pipeline.run();}
public void beam_f26750_0() throws Exception
{    String sql = "SELECT * FROM " + "(select order_id, sum(site_id) as sum_site_id FROM ORDER_DETAILS " + "          GROUP BY order_id, TUMBLE(order_time, INTERVAL '1' HOUR)) o1 " + " RIGHT OUTER JOIN " + "(select site_id as order_id, sum(site_id) as sum_site_id FROM ORDER_DETAILS " + "          GROUP BY site_id, TUMBLE(order_time, INTERVAL '1' HOUR)) o2 " + " on " + " o1.order_id=o2.order_id";    PCollection<Row> rows = compilePipeline(sql, pipeline);    PAssert.that(rows.apply(ParDo.of(new TestUtils.BeamSqlRow2StringDoFn()))).containsInAnyOrder(TestUtils.RowsBuilder.of(Schema.builder().addNullableField("order_id1", Schema.FieldType.INT32).addNullableField("sum_site_id", Schema.FieldType.INT32).addField("order_id", Schema.FieldType.INT32).addField("sum_site_id0", Schema.FieldType.INT32).build()).addRows(1, 3, 1, 1, null, null, 2, 2, 2, 5, 2, 2, null, null, 3, 3).getStringRows());    pipeline.run();}
public POutput beam_f26758_0(PCollection<Row> input)
{    input.apply(ParDo.of(new DoFn<Row, Void>() {        @ProcessElement        public void processElement(ProcessContext context) {        }    }));    return PDone.in(input.getPipeline());}
public BeamTableStatistics beam_f26760_0(PipelineOptions options)
{    return BeamTableStatistics.BOUNDED_UNKNOWN;}
public void beam_f26761_0()
{    Schema schema = Schema.builder().addInt64Field("id").build();    RelDataType type = CalciteUtils.toCalciteRowType(schema, TYPE_FACTORY);    ImmutableList<ImmutableList<RexLiteral>> tuples = ImmutableList.of(ImmutableList.of(rexBuilder.makeBigintLiteral(BigDecimal.ZERO)), ImmutableList.of(rexBuilder.makeBigintLiteral(BigDecimal.ONE)));    BeamRelNode node = new BeamIOSinkRel(cluster, RelOptTableImpl.create(null, type, ImmutableList.of(), null), null, new BeamValuesRel(cluster, type, tuples, null), null, null, null, false, new FakeTable(), null);    Enumerable<Object> enumerable = BeamEnumerableConverter.toEnumerable(options, node);    Enumerator<Object> enumerator = enumerable.enumerator();    assertTrue(enumerator.moveNext());    assertEquals(2L, enumerator.current());    assertFalse(enumerator.moveNext());    enumerator.close();}
public void beam_f26769_0()
{    String sql = "SELECT * FROM ORDER_DETAILS_BOUNDED";    RelNode root = env.parseQuery(sql);    while (!(root instanceof BeamIOSourceRel)) {        root = root.getInput(0);    }    Assert.assertEquals(5d, root.estimateRowCount(RelMetadataQuery.instance()), 0.001);}
public void beam_f26770_0()
{    String sql = "SELECT * FROM ORDER_DETAILS_UNBOUNDED";    RelNode root = env.parseQuery(sql);    while (!(root instanceof BeamIOSourceRel)) {        root = root.getInput(0);    }    Assert.assertEquals(2d, root.estimateRowCount(RelMetadataQuery.instance()), 0.001);}
public void beam_f26771_0()
{    String sql = "SELECT * FROM ORDER_DETAILS_BOUNDED";    RelNode root = env.parseQuery(sql);    while (!(root instanceof BeamIOSourceRel)) {        root = root.getInput(0);    }    NodeStats estimate = BeamSqlRelUtils.getNodeStats(root, root.getCluster().getMetadataQuery());    Assert.assertEquals(5d, estimate.getRowCount(), 0.01);    Assert.assertEquals(0d, estimate.getRate(), 0.01);    Assert.assertEquals(5d, estimate.getWindow(), 0.01);}
public static void beam_f26779_0()
{    registerTable("ORDER_DETAILS", TestBoundedTable.of(Schema.FieldType.INT64, "order_id", Schema.FieldType.INT32, "site_id", Schema.FieldType.DOUBLE, "price", Schema.FieldType.DATETIME, "order_time").addRows(1L, 1, 1.0, THE_DATE, 2L, 2, 2.0, THE_DATE));}
public void beam_f26780_0() throws Exception
{    String sql = "SELECT " + " order_id, site_id, count(*) as cnt " + "FROM ORDER_DETAILS GROUP BY order_id, site_id" + ", TUMBLE(order_time, INTERVAL '1' HOUR) " + " UNION SELECT " + " order_id, site_id, count(*) as cnt " + "FROM ORDER_DETAILS GROUP BY order_id, site_id" + ", TUMBLE(order_time, INTERVAL '1' HOUR) ";    PCollection<Row> rows = compilePipeline(sql, pipeline);        PAssert.that(rows.apply(ParDo.of(new TestUtils.BeamSqlRow2StringDoFn()))).containsInAnyOrder(TestUtils.RowsBuilder.of(Schema.FieldType.INT64, "order_id", Schema.FieldType.INT32, "site_id", Schema.FieldType.INT64, "cnt").addRows(1L, 1, 1L, 2L, 2, 1L).getStringRows());    pipeline.run();}
public void beam_f26781_0() throws Exception
{    String sql = "SELECT " + " order_id, site_id, count(*) as cnt " + "FROM ORDER_DETAILS GROUP BY order_id, site_id" + ", TUMBLE(order_time, INTERVAL '1' HOUR) " + " UNION SELECT " + " order_id, site_id, count(*) as cnt " + "FROM ORDER_DETAILS GROUP BY order_id, site_id" + ", TUMBLE(order_time, INTERVAL '2' HOUR) ";            Pipeline pipeline1 = Pipeline.create(PipelineOptionsFactory.create());    compilePipeline(sql, pipeline1);    pipeline.run();}
public void beam_f26789_0() throws Exception
{    String sql = "SELECT o1.order_id, o1.sum_site_id, o2.buyer FROM " + " ORDER_DETAILS1 o2 " + " RIGHT OUTER JOIN " + "(select order_id, sum(site_id) as sum_site_id FROM ORDER_DETAILS " + "          GROUP BY order_id, TUMBLE(order_time, INTERVAL '1' HOUR)) o1 " + " on " + " o1.order_id=o2.order_id";    PCollection<Row> rows = compilePipeline(sql, pipeline);    PAssert.that(rows.apply(ParDo.of(new TestUtils.BeamSqlRow2StringDoFn()))).containsInAnyOrder(TestUtils.RowsBuilder.of(Schema.builder().addField("order_id", Schema.FieldType.INT32).addField("sum_site_id", Schema.FieldType.INT32).addNullableField("buyer", Schema.FieldType.STRING).build()).addRows(1, 3, "james", 2, 5, "bond", 3, 3, null).getStringRows());    pipeline.run();}
public void beam_f26790_0() throws Exception
{    String sql = "SELECT o1.order_id, o1.sum_site_id, o2.buyer FROM " + "(select order_id, sum(site_id) as sum_site_id FROM ORDER_DETAILS " + "          GROUP BY order_id, TUMBLE(order_time, INTERVAL '1' HOUR)) o1 " + " RIGHT OUTER JOIN " + " ORDER_DETAILS1 o2 " + " on " + " o1.order_id=o2.order_id";    pipeline.enableAbandonedNodeEnforcement(false);    compilePipeline(sql, pipeline);    pipeline.run();}
public void beam_f26791_0() throws Exception
{    String sql = "SELECT o1.order_id, o1.sum_site_id, o2.buyer FROM " + " ORDER_DETAILS1 o2 " + " FULL OUTER JOIN " + "(select order_id, sum(site_id) as sum_site_id FROM ORDER_DETAILS " + "          GROUP BY order_id, TUMBLE(order_time, INTERVAL '1' HOUR)) o1 " + " on " + " o1.order_id=o2.order_id";    pipeline.enableAbandonedNodeEnforcement(false);    compilePipeline(sql, pipeline);    pipeline.run();}
public void beam_f26799_0() throws Exception
{    String sql = "SELECT o1.order_id, o2.site_name FROM " + " SITE_LKP o2 " + " JOIN ORDER_DETAILS1 o1 " + " on " + " o1.site_id=o2.site_id " + " WHERE o1.site_id=2 ";    PCollection<Row> rows = compilePipeline(sql, pipeline);    PAssert.that(rows.apply(ParDo.of(new TestUtils.BeamSqlRow2StringDoFn()))).containsInAnyOrder(TestUtils.RowsBuilder.of(Schema.FieldType.INT32, "order_id", Schema.FieldType.STRING, "site_name").addRows(1, "SITE1").getStringRows());    pipeline.run();}
public void beam_f26800_0() throws Exception
{    String sql = "SELECT o1.order_id, o2.site_name FROM " + "(select order_id, site_id FROM ORDER_DETAILS " + "          GROUP BY order_id, site_id, TUMBLE(order_time, INTERVAL '1' HOUR)) o1 " + " JOIN " + " SITE_LKP o2 " + " on " + " o1.site_id=o2.site_id" + " WHERE o1.site_id=2 ";    PCollection<Row> rows = compilePipeline(sql, pipeline);    PAssert.that(rows.apply(ParDo.of(new TestUtils.BeamSqlRow2StringDoFn()))).containsInAnyOrder(TestUtils.RowsBuilder.of(Schema.FieldType.INT32, "order_id", Schema.FieldType.STRING, "site_name").addRows(1, "SITE1").addRows(2, "SITE1").getStringRows());    pipeline.run();}
public void beam_f26801_0() throws Exception
{    String sql = "SELECT o1.order_id, o2.site_name FROM " + " SITE_LKP o2 " + " JOIN " + "(select order_id, site_id FROM ORDER_DETAILS " + "          GROUP BY order_id, site_id, TUMBLE(order_time, INTERVAL '1' HOUR)) o1 " + " on " + " o1.site_id=o2.site_id" + " WHERE o1.site_id=2 ";    PCollection<Row> rows = compilePipeline(sql, pipeline);    PAssert.that(rows.apply(ParDo.of(new TestUtils.BeamSqlRow2StringDoFn()))).containsInAnyOrder(TestUtils.RowsBuilder.of(Schema.FieldType.INT32, "order_id", Schema.FieldType.STRING, "site_name").addRows(1, "SITE1").addRows(2, "SITE1").getStringRows());    pipeline.run();}
public void beam_f26809_0()
{    String sql = "SELECT order_id, site_id, price, order_time " + "FROM ORDER_DETAILS " + "ORDER BY order_time desc limit 4";    PCollection<Row> rows = compilePipeline(sql, pipeline);    PAssert.that(rows).containsInAnyOrder(TestUtils.RowsBuilder.of(Schema.FieldType.INT64, "order_id", Schema.FieldType.INT32, "site_id", Schema.FieldType.DOUBLE, "price", Schema.FieldType.DATETIME, "order_time").addRows(7L, 7, 7.0, new DateTime(6), 8L, 8888, 8.0, new DateTime(7), 8L, 999, 9.0, new DateTime(8), 10L, 100, 10.0, new DateTime(9)).getRows());    pipeline.run().waitUntilFinish();}
public void beam_f26810_0()
{    Schema schema = Schema.builder().addField("order_id", Schema.FieldType.INT64).addNullableField("site_id", Schema.FieldType.INT32).addField("price", Schema.FieldType.DOUBLE).build();    registerTable("ORDER_DETAILS", TestBoundedTable.of(schema).addRows(1L, 2, 1.0, 1L, null, 2.0, 2L, 1, 3.0, 2L, null, 4.0, 5L, 5, 5.0));    registerTable("SUB_ORDER_RAM", TestBoundedTable.of(schema));    String sql = "INSERT INTO SUB_ORDER_RAM(order_id, site_id, price)  SELECT " + " order_id, site_id, price " + "FROM ORDER_DETAILS " + "ORDER BY order_id asc, site_id desc NULLS FIRST limit 4";    PCollection<Row> rows = compilePipeline(sql, pipeline);    PAssert.that(rows).containsInAnyOrder(TestUtils.RowsBuilder.of(schema).addRows(1L, null, 2.0, 1L, 2, 1.0, 2L, null, 4.0, 2L, 1, 3.0).getRows());    pipeline.run().waitUntilFinish();}
public void beam_f26811_0()
{    Schema schema = Schema.builder().addField("order_id", Schema.FieldType.INT64).addNullableField("site_id", Schema.FieldType.INT32).addField("price", Schema.FieldType.DOUBLE).build();    registerTable("ORDER_DETAILS", TestBoundedTable.of(schema).addRows(1L, 2, 1.0, 1L, null, 2.0, 2L, 1, 3.0, 2L, null, 4.0, 5L, 5, 5.0));    registerTable("SUB_ORDER_RAM", TestBoundedTable.of(schema));    String sql = "INSERT INTO SUB_ORDER_RAM(order_id, site_id, price)  SELECT " + " order_id, site_id, price " + "FROM ORDER_DETAILS " + "ORDER BY order_id asc, site_id desc NULLS LAST limit 4";    PCollection<Row> rows = compilePipeline(sql, pipeline);    PAssert.that(rows).containsInAnyOrder(TestUtils.RowsBuilder.of(schema).addRows(1L, 2, 1.0, 1L, null, 2.0, 2L, 1, 3.0, 2L, null, 4.0).getRows());    pipeline.run().waitUntilFinish();}
public void beam_f26819_0()
{    String sql = "SELECT * FROM unnest(ARRAY [1, 2, 3])";    PCollection<Row> rows = compilePipeline(sql, pipeline);    PAssert.that(rows).containsInAnyOrder(TestUtils.RowsBuilder.of(Schema.FieldType.INT32, "intField").addRows(1, 2, 3).getRows());    pipeline.run();}
public void beam_f26820_0()
{    Schema rowSchema = Schema.builder().addStringField("stringField").addInt32Field("intField").build();    List<Row> nestedRows = Arrays.asList(Row.withSchema(rowSchema).addValues("test1", 1).build(), Row.withSchema(rowSchema).addValues("test2", 2).build());    registerTable("NESTED", TestBoundedTable.of(Schema.FieldType.STRING, "user_id", Schema.FieldType.array(Schema.FieldType.row(rowSchema)), "nested").addRows("1", nestedRows));    String sql = "SELECT intField, stringField FROM unnest(SELECT nested from NESTED)";    PCollection<Row> rows = compilePipeline(sql, pipeline);    PAssert.that(rows).containsInAnyOrder(TestUtils.RowsBuilder.of(Schema.FieldType.INT32, "intField", Schema.FieldType.STRING, "stringField").addRows(1, "test1", 2, "test2").getRows());    pipeline.run();}
public static void beam_f26821_0()
{    registerTable("ORDER_DETAILS", TestBoundedTable.of(Schema.FieldType.INT64, "order_id", Schema.FieldType.INT32, "site_id", Schema.FieldType.DECIMAL, "price").addRows(1L, 1, new BigDecimal(1.0), 2L, 2, new BigDecimal(2.0)));}
public void beam_f26829_0() throws Exception
{    String sql = "insert into string_table(name, description) values " + "('hello', 'world'), ('james', 'bond')";    PCollection<Row> rows = compilePipeline(sql, pipeline);    PAssert.that(rows).containsInAnyOrder(TestUtils.RowsBuilder.of(Schema.FieldType.STRING, "name", Schema.FieldType.STRING, "description").addRows("hello", "world", "james", "bond").getRows());    pipeline.run();}
public void beam_f26830_0() throws Exception
{    String sql = "insert into int_table (c0, c1) values(cast(1 as int), cast(2 as int))";    PCollection<Row> rows = compilePipeline(sql, pipeline);    PAssert.that(rows).containsInAnyOrder(TestUtils.RowsBuilder.of(Schema.FieldType.INT32, "c0", Schema.FieldType.INT32, "c1").addRows(1, 2).getRows());    pipeline.run();}
public void beam_f26831_0() throws Exception
{    String sql = "select 1, '1'";    PCollection<Row> rows = compilePipeline(sql, pipeline);    PAssert.that(rows).containsInAnyOrder(TestUtils.RowsBuilder.of(Schema.FieldType.INT32, "EXPR$0", Schema.FieldType.STRING, "EXPR$1").addRows(1, "1").getRows());    pipeline.run();}
public void beam_f26839_0()
{    TestTableProvider tableProvider = new TestTableProvider();    createThreeTables(tableProvider);    BeamSqlEnv env = BeamSqlEnv.withTableProvider(tableProvider);            BeamRelNode parsedQuery = env.parseQuery("select * from large_table " + " JOIN medium_table on large_table.medium_key = medium_table.large_key " + " JOIN small_table on medium_table.small_key = small_table.medium_key ");    assertTopTableInJoins(parsedQuery, "large_table");}
public void beam_f26840_0()
{    TestTableProvider tableProvider = new TestTableProvider();    createThreeTables(tableProvider);    BeamSqlEnv env = BeamSqlEnv.withTableProvider(tableProvider);            BeamRelNode parsedQuery = env.parseQuery("select * from medium_table " + " JOIN large_table on large_table.medium_key = medium_table.large_key " + " JOIN small_table on medium_table.small_key = small_table.medium_key ");    assertTopTableInJoins(parsedQuery, "large_table");}
public void beam_f26841_0()
{    TestTableProvider tableProvider = new TestTableProvider();    createThreeTables(tableProvider);    List<RelOptRule> ruleSet = Arrays.stream(BeamRuleSets.getRuleSets()).flatMap(rules -> StreamSupport.stream(rules.spliterator(), false)).filter(rule -> !(rule instanceof BeamJoinPushThroughJoinRule)).filter(rule -> !(rule instanceof BeamJoinAssociateRule)).filter(rule -> !(rule instanceof JoinCommuteRule)).collect(Collectors.toList());    BeamSqlEnv env = BeamSqlEnv.builder(tableProvider).setPipelineOptions(PipelineOptionsFactory.create()).setRuleSets(new RuleSet[] { RuleSets.ofList(ruleSet) }).build();            BeamRelNode parsedQuery = env.parseQuery("select * from medium_table " + " JOIN large_table on large_table.medium_key = medium_table.large_key " + " JOIN small_table on medium_table.small_key = small_table.medium_key ");    assertTopTableInJoins(parsedQuery, "small_table");}
public RelDataType beam_f26849_0(final RelDataTypeFactory typeFactory)
{    return typeBuilder.apply(typeFactory);}
public Enumerable<Object[]> beam_f26850_0(final DataContext root)
{    return Linq4j.asEnumerable(data);}
public void beam_f26851_0() throws Exception
{    RelDataType relDataType = new JavaTypeFactoryImpl(RelDataTypeSystem.DEFAULT).builder().add("col_tinyint", SqlTypeName.TINYINT).add("col_smallint", SqlTypeName.SMALLINT).add("col_integer", SqlTypeName.INTEGER).add("col_bigint", SqlTypeName.BIGINT).add("col_float", SqlTypeName.FLOAT).add("col_double", SqlTypeName.DOUBLE).add("col_decimal", SqlTypeName.DECIMAL).add("col_string_varchar", SqlTypeName.VARCHAR).add("col_time", SqlTypeName.TIME).add("col_timestamp", SqlTypeName.TIMESTAMP).add("col_boolean", SqlTypeName.BOOLEAN).build();    Schema beamSchema = CalciteUtils.toSchema(relDataType);    Row row = Row.withSchema(beamSchema).addValues(Byte.valueOf("1"), Short.valueOf("1"), 1, 1L, 1.1F, 1.1, BigDecimal.ZERO, "hello", DateTime.now(), DateTime.now(), true).build();    Coder<Row> coder = SchemaCoder.of(beamSchema);    CoderProperties.coderDecodeEncodeEqual(coder, row);}
public void beam_f26859_0()
{    VarianceAccumulator accumulator1 = newVarianceAccumulator(FIFTEEN, THREE, FOUR);    VarianceAccumulator accumulator2 = newVarianceAccumulator(SIXTEEN, FOUR, FIVE);                                                VarianceAccumulator expectedCombined = newVarianceAccumulator(FIFTEEN.add(SIXTEEN).add(new BigDecimal(0.011904762)), THREE.add(FOUR), FOUR.add(FIVE));    VarianceAccumulator combined1 = accumulator1.combineWith(accumulator2);    VarianceAccumulator combined2 = accumulator2.combineWith(accumulator1);    assertEquals(expectedCombined.variance().doubleValue(), combined1.variance().doubleValue(), DELTA);    assertEquals(expectedCombined.variance().doubleValue(), combined2.variance().doubleValue(), DELTA);    assertEquals(expectedCombined.count(), combined1.count());    assertEquals(expectedCombined.sum(), combined1.sum());    assertEquals(expectedCombined.count(), combined2.count());    assertEquals(expectedCombined.sum(), combined2.sum());}
public static Iterable<Object[]> beam_f26860_0()
{    return Arrays.asList(new Object[][] { { VarianceFn.newPopulation(BigDecimal::intValue), newVarianceAccumulator(FIFTEEN, THREE, ZERO), 5 }, { VarianceFn.newSample(BigDecimal::intValue), newVarianceAccumulator(FIFTEEN, FOUR, ZERO), 5 } });}
public void beam_f26861_0()
{    assertEquals(VarianceAccumulator.EMPTY, varianceFn.createAccumulator());}
public void beam_f26869_0() throws Exception
{    byte[] testByets = "абвгд".getBytes(UTF_8);    ArrayUtils.reverse(testByets);    Schema resultType = Schema.builder().addByteArrayField("field").build();    Row resultRow = Row.withSchema(resultType).addValues(testByets).build();    Row resultRow2 = Row.withSchema(resultType).addValues("\1\0".getBytes(UTF_8)).build();    Row resultRow3 = Row.withSchema(resultType).addValues("".getBytes(UTF_8)).build();    String sql = "SELECT REVERSE(f_bytes) FROM PCOLLECTION WHERE f_func = 'LENGTH'";    PCollection<Row> result = boundedInputBytes.apply("testUdf", SqlTransform.query(sql));    PAssert.that(result).containsInAnyOrder(resultRow, resultRow2, resultRow3);    pipeline.run().waitUntilFinish();}
public void beam_f26870_0() throws Exception
{    Schema resultType = Schema.builder().addStringField("field").build();    Row resultRow = Row.withSchema(resultType).addValue("666f6f626172").build();    Row resultRow2 = Row.withSchema(resultType).addValue("20").build();    Row resultRow3 = Row.withSchema(resultType).addValue("616263414243").build();    Row resultRow4 = Row.withSchema(resultType).addValue("616263414243d0b6d189d184d096d0a9d0a4").build();    String sql = "SELECT TO_HEX(f_bytes) FROM PCOLLECTION WHERE f_func = 'TO_HEX'";    PCollection<Row> result = boundedInputBytes.apply("testUdf", SqlTransform.query(sql));    PAssert.that(result).containsInAnyOrder(resultRow, resultRow2, resultRow3, resultRow4);    pipeline.run().waitUntilFinish();}
public void beam_f26871_0() throws Exception
{    Schema resultType = Schema.builder().addNullableField("field", FieldType.BYTES).build();    Row resultRow = Row.withSchema(resultType).addValue("".getBytes(UTF_8)).build();    Row resultRow2 = Row.withSchema(resultType).addValue("abcdef".getBytes(UTF_8)).build();    Row resultRow3 = Row.withSchema(resultType).addValue("abcd".getBytes(UTF_8)).build();    Row resultRow4 = Row.withSchema(resultType).addValue("defgabcdef".getBytes(UTF_8)).build();    Row resultRow5 = Row.withSchema(resultType).addValue("defghdeabc".getBytes(UTF_8)).build();    Row resultRow6 = Row.withSchema(resultType).addValue("----abc".getBytes(UTF_8)).build();    Row resultRow7 = Row.withSchema(resultType).addValue("defdefd".getBytes(UTF_8)).build();    Row resultRow8 = Row.withSchema(resultType).addValue(null).build();    String sql = "SELECT LPAD(f_bytes_one, length, f_bytes_two) FROM PCOLLECTION";    PCollection<Row> result = boundedInputBytesPaddingTest.apply("testUdf", SqlTransform.query(sql));    PAssert.that(result).containsInAnyOrder(resultRow, resultRow2, resultRow3, resultRow4, resultRow5, resultRow6, resultRow7, resultRow8);    pipeline.run().waitUntilFinish();}
public void beam_f26879_0() throws Exception
{    ExpressionChecker checker = new ExpressionChecker().addExpr("REVERSE('')", "").addExpr("REVERSE('foo')", "oof").addExpr("REVERSE('中文')", "文中").addExpr("REVERSE('абвгд')", "дгвба").addExprWithNullExpectedValue("REVERSE(CAST(NULL as VARCHAR(0)))", TypeName.STRING).addExprWithNullExpectedValue("REVERSE(CAST(NULL as VARBINARY(0)))", TypeName.STRING);    checker.buildRunAndCheck();}
public void beam_f26880_0() throws Exception
{    ExpressionChecker checker = new ExpressionChecker().addExpr("FROM_HEX('666f6f626172')", "foobar".getBytes(UTF_8)).addExpr("FROM_HEX('20')", " ".getBytes(UTF_8)).addExpr("FROM_HEX('616263414243')", "abcABC".getBytes(UTF_8)).addExpr("FROM_HEX('616263414243d0b6d189d184d096d0a9d0a4')", "abcABCжщфЖЩФ".getBytes(UTF_8)).addExprWithNullExpectedValue("FROM_HEX(CAST(NULL as VARCHAR(0)))", TypeName.BYTES);    checker.buildRunAndCheck();}
public void beam_f26881_0() throws Exception
{    ExpressionChecker checker = new ExpressionChecker().addExprWithNullExpectedValue("TO_HEX(CAST(NULL as VARBINARY(0)))", TypeName.STRING);    checker.buildRunAndCheck();}
public void beam_f26889_0()
{    final Schema schema = Schema.builder().addNullableField("f1", Schema.FieldType.BYTE).addNullableField("f2", Schema.FieldType.INT16).addNullableField("f3", Schema.FieldType.INT32).addNullableField("f4", Schema.FieldType.INT64).addNullableField("f5", Schema.FieldType.FLOAT).addNullableField("f6", Schema.FieldType.DOUBLE).addNullableField("f7", Schema.FieldType.DECIMAL).addNullableField("f8", Schema.FieldType.BOOLEAN).addNullableField("f9", Schema.FieldType.BYTES).addNullableField("f10", Schema.FieldType.STRING).build();    final Map<String, RelDataType> fields = calciteRowTypeFields(schema);    assertEquals(10, fields.size());    fields.values().forEach(x -> assertTrue(x.isNullable()));    assertEquals(SqlTypeName.TINYINT, fields.get("f1").getSqlTypeName());    assertEquals(SqlTypeName.SMALLINT, fields.get("f2").getSqlTypeName());    assertEquals(SqlTypeName.INTEGER, fields.get("f3").getSqlTypeName());    assertEquals(SqlTypeName.BIGINT, fields.get("f4").getSqlTypeName());    assertEquals(SqlTypeName.FLOAT, fields.get("f5").getSqlTypeName());    assertEquals(SqlTypeName.DOUBLE, fields.get("f6").getSqlTypeName());    assertEquals(SqlTypeName.DECIMAL, fields.get("f7").getSqlTypeName());    assertEquals(SqlTypeName.BOOLEAN, fields.get("f8").getSqlTypeName());    assertEquals(SqlTypeName.VARBINARY, fields.get("f9").getSqlTypeName());    assertEquals(SqlTypeName.VARCHAR, fields.get("f10").getSqlTypeName());}
public void beam_f26890_0()
{    final Schema schema = Schema.builder().addField("f1", Schema.FieldType.BYTE).addField("f2", Schema.FieldType.INT16).addField("f3", Schema.FieldType.INT32).addField("f4", Schema.FieldType.INT64).addField("f5", Schema.FieldType.FLOAT).addField("f6", Schema.FieldType.DOUBLE).addField("f7", Schema.FieldType.DECIMAL).addField("f8", Schema.FieldType.BOOLEAN).addField("f9", Schema.FieldType.BYTES).addField("f10", Schema.FieldType.STRING).build();    final Schema out = CalciteUtils.toSchema(CalciteUtils.toCalciteRowType(schema, dataTypeFactory));    assertEquals(schema, out);}
public void beam_f26891_0()
{    final Schema schema = Schema.builder().addNullableField("f1", Schema.FieldType.BYTE).addNullableField("f2", Schema.FieldType.INT16).addNullableField("f3", Schema.FieldType.INT32).addNullableField("f4", Schema.FieldType.INT64).addNullableField("f5", Schema.FieldType.FLOAT).addNullableField("f6", Schema.FieldType.DOUBLE).addNullableField("f7", Schema.FieldType.DECIMAL).addNullableField("f8", Schema.FieldType.BOOLEAN).addNullableField("f9", Schema.FieldType.BYTES).addNullableField("f10", Schema.FieldType.STRING).build();    final Schema out = CalciteUtils.toSchema(CalciteUtils.toCalciteRowType(schema, dataTypeFactory));    assertEquals(schema, out);}
public String beam_f26899_0()
{    return buyerName;}
public void beam_f26900_0(Integer amount)
{    this.amount = amount;}
public void beam_f26901_0(String buyerName)
{    this.buyerName = buyerName;}
protected PCollection<Row> beam_f26909_0()
{    try {        return TestBoundedTable.of(ROW_TYPE_THREE).addRows(parseTimestampWithUTCTimeZone("1986-02-15 11:35:26"), 1.4).buildIOReader(pipeline.begin()).setRowSchema(ROW_TYPE_THREE);    } catch (Exception e) {        throw new RuntimeException(e);    }}
protected PCollection<Row> beam_f26910_0()
{    try {        return TestBoundedTable.of(ROW_TYPE_TWO).addRows(parseTimestampWithUTCTimeZone("1986-02-15 11:35:26"), (byte) 1, (short) 1, 1, 5, 1L, 1.0f, 1.0, 7.0, BigDecimal.valueOf(1.0)).addRows(parseTimestampWithUTCTimeZone("1986-03-15 11:35:26"), (byte) 2, (short) 2, 2, 6, 2L, 2.0f, 2.0, 8.0, BigDecimal.valueOf(2.0)).addRows(parseTimestampWithUTCTimeZone("1986-04-15 11:35:26"), (byte) 3, (short) 3, 3, 7, 3L, 3.0f, 3.0, 9.0, BigDecimal.valueOf(3.0)).addRows(null, null, null, null, null, null, null, null, null, null).buildIOReader(pipeline.begin()).setRowSchema(ROW_TYPE_TWO);    } catch (Exception e) {        throw new RuntimeException(e);    }}
private static ExpressionTestCase beam_f26911_0(String sqlExpr, Object expectedResult, FieldType resultFieldType)
{    return new AutoValue_BeamSqlBuiltinFunctionsIntegrationTestBase_ExpressionTestCase(sqlExpr, expectedResult, resultFieldType);}
private void beam_f26919_0(Pipeline pipeline)
{    for (String expr : exprs) {        pipeline.apply(expr, new CheckPTransform(expr));    }}
private void beam_f26920_0(PipelineOptions pipelineOptions) throws Exception
{                                    TestTableProvider tableProvider = new TestTableProvider();    Connection connection = JdbcDriver.connect(tableProvider, pipelineOptions);    connection.createStatement().executeUpdate("CREATE EXTERNAL TABLE dummy (dummy BOOLEAN) TYPE 'test'");    tableProvider.addRows("dummy", DUMMY_ROW);    for (String expr : exprs) {        ResultSet exprResult = connection.createStatement().executeQuery(String.format("SELECT %s FROM dummy", expr));        exprResult.next();        exprResult.getBoolean(1);        assertTrue("Test expression is false: " + expr, exprResult.getBoolean(1));    }}
public PDone beam_f26921_0(PBegin begin)
{    PCollection<Boolean> result = begin.apply(Create.of(DUMMY_ROW).withRowSchema(DUMMY_SCHEMA)).apply(SqlTransform.query("SELECT " + expr)).apply(MapElements.into(TypeDescriptors.booleans()).via(row -> row.getBoolean(0)));    PAssert.that(result).satisfies(input -> {        assertTrue("Test expression is false: " + expr, Iterables.getOnlyElement(input));        return null;    });    return PDone.in(begin.getPipeline());}
public void beam_f26929_0() throws Exception
{    ExpressionChecker checker = new ExpressionChecker().addExpr("s_string_1 LIKE 'string_true_test'", true).addExpr("s_string_1 LIKE 'string_false_test'", false).addExpr("s_string_2 LIKE 'string_false_test'", true).addExpr("s_string_2 LIKE 'string_true_test'", false).addExpr("s_string_1 LIKE 'string_true_test%'", true).addExpr("s_string_1 LIKE 'string_false_test%'", false).addExpr("s_string_1 LIKE 'string_true%'", true).addExpr("s_string_1 LIKE 'string_false%'", false).addExpr("s_string_1 LIKE 'string%test'", true).addExpr("s_string_1 LIKE '%test'", true).addExpr("s_string_1 LIKE '%string_true_test'", true).addExpr("s_string_1 LIKE '%string_false_test'", false).addExpr("s_string_1 LIKE '%false_test'", false).addExpr("s_string_2 LIKE '%false_test'", true).addExpr("s_string_1 LIKE 'string_tr_e_test'", true).addExpr("s_string_1 LIKE 'string______test'", true).addExpr("s_string_2 LIKE 'string______test'", false).addExpr("s_string_2 LIKE 'string_______test'", true).addExpr("s_string_2 LIKE 'string_false_te__'", true).addExpr("s_string_2 LIKE 'string_false_te___'", false).addExpr("s_string_2 LIKE 'string_false_te_'", false).addExpr("s_string_1 LIKE 'string_true_te__'", true).addExpr("s_string_1 LIKE '_ring_true_te__'", false).addExpr("s_string_2 LIKE '__ring_false_te__'", true).addExpr("s_string_1 LIKE '_%ring_true_te__'", true).addExpr("s_string_1 LIKE '_%tring_true_te__'", true).addExpr("s_string_2 LIKE 'string_false_te%__'", true).addExpr("s_string_2 LIKE 'string_false_te__%'", true).addExpr("s_string_2 LIKE 'string_false_t%__'", true).addExpr("s_string_2 LIKE 'string_false_t__%'", true).addExpr("s_string_2 LIKE 'string_false_te_%'", true).addExpr("s_string_2 LIKE 'string_false_te%_'", true).addExpr("s_string_1 LIKE 'string_%test'", true).addExpr("s_string_1 LIKE 'string%_test'", true).addExpr("s_string_1 LIKE 'string_%_test'", true);    checker.buildRunAndCheck();}
protected PCollection<Row> beam_f26930_0()
{    Schema type = Schema.builder().addByteField("c_tinyint_0").addByteField("c_tinyint_1").addByteField("c_tinyint_2").addInt16Field("c_smallint_0").addInt16Field("c_smallint_1").addInt16Field("c_smallint_2").addInt32Field("c_integer_0").addInt32Field("c_integer_1").addInt32Field("c_integer_2").addInt64Field("c_bigint_0").addInt64Field("c_bigint_1").addInt64Field("c_bigint_2").addFloatField("c_float_0").addFloatField("c_float_1").addFloatField("c_float_2").addDoubleField("c_double_0").addDoubleField("c_double_1").addDoubleField("c_double_2").addDecimalField("c_decimal_0").addDecimalField("c_decimal_1").addDecimalField("c_decimal_2").addStringField("c_varchar_0").addStringField("c_varchar_1").addStringField("c_varchar_2").addBooleanField("c_boolean_false").addBooleanField("c_boolean_true").addStringField("s_string_1").addStringField("s_string_2").build();    try {        return TestBoundedTable.of(type).addRows((byte) 0, (byte) 1, (byte) 2, (short) 0, (short) 1, (short) 2, 0, 1, 2, 0L, 1L, 2L, 0.0f, 1.0f, 2.0f, 0.0, 1.0, 2.0, BigDecimal.ZERO, BigDecimal.ONE, BigDecimal.ONE.add(BigDecimal.ONE), "a", "b", "c", false, true, "string_true_test", "string_false_test").buildIOReader(pipeline.begin()).setRowSchema(type);    } catch (Exception e) {        throw new RuntimeException(e);    }}
public void beam_f26931_0() throws Exception
{    String sql = "SELECT " + "LOCALTIME as l," + "LOCALTIMESTAMP as l1," + "CURRENT_DATE as c1," + "CURRENT_TIME as c2," + "CURRENT_TIMESTAMP as c3" + " FROM PCOLLECTION";    PCollection<Row> rows = getTestPCollection().apply(SqlTransform.query(sql));    PAssert.that(rows).satisfies(new Checker());    pipeline.run();}
public String beam_f26939_0()
{    return delegateTableProvider.getTableType();}
public void beam_f26940_0(Table table)
{    delegateTableProvider.createTable(table);}
public void beam_f26941_0(String tableName, Row... rows)
{    delegateTableProvider.addRows(tableName, rows);}
public void beam_f26949_0() throws Exception
{    CustomResolutionTestTableProvider tableProvider = new CustomResolutionTestTableProvider();    tableProvider.createTable(Table.builder().name("testtable_blah").schema(BASIC_SCHEMA).type("test").build());    tableProvider.addRows("testtable_blah", row(1, "one"), row(2, "two"));    PCollection<Row> result = pipeline.apply(SqlTransform.query("SELECT id, name FROM testtable.blah").withDefaultTableProvider("testprovider", tableProvider));    PAssert.that(result).containsInAnyOrder(row(1, "one"), row(2, "two"));    pipeline.run().waitUntilFinish(Duration.standardMinutes(2));}
public void beam_f26950_0() throws Exception
{    CustomResolutionTestTableProvider tableProvider = new CustomResolutionTestTableProvider();    tableProvider.createTable(Table.builder().name("testtable_blah").schema(BASIC_SCHEMA).type("test").build());    tableProvider.addRows("testtable_blah", row(1, "one"), row(2, "two"));    PCollection<Row> result = pipeline.apply(SqlTransform.query("SELECT id, name FROM testprovider.testtable.blah").withDefaultTableProvider("testprovider", tableProvider));    PAssert.that(result).containsInAnyOrder(row(1, "one"), row(2, "two"));    pipeline.run().waitUntilFinish(Duration.standardMinutes(2));}
public void beam_f26951_0() throws Exception
{    CustomResolutionTestTableProvider tableProvider = new CustomResolutionTestTableProvider();    tableProvider.createTable(Table.builder().name("testtable_blah_foo_bar").schema(BASIC_SCHEMA).type("test").build());    tableProvider.addRows("testtable_blah_foo_bar", row(1, "one"), row(2, "two"));    PCollection<Row> result = pipeline.apply(SqlTransform.query("SELECT id, name FROM testtable.blah.foo.bar").withDefaultTableProvider("testprovider", tableProvider));    PAssert.that(result).containsInAnyOrder(row(1, "one"), row(2, "two"));    pipeline.run().waitUntilFinish(Duration.standardMinutes(2));}
public void beam_f26959_0() throws Exception
{    CustomResolutionTestTableProvider tableProvider = new CustomResolutionTestTableProvider();    tableProvider.createTable(Table.builder().name("testtable_blah_foo_bar").schema(BASIC_SCHEMA).type("test").build());    tableProvider.addRows("testtable_blah_foo_bar", row(3, "customer"), row(2, "nobody"));    CustomResolutionTestTableProvider tableProvider2 = new CustomResolutionTestTableProvider();    tableProvider2.createTable(Table.builder().name("testtable_blah_foo_bar2").schema(BASIC_SCHEMA).type("test").build());    tableProvider2.addRows("testtable_blah_foo_bar2", row(4, "customer"), row(1, "nobody"));    PCollection<Row> result = pipeline.apply(SqlTransform.query("SELECT id, name \n" + "FROM \n" + "  testprovider2.testtable.blah.foo.bar2 \n" + "UNION \n" + "    SELECT id, name \n" + "      FROM \n" + "        testtable.blah.foo.bar \n").withTableProvider("testprovider2", tableProvider2).withDefaultTableProvider("testprovider", tableProvider));    PAssert.that(result).containsInAnyOrder(row(4, "customer"), row(1, "nobody"), row(3, "customer"), row(2, "nobody"));    pipeline.run().waitUntilFinish(Duration.standardMinutes(2));}
private Row beam_f26960_0(int id, String name)
{    return Row.withSchema(BASIC_SCHEMA).addValues(id, name).build();}
public void beam_f26961_0()
{    File destinationFile = new File(tempFolder.getRoot(), "person-info.avro");    BeamSqlEnv env = BeamSqlEnv.inMemory(new AvroTableProvider());    env.executeDdl(String.format("CREATE EXTERNAL TABLE PersonInfo %s TYPE avro LOCATION '%s'", AVRO_FIELD_NAMES, destinationFile.getAbsolutePath()));    BeamSqlRelUtils.toPCollection(writePipeline, env.parseQuery("INSERT INTO PersonInfo VALUES ('Alan', 22, 'England'), ('John', 42, 'USA')"));    writePipeline.run().waitUntilFinish();    PCollection<Row> rows = BeamSqlRelUtils.toPCollection(readPipeline, env.parseQuery("SELECT age, country FROM PersonInfo where age > 25"));    PAssert.that(rows).containsInAnyOrder(Row.withSchema(OUTPUT_ROW_SCHEMA).addValues(42L, "USA").build());    PipelineResult.State state = readPipeline.run().waitUntilFinish();    assertEquals(state, State.DONE);}
public void beam_f26969_0()
{    BigQueryTableProvider provider = new BigQueryTableProvider();    Table table = getTable("testTable", bigQuery.tableSpec());    pipeline.apply(Create.of(new TableRow().set("id", 1).set("name", "name1"), new TableRow().set("id", 2).set("name", "name2"), new TableRow().set("id", 3).set("name", "name3")).withCoder(TableRowJsonCoder.of())).apply(BigQueryIO.writeTableRows().to(bigQuery.tableSpec()).withSchema(new TableSchema().setFields(ImmutableList.of(new TableFieldSchema().setName("id").setType("INTEGER"), new TableFieldSchema().setName("name").setType("STRING")))).withoutValidation());    pipeline.run().waitUntilFinish();    BeamSqlTable sqlTable = provider.buildBeamSqlTable(table);    BeamTableStatistics size1 = sqlTable.getTableStatistics(TestPipeline.testingPipelineOptions());    assertNotNull(size1);    assertEquals(3d, size1.getRowCount(), 0.1);}
public void beam_f26970_0()
{    BigQueryTestTableProvider provider = new BigQueryTestTableProvider();    Table table = getTable("testTable", bigQuery.tableSpec());    provider.addTable("testTable", table);    pipeline.apply(Create.of(new TableRow().set("id", 1).set("name", "name1"), new TableRow().set("id", 2).set("name", "name2"), new TableRow().set("id", 3).set("name", "name3")).withCoder(TableRowJsonCoder.of())).apply(BigQueryIO.writeTableRows().to(bigQuery.tableSpec()).withSchema(new TableSchema().setFields(ImmutableList.of(new TableFieldSchema().setName("id").setType("INTEGER"), new TableFieldSchema().setName("name").setType("STRING")))).withoutValidation());    pipeline.run().waitUntilFinish();        readingPipeline.getOptions().setJobName(FAKE_JOB_NAME);        readingPipeline.apply(SqlTransform.query(" select * from testTable ").withDefaultTableProvider("bigquery", provider));    readingPipeline.run().waitUntilFinish();    BigQueryTestTable sqlTable = (BigQueryTestTable) provider.buildBeamSqlTable(table);    assertEquals(FAKE_JOB_NAME, sqlTable.getJobName());}
public void beam_f26971_0()
{    BigQueryTableProvider provider = new BigQueryTableProvider();    Table table = getTable("fakeTable", "project:dataset.table");    BeamSqlTable sqlTable = provider.buildBeamSqlTable(table);    BeamTableStatistics size = sqlTable.getTableStatistics(TestPipeline.testingPipelineOptions());    assertTrue(size.isUnknown());}
public Table beam_f26979_0(String tableName)
{    return tableSpecMap.get(tableName);}
public BeamSqlTable beam_f26980_0(Table table)
{    BeamSqlTable t = beamSqlTableMap.get(table.getLocation());    if (t != null) {        return t;    }    t = new BigQueryTestTable(table, BigQueryUtils.ConversionOptions.builder().setTruncateTimestamps(firstNonNull(table.getProperties().getBoolean("truncateTimestamps"), false) ? BigQueryUtils.ConversionOptions.TruncateTimestamps.TRUNCATE : BigQueryUtils.ConversionOptions.TruncateTimestamps.REJECT).build());    beamSqlTableMap.put(table.getLocation(), t);    return t;}
public void beam_f26981_0()
{    KafkaCSVTestTable table = getTable(1);    for (int i = 0; i < 100; i++) {        table.addRecord(KafkaTestRecord.create("key1", i + ",1,2", "topic1", 500 * i));    }    BeamTableStatistics stats = table.getTableStatistics(null);    Assert.assertEquals(2d, stats.getRate(), 0.001);}
public void beam_f26989_0()
{    PCollection<Row> result = pipeline.apply(Create.of("1,\"1\",1.0", "2,2,2.0")).apply(ParDo.of(new String2KvBytes())).apply(new BeamKafkaCSVTable.CsvRecorderDecoder(genSchema(), CSVFormat.DEFAULT));    PAssert.that(result).containsInAnyOrder(ROW1, ROW2);    pipeline.run();}
public void beam_f26990_0()
{    PCollection<Row> result = pipeline.apply(Create.of(ROW1, ROW2)).apply(new BeamKafkaCSVTable.CsvRecorderEncoder(genSchema(), CSVFormat.DEFAULT)).apply(new BeamKafkaCSVTable.CsvRecorderDecoder(genSchema(), CSVFormat.DEFAULT));    PAssert.that(result).containsInAnyOrder(ROW1, ROW2);    pipeline.run();}
private static Schema beam_f26991_0()
{    JavaTypeFactory typeFactory = new JavaTypeFactoryImpl(RelDataTypeSystem.DEFAULT);    return CalciteUtils.toSchema(typeFactory.builder().add("order_id", SqlTypeName.BIGINT).add("site_id", SqlTypeName.INTEGER).add("price", SqlTypeName.DOUBLE).build());}
public Void beam_f26999_0(Row input)
{    System.out.println(input.getValues() + suffix);    return null;}
public void beam_f27000_0(ProcessContext c)
{    c.output(KV.of("fake_key", c.element()));}
public void beam_f27001_0(ProcessContext context, @StateId("seenValues") BagState<Row> seenValues, @StateId("count") ValueState<Integer> countState)
{        int count = MoreObjects.firstNonNull(countState.read(), 0);    count = count + 1;    countState.write(count);    seenValues.add(context.element().getValue());    if (count >= expected.size()) {        if (StreamSupport.stream(seenValues.read().spliterator(), false).collect(Collectors.toSet()).containsAll(expected)) {            System.out.println("in second if");            FLAG.put(context.getPipelineOptions().getOptionsId(), true);        }    }}
public void beam_f27009_0(int numberOfRecordsForRate)
{    this.numberOfRecordsForRate = numberOfRecordsForRate;}
private MockConsumer<byte[], byte[]> beam_f27010_0(Map<String, Object> config)
{    OffsetResetStrategy offsetResetStrategy = OffsetResetStrategy.EARLIEST;    final Map<TopicPartition, List<ConsumerRecord<byte[], byte[]>>> kafkaRecords = new HashMap<>();    Map<String, List<PartitionInfo>> partitionInfoMap = new HashMap<>();    Map<String, List<TopicPartition>> partitionMap = new HashMap<>();        for (String topic : this.getTopics()) {        List<PartitionInfo> partIds = new ArrayList<>(partitionsPerTopic);        List<TopicPartition> topicParitions = new ArrayList<>(partitionsPerTopic);        for (int i = 0; i < partitionsPerTopic; i++) {            TopicPartition tp = new TopicPartition(topic, i);            topicParitions.add(tp);            partIds.add(new PartitionInfo(topic, i, null, null, null));            kafkaRecords.put(tp, new ArrayList<>());        }        partitionInfoMap.put(topic, partIds);        partitionMap.put(topic, topicParitions);    }    TimestampType timestampType = TimestampType.forName((String) config.getOrDefault(TIMESTAMP_TYPE_CONFIG, TimestampType.LOG_APPEND_TIME.toString()));    for (KafkaTestRecord record : this.records) {        int partitionIndex = record.getKey().hashCode() % partitionsPerTopic;        TopicPartition tp = partitionMap.get(record.getTopic()).get(partitionIndex);        byte[] key = record.getKey().getBytes(UTF_8);        byte[] value = record.getValue().getBytes(UTF_8);        kafkaRecords.get(tp).add(new ConsumerRecord<>(tp.topic(), tp.partition(), kafkaRecords.get(tp).size(), record.getTimeStamp(), timestampType, 0, key.length, value.length, key, value));    }        final AtomicReference<List<TopicPartition>> assignedPartitions = new AtomicReference<>(Collections.<TopicPartition>emptyList());    final MockConsumer<byte[], byte[]> consumer = new MockConsumer<byte[], byte[]>(offsetResetStrategy) {        @Override        public synchronized void assign(final Collection<TopicPartition> assigned) {            Collection<TopicPartition> realPartitions = assigned.stream().map(part -> partitionMap.get(part.topic()).get(part.partition())).collect(Collectors.toList());            super.assign(realPartitions);            assignedPartitions.set(ImmutableList.copyOf(realPartitions));            for (TopicPartition tp : realPartitions) {                updateBeginningOffsets(ImmutableMap.of(tp, 0L));                updateEndOffsets(ImmutableMap.of(tp, (long) kafkaRecords.get(tp).size()));            }        }                @Override        public synchronized Map<TopicPartition, OffsetAndTimestamp> offsetsForTimes(Map<TopicPartition, Long> timestampsToSearch) {            return timestampsToSearch.entrySet().stream().map(e -> {                                long maxOffset = kafkaRecords.get(e.getKey()).size();                long offset = e.getValue();                OffsetAndTimestamp value = (offset >= maxOffset) ? null : new OffsetAndTimestamp(offset, offset);                return new AbstractMap.SimpleEntry<>(e.getKey(), value);            }).collect(Collectors.toMap(AbstractMap.SimpleEntry::getKey, AbstractMap.SimpleEntry::getValue));        }    };    for (String topic : getTopics()) {        consumer.updatePartitions(topic, partitionInfoMap.get(topic));    }    Runnable recordEnqueueTask = new Runnable() {        @Override        public void run() {                        int recordsAdded = 0;            for (TopicPartition tp : assignedPartitions.get()) {                long curPos = consumer.position(tp);                for (ConsumerRecord<byte[], byte[]> r : kafkaRecords.get(tp)) {                    if (r.offset() >= curPos) {                        consumer.addRecord(r);                        recordsAdded++;                    }                }            }            if (recordsAdded == 0) {                if (config.get("inject.error.at.eof") != null) {                    consumer.setException(new KafkaException("Injected error in consumer.poll()"));                }                                                                Uninterruptibles.sleepUninterruptibly(10, TimeUnit.MILLISECONDS);            }            consumer.schedulePollTask(this);        }    };    consumer.schedulePollTask(recordEnqueueTask);    return consumer;}
public synchronized void beam_f27011_0(final Collection<TopicPartition> assigned)
{    Collection<TopicPartition> realPartitions = assigned.stream().map(part -> partitionMap.get(part.topic()).get(part.partition())).collect(Collectors.toList());    super.assign(realPartitions);    assignedPartitions.set(ImmutableList.copyOf(realPartitions));    for (TopicPartition tp : realPartitions) {        updateBeginningOffsets(ImmutableMap.of(tp, 0L));        updateEndOffsets(ImmutableMap.of(tp, (long) kafkaRecords.get(tp).size()));    }}
private String beam_f27019_0(String fileName) throws IOException
{    InputStream inputStream = getClass().getResourceAsStream("/" + fileName);    File root = temporaryFolder.getRoot();    Path tempFilePath = new File(root, fileName).toPath();    Files.copy(inputStream, tempFilePath);    return tempFilePath.toString();}
public void beam_f27020_0() throws IOException
{    String parquetPath = extractParquetFile("users.parquet");    BeamSqlEnv env = BeamSqlEnv.inMemory(new ParquetTableProvider());    env.executeDdl(String.format("CREATE EXTERNAL TABLE users %s TYPE parquet LOCATION '%s'", SQL_PARQUET_FIELD, parquetPath));    PCollection<Row> rows = BeamSqlRelUtils.toPCollection(pipeline, env.parseQuery("SELECT name, favorite_color, favorite_numbers FROM users"));    PAssert.that(rows).containsInAnyOrder(Row.withSchema(PARQUET_SCHEMA).addValues("Alyssa", null, Arrays.asList(3, 9, 15, 20)).build(), Row.withSchema(PARQUET_SCHEMA).addValues("Ben", "red", Arrays.asList()).build());    pipeline.run();}
public void beam_f27021_0() throws Exception
{    String createTableString = "CREATE EXTERNAL TABLE message (\n" + "event_timestamp TIMESTAMP, \n" + "attributes MAP<VARCHAR, VARCHAR>, \n" + "payload ROW< \n" + "             id INTEGER, \n" + "             name VARCHAR \n" + "           > \n" + ") \n" + "TYPE 'pubsub' \n" + "LOCATION '" + eventsTopic.topicPath() + "' \n" + "TBLPROPERTIES '{ \"timestampAttributeKey\" : \"ts\" }'";    String queryString = "SELECT message.payload.id, message.payload.name from message";        List<PubsubMessage> messages = ImmutableList.of(message(ts(1), 3, "foo"), message(ts(2), 5, "bar"), message(ts(3), 7, "baz"));        BeamSqlEnv sqlEnv = BeamSqlEnv.inMemory(new PubsubJsonTableProvider());    sqlEnv.executeDdl(createTableString);        PCollection<Row> queryOutput = query(sqlEnv, pipeline, queryString);        queryOutput.apply("waitForSuccess", resultSignal.signalSuccessWhen(SchemaCoder.of(PAYLOAD_SCHEMA), observedRows -> observedRows.equals(ImmutableSet.of(row(PAYLOAD_SCHEMA, 3, "foo"), row(PAYLOAD_SCHEMA, 5, "bar"), row(PAYLOAD_SCHEMA, 7, "baz")))));        Supplier<Void> start = resultSignal.waitForStart(Duration.standardMinutes(5));    pipeline.begin().apply(resultSignal.signalStart());        pipeline.run();        start.get();        eventsTopic.publish(messages);        resultSignal.waitForSuccess(Duration.standardSeconds(60));}
private PCollection<Row> beam_f27029_0(BeamSqlEnv sqlEnv, TestPipeline pipeline, String queryString) throws Exception
{    return BeamSqlRelUtils.toPCollection(pipeline, sqlEnv.parseQuery(queryString));}
private PubsubMessage beam_f27030_0(Instant timestamp, int id, String name)
{    return message(timestamp, jsonString(id, name));}
private PubsubMessage beam_f27031_0(Instant timestamp, String jsonPayload)
{    return new PubsubMessage(jsonPayload.getBytes(UTF_8), ImmutableMap.of("ts", String.valueOf(timestamp.getMillis())));}
public void beam_f27039_0()
{    PubsubJsonTableProvider provider = new PubsubJsonTableProvider();    Schema messageSchema = Schema.builder().addDateTimeField("event_timestamp").addMapField("attributes", VARCHAR, VARCHAR).addStringField("someField").addRowField("payload", Schema.builder().build()).build();    Table tableDefinition = tableDefinition().schema(messageSchema).build();    thrown.expectMessage("Unsupported");    thrown.expectMessage("'event_timestamp'");    provider.buildBeamSqlTable(tableDefinition);}
private static Table.Builder beam_f27040_0()
{    return Table.builder().name("FakeTable").comment("fake table").location("projects/project/topics/topic").schema(Schema.builder().build()).type("pubsub").properties(JSON.parseObject("{ \"timestampAttributeKey\" : \"ts_field\" }"));}
public void beam_f27041_0()
{    Schema payloadSchema = Schema.builder().addNullableField("id", FieldType.INT32).addNullableField("name", FieldType.STRING).build();    Schema messageSchema = Schema.builder().addDateTimeField("event_timestamp").addMapField("attributes", VARCHAR, VARCHAR).addRowField("payload", payloadSchema).build();    PCollection<Row> rows = pipeline.apply("create", Create.timestamped(message(1, map("attr", "val"), "{ \"id\" : 3, \"name\" : \"foo\" }"), message(2, map("bttr", "vbl"), "{ \"name\" : \"baz\", \"id\" : 5 }"), message(3, map("cttr", "vcl"), "{ \"id\" : 7, \"name\" : \"bar\" }"), message(4, map("dttr", "vdl"), "{ \"name\" : \"qaz\", \"id\" : 8 }"), message(4, map("dttr", "vdl"), "{ \"name\" : null, \"id\" : null }"))).apply("convert", ParDo.of(PubsubMessageToRow.builder().messageSchema(messageSchema).useDlq(false).build()));    PAssert.that(rows).containsInAnyOrder(Row.withSchema(messageSchema).addValues(ts(1), map("attr", "val"), row(payloadSchema, 3, "foo")).build(), Row.withSchema(messageSchema).addValues(ts(2), map("bttr", "vbl"), row(payloadSchema, 5, "baz")).build(), Row.withSchema(messageSchema).addValues(ts(3), map("cttr", "vcl"), row(payloadSchema, 7, "bar")).build(), Row.withSchema(messageSchema).addValues(ts(4), map("dttr", "vdl"), row(payloadSchema, 8, "qaz")).build(), Row.withSchema(messageSchema).addValues(ts(4), map("dttr", "vdl"), row(payloadSchema, null, null)).build());    pipeline.run();}
public void beam_f27050_0() throws Exception
{    Files.write(tempFolder.newFile("test.csv").toPath(), "hello\t13\n\ngoodbye\t42\n".getBytes(Charsets.UTF_8));    BeamSqlEnv env = BeamSqlEnv.inMemory(new TextTableProvider());    env.executeDdl(String.format("CREATE EXTERNAL TABLE test %s TYPE text LOCATION '%s/*' TBLPROPERTIES '{\"format\":\"TDF\"}'", SQL_CSV_SCHEMA, tempFolder.getRoot()));    PCollection<Row> rows = BeamSqlRelUtils.toPCollection(pipeline, env.parseQuery("SELECT * FROM test"));    rows.apply(MapElements.into(TypeDescriptors.voids()).via(r -> {        System.out.println(r.toString());        return null;    }));    PAssert.that(rows).containsInAnyOrder(Row.withSchema(CSV_SCHEMA).addValues("hello", 13).build(), Row.withSchema(CSV_SCHEMA).addValues("goodbye", 42).build());    pipeline.run();}
public void beam_f27051_0() throws Exception
{    Files.write(tempFolder.newFile("test.csv").toPath(), "hello,13\n\ngoodbye,42\n".getBytes(Charsets.UTF_8));    BeamSqlEnv env = BeamSqlEnv.inMemory(new TextTableProvider());    env.executeDdl(String.format("CREATE EXTERNAL TABLE test %s TYPE text LOCATION '%s/*' TBLPROPERTIES '{\"format\":\"csv\"}'", SQL_CSV_SCHEMA, tempFolder.getRoot()));    PCollection<Row> rows = BeamSqlRelUtils.toPCollection(pipeline, env.parseQuery("SELECT * FROM test"));    PAssert.that(rows).containsInAnyOrder(Row.withSchema(CSV_SCHEMA).addValues("hello", 13).build(), Row.withSchema(CSV_SCHEMA).addValues("goodbye", 42).build());    pipeline.run();}
public void beam_f27052_0() throws Exception
{    Files.write(tempFolder.newFile("test.csv").toPath(), "hello\n\ngoodbye\n".getBytes(Charsets.UTF_8));    BeamSqlEnv env = BeamSqlEnv.inMemory(new TextTableProvider());    env.executeDdl(String.format("CREATE EXTERNAL TABLE test %s TYPE text LOCATION '%s/*' " + "TBLPROPERTIES '{\"format\":\"csv\", \"csvFormat\":\"Excel\"}'", SINGLE_STRING_SQL_SCHEMA, tempFolder.getRoot()));    PCollection<Row> rows = BeamSqlRelUtils.toPCollection(pipeline, env.parseQuery("SELECT * FROM test"));    PAssert.that(rows).containsInAnyOrder(Row.withSchema(SINGLE_STRING_CSV_SCHEMA).addValues("hello").build(), Row.withSchema(SINGLE_STRING_CSV_SCHEMA).addValues("goodbye").build());    pipeline.run();}
public void beam_f27060_0() throws Exception
{    store.createTable(mockTable("hello"));    store.createTable(mockTable("world"));    assertEquals(2, store.getTables().size());    assertThat(store.getTables(), Matchers.hasValue(mockTable("hello")));    assertThat(store.getTables(), Matchers.hasValue(mockTable("world")));}
public void beam_f27061_0() throws Exception
{    Table table = mockTable("hello");    store.createTable(table);    BeamSqlTable actualSqlTable = store.buildBeamSqlTable(table);    assertNotNull(actualSqlTable);    assertEquals(Schema.builder().addNullableField("id", Schema.FieldType.INT32).addNullableField("name", Schema.FieldType.STRING).build(), actualSqlTable.getSchema());}
public void beam_f27062_0() throws Exception
{    store.registerProvider(new MockTableProvider("mock", "hello", "world"));    assertNotNull(store.getProviders());    assertEquals(2, store.getProviders().size());    assertEquals("text", store.getProviders().get("text").getTableType());    assertEquals("mock", store.getProviders().get("mock").getTableType());    assertEquals(2, store.getTables().size());}
public void beam_f27072_0() throws Exception
{    BeamSqlEnv sqlEnv = BeamSqlEnv.inMemory(new PubsubJsonTableProvider(), new BigQueryTableProvider());    String createTableString = "CREATE EXTERNAL TABLE pubsub_topic (\n" + "event_timestamp TIMESTAMP, \n" + "attributes MAP<VARCHAR, VARCHAR>, \n" + "payload ROW< \n" + "             id INTEGER, \n" + "             name VARCHAR \n" + "           > \n" + ") \n" + "TYPE 'pubsub' \n" + "LOCATION '" + pubsub.topicPath() + "' \n" + "TBLPROPERTIES '{ \"timestampAttributeKey\" : \"ts\" }'";    sqlEnv.executeDdl(createTableString);    String createTableStatement = "CREATE EXTERNAL TABLE bq_table( \n" + "   id BIGINT, \n" + "   name VARCHAR \n " + ") \n" + "TYPE 'bigquery' \n" + "LOCATION '" + bigQuery.tableSpec() + "'";    sqlEnv.executeDdl(createTableStatement);    String insertStatement = "INSERT INTO bq_table \n" + "SELECT \n" + "  pubsub_topic.payload.id, \n" + "  pubsub_topic.payload.name \n" + "FROM pubsub_topic";    BeamSqlRelUtils.toPCollection(pipeline, sqlEnv.parseQuery(insertStatement));    pipeline.run();    List<PubsubMessage> messages = ImmutableList.of(message(ts(1), 3, "foo"), message(ts(2), 5, "bar"), message(ts(3), 7, "baz"));    pubsub.publish(messages);    bigQuery.assertThatAllRows(SOURCE_SCHEMA).eventually(containsInAnyOrder(row(SOURCE_SCHEMA, 3L, "foo"), row(SOURCE_SCHEMA, 5L, "bar"), row(SOURCE_SCHEMA, 7L, "baz"))).pollFor(Duration.standardMinutes(5));}
private Row beam_f27073_0(Schema schema, Object... values)
{    return Row.withSchema(schema).addValues(values).build();}
private PubsubMessage beam_f27074_0(Instant timestamp, int id, String name)
{    return message(timestamp, jsonString(id, name));}
public static RowsBuilder beam_f27082_0(final Object... args)
{    Schema beamSQLSchema = TestTableUtils.buildBeamSqlSchema(args);    RowsBuilder builder = new RowsBuilder();    builder.type = beamSQLSchema;    return builder;}
public static RowsBuilder beam_f27083_0(final Object... args)
{    Schema beamSQLSchema = TestTableUtils.buildBeamSqlNullableSchema(args);    RowsBuilder builder = new RowsBuilder();    builder.type = beamSQLSchema;    return builder;}
public static RowsBuilder beam_f27084_0(final Schema schema)
{    RowsBuilder builder = new RowsBuilder();    builder.type = schema;    return builder;}
public PCollectionBuilder beam_f27092_0(List<Row> rows)
{    this.rows = rows;    return this;}
public PCollectionBuilder beam_f27093_0(String timestampField)
{    this.timestampField = timestampField;    return this;}
public PCollectionBuilder beam_f27094_0(Pipeline pipeline)
{    this.pipeline = pipeline;    return this;}
public static DateTime beam_f27102_0(String str)
{    return DateTimeFormat.forPattern("yyyy-MM-dd").withZoneUTC().parseDateTime(str);}
public static DateTime beam_f27103_0(String str)
{        if (str.indexOf('.') == -1) {        return DateTimeFormat.forPattern("HH:mm:ss").withZoneUTC().parseDateTime(str);    } else {        return DateTimeFormat.forPattern("HH:mm:ss.SSS").withZoneUTC().parseDateTime(str);    }}
public FieldType beam_f27104_0(SourceOfRandomness random, GenerationStatus status)
{    return random.choose(PRIMITIVE_TYPES);}
 void beam_f27112_0(GenerationStatus status)
{    status.setValue(NESTING_KEY, nestingLevel(status) + 1);}
public static SerializableFunction<Iterable<Row>, Void> beam_f27113_0(int expected)
{    return records -> {        Row row = Iterables.getOnlyElement(records);        assertNotNull(row);        assertEquals(expected, (int) row.getInt32(0));        return null;    };}
public static SerializableFunction<Iterable<Row>, Void> beam_f27114_0(long expected)
{    return records -> {        Row row = Iterables.getOnlyElement(records);        assertNotNull(row);        assertEquals(expected, (long) row.getInt64(0));        return null;    };}
public void beam_f27122_0() throws Exception
{    FrameworkConfig cfg = initializeCalcite();    PCollection<Row> result = applySqlTransform(pipeline, cfg, "SELECT t1.RowKey FROM " + FULL_ON_ID + " AS t1 \n" + " INNER JOIN a.`b-\\`c`.d t2 on t1.RowKey = t2.RowKey");    PAssert.that(result).containsInAnyOrder(singleValue(16L), singleValue(15L));    pipeline.run().waitUntilFinish(Duration.standardMinutes(TWO_MINUTES));}
public void beam_f27123_0() throws Exception
{    FrameworkConfig cfg = initializeCalcite();    PCollection<Row> result = applySqlTransform(pipeline, cfg, "SELECT t.Key FROM a.b AS t");    PAssert.that(result).containsInAnyOrder(singleValue(14L), singleValue(15L));    pipeline.run().waitUntilFinish(Duration.standardMinutes(TWO_MINUTES));}
public void beam_f27124_0() throws Exception
{    FrameworkConfig cfg = initializeCalcite();    PCollection<Row> result = applySqlTransform(pipeline, cfg, "SELECT t.Key FROM c.d.e AS t");    PAssert.that(result).containsInAnyOrder(singleValue(14L), singleValue(15L));    pipeline.run().waitUntilFinish(Duration.standardMinutes(TWO_MINUTES));}
public void beam_f27132_0() throws Exception
{    FrameworkConfig cfg = initializeCalcite();    PCollection<Row> result = applySqlTransform(pipeline, cfg, "SELECT struct_col.struct_col_str FROM a.`table:com`.`..::with-struct::..`");    PAssert.that(result).containsInAnyOrder(singleValue("row_one"), singleValue("row_two"));    pipeline.run().waitUntilFinish(Duration.standardMinutes(TWO_MINUTES));}
public void beam_f27133_0() throws Exception
{    FrameworkConfig cfg = initializeCalcite();    PCollection<Row> result = applySqlTransform(pipeline, cfg, "SELECT `..::with-struct::..`.struct_col.struct_col_str \n" + " FROM a.`table:com`.`..::with-struct::..`");    PAssert.that(result).containsInAnyOrder(singleValue("row_one"), singleValue("row_two"));    pipeline.run().waitUntilFinish(Duration.standardMinutes(TWO_MINUTES));}
public void beam_f27134_0() throws Exception
{    FrameworkConfig cfg = initializeCalcite();    PCollection<Row> result = applySqlTransform(pipeline, cfg, "SELECT t.struct_col.struct_col_str FROM " + TABLE_WITH_STRUCTS_ID + " t");    PAssert.that(result).containsInAnyOrder(singleValue("row_one"), singleValue("row_two"));    pipeline.run().waitUntilFinish(Duration.standardMinutes(TWO_MINUTES));}
public void beam_f27142_0()
{    String sql = "SELECT CAST (1243 as INT64), " + "CAST ('2018-09-15 12:59:59.000000+00' as TIMESTAMP), " + "CAST ('string' as STRING);";    ZetaSQLQueryPlanner zetaSQLQueryPlanner = new ZetaSQLQueryPlanner(config);    BeamRelNode beamRelNode = zetaSQLQueryPlanner.convertToBeamRel(sql);    PCollection<Row> stream = BeamSqlRelUtils.toPCollection(pipeline, beamRelNode);    final Schema schema = Schema.builder().addInt64Field("field1").addDateTimeField("field2").addStringField("field3").build();    PAssert.that(stream).containsInAnyOrder(Row.withSchema(schema).addValues(1243L, new DateTime(2018, 9, 15, 12, 59, 59, ISOChronology.getInstanceUTC()), "string").build());    pipeline.run().waitUntilFinish(Duration.standardMinutes(PIPELINE_EXECUTION_WAITTIME_MINUTES));}
public void beam_f27143_0()
{    String sql = "SELECT @p0 = @p1 AS ColA";    ImmutableMap<String, Value> params = ImmutableMap.<String, Value>builder().put("p0", Value.createSimpleNullValue(TypeKind.TYPE_BOOL)).put("p1", Value.createBoolValue(true)).build();    ZetaSQLQueryPlanner zetaSQLQueryPlanner = new ZetaSQLQueryPlanner(config);    BeamRelNode beamRelNode = zetaSQLQueryPlanner.convertToBeamRel(sql, params);    PCollection<Row> stream = BeamSqlRelUtils.toPCollection(pipeline, beamRelNode);    final Schema schema = Schema.builder().addNullableField("field1", FieldType.BOOLEAN).build();    PAssert.that(stream).containsInAnyOrder(Row.withSchema(schema).addValues((Boolean) null).build());    pipeline.run().waitUntilFinish(Duration.standardMinutes(PIPELINE_EXECUTION_WAITTIME_MINUTES));}
public void beam_f27144_0()
{    String sql = "SELECT @p0 = @p1 AS ColA";    ImmutableMap<String, Value> params = ImmutableMap.<String, Value>builder().put("p0", Value.createDoubleValue(0)).put("p1", Value.createDoubleValue(Double.POSITIVE_INFINITY)).build();    ZetaSQLQueryPlanner zetaSQLQueryPlanner = new ZetaSQLQueryPlanner(config);    BeamRelNode beamRelNode = zetaSQLQueryPlanner.convertToBeamRel(sql, params);    PCollection<Row> stream = BeamSqlRelUtils.toPCollection(pipeline, beamRelNode);    final Schema schema = Schema.builder().addBooleanField("field1").build();    PAssert.that(stream).containsInAnyOrder(Row.withSchema(schema).addValues(false).build());    pipeline.run().waitUntilFinish(Duration.standardMinutes(PIPELINE_EXECUTION_WAITTIME_MINUTES));}
public void beam_f27152_0()
{    String sql = "SELECT COALESCE(@p0, @p1, @p2) AS ColA";    ImmutableMap<String, Value> params = ImmutableMap.of("p0", Value.createSimpleNullValue(TypeKind.TYPE_STRING), "p1", Value.createStringValue("yay"), "p2", Value.createStringValue("nay"));    ZetaSQLQueryPlanner zetaSQLQueryPlanner = new ZetaSQLQueryPlanner(config);    BeamRelNode beamRelNode = zetaSQLQueryPlanner.convertToBeamRel(sql, params);    PCollection<Row> stream = BeamSqlRelUtils.toPCollection(pipeline, beamRelNode);    final Schema schema = Schema.builder().addNullableField("field1", FieldType.STRING).build();    PAssert.that(stream).containsInAnyOrder(Row.withSchema(schema).addValues("yay").build());    pipeline.run().waitUntilFinish(Duration.standardMinutes(PIPELINE_EXECUTION_WAITTIME_MINUTES));}
public void beam_f27153_0()
{    String sql = "SELECT COALESCE(@p0) AS ColA";    ImmutableMap<String, Value> params = ImmutableMap.of("p0", Value.createSimpleNullValue(TypeKind.TYPE_INT64));    ZetaSQLQueryPlanner zetaSQLQueryPlanner = new ZetaSQLQueryPlanner(config);    BeamRelNode beamRelNode = zetaSQLQueryPlanner.convertToBeamRel(sql, params);    PCollection<Row> stream = BeamSqlRelUtils.toPCollection(pipeline, beamRelNode);    final Schema schema = Schema.builder().addNullableField("field1", FieldType.array(FieldType.INT64)).build();    PAssert.that(stream).containsInAnyOrder(Row.withSchema(schema).addValue(null).build());    pipeline.run().waitUntilFinish(Duration.standardMinutes(PIPELINE_EXECUTION_WAITTIME_MINUTES));}
public void beam_f27154_0()
{    String sql = "SELECT COALESCE(@p0, @p1) AS ColA";    ImmutableMap<String, Value> params = ImmutableMap.of("p0", Value.createNullValue(TypeFactory.createArrayType(TypeFactory.createSimpleType(TypeKind.TYPE_INT64))), "p1", Value.createNullValue(TypeFactory.createArrayType(TypeFactory.createSimpleType(TypeKind.TYPE_INT64))));    ZetaSQLQueryPlanner zetaSQLQueryPlanner = new ZetaSQLQueryPlanner(config);    BeamRelNode beamRelNode = zetaSQLQueryPlanner.convertToBeamRel(sql, params);    PCollection<Row> stream = BeamSqlRelUtils.toPCollection(pipeline, beamRelNode);    final Schema schema = Schema.builder().addNullableField("field1", FieldType.array(FieldType.INT64)).build();    PAssert.that(stream).containsInAnyOrder(Row.withSchema(schema).addValue(null).build());    pipeline.run().waitUntilFinish(Duration.standardMinutes(PIPELINE_EXECUTION_WAITTIME_MINUTES));}
public void beam_f27162_0()
{    String sql = "SELECT IFNULL(@p0, @p1) AS ColA";    ImmutableMap<String, Value> params = ImmutableMap.of("p0", Value.createSimpleNullValue(TypeKind.TYPE_STRING), "p1", Value.createStringValue("yay"));    ZetaSQLQueryPlanner zetaSQLQueryPlanner = new ZetaSQLQueryPlanner(config);    BeamRelNode beamRelNode = zetaSQLQueryPlanner.convertToBeamRel(sql, params);    PCollection<Row> stream = BeamSqlRelUtils.toPCollection(pipeline, beamRelNode);    final Schema schema = Schema.builder().addNullableField("field1", FieldType.STRING).build();    PAssert.that(stream).containsInAnyOrder(Row.withSchema(schema).addValues("yay").build());    pipeline.run().waitUntilFinish(Duration.standardMinutes(PIPELINE_EXECUTION_WAITTIME_MINUTES));}
public void beam_f27163_0()
{    String sql = "SELECT @p0 AS ColA";    ImmutableMap<String, Value> params = ImmutableMap.of("p0", Value.createArrayValue(TypeFactory.createArrayType(TypeFactory.createSimpleType(TypeKind.TYPE_INT64)), ImmutableList.of()));    ZetaSQLQueryPlanner zetaSQLQueryPlanner = new ZetaSQLQueryPlanner(config);    BeamRelNode beamRelNode = zetaSQLQueryPlanner.convertToBeamRel(sql, params);    PCollection<Row> stream = BeamSqlRelUtils.toPCollection(pipeline, beamRelNode);    final Schema schema = Schema.builder().addArrayField("field1", FieldType.INT64).build();    PAssert.that(stream).containsInAnyOrder(Row.withSchema(schema).addValue(ImmutableList.of()).build());    pipeline.run().waitUntilFinish(Duration.standardMinutes(PIPELINE_EXECUTION_WAITTIME_MINUTES));}
public void beam_f27164_0()
{    String sql = "SELECT @p0 LIKE  @p1 AS ColA";    ImmutableMap<String, Value> params = ImmutableMap.of("p0", Value.createStringValue("ab%"), "p1", Value.createStringValue("ab\\%"));    ZetaSQLQueryPlanner zetaSQLQueryPlanner = new ZetaSQLQueryPlanner(config);    BeamRelNode beamRelNode = zetaSQLQueryPlanner.convertToBeamRel(sql, params);    PCollection<Row> stream = BeamSqlRelUtils.toPCollection(pipeline, beamRelNode);    final Schema schema = Schema.builder().addNullableField("field1", FieldType.BOOLEAN).build();    PAssert.that(stream).containsInAnyOrder(Row.withSchema(schema).addValues(true).build());    pipeline.run().waitUntilFinish(Duration.standardMinutes(PIPELINE_EXECUTION_WAITTIME_MINUTES));}
public void beam_f27172_0()
{    String sql = "SELECT a FROM (SELECT 1 a UNION ALL SELECT 2 UNION ALL SELECT 3)";    ZetaSQLQueryPlanner zetaSQLQueryPlanner = new ZetaSQLQueryPlanner(config);    BeamRelNode beamRelNode = zetaSQLQueryPlanner.convertToBeamRel(sql);    PCollection<Row> stream = BeamSqlRelUtils.toPCollection(pipeline, beamRelNode);    final Schema schema = Schema.builder().addInt64Field("field1").build();    PAssert.that(stream).containsInAnyOrder(Row.withSchema(schema).addValues(1L).build(), Row.withSchema(schema).addValues(2L).build(), Row.withSchema(schema).addValues(3L).build());    pipeline.run().waitUntilFinish(Duration.standardMinutes(PIPELINE_EXECUTION_WAITTIME_MINUTES));}
public void beam_f27173_0()
{    String sql = "SELECT CAST (1243 as INT64), " + "CAST ('2018-09-15 12:59:59.000000+00' as TIMESTAMP), " + "CAST ('string' as STRING) " + " UNION DISTINCT " + " SELECT CAST (1243 as INT64), " + "CAST ('2018-09-15 12:59:59.000000+00' as TIMESTAMP), " + "CAST ('string' as STRING);";    ZetaSQLQueryPlanner zetaSQLQueryPlanner = new ZetaSQLQueryPlanner(config);    BeamRelNode beamRelNode = zetaSQLQueryPlanner.convertToBeamRel(sql);    PCollection<Row> stream = BeamSqlRelUtils.toPCollection(pipeline, beamRelNode);    final Schema schema = Schema.builder().addInt64Field("field1").addDateTimeField("field2").addStringField("field3").build();    PAssert.that(stream).containsInAnyOrder(Row.withSchema(schema).addValues(1243L, new DateTime(2018, 9, 15, 12, 59, 59, ISOChronology.getInstanceUTC()), "string").build());    pipeline.run().waitUntilFinish(Duration.standardMinutes(PIPELINE_EXECUTION_WAITTIME_MINUTES));}
public void beam_f27174_0()
{    String sql = "SELECT t1.Key " + "FROM KeyValue AS t1" + " INNER JOIN BigTable AS t2" + " on " + " t1.Key = t2.RowKey AND t1.ts = t2.ts";    ZetaSQLQueryPlanner zetaSQLQueryPlanner = new ZetaSQLQueryPlanner(config);    BeamRelNode beamRelNode = zetaSQLQueryPlanner.convertToBeamRel(sql);    PCollection<Row> stream = BeamSqlRelUtils.toPCollection(pipeline, beamRelNode);    PAssert.that(stream).containsInAnyOrder(Row.withSchema(Schema.builder().addInt64Field("field1").build()).addValues(15L).build());    pipeline.run().waitUntilFinish(Duration.standardMinutes(PIPELINE_EXECUTION_WAITTIME_MINUTES));}
public void beam_f27182_0()
{    String sql = "SELECT * FROM Spanner as t1 " + "JOIN Spanner as t2 " + "ON (t1.ColId = t2.ColId) WHERE t1.ColId = 17";    ZetaSQLQueryPlanner zetaSQLQueryPlanner = new ZetaSQLQueryPlanner(config);    BeamRelNode beamRelNode = zetaSQLQueryPlanner.convertToBeamRel(sql);    PCollection<Row> stream = BeamSqlRelUtils.toPCollection(pipeline, beamRelNode);    PAssert.that(stream).containsInAnyOrder(Row.withSchema(Schema.builder().addInt64Field("field1").addStringField("field2").addInt64Field("field3").addStringField("field4").build()).addValues(17L, "Spanner237", 17L, "Spanner237").build());    pipeline.run().waitUntilFinish(Duration.standardMinutes(PIPELINE_EXECUTION_WAITTIME_MINUTES));}
public void beam_f27183_0()
{    String sql = "SELECT * FROM (SELECT \"apple\" AS fruit, \"carrot\" AS vegetable);";    ZetaSQLQueryPlanner zetaSQLQueryPlanner = new ZetaSQLQueryPlanner(config);    BeamRelNode beamRelNode = zetaSQLQueryPlanner.convertToBeamRel(sql);    PCollection<Row> stream = BeamSqlRelUtils.toPCollection(pipeline, beamRelNode);    final Schema schema = Schema.builder().addStringField("field1").addStringField("field2").build();    PAssert.that(stream).containsInAnyOrder(Row.withSchema(schema).addValues("apple", "carrot").build());    pipeline.run().waitUntilFinish(Duration.standardMinutes(PIPELINE_EXECUTION_WAITTIME_MINUTES));    Schema outputSchema = stream.getSchema();    Assert.assertEquals(2, outputSchema.getFieldCount());    Assert.assertEquals("fruit", outputSchema.getField(0).getName());    Assert.assertEquals("vegetable", outputSchema.getField(1).getName());}
public void beam_f27184_0()
{    String sql = "SELECT Key, Value FROM KeyValue;";    ZetaSQLQueryPlanner zetaSQLQueryPlanner = new ZetaSQLQueryPlanner(config);    BeamRelNode beamRelNode = zetaSQLQueryPlanner.convertToBeamRel(sql);    PCollection<Row> stream = BeamSqlRelUtils.toPCollection(pipeline, beamRelNode);    final Schema schema = Schema.builder().addInt64Field("field1").addStringField("field2").build();    PAssert.that(stream).containsInAnyOrder(Row.withSchema(schema).addValues(14L, "KeyValue234").build(), Row.withSchema(schema).addValues(15L, "KeyValue235").build());    pipeline.run().waitUntilFinish(Duration.standardMinutes(PIPELINE_EXECUTION_WAITTIME_MINUTES));}
public void beam_f27192_0()
{    String sql = "SELECT CAST(table_with_struct.id AS STRING) FROM table_with_struct WHERE" + " table_with_struct.struct_col.struct_col_str = 'row_one';";    ZetaSQLQueryPlanner zetaSQLQueryPlanner = new ZetaSQLQueryPlanner(config);    BeamRelNode beamRelNode = zetaSQLQueryPlanner.convertToBeamRel(sql);    PCollection<Row> stream = BeamSqlRelUtils.toPCollection(pipeline, beamRelNode);    final Schema schema = Schema.builder().addStringField("field").build();    PAssert.that(stream).containsInAnyOrder(Row.withSchema(schema).addValue("1").build());    pipeline.run().waitUntilFinish(Duration.standardMinutes(PIPELINE_EXECUTION_WAITTIME_MINUTES));}
public void beam_f27193_0()
{    String sql = "SELECT CAST(A.struct_col.struct_col_str AS TIMESTAMP) FROM table_with_struct_ts_string AS" + " A";    ZetaSQLQueryPlanner zetaSQLQueryPlanner = new ZetaSQLQueryPlanner(config);    BeamRelNode beamRelNode = zetaSQLQueryPlanner.convertToBeamRel(sql);    PCollection<Row> stream = BeamSqlRelUtils.toPCollection(pipeline, beamRelNode);    final Schema schema = Schema.builder().addDateTimeField("field").build();    PAssert.that(stream).containsInAnyOrder(Row.withSchema(schema).addValue(parseTimestampWithUTCTimeZone("2019-01-15 13:21:03")).build());    pipeline.run().waitUntilFinish(Duration.standardMinutes(PIPELINE_EXECUTION_WAITTIME_MINUTES));}
public void beam_f27194_0()
{    String sql = "SELECT TUMBLE_START('INTERVAL 1 MINUTE') FROM table_with_struct_ts_string AS A GROUP BY " + "TUMBLE(CAST(A.struct_col.struct_col_str AS TIMESTAMP), 'INTERVAL 1 MINUTE')";    ZetaSQLQueryPlanner zetaSQLQueryPlanner = new ZetaSQLQueryPlanner(config);    BeamRelNode beamRelNode = zetaSQLQueryPlanner.convertToBeamRel(sql);    PCollection<Row> stream = BeamSqlRelUtils.toPCollection(pipeline, beamRelNode);    final Schema schema = Schema.builder().addDateTimeField("field").build();    PAssert.that(stream).containsInAnyOrder(Row.withSchema(schema).addValue(parseTimestampWithUTCTimeZone("2019-01-15 13:21:00")).build());    pipeline.run().waitUntilFinish(Duration.standardMinutes(PIPELINE_EXECUTION_WAITTIME_MINUTES));}
public void beam_f27202_0()
{    String sql = "SELECT COUNT(Key) FROM KeyValue";    ZetaSQLQueryPlanner zetaSQLQueryPlanner = new ZetaSQLQueryPlanner(config);    BeamRelNode beamRelNode = zetaSQLQueryPlanner.convertToBeamRel(sql);    PCollection<Row> stream = BeamSqlRelUtils.toPCollection(pipeline, beamRelNode);    final Schema schema = Schema.builder().addInt64Field("field1").build();    PAssert.that(stream).containsInAnyOrder(Row.withSchema(schema).addValues(2L).build());    pipeline.run().waitUntilFinish(Duration.standardMinutes(PIPELINE_EXECUTION_WAITTIME_MINUTES));}
public void beam_f27203_0()
{    String sql = "SELECT Key, COUNT(DISTINCT Value) FROM KeyValue GROUP BY Key";    ZetaSQLQueryPlanner zetaSQLQueryPlanner = new ZetaSQLQueryPlanner(config);    thrown.expect(RuntimeException.class);    thrown.expectMessage("Does not support COUNT DISTINCT");    zetaSQLQueryPlanner.convertToBeamRel(sql);}
public void beam_f27204_0()
{    String sql = "SELECT Key, COUNT(*) FROM KeyValue GROUP BY Key";    ZetaSQLQueryPlanner zetaSQLQueryPlanner = new ZetaSQLQueryPlanner(config);    BeamRelNode beamRelNode = zetaSQLQueryPlanner.convertToBeamRel(sql);    PCollection<Row> stream = BeamSqlRelUtils.toPCollection(pipeline, beamRelNode);    final Schema schema = Schema.builder().addInt64Field("field1").addInt64Field("field2").build();    PAssert.that(stream).containsInAnyOrder(Row.withSchema(schema).addValues(14L, 1L).build(), Row.withSchema(schema).addValues(15L, 1L).build());    pipeline.run().waitUntilFinish(Duration.standardMinutes(PIPELINE_EXECUTION_WAITTIME_MINUTES));}
public void beam_f27212_0()
{    String sql = "SELECT Key, COUNT(*) FROM aggregate_test_table GROUP BY Key";    ZetaSQLQueryPlanner zetaSQLQueryPlanner = new ZetaSQLQueryPlanner(config);    BeamRelNode beamRelNode = zetaSQLQueryPlanner.convertToBeamRel(sql);    PCollection<Row> stream = BeamSqlRelUtils.toPCollection(pipeline, beamRelNode);    final Schema schema = Schema.builder().addInt64Field("field1").addInt64Field("field2").build();    PAssert.that(stream).containsInAnyOrder(Row.withSchema(schema).addValues(1L, 2L).build(), Row.withSchema(schema).addValues(2L, 3L).build(), Row.withSchema(schema).addValues(3L, 2L).build());    pipeline.run().waitUntilFinish(Duration.standardMinutes(PIPELINE_EXECUTION_WAITTIME_MINUTES));}
public void beam_f27213_0()
{    String sql = "SELECT Key, Key2, COUNT(*) FROM aggregate_test_table GROUP BY Key2, Key";    ZetaSQLQueryPlanner zetaSQLQueryPlanner = new ZetaSQLQueryPlanner(config);    BeamRelNode beamRelNode = zetaSQLQueryPlanner.convertToBeamRel(sql);    PCollection<Row> stream = BeamSqlRelUtils.toPCollection(pipeline, beamRelNode);    final Schema schema = Schema.builder().addInt64Field("field1").addInt64Field("field3").addInt64Field("field2").build();    PAssert.that(stream).containsInAnyOrder(Row.withSchema(schema).addValues(1L, 10L, 1L).build(), Row.withSchema(schema).addValues(1L, 11L, 1L).build(), Row.withSchema(schema).addValues(2L, 11L, 2L).build(), Row.withSchema(schema).addValues(2L, 12L, 1L).build(), Row.withSchema(schema).addValues(3L, 13L, 2L).build());    pipeline.run().waitUntilFinish(Duration.standardMinutes(PIPELINE_EXECUTION_WAITTIME_MINUTES));}
public void beam_f27214_0()
{    String sql = "SELECT Key, Key2, MAX(f_int_1), MIN(f_int_1), SUM(f_int_1), SUM(f_double_1) " + "FROM aggregate_test_table GROUP BY Key2, Key";    ZetaSQLQueryPlanner zetaSQLQueryPlanner = new ZetaSQLQueryPlanner(config);    BeamRelNode beamRelNode = zetaSQLQueryPlanner.convertToBeamRel(sql);    PCollection<Row> stream = BeamSqlRelUtils.toPCollection(pipeline, beamRelNode);    final Schema schema = Schema.builder().addInt64Field("field1").addInt64Field("field3").addInt64Field("field2").addInt64Field("field4").addInt64Field("field5").addDoubleField("field6").build();    PAssert.that(stream).containsInAnyOrder(Row.withSchema(schema).addValues(1L, 10L, 1L, 1L, 1L, 1.0).build(), Row.withSchema(schema).addValues(1L, 11L, 2L, 2L, 2L, 2.0).build(), Row.withSchema(schema).addValues(2L, 11L, 4L, 3L, 7L, 7.0).build(), Row.withSchema(schema).addValues(2L, 12L, 5L, 5L, 5L, 5.0).build(), Row.withSchema(schema).addValues(3L, 13L, 7L, 6L, 13L, 13.0).build());    pipeline.run().waitUntilFinish(Duration.standardMinutes(PIPELINE_EXECUTION_WAITTIME_MINUTES));}
public void beam_f27222_0()
{    String sql = "SELECT " + "COUNT(*) as field_count, " + "HOP_START(\"INTERVAL 1 SECOND\", \"INTERVAL 2 SECOND\") as window_start, " + "HOP_END(\"INTERVAL 1 SECOND\", \"INTERVAL 2 SECOND\") as window_end " + "FROM window_test_table " + "GROUP BY HOP(ts, \"INTERVAL 1 SECOND\", \"INTERVAL 2 SECOND\");";    ZetaSQLQueryPlanner zetaSQLQueryPlanner = new ZetaSQLQueryPlanner(config);    BeamRelNode beamRelNode = zetaSQLQueryPlanner.convertToBeamRel(sql);    PCollection<Row> stream = BeamSqlRelUtils.toPCollection(pipeline, beamRelNode);    final Schema schema = Schema.builder().addInt64Field("count_star").addDateTimeField("field1").addDateTimeField("field2").build();    PAssert.that(stream).containsInAnyOrder(Row.withSchema(schema).addValues(2L, new DateTime(2018, 7, 1, 21, 26, 7, ISOChronology.getInstanceUTC()), new DateTime(2018, 7, 1, 21, 26, 9, ISOChronology.getInstanceUTC())).build(), Row.withSchema(schema).addValues(1L, new DateTime(2018, 7, 1, 21, 26, 5, ISOChronology.getInstanceUTC()), new DateTime(2018, 7, 1, 21, 26, 7, ISOChronology.getInstanceUTC())).build(), Row.withSchema(schema).addValues(2L, new DateTime(2018, 7, 1, 21, 26, 6, ISOChronology.getInstanceUTC()), new DateTime(2018, 7, 1, 21, 26, 8, ISOChronology.getInstanceUTC())).build(), Row.withSchema(schema).addValues(2L, new DateTime(2018, 7, 1, 21, 26, 8, ISOChronology.getInstanceUTC()), new DateTime(2018, 7, 1, 21, 26, 10, ISOChronology.getInstanceUTC())).build(), Row.withSchema(schema).addValues(1L, new DateTime(2018, 7, 1, 21, 26, 9, ISOChronology.getInstanceUTC()), new DateTime(2018, 7, 1, 21, 26, 11, ISOChronology.getInstanceUTC())).build());    pipeline.run().waitUntilFinish(Duration.standardMinutes(PIPELINE_EXECUTION_WAITTIME_MINUTES));}
public void beam_f27223_0()
{    String sql = "SELECT " + "COUNT(*) as field_count, " + "SESSION_START(\"INTERVAL 3 SECOND\") as window_start, " + "SESSION_END(\"INTERVAL 3 SECOND\") as window_end " + "FROM window_test_table_two " + "GROUP BY SESSION(ts, \"INTERVAL 3 SECOND\");";    ZetaSQLQueryPlanner zetaSQLQueryPlanner = new ZetaSQLQueryPlanner(config);    BeamRelNode beamRelNode = zetaSQLQueryPlanner.convertToBeamRel(sql);    PCollection<Row> stream = BeamSqlRelUtils.toPCollection(pipeline, beamRelNode);    final Schema schema = Schema.builder().addInt64Field("count_star").addDateTimeField("field1").addDateTimeField("field2").build();    PAssert.that(stream).containsInAnyOrder(Row.withSchema(schema).addValues(2L, new DateTime(2018, 7, 1, 21, 26, 12, ISOChronology.getInstanceUTC()), new DateTime(2018, 7, 1, 21, 26, 12, ISOChronology.getInstanceUTC())).build(), Row.withSchema(schema).addValues(2L, new DateTime(2018, 7, 1, 21, 26, 6, ISOChronology.getInstanceUTC()), new DateTime(2018, 7, 1, 21, 26, 6, ISOChronology.getInstanceUTC())).build());    pipeline.run().waitUntilFinish(Duration.standardMinutes(PIPELINE_EXECUTION_WAITTIME_MINUTES));}
public void beam_f27224_0()
{    String sql = "SELECT a.Value, a.Key FROM (SELECT Key, Value FROM KeyValue WHERE Key = 14 OR Key = 15)" + " as a;";    ZetaSQLQueryPlanner zetaSQLQueryPlanner = new ZetaSQLQueryPlanner(config);    BeamRelNode beamRelNode = zetaSQLQueryPlanner.convertToBeamRel(sql);    PCollection<Row> stream = BeamSqlRelUtils.toPCollection(pipeline, beamRelNode);    final Schema schema = Schema.builder().addStringField("field2").addInt64Field("field1").build();    PAssert.that(stream).containsInAnyOrder(Row.withSchema(schema).addValues("KeyValue234", 14L).build(), Row.withSchema(schema).addValues("KeyValue235", 15L).build());    pipeline.run().waitUntilFinish(Duration.standardMinutes(PIPELINE_EXECUTION_WAITTIME_MINUTES));}
public void beam_f27232_0()
{    String sql = "SELECT DISTINCT Key2 FROM aggregate_test_table";    ZetaSQLQueryPlanner zetaSQLQueryPlanner = new ZetaSQLQueryPlanner(config);    BeamRelNode beamRelNode = zetaSQLQueryPlanner.convertToBeamRel(sql);    PCollection<Row> stream = BeamSqlRelUtils.toPCollection(pipeline, beamRelNode);    Schema schema = Schema.builder().addInt64Field("Key2").build();    PAssert.that(stream).containsInAnyOrder(Row.withSchema(schema).addValues(10L).build(), Row.withSchema(schema).addValues(11L).build(), Row.withSchema(schema).addValues(12L).build(), Row.withSchema(schema).addValues(13L).build());    pipeline.run().waitUntilFinish(Duration.standardMinutes(PIPELINE_EXECUTION_WAITTIME_MINUTES));}
public void beam_f27233_0()
{    String sql = "SELECT DISTINCT str_val FROM all_null_table";    ZetaSQLQueryPlanner zetaSQLQueryPlanner = new ZetaSQLQueryPlanner(config);    BeamRelNode beamRelNode = zetaSQLQueryPlanner.convertToBeamRel(sql);    PCollection<Row> stream = BeamSqlRelUtils.toPCollection(pipeline, beamRelNode);    Schema schema = Schema.builder().addNullableField("str_val", FieldType.DOUBLE).build();    PAssert.that(stream).containsInAnyOrder(Row.withSchema(schema).addValues((Object) null).build());    pipeline.run().waitUntilFinish(Duration.standardMinutes(PIPELINE_EXECUTION_WAITTIME_MINUTES));}
public void beam_f27234_0()
{    String sql = "SELECT ANY_VALUE(double_val) FROM all_null_table";    ZetaSQLQueryPlanner zetaSQLQueryPlanner = new ZetaSQLQueryPlanner(config);    BeamRelNode beamRelNode = zetaSQLQueryPlanner.convertToBeamRel(sql);    PCollection<Row> stream = BeamSqlRelUtils.toPCollection(pipeline, beamRelNode);    Schema schema = Schema.builder().addNullableField("double_val", FieldType.DOUBLE).build();    PAssert.that(stream).containsInAnyOrder(Row.withSchema(schema).addValues((Object) null).build());    pipeline.run().waitUntilFinish(Duration.standardMinutes(PIPELINE_EXECUTION_WAITTIME_MINUTES));}
public void beam_f27242_0()
{    String sql = "WITH T1 AS (SELECT * FROM KeyValue) SELECT " + "COUNT(*) as field_count, " + "TUMBLE_START(\"INTERVAL 1 SECOND\") as window_start, " + "TUMBLE_END(\"INTERVAL 1 SECOND\") as window_end " + "FROM T1 " + "GROUP BY TUMBLE(ts, \"INTERVAL 1 SECOND\");";    ZetaSQLQueryPlanner zetaSQLQueryPlanner = new ZetaSQLQueryPlanner(config);    BeamRelNode beamRelNode = zetaSQLQueryPlanner.convertToBeamRel(sql);    PCollection<Row> stream = BeamSqlRelUtils.toPCollection(pipeline, beamRelNode);    final Schema schema = Schema.builder().addInt64Field("count_start").addDateTimeField("field1").addDateTimeField("field2").build();    PAssert.that(stream).containsInAnyOrder(Row.withSchema(schema).addValues(1L, new DateTime(2018, 7, 1, 21, 26, 7, ISOChronology.getInstanceUTC()), new DateTime(2018, 7, 1, 21, 26, 8, ISOChronology.getInstanceUTC())).build(), Row.withSchema(schema).addValues(1L, new DateTime(2018, 7, 1, 21, 26, 6, ISOChronology.getInstanceUTC()), new DateTime(2018, 7, 1, 21, 26, 7, ISOChronology.getInstanceUTC())).build());    pipeline.run().waitUntilFinish(Duration.standardMinutes(PIPELINE_EXECUTION_WAITTIME_MINUTES));}
public void beam_f27243_0()
{    String sql = "SELECT * FROM UNNEST(ARRAY<STRING>['foo', 'bar']);";    ZetaSQLQueryPlanner zetaSQLQueryPlanner = new ZetaSQLQueryPlanner(config);    BeamRelNode beamRelNode = zetaSQLQueryPlanner.convertToBeamRel(sql);    PCollection<Row> stream = BeamSqlRelUtils.toPCollection(pipeline, beamRelNode);    Schema schema = Schema.builder().addStringField("str_field").build();    PAssert.that(stream).containsInAnyOrder(Row.withSchema(schema).addValues("foo").build(), Row.withSchema(schema).addValues("bar").build());    pipeline.run().waitUntilFinish(Duration.standardMinutes(PIPELINE_EXECUTION_WAITTIME_MINUTES));}
public void beam_f27244_0()
{    String sql = "SELECT *, T1 FROM UNNEST(ARRAY<STRING>['foo', 'bar']) AS T1";    ZetaSQLQueryPlanner zetaSQLQueryPlanner = new ZetaSQLQueryPlanner(config);    BeamRelNode beamRelNode = zetaSQLQueryPlanner.convertToBeamRel(sql);    PCollection<Row> stream = BeamSqlRelUtils.toPCollection(pipeline, beamRelNode);    Schema schema = Schema.builder().addStringField("str_field").addStringField("str2_field").build();    PAssert.that(stream).containsInAnyOrder(Row.withSchema(schema).addValues("foo", "foo").build(), Row.withSchema(schema).addValues("bar", "bar").build());    pipeline.run().waitUntilFinish(Duration.standardMinutes(PIPELINE_EXECUTION_WAITTIME_MINUTES));}
public void beam_f27252_0()
{    String sql = "SELECT f_int, \n" + "CASE WHEN CHAR_LENGTH(TRIM(f_string)) = 8 \n" + "    THEN CAST (CONCAT(\n" + "       SUBSTR(TRIM(f_string), 0, 4) \n" + "        , '-' \n" + "        , SUBSTR(TRIM(f_string), 4, 2) \n" + "        , '-' \n" + "        , SUBSTR(TRIM(f_string), 6, 2)) AS DATE)\n" + "    ELSE NULL\n" + "END \n" + "FROM table_for_case_when";    ZetaSQLQueryPlanner zetaSQLQueryPlanner = new ZetaSQLQueryPlanner(config);    BeamRelNode beamRelNode = zetaSQLQueryPlanner.convertToBeamRel(sql);    PCollection<Row> stream = BeamSqlRelUtils.toPCollection(pipeline, beamRelNode);    Schema resultType = Schema.builder().addInt32Field("f_int").addNullableField("f_date", DATETIME).build();    PAssert.that(stream).containsInAnyOrder(Row.withSchema(resultType).addValues(1, parseDate("2018-10-18")).build());    pipeline.run().waitUntilFinish(Duration.standardMinutes(PIPELINE_EXECUTION_WAITTIME_MINUTES));}
public void beam_f27253_0()
{    String sql = "SELECT Key FROM aggregate_test_table " + "INTERSECT ALL " + "SELECT Key FROM aggregate_test_table_two";    ZetaSQLQueryPlanner zetaSQLQueryPlanner = new ZetaSQLQueryPlanner(config);    BeamRelNode beamRelNode = zetaSQLQueryPlanner.convertToBeamRel(sql);    PCollection<Row> stream = BeamSqlRelUtils.toPCollection(pipeline, beamRelNode);    Schema resultType = Schema.builder().addInt64Field("field").build();    PAssert.that(stream).containsInAnyOrder(Row.withSchema(resultType).addValues(1L).build(), Row.withSchema(resultType).addValues(2L).build(), Row.withSchema(resultType).addValues(2L).build(), Row.withSchema(resultType).addValues(2L).build(), Row.withSchema(resultType).addValues(3L).build());    pipeline.run().waitUntilFinish(Duration.standardMinutes(PIPELINE_EXECUTION_WAITTIME_MINUTES));}
public void beam_f27254_0()
{    String sql = "SELECT Key FROM aggregate_test_table " + "INTERSECT DISTINCT " + "SELECT Key FROM aggregate_test_table_two";    ZetaSQLQueryPlanner zetaSQLQueryPlanner = new ZetaSQLQueryPlanner(config);    BeamRelNode beamRelNode = zetaSQLQueryPlanner.convertToBeamRel(sql);    PCollection<Row> stream = BeamSqlRelUtils.toPCollection(pipeline, beamRelNode);    Schema resultType = Schema.builder().addInt64Field("field").build();    PAssert.that(stream).containsInAnyOrder(Row.withSchema(resultType).addValues(1L).build(), Row.withSchema(resultType).addValues(2L).build(), Row.withSchema(resultType).addValues(3L).build());    pipeline.run().waitUntilFinish(Duration.standardMinutes(PIPELINE_EXECUTION_WAITTIME_MINUTES));}
public void beam_f27262_0()
{    String sql = "SELECT STARTS_WITH(@p0, @p1)";    ImmutableMap<String, Value> params = ImmutableMap.<String, Value>builder().put("p0", Value.createSimpleNullValue(TypeKind.TYPE_STRING)).put("p1", Value.createSimpleNullValue(TypeKind.TYPE_STRING)).build();    ZetaSQLQueryPlanner zetaSQLQueryPlanner = new ZetaSQLQueryPlanner(config);    BeamRelNode beamRelNode = zetaSQLQueryPlanner.convertToBeamRel(sql, params);    PCollection<Row> stream = BeamSqlRelUtils.toPCollection(pipeline, beamRelNode);    final Schema schema = Schema.builder().addNullableField("field1", FieldType.BOOLEAN).build();    PAssert.that(stream).containsInAnyOrder(Row.withSchema(schema).addValues((Boolean) null).build());    pipeline.run().waitUntilFinish(Duration.standardMinutes(PIPELINE_EXECUTION_WAITTIME_MINUTES));}
public void beam_f27263_0()
{    String sql = "SELECT DATETIME '2018-01-01 05:30:00.334'";    ZetaSQLQueryPlanner zetaSQLQueryPlanner = new ZetaSQLQueryPlanner(config);    thrown.expect(RuntimeException.class);    thrown.expectMessage("Unsupported ResolvedLiteral type: DATETIME");    zetaSQLQueryPlanner.convertToBeamRel(sql);}
public void beam_f27264_0()
{    String sql = "SELECT TIMESTAMP '2016-12-25 05:30:00+00'";    ZetaSQLQueryPlanner zetaSQLQueryPlanner = new ZetaSQLQueryPlanner(config);    BeamRelNode beamRelNode = zetaSQLQueryPlanner.convertToBeamRel(sql);    PCollection<Row> stream = BeamSqlRelUtils.toPCollection(pipeline, beamRelNode);    final Schema schema = Schema.builder().addDateTimeField("field1").build();    PAssert.that(stream).containsInAnyOrder(Row.withSchema(schema).addValues(parseTimestampWithUTCTimeZone("2016-12-25 05:30:00")).build());    pipeline.run().waitUntilFinish(Duration.standardMinutes(PIPELINE_EXECUTION_WAITTIME_MINUTES));}
public void beam_f27272_0()
{    String sql = "SELECT concat('abc', 'def', '  ', 'xyz', 'kkk', 'ttt')";    ZetaSQLQueryPlanner zetaSQLQueryPlanner = new ZetaSQLQueryPlanner(config);    BeamRelNode beamRelNode = zetaSQLQueryPlanner.convertToBeamRel(sql);    PCollection<Row> stream = BeamSqlRelUtils.toPCollection(pipeline, beamRelNode);    final Schema schema = Schema.builder().addStringField("field1").build();    PAssert.that(stream).containsInAnyOrder(Row.withSchema(schema).addValues("abcdef  xyzkkkttt").build());    pipeline.run().waitUntilFinish(Duration.standardMinutes(PIPELINE_EXECUTION_WAITTIME_MINUTES));}
public void beam_f27273_0()
{    String sql = "SELECT CONCAT(@p0, @p1) AS ColA";    ImmutableMap<String, Value> params = ImmutableMap.of("p0", Value.createStringValue(""), "p1", Value.createSimpleNullValue(TypeKind.TYPE_STRING));    ZetaSQLQueryPlanner zetaSQLQueryPlanner = new ZetaSQLQueryPlanner(config);    BeamRelNode beamRelNode = zetaSQLQueryPlanner.convertToBeamRel(sql, params);    PCollection<Row> stream = BeamSqlRelUtils.toPCollection(pipeline, beamRelNode);    final Schema schema = Schema.builder().addNullableField("field1", FieldType.STRING).build();    PAssert.that(stream).containsInAnyOrder(Row.withSchema(schema).addValues((String) null).build());    pipeline.run().waitUntilFinish(Duration.standardMinutes(PIPELINE_EXECUTION_WAITTIME_MINUTES));}
public void beam_f27274_0()
{    String sql = "SELECT CONCAT(@p0, @p1) AS ColA";    ImmutableMap<String, Value> params = ImmutableMap.of("p0", Value.createSimpleNullValue(TypeKind.TYPE_STRING), "p1", Value.createSimpleNullValue(TypeKind.TYPE_STRING));    ZetaSQLQueryPlanner zetaSQLQueryPlanner = new ZetaSQLQueryPlanner(config);    BeamRelNode beamRelNode = zetaSQLQueryPlanner.convertToBeamRel(sql, params);    PCollection<Row> stream = BeamSqlRelUtils.toPCollection(pipeline, beamRelNode);    final Schema schema = Schema.builder().addNullableField("field1", FieldType.STRING).build();    PAssert.that(stream).containsInAnyOrder(Row.withSchema(schema).addValues((String) null).build());    pipeline.run().waitUntilFinish(Duration.standardMinutes(PIPELINE_EXECUTION_WAITTIME_MINUTES));}
public void beam_f27282_0()
{    String sql = "SELECT trim(@p0, @p1)";    ImmutableMap<String, Value> params = ImmutableMap.of("p0", Value.createSimpleNullValue(TypeKind.TYPE_STRING), "p1", Value.createSimpleNullValue(TypeKind.TYPE_STRING));    ZetaSQLQueryPlanner zetaSQLQueryPlanner = new ZetaSQLQueryPlanner(config);    BeamRelNode beamRelNode = zetaSQLQueryPlanner.convertToBeamRel(sql, params);    PCollection<Row> stream = BeamSqlRelUtils.toPCollection(pipeline, beamRelNode);    final Schema schema = Schema.builder().addNullableField("field1", FieldType.STRING).build();    PAssert.that(stream).containsInAnyOrder(Row.withSchema(schema).addValues((String) null).build());    pipeline.run().waitUntilFinish(Duration.standardMinutes(PIPELINE_EXECUTION_WAITTIME_MINUTES));}
public void beam_f27283_0()
{    String sql = "SELECT ltrim(@p0)";    ImmutableMap<String, Value> params = ImmutableMap.of("p0", Value.createStringValue("   a b c   "));    ZetaSQLQueryPlanner zetaSQLQueryPlanner = new ZetaSQLQueryPlanner(config);    BeamRelNode beamRelNode = zetaSQLQueryPlanner.convertToBeamRel(sql, params);    PCollection<Row> stream = BeamSqlRelUtils.toPCollection(pipeline, beamRelNode);    final Schema schema = Schema.builder().addStringField("field1").build();    PAssert.that(stream).containsInAnyOrder(Row.withSchema(schema).addValues("a b c   ").build());    pipeline.run().waitUntilFinish(Duration.standardMinutes(PIPELINE_EXECUTION_WAITTIME_MINUTES));}
public void beam_f27284_0()
{    String sql = "SELECT ltrim(@p0, @p1)";    ImmutableMap<String, Value> params = ImmutableMap.of("p0", Value.createStringValue("abxyzab"), "p1", Value.createStringValue("ab"));    ZetaSQLQueryPlanner zetaSQLQueryPlanner = new ZetaSQLQueryPlanner(config);    BeamRelNode beamRelNode = zetaSQLQueryPlanner.convertToBeamRel(sql, params);    PCollection<Row> stream = BeamSqlRelUtils.toPCollection(pipeline, beamRelNode);    final Schema schema = Schema.builder().addStringField("field1").build();    PAssert.that(stream).containsInAnyOrder(Row.withSchema(schema).addValues("xyzab").build());    pipeline.run().waitUntilFinish(Duration.standardMinutes(PIPELINE_EXECUTION_WAITTIME_MINUTES));}
public void beam_f27292_0()
{    String sql = "SELECT CAST('2019-01-15 13:21:03' AS TIMESTAMP)";    ZetaSQLQueryPlanner zetaSQLQueryPlanner = new ZetaSQLQueryPlanner(config);    BeamRelNode beamRelNode = zetaSQLQueryPlanner.convertToBeamRel(sql);    PCollection<Row> stream = BeamSqlRelUtils.toPCollection(pipeline, beamRelNode);    final Schema schema = Schema.builder().addDateTimeField("field_1").build();    PAssert.that(stream).containsInAnyOrder(Row.withSchema(schema).addValues(parseTimestampWithUTCTimeZone("2019-01-15 13:21:03")).build());    pipeline.run().waitUntilFinish(Duration.standardMinutes(PIPELINE_EXECUTION_WAITTIME_MINUTES));}
public void beam_f27293_0()
{    String sql = "SELECT CAST(@p0 AS STRING)";    ImmutableMap<String, Value> params = ImmutableMap.of("p0", Value.createStringValue(""));    ZetaSQLQueryPlanner zetaSQLQueryPlanner = new ZetaSQLQueryPlanner(config);    BeamRelNode beamRelNode = zetaSQLQueryPlanner.convertToBeamRel(sql, params);    PCollection<Row> stream = BeamSqlRelUtils.toPCollection(pipeline, beamRelNode);    final Schema schema = Schema.builder().addStringField("field1").build();    PAssert.that(stream).containsInAnyOrder(Row.withSchema(schema).addValues("").build());    pipeline.run().waitUntilFinish(Duration.standardMinutes(PIPELINE_EXECUTION_WAITTIME_MINUTES));}
public void beam_f27294_0()
{    String sql = "SELECT CAST(@p0 AS INT64)";    ImmutableMap<String, Value> params = ImmutableMap.of("p0", Value.createStringValue("123"));    ZetaSQLQueryPlanner zetaSQLQueryPlanner = new ZetaSQLQueryPlanner(config);    BeamRelNode beamRelNode = zetaSQLQueryPlanner.convertToBeamRel(sql, params);    PCollection<Row> stream = BeamSqlRelUtils.toPCollection(pipeline, beamRelNode);    final Schema schema = Schema.builder().addInt64Field("field1").build();    PAssert.that(stream).containsInAnyOrder(Row.withSchema(schema).addValues(123L).build());    pipeline.run().waitUntilFinish(Duration.standardMinutes(PIPELINE_EXECUTION_WAITTIME_MINUTES));}
public void beam_f27302_0()
{    String sql = "SELECT substr(@p0, @p1, @p2)";    ImmutableMap<String, Value> params = ImmutableMap.of("p0", Value.createStringValue("abc"), "p1", Value.createInt64Value(-2L), "p2", Value.createInt64Value(1L));    ZetaSQLQueryPlanner zetaSQLQueryPlanner = new ZetaSQLQueryPlanner(config);    BeamRelNode beamRelNode = zetaSQLQueryPlanner.convertToBeamRel(sql, params);    PCollection<Row> stream = BeamSqlRelUtils.toPCollection(pipeline, beamRelNode);    final Schema schema = Schema.builder().addStringField("field1").build();    PAssert.that(stream).containsInAnyOrder(Row.withSchema(schema).addValues("b").build());    pipeline.run().waitUntilFinish(Duration.standardMinutes(PIPELINE_EXECUTION_WAITTIME_MINUTES));}
public void beam_f27303_0()
{    String sql = "SELECT substr(@p0, @p1, @p2)";    ImmutableMap<String, Value> params = ImmutableMap.of("p0", Value.createStringValue("abc"), "p1", Value.createInt64Value(Integer.MAX_VALUE + 1L), "p2", Value.createInt64Value(Integer.MIN_VALUE - 1L));    ZetaSQLQueryPlanner zetaSQLQueryPlanner = new ZetaSQLQueryPlanner(config);    BeamRelNode beamRelNode = zetaSQLQueryPlanner.convertToBeamRel(sql, params);    BeamSqlRelUtils.toPCollection(pipeline, beamRelNode);    thrown.expect(RuntimeException.class);    pipeline.run().waitUntilFinish(Duration.standardMinutes(PIPELINE_EXECUTION_WAITTIME_MINUTES));}
public void beam_f27304_0()
{    String sql = "SELECT ALL Key, Value FROM KeyValue;";    ZetaSQLQueryPlanner zetaSQLQueryPlanner = new ZetaSQLQueryPlanner(config);    BeamRelNode beamRelNode = zetaSQLQueryPlanner.convertToBeamRel(sql);    PCollection<Row> stream = BeamSqlRelUtils.toPCollection(pipeline, beamRelNode);    final Schema schema = Schema.builder().addInt64Field("field1").addStringField("field2").build();    PAssert.that(stream).containsInAnyOrder(Row.withSchema(schema).addValues(14L, "KeyValue234").build(), Row.withSchema(schema).addValues(15L, "KeyValue235").build());    pipeline.run().waitUntilFinish(Duration.standardMinutes(PIPELINE_EXECUTION_WAITTIME_MINUTES));}
public void beam_f27312_0()
{    String sql = "SELECT REVERSE('abc');";    ZetaSQLQueryPlanner zetaSQLQueryPlanner = new ZetaSQLQueryPlanner(config);    BeamRelNode beamRelNode = zetaSQLQueryPlanner.convertToBeamRel(sql);    PCollection<Row> stream = BeamSqlRelUtils.toPCollection(pipeline, beamRelNode);    final Schema schema = Schema.builder().addStringField("field2").build();    PAssert.that(stream).containsInAnyOrder(Row.withSchema(schema).addValues("cba").build());    pipeline.run().waitUntilFinish(Duration.standardMinutes(PIPELINE_EXECUTION_WAITTIME_MINUTES));}
public void beam_f27313_0()
{    String sql = "SELECT CHAR_LENGTH('abc');";    ZetaSQLQueryPlanner zetaSQLQueryPlanner = new ZetaSQLQueryPlanner(config);    BeamRelNode beamRelNode = zetaSQLQueryPlanner.convertToBeamRel(sql);    PCollection<Row> stream = BeamSqlRelUtils.toPCollection(pipeline, beamRelNode);    final Schema schema = Schema.builder().addInt64Field("field").build();    PAssert.that(stream).containsInAnyOrder(Row.withSchema(schema).addValues(3L).build());    pipeline.run().waitUntilFinish(Duration.standardMinutes(PIPELINE_EXECUTION_WAITTIME_MINUTES));}
public void beam_f27314_0()
{    String sql = "SELECT CHAR_LENGTH(@p0);";    ImmutableMap<String, Value> params = ImmutableMap.of("p0", Value.createSimpleNullValue(TypeKind.TYPE_STRING));    ZetaSQLQueryPlanner zetaSQLQueryPlanner = new ZetaSQLQueryPlanner(config);    BeamRelNode beamRelNode = zetaSQLQueryPlanner.convertToBeamRel(sql, params);    PCollection<Row> stream = BeamSqlRelUtils.toPCollection(pipeline, beamRelNode);    final Schema schema = Schema.builder().addNullableField("field", FieldType.INT64).build();    PAssert.that(stream).containsInAnyOrder(Row.withSchema(schema).addValues((Object) null).build());    pipeline.run().waitUntilFinish(Duration.standardMinutes(PIPELINE_EXECUTION_WAITTIME_MINUTES));}
private void beam_f27322_0(Context... extraContext)
{    JdbcConnection jdbcConnection = JdbcDriver.connect(tableProvider, PipelineOptionsFactory.create());    SchemaPlus defaultSchemaPlus = jdbcConnection.getCurrentSchemaPlus();    final ImmutableList<RelTraitDef> traitDefs = ImmutableList.of(ConventionTraitDef.INSTANCE);    Object[] contexts = ImmutableList.<Context>builder().add(Contexts.of(jdbcConnection.config())).add(extraContext).build().toArray();    this.config = Frameworks.newConfigBuilder().defaultSchema(defaultSchemaPlus).traitDefs(traitDefs).context(Contexts.of(contexts)).ruleSets(BeamRuleSets.getRuleSets()).costFactory(null).typeSystem(jdbcConnection.getTypeFactory().getTypeSystem()).build();}
private void beam_f27323_0()
{    Map<String, BeamSqlTable> testBoundedTableMap = new HashMap<>();    testBoundedTableMap.put("KeyValue", BASIC_TABLE_ONE);    testBoundedTableMap.put("BigTable", BASIC_TABLE_TWO);    testBoundedTableMap.put("Spanner", BASIC_TABLE_THREE);    testBoundedTableMap.put("aggregate_test_table", AGGREGATE_TABLE_ONE);    testBoundedTableMap.put("window_test_table", TIMESTAMP_TABLE_ONE);    testBoundedTableMap.put("window_test_table_two", TIMESTAMP_TABLE_TWO);    testBoundedTableMap.put("time_test_table", TIME_TABLE);    testBoundedTableMap.put("all_null_table", TABLE_ALL_NULL);    testBoundedTableMap.put("table_with_struct", TABLE_WITH_STRUCT);    testBoundedTableMap.put("table_with_struct_two", TABLE_WITH_STRUCT_TWO);    testBoundedTableMap.put("table_with_array", TABLE_WITH_ARRAY);    testBoundedTableMap.put("table_with_array_for_unnest", TABLE_WITH_ARRAY_FOR_UNNEST);    testBoundedTableMap.put("table_for_case_when", TABLE_FOR_CASE_WHEN);    testBoundedTableMap.put("aggregate_test_table_two", AGGREGATE_TABLE_TWO);    testBoundedTableMap.put("table_empty", TABLE_EMPTY);    testBoundedTableMap.put("table_all_types", TABLE_ALL_TYPES);    testBoundedTableMap.put("table_all_types_2", TABLE_ALL_TYPES_2);    testBoundedTableMap.put("table_with_map", TABLE_WITH_MAP);    testBoundedTableMap.put("table_with_struct_ts_string", TABLE_WITH_STRUCT_TIMESTAMP_STRING);    tableProvider = new ReadOnlyTableProvider("test_table_provider", testBoundedTableMap);}
public static Builder<Integer> beam_f27324_0()
{    return new Builder<>(HllCountInitFn.forInteger());}
public static Combine.PerKey<K, byte[], byte[]> beam_f27332_0()
{    return Combine.perKey(HllCountMergePartialFn.create());}
public static PTransform<PCollection<byte[]>, PCollection<Long>> beam_f27333_0()
{    return new Globally();}
public static PTransform<PCollection<KV<K, byte[]>>, PCollection<KV<K, Long>>> beam_f27334_0()
{    return new PerKey<K>();}
public Coder<HyperLogLogPlusPlus<HllT>> beam_f27342_0(CoderRegistry registry, Coder<InputT> inputCoder)
{    return HyperLogLogPlusPlusCoder.of();}
public HyperLogLogPlusPlus<HllT> beam_f27343_0(Iterable<HyperLogLogPlusPlus<HllT>> accumulators)
{    HyperLogLogPlusPlus<HllT> merged = createAccumulator();    for (HyperLogLogPlusPlus<HllT> accumulator : accumulators) {        merged.merge(accumulator);    }    return merged;}
public byte[] beam_f27344_0(HyperLogLogPlusPlus<HllT> accumulator)
{    return accumulator.serializeToByteArray();}
public HyperLogLogPlusPlus<Long> beam_f27352_0(HyperLogLogPlusPlus<Long> accumulator, Long input)
{    accumulator.add(input.longValue());    return accumulator;}
public HyperLogLogPlusPlus<String> beam_f27353_0()
{    return new HyperLogLogPlusPlus.Builder().normalPrecision(getPrecision()).buildForStrings();}
public HyperLogLogPlusPlus<String> beam_f27354_0(HyperLogLogPlusPlus<String> accumulator, String input)
{    accumulator.add(input);    return accumulator;}
public byte[] beam_f27362_0(@Nullable HyperLogLogPlusPlus<HllT> accumulator)
{    if (accumulator == null) {        return new byte[0];    } else {        return accumulator.serializeToByteArray();    }}
 static HyperLogLogPlusPlusCoder<T> beam_f27363_0()
{    return (HyperLogLogPlusPlusCoder<T>) INSTANCE;}
public void beam_f27364_0(HyperLogLogPlusPlus<T> value, OutputStream outStream) throws IOException
{    checkNotNull(value, new CoderException("Cannot encode a null HyperLogLogPlusPlus value."));    BYTE_ARRAY_CODER.encode(value.serializeToByteArray(), outStream);}
public void beam_f27372_0()
{    PCollection<byte[]> result = p.apply(Create.of(LONGS)).apply(HllCount.Init.forLongs().globally());    PAssert.thatSingleton(result).isEqualTo(LONGS_SKETCH);    p.run();}
public void beam_f27373_0()
{    PCollection<byte[]> result = p.apply(Create.empty(TypeDescriptor.of(Long.class))).apply(HllCount.Init.forLongs().globally());    PAssert.thatSingleton(result).isEqualTo(EMPTY_SKETCH);    p.run();}
public void beam_f27374_0()
{    PCollection<byte[]> result = p.apply(Create.of(STRINGS)).apply(HllCount.Init.forStrings().globally());    PAssert.thatSingleton(result).isEqualTo(STRINGS_SKETCH);    p.run();}
public void beam_f27382_0()
{    List<KV<String, String>> input = new ArrayList<>();    STRINGS.forEach(s -> input.add(KV.of("k", s)));    thrown.expect(IllegalArgumentException.class);    p.apply(Create.of(input)).apply(HllCount.Init.forStrings().withPrecision(0).perKey());}
public void beam_f27383_0()
{    List<KV<String, byte[]>> input = new ArrayList<>();    BYTES.forEach(bs -> input.add(KV.of("k", bs)));    PCollection<KV<String, byte[]>> result = p.apply(Create.of(input)).apply(HllCount.Init.forBytes().perKey());    PAssert.that(result).containsInAnyOrder(Collections.singletonList(KV.of("k", BYTES_SKETCH)));    p.run();}
public void beam_f27384_0()
{    PCollection<byte[]> result = p.apply(Create.of(INTS1_SKETCH, INTS2_SKETCH)).apply(HllCount.MergePartial.globally());    PAssert.thatSingleton(result).isEqualTo(INTS1_INTS2_SKETCH);    p.run();}
public void beam_f27392_0()
{    p.apply(Create.of(INTS1_SKETCH, STRINGS_SKETCH)).apply(HllCount.MergePartial.globally());    thrown.expect(PipelineExecutionException.class);    p.run();}
public void beam_f27393_0()
{    PCollection<KV<String, byte[]>> result = p.apply(Create.of(KV.of("k1", INTS1_SKETCH), KV.of("k2", STRINGS_SKETCH), KV.of("k1", INTS2_SKETCH))).apply(HllCount.MergePartial.perKey());    PAssert.that(result).containsInAnyOrder(Arrays.asList(KV.of("k1", INTS1_INTS2_SKETCH), KV.of("k2", STRINGS_SKETCH)));    p.run();}
public void beam_f27394_0()
{    PCollection<KV<String, byte[]>> result = p.apply(Create.of(KV.of("k1", INTS1_SKETCH), KV.of("k2", EMPTY_SKETCH), KV.of("k1", EMPTY_SKETCH))).apply(HllCount.MergePartial.perKey());    PAssert.that(result).containsInAnyOrder(Arrays.asList(KV.of("k1", INTS1_SKETCH), KV.of("k2", EMPTY_SKETCH)));    p.run();}
public void beam_f27402_0()
{    PCollection<KV<String, Long>> result = p.apply(Create.of(KV.of("k", LONGS_SKETCH_OF_EMPTY_SET))).apply(HllCount.Extract.perKey());    PAssert.thatSingleton(result).isEqualTo(KV.of("k", 0L));    p.run();}
public static ManagedChannelFactory beam_f27403_0()
{    return new Default();}
public static ManagedChannelFactory beam_f27404_0()
{    org.apache.beam.vendor.grpc.v1p21p0.io.netty.channel.epoll.Epoll.ensureAvailability();    return new Epoll();}
public static BeamFnDataBufferingOutboundObserver<T> beam_f27412_0(LogicalEndpoint endpoint, Coder<T> coder, StreamObserver<BeamFnApi.Elements> outboundObserver)
{    return forLocationWithBufferLimit(DEFAULT_BUFFER_LIMIT_BYTES, endpoint, coder, outboundObserver);}
public static BeamFnDataBufferingOutboundObserver<T> beam_f27413_0(int bufferLimit, LogicalEndpoint endpoint, Coder<T> coder, StreamObserver<BeamFnApi.Elements> outboundObserver)
{    return new BeamFnDataBufferingOutboundObserver<>(bufferLimit, endpoint, coder, outboundObserver);}
public void beam_f27414_1() throws Exception
{    if (closed) {        throw new IllegalStateException("Already closed.");    }    closed = true;    BeamFnApi.Elements.Builder elements = convertBufferForTransmission();        elements.addDataBuilder().setInstructionId(outputLocation.getInstructionId()).setTransformId(outputLocation.getTransformId());        outboundObserver.onNext(elements.build());}
public void beam_f27422_0(LogicalEndpoint inputLocation, Consumer<BeamFnApi.Elements.Data> dataBytesReceiver)
{    receiverFuture(inputLocation).complete(dataBytesReceiver);}
 boolean beam_f27423_0(LogicalEndpoint outputLocation)
{    return consumers.containsKey(outputLocation);}
public void beam_f27424_0()
{    for (CompletableFuture<Consumer<BeamFnApi.Elements.Data>> receiver : ImmutableList.copyOf(consumers.values())) {                        receiver.cancel(true);    }        outboundObserver.onError(Status.CANCELLED.withDescription("Multiplexer hanging up").asException());    inboundObserver.onCompleted();}
public void beam_f27432_0()
{    readFuture.cancel();}
public void beam_f27433_0()
{    readFuture.complete();}
public void beam_f27434_0(Throwable t)
{    readFuture.fail(t);}
public static LogicalEndpoint beam_f27442_0(String instructionId, String transformId)
{    return new AutoValue_LogicalEndpoint(instructionId, transformId);}
public static RemoteGrpcPortRead beam_f27443_0(RemoteGrpcPort port, String outputPCollectionId)
{    return new AutoValue_RemoteGrpcPortRead(port, outputPCollectionId);}
public static RemoteGrpcPortRead beam_f27444_0(PTransform pTransform) throws InvalidProtocolBufferException
{    checkArgument(URN.equals(pTransform.getSpec().getUrn()), "Expected URN for %s, got %s", RemoteGrpcPortRead.class.getSimpleName(), pTransform.getSpec().getUrn());    checkArgument(pTransform.getOutputsCount() == 1, "Expected exactly one output, got %s", pTransform.getOutputsCount());    RemoteGrpcPort port = RemoteGrpcPort.parseFrom(pTransform.getSpec().getPayload());    String outputPcollection = Iterables.getOnlyElement(pTransform.getOutputsMap().values());    return readFromPort(port, outputPcollection);}
public static void beam_f27452_0(PipelineOptions options)
{    for (JvmInitializer initializer : ReflectHelpers.loadServicesOrdered(JvmInitializer.class)) {        initializer.beforeProcessing(options);    }}
public synchronized boolean beam_f27453_0(PositionT position)
{    if (delegate.tryClaim(position)) {        claimObserver.onClaimed(position);        return true;    } else {        claimObserver.onClaimFailed(position);        return false;    }}
public synchronized RestrictionT beam_f27454_0()
{    return delegate.currentRestriction();}
public void beam_f27462_0(Throwable t)
{    synchronized (outboundObserver) {                if (!queueDrainer.isDone()) {                        try {                                while (!queueDrainer.isDone() && !queue.offerFirst((T) POISON_PILL, 60, TimeUnit.SECONDS)) {                }            } catch (InterruptedException e) {                Thread.currentThread().interrupt();                throw new RuntimeException(e);            }            waitTillFinish();        }        outboundObserver.onError(t);    }}
public void beam_f27463_0()
{    synchronized (outboundObserver) {                if (!queueDrainer.isDone()) {                        try {                                while (!queueDrainer.isDone() && !queue.offerLast((T) POISON_PILL, 60, TimeUnit.SECONDS)) {                }            } catch (InterruptedException e) {                Thread.currentThread().interrupt();                throw new RuntimeException(e);            }            waitTillFinish();        }        outboundObserver.onCompleted();    }}
public int beam_f27464_0()
{    return bufferSize;}
public void beam_f27472_0() throws IOException
{    if (output.size() > 0) {        consumer.read(output.toByteString());    }    output.close();}
private void beam_f27473_0() throws IOException
{    consumer.read(output.toByteString());    output.reset();            previousPosition = -1;}
public int beam_f27474_0() throws IOException
{    return -1;}
public boolean beam_f27482_0()
{    if (currentElement == null) {        try {            currentElement = queue.take();        } catch (InterruptedException e) {            Thread.currentThread().interrupt();            throw new IllegalStateException(e);        }    }    return currentElement != POISION_PILL;}
public T beam_f27483_0()
{    if (!hasNext()) {        throw new NoSuchElementException();    }    T rval = currentElement;    currentElement = null;    return rval;}
public void beam_f27484_0()
{    throw new UnsupportedOperationException();}
public void beam_f27492_0(ClientCallStreamObserver<RespT> stream)
{    stream.setOnReadyHandler(onReadyHandler);}
public static OutboundObserverFactory beam_f27493_0(ExecutorService executorService)
{    return new Buffered(executorService, Buffered.DEFAULT_BUFFER_SIZE);}
public static OutboundObserverFactory beam_f27494_0(ExecutorService executorService, int bufferSize)
{    return new Buffered(executorService, bufferSize);}
public static StreamObserver<V> beam_f27502_0(StreamObserver<V> underlying)
{    return new SynchronizedStreamObserver<>(underlying);}
public void beam_f27503_0(V value)
{    synchronized (underlying) {        underlying.onNext(value);    }}
public synchronized void beam_f27504_0(Throwable t)
{    synchronized (underlying) {        underlying.onError(t);    }}
protected ExecutorService beam_f27512_0()
{    return delegate;}
public static Builder<T> beam_f27513_0(Consumer<T> onNext)
{    return new Builder<>(new ForwardingCallStreamObserver<>(onNext, TestStreams.throwingErrorHandler(), TestStreams.noopRunnable(), TestStreams.alwaysTrueSupplier()));}
public Builder<T> beam_f27514_0(Supplier<Boolean> isReady)
{    return new Builder<>(new ForwardingCallStreamObserver<>(observer.onNext, observer.onError, observer.onCompleted, isReady));}
private static boolean beam_f27524_0()
{    return true;}
private static Supplier<Boolean> beam_f27525_0()
{    return () -> true;}
public void beam_f27526_0(T value)
{    onNext.accept(value);}
public boolean beam_f27538_0()
{    return true;}
public boolean beam_f27539_0(EncodedBoundedWindow value)
{    return true;}
protected long beam_f27540_0(EncodedBoundedWindow value) throws Exception
{    return (long) VarInt.getLength(value.getEncodedWindow().size()) + value.getEncodedWindow().size();}
private static BeamFnApi.Elements beam_f27548_0(byte[]... datum) throws IOException
{    ByteString.Output output = ByteString.newOutput();    for (byte[] data : datum) {        CODER.encode(valueInGlobalWindow(data), output);    }    return BeamFnApi.Elements.newBuilder().addData(BeamFnApi.Elements.Data.newBuilder().setInstructionId(OUTPUT_LOCATION.getInstructionId()).setTransformId(OUTPUT_LOCATION.getTransformId()).setData(output.toByteString())).build();}
private Consumer<Elements> beam_f27549_0(final Collection<Elements> values)
{    return values::add;}
private Runnable beam_f27550_0(final AtomicBoolean onCompletedWasCalled)
{    return () -> onCompletedWasCalled.set(true);}
public void beam_f27558_0() throws Exception
{    CompletableFuture<Object> future = new CompletableFuture<>();    InboundDataClient client = CompletableFutureInboundDataClient.forBackingFuture(future);    assertThat(future.isDone(), is(false));    assertThat(client.isDone(), is(false));    client.cancel();    assertThat(future.isDone(), is(true));    assertThat(client.isDone(), is(true));    assertThat(future.isCancelled(), is(true));    thrown.expect(CancellationException.class);        future.get();}
public void beam_f27559_0() throws Exception
{    CompletableFuture<Object> future = new CompletableFuture<>();    InboundDataClient client = CompletableFutureInboundDataClient.forBackingFuture(future);    assertThat(future.isDone(), is(false));    assertThat(client.isDone(), is(false));    client.fail(new UnsupportedOperationException("message"));    assertThat(client.isDone(), is(true));    assertThat(future.isDone(), is(true));    assertThat(future.isCompletedExceptionally(), is(true));    thrown.expect(ExecutionException.class);    thrown.expectCause(isA(UnsupportedOperationException.class));    thrown.expectMessage("message");    client.awaitCompletion();}
public void beam_f27560_0()
{    RemoteGrpcPort port = RemoteGrpcPort.newBuilder().setApiServiceDescriptor(ApiServiceDescriptor.newBuilder().setUrl("foo").setOauth2ClientCredentialsGrant(OAuth2ClientCredentialsGrant.getDefaultInstance()).build()).build();    RemoteGrpcPortRead read = RemoteGrpcPortRead.readFromPort(port, "myPort");    assertThat(read.getPort(), equalTo(port));}
public void beam_f27568_0()
{    onStartupRan = true;}
public void beam_f27569_0(PipelineOptions options)
{    beforeProcessingRan = true;    receivedOptions = options;}
public void beam_f27570_0()
{    onStartupRan = false;    beforeProcessingRan = false;    receivedOptions = null;}
public void beam_f27578_0(String position)
{    positionsObserved.add(position);    assertEquals("goodClaim", position);}
public void beam_f27579_0(String position)
{    positionsObserved.add(position);}
public double beam_f27580_0()
{    return 1;}
public void beam_f27589_0() throws Exception
{    assertEquals(ByteString.EMPTY, read());    assertEquals(ByteString.EMPTY, read(ByteString.EMPTY));    assertEquals(ByteString.EMPTY, read(ByteString.EMPTY, ByteString.EMPTY));}
public void beam_f27590_0() throws Exception
{    assertEquals(BYTES_A.concat(BYTES_B), read(BYTES_A, BYTES_B));    assertEquals(BYTES_A.concat(BYTES_B), read(BYTES_A, ByteString.EMPTY, BYTES_B));    assertEquals(BYTES_A.concat(BYTES_B), read(BYTES_A, BYTES_B, ByteString.EMPTY));}
private static ByteString beam_f27591_0(ByteString... bytes) throws IOException
{    return ByteString.readFrom(DataStreams.inbound(Arrays.asList(bytes).iterator()));}
public void beam_f27599_0() throws Exception
{    List<ByteString> output = new ArrayList<>();    ElementDelimitedOutputStream outputStream = new ElementDelimitedOutputStream(output::add, 3);    outputStream.delimitElement();    outputStream.delimitElement();    outputStream.delimitElement();    outputStream.delimitElement();    outputStream.delimitElement();    outputStream.close();    assertThat(output, contains(ByteString.copyFrom(new byte[3]), ByteString.copyFrom(new byte[2])));}
public void beam_f27600_0() throws Exception
{    List<ByteString> output = new ArrayList<>();    ElementDelimitedOutputStream outputStream = new ElementDelimitedOutputStream(output::add, 3);    outputStream.write(new byte[] { 0x01, 0x02 });    outputStream.delimitElement();    outputStream.write(new byte[] { 0x03, 0x04, 0x05, 0x06, 0x07, 0x08 });    outputStream.delimitElement();    outputStream.write(0x09);    outputStream.delimitElement();    outputStream.close();    assertThat(output, contains(ByteString.copyFrom(new byte[] { 0x01, 0x02, 0x03 }), ByteString.copyFrom(new byte[] { 0x04, 0x05, 0x06 }), ByteString.copyFrom(new byte[] { 0x07, 0x08, 0x09 })));}
public void beam_f27601_0() throws Exception
{    final List<String> onNextValues = new ArrayList<>();    AdvancingPhaser phaser = new AdvancingPhaser(1);    final AtomicBoolean isCriticalSectionShared = new AtomicBoolean();    final DirectStreamObserver<String> streamObserver = new DirectStreamObserver<>(phaser, TestStreams.withOnNext((String t) -> {                                assertFalse(isCriticalSectionShared.getAndSet(true));        Uninterruptibles.sleepUninterruptibly(50, TimeUnit.MILLISECONDS);        onNextValues.add(t);        assertTrue(isCriticalSectionShared.getAndSet(false));    }).build());    List<String> prefixes = ImmutableList.of("0", "1", "2", "3", "4");    List<Callable<String>> tasks = new ArrayList<>();    for (final String prefix : prefixes) {        tasks.add(() -> {            for (int i = 0; i < 10; i++) {                streamObserver.onNext(prefix + i);            }            return prefix;        });    }    executor.invokeAll(tasks);    streamObserver.onCompleted();        int[] prefixesIndex = new int[prefixes.size()];    assertEquals(50, onNextValues.size());    for (String onNextValue : onNextValues) {        int prefix = Integer.parseInt(onNextValue.substring(0, 1));        int suffix = Integer.parseInt(onNextValue.substring(1, 2));        assertEquals(prefixesIndex[prefix], suffix);        prefixesIndex[prefix] += 1;    }}
private OutboundObserverFactory.BasicFactory<Integer, String> beam_f27609_0()
{    return inboundObserver -> {        assertThat(inboundObserver, instanceOf(ForwardingClientResponseObserver.class));        return mockResponseObserver;    };}
public void beam_f27610_0() throws Throwable
{    ExecutorService service = Executors.newSingleThreadExecutor();    final TestExecutorService testService = TestExecutors.from(service);    final AtomicBoolean taskRan = new AtomicBoolean();    testService.apply(new Statement() {        @Override        public void evaluate() throws Throwable {            testService.submit(() -> taskRan.set(true)).get();        }    }, null).evaluate();    assertTrue(service.isTerminated());    assertTrue(taskRan.get());}
public void beam_f27611_0() throws Throwable
{    testService.submit(() -> taskRan.set(true)).get();}
private void beam_f27619_0()
{    taskStarted.set(true);    try {        while (true) {            Thread.sleep(10000);        }    } catch (InterruptedException e) {        taskWasInterrupted.set(true);        return;    }}
public void beam_f27620_0()
{    final AtomicBoolean onNextWasCalled = new AtomicBoolean();    TestStreams.withOnNext(onNextWasCalled::set).build().onNext(true);    assertTrue(onNextWasCalled.get());}
public void beam_f27621_0()
{    final AtomicBoolean isReadyWasCalled = new AtomicBoolean();    assertFalse(TestStreams.withOnNext(null).withIsReady(() -> isReadyWasCalled.getAndSet(true)).build().isReady());    assertTrue(isReadyWasCalled.get());}
 WindowedValue<T> beam_f27629_0(WindowedValue<T> input) throws Exception
{        WindowFn<T, W>.AssignContext ctxt = windowFn.new AssignContext() {        @Override        public T element() {            return input.getValue();        }        @Override        public Instant timestamp() {            return input.getTimestamp();        }        @Override        public BoundedWindow window() {            return Iterables.getOnlyElement(input.getWindows());        }    };    Collection<W> windows = windowFn.assignWindows(ctxt);    return WindowedValue.of(input.getValue(), input.getTimestamp(), windows, input.getPane());}
public T beam_f27630_0()
{    return input.getValue();}
public Instant beam_f27631_0()
{    return input.getTimestamp();}
public void beam_f27641_0(WindowedValue<InputT> value) throws Exception
{    consumer.accept(value);}
public Map<String, PTransformRunnerFactory> beam_f27642_0()
{    return ImmutableMap.of(ProcessBundleHandler.JAVA_SOURCE_URN, new Factory(), PTransformTranslation.READ_TRANSFORM_URN, new Factory());}
public BoundedSourceRunner<InputT, OutputT> beam_f27643_0(PipelineOptions pipelineOptions, BeamFnDataClient beamFnDataClient, BeamFnStateClient beamFnStateClient, String pTransformId, PTransform pTransform, Supplier<String> processBundleInstructionId, Map<String, PCollection> pCollections, Map<String, Coder> coders, Map<String, RunnerApi.WindowingStrategy> windowingStrategies, PCollectionConsumerRegistry pCollectionConsumerRegistry, PTransformFunctionRegistry startFunctionRegistry, PTransformFunctionRegistry finishFunctionRegistry, BundleSplitListener splitListener)
{    ImmutableList.Builder<FnDataReceiver<WindowedValue<?>>> consumers = ImmutableList.builder();    for (String pCollectionId : pTransform.getOutputsMap().values()) {        consumers.add(pCollectionConsumerRegistry.getMultiplexingConsumer(pCollectionId));    }    @SuppressWarnings({ "rawtypes", "unchecked" })    BoundedSourceRunner<InputT, OutputT> runner = new BoundedSourceRunner(pipelineOptions, pTransform.getSpec(), consumers.build());        startFunctionRegistry.register(pTransformId, runner::start);    FnDataReceiver runReadLoop = (FnDataReceiver<WindowedValue<InputT>>) runner::runReadLoop;    for (String pCollectionId : pTransform.getInputsMap().values()) {        pCollectionConsumerRegistry.register(pCollectionId, pTransformId, runReadLoop);    }    return runner;}
public PrecombineRunner<KeyT, InputT, AccumT> beam_f27651_0(PipelineOptions pipelineOptions, BeamFnDataClient beamFnDataClient, BeamFnStateClient beamFnStateClient, String pTransformId, PTransform pTransform, Supplier<String> processBundleInstructionId, Map<String, PCollection> pCollections, Map<String, RunnerApi.Coder> coders, Map<String, RunnerApi.WindowingStrategy> windowingStrategies, PCollectionConsumerRegistry pCollectionConsumerRegistry, PTransformFunctionRegistry startFunctionRegistry, PTransformFunctionRegistry finishFunctionRegistry, BundleSplitListener splitListener) throws IOException
{        RehydratedComponents rehydratedComponents = RehydratedComponents.forComponents(RunnerApi.Components.newBuilder().putAllCoders(coders).putAllWindowingStrategies(windowingStrategies).build());    String mainInputTag = Iterables.getOnlyElement(pTransform.getInputsMap().keySet());    RunnerApi.PCollection mainInput = pCollections.get(pTransform.getInputsOrThrow(mainInputTag));            Coder<?> uncastInputCoder = rehydratedComponents.getCoder(mainInput.getCoderId());    KvCoder<KeyT, InputT> inputCoder;    if (uncastInputCoder instanceof WindowedValueCoder) {        inputCoder = (KvCoder<KeyT, InputT>) ((WindowedValueCoder<KV<KeyT, InputT>>) uncastInputCoder).getValueCoder();    } else {        inputCoder = (KvCoder<KeyT, InputT>) rehydratedComponents.getCoder(mainInput.getCoderId());    }    Coder<KeyT> keyCoder = inputCoder.getKeyCoder();    CombinePayload combinePayload = CombinePayload.parseFrom(pTransform.getSpec().getPayload());    CombineFn<InputT, AccumT, ?> combineFn = (CombineFn) SerializableUtils.deserializeFromByteArray(combinePayload.getCombineFn().getSpec().getPayload().toByteArray(), "CombineFn");    Coder<AccumT> accumCoder = (Coder<AccumT>) rehydratedComponents.getCoder(combinePayload.getAccumulatorCoderId());    FnDataReceiver<WindowedValue<KV<KeyT, AccumT>>> consumer = (FnDataReceiver) pCollectionConsumerRegistry.getMultiplexingConsumer(Iterables.getOnlyElement(pTransform.getOutputsMap().values()));    PrecombineRunner<KeyT, InputT, AccumT> runner = new PrecombineRunner<>(pipelineOptions, combineFn, consumer, keyCoder, accumCoder);        startFunctionRegistry.register(pTransformId, runner::startBundle);    pCollectionConsumerRegistry.register(Iterables.getOnlyElement(pTransform.getInputsMap().values()), pTransformId, (FnDataReceiver) (FnDataReceiver<WindowedValue<KV<KeyT, InputT>>>) runner::processElement);    finishFunctionRegistry.register(pTransformId, runner::finishBundle);    return runner;}
 static ThrowingFunction<KV<KeyT, Iterable<AccumT>>, KV<KeyT, AccumT>> beam_f27652_0(String pTransformId, PTransform pTransform) throws IOException
{    CombinePayload combinePayload = CombinePayload.parseFrom(pTransform.getSpec().getPayload());    CombineFn<?, AccumT, ?> combineFn = (CombineFn) SerializableUtils.deserializeFromByteArray(combinePayload.getCombineFn().getSpec().getPayload().toByteArray(), "CombineFn");    return (KV<KeyT, Iterable<AccumT>> input) -> KV.of(input.getKey(), combineFn.mergeAccumulators(input.getValue()));}
 static ThrowingFunction<KV<KeyT, AccumT>, KV<KeyT, OutputT>> beam_f27653_0(String pTransformId, PTransform pTransform) throws IOException
{    CombinePayload combinePayload = CombinePayload.parseFrom(pTransform.getSpec().getPayload());    CombineFn<?, AccumT, OutputT> combineFn = (CombineFn) SerializableUtils.deserializeFromByteArray(combinePayload.getCombineFn().getSpec().getPayload().toByteArray(), "CombineFn");    return (KV<KeyT, AccumT> input) -> KV.of(input.getKey(), combineFn.extractOutput(input.getValue()));}
public BeamFnApi.InstructionResponse beam_f27661_1(BeamFnApi.InstructionRequest value)
{    try {        return handlers.getOrDefault(value.getRequestCase(), this::missingHandler).apply(value).setInstructionId(value.getInstructionId()).build();    } catch (Exception e) {                return BeamFnApi.InstructionResponse.newBuilder().setInstructionId(value.getInstructionId()).setError(getStackTraceAsString(e)).build();    } catch (Error e) {                throw e;    }}
public void beam_f27662_1(BeamFnApi.InstructionResponse value)
{        outboundObserver.onNext(value);}
private void beam_f27663_0(Error e)
{    onFinish.completeExceptionally(e);    outboundObserver.onError(Status.INTERNAL.withDescription(String.format("%s: %s", e.getClass().getName(), e.getMessage())).asException());}
public Message beam_f27672_1(String id)
{    try {                @SuppressWarnings("unchecked")        CompletableFuture<Message> returnValue = computeIfAbsent(id);        /*       * TODO: Even though the register request instruction occurs before the process bundle       * instruction in the control stream, the instructions are being processed in parallel       * in the Java harness causing a data race which is why we use a future. This will block       * forever in the case of a runner having a bug sending the wrong ids. Instead of blocking       * forever, we could do a timed wait or come up with another way of ordering the instruction       * processing to remove the data race.       */        return returnValue.get();    } catch (ExecutionException e) {        throw new RuntimeException(String.format("Failed to load %s", id), e);    } catch (InterruptedException e) {        Thread.currentThread().interrupt();        throw new RuntimeException(String.format("Failed to load %s", id), e);    }}
public BeamFnApi.InstructionResponse.Builder beam_f27673_1(BeamFnApi.InstructionRequest request)
{    BeamFnApi.InstructionResponse.Builder response = BeamFnApi.InstructionResponse.newBuilder().setRegister(RegisterResponse.getDefaultInstance());    BeamFnApi.RegisterRequest registerRequest = request.getRegister();    for (BeamFnApi.ProcessBundleDescriptor processBundleDescriptor : registerRequest.getProcessBundleDescriptorList()) {                computeIfAbsent(processBundleDescriptor.getId()).complete(processBundleDescriptor);        for (Map.Entry<String, RunnerApi.Coder> entry : processBundleDescriptor.getCodersMap().entrySet()) {                        computeIfAbsent(entry.getKey()).complete(entry.getValue());        }    }    return response;}
private CompletableFuture<Message> beam_f27674_0(String id)
{    return idToObject.computeIfAbsent(id, (String ignored) -> new CompletableFuture<>());}
public void beam_f27682_0(String pCollectionId, String pTransformId, FnDataReceiver<WindowedValue<T>> consumer)
{                ElementCountFnDataReceiver wrappedConsumer = pCollectionIdsToWrappedConsumer.getOrDefault(pCollectionId, null);    if (wrappedConsumer != null) {        throw new RuntimeException("New consumers for a pCollectionId cannot be register()-d after " + "calling getMultiplexingConsumer.");    }    HashMap<String, String> labelsMetadata = new HashMap<String, String>();    labelsMetadata.put(MonitoringInfoConstants.Labels.PTRANSFORM, pTransformId);    SimpleExecutionState state = new SimpleExecutionState(ExecutionStateTracker.PROCESS_STATE_NAME, MonitoringInfoConstants.Urns.PROCESS_BUNDLE_MSECS, labelsMetadata);    executionStates.register(state);                    FnDataReceiver<WindowedValue<T>> wrapAndEnableMetricContainer = (WindowedValue<T> input) -> {        MetricsContainerImpl container = metricsContainerRegistry.getContainer(pTransformId);        try (Closeable closeable = MetricsEnvironment.scopedMetricsContainer(container)) {            try (Closeable trackerCloseable = this.stateTracker.enterState(state)) {                consumer.accept(input);            }        }    };    pCollectionIdsToConsumers.put(pCollectionId, (FnDataReceiver) wrapAndEnableMetricContainer);}
public Set<String> beam_f27683_0()
{    return pCollectionIdsToConsumers.keySet();}
public FnDataReceiver<WindowedValue<?>> beam_f27684_0(String pCollectionId)
{    ElementCountFnDataReceiver wrappedConsumer = pCollectionIdsToWrappedConsumer.getOrDefault(pCollectionId, null);    if (wrappedConsumer == null) {        List<FnDataReceiver<WindowedValue<?>>> consumers = pCollectionIdsToConsumers.get(pCollectionId);        FnDataReceiver<WindowedValue<?>> consumer = MultiplexingFnDataReceiver.forConsumers(consumers);        wrappedConsumer = new ElementCountFnDataReceiver(consumer, pCollectionId, metricsContainerRegistry);        pCollectionIdsToWrappedConsumer.put(pCollectionId, wrappedConsumer);    }    return wrappedConsumer;}
public void beam_f27692_1() throws Exception
{    while (true) {        try {            ConsumerAndData tuple = queue.poll(200, TimeUnit.MILLISECONDS);            if (tuple != null) {                                tuple.consumer.accept(tuple.data);            } else {                                if (allDone()) {                    break;                }            }        } catch (Exception e) {                        for (InboundDataClient inboundDataClient : inboundDataClients.keySet()) {                inboundDataClient.fail(e);            }            throw e;        }    }}
public CloseableFnDataReceiver<T> beam_f27693_1(Endpoints.ApiServiceDescriptor apiServiceDescriptor, LogicalEndpoint outputLocation, Coder<T> coder)
{        return this.mainClient.send(apiServiceDescriptor, outputLocation, coder);}
public void beam_f27694_1(T value) throws Exception
{    try {        ConsumerAndData offering = new ConsumerAndData(this.consumer, value);        while (!queue.offer(offering, 200, TimeUnit.MILLISECONDS)) {            if (inboundDataClient.isDone()) {                                break;            }        }    } catch (Exception e) {                inboundDataClient.fail(e);        throw e;    }}
public void beam_f27702_0(OutputT output, Instant timestamp, BoundedWindow window)
{    outputTo(mainOutputConsumers, WindowedValue.of(output, timestamp, window, PaneInfo.NO_FIRING));}
public void beam_f27703_0(TupleTag<T> tag, T output, Instant timestamp, BoundedWindow window)
{    Collection<FnDataReceiver<WindowedValue<T>>> consumers = (Collection) context.localNameToConsumer.get(tag.getId());    if (consumers == null) {        throw new IllegalArgumentException(String.format("Unknown output tag %s", tag));    }    outputTo(consumers, WindowedValue.of(output, timestamp, window, PaneInfo.NO_FIRING));}
public void beam_f27704_0()
{    this.stateAccessor = new FnApiStateAccessor(context.pipelineOptions, context.ptransformId, context.processBundleInstructionId, context.tagToSideInputSpecMap, context.beamFnStateClient, context.keyCoder, (Coder<BoundedWindow>) context.windowCoder, () -> MoreObjects.firstNonNull(currentElement, currentTimer), () -> currentWindow);    doFnInvoker.invokeStartBundle(startBundleContext);}
public org.apache.beam.sdk.state.Timer beam_f27712_0(Duration period)
{    this.period = period;    return this;}
private Instant beam_f27713_0(Instant target)
{    if (TimeDomain.EVENT_TIME.equals(timeDomain)) {        Instant windowExpiry = LateDataUtils.garbageCollectionTime(currentWindow, allowedLateness);        if (target.isAfter(windowExpiry)) {            return windowExpiry;        }    }    return target;}
private void beam_f27714_0(Instant scheduledTime)
{    Object key = ((KV) currentElementOrTimer.getValue()).getKey();    Collection<FnDataReceiver<WindowedValue<KV<Object, Timer>>>> consumers = (Collection) context.localNameToConsumer.get(timerId);    outputTo(consumers, currentElementOrTimer.withValue(KV.of(key, Timer.of(scheduledTime))));}
public Object beam_f27722_0(int index)
{    SerializableFunction converter = doFnSchemaInformation.getElementConverters().get(index);    return converter.apply(element());}
public Instant beam_f27723_0(DoFn<InputT, OutputT> doFn)
{    return timestamp();}
public TimeDomain beam_f27724_0(DoFn<InputT, OutputT> doFn)
{    throw new UnsupportedOperationException("Cannot access time domain outside of @ProcessTimer method.");}
public PipelineOptions beam_f27732_0()
{    return context.pipelineOptions;}
public PipelineOptions beam_f27733_0()
{    return context.pipelineOptions;}
public void beam_f27734_0(OutputT output)
{    outputTo(mainOutputConsumers, WindowedValue.of(output, currentElement.getTimestamp(), currentWindow, currentElement.getPane()));}
public void beam_f27742_0(Instant watermark)
{    throw new UnsupportedOperationException("TODO: Add support for SplittableDoFn");}
public BoundedWindow beam_f27743_0()
{    return currentWindow;}
public PaneInfo beam_f27744_0(DoFn<InputT, OutputT> doFn)
{    throw new UnsupportedOperationException("Cannot access paneInfo outside of @ProcessElement methods.");}
public TimeDomain beam_f27752_0(DoFn<InputT, OutputT> doFn)
{    return timeDomain();}
public OutputReceiver<OutputT> beam_f27753_0(DoFn<InputT, OutputT> doFn)
{    return DoFnOutputReceivers.windowedReceiver(this, null);}
public OutputReceiver<Row> beam_f27754_0(DoFn<InputT, OutputT> doFn)
{    return DoFnOutputReceivers.rowReceiver(this, null, context.mainOutputSchemaCoder);}
public void beam_f27762_0(OutputT output)
{    outputTo(mainOutputConsumers, WindowedValue.of(output, currentTimer.getTimestamp(), currentWindow, PaneInfo.NO_FIRING));}
public void beam_f27763_0(OutputT output, Instant timestamp)
{    checkArgument(!currentTimer.getTimestamp().isAfter(timestamp), "Output time %s can not be before timer timestamp %s.", timestamp, currentTimer.getTimestamp());    outputTo(mainOutputConsumers, WindowedValue.of(output, timestamp, currentWindow, PaneInfo.NO_FIRING));}
public void beam_f27764_0(TupleTag<T> tag, T output)
{    Collection<FnDataReceiver<WindowedValue<T>>> consumers = (Collection) context.localNameToConsumer.get(tag.getId());    if (consumers == null) {        throw new IllegalArgumentException(String.format("Unknown output tag %s", tag));    }    outputTo(consumers, WindowedValue.of(output, currentTimer.getTimestamp(), currentWindow, PaneInfo.NO_FIRING));}
public static void beam_f27772_1(String id, PipelineOptions options, Endpoints.ApiServiceDescriptor loggingApiServiceDescriptor, Endpoints.ApiServiceDescriptor controlApiServiceDescriptor, ManagedChannelFactory channelFactory, OutboundObserverFactory outboundObserverFactory) throws Exception
{    IdGenerator idGenerator = IdGenerators.decrementingLongs();    ExecutorService executorService = options.as(GcsOptions.class).getExecutorService();        try (BeamFnLoggingClient logging = new BeamFnLoggingClient(options, loggingApiServiceDescriptor, channelFactory::forDescriptor)) {                        FileSystems.setDefaultPipelineOptions(options);        EnumMap<BeamFnApi.InstructionRequest.RequestCase, ThrowingFunction<InstructionRequest, Builder>> handlers = new EnumMap<>(BeamFnApi.InstructionRequest.RequestCase.class);        RegisterHandler fnApiRegistry = new RegisterHandler();        BeamFnDataGrpcClient beamFnDataMultiplexer = new BeamFnDataGrpcClient(options, channelFactory::forDescriptor, outboundObserverFactory);        BeamFnStateGrpcClientCache beamFnStateGrpcClientCache = new BeamFnStateGrpcClientCache(idGenerator, channelFactory::forDescriptor, outboundObserverFactory);        ProcessBundleHandler processBundleHandler = new ProcessBundleHandler(options, fnApiRegistry::getById, beamFnDataMultiplexer, beamFnStateGrpcClientCache);        handlers.put(BeamFnApi.InstructionRequest.RequestCase.REGISTER, fnApiRegistry::register);                handlers.put(BeamFnApi.InstructionRequest.RequestCase.PROCESS_BUNDLE, processBundleHandler::processBundle);        BeamFnControlClient control = new BeamFnControlClient(id, controlApiServiceDescriptor, channelFactory, outboundObserverFactory, handlers);        JvmInitializers.runBeforeProcessing(options);                control.processInstructionRequests(executorService);    } finally {        System.out.println("Shutting SDK harness down.");        executorService.shutdown();    }}
public void beam_f27773_0() throws Exception
{    try {                for (Logger logger : configuredLoggers) {            logger.setLevel(null);        }        configuredLoggers.clear();        LogManager.getLogManager().readConfiguration();                logRecordHandler.close();                inboundObserverCompletion.get();    } finally {                channel.shutdown();        if (!channel.awaitTermination(10, TimeUnit.SECONDS)) {            channel.shutdownNow();        }    }}
public String beam_f27774_0()
{    return MoreObjects.toStringHelper(BeamFnLoggingClient.class).add("apiServiceDescriptor", apiServiceDescriptor).toString();}
public static PTransformRunnerFactory<?> beam_f27784_0(WindowedValueMapFnFactory<InputT, OutputT> fnFactory)
{    return new Factory<>(new ExplodedWindowedValueMapperFactory<>(fnFactory));}
public Mapper<InputT, OutputT> beam_f27785_0(PipelineOptions pipelineOptions, BeamFnDataClient beamFnDataClient, BeamFnStateClient beamFnStateClient, String pTransformId, PTransform pTransform, Supplier<String> processBundleInstructionId, Map<String, PCollection> pCollections, Map<String, RunnerApi.Coder> coders, Map<String, RunnerApi.WindowingStrategy> windowingStrategies, PCollectionConsumerRegistry pCollectionConsumerRegistry, PTransformFunctionRegistry startFunctionRegistry, PTransformFunctionRegistry finishFunctionRegistry, BundleSplitListener splitListener) throws IOException
{    FnDataReceiver<WindowedValue<InputT>> consumer = (FnDataReceiver) pCollectionConsumerRegistry.getMultiplexingConsumer(getOnlyElement(pTransform.getOutputsMap().values()));    Mapper<InputT, OutputT> mapper = mapperFactory.create(pTransformId, pTransform, consumer);    pCollectionConsumerRegistry.register(Iterables.getOnlyElement(pTransform.getInputsMap().values()), pTransformId, (FnDataReceiver) (FnDataReceiver<WindowedValue<InputT>>) mapper::map);    return mapper;}
public Mapper<InputT, OutputT> beam_f27786_0(String ptransformId, PTransform ptransform, FnDataReceiver<WindowedValue<OutputT>> outputs) throws IOException
{    ThrowingFunction<WindowedValue<InputT>, WindowedValue<OutputT>> fn = fnFactory.forPTransform(ptransformId, ptransform);    return input -> {        for (WindowedValue<InputT> exploded : input.explodeWindows()) {            outputs.accept(fn.apply(exploded));        }    };}
public Object beam_f27794_0(Object pair)
{    @SuppressWarnings("unchecked")    WindowedValue<KV<?, ?>> windowedKv = (WindowedValue<KV<?, ?>>) pair;    return windowedKv.withValue(windowedKv.getValue().getKey());}
public Object beam_f27795_0(Object pair)
{    @SuppressWarnings("unchecked")    WindowedValue<KV<?, ?>> windowedKv = (WindowedValue<KV<?, ?>>) pair;    return windowedKv.getValue().getValue();}
public Object beam_f27796_0(Object key, Object values)
{    WindowedValue<?> windowedKey = (WindowedValue<?>) key;    return windowedKey.withValue(KV.of(windowedKey.getValue(), values));}
public AccumT beam_f27804_0()
{    return accumulator;}
public long beam_f27805_0()
{    return keySize + accumulatorSize;}
public void beam_f27806_0() throws Exception
{    AccumT newAccumulator = combiner.compact(key, accumulator);    if (newAccumulator != accumulator) {        accumulator = newAccumulator;        accumulatorSize = accumulatorSizer.estimateSize(newAccumulator);    }}
private static int beam_f27814_0()
{    String wordSizeInBits = System.getProperty("sun.arch.data.model");    try {        return Integer.parseInt(wordSizeInBits) / 8;    } catch (NumberFormatException e) {                return 8;    }}
public long beam_f27815_0(T element) throws Exception
{    if (sampleNow()) {        return recordSample(underlying.estimateSize(element));    } else {        return estimate;    }}
private boolean beam_f27816_0()
{    totalElements++;    return --nextSample < 0;}
public void beam_f27824_0(OutputT output, Instant timestamp, BoundedWindow window)
{    throw new UnsupportedOperationException();}
public void beam_f27825_0(TupleTag<T> tag, T output, Instant timestamp, BoundedWindow window)
{    throw new UnsupportedOperationException();}
public void beam_f27826_0()
{    doFnInvoker.invokeStartBundle(startBundleContext);}
public Iterable<T> beam_f27834_0()
{    checkState(!isClosed, "Bag user state is no longer usable because it is closed for %s", request.getStateKey());    if (oldValues == null) {                return Iterables.limit(Collections.unmodifiableList(newValues), newValues.size());    } else if (newValues.isEmpty()) {                return oldValues;    }    return Iterables.concat(oldValues, Iterables.limit(Collections.unmodifiableList(newValues), newValues.size()));}
public void beam_f27835_0(T t)
{    checkState(!isClosed, "Bag user state is no longer usable because it is closed for %s", request.getStateKey());    newValues.add(t);}
public void beam_f27836_0()
{    checkState(!isClosed, "Bag user state is no longer usable because it is closed for %s", request.getStateKey());    oldValues = null;    newValues = new ArrayList<>();}
public void beam_f27844_0()
{    closeAndCleanUp(new RuntimeException("Server hanged up."));}
private static Supplier<ResultT> beam_f27845_0(Supplier<ArgT> arg, Function<ArgT, ResultT> f)
{    return new Supplier<ResultT>() {        private ArgT memoizedArg;        private ResultT memoizedResult;        @Override        public ResultT get() {            ArgT currentArg = arg.get();            if (currentArg != memoizedArg) {                memoizedResult = f.apply(this.memoizedArg = currentArg);            }            return memoizedResult;        }    };}
public ResultT beam_f27846_0()
{    ArgT currentArg = arg.get();    if (currentArg != memoizedArg) {        memoizedResult = f.apply(this.memoizedArg = currentArg);    }    return memoizedResult;}
public T beam_f27854_0()
{    Iterator<T> value = impl.get().iterator();    if (value.hasNext()) {        return value.next();    } else {        return null;    }}
public ValueState<T> beam_f27855_0()
{        return this;}
public BagState<T> beam_f27856_0(String id, StateSpec<BagState<T>> spec, Coder<T> elemCoder)
{    return (BagState<T>) stateKeyObjectCache.computeIfAbsent(createBagUserStateKey(id), new Function<StateKey, Object>() {        @Override        public Object apply(StateKey key) {            return new BagState<T>() {                private final BagUserState<T> impl = createBagUserState(id, elemCoder);                @Override                public void add(T value) {                    impl.append(value);                }                @Override                public ReadableState<Boolean> isEmpty() {                    return new ReadableState<Boolean>() {                        @Nullable                        @Override                        public Boolean read() {                            return !impl.get().iterator().hasNext();                        }                        @Override                        public ReadableState<Boolean> readLater() {                            return this;                        }                    };                }                @Override                public Iterable<T> read() {                    return impl.get();                }                @Override                public BagState<T> readLater() {                                        return this;                }                @Override                public void clear() {                    impl.clear();                }            };        }    });}
public void beam_f27864_0()
{    impl.clear();}
public SetState<T> beam_f27865_0(String id, StateSpec<SetState<T>> spec, Coder<T> elemCoder)
{    throw new UnsupportedOperationException("TODO: Add support for a map state to the Fn API.");}
public MapState<KeyT, ValueT> beam_f27866_0(String id, StateSpec<MapState<KeyT, ValueT>> spec, Coder<KeyT> mapKeyCoder, Coder<ValueT> mapValueCoder)
{    throw new UnsupportedOperationException("TODO: Add support for a map state to the Fn API.");}
public void beam_f27874_0(ElementT value)
{    AccumT newAccumulator = combineFn.addInput(getAccum(), value);    impl.clear();    impl.append(newAccumulator);}
public ReadableState<Boolean> beam_f27875_0()
{    return ReadableStates.immediate(!impl.get().iterator().hasNext());}
public void beam_f27876_0()
{    impl.clear();}
public void beam_f27884_0()
{        try {        for (ThrowingRunnable runnable : stateFinalizers) {            runnable.run();        }    } catch (InterruptedException e) {        Thread.currentThread().interrupt();        throw new IllegalStateException(e);    } catch (Exception e) {        throw new IllegalStateException(e);    }}
public Iterator<T> beam_f27885_0()
{    return new CachingIterator();}
public boolean beam_f27886_0()
{        return position < cachedElements.size() || iterator.hasNext();}
public boolean beam_f27894_0()
{    switch(currentState) {        case EOF:            return false;        case READ_REQUIRED:            CompletableFuture<StateResponse> stateResponseFuture = new CompletableFuture<>();            beamFnStateClient.handle(stateRequestForFirstChunk.toBuilder().setGet(StateGetRequest.newBuilder().setContinuationToken(continuationToken)), stateResponseFuture);            StateResponse stateResponse;            try {                stateResponse = stateResponseFuture.get();            } catch (InterruptedException e) {                Thread.currentThread().interrupt();                throw new IllegalStateException(e);            } catch (ExecutionException e) {                if (e.getCause() == null) {                    throw new IllegalStateException(e);                }                Throwables.throwIfUnchecked(e.getCause());                throw new IllegalStateException(e.getCause());            }            continuationToken = stateResponse.getGet().getContinuationToken();            next = stateResponse.getGet().getData();            currentState = State.HAS_NEXT;            return true;        case HAS_NEXT:            return true;    }    throw new IllegalStateException(String.format("Unknown state %s", currentState));}
public ByteString beam_f27895_0()
{    if (!hasNext()) {        throw new NoSuchElementException();    }        currentState = ByteString.EMPTY.equals(continuationToken) ? State.EOF : State.READ_REQUIRED;    return next;}
public static OutboundObserverFactory beam_f27896_0(PipelineOptions options)
{    List<String> experiments = options.as(ExperimentalOptions.class).getExperiments();    if (experiments != null && experiments.contains("beam_fn_api_buffered_stream")) {        int bufferSize = getBufferSize(experiments);        if (bufferSize > 0) {            return OutboundObserverFactory.clientBuffered(options.as(GcsOptions.class).getExecutorService(), bufferSize);        }        return OutboundObserverFactory.clientBuffered(options.as(GcsOptions.class).getExecutorService());    }    return OutboundObserverFactory.clientDirect();}
public Collection<W> beam_f27904_0()
{    return currentWindows;}
public void beam_f27905_0(Collection<W> toBeMerged, W mergeResult) throws Exception
{    mergedWindows.add(KV.of(mergeResult, toBeMerged));}
 KV<T, KV<Iterable<W>, Iterable<KV<W, Iterable<W>>>>> beam_f27906_0(KV<T, Iterable<W>> windowsToMerge) throws Exception
{    currentWindows = Sets.newHashSet(windowsToMerge.getValue());    windowFn.mergeWindows((MergeContext) mergeContext);    for (KV<W, Collection<W>> mergedWindow : mergedWindows) {        currentWindows.removeAll(mergedWindow.getValue());    }    return KV.of(windowsToMerge.getKey(), KV.of(currentWindows, (Iterable) mergedWindows));}
public Coder<BoundedWindow> beam_f27914_0()
{    throw new UnsupportedOperationException();}
public void beam_f27915_0() throws Exception
{    WindowFn<Object, BoundedWindow> windowFn = new WindowFn<Object, BoundedWindow>() {        @Override        public Collection<BoundedWindow> assignWindows(AssignContext c) throws Exception {            return Collections.singleton(c.window());        }        @Override        public void mergeWindows(MergeContext c) throws Exception {            throw new UnsupportedOperationException();        }        @Override        public WindowMappingFn<BoundedWindow> getDefaultWindowMappingFn() {            throw new UnsupportedOperationException();        }        @Override        public boolean isCompatible(WindowFn<?, ?> other) {            throw new UnsupportedOperationException();        }        @Override        public Coder<BoundedWindow> windowCoder() {            throw new UnsupportedOperationException();        }    };    AssignWindowsRunner<Integer, BoundedWindow> runner = AssignWindowsRunner.create(windowFn);    thrown.expect(IllegalArgumentException.class);    runner.assignWindows(WindowedValue.of(2, new Instant(-10L), ImmutableList.of(new IntervalWindow(new Instant(-22L), Duration.standardMinutes(5L)), new IntervalWindow(new Instant(-120000L), Duration.standardMinutes(3L))), PaneInfo.ON_TIME_AND_ONLY_FIRING));}
public Collection<BoundedWindow> beam_f27916_0(AssignContext c) throws Exception
{    return Collections.singleton(c.window());}
public boolean beam_f27924_0(WindowFn<?, ?> other)
{    return equals(other);}
public Coder<IntervalWindow> beam_f27925_0()
{    return IntervalWindowCoder.of();}
public void beam_f27926_0()
{    MockitoAnnotations.initMocks(this);}
public void beam_f27934_0() throws Exception
{    throw new UnsupportedOperationException("Flush is not supported");}
public void beam_f27935_0() throws Exception
{    RecordingReceiver<WindowedValue<String>> valuesA = new RecordingReceiver<>();    RecordingReceiver<WindowedValue<String>> valuesB = new RecordingReceiver<>();    when(mockBeamFnDataClient.send(any(), any(), Matchers.<Coder<WindowedValue<String>>>any())).thenReturn(valuesA).thenReturn(valuesB);    AtomicReference<String> bundleId = new AtomicReference<>("0");    BeamFnDataWriteRunner<String> writeRunner = new BeamFnDataWriteRunner<>(TRANSFORM_ID, RemoteGrpcPortWrite.writeToPort("myWrite", PORT_SPEC).toPTransform(), bundleId::get, WIRE_CODER_SPEC, COMPONENTS.getCodersMap(), mockBeamFnDataClient);        writeRunner.registerForOutput();    verify(mockBeamFnDataClient).send(eq(PORT_SPEC.getApiServiceDescriptor()), eq(LogicalEndpoint.of(bundleId.get(), TRANSFORM_ID)), eq(WIRE_CODER));    writeRunner.consume(valueInGlobalWindow("ABC"));    writeRunner.consume(valueInGlobalWindow("DEF"));    writeRunner.close();    assertTrue(valuesA.closed);    assertThat(valuesA, contains(valueInGlobalWindow("ABC"), valueInGlobalWindow("DEF")));        bundleId.set("1");    valuesA.clear();    valuesB.clear();    writeRunner.registerForOutput();    verify(mockBeamFnDataClient).send(eq(PORT_SPEC.getApiServiceDescriptor()), eq(LogicalEndpoint.of(bundleId.get(), TRANSFORM_ID)), eq(WIRE_CODER));    writeRunner.consume(valueInGlobalWindow("GHI"));    writeRunner.consume(valueInGlobalWindow("JKL"));    writeRunner.close();    assertTrue(valuesB.closed);    assertThat(valuesB, contains(valueInGlobalWindow("GHI"), valueInGlobalWindow("JKL")));    verifyNoMoreInteractions(mockBeamFnDataClient);}
public void beam_f27936_0() throws Exception
{    closed = true;}
public void beam_f27944_0()
{    for (Registrar registrar : ServiceLoader.load(Registrar.class)) {        if (registrar instanceof BoundedSourceRunner.Registrar) {            assertThat(registrar.getPTransformRunnerFactories(), IsMapContaining.hasKey(URN));            return;        }    }    fail("Expected registrar not found.");}
public Integer beam_f27945_0()
{    return 0;}
public Integer beam_f27946_0(Integer accum, String input)
{    accum += Integer.parseInt(input);    return accum;}
public void beam_f27954_0() throws Exception
{    AtomicBoolean clientClosedStream = new AtomicBoolean();    BlockingQueue<BeamFnApi.InstructionResponse> values = new LinkedBlockingQueue<>();    BlockingQueue<StreamObserver<BeamFnApi.InstructionRequest>> outboundServerObservers = new LinkedBlockingQueue<>();    CallStreamObserver<BeamFnApi.InstructionResponse> inboundServerObserver = TestStreams.withOnNext(values::add).withOnCompleted(() -> clientClosedStream.set(true)).build();    Endpoints.ApiServiceDescriptor apiServiceDescriptor = Endpoints.ApiServiceDescriptor.newBuilder().setUrl(this.getClass().getName() + "-" + UUID.randomUUID().toString()).build();    Server server = InProcessServerBuilder.forName(apiServiceDescriptor.getUrl()).addService(new BeamFnControlGrpc.BeamFnControlImplBase() {        @Override        public StreamObserver<BeamFnApi.InstructionResponse> control(StreamObserver<BeamFnApi.InstructionRequest> outboundObserver) {            Uninterruptibles.putUninterruptibly(outboundServerObservers, outboundObserver);            return inboundServerObserver;        }    }).build();    server.start();    try {        EnumMap<BeamFnApi.InstructionRequest.RequestCase, ThrowingFunction<BeamFnApi.InstructionRequest, BeamFnApi.InstructionResponse.Builder>> handlers = new EnumMap<>(BeamFnApi.InstructionRequest.RequestCase.class);        handlers.put(BeamFnApi.InstructionRequest.RequestCase.PROCESS_BUNDLE, value -> BeamFnApi.InstructionResponse.newBuilder().setProcessBundle(BeamFnApi.ProcessBundleResponse.getDefaultInstance()));        handlers.put(BeamFnApi.InstructionRequest.RequestCase.REGISTER, value -> {            throw FAILURE;        });        BeamFnControlClient client = new BeamFnControlClient("", apiServiceDescriptor, InProcessManagedChannelFactory.create(), OutboundObserverFactory.trivial(), handlers);                StreamObserver<BeamFnApi.InstructionRequest> outboundServerObserver = outboundServerObservers.take();        ExecutorService executor = Executors.newCachedThreadPool();        Future<Void> future = executor.submit(() -> {            client.processInstructionRequests(executor);            return null;        });        outboundServerObserver.onNext(SUCCESSFUL_REQUEST);        assertEquals(SUCCESSFUL_RESPONSE, values.take());                        outboundServerObserver.onNext(UNKNOWN_HANDLER_REQUEST);        assertEquals(UNKNOWN_HANDLER_RESPONSE, values.take());                outboundServerObserver.onNext(FAILURE_REQUEST);        assertEquals(FAILURE_RESPONSE, values.take());                        outboundServerObserver.onCompleted();        future.get();    } finally {        server.shutdownNow();    }}
public StreamObserver<BeamFnApi.InstructionResponse> beam_f27955_0(StreamObserver<BeamFnApi.InstructionRequest> outboundObserver)
{    Uninterruptibles.putUninterruptibly(outboundServerObservers, outboundObserver);    return inboundServerObserver;}
public void beam_f27956_0() throws Exception
{    BlockingQueue<StreamObserver<BeamFnApi.InstructionRequest>> outboundServerObservers = new LinkedBlockingQueue<>();    BlockingQueue<Throwable> error = new LinkedBlockingQueue<>();    CallStreamObserver<BeamFnApi.InstructionResponse> inboundServerObserver = TestStreams.<BeamFnApi.InstructionResponse>withOnNext(response -> fail(String.format("Unexpected Response %s", response))).withOnError(error::add).build();    Endpoints.ApiServiceDescriptor apiServiceDescriptor = Endpoints.ApiServiceDescriptor.newBuilder().setUrl(this.getClass().getName() + "-" + UUID.randomUUID().toString()).build();    Server server = InProcessServerBuilder.forName(apiServiceDescriptor.getUrl()).addService(new BeamFnControlGrpc.BeamFnControlImplBase() {        @Override        public StreamObserver<BeamFnApi.InstructionResponse> control(StreamObserver<BeamFnApi.InstructionRequest> outboundObserver) {            Uninterruptibles.putUninterruptibly(outboundServerObservers, outboundObserver);            return inboundServerObserver;        }    }).build();    server.start();    try {        EnumMap<BeamFnApi.InstructionRequest.RequestCase, ThrowingFunction<BeamFnApi.InstructionRequest, BeamFnApi.InstructionResponse.Builder>> handlers = new EnumMap<>(BeamFnApi.InstructionRequest.RequestCase.class);        handlers.put(BeamFnApi.InstructionRequest.RequestCase.REGISTER, value -> {            throw new Error("Test Error");        });        BeamFnControlClient client = new BeamFnControlClient("", apiServiceDescriptor, InProcessManagedChannelFactory.create(), OutboundObserverFactory.trivial(), handlers);                StreamObserver<BeamFnApi.InstructionRequest> outboundServerObserver = outboundServerObservers.take();        ExecutorService executor = Executors.newCachedThreadPool();        Future<Void> future = executor.submit(() -> {            client.processInstructionRequests(executor);            return null;        });                outboundServerObserver.onNext(InstructionRequest.newBuilder().setInstructionId("0").setRegister(RegisterRequest.getDefaultInstance()).build());                assertThat(error.take(), not(nullValue()));                try {            future.get();            throw new IllegalStateException("The future should have terminated with an error");        } catch (ExecutionException errorWrapper) {            assertThat(errorWrapper.getCause().getMessage(), containsString("Test Error"));        }    } finally {        server.shutdownNow();    }}
public Object beam_f27964_0(PipelineOptions pipelineOptions, BeamFnDataClient beamFnDataClient, BeamFnStateClient beamFnStateClient, String pTransformId, PTransform pTransform, Supplier<String> processBundleInstructionId, Map<String, PCollection> pCollections, Map<String, Coder> coders, Map<String, WindowingStrategy> windowingStrategies, PCollectionConsumerRegistry pCollectionConsumerRegistry, PTransformFunctionRegistry startFunctionRegistry, PTransformFunctionRegistry finishFunctionRegistry, BundleSplitListener splitListener) throws IOException
{    startFunctionRegistry.register(pTransformId, () -> doStateCalls(beamFnStateClient));    return null;}
private void beam_f27965_0(BeamFnStateClient beamFnStateClient)
{    beamFnStateClient.handle(StateRequest.newBuilder().setInstructionId("SUCCESS"), successfulResponse);    beamFnStateClient.handle(StateRequest.newBuilder().setInstructionId("FAIL"), unsuccessfulResponse);}
public void beam_f27966_0() throws Exception
{    BeamFnApi.ProcessBundleDescriptor processBundleDescriptor = BeamFnApi.ProcessBundleDescriptor.newBuilder().putTransforms("2L", RunnerApi.PTransform.newBuilder().setSpec(RunnerApi.FunctionSpec.newBuilder().setUrn(DATA_INPUT_URN).build()).build()).build();    Map<String, Message> fnApiRegistry = ImmutableMap.of("1L", processBundleDescriptor);    ProcessBundleHandler handler = new ProcessBundleHandler(PipelineOptionsFactory.create(), fnApiRegistry::get, beamFnDataClient, null, /* beamFnStateGrpcClientCache */    ImmutableMap.of(DATA_INPUT_URN, new PTransformRunnerFactory<Object>() {        @Override        public Object createRunnerForPTransform(PipelineOptions pipelineOptions, BeamFnDataClient beamFnDataClient, BeamFnStateClient beamFnStateClient, String pTransformId, PTransform pTransform, Supplier<String> processBundleInstructionId, Map<String, PCollection> pCollections, Map<String, Coder> coders, Map<String, WindowingStrategy> windowingStrategies, PCollectionConsumerRegistry pCollectionConsumerRegistry, PTransformFunctionRegistry startFunctionRegistry, PTransformFunctionRegistry finishFunctionRegistry, BundleSplitListener splitListener) throws IOException {            startFunctionRegistry.register(pTransformId, () -> doStateCalls(beamFnStateClient));            return null;        }        private void doStateCalls(BeamFnStateClient beamFnStateClient) {            thrown.expect(IllegalStateException.class);            thrown.expectMessage("State API calls are unsupported");            beamFnStateClient.handle(StateRequest.newBuilder().setInstructionId("SUCCESS"), new CompletableFuture<>());        }    }));    handler.processBundle(BeamFnApi.InstructionRequest.newBuilder().setProcessBundle(BeamFnApi.ProcessBundleRequest.newBuilder().setProcessBundleDescriptorId("1L")).build());}
public StreamObserver<BeamFnApi.Elements> beam_f27974_0(StreamObserver<BeamFnApi.Elements> outboundObserver)
{    outboundServerObserver.set(outboundObserver);    waitForClientToConnect.countDown();    return inboundServerObserver;}
public void beam_f27975_0() throws Exception
{    CountDownLatch waitForInboundServerValuesCompletion = new CountDownLatch(2);    Collection<BeamFnApi.Elements> inboundServerValues = new ConcurrentLinkedQueue<>();    CallStreamObserver<BeamFnApi.Elements> inboundServerObserver = TestStreams.withOnNext((BeamFnApi.Elements t) -> {        inboundServerValues.add(t);        waitForInboundServerValuesCompletion.countDown();    }).build();    Endpoints.ApiServiceDescriptor apiServiceDescriptor = Endpoints.ApiServiceDescriptor.newBuilder().setUrl(this.getClass().getName() + "-" + UUID.randomUUID().toString()).build();    Server server = InProcessServerBuilder.forName(apiServiceDescriptor.getUrl()).addService(new BeamFnDataGrpc.BeamFnDataImplBase() {        @Override        public StreamObserver<BeamFnApi.Elements> data(StreamObserver<BeamFnApi.Elements> outboundObserver) {            return inboundServerObserver;        }    }).build();    server.start();    try {        ManagedChannel channel = InProcessChannelBuilder.forName(apiServiceDescriptor.getUrl()).build();        BeamFnDataGrpcClient clientFactory = new BeamFnDataGrpcClient(PipelineOptionsFactory.fromArgs(new String[] { "--experiments=beam_fn_api_data_buffer_limit=20" }).create(), (Endpoints.ApiServiceDescriptor descriptor) -> channel, OutboundObserverFactory.trivial());        try (CloseableFnDataReceiver<WindowedValue<String>> consumer = clientFactory.send(apiServiceDescriptor, ENDPOINT_A, CODER)) {            consumer.accept(valueInGlobalWindow("ABC"));            consumer.accept(valueInGlobalWindow("DEF"));            consumer.accept(valueInGlobalWindow("GHI"));        }        waitForInboundServerValuesCompletion.await();        assertThat(inboundServerValues, contains(ELEMENTS_A_1, ELEMENTS_A_2));    } finally {        server.shutdownNow();    }}
public StreamObserver<BeamFnApi.Elements> beam_f27976_0(StreamObserver<BeamFnApi.Elements> outboundObserver)
{    return inboundServerObserver;}
public void beam_f27984_0() throws Exception
{    String message = "my_exception";    FnDataReceiver<Integer> multiplexer = MultiplexingFnDataReceiver.forConsumers(ImmutableList.<FnDataReceiver<Integer>>of((Integer i) -> {        if (i > 1) {            throw new Exception(message);        }    }));    multiplexer.accept(0);    multiplexer.accept(1);    thrown.expectMessage(message);    thrown.expect(Exception.class);    multiplexer.accept(2);}
public void beam_f27985_0() throws Exception
{    List<String> consumer = new ArrayList<>();    Set<String> otherConsumer = new HashSet<>();    FnDataReceiver<String> multiplexer = MultiplexingFnDataReceiver.forConsumers(ImmutableList.<FnDataReceiver<String>>of(consumer::add, otherConsumer::add));    multiplexer.accept("foo");    multiplexer.accept("bar");    multiplexer.accept("foo");    assertThat(consumer, contains("foo", "bar", "foo"));    assertThat(otherConsumer, containsInAnyOrder("foo", "bar"));}
public void beam_f27986_0() throws Exception
{    String message = "my_exception";    List<Integer> consumer = new ArrayList<>();    FnDataReceiver<Integer> multiplexer = MultiplexingFnDataReceiver.forConsumers(ImmutableList.<FnDataReceiver<Integer>>of(consumer::add, (Integer i) -> {        if (i > 1) {            throw new Exception(message);        }    }));    multiplexer.accept(0);    multiplexer.accept(1);    assertThat(consumer, containsInAnyOrder(0, 1));    thrown.expectMessage(message);    thrown.expect(Exception.class);    multiplexer.accept(2);}
public void beam_f27994_1() throws Exception
{    CountDownLatch waitForClientToConnect = new CountDownLatch(1);        Collection<WindowedValue<String>> inboundValuesB = new ConcurrentLinkedQueue<>();    Collection<BeamFnApi.Elements> inboundServerValues = new ConcurrentLinkedQueue<>();    AtomicReference<StreamObserver<BeamFnApi.Elements>> outboundServerObserver = new AtomicReference<>();    CallStreamObserver<BeamFnApi.Elements> inboundServerObserver = TestStreams.withOnNext(inboundServerValues::add).build();    Endpoints.ApiServiceDescriptor apiServiceDescriptor = Endpoints.ApiServiceDescriptor.newBuilder().setUrl(this.getClass().getName() + "-" + UUID.randomUUID().toString()).build();    Server server = InProcessServerBuilder.forName(apiServiceDescriptor.getUrl()).addService(new BeamFnDataGrpc.BeamFnDataImplBase() {        @Override        public StreamObserver<BeamFnApi.Elements> data(StreamObserver<BeamFnApi.Elements> outboundObserver) {            outboundServerObserver.set(outboundObserver);            waitForClientToConnect.countDown();            return inboundServerObserver;        }    }).build();    server.start();    try {        ManagedChannel channel = InProcessChannelBuilder.forName(apiServiceDescriptor.getUrl()).build();        BeamFnDataGrpcClient clientFactory = new BeamFnDataGrpcClient(PipelineOptionsFactory.create(), (Endpoints.ApiServiceDescriptor descriptor) -> channel, OutboundObserverFactory.trivial());        QueueingBeamFnDataClient queueingClient = new QueueingBeamFnDataClient(clientFactory);        InboundDataClient readFutureA = queueingClient.receive(apiServiceDescriptor, ENDPOINT_A, CODER, (WindowedValue<String> wv) -> {                        throw new RuntimeException("Intentionally fail!");        });        waitForClientToConnect.await();        Future<?> sendElementsFuture = executor.submit(() -> {            outboundServerObserver.get().onNext(ELEMENTS_A_1);                                    outboundServerObserver.get().onNext(ELEMENTS_B_1);        });        InboundDataClient readFutureB = queueingClient.receive(apiServiceDescriptor, ENDPOINT_B, CODER, (WindowedValue<String> wv) -> {            inboundValuesB.add(wv);        });        Future<?> drainElementsFuture = executor.submit(() -> {            boolean intentionallyFailed = false;            try {                queueingClient.drainAndBlock();            } catch (RuntimeException e) {                intentionallyFailed = true;            } catch (Exception e) {                                fail();            }            assertTrue(intentionallyFailed);        });                                sendElementsFuture.get();        drainElementsFuture.get();        boolean intentionallyFailedA = false;        try {            readFutureA.awaitCompletion();        } catch (ExecutionException e) {            if (e.getCause() instanceof RuntimeException) {                intentionallyFailedA = true;            }        }        assertTrue(intentionallyFailedA);        boolean intentionallyFailedB = false;        try {            readFutureB.awaitCompletion();        } catch (ExecutionException e) {            if (e.getCause() instanceof RuntimeException) {                intentionallyFailedB = true;            }        }        assertTrue(intentionallyFailedB);    } finally {        server.shutdownNow();    }}
public StreamObserver<BeamFnApi.Elements> beam_f27995_0(StreamObserver<BeamFnApi.Elements> outboundObserver)
{    outboundServerObserver.set(outboundObserver);    waitForClientToConnect.countDown();    return inboundServerObserver;}
public void beam_f27996_0() throws Exception
{    String pTransformId = "pTransformId";    String mainOutputId = "101";    RunnerApi.FunctionSpec functionSpec = RunnerApi.FunctionSpec.newBuilder().setUrn(PTransformTranslation.FLATTEN_TRANSFORM_URN).build();    RunnerApi.PTransform pTransform = RunnerApi.PTransform.newBuilder().setSpec(functionSpec).putInputs("inputA", "inputATarget").putInputs("inputB", "inputBTarget").putInputs("inputC", "inputCTarget").putOutputs(mainOutputId, "mainOutputTarget").build();    List<WindowedValue<String>> mainOutputValues = new ArrayList<>();    MetricsContainerStepMap metricsContainerRegistry = new MetricsContainerStepMap();    PCollectionConsumerRegistry consumers = new PCollectionConsumerRegistry(metricsContainerRegistry, mock(ExecutionStateTracker.class));    consumers.register("mainOutputTarget", pTransformId, (FnDataReceiver) (FnDataReceiver<WindowedValue<String>>) mainOutputValues::add);    new FlattenRunner.Factory<>().createRunnerForPTransform(PipelineOptionsFactory.create(), null, /* beamFnDataClient */    null, /* beamFnStateClient */    pTransformId, pTransform, Suppliers.ofInstance("57L")::get, Collections.emptyMap(), Collections.emptyMap(), Collections.emptyMap(), consumers, null, /* startFunctionRegistry */    null, /* finishFunctionRegistry */    null);    mainOutputValues.clear();    assertThat(consumers.keySet(), containsInAnyOrder("inputATarget", "inputBTarget", "inputCTarget", "mainOutputTarget"));    consumers.getMultiplexingConsumer("inputATarget").accept(valueInGlobalWindow("A1"));    consumers.getMultiplexingConsumer("inputATarget").accept(valueInGlobalWindow("A2"));    consumers.getMultiplexingConsumer("inputBTarget").accept(valueInGlobalWindow("B"));    consumers.getMultiplexingConsumer("inputCTarget").accept(valueInGlobalWindow("C"));    assertThat(mainOutputValues, contains(valueInGlobalWindow("A1"), valueInGlobalWindow("A2"), valueInGlobalWindow("B"), valueInGlobalWindow("C")));    mainOutputValues.clear();}
private StateKey beam_f28004_0(String userStateId, String key) throws IOException
{    return StateKey.newBuilder().setBagUserState(StateKey.BagUserState.newBuilder().setTransformId(TEST_TRANSFORM_ID).setUserStateId(userStateId).setKey(encode(key)).setWindow(ByteString.copyFrom(CoderUtils.encodeToByteArray(GlobalWindow.Coder.INSTANCE, GlobalWindow.INSTANCE)))).build();}
public void beam_f28005_0(ProcessContext context)
{    context.output(context.element() + ":" + context.sideInput(defaultSingletonSideInput));    context.output(context.element() + ":" + context.sideInput(singletonSideInput));    for (String sideInputValue : context.sideInput(iterableSideInput)) {        context.output(context.element() + ":" + sideInputValue);    }    context.output(additionalOutput, context.element() + ":additional");}
public void beam_f28006_0() throws Exception
{    Pipeline p = Pipeline.create();    PCollection<String> valuePCollection = p.apply(Create.of("unused"));    PCollectionView<String> defaultSingletonSideInputView = valuePCollection.apply(View.<String>asSingleton().withDefaultValue("defaultSingletonValue"));    PCollectionView<String> singletonSideInputView = valuePCollection.apply(View.asSingleton());    PCollectionView<Iterable<String>> iterableSideInputView = valuePCollection.apply(View.asIterable());    TupleTag<String> mainOutput = new TupleTag<String>("main") {    };    TupleTag<String> additionalOutput = new TupleTag<String>("additional") {    };    PCollectionTuple outputPCollection = valuePCollection.apply(TEST_TRANSFORM_ID, ParDo.of(new TestSideInputDoFn(defaultSingletonSideInputView, singletonSideInputView, iterableSideInputView, additionalOutput)).withSideInputs(defaultSingletonSideInputView, singletonSideInputView, iterableSideInputView).withOutputTags(mainOutput, TupleTagList.of(additionalOutput)));    SdkComponents sdkComponents = SdkComponents.create(p.getOptions());    RunnerApi.Pipeline pProto = PipelineTranslation.toProto(p, sdkComponents, true);    String inputPCollectionId = sdkComponents.registerPCollection(valuePCollection);    String outputPCollectionId = sdkComponents.registerPCollection(outputPCollection.get(mainOutput));    String additionalPCollectionId = sdkComponents.registerPCollection(outputPCollection.get(additionalOutput));    RunnerApi.PTransform pTransform = pProto.getComponents().getTransformsOrThrow(TEST_TRANSFORM_ID);    ImmutableMap<StateKey, ByteString> stateData = ImmutableMap.of(multimapSideInputKey(singletonSideInputView.getTagInternal().getId(), ByteString.EMPTY), encode("singletonValue"), multimapSideInputKey(iterableSideInputView.getTagInternal().getId(), ByteString.EMPTY), encode("iterableValue1", "iterableValue2", "iterableValue3"));    FakeBeamFnStateClient fakeClient = new FakeBeamFnStateClient(stateData);    List<WindowedValue<String>> mainOutputValues = new ArrayList<>();    List<WindowedValue<String>> additionalOutputValues = new ArrayList<>();    MetricsContainerStepMap metricsContainerRegistry = new MetricsContainerStepMap();    PCollectionConsumerRegistry consumers = new PCollectionConsumerRegistry(metricsContainerRegistry, mock(ExecutionStateTracker.class));    consumers.register(outputPCollectionId, TEST_TRANSFORM_ID, (FnDataReceiver) (FnDataReceiver<WindowedValue<String>>) mainOutputValues::add);    consumers.register(additionalPCollectionId, TEST_TRANSFORM_ID, (FnDataReceiver) (FnDataReceiver<WindowedValue<String>>) additionalOutputValues::add);    PTransformFunctionRegistry startFunctionRegistry = new PTransformFunctionRegistry(mock(MetricsContainerStepMap.class), mock(ExecutionStateTracker.class), "start");    PTransformFunctionRegistry finishFunctionRegistry = new PTransformFunctionRegistry(mock(MetricsContainerStepMap.class), mock(ExecutionStateTracker.class), "finish");    new FnApiDoFnRunner.Factory<>().createRunnerForPTransform(PipelineOptionsFactory.create(), null, /* beamFnDataClient */    fakeClient, TEST_TRANSFORM_ID, pTransform, Suppliers.ofInstance("57L")::get, pProto.getComponents().getPcollectionsMap(), pProto.getComponents().getCodersMap(), pProto.getComponents().getWindowingStrategiesMap(), consumers, startFunctionRegistry, finishFunctionRegistry, null);    Iterables.getOnlyElement(startFunctionRegistry.getFunctions()).run();    mainOutputValues.clear();    assertThat(consumers.keySet(), containsInAnyOrder(inputPCollectionId, outputPCollectionId, additionalPCollectionId));            FnDataReceiver<WindowedValue<?>> mainInput = consumers.getMultiplexingConsumer(inputPCollectionId);    mainInput.accept(valueInGlobalWindow("X"));    mainInput.accept(valueInGlobalWindow("Y"));    assertThat(mainOutputValues, contains(valueInGlobalWindow("X:defaultSingletonValue"), valueInGlobalWindow("X:singletonValue"), valueInGlobalWindow("X:iterableValue1"), valueInGlobalWindow("X:iterableValue2"), valueInGlobalWindow("X:iterableValue3"), valueInGlobalWindow("Y:defaultSingletonValue"), valueInGlobalWindow("Y:singletonValue"), valueInGlobalWindow("Y:iterableValue1"), valueInGlobalWindow("Y:iterableValue2"), valueInGlobalWindow("Y:iterableValue3")));    assertThat(additionalOutputValues, contains(valueInGlobalWindow("X:additional"), valueInGlobalWindow("Y:additional")));    mainOutputValues.clear();    Iterables.getOnlyElement(finishFunctionRegistry.getFunctions()).run();    assertThat(mainOutputValues, empty());        assertEquals(stateData, fakeClient.getData());    mainOutputValues.clear();}
public void beam_f28014_0() throws Exception
{    dateTimeProvider.setDateTimeFixed(10000L);    Pipeline p = Pipeline.create();    PCollection<KV<String, String>> valuePCollection = p.apply(Create.of(KV.of("unused", "unused")));    PCollection<String> outputPCollection = valuePCollection.apply(TEST_TRANSFORM_ID, ParDo.of(new TestTimerfulDoFn()));    SdkComponents sdkComponents = SdkComponents.create();    sdkComponents.registerEnvironment(Environment.getDefaultInstance());                RunnerApi.Pipeline pProto = PipelineTranslation.toProto(p, sdkComponents);    String inputPCollectionId = sdkComponents.registerPCollection(valuePCollection);    String outputPCollectionId = sdkComponents.registerPCollection(outputPCollection);    String eventTimerInputPCollectionId = "pTransformId/ParMultiDo(TestTimerful).event";    String eventTimerOutputPCollectionId = "pTransformId/ParMultiDo(TestTimerful).event.output";    String processingTimerInputPCollectionId = "pTransformId/ParMultiDo(TestTimerful).processing";    String processingTimerOutputPCollectionId = "pTransformId/ParMultiDo(TestTimerful).processing.output";    RunnerApi.PTransform pTransform = pProto.getComponents().getTransformsOrThrow(pProto.getComponents().getTransformsOrThrow(TEST_TRANSFORM_ID).getSubtransforms(0)).toBuilder().putOutputs("event", eventTimerOutputPCollectionId).putOutputs("processing", processingTimerOutputPCollectionId).build();    FakeBeamFnStateClient fakeClient = new FakeBeamFnStateClient(ImmutableMap.of(bagUserStateKey("bag", "X"), encode("X0"), bagUserStateKey("bag", "A"), encode("A0"), bagUserStateKey("bag", "C"), encode("C0")));    List<WindowedValue<String>> mainOutputValues = new ArrayList<>();    List<WindowedValue<KV<String, Timer>>> eventTimerOutputValues = new ArrayList<>();    List<WindowedValue<KV<String, Timer>>> processingTimerOutputValues = new ArrayList<>();    MetricsContainerStepMap metricsContainerRegistry = new MetricsContainerStepMap();    PCollectionConsumerRegistry consumers = new PCollectionConsumerRegistry(metricsContainerRegistry, mock(ExecutionStateTracker.class));    consumers.register(outputPCollectionId, TEST_TRANSFORM_ID, (FnDataReceiver) (FnDataReceiver<WindowedValue<String>>) mainOutputValues::add);    consumers.register(eventTimerOutputPCollectionId, TEST_TRANSFORM_ID, (FnDataReceiver) (FnDataReceiver<WindowedValue<KV<String, Timer>>>) eventTimerOutputValues::add);    consumers.register(processingTimerOutputPCollectionId, TEST_TRANSFORM_ID, (FnDataReceiver) (FnDataReceiver<WindowedValue<KV<String, Timer>>>) processingTimerOutputValues::add);    PTransformFunctionRegistry startFunctionRegistry = new PTransformFunctionRegistry(mock(MetricsContainerStepMap.class), mock(ExecutionStateTracker.class), "start");    PTransformFunctionRegistry finishFunctionRegistry = new PTransformFunctionRegistry(mock(MetricsContainerStepMap.class), mock(ExecutionStateTracker.class), "finish");    new FnApiDoFnRunner.Factory<>().createRunnerForPTransform(PipelineOptionsFactory.create(), null, /* beamFnDataClient */    fakeClient, TEST_TRANSFORM_ID, pTransform, Suppliers.ofInstance("57L")::get, ImmutableMap.<String, RunnerApi.PCollection>builder().putAll(pProto.getComponents().getPcollectionsMap()).put(eventTimerOutputPCollectionId, pProto.getComponents().getPcollectionsOrThrow(eventTimerInputPCollectionId)).put(processingTimerOutputPCollectionId, pProto.getComponents().getPcollectionsOrThrow(processingTimerInputPCollectionId)).build(), pProto.getComponents().getCodersMap(), pProto.getComponents().getWindowingStrategiesMap(), consumers, startFunctionRegistry, finishFunctionRegistry, null);    Iterables.getOnlyElement(startFunctionRegistry.getFunctions()).run();    mainOutputValues.clear();    assertThat(consumers.keySet(), containsInAnyOrder(inputPCollectionId, outputPCollectionId, eventTimerInputPCollectionId, eventTimerOutputPCollectionId, processingTimerInputPCollectionId, processingTimerOutputPCollectionId));            FnDataReceiver<WindowedValue<?>> mainInput = consumers.getMultiplexingConsumer(inputPCollectionId);    FnDataReceiver<WindowedValue<?>> eventTimerInput = consumers.getMultiplexingConsumer(eventTimerInputPCollectionId);    FnDataReceiver<WindowedValue<?>> processingTimerInput = consumers.getMultiplexingConsumer(processingTimerInputPCollectionId);    mainInput.accept(timestampedValueInGlobalWindow(KV.of("X", "X1"), new Instant(1000L)));    mainInput.accept(timestampedValueInGlobalWindow(KV.of("Y", "Y1"), new Instant(1100L)));    mainInput.accept(timestampedValueInGlobalWindow(KV.of("X", "X2"), new Instant(1200L)));    mainInput.accept(timestampedValueInGlobalWindow(KV.of("Y", "Y2"), new Instant(1300L)));    eventTimerInput.accept(timerInGlobalWindow("A", new Instant(1400L), new Instant(2400L)));    eventTimerInput.accept(timerInGlobalWindow("B", new Instant(1500L), new Instant(2500L)));    eventTimerInput.accept(timerInGlobalWindow("A", new Instant(1600L), new Instant(2600L)));    processingTimerInput.accept(timerInGlobalWindow("X", new Instant(1700L), new Instant(2700L)));    processingTimerInput.accept(timerInGlobalWindow("C", new Instant(1800L), new Instant(2800L)));    processingTimerInput.accept(timerInGlobalWindow("B", new Instant(1900L), new Instant(2900L)));    assertThat(mainOutputValues, contains(timestampedValueInGlobalWindow("mainX[X0]", new Instant(1000L)), timestampedValueInGlobalWindow("mainY[]", new Instant(1100L)), timestampedValueInGlobalWindow("mainX[X0, X1]", new Instant(1200L)), timestampedValueInGlobalWindow("mainY[Y1]", new Instant(1300L)), timestampedValueInGlobalWindow("event[A0]", new Instant(1400L)), timestampedValueInGlobalWindow("event[]", new Instant(1500L)), timestampedValueInGlobalWindow("event[A0, event]", new Instant(1600L)), timestampedValueInGlobalWindow("processing[X0, X1, X2]", new Instant(1700L)), timestampedValueInGlobalWindow("processing[C0]", new Instant(1800L)), timestampedValueInGlobalWindow("processing[event]", new Instant(1900L))));    assertThat(eventTimerOutputValues, contains(timerInGlobalWindow("X", new Instant(1000L), new Instant(1001L)), timerInGlobalWindow("Y", new Instant(1100L), new Instant(1101L)), timerInGlobalWindow("X", new Instant(1200L), new Instant(1201L)), timerInGlobalWindow("Y", new Instant(1300L), new Instant(1301L)), timerInGlobalWindow("A", new Instant(1400L), new Instant(1411L)), timerInGlobalWindow("B", new Instant(1500L), new Instant(1511L)), timerInGlobalWindow("A", new Instant(1600L), new Instant(1611L)), timerInGlobalWindow("X", new Instant(1700L), new Instant(1721L)), timerInGlobalWindow("C", new Instant(1800L), new Instant(1821L)), timerInGlobalWindow("B", new Instant(1900L), new Instant(1921L))));    assertThat(processingTimerOutputValues, contains(timerInGlobalWindow("X", new Instant(1000L), new Instant(10002L)), timerInGlobalWindow("Y", new Instant(1100L), new Instant(10002L)), timerInGlobalWindow("X", new Instant(1200L), new Instant(10002L)), timerInGlobalWindow("Y", new Instant(1300L), new Instant(10002L)), timerInGlobalWindow("A", new Instant(1400L), new Instant(10012L)), timerInGlobalWindow("B", new Instant(1500L), new Instant(10012L)), timerInGlobalWindow("A", new Instant(1600L), new Instant(10012L)), timerInGlobalWindow("X", new Instant(1700L), new Instant(10022L)), timerInGlobalWindow("C", new Instant(1800L), new Instant(10022L)), timerInGlobalWindow("B", new Instant(1900L), new Instant(10022L))));    mainOutputValues.clear();    Iterables.getOnlyElement(finishFunctionRegistry.getFunctions()).run();    assertThat(mainOutputValues, empty());    assertEquals(ImmutableMap.<StateKey, ByteString>builder().put(bagUserStateKey("bag", "X"), encode("X0", "X1", "X2", "processing")).put(bagUserStateKey("bag", "Y"), encode("Y1", "Y2")).put(bagUserStateKey("bag", "A"), encode("A0", "event", "event")).put(bagUserStateKey("bag", "B"), encode("event", "processing")).put(bagUserStateKey("bag", "C"), encode("C0", "processing")).build(), fakeClient.getData());    mainOutputValues.clear();}
private WindowedValue<T> beam_f28015_0(T value, BoundedWindow window)
{    return WindowedValue.of(value, window.maxTimestamp(), window, PaneInfo.ON_TIME_AND_ONLY_FIRING);}
private WindowedValue<KV<T, org.apache.beam.runners.core.construction.Timer>> beam_f28016_0(T value, Instant valueTimestamp, Instant scheduledTimestamp)
{    return timestampedValueInGlobalWindow(KV.of(value, org.apache.beam.runners.core.construction.Timer.of(scheduledTimestamp)), valueTimestamp);}
public StreamObserver<BeamFnApi.LogEntry.List> beam_f28024_0(StreamObserver<LogControl> responseObserver)
{    return TestStreams.withOnNext((BeamFnApi.LogEntry.List entries) -> logEntries.addAll(entries.getLogEntriesList())).withOnCompleted(responseObserver::onCompleted).build();}
public StreamObserver<InstructionResponse> beam_f28025_0(StreamObserver<InstructionRequest> responseObserver)
{    CountDownLatch waitForResponses = new CountDownLatch(1);    options.as(GcsOptions.class).getExecutorService().submit(() -> {        responseObserver.onNext(INSTRUCTION_REQUEST);        Uninterruptibles.awaitUninterruptibly(waitForResponses);        responseObserver.onCompleted();    });    return TestStreams.withOnNext((InstructionResponse t) -> {        instructionResponses.add(t);        waitForResponses.countDown();    }).withOnCompleted(waitForResponses::countDown).build();}
public void beam_f28026_0() throws Exception
{    AtomicBoolean clientClosedStream = new AtomicBoolean();    Collection<BeamFnApi.LogEntry> values = new ConcurrentLinkedQueue<>();    AtomicReference<StreamObserver<BeamFnApi.LogControl>> outboundServerObserver = new AtomicReference<>();    CallStreamObserver<BeamFnApi.LogEntry.List> inboundServerObserver = TestStreams.withOnNext((BeamFnApi.LogEntry.List logEntries) -> values.addAll(logEntries.getLogEntriesList())).withOnCompleted(() -> {                clientClosedStream.set(true);        outboundServerObserver.get().onCompleted();    }).build();    Endpoints.ApiServiceDescriptor apiServiceDescriptor = Endpoints.ApiServiceDescriptor.newBuilder().setUrl(this.getClass().getName() + "-" + UUID.randomUUID().toString()).build();    Server server = InProcessServerBuilder.forName(apiServiceDescriptor.getUrl()).addService(new BeamFnLoggingGrpc.BeamFnLoggingImplBase() {        @Override        public StreamObserver<BeamFnApi.LogEntry.List> logging(StreamObserver<BeamFnApi.LogControl> outboundObserver) {            outboundServerObserver.set(outboundObserver);            return inboundServerObserver;        }    }).build();    server.start();    ManagedChannel channel = InProcessChannelBuilder.forName(apiServiceDescriptor.getUrl()).build();    try {        BeamFnLoggingClient client = new BeamFnLoggingClient(PipelineOptionsFactory.fromArgs(new String[] { "--defaultSdkHarnessLogLevel=OFF", "--sdkHarnessLogLevelOverrides={\"ConfiguredLogger\": \"DEBUG\"}" }).create(), apiServiceDescriptor, (Endpoints.ApiServiceDescriptor descriptor) -> channel);                assertEquals(Level.OFF, LogManager.getLogManager().getLogger("").getLevel());        assertEquals(Level.FINE, LogManager.getLogManager().getLogger("ConfiguredLogger").getLevel());                LogManager.getLogManager().getLogger("").log(FILTERED_RECORD);                LogManager.getLogManager().getLogger("ConfiguredLogger").log(TEST_RECORD);        LogManager.getLogManager().getLogger("ConfiguredLogger").log(TEST_RECORD_WITH_EXCEPTION);        client.close();                assertEquals(Level.INFO, LogManager.getLogManager().getLogger("").getLevel());        assertNull(LogManager.getLogManager().getLogger("ConfiguredLogger").getLevel());        assertTrue(clientClosedStream.get());        assertTrue(channel.isShutdown());        assertThat(values, contains(TEST_ENTRY, TEST_ENTRY_WITH_EXCEPTION));    } finally {        server.shutdownNow();    }}
public void beam_f28034_0() throws Exception
{    List<WindowedValue<?>> outputConsumer = new ArrayList<>();    PCollectionConsumerRegistry consumers = new PCollectionConsumerRegistry(mock(MetricsContainerStepMap.class), mock(ExecutionStateTracker.class));    consumers.register("outputPC", "pTransformId", outputConsumer::add);    PTransformFunctionRegistry startFunctionRegistry = new PTransformFunctionRegistry(mock(MetricsContainerStepMap.class), mock(ExecutionStateTracker.class), "start");    PTransformFunctionRegistry finishFunctionRegistry = new PTransformFunctionRegistry(mock(MetricsContainerStepMap.class), mock(ExecutionStateTracker.class), "finish");    MapFnRunners.forWindowedValueMapFnFactory(this::createMapFunctionForPTransform).createRunnerForPTransform(PipelineOptionsFactory.create(), null, /* beamFnDataClient */    null, /* beamFnStateClient */    EXPECTED_ID, EXPECTED_PTRANSFORM, Suppliers.ofInstance("57L")::get, Collections.emptyMap(), Collections.emptyMap(), Collections.emptyMap(), consumers, startFunctionRegistry, finishFunctionRegistry, null);    assertThat(startFunctionRegistry.getFunctions(), empty());    assertThat(finishFunctionRegistry.getFunctions(), empty());    assertThat(consumers.keySet(), containsInAnyOrder("inputPC", "outputPC"));    IntervalWindow firstWindow = new IntervalWindow(new Instant(0L), Duration.standardMinutes(10L));    IntervalWindow secondWindow = new IntervalWindow(new Instant(-10L), Duration.standardSeconds(22L));    consumers.getMultiplexingConsumer("inputPC").accept(WindowedValue.of("abc", new Instant(12), ImmutableSet.of(firstWindow, GlobalWindow.INSTANCE, secondWindow), PaneInfo.NO_FIRING));    assertThat(outputConsumer, containsInAnyOrder(WindowedValue.timestampedValueInGlobalWindow("ABC", new Instant(12)), WindowedValue.of("ABC", new Instant(12), secondWindow, PaneInfo.NO_FIRING), WindowedValue.of("ABC", new Instant(12), firstWindow, PaneInfo.NO_FIRING)));}
public ThrowingFunction<WindowedValue<String>, WindowedValue<String>> beam_f28035_0(String ptransformId, PTransform pTransform)
{    assertEquals(EXPECTED_ID, ptransformId);    assertEquals(EXPECTED_PTRANSFORM, pTransform);    return (WindowedValue<String> str) -> str.withValue(str.getValue().toUpperCase());}
public void beam_f28036_0(Object elem)
{    outputElems.add(elem);}
public void beam_f28044_0() throws Exception
{    IdentitySizeEstimator underlying = new IdentitySizeEstimator();    SizeEstimator<Long> estimator = new SamplingSizeEstimator<>(underlying, 0.05, 1.0, 10, new Random(1));        for (int k = 0; k < 10; k += 2) {        assertEquals(100, estimator.estimateSize(100L));        assertEquals(102, estimator.estimateSize(102L));        assertEquals(k + 2, underlying.calls);    }        for (int k = 10; k < 20; k += 2) {        assertThat(estimator.estimateSize(100L), between(100L, 102L));        assertThat(estimator.estimateSize(102L), between(100L, 102L));    }    assertThat(underlying.calls, between(11, 19));    int initialCalls = underlying.calls;        for (int k = 20; k < 1020; k += 2) {        assertThat(estimator.estimateSize(100L), between(100L, 102L));        assertThat(estimator.estimateSize(102L), between(100L, 102L));    }    assertThat(underlying.calls - initialCalls, between(40, 60));}
public void beam_f28045_0() throws Exception
{        List<Long> sizes = Arrays.asList(1L, 10L, 100L, 1000L);    IdentitySizeEstimator underlying = new IdentitySizeEstimator();    SizeEstimator<Long> estimator = new SamplingSizeEstimator<>(underlying, 0.1, 0.2, 10, new Random(1));        for (int k = 0; k < 10; k++) {        long size = sizes.get(k % sizes.size());        assertEquals(size, estimator.estimateSize(size));        assertEquals(k + 1, underlying.calls);    }        for (int k = 10; k < 20; k++) {        long size = sizes.get(k % sizes.size());        assertEquals(size, estimator.estimateSize(size));        assertEquals(k + 1, underlying.calls);    }        for (int k = 20; k < 500; k++) {        estimator.estimateSize(sizes.get(k % sizes.size()));    }        int initialCalls = underlying.calls;    for (int k = 500; k < 1500; k++) {        long size = sizes.get(k % sizes.size());        assertThat(estimator.estimateSize(size), anyOf(is(in(sizes)), between(250L, 350L)));    }    assertThat(underlying.calls - initialCalls, between(180, 220));        for (int k = 1500; k < 3000; k++) {        estimator.estimateSize(sizes.get(k % sizes.size()));    }        initialCalls = underlying.calls;    for (int k = 3000; k < 4000; k++) {        long size = sizes.get(k % sizes.size());        assertThat(estimator.estimateSize(size), anyOf(is(in(sizes)), between(250L, 350L)));    }    assertThat(underlying.calls - initialCalls, between(90, 110));}
public void beam_f28046_0() throws Exception
{    IdentitySizeEstimator underlying = new IdentitySizeEstimator();    SizeEstimator<Long> estimator = new SamplingSizeEstimator<>(underlying, 0.05, 1.0, 10, new Random(1));        for (int k = 0; k < 10; k++) {        assertEquals(100, estimator.estimateSize(100L));        assertEquals(k + 1, underlying.calls);    }        for (int k = 10; k < 20; k++) {        assertEquals(100, estimator.estimateSize(100L));    }    assertThat(underlying.calls, between(11, 19));    int initialCalls = underlying.calls;        for (int k = 20; k < 1020; k++) {        assertEquals(100, estimator.estimateSize(100L));    }    assertThat(underlying.calls - initialCalls, between(40, 60));        while (estimator.estimateSize(1000000L) == 100) {    }        assertEquals(99, estimator.estimateSize(99L));}
public Object beam_f28054_0(Object pair)
{    return ((KV<?, ?>) pair).getValue();}
public Object beam_f28055_0(Object key, Object value)
{    return KV.of(key, value);}
public void beam_f28056_0() throws Exception
{    FakeBeamFnStateClient fakeClient = new FakeBeamFnStateClient(ImmutableMap.of(key("A"), encode("A1", "A2", "A3")));    BagUserState<String> userState = new BagUserState<>(fakeClient, "instructionId", "ptransformId", "stateId", ByteString.copyFromUtf8("encodedWindow"), encode("A"), StringUtf8Coder.of());    assertArrayEquals(new String[] { "A1", "A2", "A3" }, Iterables.toArray(userState.get(), String.class));    userState.asyncClose();    thrown.expect(IllegalStateException.class);    userState.get();}
public void beam_f28064_0() throws Exception
{    assertSame(clientCache.forApiServiceDescriptor(apiServiceDescriptor), clientCache.forApiServiceDescriptor(apiServiceDescriptor));    assertNotSame(clientCache.forApiServiceDescriptor(apiServiceDescriptor), clientCache.forApiServiceDescriptor(Endpoints.ApiServiceDescriptor.getDefaultInstance()));}
public void beam_f28065_0() throws Exception
{    BeamFnStateClient client = clientCache.forApiServiceDescriptor(apiServiceDescriptor);    CompletableFuture<StateResponse> successfulResponse = new CompletableFuture<>();    CompletableFuture<StateResponse> unsuccessfulResponse = new CompletableFuture<>();    client.handle(StateRequest.newBuilder().setInstructionId(SUCCESS), successfulResponse);    client.handle(StateRequest.newBuilder().setInstructionId(FAIL), unsuccessfulResponse);        StreamObserver<StateResponse> outboundServerObserver = outboundServerObservers.take();        outboundServerObserver.onNext(StateResponse.newBuilder().setId("UNKNOWN ID").build());        handleServerRequest(outboundServerObserver, values.take());    handleServerRequest(outboundServerObserver, values.take());        assertNotNull(successfulResponse.get());    try {        unsuccessfulResponse.get();        fail("Expected unsuccessful response");    } catch (ExecutionException e) {        assertThat(e.toString(), containsString(TEST_ERROR));    }}
public void beam_f28066_0() throws Exception
{    BeamFnStateClient client = clientCache.forApiServiceDescriptor(apiServiceDescriptor);    CompletableFuture<StateResponse> inflight = new CompletableFuture<>();    client.handle(StateRequest.newBuilder().setInstructionId(SUCCESS), inflight);        StreamObserver<StateResponse> outboundServerObserver = outboundServerObservers.take();        outboundServerObserver.onError(new StatusRuntimeException(Status.INTERNAL.withDescription(SERVER_ERROR)));    try {        inflight.get();        fail("Expected unsuccessful response due to server error");    } catch (ExecutionException e) {        assertThat(e.toString(), containsString(SERVER_ERROR));    }        CompletableFuture<StateResponse> late = new CompletableFuture<>();    client.handle(StateRequest.newBuilder().setInstructionId(SUCCESS), late);    try {        inflight.get();        fail("Expected unsuccessful response due to server error");    } catch (ExecutionException e) {        assertThat(e.toString(), containsString(SERVER_ERROR));    }}
public void beam_f28074_0()
{    Iterable<String> iterA = new LazyCachingIteratorToIterable<>(Iterators.forArray("A", "B", "C"));    Iterable<String> iterB = new LazyCachingIteratorToIterable<>(Iterators.forArray("A", "B", "C"));    Iterable<String> iterC = new LazyCachingIteratorToIterable<>(Iterators.forArray());    Iterable<String> iterD = new LazyCachingIteratorToIterable<>(Iterators.forArray());    assertEquals(iterA, iterB);    assertEquals(iterC, iterD);    assertNotEquals(iterA, iterC);    assertEquals(iterA.hashCode(), iterB.hashCode());    assertEquals(iterC.hashCode(), iterD.hashCode());}
public void beam_f28075_0() throws Exception
{    FakeBeamFnStateClient fakeBeamFnStateClient = new FakeBeamFnStateClient(ImmutableMap.of(key("A"), encode("A1", "A2", "A3"), key("B"), encode("B1", "B2")));    MultimapSideInput<String, String> multimapSideInput = new MultimapSideInput<>(fakeBeamFnStateClient, "instructionId", "ptransformId", "sideInputId", ByteString.copyFromUtf8("encodedWindow"), StringUtf8Coder.of(), StringUtf8Coder.of());    assertArrayEquals(new String[] { "A1", "A2", "A3" }, Iterables.toArray(multimapSideInput.get("A"), String.class));    assertArrayEquals(new String[] { "B1", "B2" }, Iterables.toArray(multimapSideInput.get("B"), String.class));    assertArrayEquals(new String[] {}, Iterables.toArray(multimapSideInput.get("unknown"), String.class));}
private StateKey beam_f28076_0(String id) throws IOException
{    return StateKey.newBuilder().setMultimapSideInput(StateKey.MultimapSideInput.newBuilder().setTransformId("ptransformId").setSideInputId("sideInputId").setWindow(ByteString.copyFromUtf8("encodedWindow")).setKey(encode(id))).build();}
public void beam_f28084_0()
{    MockitoAnnotations.initMocks(this);}
public void beam_f28085_0()
{    StreamObserver<String> observer = HarnessStreamObserverFactories.fromOptions(PipelineOptionsFactory.create()).outboundObserverFor(this::fakeFactory, mockRequestObserver);    assertThat(observer, instanceOf(DirectStreamObserver.class));}
public void beam_f28086_0()
{    StreamObserver<String> observer = HarnessStreamObserverFactories.fromOptions(PipelineOptionsFactory.fromArgs(new String[] { "--experiments=beam_fn_api_buffered_stream" }).create()).outboundObserverFor(this::fakeFactory, mockRequestObserver);    assertThat(observer, instanceOf(BufferingStreamObserver.class));}
public void beam_f28094_0(AttributeValue value, OutputStream outStream) throws IOException
{    if (value.getS() != null) {        StringUtf8Coder.of().encode(AttributeValueType.s.toString(), outStream);        StringUtf8Coder.of().encode(value.getS(), outStream);    } else if (value.getN() != null) {        StringUtf8Coder.of().encode(AttributeValueType.n.toString(), outStream);        StringUtf8Coder.of().encode(value.getN(), outStream);    } else if (value.getBOOL() != null) {        StringUtf8Coder.of().encode(AttributeValueType.bOOL.toString(), outStream);        BooleanCoder.of().encode(value.getBOOL(), outStream);    } else if (value.getB() != null) {        StringUtf8Coder.of().encode(AttributeValueType.b.toString(), outStream);        ByteArrayCoder.of().encode(convertToByteArray(value.getB()), outStream);    } else if (value.getSS() != null) {        StringUtf8Coder.of().encode(AttributeValueType.sS.toString(), outStream);        LIST_STRING_CODER.encode(value.getSS(), outStream);    } else if (value.getNS() != null) {        StringUtf8Coder.of().encode(AttributeValueType.nS.toString(), outStream);        LIST_STRING_CODER.encode(value.getNS(), outStream);    } else if (value.getBS() != null) {        StringUtf8Coder.of().encode(AttributeValueType.bS.toString(), outStream);        LIST_BYTE_CODER.encode(convertToListByteArray(value.getBS()), outStream);    } else if (value.getL() != null) {        StringUtf8Coder.of().encode(AttributeValueType.l.toString(), outStream);        LIST_ATTRIBUTE_CODER.encode(value.getL(), outStream);    } else if (value.getM() != null) {        StringUtf8Coder.of().encode(AttributeValueType.m.toString(), outStream);        MAP_ATTRIBUTE_CODER.encode(value.getM(), outStream);    } else if (value.getNULL() != null) {        StringUtf8Coder.of().encode(AttributeValueType.nULLValue.toString(), outStream);        BooleanCoder.of().encode(value.getNULL(), outStream);    } else {        throw new CoderException("Unknown Type");    }}
public AttributeValue beam_f28095_0(InputStream inStream) throws IOException
{    AttributeValue attrValue = new AttributeValue();    String type = StringUtf8Coder.of().decode(inStream);    AttributeValueType attrType = AttributeValueType.valueOf(type);    switch(attrType) {        case s:            attrValue.setS(StringUtf8Coder.of().decode(inStream));            break;        case n:            attrValue.setN(StringUtf8Coder.of().decode(inStream));            break;        case bOOL:            attrValue.setBOOL(BooleanCoder.of().decode(inStream));            break;        case b:            attrValue.setB(ByteBuffer.wrap(ByteArrayCoder.of().decode(inStream)));            break;        case sS:            attrValue.setSS(LIST_STRING_CODER.decode(inStream));            break;        case nS:            attrValue.setNS(LIST_STRING_CODER.decode(inStream));            break;        case bS:            attrValue.setBS(convertToListByteBuffer(LIST_BYTE_CODER.decode(inStream)));            break;        case l:            attrValue.setL(LIST_ATTRIBUTE_CODER.decode(inStream));            break;        case m:            attrValue.setM(MAP_ATTRIBUTE_CODER.decode(inStream));            break;        case nULLValue:            attrValue.setNULL(BooleanCoder.of().decode(inStream));            break;        default:            throw new CoderException("Unknown Type");    }    return attrValue;}
private List<byte[]> beam_f28096_0(List<ByteBuffer> listByteBuffer)
{    return listByteBuffer.stream().map(this::convertToByteArray).collect(Collectors.toList());}
public static Write<T> beam_f28104_0()
{    return new AutoValue_DynamoDBIO_Write.Builder().build();}
public Read<T> beam_f28105_0(AwsClientsProvider awsClientsProvider)
{    return toBuilder().setAwsClientsProvider(awsClientsProvider).build();}
public Read<T> beam_f28106_0(String awsAccessKey, String awsSecretKey, Regions region, String serviceEndpoint)
{    return withAwsClientsProvider(new BasicDynamoDBProvider(awsAccessKey, awsSecretKey, region, serviceEndpoint));}
public void beam_f28114_0(@Element Read<T> spec, OutputReceiver<Read<T>> out)
{    ScanRequest scanRequest = spec.getScanRequestFn().apply(null);    for (int i = 0; i < scanRequest.getTotalSegments(); i++) {        out.output(spec.withSegmentId(i));    }}
public void beam_f28115_0(@Element Read<T> spec, OutputReceiver<T> out)
{    AmazonDynamoDB client = spec.getAwsClientsProvider().createDynamoDB();    ScanRequest scanRequest = spec.getScanRequestFn().apply(null);    scanRequest.setSegment(spec.getSegmentId());    ScanResult scanResult = client.scan(scanRequest);    out.output(spec.getScanResultMapperFn().apply(scanResult));}
public List<Map<String, AttributeValue>> beam_f28116_0(@Nullable ScanResult scanResult)
{    if (scanResult == null) {        return Collections.emptyList();    }    return scanResult.getItems();}
public PCollection<Void> beam_f28124_0(PCollection<T> input)
{    return input.apply(ParDo.of(new WriteFn<>(this)));}
public void beam_f28125_0()
{    client = spec.getAwsClientsProvider().createDynamoDB();    retryBackoff = FluentBackoff.DEFAULT.withMaxRetries(    0).withInitialBackoff(RETRY_INITIAL_BACKOFF);    if (spec.getRetryConfiguration() != null) {        retryBackoff = retryBackoff.withMaxRetries(spec.getRetryConfiguration().getMaxAttempts() - 1).withMaxCumulativeBackoff(spec.getRetryConfiguration().getMaxDuration());    }}
public void beam_f28126_0(StartBundleContext context)
{    batch = new ArrayList<>();}
public void beam_f28134_0(AWSCredentialsProvider credentialsProvider, JsonGenerator jsonGenerator, SerializerProvider serializers, TypeSerializer typeSerializer) throws IOException
{    typeSerializer.writeTypePrefixForObject(credentialsProvider, jsonGenerator);    if (credentialsProvider.getClass().equals(AWSStaticCredentialsProvider.class)) {        jsonGenerator.writeStringField(AWS_ACCESS_KEY_ID, credentialsProvider.getCredentials().getAWSAccessKeyId());        jsonGenerator.writeStringField(AWS_SECRET_KEY, credentialsProvider.getCredentials().getAWSSecretKey());    } else if (credentialsProvider.getClass().equals(PropertiesFileCredentialsProvider.class)) {        try {            PropertiesFileCredentialsProvider specificProvider = (PropertiesFileCredentialsProvider) credentialsProvider;            Field field = PropertiesFileCredentialsProvider.class.getDeclaredField(CREDENTIALS_FILE_PATH);            field.setAccessible(true);            String credentialsFilePath = (String) field.get(specificProvider);            jsonGenerator.writeStringField(CREDENTIALS_FILE_PATH, credentialsFilePath);        } catch (NoSuchFieldException | IllegalAccessException e) {            throw new IOException("failed to access private field with reflection", e);        }    } else if (credentialsProvider.getClass().equals(ClasspathPropertiesFileCredentialsProvider.class)) {        try {            ClasspathPropertiesFileCredentialsProvider specificProvider = (ClasspathPropertiesFileCredentialsProvider) credentialsProvider;            Field field = ClasspathPropertiesFileCredentialsProvider.class.getDeclaredField(CREDENTIALS_FILE_PATH);            field.setAccessible(true);            String credentialsFilePath = (String) field.get(specificProvider);            jsonGenerator.writeStringField(CREDENTIALS_FILE_PATH, credentialsFilePath);        } catch (NoSuchFieldException | IllegalAccessException e) {            throw new IOException("failed to access private field with reflection", e);        }    } else if (!SINGLETON_CREDENTIAL_PROVIDERS.contains(credentialsProvider.getClass())) {        throw new IllegalArgumentException("Unsupported AWS credentials provider type " + credentialsProvider.getClass());    }    typeSerializer.writeTypeSuffixForObject(credentialsProvider, jsonGenerator);}
public SSECustomerKey beam_f28135_0(JsonParser parser, DeserializationContext context) throws IOException
{    Map<String, String> asMap = parser.readValueAs(new TypeReference<Map<String, String>>() {    });    final String key = asMap.getOrDefault("key", null);    final String algorithm = asMap.getOrDefault("algorithm", null);    final String md5 = asMap.getOrDefault("md5", null);    SSECustomerKey sseCustomerKey = new SSECustomerKey(key);    if (algorithm != null) {        sseCustomerKey.setAlgorithm(algorithm);    }    if (md5 != null) {        sseCustomerKey.setMd5(md5);    }    return sseCustomerKey;}
public SSEAwsKeyManagementParams beam_f28136_0(JsonParser parser, DeserializationContext context) throws IOException
{    Map<String, String> asMap = parser.readValueAs(new TypeReference<Map<String, String>>() {    });    final String awsKmsKeyId = asMap.getOrDefault("awsKmsKeyId", null);    return new SSEAwsKeyManagementParams(awsKmsKeyId);}
 AmazonS3 beam_f28144_0()
{    return this.amazonS3.get();}
protected List<MatchResult> beam_f28145_0(List<String> specs) throws IOException
{    List<S3ResourceId> paths = specs.stream().map(S3ResourceId::fromUri).collect(Collectors.toList());    List<S3ResourceId> globs = new ArrayList<>();    List<S3ResourceId> nonGlobs = new ArrayList<>();    List<Boolean> isGlobBooleans = new ArrayList<>();    for (S3ResourceId path : paths) {        if (path.isWildcard()) {            globs.add(path);            isGlobBooleans.add(true);        } else {            nonGlobs.add(path);            isGlobBooleans.add(false);        }    }    Iterator<MatchResult> globMatches = matchGlobPaths(globs).iterator();    Iterator<MatchResult> nonGlobMatches = matchNonGlobPaths(nonGlobs).iterator();    ImmutableList.Builder<MatchResult> matchResults = ImmutableList.builder();    for (Boolean isGlob : isGlobBooleans) {        if (isGlob) {            checkState(globMatches.hasNext(), "Expect globMatches has next.");            matchResults.add(globMatches.next());        } else {            checkState(nonGlobMatches.hasNext(), "Expect nonGlobMatches has next.");            matchResults.add(nonGlobMatches.next());        }    }    checkState(!globMatches.hasNext(), "Expect no more elements in globMatches.");    checkState(!nonGlobMatches.hasNext(), "Expect no more elements in nonGlobMatches.");    return matchResults.build();}
 List<MatchResult> beam_f28146_0(Collection<S3ResourceId> globPaths) throws IOException
{    List<Callable<ExpandedGlob>> expandTasks = new ArrayList<>(globPaths.size());    for (final S3ResourceId path : globPaths) {        expandTasks.add(() -> expandGlob(path));    }    Map<S3ResourceId, ExpandedGlob> expandedGlobByGlobPath = new HashMap<>();    List<Callable<PathWithEncoding>> contentTypeTasks = new ArrayList<>(globPaths.size());    for (ExpandedGlob expandedGlob : callTasks(expandTasks)) {        expandedGlobByGlobPath.put(expandedGlob.getGlobPath(), expandedGlob);        if (expandedGlob.getExpandedPaths() != null) {            for (final S3ResourceId path : expandedGlob.getExpandedPaths()) {                contentTypeTasks.add(() -> getPathContentEncoding(path));            }        }    }    Map<S3ResourceId, PathWithEncoding> exceptionByPath = new HashMap<>();    for (PathWithEncoding pathWithException : callTasks(contentTypeTasks)) {        exceptionByPath.put(pathWithException.getPath(), pathWithException);    }    List<MatchResult> results = new ArrayList<>(globPaths.size());    for (S3ResourceId globPath : globPaths) {        ExpandedGlob expandedGlob = expandedGlobByGlobPath.get(globPath);        if (expandedGlob.getException() != null) {            results.add(MatchResult.create(MatchResult.Status.ERROR, expandedGlob.getException()));        } else {            List<MatchResult.Metadata> metadatas = new ArrayList<>();            IOException exception = null;            for (S3ResourceId expandedPath : expandedGlob.getExpandedPaths()) {                PathWithEncoding pathWithEncoding = exceptionByPath.get(expandedPath);                if (pathWithEncoding.getException() != null) {                    exception = pathWithEncoding.getException();                    break;                } else {                    metadatas.add(createBeamMetadata(pathWithEncoding.getPath(), pathWithEncoding.getContentEncoding()));                }            }            if (exception != null) {                if (exception instanceof FileNotFoundException) {                    results.add(MatchResult.create(MatchResult.Status.NOT_FOUND, exception));                } else {                    results.add(MatchResult.create(MatchResult.Status.ERROR, exception));                }            } else {                results.add(MatchResult.create(MatchResult.Status.OK, metadatas));            }        }    }    return ImmutableList.copyOf(results);}
private ObjectMetadata beam_f28154_0(S3ResourceId s3ResourceId) throws AmazonClientException
{    GetObjectMetadataRequest request = new GetObjectMetadataRequest(s3ResourceId.getBucket(), s3ResourceId.getKey());    request.setSSECustomerKey(options.getSSECustomerKey());    return amazonS3.get().getObjectMetadata(request);}
 MatchResult beam_f28155_0(S3ResourceId path)
{    ObjectMetadata s3Metadata;    try {        s3Metadata = getObjectMetadata(path);    } catch (AmazonClientException e) {        if (e instanceof AmazonS3Exception && ((AmazonS3Exception) e).getStatusCode() == 404) {            return MatchResult.create(MatchResult.Status.NOT_FOUND, new FileNotFoundException());        }        return MatchResult.create(MatchResult.Status.ERROR, new IOException(e));    }    return MatchResult.create(MatchResult.Status.OK, ImmutableList.of(createBeamMetadata(path.withSize(s3Metadata.getContentLength()).withLastModified(s3Metadata.getLastModified()), Strings.nullToEmpty(s3Metadata.getContentEncoding()))));}
private static MatchResult.Metadata beam_f28156_0(S3ResourceId path, String contentEncoding)
{    checkArgument(path.getSize().isPresent(), "path has size");    checkNotNull(contentEncoding, "contentEncoding");    boolean isReadSeekEfficient = !NON_READ_SEEK_EFFICIENT_ENCODINGS.contains(contentEncoding);    return MatchResult.Metadata.builder().setIsReadSeekEfficient(isReadSeekEfficient).setResourceId(path).setSizeBytes(path.getSize().get()).setLastModifiedMillis(path.getLastModified().transform(Date::getTime).or(0L)).build();}
 CompleteMultipartUploadResult beam_f28164_0(S3ResourceId sourcePath, S3ResourceId destinationPath, ObjectMetadata sourceObjectMetadata) throws AmazonClientException
{    InitiateMultipartUploadRequest initiateUploadRequest = new InitiateMultipartUploadRequest(destinationPath.getBucket(), destinationPath.getKey()).withStorageClass(options.getS3StorageClass()).withObjectMetadata(sourceObjectMetadata);    initiateUploadRequest.setSSECustomerKey(options.getSSECustomerKey());    InitiateMultipartUploadResult initiateUploadResult = amazonS3.get().initiateMultipartUpload(initiateUploadRequest);    final String uploadId = initiateUploadResult.getUploadId();    List<PartETag> eTags = new ArrayList<>();    final long objectSize = sourceObjectMetadata.getContentLength();        if (objectSize == 0) {        final CopyPartRequest copyPartRequest = new CopyPartRequest().withSourceBucketName(sourcePath.getBucket()).withSourceKey(sourcePath.getKey()).withDestinationBucketName(destinationPath.getBucket()).withDestinationKey(destinationPath.getKey()).withUploadId(uploadId).withPartNumber(1);        copyPartRequest.setSourceSSECustomerKey(options.getSSECustomerKey());        copyPartRequest.setDestinationSSECustomerKey(options.getSSECustomerKey());        CopyPartResult copyPartResult = amazonS3.get().copyPart(copyPartRequest);        eTags.add(copyPartResult.getPartETag());    } else {        long bytePosition = 0;        Integer uploadBufferSizeBytes = options.getS3UploadBufferSizeBytes();                for (int partNumber = 1; bytePosition < objectSize; partNumber++) {            final CopyPartRequest copyPartRequest = new CopyPartRequest().withSourceBucketName(sourcePath.getBucket()).withSourceKey(sourcePath.getKey()).withDestinationBucketName(destinationPath.getBucket()).withDestinationKey(destinationPath.getKey()).withUploadId(uploadId).withPartNumber(partNumber).withFirstByte(bytePosition).withLastByte(Math.min(objectSize - 1, bytePosition + uploadBufferSizeBytes - 1));            copyPartRequest.setSourceSSECustomerKey(options.getSSECustomerKey());            copyPartRequest.setDestinationSSECustomerKey(options.getSSECustomerKey());            CopyPartResult copyPartResult = amazonS3.get().copyPart(copyPartRequest);            eTags.add(copyPartResult.getPartETag());            bytePosition += uploadBufferSizeBytes;        }    }    CompleteMultipartUploadRequest completeUploadRequest = new CompleteMultipartUploadRequest().withBucketName(destinationPath.getBucket()).withKey(destinationPath.getKey()).withUploadId(uploadId).withPartETags(eTags);    return amazonS3.get().completeMultipartUpload(completeUploadRequest);}
protected void beam_f28165_0(List<S3ResourceId> sourceResourceIds, List<S3ResourceId> destinationResourceIds) throws IOException
{    copy(sourceResourceIds, destinationResourceIds);    delete(sourceResourceIds);}
protected void beam_f28166_0(Collection<S3ResourceId> resourceIds) throws IOException
{    List<S3ResourceId> nonDirectoryPaths = resourceIds.stream().filter(s3ResourceId -> !s3ResourceId.isDirectory()).collect(Collectors.toList());    Multimap<String, String> keysByBucket = ArrayListMultimap.create();    for (S3ResourceId path : nonDirectoryPaths) {        keysByBucket.put(path.getBucket(), path.getKey());    }    List<Callable<Void>> tasks = new ArrayList<>();    for (final String bucket : keysByBucket.keySet()) {        for (final List<String> keysPartition : Iterables.partition(keysByBucket.get(bucket), MAX_DELETE_OBJECTS_PER_REQUEST)) {            tasks.add(() -> {                delete(bucket, keysPartition);                return null;            });        }    }    callTasks(tasks);}
public long beam_f28174_0() throws ClosedChannelException
{    if (!isOpen()) {        throw new ClosedChannelException();    }    return contentLength;}
public void beam_f28175_0() throws IOException
{    if (s3Object != null) {        s3Object.close();    }    open = false;}
public boolean beam_f28176_0()
{    return open;}
 S3ResourceId beam_f28184_0(long size)
{    return new S3ResourceId(bucket, key, size, lastModified);}
 Optional<Date> beam_f28185_0()
{    return Optional.fromNullable(lastModified);}
 S3ResourceId beam_f28186_0(Date lastModified)
{    return new S3ResourceId(bucket, key, size, lastModified);}
public String beam_f28194_0()
{    return String.format("%s://%s%s", SCHEME, bucket, key);}
public boolean beam_f28195_0(Object obj)
{    if (!(obj instanceof S3ResourceId)) {        return false;    }    return bucket.equals(((S3ResourceId) obj).bucket) && key.equals(((S3ResourceId) obj).key);}
public int beam_f28196_0()
{    return Objects.hash(bucket, key);}
public AmazonCloudWatch beam_f28204_0()
{    AmazonCloudWatchClientBuilder clientBuilder = AmazonCloudWatchClientBuilder.standard().withCredentials(getCredentialsProvider());    if (serviceEndpoint == null) {        clientBuilder.withRegion(region);    } else {        clientBuilder.withEndpointConfiguration(new AwsClientBuilder.EndpointConfiguration(serviceEndpoint, region.getName()));    }    return clientBuilder.build();}
public AmazonSNS beam_f28205_0()
{    return AmazonSNSClientBuilder.standard().withCredentials(getCredentialsProvider()).withRegion(region).build();}
 static PublishResultCoder beam_f28206_0()
{    return INSTANCE;}
public boolean beam_f28214_0(Throwable throwable)
{    return (throwable instanceof IOException || (throwable instanceof InternalErrorException) || (throwable instanceof InternalErrorException && ELIGIBLE_CODES.contains(((InternalErrorException) throwable).getStatusCode())));}
public Write beam_f28215_0(String topicName)
{    return builder().setTopicName(topicName).build();}
public Write beam_f28216_0(AwsClientsProvider awsClientsProvider)
{    return builder().setAWSClientsProvider(awsClientsProvider).build();}
public void beam_f28224_0()
{    if (producer != null) {        producer.shutdown();        producer = null;    }}
private static boolean beam_f28225_1(AmazonSNS client, String topicName)
{    try {        GetTopicAttributesResult topicAttributesResult = client.getTopicAttributes(topicName);        return topicAttributesResult != null && topicAttributesResult.getSdkHttpMetadata().getHttpStatusCode() == 200;    } catch (Exception e) {                throw e;    }}
public void beam_f28226_0()
{    reader.ifPresent(r -> r.delete(messagesToDelete));}
public static Write beam_f28234_0()
{    return new AutoValue_SqsIO_Write.Builder().build();}
public Read beam_f28235_0(long maxNumRecords)
{    return toBuilder().setMaxNumRecords(maxNumRecords).build();}
public Read beam_f28236_0(Duration maxReadTime)
{    return toBuilder().setMaxReadTime(maxReadTime).build();}
public Message beam_f28244_0() throws NoSuchElementException
{    if (current == null) {        throw new NoSuchElementException();    }    return current;}
public Instant beam_f28245_0() throws NoSuchElementException
{    if (current == null) {        throw new NoSuchElementException();    }    return getTimestamp(current);}
public byte[] beam_f28246_0() throws NoSuchElementException
{    if (current == null) {        throw new NoSuchElementException();    }    return current.getMessageId().getBytes(StandardCharsets.UTF_8);}
public List<SqsUnboundedSource> beam_f28255_0(int desiredNumSplits, PipelineOptions options)
{    List<SqsUnboundedSource> sources = new ArrayList<>();    for (int i = 0; i < Math.max(1, desiredNumSplits); ++i) {        sources.add(new SqsUnboundedSource(read, sqsConfiguration));    }    return sources;}
public UnboundedReader<Message> beam_f28256_0(PipelineOptions options, @Nullable SqsCheckpointMark checkpointMark)
{    return new SqsUnboundedReader(this, checkpointMark);}
public Coder<SqsCheckpointMark> beam_f28257_0()
{    return SerializableCoder.of(SqsCheckpointMark.class);}
public void beam_f28265_0() throws IOException
{    AttributeValue expected = new AttributeValue();    expected.setB(ByteBuffer.wrap("hello".getBytes(StandardCharsets.UTF_8)));    AttributeValueCoder coder = AttributeValueCoder.of();    ByteArrayOutputStream output = new ByteArrayOutputStream();    coder.encode(expected, output);    ByteArrayInputStream in = new ByteArrayInputStream(output.toByteArray());    AttributeValue actual = coder.decode(in);    Assert.assertEquals(expected, actual);}
public void beam_f28266_0() throws IOException
{    AttributeValue expected = new AttributeValue();    expected.setSS(ImmutableList.of("foo", "bar"));    AttributeValueCoder coder = AttributeValueCoder.of();    ByteArrayOutputStream output = new ByteArrayOutputStream();    coder.encode(expected, output);    ByteArrayInputStream in = new ByteArrayInputStream(output.toByteArray());    AttributeValue actual = coder.decode(in);    Assert.assertEquals(expected, actual);}
public void beam_f28267_0() throws IOException
{    AttributeValue expected = new AttributeValue();    expected.setNS(ImmutableList.of("123", "456"));    AttributeValueCoder coder = AttributeValueCoder.of();    ByteArrayOutputStream output = new ByteArrayOutputStream();    coder.encode(expected, output);    ByteArrayInputStream in = new ByteArrayInputStream(output.toByteArray());    AttributeValue actual = coder.decode(in);    Assert.assertEquals(expected, actual);}
public static void beam_f28275_0()
{    DynamoDBIOTestHelper.startServerClient();    DynamoDBIOTestHelper.createTestTable(tableName);    expected = DynamoDBIOTestHelper.generateTestData(tableName, numOfItems);}
public static void beam_f28276_0()
{    DynamoDBIOTestHelper.stopServerClient(tableName);}
public void beam_f28277_0()
{    PCollection<List<Map<String, AttributeValue>>> actual = pipeline.apply(DynamoDBIO.<List<Map<String, AttributeValue>>>read().withAwsClientsProvider(AwsClientsProviderMock.of(DynamoDBIOTestHelper.getDynamoDBClient())).withScanRequestFn((SerializableFunction<Void, ScanRequest>) input -> new ScanRequest(tableName).withTotalSegments(1)).items());    PAssert.that(actual).containsInAnyOrder(expected);    pipeline.run().waitUntilFinish();}
 static void beam_f28285_0(String tableName)
{    if (dynamoDBClient != null) {        dynamoDBClient.deleteTable(tableName);        dynamoDBClient.shutdown();    }    localStackContainer.stop();}
 static AmazonDynamoDB beam_f28286_0()
{        return AmazonDynamoDBClientBuilder.standard().withEndpointConfiguration(localStackContainer.getEndpointConfiguration(LocalStackContainer.Service.DYNAMODB)).withCredentials(localStackContainer.getDefaultCredentialsProvider()).build();}
 static List<Map<String, AttributeValue>> beam_f28287_0(String tableName, int numOfItems)
{    BatchWriteItemRequest batchWriteItemRequest = generateBatchWriteItemRequest(tableName, numOfItems);    dynamoDBClient.batchWriteItem(batchWriteItemRequest);    ScanResult scanResult = dynamoDBClient.scan(new ScanRequest().withTableName(tableName));    List<Map<String, AttributeValue>> items = scanResult.getItems();    Assert.assertEquals(numOfItems, items.size());    return items;}
public void beam_f28295_0() throws Exception
{    String credentialsFilePath = "/path/to/file";    PropertiesFileCredentialsProvider credentialsProvider = new PropertiesFileCredentialsProvider(credentialsFilePath);    String serializedCredentialsProvider = objectMapper.writeValueAsString(credentialsProvider);    AWSCredentialsProvider deserializedCredentialsProvider = objectMapper.readValue(serializedCredentialsProvider, AWSCredentialsProvider.class);    assertEquals(credentialsProvider.getClass(), deserializedCredentialsProvider.getClass());    Field field = PropertiesFileCredentialsProvider.class.getDeclaredField("credentialsFilePath");    field.setAccessible(true);    String deserializedCredentialsFilePath = (String) field.get(deserializedCredentialsProvider);    assertEquals(credentialsFilePath, deserializedCredentialsFilePath);}
public void beam_f28296_0() throws Exception
{    String credentialsFilePath = "/path/to/file";    ClasspathPropertiesFileCredentialsProvider credentialsProvider = new ClasspathPropertiesFileCredentialsProvider(credentialsFilePath);    String serializedCredentialsProvider = objectMapper.writeValueAsString(credentialsProvider);    AWSCredentialsProvider deserializedCredentialsProvider = objectMapper.readValue(serializedCredentialsProvider, AWSCredentialsProvider.class);    assertEquals(credentialsProvider.getClass(), deserializedCredentialsProvider.getClass());    Field field = ClasspathPropertiesFileCredentialsProvider.class.getDeclaredField("credentialsFilePath");    field.setAccessible(true);    String deserializedCredentialsFilePath = (String) field.get(deserializedCredentialsProvider);    assertEquals(credentialsFilePath, deserializedCredentialsFilePath);}
public void beam_f28297_0() throws Exception
{    AWSCredentialsProvider credentialsProvider;    String serializedCredentialsProvider;    AWSCredentialsProvider deserializedCredentialsProvider;    credentialsProvider = new DefaultAWSCredentialsProviderChain();    serializedCredentialsProvider = objectMapper.writeValueAsString(credentialsProvider);    deserializedCredentialsProvider = objectMapper.readValue(serializedCredentialsProvider, AWSCredentialsProvider.class);    assertEquals(credentialsProvider.getClass(), deserializedCredentialsProvider.getClass());    credentialsProvider = new EnvironmentVariableCredentialsProvider();    serializedCredentialsProvider = objectMapper.writeValueAsString(credentialsProvider);    deserializedCredentialsProvider = objectMapper.readValue(serializedCredentialsProvider, AWSCredentialsProvider.class);    assertEquals(credentialsProvider.getClass(), deserializedCredentialsProvider.getClass());    credentialsProvider = new SystemPropertiesCredentialsProvider();    serializedCredentialsProvider = objectMapper.writeValueAsString(credentialsProvider);    deserializedCredentialsProvider = objectMapper.readValue(serializedCredentialsProvider, AWSCredentialsProvider.class);    assertEquals(credentialsProvider.getClass(), deserializedCredentialsProvider.getClass());    credentialsProvider = new ProfileCredentialsProvider();    serializedCredentialsProvider = objectMapper.writeValueAsString(credentialsProvider);    deserializedCredentialsProvider = objectMapper.readValue(serializedCredentialsProvider, AWSCredentialsProvider.class);    assertEquals(credentialsProvider.getClass(), deserializedCredentialsProvider.getClass());    credentialsProvider = new EC2ContainerCredentialsProviderWrapper();    serializedCredentialsProvider = objectMapper.writeValueAsString(credentialsProvider);    deserializedCredentialsProvider = objectMapper.readValue(serializedCredentialsProvider, AWSCredentialsProvider.class);    assertEquals(credentialsProvider.getClass(), deserializedCredentialsProvider.getClass());}
 static MatchResultMatcher beam_f28305_0(MatchResult expected)
{    MatchResult.Status expectedStatus = expected.status();    List<MatchResult.Metadata> expectedMetadata = null;    IOException expectedException = null;    try {        expectedMetadata = expected.metadata();    } catch (IOException e) {        expectedException = e;    }    return new MatchResultMatcher(expectedStatus, expectedMetadata, expectedException);}
public boolean beam_f28306_0(Object actual)
{    if (actual == null) {        return false;    }    if (!(actual instanceof MatchResult)) {        return false;    }    MatchResult actualResult = (MatchResult) actual;    if (!expectedStatus.equals(actualResult.status())) {        return false;    }    List<MatchResult.Metadata> actualMetadata;    try {        actualMetadata = actualResult.metadata();    } catch (IOException e) {        return expectedException != null && expectedException.toString().equals(e.toString());    }    return expectedMetadata != null && expectedMetadata.equals(actualMetadata);}
public void beam_f28307_0(Description description)
{    if (expectedMetadata != null) {        description.appendText(MatchResult.create(expectedStatus, expectedMetadata).toString());    } else {        description.appendText(MatchResult.create(expectedStatus, expectedException).toString());    }}
private void beam_f28315_0(S3FileSystem s3FileSystem, GetObjectMetadataRequest request, S3Options options, ObjectMetadata objectMetadata)
{    when(s3FileSystem.getAmazonS3Client().getObjectMetadata(argThat(new GetObjectMetadataRequestMatcher(request)))).thenReturn(objectMetadata);    assertEquals(getSSECustomerKeyMd5(options), s3FileSystem.getAmazonS3Client().getObjectMetadata(request).getSSECustomerKeyMd5());}
private void beam_f28316_0(S3Options options) throws IOException
{    S3FileSystem s3FileSystem = buildMockedS3FileSystem(s3Options());    S3ResourceId sourcePath = S3ResourceId.fromUri("s3://bucket/from");    S3ResourceId destinationPath = S3ResourceId.fromUri("s3://bucket/to");    ObjectMetadata objectMetadata = new ObjectMetadata();    objectMetadata.setContentLength(0);    if (getSSECustomerKeyMd5(options) != null) {        objectMetadata.setSSECustomerKeyMd5(getSSECustomerKeyMd5(options));    }    assertGetObjectMetadata(s3FileSystem, createObjectMetadataRequest(sourcePath, options), options, objectMetadata);    s3FileSystem.copy(sourcePath, destinationPath);    verify(s3FileSystem.getAmazonS3Client(), times(1)).copyObject(any(CopyObjectRequest.class));        objectMetadata.setContentLength(5_368_709_120L);    assertGetObjectMetadata(s3FileSystem, createObjectMetadataRequest(sourcePath, options), options, objectMetadata);    try {        s3FileSystem.copy(sourcePath, destinationPath);    } catch (NullPointerException e) {        }    verify(s3FileSystem.getAmazonS3Client(), never()).copyObject(null);}
public void beam_f28317_0()
{    testAtomicCopy(s3Options());    testAtomicCopy(s3OptionsWithSSECustomerKey());}
public void beam_f28325_0()
{    S3FileSystem s3FileSystem = buildMockedS3FileSystem(s3Options());    S3ResourceId path = S3ResourceId.fromUri("s3://testbucket/testdirectory/nonexistentfile");    AmazonS3Exception exception = new AmazonS3Exception("mock exception");    exception.setStatusCode(404);    when(s3FileSystem.getAmazonS3Client().getObjectMetadata(argThat(new GetObjectMetadataRequestMatcher(new GetObjectMetadataRequest(path.getBucket(), path.getKey()))))).thenThrow(exception);    MatchResult result = s3FileSystem.matchNonGlobPath(path);    assertThat(result, MatchResultMatcher.create(MatchResult.Status.NOT_FOUND, new FileNotFoundException()));}
public void beam_f28326_0()
{    S3FileSystem s3FileSystem = buildMockedS3FileSystem(s3Options());    AmazonS3Exception exception = new AmazonS3Exception("mock exception");    exception.setStatusCode(403);    S3ResourceId path = S3ResourceId.fromUri("s3://testbucket/testdirectory/keyname");    when(s3FileSystem.getAmazonS3Client().getObjectMetadata(argThat(new GetObjectMetadataRequestMatcher(new GetObjectMetadataRequest(path.getBucket(), path.getKey()))))).thenThrow(exception);    assertThat(s3FileSystem.matchNonGlobPath(path), MatchResultMatcher.create(MatchResult.Status.ERROR, new IOException(exception)));}
public boolean beam_f28327_0(ListObjectsV2Request argument)
{    if (argument instanceof ListObjectsV2Request) {        ListObjectsV2Request actual = (ListObjectsV2Request) argument;        return expected.getBucketName().equals(actual.getBucketName()) && expected.getPrefix().equals(actual.getPrefix()) && (expected.getContinuationToken() == null ? actual.getContinuationToken() == null : expected.getContinuationToken().equals(actual.getContinuationToken()));    }    return false;}
public void beam_f28335_0()
{    ResourceId tmpDir = S3ResourceId.fromUri("s3://my_bucket/").resolve("tmp dir", RESOLVE_FILE);    thrown.expect(IllegalStateException.class);    thrown.expectMessage("Expected this resource to be a directory, but was [s3://my_bucket/tmp dir]");    tmpDir.resolve("aa", RESOLVE_FILE);}
public void beam_f28336_0()
{    ResourceId resourceId = S3ResourceId.fromUri("s3://bucket/path/to/file");    thrown.expect(IllegalStateException.class);        resourceId.resolve("child-path", RESOLVE_DIRECTORY);}
public void beam_f28337_0()
{    ResourceId resourceId = S3ResourceId.fromUri("s3://bucket/path/to/dir/");    thrown.expect(IllegalArgumentException.class);        resourceId.resolve("..", RESOLVE_FILE);}
public void beam_f28345_0()
{    S3ResourceId path = S3ResourceId.fromUri("s3://bucket");    S3ResourceId path2 = S3ResourceId.fromUri("s3://bucket/");    assertEquals(path, path2);    assertEquals(path.toString(), path2.toString());}
public void beam_f28346_0()
{    String filename = "s3://some-bucket/some/file.txt";    S3ResourceId path = S3ResourceId.fromUri(filename);    assertEquals(filename, path.toString());    filename = "s3://some-bucket/some/";    path = S3ResourceId.fromUri(filename);    assertEquals(filename, path.toString());    filename = "s3://some-bucket/";    path = S3ResourceId.fromUri(filename);    assertEquals(filename, path.toString());}
public void beam_f28347_0()
{    S3ResourceId a = S3ResourceId.fromComponents("bucket", "a/b/c");    S3ResourceId b = S3ResourceId.fromComponents("bucket", "a/b/c");    assertEquals(a, a);    assertEquals(a, b);    b = S3ResourceId.fromComponents(a.getBucket(), "a/b/c/");    assertNotEquals(a, b);    assertNotEquals(b, a);    b = S3ResourceId.fromComponents(a.getBucket(), "x/y/z");    assertNotEquals(a, b);    assertNotEquals(b, a);    b = S3ResourceId.fromComponents("other-bucket", a.getKey());    assertNotEquals(a, b);    assertNotEquals(b, a);}
 static S3Options beam_f28355_0()
{    S3Options options = s3Options();    String awsKmsKeyId = "arn:aws:kms:eu-west-1:123456789012:key/dc123456-7890-ABCD-EF01-234567890ABC";    SSEAwsKeyManagementParams sseAwsKeyManagementParams = new SSEAwsKeyManagementParams(awsKmsKeyId);    options.setSSEAwsKeyManagementParams(sseAwsKeyManagementParams);    return options;}
 static S3Options beam_f28356_0()
{    S3Options options = s3OptionsWithSSEAwsKeyManagementParams();    options.setSSECustomerKey(new SSECustomerKey("86glyTlCNZgccSxW8JxMa6ZdjdK3N141glAysPUZ3AA="));    return options;}
 static S3FileSystem beam_f28357_0(S3Options options)
{    return buildMockedS3FileSystem(options, Mockito.mock(AmazonS3.class));}
public AddPermissionResult beam_f28367_0(String topicArn, String label, List<String> aWSAccountIds, List<String> actionNames)
{    throw new RuntimeException("Not implemented");}
public CheckIfPhoneNumberIsOptedOutResult beam_f28368_0(CheckIfPhoneNumberIsOptedOutRequest checkIfPhoneNumberIsOptedOutRequest)
{    throw new RuntimeException("Not implemented");}
public ConfirmSubscriptionResult beam_f28369_0(ConfirmSubscriptionRequest confirmSubscriptionRequest)
{    throw new RuntimeException("Not implemented");}
public DeletePlatformApplicationResult beam_f28377_0(DeletePlatformApplicationRequest deletePlatformApplicationRequest)
{    throw new RuntimeException("Not implemented");}
public DeleteTopicResult beam_f28378_0(DeleteTopicRequest deleteTopicRequest)
{    throw new RuntimeException("Not implemented");}
public DeleteTopicResult beam_f28379_0(String topicArn)
{    throw new RuntimeException("Not implemented");}
public ListEndpointsByPlatformApplicationResult beam_f28387_0(ListEndpointsByPlatformApplicationRequest listEndpointsByPlatformApplicationRequest)
{    throw new RuntimeException("Not implemented");}
public ListPhoneNumbersOptedOutResult beam_f28388_0(ListPhoneNumbersOptedOutRequest listPhoneNumbersOptedOutRequest)
{    throw new RuntimeException("Not implemented");}
public ListPlatformApplicationsResult beam_f28389_0(ListPlatformApplicationsRequest listPlatformApplicationsRequest)
{    throw new RuntimeException("Not implemented");}
public ListTopicsResult beam_f28397_0(ListTopicsRequest listTopicsRequest)
{    throw new RuntimeException("Not implemented");}
public ListTopicsResult beam_f28398_0()
{    throw new RuntimeException("Not implemented");}
public ListTopicsResult beam_f28399_0(String nextToken)
{    throw new RuntimeException("Not implemented");}
public SetSMSAttributesResult beam_f28407_0(SetSMSAttributesRequest setSMSAttributesRequest)
{    throw new RuntimeException("Not implemented");}
public SetSubscriptionAttributesResult beam_f28408_0(SetSubscriptionAttributesRequest setSubscriptionAttributesRequest)
{    throw new RuntimeException("Not implemented");}
public SetSubscriptionAttributesResult beam_f28409_0(String subscriptionArn, String attributeName, String attributeValue)
{    throw new RuntimeException("Not implemented");}
public PublishResult beam_f28418_0(PublishRequest publishRequest)
{    throw new InternalErrorException("Service unavailable");}
public PublishResult beam_f28419_0(PublishRequest publishRequest)
{    PublishResult result = Mockito.mock(PublishResult.class);    SdkHttpMetadata metadata = Mockito.mock(SdkHttpMetadata.class);    Mockito.when(metadata.getHttpHeaders()).thenReturn(new HashMap<>());    Mockito.when(metadata.getHttpStatusCode()).thenReturn(200);    Mockito.when(result.getSdkHttpMetadata()).thenReturn(metadata);    Mockito.when(result.getMessageId()).thenReturn(UUID.randomUUID().toString());    return result;}
private static PublishRequest beam_f28420_0(String message)
{    return new PublishRequest().withTopicArn(topicName).withMessage(message);}
protected void beam_f28428_0()
{    sqsRestServer.stopAndWait();}
public AmazonSQS beam_f28429_0()
{    return client;}
public String beam_f28430_0()
{    return queueUrl;}
public static Write<T> beam_f28438_0()
{    return new AutoValue_DynamoDBIO_Write.Builder().build();}
public Read<T> beam_f28439_0(DynamoDbClientProvider dynamoDbClientProvider)
{    return toBuilder().setDynamoDbClientProvider(dynamoDbClientProvider).build();}
public Read<T> beam_f28440_0(AwsCredentialsProvider credentialsProvider, String region, URI serviceEndpoint)
{    return withDynamoDbClientProvider(new BasicDynamoDbClientProvider(credentialsProvider, region, serviceEndpoint));}
public void beam_f28448_0(@Element Read<T> spec, OutputReceiver<Read<T>> out)
{    ScanRequest scanRequest = spec.getScanRequestFn().apply(null);    for (int i = 0; i < scanRequest.totalSegments(); i++) {        out.output(spec.withSegmentId(i));    }}
public void beam_f28449_0(@Element Read<T> spec, OutputReceiver<T> out)
{    DynamoDbClient client = spec.getDynamoDbClientProvider().getDynamoDbClient();    ScanRequest scanRequest = spec.getScanRequestFn().apply(null);    ScanRequest scanRequestWithSegment = scanRequest.toBuilder().segment(spec.getSegmentId()).build();    ScanResponse scanResponse = client.scan(scanRequestWithSegment);    out.output(spec.getScanResponseMapperFn().apply(scanResponse));}
public List<Map<String, AttributeValue>> beam_f28450_0(@Nullable ScanResponse scanResponse)
{    if (scanResponse == null) {        return Collections.emptyList();    }    return scanResponse.items();}
public PCollection<Void> beam_f28458_0(PCollection<T> input)
{    return input.apply(ParDo.of(new WriteFn<>(this)));}
public void beam_f28459_0()
{    client = spec.getDynamoDbClientProvider().getDynamoDbClient();    retryBackoff = FluentBackoff.DEFAULT.withMaxRetries(    0).withInitialBackoff(RETRY_INITIAL_BACKOFF);    if (spec.getRetryConfiguration() != null) {        retryBackoff = retryBackoff.withMaxRetries(spec.getRetryConfiguration().getMaxAttempts() - 1).withMaxCumulativeBackoff(spec.getRetryConfiguration().getMaxDuration());    }}
public void beam_f28460_0(StartBundleContext context)
{    batch = new ArrayList<>();}
public void beam_f28468_0(AwsCredentialsProvider credentialsProvider, JsonGenerator jsonGenerator, SerializerProvider serializer, TypeSerializer typeSerializer) throws IOException
{    typeSerializer.writeTypePrefixForObject(credentialsProvider, jsonGenerator);    if (credentialsProvider.getClass().equals(StaticCredentialsProvider.class)) {        jsonGenerator.writeStringField(ACCESS_KEY_ID, credentialsProvider.resolveCredentials().accessKeyId());        jsonGenerator.writeStringField(SECRET_ACCESS_KEY, credentialsProvider.resolveCredentials().secretAccessKey());    } else if (!SINGLETON_CREDENTIAL_PROVIDERS.contains(credentialsProvider.getClass())) {        throw new IllegalArgumentException("Unsupported AWS credentials provider type " + credentialsProvider.getClass());    }    typeSerializer.writeTypeSuffixForObject(credentialsProvider, jsonGenerator);}
public ProxyConfiguration beam_f28469_0(JsonParser jsonParser, DeserializationContext context) throws IOException
{    Map<String, String> asMap = jsonParser.readValueAs(new TypeReference<Map<String, String>>() {    });    return ProxyConfiguration.builder().endpoint(URI.create(asMap.get("endpoint"))).username(asMap.get("username")).password(asMap.get("password")).useSystemPropertyValues(Boolean.valueOf(asMap.get("useSystemPropertyValues"))).build();}
public void beam_f28470_0(ProxyConfiguration proxyConfiguration, JsonGenerator jsonGenerator, SerializerProvider serializer) throws IOException
{        final String endpoint = proxyConfiguration.scheme() + "://" + proxyConfiguration.host() + ":" + proxyConfiguration.port();    jsonGenerator.writeStartObject();    jsonGenerator.writeStringField("endpoint", endpoint);    jsonGenerator.writeStringField("username", proxyConfiguration.username());    jsonGenerator.writeStringField("password", proxyConfiguration.password());    jsonGenerator.writeEndObject();}
public void beam_f28478_0() throws IOException
{    AttributeValue expected = AttributeValue.builder().ns(ImmutableList.of("123", "456")).build();    AttributeValueCoder coder = AttributeValueCoder.of();    ByteArrayOutputStream output = new ByteArrayOutputStream();    coder.encode(expected, output);    ByteArrayInputStream in = new ByteArrayInputStream(output.toByteArray());    AttributeValue actual = coder.decode(in);    Assert.assertEquals(expected, actual);}
public void beam_f28479_0() throws IOException
{    AttributeValue expected = AttributeValue.builder().bs(ImmutableList.of(SdkBytes.fromByteArray("mylistbyte1".getBytes(StandardCharsets.UTF_8)), SdkBytes.fromByteArray(("mylistbyte2".getBytes(StandardCharsets.UTF_8))))).build();    AttributeValueCoder coder = AttributeValueCoder.of();    ByteArrayOutputStream output = new ByteArrayOutputStream();    coder.encode(expected, output);    ByteArrayInputStream in = new ByteArrayInputStream(output.toByteArray());    AttributeValue actual = coder.decode(in);    Assert.assertEquals(expected, actual);}
public void beam_f28480_0() throws IOException
{    List<AttributeValue> listAttr = new ArrayList<>();    listAttr.add(AttributeValue.builder().s("innerMapValue1").build());    listAttr.add(AttributeValue.builder().n("8976234").build());    AttributeValue expected = AttributeValue.builder().l(listAttr).build();    AttributeValueCoder coder = AttributeValueCoder.of();    ByteArrayOutputStream output = new ByteArrayOutputStream();    coder.encode(expected, output);    ByteArrayInputStream in = new ByteArrayInputStream(output.toByteArray());    AttributeValue actual = coder.decode(in);    Assert.assertEquals(expected, actual);}
public void beam_f28488_0()
{    DynamoDBIOTestHelper.deleteTestTable(tableName);}
public void beam_f28489_0()
{    List<Map<String, AttributeValue>> expected = DynamoDBIOTestHelper.generateTestData(tableName, numOfItems);    PCollection<List<Map<String, AttributeValue>>> actual = pipeline.apply(DynamoDBIO.<List<Map<String, AttributeValue>>>read().withDynamoDbClientProvider(DynamoDbClientProviderMock.of(DynamoDBIOTestHelper.getDynamoDBClient())).withScanRequestFn((SerializableFunction<Void, ScanRequest>) input -> ScanRequest.builder().tableName(tableName).totalSegments(1).build()).items());    PAssert.that(actual).containsInAnyOrder(expected);    pipeline.run().waitUntilFinish();}
public void beam_f28490_0()
{    TupleTag<List<Map<String, AttributeValue>>> outputTag = new TupleTag<>();    PCollectionTuple writeOutput = pipeline.apply(DynamoDBIO.<List<Map<String, AttributeValue>>>read().withDynamoDbClientProvider(DynamoDbClientProviderMock.of(DynamoDBIOTestHelper.getDynamoDBClient())).withScanRequestFn((SerializableFunction<Void, ScanRequest>) input -> ScanRequest.builder().tableName(tableName).totalSegments(3).build()).items()).apply(ParDo.of(new DoFn<List<Map<String, AttributeValue>>, List<Map<String, AttributeValue>>>() {        @ProcessElement        public void processElement(@Element List<Map<String, AttributeValue>> input, OutputReceiver<List<Map<String, AttributeValue>>> out) {            out.output(input);        }    }).withOutputTags(outputTag, TupleTagList.empty()));    final PCollection<Long> resultSetCount = writeOutput.get(outputTag).apply(Count.globally());        PAssert.that(resultSetCount).containsInAnyOrder(ImmutableList.of(3L));    pipeline.run().waitUntilFinish();}
 static void beam_f28498_0()
{    dynamoContainer.start();    if (dynamoDBClient == null) {        dynamoDBClient = getDynamoDBClient();    }}
 static void beam_f28499_0(String tableName)
{    if (dynamoDBClient != null) {        dynamoDBClient.close();    }    dynamoContainer.stop();}
 static DynamoDbClient beam_f28500_0()
{        return DynamoDbClient.builder().endpointOverride(URI.create(getContainerEndpoint())).region(Region.US_EAST_1).credentialsProvider(StaticCredentialsProvider.create(AwsBasicCredentials.create("accessKey", "secretKey"))).build();}
private static CreateTableResponse beam_f28508_0(String tableName)
{    ImmutableList<AttributeDefinition> attributeDefinitions = ImmutableList.of(AttributeDefinition.builder().attributeName(ATTR_NAME_1).attributeType(ScalarAttributeType.S).build(), AttributeDefinition.builder().attributeName(ATTR_NAME_2).attributeType(ScalarAttributeType.N).build());    ImmutableList<KeySchemaElement> ks = ImmutableList.of(KeySchemaElement.builder().attributeName(ATTR_NAME_1).keyType(KeyType.HASH).build(), KeySchemaElement.builder().attributeName(ATTR_NAME_2).keyType(KeyType.RANGE).build());    ProvisionedThroughput provisionedthroughput = ProvisionedThroughput.builder().readCapacityUnits(1000L).writeCapacityUnits(1000L).build();    CreateTableRequest request = CreateTableRequest.builder().tableName(tableName).attributeDefinitions(attributeDefinitions).keySchema(ks).provisionedThroughput(provisionedthroughput).build();    return dynamoDBClient.createTable(request);}
private static String beam_f28509_0()
{    final String address = dynamoContainer.getContainerIpAddress();    String ipAddress = address;    try {        ipAddress = InetAddress.getByName(address).getHostAddress();    } catch (UnknownHostException ignored) {    }    ipAddress = ipAddress + ".nip.io";    while (true) {        try {                        InetAddress.getAllByName(ipAddress);            break;        } catch (UnknownHostException ignored) {        }    }    return "http://" + ipAddress + ":" + dynamoContainer.getFirstMappedPort();}
public void beam_f28510_0()
{    List<Module> modules = ObjectMapper.findModules(ReflectHelpers.findClassLoader());    assertThat(modules, hasItem(Matchers.instanceOf(AwsModule.class)));}
public Read beam_f28518_0(Duration maxReadTime)
{    return builder().setMaxReadTime(maxReadTime).build();}
public void beam_f28519_0(DisplayData.Builder builder)
{    builder.add(DisplayData.item("addresses", Joiner.on(" ").join(addresses())));}
public PCollection<Message> beam_f28520_0(PBegin input)
{    checkArgument(addresses() != null, "withAddresses() is required");    org.apache.beam.sdk.io.Read.Unbounded<Message> unbounded = org.apache.beam.sdk.io.Read.from(new UnboundedAmqpSource(this));    PTransform<PBegin, PCollection<Message>> transform = unbounded;    if (maxNumRecords() < Long.MAX_VALUE || maxReadTime() != null) {        transform = unbounded.withMaxReadTime(maxReadTime()).withMaxNumRecords(maxNumRecords());    }    return input.getPipeline().apply(transform);}
public Instant beam_f28528_0()
{    if (current == null) {        throw new NoSuchElementException();    }    return currentTimestamp;}
public Message beam_f28529_0()
{    if (current == null) {        throw new NoSuchElementException();    }    return current;}
public UnboundedSource.CheckpointMark beam_f28530_0()
{    return checkpointMark;}
public void beam_f28538_0() throws Exception
{    if (messenger != null) {        messenger.stop();    }}
 static AmqpMessageCoder beam_f28539_0()
{    return new AmqpMessageCoder();}
public void beam_f28540_0(Message value, OutputStream outStream) throws CoderException, IOException
{    for (int maxMessageSize : MESSAGE_SIZES) {        try {            encode(value, outStream, maxMessageSize);            return;        } catch (Exception e) {            continue;        }    }    throw new CoderException("Message is larger than the max size supported by the coder");}
public void beam_f28548_0() throws Exception
{    Message message = Message.Factory.create();    message.setBody(new AmqpValue("body"));    message.setAddress("address");    message.setSubject("test");    AmqpMessageCoder coder = AmqpMessageCoder.of();    Message clone = CoderUtils.clone(coder, message);    assertEquals("AmqpValue{body}", clone.getBody().toString());    assertEquals("address", clone.getAddress());    assertEquals("test", clone.getSubject());}
public void beam_f28549_0() throws Exception
{    thrown.expect(CoderException.class);    Message message = Message.Factory.create();    message.setAddress("address");    message.setSubject("subject");    String body = Joiner.on("").join(Collections.nCopies(64 * 1024 * 1024, " "));    message.setBody(new AmqpValue(body));    AmqpMessageCoder coder = AmqpMessageCoder.of();    byte[] encoded = CoderUtils.encodeToByteArray(coder, message);}
public void beam_f28550_0() throws Exception
{    Message message = Message.Factory.create();    message.setAddress("address");    message.setSubject("subject");    String body = Joiner.on("").join(Collections.nCopies(32 * 1024 * 1024, " "));    message.setBody(new AmqpValue(body));    AmqpMessageCoder coder = AmqpMessageCoder.of();    Message clone = CoderUtils.clone(coder, message);    clone.getBody().toString().equals(message.getBody().toString());}
public void beam_f28558_0(ProcessContext context)
{    context.output(context.element().getValue());}
public static Read<T> beam_f28559_0()
{    return new AutoValue_CassandraIO_Read.Builder<T>().build();}
public static Write<T> beam_f28560_0()
{    return Write.<T>builder(MutationType.WRITE).build();}
public Read<T> beam_f28568_0(String table)
{    checkArgument(table != null, "table can not be null");    return withTable(ValueProvider.StaticValueProvider.of(table));}
public Read<T> beam_f28569_0(ValueProvider<String> table)
{    return builder().setTable(table).build();}
public Read<T> beam_f28570_0(String query)
{    checkArgument(query != null && query.length() > 0, "query cannot be null");    return withQuery(ValueProvider.StaticValueProvider.of(query));}
public Read<T> beam_f28578_0(String localDc)
{    checkArgument(localDc != null, "localDc can not be null");    return withLocalDc(ValueProvider.StaticValueProvider.of(localDc));}
public Read<T> beam_f28579_0(ValueProvider<String> localDc)
{    return builder().setLocalDc(localDc).build();}
public Read<T> beam_f28580_0(String consistencyLevel)
{    checkArgument(consistencyLevel != null, "consistencyLevel can not be null");    return withConsistencyLevel(ValueProvider.StaticValueProvider.of(consistencyLevel));}
public BoundedReader<T> beam_f28588_0(PipelineOptions pipelineOptions)
{    return new CassandraReader(this);}
public List<BoundedSource<T>> beam_f28589_1(long desiredBundleSizeBytes, PipelineOptions pipelineOptions)
{    try (Cluster cluster = getCluster(spec.hosts(), spec.port(), spec.username(), spec.password(), spec.localDc(), spec.consistencyLevel())) {        if (isMurmur3Partitioner(cluster)) {                        return splitWithTokenRanges(spec, desiredBundleSizeBytes, getEstimatedSizeBytes(pipelineOptions), cluster);        } else {                        return Collections.singletonList(new CassandraIO.CassandraSource<>(spec, Collections.singletonList(buildQuery(spec))));        }    }}
private static String beam_f28590_0(Read spec)
{    return (spec.query() == null) ? String.format("SELECT * FROM %s.%s", spec.keyspace().get(), spec.table().get()) : spec.query().get().toString();}
 static double beam_f28598_0(List<TokenRange> tokenRanges)
{    double ringFraction = 0;    for (TokenRange tokenRange : tokenRanges) {        ringFraction = ringFraction + (distance(tokenRange.rangeStart, tokenRange.rangeEnd).doubleValue() / SplitGenerator.getRangeSize(MURMUR3PARTITIONER).doubleValue());    }    return ringFraction;}
 static boolean beam_f28599_0(Cluster cluster)
{    return MURMUR3PARTITIONER.equals(cluster.getMetadata().getPartitioner());}
 static BigInteger beam_f28600_0(BigInteger left, BigInteger right)
{    return (right.compareTo(left) > 0) ? right.subtract(left) : right.subtract(left).add(SplitGenerator.getRangeSize(MURMUR3PARTITIONER));}
public Write<T> beam_f28608_0(List<String> hosts)
{    checkArgument(hosts != null, "CassandraIO." + getMutationTypeName() + "().withHosts(hosts) called with null hosts");    checkArgument(!hosts.isEmpty(), "CassandraIO." + getMutationTypeName() + "().withHosts(hosts) called with empty " + "hosts list");    return withHosts(ValueProvider.StaticValueProvider.of(hosts));}
public Write<T> beam_f28609_0(ValueProvider<List<String>> hosts)
{    return builder().setHosts(hosts).build();}
public Write<T> beam_f28610_0(int port)
{    checkArgument(port > 0, "CassandraIO." + getMutationTypeName() + "().withPort(port) called with invalid port " + "number (%s)", port);    return withPort(ValueProvider.StaticValueProvider.of(port));}
public Write<T> beam_f28618_0(ValueProvider<String> password)
{    return builder().setPassword(password).build();}
public Write<T> beam_f28619_0(String localDc)
{    checkArgument(localDc != null, "CassandraIO." + getMutationTypeName() + "().withLocalDc(localDc) called with null" + " localDc");    return withLocalDc(ValueProvider.StaticValueProvider.of(localDc));}
public Write<T> beam_f28620_0(ValueProvider<String> localDc)
{    return builder().setLocalDc(localDc).build();}
public void beam_f28628_0()
{    writer = new Mutator<>(spec, Mapper::saveAsync, "writes");}
public void beam_f28629_0(ProcessContext c) throws ExecutionException, InterruptedException
{    writer.mutate(c.element());}
public void beam_f28630_0() throws Exception
{    writer.close();    writer = null;}
public Iterator<T> beam_f28638_0(ResultSet resultSet)
{    return mapper.map(resultSet).iterator();}
public Future<Void> beam_f28639_0(T entity)
{    return mapper.deleteAsync(entity);}
public Future<Void> beam_f28640_0(T entity)
{    return mapper.saveAsync(entity);}
private static BigInteger beam_f28648_0(String partitioner)
{    if (partitioner.endsWith("RandomPartitioner")) {        return new BigInteger("2").pow(127).subtract(BigInteger.ONE);    } else if (partitioner.endsWith("Murmur3Partitioner")) {        return new BigInteger("2").pow(63).subtract(BigInteger.ONE);    } else {        throw new UnsupportedOperationException("Unsupported partitioner. " + "Only Random and Murmur3 are supported");    }}
 static BigInteger beam_f28649_0(String partitioner)
{    return getRangeMax(partitioner).subtract(getRangeMin(partitioner)).add(BigInteger.ONE);}
 List<List<RingRange>> beam_f28650_1(long totalSplitCount, List<BigInteger> ringTokens)
{    int tokenRangeCount = ringTokens.size();    List<RingRange> splits = new ArrayList<>();    for (int i = 0; i < tokenRangeCount; i++) {        BigInteger start = ringTokens.get(i);        BigInteger stop = ringTokens.get((i + 1) % tokenRangeCount);        if (!inRange(start) || !inRange(stop)) {            throw new RuntimeException(String.format("Tokens (%s,%s) not in range of %s", start, stop, partitioner));        }        if (start.equals(stop) && tokenRangeCount != 1) {            throw new RuntimeException(String.format("Tokens (%s,%s): two nodes have the same token", start, stop));        }        BigInteger rs = stop.subtract(start);        if (rs.compareTo(BigInteger.ZERO) <= 0) {                        rs = rs.add(rangeSize);        }                        BigInteger[] splitCountAndRemainder = rs.multiply(BigInteger.valueOf(totalSplitCount)).divideAndRemainder(rangeSize);        int splitCount = splitCountAndRemainder[0].intValue() + (splitCountAndRemainder[1].equals(BigInteger.ZERO) ? 0 : 1);                        List<BigInteger> endpointTokens = new ArrayList<>();        for (int j = 0; j <= splitCount; j++) {            BigInteger offset = rs.multiply(BigInteger.valueOf(j)).divide(BigInteger.valueOf(splitCount));            BigInteger token = start.add(offset);            if (token.compareTo(rangeMax) > 0) {                token = token.subtract(rangeSize);            }                                    endpointTokens.add(token.equals(BigInteger.valueOf(Long.MIN_VALUE)) ? token.add(BigInteger.ONE) : token);        }                for (int j = 0; j < splitCount; j++) {            splits.add(new RingRange(endpointTokens.get(j), endpointTokens.get(j + 1)));                    }    }    BigInteger total = BigInteger.ZERO;    for (RingRange split : splits) {        BigInteger size = split.span(rangeSize);        total = total.add(size);    }    if (!total.equals(rangeSize)) {        throw new RuntimeException("Some tokens are missing from the splits. " + "This should not happen.");    }    return coalesceSplits(getTargetSplitSize(totalSplitCount), splits);}
private void beam_f28658_0()
{    PCollection<Scientist> output = pipelineRead.apply(CassandraIO.<Scientist>read().withHosts(options.getCassandraHost()).withPort(options.getCassandraPort()).withMinNumberOfSplits(20).withKeyspace(KEYSPACE).withTable(TABLE).withEntity(Scientist.class).withCoder(SerializableCoder.of(Scientist.class)));    PCollection<String> consolidatedHashcode = output.apply(ParDo.of(new SelectNameFn())).apply("Hash row contents", Combine.globally(new HashingFn()).withoutDefaults());    PAssert.thatSingleton(consolidatedHashcode).isEqualTo(TestRow.getExpectedHashForRowCount(options.getNumberOfRecords()));    pipelineRead.run().waitUntilFinish();}
private static Cluster beam_f28659_0(CassandraIOITOptions options)
{    return Cluster.builder().addContactPoints(options.getCassandraHost().toArray(new String[0])).withPort(options.getCassandraPort()).build();}
private static void beam_f28660_1(CassandraIOITOptions options, String keyspace, String tableName)
{    try (Cluster cluster = getCluster(options);        Session session = cluster.connect()) {                session.execute("CREATE KEYSPACE IF NOT EXISTS " + KEYSPACE + " WITH REPLICATION = " + "{'class':'SimpleStrategy', 'replication_factor':3};");        session.execute("USE " + keyspace);                session.execute("CREATE TABLE IF NOT EXISTS " + tableName + "(id bigint, name text, PRIMARY " + "KEY(id))");    }}
private static void beam_f28668_0() throws Exception
{    JMXServiceURL url = new JMXServiceURL(String.format("service:jmx:rmi://%s/jndi/rmi://%s:%s/jmxrmi", CASSANDRA_HOST, CASSANDRA_HOST, jmxPort));    JMXConnector jmxConnector = JMXConnectorFactory.connect(url, null);    MBeanServerConnection mBeanServerConnection = jmxConnector.getMBeanServerConnection();    ObjectName objectName = new ObjectName(STORAGE_SERVICE_MBEAN);    StorageServiceMBean mBeanProxy = JMX.newMBeanProxy(mBeanServerConnection, objectName, StorageServiceMBean.class);    mBeanProxy.forceKeyspaceFlush(CASSANDRA_KEYSPACE, CASSANDRA_TABLE);    jmxConnector.close();    Thread.sleep(FLUSH_TIMEOUT);}
public void beam_f28669_0() throws Exception
{    PipelineOptions pipelineOptions = PipelineOptionsFactory.create();    CassandraIO.Read<Scientist> read = CassandraIO.<Scientist>read().withHosts(Collections.singletonList(CASSANDRA_HOST)).withPort(cassandraPort).withKeyspace(CASSANDRA_KEYSPACE).withTable(CASSANDRA_TABLE);    CassandraIO.CassandraSource<Scientist> source = new CassandraIO.CassandraSource<>(read, null);    long estimatedSizeBytes = source.getEstimatedSizeBytes(pipelineOptions);        assertTrue((estimatedSizeBytes >= 12960L * 0.9f) && (estimatedSizeBytes <= 12960L * 1.1f));}
public void beam_f28670_0() throws Exception
{    PCollection<Scientist> output = pipeline.apply(CassandraIO.<Scientist>read().withHosts(Collections.singletonList(CASSANDRA_HOST)).withPort(cassandraPort).withKeyspace(CASSANDRA_KEYSPACE).withTable(CASSANDRA_TABLE).withCoder(SerializableCoder.of(Scientist.class)).withEntity(Scientist.class));    PAssert.thatSingleton(output.apply("Count", Count.globally())).isEqualTo(NUM_ROWS);    PCollection<KV<String, Integer>> mapped = output.apply(MapElements.via(new SimpleFunction<Scientist, KV<String, Integer>>() {        @Override        public KV<String, Integer> apply(Scientist scientist) {            return KV.of(scientist.name, scientist.id);        }    }));    PAssert.that(mapped.apply("Count occurrences per scientist", Count.perKey())).satisfies(input -> {        for (KV<String, Long> element : input) {            assertEquals(element.getKey(), NUM_ROWS / 10, element.getValue().longValue());        }        return null;    });    pipeline.run();}
public void beam_f28678_0() throws Exception
{    counter.set(0);    SerializableFunction<Session, Mapper> factory = new NOOPMapperFactory();    pipeline.apply(CassandraIO.<String>read().withHosts(Collections.singletonList(CASSANDRA_HOST)).withPort(cassandraPort).withKeyspace(CASSANDRA_KEYSPACE).withTable(CASSANDRA_TABLE).withCoder(SerializableCoder.of(String.class)).withEntity(String.class).withMapperFactoryFn(factory));    pipeline.run();    assertEquals(NUM_ROWS, counter.intValue());}
public void beam_f28679_0() throws Exception
{    counter.set(0);    SerializableFunction<Session, Mapper> factory = new NOOPMapperFactory();    pipeline.apply(Create.of("")).apply(CassandraIO.<String>write().withHosts(Collections.singletonList(CASSANDRA_HOST)).withPort(cassandraPort).withKeyspace(CASSANDRA_KEYSPACE).withMapperFactoryFn(factory).withEntity(String.class));    pipeline.run();    assertEquals(1, counter.intValue());}
public void beam_f28680_0()
{    counter.set(0);    SerializableFunction<Session, Mapper> factory = new NOOPMapperFactory();    pipeline.apply(Create.of("")).apply(CassandraIO.<String>delete().withHosts(Collections.singletonList(CASSANDRA_HOST)).withPort(cassandraPort).withKeyspace(CASSANDRA_KEYSPACE).withMapperFactoryFn(factory).withEntity(String.class));    pipeline.run();    assertEquals(1, counter.intValue());}
public String beam_f28688_0()
{    return id + ":" + name;}
public boolean beam_f28689_0(Object o)
{    if (this == o) {        return true;    }    if (o == null || getClass() != o.getClass()) {        return false;    }    Scientist scientist = (Scientist) o;    return id == scientist.id && Objects.equal(name, scientist.name);}
public int beam_f28690_0()
{    return Objects.hashCode(name, id);}
public Write<T> beam_f28698_0(@Nullable Boolean value)
{    return toBuilder().insertDistributedSync(value).build();}
public Write<T> beam_f28699_0(@Nullable Long value)
{    return toBuilder().insertQuorum(value).build();}
public Write<T> beam_f28700_0(Boolean value)
{    return toBuilder().insertDeduplicate(value).build();}
public void beam_f28708_0() throws Exception
{    connection.close();}
public void beam_f28709_0()
{    buffer.clear();}
public void beam_f28710_0() throws Exception
{    flush();}
public static TableSchema beam_f28718_0(Column... columns)
{    return new AutoValue_TableSchema(Arrays.asList(columns));}
public static Schema beam_f28719_0(TableSchema tableSchema)
{    return tableSchema.columns().stream().map(x -> {        if (x.columnType().nullable()) {            return Schema.Field.nullable(x.name(), getEquivalentFieldType(x.columnType()));        } else {            return Schema.Field.of(x.name(), getEquivalentFieldType(x.columnType()));        }    }).collect(Schema.toSchema());}
public static Schema.FieldType beam_f28720_0(ColumnType columnType)
{    switch(columnType.typeName()) {        case DATE:        case DATETIME:            return Schema.FieldType.DATETIME;        case STRING:            return Schema.FieldType.STRING;        case FIXEDSTRING:                        int size = columnType.fixedStringSize();            return Schema.FieldType.logicalType(LogicalTypes.FixedBytes.of(size));        case FLOAT32:            return Schema.FieldType.FLOAT;        case FLOAT64:            return Schema.FieldType.DOUBLE;        case INT8:            return Schema.FieldType.BYTE;        case INT16:            return Schema.FieldType.INT16;        case INT32:            return Schema.FieldType.INT32;        case INT64:            return Schema.FieldType.INT64;        case UINT8:            return Schema.FieldType.INT16;        case UINT16:            return Schema.FieldType.INT32;        case UINT32:            return Schema.FieldType.INT64;        case UINT64:            return Schema.FieldType.INT64;        case ARRAY:            return Schema.FieldType.array(getEquivalentFieldType(columnType.arrayElementType()));        case ENUM8:        case ENUM16:            return Schema.FieldType.STRING;    }        throw new AssertionError("Unexpected type: " + columnType.typeName());}
public static ColumnType beam_f28728_0(int size)
{    return ColumnType.builder().typeName(TypeName.FIXEDSTRING).nullable(false).fixedStringSize(size).build();}
public static ColumnType beam_f28729_0(Map<String, Integer> enumValues)
{    return ColumnType.builder().typeName(TypeName.ENUM8).nullable(false).enumValues(enumValues).build();}
public static ColumnType beam_f28730_0(Map<String, Integer> enumValues)
{    return ColumnType.builder().typeName(TypeName.ENUM16).nullable(false).enumValues(enumValues).build();}
 static RangeBundle beam_f28738_0(int size)
{    return new RangeBundle(size);}
public PCollection<Row> beam_f28739_0(PBegin input)
{    Schema schema = Schema.of(Schema.Field.of("f0", Schema.FieldType.INT64));    Iterable<Row> bundle = IntStream.range(0, size).mapToObj(x -> Row.withSchema(schema).addValue((long) x).build()).collect(Collectors.toList());        return input.getPipeline().apply(Create.<Iterable<Row>>of(bundle).withCoder(IterableCoder.of(RowCoder.of(schema)))).apply(Flatten.iterables()).setRowSchema(schema);}
private boolean beam_f28740_0()
{    try {        return pipeline.run().waitUntilFinish() == PipelineResult.State.DONE;    } catch (Pipeline.PipelineExecutionException e) {        return false;    }}
public void beam_f28748_0() throws Exception
{    Schema schema = Schema.of(Schema.Field.nullable("f0", FieldType.INT64));    Row row1 = Row.withSchema(schema).addValue(1L).build();    Row row2 = Row.withSchema(schema).addValue(null).build();    Row row3 = Row.withSchema(schema).addValue(3L).build();    executeSql("CREATE TABLE test_int64_with_default (f0 Int64 DEFAULT -1) ENGINE=Log");    pipeline.apply(Create.of(row1, row2, row3).withRowSchema(schema)).apply(write("test_int64_with_default"));    pipeline.run().waitUntilFinish();    long sum = executeQueryAsLong("SELECT SUM(f0) FROM test_int64_with_default");    assertEquals(3L, sum);}
public void beam_f28749_0() throws Exception
{    Schema schema = Schema.of(Schema.Field.of("f0", FieldType.array(FieldType.array(FieldType.INT64))));    Row row1 = Row.withSchema(schema).addValue(Arrays.asList(Arrays.asList(1L, 2L), Arrays.asList(2L, 3L), Arrays.asList(3L, 4L))).build();    executeSql("CREATE TABLE test_array_of_array_of_int64 (f0 Array(Array(Int64))) ENGINE=Log");    pipeline.apply(Create.of(row1).withRowSchema(schema)).apply(write("test_array_of_array_of_int64"));    pipeline.run().waitUntilFinish();    long sum0 = executeQueryAsLong("SELECT SUM(arraySum(arrayMap(x -> arraySum(x), f0))) " + "FROM test_array_of_array_of_int64");    assertEquals(15L, sum0);}
public void beam_f28750_0() throws Exception
{    Schema schema = Schema.of(Schema.Field.of("f0", FieldType.DATETIME), Schema.Field.of("f1", FieldType.DATETIME), Schema.Field.of("f2", FieldType.FLOAT), Schema.Field.of("f3", FieldType.DOUBLE), Schema.Field.of("f4", FieldType.BYTE), Schema.Field.of("f5", FieldType.INT16), Schema.Field.of("f6", FieldType.INT32), Schema.Field.of("f7", FieldType.INT64), Schema.Field.of("f8", FieldType.STRING), Schema.Field.of("f9", FieldType.INT16), Schema.Field.of("f10", FieldType.INT32), Schema.Field.of("f11", FieldType.INT64), Schema.Field.of("f12", FieldType.INT64), Schema.Field.of("f13", FieldType.STRING), Schema.Field.of("f14", FieldType.STRING), Schema.Field.of("f15", FieldType.STRING), Schema.Field.of("f16", FieldType.BYTES), Schema.Field.of("f17", FieldType.logicalType(LogicalTypes.FixedBytes.of(3))));    Row row1 = Row.withSchema(schema).addValue(new DateTime(2030, 10, 1, 0, 0, 0, DateTimeZone.UTC)).addValue(new DateTime(2030, 10, 9, 8, 7, 6, DateTimeZone.UTC)).addValue(2.2f).addValue(3.3).addValue((byte) 4).addValue((short) 5).addValue(6).addValue(7L).addValue("eight").addValue((short) 9).addValue(10).addValue(11L).addValue(12L).addValue("abc").addValue("cde").addValue("qwe").addValue(new byte[] { 'a', 's', 'd' }).addValue(new byte[] { 'z', 'x', 'c' }).build();    executeSql("CREATE TABLE test_primitive_types (" + "f0  Date," + "f1  DateTime," + "f2  Float32," + "f3  Float64," + "f4  Int8," + "f5  Int16," + "f6  Int32," + "f7  Int64," + "f8  String," + "f9  UInt8," + "f10 UInt16," + "f11 UInt32," + "f12 UInt64," + "f13 Enum8('abc' = 1, 'cde' = 2)," + "f14 Enum16('abc' = -1, 'cde' = -2)," + "f15 FixedString(3)," + "f16 FixedString(3)," + "f17 FixedString(3)" + ") ENGINE=Log");    pipeline.apply(Create.of(row1).withRowSchema(schema)).apply(write("test_primitive_types"));    pipeline.run().waitUntilFinish();    try (ResultSet rs = executeQuery("SELECT * FROM test_primitive_types")) {        rs.next();        assertEquals("2030-10-01", rs.getString("f0"));        assertEquals("2030-10-09 08:07:06", rs.getString("f1"));        assertEquals("2.2", rs.getString("f2"));        assertEquals("3.3", rs.getString("f3"));        assertEquals("4", rs.getString("f4"));        assertEquals("5", rs.getString("f5"));        assertEquals("6", rs.getString("f6"));        assertEquals("7", rs.getString("f7"));        assertEquals("eight", rs.getString("f8"));        assertEquals("9", rs.getString("f9"));        assertEquals("10", rs.getString("f10"));        assertEquals("11", rs.getString("f11"));        assertEquals("12", rs.getString("f12"));        assertEquals("abc", rs.getString("f13"));        assertEquals("cde", rs.getString("f14"));        assertArrayEquals(new byte[] { 'q', 'w', 'e' }, rs.getBytes("f15"));        assertArrayEquals(new byte[] { 'a', 's', 'd' }, rs.getBytes("f16"));        assertArrayEquals(new byte[] { 'z', 'x', 'c' }, rs.getBytes("f17"));    }}
public void beam_f28758_0()
{    assertEquals(ColumnType.DATETIME, ColumnType.parse("DateTime"));}
public void beam_f28759_0()
{    assertEquals(ColumnType.FLOAT32, ColumnType.parse("Float32"));}
public void beam_f28760_0()
{    assertEquals(ColumnType.FLOAT64, ColumnType.parse("Float64"));}
public void beam_f28768_0()
{    assertEquals(ColumnType.UINT64, ColumnType.parse("UInt64"));}
public void beam_f28769_0()
{    assertEquals(ColumnType.STRING, ColumnType.parse("String"));}
public void beam_f28770_0()
{    assertEquals(ColumnType.array(ColumnType.STRING), ColumnType.parse("Array(String)"));}
public void beam_f28778_0()
{    assertEquals(ColumnType.array(ColumnType.array(ColumnType.STRING)), ColumnType.parse("Array(Array(String))"));}
public void beam_f28779_0()
{    assertEquals("abc", ColumnType.parseDefaultExpression(ColumnType.STRING, "CAST('abc' AS String)"));}
public void beam_f28780_0()
{    assertEquals(-1L, ColumnType.parseDefaultExpression(ColumnType.INT64, "CAST(-1 AS Int64)"));}
public static void beam_f28788_0(DataSource dataSource, String stmt) throws SQLException
{    try (Connection connection = dataSource.getConnection()) {        try (Statement statement = connection.createStatement()) {            statement.execute(stmt);        }    }}
private void beam_f28789_0(ObjectInputStream in) throws IOException, ClassNotFoundException
{    in.defaultReadObject();}
private void beam_f28790_0(ObjectOutputStream out) throws IOException
{    out.defaultWriteObject();}
public Accum beam_f28798_0()
{    return new Accum(null);}
public static String beam_f28799_0(int recordCount, Map<Integer, String> hashes)
{    String hash = hashes.get(recordCount);    if (hash == null) {        throw new UnsupportedOperationException(String.format("No hash for that record count: %s", recordCount));    }    return hash;}
public static T beam_f28800_0(Class<T> optionsType)
{    PipelineOptionsFactory.register(optionsType);    PipelineOptions options = TestPipeline.testingPipelineOptions().as(optionsType);    return PipelineOptionsValidator.validate(optionsType, options);}
public void beam_f28808_0() throws Exception
{    startTimeMeasure = System.currentTimeMillis();    executeWithRetry(3, 2_000, IOITHelperTest::recoveringFunctionWithBiggerDelay);    assertEquals(1, listOfExceptionsThrown.size());}
private static void beam_f28809_0() throws SQLException
{    SQLException e = new SQLException("Problem with connection");    listOfExceptionsThrown.add(e);    throw e;}
private static void beam_f28810_0() throws SQLException
{    if (System.currentTimeMillis() - startTimeMeasure < 1_001) {        SQLException e = new SQLException("Problem with connection");        listOfExceptionsThrown.add(e);        throw e;    }}
public static Iterable<TestRow> beam_f28818_0(int rangeStart, int rangeEnd)
{    List<TestRow> ret = new ArrayList<>(rangeEnd - rangeStart + 1);    for (int i = rangeStart; i < rangeEnd; i++) {        ret.add(fromSeed(i));    }    return ret;}
public void beam_f28819_0(ProcessContext c)
{    c.output(fromSeed(c.element().intValue()));}
public void beam_f28820_0(ProcessContext c)
{    c.output(c.element().name());}
public ConnectionConfiguration beam_f28828_0(String password)
{    checkArgument(password != null, "password can not be null");    checkArgument(!password.isEmpty(), "password can not be empty");    return builder().setPassword(password).build();}
public ConnectionConfiguration beam_f28829_0(String keystorePath)
{    checkArgument(keystorePath != null, "keystorePath can not be null");    checkArgument(!keystorePath.isEmpty(), "keystorePath can not be empty");    return builder().setKeystorePath(keystorePath).build();}
public ConnectionConfiguration beam_f28830_0(String keystorePassword)
{    checkArgument(keystorePassword != null, "keystorePassword can not be null");    return builder().setKeystorePassword(keystorePassword).build();}
public Read beam_f28838_0(ValueProvider<String> query)
{    checkArgument(query != null, "query can not be null");    return builder().setQuery(query).build();}
public Read beam_f28839_0()
{    return builder().setWithMetadata(true).build();}
public Read beam_f28840_0(String scrollKeepalive)
{    checkArgument(scrollKeepalive != null, "scrollKeepalive can not be null");    checkArgument(!"0m".equals(scrollKeepalive), "scrollKeepalive can not be 0m");    return builder().setScrollKeepalive(scrollKeepalive).build();}
public BoundedReader<String> beam_f28848_0(PipelineOptions options)
{    return new BoundedElasticsearchReader(this);}
public void beam_f28849_0()
{    spec.validate(null);}
public Coder<String> beam_f28850_0()
{    return StringUtf8Coder.of();}
public BoundedSource<String> beam_f28858_0()
{    return source;}
public static RetryConfiguration beam_f28859_0(int maxAttempts, Duration maxDuration)
{    checkArgument(maxAttempts > 0, "maxAttempts must be greater than 0");    checkArgument(maxDuration != null && maxDuration.isLongerThan(Duration.ZERO), "maxDuration must be greater than 0");    return new AutoValue_ElasticsearchIO_RetryConfiguration.Builder().setMaxAttempts(maxAttempts).setMaxDuration(maxDuration).setRetryPredicate(DEFAULT_RETRY_PREDICATE).build();}
 RetryConfiguration beam_f28860_0(RetryPredicate predicate)
{    checkArgument(predicate != null, "predicate must be provided");    return builder().setRetryPredicate(predicate).build();}
public Write beam_f28868_0(FieldValueExtractFn typeFn)
{    checkArgument(typeFn != null, "typeFn must not be null");    return builder().setTypeFn(typeFn).build();}
public Write beam_f28869_0(boolean usePartialUpdate)
{    return builder().setUsePartialUpdate(usePartialUpdate).build();}
public Write beam_f28870_0(RetryConfiguration retryConfiguration)
{    checkArgument(retryConfiguration != null, "retryConfiguration is required");    return builder().setRetryConfiguration(retryConfiguration).build();}
private void beam_f28878_0() throws IOException, InterruptedException
{    if (batch.isEmpty()) {        return;    }    StringBuilder bulkRequest = new StringBuilder();    for (String json : batch) {        bulkRequest.append(json);    }    batch.clear();    currentBatchSizeBytes = 0;    Response response;    HttpEntity responseEntity;                String endPoint = String.format("/%s/%s/_bulk", spec.getConnectionConfiguration().getIndex(), spec.getConnectionConfiguration().getType());    HttpEntity requestBody = new NStringEntity(bulkRequest.toString(), ContentType.APPLICATION_JSON);    response = restClient.performRequest("POST", endPoint, Collections.emptyMap(), requestBody);    responseEntity = new BufferedHttpEntity(response.getEntity());    if (spec.getRetryConfiguration() != null && spec.getRetryConfiguration().getRetryPredicate().test(responseEntity)) {        responseEntity = handleRetry("POST", endPoint, Collections.emptyMap(), requestBody);    }    checkForErrors(responseEntity, backendVersion, spec.getUsePartialUpdate());}
private HttpEntity beam_f28879_1(String method, String endpoint, Map<String, String> params, HttpEntity requestBody) throws IOException, InterruptedException
{    Response response;    HttpEntity responseEntity;    Sleeper sleeper = Sleeper.DEFAULT;    BackOff backoff = retryBackoff.backoff();    int attempt = 0;        while (BackOffUtils.next(sleeper, backoff)) {                response = restClient.performRequest(method, endpoint, params, requestBody);        responseEntity = new BufferedHttpEntity(response.getEntity());                if (!spec.getRetryConfiguration().getRetryPredicate().test(responseEntity)) {            return responseEntity;        }    }    throw new IOException(String.format(RETRY_FAILED_LOG, attempt));}
public void beam_f28880_0() throws IOException
{    if (restClient != null) {        restClient.close();    }}
public void beam_f28888_0() throws Exception
{        ElasticsearchIOTestCommon elasticsearchIOTestCommonWrite = new ElasticsearchIOTestCommon(writeConnectionConfiguration, restClient, true);    elasticsearchIOTestCommonWrite.setPipeline(pipeline);    elasticsearchIOTestCommonWrite.testWriteWithFullAddressing();}
public void beam_f28889_0() throws Exception
{    ElasticsearchIOTestUtils.copyIndex(restClient, readConnectionConfiguration.getIndex(), updateConnectionConfiguration.getIndex());        ElasticsearchIOTestCommon elasticsearchIOTestCommonUpdate = new ElasticsearchIOTestCommon(updateConnectionConfiguration, restClient, true);    elasticsearchIOTestCommonUpdate.setPipeline(pipeline);    elasticsearchIOTestCommonUpdate.testWritePartialUpdate();}
public static void beam_f28890_1() throws IOException
{    esHttpPort = NetworkTestHelper.getAvailableLocalPort();        Settings.Builder settingsBuilder = Settings.settingsBuilder().put("cluster.name", "beam").put("http.enabled", "true").put("node.data", "true").put("path.data", TEMPORARY_FOLDER.getRoot().getPath()).put("path.home", TEMPORARY_FOLDER.getRoot().getPath()).put("node.name", "beam").put("network.host", ES_IP).put("http.port", esHttpPort).put("index.store.stats_refresh_interval", 0).put("threadpool.bulk.queue_size", 400);    node = new Node(settingsBuilder.build());        node.start();    connectionConfiguration = ConnectionConfiguration.create(new String[] { "http://" + ES_IP + ":" + esHttpPort }, getEsIndex(), ES_TYPE).withSocketAndRetryTimeout(120000).withConnectTimeout(5000);    restClient = connectionConfiguration.createClient();    elasticsearchIOTestCommon = new ElasticsearchIOTestCommon(connectionConfiguration, restClient, false);    int waitingTime = 0;    int healthCheckFrequency = 500;    while ((waitingTime < MAX_STARTUP_WAITING_TIME_MSEC) && restClient.performRequest("HEAD", "/").getStatusLine().getStatusCode() != 200) {        try {            Thread.sleep(healthCheckFrequency);            waitingTime += healthCheckFrequency;        } catch (InterruptedException e) {                    }    }    if (waitingTime >= MAX_STARTUP_WAITING_TIME_MSEC) {        throw new IOException("Max startup waiting for embedded Elasticsearch to start was exceeded");    }}
public void beam_f28898_0() throws Exception
{    elasticsearchIOTestCommon.setExpectedException(expectedException);    elasticsearchIOTestCommon.testWriteWithErrors();}
public void beam_f28899_0() throws Exception
{    elasticsearchIOTestCommon.testWriteWithMaxBatchSize();}
public void beam_f28900_0() throws Exception
{    elasticsearchIOTestCommon.testWriteWithMaxBatchSizeBytes();}
public void beam_f28908_0() throws Exception
{    elasticsearchIOTestCommon.setPipeline(pipeline);    elasticsearchIOTestCommon.testReadWithMetadata();}
public void beam_f28909_0() throws IOException
{    elasticsearchIOTestCommon.testDefaultRetryPredicate(restClient);}
public void beam_f28910_0() throws Throwable
{    elasticsearchIOTestCommon.setExpectedException(expectedException);    elasticsearchIOTestCommon.setPipeline(pipeline);    elasticsearchIOTestCommon.testWriteRetry();}
public void beam_f28918_0() throws Exception
{        ElasticsearchIOTestCommon elasticsearchIOTestCommonWrite = new ElasticsearchIOTestCommon(writeConnectionConfiguration, restClient, true);    elasticsearchIOTestCommonWrite.setPipeline(pipeline);    elasticsearchIOTestCommonWrite.testWriteWithFullAddressing();}
public void beam_f28919_0() throws Exception
{    ElasticsearchIOTestUtils.copyIndex(restClient, readConnectionConfiguration.getIndex(), updateConnectionConfiguration.getIndex());        ElasticsearchIOTestCommon elasticsearchIOTestCommonUpdate = new ElasticsearchIOTestCommon(updateConnectionConfiguration, restClient, true);    elasticsearchIOTestCommonUpdate.setPipeline(pipeline);    elasticsearchIOTestCommonUpdate.testWritePartialUpdate();}
private String[] beam_f28920_0()
{    ArrayList<String> result = new ArrayList<>();    for (InetSocketAddress address : cluster().httpAddresses()) {        result.add(String.format("http://%s:%s", address.getHostString(), address.getPort()));    }    return result.toArray(new String[result.size()]);}
public void beam_f28928_0() throws Exception
{            createIndex(getEsIndex());    elasticsearchIOTestCommon.setPipeline(pipeline);    elasticsearchIOTestCommon.testReadWithQueryValueProvider();}
public void beam_f28929_0() throws Exception
{    elasticsearchIOTestCommon.setPipeline(pipeline);    elasticsearchIOTestCommon.testWrite();}
public void beam_f28930_0() throws Exception
{    elasticsearchIOTestCommon.setExpectedException(expectedException);    elasticsearchIOTestCommon.testWriteWithErrors();}
public void beam_f28938_0() throws Exception
{    elasticsearchIOTestCommon.setPipeline(pipeline);    elasticsearchIOTestCommon.testWritePartialUpdate();}
public void beam_f28939_0() throws Exception
{        ConnectionConfiguration connectionConfiguration = ConnectionConfiguration.create(fillAddresses(), UPDATE_INDEX, UPDATE_TYPE);    ElasticsearchIOTestCommon elasticsearchIOTestCommonWithErrors = new ElasticsearchIOTestCommon(connectionConfiguration, getRestClient(), false);    elasticsearchIOTestCommonWithErrors.setPipeline(pipeline);    elasticsearchIOTestCommonWithErrors.testWritePartialUpdateWithErrors();}
public void beam_f28940_0() throws Exception
{    elasticsearchIOTestCommon.setPipeline(pipeline);    elasticsearchIOTestCommon.testReadWithMetadata();}
public void beam_f28949_0() throws Exception
{        ElasticsearchIOTestCommon elasticsearchIOTestCommonWrite = new ElasticsearchIOTestCommon(writeConnectionConfiguration, restClient, true);    elasticsearchIOTestCommonWrite.setPipeline(pipeline);    elasticsearchIOTestCommonWrite.testWrite();}
public void beam_f28950_0() throws Exception
{    elasticsearchIOTestCommon.testSizes();}
public void beam_f28951_0() throws Exception
{        ElasticsearchIOTestCommon elasticsearchIOTestCommonWrite = new ElasticsearchIOTestCommon(writeConnectionConfiguration, restClient, true);    elasticsearchIOTestCommonWrite.setPipeline(pipeline);    elasticsearchIOTestCommonWrite.testWriteWithFullAddressing();}
public void beam_f28959_0() throws Exception
{            createIndex(getEsIndex());    elasticsearchIOTestCommon.setPipeline(pipeline);    elasticsearchIOTestCommon.testRead();}
public void beam_f28960_0() throws Exception
{            createIndex(getEsIndex());    elasticsearchIOTestCommon.setPipeline(pipeline);    elasticsearchIOTestCommon.testReadWithQueryString();}
public void beam_f28961_0() throws Exception
{            createIndex(getEsIndex());    elasticsearchIOTestCommon.setPipeline(pipeline);    elasticsearchIOTestCommon.testReadWithQueryValueProvider();}
public void beam_f28969_0() throws Exception
{    elasticsearchIOTestCommon.setPipeline(pipeline);    elasticsearchIOTestCommon.testWriteWithFullAddressing();}
public void beam_f28970_0() throws Exception
{    elasticsearchIOTestCommon.setPipeline(pipeline);    elasticsearchIOTestCommon.testWritePartialUpdate();}
public void beam_f28971_0() throws Exception
{        ConnectionConfiguration connectionConfiguration = ConnectionConfiguration.create(fillAddresses(), UPDATE_INDEX, UPDATE_TYPE);    ElasticsearchIOTestCommon elasticsearchIOTestCommonWithErrors = new ElasticsearchIOTestCommon(connectionConfiguration, getRestClient(), false);    elasticsearchIOTestCommonWithErrors.setPipeline(pipeline);    elasticsearchIOTestCommonWithErrors.testWritePartialUpdateWithErrors();}
 static ConnectionConfiguration beam_f28980_0(ElasticsearchPipelineOptions options, IndexMode mode)
{    return ConnectionConfiguration.create(new String[] { "http://" + options.getElasticsearchServer() + ":" + options.getElasticsearchHttpPort() }, mode.getIndex(), ES_TYPE);}
 static String beam_f28981_0()
{    return "beam" + Thread.currentThread().getId();}
 void beam_f28982_0(TestPipeline pipeline)
{    this.pipeline = pipeline;}
 void beam_f28990_0() throws Exception
{    if (!useAsITests) {        ElasticsearchIOTestUtils.insertTestDocuments(connectionConfiguration, 1, restClient);    }    PCollection<String> output = pipeline.apply(ElasticsearchIO.read().withConnectionConfiguration(connectionConfiguration).withMetadata());    PAssert.that(output).satisfies(new ContainsStringCheckerFn("\"_id\":\"0\""));    pipeline.run();}
 void beam_f28991_0() throws Exception
{    Write write = ElasticsearchIO.write().withConnectionConfiguration(connectionConfiguration);    executeWriteTest(write);}
 void beam_f28992_0() throws Exception
{    Write write = ElasticsearchIO.write().withConnectionConfiguration(connectionConfiguration).withMaxBatchSize(BATCH_SIZE);    List<String> input = ElasticsearchIOTestUtils.createDocuments(numDocs, ElasticsearchIOTestUtils.InjectionMode.INJECT_SOME_INVALID_DOCS);    expectedException.expect(isA(IOException.class));    expectedException.expectMessage(new CustomMatcher<String>("RegExp matcher") {        @Override        public boolean matches(Object o) {            String message = (String) o;                        return message.matches("(?is).*Error writing to Elasticsearch, some elements could not be inserted" + ".*Document id .+: failed to parse \\(.+\\).*Caused by: .+ \\(.+\\).*" + "Document id .+: failed to parse \\(.+\\).*Caused by: .+ \\(.+\\).*");        }    });        try (DoFnTester<String, Void> fnTester = DoFnTester.of(new Write.WriteFn(write))) {                fnTester.processBundle(input);    }}
 void beam_f29000_0() throws Exception
{        long adjustedNumDocs = (numDocs & 1) == 0 ? numDocs : numDocs + 1;    List<String> data = ElasticsearchIOTestUtils.createDocuments(adjustedNumDocs, ElasticsearchIOTestUtils.InjectionMode.DO_NOT_INJECT_INVALID_DOCS);    pipeline.apply(Create.of(data)).apply(ElasticsearchIO.write().withConnectionConfiguration(connectionConfiguration).withTypeFn(new Modulo2ValueFn("id")));    pipeline.run();    for (int i = 0; i < 2; i++) {        String type = "TYPE_" + i;        long count = refreshIndexAndGetCurrentNumDocs(restClient, connectionConfiguration.getIndex(), type);        assertEquals(type + " holds incorrect count", adjustedNumDocs / 2, count);    }}
 void beam_f29001_0() throws Exception
{    List<String> data = ElasticsearchIOTestUtils.createDocuments(numDocs, ElasticsearchIOTestUtils.InjectionMode.DO_NOT_INJECT_INVALID_DOCS);    pipeline.apply(Create.of(data)).apply(ElasticsearchIO.write().withConnectionConfiguration(connectionConfiguration).withIdFn(new ExtractValueFn("id")).withIndexFn(new ExtractValueFn("scientist")).withTypeFn(new Modulo2ValueFn("scientist")));    pipeline.run();    for (String scientist : FAMOUS_SCIENTISTS) {        String index = scientist.toLowerCase();        for (int i = 0; i < 2; i++) {            String type = "TYPE_" + scientist.hashCode() % 2;            long count = refreshIndexAndGetCurrentNumDocs(restClient, index, type);            assertEquals("Incorrect count for " + index + "/" + type, numDocs / NUM_SCIENTISTS, count);        }    }}
 void beam_f29002_0() throws Exception
{    if (!useAsITests) {        ElasticsearchIOTestUtils.insertTestDocuments(connectionConfiguration, numDocs, restClient);    }        long currentNumDocs = refreshIndexAndGetCurrentNumDocs(connectionConfiguration, restClient);    assertEquals(numDocs, currentNumDocs);        List<String> data = new ArrayList<>();    for (int i = 0; i < numDocs; i++) {        data.add(String.format("{\"id\" : %s, \"group\" : %s}", i, i % 2));    }    pipeline.apply(Create.of(data)).apply(ElasticsearchIO.write().withConnectionConfiguration(connectionConfiguration).withIdFn(new ExtractValueFn("id")).withUsePartialUpdate(true));    pipeline.run();    currentNumDocs = refreshIndexAndGetCurrentNumDocs(connectionConfiguration, restClient);        assertEquals(numDocs, currentNumDocs);    assertEquals(numDocs / NUM_SCIENTISTS, countByScientistName(connectionConfiguration, restClient, "Einstein"));        assertEquals(numDocs / 2, countByMatch(connectionConfiguration, restClient, "group", "0"));    assertEquals(numDocs / 2, countByMatch(connectionConfiguration, restClient, "group", "1"));}
private static void beam_f29010_0(RestClient restClient, String index) throws IOException
{    restClient.performRequest("POST", String.format("/%s/_close", index));}
private static void beam_f29011_0(RestClient restClient, String index) throws IOException
{    try {        closeIndex(restClient, index);        restClient.performRequest("DELETE", String.format("/%s", index), Collections.singletonMap("refresh", "wait_for"));    } catch (IOException e) {                if (!e.getMessage().contains("index_not_found_exception")) {            throw e;        }    }}
 static void beam_f29012_0(RestClient restClient, String source, String target) throws IOException
{    deleteIndex(restClient, target);    HttpEntity entity = new NStringEntity(String.format("{\"source\" : { \"index\" : \"%s\" }, \"dest\" : { \"index\" : \"%s\" } }", source, target), ContentType.APPLICATION_JSON);    restClient.performRequest("POST", "/_reindex", Collections.singletonMap("refresh", "wait_for"), entity);}
public static void beam_f29020_0()
{    FileBasedIOTestPipelineOptions options = readFileBasedIOITPipelineOptions();    numberOfTextLines = options.getNumberOfRecords();    filenamePrefix = appendTimestampSuffix(options.getFilenamePrefix());    bigQueryDataset = options.getBigQueryDataset();    bigQueryTable = options.getBigQueryTable();}
public void beam_f29021_0()
{    PCollection<String> testFilenames = pipeline.apply("Generate sequence", GenerateSequence.from(0).to(numberOfTextLines)).apply("Produce text lines", ParDo.of(new FileBasedIOITHelper.DeterministicallyConstructTestTextLineFn())).apply("Produce Avro records", ParDo.of(new DeterministicallyConstructAvroRecordsFn())).setCoder(AvroCoder.of(AVRO_SCHEMA)).apply("Collect start time", ParDo.of(new TimeMonitor<>(AVRO_NAMESPACE, "writeStart"))).apply("Write Avro records to files", AvroIO.writeGenericRecords(AVRO_SCHEMA).to(filenamePrefix).withOutputFilenames().withSuffix(".avro")).getPerDestinationOutputFilenames().apply("Collect middle time", ParDo.of(new TimeMonitor<>(AVRO_NAMESPACE, "middlePoint"))).apply(Values.create());    PCollection<String> consolidatedHashcode = testFilenames.apply("Match all files", FileIO.matchAll()).apply("Read matches", FileIO.readMatches().withDirectoryTreatment(DirectoryTreatment.PROHIBIT)).apply("Read files", AvroIO.readFilesGenericRecords(AVRO_SCHEMA)).apply("Collect end time", ParDo.of(new TimeMonitor<>(AVRO_NAMESPACE, "endPoint"))).apply("Parse Avro records to Strings", ParDo.of(new ParseAvroRecordsFn())).apply("Calculate hashcode", Combine.globally(new HashingFn()));    String expectedHash = getExpectedHashForLineCount(numberOfTextLines);    PAssert.thatSingleton(consolidatedHashcode).isEqualTo(expectedHash);    testFilenames.apply("Delete test files", ParDo.of(new DeleteFileFn()).withSideInputs(consolidatedHashcode.apply(View.asSingleton())));    PipelineResult result = pipeline.run();    result.waitUntilFinish();    collectAndPublishMetrics(result);}
private void beam_f29022_0(PipelineResult result)
{    String uuid = UUID.randomUUID().toString();    String timestamp = Timestamp.now().toString();    Set<Function<MetricsReader, NamedTestResult>> metricSuppliers = fillMetricSuppliers(uuid, timestamp);    new IOITMetrics(metricSuppliers, result, AVRO_NAMESPACE, uuid, timestamp).publish(bigQueryDataset, bigQueryTable);}
public void beam_f29030_0(ProcessContext c) throws IOException
{    MatchResult match = Iterables.getOnlyElement(FileSystems.match(Collections.singletonList(c.element())));    Set<ResourceId> resourceIds = new HashSet<>();    for (MatchResult.Metadata metadataElem : match.metadata()) {        resourceIds.add(metadataElem.resourceId());    }    FileSystems.delete(resourceIds);}
public static void beam_f29031_0()
{    FileBasedIOTestPipelineOptions options = readFileBasedIOITPipelineOptions();    numberOfRecords = options.getNumberOfRecords();    filenamePrefix = appendTimestampSuffix(options.getFilenamePrefix());    bigQueryDataset = options.getBigQueryDataset();    bigQueryTable = options.getBigQueryTable();}
public void beam_f29032_0()
{    PCollection<String> testFiles = pipeline.apply("Generate sequence", GenerateSequence.from(0).to(numberOfRecords)).apply("Produce text lines", ParDo.of(new FileBasedIOITHelper.DeterministicallyConstructTestTextLineFn())).apply("Produce Avro records", ParDo.of(new DeterministicallyConstructAvroRecordsFn())).setCoder(AvroCoder.of(SCHEMA)).apply("Gather write start times", ParDo.of(new TimeMonitor<>(PARQUET_NAMESPACE, "writeStart"))).apply("Write Parquet files", FileIO.<GenericRecord>write().via(ParquetIO.sink(SCHEMA)).to(filenamePrefix)).getPerDestinationOutputFilenames().apply("Gather write end times", ParDo.of(new TimeMonitor<>(PARQUET_NAMESPACE, "writeEnd"))).apply("Get file names", Values.create());    PCollection<String> consolidatedHashcode = testFiles.apply("Find files", FileIO.matchAll()).apply("Read matched files", FileIO.readMatches()).apply("Gather read start time", ParDo.of(new TimeMonitor<>(PARQUET_NAMESPACE, "readStart"))).apply("Read parquet files", ParquetIO.readFiles(SCHEMA)).apply("Gather read end time", ParDo.of(new TimeMonitor<>(PARQUET_NAMESPACE, "readEnd"))).apply("Map records to strings", MapElements.into(strings()).via((SerializableFunction<GenericRecord, String>) record -> String.valueOf(record.get("row")))).apply("Calculate hashcode", Combine.globally(new HashingFn()));    String expectedHash = getExpectedHashForLineCount(numberOfRecords);    PAssert.thatSingleton(consolidatedHashcode).isEqualTo(expectedHash);    testFiles.apply("Delete test files", ParDo.of(new FileBasedIOITHelper.DeleteFileFn()).withSideInputs(consolidatedHashcode.apply(View.asSingleton())));    PipelineResult result = pipeline.run();    result.waitUntilFinish();    collectAndPublishMetrics(result);}
public static void beam_f29040_0()
{    FileBasedIOTestPipelineOptions options = readFileBasedIOITPipelineOptions();    numberOfTextLines = options.getNumberOfRecords();    filenamePrefix = appendTimestampSuffix(options.getFilenamePrefix());    compressionType = Compression.valueOf(options.getCompressionType());    bigQueryDataset = options.getBigQueryDataset();    bigQueryTable = options.getBigQueryTable();}
private static String beam_f29041_0()
{    return filenamePrefix + "*";}
public void beam_f29042_0()
{    TFRecordIO.Write writeTransform = TFRecordIO.write().to(filenamePrefix).withCompression(compressionType).withSuffix(".tfrecord");    writePipeline.apply("Generate sequence", GenerateSequence.from(0).to(numberOfTextLines)).apply("Produce text lines", ParDo.of(new FileBasedIOITHelper.DeterministicallyConstructTestTextLineFn())).apply("Transform strings to bytes", MapElements.via(new StringToByteArray())).apply("Record time before writing", ParDo.of(new TimeMonitor<>(TFRECORD_NAMESPACE, "writeTime"))).apply("Write content to files", writeTransform);    writePipeline.run().waitUntilFinish();    String filenamePattern = createFilenamePattern();    PCollection<String> consolidatedHashcode = readPipeline.apply(TFRecordIO.read().from(filenamePattern).withCompression(AUTO)).apply("Record time after reading", ParDo.of(new TimeMonitor<>(TFRECORD_NAMESPACE, "readTime"))).apply("Transform bytes to strings", MapElements.via(new ByteArrayToString())).apply("Calculate hashcode", Combine.globally(new HashingFn())).apply(Reshuffle.viaRandomKey());    String expectedHash = getExpectedHashForLineCount(numberOfTextLines);    PAssert.thatSingleton(consolidatedHashcode).isEqualTo(expectedHash);    readPipeline.apply(Create.of(filenamePattern)).apply("Delete test files", ParDo.of(new DeleteFileFn()).withSideInputs(consolidatedHashcode.apply(View.asSingleton())));    PipelineResult result = readPipeline.run();    result.waitUntilFinish();    collectAndPublishMetrics(result);}
private Set<Function<MetricsReader, NamedTestResult>> beam_f29050_0(String uuid, String timestamp)
{    Set<Function<MetricsReader, NamedTestResult>> suppliers = new HashSet<>();    suppliers.add(reader -> {        long writeStart = reader.getStartTimeMetric("writeStart");        long writeEnd = reader.getEndTimeMetric("writeEnd");        double writeTime = (writeEnd - writeStart) / 1e3;        return NamedTestResult.create(uuid, timestamp, "write_time", writeTime);    });    suppliers.add(reader -> {        long readStart = reader.getStartTimeMetric("readStart");        long readEnd = reader.getEndTimeMetric("readEnd");        double readTime = (readEnd - readStart) / 1e3;        return NamedTestResult.create(uuid, timestamp, "read_time", readTime);    });    suppliers.add(reader -> {        long writeStart = reader.getStartTimeMetric("writeStart");        long readEnd = reader.getEndTimeMetric("readEnd");        double runTime = (readEnd - writeStart) / 1e3;        return NamedTestResult.create(uuid, timestamp, "run_time", runTime);    });    return suppliers;}
public Bird beam_f29051_0(Long input)
{    return new Bird("Testing", "Bird number " + input);}
public String beam_f29052_0(Bird input)
{    return input.toString();}
 void beam_f29060_0(BigQueryServices bigQueryServices)
{    this.bigQueryServices = bigQueryServices;}
public int beam_f29061_0()
{    return maxNumWritersPerBundle;}
public void beam_f29062_0(int maxNumWritersPerBundle)
{    this.maxNumWritersPerBundle = maxNumWritersPerBundle;}
public void beam_f29070_0(PipelineOptions options)
{        String tempLocation;    if (customGcsTempLocation == null) {        tempLocation = options.getTempLocation();    } else {        if (!customGcsTempLocation.isAccessible()) {                        return;        }        tempLocation = customGcsTempLocation.get();    }    checkArgument(!Strings.isNullOrEmpty(tempLocation), "BigQueryIO.Write needs a GCS temp location to store temp files.");    if (bigQueryServices == null) {        try {            GcsPath.fromUri(tempLocation);        } catch (IllegalArgumentException e) {            throw new IllegalArgumentException(String.format("BigQuery temp location expected a valid 'gs://' path, but was given '%s'", tempLocation), e);        }    }}
private WriteResult beam_f29071_0(PCollection<KV<DestinationT, ElementT>> input)
{    checkArgument(numFileShards > 0);    Pipeline p = input.getPipeline();    final PCollectionView<String> loadJobIdPrefixView = createLoadJobIdPrefixView(p);    final PCollectionView<String> tempFilePrefixView = createTempFilePrefixView(p, loadJobIdPrefixView);                            PCollection<KV<DestinationT, ElementT>> inputInGlobalWindow = input.apply("rewindowIntoGlobal", Window.<KV<DestinationT, ElementT>>into(new GlobalWindows()).triggering(Repeatedly.forever(AfterFirst.of(AfterProcessingTime.pastFirstElementInPane().plusDelayOf(triggeringFrequency), AfterPane.elementCountAtLeast(FILE_TRIGGERING_RECORD_COUNT)))).discardingFiredPanes());    PCollection<WriteBundlesToFiles.Result<DestinationT>> results = writeShardedFiles(inputInGlobalWindow, tempFilePrefixView);        results = results.apply("applyUserTrigger", Window.<WriteBundlesToFiles.Result<DestinationT>>into(new GlobalWindows()).triggering(Repeatedly.forever(AfterProcessingTime.pastFirstElementInPane().plusDelayOf(triggeringFrequency))).discardingFiredPanes());    TupleTag<KV<ShardedKey<DestinationT>, List<String>>> multiPartitionsTag = new TupleTag<>("multiPartitionsTag");    TupleTag<KV<ShardedKey<DestinationT>, List<String>>> singlePartitionTag = new TupleTag<>("singlePartitionTag");                PCollectionTuple partitions = results.apply("AttachSingletonKey", WithKeys.of((Void) null)).setCoder(KvCoder.of(VoidCoder.of(), WriteBundlesToFiles.ResultCoder.of(destinationCoder))).apply("GroupOntoSingleton", GroupByKey.create()).apply("ExtractResultValues", Values.create()).apply("WritePartitionTriggered", ParDo.of(new WritePartition<>(singletonTable, dynamicDestinations, tempFilePrefixView, maxFilesPerPartition, maxBytesPerPartition, multiPartitionsTag, singlePartitionTag)).withSideInputs(tempFilePrefixView).withOutputTags(multiPartitionsTag, TupleTagList.of(singlePartitionTag)));    PCollection<KV<TableDestination, String>> tempTables = writeTempTables(partitions.get(multiPartitionsTag), loadJobIdPrefixView);    tempTables.apply(Window.<KV<TableDestination, String>>into(new GlobalWindows()).triggering(Repeatedly.forever(AfterPane.elementCountAtLeast(1)))).apply(WithKeys.of((Void) null)).setCoder(KvCoder.of(VoidCoder.of(), KvCoder.of(TableDestinationCoderV2.of(), StringUtf8Coder.of()))).apply(GroupByKey.create()).apply(Values.create()).apply("WriteRenameTriggered", ParDo.of(new WriteRename(bigQueryServices, loadJobIdPrefixView, writeDisposition, createDisposition, maxRetryJobs, kmsKey)).withSideInputs(loadJobIdPrefixView));    writeSinglePartition(partitions.get(singlePartitionTag), loadJobIdPrefixView);    return writeResult(p);}
public WriteResult beam_f29072_0(PCollection<KV<DestinationT, ElementT>> input)
{    Pipeline p = input.getPipeline();    final PCollectionView<String> loadJobIdPrefixView = createLoadJobIdPrefixView(p);    final PCollectionView<String> tempFilePrefixView = createTempFilePrefixView(p, loadJobIdPrefixView);    PCollection<KV<DestinationT, ElementT>> inputInGlobalWindow = input.apply("rewindowIntoGlobal", Window.<KV<DestinationT, ElementT>>into(new GlobalWindows()).triggering(DefaultTrigger.of()).discardingFiredPanes());    PCollection<WriteBundlesToFiles.Result<DestinationT>> results = (numFileShards == 0) ? writeDynamicallyShardedFiles(inputInGlobalWindow, tempFilePrefixView) : writeShardedFiles(inputInGlobalWindow, tempFilePrefixView);    TupleTag<KV<ShardedKey<DestinationT>, List<String>>> multiPartitionsTag = new TupleTag<KV<ShardedKey<DestinationT>, List<String>>>("multiPartitionsTag") {    };    TupleTag<KV<ShardedKey<DestinationT>, List<String>>> singlePartitionTag = new TupleTag<KV<ShardedKey<DestinationT>, List<String>>>("singlePartitionTag") {    };                PCollectionTuple partitions = results.apply("ReifyResults", new ReifyAsIterable<>()).setCoder(IterableCoder.of(WriteBundlesToFiles.ResultCoder.of(destinationCoder))).apply("WritePartitionUntriggered", ParDo.of(new WritePartition<>(singletonTable, dynamicDestinations, tempFilePrefixView, maxFilesPerPartition, maxBytesPerPartition, multiPartitionsTag, singlePartitionTag)).withSideInputs(tempFilePrefixView).withOutputTags(multiPartitionsTag, TupleTagList.of(singlePartitionTag)));    PCollection<KV<TableDestination, String>> tempTables = writeTempTables(partitions.get(multiPartitionsTag), loadJobIdPrefixView);    tempTables.apply("ReifyRenameInput", new ReifyAsIterable<>()).setCoder(IterableCoder.of(KvCoder.of(TableDestinationCoderV2.of(), StringUtf8Coder.of()))).apply("WriteRenameUntriggered", ParDo.of(new WriteRename(bigQueryServices, loadJobIdPrefixView, writeDisposition, createDisposition, maxRetryJobs, kmsKey)).withSideInputs(loadJobIdPrefixView));    writeSinglePartition(partitions.get(singlePartitionTag), loadJobIdPrefixView);    return writeResult(p);}
public void beam_f29080_0(@Element KV<DestinationT, ElementT> element, OutputReceiver<KV<ShardedKey<DestinationT>, ElementT>> o)
{    DestinationT destination = element.getKey();    o.output(KV.of(ShardedKey.of(destination, ++shardNumber % numFileShards), element.getValue()));}
private PCollection<Result<DestinationT>> beam_f29081_0(PCollection<KV<ShardedKey<DestinationT>, ElementT>> shardedRecords, PCollectionView<String> tempFilePrefix)
{    return shardedRecords.apply("GroupByDestination", GroupByKey.create()).apply("WriteGroupedRecords", ParDo.of(new WriteGroupedRecordsToFiles<DestinationT, ElementT>(tempFilePrefix, maxFileSize, toRowFunction)).withSideInputs(tempFilePrefix)).setCoder(WriteBundlesToFiles.ResultCoder.of(destinationCoder));}
private PCollection<KV<TableDestination, String>> beam_f29082_0(PCollection<KV<ShardedKey<DestinationT>, List<String>>> input, PCollectionView<String> jobIdTokenView)
{    List<PCollectionView<?>> sideInputs = Lists.newArrayList(jobIdTokenView);    sideInputs.addAll(dynamicDestinations.getSideInputs());    Coder<KV<ShardedKey<DestinationT>, List<String>>> partitionsCoder = KvCoder.of(ShardedKeyCoder.of(NullableCoder.of(destinationCoder)), ListCoder.of(StringUtf8Coder.of()));                @SuppressWarnings("unchecked")    DynamicDestinations<?, DestinationT> destinations = dynamicDestinations;    if (createDisposition.equals(CreateDisposition.CREATE_IF_NEEDED) || createDisposition.equals(CreateDisposition.CREATE_NEVER)) {        destinations = DynamicDestinationsHelpers.matchTableDynamicDestinations(destinations, bigQueryServices);    }        return input.setCoder(partitionsCoder).apply("MultiPartitionsReshuffle", Reshuffle.of()).apply("MultiPartitionsWriteTables", new WriteTables<>(true, bigQueryServices, jobIdTokenView, WriteDisposition.WRITE_EMPTY, CreateDisposition.CREATE_IF_NEEDED, sideInputs, destinations, loadJobProjectId, maxRetryJobs, ignoreUnknownValues, kmsKey));}
private static TableRow beam_f29090_0(GenericRecord record, List<TableFieldSchema> fields)
{    TableRow row = new TableRow();    for (TableFieldSchema subSchema : fields) {                        Field field = record.getSchema().getField(subSchema.getName());        Object convertedValue = getTypedCellValue(field.schema(), subSchema, record.get(field.name()));        if (convertedValue != null) {                        row.set(field.name(), convertedValue);        }    }    return row;}
private static Object beam_f29091_0(Schema schema, TableFieldSchema fieldSchema, Object v)
{            String mode = firstNonNull(fieldSchema.getMode(), "NULLABLE");    switch(mode) {        case "REQUIRED":            return convertRequiredField(schema.getType(), schema.getLogicalType(), fieldSchema, v);        case "REPEATED":            return convertRepeatedField(schema, fieldSchema, v);        case "NULLABLE":            return convertNullableField(schema, fieldSchema, v);        default:            throw new UnsupportedOperationException("Parsing a field with BigQuery field schema mode " + fieldSchema.getMode());    }}
private static List<Object> beam_f29092_0(Schema schema, TableFieldSchema fieldSchema, Object v)
{    Type arrayType = schema.getType();    verify(arrayType == Type.ARRAY, "BigQuery REPEATED field %s should be Avro ARRAY, not %s", fieldSchema.getName(), arrayType);        if (v == null) {                return new ArrayList<>();    }    @SuppressWarnings("unchecked")    List<Object> elements = (List<Object>) v;    ArrayList<Object> values = new ArrayList<>();    Type elementType = schema.getElementType().getType();    LogicalType elementLogicalType = schema.getElementType().getLogicalType();    for (Object element : elements) {        values.add(convertRequiredField(elementType, elementLogicalType, fieldSchema, element));    }    return values;}
private static boolean beam_f29100_0(Sleeper sleeper, BackOff backOff) throws InterruptedException
{    try {        return BackOffUtils.next(sleeper, backOff);    } catch (IOException e) {        throw new RuntimeException(e);    }}
 void beam_f29101_1() throws IOException
{    ++currentAttempt;    if (!shouldRetry()) {        throw new RuntimeException(String.format("Failed to create job with prefix %s, " + "reached max retries: %d, last failed job: %s.", currentJobId.getJobIdPrefix(), maxRetries, BigQueryHelpers.jobToPrettyString(lastJobAttempted)));    }    try {        this.started = false;        executeJob.apply(currentJobId);    } catch (RuntimeException e) {                                                RetryJobIdResult result = getRetryJobId(currentJobId, lookupJob);        currentJobId = result.jobId;        if (result.shouldRetry) {                                                return;        }    }                this.started = true;}
 boolean beam_f29102_1() throws IOException
{    if (started) {        Job job = pollJob.apply(currentJobId);        this.lastJobAttempted = job;        Status jobStatus = parseStatus(job);        switch(jobStatus) {            case SUCCEEDED:                                return true;            case UNKNOWN:                                                                return false;            case FAILED:                String oldJobId = currentJobId.getJobId();                currentJobId = BigQueryHelpers.getRetryJobId(currentJobId, lookupJob).jobId;                                return false;            default:                throw new IllegalStateException(String.format("Unexpected status [%s] of load job: %s.", job.getStatus(), BigQueryHelpers.jobToPrettyString(job)));        }    }    return false;}
 static TableReference beam_f29110_0(TableReferenceProto.TableReference ref)
{    return new TableReference().setProjectId(ref.getProjectId()).setDatasetId(ref.getDatasetId()).setTableId(ref.getTableId());}
 static ValueProvider<String> beam_f29111_0(@Nullable ValueProvider<TableReference> table)
{    if (table == null) {        return null;    }    return NestedValueProvider.of(table, new TableRefToTableSpec());}
public static String beam_f29112_0(TableReference ref)
{    StringBuilder sb = new StringBuilder();    if (ref.getProjectId() != null) {        sb.append(ref.getProjectId());        sb.append(":");    }    sb.append(ref.getDatasetId()).append('.').append(ref.getTableId());    return sb.toString();}
 static Status beam_f29120_0(@Nullable Job job)
{    if (job == null) {        return Status.UNKNOWN;    }    JobStatus status = job.getStatus();    if (status.getErrorResult() != null) {        return Status.FAILED;    } else if (status.getErrors() != null && !status.getErrors().isEmpty()) {        return Status.FAILED;    } else {        return Status.SUCCEEDED;    }}
public static String beam_f29121_0(Object item)
{    if (item == null) {        return null;    }    try {        return BigQueryIO.JSON_FACTORY.toString(item);    } catch (IOException e) {        throw new RuntimeException(String.format("Cannot serialize %s to a JSON string.", item.getClass().getSimpleName()), e);    }}
public static T beam_f29122_0(String json, Class<T> clazz)
{    if (json == null) {        return null;    }    try {        return BigQueryIO.JSON_FACTORY.fromString(json, clazz);    } catch (IOException e) {        throw new RuntimeException(String.format("Cannot deserialize %s from a JSON string: %s.", clazz, json), e);    }}
public TableSchema beam_f29130_0(String from)
{    return fromJsonString(from, TableSchema.class);}
public String beam_f29131_0(TableSchema from)
{    return toJsonString(from);}
public TableReference beam_f29132_0(String from)
{    return fromJsonString(from, TableReference.class);}
 static String beam_f29140_0(String jobName, String stepUuid)
{    return String.format("beam_job_%s_%s", stepUuid, jobName.replaceAll("-", ""));}
 static String beam_f29141_0(String jobIdToken)
{    return String.format("%s-extract", jobIdToken);}
 static TableReference beam_f29142_0(String projectId, String jobUuid)
{    String queryTempDatasetId = "temp_dataset_" + jobUuid;    String queryTempTableId = "temp_table_" + jobUuid;    return new TableReference().setProjectId(projectId).setDatasetId(queryTempDatasetId).setTableId(queryTempTableId);}
public void beam_f29150_0(BigQueryInsertError value, OutputStream outStream) throws IOException
{    String errorStrValue = MAPPER.writeValueAsString(value.getError());    StringUtf8Coder.of().encode(errorStrValue, outStream);    TableRowJsonCoder.of().encode(value.getRow(), outStream);    StringUtf8Coder.of().encode(BigQueryHelpers.toTableSpec(value.getTable()), outStream);}
public BigQueryInsertError beam_f29151_0(InputStream inStream) throws IOException
{    TableDataInsertAllResponse.InsertErrors err = MAPPER.readValue(StringUtf8Coder.of().decode(inStream), TableDataInsertAllResponse.InsertErrors.class);    TableRow row = TableRowJsonCoder.of().decode(inStream);    TableReference ref = BigQueryHelpers.parseTableSpec(StringUtf8Coder.of().decode(inStream));    return new BigQueryInsertError(row, err, ref);}
protected long beam_f29152_0(BigQueryInsertError value) throws Exception
{    String errorStrValue = MAPPER.writeValueAsString(value.getError());    String tableStrValue = MAPPER.writeValueAsString(value.getTable());    return StringUtf8Coder.of().getEncodedElementByteSize(errorStrValue) + TableRowJsonCoder.of().getEncodedElementByteSize(value.getRow()) + StringUtf8Coder.of().getEncodedElementByteSize(tableStrValue);}
public PCollection<TableRow> beam_f29160_0(PBegin input)
{    return input.apply(inner);}
public void beam_f29161_0(DisplayData.Builder builder)
{    this.inner.populateDisplayData(builder);}
 boolean beam_f29162_0()
{    return this.inner.getValidate();}
public Read beam_f29170_0(String query)
{    return new Read(this.inner.fromQuery(query));}
public Read beam_f29171_0(ValueProvider<String> query)
{    return new Read(this.inner.fromQuery(query));}
public Read beam_f29172_0()
{    return new Read(this.inner.withoutValidation());}
public PCollection<T> beam_f29180_1(PBegin input)
{    ValueProvider<TableReference> table = getTableProvider();    if (table != null) {        checkArgument(getQuery() == null, "from() and fromQuery() are exclusive");        checkArgument(getQueryPriority() == null, "withQueryPriority() can only be specified when using fromQuery()");        checkArgument(getFlattenResults() == null, "Invalid BigQueryIO.Read: Specifies a table with a result flattening" + " preference, which only applies to queries");        checkArgument(getUseLegacySql() == null, "Invalid BigQueryIO.Read: Specifies a table with a SQL dialect" + " preference, which only applies to queries");        if (table.isAccessible() && Strings.isNullOrEmpty(table.get().getProjectId())) {                    }    } else {        checkArgument(getQuery() != null, "Either from() or fromQuery() is required");        checkArgument(getFlattenResults() != null, "flattenResults should not be null if query is set");        checkArgument(getUseLegacySql() != null, "useLegacySql should not be null if query is set");    }    checkArgument(getParseFn() != null, "A parseFn is required");        boolean beamSchemaEnabled = false;    if (getToBeamRowFn() != null && getFromBeamRowFn() != null) {        beamSchemaEnabled = true;    }    Pipeline p = input.getPipeline();    final Coder<T> coder = inferCoder(p.getCoderRegistry());    if (getMethod() == Method.DIRECT_READ) {        return expandForDirectRead(input, coder);    }    checkArgument(getReadOptions() == null, "Invalid BigQueryIO.Read: Specifies table read options, " + "which only applies when using Method.DIRECT_READ");    checkArgument(getSelectedFields() == null, "Invalid BigQueryIO.Read: Specifies selected fields, " + "which only applies when using Method.DIRECT_READ");    checkArgument(getRowRestriction() == null, "Invalid BigQueryIO.Read: Specifies row restriction, " + "which only applies when using Method.DIRECT_READ");    final BigQuerySourceDef sourceDef = createSourceDef();    final PCollectionView<String> jobIdTokenView;    PCollection<String> jobIdTokenCollection;    PCollection<T> rows;    if (!getWithTemplateCompatibility()) {                final String staticJobUuid = BigQueryHelpers.randomUUIDString();        jobIdTokenView = p.apply("TriggerIdCreation", Create.of(staticJobUuid)).apply("ViewId", View.asSingleton());                rows = p.apply(org.apache.beam.sdk.io.Read.from(sourceDef.toSource(staticJobUuid, coder, getParseFn())));    } else {                jobIdTokenCollection = p.apply("TriggerIdCreation", Create.of("ignored")).apply("CreateJobId", MapElements.via(new SimpleFunction<String, String>() {            @Override            public String apply(String input) {                return BigQueryHelpers.randomUUIDString();            }        }));        jobIdTokenView = jobIdTokenCollection.apply("ViewId", View.asSingleton());        final TupleTag<String> filesTag = new TupleTag<>();        final TupleTag<String> tableSchemaTag = new TupleTag<>();        PCollectionTuple tuple = jobIdTokenCollection.apply("RunCreateJob", ParDo.of(new DoFn<String, String>() {            @ProcessElement            public void processElement(ProcessContext c) throws Exception {                String jobUuid = c.element();                BigQuerySourceBase<T> source = sourceDef.toSource(jobUuid, coder, getParseFn());                BigQueryOptions options = c.getPipelineOptions().as(BigQueryOptions.class);                ExtractResult res = source.extractFiles(options);                                source.cleanupTempResource(options);                for (ResourceId file : res.extractedFiles) {                    c.output(file.toString());                }                c.output(tableSchemaTag, BigQueryHelpers.toJsonString(res.schema));            }        }).withOutputTags(filesTag, TupleTagList.of(tableSchemaTag)));        tuple.get(filesTag).setCoder(StringUtf8Coder.of());        tuple.get(tableSchemaTag).setCoder(StringUtf8Coder.of());        final PCollectionView<String> schemaView = tuple.get(tableSchemaTag).apply(View.asSingleton());        rows = tuple.get(filesTag).apply(Reshuffle.viaRandomKey()).apply("ReadFiles", ParDo.of(new DoFn<String, T>() {            @ProcessElement            public void processElement(ProcessContext c) throws Exception {                TableSchema schema = BigQueryHelpers.fromJsonString(c.sideInput(schemaView), TableSchema.class);                String jobUuid = c.sideInput(jobIdTokenView);                BigQuerySourceBase<T> source = sourceDef.toSource(jobUuid, coder, getParseFn());                List<BoundedSource<T>> sources = source.createSources(ImmutableList.of(FileSystems.matchNewResource(c.element(), false)), schema, null);                checkArgument(sources.size() == 1, "Expected exactly one source.");                BoundedSource<T> avroSource = sources.get(0);                BoundedSource.BoundedReader<T> reader = avroSource.createReader(c.getPipelineOptions());                for (boolean more = reader.start(); more; more = reader.advance()) {                    c.output(reader.getCurrent());                }            }        }).withSideInputs(schemaView, jobIdTokenView)).setCoder(coder);    }    PassThroughThenCleanup.CleanupOperation cleanupOperation = new PassThroughThenCleanup.CleanupOperation() {        @Override        void cleanup(PassThroughThenCleanup.ContextContainer c) throws Exception {            PipelineOptions options = c.getPipelineOptions();            BigQueryOptions bqOptions = options.as(BigQueryOptions.class);            String jobUuid = c.getJobId();            final String extractDestinationDir = resolveTempLocation(bqOptions.getTempLocation(), "BigQueryExtractTemp", jobUuid);            final String executingProject = bqOptions.getProject();            JobReference jobRef = new JobReference().setProjectId(executingProject).setJobId(getExtractJobId(createJobIdToken(bqOptions.getJobName(), jobUuid)));            Job extractJob = getBigQueryServices().getJobService(bqOptions).getJob(jobRef);            if (extractJob != null) {                List<ResourceId> extractFiles = getExtractFilePaths(extractDestinationDir, extractJob);                if (extractFiles != null && !extractFiles.isEmpty()) {                    FileSystems.delete(extractFiles, MoveOptions.StandardMoveOptions.IGNORE_MISSING_FILES);                }            }        }    };    rows = rows.apply(new PassThroughThenCleanup<>(cleanupOperation, jobIdTokenView));    if (beamSchemaEnabled) {        BigQueryOptions bqOptions = p.getOptions().as(BigQueryOptions.class);        Schema beamSchema = sourceDef.getBeamSchema(bqOptions);        SerializableFunction<T, Row> toBeamRow = getToBeamRowFn().apply(beamSchema);        SerializableFunction<Row, T> fromBeamRow = getFromBeamRowFn().apply(beamSchema);        rows.setSchema(beamSchema, toBeamRow, fromBeamRow);    }    return rows;}
public String beam_f29181_0(String input)
{    return BigQueryHelpers.randomUUIDString();}
public void beam_f29182_1(ProcessContext c) throws Exception
{    String jobUuid = c.element();    BigQuerySourceBase<T> source = sourceDef.toSource(jobUuid, coder, getParseFn());    BigQueryOptions options = c.getPipelineOptions().as(BigQueryOptions.class);    ExtractResult res = source.extractFiles(options);        source.cleanupTempResource(options);    for (ResourceId file : res.extractedFiles) {        c.output(file.toString());    }    c.output(tableSchemaTag, BigQueryHelpers.toJsonString(res.schema));}
public void beam_f29190_0(DisplayData.Builder builder)
{    super.populateDisplayData(builder);    builder.addIfNotNull(DisplayData.item("table", BigQueryHelpers.displayTable(getTableProvider())).withLabel("Table")).addIfNotNull(DisplayData.item("query", getQuery()).withLabel("Query")).addIfNotNull(DisplayData.item("flattenResults", getFlattenResults()).withLabel("Flatten Query Results")).addIfNotNull(DisplayData.item("useLegacySql", getUseLegacySql()).withLabel("Use Legacy SQL Dialect")).addIfNotDefault(DisplayData.item("validation", getValidate()).withLabel("Validation Enabled"), true);}
private void beam_f29191_0()
{    checkState(getJsonTableRef() == null && getQuery() == null, "from() or fromQuery() already called");}
private void beam_f29192_0()
{    checkState(getReadOptions() == null, "withReadOptions() already called");}
public TypedRead<T> beam_f29200_0(ValueProvider<String> tableSpec)
{    ensureFromNotCalledYet();    return toBuilder().setJsonTableRef(NestedValueProvider.of(NestedValueProvider.of(tableSpec, new TableSpecToTableRef()), new TableRefToJson())).build();}
public TypedRead<T> beam_f29201_0(String query)
{    return fromQuery(StaticValueProvider.of(query));}
public TypedRead<T> beam_f29202_0(ValueProvider<String> query)
{    ensureFromNotCalledYet();    return toBuilder().setQuery(query).setFlattenResults(true).setUseLegacySql(true).build();}
public TypedRead<T> beam_f29210_0(TableReadOptions readOptions)
{    ensureReadOptionsFieldsNotSet();    return toBuilder().setReadOptions(readOptions).build();}
public TypedRead<T> beam_f29211_0(List<String> selectedFields)
{    return withSelectedFields(StaticValueProvider.of(selectedFields));}
public TypedRead<T> beam_f29212_0(ValueProvider<List<String>> selectedFields)
{    ensureReadOptionsNotSet();    return toBuilder().setSelectedFields(selectedFields).build();}
public static Write<TableRow> beam_f29220_0()
{    return BigQueryIO.<TableRow>write().withFormatFunction(IDENTITY_FORMATTER);}
public Write<T> beam_f29221_0(String tableSpec)
{    return to(StaticValueProvider.of(tableSpec));}
public Write<T> beam_f29222_0(TableReference table)
{    return to(StaticValueProvider.of(BigQueryHelpers.toTableSpec(table)));}
public Write<T> beam_f29230_0(ValueProvider<String> jsonSchema)
{    checkArgument(jsonSchema != null, "jsonSchema can not be null");    return toBuilder().setJsonSchema(jsonSchema).build();}
public Write<T> beam_f29231_0(PCollectionView<Map<String, String>> view)
{    checkArgument(view != null, "view can not be null");    return toBuilder().setSchemaFromView(view).build();}
public Write<T> beam_f29232_0(TimePartitioning partitioning)
{    checkArgument(partitioning != null, "partitioning can not be null");    return withJsonTimePartitioning(StaticValueProvider.of(BigQueryHelpers.toJsonString(partitioning)));}
public Write<T> beam_f29240_0(InsertRetryPolicy retryPolicy)
{    checkArgument(retryPolicy != null, "retryPolicy can not be null");    return toBuilder().setFailedInsertRetryPolicy(retryPolicy).build();}
public Write<T> beam_f29241_0()
{    return toBuilder().setValidate(false).build();}
public Write<T> beam_f29242_0(Method method)
{    checkArgument(method != null, "method can not be null");    return toBuilder().setMethod(method).build();}
public Write<T> beam_f29250_0()
{    return toBuilder().setIgnoreUnknownValues(true).build();}
public Write<T> beam_f29251_0(String kmsKey)
{    return toBuilder().setKmsKey(kmsKey).build();}
public Write<T> beam_f29252_0()
{    return toBuilder().setOptimizeWrites(true).build();}
private Method beam_f29260_0(PCollection<T> input)
{    if (getMethod() != Method.DEFAULT) {        return getMethod();    }        return (input.isBounded() == IsBounded.UNBOUNDED) ? Method.STREAMING_INSERTS : Method.FILE_LOADS;}
public WriteResult beam_f29261_0(PCollection<T> input)
{        checkArgument(getTableFunction() != null || getJsonTableRef() != null || getDynamicDestinations() != null, "must set the table reference of a BigQueryIO.Write transform");    List<?> allToArgs = Lists.newArrayList(getJsonTableRef(), getTableFunction(), getDynamicDestinations());    checkArgument(1 == Iterables.size(allToArgs.stream().filter(Predicates.notNull()::apply).collect(Collectors.toList())), "Exactly one of jsonTableRef, tableFunction, or " + "dynamicDestinations must be set");    List<?> allSchemaArgs = Lists.newArrayList(getJsonSchema(), getSchemaFromView(), getDynamicDestinations());    checkArgument(2 > Iterables.size(allSchemaArgs.stream().filter(Predicates.notNull()::apply).collect(Collectors.toList())), "No more than one of jsonSchema, schemaFromView, or dynamicDestinations may " + "be set");    Method method = resolveMethod(input);    if (input.isBounded() == IsBounded.UNBOUNDED && method == Method.FILE_LOADS) {        checkArgument(getTriggeringFrequency() != null, "When writing an unbounded PCollection via FILE_LOADS, " + "triggering frequency must be specified");    } else {        checkArgument(getTriggeringFrequency() == null && getNumFileShards() == 0, "Triggering frequency or number of file shards can be specified only when writing " + "an unbounded PCollection via FILE_LOADS, but: the collection was %s " + "and the method was %s", input.isBounded(), method);    }    if (getJsonTimePartitioning() != null) {        checkArgument(getDynamicDestinations() == null, "The supplied DynamicDestinations object can directly set TimePartitioning." + " There is no need to call BigQueryIO.Write.withTimePartitioning.");        checkArgument(getTableFunction() == null, "The supplied getTableFunction object can directly set TimePartitioning." + " There is no need to call BigQueryIO.Write.withTimePartitioning.");    }    if (getClustering() != null && getClustering().getFields() != null) {        checkArgument(getJsonTimePartitioning() != null, "Clustering fields can only be set when TimePartitioning is set.");    }    DynamicDestinations<T, ?> dynamicDestinations = getDynamicDestinations();    if (dynamicDestinations == null) {        if (getJsonTableRef() != null) {            dynamicDestinations = DynamicDestinationsHelpers.ConstantTableDestinations.fromJsonTableRef(getJsonTableRef(), getTableDescription());        } else if (getTableFunction() != null) {            dynamicDestinations = new TableFunctionDestinations<>(getTableFunction(), getClustering() != null);        }                if (getJsonSchema() != null) {            dynamicDestinations = new ConstantSchemaDestinations<>((DynamicDestinations<T, TableDestination>) dynamicDestinations, getJsonSchema());        } else if (getSchemaFromView() != null) {            dynamicDestinations = new SchemaFromViewDestinations<>((DynamicDestinations<T, TableDestination>) dynamicDestinations, getSchemaFromView());        }                if (getJsonTimePartitioning() != null) {            dynamicDestinations = new ConstantTimePartitioningDestinations<>((DynamicDestinations<T, TableDestination>) dynamicDestinations, getJsonTimePartitioning(), StaticValueProvider.of(BigQueryHelpers.toJsonString(getClustering())));        }    }    return expandTyped(input, dynamicDestinations);}
private WriteResult beam_f29262_0(PCollection<T> input, DynamicDestinations<T, DestinationT> dynamicDestinations)
{    boolean optimizeWrites = getOptimizeWrites();    SerializableFunction<T, TableRow> formatFunction = getFormatFunction();    if (getUseBeamSchema()) {        checkArgument(input.hasSchema());        optimizeWrites = true;        if (formatFunction == null) {                                    formatFunction = BigQueryUtils.toTableRow(input.getToRowFunction());        }                TableSchema tableSchema = BigQueryUtils.toTableSchema(input.getSchema());        dynamicDestinations = new ConstantSchemaDestinations<>(dynamicDestinations, StaticValueProvider.of(BigQueryHelpers.toJsonString(tableSchema)));    } else {                checkArgument(getCreateDisposition() != CreateDisposition.CREATE_IF_NEEDED || getJsonSchema() != null || getDynamicDestinations() != null || getSchemaFromView() != null, "CreateDisposition is CREATE_IF_NEEDED, however no schema was provided.");    }    checkArgument(formatFunction != null, "A function must be provided to convert type into a TableRow. " + "use BigQueryIO.Write.withFormatFunction to provide a formatting function." + "A format function is not required if Beam schemas are used.");    Coder<DestinationT> destinationCoder = null;    try {        destinationCoder = dynamicDestinations.getDestinationCoderWithDefault(input.getPipeline().getCoderRegistry());    } catch (CannotProvideCoderException e) {        throw new RuntimeException(e);    }    Method method = resolveMethod(input);    if (optimizeWrites) {        PCollection<KV<DestinationT, T>> rowsWithDestination = input.apply("PrepareWrite", new PrepareWrite<>(dynamicDestinations, SerializableFunctions.identity())).setCoder(KvCoder.of(destinationCoder, input.getCoder()));        return continueExpandTyped(rowsWithDestination, input.getCoder(), destinationCoder, dynamicDestinations, formatFunction, method);    } else {        PCollection<KV<DestinationT, TableRow>> rowsWithDestination = input.apply("PrepareWrite", new PrepareWrite<>(dynamicDestinations, formatFunction)).setCoder(KvCoder.of(destinationCoder, TableRowJsonCoder.of()));        return continueExpandTyped(rowsWithDestination, TableRowJsonCoder.of(), destinationCoder, dynamicDestinations, SerializableFunctions.identity(), method);    }}
private static JobConfigurationQuery beam_f29270_0(String query, Boolean flattenResults, Boolean useLegacySql)
{    return new JobConfigurationQuery().setQuery(query).setFlattenResults(flattenResults).setUseLegacySql(useLegacySql);}
 static BigQueryQuerySource<T> beam_f29271_0(String stepUuid, BigQueryQuerySourceDef queryDef, BigQueryServices bqServices, Coder<T> coder, SerializableFunction<SchemaAndRecord, T> parseFn)
{    return new BigQueryQuerySource<>(stepUuid, queryDef, bqServices, coder, parseFn);}
public long beam_f29272_0(PipelineOptions options) throws Exception
{    return queryDef.getEstimatedSizeBytes(options.as(BigQueryOptions.class));}
 void beam_f29280_1(BigQueryOptions bqOptions, String stepUuid) throws Exception
{    TableReference tableToRemove = createTempTableReference(bqOptions.getProject(), createJobIdToken(bqOptions.getJobName(), stepUuid));    BigQueryServices.DatasetService tableService = bqServices.getDatasetService(bqOptions);        tableService.deleteTable(tableToRemove);        tableService.deleteDataset(tableToRemove.getProjectId(), tableToRemove.getDatasetId());}
public BigQuerySourceBase<T> beam_f29281_0(String stepUuid, Coder<T> coder, SerializableFunction<SchemaAndRecord, T> parseFn)
{    return BigQueryQuerySource.create(stepUuid, this, bqServices, coder, parseFn);}
public Schema beam_f29282_0(BigQueryOptions bqOptions)
{    try {        JobStatistics stats = BigQueryQueryHelper.dryRunQueryIfNeeded(bqServices, bqOptions, dryRunJobStats, query.get(), flattenResults, useLegacySql, location);        TableSchema tableSchema = stats.getQuery().getSchema();        return BigQueryUtils.fromTableSchema(tableSchema);    } catch (IOException | InterruptedException | NullPointerException e) {        throw new BigQuerySchemaRetrievalException("Exception while trying to retrieve schema of query", e);    }}
public void beam_f29290_0(JobReference jobRef, JobConfigurationQuery queryConfig) throws IOException, InterruptedException
{    Job job = new Job().setJobReference(jobRef).setConfiguration(new JobConfiguration().setQuery(queryConfig));    startJob(job, errorExtractor, client);}
public void beam_f29291_0(JobReference jobRef, JobConfigurationTableCopy copyConfig) throws IOException, InterruptedException
{    Job job = new Job().setJobReference(jobRef).setConfiguration(new JobConfiguration().setCopy(copyConfig));    startJob(job, errorExtractor, client);}
private static void beam_f29292_0(Job job, ApiErrorExtractor errorExtractor, Bigquery client) throws IOException, InterruptedException
{    startJob(job, errorExtractor, client, Sleeper.DEFAULT, createDefaultBackoff());}
public Table beam_f29300_0(TableReference tableRef) throws IOException, InterruptedException
{    return getTable(tableRef, null);}
public Table beam_f29301_0(TableReference tableRef, List<String> selectedFields) throws IOException, InterruptedException
{    return getTable(tableRef, selectedFields, createDefaultBackoff(), Sleeper.DEFAULT);}
 Table beam_f29302_0(TableReference ref, @Nullable List<String> selectedFields, BackOff backoff, Sleeper sleeper) throws IOException, InterruptedException
{    Tables.Get get = client.tables().get(ref.getProjectId(), ref.getDatasetId(), ref.getTableId());    if (selectedFields != null && !selectedFields.isEmpty()) {        get.setSelectedFields(String.join(",", selectedFields));    }    try {        return executeWithRetries(get, String.format("Unable to get table: %s, aborting after %d retries.", ref.getTableId(), MAX_RPC_RETRIES), sleeper, backoff, DONT_RETRY_NOT_FOUND);    } catch (IOException e) {        if (errorExtractor.itemNotFound(e)) {            return null;        }        throw e;    }}
private void beam_f29310_1(String projectId, String datasetId, @Nullable String location, @Nullable String description, @Nullable Long defaultTableExpirationMs, Sleeper sleeper, BackOff backoff) throws IOException, InterruptedException
{    DatasetReference datasetRef = new DatasetReference().setProjectId(projectId).setDatasetId(datasetId);    Dataset dataset = new Dataset().setDatasetReference(datasetRef);    if (location != null) {        dataset.setLocation(location);    }    if (description != null) {        dataset.setFriendlyName(description);        dataset.setDescription(description);    }    if (defaultTableExpirationMs != null) {        dataset.setDefaultTableExpirationMs(defaultTableExpirationMs);    }    Exception lastException;    do {        try {            client.datasets().insert(projectId, dataset).execute();                        return;        } catch (GoogleJsonResponseException e) {            if (errorExtractor.itemAlreadyExists(e)) {                                return;            }                                    lastException = e;        } catch (IOException e) {                        lastException = e;        }    } while (nextBackOff(sleeper, backoff));    throw new IOException(String.format("Unable to create dataset: %s, aborting after %d .", datasetId, MAX_RPC_RETRIES), lastException);}
public void beam_f29311_0(String projectId, String datasetId) throws IOException, InterruptedException
{    executeWithRetries(client.datasets().delete(projectId, datasetId), String.format("Unable to delete table: %s, aborting after %d retries.", datasetId, MAX_RPC_RETRIES), Sleeper.DEFAULT, createDefaultBackoff(), ALWAYS_RETRY);}
 long beam_f29312_1(TableReference ref, List<ValueInSingleWindow<TableRow>> rowList, @Nullable List<String> insertIdList, BackOff backoff, final Sleeper sleeper, InsertRetryPolicy retryPolicy, List<ValueInSingleWindow<T>> failedInserts, ErrorContainer<T> errorContainer, boolean skipInvalidRows, boolean ignoreUnkownValues) throws IOException, InterruptedException
{    checkNotNull(ref, "ref");    if (executor == null) {        this.executor = new BoundedExecutorService(options.as(GcsOptions.class).getExecutorService(), options.as(BigQueryOptions.class).getInsertBundleParallelism());    }    if (insertIdList != null && rowList.size() != insertIdList.size()) {        throw new AssertionError("If insertIdList is not null it needs to have at least " + "as many elements as rowList");    }    long retTotalDataSize = 0;    List<TableDataInsertAllResponse.InsertErrors> allErrors = new ArrayList<>();            List<ValueInSingleWindow<TableRow>> rowsToPublish = rowList;    List<String> idsToPublish = insertIdList;    while (true) {        List<ValueInSingleWindow<TableRow>> retryRows = new ArrayList<>();        List<String> retryIds = (idsToPublish != null) ? new ArrayList<>() : null;        int strideIndex = 0;                List<TableDataInsertAllRequest.Rows> rows = new ArrayList<>();        int dataSize = 0;        List<Future<List<TableDataInsertAllResponse.InsertErrors>>> futures = new ArrayList<>();        List<Integer> strideIndices = new ArrayList<>();        for (int i = 0; i < rowsToPublish.size(); ++i) {            TableRow row = rowsToPublish.get(i).getValue();            TableDataInsertAllRequest.Rows out = new TableDataInsertAllRequest.Rows();            if (idsToPublish != null) {                out.setInsertId(idsToPublish.get(i));            }            out.setJson(row.getUnknownKeys());            rows.add(out);            dataSize += row.toString().length();            if (dataSize >= maxRowBatchSize || rows.size() >= maxRowsPerBatch || i == rowsToPublish.size() - 1) {                TableDataInsertAllRequest content = new TableDataInsertAllRequest();                content.setRows(rows);                content.setSkipInvalidRows(skipInvalidRows);                content.setIgnoreUnknownValues(ignoreUnkownValues);                final Bigquery.Tabledata.InsertAll insert = client.tabledata().insertAll(ref.getProjectId(), ref.getDatasetId(), ref.getTableId(), content);                futures.add(executor.submit(() -> {                                        BackOff backoff1 = BackOffAdapter.toGcpBackOff(RATE_LIMIT_BACKOFF_FACTORY.backoff());                    while (true) {                        try {                            return insert.execute().getInsertErrors();                        } catch (IOException e) {                                                        try {                                sleeper.sleep(backoff1.nextBackOffMillis());                            } catch (InterruptedException interrupted) {                                throw new IOException("Interrupted while waiting before retrying insertAll");                            }                        }                    }                }));                strideIndices.add(strideIndex);                retTotalDataSize += dataSize;                dataSize = 0;                strideIndex = i + 1;                rows = new ArrayList<>();            }        }        try {            for (int i = 0; i < futures.size(); i++) {                List<TableDataInsertAllResponse.InsertErrors> errors = futures.get(i).get();                if (errors == null) {                    continue;                }                for (TableDataInsertAllResponse.InsertErrors error : errors) {                    if (error.getIndex() == null) {                        throw new IOException("Insert failed: " + error + ", other errors: " + allErrors);                    }                    int errorIndex = error.getIndex().intValue() + strideIndices.get(i);                    if (retryPolicy.shouldRetry(new InsertRetryPolicy.Context(error))) {                        allErrors.add(error);                        retryRows.add(rowsToPublish.get(errorIndex));                        if (retryIds != null) {                            retryIds.add(idsToPublish.get(errorIndex));                        }                    } else {                        errorContainer.add(failedInserts, error, ref, rowsToPublish.get(errorIndex));                    }                }            }        } catch (InterruptedException e) {            Thread.currentThread().interrupt();            throw new IOException("Interrupted while inserting " + rowsToPublish);        } catch (ExecutionException e) {            throw new RuntimeException(e.getCause());        }        if (allErrors.isEmpty()) {            break;        }        long nextBackoffMillis = backoff.nextBackOffMillis();        if (nextBackoffMillis == BackOff.STOP) {            break;        }        try {            sleeper.sleep(nextBackoffMillis);        } catch (InterruptedException e) {            Thread.currentThread().interrupt();            throw new IOException("Interrupted while waiting before retrying insert of " + retryRows);        }        rowsToPublish = retryRows;        idsToPublish = retryIds;        allErrors.clear();            }    if (!allErrors.isEmpty()) {        throw new IOException("Insert failed: " + allErrors);    } else {        return retTotalDataSize;    }}
public Iterator<T> beam_f29320_0()
{    return serverStream.iterator();}
public void beam_f29321_0()
{    serverStream.cancel();}
public ReadSession beam_f29322_0(CreateReadSessionRequest request)
{    return client.createReadSession(request);}
public boolean beam_f29330_0(long l, TimeUnit timeUnit) throws InterruptedException
{    return executor.awaitTermination(l, timeUnit);}
public Future<T> beam_f29331_0(Callable<T> callable)
{    return executor.submit(new SemaphoreCallable<>(callable));}
public Future<T> beam_f29332_0(Runnable runnable, T t)
{    return executor.submit(new SemaphoreRunnable(runnable), t);}
public V beam_f29340_0() throws Exception
{    semaphore.acquire();    try {        return callable.call();    } finally {        semaphore.release();    }}
protected ExtractResult beam_f29341_0(PipelineOptions options) throws Exception
{    BigQueryOptions bqOptions = options.as(BigQueryOptions.class);    TableReference tableToExtract = getTableToExtract(bqOptions);    BigQueryServices.DatasetService datasetService = bqServices.getDatasetService(bqOptions);    Table table = datasetService.getTable(tableToExtract);    if (table == null) {        throw new IOException(String.format("Cannot start an export job since table %s does not exist", BigQueryHelpers.toTableSpec(tableToExtract)));    }    TableSchema schema = table.getSchema();    JobService jobService = bqServices.getJobService(bqOptions);    String extractJobId = getExtractJobId(createJobIdToken(options.getJobName(), stepUuid));    final String extractDestinationDir = resolveTempLocation(bqOptions.getTempLocation(), "BigQueryExtractTemp", stepUuid);    String bqLocation = BigQueryHelpers.getDatasetLocation(datasetService, tableToExtract.getProjectId(), tableToExtract.getDatasetId());    List<ResourceId> tempFiles = executeExtract(extractJobId, tableToExtract, jobService, bqOptions.getProject(), extractDestinationDir, bqLocation);    return new ExtractResult(schema, tempFiles);}
public List<BoundedSource<T>> beam_f29342_1(long desiredBundleSizeBytes, PipelineOptions options) throws Exception
{        if (cachedSplitResult == null) {        ExtractResult res = extractFiles(options);                if (res.extractedFiles.size() > 0) {            BigQueryOptions bqOptions = options.as(BigQueryOptions.class);            final String extractDestinationDir = resolveTempLocation(bqOptions.getTempLocation(), "BigQueryExtractTemp", stepUuid);                        List<MatchResult> matches = match(ImmutableList.of(extractDestinationDir + "*"));            if (matches.size() > 0) {                res.metadata = matches.get(0).metadata();            }        }        cleanupTempResource(options.as(BigQueryOptions.class));        cachedSplitResult = checkNotNull(createSources(res.extractedFiles, res.schema, res.metadata));    }    return cachedSplitResult;}
private void beam_f29351_0(ObjectInputStream in) throws ClassNotFoundException, IOException
{    in.defaultReadObject();    dryRunJobStats = new AtomicReference<>();}
public void beam_f29352_0(DisplayData.Builder builder)
{    super.populateDisplayData(builder);    builder.add(DisplayData.item("query", queryProvider).withLabel("Query"));}
public long beam_f29353_0(PipelineOptions options) throws Exception
{    return BigQueryQueryHelper.dryRunQueryIfNeeded(bqServices, options.as(BigQueryOptions.class), dryRunJobStats, queryProvider.get(), flattenResults, useLegacySql, location).getQuery().getTotalBytesProcessed();}
public long beam_f29362_0(PipelineOptions options)
{        return 0L;}
public List<? extends BoundedSource<T>> beam_f29363_0(long desiredBundleSizeBytes, PipelineOptions options)
{        return ImmutableList.of(this);}
public BigQueryStorageStreamReader<T> beam_f29364_0(PipelineOptions options) throws IOException
{    return new BigQueryStorageStreamReader<>(this, options.as(BigQueryOptions.class));}
public BoundedSource<T> beam_f29372_1(double fraction)
{    Metrics.counter(BigQueryStorageStreamReader.class, "split-at-fraction-calls").inc();        SplitReadStreamRequest splitRequest = SplitReadStreamRequest.newBuilder().setOriginalStream(source.stream).setUnknownFields(UnknownFieldSet.newBuilder().addField(2, UnknownFieldSet.Field.newBuilder().addFixed32(java.lang.Float.floatToIntBits((float) fraction)).build()).build()).build();    SplitReadStreamResponse splitResponse = storageClient.splitReadStream(splitRequest);    if (!splitResponse.hasPrimaryStream() || !splitResponse.hasRemainderStream()) {                Metrics.counter(BigQueryStorageStreamReader.class, "split-at-fraction-calls-failed-due-to-impossible-split-point").inc();                return null;    }        synchronized (this) {        BigQueryServerStream<ReadRowsResponse> newResponseStream;        Iterator<ReadRowsResponse> newResponseIterator;        try {            newResponseStream = storageClient.readRows(ReadRowsRequest.newBuilder().setReadPosition(StreamPosition.newBuilder().setStream(splitResponse.getPrimaryStream()).setOffset(currentOffset + 1)).build());            newResponseIterator = newResponseStream.iterator();            newResponseIterator.hasNext();        } catch (FailedPreconditionException e) {                                    Metrics.counter(BigQueryStorageStreamReader.class, "split-at-fraction-calls-failed-due-to-bad-split-point").inc();                        return null;        } catch (Exception e) {            Metrics.counter(BigQueryStorageStreamReader.class, "split-at-fraction-calls-failed-due-to-other-reasons").inc();                        return null;        }                responseStream.cancel();        source = source.fromExisting(splitResponse.getPrimaryStream());        responseStream = newResponseStream;        responseIterator = newResponseIterator;                                                                fractionConsumedFromCurrentResponse = fractionConsumed;        decoder = null;    }    Metrics.counter(BigQueryStorageStreamReader.class, "split-at-fraction-calls-successful").inc();        return source.fromExisting(splitResponse.getRemainderStream());}
public synchronized Double beam_f29373_0()
{    return fractionConsumed;}
private static float beam_f29374_0(ReadRowsResponse response)
{            List<Integer> fractionConsumedField = response.getStatus().getUnknownFields().getField(2).getFixed32List();    if (fractionConsumedField.isEmpty()) {        Metrics.counter(BigQueryStorageStreamReader.class, "fraction-consumed-not-set").inc();        return 0f;    }    return Float.intBitsToFloat(Iterables.getOnlyElement(fractionConsumedField));}
protected TableReference beam_f29382_0(BigQueryOptions bqOptions) throws IOException
{    return tableDef.getTableReference(bqOptions);}
public synchronized long beam_f29383_0(PipelineOptions options) throws Exception
{    if (tableSizeBytes.get() == null) {        BigQueryOptions bqOptions = options.as(BigQueryOptions.class);        TableReference tableRef = tableDef.getTableReference(bqOptions);        Table table = bqServices.getDatasetService(bqOptions).getTable(tableRef);        Long numBytes = table.getNumBytes();        if (table.getStreamingBuffer() != null) {            numBytes += table.getStreamingBuffer().getEstimatedBytes().longValue();        }        tableSizeBytes.compareAndSet(null, numBytes);    }    return tableSizeBytes.get();}
public void beam_f29385_0(DisplayData.Builder builder)
{    super.populateDisplayData(builder);    builder.add(DisplayData.item("table", tableDef.getJsonTable()));}
private static StandardSQLTypeName beam_f29393_0(FieldType fieldType)
{    if (fieldType.getTypeName().isLogicalType()) {        StandardSQLTypeName foundType = BEAM_TO_BIGQUERY_LOGICAL_MAPPING.get(fieldType.getLogicalType().getIdentifier());        if (foundType != null) {            return foundType;        }    }    return BEAM_TO_BIGQUERY_TYPE_MAPPING.get(fieldType.getTypeName());}
private static FieldType beam_f29394_0(String typeName, List<TableFieldSchema> nestedFields)
{    switch(typeName) {        case "STRING":            return FieldType.STRING;        case "BYTES":            return FieldType.BYTES;        case "INT64":        case "INTEGER":            return FieldType.INT64;        case "FLOAT64":        case "FLOAT":            return FieldType.DOUBLE;        case "BOOL":        case "BOOLEAN":            return FieldType.BOOLEAN;        case "TIMESTAMP":            return FieldType.DATETIME;        case "TIME":            return FieldType.logicalType(new LogicalTypes.PassThroughLogicalType<Instant>("SqlTimeType", "", FieldType.DATETIME) {            });        case "DATE":            return FieldType.logicalType(new LogicalTypes.PassThroughLogicalType<Instant>("SqlDateType", "", FieldType.DATETIME) {            });        case "DATETIME":            return FieldType.logicalType(new LogicalTypes.PassThroughLogicalType<Instant>("SqlTimestampWithLocalTzType", "", FieldType.DATETIME) {            });        case "STRUCT":        case "RECORD":            Schema rowSchema = fromTableFieldSchema(nestedFields);            return FieldType.row(rowSchema);        default:            throw new UnsupportedOperationException("Converting BigQuery type " + typeName + " to Beam type is unsupported");    }}
private static Schema beam_f29395_0(List<TableFieldSchema> tableFieldSchemas)
{    Schema.Builder schemaBuilder = Schema.builder();    for (TableFieldSchema tableFieldSchema : tableFieldSchemas) {        FieldType fieldType = fromTableFieldSchemaType(tableFieldSchema.getType(), tableFieldSchema.getFields());        Optional<Mode> fieldMode = Optional.ofNullable(tableFieldSchema.getMode()).map(Mode::valueOf);        if (fieldMode.filter(m -> m == Mode.REPEATED).isPresent()) {            fieldType = FieldType.array(fieldType);        }                boolean nullable = !fieldMode.isPresent() || fieldMode.filter(m -> m == Mode.NULLABLE).isPresent();        Field field = Field.of(tableFieldSchema.getName(), fieldType).withNullable(nullable);        if (tableFieldSchema.getDescription() != null && !"".equals(tableFieldSchema.getDescription())) {            field = field.withDescription(tableFieldSchema.getDescription());        }        schemaBuilder.addField(field);    }    return schemaBuilder.build();}
public static SerializableFunction<T, TableRow> beam_f29403_0(SerializableFunction<T, Row> toRow)
{    return new ToTableRow<>(toRow);}
public TableRow beam_f29404_0(T input)
{    return toTableRow(toRow.apply(input));}
public static Row beam_f29405_0(GenericRecord record, Schema schema, ConversionOptions options)
{    List<Object> valuesInOrder = schema.getFields().stream().map(field -> {        try {            return convertAvroFormat(field.getType(), record.get(field.getName()), options);        } catch (Exception cause) {            throw new IllegalArgumentException("Error converting field " + field + ": " + cause.getMessage(), cause);        }    }).collect(toList());    return Row.withSchema(schema).addValues(valuesInOrder).build();}
private static ReadableInstant beam_f29413_0(Object value)
{    long subMilliPrecision = ((long) value) % 1000;    if (subMilliPrecision != 0) {        throw new IllegalArgumentException(String.format("BigQuery data contained value %s with sub-millisecond precision, which Beam does" + " not currently support." + " You can enable truncating timestamps to millisecond precision" + " by using BigQueryIO.withTruncatedTimestamps", value));    } else {        return truncateToMillis(value);    }}
private static ReadableInstant beam_f29414_0(Object value)
{    return new Instant((long) value / 1000);}
private static Object beam_f29415_0(FieldType beamField, Object value, BigQueryUtils.ConversionOptions options)
{        List<Object> values = (List<Object>) value;    List<Object> ret = new ArrayList();    FieldType collectionElement = beamField.getCollectionElementType();    for (Object v : values) {        ret.add(convertAvroFormat(collectionElement, v, options));    }    return (Object) ret;}
private TableDestination beam_f29423_0(ProcessContext context, DestinationT destination)
{    TableDestination tableDestination = dynamicDestinations.getTable(destination);    checkArgument(tableDestination != null, "DynamicDestinations.getTable() may not return null, " + "but %s returned null for destination %s", dynamicDestinations, destination);    checkArgument(tableDestination.getTableSpec() != null, "DynamicDestinations.getTable() must return a TableDestination " + "with a non-null table spec, but %s returned %s for destination %s," + "which has a null table spec", dynamicDestinations, tableDestination, destination);    boolean destinationCoderSupportsClustering = !(dynamicDestinations.getDestinationCoder() instanceof TableDestinationCoderV2);    checkArgument(tableDestination.getClustering() == null || destinationCoderSupportsClustering, "DynamicDestinations.getTable() may only return destinations with clustering configured" + " if a destination coder is supplied that supports clustering, but %s is configured" + " to use TableDestinationCoderV2. Set withClustering() on BigQueryIO.write() and, " + " if you provided a custom DynamicDestinations instance, override" + " getDestinationCoder() to return TableDestinationCoderV3.", dynamicDestinations);    TableReference tableReference = tableDestination.getTableReference().clone();    if (Strings.isNullOrEmpty(tableReference.getProjectId())) {        tableReference.setProjectId(context.getPipelineOptions().as(BigQueryOptions.class).getProject());        tableDestination = tableDestination.withTableReference(tableReference);    }    if (createDisposition == CreateDisposition.CREATE_NEVER) {        return tableDestination;    }    String tableSpec = BigQueryHelpers.stripPartitionDecorator(tableDestination.getTableSpec());    if (!createdTables.contains(tableSpec)) {                synchronized (createdTables) {            if (!createdTables.contains(tableSpec)) {                tryCreateTable(context, destination, tableDestination, tableSpec, kmsKey);            }        }    }    return tableDestination;}
private void beam_f29424_0(ProcessContext context, DestinationT destination, TableDestination tableDestination, String tableSpec, String kmsKey)
{    DatasetService datasetService = bqServices.getDatasetService(context.getPipelineOptions().as(BigQueryOptions.class));    TableReference tableReference = tableDestination.getTableReference().clone();    tableReference.setTableId(BigQueryHelpers.stripPartitionDecorator(tableReference.getTableId()));    try {        if (datasetService.getTable(tableReference) == null) {            TableSchema tableSchema = dynamicDestinations.getSchema(destination);            checkArgument(tableSchema != null, "Unless create disposition is %s, a schema must be specified, i.e. " + "DynamicDestinations.getSchema() may not return null. " + "However, create disposition is %s, and " + " %s returned null for destination %s", CreateDisposition.CREATE_NEVER, createDisposition, dynamicDestinations, destination);            Table table = new Table().setTableReference(tableReference).setSchema(tableSchema).setDescription(tableDestination.getTableDescription());            if (tableDestination.getTimePartitioning() != null) {                table.setTimePartitioning(tableDestination.getTimePartitioning());                if (tableDestination.getClustering() != null) {                    table.setClustering(tableDestination.getClustering());                }            }            if (kmsKey != null) {                table.setEncryptionConfiguration(new EncryptionConfiguration().setKmsKeyName(kmsKey));            }            datasetService.createTable(table);        }    } catch (Exception e) {        throw new RuntimeException(e);    }    createdTables.add(tableSpec);}
 static void beam_f29425_0()
{    synchronized (createdTables) {        createdTables.clear();    }}
 static ConstantTableDestinations<T> beam_f29433_0(ValueProvider<String> tableSpec, String tableDescription)
{    return new ConstantTableDestinations<>(tableSpec, tableDescription);}
 static ConstantTableDestinations<T> beam_f29434_0(ValueProvider<String> jsonTableRef, String tableDescription)
{    return new ConstantTableDestinations<>(NestedValueProvider.of(jsonTableRef, new JsonTableRefToTableSpec()), tableDescription);}
public TableDestination beam_f29435_0(ValueInSingleWindow<T> element)
{    String tableSpec = this.tableSpec.get();    checkArgument(tableSpec != null, "tableSpec can not be null");    return new TableDestination(tableSpec, tableDescription);}
public DestinationT beam_f29443_0(ValueInSingleWindow<T> element)
{    return inner.getDestination(element);}
public TableSchema beam_f29444_0(DestinationT destination)
{    return inner.getSchema(destination);}
public TableDestination beam_f29445_0(DestinationT destination)
{    return inner.getTable(destination);}
public TableDestination beam_f29453_0(ValueInSingleWindow<T> element)
{    TableDestination destination = super.getDestination(element);    String partitioning = this.jsonTimePartitioning.get();    checkArgument(partitioning != null, "jsonTimePartitioning can not be null");    return new TableDestination(destination.getTableSpec(), destination.getTableDescription(), partitioning, Optional.ofNullable(jsonClustering).map(ValueProvider::get).orElse(null));}
public Coder<TableDestination> beam_f29454_0()
{    if (jsonClustering != null) {        return TableDestinationCoderV3.of();    } else {        return TableDestinationCoderV2.of();    }}
public String beam_f29455_0()
{    MoreObjects.ToStringHelper helper = MoreObjects.toStringHelper(this).add("inner", inner).add("jsonTimePartitioning", jsonTimePartitioning);    if (jsonClustering != null) {        helper.add("jsonClustering", jsonClustering);    }    return helper.toString();}
public TableSchema beam_f29463_0(DestinationT destination)
{    TableDestination wrappedDestination = super.getTable(destination);    Table existingTable = getBigQueryTable(wrappedDestination.getTableReference());    if (existingTable == null) {        return super.getSchema(destination);    } else {        return existingTable.getSchema();    }}
public void beam_f29464_0(ProcessContext context, BoundedWindow window) throws IOException
{    ThreadLocalRandom randomGenerator = ThreadLocalRandom.current();        String tableSpec = context.element().getKey().getTableSpec();    context.output(KV.of(ShardedKey.of(tableSpec, randomGenerator.nextInt(0, numShards)), context.element().getValue()));}
public static InsertRetryPolicy beam_f29465_0()
{    return new InsertRetryPolicy() {        @Override        public boolean shouldRetry(Context context) {            return false;        }    };}
public void beam_f29473_0(ProcessContext c)
{    c.output(c.element());}
public int beam_f29474_0()
{    return 0;}
public boolean beam_f29475_0(Object obj)
{    return obj != null && obj.getClass() == this.getClass();}
public TableSchema beam_f29483_0()
{    return tableSchema;}
public StreamingInserts<DestinationT, ElementT> beam_f29484_0(InsertRetryPolicy retryPolicy)
{    return new StreamingInserts<>(createDisposition, dynamicDestinations, bigQueryServices, retryPolicy, extendedErrorInfo, skipInvalidRows, ignoreUnknownValues, elementCoder, toTableRow, kmsKey);}
public StreamingInserts<DestinationT, ElementT> beam_f29485_0(boolean extendedErrorInfo)
{    return new StreamingInserts<>(createDisposition, dynamicDestinations, bigQueryServices, retryPolicy, extendedErrorInfo, skipInvalidRows, ignoreUnknownValues, elementCoder, toTableRow, kmsKey);}
public void beam_f29493_0(FinishBundleContext context) throws Exception
{    List<ValueInSingleWindow<ErrorT>> failedInserts = Lists.newArrayList();    BigQueryOptions options = context.getPipelineOptions().as(BigQueryOptions.class);    for (Map.Entry<String, List<ValueInSingleWindow<TableRow>>> entry : tableRows.entrySet()) {        TableReference tableReference = BigQueryHelpers.parseTableSpec(entry.getKey());        flushRows(tableReference, entry.getValue(), uniqueIdsForTableRows.get(entry.getKey()), options, failedInserts);    }    tableRows.clear();    uniqueIdsForTableRows.clear();    for (ValueInSingleWindow<ErrorT> row : failedInserts) {        context.output(failedOutputTag, row.getValue(), row.getTimestamp(), row.getWindow());    }}
private void beam_f29494_0(TableReference tableReference, List<ValueInSingleWindow<TableRow>> tableRows, List<String> uniqueIds, BigQueryOptions options, List<ValueInSingleWindow<ErrorT>> failedInserts) throws InterruptedException
{    if (!tableRows.isEmpty()) {        try {            long totalBytes = bqServices.getDatasetService(options).insertAll(tableReference, tableRows, uniqueIds, retryPolicy, failedInserts, errorContainer, skipInvalidRows, ignoreUnknownValues);            byteCounter.inc(totalBytes);        } catch (IOException e) {            throw new RuntimeException(e);        }    }}
 StreamingWriteTables<ElementT> beam_f29495_0(BigQueryServices bigQueryServices)
{    return new StreamingWriteTables<>(bigQueryServices, retryPolicy, extendedErrorInfo, skipInvalidRows, ignoreUnknownValues, elementCoder, toTableRow);}
private PCollection<T> beam_f29503_0(PCollection<KV<TableDestination, ElementT>> input, TupleTag<T> failedInsertsTag, AtomicCoder<T> coder, ErrorContainer<T> errorContainer)
{    BigQueryOptions options = input.getPipeline().getOptions().as(BigQueryOptions.class);    int numShards = options.getNumStreamingKeys();                                            PCollection<KV<ShardedKey<String>, TableRowInfo<ElementT>>> tagged = input.apply("ShardTableWrites", ParDo.of(new GenerateShardedTable<>(numShards))).setCoder(KvCoder.of(ShardedKeyCoder.of(StringUtf8Coder.of()), elementCoder)).apply("TagWithUniqueIds", ParDo.of(new TagWithUniqueIds<>())).setCoder(KvCoder.of(ShardedKeyCoder.of(StringUtf8Coder.of()), TableRowInfoCoder.of(elementCoder)));    TupleTag<Void> mainOutputTag = new TupleTag<>("mainOutput");                    PCollectionTuple tuple = tagged.apply(Reshuffle.of()).apply("GlobalWindow", Window.<KV<ShardedKey<String>, TableRowInfo<ElementT>>>into(new GlobalWindows()).triggering(DefaultTrigger.of()).discardingFiredPanes()).apply("StreamingWrite", ParDo.of(new StreamingWriteFn<>(bigQueryServices, retryPolicy, failedInsertsTag, errorContainer, skipInvalidRows, ignoreUnknownValues, toTableRow)).withOutputTags(mainOutputTag, TupleTagList.of(failedInsertsTag)));    PCollection<T> failedInserts = tuple.get(failedInsertsTag);    failedInserts.setCoder(coder);    return failedInserts;}
public TableDestination beam_f29504_0(TableReference tableReference)
{    return new TableDestination(tableReference, tableDescription, jsonTimePartitioning, jsonClustering);}
public String beam_f29505_0()
{    return tableSpec;}
public boolean beam_f29513_0(Object o)
{    if (!(o instanceof TableDestination)) {        return false;    }    TableDestination other = (TableDestination) o;    return Objects.equals(this.tableSpec, other.tableSpec) && Objects.equals(this.tableDescription, other.tableDescription) && Objects.equals(this.jsonTimePartitioning, other.jsonTimePartitioning);}
public int beam_f29514_0()
{    return Objects.hash(tableSpec, tableDescription, jsonTimePartitioning);}
public static TableDestinationCoder beam_f29515_0()
{    return INSTANCE;}
public TableDestination beam_f29525_0(InputStream inStream) throws IOException
{    TableDestination destination = TableDestinationCoder.of().decode(inStream);    String jsonTimePartitioning = timePartitioningCoder.decode(inStream);    String jsonClustering = clusteringCoder.decode(inStream);    return new TableDestination(destination.getTableSpec(), destination.getTableDescription(), jsonTimePartitioning, jsonClustering);}
public static TableRowInfoCoder beam_f29527_0(Coder<ElementT> elementCoder)
{    return new TableRowInfoCoder(elementCoder);}
public void beam_f29528_0(TableRowInfo<ElementT> value, OutputStream outStream) throws IOException
{    encode(value, outStream, Context.NESTED);}
public TableRow beam_f29536_0(InputStream inStream) throws IOException
{    return decode(inStream, Context.NESTED);}
public TableRow beam_f29537_0(InputStream inStream, Context context) throws IOException
{    String strValue = StringUtf8Coder.of().decode(inStream, context);    return MAPPER.readValue(strValue, TableRow.class);}
protected long beam_f29538_0(TableRow value) throws Exception
{    String strValue = MAPPER.writeValueAsString(value);    return StringUtf8Coder.of().getEncodedElementByteSize(strValue);}
public void beam_f29546_0(ProcessContext context, BoundedWindow window) throws IOException
{    String uniqueId = randomUUID + sequenceNo++;            context.output(KV.of(context.element().getKey(), new TableRowInfo(context.element().getValue(), uniqueId)));}
public static TestBigQuery beam_f29547_0(Schema tableSchema)
{    return new TestBigQuery(TestPipeline.testingPipelineOptions().as(TestBigQueryOptions.class), tableSchema);}
public Statement beam_f29548_0(Statement base, Description description)
{    return new Statement() {        @Override        public void evaluate() throws Throwable {            if (TestBigQuery.this.datasetService != null) {                throw new AssertionError("BigQuery test was not shutdown previously. " + "Table is'" + table + "'. " + "Current test: " + description.getDisplayName());            }            try {                initializeBigQuery(description);                base.evaluate();            } finally {                tearDown();            }        }    };}
public List<Row> beam_f29556_0(Schema rowSchema)
{    Bigquery bq = newBigQueryClient(pipelineOptions);    return bqRowsToBeamRows(getSchema(bq), getTableRows(bq), rowSchema);}
public RowsAssertion beam_f29557_0(Schema rowSchema)
{    return matcher -> duration -> pollAndAssert(rowSchema, matcher, duration);}
private void beam_f29558_0(Schema rowSchema, Matcher<Iterable<? extends Row>> matcher, Duration duration)
{    DateTime start = DateTime.now();    while (true) {        try {            assertThat(getFlatJsonRows(rowSchema), matcher);            break;        } catch (AssertionError assertionError) {            if (secondsBetween(start, DateTime.now()).isGreaterThan(duration.toStandardSeconds())) {                throw assertionError;            }            sleep(15_000);        }    }}
public int beam_f29566_0()
{    return Objects.hash(filename, fileByteSize, destination);}
public String beam_f29567_0()
{    return "Result{" + "filename='" + filename + '\'' + ", fileByteSize=" + fileByteSize + ", destination=" + destination + '}';}
public static ResultCoder<DestinationT> beam_f29568_0(Coder<DestinationT> destinationCoder)
{    return new ResultCoder<>(destinationCoder);}
public void beam_f29577_0(ProcessContext c, @Element KV<ShardedKey<DestinationT>, Iterable<ElementT>> element, OutputReceiver<WriteBundlesToFiles.Result<DestinationT>> o) throws Exception
{    String tempFilePrefix = c.sideInput(this.tempFilePrefix);    TableRowWriter writer = new TableRowWriter(tempFilePrefix);    try {        for (ElementT tableRow : element.getValue()) {            if (writer.getByteSize() > maxFileSize) {                writer.close();                writer = new TableRowWriter(tempFilePrefix);                TableRowWriter.Result result = writer.getResult();                o.output(new WriteBundlesToFiles.Result<>(result.resourceId.toString(), result.byteSize, c.element().getKey().getKey()));            }            writer.write(toRowFunction.apply(tableRow));        }    } finally {        writer.close();    }    TableRowWriter.Result result = writer.getResult();    o.output(new WriteBundlesToFiles.Result<>(result.resourceId.toString(), result.byteSize, c.element().getKey().getKey()));}
 static PartitionData beam_f29578_0(int maxNumFiles, long maxSizeBytes)
{    return new PartitionData(maxNumFiles, maxSizeBytes);}
 int beam_f29579_0()
{    return numFiles;}
 List<PartitionData> beam_f29587_0()
{    return partitions;}
 PartitionData beam_f29588_0()
{    return partitions.get(partitions.size() - 1);}
 void beam_f29589_0(PartitionData partition)
{    partitions.add(partition);}
public void beam_f29597_0(DisplayData.Builder builder)
{    super.populateDisplayData(builder);    builder.add(DisplayData.item("firstPaneWriteDisposition", firstPaneWriteDisposition.toString()).withLabel("Write Disposition")).add(DisplayData.item("firstPaneCreateDisposition", firstPaneCreateDisposition.toString()).withLabel("Create Disposition"));}
 static WriteResult beam_f29598_0(Pipeline pipeline, TupleTag<TableRow> failedInsertsTag, PCollection<TableRow> failedInserts)
{    return new WriteResult(pipeline, failedInsertsTag, failedInserts, null, null);}
 static WriteResult beam_f29599_0(Pipeline pipeline, TupleTag<BigQueryInsertError> failedInsertsTag, PCollection<BigQueryInsertError> failedInserts)
{    return new WriteResult(pipeline, null, null, failedInsertsTag, failedInserts);}
public void beam_f29608_0(ProcessContext c) throws Exception
{    removeTemporaryFiles(c.element());}
public PCollection<KV<TableDestination, String>> beam_f29609_0(PCollection<KV<ShardedKey<DestinationT>, List<String>>> input)
{    PCollectionTuple writeTablesOutputs = input.apply(ParDo.of(new WriteTablesDoFn()).withSideInputs(sideInputs).withOutputTags(mainOutputTag, TupleTagList.of(temporaryFilesTag)));                            writeTablesOutputs.get(temporaryFilesTag).setCoder(StringUtf8Coder.of()).apply(WithKeys.of((Void) null)).setCoder(KvCoder.of(VoidCoder.of(), StringUtf8Coder.of())).apply(Window.<KV<Void, String>>into(new GlobalWindows()).triggering(Repeatedly.forever(AfterPane.elementCountAtLeast(1))).discardingFiredPanes()).apply(GroupByKey.create()).apply(Values.create()).apply(ParDo.of(new GarbageCollectTemporaryFiles()));    return writeTablesOutputs.get(mainOutputTag);}
private PendingJob beam_f29610_1(JobService jobService, DatasetService datasetService, String jobIdPrefix, TableReference ref, TimePartitioning timePartitioning, Clustering clustering, @Nullable TableSchema schema, List<String> gcsUris, WriteDisposition writeDisposition, CreateDisposition createDisposition)
{    JobConfigurationLoad loadConfig = new JobConfigurationLoad().setDestinationTable(ref).setSchema(schema).setSourceUris(gcsUris).setWriteDisposition(writeDisposition.name()).setCreateDisposition(createDisposition.name()).setSourceFormat("NEWLINE_DELIMITED_JSON").setIgnoreUnknownValues(ignoreUnknownValues);    if (timePartitioning != null) {        loadConfig.setTimePartitioning(timePartitioning);                if (clustering != null) {            loadConfig.setClustering(clustering);        }    }    if (kmsKey != null) {        loadConfig.setDestinationEncryptionConfiguration(new EncryptionConfiguration().setKmsKeyName(kmsKey));    }    String projectId = loadJobProjectId == null ? ref.getProjectId() : loadJobProjectId.get();    String bqLocation = BigQueryHelpers.getDatasetLocation(datasetService, ref.getProjectId(), ref.getDatasetId());    PendingJob retryJob = new PendingJob(    jobId -> {        JobReference jobRef = new JobReference().setProjectId(projectId).setJobId(jobId.getJobId()).setLocation(bqLocation);                try {            jobService.startLoadJob(jobRef, loadConfig);        } catch (IOException | InterruptedException e) {                        throw new RuntimeException(e);        }        return null;    },     jobId -> {        JobReference jobRef = new JobReference().setProjectId(projectId).setJobId(jobId.getJobId()).setLocation(bqLocation);        try {            return jobService.pollJob(jobRef, BatchLoads.LOAD_JOB_POLL_MAX_RETRIES);        } catch (InterruptedException e) {            throw new RuntimeException(e);        }    },     jobId -> {        JobReference jobRef = new JobReference().setProjectId(projectId).setJobId(jobId.getJobId()).setLocation(bqLocation);        try {            return jobService.getJob(jobRef);        } catch (InterruptedException | IOException e) {            throw new RuntimeException(e);        }    }, maxRetryJobs, jobIdPrefix);    return retryJob;}
 BigtableConfig beam_f29618_0(boolean isEnabled)
{    return toBuilder().setValidate(isEnabled).build();}
 BigtableConfig beam_f29619_0(BigtableService bigtableService)
{    checkArgument(bigtableService != null, "bigtableService can not be null");    return toBuilder().setBigtableService(bigtableService).build();}
 void beam_f29620_0()
{    checkArgument(getTableId() != null && (!getTableId().isAccessible() || !getTableId().get().isEmpty()), "Could not obtain Bigtable table id");    checkArgument((getProjectId() != null && (!getProjectId().isAccessible() || !getProjectId().get().isEmpty())) || (getBigtableOptions() != null && getBigtableOptions().getProjectId() != null && !getBigtableOptions().getProjectId().isEmpty()), "Could not obtain Bigtable project id");    checkArgument((getInstanceId() != null && (!getInstanceId().isAccessible() || !getInstanceId().get().isEmpty())) || (getBigtableOptions() != null && getBigtableOptions().getInstanceId() != null && !getBigtableOptions().getInstanceId().isEmpty()), "Could not obtain Bigtable instance id");}
public String beam_f29628_0()
{    ValueProvider<String> tableId = getBigtableConfig().getTableId();    return tableId != null && tableId.isAccessible() ? tableId.get() : null;}
public BigtableOptions beam_f29629_0()
{    return getBigtableConfig().getBigtableOptions();}
 static Read beam_f29630_0()
{    BigtableConfig config = BigtableConfig.builder().setTableId(ValueProvider.StaticValueProvider.of("")).setValidate(true).build();    return new AutoValue_BigtableIO_Read.Builder().setBigtableConfig(config).setKeyRanges(Arrays.asList(ByteKeyRange.ALL_KEYS)).build();}
public Read beam_f29638_0(BigtableOptions.Builder optionsBuilder)
{    BigtableConfig config = getBigtableConfig();        return toBuilder().setBigtableConfig(config.withBigtableOptions(optionsBuilder.build().toBuilder().build())).build();}
public Read beam_f29639_0(SerializableFunction<BigtableOptions.Builder, BigtableOptions.Builder> configurator)
{    BigtableConfig config = getBigtableConfig();    return toBuilder().setBigtableConfig(config.withBigtableOptionsConfigurator(configurator)).build();}
public Read beam_f29640_0(RowFilter filter)
{    checkArgument(filter != null, "filter can not be null");    return toBuilder().setRowFilter(filter).build();}
public String beam_f29648_0()
{    ToStringHelper helper = MoreObjects.toStringHelper(Read.class).add("config", getBigtableConfig());    for (int i = 0; i < getKeyRanges().size(); i++) {        helper.add("keyRange " + i, getKeyRanges().get(i));    }    return helper.add("filter", getRowFilter()).toString();}
 static SerializableFunction<BigtableOptions.Builder, BigtableOptions.Builder> beam_f29649_0(@Nullable final SerializableFunction<BigtableOptions.Builder, BigtableOptions.Builder> userConfigurator)
{    return optionsBuilder -> {        if (userConfigurator != null) {            optionsBuilder = userConfigurator.apply(optionsBuilder);        }        return optionsBuilder.setBulkOptions(optionsBuilder.build().getBulkOptions().toBuilder().setUseBulkApi(true).build());    };}
public BigtableOptions beam_f29650_0()
{    return getBigtableConfig().getBigtableOptions();}
public Write beam_f29658_0(BigtableOptions options)
{    checkArgument(options != null, "options can not be null");    return withBigtableOptions(options.toBuilder());}
public Write beam_f29659_0(BigtableOptions.Builder optionsBuilder)
{    BigtableConfig config = getBigtableConfig();        return toBuilder().setBigtableConfig(config.withBigtableOptions(optionsBuilder.build().toBuilder().build())).build();}
public Write beam_f29660_0(SerializableFunction<BigtableOptions.Builder, BigtableOptions.Builder> configurator)
{    BigtableConfig config = getBigtableConfig();    return toBuilder().setBigtableConfig(config.withBigtableOptionsConfigurator(enableBulkApiConfigurator(configurator))).build();}
public PCollection<BigtableWriteResult> beam_f29668_0(PCollection<KV<ByteString, Iterable<Mutation>>> input)
{    bigtableConfig.validate();    return input.apply(ParDo.of(new BigtableWriterFn(bigtableConfig)));}
public void beam_f29669_0(PipelineOptions options)
{    validateTableExists(bigtableConfig, options);}
public void beam_f29670_0(DisplayData.Builder builder)
{    super.populateDisplayData(builder);    bigtableConfig.populateDisplayData(builder);}
private static ByteKey beam_f29678_0(ByteString key)
{    return ByteKey.copyFrom(key.asReadOnlyByteBuffer());}
public String beam_f29679_0()
{    return MoreObjects.toStringHelper(BigtableSource.class).add("config", config).add("filter", filter).add("ranges", ranges).add("estimatedSizeBytes", estimatedSizeBytes).toString();}
protected BigtableSource beam_f29680_0(ByteKeyRange range)
{    checkArgument(range != null, "range can not be null");    return new BigtableSource(config, filter, Arrays.asList(range), estimatedSizeBytes);}
private List<BigtableSource> beam_f29688_1(long desiredBundleSizeBytes, List<SampleRowKeysResponse> sampleRowKeys)
{        if (sampleRowKeys.isEmpty()) {                return Collections.singletonList(this);    }        ImmutableList.Builder<BigtableSource> splits = ImmutableList.builder();    for (ByteKeyRange range : ranges) {        splits.addAll(splitRangeBasedOnSamples(desiredBundleSizeBytes, sampleRowKeys, range));    }    return splits.build();}
private List<BigtableSource> beam_f29689_1(long desiredBundleSizeBytes, List<SampleRowKeysResponse> sampleRowKeys, ByteKeyRange range)
{                ByteKey lastEndKey = ByteKey.EMPTY;    long lastOffset = 0;    ImmutableList.Builder<BigtableSource> splits = ImmutableList.builder();    for (SampleRowKeysResponse response : sampleRowKeys) {        ByteKey responseEndKey = makeByteKey(response.getRowKey());        long responseOffset = response.getOffsetBytes();        checkState(responseOffset >= lastOffset, "Expected response byte offset %s to come after the last offset %s", responseOffset, lastOffset);        if (!range.overlaps(ByteKeyRange.of(lastEndKey, responseEndKey))) {                        lastOffset = responseOffset;            lastEndKey = responseEndKey;            continue;        }                        ByteKey splitStartKey = lastEndKey;        if (splitStartKey.compareTo(range.getStartKey()) < 0) {            splitStartKey = range.getStartKey();        }                        ByteKey splitEndKey = responseEndKey;        if (!range.containsKey(splitEndKey)) {            splitEndKey = range.getEndKey();        }                        long sampleSizeBytes = responseOffset - lastOffset;        List<BigtableSource> subSplits = splitKeyRangeIntoBundleSizedSubranges(sampleSizeBytes, desiredBundleSizeBytes, ByteKeyRange.of(splitStartKey, splitEndKey));        splits.addAll(subSplits);                lastEndKey = responseEndKey;        lastOffset = responseOffset;    }        if (!lastEndKey.isEmpty() && (range.getEndKey().isEmpty() || lastEndKey.compareTo(range.getEndKey()) < 0)) {        splits.add(this.withSingleRange(ByteKeyRange.of(lastEndKey, range.getEndKey())));    }    List<BigtableSource> ret = splits.build();        return ret;}
public long beam_f29690_0(PipelineOptions options) throws IOException
{        if (estimatedSizeBytes == null) {        estimatedSizeBytes = getEstimatedSizeBytesBasedOnSamples(getSampleRowKeys(options));    }    return estimatedSizeBytes;}
public RowFilter beam_f29698_0()
{    return filter;}
public ValueProvider<String> beam_f29699_0()
{    return config.getTableId();}
public boolean beam_f29700_0() throws IOException
{    reader = service.createReader(getCurrentSource());    boolean hasRecord = (reader.start() && rangeTracker.tryReturnRecordAt(true, makeByteKey(reader.getCurrentRow().getKey()))) || rangeTracker.markDone();    if (hasRecord) {        ++recordsReturned;    }    return hasRecord;}
public BigtableOptions beam_f29709_0()
{    return options;}
public BigtableWriterImpl beam_f29710_0(String tableId) throws IOException
{    BigtableSession session = new BigtableSession(options);    BigtableTableName tableName = options.getInstanceName().toTableName(tableId);    return new BigtableWriterImpl(session, tableName);}
public boolean beam_f29711_1(String tableId) throws IOException
{    try (BigtableSession session = new BigtableSession(options)) {        GetTableRequest getTable = GetTableRequest.newBuilder().setName(options.getInstanceName().toTableNameStr(tableId)).build();        session.getTableAdminClient().getTable(getTable);        return true;    } catch (StatusRuntimeException e) {        if (e.getStatus().getCode() == Code.NOT_FOUND) {            return false;        }        String message = String.format("Error checking whether table %s (BigtableOptions %s) exists", tableId, options);                throw new IOException(message, e);    }}
public void beam_f29719_0(MutateRowResponse mutateRowResponse)
{    result.complete(mutateRowResponse);}
public void beam_f29720_0(Throwable throwable)
{    result.completeExceptionally(throwable);}
public String beam_f29721_0()
{    return MoreObjects.toStringHelper(BigtableServiceImpl.class).add("options", options).toString();}
public void beam_f29729_0(Runnable listener, Executor executor)
{    underlying.addListener(listener, executor);}
public boolean beam_f29730_0(boolean mayInterruptIfRunning)
{    return underlying.cancel(mayInterruptIfRunning);}
public boolean beam_f29731_0()
{    return underlying.isCancelled();}
public static DatastoreV1 beam_f29739_0()
{    return new DatastoreV1();}
public DatastoreV1.Read beam_f29740_0()
{    return new AutoValue_DatastoreV1_Read.Builder().setNumQuerySplits(0).build();}
 static int beam_f29741_1(Datastore datastore, Query query, @Nullable String namespace)
{    int numSplits;    try {        long estimatedSizeBytes = getEstimatedSizeBytes(datastore, query, namespace);                numSplits = (int) Math.min(NUM_QUERY_SPLITS_MAX, Math.round(((double) estimatedSizeBytes) / DEFAULT_BUNDLE_SIZE_BYTES));    } catch (Exception e) {                        numSplits = NUM_QUERY_SPLITS_MIN;    }    return Math.max(numSplits, NUM_QUERY_SPLITS_MIN);}
private static Query beam_f29749_0(String gql, Datastore datastore, String namespace) throws DatastoreException
{    GqlQuery gqlQuery = GqlQuery.newBuilder().setQueryString(gql).setAllowLiterals(true).build();    RunQueryRequest req = makeRequest(gqlQuery, namespace);    return datastore.runQuery(req).getQuery();}
public DatastoreV1.Read beam_f29750_0(String projectId)
{    checkArgument(projectId != null, "projectId can not be null");    return toBuilder().setProjectId(StaticValueProvider.of(projectId)).build();}
public DatastoreV1.Read beam_f29751_0(ValueProvider<String> projectId)
{    checkArgument(projectId != null, "projectId can not be null");    return toBuilder().setProjectId(projectId).build();}
public PCollection<Entity> beam_f29759_0(PBegin input)
{    checkArgument(getProjectId() != null, "projectId provider cannot be null");    if (getProjectId().isAccessible()) {        checkArgument(getProjectId().get() != null, "projectId cannot be null");    }    checkArgument(getQuery() != null || getLiteralGqlQuery() != null, "Either withQuery() or withLiteralGqlQuery() is required");    checkArgument(getQuery() == null || getLiteralGqlQuery() == null, "withQuery() and withLiteralGqlQuery() are exclusive");    V1Options v1Options = V1Options.from(getProjectId(), getNamespace(), getLocalhost());    /*       * This composite transform involves the following steps:       *   1. Create a singleton of the user provided {@code query} or if {@code gqlQuery} is       *   provided apply a {@link ParDo} that translates the {@code gqlQuery} into a {@code query}.       *       *   2. A {@link ParDo} splits the resulting query into {@code numQuerySplits} and       *   assign each split query a unique {@code Integer} as the key. The resulting output is       *   of the type {@code PCollection<KV<Integer, Query>>}.       *       *   If the value of {@code numQuerySplits} is less than or equal to 0, then the number of       *   splits will be computed dynamically based on the size of the data for the {@code query}.       *       *   3. The resulting {@code PCollection} is sharded using a {@link GroupByKey} operation. The       *   queries are extracted from they {@code KV<Integer, Iterable<Query>>} and flattened to       *   output a {@code PCollection<Query>}.       *       *   4. In the third step, a {@code ParDo} reads entities for each query and outputs       *   a {@code PCollection<Entity>}.       */    PCollection<Query> inputQuery;    if (getQuery() != null) {        inputQuery = input.apply(Create.of(getQuery()));    } else {        inputQuery = input.apply(Create.ofProvider(getLiteralGqlQuery(), StringUtf8Coder.of())).apply(ParDo.of(new GqlQueryTranslateFn(v1Options)));    }    return inputQuery.apply("Split", ParDo.of(new SplitQueryFn(v1Options, getNumQuerySplits()))).apply("Reshuffle", Reshuffle.viaRandomKey()).apply("Read", ParDo.of(new ReadFn(v1Options)));}
public void beam_f29760_0(DisplayData.Builder builder)
{    super.populateDisplayData(builder);    String query = getQuery() == null ? null : getQuery().toString();    builder.addIfNotNull(DisplayData.item("projectId", getProjectId()).withLabel("ProjectId")).addIfNotNull(DisplayData.item("namespace", getNamespace()).withLabel("Namespace")).addIfNotNull(DisplayData.item("query", query).withLabel("Query")).addIfNotNull(DisplayData.item("gqlQuery", getLiteralGqlQuery()).withLabel("GqlQuery"));}
public static V1Options beam_f29761_0(String projectId, String namespace, String localhost)
{    return from(StaticValueProvider.of(projectId), StaticValueProvider.of(namespace), localhost);}
public void beam_f29769_0(StartBundleContext c) throws Exception
{    datastore = datastoreFactory.getDatastore(c.getPipelineOptions(), v1Options.getProjectId(), v1Options.getLocalhost());}
public void beam_f29770_1(ProcessContext c) throws Exception
{    String gqlQuery = c.element();        Query query = translateGqlQueryWithLimitCheck(gqlQuery, datastore, v1Options.getNamespace());        c.output(query);}
public void beam_f29771_0(StartBundleContext c) throws Exception
{    datastore = datastoreFactory.getDatastore(c.getPipelineOptions(), options.getProjectId(), options.getLocalhost());    querySplitter = datastoreFactory.getQuerySplitter();}
public DeleteEntity beam_f29779_0()
{    return new DeleteEntity(null, null);}
public DeleteKey beam_f29780_0()
{    return new DeleteKey(null, null);}
public Write beam_f29781_0(String projectId)
{    checkArgument(projectId != null, "projectId can not be null");    return withProjectId(StaticValueProvider.of(projectId));}
public DeleteKey beam_f29789_0(ValueProvider<String> projectId)
{    checkArgument(projectId != null, "projectId can not be null");    return new DeleteKey(projectId, localhost);}
public PDone beam_f29790_0(PCollection<T> input)
{    checkArgument(projectId != null, "withProjectId() is required");    if (projectId.isAccessible()) {        checkArgument(projectId.get() != null, "projectId can not be null");    }    checkArgument(mutationFn != null, "mutationFn can not be null");    input.apply("Convert to Mutation", MapElements.via(mutationFn)).apply("Write Mutation to Datastore", ParDo.of(new DatastoreWriterFn(projectId, localhost)));    return PDone.in(input.getPipeline());}
public String beam_f29791_0()
{    return MoreObjects.toStringHelper(getClass()).add("projectId", projectId).add("mutationFn", mutationFn.getClass().getName()).toString();}
public void beam_f29799_0() throws Exception
{    if (!mutations.isEmpty()) {        flushBatch();    }}
private void beam_f29800_1() throws DatastoreException, IOException, InterruptedException
{        Sleeper sleeper = Sleeper.DEFAULT;    BackOff backoff = BUNDLE_WRITE_BACKOFF.backoff();    while (true) {                CommitRequest.Builder commitRequest = CommitRequest.newBuilder();        commitRequest.addAllMutations(mutations);        commitRequest.setMode(CommitRequest.Mode.NON_TRANSACTIONAL);        long startTime = System.currentTimeMillis(), endTime;        if (throttler.throttleRequest(startTime)) {                        throttledSeconds.inc(WriteBatcherImpl.DATASTORE_BATCH_TARGET_LATENCY_MS / 1000);            sleeper.sleep(WriteBatcherImpl.DATASTORE_BATCH_TARGET_LATENCY_MS);            continue;        }        try {            datastore.commit(commitRequest.build());            endTime = System.currentTimeMillis();            writeBatcher.addRequestLatency(endTime, endTime - startTime, mutations.size());            throttler.successfulRequest(startTime);            rpcSuccesses.inc();                        break;        } catch (DatastoreException exception) {            if (exception.getCode() == Code.DEADLINE_EXCEEDED) {                /* Most errors are not related to request size, and should not change our expectation of             * the latency of successful requests. DEADLINE_EXCEEDED can be taken into             * consideration, though. */                endTime = System.currentTimeMillis();                writeBatcher.addRequestLatency(endTime, endTime - startTime, mutations.size());            }                                                rpcErrors.inc();            if (NON_RETRYABLE_ERRORS.contains(exception.getCode())) {                throw exception;            }            if (!BackOffUtils.next(sleeper, backoff)) {                                throw exception;            }        }    }        mutations.clear();    mutationsSize = 0;}
public void beam_f29801_0(Builder builder)
{    super.populateDisplayData(builder);    builder.addIfNotNull(DisplayData.item("projectId", projectId).withLabel("Output Project"));}
public Datastore beam_f29809_0(PipelineOptions pipelineOptions, String projectId)
{    return getDatastore(pipelineOptions, projectId, null);}
public Datastore beam_f29810_0(PipelineOptions pipelineOptions, String projectId, @Nullable String localhost)
{    Credentials credential = pipelineOptions.as(GcpOptions.class).getGcpCredential();    HttpRequestInitializer initializer;    if (credential != null) {        initializer = new ChainingHttpRequestInitializer(new HttpCredentialsAdapter(credential), new RetryHttpRequestInitializer());    } else {        initializer = new RetryHttpRequestInitializer();    }    DatastoreOptions.Builder builder = new DatastoreOptions.Builder().projectId(projectId).initializer(initializer);    if (localhost != null) {        builder.localHost(localhost);    } else {        builder.host("batch-datastore.googleapis.com");    }    return DatastoreFactory.get().create(builder.build());}
public QuerySplitter beam_f29811_0()
{    return DatastoreHelper.getQuerySplitter();}
public boolean beam_f29819_0(Object o)
{    if (this == o) {        return true;    }    if (o == null || getClass() != o.getClass()) {        return false;    }    ProjectPath that = (ProjectPath) o;    return projectId.equals(that.projectId);}
public int beam_f29820_0()
{    return projectId.hashCode();}
public String beam_f29821_0()
{    return getPath();}
public String beam_f29829_0()
{    return getPath();}
public static SubscriptionPath beam_f29830_0(String path)
{    return new SubscriptionPath(path);}
public static SubscriptionPath beam_f29831_0(String projectId, String subscriptionName)
{    return new SubscriptionPath(String.format("projects/%s/subscriptions/%s", projectId, subscriptionName));}
public static TopicPath beam_f29839_0(String projectId, String topicName)
{    return new TopicPath(String.format("projects/%s/topics/%s", projectId, topicName));}
public String beam_f29840_0()
{    return String.format("OutgoingMessage(%db, %dms)", elementBytes.length, timestampMsSinceEpoch);}
public boolean beam_f29841_0(Object o)
{    if (this == o) {        return true;    }    if (o == null || getClass() != o.getClass()) {        return false;    }    OutgoingMessage that = (OutgoingMessage) o;    return timestampMsSinceEpoch == that.timestampMsSinceEpoch && Arrays.equals(elementBytes, that.elementBytes) && Objects.equal(attributes, that.attributes) && Objects.equal(recordId, that.recordId);}
public PubsubClient beam_f29849_0(@Nullable String timestampAttribute, @Nullable String idAttribute, PubsubOptions options) throws IOException
{    ManagedChannel channel = NettyChannelBuilder.forAddress(PUBSUB_ADDRESS, PUBSUB_PORT).negotiationType(NegotiationType.TLS).sslContext(GrpcSslContexts.forClient().ciphers(null).build()).build();    return new PubsubGrpcClient(timestampAttribute, idAttribute, DEFAULT_TIMEOUT_S, channel, options.getGcpCredential());}
public String beam_f29850_0()
{    return "Grpc";}
public void beam_f29851_0()
{    if (publisherChannel == null) {                return;    }        cachedPublisherStub = null;    cachedSubscriberStub = null;            ManagedChannel publisherChannel = this.publisherChannel;    this.publisherChannel = null;        publisherChannel.shutdown();    try {        publisherChannel.awaitTermination(timeoutSec, TimeUnit.SECONDS);    } catch (InterruptedException e) {                Thread.currentThread().interrupt();    }}
public void beam_f29859_0(TopicPath topic) throws IOException
{    Topic request = Topic.newBuilder().setName(topic.getPath()).build();        publisherStub().createTopic(request);}
public void beam_f29860_0(TopicPath topic) throws IOException
{    DeleteTopicRequest request = DeleteTopicRequest.newBuilder().setTopic(topic.getPath()).build();        publisherStub().deleteTopic(request);}
public List<TopicPath> beam_f29861_0(ProjectPath project) throws IOException
{    ListTopicsRequest.Builder request = ListTopicsRequest.newBuilder().setProject(project.getPath()).setPageSize(LIST_BATCH_SIZE);    ListTopicsResponse response = publisherStub().listTopics(request.build());    if (response.getTopicsCount() == 0) {        return ImmutableList.of();    }    List<TopicPath> topics = new ArrayList<>(response.getTopicsCount());    while (true) {        for (Topic topic : response.getTopicsList()) {            topics.add(topicPathFromPath(topic.getName()));        }        if (response.getNextPageToken().isEmpty()) {            break;        }        request.setPageToken(response.getNextPageToken());        response = publisherStub().listTopics(request.build());    }    return topics;}
private static void beam_f29869_0(DisplayData.Builder builder, String timestampAttribute, String idAttribute, ValueProvider<PubsubTopic> topic)
{    builder.addIfNotNull(DisplayData.item("timestampAttribute", timestampAttribute).withLabel("Timestamp Attribute")).addIfNotNull(DisplayData.item("idAttribute", idAttribute).withLabel("ID Attribute")).addIfNotNull(DisplayData.item("topic", topic).withLabel("Pubsub Topic"));}
public static PubsubSubscription beam_f29870_1(String path)
{    if (path.startsWith(SUBSCRIPTION_RANDOM_TEST_PREFIX) || path.startsWith(SUBSCRIPTION_STARTING_SIGNAL)) {        return new PubsubSubscription(Type.FAKE, "", path);    }    String projectName, subscriptionName;    Matcher v1beta1Match = V1BETA1_SUBSCRIPTION_REGEXP.matcher(path);    if (v1beta1Match.matches()) {                projectName = v1beta1Match.group(1);        subscriptionName = v1beta1Match.group(2);    } else {        Matcher match = SUBSCRIPTION_REGEXP.matcher(path);        if (!match.matches()) {            throw new IllegalArgumentException("Pubsub subscription is not in " + "projects/<project_id>/subscriptions/<subscription_name> format: " + path);        }        projectName = match.group(1);        subscriptionName = match.group(2);    }    validateProjectName(projectName);    validatePubsubName(subscriptionName);    return new PubsubSubscription(Type.NORMAL, projectName, subscriptionName);}
public String beam_f29871_0()
{    if (type == Type.NORMAL) {        return "/subscriptions/" + project + "/" + subscription;    } else {        return subscription;    }}
public ProjectPath beam_f29879_0(PubsubSubscription from)
{    return PubsubClient.projectPathFromId(from.project);}
public static PubsubTopic beam_f29880_1(String path)
{    if (path.equals(TOPIC_DEV_NULL_TEST_NAME)) {        return new PubsubTopic(Type.FAKE, "", path);    }    String projectName, topicName;    Matcher v1beta1Match = V1BETA1_TOPIC_REGEXP.matcher(path);    if (v1beta1Match.matches()) {                projectName = v1beta1Match.group(1);        topicName = v1beta1Match.group(2);    } else {        Matcher match = TOPIC_REGEXP.matcher(path);        if (!match.matches()) {            throw new IllegalArgumentException("Pubsub topic is not in projects/<project_id>/topics/<topic_name> format: " + path);        }        projectName = match.group(1);        topicName = match.group(2);    }    validateProjectName(projectName);    validatePubsubName(topicName);    return new PubsubTopic(Type.NORMAL, projectName, topicName);}
public String beam_f29881_0()
{    if (type == Type.NORMAL) {        return "/topics/" + project + "/" + topic;    } else {        return topic;    }}
public static Read<PubsubMessage> beam_f29889_0()
{    return new AutoValue_PubsubIO_Read.Builder<PubsubMessage>().setPubsubClientFactory(FACTORY).setCoder(PubsubMessageWithAttributesAndMessageIdCoder.of()).setParseFn(new IdentityMessageFn()).setNeedsAttributes(true).setNeedsMessageId(true).build();}
public static Read<String> beam_f29890_0()
{    return new AutoValue_PubsubIO_Read.Builder<String>().setNeedsAttributes(false).setNeedsMessageId(false).setPubsubClientFactory(FACTORY).setCoder(StringUtf8Coder.of()).setParseFn(new ParsePayloadAsUtf8()).build();}
public static Read<T> beam_f29891_0(Class<T> messageClass)
{                ProtoCoder<T> coder = ProtoCoder.of(messageClass);    return new AutoValue_PubsubIO_Read.Builder<T>().setNeedsAttributes(false).setNeedsMessageId(false).setPubsubClientFactory(FACTORY).setCoder(coder).setParseFn(new ParsePayloadUsingCoder<>(coder)).build();}
public static Write<T> beam_f29899_0(Class<T> messageClass)
{        return PubsubIO.<T>write().withFormatFn(new FormatPayloadUsingCoder<>(ProtoCoder.of(messageClass)));}
public static Write<T> beam_f29900_0(Class<T> clazz)
{        return PubsubIO.<T>write().withFormatFn(new FormatPayloadUsingCoder<>(AvroCoder.of(clazz)));}
public Read<T> beam_f29901_0(String subscription)
{    return fromSubscription(StaticValueProvider.of(subscription));}
 Read<T> beam_f29909_0(Clock clock)
{    return toBuilder().setClock(clock).build();}
public PCollection<T> beam_f29910_0(PBegin input)
{    if (getTopicProvider() == null && getSubscriptionProvider() == null) {        throw new IllegalStateException("Need to set either the topic or the subscription for " + "a PubsubIO.Read transform");    }    if (getTopicProvider() != null && getSubscriptionProvider() != null) {        throw new IllegalStateException("Can't set both the topic and the subscription for " + "a PubsubIO.Read transform");    }    @Nullable    ValueProvider<TopicPath> topicPath = getTopicProvider() == null ? null : NestedValueProvider.of(getTopicProvider(), new TopicPathTranslator());    @Nullable    ValueProvider<SubscriptionPath> subscriptionPath = getSubscriptionProvider() == null ? null : NestedValueProvider.of(getSubscriptionProvider(), new SubscriptionPathTranslator());    PubsubUnboundedSource source = new PubsubUnboundedSource(getClock(), Optional.ofNullable(getPubsubClientFactory()).orElse(FACTORY), null, /* always get project from runtime PipelineOptions */    topicPath, subscriptionPath, getTimestampAttribute(), getIdAttribute(), getNeedsAttributes(), getNeedsMessageId());    PCollection<T> read = input.apply(source).apply(MapElements.via(getParseFn()));    return (getBeamSchema() != null) ? read.setSchema(getBeamSchema(), getToRowFn(), getFromRowFn()) : read.setCoder(getCoder());}
public void beam_f29911_0(DisplayData.Builder builder)
{    super.populateDisplayData(builder);    populateCommonDisplayData(builder, getTimestampAttribute(), getIdAttribute(), getTopicProvider());    builder.addIfNotNull(DisplayData.item("subscription", getSubscriptionProvider()).withLabel("Pubsub Subscription"));}
private Write<T> beam_f29919_0(SimpleFunction<T, PubsubMessage> formatFn)
{    return toBuilder().setFormatFn(formatFn).build();}
public PDone beam_f29920_0(PCollection<T> input)
{    if (getTopicProvider() == null) {        throw new IllegalStateException("need to set the topic of a PubsubIO.Write transform");    }    switch(input.isBounded()) {        case BOUNDED:            input.apply(ParDo.of(new PubsubBoundedWriter(MoreObjects.firstNonNull(getMaxBatchSize(), MAX_PUBLISH_BATCH_SIZE), MoreObjects.firstNonNull(getMaxBatchBytesSize(), MAX_PUBLISH_BATCH_BYTE_SIZE_DEFAULT))));            return PDone.in(input.getPipeline());        case UNBOUNDED:            return input.apply(MapElements.via(getFormatFn())).apply(new PubsubUnboundedSink(Optional.ofNullable(getPubsubClientFactory()).orElse(FACTORY), NestedValueProvider.of(getTopicProvider(), new TopicPathTranslator()), getTimestampAttribute(), getIdAttribute(), 100, /* numShards */            MoreObjects.firstNonNull(getMaxBatchSize(), PubsubUnboundedSink.DEFAULT_PUBLISH_BATCH_SIZE), MoreObjects.firstNonNull(getMaxBatchBytesSize(), PubsubUnboundedSink.DEFAULT_PUBLISH_BATCH_BYTES)));    }        throw new RuntimeException();}
public void beam_f29921_0(DisplayData.Builder builder)
{    super.populateDisplayData(builder);    populateCommonDisplayData(builder, getTimestampAttribute(), getIdAttribute(), getTopicProvider());}
public PubsubMessage beam_f29929_0(String input)
{    return new PubsubMessage(input.getBytes(StandardCharsets.UTF_8), ImmutableMap.of());}
public PubsubMessage beam_f29930_0(T input)
{    try {        return new PubsubMessage(CoderUtils.encodeToByteArray(coder, input), ImmutableMap.of());    } catch (CoderException e) {        throw new RuntimeException("Could not decode Pubsub message", e);    }}
public PubsubMessage beam_f29931_0(PubsubMessage input)
{    return input;}
public void beam_f29940_0(SubscriptionPath subscription, List<String> ackIds, int deadlineSeconds) throws IOException
{    ModifyAckDeadlineRequest request = new ModifyAckDeadlineRequest().setAckIds(ackIds).setAckDeadlineSeconds(deadlineSeconds);    pubsub.projects().subscriptions().modifyAckDeadline(subscription.getPath(), request).execute();}
public void beam_f29941_0(TopicPath topic) throws IOException
{    pubsub.projects().topics().create(topic.getPath(), new Topic()).execute();}
public void beam_f29942_0(TopicPath topic) throws IOException
{        pubsub.projects().topics().delete(topic.getPath()).execute();}
public String beam_f29950_0(String attribute)
{    checkNotNull(attribute, "attribute");    return attributes.get(attribute);}
public Map<String, String> beam_f29951_0()
{    return attributes;}
public String beam_f29952_0()
{    return messageId;}
public void beam_f29960_0(PubsubMessage value, OutputStream outStream) throws IOException
{    PAYLOAD_CODER.encode(value.getPayload(), outStream);    ATTRIBUTES_CODER.encode(value.getAttributeMap(), outStream);    MESSAGE_ID_CODER.encode(value.getMessageId(), outStream);}
public PubsubMessage beam_f29961_0(InputStream inStream) throws IOException
{    byte[] payload = PAYLOAD_CODER.decode(inStream);    Map<String, String> attributes = ATTRIBUTES_CODER.decode(inStream);    String messageId = MESSAGE_ID_CODER.decode(inStream);    return new PubsubMessage(payload, attributes, messageId);}
public static Coder<PubsubMessage> beam_f29962_0(TypeDescriptor<PubsubMessage> ignored)
{    return of();}
public PubsubMessage beam_f29970_0(InputStream inStream) throws IOException
{    byte[] payload = PAYLOAD_CODER.decode(inStream);    String messageId = MESSAGE_ID_CODER.decode(inStream);    return new PubsubMessage(payload, ImmutableMap.of(), messageId);}
 static PubsubTestClientFactory beam_f29971_0(final TopicPath expectedTopic, final Iterable<OutgoingMessage> expectedOutgoingMessages, final Iterable<OutgoingMessage> failingOutgoingMessages)
{    synchronized (STATE) {        checkState(!STATE.isActive, "Test still in flight");        STATE.expectedTopic = expectedTopic;        STATE.remainingExpectedOutgoingMessages = Sets.newHashSet(expectedOutgoingMessages);        STATE.remainingFailingOutgoingMessages = Sets.newHashSet(failingOutgoingMessages);        STATE.isActive = true;    }    return new PubsubTestClientFactory() {        @Override        public PubsubClient newClient(@Nullable String timestampAttribute, @Nullable String idAttribute, PubsubOptions options) throws IOException {            return new PubsubTestClient();        }        @Override        public String getKind() {            return "PublishTest";        }        @Override        public void close() {            synchronized (STATE) {                checkState(STATE.isActive, "No test still in flight");                checkState(STATE.remainingExpectedOutgoingMessages.isEmpty(), "Still waiting for %s messages to be published", STATE.remainingExpectedOutgoingMessages.size());                STATE.isActive = false;                STATE.remainingExpectedOutgoingMessages = null;            }        }    };}
public PubsubClient beam_f29972_0(@Nullable String timestampAttribute, @Nullable String idAttribute, PubsubOptions options) throws IOException
{    return new PubsubTestClient();}
public void beam_f29980_0() throws IOException
{    checkState(numCalls == 1, "Expected exactly one subscription to be created, got %s", numCalls);}
public PubsubClient beam_f29981_0(@Nullable String timestampAttribute, @Nullable String idAttribute, PubsubOptions options) throws IOException
{    return new PubsubTestClient() {        @Override        public void createSubscription(TopicPath topic, SubscriptionPath subscription, int ackDeadlineSeconds) throws IOException {            checkState(numCalls == 0, "Expected at most one subscription to be created");            numCalls++;        }    };}
public void beam_f29982_0(TopicPath topic, SubscriptionPath subscription, int ackDeadlineSeconds) throws IOException
{    checkState(numCalls == 0, "Expected at most one subscription to be created");    numCalls++;}
public void beam_f29991_0(SubscriptionPath subscription, List<String> ackIds, int deadlineSeconds) throws IOException
{    synchronized (STATE) {        checkState(inPullMode(), "Can only modify ack deadline in pull mode");        checkState(subscription.equals(STATE.expectedSubscription), "Subscription %s does not match expected %s", subscription, STATE.expectedSubscription);        for (String ackId : ackIds) {            if (deadlineSeconds > 0) {                checkState(STATE.ackDeadline.remove(ackId) != null, "No message with ACK id %s is waiting for an ACK", ackId);                checkState(STATE.pendingAckIncomingMessages.containsKey(ackId), "No message with ACK id %s is waiting for an ACK", ackId);                STATE.ackDeadline.put(ackId, STATE.clock.currentTimeMillis() + deadlineSeconds * 1000);            } else {                checkState(STATE.ackDeadline.remove(ackId) != null, "No message with ACK id %s is waiting for an ACK", ackId);                IncomingMessage message = STATE.pendingAckIncomingMessages.remove(ackId);                checkState(message != null, "No message with ACK id %s is waiting for an ACK", ackId);                STATE.remainingPendingIncomingMessages.add(message);            }        }    }}
public void beam_f29992_0(TopicPath topic) throws IOException
{    throw new UnsupportedOperationException();}
public void beam_f29993_0(TopicPath topic) throws IOException
{    throw new UnsupportedOperationException();}
public OutgoingMessage beam_f30001_0(InputStream inStream) throws CoderException, IOException
{    byte[] elementBytes = ByteArrayCoder.of().decode(inStream);    Map<String, String> attributes = ATTRIBUTES_CODER.decode(inStream);    long timestampMsSinceEpoch = BigEndianLongCoder.of().decode(inStream);    @Nullable    String recordId = RECORD_ID_CODER.decode(inStream);    return new OutgoingMessage(elementBytes, attributes, timestampMsSinceEpoch, recordId);}
public void beam_f30002_0(ProcessContext c) throws Exception
{    elementCounter.inc();    PubsubMessage message = c.element();    byte[] elementBytes = message.getPayload();    Map<String, String> attributes = message.getAttributeMap();    long timestampMsSinceEpoch = c.timestamp().getMillis();    @Nullable    String recordId = null;    switch(recordIdMethod) {        case NONE:            break;        case DETERMINISTIC:            recordId = Hashing.murmur3_128().hashBytes(elementBytes).toString();            break;        case RANDOM:                                                            recordId = UUID.randomUUID().toString();            break;    }    c.output(KV.of(ThreadLocalRandom.current().nextInt(numShards), new OutgoingMessage(elementBytes, attributes, timestampMsSinceEpoch, recordId)));}
public void beam_f30003_0(Builder builder)
{    super.populateDisplayData(builder);    builder.add(DisplayData.item("numShards", numShards));}
public String beam_f30011_0()
{    return timestampAttribute;}
public String beam_f30012_0()
{    return idAttribute;}
public PDone beam_f30013_0(PCollection<PubsubMessage> input)
{    input.apply("PubsubUnboundedSink.Window", Window.<PubsubMessage>into(new GlobalWindows()).triggering(Repeatedly.forever(AfterFirst.of(AfterPane.elementCountAtLeast(publishBatchSize), AfterProcessingTime.pastFirstElementInPane().plusDelayOf(maxLatency)))).discardingFiredPanes()).apply("PubsubUnboundedSink.Shard", ParDo.of(new ShardFn(numShards, recordIdMethod))).setCoder(KvCoder.of(VarIntCoder.of(), CODER)).apply(GroupByKey.create()).apply("PubsubUnboundedSink.Writer", ParDo.of(new WriterFn(pubsubFactory, topic, timestampAttribute, idAttribute, publishBatchSize, publishBatchBytes)));    return PDone.in(input.getPipeline());}
public void beam_f30021_0(PubsubReader reader) throws IOException
{    checkState(this.reader == null, "Cannot nackAll on persisting checkpoint");    List<String> batchYetToAckIds = new ArrayList<>(Math.min(notYetReadIds.size(), ACK_BATCH_SIZE));    for (String ackId : notYetReadIds) {        batchYetToAckIds.add(ackId);        if (batchYetToAckIds.size() >= ACK_BATCH_SIZE) {            long nowMsSinceEpoch = now(reader);            reader.nackBatch(nowMsSinceEpoch, batchYetToAckIds);            batchYetToAckIds.clear();        }    }    if (!batchYetToAckIds.isEmpty()) {        long nowMsSinceEpoch = now(reader);        reader.nackBatch(nowMsSinceEpoch, batchYetToAckIds);    }}
public static PubsubCheckpointCoder<T> beam_f30022_0()
{    return new PubsubCheckpointCoder<>();}
public void beam_f30023_0(PubsubCheckpoint value, OutputStream outStream) throws IOException
{    SUBSCRIPTION_PATH_CODER.encode(value.subscriptionPath, outStream);    LIST_CODER.encode(value.notYetReadIds, outStream);}
private void beam_f30031_0() throws IOException
{    long nowMsSinceEpoch = now();    while (true) {        List<String> ackIds = ackedIds.poll();        if (ackIds == null) {            return;        }        numAcked.add(nowMsSinceEpoch, ackIds.size());        for (String ackId : ackIds) {            inFlight.remove(ackId);            safeToAckIds.remove(ackId);        }    }}
private void beam_f30032_0() throws IOException
{    while (true) {        long nowMsSinceEpoch = now();        List<String> assumeExpired = new ArrayList<>();        List<String> toBeExtended = new ArrayList<>();        List<String> toBeExpired = new ArrayList<>();                for (Map.Entry<String, InFlightState> entry : inFlight.entrySet()) {            if (entry.getValue().ackDeadlineMsSinceEpoch - (ackTimeoutMs * ACK_SAFETY_PCT) / 100 > nowMsSinceEpoch) {                                break;            }            if (entry.getValue().ackDeadlineMsSinceEpoch - ACK_TOO_LATE.getMillis() < nowMsSinceEpoch) {                                                                                assumeExpired.add(entry.getKey());                continue;            }            if (entry.getValue().requestTimeMsSinceEpoch + PROCESSING_TIMEOUT.getMillis() < nowMsSinceEpoch) {                                                toBeExpired.add(entry.getKey());                continue;            }                        toBeExtended.add(entry.getKey());            if (toBeExtended.size() >= ACK_BATCH_SIZE) {                                break;            }        }        if (assumeExpired.isEmpty() && toBeExtended.isEmpty() && toBeExpired.isEmpty()) {                        return;        }        if (!assumeExpired.isEmpty()) {                        numLateDeadlines.add(nowMsSinceEpoch, assumeExpired.size());            for (String ackId : assumeExpired) {                inFlight.remove(ackId);            }        }        if (!toBeExpired.isEmpty()) {                        numExpired.add(nowMsSinceEpoch, toBeExpired.size());            for (String ackId : toBeExpired) {                inFlight.remove(ackId);            }        }        if (!toBeExtended.isEmpty()) {                                    long newDeadlineMsSinceEpoch = nowMsSinceEpoch + (ackTimeoutMs * ACK_EXTENSION_PCT) / 100;            for (String ackId : toBeExtended) {                                InFlightState state = inFlight.remove(ackId);                inFlight.put(ackId, new InFlightState(state.requestTimeMsSinceEpoch, newDeadlineMsSinceEpoch));            }                        extendBatch(nowMsSinceEpoch, toBeExtended);        }    }}
private void beam_f30033_0() throws IOException
{    if (inFlight.size() >= MAX_IN_FLIGHT) {                return;    }    long requestTimeMsSinceEpoch = now();    long deadlineMsSinceEpoch = requestTimeMsSinceEpoch + ackTimeoutMs;            Collection<PubsubClient.IncomingMessage> receivedMessages = pubsubClient.get().pull(requestTimeMsSinceEpoch, subscription, PULL_BATCH_SIZE, true);    if (receivedMessages.isEmpty()) {                return;    }    lastReceivedMsSinceEpoch = requestTimeMsSinceEpoch;        for (PubsubClient.IncomingMessage incomingMessage : receivedMessages) {        notYetRead.add(incomingMessage);        notYetReadBytes += incomingMessage.elementBytes.length;        inFlight.put(incomingMessage.ackId, new InFlightState(requestTimeMsSinceEpoch, deadlineMsSinceEpoch));        numReceived++;        numReceivedRecently.add(requestTimeMsSinceEpoch, 1L);        minReceivedTimestampMsSinceEpoch.add(requestTimeMsSinceEpoch, incomingMessage.timestampMsSinceEpoch);        maxReceivedTimestampMsSinceEpoch.add(requestTimeMsSinceEpoch, incomingMessage.timestampMsSinceEpoch);        minUnreadTimestampMsSinceEpoch.add(requestTimeMsSinceEpoch, incomingMessage.timestampMsSinceEpoch);    }}
private void beam_f30041_0() throws IOException
{    if (!active.get() && numInFlightCheckpoints.get() == 0) {                        PubsubClient client = pubsubClient.getAndSet(null);        if (client != null) {            client.close();        }    }}
public PubsubSource beam_f30042_0()
{    return outer;}
public Instant beam_f30043_0()
{    if (pubsubClient.get().isEOF() && notYetRead.isEmpty()) {                return BoundedWindow.TIMESTAMP_MAX_VALUE;    }                            long nowMsSinceEpoch = now();    long readMin = minReadTimestampMsSinceEpoch.get(nowMsSinceEpoch);    long unreadMin = minUnreadTimestampMsSinceEpoch.get();    if (readMin == Long.MAX_VALUE && unreadMin == Long.MAX_VALUE && lastReceivedMsSinceEpoch >= 0 && nowMsSinceEpoch > lastReceivedMsSinceEpoch + SAMPLE_PERIOD.getMillis()) {                                        lastWatermarkMsSinceEpoch = nowMsSinceEpoch;    } else if (minReadTimestampMsSinceEpoch.isSignificant() || minUnreadTimestampMsSinceEpoch.isSignificant()) {                lastWatermarkMsSinceEpoch = Math.min(readMin, unreadMin);    }        minWatermarkMsSinceEpoch.add(nowMsSinceEpoch, lastWatermarkMsSinceEpoch);    maxWatermarkMsSinceEpoch.add(nowMsSinceEpoch, lastWatermarkMsSinceEpoch);    return new Instant(lastWatermarkMsSinceEpoch);}
public void beam_f30052_0(ProcessContext c) throws Exception
{    elementCounter.inc();    c.output(c.element());}
public void beam_f30053_0(Builder builder)
{    super.populateDisplayData(builder);    builder.addIfNotNull(DisplayData.item("subscription", subscription)).addIfNotNull(DisplayData.item("topic", topic)).add(DisplayData.item("transport", pubsubFactory.getKind())).addIfNotNull(DisplayData.item("timestampAttribute", timestampAttribute)).addIfNotNull(DisplayData.item("idAttribute", idAttribute));}
public ProjectPath beam_f30054_0()
{    return project == null ? null : project.get();}
public boolean beam_f30062_0()
{    return needsMessageId;}
public PCollection<PubsubMessage> beam_f30063_0(PBegin input)
{    return input.getPipeline().begin().apply(Read.from(new PubsubSource(this))).apply("PubsubUnboundedSource.Stats", ParDo.of(new StatsFn(pubsubFactory, subscription, topic, timestampAttribute, idAttribute)));}
private SubscriptionPath beam_f30064_1(PipelineOptions options)
{    TopicPath topicPath = topic.get();    ProjectPath projectPath;    if (project != null) {        projectPath = project.get();    } else {        String projectId = options.as(GcpOptions.class).getProject();        checkState(projectId != null, "Cannot create subscription to topic %s because pipeline option 'project' not specified", topicPath);        projectPath = PubsubClient.projectPathFromId(options.as(GcpOptions.class).getProject());    }    try {        try (PubsubClient pubsubClient = pubsubFactory.newClient(timestampAttribute, idAttribute, options.as(PubsubOptions.class))) {            SubscriptionPath subscriptionPath = pubsubClient.createRandomSubscription(projectPath, topicPath, DEAULT_ACK_TIMEOUT_SEC);                        return subscriptionPath;        }    } catch (Exception e) {        throw new RuntimeException(String.format("Failed to create subscription to topic %s on project %s: %s", topicPath, projectPath, e.getMessage()), e);    }}
private List<SubscriptionPath> beam_f30072_0(ProjectPath projectPath, TopicPath topicPath) throws IOException
{    return pubsub.listSubscriptions(projectPath, topicPath);}
public void beam_f30073_0(List<PubsubMessage> messages) throws IOException
{    List<PubsubClient.OutgoingMessage> outgoingMessages = messages.stream().map(this::toOutgoingMessage).collect(toList());    pubsub.publish(eventsTopicPath, outgoingMessages);}
public void beam_f30074_0(String project, Duration timeoutDuration) throws InterruptedException, IllegalArgumentException, IOException, TimeoutException
{    if (timeoutDuration.getMillis() <= 0) {        throw new IllegalArgumentException(String.format("timeoutDuration should be greater than 0"));    }    DateTime startTime = new DateTime();    int sizeOfSubscriptionList = 0;    while (sizeOfSubscriptionList == 0 && Seconds.secondsBetween(new DateTime(), startTime).getSeconds() < timeoutDuration.toStandardSeconds().getSeconds()) {                Thread.sleep(1000);        sizeOfSubscriptionList = listSubscriptions(projectPathFromPath(String.format("projects/%s", project)), topicPath()).size();    }    if (sizeOfSubscriptionList > 0) {        return;    } else {        throw new TimeoutException("Timed out when checking if topics exist for " + topicPath());    }}
public PTransform<PCollection<? extends T>, POutput> beam_f30082_0(Coder<T> coder, SerializableFunction<T, String> formatter, SerializableFunction<Set<T>, Boolean> successPredicate)
{    return new PublishSuccessWhen<>(coder, formatter, successPredicate, resultTopicPath);}
public PTransform<PCollection<? extends T>, POutput> beam_f30083_0(Coder<T> coder, SerializableFunction<Set<T>, Boolean> successPredicate)
{    return signalSuccessWhen(coder, T::toString, successPredicate);}
public Supplier<Void> beam_f30084_0(Duration duration) throws IOException
{    SubscriptionPath startSubscriptionPath = PubsubClient.subscriptionPathFromName(pipelineOptions.getProject(), "start-subscription-" + String.valueOf(ThreadLocalRandom.current().nextLong()));    pubsub.createSubscription(startTopicPath, startSubscriptionPath, (int) duration.getStandardSeconds());    return Suppliers.memoize(() -> {        try {            String result = pollForResultForDuration(startSubscriptionPath, duration);            checkState(START_SIGNAL_MESSAGE.equals(result));            return null;        } catch (IOException e) {            throw new RuntimeException(e);        }    });}
public PCollection<Struct> beam_f30092_0(PCollection<ReadOperation> input)
{    PCollectionView<Transaction> txView = getTxView();    if (txView == null) {        Pipeline begin = input.getPipeline();        SpannerIO.CreateTransaction createTx = SpannerIO.createTransaction().withSpannerConfig(getSpannerConfig()).withTimestampBound(getTimestampBound());        txView = begin.apply(createTx);    }    return input.apply("Generate Partitions", ParDo.of(new GeneratePartitionsFn(getSpannerConfig(), txView)).withSideInputs(txView)).apply("Shuffle partitions", Reshuffle.<Partition>viaRandomKey()).apply("Read from Partitions", ParDo.of(new ReadFromPartitionFn(getSpannerConfig(), txView)).withSideInputs(txView));}
public void beam_f30093_0() throws Exception
{    spannerAccessor = config.connectToSpanner();}
public void beam_f30094_0() throws Exception
{    spannerAccessor.close();}
public void beam_f30102_0(ProcessContext c) throws Exception
{    BatchReadOnlyTransaction tx = spannerAccessor.getBatchClient().batchReadOnlyTransaction(config.getTimestampBound());    c.output(Transaction.create(tx.getBatchTransactionId()));}
public static long beam_f30103_0(SpannerSchema spannerSchema, MutationGroup mutationGroup)
{    long mutatedCells = 0L;    for (Mutation mutation : mutationGroup) {        if (mutation.getOperation() == Op.DELETE) {                        if (isPointDelete(mutation)) {                final KeySet keySet = mutation.getKeySet();                final long rows = Iterables.size(keySet.getKeys());                mutatedCells += rows * spannerSchema.getCellsMutatedPerRow(mutation.getTable());            }        } else {                        for (String column : mutation.getColumns()) {                mutatedCells += spannerSchema.getCellsMutatedPerColumn(mutation.getTable(), column);            }        }    }    return mutatedCells;}
public static MutationGroup beam_f30104_0(Mutation primary, Mutation... other)
{    return create(primary, Arrays.asList(other));}
public byte[] beam_f30112_0(Mutation m)
{    OrderedCode orderedCode = new OrderedCode();    orderedCode.writeBytes(m.getTable().getBytes(StandardCharsets.UTF_8));    if (m.getOperation() == Op.DELETE) {        if (isPointDelete(m)) {            Key next = m.getKeySet().getKeys().iterator().next();            encodeKey(orderedCode, m.getTable(), next);        } else {                }    } else {        encodeKey(orderedCode, m);    }    return orderedCode.getEncodedBytes();}
private void beam_f30113_0(OrderedCode orderedCode, Mutation m)
{    Map<String, Value> mutationMap = mutationAsMap(m);    for (SpannerSchema.KeyPart part : schema.getKeyParts(m.getTable())) {        Value val = mutationMap.get(part.getField());        if (val == null || val.isNull()) {            if (part.isDesc()) {                orderedCode.writeInfinityDecreasing();            } else {                orderedCode.writeInfinity();            }        } else {            Type.Code code = val.getType().getCode();            switch(code) {                case BOOL:                    writeNumber(orderedCode, part, (long) (val.getBool() ? 0 : 1));                    break;                case INT64:                    writeNumber(orderedCode, part, val.getInt64());                    break;                case FLOAT64:                    writeNumber(orderedCode, part, Double.doubleToLongBits(val.getFloat64()));                    break;                case STRING:                    writeString(orderedCode, part, val.getString());                    break;                case BYTES:                    writeBytes(orderedCode, part, val.getBytes());                    break;                case TIMESTAMP:                    writeTimestamp(orderedCode, part, val.getTimestamp());                    break;                case DATE:                    writeNumber(orderedCode, part, encodeDate(val.getDate()));                    break;                default:                    throw new IllegalArgumentException("Unknown type " + val.getType());            }        }    }}
private void beam_f30114_0(OrderedCode orderedCode, String table, Key key)
{    List<SpannerSchema.KeyPart> parts = schema.getKeyParts(table);    Iterator<Object> it = key.getParts().iterator();    for (SpannerSchema.KeyPart part : parts) {        Object value = it.next();        if (value == null) {            if (part.isDesc()) {                orderedCode.writeInfinityDecreasing();            } else {                orderedCode.writeInfinity();            }        } else {            if (value instanceof Boolean) {                writeNumber(orderedCode, part, (long) ((Boolean) value ? 0 : 1));            } else if (value instanceof Long) {                writeNumber(orderedCode, part, (long) value);            } else if (value instanceof Double) {                writeNumber(orderedCode, part, Double.doubleToLongBits((double) value));            } else if (value instanceof String) {                writeString(orderedCode, part, (String) value);            } else if (value instanceof ByteArray) {                writeBytes(orderedCode, part, (ByteArray) value);            } else if (value instanceof Timestamp) {                writeTimestamp(orderedCode, part, (Timestamp) value);            } else if (value instanceof Date) {                writeNumber(orderedCode, part, encodeDate((Date) value));            } else {                throw new IllegalArgumentException("Unknown key part " + value);            }        }    }}
private static long beam_f30122_0(KeySet keySet)
{    long result = 0;    for (Key k : keySet.getKeys()) {        result += sizeOf(k);    }    for (KeyRange kr : keySet.getRanges()) {        result += sizeOf(kr);    }    return result;}
private static long beam_f30123_0(KeyRange kr)
{    return sizeOf(kr.getStart()) + sizeOf(kr.getEnd());}
private static long beam_f30124_0(Key k)
{    long result = 0;    for (Object part : k.getParts()) {        if (part == null) {            continue;        }        if (part instanceof Boolean) {            result += 1;        } else if (part instanceof Long) {            result += 8;        } else if (part instanceof Double) {            result += 8;        } else if (part instanceof String) {            result += ((String) part).length();        } else if (part instanceof ByteArray) {            result += ((ByteArray) part).length();        } else if (part instanceof Timestamp) {            result += 12;        } else if (part instanceof Date) {            result += 12;        }    }    return result;}
public void beam_f30132_0() throws Exception
{    spannerAccessor.close();}
public void beam_f30133_0(ProcessContext c) throws Exception
{    Transaction tx = c.sideInput(txView);    ReadOperation op = c.element();    BatchReadOnlyTransaction context = spannerAccessor.getBatchClient().batchReadOnlyTransaction(tx.transactionId());    try (ResultSet resultSet = execute(op, context)) {        while (resultSet.next()) {            c.output(resultSet.getCurrentRowAsStruct());        }    }}
private ResultSet beam_f30134_0(ReadOperation op, BatchReadOnlyTransaction readOnlyTransaction)
{    if (op.getQuery() != null) {        return readOnlyTransaction.executeQuery(op.getQuery());    }    if (op.getIndex() != null) {        return readOnlyTransaction.readUsingIndex(op.getTable(), op.getIndex(), op.getKeySet(), op.getColumns());    }    return readOnlyTransaction.read(op.getTable(), op.getKeySet(), op.getColumns());}
public void beam_f30142_0(UnsignedInteger unsignedInt)
{    writeNumIncreasing(unsignedInt.longValue());}
public void beam_f30143_0(long value)
{                    byte[] bufer = new byte[9];    int len = 0;    while (value != 0) {        len++;        bufer[9 - len] = (byte) ~(value & 0xff);        value >>>= 8;    }    bufer[9 - len - 1] = (byte) ~len;    len++;    byte[] encodedArray = new byte[len];    System.arraycopy(bufer, 9 - len, encodedArray, 0, len);    encodedArrays.add(encodedArray);}
public void beam_f30144_0(UnsignedInteger unsignedInt)
{    writeNumDecreasing(unsignedInt.longValue());}
public byte[] beam_f30152_0()
{    return readBytes(false);}
public byte[] beam_f30153_0()
{    return readBytes(true);}
private byte[] beam_f30154_0(boolean invert)
{    if (encodedArrays.isEmpty() || ((encodedArrays.get(0)).length - firstArrayPosition <= 0)) {        throw new IllegalArgumentException("Invalid encoded byte array");    }                byte[] store = encodedArrays.get(0);    int decodedLength = 0;    boolean valid = false;    int i = firstArrayPosition;    while (i < store.length - 1) {        byte b = store[i++];        if (b == convert(invert, ESCAPE1)) {            b = store[i++];            if (b == convert(invert, SEPARATOR)) {                valid = true;                break;            } else if (b == convert(invert, NULL_CHARACTER)) {                decodedLength++;            } else {                throw new IllegalArgumentException("Invalid encoded byte array");            }        } else if (b == convert(invert, ESCAPE2)) {            b = store[i++];            if (b == convert(invert, FF_CHARACTER)) {                decodedLength++;            } else {                throw new IllegalArgumentException("Invalid encoded byte array");            }        } else {            decodedLength++;        }    }    if (!valid) {        throw new IllegalArgumentException("Invalid encoded byte array");    }    byte[] decodedArray = new byte[decodedLength];    int copyStart = firstArrayPosition;    int outIndex = 0;    int j = firstArrayPosition;    while (j < store.length - 1) {                byte b = store[j++];        if (b == convert(invert, ESCAPE1)) {            arraycopy(invert, store, copyStart, decodedArray, outIndex, j - copyStart - 1);            outIndex += j - copyStart - 1;                                    b = store[j++];            if (b == convert(invert, SEPARATOR)) {                if ((store.length - j) == 0) {                                        encodedArrays.remove(0);                    firstArrayPosition = 0;                } else {                    firstArrayPosition = j;                }                return decodedArray;            } else if (b == convert(invert, NULL_CHARACTER)) {                decodedArray[outIndex++] = 0x00;            }                        copyStart = j;        } else if (b == convert(invert, ESCAPE2)) {            arraycopy(invert, store, copyStart, decodedArray, outIndex, j - copyStart - 1);            outIndex += j - copyStart - 1;                                    b = store[j++];            if (b == convert(invert, FF_CHARACTER)) {                decodedArray[outIndex++] = (byte) 0xff;            }                        copyStart = j;        }    }        throw new IllegalArgumentException("Invalid encoded byte array");}
public byte[] beam_f30162_0()
{        if (encodedArrays.size() != 1) {        throw new IllegalArgumentException("Invalid encoded byte array");    }    byte[] store = encodedArrays.get(0);    encodedArrays.remove(0);    assert encodedArrays.isEmpty();    return Arrays.copyOfRange(store, firstArrayPosition, store.length);}
public byte[] beam_f30163_0()
{    if (encodedArrays.isEmpty()) {        return new byte[0];    }    if ((encodedArrays.size() == 1) && (firstArrayPosition == 0)) {        return encodedArrays.get(0);    }    int totalLength = 0;    for (int i = 0; i < encodedArrays.size(); i++) {        byte[] bytes = encodedArrays.get(i);        if (i == 0) {            totalLength += bytes.length - firstArrayPosition;        } else {            totalLength += bytes.length;        }    }    byte[] encodedBytes = new byte[totalLength];    int destPos = 0;    for (int i = 0; i < encodedArrays.size(); i++) {        byte[] bytes = encodedArrays.get(i);        if (i == 0) {            System.arraycopy(bytes, firstArrayPosition, encodedBytes, destPos, bytes.length - firstArrayPosition);            destPos += bytes.length - firstArrayPosition;        } else {            System.arraycopy(bytes, 0, encodedBytes, destPos, bytes.length);            destPos += bytes.length;        }    }            encodedArrays.clear();    encodedArrays.add(encodedBytes);    firstArrayPosition = 0;    return encodedBytes;}
public boolean beam_f30164_0()
{        return encodedArrays.size() != 0;}
public ReadOperation beam_f30172_0(String index)
{    return toBuilder().setIndex(index).build();}
public ReadOperation beam_f30173_0(PartitionOptions partitionOptions)
{    return toBuilder().setPartitionOptions(partitionOptions).build();}
public void beam_f30174_0() throws Exception
{    spannerAccessor = config.connectToSpanner();}
public void beam_f30182_0()
{    spanner.close();}
public static SpannerConfig beam_f30183_0()
{    return builder().setHost(ValueProvider.StaticValueProvider.of(DEFAULT_HOST)).build();}
 static Builder beam_f30184_0()
{    return new AutoValue_SpannerConfig.Builder();}
public SpannerConfig beam_f30192_0(String databaseId)
{    return withDatabaseId(ValueProvider.StaticValueProvider.of(databaseId));}
public SpannerConfig beam_f30193_0(ValueProvider<String> host)
{    return toBuilder().setHost(host).build();}
 SpannerConfig beam_f30194_0(ServiceFactory<Spanner, SpannerOptions> serviceFactory)
{    return toBuilder().setServiceFactory(serviceFactory).build();}
public ReadAll beam_f30202_0(ValueProvider<String> projectId)
{    SpannerConfig config = getSpannerConfig();    return withSpannerConfig(config.withProjectId(projectId));}
public ReadAll beam_f30203_0(String instanceId)
{    return withInstanceId(ValueProvider.StaticValueProvider.of(instanceId));}
public ReadAll beam_f30204_0(ValueProvider<String> instanceId)
{    SpannerConfig config = getSpannerConfig();    return withSpannerConfig(config.withInstanceId(instanceId));}
public ReadAll beam_f30212_0(TimestampBound timestampBound)
{    return toBuilder().setTimestampBound(timestampBound).build();}
public ReadAll beam_f30213_0(boolean batching)
{    return toBuilder().setBatching(batching).build();}
public PCollection<Struct> beam_f30214_0(PCollection<ReadOperation> input)
{    PTransform<PCollection<ReadOperation>, PCollection<Struct>> readTransform;    if (getBatching()) {        readTransform = BatchSpannerRead.create(getSpannerConfig(), getTransaction(), getTimestampBound());    } else {        readTransform = NaiveSpannerRead.create(getSpannerConfig(), getTransaction(), getTimestampBound());    }    return input.apply("Reshuffle", Reshuffle.viaRandomKey()).apply("Read from Cloud Spanner", readTransform);}
public Read beam_f30222_0(ValueProvider<String> host)
{    SpannerConfig config = getSpannerConfig();    return withSpannerConfig(config.withHost(host));}
public Read beam_f30223_0(String host)
{    return withHost(ValueProvider.StaticValueProvider.of(host));}
public Read beam_f30224_0(boolean batching)
{    return toBuilder().setBatching(batching).build();}
public Read beam_f30232_0(List<String> columns)
{    return withReadOperation(getReadOperation().withColumns(columns));}
public Read beam_f30233_0(Statement statement)
{    return withReadOperation(getReadOperation().withQuery(statement));}
public Read beam_f30234_0(String sql)
{    return withQuery(Statement.of(sql));}
public CreateTransaction beam_f30242_0(ValueProvider<String> projectId)
{    SpannerConfig config = getSpannerConfig();    return withSpannerConfig(config.withProjectId(projectId));}
public CreateTransaction beam_f30243_0(String instanceId)
{    return withInstanceId(ValueProvider.StaticValueProvider.of(instanceId));}
public CreateTransaction beam_f30244_0(ValueProvider<String> instanceId)
{    SpannerConfig config = getSpannerConfig();    return withSpannerConfig(config.withInstanceId(instanceId));}
public Write beam_f30252_0(String projectId)
{    return withProjectId(ValueProvider.StaticValueProvider.of(projectId));}
public Write beam_f30253_0(ValueProvider<String> projectId)
{    SpannerConfig config = getSpannerConfig();    return withSpannerConfig(config.withProjectId(projectId));}
public Write beam_f30254_0(String instanceId)
{    return withInstanceId(ValueProvider.StaticValueProvider.of(instanceId));}
public Write beam_f30262_0(long batchSizeBytes)
{    return toBuilder().setBatchSizeBytes(batchSizeBytes).build();}
public Write beam_f30263_0(FailureMode failureMode)
{    return toBuilder().setFailureMode(failureMode).build();}
public Write beam_f30264_0(long maxNumMutations)
{    return toBuilder().setMaxNumMutations(maxNumMutations).build();}
 static byte[] beam_f30272_0(MutationGroup g)
{    ByteArrayOutputStream bos = new ByteArrayOutputStream();    try {        CODER.encode(g, bos);    } catch (IOException e) {        throw new RuntimeException(e);    }    return bos.toByteArray();}
public void beam_f30273_0(ProcessContext c)
{    Mutation value = c.element();    c.output(MutationGroup.create(value));}
public synchronized void beam_f30274_0() throws Exception
{    if (mutationsToSort == null) {        initSorter();    } else {        throw new IllegalStateException("Sorter should be null here");    }}
public void beam_f30282_0() throws Exception
{    spannerAccessor.close();}
public void beam_f30283_1(ProcessContext c) throws Exception
{    Iterable<MutationGroup> mutations = c.element();    boolean tryIndividual = false;        try {        mutationGroupBatchesCounter.inc();        Iterable<Mutation> batch = Iterables.concat(mutations);        spannerAccessor.getDatabaseClient().writeAtLeastOnce(batch);        mutationGroupWriteSuccessCounter.inc(Iterables.size(mutations));        return;    } catch (SpannerException e) {        if (failureMode == FailureMode.REPORT_FAILURES) {            tryIndividual = true;        } else if (failureMode == FailureMode.FAIL_FAST) {            throw e;        } else {            throw new IllegalArgumentException("Unknown failure mode " + failureMode);        }    }    if (tryIndividual) {        for (MutationGroup mg : mutations) {            try {                spannerAccessor.getDatabaseClient().writeAtLeastOnce(mg);                mutationGroupWriteSuccessCounter.inc();            } catch (SpannerException e) {                mutationGroupWriteFailCounter.inc();                                c.output(failedTag, mg);            }        }    }}
public static Builder beam_f30284_0()
{    return new AutoValue_SpannerSchema.Builder();}
public long beam_f30292_0(String table, String column)
{    return cellsMutatedPerColumn().row(table.toLowerCase()).getOrDefault(column.toLowerCase(), 1L);}
public long beam_f30293_0(String table)
{    return cellsMutatedPerRow().getOrDefault(table.toLowerCase(), 1L);}
 static KeyPart beam_f30294_0(String field, boolean desc)
{    return new AutoValue_SpannerSchema_KeyPart(field, desc);}
public static Transaction beam_f30303_0(BatchTransactionId txId)
{    return new AutoValue_Transaction(txId);}
private static Credentials beam_f30304_0()
{    GoogleCredentials credential;    try {        credential = GoogleCredentials.getApplicationDefault();    } catch (IOException e) {        throw new RuntimeException("Failed to get application default credential.", e);    }    if (credential.createScopedRequired()) {        Collection<String> bigqueryScope = Lists.newArrayList(BigqueryScopes.all());        credential = credential.createScoped(bigqueryScope);    }    return credential;}
public static Bigquery beam_f30305_0(String applicationName)
{    HttpTransport transport = Transport.getTransport();    JsonFactory jsonFactory = Transport.getJsonFactory();    Credentials credential = getDefaultCredential();    return new Bigquery.Builder(transport, jsonFactory, new HttpCredentialsAdapter(credential)).setApplicationName(applicationName).build();}
public QueryResponse beam_f30313_0(String query, String projectId, boolean typed) throws IOException, InterruptedException
{    return queryWithRetries(query, projectId, typed, false);}
private QueryResponse beam_f30314_1(String query, String projectId, boolean typed, boolean useStandardSql) throws IOException, InterruptedException
{    Sleeper sleeper = Sleeper.DEFAULT;    BackOff backoff = BackOffAdapter.toGcpBackOff(BACKOFF_FACTORY.backoff());    IOException lastException = null;    QueryRequest bqQueryRequest = new QueryRequest().setQuery(query).setTimeoutMs(QUERY_TIMEOUT_MS).setUseLegacySql(!useStandardSql);    do {        if (lastException != null) {                    }        try {            QueryResponse response = bqClient.jobs().query(projectId, bqQueryRequest).execute();            if (response != null) {                return typed ? getTypedTableRows(response) : response;            } else {                lastException = new IOException("Expected valid response from query job, but received null.");            }        } catch (IOException e) {                        lastException = e;        }    } while (BackOffUtils.next(sleeper, backoff));    throw new RuntimeException(String.format("Unable to get BigQuery response after retrying %d times using query (%s)", MAX_QUERY_RETRIES, bqQueryRequest.getQuery()), lastException);}
public void beam_f30315_1(String projectId, String datasetId) throws IOException, InterruptedException
{    Sleeper sleeper = Sleeper.DEFAULT;    BackOff backoff = BackOffAdapter.toGcpBackOff(BACKOFF_FACTORY.backoff());    IOException lastException = null;    do {        if (lastException != null) {                    }        try {            Dataset response = bqClient.datasets().insert(projectId, new Dataset().setDatasetReference(new DatasetReference().setDatasetId(datasetId))).execute();            if (response != null) {                                return;            } else {                lastException = new IOException("Expected valid response from insert dataset job, but received null.");            }        } catch (IOException e) {                        lastException = e;        }    } while (BackOffUtils.next(sleeper, backoff));    throw new RuntimeException(String.format("Unable to get BigQuery response after retrying %d times for dataset (%s)", MAX_QUERY_RETRIES, datasetId), lastException);}
private void beam_f30323_0(String name, String value)
{    checkArgument(!Strings.isNullOrEmpty(value), "Expected valid %s, but was %s", name, value);}
private String beam_f30324_0(@Nonnull List<TableRow> rows)
{    List<HashCode> rowHashes = Lists.newArrayList();    for (TableRow row : rows) {        List<String> cellsInOneRow = Lists.newArrayList();        for (TableCell cell : row.getF()) {            cellsInOneRow.add(Objects.toString(cell.getV()));            Collections.sort(cellsInOneRow);        }        rowHashes.add(Hashing.sha1().hashString(cellsInOneRow.toString(), StandardCharsets.UTF_8));    }    return Hashing.combineUnordered(rowHashes).toString();}
public void beam_f30325_0(Description description)
{    description.appendText("Expected checksum is (").appendText(expectedChecksum).appendText(")");}
public StorageClient beam_f30333_0(BigQueryOptions bqOptions)
{    return storageClient;}
public Iterator<T> beam_f30334_0()
{    return items.iterator();}
public static String beam_f30336_0(Table table) throws IOException
{    return encodeQueryResult(table, ImmutableList.of());}
private TableContainer beam_f30344_0(String projectId, String datasetId, String tableId) throws InterruptedException, IOException
{    synchronized (tables) {        Map<String, TableContainer> dataset = tables.get(projectId, datasetId);        if (dataset == null) {            throwNotFound("Tried to get a dataset %s:%s, but no such dataset was set", projectId, datasetId);        }        TableContainer tableContainer = dataset.get(tableId);        if (tableContainer == null) {            throwNotFound("Tried to get a table %s:%s.%s, but no such table was set", projectId, datasetId, tableId);        }        return tableContainer;    }}
public void beam_f30345_0(TableReference tableRef) throws IOException, InterruptedException
{    validateWholeTableReference(tableRef);    synchronized (tables) {        Map<String, TableContainer> dataset = tables.get(tableRef.getProjectId(), tableRef.getDatasetId());        if (dataset == null) {            throwNotFound("Tried to get a dataset %s:%s, but no such table was set", tableRef.getProjectId(), tableRef.getDatasetId());        }        dataset.remove(tableRef.getTableId());    }}
private static void beam_f30346_0(TableReference tableReference) throws IOException
{    final Pattern tableRegexp = Pattern.compile("[-\\w]{1,1024}");    if (!tableRegexp.matcher(tableReference.getTableId()).matches()) {        throw new IOException(String.format("invalid table ID %s. Table IDs must be alphanumeric " + "(plus underscores) and must be at most 1024 characters long. Also, table" + " decorators cannot be used.", tableReference.getTableId()));    }}
public Table beam_f30354_0(TableReference tableReference, @Nullable String tableDescription) throws IOException, InterruptedException
{    validateWholeTableReference(tableReference);    synchronized (tables) {        TableContainer tableContainer = getTableContainer(tableReference.getProjectId(), tableReference.getDatasetId(), tableReference.getTableId());        tableContainer.getTable().setDescription(tableDescription);        return tableContainer.getTable();    }}
public void beam_f30355_0(Map<TableRow, List<TableDataInsertAllResponse.InsertErrors>> insertErrors)
{    synchronized (tables) {        for (Map.Entry<TableRow, List<TableDataInsertAllResponse.InsertErrors>> entry : insertErrors.entrySet()) {            List<String> errorStrings = Lists.newArrayList();            for (TableDataInsertAllResponse.InsertErrors errors : entry.getValue()) {                errorStrings.add(BigQueryHelpers.toJsonString(errors));            }            this.insertErrors.put(BigQueryHelpers.toJsonString(entry.getKey()), errorStrings);        }    }}
 Map<TableRow, List<TableDataInsertAllResponse.InsertErrors>> beam_f30356_0()
{    Map<TableRow, List<TableDataInsertAllResponse.InsertErrors>> parsedInsertErrors = Maps.newHashMap();    synchronized (tables) {        for (Map.Entry<String, List<String>> entry : this.insertErrors.entrySet()) {            TableRow tableRow = BigQueryHelpers.fromJsonString(entry.getKey(), TableRow.class);            List<TableDataInsertAllResponse.InsertErrors> allErrors = Lists.newArrayList();            for (String errorsString : entry.getValue()) {                allErrors.add(BigQueryHelpers.fromJsonString(errorsString, TableDataInsertAllResponse.InsertErrors.class));            }            parsedInsertErrors.put(tableRow, allErrors);        }    }    return parsedInsertErrors;}
public void beam_f30364_0(JobReference jobRef, JobConfigurationTableCopy copyConfig) throws IOException
{    synchronized (allJobs) {        verifyUniqueJobId(jobRef.getJobId());        Job job = new Job();        job.setJobReference(jobRef);        job.setConfiguration(new JobConfiguration().setCopy(copyConfig));        job.setKind(" bigquery#job");        job.setStatus(new JobStatus().setState("PENDING"));        allJobs.put(jobRef.getProjectId(), jobRef.getJobId(), new JobInfo(job));    }}
public Job beam_f30365_0(JobReference jobRef, int maxAttempts) throws InterruptedException
{    BackOff backoff = BackOffAdapter.toGcpBackOff(FluentBackoff.DEFAULT.withMaxRetries(maxAttempts).withInitialBackoff(Duration.millis(10)).withMaxBackoff(Duration.standardSeconds(1)).backoff());    Sleeper sleeper = Sleeper.DEFAULT;    try {        do {            Job job = getJob(jobRef);            if (job != null) {                JobStatus status = job.getStatus();                if (status != null && ("DONE".equals(status.getState()) || "FAILED".equals(status.getState()))) {                    return job;                }            }        } while (BackOffUtils.next(sleeper, backoff));    } catch (IOException e) {        return null;    }    return null;}
public void beam_f30366_0(String projectId, String query, JobStatistics result)
{    synchronized (dryRunQueryResults) {        dryRunQueryResults.put(projectId, query, result);    }}
private JobStatus beam_f30374_0(Job job, JobConfigurationExtract extract) throws InterruptedException, IOException
{    TableReference sourceTable = extract.getSourceTable();    List<TableRow> rows = datasetService.getAllRows(sourceTable.getProjectId(), sourceTable.getDatasetId(), sourceTable.getTableId());    TableSchema schema = datasetService.getTable(sourceTable).getSchema();    List<Long> destinationFileCounts = Lists.newArrayList();    for (String destination : extract.getDestinationUris()) {        destinationFileCounts.add(writeRows(sourceTable.getTableId(), rows, schema, destination));    }    job.setStatistics(new JobStatistics().setExtract(new JobStatistics4().setDestinationUriFileCounts(destinationFileCounts)));    return new JobStatus().setState("DONE");}
private JobStatus beam_f30375_0(JobConfigurationQuery query) throws IOException, InterruptedException
{    KV<Table, List<TableRow>> result = FakeBigQueryServices.decodeQueryResult(query.getQuery());    datasetService.createTable(result.getKey().setTableReference(query.getDestinationTable()));    datasetService.insertAll(query.getDestinationTable(), result.getValue(), null);    return new JobStatus().setState("DONE");}
private List<TableRow> beam_f30376_0(String filename) throws IOException
{    Coder<TableRow> coder = TableRowJsonCoder.of();    List<TableRow> tableRows = Lists.newArrayList();    try (BufferedReader reader = Files.newBufferedReader(Paths.get(filename), StandardCharsets.UTF_8)) {        String line;        while ((line = reader.readLine()) != null) {            TableRow tableRow = coder.decode(new ByteArrayInputStream(line.getBytes(StandardCharsets.UTF_8)), Context.OUTER);            tableRows.add(tableRow);        }    }    return tableRows;}
public static Partition beam_f30384_0(ByteString token)
{    return Partition.createQueryPartition(token, PartitionOptions.getDefaultInstance(), Statement.of(""), Options.fromQueryOptions());}
public static Partition beam_f30385_0(ByteString token)
{    return Partition.createReadPartition(token, PartitionOptions.getDefaultInstance(), "", "", KeySet.all(), Arrays.asList(), Options.fromReadOptions());}
public void beam_f30386_0() throws Exception
{    TableSchema tableSchema = new TableSchema();    tableSchema.setFields(fields);                    BigDecimal birthdayMoney = new BigDecimal("123456789.123456789");    Schema birthdayMoneySchema = Schema.create(Type.BYTES);    LogicalType birthdayMoneyLogicalType = LogicalTypes.decimal(birthdayMoney.precision(), birthdayMoney.scale());                byte[] birthdayMoneyBytes = new Conversions.DecimalConversion().toBytes(birthdayMoney, birthdayMoneySchema, birthdayMoneyLogicalType).array();        List<Schema.Field> avroFields = new ArrayList<>();    for (Schema.Field field : AvroCoder.of(Bird.class).getSchema().getFields()) {        Schema schema = field.schema();        if ("birthdayMoney".equals(field.name())) {                        schema = Schema.createUnion(Schema.create(Type.NULL), birthdayMoneyLogicalType.addToSchema(birthdayMoneySchema));        }                        avroFields.add(new Schema.Field(field.name(), schema, field.doc(), field.defaultValue()));    }    Schema avroSchema = Schema.createRecord(avroFields);    {                GenericRecord record = new GenericData.Record(avroSchema);        record.put("number", 5L);        TableRow convertedRow = BigQueryAvroUtils.convertGenericRecordToTableRow(record, tableSchema);        TableRow row = new TableRow().set("number", "5").set("associates", new ArrayList<TableRow>());        assertEquals(row, convertedRow);        TableRow clonedRow = convertedRow.clone();        assertEquals(convertedRow, clonedRow);    }    {                        GenericRecord record = new GenericData.Record(avroSchema);        byte[] soundBytes = "chirp,chirp".getBytes(StandardCharsets.UTF_8);        ByteBuffer soundByteBuffer = ByteBuffer.wrap(soundBytes);        soundByteBuffer.rewind();        record.put("number", 5L);        record.put("quality", 5.0);        record.put("birthday", 5L);        record.put("birthdayMoney", ByteBuffer.wrap(birthdayMoneyBytes));        record.put("flighted", Boolean.TRUE);        record.put("sound", soundByteBuffer);        record.put("anniversaryDate", new Utf8("2000-01-01"));        record.put("anniversaryDatetime", new String("2000-01-01 00:00:00.000005"));        record.put("anniversaryTime", new Utf8("00:00:00.000005"));        record.put("geoPositions", new String("LINESTRING(1 2, 3 4, 5 6, 7 8)"));        TableRow convertedRow = BigQueryAvroUtils.convertGenericRecordToTableRow(record, tableSchema);        TableRow row = new TableRow().set("number", "5").set("birthday", "1970-01-01 00:00:00.000005 UTC").set("birthdayMoney", birthdayMoney.toString()).set("quality", 5.0).set("associates", new ArrayList<TableRow>()).set("flighted", Boolean.TRUE).set("sound", BaseEncoding.base64().encode(soundBytes)).set("anniversaryDate", "2000-01-01").set("anniversaryDatetime", "2000-01-01 00:00:00.000005").set("anniversaryTime", "00:00:00.000005").set("geoPositions", "LINESTRING(1 2, 3 4, 5 6, 7 8)");        TableRow clonedRow = convertedRow.clone();        assertEquals(convertedRow, clonedRow);        assertEquals(row, convertedRow);    }    {                Schema subBirdSchema = AvroCoder.of(Bird.SubBird.class).getSchema();        GenericRecord nestedRecord = new GenericData.Record(subBirdSchema);        nestedRecord.put("species", "other");        GenericRecord record = new GenericData.Record(avroSchema);        record.put("number", 5L);        record.put("associates", Lists.newArrayList(nestedRecord));        record.put("birthdayMoney", ByteBuffer.wrap(birthdayMoneyBytes));        TableRow convertedRow = BigQueryAvroUtils.convertGenericRecordToTableRow(record, tableSchema);        TableRow row = new TableRow().set("associates", Lists.newArrayList(new TableRow().set("species", "other"))).set("number", "5").set("birthdayMoney", birthdayMoney.toString());        assertEquals(row, convertedRow);        TableRow clonedRow = convertedRow.clone();        assertEquals(convertedRow, clonedRow);    }}
public void beam_f30394_0()
{    BigQueryHelpers.parseTableSpec("a123-456:foo_bar.d");    BigQueryHelpers.parseTableSpec("a12345:b.c");    BigQueryHelpers.parseTableSpec("b12345.c");}
public void beam_f30395_0()
{    TableReference ref = BigQueryHelpers.parseTableSpec("data_set.table_name");    assertEquals(null, ref.getProjectId());    assertEquals("data_set", ref.getDatasetId());    assertEquals("table_name", ref.getTableId());}
public void beam_f30396_0()
{    thrown.expect(IllegalArgumentException.class);    BigQueryHelpers.parseTableSpec("0123456:foo.bar");}
public void beam_f30404_0()
{    CoderProperties.coderSerializable(WindowedValue.getFullCoder(KvCoder.of(ShardedKeyCoder.of(StringUtf8Coder.of()), TableRowInfoCoder.of(TableRowJsonCoder.of())), IntervalWindow.getCoder()));}
public void beam_f30405_0() throws Exception
{    PendingJobManager jobManager = new PendingJobManager(BackOffAdapter.toGcpBackOff(FluentBackoff.DEFAULT.withMaxRetries(Integer.MAX_VALUE).withInitialBackoff(Duration.millis(10)).withMaxBackoff(Duration.millis(10)).backoff()));    Set<String> succeeded = Sets.newHashSet();    for (int i = 0; i < 5; i++) {        Job currentJob = new Job();        currentJob.setKind(" bigquery#job");        PendingJob pendingJob = new PendingJob(retryId -> {            if (new Random().nextInt(2) == 0) {                throw new RuntimeException("Failing to start.");            }            currentJob.setJobReference(new JobReference().setProjectId("").setLocation("").setJobId(retryId.getJobId()));            return null;        }, retryId -> {            if (retryId.getRetryIndex() < 5) {                currentJob.setStatus(new JobStatus().setErrorResult(new ErrorProto()));            } else {                currentJob.setStatus(new JobStatus().setErrorResult(null));            }            return currentJob;        }, retryId -> {            if (retryId.getJobId().equals(currentJob.getJobReference().getJobId())) {                return currentJob;            } else {                return null;            }        }, 100, "JOB_" + i);        jobManager.addPendingJob(pendingJob, j -> {            succeeded.add(j.currentJobId.getJobId());            return null;        });    }    jobManager.waitForDone();    Set<String> expectedJobs = ImmutableSet.of("JOB_0-5", "JOB_1-5", "JOB_2-5", "JOB_3-5", "JOB_4-5");    assertEquals(expectedJobs, succeeded);}
public void beam_f30406_0() throws Exception
{    BigQueryInsertError value = new BigQueryInsertError(new TableRow().setF(Collections.singletonList(new TableCell().setV("Value"))), new TableDataInsertAllResponse.InsertErrors().setIndex(0L).setErrors(Collections.singletonList(new ErrorProto().setReason("a Reason").setLocation("A location").setMessage("A message").setDebugInfo("The debug info"))), new TableReference().setProjectId("dummy-project-id").setDatasetId("dummy-dataset-id").setTableId("dummy-table-id"));    CoderProperties.coderDecodeEncodeEqual(TEST_CODER, value);}
public void beam_f30414_0() throws Exception
{    setupTestEnvironment("1T", true);    runBigQueryIOReadPipeline();}
public Statement beam_f30415_0(final Statement base, final Description description)
{                    Statement withPipeline = new Statement() {        @Override        public void evaluate() throws Throwable {            options = TestPipeline.testingPipelineOptions();            options.as(BigQueryOptions.class).setProject("project-id");            options.as(BigQueryOptions.class).setTempLocation(testFolder.getRoot().getAbsolutePath());            p = TestPipeline.fromOptions(options);            p.apply(base, description).evaluate();        }    };    return testFolder.apply(withPipeline, description);}
public void beam_f30416_0() throws Throwable
{    options = TestPipeline.testingPipelineOptions();    options.as(BigQueryOptions.class).setProject("project-id");    options.as(BigQueryOptions.class).setTempLocation(testFolder.getRoot().getAbsolutePath());    p = TestPipeline.fromOptions(options);    p.apply(base, description).evaluate();}
public void beam_f30424_0()
{    BigQueryIO.Read read = BigQueryIO.read().from("foo.com:project:somedataset.sometable");    checkReadTableObject(read, "foo.com:project", "somedataset", "sometable");}
public void beam_f30425_0()
{    BigQueryIO.Read read = BigQueryIO.read().fromQuery("foo_query");    checkReadQueryObject(read, "foo_query");}
public void beam_f30426_0()
{            BigQueryIO.Read read = BigQueryIO.read().from("foo.com:project:somedataset.sometable").withoutValidation();    checkReadTableObjectWithValidate(read, "foo.com:project", "somedataset", "sometable", false);}
public void beam_f30434_0()
{    thrown.expect(IllegalArgumentException.class);    thrown.expectMessage("Invalid BigQueryIO.Read: Specifies a table with a result flattening preference," + " which only applies to queries");    p.apply(BigQueryIO.read().from("foo.com:project:somedataset.sometable").withoutValidation().withoutResultFlattening());    p.run();}
public void beam_f30435_0()
{    thrown.expect(IllegalArgumentException.class);    thrown.expectMessage("Invalid BigQueryIO.Read: Specifies a table with a SQL dialect preference," + " which only applies to queries");    p.apply(BigQueryIO.read().from("foo.com:project:somedataset.sometable").usingStandardSql());    p.run();}
public void beam_f30436_0() throws IOException, InterruptedException
{    testReadFromTable(false, false);}
public void beam_f30444_0()
{    BigQueryIO.Read read = BigQueryIO.read().fromQuery("myQuery").withoutResultFlattening().usingStandardSql().withoutValidation();    DisplayData displayData = DisplayData.from(read);    assertThat(displayData, hasDisplayItem("query", "myQuery"));    assertThat(displayData, hasDisplayItem("flattenResults", false));    assertThat(displayData, hasDisplayItem("useLegacySql", false));    assertThat(displayData, hasDisplayItem("validation", false));}
public void beam_f30445_0() throws IOException, InterruptedException
{    DisplayDataEvaluator evaluator = DisplayDataEvaluator.create();    BigQueryIO.Read read = BigQueryIO.read().from("project:dataset.tableId").withTestServices(new FakeBigQueryServices().withDatasetService(new FakeDatasetService()).withJobService(new FakeJobService())).withoutValidation();    Set<DisplayData> displayData = evaluator.displayDataForPrimitiveSourceTransforms(read);    assertThat("BigQueryIO.Read should include the table spec in its primitive display data", displayData, hasItem(hasDisplayItem("table")));}
public void beam_f30446_0() throws IOException, InterruptedException
{    DisplayDataEvaluator evaluator = DisplayDataEvaluator.create();    BigQueryIO.Read read = BigQueryIO.read().fromQuery("foobar").withTestServices(new FakeBigQueryServices().withDatasetService(new FakeDatasetService()).withJobService(new FakeJobService())).withoutValidation();    Set<DisplayData> displayData = evaluator.displayDataForPrimitiveSourceTransforms(read);    assertThat("BigQueryIO.Read should include the query in its primitive display data", displayData, hasItem(hasDisplayItem("query")));}
public void beam_f30454_0() throws Exception
{    PCollection<Integer> output = p.apply(Create.of(1, 2, 3)).apply(new PassThroughThenCleanup<>(new PassThroughThenCleanup.CleanupOperation() {        @Override        void cleanup(PassThroughThenCleanup.ContextContainer c) throws Exception {                }    }, p.apply("Create1", Create.of("")).apply(View.asSingleton())));    PAssert.that(output).containsInAnyOrder(1, 2, 3);    p.run();}
public void beam_f30456_0() throws Exception
{    p.apply(Create.empty(VarIntCoder.of())).apply(new PassThroughThenCleanup<>(new PassThroughThenCleanup.CleanupOperation() {        @Override        void cleanup(PassThroughThenCleanup.ContextContainer c) throws Exception {            throw new RuntimeException("cleanup executed");        }    }, p.apply("Create1", Create.of("")).apply(View.asSingleton())));    thrown.expect(RuntimeException.class);    thrown.expectMessage("cleanup executed");    p.run();}
 void beam_f30457_0(PassThroughThenCleanup.ContextContainer c) throws Exception
{    throw new RuntimeException("cleanup executed");}
public void beam_f30465_0() throws Exception
{    setUpTestEnvironment("1G");    runBigQueryIOStorageQueryPipeline();}
public Statement beam_f30466_0(Statement base, Description description)
{                    Statement withPipeline = new Statement() {        @Override        public void evaluate() throws Throwable {            options = TestPipeline.testingPipelineOptions().as(BigQueryOptions.class);            options.setProject("project-id");            options.setTempLocation(testFolder.getRoot().getAbsolutePath());            p = TestPipeline.fromOptions(options);            p.apply(base, description).evaluate();        }    };    return testFolder.apply(withPipeline, description);}
public void beam_f30467_0() throws Throwable
{    options = TestPipeline.testingPipelineOptions().as(BigQueryOptions.class);    options.setProject("project-id");    options.setTempLocation(testFolder.getRoot().getAbsolutePath());    p = TestPipeline.fromOptions(options);    p.apply(base, description).evaluate();}
public void beam_f30475_0() throws Exception
{    TypedRead<TableRow> typedRead = getDefaultTypedRead().withQueryLocation("US");    checkTypedReadQueryObject(typedRead, DEFAULT_QUERY);    assertEquals("US", typedRead.getQueryLocation());}
public void beam_f30476_0() throws Exception
{    TypedRead<TableRow> typedRead = getDefaultTypedRead().withKmsKey("kms_key");    checkTypedReadQueryObject(typedRead, DEFAULT_QUERY);    assertEquals("kms_key", typedRead.getKmsKey());}
public void beam_f30477_0() throws Exception
{    TypedRead<TableRow> typedRead = getDefaultTypedRead().withTemplateCompatibility();    checkTypedReadQueryObject(typedRead, DEFAULT_QUERY);    assertTrue(typedRead.getWithTemplateCompatibility());}
public void beam_f30485_0()
{    assertEquals("BigQueryIO.TypedRead", getDefaultTypedRead().getName());}
public void beam_f30486_0()
{    SerializableFunction<SchemaAndRecord, KV<ByteString, ReadSession>> parseFn = new SerializableFunction<SchemaAndRecord, KV<ByteString, ReadSession>>() {        @Override        public KV<ByteString, ReadSession> apply(SchemaAndRecord input) {            return null;        }    };    assertEquals(KvCoder.of(ByteStringCoder.of(), ProtoCoder.of(ReadSession.class)), BigQueryIO.read(parseFn).inferCoder(CoderRegistry.createDefault()));}
public KV<ByteString, ReadSession> beam_f30487_0(SchemaAndRecord input)
{    return null;}
private static ReadRowsResponse beam_f30495_0(Schema schema, Collection<GenericRecord> genericRecords, double fractionConsumed) throws Exception
{    GenericDatumWriter<GenericRecord> writer = new GenericDatumWriter<>(schema);    ByteArrayOutputStream outputStream = new ByteArrayOutputStream();    Encoder binaryEncoder = ENCODER_FACTORY.binaryEncoder(outputStream, null);    for (GenericRecord genericRecord : genericRecords) {        writer.write(genericRecord, binaryEncoder);    }    binaryEncoder.flush();    return ReadRowsResponse.newBuilder().setAvroRows(AvroRows.newBuilder().setSerializedBinaryRows(ByteString.copyFrom(outputStream.toByteArray())).setRowCount(genericRecords.size())).setStatus(StreamStatus.newBuilder().setUnknownFields(UnknownFieldSet.newBuilder().addField(2, UnknownFieldSet.Field.newBuilder().addFixed32(java.lang.Float.floatToIntBits((float) fractionConsumed)).build()).build())).build();}
public KV<String, Long> beam_f30496_0(SchemaAndRecord input)
{    return KV.of(input.getRecord().get("name").toString(), (Long) input.getRecord().get("number"));}
public void beam_f30497_0() throws Exception
{    TableReference sourceTableRef = BigQueryHelpers.parseTableSpec("project:dataset.table");    fakeDatasetService.createDataset(sourceTableRef.getProjectId(), sourceTableRef.getDatasetId(), "asia-northeast1", "Fake plastic tree^H^H^H^Htables", null);    fakeDatasetService.createTable(new Table().setTableReference(sourceTableRef).setLocation("asia-northeast1"));    Table queryResultTable = new Table().setSchema(new TableSchema().setFields(ImmutableList.of(new TableFieldSchema().setName("name").setType("STRING"), new TableFieldSchema().setName("number").setType("INTEGER")))).setNumBytes(0L);    String encodedQuery = FakeBigQueryServices.encodeQueryResult(queryResultTable);    fakeJobService.expectDryRunQuery(options.getProject(), encodedQuery, new JobStatistics().setQuery(new JobStatistics2().setTotalBytesProcessed(1024L * 1024L).setReferencedTables(ImmutableList.of(sourceTableRef))));    String stepUuid = "testStepUuid";    TableReference tempTableReference = createTempTableReference(options.getProject(), createJobIdToken(options.getJobName(), stepUuid));    CreateReadSessionRequest expectedRequest = CreateReadSessionRequest.newBuilder().setParent("projects/" + options.getProject()).setTableReference(BigQueryHelpers.toTableRefProto(tempTableReference)).setRequestedStreams(10).setUnknownFields(UnknownFieldSet.newBuilder().addField(7, UnknownFieldSet.Field.newBuilder().addVarint(2).build()).build()).build();    ReadSession emptyReadSession = ReadSession.newBuilder().build();    StorageClient fakeStorageClient = mock(StorageClient.class);    when(fakeStorageClient.createReadSession(expectedRequest)).thenReturn(emptyReadSession);    BigQueryStorageQuerySource<TableRow> querySource = BigQueryStorageQuerySource.create(stepUuid, ValueProvider.StaticValueProvider.of(encodedQuery), /* flattenResults = */    true, /* useLegacySql = */    true, /* priority = */    QueryPriority.BATCH, /* location = */    null, /* kmsKey = */    null, new TableRowParser(), TableRowJsonCoder.of(), new FakeBigQueryServices().withDatasetService(fakeDatasetService).withJobService(fakeJobService).withStorageClient(fakeStorageClient));    List<? extends BoundedSource<TableRow>> sources = querySource.split(1024L, options);    assertTrue(sources.isEmpty());}
public KV<String, String> beam_f30505_0(TableRow input)
{    CharSequence sampleString = (CharSequence) input.get("sample_string");    String key = sampleString != null ? sampleString.toString() : "null";    return KV.of(key, BigQueryHelpers.toJsonString(input));}
private void beam_f30506_0(String tableName)
{    PipelineOptionsFactory.register(BigQueryIOStorageReadTableRowOptions.class);    options = TestPipeline.testingPipelineOptions().as(BigQueryIOStorageReadTableRowOptions.class);    String project = TestPipeline.testingPipelineOptions().as(GcpOptions.class).getProject();    options.setInputTable(project + ":" + DATASET_ID + "." + TABLE_PREFIX + tableName);    options.setTempLocation(options.getTempRoot() + "/temp-it/");}
private static void beam_f30507_0(BigQueryIOStorageReadTableRowOptions pipelineOptions)
{    Pipeline pipeline = Pipeline.create(pipelineOptions);    PCollection<KV<String, String>> jsonTableRowsFromExport = pipeline.apply("ExportTable", BigQueryIO.readTableRows().from(pipelineOptions.getInputTable()).withMethod(Method.EXPORT)).apply("MapExportedRows", MapElements.via(new TableRowToKVPairFn()));    PCollection<KV<String, String>> jsonTableRowsFromDirectRead = pipeline.apply("DirectReadTable", BigQueryIO.readTableRows().from(pipelineOptions.getInputTable()).withMethod(Method.DIRECT_READ)).apply("MapDirectReadRows", MapElements.via(new TableRowToKVPairFn()));    final TupleTag<String> exportTag = new TupleTag<>();    final TupleTag<String> directReadTag = new TupleTag<>();    PCollection<KV<String, Set<String>>> unmatchedRows = KeyedPCollectionTuple.of(exportTag, jsonTableRowsFromExport).and(directReadTag, jsonTableRowsFromDirectRead).apply(CoGroupByKey.create()).apply(ParDo.of(new DoFn<KV<String, CoGbkResult>, KV<String, Set<String>>>() {        @ProcessElement        public void processElement(ProcessContext c) throws Exception {            KV<String, CoGbkResult> element = c.element();                        Set<String> uniqueRows = new HashSet<>();            for (String row : element.getValue().getAll(exportTag)) {                uniqueRows.add(row);            }                        for (String row : element.getValue().getAll(directReadTag)) {                if (uniqueRows.contains(row)) {                    uniqueRows.remove(row);                } else {                    uniqueRows.add(row);                }            }                        if (!uniqueRows.isEmpty()) {                c.output(KV.of(element.getKey(), uniqueRows));            }        }    }));    PAssert.that(unmatchedRows).empty();    pipeline.run().waitUntilFinish();}
public void beam_f30515_0()
{    BigQueryIO.TypedRead<TableRow> typedRead = BigQueryIO.read(new TableRowParser()).withCoder(TableRowJsonCoder.of()).withMethod(Method.DIRECT_READ).from("foo.com:project:dataset.table");    checkTypedReadTableObject(typedRead, "foo.com:project", "dataset", "table");    assertTrue(typedRead.getValidate());}
public void beam_f30516_0()
{    BigQueryIO.TypedRead<TableRow> typedRead = BigQueryIO.read(new TableRowParser()).withCoder(TableRowJsonCoder.of()).withMethod(Method.DIRECT_READ).from("foo.com:project:dataset.table").withoutValidation();    checkTypedReadTableObject(typedRead, "foo.com:project", "dataset", "table");    assertFalse(typedRead.getValidate());}
public void beam_f30517_0()
{    BigQueryIO.TypedRead<TableRow> typedRead = BigQueryIO.read(new TableRowParser()).withCoder(TableRowJsonCoder.of()).withMethod(Method.DIRECT_READ).from("myDataset.myTable");    checkTypedReadTableObject(typedRead, null, "myDataset", "myTable");}
public void beam_f30525_0()
{    String tableSpec = "foo.com:project:dataset.table";    BigQueryIO.TypedRead<TableRow> typedRead = BigQueryIO.read(new TableRowParser()).withCoder(TableRowJsonCoder.of()).withMethod(Method.DIRECT_READ).from(tableSpec);    DisplayData displayData = DisplayData.from(typedRead);    assertThat(displayData, hasDisplayItem("table", tableSpec));}
public void beam_f30526_0()
{    DisplayDataEvaluator evaluator = DisplayDataEvaluator.create();    BigQueryIO.TypedRead<TableRow> typedRead = BigQueryIO.read(new TableRowParser()).withCoder(TableRowJsonCoder.of()).withMethod(Method.DIRECT_READ).from("foo.com:project:dataset.table");    Set<DisplayData> displayData = evaluator.displayDataForPrimitiveSourceTransforms(typedRead);    assertThat(displayData, hasItem(hasDisplayItem("table")));}
public void beam_f30527_0()
{    assertEquals("BigQueryIO.TypedRead", BigQueryIO.read(new TableRowParser()).withCoder(TableRowJsonCoder.of()).withMethod(Method.DIRECT_READ).from("foo.com:project:dataset.table").getName());}
public void beam_f30535_0() throws Exception
{    doTableSourceInitialSplitTest(1024L * 1024L, 10);}
public void beam_f30536_0() throws Exception
{    doTableSourceInitialSplitTest(10L, 10_000);}
private void beam_f30537_0(long bundleSize, int streamCount) throws Exception
{    fakeDatasetService.createDataset("foo.com:project", "dataset", "", "", null);    TableReference tableRef = BigQueryHelpers.parseTableSpec("foo.com:project:dataset.table");    Table table = new Table().setTableReference(tableRef).setNumBytes(1024L * 1024L).setSchema(new TableSchema());    fakeDatasetService.createTable(table);    CreateReadSessionRequest expectedRequest = CreateReadSessionRequest.newBuilder().setParent("projects/project-id").setTableReference(BigQueryHelpers.toTableRefProto(tableRef)).setRequestedStreams(streamCount).setUnknownFields(UnknownFieldSet.newBuilder().addField(7, UnknownFieldSet.Field.newBuilder().addVarint(2).build()).build()).build();    ReadSession.Builder builder = ReadSession.newBuilder();    for (int i = 0; i < streamCount; i++) {        builder.addStreams(Stream.newBuilder().setName("stream-" + i));    }    StorageClient fakeStorageClient = mock(StorageClient.class);    when(fakeStorageClient.createReadSession(expectedRequest)).thenReturn(builder.build());    BigQueryStorageTableSource<TableRow> tableSource = BigQueryStorageTableSource.create(ValueProvider.StaticValueProvider.of(tableRef), null, null, null, new TableRowParser(), TableRowJsonCoder.of(), new FakeBigQueryServices().withDatasetService(fakeDatasetService).withStorageClient(fakeStorageClient));    List<? extends BoundedSource<TableRow>> sources = tableSource.split(bundleSize, options);    assertEquals(streamCount, sources.size());}
public void beam_f30545_0() throws Exception
{    BigQueryStorageStreamSource<TableRow> streamSource = BigQueryStorageStreamSource.create(ReadSession.getDefaultInstance(), Stream.getDefaultInstance(), TABLE_SCHEMA, new TableRowParser(), TableRowJsonCoder.of(), new FakeBigQueryServices());    assertEquals(0, streamSource.getEstimatedSizeBytes(options));}
public void beam_f30546_0() throws Exception
{    BigQueryStorageStreamSource<TableRow> streamSource = BigQueryStorageStreamSource.create(ReadSession.getDefaultInstance(), Stream.getDefaultInstance(), TABLE_SCHEMA, new TableRowParser(), TableRowJsonCoder.of(), new FakeBigQueryServices());    assertThat(streamSource.split(0, options), containsInAnyOrder(streamSource));}
public void beam_f30547_0() throws Exception
{    ReadSession readSession = ReadSession.newBuilder().setName("readSession").setAvroSchema(AvroSchema.newBuilder().setSchema(AVRO_SCHEMA_STRING)).build();    Stream stream = Stream.newBuilder().setName("stream").build();    ReadRowsRequest expectedRequest = ReadRowsRequest.newBuilder().setReadPosition(StreamPosition.newBuilder().setStream(stream)).build();    List<GenericRecord> records = Lists.newArrayList(createRecord("A", 1, AVRO_SCHEMA), createRecord("B", 2, AVRO_SCHEMA), createRecord("C", 3, AVRO_SCHEMA));    List<ReadRowsResponse> responses = Lists.newArrayList(createResponse(AVRO_SCHEMA, records.subList(0, 2), 0.50), createResponse(AVRO_SCHEMA, records.subList(2, 3), 0.75));    StorageClient fakeStorageClient = mock(StorageClient.class);    when(fakeStorageClient.readRows(expectedRequest)).thenReturn(new FakeBigQueryServerStream<>(responses));    BigQueryStorageStreamSource<TableRow> streamSource = BigQueryStorageStreamSource.create(readSession, stream, TABLE_SCHEMA, new TableRowParser(), TableRowJsonCoder.of(), new FakeBigQueryServices().withStorageClient(fakeStorageClient));    List<TableRow> rows = new ArrayList<>();    BoundedReader<TableRow> reader = streamSource.createReader(options);    for (boolean hasNext = reader.start(); hasNext; hasNext = reader.advance()) {        rows.add(reader.getCurrent());    }    System.out.println("Rows: " + rows);    assertEquals(3, rows.size());}
public void beam_f30555_0() throws Exception
{    fakeDatasetService.createDataset("foo.com:project", "dataset", "", "", null);    TableReference tableRef = BigQueryHelpers.parseTableSpec("foo.com:project:dataset.table");    Table table = new Table().setTableReference(tableRef).setNumBytes(10L).setSchema(new TableSchema());    fakeDatasetService.createTable(table);    CreateReadSessionRequest expectedCreateReadSessionRequest = CreateReadSessionRequest.newBuilder().setParent("projects/project-id").setTableReference(BigQueryHelpers.toTableRefProto(tableRef)).setRequestedStreams(10).setReadOptions(TableReadOptions.newBuilder().addSelectedFields("name").addSelectedFields("number")).setUnknownFields(UnknownFieldSet.newBuilder().addField(7, UnknownFieldSet.Field.newBuilder().addVarint(2).build()).build()).build();    ReadSession readSession = ReadSession.newBuilder().setName("readSessionName").setAvroSchema(AvroSchema.newBuilder().setSchema(AVRO_SCHEMA_STRING)).addStreams(Stream.newBuilder().setName("streamName")).build();    ReadRowsRequest expectedReadRowsRequest = ReadRowsRequest.newBuilder().setReadPosition(StreamPosition.newBuilder().setStream(Stream.newBuilder().setName("streamName"))).build();    List<GenericRecord> records = Lists.newArrayList(createRecord("A", 1, AVRO_SCHEMA), createRecord("B", 2, AVRO_SCHEMA), createRecord("C", 3, AVRO_SCHEMA), createRecord("D", 4, AVRO_SCHEMA));    List<ReadRowsResponse> readRowsResponses = Lists.newArrayList(createResponse(AVRO_SCHEMA, records.subList(0, 2), 0.50), createResponse(AVRO_SCHEMA, records.subList(2, 4), 0.75));    StorageClient fakeStorageClient = mock(StorageClient.class, withSettings().serializable());    when(fakeStorageClient.createReadSession(expectedCreateReadSessionRequest)).thenReturn(readSession);    when(fakeStorageClient.readRows(expectedReadRowsRequest)).thenReturn(new FakeBigQueryServerStream<>(readRowsResponses));    PCollection<KV<String, Long>> output = p.apply(BigQueryIO.read(new ParseKeyValue()).from("foo.com:project:dataset.table").withMethod(Method.DIRECT_READ).withSelectedFields(p.newProvider(Lists.newArrayList("name", "number"))).withTestServices(new FakeBigQueryServices().withDatasetService(fakeDatasetService).withStorageClient(fakeStorageClient)));    PAssert.that(output).containsInAnyOrder(ImmutableList.of(KV.of("A", 1L), KV.of("B", 2L), KV.of("C", 3L), KV.of("D", 4L)));    p.run();}
public Statement beam_f30556_0(final Statement base, final Description description)
{                    Statement withPipeline = new Statement() {        @Override        public void evaluate() throws Throwable {            options = TestPipeline.testingPipelineOptions();            options.as(BigQueryOptions.class).setProject("project-id");            options.as(BigQueryOptions.class).setTempLocation(testFolder.getRoot().getAbsolutePath());            p = TestPipeline.fromOptions(options);            p.apply(base, description).evaluate();        }    };    return testFolder.apply(withPipeline, description);}
public void beam_f30557_0() throws Throwable
{    options = TestPipeline.testingPipelineOptions();    options.as(BigQueryOptions.class).setProject("project-id");    options.as(BigQueryOptions.class).setTempLocation(testFolder.getRoot().getAbsolutePath());    p = TestPipeline.fromOptions(options);    p.apply(base, description).evaluate();}
public void beam_f30565_0(boolean streaming, boolean schemas) throws Exception
{    final Schema schema = Schema.builder().addField("name", FieldType.STRING).addField("id", FieldType.INT32).build();    final Pattern userPattern = Pattern.compile("([a-z]+)([0-9]+)");    final PCollectionView<List<String>> sideInput1 = p.apply("Create SideInput 1", Create.of("a", "b", "c").withCoder(StringUtf8Coder.of())).apply("asList", View.asList());    final PCollectionView<Map<String, String>> sideInput2 = p.apply("Create SideInput2", Create.of(KV.of("a", "a"), KV.of("b", "b"), KV.of("c", "c"))).apply("AsMap", View.asMap());    final List<String> allUsernames = ImmutableList.of("bill", "bob", "randolph");    List<String> userList = Lists.newArrayList();        for (int i = 0; i < BatchLoads.DEFAULT_MAX_NUM_WRITERS_PER_BUNDLE * 10; ++i) {                for (int j = 0; j < 10; ++j) {            String nickname = allUsernames.get(ThreadLocalRandom.current().nextInt(allUsernames.size()));            userList.add(nickname + i);        }    }    PCollection<String> users = p.apply("CreateUsers", Create.of(userList)).apply(Window.into(new PartitionedGlobalWindows<>(arg -> arg)));    if (streaming) {        users = users.setIsBoundedInternal(PCollection.IsBounded.UNBOUNDED);    }    if (schemas) {        users = users.setSchema(schema, user -> {            Matcher matcher = userPattern.matcher(user);            checkState(matcher.matches());            return Row.withSchema(schema).addValue(matcher.group(1)).addValue(Integer.valueOf(matcher.group(2))).build();        }, r -> r.getString(0) + r.getInt32(1));    }        final String partitionDecorator = "20171127";    BigQueryIO.Write<String> write = BigQueryIO.<String>write().withTestServices(fakeBqServices).withMaxFilesPerBundle(5).withMaxFileSize(10).withCreateDisposition(BigQueryIO.Write.CreateDisposition.CREATE_IF_NEEDED).to(new StringLongDestinations() {        @Override        public Long getDestination(ValueInSingleWindow<String> element) {            assertThat(element.getWindow(), Matchers.instanceOf(PartitionedGlobalWindow.class));            Matcher matcher = userPattern.matcher(element.getValue());            checkState(matcher.matches());                        return Long.valueOf(matcher.group(2));        }        @Override        public TableDestination getTable(Long userId) {            verifySideInputs();                        return new TableDestination("dataset-id.userid-" + userId + "$" + partitionDecorator, "table for userid " + userId);        }        @Override        public TableSchema getSchema(Long userId) {            verifySideInputs();            return new TableSchema().setFields(ImmutableList.of(new TableFieldSchema().setName("name").setType("STRING"), new TableFieldSchema().setName("id").setType("INTEGER")));        }        @Override        public List<PCollectionView<?>> getSideInputs() {            return ImmutableList.of(sideInput1, sideInput2);        }        private void verifySideInputs() {            assertThat(sideInput(sideInput1), containsInAnyOrder("a", "b", "c"));            Map<String, String> mapSideInput = sideInput(sideInput2);            assertEquals(3, mapSideInput.size());            assertThat(mapSideInput, allOf(hasEntry("a", "a"), hasEntry("b", "b"), hasEntry("c", "c")));        }    }).withoutValidation();    if (schemas) {        write = write.useBeamSchema();    } else {        write = write.withFormatFunction(user -> {            Matcher matcher = userPattern.matcher(user);            checkState(matcher.matches());            return new TableRow().set("name", matcher.group(1)).set("id", matcher.group(2));        });    }    users.apply("WriteBigQuery", write);    p.run();    Map<Long, List<TableRow>> expectedTableRows = Maps.newHashMap();    for (String anUserList : userList) {        Matcher matcher = userPattern.matcher(anUserList);        checkState(matcher.matches());        String nickname = matcher.group(1);        Long userid = Long.valueOf(matcher.group(2));        List<TableRow> expected = expectedTableRows.computeIfAbsent(userid, k -> Lists.newArrayList());        expected.add(new TableRow().set("name", nickname).set("id", userid.toString()));    }    for (Map.Entry<Long, List<TableRow>> entry : expectedTableRows.entrySet()) {        assertThat(fakeDatasetService.getAllRows("project-id", "dataset-id", "userid-" + entry.getKey()), containsInAnyOrder(Iterables.toArray(entry.getValue(), TableRow.class)));    }}
public Long beam_f30566_0(ValueInSingleWindow<String> element)
{    assertThat(element.getWindow(), Matchers.instanceOf(PartitionedGlobalWindow.class));    Matcher matcher = userPattern.matcher(element.getValue());    checkState(matcher.matches());        return Long.valueOf(matcher.group(2));}
public TableDestination beam_f30567_0(Long userId)
{    verifySideInputs();        return new TableDestination("dataset-id.userid-" + userId + "$" + partitionDecorator, "table for userid " + userId);}
public void beam_f30575_0() throws Exception
{    testTimePartitioning(BigQueryIO.Write.Method.FILE_LOADS);}
public void beam_f30576_0() throws Exception
{    testClustering(BigQueryIO.Write.Method.STREAMING_INSERTS);}
public void beam_f30577_0() throws Exception
{    testClustering(BigQueryIO.Write.Method.FILE_LOADS);}
public void beam_f30585_0() throws Exception
{    p.apply(Create.of(new TableRow().set("name", "a").set("number", 1), new TableRow().set("name", "b").set("number", 2), new TableRow().set("name", "c").set("number", 3), new TableRow().set("name", "d").set("number", 4)).withCoder(TableRowJsonCoder.of())).setIsBoundedInternal(PCollection.IsBounded.UNBOUNDED).apply(BigQueryIO.writeTableRows().to("project-id:dataset-id.table-id").withCreateDisposition(BigQueryIO.Write.CreateDisposition.CREATE_IF_NEEDED).withSchema(new TableSchema().setFields(ImmutableList.of(new TableFieldSchema().setName("name").setType("STRING"), new TableFieldSchema().setName("number").setType("INTEGER")))).withTestServices(fakeBqServices).withoutValidation());    p.run();    assertThat(fakeDatasetService.getAllRows("project-id", "dataset-id", "table-id"), containsInAnyOrder(new TableRow().set("name", "a").set("number", 1), new TableRow().set("name", "b").set("number", 2), new TableRow().set("name", "c").set("number", 3), new TableRow().set("name", "d").set("number", 4)));}
public void beam_f30586_0() throws Exception
{    p.apply(Create.of(new SchemaPojo("a", 1), new SchemaPojo("b", 2), new SchemaPojo("c", 3), new SchemaPojo("d", 4))).apply(BigQueryIO.<SchemaPojo>write().to("project-id:dataset-id.table-id").withCreateDisposition(BigQueryIO.Write.CreateDisposition.CREATE_IF_NEEDED).withMethod(Method.FILE_LOADS).useBeamSchema().withTestServices(fakeBqServices).withoutValidation());    p.run();    assertThat(fakeDatasetService.getAllRows("project-id", "dataset-id", "table-id"), containsInAnyOrder(new TableRow().set("name", "a").set("number", "1"), new TableRow().set("name", "b").set("number", "2"), new TableRow().set("name", "c").set("number", "3"), new TableRow().set("name", "d").set("number", "4")));}
public void beam_f30587_0() throws Exception
{    p.apply(Create.of(new SchemaPojo("a", 1), new SchemaPojo("b", 2), new SchemaPojo("c", 3), new SchemaPojo("d", 4))).apply(BigQueryIO.<SchemaPojo>write().to("project-id:dataset-id.table-id").withCreateDisposition(BigQueryIO.Write.CreateDisposition.CREATE_IF_NEEDED).withMethod(Method.STREAMING_INSERTS).useBeamSchema().withTestServices(fakeBqServices).withoutValidation());    p.run();    assertThat(fakeDatasetService.getAllRows("project-id", "dataset-id", "table-id"), containsInAnyOrder(new TableRow().set("name", "a").set("number", "1"), new TableRow().set("name", "b").set("number", "2"), new TableRow().set("name", "c").set("number", "3"), new TableRow().set("name", "d").set("number", "4")));}
public boolean beam_f30595_0(Object other)
{    if (other instanceof PartitionedGlobalWindow) {        return value.equals(((PartitionedGlobalWindow) other).value);    }    return false;}
public int beam_f30596_0()
{    return value.hashCode();}
public void beam_f30597_0(PartitionedGlobalWindow window, OutputStream outStream) throws IOException
{    encode(window, outStream, Context.NESTED);}
public void beam_f30606_0() throws Exception
{    p.apply(Create.of(new TableRow().set("name", "a").set("number", 1), new TableRow().set("name", "b").set("number", 2), new TableRow().set("name", "c").set("number", 3)).withCoder(TableRowJsonCoder.of())).apply(BigQueryIO.writeTableRows().to("dataset-id.table-id").withCreateDisposition(BigQueryIO.Write.CreateDisposition.CREATE_NEVER).withTestServices(fakeBqServices).withoutValidation());    thrown.expect(RuntimeException.class);    thrown.expectMessage("Failed to create job with prefix");    thrown.expectMessage("reached max retries");    thrown.expectMessage("last failed job");    p.run();}
public void beam_f30607_0() throws Exception
{    PCollectionView<Map<String, String>> view = p.apply("Create schema view", Create.of(KV.of("foo", "bar"), KV.of("bar", "boo"))).apply(View.asMap());    p.apply(Create.empty(TableRowJsonCoder.of())).apply(BigQueryIO.writeTableRows().to("dataset-id.table-id").withCreateDisposition(BigQueryIO.Write.CreateDisposition.CREATE_IF_NEEDED).withSchemaFromView(view).withTestServices(fakeBqServices).withoutValidation());    thrown.expectMessage("does not contain data for table destination dataset-id.table-id");    p.run();}
public void beam_f30608_0() throws Exception
{    p.apply(Create.<TableRow>of(new TableRow().set("foo", "bar"))).apply(BigQueryIO.writeTableRows().to(input -> null).withCreateDisposition(BigQueryIO.Write.CreateDisposition.CREATE_NEVER).withTestServices(fakeBqServices).withoutValidation());    thrown.expectMessage("result of tableFunction can not be null");    thrown.expectMessage("foo");    p.run();}
public void beam_f30616_0() throws Exception
{    testWriteValidatesDataset(true);}
public void beam_f30617_0() throws Exception
{    p.enableAbandonedNodeEnforcement(false);    TableReference tableRef = new TableReference();    tableRef.setDatasetId("dataset");    tableRef.setTableId("sometable");    PCollection<TableRow> tableRows = p.apply(GenerateSequence.from(0)).apply(MapElements.via(new SimpleFunction<Long, TableRow>() {        @Override        public TableRow apply(Long input) {            return null;        }    })).setCoder(TableRowJsonCoder.of());    tableRows.apply(BigQueryIO.writeTableRows().to(tableRef).withCreateDisposition(BigQueryIO.Write.CreateDisposition.CREATE_NEVER).withoutValidation());}
public TableRow beam_f30618_0(Long input)
{    return null;}
public String beam_f30626_0(ValueInSingleWindow<String> element)
{    throw new UnsupportedOperationException("getDestination not expected in this test.");}
public TableDestination beam_f30627_0(String destination)
{    return new TableDestination(destination, destination);}
public TableSchema beam_f30628_0(String destination)
{    return new TableSchema();}
public void beam_f30636_0() throws Exception
{    TableRow row1 = new TableRow().set("name", "a").set("number", "1");    TableRow row2 = new TableRow().set("name", "b").set("number", "2");    TableRow row3 = new TableRow().set("name", "c").set("number", "3");    String tableSpec = "project-id:dataset-id.table-id";    TableDataInsertAllResponse.InsertErrors ephemeralError = new TableDataInsertAllResponse.InsertErrors().setErrors(ImmutableList.of(new ErrorProto().setReason("timeout")));    TableDataInsertAllResponse.InsertErrors persistentError = new TableDataInsertAllResponse.InsertErrors().setErrors(Lists.newArrayList(new ErrorProto().setReason("invalidQuery")));    fakeDatasetService.failOnInsert(ImmutableMap.of(row1, ImmutableList.of(ephemeralError, ephemeralError), row2, ImmutableList.of(ephemeralError, ephemeralError, persistentError)));    PCollection<BigQueryInsertError> failedRows = p.apply(Create.of(row1, row2, row3)).apply(BigQueryIO.writeTableRows().to(tableSpec).withCreateDisposition(BigQueryIO.Write.CreateDisposition.CREATE_IF_NEEDED).withMethod(BigQueryIO.Write.Method.STREAMING_INSERTS).withSchema(new TableSchema().setFields(ImmutableList.of(new TableFieldSchema().setName("name").setType("STRING"), new TableFieldSchema().setName("number").setType("INTEGER")))).withFailedInsertRetryPolicy(InsertRetryPolicy.retryTransientErrors()).withTestServices(fakeBqServices).withoutValidation().withExtendedErrorInfo()).getFailedInsertsWithErr();            PAssert.that(failedRows).containsInAnyOrder(new BigQueryInsertError(row2, persistentError, BigQueryHelpers.parseTableSpec(tableSpec)));    p.run();        assertThat(fakeDatasetService.getAllRows("project-id", "dataset-id", "table-id"), containsInAnyOrder(row1, row3));}
public void beam_f30637_0()
{    p.enableAutoRunIfMissing(true);    TableRow row1 = new TableRow().set("name", "a").set("number", "1");    BigQueryIO.Write<TableRow> bqIoWrite = BigQueryIO.writeTableRows().to("project-id:dataset-id.table-id").withCreateDisposition(BigQueryIO.Write.CreateDisposition.CREATE_IF_NEEDED).withMethod(BigQueryIO.Write.Method.STREAMING_INSERTS).withSchema(new TableSchema().setFields(ImmutableList.of(new TableFieldSchema().setName("name").setType("STRING"), new TableFieldSchema().setName("number").setType("INTEGER")))).withFailedInsertRetryPolicy(InsertRetryPolicy.retryTransientErrors()).withTestServices(fakeBqServices).withoutValidation();    try {        p.apply("Create1", Create.<TableRow>of(row1)).apply("Write 1", bqIoWrite).getFailedInsertsWithErr();        fail();    } catch (IllegalArgumentException e) {        assertThat(e.getMessage(), is("Cannot use getFailedInsertsWithErr as this WriteResult " + "does not use extended errors. Use getFailedInserts instead"));    }    try {        p.apply("Create2", Create.<TableRow>of(row1)).apply("Write2", bqIoWrite.withExtendedErrorInfo()).getFailedInserts();        fail();    } catch (IllegalArgumentException e) {        assertThat(e.getMessage(), is("Cannot use getFailedInserts as this WriteResult " + "uses extended errors information. Use getFailedInsertsWithErr instead"));    }}
public static void beam_f30638_0() throws Exception
{    options = TestPipeline.testingPipelineOptions().as(TestPipelineOptions.class);    project = options.as(GcpOptions.class).getProject();    BQ_CLIENT.createNewDataset(project, BIG_QUERY_DATASET_ID);}
public LowLevelHttpResponse beam_f30646_0() throws IOException
{    return response;}
public void beam_f30647_0() throws IOException, InterruptedException
{    Job testJob = new Job();    JobReference jobRef = new JobReference();    jobRef.setJobId("jobId");    jobRef.setProjectId("projectId");    testJob.setJobReference(jobRef);    when(response.getContentType()).thenReturn(Json.MEDIA_TYPE);    when(response.getStatusCode()).thenReturn(200);    when(response.getContent()).thenReturn(toStream(testJob));    Sleeper sleeper = new FastNanoClockAndSleeper();    JobServiceImpl.startJob(testJob, new ApiErrorExtractor(), bigquery, sleeper, BackOffAdapter.toGcpBackOff(FluentBackoff.DEFAULT.backoff()));    verify(response, times(1)).getStatusCode();    verify(response, times(1)).getContent();    verify(response, times(1)).getContentType();    expectedLogs.verifyInfo(String.format("Started BigQuery job: %s", jobRef));}
public void beam_f30648_0() throws IOException, InterruptedException
{    Job testJob = new Job();    JobReference jobRef = new JobReference();    jobRef.setJobId("jobId");    jobRef.setProjectId("projectId");    testJob.setJobReference(jobRef);        when(response.getStatusCode()).thenReturn(409);    Sleeper sleeper = new FastNanoClockAndSleeper();    JobServiceImpl.startJob(testJob, new ApiErrorExtractor(), bigquery, sleeper, BackOffAdapter.toGcpBackOff(FluentBackoff.DEFAULT.backoff()));    verify(response, times(1)).getStatusCode();    verify(response, times(1)).getContent();    verify(response, times(1)).getContentType();    expectedLogs.verifyNotLogged("Started BigQuery job");}
public void beam_f30656_0() throws Exception
{    TableReference tableRef = new TableReference().setProjectId("projectId").setDatasetId("datasetId").setTableId("tableId");    Table testTable = new Table();    testTable.setTableReference(tableRef);    when(response.getContentType()).thenReturn(Json.MEDIA_TYPE);    when(response.getStatusCode()).thenReturn(403).thenReturn(200);    when(response.getContent()).thenReturn(toStream(errorWithReasonAndStatus("rateLimitExceeded", 403))).thenReturn(toStream(testTable));    BigQueryServicesImpl.DatasetServiceImpl datasetService = new BigQueryServicesImpl.DatasetServiceImpl(bigquery, PipelineOptionsFactory.create());    Table table = datasetService.getTable(tableRef, null, BackOff.ZERO_BACKOFF, Sleeper.DEFAULT);    assertEquals(testTable, table);    verify(response, times(2)).getStatusCode();    verify(response, times(2)).getContent();    verify(response, times(2)).getContentType();}
public void beam_f30657_0() throws IOException, InterruptedException
{    when(response.getContentType()).thenReturn(Json.MEDIA_TYPE);    when(response.getStatusCode()).thenReturn(404);    BigQueryServicesImpl.DatasetServiceImpl datasetService = new BigQueryServicesImpl.DatasetServiceImpl(bigquery, PipelineOptionsFactory.create());    TableReference tableRef = new TableReference().setProjectId("projectId").setDatasetId("datasetId").setTableId("tableId");    Table table = datasetService.getTable(tableRef, null, BackOff.ZERO_BACKOFF, Sleeper.DEFAULT);    assertNull(table);    verify(response, times(1)).getStatusCode();    verify(response, times(1)).getContent();    verify(response, times(1)).getContentType();}
public void beam_f30658_0() throws Exception
{    when(response.getContentType()).thenReturn(Json.MEDIA_TYPE);    when(response.getStatusCode()).thenReturn(401);    TableReference tableRef = new TableReference().setProjectId("projectId").setDatasetId("datasetId").setTableId("tableId");    thrown.expect(IOException.class);    thrown.expectMessage(String.format("Unable to get table: %s", tableRef.getTableId()));    BigQueryServicesImpl.DatasetServiceImpl datasetService = new BigQueryServicesImpl.DatasetServiceImpl(bigquery, PipelineOptionsFactory.create());    datasetService.getTable(tableRef, null, BackOff.STOP_BACKOFF, Sleeper.DEFAULT);}
public void beam_f30666_0() throws Exception
{    TableReference ref = new TableReference().setProjectId("project").setDatasetId("dataset").setTableId("table");    List<ValueInSingleWindow<TableRow>> rows = ImmutableList.of(wrapValue(new TableRow().set("row", "a")), wrapValue(new TableRow().set("row", "b")));    List<String> insertIds = ImmutableList.of("a", "b");    final TableDataInsertAllResponse bFailed = new TableDataInsertAllResponse().setInsertErrors(ImmutableList.of(new InsertErrors().setIndex(1L).setErrors(ImmutableList.of(new ErrorProto()))));    final TableDataInsertAllResponse allRowsSucceeded = new TableDataInsertAllResponse();    when(response.getContentType()).thenReturn(Json.MEDIA_TYPE);    when(response.getStatusCode()).thenReturn(200).thenReturn(200);    when(response.getContent()).thenReturn(toStream(bFailed)).thenReturn(toStream(allRowsSucceeded));    DatasetServiceImpl dataService = new DatasetServiceImpl(bigquery, PipelineOptionsFactory.create());    dataService.insertAll(ref, rows, insertIds, BackOffAdapter.toGcpBackOff(TEST_BACKOFF.backoff()), new MockSleeper(), InsertRetryPolicy.alwaysRetry(), null, null, false, false);    verify(response, times(2)).getStatusCode();    verify(response, times(2)).getContent();    verify(response, times(2)).getContentType();}
public void beam_f30667_0() throws Exception
{    TableReference ref = new TableReference().setProjectId("project").setDatasetId("dataset").setTableId("table");    List<ValueInSingleWindow<TableRow>> rows = ImmutableList.of(wrapValue(new TableRow()), wrapValue(new TableRow()));    final TableDataInsertAllResponse row1Failed = new TableDataInsertAllResponse().setInsertErrors(ImmutableList.of(new InsertErrors().setIndex(1L)));    final TableDataInsertAllResponse row0Failed = new TableDataInsertAllResponse().setInsertErrors(ImmutableList.of(new InsertErrors().setIndex(0L)));    when(response.getContentType()).thenReturn(Json.MEDIA_TYPE);        when(response.getStatusCode()).thenReturn(200);        when(response.getContent()).thenReturn(toStream(row1Failed)).thenAnswer(invocation -> toStream(row0Failed));    DatasetServiceImpl dataService = new DatasetServiceImpl(bigquery, PipelineOptionsFactory.create());        try {        dataService.insertAll(ref, rows, null, BackOffAdapter.toGcpBackOff(TEST_BACKOFF.backoff()), new MockSleeper(), InsertRetryPolicy.alwaysRetry(), null, null, false, false);        fail();    } catch (IOException e) {        assertThat(e, instanceOf(IOException.class));        assertThat(e.getMessage(), containsString("Insert failed:"));        assertThat(e.getMessage(), containsString("[{\"index\":0}]"));    }        verify(response, times(4)).getStatusCode();    verify(response, times(4)).getContent();    verify(response, times(4)).getContentType();    expectedLogs.verifyInfo("Retrying 1 failed inserts to BigQuery");}
public void beam_f30668_0() throws Throwable
{    TableReference ref = new TableReference().setProjectId("project").setDatasetId("dataset").setTableId("table");    List<ValueInSingleWindow<TableRow>> rows = new ArrayList<>();    rows.add(wrapValue(new TableRow()));                when(response.getContentType()).thenReturn(Json.MEDIA_TYPE);    when(response.getStatusCode()).thenReturn(403).thenReturn(200);    when(response.getContent()).thenReturn(toStream(errorWithReasonAndStatus("actually forbidden", 403))).thenReturn(toStream(new TableDataInsertAllResponse()));    DatasetServiceImpl dataService = new DatasetServiceImpl(bigquery, PipelineOptionsFactory.create());    dataService.insertAll(ref, rows, null, BackOffAdapter.toGcpBackOff(TEST_BACKOFF.backoff()), new MockSleeper(), InsertRetryPolicy.alwaysRetry(), null, null, false, false);    verify(response, times(2)).getStatusCode();    verify(response, times(2)).getContent();    verify(response, times(2)).getContentType();    expectedLogs.verifyInfo("BigQuery insertAll error, retrying:");}
public void beam_f30676_0() throws IOException
{    TableReference ref = new TableReference().setProjectId("project").setDatasetId("dataset").setTableId("table");    TableSchema schema = new TableSchema().setFields(ImmutableList.of(new TableFieldSchema().setName("column1").setType("String"), new TableFieldSchema().setName("column2").setType("Integer")));    Table testTable = new Table().setTableReference(ref).setSchema(schema);        when(response.getStatusCode()).thenReturn(409);    BigQueryServicesImpl.DatasetServiceImpl services = new BigQueryServicesImpl.DatasetServiceImpl(bigquery, PipelineOptionsFactory.create());    Table ret = services.tryCreateTable(testTable, new RetryBoundedBackOff(0, BackOff.ZERO_BACKOFF), Sleeper.DEFAULT);    assertNull(ret);    verify(response, times(1)).getStatusCode();    verify(response, times(1)).getContent();    verify(response, times(1)).getContentType();}
public void beam_f30677_0() throws IOException
{    TableReference ref = new TableReference().setProjectId("project").setDatasetId("dataset").setTableId("table");    Table testTable = new Table().setTableReference(ref);        when(response.getContentType()).thenReturn(Json.MEDIA_TYPE);    when(response.getStatusCode()).thenReturn(403).thenReturn(200);    when(response.getContent()).thenReturn(toStream(errorWithReasonAndStatus("rateLimitExceeded", 403))).thenReturn(toStream(testTable));    BigQueryServicesImpl.DatasetServiceImpl services = new BigQueryServicesImpl.DatasetServiceImpl(bigquery, PipelineOptionsFactory.create());    Table ret = services.tryCreateTable(testTable, new RetryBoundedBackOff(3, BackOff.ZERO_BACKOFF), Sleeper.DEFAULT);    assertEquals(testTable, ret);    verify(response, times(2)).getStatusCode();    verify(response, times(2)).getContent();    verify(response, times(2)).getContentType();    verifyNotNull(ret.getTableReference());    expectedLogs.verifyInfo("Quota limit reached when creating table project:dataset.table, " + "retrying up to 5.0 minutes");}
public void beam_f30678_0() throws InterruptedException, IOException
{    TableReference ref = new TableReference().setProjectId("project").setDatasetId("dataset").setTableId("table");    List<ValueInSingleWindow<TableRow>> rows = ImmutableList.of(wrapValue(new TableRow().set("a", 1)), wrapValue(new TableRow().set("b", 2)));    final TableDataInsertAllResponse failures = new TableDataInsertAllResponse().setInsertErrors(ImmutableList.of(new InsertErrors().setIndex(0L).setErrors(ImmutableList.of(new ErrorProto().setReason("timeout"))), new InsertErrors().setIndex(1L).setErrors(ImmutableList.of(new ErrorProto().setReason("invalid")))));    when(response.getContentType()).thenReturn(Json.MEDIA_TYPE);    when(response.getStatusCode()).thenReturn(200);    when(response.getContentType()).thenReturn(Json.MEDIA_TYPE);    when(response.getContent()).thenReturn(toStream(failures));    DatasetServiceImpl dataService = new DatasetServiceImpl(bigquery, PipelineOptionsFactory.create());    List<ValueInSingleWindow<TableRow>> failedInserts = Lists.newArrayList();    dataService.insertAll(ref, rows, null, BackOffAdapter.toGcpBackOff(TEST_BACKOFF.backoff()), new MockSleeper(), InsertRetryPolicy.neverRetry(), failedInserts, ErrorContainer.TABLE_ROW_ERROR_CONTAINER, false, false);    assertThat(failedInserts, is(rows));}
public void beam_f30686_0() throws Exception
{    String tableName = "weather_stations_time_partitioned_" + System.currentTimeMillis();    Pipeline p = Pipeline.create(options);    p.apply(BigQueryIO.readTableRows().from(options.getBqcInput())).apply(ParDo.of(new KeepStationNumberAndConvertDate())).apply(BigQueryIO.writeTableRows().to(String.format("%s.%s", DATASET_NAME, tableName)).withTimePartitioning(TIME_PARTITIONING).withSchema(SCHEMA).withCreateDisposition(BigQueryIO.Write.CreateDisposition.CREATE_IF_NEEDED).withWriteDisposition(BigQueryIO.Write.WriteDisposition.WRITE_TRUNCATE));    p.run().waitUntilFinish();    bqClient = BigqueryClient.getNewBigquerryClient(options.getAppName());    Table table = bqClient.tables().get(options.getProject(), DATASET_NAME, tableName).execute();    Assert.assertEquals(table.getTimePartitioning(), TIME_PARTITIONING);}
public void beam_f30687_0() throws Exception
{    String tableName = "weather_stations_clustered_" + System.currentTimeMillis();    Pipeline p = Pipeline.create(options);    p.apply(BigQueryIO.readTableRows().from(options.getBqcInput())).apply(ParDo.of(new KeepStationNumberAndConvertDate())).apply(BigQueryIO.writeTableRows().to(String.format("%s.%s", DATASET_NAME, tableName)).withTimePartitioning(TIME_PARTITIONING).withClustering(CLUSTERING).withSchema(SCHEMA).withCreateDisposition(BigQueryIO.Write.CreateDisposition.CREATE_IF_NEEDED).withWriteDisposition(BigQueryIO.Write.WriteDisposition.WRITE_TRUNCATE));    p.run().waitUntilFinish();    Table table = bqClient.tables().get(options.getProject(), DATASET_NAME, tableName).execute();    Assert.assertEquals(table.getClustering(), CLUSTERING);}
public void beam_f30688_0() throws Exception
{    String tableName = "weather_stations_clustered_table_function_" + System.currentTimeMillis();    Pipeline p = Pipeline.create(options);    p.apply(BigQueryIO.readTableRows().from(options.getBqcInput())).apply(ParDo.of(new KeepStationNumberAndConvertDate())).apply(BigQueryIO.writeTableRows().to((ValueInSingleWindow<TableRow> vsw) -> new TableDestination(String.format("%s.%s", DATASET_NAME, tableName), null, TIME_PARTITIONING, CLUSTERING)).withClustering().withSchema(SCHEMA).withCreateDisposition(BigQueryIO.Write.CreateDisposition.CREATE_IF_NEEDED).withWriteDisposition(BigQueryIO.Write.WriteDisposition.WRITE_TRUNCATE));    p.run().waitUntilFinish();    Table table = bqClient.tables().get(options.getProject(), DATASET_NAME, tableName).execute();    Assert.assertEquals(table.getClustering(), CLUSTERING);}
private void beam_f30696_0(String outputTable) throws Exception
{    List<String> newTypeQueryExpectedRes = ImmutableList.of("abc=,2000-01-01,00:00:00", "dec=,3000-12-31,23:59:59.990000", "xyw=,2011-01-01,23:59:59.999999");    QueryResponse response = BQ_CLIENT.queryWithRetries(String.format("SELECT bytes, date, time FROM [%s];", outputTable), project);    List<TableRow> tableRows = getTableRowsFromQuery(String.format("SELECT bytes, date, time FROM [%s];", outputTable), MAX_RETRY);    List<String> tableResult = tableRows.stream().map(row -> {        String res = "";        for (TableCell cell : row.getF()) {            if (res.isEmpty()) {                res = cell.getV().toString();            } else {                res = res + "," + cell.getV().toString();            }        }        return res;    }).sorted().collect(Collectors.toList());    assertEquals(newTypeQueryExpectedRes, tableResult);}
private void beam_f30697_0(String outputTable) throws Exception
{    this.verifyLegacyQueryRes(outputTable);}
public static void beam_f30698_0() throws Exception
{    PipelineOptionsFactory.register(BigQueryToTableOptions.class);    project = TestPipeline.testingPipelineOptions().as(GcpOptions.class).getProject();        BQ_CLIENT.createNewDataset(project, BIG_QUERY_DATASET_ID);        BQ_CLIENT.createNewTable(project, BIG_QUERY_DATASET_ID, new Table().setSchema(BigQueryToTableIT.NEW_TYPES_QUERY_TABLE_SCHEMA).setTableReference(new TableReference().setTableId(BigQueryToTableIT.NEW_TYPES_QUERY_TABLE_NAME).setDatasetId(BIG_QUERY_DATASET_ID).setProjectId(project)));    BQ_CLIENT.insertDataToTable(project, BIG_QUERY_DATASET_ID, BigQueryToTableIT.NEW_TYPES_QUERY_TABLE_NAME, BigQueryToTableIT.NEW_TYPES_QUERY_TABLE_DATA);}
public void beam_f30706_0() throws Exception
{    final String outputTable = project + ":" + BIG_QUERY_DATASET_ID + "." + "testStandardQueryWithoutReshuffleWithCustom";    BigQueryToTableOptions options = this.setupStandardQueryTest(outputTable);    options.setExperiments(ImmutableList.of("enable_custom_bigquery_sink", "enable_custom_bigquery_source"));    this.runBigQueryToTablePipeline(options);    this.verifyStandardQueryRes(outputTable);}
public void beam_f30707_0()
{    TableSchema schema = toTableSchema(FLAT_TYPE);    assertThat(schema.getFields(), containsInAnyOrder(ID, VALUE, NAME, TIMESTAMP_VARIANT1, TIMESTAMP_VARIANT2, TIMESTAMP_VARIANT3, TIMESTAMP_VARIANT4, VALID, BINARY));}
public void beam_f30708_0()
{    TableSchema schema = toTableSchema(ARRAY_TYPE);    assertThat(schema.getFields(), contains(IDS));}
public void beam_f30716_0()
{    assertThrows("precision", IllegalArgumentException.class, () -> BigQueryUtils.convertAvroFormat(Schema.Field.of("dummy", Schema.FieldType.DATETIME).getType(), 1000000001L, REJECT_OPTIONS));}
public void beam_f30717_0()
{    long millis = 123456789L;    assertThat(BigQueryUtils.convertAvroFormat(Schema.Field.of("dummy", Schema.FieldType.DATETIME).getType(), millis * 1000, REJECT_OPTIONS), equalTo(new Instant(millis)));}
public void beam_f30718_0()
{    long millis = 123456789L;    assertThat(BigQueryUtils.convertAvroFormat(Schema.Field.of("dummy", Schema.FieldType.DATETIME).getType(), millis * 1000 + 123, TRUNCATE_OPTIONS), equalTo(new Instant(millis)));}
public void beam_f30726_0()
{    Schema beamSchema = BigQueryUtils.fromTableSchema(BQ_FLAT_TYPE);    assertEquals(FLAT_TYPE, beamSchema);}
public void beam_f30727_0()
{    Schema beamSchema = BigQueryUtils.fromTableSchema(BQ_ARRAY_TYPE);    assertEquals(ARRAY_TYPE, beamSchema);}
public void beam_f30728_0()
{    Schema beamSchema = BigQueryUtils.fromTableSchema(BQ_ROW_TYPE);    assertEquals(ROW_TYPE, beamSchema);}
public void beam_f30736_0()
{    Row flatRowExpected = Row.withSchema(AVRO_FLAT_TYPE).addValues(123L, 123.456, "test", false).build();    Row arrayRowExpected = Row.withSchema(AVRO_ARRAY_TYPE).addValues((Object) Arrays.asList(flatRowExpected)).build();    Row expected = Row.withSchema(AVRO_ARRAY_ARRAY_TYPE).addValues((Object) Arrays.asList(arrayRowExpected)).build();    GenericData.Record arrayRecord = new GenericData.Record(AvroUtils.toAvroSchema(AVRO_ARRAY_TYPE));    GenericData.Record flat = new GenericData.Record(AvroUtils.toAvroSchema(AVRO_FLAT_TYPE));    GenericData.Record record = new GenericData.Record(AvroUtils.toAvroSchema(AVRO_ARRAY_ARRAY_TYPE));    flat.put("id", 123L);    flat.put("value", 123.456);    flat.put("name", "test");    flat.put("valid", false);    arrayRecord.put("rows", Arrays.asList(flat));    record.put("array_rows", Arrays.asList(arrayRecord));    Row beamRow = BigQueryUtils.toBeamRow(record, AVRO_ARRAY_ARRAY_TYPE, BigQueryUtils.ConversionOptions.builder().build());    assertEquals(expected, beamRow);}
public void beam_f30737_0()
{    MockitoAnnotations.initMocks(this);    this.options = PipelineOptionsFactory.create();}
public void beam_f30738_0()
{    verifyNoMoreInteractions(mockClient);    verifyNoMoreInteractions(mockTables);    verifyNoMoreInteractions(mockTablesGet);    verifyNoMoreInteractions(mockTabledata);    verifyNoMoreInteractions(mockTabledataList);}
public void beam_f30746_0() throws InterruptedException, IOException
{    onTableGet(basicTableSchema());    TableDataList dataList = new TableDataList().setTotalRows(0L);    onTableList(dataList);    BigQueryServicesImpl.DatasetServiceImpl services = new BigQueryServicesImpl.DatasetServiceImpl(mockClient, options);    services.getTable(new TableReference().setProjectId("project").setDatasetId("dataset").setTableId("table"));    verifyTableGet();}
public void beam_f30747_0() throws Exception
{            List<List<Long>> errorsIndices = new ArrayList<>();    errorsIndices.add(Arrays.asList(0L, 5L, 10L, 15L, 20L));    errorsIndices.add(Arrays.asList(0L, 2L, 4L));    errorsIndices.add(Arrays.asList(0L, 2L));    errorsIndices.add(new ArrayList<>());    onInsertAll(errorsIndices);    TableReference ref = BigQueryHelpers.parseTableSpec("project:dataset.table");    DatasetServiceImpl datasetService = new DatasetServiceImpl(mockClient, options, 5);    List<ValueInSingleWindow<TableRow>> rows = new ArrayList<>();    List<String> ids = new ArrayList<>();    for (int i = 0; i < 25; ++i) {        rows.add(ValueInSingleWindow.of(rawRow("foo", 1234), GlobalWindow.TIMESTAMP_MAX_VALUE, GlobalWindow.INSTANCE, PaneInfo.ON_TIME_AND_ONLY_FIRING));        ids.add("");    }    long totalBytes = 0;    try {        totalBytes = datasetService.insertAll(ref, rows, ids, InsertRetryPolicy.alwaysRetry(), null, null, false, false);    } finally {        verifyInsertAll(5);                assertEquals("Incorrect byte count", 25L * 23L, totalBytes);    }}
public void beam_f30748_0()
{    assertFalse(InsertRetryPolicy.neverRetry().shouldRetry(new Context(new TableDataInsertAllResponse.InsertErrors())));}
public void beam_f30756_0() throws Exception
{    assertThat(TEST_CODER.getEncodedTypeDescriptor(), equalTo(TypeDescriptor.of(TableRow.class)));}
public String beam_f30757_0()
{    throw new IllegalStateException("Value is not accessible");}
public boolean beam_f30758_0()
{    return false;}
public void beam_f30766_0()
{    config.withProjectId(PROJECT_ID).withInstanceId(INSTANCE_ID).withTableId(TABLE_ID).validate();}
public void beam_f30767_0()
{    config.withInstanceId(INSTANCE_ID).withTableId(TABLE_ID);    thrown.expect(IllegalArgumentException.class);    config.validate();}
public void beam_f30768_0()
{    config.withProjectId(PROJECT_ID).withTableId(TABLE_ID);    thrown.expect(IllegalArgumentException.class);    config.validate();}
public void beam_f30776_0() throws Exception
{    service = new FakeBigtableService();    defaultRead = defaultRead.withBigtableService(service);    defaultWrite = defaultWrite.withBigtableService(service);    bigtableCoder = p.getCoderRegistry().getCoder(BIGTABLE_WRITE_TYPE);    config = BigtableConfig.builder().setValidate(true).setBigtableService(service).build();}
private static ByteKey beam_f30777_0(ByteString key)
{    return ByteKey.copyFrom(key.asReadOnlyByteBuffer());}
public void beam_f30778_0()
{    BigtableIO.Read read = BigtableIO.read().withBigtableOptions(BIGTABLE_OPTIONS).withTableId("table").withInstanceId("instance").withProjectId("project").withBigtableOptionsConfigurator(PORT_CONFIGURATOR);    assertEquals("options_project", read.getBigtableOptions().getProjectId());    assertEquals("options_instance", read.getBigtableOptions().getInstanceId());    assertEquals("instance", read.getBigtableConfig().getInstanceId().get());    assertEquals("project", read.getBigtableConfig().getProjectId().get());    assertEquals("table", read.getTableId());    assertEquals(PORT_CONFIGURATOR, read.getBigtableConfig().getBigtableOptionsConfigurator());}
public void beam_f30786_0()
{    BigtableIO.WriteWithResults write = BigtableIO.write().withTableId("table").withProjectId("project").withBigtableOptions(new BigtableOptions.Builder().build()).withWriteResults();    thrown.expect(IllegalArgumentException.class);    write.expand(null);}
public void beam_f30787_0()
{    BigtableIO.WriteWithResults write = BigtableIO.write().withTableId("table").withInstanceId("instance").withBigtableOptions(new BigtableOptions.Builder().build()).withWriteResults();    thrown.expect(IllegalArgumentException.class);    write.expand(null);}
public void beam_f30788_0()
{    BigtableIO.WriteWithResults write = BigtableIO.write().withTableId("table").withBigtableOptions(new BigtableOptions.Builder().build()).withWriteResults();    thrown.expect(IllegalArgumentException.class);    write.expand(null);}
public void beam_f30796_0() throws Exception
{    final String table = "TEST-MANY-ROWS-TABLE";    final int numRows = 1001;    List<Row> testRows = makeTableData(table, numRows);    service.setupSampleRowKeys(table, 3, 1000L);    runReadTest(defaultRead.withTableId(table), testRows);    logged.verifyInfo(String.format("Closing reader after reading %d records.", numRows / 3));}
public boolean beam_f30797_0(@Nullable ByteString input)
{    verifyNotNull(input, "input");    return input.toStringUtf8().matches(regex);}
private static List<Row> beam_f30798_0(List<Row> rows, final ByteKeyRange range)
{    return filterToRanges(rows, ImmutableList.of(range));}
public void beam_f30806_0() throws Exception
{    final String table = "TEST-MANY-ROWS-SPLITS-TABLE";    final int numRows = 1500;    final int numSamples = 10;    final long bytesPerRow = 100L;        makeTableData(table, numRows);    service.setupSampleRowKeys(table, numSamples, bytesPerRow);        BigtableSource source = new BigtableSource(config.withTableId(ValueProvider.StaticValueProvider.of(table)), null, /*filter*/    Arrays.asList(ByteKeyRange.ALL_KEYS), null);    List<BigtableSource> splits = source.split(numRows * bytesPerRow / numSamples, null);        assertThat(splits, hasSize(numSamples));    assertSourcesEqualReferenceSource(source, splits, null);}
private void beam_f30807_0(List<BigtableSource> sources)
{    if (sources.size() > 0) {        assertThat(sources.get(0).getRanges(), hasSize(1));        for (int i = 1; i < sources.size(); i++) {            assertThat(sources.get(i).getRanges(), hasSize(1));            ByteKey lastEndKey = sources.get(i - 1).getRanges().get(0).getEndKey();            ByteKey currentStartKey = sources.get(i).getRanges().get(0).getStartKey();            assertEquals(lastEndKey, currentStartKey);        }    }}
private void beam_f30808_0(List<BigtableSource> sources)
{    for (BigtableSource source : sources) {        assertThat(source.getRanges(), hasSize(1));    }}
public void beam_f30816_0() throws Exception
{    final String table = "TEST-FILTER-SUB-SPLITS";    final int numRows = 1700;    final int numSamples = 10;    final int numSplits = 20;    final long bytesPerRow = 100L;        makeTableData(table, numRows);    service.setupSampleRowKeys(table, numSamples, bytesPerRow);        RowFilter filter = RowFilter.newBuilder().setRowKeyRegexFilter(ByteString.copyFromUtf8(".*17.*")).build();    BigtableSource source = new BigtableSource(config.withTableId(ValueProvider.StaticValueProvider.of(table)), filter, Arrays.asList(ByteKeyRange.ALL_KEYS), null);    List<BigtableSource> splits = source.split(numRows * bytesPerRow / numSplits, null);        assertThat(splits, hasSize(numSplits));    assertSourcesEqualReferenceSource(source, splits, null);}
public void beam_f30817_0()
{    RowFilter rowFilter = RowFilter.newBuilder().setRowKeyRegexFilter(ByteString.copyFromUtf8("foo.*")).build();    ByteKeyRange keyRange = ByteKeyRange.ALL_KEYS.withEndKey(ByteKey.of(0xab, 0xcd));    BigtableIO.Read read = BigtableIO.read().withBigtableOptions(BIGTABLE_OPTIONS).withTableId("fooTable").withRowFilter(rowFilter).withKeyRange(keyRange);    DisplayData displayData = DisplayData.from(read);    assertThat(displayData, hasDisplayItem(allOf(hasKey("tableId"), hasLabel("Bigtable Table Id"), hasValue("fooTable"))));    assertThat(displayData, hasDisplayItem("rowFilter", rowFilter.toString()));    assertThat(displayData, hasDisplayItem("keyRange 0", keyRange.toString()));        assertThat(displayData, hasDisplayItem("bigtableOptions"));}
public void beam_f30818_0() throws IOException, InterruptedException
{    final String table = "fooTable";    service.createTable(table);    RowFilter rowFilter = RowFilter.newBuilder().setRowKeyRegexFilter(ByteString.copyFromUtf8("foo.*")).build();    DisplayDataEvaluator evaluator = DisplayDataEvaluator.create();    BigtableIO.Read read = BigtableIO.read().withBigtableOptions(BIGTABLE_OPTIONS).withTableId(table).withRowFilter(rowFilter).withBigtableService(service);    Set<DisplayData> displayData = evaluator.displayDataForPrimitiveSourceTransforms(read);    assertThat("BigtableIO.Read should include the table id in its primitive display data", displayData, Matchers.hasItem(hasDisplayItem("tableId")));    assertThat("BigtableIO.Read should include the row filter, if it exists, in its primitive " + "display data", displayData, Matchers.hasItem(hasDisplayItem("rowFilter")));}
public void beam_f30826_0() throws Exception
{    final String table = "table";    final String key = "key";    final String value = "value";    service.createTable(table);    Instant elementTimestamp = Instant.parse("2019-06-10T00:00:00");    Duration windowDuration = Duration.standardMinutes(1);    TestStream<Integer> input = TestStream.create(VarIntCoder.of()).advanceWatermarkTo(elementTimestamp).addElements(1).advanceWatermarkTo(elementTimestamp.plus(windowDuration)).addElements(2).advanceWatermarkToInfinity();    BoundedWindow expectedFirstWindow = new IntervalWindow(elementTimestamp, windowDuration);    BoundedWindow expectedSecondWindow = new IntervalWindow(elementTimestamp.plus(windowDuration), windowDuration);    PCollection<BigtableWriteResult> results = p.apply("rows", input).apply("window", Window.into(FixedWindows.of(windowDuration))).apply("expand", ParDo.of(new WriteGeneratorDoFn())).apply("write", defaultWrite.withTableId(table).withWriteResults());    PAssert.that(results).inWindow(expectedFirstWindow).containsInAnyOrder(BigtableWriteResult.create(1));    PAssert.that(results).inWindow(expectedSecondWindow).containsInAnyOrder(BigtableWriteResult.create(2));    p.run();}
public void beam_f30827_0() throws Exception
{    final String table = "TEST-TABLE";    PCollection<KV<ByteString, Iterable<Mutation>>> emptyInput = p.apply(Create.empty(KvCoder.of(ByteStringCoder.of(), IterableCoder.of(ProtoCoder.of(Mutation.class)))));        thrown.expect(IllegalArgumentException.class);    thrown.expectMessage(String.format("Table %s does not exist", table));    emptyInput.apply("write", defaultWrite.withTableId(table));    p.run();}
public void beam_f30828_0() throws Exception
{    PCollection<KV<ByteString, Iterable<Mutation>>> emptyInput = p.apply(Create.empty(KvCoder.of(ByteStringCoder.of(), IterableCoder.of(ProtoCoder.of(Mutation.class)))));    emptyInput.apply("write", defaultWrite.withTableId(NOT_ACCESSIBLE_VALUE));    p.run();}
public BigtableOptions beam_f30836_0()
{    return null;}
public SortedMap<ByteString, ByteString> beam_f30837_0(String tableId)
{    return tables.get(tableId);}
public ByteKeyRange beam_f30838_0(String tableId)
{    verifyTableExists(tableId);    SortedMap<ByteString, ByteString> data = tables.get(tableId);    return ByteKeyRange.of(makeByteKey(data.firstKey()), makeByteKey(data.lastKey()));}
public boolean beam_f30846_0()
{    rows = service.tables.get(source.getTableId().get()).entrySet().iterator();    return advance();}
public boolean beam_f30847_0()
{        Map.Entry<ByteString, ByteString> entry = null;    while (rows.hasNext()) {        entry = rows.next();        if (!filter.apply(entry.getKey()) || !rangesContainsKey(source.getRanges(), makeByteKey(entry.getKey()))) {                        entry = null;            continue;        }                break;    }        if (entry == null) {        currentRow = null;        return false;    }        currentRow = makeRow(entry.getKey(), entry.getValue());    return true;}
private boolean beam_f30848_0(List<ByteKeyRange> ranges, ByteKey key)
{    for (ByteKeyRange range : ranges) {        if (range.containsKey(key)) {            return true;        }    }    return false;}
public void beam_f30858_0() throws IOException, InterruptedException
{    BigtableService.Writer underTest = new BigtableServiceImpl.BigtableWriterImpl(mockSession, TABLE_NAME);    Mutation mutation = Mutation.newBuilder().setSetCell(SetCell.newBuilder().setFamilyName("Family").build()).build();    ByteString key = ByteString.copyFromUtf8("key");    SettableFuture<MutateRowResponse> fakeResponse = SettableFuture.create();    when(mockBulkMutation.add(any(MutateRowsRequest.Entry.class))).thenReturn(fakeResponse);    underTest.writeRecord(KV.of(key, ImmutableList.of(mutation)));    Entry expected = MutateRowsRequest.Entry.newBuilder().setRowKey(key).addMutations(mutation).build();    verify(mockBulkMutation, times(1)).add(expected);    underTest.close();    verify(mockBulkMutation, times(1)).flush();}
public void beam_f30859_0() throws Exception
{    PipelineOptionsFactory.register(BigtableTestOptions.class);    options = TestPipeline.testingPipelineOptions().as(BigtableTestOptions.class);    project = options.as(GcpOptions.class).getProject();    bigtableOptions = new Builder().setProjectId(project).setInstanceId(options.getInstanceId()).setUserAgent("apache-beam-test").build();    session = new BigtableSession(bigtableOptions.toBuilder().setCredentialOptions(CredentialOptions.credential(options.as(GcpOptions.class).getGcpCredential())).build());    tableAdminClient = session.getTableAdminClient();}
public void beam_f30860_0() throws Exception
{    final String tableName = bigtableOptions.getInstanceName().toTableNameStr(tableId);    final String instanceName = bigtableOptions.getInstanceName().toString();    final int numRows = 1000;    final List<KV<ByteString, ByteString>> testData = generateTableData(numRows);    createEmptyTable(instanceName, tableId);    Pipeline p = Pipeline.create(options);    p.apply(GenerateSequence.from(0).to(numRows)).apply(ParDo.of(new DoFn<Long, KV<ByteString, Iterable<Mutation>>>() {        @ProcessElement        public void processElement(ProcessContext c) {            int index = c.element().intValue();            Iterable<Mutation> mutations = ImmutableList.of(Mutation.newBuilder().setSetCell(Mutation.SetCell.newBuilder().setValue(testData.get(index).getValue()).setFamilyName(COLUMN_FAMILY_NAME)).build());            c.output(KV.of(testData.get(index).getKey(), mutations));        }    })).apply(BigtableIO.write().withBigtableOptions(bigtableOptions).withTableId(tableId));    p.run();        Table table = getTable(tableName);    assertThat(table.getColumnFamiliesMap().keySet(), Matchers.hasSize(1));    assertThat(table.getColumnFamiliesMap(), Matchers.hasKey(COLUMN_FAMILY_NAME));        List<KV<ByteString, ByteString>> tableData = getTableData(tableName);    assertThat(tableData, Matchers.containsInAnyOrder(testData.toArray()));}
 AdaptiveThrottler beam_f30868_0()
{    return new AdaptiveThrottler(SAMPLE_PERIOD_MS, SAMPLE_BUCKET_MS, OVERLOAD_RATIO);}
public void beam_f30869_0() throws Exception
{    AdaptiveThrottler throttler = getThrottler();    assertThat(throttler.throttlingProbability(START_TIME_MS), equalTo(0.0));    assertThat("first request is not throttled", throttler.throttleRequest(START_TIME_MS), equalTo(false));}
public void beam_f30870_0() throws Exception
{    AdaptiveThrottler throttler = getThrottler();    long t = START_TIME_MS;    for (; t < START_TIME_MS + 20; t++) {        assertFalse(throttler.throttleRequest(t));        throttler.successfulRequest(t);    }    assertThat(throttler.throttlingProbability(t), equalTo(0.0));}
public void beam_f30878_0() throws Exception
{    Query invalidLimit = Query.newBuilder().setLimit(Int32Value.newBuilder().setValue(0)).build();    thrown.expect(IllegalArgumentException.class);    thrown.expectMessage("Invalid query limit 0: must be positive");    DatastoreIO.v1().read().withQuery(invalidLimit);}
public void beam_f30879_0() throws Exception
{    Query invalidLimit = Query.newBuilder().setLimit(Int32Value.newBuilder().setValue(-5)).build();    thrown.expect(IllegalArgumentException.class);    thrown.expectMessage("Invalid query limit -5: must be positive");    DatastoreIO.v1().read().withQuery(invalidLimit);}
public void beam_f30880_0()
{    DatastoreV1.Read read = DatastoreIO.v1().read().withProjectId(PROJECT_ID).withQuery(QUERY).withNamespace(NAMESPACE);    DisplayData displayData = DisplayData.from(read);    assertThat(displayData, hasDisplayItem("projectId", PROJECT_ID));    assertThat(displayData, hasDisplayItem("query", QUERY.toString()));    assertThat(displayData, hasDisplayItem("namespace", NAMESPACE));}
public void beam_f30888_0()
{    DisplayDataEvaluator evaluator = DisplayDataEvaluator.create();    PTransform<PCollection<Key>, ?> write = DatastoreIO.v1().deleteKey().withProjectId("myProject");    Set<DisplayData> displayData = evaluator.displayDataForPrimitiveTransforms(write);    assertThat("DatastoreIO write should include the project in its primitive display data", displayData, hasItem(hasDisplayItem("projectId")));    assertThat("DatastoreIO write should include the deleteKeyFn in its primitive display data", displayData, hasItem(hasDisplayItem("deleteKeyFn")));}
public void beam_f30889_0() throws Exception
{    DatastoreV1.Write write = DatastoreIO.v1().write().withProjectId(PROJECT_ID);    assertEquals(PROJECT_ID, write.getProjectId());}
public void beam_f30890_0()
{    Key key;        key = makeKey("bird", "finch").build();    assertTrue(isValidKey(key));        key = makeKey("bird", 123).build();    assertTrue(isValidKey(key));        key = makeKey("bird").build();    assertFalse(isValidKey(key));        key = makeKey("bird", "owl").build();    key = makeKey(key, "bird", "horned").build();    assertTrue(isValidKey(key));        key = makeKey("bird", "owl").build();    key = makeKey(key, "bird", 123).build();    assertTrue(isValidKey(key));        key = makeKey("bird", "owl").build();    key = makeKey(key, "bird").build();    assertFalse(isValidKey(key));    key = makeKey().build();    assertFalse(isValidKey(key));}
public void beam_f30898_0() throws Exception
{    datastoreWriterFnTest(100);}
public void beam_f30899_0() throws Exception
{    datastoreWriterFnTest(DatastoreV1.DATASTORE_BATCH_UPDATE_ENTITIES_START * 3 + 100);}
public void beam_f30900_0() throws Exception
{    datastoreWriterFnTest(DatastoreV1.DATASTORE_BATCH_UPDATE_ENTITIES_START * 2);}
public void beam_f30908_0() throws Exception
{    readFnTest(5);}
public void beam_f30909_0() throws Exception
{    readFnTest(QUERY_BATCH_LIMIT + 5);}
public void beam_f30910_0() throws Exception
{    readFnTest(5 * QUERY_BATCH_LIMIT);}
public void beam_f30918_0()
{    DatastoreV1.WriteBatcher writeBatcher = new DatastoreV1.WriteBatcherImpl();    writeBatcher.start();    writeBatcher.addRequestLatency(0, 10000, 200);    writeBatcher.addRequestLatency(0, 10000, 200);    assertEquals(100, writeBatcher.nextBatchSize(0));}
public void beam_f30919_0()
{    DatastoreV1.WriteBatcher writeBatcher = new DatastoreV1.WriteBatcherImpl();    writeBatcher.start();    writeBatcher.addRequestLatency(0, 30000, 50);    writeBatcher.addRequestLatency(0, 30000, 50);    assertEquals(DatastoreV1.DATASTORE_BATCH_UPDATE_ENTITIES_MIN, writeBatcher.nextBatchSize(0));}
public void beam_f30920_0()
{    DatastoreV1.WriteBatcher writeBatcher = new DatastoreV1.WriteBatcherImpl();    writeBatcher.start();    writeBatcher.addRequestLatency(0, 30000, 50);    writeBatcher.addRequestLatency(50000, 5000, 200);    writeBatcher.addRequestLatency(100000, 5000, 200);    assertEquals(200, writeBatcher.nextBatchSize(150000));}
private List<Query> beam_f30928_0(Query query, int numSplits)
{    List<Query> queries = new ArrayList<>();    int offsetOfOriginal = query.getOffset();    for (int i = 0; i < numSplits; i++) {        Query.Builder q = Query.newBuilder();        q.addKindBuilder().setName(KIND);                        q.setOffset(++offsetOfOriginal);        queries.add(q.build());    }    return queries;}
public int beam_f30931_0(long timeSinceEpochMillis)
{    return DatastoreV1.DATASTORE_BATCH_UPDATE_ENTITIES_START;}
public void beam_f30932_0() throws Exception
{    String projectId = "apache-beam-testing";    String kind = "sort_1G";    String namespace = null;            int expectedNumSplits = 32;    testSplitQueryFn(projectId, kind, namespace, expectedNumSplits);}
private void beam_f30940_0(long limit) throws Exception
{    String gqlQuery = String.format("SELECT * from %s WHERE __key__ HAS ANCESTOR KEY(%s, '%s')", options.getKind(), options.getKind(), ancestor);    long expectedNumEntities = numEntities;    if (limit > 0) {        gqlQuery = String.format("%s LIMIT %d", gqlQuery, limit);        expectedNumEntities = limit;    }    DatastoreV1.Read read = DatastoreIO.v1().read().withProjectId(project).withLiteralGqlQuery(gqlQuery).withNamespace(options.getNamespace());        Pipeline p = Pipeline.create(options);    PCollection<Long> count = p.apply(read).apply(Count.globally());    PAssert.thatSingleton(count).isEqualTo(expectedNumEntities);    p.run();}
private static void beam_f30941_0(V1TestOptions options, String project, String ancestor, long numEntities) throws Exception
{    Datastore datastore = getDatastore(options, project);        V1TestWriter writer = new V1TestWriter(datastore, new UpsertMutationBuilder());    Key ancestorKey = makeAncestorKey(options.getNamespace(), options.getKind(), ancestor);    for (long i = 0; i < numEntities; i++) {        Entity entity = makeEntity(i, ancestorKey, options.getKind(), options.getNamespace(), 0);        writer.write(entity);    }    writer.close();}
 static Key beam_f30942_0(@Nullable String namespace, String kind, String ancestor)
{    Key.Builder keyBuilder = makeKey(kind, ancestor);    if (namespace != null) {        keyBuilder.getPartitionIdBuilder().setNamespaceId(namespace);    }    return keyBuilder.build();}
public Mutation.Builder beam_f30950_0(Entity entity)
{    return makeUpsert(entity);}
public Mutation.Builder beam_f30951_0(Entity entity)
{    return makeDelete(entity.getKey());}
 static boolean beam_f30952_0(Key key)
{    List<PathElement> elementList = key.getPathList();    if (elementList.isEmpty()) {        return false;    }    PathElement lastElement = elementList.get(elementList.size() - 1);    return (lastElement.getId() != 0 || !lastElement.getName().isEmpty());}
public void beam_f30960_0() throws Exception
{    Pipeline p = Pipeline.create(options);        p.apply(GenerateSequence.from(0).to(numEntities)).apply(ParDo.of(new CreateEntityFn(options.getKind(), options.getNamespace(), ancestor, 0))).apply(DatastoreIO.v1().write().withProjectId(project));    p.run();        long numEntitiesWritten = countEntities(options, project, ancestor);    assertEquals(numEntities, numEntitiesWritten);}
public void beam_f30961_0() throws Exception
{    Pipeline p = Pipeline.create(options);    /*     * Datastore has a limit of 1MB per entity, and 10MB per write RPC. If each entity is around     * 1MB in size, then we hit the limit on the size of the write long before we hit the limit on     * the number of entities per writes.     */    final int rawPropertySize = 900_000;    final int numLargeEntities = 100;        p.apply(GenerateSequence.from(0).to(numLargeEntities)).apply(ParDo.of(new CreateEntityFn(options.getKind(), options.getNamespace(), ancestor, rawPropertySize))).apply(DatastoreIO.v1().write().withProjectId(project));    p.run();        long numEntitiesWritten = countEntities(options, project, ancestor);    assertEquals(numLargeEntities, numEntitiesWritten);}
public void beam_f30962_0() throws Exception
{    deleteAllEntities(options, project, ancestor);}
public void beam_f30970_0()
{    thrown.expect(RuntimeException.class);    thrown.expectMessage("PubSub message is missing a value for timestamp attribute myAttribute");    Map<String, String> map = ImmutableMap.of("otherLabel", "whatever");    PubsubClient.extractTimestamp("myAttribute", null, map);}
public void beam_f30971_0()
{    long time = 1446162101123L;    Map<String, String> map = ImmutableMap.of("myAttribute", String.valueOf(time));    long timestamp = PubsubClient.extractTimestamp("myAttribute", null, map);    assertEquals(time, timestamp);}
public void beam_f30972_0()
{    roundTripRfc339("2015-10-29T23:41:41Z");}
public void beam_f30980_0()
{    thrown.expect(NumberFormatException.class);    parse("2015-10");}
public void beam_f30981_0()
{            roundTripRfc339("1582-10-15T01:23:45.123Z");}
public void beam_f30982_0()
{        roundTripRfc339("9999-10-29T23:41:41.123999Z");}
public void beam_f30990_0(PullRequest request, StreamObserver<PullResponse> responseObserver)
{    requestsReceived.add(request);    responseObserver.onNext(response);    responseObserver.onCompleted();}
public void beam_f30991_0() throws IOException
{    String expectedTopic = TOPIC.getPath();    PubsubMessage expectedPubsubMessage = PubsubMessage.newBuilder().setData(ByteString.copyFrom(DATA.getBytes(StandardCharsets.UTF_8))).putAllAttributes(ATTRIBUTES).putAllAttributes(ImmutableMap.of(TIMESTAMP_ATTRIBUTE, String.valueOf(MESSAGE_TIME), ID_ATTRIBUTE, RECORD_ID)).build();    final PublishRequest expectedRequest = PublishRequest.newBuilder().setTopic(expectedTopic).addAllMessages(ImmutableList.of(expectedPubsubMessage)).build();    final PublishResponse response = PublishResponse.newBuilder().addAllMessageIds(ImmutableList.of(MESSAGE_ID)).build();    final List<PublishRequest> requestsReceived = new ArrayList<>();    PublisherImplBase publisherImplBase = new PublisherImplBase() {        @Override        public void publish(PublishRequest request, StreamObserver<PublishResponse> responseObserver) {            requestsReceived.add(request);            responseObserver.onNext(response);            responseObserver.onCompleted();        }    };    Server server = InProcessServerBuilder.forName(channelName).addService(publisherImplBase).build().start();    try {        OutgoingMessage actualMessage = new OutgoingMessage(DATA.getBytes(StandardCharsets.UTF_8), ATTRIBUTES, MESSAGE_TIME, RECORD_ID);        int n = client.publish(TOPIC, ImmutableList.of(actualMessage));        assertEquals(1, n);        assertEquals(expectedRequest, Iterables.getOnlyElement(requestsReceived));    } finally {        server.shutdownNow();    }}
public void beam_f30992_0(PublishRequest request, StreamObserver<PublishResponse> responseObserver)
{    requestsReceived.add(request);    responseObserver.onNext(response);    responseObserver.onCompleted();}
public void beam_f31000_0()
{    String topic = "projects/project/topics/topic";    PubsubIO.Read<String> read = PubsubIO.readStrings().fromTopic(StaticValueProvider.of(topic));    assertNotNull(read.getTopicProvider());    assertNull(read.getSubscriptionProvider());    assertNotNull(DisplayData.from(read));}
public void beam_f31001_0()
{    StaticValueProvider<String> provider = StaticValueProvider.of("projects/project/subscriptions/subscription");    Read<String> pubsubRead = PubsubIO.readStrings().fromSubscription(provider);    Pipeline.create().apply(pubsubRead);    assertThat(pubsubRead.getSubscriptionProvider(), not(nullValue()));    assertThat(pubsubRead.getSubscriptionProvider().isAccessible(), is(true));    assertThat(pubsubRead.getSubscriptionProvider().get().asPath(), equalTo(provider.get()));}
public void beam_f31002_0()
{    TestPipeline pipeline = TestPipeline.create();    ValueProvider<String> subscription = pipeline.newProvider("projects/project/subscriptions/subscription");    Read<String> pubsubRead = PubsubIO.readStrings().fromSubscription(subscription);    pipeline.apply(pubsubRead);    assertThat(pubsubRead.getSubscriptionProvider(), not(nullValue()));    assertThat(pubsubRead.getSubscriptionProvider().isAccessible(), is(false));}
public int beam_f31010_0()
{    return Objects.hash(intField, stringField);}
public boolean beam_f31011_0(Object other)
{    if (other == null || !(other instanceof GenericClass)) {        return false;    }    GenericClass o = (GenericClass) other;    return Objects.equals(intField, o.intField) && Objects.equals(stringField, o.stringField);}
public Statement beam_f31012_0(final Statement base, final Description description)
{                    Statement withPipeline = new Statement() {        @Override        public void evaluate() throws Throwable {            options = TestPipeline.testingPipelineOptions();            options.as(PubsubOptions.class).setProject("test-project");            readPipeline = TestPipeline.fromOptions(options);            readPipeline.apply(base, description).evaluate();        }    };    return withPipeline;}
public String beam_f31020_0(PubsubMessage input)
{    return new String(input.getPayload(), StandardCharsets.UTF_8);}
public void beam_f31021_0()
{    Coder<PubsubMessage> coder = PubsubMessagePayloadOnlyCoder.of();    List<PubsubMessage> inputs = ImmutableList.of(new PubsubMessage("foo".getBytes(StandardCharsets.UTF_8), new HashMap<>()), new PubsubMessage("bar".getBytes(StandardCharsets.UTF_8), new HashMap<>()));    setupTestClient(inputs, coder);    PCollection<String> read = readPipeline.apply(PubsubIO.readMessagesWithCoderAndParseFn(StringUtf8Coder.of(), new StringPayloadParseFn()).fromSubscription(SUBSCRIPTION.getPath()).withClock(CLOCK).withClientFactory(clientFactory));    List<String> outputs = ImmutableList.of("foo", "bar");    PAssert.that(read).containsInAnyOrder(outputs);    readPipeline.run();}
public void beam_f31022_0()
{    mockPubsub = Mockito.mock(Pubsub.class, Mockito.RETURNS_DEEP_STUBS);    client = new PubsubJsonClient(TIMESTAMP_ATTRIBUTE, ID_ATTRIBUTE, mockPubsub);}
private static Topic beam_f31030_0(int i)
{    Topic topic = new Topic();    topic.setName(PubsubClient.topicPathFromName(PROJECT.getId(), "Topic" + i).getPath());    return topic;}
public void beam_f31031_0() throws Exception
{    ListSubscriptionsResponse expectedResponse1 = new ListSubscriptionsResponse();    expectedResponse1.setSubscriptions(Collections.singletonList(buildSubscription(1)));    expectedResponse1.setNextPageToken("AVgJH3Z7aHxiDBs");    ListSubscriptionsResponse expectedResponse2 = new ListSubscriptionsResponse();    expectedResponse2.setSubscriptions(Collections.singletonList(buildSubscription(2)));    Subscriptions.List request = mockPubsub.projects().subscriptions().list(PROJECT.getPath());    when((Object) (request.execute())).thenReturn(expectedResponse1, expectedResponse2);    final TopicPath topic101 = PubsubClient.topicPathFromName("testProject", "Topic2");    List<SubscriptionPath> subscriptionPaths = client.listSubscriptions(PROJECT, topic101);    assertEquals(1, subscriptionPaths.size());}
private static Subscription beam_f31032_0(int i)
{    Subscription subscription = new Subscription();    subscription.setName(PubsubClient.subscriptionPathFromName(PROJECT.getId(), "Subscription" + i).getPath());    subscription.setTopic(PubsubClient.topicPathFromName(PROJECT.getId(), "Topic" + i).getPath());    return subscription;}
public void beam_f31040_0() throws Exception
{    CoderProperties.structuralValueDecodeEncodeEqual(TEST_CODER, TEST_VALUE);}
public void beam_f31041_0() throws Exception
{    TypeDescriptor<PubsubMessage> typeDescriptor = new TypeDescriptor<PubsubMessage>() {    };    assertThat(TEST_CODER.getEncodedTypeDescriptor(), equalTo(typeDescriptor));}
public void beam_f31042_0() throws Exception
{    SerializableUtils.ensureSerializableByCoder(TEST_CODER, TEST_VALUE, "error");}
public void beam_f31050_0(ProcessContext c)
{    c.outputWithTimestamp(new PubsubMessage(c.element().getBytes(StandardCharsets.UTF_8), attributes), new Instant(TIMESTAMP));}
private String beam_f31051_0(String data)
{    return Hashing.murmur3_128().hashBytes(data.getBytes(StandardCharsets.UTF_8)).toString();}
public void beam_f31052_0() throws Exception
{    OutgoingMessage message = new OutgoingMessage(DATA.getBytes(StandardCharsets.UTF_8), ImmutableMap.of(), TIMESTAMP, getRecordId(DATA));    CoderProperties.coderDecodeEncodeEqual(PubsubUnboundedSink.CODER, message);    CoderProperties.coderSerializable(PubsubUnboundedSink.CODER);}
private static String beam_f31060_0(PubsubMessage message)
{    return new String(message.getPayload(), StandardCharsets.UTF_8);}
public void beam_f31061_0()
{    setupOneMessage(ImmutableList.of());    CoderProperties.coderSerializable(primSource.getCheckpointMarkCoder());}
public void beam_f31062_0() throws IOException
{    setupOneMessage();    PubsubReader reader = primSource.createReader(p.getOptions(), null);        assertTrue(reader.start());    assertEquals(DATA, data(reader.getCurrent()));    assertFalse(reader.advance());        PubsubCheckpoint checkpoint = reader.getCheckpointMark();    checkpoint.finalizeCheckpoint();    reader.close();}
public void beam_f31070_0() throws Exception
{    TopicPath topicPath = PubsubClient.topicPathFromName("my_project", "my_topic");    factory = PubsubTestClient.createFactoryForCreateSubscription();    PubsubUnboundedSource source = new PubsubUnboundedSource(factory, StaticValueProvider.of(PubsubClient.projectPathFromId("my_project")), StaticValueProvider.of(topicPath), null, /* subscription */    null, /* timestampLabel */    null, /* idLabel */    false);    assertThat(source.getSubscription(), nullValue());    assertThat(source.getSubscription(), nullValue());    PipelineOptions options = PipelineOptionsFactory.create();    PubsubSource actualSource = new PubsubSource(source);    PubsubReader reader = actualSource.createReader(options, null);    SubscriptionPath createdSubscription = reader.subscription;    assertThat(createdSubscription, not(nullValue()));    PubsubCheckpoint checkpoint = reader.getCheckpointMark();    assertThat(checkpoint.subscriptionPath, equalTo(createdSubscription.getPath()));    checkpoint.finalizeCheckpoint();    PubsubCheckpoint deserCheckpoint = CoderUtils.clone(actualSource.getCheckpointMarkCoder(), checkpoint);    assertThat(checkpoint.subscriptionPath, not(nullValue()));    assertThat(checkpoint.subscriptionPath, equalTo(deserCheckpoint.subscriptionPath));    PubsubReader readerFromOriginal = actualSource.createReader(options, checkpoint);    PubsubReader readerFromDeser = actualSource.createReader(options, deserCheckpoint);    assertThat(readerFromOriginal.subscription, equalTo(createdSubscription));    assertThat(readerFromDeser.subscription, equalTo(createdSubscription));}
public void beam_f31071_0() throws Exception
{    setupOneMessage();    PubsubReader reader = primSource.createReader(p.getOptions(), null);    reader.start();    PubsubCheckpoint checkpoint = reader.getCheckpointMark();    reader.close();    checkpoint.finalizeCheckpoint();}
 DatabaseClient beam_f31072_0()
{    synchronized (lock) {        return mockDatabaseClients.get(index);    }}
public void beam_f31080_0() throws Exception
{    SpannerSchema.Builder builder = SpannerSchema.builder();    builder.addColumn("test", "key", "BYTES");    builder.addKeyPart("test", "key", false);    builder.addColumn("test", "keydesc", "BYTES");    builder.addKeyPart("test", "keydesc", true);    SpannerSchema schema = builder.build();    List<Mutation> sortedMutations = Arrays.asList(Mutation.newInsertOrUpdateBuilder("test").set("key").to(ByteArray.fromBase64("abc")).set("keydesc").to(ByteArray.fromBase64("zzz")).build(), Mutation.newInsertOrUpdateBuilder("test").set("key").to(ByteArray.fromBase64("xxx")).set("keydesc").to((ByteArray) null).build(), Mutation.newInsertOrUpdateBuilder("test").set("key").to(ByteArray.fromBase64("xxx")).set("keydesc").to(ByteArray.fromBase64("zzzz")).build(), Mutation.newInsertOrUpdateBuilder("test").set("key").to(ByteArray.fromBase64("xxx")).set("keydesc").to(ByteArray.fromBase64("ssss")).build(), Mutation.newInsertOrUpdateBuilder("test").set("key").to(ByteArray.fromBase64("xxx")).set("keydesc").to(ByteArray.fromBase64("aaa")).build());    verifyEncodedOrdering(schema, sortedMutations);}
public void beam_f31081_0() throws Exception
{    SpannerSchema.Builder builder = SpannerSchema.builder();    builder.addColumn("test", "key", "DATE");    builder.addKeyPart("test", "key", false);    builder.addColumn("test", "keydesc", "DATE");    builder.addKeyPart("test", "keydesc", true);    SpannerSchema schema = builder.build();    List<Mutation> sortedMutations = Arrays.asList(Mutation.newInsertOrUpdateBuilder("test").set("key").to(Date.fromYearMonthDay(2012, 10, 10)).set("keydesc").to(Date.fromYearMonthDay(2000, 10, 10)).build(), Mutation.newInsertOrUpdateBuilder("test").set("key").to(Date.fromYearMonthDay(2020, 10, 10)).set("keydesc").to((Date) null).build(), Mutation.newInsertOrUpdateBuilder("test").set("key").to(Date.fromYearMonthDay(2020, 10, 10)).set("keydesc").to(Date.fromYearMonthDay(2050, 10, 10)).build(), Mutation.newInsertOrUpdateBuilder("test").set("key").to(Date.fromYearMonthDay(2020, 10, 10)).set("keydesc").to(Date.fromYearMonthDay(2000, 10, 10)).build(), Mutation.newInsertOrUpdateBuilder("test").set("key").to(Date.fromYearMonthDay(2020, 10, 10)).set("keydesc").to(Date.fromYearMonthDay(1900, 10, 10)).build());    verifyEncodedOrdering(schema, sortedMutations);}
public void beam_f31082_0() throws Exception
{    SpannerSchema.Builder builder = SpannerSchema.builder();    builder.addColumn("test", "key", "TIMESTAMP");    builder.addKeyPart("test", "key", false);    builder.addColumn("test", "keydesc", "TIMESTAMP");    builder.addKeyPart("test", "keydesc", true);    SpannerSchema schema = builder.build();    List<Mutation> sortedMutations = Arrays.asList(Mutation.newInsertOrUpdateBuilder("test").set("key").to(Timestamp.ofTimeMicroseconds(10000)).set("keydesc").to(Timestamp.ofTimeMicroseconds(50000)).build(), Mutation.newInsertOrUpdateBuilder("test").set("key").to(Timestamp.ofTimeMicroseconds(20000)).set("keydesc").to((Timestamp) null).build(), Mutation.newInsertOrUpdateBuilder("test").set("key").to(Timestamp.ofTimeMicroseconds(20000)).set("keydesc").to(Timestamp.ofTimeMicroseconds(90000)).build(), Mutation.newInsertOrUpdateBuilder("test").set("key").to(Timestamp.ofTimeMicroseconds(20000)).set("keydesc").to(Timestamp.ofTimeMicroseconds(50000)).build(), Mutation.newInsertOrUpdateBuilder("test").set("key").to(Timestamp.ofTimeMicroseconds(20000)).set("keydesc").to(Timestamp.ofTimeMicroseconds(10000)).build());    verifyEncodedOrdering(schema, sortedMutations);}
public void beam_f31090_0() throws Exception
{    Mutation emptyString = Mutation.newInsertOrUpdateBuilder("test").set("one").to("").build();    Mutation nullString = Mutation.newInsertOrUpdateBuilder("test").set("one").to((String) null).build();    Mutation sampleString = Mutation.newInsertOrUpdateBuilder("test").set("one").to("abc").build();    Mutation sampleArray = Mutation.newInsertOrUpdateBuilder("test").set("one").toStringArray(Arrays.asList("one", "two", null)).build();    Mutation nullArray = Mutation.newInsertOrUpdateBuilder("test").set("one").toStringArray(null).build();    assertThat(MutationSizeEstimator.sizeOf(emptyString), is(0L));    assertThat(MutationSizeEstimator.sizeOf(nullString), is(0L));    assertThat(MutationSizeEstimator.sizeOf(sampleString), is(3L));    assertThat(MutationSizeEstimator.sizeOf(sampleArray), is(6L));    assertThat(MutationSizeEstimator.sizeOf(nullArray), is(0L));}
public void beam_f31091_0() throws Exception
{    Mutation empty = Mutation.newInsertOrUpdateBuilder("test").set("one").to(ByteArray.fromBase64("")).build();    Mutation nullValue = Mutation.newInsertOrUpdateBuilder("test").set("one").to((ByteArray) null).build();    Mutation sample = Mutation.newInsertOrUpdateBuilder("test").set("one").to(ByteArray.fromBase64("abcdabcd")).build();    Mutation nullArray = Mutation.newInsertOrUpdateBuilder("test").set("one").toBytesArray(null).build();    assertThat(MutationSizeEstimator.sizeOf(empty), is(0L));    assertThat(MutationSizeEstimator.sizeOf(nullValue), is(0L));    assertThat(MutationSizeEstimator.sizeOf(sample), is(6L));    assertThat(MutationSizeEstimator.sizeOf(nullArray), is(0L));}
public void beam_f31092_0() throws Exception
{    Mutation timestamp = Mutation.newInsertOrUpdateBuilder("test").set("one").to(Timestamp.now()).build();    Mutation nullTimestamp = Mutation.newInsertOrUpdateBuilder("test").set("one").to((Timestamp) null).build();    Mutation date = Mutation.newInsertOrUpdateBuilder("test").set("one").to(Date.fromYearMonthDay(2017, 10, 10)).build();    Mutation nullDate = Mutation.newInsertOrUpdateBuilder("test").set("one").to((Date) null).build();    Mutation timestampArray = Mutation.newInsertOrUpdateBuilder("test").set("one").toTimestampArray(Arrays.asList(Timestamp.now(), null)).build();    Mutation dateArray = Mutation.newInsertOrUpdateBuilder("test").set("one").toDateArray(Arrays.asList(null, Date.fromYearMonthDay(2017, 1, 1), null, Date.fromYearMonthDay(2017, 1, 2))).build();    Mutation nullTimestampArray = Mutation.newInsertOrUpdateBuilder("test").set("one").toTimestampArray(null).build();    Mutation nullDateArray = Mutation.newInsertOrUpdateBuilder("test").set("one").toDateArray(null).build();    assertThat(MutationSizeEstimator.sizeOf(timestamp), is(12L));    assertThat(MutationSizeEstimator.sizeOf(date), is(12L));    assertThat(MutationSizeEstimator.sizeOf(nullTimestamp), is(12L));    assertThat(MutationSizeEstimator.sizeOf(nullDate), is(12L));    assertThat(MutationSizeEstimator.sizeOf(timestampArray), is(24L));    assertThat(MutationSizeEstimator.sizeOf(dateArray), is(48L));    assertThat(MutationSizeEstimator.sizeOf(nullTimestampArray), is(0L));    assertThat(MutationSizeEstimator.sizeOf(nullDateArray), is(0L));}
private static UnsignedNumber beam_f31100_0(long value, String increasingBytes, String decreasingBytes)
{    return new AutoValue_OrderedCodeTest_UnsignedNumber(value, increasingBytes, decreasingBytes);}
 byte[] beam_f31101_0()
{    OrderedCode orderedCode = new OrderedCode();    orderedCode.writeBytes(bytesFromHexString(value()));    return orderedCode.getEncodedBytes();}
 byte[] beam_f31102_0()
{    OrderedCode orderedCode = new OrderedCode();    orderedCode.writeBytesDecreasing(bytesFromHexString(value()));    return orderedCode.getEncodedBytes();}
public void beam_f31110_0()
{    testDecoding(BytesTest.TEST_CASES);}
public void beam_f31111_0()
{    testOrdering(BytesTest.TEST_CASES);}
private void beam_f31112_0(List<? extends CodingTestCase<?>> testCases)
{    for (CodingTestCase<?> testCase : testCases) {        byte[] actualIncreasing = testCase.encodeIncreasing();        byte[] expectedIncreasing = bytesFromHexString(testCase.increasingBytes());        assertEquals(0, compare(actualIncreasing, expectedIncreasing));        byte[] actualDecreasing = testCase.encodeDecreasing();        byte[] expectedDecreasing = bytesFromHexString(testCase.decreasingBytes());        assertEquals(0, compare(actualDecreasing, expectedDecreasing));    }}
public void beam_f31120_0()
{    OrderedCode orderedCode = new OrderedCode();    orderedCode.writeNumIncreasing(UnsignedInteger.fromIntBits(0));    orderedCode.writeNumIncreasing(UnsignedInteger.fromIntBits(1));    orderedCode.writeNumIncreasing(UnsignedInteger.fromIntBits(Integer.MIN_VALUE));    orderedCode.writeNumIncreasing(UnsignedInteger.fromIntBits(Integer.MAX_VALUE));    assertEquals(0, orderedCode.readNumIncreasing());    assertEquals(1, orderedCode.readNumIncreasing());    assertEquals((long) Integer.MAX_VALUE + 1L, orderedCode.readNumIncreasing());    assertEquals(Integer.MAX_VALUE, orderedCode.readNumIncreasing());}
public void beam_f31121_0()
{    OrderedCode orderedCode = new OrderedCode();    orderedCode.writeNumDecreasing(0);    orderedCode.writeNumDecreasing(1);    orderedCode.writeNumDecreasing(Long.MIN_VALUE);    orderedCode.writeNumDecreasing(Long.MAX_VALUE);    assertEquals(0, orderedCode.readNumDecreasing());    assertEquals(1, orderedCode.readNumDecreasing());    assertEquals(Long.MIN_VALUE, orderedCode.readNumDecreasing());    assertEquals(Long.MAX_VALUE, orderedCode.readNumDecreasing());}
public void beam_f31122_0()
{    OrderedCode orderedCode = new OrderedCode();    orderedCode.writeNumDecreasing(UnsignedInteger.fromIntBits(0));    orderedCode.writeNumDecreasing(UnsignedInteger.fromIntBits(1));    orderedCode.writeNumDecreasing(UnsignedInteger.fromIntBits(Integer.MIN_VALUE));    orderedCode.writeNumDecreasing(UnsignedInteger.fromIntBits(Integer.MAX_VALUE));    assertEquals(0, orderedCode.readNumDecreasing());    assertEquals(1, orderedCode.readNumDecreasing());    assertEquals((long) Integer.MAX_VALUE + 1L, orderedCode.readNumDecreasing());    assertEquals(Integer.MAX_VALUE, orderedCode.readNumDecreasing());}
public void beam_f31130_0()
{    assertSignedNumIncreasingWriteAndReadIsLossless(Long.MIN_VALUE);    assertSignedNumIncreasingWriteAndReadIsLossless(Long.MIN_VALUE + 1);    assertSignedNumIncreasingWriteAndReadIsLossless(Integer.MIN_VALUE - 1L);    assertSignedNumIncreasingWriteAndReadIsLossless(Integer.MIN_VALUE);    assertSignedNumIncreasingWriteAndReadIsLossless(Integer.MIN_VALUE + 1);    assertSignedNumIncreasingWriteAndReadIsLossless(-65);    assertSignedNumIncreasingWriteAndReadIsLossless(-64);    assertSignedNumIncreasingWriteAndReadIsLossless(-63);    assertSignedNumIncreasingWriteAndReadIsLossless(-3);    assertSignedNumIncreasingWriteAndReadIsLossless(-2);    assertSignedNumIncreasingWriteAndReadIsLossless(-1);    assertSignedNumIncreasingWriteAndReadIsLossless(0);    assertSignedNumIncreasingWriteAndReadIsLossless(1);    assertSignedNumIncreasingWriteAndReadIsLossless(2);    assertSignedNumIncreasingWriteAndReadIsLossless(3);    assertSignedNumIncreasingWriteAndReadIsLossless(63);    assertSignedNumIncreasingWriteAndReadIsLossless(64);    assertSignedNumIncreasingWriteAndReadIsLossless(65);    assertSignedNumIncreasingWriteAndReadIsLossless(Integer.MAX_VALUE - 1);    assertSignedNumIncreasingWriteAndReadIsLossless(Integer.MAX_VALUE);    assertSignedNumIncreasingWriteAndReadIsLossless(Integer.MAX_VALUE + 1L);    assertSignedNumIncreasingWriteAndReadIsLossless(Long.MAX_VALUE - 1);    assertSignedNumIncreasingWriteAndReadIsLossless(Long.MAX_VALUE);}
private static void beam_f31131_0(long num)
{    OrderedCode orderedCode = new OrderedCode();    orderedCode.writeSignedNumDecreasing(num);    assertEquals("Unexpected result when decoding writeSignedNumDecreasing(" + num + ")", num, orderedCode.readSignedNumDecreasing());    assertFalse("Unexpected remaining encoded bytes after decoding " + num, orderedCode.hasRemainingEncodedBytes());}
public void beam_f31132_0()
{    assertSignedNumDecreasingWriteAndReadIsLossless(Long.MIN_VALUE);    assertSignedNumDecreasingWriteAndReadIsLossless(Long.MIN_VALUE + 1);    assertSignedNumDecreasingWriteAndReadIsLossless(Integer.MIN_VALUE - 1L);    assertSignedNumDecreasingWriteAndReadIsLossless(Integer.MIN_VALUE);    assertSignedNumDecreasingWriteAndReadIsLossless(Integer.MIN_VALUE + 1);    assertSignedNumDecreasingWriteAndReadIsLossless(-65);    assertSignedNumDecreasingWriteAndReadIsLossless(-64);    assertSignedNumDecreasingWriteAndReadIsLossless(-63);    assertSignedNumDecreasingWriteAndReadIsLossless(-3);    assertSignedNumDecreasingWriteAndReadIsLossless(-2);    assertSignedNumDecreasingWriteAndReadIsLossless(-1);    assertSignedNumDecreasingWriteAndReadIsLossless(0);    assertSignedNumDecreasingWriteAndReadIsLossless(1);    assertSignedNumDecreasingWriteAndReadIsLossless(2);    assertSignedNumDecreasingWriteAndReadIsLossless(3);    assertSignedNumDecreasingWriteAndReadIsLossless(63);    assertSignedNumDecreasingWriteAndReadIsLossless(64);    assertSignedNumDecreasingWriteAndReadIsLossless(65);    assertSignedNumDecreasingWriteAndReadIsLossless(Integer.MAX_VALUE - 1);    assertSignedNumDecreasingWriteAndReadIsLossless(Integer.MAX_VALUE);    assertSignedNumDecreasingWriteAndReadIsLossless(Integer.MAX_VALUE + 1L);    assertSignedNumDecreasingWriteAndReadIsLossless(Long.MAX_VALUE - 1);    assertSignedNumDecreasingWriteAndReadIsLossless(Long.MAX_VALUE);}
public void beam_f31140_0()
{    byte[] ffChar = { OrderedCode.FF_CHARACTER };    byte[] nullChar = { OrderedCode.NULL_CHARACTER };    byte[] separatorEncoded = { OrderedCode.ESCAPE1, OrderedCode.SEPARATOR };    byte[] ffCharEncoded = { OrderedCode.ESCAPE1, OrderedCode.NULL_CHARACTER };    byte[] nullCharEncoded = { OrderedCode.ESCAPE2, OrderedCode.FF_CHARACTER };    byte[] infinityEncoded = { OrderedCode.ESCAPE2, OrderedCode.INFINITY };    OrderedCode orderedCode = new OrderedCode();    orderedCode.writeBytes(ffChar);    orderedCode.writeBytes(nullChar);    orderedCode.writeInfinity();    assertArrayEquals(orderedCode.getEncodedBytes(), Bytes.concat(ffCharEncoded, separatorEncoded, nullCharEncoded, separatorEncoded, infinityEncoded));    assertArrayEquals(orderedCode.readBytes(), ffChar);    assertArrayEquals(orderedCode.readBytes(), nullChar);    assertTrue(orderedCode.readInfinity());    orderedCode = new OrderedCode(Bytes.concat(ffCharEncoded, separatorEncoded));    assertArrayEquals(orderedCode.readBytes(), ffChar);    orderedCode = new OrderedCode(Bytes.concat(nullCharEncoded, separatorEncoded));    assertArrayEquals(orderedCode.readBytes(), nullChar);    byte[] invalidEncodingForRead = { OrderedCode.ESCAPE2, OrderedCode.ESCAPE2, OrderedCode.ESCAPE1, OrderedCode.SEPARATOR };    orderedCode = new OrderedCode(invalidEncodingForRead);    try {        orderedCode.readBytes();        fail("Should have failed.");    } catch (Exception e) {        }    assertTrue(orderedCode.hasRemainingEncodedBytes());}
public void beam_f31141_0()
{    byte[] bytes = { 'a', 'b', 'c' };    long number = 12345;        OrderedCode orderedCode = new OrderedCode();    assertFalse(orderedCode.hasRemainingEncodedBytes());        orderedCode.writeBytes(bytes);    assertTrue(orderedCode.hasRemainingEncodedBytes());    assertArrayEquals(orderedCode.readBytes(), bytes);    assertFalse(orderedCode.hasRemainingEncodedBytes());    orderedCode.writeNumIncreasing(number);    assertTrue(orderedCode.hasRemainingEncodedBytes());    assertEquals(orderedCode.readNumIncreasing(), number);    assertFalse(orderedCode.hasRemainingEncodedBytes());    orderedCode.writeSignedNumIncreasing(number);    assertTrue(orderedCode.hasRemainingEncodedBytes());    assertEquals(orderedCode.readSignedNumIncreasing(), number);    assertFalse(orderedCode.hasRemainingEncodedBytes());    orderedCode.writeInfinity();    assertTrue(orderedCode.hasRemainingEncodedBytes());    assertTrue(orderedCode.readInfinity());    assertFalse(orderedCode.hasRemainingEncodedBytes());    orderedCode.writeTrailingBytes(bytes);    assertTrue(orderedCode.hasRemainingEncodedBytes());    assertArrayEquals(orderedCode.readTrailingBytes(), bytes);    assertFalse(orderedCode.hasRemainingEncodedBytes());        orderedCode.writeBytes(bytes);    orderedCode.writeBytes(bytes);    assertTrue(orderedCode.hasRemainingEncodedBytes());    assertArrayEquals(orderedCode.readBytes(), bytes);    assertArrayEquals(orderedCode.readBytes(), bytes);    assertFalse(orderedCode.hasRemainingEncodedBytes());}
public void beam_f31142_0()
{    OrderedCode inf = new OrderedCode();    inf.writeInfinity();    OrderedCode negInf = new OrderedCode();    negInf.writeInfinityDecreasing();    OrderedCode longValue = new OrderedCode();    longValue.writeSignedNumIncreasing(1);    assertTrue(compare(inf.getEncodedBytes(), negInf.getEncodedBytes()) > 0);    assertTrue(compare(longValue.getEncodedBytes(), negInf.getEncodedBytes()) > 0);    assertTrue(compare(inf.getEncodedBytes(), longValue.getEncodedBytes()) > 0);}
public boolean beam_f31150_0(Statement argument)
{    if (!(argument instanceof Statement)) {        return false;    }    Statement st = (Statement) argument;    return st.getSql().contains("information_schema.index_columns");}
public void beam_f31151_0() throws Exception
{    serviceFactory = new FakeServiceFactory();    mockTx = mock(ReadOnlyTransaction.class);}
public void beam_f31152_0() throws Exception
{        ReadOnlyTransaction tx = mock(ReadOnlyTransaction.class);    when(serviceFactory.mockDatabaseClient().readOnlyTransaction()).thenReturn(tx);    preparePkMetadata(tx, Arrays.asList(pkMetadata("test", "key", "ASC")));    prepareColumnMetadata(tx, Arrays.asList(columnMetadata("test", "key", "INT64")));    SpannerConfig config = SpannerConfig.create().withProjectId("test-project").withInstanceId("test-instance").withDatabaseId("test-database").withServiceFactory(serviceFactory);    DoFnTester<Void, SpannerSchema> tester = DoFnTester.of(new ReadSpannerSchema(config));    List<SpannerSchema> schemas = tester.processBundle(Arrays.asList((Void) null));    assertEquals(1, schemas.size());    SpannerSchema schema = schemas.get(0);    assertEquals(1, schema.getTables().size());    SpannerSchema.Column column = SpannerSchema.Column.create("key", Type.int64());    SpannerSchema.KeyPart keyPart = SpannerSchema.KeyPart.create("key", false);    assertThat(schema.getColumns("test"), contains(column));    assertThat(schema.getKeyParts("test"), contains(keyPart));}
private SpannerSchema beam_f31160_0()
{    return SpannerSchema.builder().addColumn("tEsT", "key", "INT64", CELLS_PER_KEY).addKeyPart("tEsT", "key", false).build();}
private static Struct beam_f31161_0(String tableName, String columnName, String type, long cellsMutated)
{    return Struct.newBuilder().set("table_name").to(tableName).set("column_name").to(columnName).set("spanner_type").to(type).set("cells_mutated").to(cellsMutated).build();}
private static Struct beam_f31162_0(String tableName, String columnName, String ordering)
{    return Struct.newBuilder().set("table_name").to(tableName).set("column_name").to(columnName).set("column_ordering").to(ordering).build();}
public void beam_f31170_0() throws Exception
{    Mutation mutation = m(2L);    PCollection<Mutation> mutations = pipeline.apply(Create.of(mutation));    mutations.apply(SpannerIO.write().withProjectId("test-project").withInstanceId("test-instance").withDatabaseId("test-database").withServiceFactory(serviceFactory));    pipeline.run();    verifyBatches(batch(m(2L)));}
public void beam_f31171_0() throws Exception
{    PCollection<MutationGroup> mutations = pipeline.apply(Create.<MutationGroup>of(g(m(1L), m(2L), m(3L))));    mutations.apply(SpannerIO.write().withProjectId("test-project").withInstanceId("test-instance").withDatabaseId("test-database").withServiceFactory(serviceFactory).grouped());    pipeline.run();    verifyBatches(batch(m(1L), m(2L), m(3L)));}
private void beam_f31172_0(Iterable<Mutation>... batches)
{    for (Iterable<Mutation> b : batches) {        verify(serviceFactory.mockDatabaseClient(), times(1)).writeAtLeastOnce(mutationsInNoOrder(b));    }}
public void beam_f31180_0() throws Exception
{    GatherBundleAndSortFn testFn = new GatherBundleAndSortFn(10000000, 10, 100, null);    ProcessContext mockProcessContext = Mockito.mock(ProcessContext.class);    FinishBundleContext mockFinishBundleContext = Mockito.mock(FinishBundleContext.class);    when(mockProcessContext.sideInput(any())).thenReturn(getSchema());        doNothing().when(mockProcessContext).output(byteArrayKvListCaptor.capture());        doNothing().when(mockFinishBundleContext).output(byteArrayKvListCaptor.capture(), any(), any());    MutationGroup[] mutationGroups = new MutationGroup[] { g(m(4L)), g(m(1L)), g(m(5L), m(6L), m(7L), m(8L), m(9L)), g(del(2L)), g(m(3L)) };        testFn.startBundle();    for (MutationGroup m : mutationGroups) {        when(mockProcessContext.element()).thenReturn(m);        testFn.processElement(mockProcessContext);    }    testFn.finishBundle(mockFinishBundleContext);    verify(mockProcessContext, never()).output(any());    verify(mockFinishBundleContext, times(1)).output(any(), any(), any());        List<MutationGroup> sorted = byteArrayKvListCaptor.getValue().stream().map(kv -> WriteGrouped.decode(kv.getValue())).collect(Collectors.toList());    assertThat(sorted, contains(g(m(1L)), g(del(2L)), g(m(3L)), g(m(4L)), g(m(5L), m(6L), m(7L), m(8L), m(9L))));}
public void beam_f31181_0() throws Exception
{        GatherBundleAndSortFn testFn = new GatherBundleAndSortFn(10000000, CELLS_PER_KEY, 3, null);    ProcessContext mockProcessContext = Mockito.mock(ProcessContext.class);    FinishBundleContext mockFinishBundleContext = Mockito.mock(FinishBundleContext.class);    when(mockProcessContext.sideInput(any())).thenReturn(getSchema());        doNothing().when(mockProcessContext).output(byteArrayKvListCaptor.capture());        doNothing().when(mockFinishBundleContext).output(byteArrayKvListCaptor.capture(), any(), any());    MutationGroup[] mutationGroups = new MutationGroup[] { g(m(4L)), g(m(1L)),     g(m(5L), m(6L), m(7L), m(8L), m(9L)),     g(m(10L)), g(m(3L)), g(m(11L)),     g(m(2L)) };        testFn.startBundle();    for (MutationGroup m : mutationGroups) {        when(mockProcessContext.element()).thenReturn(m);        testFn.processElement(mockProcessContext);    }    testFn.finishBundle(mockFinishBundleContext);    verify(mockProcessContext, times(3)).output(any());    verify(mockFinishBundleContext, times(1)).output(any(), any(), any());        List<List<KV<byte[], byte[]>>> kvGroups = byteArrayKvListCaptor.getAllValues();    assertEquals(4, kvGroups.size());        List<List<MutationGroup>> mgListGroups = kvGroups.stream().map(l -> l.stream().map(kv -> WriteGrouped.decode(kv.getValue())).collect(Collectors.toList())).collect(Collectors.toList());        assertThat(mgListGroups.get(0), contains(g(m(1L)), g(m(4L))));    assertThat(mgListGroups.get(1), contains(g(m(5L), m(6L), m(7L), m(8L), m(9L))));    assertThat(mgListGroups.get(2), contains(g(m(3L)), g(m(10L)), g(m(11L))));    assertThat(mgListGroups.get(3), contains(g(m(2L))));}
public void beam_f31182_0() throws Exception
{        BatchFn testFn = new BatchFn(10000000, 3 * CELLS_PER_KEY, null);    ProcessContext mockProcessContext = Mockito.mock(ProcessContext.class);    when(mockProcessContext.sideInput(any())).thenReturn(getSchema());        doNothing().when(mockProcessContext).output(mutationGroupListCaptor.capture());    List<MutationGroup> mutationGroups = Arrays.asList(g(m(1L)), g(m(4L)), g(m(5L), m(6L), m(7L), m(8L), m(9L)), g(m(3L)), g(m(10L)), g(m(11L)), g(m(2L)));    List<KV<byte[], byte[]>> encodedInput = mutationGroups.stream().map(mg -> KV.of((byte[]) null, WriteGrouped.encode(mg))).collect(Collectors.toList());        when(mockProcessContext.element()).thenReturn(encodedInput);    testFn.processElement(mockProcessContext);    verify(mockProcessContext, times(4)).output(any());    List<Iterable<MutationGroup>> batches = mutationGroupListCaptor.getAllValues();    assertEquals(4, batches.size());        assertThat(batches.get(0), contains(g(m(1L)), g(m(4L))));    assertThat(batches.get(1), contains(g(m(5L), m(6L), m(7L), m(8L), m(9L))));    assertThat(batches.get(2), contains(g(m(3L)), g(m(10L)), g(m(11L))));    assertThat(batches.get(3), contains(g(m(2L))));}
public boolean beam_f31190_0(Iterable<Mutation> argument)
{    if (!(argument instanceof Iterable)) {        return false;    }    ImmutableSet<Mutation> actual = ImmutableSet.copyOf((Iterable) argument);    return actual.equals(mutations);}
public String beam_f31191_0()
{    return "Iterable must match " + mutations;}
private Iterable<Mutation> beam_f31192_0(final int size)
{    return argThat(new ArgumentMatcher<Iterable<Mutation>>() {        @Override        public boolean matches(Iterable<Mutation> argument) {            return argument instanceof Iterable && Iterables.size((Iterable<?>) argument) == size;        }        @Override        public String toString() {            return "The size of the iterable must equal " + size;        }    });}
private SpannerConfig beam_f31200_0()
{    return SpannerConfig.create().withProjectId(project).withInstanceId(options.getInstanceId()).withDatabaseId(databaseName);}
private DatabaseClient beam_f31201_0()
{    return spanner.getDatabaseClient(DatabaseId.of(project, options.getInstanceId(), databaseName));}
public void beam_f31202_0() throws Exception
{    databaseAdminClient.dropDatabase(options.getInstanceId(), databaseName);    spanner.close();}
public void beam_f31210_0() throws Exception
{    int numRecords = 100;    p.apply(GenerateSequence.from(0).to(2 * numRecords)).apply(ParDo.of(new GenerateMutations(options.getTable(), new DivBy2()))).apply(SpannerIO.write().withProjectId(project).withInstanceId(options.getInstanceId()).withDatabaseId(databaseName).withFailureMode(SpannerIO.FailureMode.REPORT_FAILURES));    PipelineResult result = p.run();    result.waitUntilFinish();    assertThat(result.getState(), is(PipelineResult.State.DONE));    assertThat(countNumberOfRecords(), equalTo((long) numRecords));}
public void beam_f31211_0() throws Exception
{    thrown.expect(new StackTraceContainsString("SpannerException"));    thrown.expect(new StackTraceContainsString("Value must not be NULL in table users"));    int numRecords = 100;    p.apply(GenerateSequence.from(0).to(2 * numRecords)).apply(ParDo.of(new GenerateMutations(options.getTable(), new DivBy2()))).apply(SpannerIO.write().withProjectId(project).withInstanceId(options.getInstanceId()).withDatabaseId(databaseName));    PipelineResult result = p.run();    result.waitUntilFinish();}
public void beam_f31212_0() throws Exception
{    databaseAdminClient.dropDatabase(options.getInstanceId(), databaseName);    spanner.close();}
public void beam_f31220_0() throws Exception
{    when(mockQuery.execute()).thenThrow(new IOException());    thrown.expect(RuntimeException.class);    thrown.expectMessage("Unable to get BigQuery response after retrying");    try {        bqClient.queryWithRetries(query, projectId);    } finally {        verify(mockJobs, atLeast(BigqueryClient.MAX_QUERY_RETRIES)).query(eq(projectId), any(QueryRequest.class));    }}
public void beam_f31221_0() throws Exception
{    when(mockQuery.execute()).thenReturn(null);    thrown.expect(RuntimeException.class);    thrown.expectMessage("Unable to get BigQuery response after retrying");    try {        bqClient.queryWithRetries(query, projectId);    } finally {        verify(mockJobs, atLeast(BigqueryClient.MAX_QUERY_RETRIES)).query(eq(projectId), any(QueryRequest.class));    }}
public void beam_f31222_0()
{    MockitoAnnotations.initMocks(this);    PowerMockito.mockStatic(BigqueryClient.class);    when(BigqueryClient.getClient(anyString())).thenReturn(mockBigqueryClient);}
public static Job beam_f31230_0(@Nullable SerializableConfiguration conf) throws IOException
{    if (conf == null) {        return Job.getInstance();    } else {                Job job = Job.getInstance(new Configuration(false));        for (Map.Entry<String, String> entry : conf.get()) {            job.getConfiguration().set(entry.getKey(), entry.getValue());        }        return job;    }}
public static Configuration beam_f31231_0(@Nullable SerializableConfiguration conf)
{    if (conf == null) {        return new Configuration();    } else {        return conf.get();    }}
public static WritableCoder<T> beam_f31232_0(Class<T> clazz)
{    return new WritableCoder<>(clazz);}
public List<CoderProvider> beam_f31240_0()
{    return Collections.singletonList(getCoderProvider());}
public Coder<T> beam_f31241_0(TypeDescriptor<T> typeDescriptor, List<? extends Coder<?>> componentCoders) throws CannotProvideCoderException
{    if (!typeDescriptor.isSubtypeOf(WRITABLE_TYPE)) {        throw new CannotProvideCoderException(String.format("Cannot provide %s because %s does not implement the interface %s", WritableCoder.class.getSimpleName(), typeDescriptor, Writable.class.getName()));    }    try {        @SuppressWarnings("unchecked")        Coder<T> coder = WritableCoder.of((Class) typeDescriptor.getRawType());        return coder;    } catch (IllegalArgumentException e) {        throw new CannotProvideCoderException(e);    }}
public void beam_f31242_0()
{    Configuration conf = new Configuration();    conf.set("hadoop.silly.test", "test-value");    byte[] object = SerializationUtils.serialize(new SerializableConfiguration(conf));    SerializableConfiguration serConf = SerializationUtils.deserialize(object);    assertNotNull(serConf);    assertEquals("test-value", serConf.get().get("hadoop.silly.test"));}
private Set<Metadata> beam_f31250_0(String directorySpec, String fileSpec) throws IOException
{    final org.apache.hadoop.fs.FileSystem fs = new Path(directorySpec).getFileSystem(configuration);    Set<Metadata> metadata = new HashSet<>();    if (directorySpec.contains("*")) {                FileStatus[] directoryStatuses = fs.globStatus(new Path(directorySpec));        for (FileStatus directoryStatus : directoryStatuses) {            if (directoryStatus.isDirectory()) {                metadata.addAll(matchRecursiveGlob(directoryStatus.getPath().toUri().toString(), fileSpec));            }        }    } else {                FileStatus[] fileStatuses = fs.globStatus(new Path(directorySpec + "/" + fileSpec));        for (FileStatus fileStatus : fileStatuses) {            if (fileStatus.isFile()) {                metadata.add(toMetadata(fileStatus));            }        }                FileStatus[] directoryStatuses = fs.globStatus(new Path(directorySpec + "/*"));        for (FileStatus directoryStatus : directoryStatuses) {            if (directoryStatus.isDirectory()) {                metadata.addAll(matchRecursiveGlob(directoryStatus.getPath().toUri().toString(), fileSpec));            }        }                if (fileSpec.contains("**")) {            int index = fileSpec.indexOf("**");            metadata.addAll(matchRecursiveGlob(directorySpec + "/" + fileSpec.substring(0, index + 1), fileSpec.substring(index + 1)));        }    }    return metadata;}
private Metadata beam_f31251_0(FileStatus fileStatus)
{    URI uri = dropEmptyAuthority(fileStatus.getPath().toUri().toString());    return Metadata.builder().setResourceId(new HadoopResourceId(uri)).setIsReadSeekEfficient(true).setSizeBytes(fileStatus.getLen()).setLastModifiedMillis(fileStatus.getModificationTime()).build();}
protected WritableByteChannel beam_f31252_0(HadoopResourceId resourceId, CreateOptions createOptions) throws IOException
{    return Channels.newChannel(resourceId.toPath().getFileSystem(configuration).create(resourceId.toPath()));}
public int beam_f31260_0(ByteBuffer dst) throws IOException
{    if (closed) {        throw new IOException("Channel is closed");    }        int read = 0;        if (dst.hasArray()) {                                read = inputStream.read(dst.array(), dst.position() + dst.arrayOffset(), dst.remaining());    } else {                        read = inputStream.read(dst);    }    if (read > 0) {        dst.position(dst.position() + read);    }    return read;}
public int beam_f31261_0(ByteBuffer src)
{    throw new UnsupportedOperationException();}
public long beam_f31262_0() throws IOException
{    if (closed) {        throw new IOException("Channel is closed");    }    return inputStream.getPos();}
public void beam_f31270_0(Configuration configuration, JsonGenerator jsonGenerator, SerializerProvider serializerProvider) throws IOException
{    Map<String, String> map = new TreeMap<>();    for (Map.Entry<String, String> entry : configuration) {        map.put(entry.getKey(), entry.getValue());    }    jsonGenerator.writeObject(map);}
public List<Configuration> beam_f31271_0(PipelineOptions options)
{        List<Configuration> configurationList = readConfigurationFromHadoopYarnConfigDirs();    return configurationList.size() > 0 ? configurationList : null;}
private List<Configuration> beam_f31272_1()
{    List<Configuration> configurationList = Lists.newArrayList();    /*       * If we find a configuration in HADOOP_CONF_DIR and YARN_CONF_DIR,       * we should be returning them both separately.       *       * Also, ensure that we only load one configuration if both       * HADOOP_CONF_DIR and YARN_CONF_DIR point to the same location.       */    Set<String> confDirs = Sets.newHashSet();    for (String confDir : Lists.newArrayList("HADOOP_CONF_DIR", "YARN_CONF_DIR")) {        if (getEnvironment().containsKey(confDir)) {            String hadoopConfDir = getEnvironment().get(confDir);            if (!Strings.isNullOrEmpty(hadoopConfDir)) {                confDirs.add(hadoopConfDir);            }        }    }    for (String confDir : confDirs) {        if (new File(confDir).exists()) {            Configuration conf = new Configuration(false);            boolean confLoaded = false;            for (String confName : Lists.newArrayList("core-site.xml", "hdfs-site.xml")) {                File confFile = new File(confDir, confName);                if (confFile.exists()) {                                        conf.addResource(new Path(confFile.getAbsolutePath()));                    confLoaded = true;                }            }            if (confLoaded) {                configurationList.add(conf);            }        }    }    return configurationList;}
public String beam_f31280_0()
{    return uri.getScheme();}
public String beam_f31281_0()
{    return uri.toString();}
public boolean beam_f31282_0(Object obj)
{    if (!(obj instanceof HadoopResourceId)) {        return false;    }    return Objects.equals(uri, ((HadoopResourceId) obj).uri);}
public void beam_f31290_0() throws IOException
{    Files.write(createPropertyData("A"), tmpFolder.newFile("core-site.xml"), StandardCharsets.UTF_8);    Files.write(createPropertyData("B"), tmpFolder.newFile("hdfs-site.xml"), StandardCharsets.UTF_8);    HadoopFileSystemOptions.ConfigurationLocator configurationLocator = spy(new HadoopFileSystemOptions.ConfigurationLocator());    Map<String, String> environment = Maps.newHashMap();    environment.put("HADOOP_CONF_DIR", tmpFolder.getRoot().getAbsolutePath());    when(configurationLocator.getEnvironment()).thenReturn(environment);    List<Configuration> configurationList = configurationLocator.create(PipelineOptionsFactory.create());    assertEquals(1, configurationList.size());    assertThat(configurationList.get(0).get("propertyA"), Matchers.equalTo("A"));    assertThat(configurationList.get(0).get("propertyB"), Matchers.equalTo("B"));}
public void beam_f31291_0() throws IOException
{    Files.write(createPropertyData("A"), tmpFolder.newFile("core-site.xml"), StandardCharsets.UTF_8);    Files.write(createPropertyData("B"), tmpFolder.newFile("hdfs-site.xml"), StandardCharsets.UTF_8);    HadoopFileSystemOptions.ConfigurationLocator configurationLocator = spy(new HadoopFileSystemOptions.ConfigurationLocator());    Map<String, String> environment = Maps.newHashMap();    environment.put("YARN_CONF_DIR", tmpFolder.getRoot().getAbsolutePath());    when(configurationLocator.getEnvironment()).thenReturn(environment);    List<Configuration> configurationList = configurationLocator.create(PipelineOptionsFactory.create());    assertEquals(1, configurationList.size());    assertThat(configurationList.get(0).get("propertyA"), Matchers.equalTo("A"));    assertThat(configurationList.get(0).get("propertyB"), Matchers.equalTo("B"));}
public void beam_f31292_0() throws IOException
{    Files.write(createPropertyData("A"), tmpFolder.newFile("core-site.xml"), StandardCharsets.UTF_8);    Files.write(createPropertyData("B"), tmpFolder.newFile("hdfs-site.xml"), StandardCharsets.UTF_8);    HadoopFileSystemOptions.ConfigurationLocator configurationLocator = spy(new HadoopFileSystemOptions.ConfigurationLocator());    Map<String, String> environment = Maps.newHashMap();    environment.put("YARN_CONF_DIR", tmpFolder.getRoot().getAbsolutePath());    environment.put("HADOOP_CONF_DIR", tmpFolder.getRoot().getAbsolutePath());    when(configurationLocator.getEnvironment()).thenReturn(environment);    List<Configuration> configurationList = configurationLocator.create(PipelineOptionsFactory.create());    assertEquals(1, configurationList.size());    assertThat(configurationList.get(0).get("propertyA"), Matchers.equalTo("A"));    assertThat(configurationList.get(0).get("propertyB"), Matchers.equalTo("B"));}
public void beam_f31300_0() throws Exception
{    byte[] bytes = "testData".getBytes(StandardCharsets.UTF_8);    create("testFile", bytes);    assertArrayEquals(bytes, read("testFile", 0));}
public void beam_f31301_0() throws Exception
{    byte[] bytes = "testData".getBytes(StandardCharsets.UTF_8);    create("testFile", bytes);    int bytesToSkip = 3;    byte[] expected = Arrays.copyOfRange(bytes, bytesToSkip, bytes.length);    byte[] actual = read("testFile", bytesToSkip);    assertArrayEquals(expected, actual);}
public void beam_f31302_0() throws Exception
{    byte[] bytes = "testData".getBytes(StandardCharsets.UTF_8);    create("testFile", bytes);    int bytesToSkip = bytes.length;    byte[] expected = Arrays.copyOfRange(bytes, bytesToSkip, bytes.length);    assertArrayEquals(expected, read("testFile", bytesToSkip));}
public void beam_f31310_0() throws Exception
{    create("1/testFile1", "testData1".getBytes(StandardCharsets.UTF_8));    create("1/A/testFile1A", "testData1A".getBytes(StandardCharsets.UTF_8));    create("1/A/A/testFile1AA", "testData1AA".getBytes(StandardCharsets.UTF_8));    create("1/B/testFile1B", "testData1B".getBytes(StandardCharsets.UTF_8));        assertArrayEquals("testData1".getBytes(StandardCharsets.UTF_8), read("1/testFile1", 0));    assertArrayEquals("testData1A".getBytes(StandardCharsets.UTF_8), read("1/A/testFile1A", 0));    assertArrayEquals("testData1AA".getBytes(StandardCharsets.UTF_8), read("1/A/A/testFile1AA", 0));    assertArrayEquals("testData1B".getBytes(StandardCharsets.UTF_8), read("1/B/testFile1B", 0));    List<MatchResult> matchResults = fileSystem.match(ImmutableList.of(testPath("**testFile1*").toString()));    assertThat(matchResults, hasSize(1));    assertThat(Iterables.getOnlyElement(matchResults).metadata(), containsInAnyOrder(Metadata.builder().setResourceId(testPath("1/testFile1")).setIsReadSeekEfficient(true).setSizeBytes("testData1".getBytes(StandardCharsets.UTF_8).length).setLastModifiedMillis(lastModified("1/testFile1")).build(), Metadata.builder().setResourceId(testPath("1/A/testFile1A")).setIsReadSeekEfficient(true).setSizeBytes("testData1A".getBytes(StandardCharsets.UTF_8).length).setLastModifiedMillis(lastModified("1/A/testFile1A")).build(), Metadata.builder().setResourceId(testPath("1/A/A/testFile1AA")).setIsReadSeekEfficient(true).setSizeBytes("testData1AA".getBytes(StandardCharsets.UTF_8).length).setLastModifiedMillis(lastModified("1/A/A/testFile1AA")).build(), Metadata.builder().setResourceId(testPath("1/B/testFile1B")).setIsReadSeekEfficient(true).setSizeBytes("testData1B".getBytes(StandardCharsets.UTF_8).length).setLastModifiedMillis(lastModified("1/B/testFile1B")).build()));    matchResults = fileSystem.match(ImmutableList.of(testPath("1**File1A").toString(), testPath("1**A**testFile1AA").toString(), testPath("1/B**").toString(), testPath("2**").toString()));    final List<MatchResult> expected = ImmutableList.of(MatchResult.create(Status.OK, ImmutableList.of(Metadata.builder().setResourceId(testPath("1/A/testFile1A")).setIsReadSeekEfficient(true).setSizeBytes("testData1A".getBytes(StandardCharsets.UTF_8).length).setLastModifiedMillis(lastModified("1/A/testFile1A")).build())), MatchResult.create(Status.OK, ImmutableList.of(Metadata.builder().setResourceId(testPath("1/A/A/testFile1AA")).setIsReadSeekEfficient(true).setSizeBytes("testData1AA".getBytes(StandardCharsets.UTF_8).length).setLastModifiedMillis(lastModified("1/A/A/testFile1AA")).build())), MatchResult.create(Status.OK, ImmutableList.of(Metadata.builder().setResourceId(testPath("1/B/testFile1B")).setIsReadSeekEfficient(true).setSizeBytes("testData1B".getBytes(StandardCharsets.UTF_8).length).setLastModifiedMillis(lastModified("1/B/testFile1B")).build())), MatchResult.create(Status.NOT_FOUND, ImmutableList.of()));    assertThat(matchResults, hasSize(4));    assertThat(matchResults, equalTo(expected));}
public void beam_f31311_0() throws Exception
{    create("testFileA", "testDataA".getBytes(StandardCharsets.UTF_8));    create("testFileB", "testDataB".getBytes(StandardCharsets.UTF_8));        assertArrayEquals("testDataA".getBytes(StandardCharsets.UTF_8), read("testFileA", 0));    assertArrayEquals("testDataB".getBytes(StandardCharsets.UTF_8), read("testFileB", 0));    fileSystem.rename(ImmutableList.of(testPath("testFileA"), testPath("testFileB")), ImmutableList.of(testPath("renameFileA"), testPath("renameFileB")));    List<MatchResult> results = fileSystem.match(ImmutableList.of(testPath("*").toString()));    assertEquals(Status.OK, Iterables.getOnlyElement(results).status());    assertThat(Iterables.getOnlyElement(results).metadata(), containsInAnyOrder(Metadata.builder().setResourceId(testPath("renameFileA")).setIsReadSeekEfficient(true).setSizeBytes("testDataA".getBytes(StandardCharsets.UTF_8).length).setLastModifiedMillis(lastModified("renameFileA")).build(), Metadata.builder().setResourceId(testPath("renameFileB")).setIsReadSeekEfficient(true).setSizeBytes("testDataB".getBytes(StandardCharsets.UTF_8).length).setLastModifiedMillis(lastModified("renameFileB")).build()));        assertArrayEquals("testDataA".getBytes(StandardCharsets.UTF_8), read("renameFileA", 0));    assertArrayEquals("testDataB".getBytes(StandardCharsets.UTF_8), read("renameFileB", 0));}
public void beam_f31312_0() throws Exception
{    create("pathA/testFileA", "testDataA".getBytes(StandardCharsets.UTF_8));    create("pathA/testFileB", "testDataB".getBytes(StandardCharsets.UTF_8));        assertArrayEquals("testDataA".getBytes(StandardCharsets.UTF_8), read("pathA/testFileA", 0));    assertArrayEquals("testDataB".getBytes(StandardCharsets.UTF_8), read("pathA/testFileB", 0));        fileSystem.rename(ImmutableList.of(testPath("pathA/testFileA"), testPath("pathA/testFileB")), ImmutableList.of(testPath("pathB/testFileA"), testPath("pathB/pathC/pathD/testFileB")));        expectedLogs.verifyDebug(String.format(HadoopFileSystem.LOG_CREATE_DIRECTORY, "/pathB"));    expectedLogs.verifyDebug(String.format(HadoopFileSystem.LOG_CREATE_DIRECTORY, "/pathB/pathC/pathD"));    assertArrayEquals("testDataA".getBytes(StandardCharsets.UTF_8), read("pathB/testFileA", 0));    assertArrayEquals("testDataB".getBytes(StandardCharsets.UTF_8), read("pathB/pathC/pathD/testFileB", 0));}
private long beam_f31320_0(String relativePath) throws Exception
{    final Path testPath = testPath(relativePath).toPath();    return testPath.getFileSystem(fileSystem.configuration).getFileStatus(testPath(relativePath).toPath()).getModificationTime();}
private HadoopResourceId beam_f31321_0(String relativePath)
{    return new HadoopResourceId(hdfsClusterBaseUri.resolve(relativePath));}
public void beam_f31322_0() throws Exception
{    Configuration configuration = new Configuration();    configuration.set(MiniDFSCluster.HDFS_MINIDFS_BASEDIR, tmpFolder.getRoot().getAbsolutePath());    MiniDFSCluster.Builder builder = new MiniDFSCluster.Builder(configuration);    hdfsCluster = builder.build();    hdfsClusterBaseUri = new URI(configuration.get("fs.defaultFS") + "/");        HadoopFileSystemOptions options = PipelineOptionsFactory.as(HadoopFileSystemOptions.class);    options.setHdfsConfiguration(Collections.singletonList(configuration));    FileSystems.setDefaultPipelineOptions(options);}
public PCollection<KV<K, V>> beam_f31330_0(PBegin input)
{    validateTransform();        CoderRegistry coderRegistry = input.getPipeline().getCoderRegistry();    Coder<K> keyCoder = getDefaultCoder(getKeyTypeDescriptor(), coderRegistry);    Coder<V> valueCoder = getDefaultCoder(getValueTypeDescriptor(), coderRegistry);    HadoopInputFormatBoundedSource<K, V> source = new HadoopInputFormatBoundedSource<>(getConfiguration(), keyCoder, valueCoder, getKeyTranslationFunction(), getValueTranslationFunction());    return input.getPipeline().apply(org.apache.beam.sdk.io.Read.from(source));}
private void beam_f31331_0(Configuration configuration)
{    checkArgument(configuration != null, "configuration can not be null");    checkArgument(configuration.get("mapreduce.job.inputformat.class") != null, "Configuration must contain \"mapreduce.job.inputformat.class\"");    checkArgument(configuration.get("key.class") != null, "configuration must contain \"key.class\"");    checkArgument(configuration.get("value.class") != null, "configuration must contain \"value.class\"");    if (configuration.get("mapreduce.job.inputformat.class").endsWith("DBInputFormat")) {        checkArgument(configuration.get(DBConfiguration.INPUT_ORDER_BY_PROPERTY) != null, "Configuration must contain \"" + DBConfiguration.INPUT_ORDER_BY_PROPERTY + "\" when using DBInputFormat");    }}
public void beam_f31332_0()
{    checkArgument(getConfiguration() != null, "withConfiguration() is required");        validateTranslationFunction(getinputFormatKeyClass(), getKeyTranslationFunction(), "Key translation's input type is not same as hadoop InputFormat : %s key class : %s");        validateTranslationFunction(getinputFormatValueClass(), getValueTranslationFunction(), "Value translation's input type is not same as hadoop InputFormat :  " + "%s value class : %s");}
 void beam_f31340_0() throws IOException, InterruptedException
{    if (inputSplits != null) {        return;    }    createInputFormatInstance();    List<InputSplit> splits = inputFormatObj.getSplits(Job.getInstance(conf.get()));    if (splits == null) {        throw new IOException("Error in computing splits, getSplits() returns null.");    }    if (splits.isEmpty()) {        throw new IOException("Error in computing splits, getSplits() returns a empty list");    }    boundedSourceEstimatedSize = 0;    inputSplits = new ArrayList<>();    for (InputSplit inputSplit : splits) {        if (inputSplit == null) {            throw new IOException("Error in computing splits, split is null in InputSplits list " + "populated by getSplits() : ");        }        boundedSourceEstimatedSize += inputSplit.getLength();        inputSplits.add(new SerializableSplit(inputSplit));    }}
protected void beam_f31341_0() throws IOException
{    if (inputFormatObj == null) {        try {            taskAttemptContext = new TaskAttemptContextImpl(conf.get(), new TaskAttemptID());            inputFormatObj = (InputFormat<?, ?>) conf.get().getClassByName(conf.get().get("mapreduce.job.inputformat.class")).getConstructor().newInstance();            /*           * If InputFormat explicitly implements interface {@link Configurable}, then setConf()           * method of {@link Configurable} needs to be explicitly called to set all the           * configuration parameters. For example: InputFormat classes which implement Configurable           * are {@link org.apache.hadoop.mapreduce.lib.db.DBInputFormat DBInputFormat}, {@link           * org.apache.hadoop.hbase.mapreduce.TableInputFormat TableInputFormat}, etc.           */            if (Configurable.class.isAssignableFrom(inputFormatObj.getClass())) {                ((Configurable) inputFormatObj).setConf(conf.get());            }        } catch (InstantiationException | IllegalAccessException | ClassNotFoundException | NoSuchMethodException | InvocationTargetException e) {            throw new IOException("Unable to create InputFormat object: ", e);        }    }}
 InputFormat<?, ?> beam_f31342_0()
{    return inputFormatObj;}
private T3 beam_f31350_0(T input, @Nullable SimpleFunction<T, T3> simpleFunction, Coder<T3> coder) throws CoderException, ClassCastException
{    T3 output;    if (null != simpleFunction) {        output = simpleFunction.apply(input);    } else {        output = (T3) input;    }    return cloneIfPossiblyMutable(output, coder);}
private T beam_f31351_0(T input, Coder<T> coder) throws CoderException, ClassCastException
{        if (!isKnownImmutable(input)) {        input = CoderUtils.clone(coder, input);    }    return input;}
private boolean beam_f31352_0(Object o)
{    return immutableTypes.contains(o.getClass());}
public PCollection<KV<Integer, KV<KeyT, ValueT>>> beam_f31360_0(PCollection<KV<KeyT, ValueT>> input)
{    return input.apply("AssignTask", ParDo.of(new AssignTaskFn<KeyT, ValueT>(configView)).withSideInputs(configView)).setTypeDescriptor(TypeDescriptors.kvs(TypeDescriptors.integers(), input.getTypeDescriptor())).apply("GroupByTaskId", GroupByKey.create()).apply("FlattenGroupedTasks", ParDo.of(new FlattenGroupedTasks<>()));}
public void beam_f31361_0(@Element KV<Integer, Iterable<KV<KeyT, ValueT>>> input, OutputReceiver<KV<Integer, KV<KeyT, ValueT>>> outputReceiver)
{    final Integer key = input.getKey();    for (KV<KeyT, ValueT> element : requireNonNull(input.getValue(), "Iterable can not be null.")) {        outputReceiver.output(KV.of(key, element));    }}
public PartitionedWriterBuilder<KeyT, ValueT> beam_f31362_0(Configuration config)
{    checkNotNull(config, "Hadoop configuration cannot be null");    this.configuration = config;    return this;}
private void beam_f31371_0(PCollection<KV<KeyT, ValueT>> input)
{    if (input.isBounded().equals(PCollection.IsBounded.UNBOUNDED)) {        checkArgument(!input.getWindowingStrategy().equals(WindowingStrategy.globalDefault()), "Cannot work with %s and GLOBAL %s", PCollection.IsBounded.UNBOUNDED, WindowingStrategy.class.getSimpleName());        checkArgument(input.getWindowingStrategy().getTrigger().getClass().equals(DefaultTrigger.class), "Cannot work with %s trigger. Write works correctly only with %s", input.getWindowingStrategy().getTrigger().getClass().getSimpleName(), DefaultTrigger.class.getSimpleName());        checkArgument(input.getWindowingStrategy().getAllowedLateness().equals(Duration.ZERO), "Write does not allow late data.");    }}
private PCollectionView<Configuration> beam_f31372_0(PCollection<KV<KeyT, ValueT>> input)
{    PCollectionView<Configuration> config;    if (configuration != null) {        config = input.getPipeline().apply("CreateOutputConfig", Create.<Configuration>of(configuration)).apply(View.<Configuration>asSingleton().withDefaultValue(configuration));    } else {        config = input.apply("TransformDataIntoConfig", configTransform);    }    return config;}
 RecordWriter<KeyT, ValueT> beam_f31373_0()
{    return recordWriter;}
public String beam_f31381_0()
{    return "TaskContext{" + "jobId=" + getJobId() + ", taskId=" + getTaskId() + ", attemptId=" + taskAttemptContext.getTaskAttemptID().getId() + '}';}
public void beam_f31382_0(Configuration value, OutputStream outStream) throws IOException
{    DataOutputStream dataOutputStream = new DataOutputStream(outStream);    value.write(dataOutputStream);    dataOutputStream.flush();}
public Configuration beam_f31383_0(InputStream inStream) throws IOException
{    DataInputStream dataInputStream = new DataInputStream(inStream);    Configuration config = new Configuration(false);    config.readFields(dataInputStream);    return config;}
public void beam_f31391_0()
{    partitionToTaskContext = new HashMap<>();    partitioner = null;    jobId = null;    reducersCount = null;}
public void beam_f31392_0(@Element KV<KeyT, ValueT> element, OutputReceiver<KV<Integer, KV<KeyT, ValueT>>> receiver, ProcessContext c)
{    Configuration config = c.sideInput(configView);    TaskID taskID = createTaskIDForKV(element, config);    int taskId = taskID.getId();    receiver.output(KV.of(taskId, element));}
private TaskID beam_f31393_0(KV<KeyT, ValueT> kv, Configuration config)
{    int taskContextKey = getPartitioner(config).getPartition(kv.getKey(), kv.getValue(), getReducersCount(config));    return partitionToTaskContext.computeIfAbsent(taskContextKey, (key) -> HadoopFormats.createTaskID(getJobId(config), key));}
private void beam_f31401_0(KV<KeyT, ValueT> kv, TaskContext<KeyT, ValueT> taskContext)
{    try {        RecordWriter<KeyT, ValueT> recordWriter = taskContext.getRecordWriter();        recordWriter.write(kv.getKey(), kv.getValue());    } catch (Exception e) {        processTaskException(taskContext, e);    }}
private TaskContext<KeyT, ValueT> beam_f31402_1(Integer taskId, ProcessContext c) throws IllegalStateException
{    final Configuration conf = c.sideInput(configView);    TaskAttemptID taskAttemptID = externalSynchronization.acquireTaskAttemptIdLock(conf, taskId);    TaskContext<KeyT, ValueT> taskContext = new TaskContext<>(taskAttemptID, conf);    try {        taskContext.getOutputCommitter().setupTask(taskContext.getTaskAttemptContext());    } catch (Exception e) {        processTaskException(taskContext, e);    }        return taskContext;}
public void beam_f31403_0()
{    taskId = null;}
 static TaskID beam_f31411_0(JobID jobID, int taskNumber)
{    return new TaskID(jobID, TaskType.REDUCE, taskNumber);}
 static TaskAttemptContext beam_f31412_0(Configuration conf, JobID jobID)
{    final TaskID taskId = new TaskID(jobID, TaskType.JOB_CLEANUP, 0);    return createTaskAttemptContext(conf, new TaskAttemptID(taskId, 0));}
 static OutputFormat<KeyT, ValueT> beam_f31413_0(Configuration conf) throws IllegalArgumentException
{    return (OutputFormat<KeyT, ValueT>) createInstanceFromConfig(conf, MRJobConfig.OUTPUT_FORMAT_CLASS_ATTR, null, OutputFormat.class);}
public TaskAttemptID beam_f31421_0(Configuration conf, int taskId)
{    String jobJtIdentifier = getJobJtIdentifier(conf);    JobID jobId = HadoopFormats.getJobId(conf);    int taskAttemptCandidate = 0;    boolean taskAttemptAcquired = false;    while (!taskAttemptAcquired) {        taskAttemptCandidate++;        Path path = new Path(locksDir, String.format(LOCKS_DIR_TASK_ATTEMPT_PATTERN, jobJtIdentifier, taskId, taskAttemptCandidate));        taskAttemptAcquired = tryCreateFile(conf, path);    }    return HadoopFormats.createTaskAttemptID(jobId, taskId, taskAttemptCandidate);}
private boolean beam_f31422_0(Configuration conf, Path path)
{    try (FileSystem fileSystem = fileSystemFactory.apply(conf)) {        try {            return fileSystem.createNewFile(path);        } catch (FileAlreadyExistsException | org.apache.hadoop.fs.FileAlreadyExistsException e) {            return false;        } catch (RemoteException e) {                        if (e.getClassName().equals(AlreadyBeingCreatedException.class.getName())) {                return false;            }            throw e;        }    } catch (IOException e) {        throw new IllegalStateException(String.format("Creation of file on path %s failed", path), e);    }}
private String beam_f31423_0(Configuration conf)
{    JobID job = Preconditions.checkNotNull(HadoopFormats.getJobId(conf), "Configuration must contain jobID under key %s.", HadoopFormatIO.JOB_ID);    return job.getJtIdentifier();}
public void beam_f31431_0(IterableCombinerFn.CollectionAccumulator<T> value, OutputStream outStream) throws IOException
{    listCoder.encode(value.collection, outStream);}
public IterableCombinerFn.CollectionAccumulator<T> beam_f31432_0(InputStream inStream) throws IOException
{    List<T> decodedList = listCoder.decode(inStream);    return new IterableCombinerFn.CollectionAccumulator<>(decodedList);}
public Configuration beam_f31433_0()
{    return null;}
public Employee beam_f31444_0()
{    return null;}
public float beam_f31445_0()
{    return 0;}
public String beam_f31447_0()
{    return empName;}
public List<InputSplit> beam_f31455_0(JobContext arg0)
{    List<InputSplit> inputSplitList = new ArrayList<>();    for (int i = 1; i <= TestEmployeeDataSet.NUMBER_OF_SPLITS; i++) {        InputSplit inputSplitObj = new NewObjectsEmployeeInputSplit((i - 1) * TestEmployeeDataSet.NUMBER_OF_RECORDS_IN_EACH_SPLIT, i * TestEmployeeDataSet.NUMBER_OF_RECORDS_IN_EACH_SPLIT - 1);        inputSplitList.add(inputSplitObj);    }    return inputSplitList;}
public long beam_f31456_0()
{    return this.endIndex - this.startIndex + 1;}
public String[] beam_f31457_0()
{    return null;}
public void beam_f31466_0(InputSplit split, TaskAttemptContext arg1)
{    this.split = (NewObjectsEmployeeInputSplit) split;    employeeListIndex = this.split.getStartIndex() - 1;    recordsRead = 0;    employeeDataList = TestEmployeeDataSet.populateEmployeeData();    currentValue = new Employee(null, null);}
public boolean beam_f31467_0()
{    if ((recordsRead++) >= split.getLength()) {        return false;    }    employeeListIndex++;    KV<String, String> employeeDetails = employeeDataList.get((int) employeeListIndex);    List<String> empData = Splitter.on('_').splitToList(employeeDetails.getValue());    /*       * New objects must be returned every time for key and value in order to test the scenario as       * discussed the in the class' javadoc.       */    currentKey = new Text(employeeDetails.getKey());    currentValue = new Employee(empData.get(0), empData.get(1));    return true;}
public RecordWriter<Text, Employee> beam_f31468_0(TaskAttemptContext context)
{    return new RecordWriter<Text, Employee>() {        @Override        public void write(Text key, Employee value) {            output.add(KV.of(key, value));        }        @Override        public void close(TaskAttemptContext context) {        }    };}
public String beam_f31478_0(Row input)
{    return input.getString("y_id") + "|" + input.getString("field0") + "|" + input.getString("field1") + "|" + input.getString("field2") + "|" + input.getString("field3") + "|" + input.getString("field4") + "|" + input.getString("field5") + "|" + input.getString("field6") + "|" + input.getString("field7") + "|" + input.getString("field8") + "|" + input.getString("field9");}
public void beam_f31479_0()
{    String expectedHashCode = "7bead6d6385c5f4dd0524720cd320b49";    Long expectedNumRows = 1L;    Configuration conf = getConfiguration(options);    conf.set("cassandra.input.cql", "select * from " + CASSANDRA_KEYSPACE + "." + CASSANDRA_TABLE + " where token(y_id) > ? and token(y_id) <= ? " + "and field0 = 'user48:field0:431531'");    PCollection<KV<Long, String>> cassandraData = pipeline.apply(HadoopFormatIO.<Long, String>read().withConfiguration(conf).withValueTranslation(myValueTranslate));    PAssert.thatSingleton(cassandraData.apply("Count", Count.globally())).isEqualTo(expectedNumRows);    PCollection<String> textValues = cassandraData.apply(Values.create());        PCollection<String> consolidatedHashcode = textValues.apply(Combine.globally(new HashingFn()).withoutDefaults());    PAssert.that(consolidatedHashcode).containsInAnyOrder(expectedHashCode);    pipeline.run().waitUntilFinish();}
private static Configuration beam_f31480_0(HadoopFormatIOTestOptions options)
{    Configuration conf = new Configuration();    conf.set(CASSANDRA_THRIFT_PORT_PROPERTY, options.getCassandraServerPort().toString());    conf.set(CASSANDRA_THRIFT_ADDRESS_PROPERTY, options.getCassandraServerIp());    conf.set(CASSANDRA_PARTITIONER_CLASS_PROPERTY, CASSANDRA_PARTITIONER_CLASS_VALUE);    conf.set(CASSANDRA_KEYSPACE_PROPERTY, CASSANDRA_KEYSPACE);    conf.set(CASSANDRA_COLUMNFAMILY_PROPERTY, CASSANDRA_TABLE);        conf.set(USERNAME, options.getCassandraUserName());    conf.set(PASSWORD, options.getCassandraPassword());    conf.set(INPUT_KEYSPACE_USERNAME_CONFIG, options.getCassandraUserName());    conf.set(INPUT_KEYSPACE_PASSWD_CONFIG, options.getCassandraPassword());    conf.setClass("mapreduce.job.inputformat.class", org.apache.cassandra.hadoop.cql3.CqlInputFormat.class, InputFormat.class);    conf.setClass("key.class", Long.class, Object.class);    conf.setClass("value.class", Row.class, Object.class);    return conf;}
public static void beam_f31488_0()
{    session.close();    cluster.close();}
public String beam_f31489_0()
{    return name;}
public void beam_f31490_0(String name)
{    this.name = name;}
private String beam_f31498_0(String row, MapWritable mapw, String columnName)
{    Object valueObj = mapw.get(new Text(columnName));    row += valueObj.toString() + "|";    return row;}
public void beam_f31499_0()
{    String expectedHashCode = "d7a7e4e42c2ca7b83ef7c1ad1ebce000";    Long expectedRecordsCount = 1L;    Configuration conf = getConfiguration(options);    String query = "{" + "  \"query\": {" + "  \"match\" : {" + "    \"Title\" : {" + "      \"query\" : \"Title9\"," + "      \"type\" : \"boolean\"" + "    }" + "  }" + "  }" + "}";    conf.set(ConfigurationOptions.ES_QUERY, query);    PCollection<KV<Text, LinkedMapWritable>> esData = pipeline.apply(HadoopFormatIO.<Text, LinkedMapWritable>read().withConfiguration(conf));    PCollection<Long> count = esData.apply(Count.globally());        PAssert.thatSingleton(count).isEqualTo(expectedRecordsCount);    PCollection<LinkedMapWritable> values = esData.apply(Values.create());    PCollection<String> textValues = values.apply(transformFunc);        PCollection<String> consolidatedHashcode = textValues.apply(Combine.globally(new HashingFn()).withoutDefaults());    PAssert.that(consolidatedHashcode).containsInAnyOrder(expectedHashCode);    pipeline.run().waitUntilFinish();}
private static Configuration beam_f31500_0(HadoopFormatIOTestOptions options)
{    Configuration conf = new Configuration();    conf.set(ConfigurationOptions.ES_NODES, options.getElasticServerIp());    conf.set(ConfigurationOptions.ES_PORT, options.getElasticServerPort().toString());    conf.set(ConfigurationOptions.ES_NODES_WAN_ONLY, TRUE);        conf.set(ConfigurationOptions.ES_NET_HTTP_AUTH_USER, options.getElasticUserName());    conf.set(ConfigurationOptions.ES_NET_HTTP_AUTH_PASS, options.getElasticPassword());    conf.set(ConfigurationOptions.ES_RESOURCE, ELASTIC_RESOURCE);    conf.set("es.internal.es.version", ELASTIC_INTERNAL_VERSION);    conf.set(ConfigurationOptions.ES_INDEX_AUTO_CREATE, TRUE);    conf.setClass("mapreduce.job.inputformat.class", org.elasticsearch.hadoop.mr.EsInputFormat.class, InputFormat.class);    conf.setClass("key.class", Text.class, Object.class);    conf.setClass("value.class", LinkedMapWritable.class, Object.class);            conf.set("es.input.max.docs.per.partition", "50000");    conf.set("es.scroll.size", "400");    conf.set("es.batch.size.bytes", "8mb");    return conf;}
 static void beam_f31508_1() throws NodeValidationException
{    Settings settings = Settings.builder().put("node.data", TRUE).put("network.host", ELASTIC_IN_MEM_HOSTNAME).put("http.port", port).put("path.data", elasticTempFolder.getRoot().getPath()).put("path.home", elasticTempFolder.getRoot().getPath()).put("transport.type", "local").put("http.enabled", TRUE).put("node.ingest", TRUE).build();    node = new PluginNode(settings);    node.start();        prepareElasticIndex();    }
private static void beam_f31509_0()
{    CreateIndexRequest indexRequest = new CreateIndexRequest(ELASTIC_INDEX_NAME);    node.client().admin().indices().create(indexRequest).actionGet();    for (int i = 0; i < TEST_DATA_ROW_COUNT; i++) {        node.client().prepareIndex(ELASTIC_INDEX_NAME, ELASTIC_TYPE_NAME, String.valueOf(i)).setSource(createElasticRow(ELASTIC_TYPE_ID_PREFIX + i, "Faraday" + i)).execute().actionGet();    }    node.client().admin().indices().prepareRefresh(ELASTIC_INDEX_NAME).get();}
 static void beam_f31510_1() throws IOException
{    DeleteIndexRequest indexRequest = new DeleteIndexRequest(ELASTIC_INDEX_NAME);    node.client().admin().indices().delete(indexRequest).actionGet();        node.close();        deleteElasticDataDirectory();}
private void beam_f31518_0(PipelineResult writeResult, PipelineResult readResult)
{    String uuid = UUID.randomUUID().toString();    String timestamp = Timestamp.now().toString();    Set<Function<MetricsReader, NamedTestResult>> readSuppliers = getReadSuppliers(uuid, timestamp);    Set<Function<MetricsReader, NamedTestResult>> writeSuppliers = getWriteSuppliers(uuid, timestamp);    IOITMetrics readMetrics = new IOITMetrics(readSuppliers, readResult, NAMESPACE, uuid, timestamp);    IOITMetrics writeMetrics = new IOITMetrics(writeSuppliers, writeResult, NAMESPACE, uuid, timestamp);    readMetrics.publish(bigQueryDataset, bigQueryTable);    writeMetrics.publish(bigQueryDataset, bigQueryTable);}
private Set<Function<MetricsReader, NamedTestResult>> beam_f31519_0(String uuid, String timestamp)
{    Set<Function<MetricsReader, NamedTestResult>> suppliers = new HashSet<>();    suppliers.add(reader -> {        long writeStart = reader.getStartTimeMetric("write_time");        long writeEnd = reader.getEndTimeMetric("write_time");        return NamedTestResult.create(uuid, timestamp, "write_time", (writeEnd - writeStart) / 1e3);    });    return suppliers;}
private Set<Function<MetricsReader, NamedTestResult>> beam_f31520_0(String uuid, String timestamp)
{    Set<Function<MetricsReader, NamedTestResult>> suppliers = new HashSet<>();    suppliers.add(reader -> {        long readStart = reader.getStartTimeMetric("read_time");        long readEnd = reader.getEndTimeMetric("read_time");        return NamedTestResult.create(uuid, timestamp, "read_time", (readEnd - readStart) / 1e3);    });    return suppliers;}
public void beam_f31528_0()
{    thrown.expect(IllegalArgumentException.class);    HadoopFormatIO.<Text, Employee>read().withConfiguration(null);}
public void beam_f31529_0()
{    HadoopFormatIO.Read<Text, Employee> read = HadoopFormatIO.<Text, Employee>read().withConfiguration(serConf.get());    assertEquals(serConf.get(), read.getConfiguration().get());    assertEquals(null, read.getKeyTranslationFunction());    assertEquals(null, read.getValueTranslationFunction());    assertEquals(serConf.get().getClass("key.class", Object.class), read.getKeyTypeDescriptor().getRawType());    assertEquals(serConf.get().getClass("value.class", Object.class), read.getValueTypeDescriptor().getRawType());}
public void beam_f31530_0()
{    thrown.expect(IllegalArgumentException.class);    HadoopFormatIO.<String, Employee>read().withConfiguration(serConf.get()).withKeyTranslation(null);}
public void beam_f31538_0()
{    Configuration configuration = new Configuration();    configuration.setClass("mapreduce.job.inputformat.class", EmployeeInputFormat.class, InputFormat.class);    configuration.setClass("key.class", Text.class, Object.class);    thrown.expect(IllegalArgumentException.class);    HadoopFormatIO.<Text, Employee>read().withConfiguration(configuration);}
public void beam_f31539_0()
{    SimpleFunction<LongWritable, String> myKeyTranslateWithWrongInputType = new SimpleFunction<LongWritable, String>() {        @Override        public String apply(LongWritable input) {            return input.toString();        }    };    HadoopFormatIO.Read<String, Employee> read = HadoopFormatIO.<String, Employee>read().withConfiguration(serConf.get()).withKeyTranslation(myKeyTranslateWithWrongInputType);    thrown.expect(IllegalArgumentException.class);    thrown.expectMessage(String.format("Key translation's input type is not same as hadoop InputFormat : %s key " + "class : %s", serConf.get().getClass("mapreduce.job.inputformat.class", InputFormat.class), serConf.get().getClass("key.class", Object.class)));    read.validateTransform();}
public String beam_f31540_0(LongWritable input)
{    return input.toString();}
public void beam_f31548_0() throws Exception
{    List<KV<Text, Employee>> referenceRecords = TestEmployeeDataSet.getEmployeeData();    HadoopInputFormatBoundedSource<Text, Employee> hifSource = getTestHIFSource(EmployeeInputFormat.class, Text.class, Employee.class, WritableCoder.of(Text.class), AvroCoder.of(Employee.class));    long estimatedSize = hifSource.getEstimatedSizeBytes(p.getOptions());        assertEquals(referenceRecords.size(), estimatedSize);    List<BoundedSource<KV<Text, Employee>>> boundedSourceList = hifSource.split(0, p.getOptions());        assertEquals(TestEmployeeDataSet.NUMBER_OF_SPLITS, boundedSourceList.size());    List<KV<Text, Employee>> bundleRecords = new ArrayList<>();    for (BoundedSource<KV<Text, Employee>> source : boundedSourceList) {        List<KV<Text, Employee>> elements = new ArrayList<>();        BoundedReader<KV<Text, Employee>> reader = source.createReader(p.getOptions());        float recordsRead = 0;                assertEquals(Double.valueOf(0), reader.getFractionConsumed());        boolean start = reader.start();        assertTrue(start);        if (start) {            elements.add(reader.getCurrent());            boolean advance = reader.advance();                                    assertEquals(Double.valueOf(++recordsRead / TestEmployeeDataSet.NUMBER_OF_RECORDS_IN_EACH_SPLIT), reader.getFractionConsumed());            assertTrue(advance);            while (advance) {                elements.add(reader.getCurrent());                advance = reader.advance();                assertEquals(Double.valueOf(++recordsRead / TestEmployeeDataSet.NUMBER_OF_RECORDS_IN_EACH_SPLIT), reader.getFractionConsumed());            }            bundleRecords.addAll(elements);        }                assertEquals(Double.valueOf(1), reader.getFractionConsumed());        reader.close();    }    assertThat(bundleRecords, containsInAnyOrder(referenceRecords.toArray()));}
public void beam_f31549_0() throws Exception
{    InputFormat<Text, Employee> mockInputFormat = Mockito.mock(EmployeeInputFormat.class);    EmployeeRecordReader mockReader = Mockito.mock(EmployeeRecordReader.class);    Mockito.when(mockInputFormat.createRecordReader(Mockito.any(), Mockito.any())).thenReturn(mockReader);    Mockito.when(mockReader.nextKeyValue()).thenReturn(true);        Mockito.when(mockReader.getProgress()).thenReturn(2.0F);    InputSplit mockInputSplit = Mockito.mock(NewObjectsEmployeeInputSplit.class);    HadoopInputFormatBoundedSource<Text, Employee> boundedSource = new HadoopInputFormatBoundedSource<>(serConf, WritableCoder.of(Text.class), AvroCoder.of(Employee.class),     null,     null, new SerializableSplit(mockInputSplit));    boundedSource.setInputFormatObj(mockInputFormat);    BoundedReader<KV<Text, Employee>> reader = boundedSource.createReader(p.getOptions());    assertEquals(Double.valueOf(0), reader.getFractionConsumed());    boolean start = reader.start();    assertTrue(start);    if (start) {        boolean advance = reader.advance();        assertEquals(null, reader.getFractionConsumed());        assertTrue(advance);        if (advance) {            advance = reader.advance();            assertEquals(null, reader.getFractionConsumed());        }    }            assertEquals(null, reader.getFractionConsumed());    reader.close();}
public void beam_f31550_0() throws Exception
{    InputSplit mockInputSplit = Mockito.mock(NewObjectsEmployeeInputSplit.class);    HadoopInputFormatBoundedSource<Text, Employee> boundedSource = new HadoopInputFormatBoundedSource<>(serConf, WritableCoder.of(Text.class), AvroCoder.of(Employee.class),     null,     null, new SerializableSplit(mockInputSplit));    BoundedReader<KV<Text, Employee>> reader = boundedSource.createReader(p.getOptions());    SourceTestUtils.assertUnstartedReaderReadsSameAsItsSource(reader, p.getOptions());}
public void beam_f31558_0() throws Exception
{    List<BoundedSource<KV<Text, Employee>>> boundedSourceList = getBoundedSourceList(EmployeeInputFormat.class, Text.class, Employee.class, WritableCoder.of(Text.class), AvroCoder.of(Employee.class));    List<KV<Text, Employee>> bundleRecords = new ArrayList<>();    for (BoundedSource<KV<Text, Employee>> source : boundedSourceList) {        List<KV<Text, Employee>> elems = SourceTestUtils.readFromSource(source, p.getOptions());        bundleRecords.addAll(elems);    }    List<KV<Text, Employee>> referenceRecords = TestEmployeeDataSet.getEmployeeData();    assertThat(bundleRecords, containsInAnyOrder(referenceRecords.toArray()));}
public void beam_f31559_0()
{    Configuration conf = new Configuration();    conf.setClass("key.class", LongWritable.class, Object.class);    conf.setClass("value.class", Text.class, Object.class);    conf.setClass("mapreduce.job.inputformat.class", DBInputFormat.class, InputFormat.class);    thrown.expect(IllegalArgumentException.class);    HadoopFormatIO.<String, String>read().withConfiguration(new SerializableConfiguration(conf).get()).withKeyTranslation(myKeyTranslate).withValueTranslation(myValueTranslate);}
private static SerializableConfiguration beam_f31560_0(Class<?> inputFormatClassName, Class<?> keyClass, Class<?> valueClass)
{    Configuration conf = new Configuration();    conf.setClass("mapreduce.job.inputformat.class", inputFormatClassName, InputFormat.class);    conf.setClass("key.class", keyClass, Object.class);    conf.setClass("value.class", valueClass, Object.class);    return new SerializableConfiguration(conf);}
private String beam_f31568_0(String testName)
{    return Paths.get(tmpFolder.getRoot().getAbsolutePath(), TEST_FOLDER_NAME + "/" + testName).toAbsolutePath().toString();}
private String beam_f31569_0()
{    return Paths.get(tmpFolder.getRoot().getAbsolutePath(), LOCKS_FOLDER_NAME).toAbsolutePath().toString();}
private Stream<KV<Text, LongWritable>> beam_f31570_0(String fileName)
{    try (SequenceFileRecordReader<Text, LongWritable> reader = new SequenceFileRecordReader<>()) {        Path path = new Path(fileName);        TaskAttemptContext taskContext = HadoopFormats.createTaskAttemptContext(new Configuration(), new JobID("readJob", 0), 0);        reader.initialize(new FileSplit(path, 0L, Long.MAX_VALUE, new String[] { "localhost" }), taskContext);        List<KV<Text, LongWritable>> result = new ArrayList<>();        while (reader.nextKeyValue()) {            result.add(KV.of(new Text(reader.getCurrentKey().toString()), new LongWritable(reader.getCurrentValue().get())));        }        return result.stream();    } catch (Exception e) {        throw new RuntimeException(e);    }}
public PCollectionView<Configuration> beam_f31578_0(PCollection<? extends KV<KeyT, ValueT>> input)
{    Configuration conf = createWriteConf(SequenceFileOutputFormat.class, keyClass, valueClass, outputDirPath, REDUCERS_COUNT, String.valueOf(windowNum++));    return input.getPipeline().apply(Create.<Configuration>of(conf)).apply(View.<Configuration>asSingleton().withDefaultValue(conf));}
public void beam_f31579_0()
{    conf = loadTestConfiguration(EmployeeOutputFormat.class, Text.class, Employee.class);    OutputCommitter mockedOutputCommitter = Mockito.mock(OutputCommitter.class);    EmployeeOutputFormat.initWrittenOutput(mockedOutputCommitter);}
private static Configuration beam_f31580_0(Class<?> outputFormatClassName, Class<?> keyClass, Class<?> valueClass)
{    Configuration conf = new Configuration();    conf.setClass(MRJobConfig.OUTPUT_FORMAT_CLASS_ATTR, outputFormatClassName, OutputFormat.class);    conf.setClass(MRJobConfig.OUTPUT_KEY_CLASS, keyClass, Object.class);    conf.setClass(MRJobConfig.OUTPUT_VALUE_CLASS, valueClass, Object.class);    conf.setInt(MRJobConfig.NUM_REDUCES, REDUCERS_COUNT);    conf.set(MRJobConfig.ID, String.valueOf(1));    return conf;}
public void beam_f31588_0()
{    conf.set(HadoopFormatIO.OUTPUT_DIR, tmpFolder.getRoot().getAbsolutePath());    List<KV<String, Employee>> data = new ArrayList<>();    data.add(KV.of("key", new Employee("name", "address")));    PCollection<KV<String, Employee>> input = p.apply("CreateData", Create.of(data)).setTypeDescriptor(TypeDescriptors.kvs(new TypeDescriptor<String>() {    }, new TypeDescriptor<Employee>() {    }));    thrown.expect(Pipeline.PipelineExecutionException.class);    thrown.expectMessage(String.class.getName());    input.apply("Write", HadoopFormatIO.<String, Employee>write().withConfiguration(conf).withPartitioning().withExternalSynchronization(new HDFSSynchronization(getLocksDirPath())));    p.run().waitUntilFinish();}
public void beam_f31589_0()
{    conf.set(HadoopFormatIO.OUTPUT_DIR, tmpFolder.getRoot().getAbsolutePath());    List<KV<Text, Text>> data = new ArrayList<>();    data.add(KV.of(new Text("key"), new Text("value")));    TypeDescriptor<Text> textTypeDescriptor = new TypeDescriptor<Text>() {    };    PCollection<KV<Text, Text>> input = p.apply(Create.of(data)).setTypeDescriptor(TypeDescriptors.kvs(textTypeDescriptor, textTypeDescriptor));    thrown.expect(Pipeline.PipelineExecutionException.class);    thrown.expectMessage(Text.class.getName());    input.apply("Write", HadoopFormatIO.<Text, Text>write().withConfiguration(conf).withPartitioning().withExternalSynchronization(new HDFSSynchronization(getLocksDirPath())));    p.run().waitUntilFinish();}
public void beam_f31590_0()
{    HadoopFormatIO.Write<String, String> write = HadoopFormatIO.<String, String>write().withConfiguration(conf).withPartitioning().withExternalSynchronization(new HDFSSynchronization(getLocksDirPath()));    DisplayData displayData = DisplayData.from(write);    assertThat(displayData, hasDisplayItem(HadoopFormatIO.OUTPUT_FORMAT_CLASS_ATTR, conf.get(HadoopFormatIO.OUTPUT_FORMAT_CLASS_ATTR)));    assertThat(displayData, hasDisplayItem(HadoopFormatIO.OUTPUT_KEY_CLASS, conf.get(HadoopFormatIO.OUTPUT_KEY_CLASS)));    assertThat(displayData, hasDisplayItem(HadoopFormatIO.OUTPUT_VALUE_CLASS, conf.get(HadoopFormatIO.OUTPUT_VALUE_CLASS)));    assertThat(displayData, hasDisplayItem(HadoopFormatIO.PARTITIONER_CLASS_ATTR, HadoopFormats.DEFAULT_PARTITIONER_CLASS_ATTR.getName()));}
public void beam_f31598_0() throws IOException
{    FileSystem mockedFileSystem = Mockito.mock(FileSystem.class);    RemoteException thrownException = new RemoteException(AlreadyBeingCreatedException.class.getName(), "Failed to CREATE_FILE");    Mockito.when(mockedFileSystem.createNewFile(Mockito.any())).thenThrow(thrownException);    HDFSSynchronization synchronization = new HDFSSynchronization("someDir", (conf) -> mockedFileSystem);    assertFalse(synchronization.tryAcquireJobLock(configuration));}
private String beam_f31599_0(int taskId, int taskAttemptId)
{    return getFileInJobFolder(taskId + "_" + taskAttemptId);}
private String beam_f31600_0(TaskID taskID)
{    return getFileInJobFolder(String.valueOf(taskID.getId()));}
public List<InputSplit> beam_f31608_0(JobContext arg0)
{    List<InputSplit> inputSplitList = new ArrayList<>();    for (int i = 1; i <= TestEmployeeDataSet.NUMBER_OF_SPLITS; i++) {        InputSplit inputSplitObj = new ReuseEmployeeInputSplit((i - 1) * TestEmployeeDataSet.NUMBER_OF_RECORDS_IN_EACH_SPLIT, i * TestEmployeeDataSet.NUMBER_OF_RECORDS_IN_EACH_SPLIT - 1);        inputSplitList.add(inputSplitObj);    }    return inputSplitList;}
public long beam_f31609_0()
{    return this.endIndex - this.startIndex + 1;}
public String[] beam_f31610_0()
{    return null;}
public void beam_f31619_0(InputSplit split, TaskAttemptContext arg1)
{    this.split = (ReuseEmployeeInputSplit) split;    employeeListIndex = this.split.getStartIndex() - 1;    recordsRead = 0;    employeeDataList = TestEmployeeDataSet.populateEmployeeData();}
public boolean beam_f31620_0()
{    if ((recordsRead++) >= split.getLength()) {        return false;    }    employeeListIndex++;    KV<String, String> employeeDetails = employeeDataList.get((int) employeeListIndex);    List<String> empData = Splitter.on('_').splitToList(employeeDetails.getValue());        currentKey.set(employeeDetails.getKey());    currentValue.setEmpName(empData.get(0));    currentValue.setEmpAddress(empData.get(1));    return true;}
public static List<KV<String, String>> beam_f31621_0()
{    if (!data.isEmpty()) {        return data;    }    data.add(KV.of("0", "Alex_US"));    data.add(KV.of("1", "John_UK"));    data.add(KV.of("2", "Tom_UK"));    data.add(KV.of("3", "Nick_UAE"));    data.add(KV.of("4", "Smith_IND"));    data.add(KV.of("5", "Taylor_US"));    data.add(KV.of("6", "Gray_UK"));    data.add(KV.of("7", "James_UAE"));    data.add(KV.of("8", "Jordan_IND"));    data.add(KV.of("9", "Leena_UK"));    data.add(KV.of("10", "Zara_UAE"));    data.add(KV.of("11", "Talia_IND"));    data.add(KV.of("12", "Rose_UK"));    data.add(KV.of("13", "Kelvin_UAE"));    data.add(KV.of("14", "Goerge_IND"));    return data;}
public void beam_f31629_0(TestRow element, PreparedStatement statement) throws SQLException
{    statement.setLong(1, element.id());    statement.setString(2, element.name());}
public List<CoderProvider> beam_f31630_0()
{    return ImmutableList.of(HBaseMutationCoder.getCoderProvider(), CoderProviders.forCoder(TypeDescriptor.of(Result.class), HBaseResultCoder.of()), CoderProviders.forCoder(TypeDescriptor.of(HBaseQuery.class), HBaseQueryCoder.of()));}
public static Read beam_f31631_0()
{    return new Read(null, "", new SerializableScan(new Scan()));}
public Configuration beam_f31640_0()
{    return serializableConfiguration.get();}
public String beam_f31641_0()
{    return tableId;}
public Scan beam_f31642_0()
{    return serializableScan.get();}
public List<? extends BoundedSource<Result>> beam_f31650_1(long desiredBundleSizeBytes, PipelineOptions options) throws Exception
{        long estimatedSizeBytes = getEstimatedSizeBytes(options);    int numSplits = 1;    if (estimatedSizeBytes > 0 && desiredBundleSizeBytes > 0) {        numSplits = (int) Math.ceil((double) estimatedSizeBytes / desiredBundleSizeBytes);    }    try (Connection connection = ConnectionFactory.createConnection(read.getConfiguration())) {        List<HRegionLocation> regionLocations = HBaseUtils.getRegionLocations(connection, read.tableId, read.serializableScan.get());                        List<ByteKeyRange> ranges = HBaseUtils.getRanges(regionLocations, read.tableId, read.serializableScan.get());        final int numSources = ranges.size();                if (numSources > 0) {            List<HBaseSource> sources = new ArrayList<>(numSources);            for (int i = 0; i < numSources; i++) {                final ByteKeyRange range = ranges.get(i);                                                sources.add(new HBaseSource(new Read(read.serializableConfiguration, read.tableId, new SerializableScan(new Scan(read.serializableScan.get()).setStartRow(range.getStartKey().getBytes()).setStopRow(range.getEndKey().getBytes()))), estimatedSizeBytes));            }            return sources;        }    }    return Collections.singletonList(this);}
public BoundedReader<Result> beam_f31651_0(PipelineOptions pipelineOptions)
{    return new HBaseReader(this);}
public void beam_f31652_0()
{    read.validate(null);}
public final Double beam_f31660_0()
{    return rangeTracker.getFractionConsumed();}
public final long beam_f31661_0()
{    return rangeTracker.getSplitPointsConsumed();}
public final synchronized HBaseSource beam_f31662_1(double fraction)
{    ByteKey splitKey;    try {        splitKey = rangeTracker.getRange().interpolateKey(fraction);    } catch (RuntimeException e) {                return null;    }        HBaseSource primary;    HBaseSource residual;    try {        primary = source.withEndKey(splitKey);        residual = source.withStartKey(splitKey);    } catch (Exception e) {                return null;    }    if (!rangeTracker.trySplitAtPosition(splitKey)) {        return null;    }    this.source = primary;    return residual;}
public void beam_f31671_0(StartBundleContext c) throws IOException
{    BufferedMutatorParams params = new BufferedMutatorParams(TableName.valueOf(tableId));    mutator = connection.getBufferedMutator(params);    recordsWritten = 0;}
public void beam_f31672_0(ProcessContext c) throws Exception
{    mutator.mutate(c.element());    ++recordsWritten;}
public void beam_f31673_1() throws Exception
{    mutator.flush();    }
public Coder<T> beam_f31681_0(TypeDescriptor<T> typeDescriptor, List<? extends Coder<?>> componentCoders) throws CannotProvideCoderException
{    if (!typeDescriptor.isSubtypeOf(HBASE_MUTATION_TYPE_DESCRIPTOR)) {        throw new CannotProvideCoderException(String.format("Cannot provide %s because %s is not a subclass of %s", HBaseMutationCoder.class.getSimpleName(), typeDescriptor, Mutation.class.getName()));    }    try {        @SuppressWarnings("unchecked")        Coder<T> coder = (Coder<T>) HBaseMutationCoder.of();        return coder;    } catch (IllegalArgumentException e) {        throw new CannotProvideCoderException(e);    }}
public static HBaseQuery beam_f31682_0(String tableId, Scan scan)
{    return new HBaseQuery(tableId, scan);}
public String beam_f31683_0()
{    return tableId;}
public void beam_f31691_0(ProcessContext c, RestrictionTracker<ByteKeyRange, ByteKey> tracker) throws Exception
{    final HBaseQuery query = c.element();    TableName tableName = TableName.valueOf(query.getTableId());    Table table = connection.getTable(tableName);    final ByteKeyRange range = tracker.currentRestriction();    try (ResultScanner scanner = table.getScanner(newScanInRange(query.getScan(), range))) {        for (Result result : scanner) {            ByteKey key = ByteKey.copyFrom(result.getRow());            if (!tracker.tryClaim(key)) {                return;            }            c.output(result);        }        tracker.tryClaim(ByteKey.EMPTY);    }}
public ByteKeyRange beam_f31692_0(HBaseQuery query)
{    return ByteKeyRange.of(ByteKey.copyFrom(query.getScan().getStartRow()), ByteKey.copyFrom(query.getScan().getStopRow()));}
public void beam_f31693_0(HBaseQuery query, ByteKeyRange range, OutputReceiver<ByteKeyRange> receiver) throws Exception
{    List<HRegionLocation> regionLocations = HBaseUtils.getRegionLocations(connection, query.getTableId(), query.getScan());    List<ByteKeyRange> splitRanges = HBaseUtils.getRanges(regionLocations, query.getTableId(), query.getScan());    for (ByteKeyRange splitRange : splitRanges) {        receiver.output(ByteKeyRange.of(splitRange.getStartKey(), splitRange.getEndKey()));    }}
 static List<ByteKeyRange> beam_f31701_0(List<HRegionLocation> regionLocations, String tableId, Scan scan)
{    byte[] startRow = scan.getStartRow();    byte[] stopRow = scan.getStopRow();    final List<ByteKeyRange> splits = new ArrayList<>();    final boolean scanWithNoLowerBound = startRow.length == 0;    final boolean scanWithNoUpperBound = stopRow.length == 0;    for (HRegionLocation regionLocation : regionLocations) {        final byte[] startKey = regionLocation.getRegionInfo().getStartKey();        final byte[] endKey = regionLocation.getRegionInfo().getEndKey();        boolean isLastRegion = endKey.length == 0;        final byte[] splitStart = (scanWithNoLowerBound || Bytes.compareTo(startKey, startRow) >= 0) ? startKey : startRow;        final byte[] splitStop = (scanWithNoUpperBound || Bytes.compareTo(endKey, stopRow) <= 0) && !isLastRegion ? endKey : stopRow;        splits.add(ByteKeyRange.of(ByteKey.copyFrom(splitStart), ByteKey.copyFrom(splitStop)));    }    return splits;}
private void beam_f31702_0(ObjectOutputStream out) throws IOException
{    ProtobufUtil.toScan(scan).writeDelimitedTo(out);}
private void beam_f31703_0(ObjectInputStream in) throws IOException
{    scan = ProtobufUtil.toScan(ClientProtos.Scan.parseDelimitedFrom(in));}
private void beam_f31711_0()
{    PCollection<Result> tableRows = pipelineRead.apply(HBaseIO.read().withConfiguration(conf).withTableId(TABLE_NAME));    PAssert.thatSingleton(tableRows.apply("Count All", Count.<Result>globally())).isEqualTo((long) numberOfRows);    PCollection<String> consolidatedHashcode = tableRows.apply(ParDo.of(new SelectNameFn())).apply("Hash row contents", Combine.globally(new HashingFn()).withoutDefaults());    PAssert.that(consolidatedHashcode).containsInAnyOrder(TestRow.getExpectedHashForRowCount(numberOfRows));    pipelineRead.run().waitUntilFinish();}
public void beam_f31712_0(ProcessContext c)
{    c.output(new Put(c.element().id().toString().getBytes(StandardCharsets.UTF_8)).addColumn(COLUMN_FAMILY, COLUMN_HASH, Bytes.toBytes(c.element().name())));}
public void beam_f31713_0(ProcessContext c)
{    c.output(new String(c.element().getValue(COLUMN_FAMILY, COLUMN_HASH), StandardCharsets.UTF_8));}
public void beam_f31721_0()
{    HBaseIO.Write write = HBaseIO.write().withTableId("table");    thrown.expect(IllegalArgumentException.class);    write.expand(null);}
public void beam_f31722_0()
{    final String table = tmpTable.getName();        thrown.expect(IllegalArgumentException.class);    thrown.expectMessage(String.format("Table %s does not exist", table));    runReadTest(HBaseIO.read().withConfiguration(conf).withTableId(table), false, new ArrayList<>());    runReadTest(HBaseIO.read().withConfiguration(conf).withTableId(table), true, new ArrayList<>());}
public void beam_f31723_0() throws Exception
{    final String table = tmpTable.getName();    createTable(table);    runReadTest(HBaseIO.read().withConfiguration(conf).withTableId(table), false, new ArrayList<>());    runReadTest(HBaseIO.read().withConfiguration(conf).withTableId(table), true, new ArrayList<>());}
public void beam_f31731_0() throws Exception
{    final String table = tmpTable.getName();    final int numRows = 1001;    final ByteKey startKey = ByteKey.copyFrom("2".getBytes(StandardCharsets.UTF_8));    createAndWriteData(table, numRows);        final ByteKeyRange suffixRange = ByteKeyRange.ALL_KEYS.withStartKey(startKey);    runReadTestLength(HBaseIO.read().withConfiguration(conf).withTableId(table).withKeyRange(suffixRange), false, 875);}
public void beam_f31732_0() throws Exception
{    final String table = tmpTable.getName();    final int numRows = 1001;    final byte[] startRow = "2".getBytes(StandardCharsets.UTF_8);    final byte[] stopRow = "9".getBytes(StandardCharsets.UTF_8);    createAndWriteData(table, numRows);            runReadTestLength(HBaseIO.read().withConfiguration(conf).withTableId(table).withKeyRange(startRow, stopRow), false, 441);}
public void beam_f31733_0() throws Exception
{    final String table = tmpTable.getName();    final int numRows = 1001;    final byte[] startRow = "2".getBytes(StandardCharsets.UTF_8);    final byte[] stopRow = "9".getBytes(StandardCharsets.UTF_8);    createAndWriteData(table, numRows);            runReadTestLength(HBaseIO.read().withConfiguration(conf).withTableId(table).withKeyRange(startRow, stopRow), true, 441);}
private static void beam_f31741_0(String tableId) throws Exception
{    byte[][] splitKeys = { "4".getBytes(StandardCharsets.UTF_8), "8".getBytes(StandardCharsets.UTF_8), "C".getBytes(StandardCharsets.UTF_8) };    createTable(tableId, COLUMN_FAMILY, splitKeys);}
private static void beam_f31742_0(String tableId, byte[] columnFamily, byte[][] splitKeys) throws Exception
{    TableName tableName = TableName.valueOf(tableId);    HTableDescriptor desc = new HTableDescriptor(tableName);    HColumnDescriptor colDef = new HColumnDescriptor(columnFamily);    desc.addFamily(colDef);    admin.createTable(desc, splitKeys);}
private static void beam_f31743_0(String tableId, int numRows) throws Exception
{    Connection connection = admin.getConnection();    TableName tableName = TableName.valueOf(tableId);    BufferedMutator mutator = connection.getBufferedMutator(tableName);    List<Mutation> mutations = makeTableData(numRows);    mutator.mutate(mutations);    mutator.flush();    mutator.close();}
private void beam_f31751_0(HBaseIO.Read read, boolean useSdf, List<Result> expected)
{    PCollection<Result> rows = applyRead(read, useSdf);    PAssert.that(rows).containsInAnyOrder(expected);    p.run().waitUntilFinish();}
private void beam_f31752_0(HBaseIO.Read read, boolean useSdf, long numElements)
{    PCollection<Result> rows = applyRead(read, useSdf);    final String transformId = read.getTableId() + "_" + read.getKeyRange();    PAssert.thatSingleton(rows.apply("Count" + transformId, Count.globally())).isEqualTo(numElements);    p.run().waitUntilFinish();}
private PCollection<Result> beam_f31753_0(HBaseIO.Read read, boolean useSdf)
{    final String transformId = read.getTableId() + "_" + read.getKeyRange();    return useSdf ? p.apply("Create" + transformId, Create.of(HBaseQuery.of(read.getTableId(), read.getScan()))).apply("ReadAll" + transformId, HBaseIO.readAll().withConfiguration(read.getConfiguration())) : p.apply("Read" + transformId, read);}
public boolean beam_f31761_0(String dbName)
{    try {        metastore.getDatabase(dbName);        return true;    } catch (NoSuchObjectException e) {        return false;    } catch (Exception e) {        throw new RuntimeException(e);    }}
public Optional<Schema> beam_f31762_0(String db, String table)
{    try {        org.apache.hadoop.hive.metastore.api.Table metastoreTable = metastore.getTable(db, table);        List<FieldSchema> fields = Lists.newArrayList(metastoreTable.getSd().getCols());        fields.addAll(metastoreTable.getPartitionKeys());        Schema schema = SchemaUtils.toBeamSchema(fields);        return Optional.of(schema);    } catch (NoSuchObjectException e) {        return Optional.absent();    } catch (Exception e) {        throw new RuntimeException(e);    }}
public static Write beam_f31763_0()
{    return new AutoValue_HCatalogIO_Write.Builder().setBatchSize(BATCH_SIZE).build();}
public Read beam_f31771_0(TerminationCondition<Read, ?> terminationCondition)
{    return toBuilder().setTerminationCondition(terminationCondition).build();}
 Read beam_f31772_0(int splitId)
{    checkArgument(splitId >= 0, "Invalid split id-" + splitId);    return toBuilder().setSplitId(splitId).build();}
 Read beam_f31773_0(ReaderContext context)
{    return toBuilder().setContext(context).build();}
private ReaderContext beam_f31781_0(long desiredSplitCount) throws HCatException
{    ReadEntity entity = new ReadEntity.Builder().withDatabase(spec.getDatabase()).withTable(spec.getTable()).withFilter(spec.getFilter()).build();        Map<String, String> configProps = new HashMap<>(spec.getConfigProperties());    configProps.put(HCatConstants.HCAT_DESIRED_PARTITION_NUM_SPLITS, String.valueOf(desiredSplitCount));    return DataTransferFactory.getHCatReader(entity, configProps).prepareRead();}
public boolean beam_f31782_0() throws HCatException
{    HCatReader reader = DataTransferFactory.getHCatReader(source.spec.getContext(), source.spec.getSplitId());    hcatIterator = reader.read();    return advance();}
public boolean beam_f31783_0()
{    if (hcatIterator.hasNext()) {        current = hcatIterator.next();        return true;    } else {        current = null;        return false;    }}
public PDone beam_f31792_0(PCollection<HCatRecord> input)
{    checkArgument(getConfigProperties() != null, "withConfigProperties() is required");    checkArgument(getTable() != null, "withTable() is required");    input.apply(ParDo.of(new WriteFn(this)));    return PDone.in(input.getPipeline());}
public void beam_f31793_0(DisplayData.Builder builder)
{    super.populateDisplayData(builder);    builder.addIfNotNull(DisplayData.item("database", spec.getDatabase()));    builder.add(DisplayData.item("table", spec.getTable()));    builder.addIfNotNull(DisplayData.item("partition", String.valueOf(spec.getPartition())));    builder.add(DisplayData.item("configProperties", spec.getConfigProperties().toString()));    builder.add(DisplayData.item("batchSize", spec.getBatchSize()));}
public void beam_f31794_0() throws HCatException
{    WriteEntity entity = new WriteEntity.Builder().withDatabase(spec.getDatabase()).withTable(spec.getTable()).withPartition(spec.getPartition()).build();    masterWriter = DataTransferFactory.getHCatWriter(entity, spec.getConfigProperties());    writerContext = masterWriter.prepareWrite();    slaveWriter = DataTransferFactory.getHCatWriter(writerContext);}
 static int beam_f31802_0(Read readRequest, Partition partitionToRead) throws Exception
{    int desiredSplitCount = 1;    long estimatedSizeBytes = getFileSizeForPartition(readRequest, partitionToRead);    if (estimatedSizeBytes > 0) {        desiredSplitCount = (int) Math.ceil((double) estimatedSizeBytes / DESIRED_BUNDLE_SIZE_BYTES);    }    return desiredSplitCount;}
 static Configuration beam_f31803_0(Map<String, String> configProperties)
{    Configuration conf = new Configuration();    for (Map.Entry<String, String> entry : configProperties.entrySet()) {        conf.set(entry.getKey(), entry.getValue());    }    return conf;}
private static long beam_f31804_0(Read readRequest, Partition partitionToRead) throws Exception
{    IMetaStoreClient client = null;    try {        HiveConf hiveConf = HCatalogUtils.createHiveConf(readRequest);        client = HCatalogUtils.createMetaStoreClient(hiveConf);        List<org.apache.hadoop.hive.ql.metadata.Partition> p = new ArrayList<>();        Table table = HCatUtil.getTable(client, readRequest.getDatabase(), readRequest.getTable());        final org.apache.hadoop.hive.ql.metadata.Partition partition = new org.apache.hadoop.hive.ql.metadata.Partition(table, partitionToRead);        p.add(partition);        final List<Long> fileSizeForPartitions = StatsUtils.getFileSizeForPartitions(hiveConf, p);        return fileSizeForPartitions.get(0);    } finally {                if (client != null) {            client.close();        }    }}
public void beam_f31812_0(ProcessContext c) throws Exception
{    final Read readRequest = c.element().getKey();    final Integer partitionIndexToRead = c.element().getValue();    ReaderContext readerContext = getReaderContext(readRequest, partitionIndexToRead);    for (int i = 0; i < readerContext.numSplits(); i++) {        HCatReader reader = DataTransferFactory.getHCatReader(readerContext, i);        Iterator<HCatRecord> hcatIterator = reader.read();        while (hcatIterator.hasNext()) {            final HCatRecord record = hcatIterator.next();            c.output(record);        }    }}
public void beam_f31813_0() throws Exception
{    final Configuration conf = HCatalogUtils.createConfiguration(configProperties);    metaStoreClient = HCatalogUtils.createMetaStoreClient(conf);}
public void beam_f31814_0()
{    if (metaStoreClient != null) {        metaStoreClient.close();    }}
private static WriterContext beam_f31822_0(Map<String, String> config) throws HCatException
{    return DataTransferFactory.getHCatWriter(WRITE_ENTITY, config).prepareWrite();}
private static void beam_f31823_0(WriterContext context) throws HCatException
{    DataTransferFactory.getHCatWriter(context).write(buildHCatRecords(TEST_RECORDS_COUNT).iterator());}
private static void beam_f31824_0(Map<String, String> config, WriterContext context) throws IOException
{    DataTransferFactory.getHCatWriter(WRITE_ENTITY, config).commit(context);}
public static void beam_f31832_0() throws Exception
{    if (service != null) {        service.executeQuery("drop table " + TEST_TABLE);        service.close();    }}
public void beam_f31833_0() throws Exception
{    prepareTestData();}
public void beam_f31834_0()
{    HCatalogBeamSchema hcatSchema = HCatalogBeamSchema.create(service.getHiveConfAsMap());    assertTrue(hcatSchema.hasDatabase(TEST_DATABASE));}
public void beam_f31842_0() throws Exception
{    try {        helper.dropHiveTable(tableName);    } catch (Exception e) {        helper.closeConnection();        throw new Exception("Problem with deleting table " + tableName + ": " + e, e);    } finally {        helper.closeConnection();    }}
public void beam_f31843_0()
{    pipelineWrite.apply("Generate sequence", Create.of(buildHCatRecords(options.getNumberOfRecords()))).apply(HCatalogIO.write().withConfigProperties(configProperties).withDatabase(options.getHCatalogHiveDatabaseName()).withTable(tableName));    pipelineWrite.run().waitUntilFinish();    PCollection<String> testRecords = pipelineRead.apply(HCatalogIO.read().withConfigProperties(configProperties).withDatabase(options.getHCatalogHiveDatabaseName()).withTable(tableName)).apply(ParDo.of(new CreateHCatFn()));    PCollection<String> consolidatedHashcode = testRecords.apply("Calculate hashcode", Combine.globally(new HashingFn()));    String expectedHash = getHashForRecordCount(options.getNumberOfRecords(), EXPECTED_HASHES);    PAssert.thatSingleton(consolidatedHashcode).isEqualTo(expectedHash);    pipelineRead.run().waitUntilFinish();}
public void beam_f31844_0(ProcessContext c)
{    c.output(c.element().get(0).toString());}
public void beam_f31852_0() throws Exception
{    defaultPipeline.apply(Create.of(buildHCatRecords(TEST_RECORDS_COUNT))).apply(HCatalogIO.write().withConfigProperties(getConfigPropertiesAsMap(service.getHiveConf())).withDatabase(TEST_DATABASE).withTable(TEST_TABLE).withPartition(getPartitions()).withBatchSize(512L));    defaultPipeline.run();    final ImmutableList<String> partitions = ImmutableList.of("load_date", "product_type");    final PCollection<HCatRecord> data = readAfterWritePipeline.apply("ReadData", HCatalogIO.read().withConfigProperties(getConfigPropertiesAsMap(service.getHiveConf())).withDatabase(TEST_DATABASE).withPartitionCols(partitions).withTable(TEST_TABLE).withPollingInterval(Duration.millis(15000)).withTerminationCondition(Watch.Growth.afterTotalOf(Duration.millis(60000)))).setCoder((Coder) WritableCoder.of(DefaultHCatRecord.class));    final PCollection<String> output = data.apply(ParDo.of(new DoFn<HCatRecord, String>() {        @ProcessElement        public void processElement(ProcessContext c) {            c.output(c.element().get(0).toString());        }    }));    PAssert.that(output).containsInAnyOrder(getExpectedRecords(TEST_RECORDS_COUNT));    readAfterWritePipeline.run();}
public void beam_f31853_0(ProcessContext c)
{    c.output(c.element().get(0).toString());}
public void beam_f31854_0()
{    thrown.expectCause(isA(UserCodeException.class));    thrown.expectMessage(containsString("org.apache.hive.hcatalog.common.HCatException"));    thrown.expectMessage(containsString("NoSuchObjectException"));    defaultPipeline.apply(Create.of(buildHCatRecords(TEST_RECORDS_COUNT))).apply(HCatalogIO.write().withConfigProperties(getConfigPropertiesAsMap(service.getHiveConf())).withTable("myowntable"));    defaultPipeline.run();}
private void beam_f31862_0()
{    service.executeQuery("drop table " + TEST_TABLE);    service.executeQuery("create table " + TEST_TABLE + "(mycol1 string, mycol2 int)");}
private void beam_f31863_0() throws CommandNeedRetryException
{    service.executeQuery("drop table " + TEST_TABLE);    service.executeQuery("create table " + TEST_TABLE + "(mycol1 string, mycol2 int)  " + "partitioned by (load_date string, product_type string)");    service.executeQuery("ALTER TABLE " + TEST_TABLE + " ADD PARTITION (load_date='2019-05-14T23:28:04.425Z', product_type='1')");}
private void beam_f31864_0() throws Exception
{    reCreateTestTable();    insertTestData(getConfigPropertiesAsMap(service.getHiveConf()));}
public static WriteVoid<T> beam_f31872_0()
{    return new AutoValue_JdbcIO_WriteVoid.Builder<T>().setBatchSize(DEFAULT_BATCH_SIZE).setRetryStrategy(new DefaultRetryStrategy()).build();}
public boolean beam_f31873_0(SQLException e)
{    return "40001".equals(e.getSQLState());}
public static DataSourceConfiguration beam_f31874_0(DataSource dataSource)
{    checkArgument(dataSource != null, "dataSource can not be null");    checkArgument(dataSource instanceof Serializable, "dataSource must be Serializable");    return new AutoValue_JdbcIO_DataSourceConfiguration.Builder().setDataSource(dataSource).build();}
public DataSourceConfiguration beam_f31882_0(ValueProvider<String> connectionProperties)
{    checkArgument(connectionProperties != null, "connectionProperties can not be null");    return builder().setConnectionProperties(connectionProperties).build();}
 void beam_f31883_0(DisplayData.Builder builder)
{    if (getDataSource() != null) {        builder.addIfNotNull(DisplayData.item("dataSource", getDataSource().getClass().getName()));    } else {        builder.addIfNotNull(DisplayData.item("jdbcDriverClassName", getDriverClassName()));        builder.addIfNotNull(DisplayData.item("jdbcUrl", getUrl()));        builder.addIfNotNull(DisplayData.item("username", getUsername()));    }}
 DataSource beam_f31884_0()
{    if (getDataSource() == null) {        BasicDataSource basicDataSource = new BasicDataSource();        if (getDriverClassName() != null) {            basicDataSource.setDriverClassName(getDriverClassName().get());        }        if (getUrl() != null) {            basicDataSource.setUrl(getUrl().get());        }        if (getUsername() != null) {            basicDataSource.setUsername(getUsername().get());        }        if (getPassword() != null) {            basicDataSource.setPassword(getPassword().get());        }        if (getConnectionProperties() != null && getConnectionProperties().get() != null) {            basicDataSource.setConnectionProperties(getConnectionProperties().get());        }        return basicDataSource;    }    return getDataSource();}
private Schema beam_f31892_0()
{    DataSource ds = getDataSourceProviderFn().apply(null);    try (Connection conn = ds.getConnection();        PreparedStatement statement = conn.prepareStatement(getQuery().get(), ResultSet.TYPE_FORWARD_ONLY, ResultSet.CONCUR_READ_ONLY)) {        return SchemaUtil.toBeamSchema(statement.getMetaData());    } catch (SQLException e) {        throw new BeamSchemaInferenceException("Failed to infer Beam schema", e);    }}
public void beam_f31893_0(DisplayData.Builder builder)
{    super.populateDisplayData(builder);    builder.add(DisplayData.item("query", getQuery()));    if (getDataSourceProviderFn() instanceof HasDisplayData) {        ((HasDisplayData) getDataSourceProviderFn()).populateDisplayData(builder);    }}
public Read<T> beam_f31894_0(final DataSourceConfiguration config)
{    return withDataSourceProviderFn(new DataSourceProviderFromDataSourceConfiguration(config));}
public Read<T> beam_f31902_0(boolean outputParallelization)
{    return toBuilder().setOutputParallelization(outputParallelization).build();}
public PCollection<T> beam_f31903_0(PBegin input)
{    checkArgument(getQuery() != null, "withQuery() is required");    checkArgument(getRowMapper() != null, "withRowMapper() is required");    checkArgument(getCoder() != null, "withCoder() is required");    checkArgument((getDataSourceProviderFn() != null), "withDataSourceConfiguration() or withDataSourceProviderFn() is required");    return input.apply(Create.of((Void) null)).apply(JdbcIO.<Void, T>readAll().withDataSourceProviderFn(getDataSourceProviderFn()).withQuery(getQuery()).withCoder(getCoder()).withRowMapper(getRowMapper()).withFetchSize(getFetchSize()).withOutputParallelization(getOutputParallelization()).withParameterSetter((element, preparedStatement) -> {        if (getStatementPreparator() != null) {            getStatementPreparator().setParameters(preparedStatement);        }    }));}
public void beam_f31904_0(DisplayData.Builder builder)
{    super.populateDisplayData(builder);    builder.add(DisplayData.item("query", getQuery()));    builder.add(DisplayData.item("rowMapper", getRowMapper().getClass().getName()));    builder.add(DisplayData.item("coder", getCoder().getClass().getName()));    if (getDataSourceProviderFn() instanceof HasDisplayData) {        ((HasDisplayData) getDataSourceProviderFn()).populateDisplayData(builder);    }}
public ReadAll<ParameterT, OutputT> beam_f31912_0(int fetchSize)
{    checkArgument(fetchSize > 0, "fetch size must be >0");    return toBuilder().setFetchSize(fetchSize).build();}
public ReadAll<ParameterT, OutputT> beam_f31913_0(boolean outputParallelization)
{    return toBuilder().setOutputParallelization(outputParallelization).build();}
public PCollection<OutputT> beam_f31914_0(PCollection<ParameterT> input)
{    PCollection<OutputT> output = input.apply(ParDo.of(new ReadFn<>(getDataSourceProviderFn(), getQuery(), getParameterSetter(), getRowMapper(), getFetchSize()))).setCoder(getCoder());    if (getOutputParallelization()) {        output = output.apply(new Reparallelize<>());    }    try {        TypeDescriptor<OutputT> typeDesc = getCoder().getEncodedTypeDescriptor();        SchemaRegistry registry = input.getPipeline().getSchemaRegistry();        Schema schema = registry.getSchema(typeDesc);        output.setSchema(schema, registry.getToRowFunction(typeDesc), registry.getFromRowFunction(typeDesc));    } catch (NoSuchSchemaException e) {        }    return output;}
public Write<T> beam_f31922_0(PreparedStatementSetter<T> setter)
{    return new Write(inner.withPreparedStatementSetter(setter));}
public Write<T> beam_f31923_0(long batchSize)
{    return new Write(inner.withBatchSize(batchSize));}
public Write<T> beam_f31924_0(RetryStrategy retryStrategy)
{    return new Write(inner.withRetryStrategy(retryStrategy));}
public void beam_f31932_0(T element, PreparedStatement preparedStatement) throws Exception
{    Row row = (element instanceof Row) ? (Row) element : toRowFn.apply(element);    IntStream.range(0, fields.size()).forEach((index) -> {        try {            preparedStatementFieldSetterList.get(index).set(row, preparedStatement, index, fields.get(index));        } catch (SQLException | NullPointerException e) {            throw new RuntimeException("Error while setting data to preparedStatement", e);        }    });}
public WriteVoid<T> beam_f31933_0(DataSourceConfiguration config)
{    return withDataSourceProviderFn(new DataSourceProviderFromDataSourceConfiguration(config));}
public WriteVoid<T> beam_f31934_0(SerializableFunction<Void, DataSource> dataSourceProviderFn)
{    return toBuilder().setDataSourceProviderFn(dataSourceProviderFn).build();}
public void beam_f31942_0()
{    dataSource = spec.getDataSourceProviderFn().apply(null);}
public void beam_f31943_0() throws Exception
{    connection = dataSource.getConnection();    connection.setAutoCommit(false);    preparedStatement = connection.prepareStatement(spec.getStatement().get());}
public void beam_f31944_0(ProcessContext context) throws Exception
{    T record = context.element();    records.add(record);    if (records.size() >= spec.getBatchSize()) {        executeBatch();    }}
 static synchronized DataSource beam_f31952_0(Void input)
{    if (source == null) {        DataSource basicSource = dataSourceProviderFn.apply(input);        DataSourceConnectionFactory connectionFactory = new DataSourceConnectionFactory(basicSource);        PoolableConnectionFactory poolableConnectionFactory = new PoolableConnectionFactory(connectionFactory, null);        GenericObjectPoolConfig poolConfig = new GenericObjectPoolConfig();        poolConfig.setMaxTotal(1);        poolConfig.setMinIdle(0);        poolConfig.setMinEvictableIdleTimeMillis(10000);        poolConfig.setSoftMinEvictableIdleTimeMillis(30000);        GenericObjectPool connectionPool = new GenericObjectPool(poolableConnectionFactory, poolConfig);        poolableConnectionFactory.setPool(connectionPool);        poolableConnectionFactory.setDefaultAutoCommit(false);        poolableConnectionFactory.setDefaultReadOnly(false);        source = new PoolingDataSource(connectionPool);    }    return source;}
public void beam_f31953_0(DisplayData.Builder builder)
{    if (dataSourceProviderFn instanceof HasDisplayData) {        ((HasDisplayData) dataSourceProviderFn).populateDisplayData(builder);    }}
public static SerializableFunction<Void, DataSource> beam_f31954_0(DataSourceConfiguration config)
{    if (instance == null) {        instance = new DataSourceProviderFromDataSourceConfiguration(config);    }    return instance;}
private static Calendar beam_f31962_0(DateTime dateTime, boolean wantDateOnly)
{    Calendar cal = Calendar.getInstance();    cal.setTimeZone(TimeZone.getTimeZone(dateTime.getZone().getID()));    if (wantDateOnly) {                cal.set(Calendar.YEAR, dateTime.getYear());        cal.set(Calendar.MONTH, dateTime.getMonthOfYear() - 1);        cal.set(Calendar.DATE, dateTime.getDayOfMonth());        cal.set(Calendar.HOUR_OF_DAY, 0);        cal.set(Calendar.MINUTE, 0);        cal.set(Calendar.SECOND, 0);        cal.set(Calendar.MILLISECOND, 0);    } else {                cal.set(Calendar.YEAR, 1970);        cal.set(Calendar.MONTH, Calendar.JANUARY);        cal.set(Calendar.DATE, 1);        cal.set(Calendar.HOUR_OF_DAY, dateTime.getHourOfDay());        cal.set(Calendar.MINUTE, dateTime.getMinuteOfHour());        cal.set(Calendar.SECOND, dateTime.getSecondOfMinute());        cal.set(Calendar.MILLISECOND, dateTime.getMillisOfSecond());    }    return cal;}
private static Calendar beam_f31963_0(DateTime dateTime)
{    Calendar calendar = Calendar.getInstance(TimeZone.getTimeZone(dateTime.getZone().getID()));    calendar.setTimeInMillis(dateTime.getMillis());    return calendar;}
 static Schema.FieldType beam_f31964_0(JDBCType jdbcType, int length)
{    return Schema.FieldType.logicalType(FixedLengthString.of(jdbcType.getName(), length));}
public T beam_f31972_0(T input)
{    return input;}
public boolean beam_f31973_0(Object o)
{    if (this == o) {        return true;    }    if (!(o instanceof JdbcLogicalType)) {        return false;    }    JdbcLogicalType<?> that = (JdbcLogicalType<?>) o;    return Objects.equals(identifier, that.identifier) && Objects.equals(baseType, that.baseType) && Objects.equals(argument, that.argument);}
public int beam_f31974_0()
{    return Objects.hash(identifier, baseType, argument);}
public byte[] beam_f31982_0(byte[] base)
{    checkArgument(base == null || base.length <= maxLength);    return base;}
public static FixedPrecisionNumeric beam_f31983_0(String identifier, int precision, int scale)
{    return new FixedPrecisionNumeric(identifier, precision, scale);}
public BigDecimal beam_f31984_0(BigDecimal base)
{    checkArgument(base == null || (base.precision() == precision && base.scale() == scale));    return base;}
private static ResultSetFieldExtractor beam_f31992_0(ResultSetFieldExtractor elementExtractor)
{    return (rs, index) -> {        Array arrayVal = rs.getArray(index);        if (arrayVal == null) {            return null;        }        List<Object> arrayElements = new ArrayList<>();        ResultSet arrayRs = arrayVal.getResultSet();        while (arrayRs.next()) {            arrayElements.add(elementExtractor.extract(arrayRs, 1));        }        return arrayElements;    };}
private static ResultSetFieldExtractor beam_f31993_0(final Schema.LogicalType<InputT, BaseT> fieldType)
{    String logicalTypeName = fieldType.getIdentifier();    JDBCType underlyingType = JDBCType.valueOf(logicalTypeName);    switch(underlyingType) {        case DATE:            return DATE_EXTRACTOR;        case TIME:            return TIME_EXTRACTOR;        case TIMESTAMP_WITH_TIMEZONE:            return TIMESTAMP_EXTRACTOR;        default:            ResultSetFieldExtractor extractor = createFieldExtractor(fieldType.getBaseType());            return (rs, index) -> fieldType.toInputType((BaseT) extractor.extract(rs, index));    }}
private static ResultSetFieldExtractor beam_f31994_0()
{    return (rs, i) -> {        Date date = rs.getDate(i, Calendar.getInstance(TimeZone.getTimeZone(ZoneOffset.UTC)));        if (date == null) {            return null;        }        ZonedDateTime zdt = ZonedDateTime.of(date.toLocalDate(), LocalTime.MIDNIGHT, ZoneOffset.UTC);        return new DateTime(zdt.toInstant().toEpochMilli(), ISOChronology.getInstanceUTC());    };}
public static FieldWithIndex beam_f32002_0(Schema.Field field, Integer index)
{    checkArgument(field != null);    checkArgument(index != null);    return new FieldWithIndex(field, index);}
public Schema.Field beam_f32003_0()
{    return field;}
public Integer beam_f32004_0()
{    return index;}
private Set<Function<MetricsReader, NamedTestResult>> beam_f32012_0(String uuid, String timestamp)
{    Set<Function<MetricsReader, NamedTestResult>> suppliers = new HashSet<>();    suppliers.add(reader -> {        long readStart = reader.getStartTimeMetric("read_time");        long readEnd = reader.getEndTimeMetric("read_time");        return NamedTestResult.create(uuid, timestamp, "read_time", (readEnd - readStart) / 1e3);    });    return suppliers;}
private PipelineResult beam_f32013_0()
{    pipelineWrite.apply(GenerateSequence.from(0).to(numberOfRows)).apply(ParDo.of(new TestRow.DeterministicallyConstructTestRowFn())).apply(ParDo.of(new TimeMonitor<>(NAMESPACE, "write_time"))).apply(JdbcIO.<TestRow>write().withDataSourceConfiguration(JdbcIO.DataSourceConfiguration.create(dataSource)).withStatement(String.format("insert into %s values(?, ?)", tableName)).withPreparedStatementSetter(new JdbcTestHelper.PrepareStatementFromTestRow()));    return pipelineWrite.run();}
private PipelineResult beam_f32014_0()
{    PCollection<TestRow> namesAndIds = pipelineRead.apply(JdbcIO.<TestRow>read().withDataSourceConfiguration(JdbcIO.DataSourceConfiguration.create(dataSource)).withQuery(String.format("select name,id from %s;", tableName)).withRowMapper(new JdbcTestHelper.CreateTestRowOfNameAndId()).withCoder(SerializableCoder.of(TestRow.class))).apply(ParDo.of(new TimeMonitor<>(NAMESPACE, "read_time")));    PAssert.thatSingleton(namesAndIds.apply("Count All", Count.globally())).isEqualTo((long) numberOfRows);    PCollection<String> consolidatedHashcode = namesAndIds.apply(ParDo.of(new TestRow.SelectNameFn())).apply("Hash row contents", Combine.globally(new HashingFn()).withoutDefaults());    PAssert.that(consolidatedHashcode).containsInAnyOrder(TestRow.getExpectedHashForRowCount(numberOfRows));    PCollection<List<TestRow>> frontOfList = namesAndIds.apply(Top.smallest(500));    Iterable<TestRow> expectedFrontOfList = TestRow.getExpectedValues(0, 500);    PAssert.thatSingletonIterable(frontOfList).containsInAnyOrder(expectedFrontOfList);    PCollection<List<TestRow>> backOfList = namesAndIds.apply(Top.largest(500));    Iterable<TestRow> expectedBackOfList = TestRow.getExpectedValues(numberOfRows - 500, numberOfRows);    PAssert.thatSingletonIterable(backOfList).containsInAnyOrder(expectedBackOfList);    return pipelineRead.run();}
public void beam_f32022_0() throws Exception
{    String username = "sa";    String password = null;    JdbcIO.DataSourceConfiguration config = JdbcIO.DataSourceConfiguration.create("org.apache.derby.jdbc.ClientDriver", "jdbc:derby://localhost:" + port + "/target/beam").withUsername(username).withPassword(password);    try (Connection conn = config.buildDatasource().getConnection()) {        assertTrue(conn.isValid(0));    }}
public void beam_f32023_0() throws Exception
{    String username = null;    String password = null;    JdbcIO.DataSourceConfiguration config = JdbcIO.DataSourceConfiguration.create("org.apache.derby.jdbc.ClientDriver", "jdbc:derby://localhost:" + port + "/target/beam").withUsername(username).withPassword(password);    try (Connection conn = config.buildDatasource().getConnection()) {        assertTrue(conn.isValid(0));    }}
private static void beam_f32024_0(DataSource dataSource, String tableName) throws SQLException
{    try (Connection connection = dataSource.getConnection()) {        connection.setAutoCommit(false);        try (PreparedStatement preparedStatement = connection.prepareStatement(String.format("insert into %s values (?,?)", tableName))) {            for (int i = 0; i < EXPECTED_ROW_COUNT; i++) {                preparedStatement.clearParameters();                preparedStatement.setInt(1, i);                preparedStatement.setString(2, TestRow.getNameForSeed(i));                preparedStatement.executeUpdate();            }        }        connection.commit();    }}
private static JdbcIO.Write<KV<Integer, String>> beam_f32032_0(String tableName)
{    return JdbcIO.<KV<Integer, String>>write().withDataSourceConfiguration(JdbcIO.DataSourceConfiguration.create("org.apache.derby.jdbc.ClientDriver", "jdbc:derby://localhost:" + port + "/target/beam")).withStatement(String.format("insert into %s values(?, ?)", tableName)).withBatchSize(10L).withPreparedStatementSetter((element, statement) -> {        statement.setInt(1, element.getKey());        statement.setString(2, element.getValue());    });}
private static ArrayList<KV<Integer, String>> beam_f32033_0(long rowsToAdd)
{    ArrayList<KV<Integer, String>> data = new ArrayList<>();    for (int i = 0; i < rowsToAdd; i++) {        KV<Integer, String> kv = KV.of(i, "Test");        data.add(kv);    }    return data;}
private static void beam_f32034_0(String tableName, int expectedRowCount) throws SQLException
{    try (Connection connection = dataSource.getConnection()) {        try (Statement statement = connection.createStatement()) {            try (ResultSet resultSet = statement.executeQuery("select count(*) from " + tableName)) {                resultSet.next();                int count = resultSet.getInt(1);                Assert.assertEquals(expectedRowCount, count);            }        }    }}
public void beam_f32042_0() throws Exception
{    Schema schema = Schema.builder().addField("logical_date_col", LogicalTypes.JDBC_DATE_TYPE).addField("logical_time_col", LogicalTypes.JDBC_TIME_TYPE).addField("logical_time_with_tz_col", LogicalTypes.JDBC_TIMESTAMP_WITH_TIMEZONE_TYPE).build();    long epochMilli = 1558719710000L;    DateTime dateTime = new DateTime(epochMilli, ISOChronology.getInstanceUTC());    Row row = Row.withSchema(schema).addValues(dateTime.withTimeAtStartOfDay(), dateTime.withDate(new LocalDate(0L)), dateTime).build();    PreparedStatement psMocked = mock(PreparedStatement.class);    JdbcUtil.getPreparedStatementSetCaller(LogicalTypes.JDBC_DATE_TYPE).set(row, psMocked, 0, SchemaUtil.FieldWithIndex.of(schema.getField(0), 0));    JdbcUtil.getPreparedStatementSetCaller(LogicalTypes.JDBC_TIME_TYPE).set(row, psMocked, 1, SchemaUtil.FieldWithIndex.of(schema.getField(1), 1));    JdbcUtil.getPreparedStatementSetCaller(LogicalTypes.JDBC_TIMESTAMP_WITH_TIMEZONE_TYPE).set(row, psMocked, 2, SchemaUtil.FieldWithIndex.of(schema.getField(2), 2));    verify(psMocked, times(1)).setDate(1, new Date(row.getDateTime(0).getMillis()));    verify(psMocked, times(1)).setTime(2, new Time(row.getDateTime(1).getMillis()));    Calendar cal = Calendar.getInstance(TimeZone.getTimeZone("UTC"));    cal.setTimeInMillis(epochMilli);    verify(psMocked, times(1)).setTimestamp(3, new Timestamp(cal.getTime().getTime()), cal);}
public void beam_f32043_0() throws Exception
{    Schema schema = Schema.builder().addField("string_array_col", Schema.FieldType.array(Schema.FieldType.STRING)).build();    List<String> stringList = Arrays.asList("string 1", "string 2");    Row row = Row.withSchema(schema).addValues(stringList).build();    PreparedStatement psMocked = mock(PreparedStatement.class);    Connection connectionMocked = mock(Connection.class);    Array arrayMocked = mock(Array.class);    when(psMocked.getConnection()).thenReturn(connectionMocked);    when(connectionMocked.createArrayOf(anyString(), any())).thenReturn(arrayMocked);    JdbcUtil.getPreparedStatementSetCaller(Schema.FieldType.array(Schema.FieldType.STRING)).set(row, psMocked, 0, SchemaUtil.FieldWithIndex.of(schema.getField(0), 0));    verify(psMocked, times(1)).setArray(1, arrayMocked);}
private static ArrayList<Row> beam_f32044_0(long rowsToAdd, Schema schema)
{    ArrayList<Row> data = new ArrayList<>();    for (int i = 0; i < rowsToAdd; i++) {        List<Object> fields = new ArrayList<>();        Row row = schema.getFields().stream().map(field -> dummyFieldValue(field.getType())).collect(Row.toRow(schema));        data.add(row);    }    return data;}
public int beam_f32052_0()
{    return id;}
public void beam_f32053_0() throws SQLException
{    ResultSetMetaData mockResultSetMetaData = mock(ResultSetMetaData.class);    ImmutableList<JdbcFieldInfo> fieldInfo = ImmutableList.of(JdbcFieldInfo.of("int_array_col", Types.ARRAY, JDBCType.INTEGER.getName(), false), JdbcFieldInfo.of("bigint_col", Types.BIGINT), JdbcFieldInfo.of("binary_col", Types.BINARY, 255), JdbcFieldInfo.of("bit_col", Types.BIT), JdbcFieldInfo.of("boolean_col", Types.BOOLEAN), JdbcFieldInfo.of("char_col", Types.CHAR, 255), JdbcFieldInfo.of("date_col", Types.DATE), JdbcFieldInfo.of("decimal_col", Types.DECIMAL), JdbcFieldInfo.of("double_col", Types.DOUBLE), JdbcFieldInfo.of("float_col", Types.FLOAT), JdbcFieldInfo.of("integer_col", Types.INTEGER), JdbcFieldInfo.of("longnvarchar_col", Types.LONGNVARCHAR, 1024), JdbcFieldInfo.of("longvarchar_col", Types.LONGVARCHAR, 1024), JdbcFieldInfo.of("longvarbinary_col", Types.LONGVARBINARY, 1024), JdbcFieldInfo.of("nchar_col", Types.NCHAR, 255), JdbcFieldInfo.of("numeric_col", Types.NUMERIC, 12, 4), JdbcFieldInfo.of("nvarchar_col", Types.NVARCHAR, 255), JdbcFieldInfo.of("real_col", Types.REAL), JdbcFieldInfo.of("smallint_col", Types.SMALLINT), JdbcFieldInfo.of("time_col", Types.TIME), JdbcFieldInfo.of("timestamp_col", Types.TIMESTAMP), JdbcFieldInfo.of("timestamptz_col", Types.TIMESTAMP_WITH_TIMEZONE), JdbcFieldInfo.of("tinyint_col", Types.TINYINT), JdbcFieldInfo.of("varbinary_col", Types.VARBINARY, 255), JdbcFieldInfo.of("varchar_col", Types.VARCHAR, 255));    when(mockResultSetMetaData.getColumnCount()).thenReturn(fieldInfo.size());    for (int i = 0; i < fieldInfo.size(); i++) {        JdbcFieldInfo f = fieldInfo.get(i);        when(mockResultSetMetaData.getColumnLabel(eq(i + 1))).thenReturn(f.columnLabel);        when(mockResultSetMetaData.getColumnType(eq(i + 1))).thenReturn(f.columnType);        when(mockResultSetMetaData.getColumnTypeName(eq(i + 1))).thenReturn(f.columnTypeName);        when(mockResultSetMetaData.getPrecision(eq(i + 1))).thenReturn(f.precision);        when(mockResultSetMetaData.getScale(eq(i + 1))).thenReturn(f.scale);        when(mockResultSetMetaData.isNullable(eq(i + 1))).thenReturn(f.nullable ? ResultSetMetaData.columnNullable : ResultSetMetaData.columnNoNulls);    }    Schema wantBeamSchema = Schema.builder().addArrayField("int_array_col", Schema.FieldType.INT32).addField("bigint_col", Schema.FieldType.INT64).addField("binary_col", LogicalTypes.fixedLengthBytes(JDBCType.BINARY, 255)).addField("bit_col", LogicalTypes.JDBC_BIT_TYPE).addField("boolean_col", Schema.FieldType.BOOLEAN).addField("char_col", LogicalTypes.fixedLengthString(JDBCType.CHAR, 255)).addField("date_col", LogicalTypes.JDBC_DATE_TYPE).addField("decimal_col", Schema.FieldType.DECIMAL).addField("double_col", Schema.FieldType.DOUBLE).addField("float_col", LogicalTypes.JDBC_FLOAT_TYPE).addField("integer_col", Schema.FieldType.INT32).addField("longnvarchar_col", LogicalTypes.variableLengthString(JDBCType.LONGNVARCHAR, 1024)).addField("longvarchar_col", LogicalTypes.variableLengthString(JDBCType.LONGVARCHAR, 1024)).addField("longvarbinary_col", LogicalTypes.variableLengthBytes(JDBCType.LONGVARBINARY, 1024)).addField("nchar_col", LogicalTypes.fixedLengthString(JDBCType.NCHAR, 255)).addField("numeric_col", LogicalTypes.numeric(12, 4)).addField("nvarchar_col", LogicalTypes.variableLengthString(JDBCType.NVARCHAR, 255)).addField("real_col", Schema.FieldType.FLOAT).addField("smallint_col", Schema.FieldType.INT16).addField("time_col", LogicalTypes.JDBC_TIME_TYPE).addField("timestamp_col", Schema.FieldType.DATETIME).addField("timestamptz_col", LogicalTypes.JDBC_TIMESTAMP_WITH_TIMEZONE_TYPE).addField("tinyint_col", Schema.FieldType.BYTE).addField("varbinary_col", LogicalTypes.variableLengthBytes(JDBCType.VARBINARY, 255)).addField("varchar_col", LogicalTypes.variableLengthString(JDBCType.VARCHAR, 255)).build();    Schema haveBeamSchema = SchemaUtil.toBeamSchema(mockResultSetMetaData);    assertEquals(wantBeamSchema, haveBeamSchema);}
public void beam_f32054_0() throws Exception
{    ResultSet mockArrayElementsResultSet = mock(ResultSet.class);    when(mockArrayElementsResultSet.next()).thenReturn(true, true, true, false);    when(mockArrayElementsResultSet.getInt(eq(1))).thenReturn(10, 20, 30);    Array mockArray = mock(Array.class);    when(mockArray.getResultSet()).thenReturn(mockArrayElementsResultSet);    ResultSet mockResultSet = mock(ResultSet.class);    when(mockResultSet.getArray(eq(1))).thenReturn(mockArray);    Schema wantSchema = Schema.builder().addField("array", Schema.FieldType.array(Schema.FieldType.INT32)).build();    Row wantRow = Row.withSchema(wantSchema).addValues((Object) ImmutableList.of(10, 20, 30)).build();    SchemaUtil.BeamRowMapper beamRowMapper = SchemaUtil.BeamRowMapper.of(wantSchema);    Row haveRow = beamRowMapper.mapRow(mockResultSet);    assertEquals(wantRow, haveRow);}
public void beam_f32062_0()
{    assertTrue(SchemaUtil.compareSchemaField(Schema.Field.of("name", Schema.FieldType.STRING), Schema.Field.of("name", Schema.FieldType.STRING)));    assertFalse(SchemaUtil.compareSchemaField(Schema.Field.of("name", Schema.FieldType.STRING), Schema.Field.of("anotherName", Schema.FieldType.STRING)));    assertFalse(SchemaUtil.compareSchemaField(Schema.Field.of("name", Schema.FieldType.STRING), Schema.Field.of("name", Schema.FieldType.INT64)));}
public void beam_f32063_0()
{    assertTrue(SchemaUtil.compareSchemaFieldType(Schema.FieldType.STRING, Schema.FieldType.STRING));    assertFalse(SchemaUtil.compareSchemaFieldType(Schema.FieldType.STRING, Schema.FieldType.INT16));    assertTrue(SchemaUtil.compareSchemaFieldType(LogicalTypes.variableLengthString(JDBCType.VARCHAR, 255), LogicalTypes.variableLengthString(JDBCType.VARCHAR, 255)));    assertFalse(SchemaUtil.compareSchemaFieldType(LogicalTypes.variableLengthString(JDBCType.VARCHAR, 255), LogicalTypes.fixedLengthBytes(JDBCType.BIT, 255)));    assertTrue(SchemaUtil.compareSchemaFieldType(Schema.FieldType.STRING, LogicalTypes.variableLengthString(JDBCType.VARCHAR, 255)));    assertFalse(SchemaUtil.compareSchemaFieldType(Schema.FieldType.INT16, LogicalTypes.variableLengthString(JDBCType.VARCHAR, 255)));    assertTrue(SchemaUtil.compareSchemaFieldType(LogicalTypes.variableLengthString(JDBCType.VARCHAR, 255), Schema.FieldType.STRING));}
protected List<Message> beam_f32064_0()
{    return state.getMessages();}
public void beam_f32072_0(List<Message> messages)
{    atomicWrite(() -> this.messages.removeAll(messages));}
private void beam_f32073_0(Instant candidate, BiFunction<Instant, Instant, Boolean> check)
{    atomicWrite(() -> {        if (check.apply(candidate, oldestPendingTimestamp)) {            oldestPendingTimestamp = candidate;        }    });}
public T beam_f32074_0(Supplier<T> operation)
{    lock.readLock().lock();    try {        return operation.get();    } finally {        lock.readLock().unlock();    }}
public Read<T> beam_f32082_0(String topic)
{    checkArgument(topic != null, "topic can not be null");    return builder().setTopic(topic).build();}
public Read<T> beam_f32083_0(String username)
{    checkArgument(username != null, "username can not be null");    return builder().setUsername(username).build();}
public Read<T> beam_f32084_0(String password)
{    checkArgument(password != null, "password can not be null");    return builder().setPassword(password).build();}
public List<UnboundedJmsSource<T>> beam_f32092_0(int desiredNumSplits, PipelineOptions options) throws Exception
{    List<UnboundedJmsSource<T>> sources = new ArrayList<>();    if (spec.getTopic() != null) {                        sources.add(new UnboundedJmsSource<T>(spec));    } else {                for (int i = 0; i < desiredNumSplits; i++) {            sources.add(new UnboundedJmsSource<T>(spec));        }    }    return sources;}
public UnboundedJmsReader<T> beam_f32093_0(PipelineOptions options, JmsCheckpointMark checkpointMark)
{    return new UnboundedJmsReader<T>(this, checkpointMark);}
public Coder<JmsCheckpointMark> beam_f32094_0()
{    return AvroCoder.of(JmsCheckpointMark.class);}
public UnboundedSource<T, ?> beam_f32102_0()
{    return source;}
public void beam_f32103_0() throws IOException
{    try {        if (consumer != null) {            consumer.close();            consumer = null;        }        if (session != null) {            session.close();            session = null;        }        if (connection != null) {            connection.stop();            connection.close();            connection = null;        }    } catch (Exception e) {        throw new IOException(e);    }}
public Write beam_f32104_0(ConnectionFactory connectionFactory)
{    checkArgument(connectionFactory != null, "connectionFactory can not be null");    return builder().setConnectionFactory(connectionFactory).build();}
public void beam_f32112_0() throws Exception
{    producer.close();    producer = null;    session.close();    session = null;    connection.stop();    connection.close();    connection = null;}
public String beam_f32113_0()
{    return jmsMessageID;}
public long beam_f32114_0()
{    return jmsTimestamp;}
public int beam_f32122_0()
{    return jmsPriority;}
public Map<String, Object> beam_f32123_0()
{    return this.properties;}
public String beam_f32124_0()
{    return this.text;}
public void beam_f32132_0() throws Exception
{        Connection connection = connectionFactory.createConnection(USERNAME, PASSWORD);    Session session = connection.createSession(false, Session.AUTO_ACKNOWLEDGE);    MessageProducer producer = session.createProducer(session.createQueue(QUEUE));    TextMessage message = session.createTextMessage("This Is A Test");    producer.send(message);    producer.send(message);    producer.send(message);    producer.send(message);    producer.send(message);    producer.send(message);    producer.close();    session.close();    connection.close();        PCollection<JmsRecord> output = pipeline.apply(JmsIO.read().withConnectionFactory(connectionFactory).withQueue(QUEUE).withUsername(USERNAME).withPassword(PASSWORD).withMaxNumRecords(5));    PAssert.thatSingleton(output.apply("Count", Count.globally())).isEqualTo(5L);    pipeline.run();    connection = connectionFactory.createConnection(USERNAME, PASSWORD);    session = connection.createSession(false, Session.AUTO_ACKNOWLEDGE);    MessageConsumer consumer = session.createConsumer(session.createQueue(QUEUE));    Message msg = consumer.receiveNoWait();    assertNull(msg);}
public void beam_f32133_0() throws Exception
{        Connection connection = connectionFactory.createConnection(USERNAME, PASSWORD);    Session session = connection.createSession(false, Session.AUTO_ACKNOWLEDGE);    MessageProducer producer = session.createProducer(session.createQueue(QUEUE));    BytesMessage message = session.createBytesMessage();    message.writeBytes("This Is A Test".getBytes(StandardCharsets.UTF_8));    producer.send(message);    producer.close();    session.close();    connection.close();        PCollection<String> output = pipeline.apply(JmsIO.<String>readMessage().withConnectionFactory(connectionFactory).withQueue(QUEUE).withUsername(USERNAME).withPassword(PASSWORD).withMaxNumRecords(1).withCoder(SerializableCoder.of(String.class)).withMessageMapper(new BytesMessageToStringMessageMapper()));    PAssert.thatSingleton(output.apply("Count", Count.<String>globally())).isEqualTo(1L);    pipeline.run();    connection = connectionFactory.createConnection(USERNAME, PASSWORD);    session = connection.createSession(false, Session.AUTO_ACKNOWLEDGE);    MessageConsumer consumer = session.createConsumer(session.createQueue(QUEUE));    Message msg = consumer.receiveNoWait();    assertNull(msg);}
public void beam_f32134_0() throws Exception
{    ArrayList<String> data = new ArrayList<>();    for (int i = 0; i < 100; i++) {        data.add("Message " + i);    }    pipeline.apply(Create.of(data)).apply(JmsIO.write().withConnectionFactory(connectionFactory).withQueue(QUEUE).withUsername(USERNAME).withPassword(PASSWORD));    pipeline.run();    Connection connection = connectionFactory.createConnection(USERNAME, PASSWORD);    connection.start();    Session session = connection.createSession(false, Session.AUTO_ACKNOWLEDGE);    MessageConsumer consumer = session.createConsumer(session.createQueue(QUEUE));    int count = 0;    while (consumer.receive(1000) != null) {        count++;    }    assertEquals(100, count);}
private T beam_f32142_0(T target, Class<? super T> proxyInterface, String methodName, Function<MethodArgT, MethodResultT> resultTransformer)
{    return (T) Proxy.newProxyInstance(this.getClass().getClassLoader(), new Class[] { proxyInterface }, (proxy, method, args) -> {        Object result = method.invoke(target, args);        if (method.getName().equals(methodName)) {            result = resultTransformer.apply((MethodArgT) result);        }        return result;    });}
 static boolean beam_f32143_1()
{    boolean clientHasHeaders = false;    try {                clientHasHeaders = "org.apache.kafka.common.header.Headers".equals(ConsumerRecord.class.getMethod("headers", (Class<?>[]) null).getReturnType().getName());    } catch (NoSuchMethodException | SecurityException e) {            }    return clientHasHeaders;}
public void beam_f32144_0(Consumer consumer, TopicPartition topicPartition)
{    StandardEvaluationContext mapContext = new StandardEvaluationContext();    mapContext.setVariable("consumer", consumer);    mapContext.setVariable("tp", topicPartition);    seek2endExpression.getValue(mapContext);}
 Instant beam_f32152_0(PartitionContext ctx, Instant now)
{    if (maxEventTimestamp.isAfter(now)) {                return now.minus(maxDelay);    } else if (ctx.getMessageBacklog() == 0 &&     ctx.getBacklogCheckTime().minus(maxDelay).isAfter(maxEventTimestamp) && maxEventTimestamp.getMillis() > 0) {                return ctx.getBacklogCheckTime().minus(maxDelay);    } else {        return maxEventTimestamp.minus(maxDelay);    }}
public List<PartitionMark> beam_f32153_0()
{    return partitions;}
public void beam_f32154_0()
{    reader.ifPresent(r -> r.finalizeCheckpointMarkAsync(this));}
public PCollection<Void> beam_f32162_1(PCollection<ProducerRecord<K, V>> input)
{    int numShards = spec.getNumShards();    if (numShards <= 0) {        try (Consumer<?, ?> consumer = openConsumer(spec)) {            numShards = consumer.partitionsFor(spec.getTopic()).size();                    }    }    checkState(numShards > 0, "Could not set number of shards");    return input.apply(    Window.<ProducerRecord<K, V>>into(new GlobalWindows()).triggering(Repeatedly.forever(AfterPane.elementCountAtLeast(1))).discardingFiredPanes()).apply(String.format("Shuffle across %d shards", numShards), ParDo.of(new Reshard<>(numShards))).apply("Persist sharding", GroupByKey.create()).apply("Assign sequential ids", ParDo.of(new Sequencer<>())).apply("Persist ids", GroupByKey.create()).apply(String.format("Write to Kafka topic '%s'", spec.getTopic()), ParDo.of(new ExactlyOnceWriter<>(spec, input.getCoder())));}
public void beam_f32163_0()
{    shardId = ThreadLocalRandom.current().nextInt(numShards);}
public void beam_f32164_0(ProcessContext ctx)
{        shardId = (shardId + 1) % numShards;    ctx.output(KV.of(shardId, TimestampedValue.of(ctx.element(), ctx.timestamp())));}
 ShardWriter<K, V> beam_f32172_0(int shard)
{    return cache.asMap().remove(shard);}
 void beam_f32173_0(int shard, ShardWriter<K, V> writer)
{    ShardWriter<K, V> existing = cache.asMap().putIfAbsent(shard, writer);    checkState(existing == null, "Unexpected multiple instances of writers for shard %s", shard);}
public ShardWriterCache<?, ?> beam_f32174_0(String key) throws Exception
{    return new ShardWriterCache<>();}
private static Coder beam_f32182_0(Class deserializer)
{    for (Method method : deserializer.getDeclaredMethods()) {        if (method.getName().equals("deserialize")) {            Class<?> returnType = method.getReturnType();            if (returnType.equals(Object.class)) {                continue;            }            if (returnType.equals(byte[].class)) {                return ByteArrayCoder.of();            } else if (returnType.equals(Integer.class)) {                return VarIntCoder.of();            } else if (returnType.equals(Long.class)) {                return VarLongCoder.of();            } else {                throw new RuntimeException("Couldn't infer Coder from " + deserializer);            }        }    }    throw new RuntimeException("Couldn't resolve coder for Deserializer: " + deserializer);}
public Map<String, Class<? extends ExternalTransformBuilder>> beam_f32183_0()
{    return ImmutableMap.of(URN, AutoValue_KafkaIO_Read.Builder.class);}
public void beam_f32184_0(Iterable<KV<String, String>> consumerConfig)
{    this.consumerConfig = consumerConfig;}
public Read<K, V> beam_f32192_0(Class<? extends Deserializer<K>> keyDeserializer)
{    return toBuilder().setKeyDeserializer(keyDeserializer).build();}
public Read<K, V> beam_f32193_0(Class<? extends Deserializer<K>> keyDeserializer, Coder<K> keyCoder)
{    return toBuilder().setKeyDeserializer(keyDeserializer).setKeyCoder(keyCoder).build();}
public Read<K, V> beam_f32194_0(Class<? extends Deserializer<V>> valueDeserializer)
{    return toBuilder().setValueDeserializer(valueDeserializer).build();}
public Read<K, V> beam_f32202_0()
{    return withTimestampPolicyFactory(TimestampPolicyFactory.withProcessingTime());}
public Read<K, V> beam_f32203_0(Duration maxDelay)
{    return withTimestampPolicyFactory(TimestampPolicyFactory.withCreateTime(maxDelay));}
public Read<K, V> beam_f32204_0(TimestampPolicyFactory<K, V> timestampPolicyFactory)
{    return toBuilder().setTimestampPolicyFactory(timestampPolicyFactory).build();}
public Read<K, V> beam_f32212_0(Map<String, Object> configUpdates)
{    Map<String, Object> config = updateKafkaProperties(getConsumerConfig(), IGNORED_CONSUMER_PROPERTIES, configUpdates);    return toBuilder().setConsumerConfig(config).build();}
public PTransform<PBegin, PCollection<KV<K, V>>> beam_f32213_0()
{    return new TypedWithoutMetadata<>(this);}
public PCollection<KafkaRecord<K, V>> beam_f32214_1(PBegin input)
{    checkArgument(getConsumerConfig().get(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG) != null, "withBootstrapServers() is required");    checkArgument(getTopics().size() > 0 || getTopicPartitions().size() > 0, "Either withTopic(), withTopics() or withTopicPartitions() is required");    checkArgument(getKeyDeserializer() != null, "withKeyDeserializer() is required");    checkArgument(getValueDeserializer() != null, "withValueDeserializer() is required");    ConsumerSpEL consumerSpEL = new ConsumerSpEL();    if (!consumerSpEL.hasOffsetsForTimes()) {            }    if (getStartReadTime() != null) {        checkArgument(consumerSpEL.hasOffsetsForTimes(), "Consumer.offsetsForTimes is only supported by Kafka Client 0.10.1.0 onwards, " + "current version of Kafka Client is " + AppInfoParser.getVersion() + ". If you are building with maven, set \"kafka.clients.version\" " + "maven property to 0.10.1.0 or newer.");    }    if (isCommitOffsetsInFinalizeEnabled()) {        checkArgument(getConsumerConfig().get(ConsumerConfig.GROUP_ID_CONFIG) != null, "commitOffsetsInFinalize() is enabled, but group.id in Kafka consumer config " + "is not set. Offset management requires group.id.");        if (Boolean.TRUE.equals(getConsumerConfig().get(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG))) {                    }    }        CoderRegistry registry = input.getPipeline().getCoderRegistry();    Coder<K> keyCoder = getKeyCoder() != null ? getKeyCoder() : inferCoder(registry, getKeyDeserializer());    checkArgument(keyCoder != null, "Key coder could not be inferred from key deserializer. Please provide" + "key coder explicitly using withKeyDeserializerAndCoder()");    Coder<V> valueCoder = getValueCoder() != null ? getValueCoder() : inferCoder(registry, getValueDeserializer());    checkArgument(valueCoder != null, "Value coder could not be inferred from value deserializer. Please provide" + "value coder explicitly using withValueDeserializerAndCoder()");        Unbounded<KafkaRecord<K, V>> unbounded = org.apache.beam.sdk.io.Read.from(toBuilder().setKeyCoder(keyCoder).setValueCoder(valueCoder).build().makeSource());    PTransform<PBegin, PCollection<KafkaRecord<K, V>>> transform = unbounded;    if (getMaxNumRecords() < Long.MAX_VALUE || getMaxReadTime() != null) {        transform = unbounded.withMaxReadTime(getMaxReadTime()).withMaxNumRecords(getMaxNumRecords());    }    return input.getPipeline().apply(transform);}
public WriteRecords<K, V> beam_f32222_0(String bootstrapServers)
{    return withProducerConfigUpdates(ImmutableMap.of(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers));}
public WriteRecords<K, V> beam_f32223_0(String topic)
{    return toBuilder().setTopic(topic).build();}
public WriteRecords<K, V> beam_f32224_0(Class<? extends Serializer<K>> keySerializer)
{    return toBuilder().setKeySerializer(keySerializer).build();}
public WriteRecords<K, V> beam_f32232_0(SerializableFunction<Map<String, Object>, ? extends Consumer<?, ?>> consumerFactoryFn)
{    return toBuilder().setConsumerFactoryFn(consumerFactoryFn).build();}
public PDone beam_f32233_0(PCollection<ProducerRecord<K, V>> input)
{    checkArgument(getProducerConfig().get(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG) != null, "withBootstrapServers() is required");    checkArgument(getKeySerializer() != null, "withKeySerializer() is required");    checkArgument(getValueSerializer() != null, "withValueSerializer() is required");    if (isEOS()) {        checkArgument(getTopic() != null, "withTopic() is required when isEOS() is true");        KafkaExactlyOnceSink.ensureEOSSupport();                                                input.apply(new KafkaExactlyOnceSink<>(this));    } else {        input.apply(ParDo.of(new KafkaWriter<>(this)));    }    return PDone.in(input.getPipeline());}
public void beam_f32234_0(PipelineOptions options)
{    if (isEOS()) {        String runner = options.getRunner().getName();        if ("org.apache.beam.runners.direct.DirectRunner".equals(runner) || runner.startsWith("org.apache.beam.runners.dataflow.") || runner.startsWith("org.apache.beam.runners.spark.") || runner.startsWith("org.apache.beam.runners.flink.")) {            return;        }        throw new UnsupportedOperationException(runner + " is not whitelisted among runners compatible with Kafka exactly-once sink. " + "This implementation of exactly-once sink relies on specific checkpoint guarantees. " + "Only the runners with known to have compatible checkpoint semantics are whitelisted.");    }}
private Write<K, V> beam_f32242_0(WriteRecords<K, V> transform)
{    return toBuilder().setWriteRecordsTransform(transform).build();}
public Write<K, V> beam_f32243_0(String bootstrapServers)
{    return withWriteRecordsTransform(getWriteRecordsTransform().withBootstrapServers(bootstrapServers));}
public Write<K, V> beam_f32244_0(String topic)
{    return toBuilder().setTopic(topic).setWriteRecordsTransform(getWriteRecordsTransform().withTopic(topic)).build();}
public Write<K, V> beam_f32252_0(Map<String, Object> configUpdates)
{    return withWriteRecordsTransform(getWriteRecordsTransform().updateProducerProperties(configUpdates));}
public Write<K, V> beam_f32253_0(Map<String, Object> configUpdates)
{    return withWriteRecordsTransform(getWriteRecordsTransform().withProducerConfigUpdates(configUpdates));}
public PDone beam_f32254_0(PCollection<KV<K, V>> input)
{    checkArgument(getTopic() != null, "withTopic() is required");    KvCoder<K, V> kvCoder = (KvCoder<K, V>) input.getCoder();    return input.apply("Kafka ProducerRecord", MapElements.via(new SimpleFunction<KV<K, V>, ProducerRecord<K, V>>() {        @Override        public ProducerRecord<K, V> apply(KV<K, V> element) {            return new ProducerRecord<>(getTopic(), element.getKey(), element.getValue());        }    })).setCoder(ProducerRecordCoder.of(kvCoder.getKeyCoder(), kvCoder.getValueCoder())).apply(getWriteRecordsTransform());}
public void beam_f32262_0(DisplayData.Builder builder)
{    super.populateDisplayData(builder);    kvWriteTransform.populateDisplayData(builder);}
public void beam_f32263_0(T value, OutputStream outStream)
{    checkArgument(value == null, "Can only encode nulls");}
public T beam_f32264_0(InputStream inStream)
{    return null;}
public KV<K, V> beam_f32272_0()
{    return kv;}
public long beam_f32273_0()
{    return timestamp;}
public KafkaTimestampType beam_f32274_0()
{    return timestampType;}
public List<? extends Coder<?>> beam_f32282_0()
{    return kvCoder.getCoderArguments();}
public void beam_f32283_0() throws NonDeterministicException
{    kvCoder.verifyDeterministic();}
public boolean beam_f32284_0(KafkaRecord<K, V> value)
{    return kvCoder.isRegisterByteSizeObserverCheap(value.getKV());}
public CheckpointMark beam_f32292_0()
{    reportBacklog();    return new KafkaCheckpointMark(partitionStates.stream().map(p -> new PartitionMark(p.topicPartition.topic(), p.topicPartition.partition(), p.nextOffset, p.lastWatermark.getMillis())).collect(Collectors.toList()), source.getSpec().isCommitOffsetsInFinalizeEnabled() ? Optional.of(this) : Optional.empty());}
public UnboundedSource<KafkaRecord<K, V>, ?> beam_f32293_0()
{    return source;}
public KafkaRecord<K, V> beam_f32294_0() throws NoSuchElementException
{        return curRecord;}
 void beam_f32302_0(long offset, int size, long offsetGap)
{    nextOffset = offset + 1;        avgRecordSize.update(size);    avgOffsetGap.update(offsetGap);}
 synchronized void beam_f32303_1(long latestOffset, Instant fetchTime)
{    this.latestOffset = latestOffset;    this.latestOffsetFetchTime = fetchTime;    }
 synchronized long beam_f32304_0()
{        long backlogMessageCount = backlogMessageCount();    if (backlogMessageCount == UnboundedReader.BACKLOG_UNKNOWN) {        return UnboundedReader.BACKLOG_UNKNOWN;    }    return (long) (backlogMessageCount * avgRecordSize.get());}
private void beam_f32312_0(PartitionState pState)
{    Read<K, V> spec = source.getSpec();    if (pState.nextOffset != UNINITIALIZED_OFFSET) {        consumer.seek(pState.topicPartition, pState.nextOffset);    } else {                                        Instant startReadTime = spec.getStartReadTime();        if (startReadTime != null) {            pState.nextOffset = consumerSpEL.offsetForTime(consumer, pState.topicPartition, spec.getStartReadTime());            consumer.seek(pState.topicPartition, pState.nextOffset);        } else {            pState.nextOffset = consumer.position(pState.topicPartition);        }    }}
private void beam_f32313_1()
{    for (PartitionState p : partitionStates) {        try {            Instant fetchTime = Instant.now();            consumerSpEL.evaluateSeek2End(offsetConsumer, p.topicPartition);            long offset = offsetConsumer.position(p.topicPartition);            p.setLatestOffset(offset, fetchTime);        } catch (Exception e) {            if (closed.get()) {                                break;            }                            }    }    }
private void beam_f32314_0()
{    long splitBacklogBytes = getSplitBacklogBytes();    if (splitBacklogBytes < 0) {        splitBacklogBytes = UnboundedReader.BACKLOG_UNKNOWN;    }    backlogBytesOfSplit.set(splitBacklogBytes);    long splitBacklogMessages = getSplitBacklogMessageCount();    if (splitBacklogMessages < 0) {        splitBacklogMessages = UnboundedReader.BACKLOG_UNKNOWN;    }    backlogElementsOfSplit.set(splitBacklogMessages);}
public Coder<KafkaRecord<K, V>> beam_f32322_0()
{    return KafkaRecordCoder.of(spec.getKeyCoder(), spec.getValueCoder());}
 Read<K, V> beam_f32323_0()
{    return spec;}
 int beam_f32324_0()
{    return id;}
public void beam_f32332_0(ProducerRecord<K, V> value, OutputStream outStream) throws IOException
{    stringCoder.encode(value.topic(), outStream);    intCoder.encode(value.partition() != null ? value.partition() : -1, outStream);    longCoder.encode(value.timestamp() != null ? value.timestamp() : Long.MAX_VALUE, outStream);    headerCoder.encode(toIterable(value), outStream);    kvCoder.encode(KV.of(value.key(), value.value()), outStream);}
public ProducerRecord<K, V> beam_f32333_0(InputStream inStream) throws IOException
{    String topic = stringCoder.decode(inStream);    Integer partition = intCoder.decode(inStream);    if (partition == -1) {        partition = null;    }    Long timestamp = longCoder.decode(inStream);    if (timestamp == Long.MAX_VALUE) {        timestamp = null;    }    Headers headers = (Headers) toHeaders(headerCoder.decode(inStream));    KV<K, V> kv = kvCoder.decode(inStream);    if (ConsumerSpEL.hasHeaders()) {        return new ProducerRecord<>(topic, partition, timestamp, kv.getKey(), kv.getValue(), headers);    }    return new ProducerRecord<>(topic, partition, timestamp, kv.getKey(), kv.getValue());}
private Object beam_f32334_0(Iterable<KV<String, byte[]>> records)
{    if (!ConsumerSpEL.hasHeaders()) {        return null;    }        ConsumerRecord<String, String> consumerRecord = new ConsumerRecord<>("", 0, 0L, "", "");    records.forEach(kv -> consumerRecord.headers().add(kv.getKey(), kv.getValue()));    return consumerRecord.headers();}
private static void beam_f32342_0()
{    checkArgument(supportsTransactions(), "This version of Kafka client library does not support transactions. ", "Please used version 0.11 or later.");}
private static void beam_f32343_0(Method method, Object obj, Object... args)
{    try {        method.invoke(obj, args);    } catch (IllegalAccessException | InvocationTargetException e) {        throw new RuntimeException(e);    } catch (ApiException e) {        Class<?> eClass = e.getClass();        if (producerFencedExceptionClass.isAssignableFrom(eClass) || outOfOrderSequenceExceptionClass.isAssignableFrom(eClass) || AuthorizationException.class.isAssignableFrom(eClass)) {            throw new UnrecoverableProducerException(e);        }        throw e;    }}
 static void beam_f32344_0(Producer<?, ?> producer)
{    ensureTransactionsSupport();    invoke(initTransactionsMethod, producer);}
 static TimestampPolicyFactory<K, V> beam_f32356_0()
{    return (tp, previousWatermark) -> new LogAppendTimePolicy<>(previousWatermark);}
 static TimestampPolicyFactory<K, V> beam_f32357_0(Duration maxDelay)
{    SerializableFunction<KafkaRecord<K, V>, Instant> timestampFunction = record -> {        checkArgument(record.getTimestampType() == KafkaTimestampType.CREATE_TIME, "Kafka record's timestamp is not 'CREATE_TIME' " + "(topic: %s, partition %s, offset %s, timestamp type '%s')", record.getTopic(), record.getPartition(), record.getOffset(), record.getTimestampType());        return new Instant(record.getTimestamp());    };    return (tp, previousWatermark) -> new CustomTimestampPolicyWithLimitedDelay<>(timestampFunction, maxDelay, previousWatermark);}
 static TimestampPolicyFactory<K, V> beam_f32358_0(final SerializableFunction<KafkaRecord<K, V>, Instant> timestampFn)
{    return (tp, previousWatermark) -> new TimestampFnPolicy<>(timestampFn, previousWatermark);}
public void beam_f32366_0()
{        Duration maxDelay = Duration.standardSeconds(60);    CustomTimestampPolicyWithLimitedDelay<String, String> policy = new CustomTimestampPolicyWithLimitedDelay<>((record -> new Instant(record.getTimestamp())), maxDelay, Optional.empty());    Instant now = Instant.now();    TimestampPolicy.PartitionContext ctx = mock(TimestampPolicy.PartitionContext.class);    when(ctx.getMessageBacklog()).thenReturn(100L);    when(ctx.getBacklogCheckTime()).thenReturn(now);    assertThat(policy.getWatermark(ctx), is(BoundedWindow.TIMESTAMP_MIN_VALUE));        List<Long> input = ImmutableList.of(    -200_000L,     -150_000L,     -120_000L,     -140_000L,     -100_000L, -110_000L);    assertThat(getTimestampsForRecords(policy, now, input), is(input));        assertThat(policy.getWatermark(ctx), is(now.minus(Duration.standardSeconds(100)).minus(maxDelay)));        input = ImmutableList.of(    -200_000L,     -150_000L,     -120_000L,     -140_000L,     100_000L, -100_000L, -110_000L);    assertThat(getTimestampsForRecords(policy, now, input), is(input));        assertThat(policy.getWatermark(ctx, now), is(now.minus(maxDelay)));            now = now.plus(Duration.standardMinutes(5));    Instant backlogCheckTime = now.minus(Duration.standardSeconds(10));    when(ctx.getMessageBacklog()).thenReturn(0L);    when(ctx.getBacklogCheckTime()).thenReturn(backlogCheckTime);    assertThat(policy.getWatermark(ctx, now), is(backlogCheckTime.minus(maxDelay)));}
public void beam_f32367_0() throws Exception
{    List<String> topics = ImmutableList.of("topic1", "topic2");    String keyDeserializer = "org.apache.kafka.common.serialization.ByteArrayDeserializer";    String valueDeserializer = "org.apache.kafka.common.serialization.LongDeserializer";    ImmutableMap<String, String> consumerConfig = ImmutableMap.<String, String>builder().put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, "server1:port,server2:port").put("key2", "value2").put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, keyDeserializer).put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, valueDeserializer).build();    ExternalTransforms.ExternalConfigurationPayload payload = ExternalTransforms.ExternalConfigurationPayload.newBuilder().putConfiguration("topics", ExternalTransforms.ConfigValue.newBuilder().addCoderUrn("beam:coder:iterable:v1").addCoderUrn("beam:coder:string_utf8:v1").setPayload(ByteString.copyFrom(listAsBytes(topics))).build()).putConfiguration("consumer_config", ExternalTransforms.ConfigValue.newBuilder().addCoderUrn("beam:coder:iterable:v1").addCoderUrn("beam:coder:kv:v1").addCoderUrn("beam:coder:string_utf8:v1").addCoderUrn("beam:coder:string_utf8:v1").setPayload(ByteString.copyFrom(mapAsBytes(consumerConfig))).build()).putConfiguration("key_deserializer", ExternalTransforms.ConfigValue.newBuilder().addCoderUrn("beam:coder:string_utf8:v1").setPayload(ByteString.copyFrom(encodeString(keyDeserializer))).build()).putConfiguration("value_deserializer", ExternalTransforms.ConfigValue.newBuilder().addCoderUrn("beam:coder:string_utf8:v1").setPayload(ByteString.copyFrom(encodeString(valueDeserializer))).build()).build();    RunnerApi.Components defaultInstance = RunnerApi.Components.getDefaultInstance();    ExpansionApi.ExpansionRequest request = ExpansionApi.ExpansionRequest.newBuilder().setComponents(defaultInstance).setTransform(RunnerApi.PTransform.newBuilder().setUniqueName("test").setSpec(RunnerApi.FunctionSpec.newBuilder().setUrn("beam:external:java:kafka:read:v1").setPayload(payload.toByteString()))).setNamespace("test_namespace").build();    ExpansionService expansionService = new ExpansionService();    TestStreamObserver<ExpansionApi.ExpansionResponse> observer = new TestStreamObserver<>();    expansionService.expand(request, observer);    ExpansionApi.ExpansionResponse result = observer.result;    RunnerApi.PTransform transform = result.getTransform();    assertThat(transform.getSubtransformsList(), Matchers.contains("test_namespacetest/KafkaIO.Read", "test_namespacetest/Remove Kafka Metadata"));    assertThat(transform.getInputsCount(), Matchers.is(0));    assertThat(transform.getOutputsCount(), Matchers.is(1));    RunnerApi.PTransform kafkaComposite = result.getComponents().getTransformsOrThrow(transform.getSubtransforms(0));    RunnerApi.PTransform kafkaRead = result.getComponents().getTransformsOrThrow(kafkaComposite.getSubtransforms(0));    RunnerApi.ReadPayload readPayload = RunnerApi.ReadPayload.parseFrom(kafkaRead.getSpec().getPayload());    KafkaUnboundedSource source = (KafkaUnboundedSource) ReadTranslation.unboundedSourceFromProto(readPayload);    KafkaIO.Read spec = source.getSpec();    assertThat(spec.getConsumerConfig(), Matchers.is(consumerConfig));    assertThat(spec.getTopics(), Matchers.is(topics));    assertThat(spec.getKeyDeserializer().getName(), Matchers.is(keyDeserializer));    assertThat(spec.getValueDeserializer().getName(), Matchers.is(valueDeserializer));}
public void beam_f32368_0() throws Exception
{    String topic = "topic";    String keySerializer = "org.apache.kafka.common.serialization.ByteArraySerializer";    String valueSerializer = "org.apache.kafka.common.serialization.LongSerializer";    ImmutableMap<String, String> producerConfig = ImmutableMap.<String, String>builder().put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "server1:port,server2:port").put("retries", "3").build();    ExternalTransforms.ExternalConfigurationPayload payload = ExternalTransforms.ExternalConfigurationPayload.newBuilder().putConfiguration("topic", ExternalTransforms.ConfigValue.newBuilder().addCoderUrn("beam:coder:string_utf8:v1").setPayload(ByteString.copyFrom(encodeString(topic))).build()).putConfiguration("producer_config", ExternalTransforms.ConfigValue.newBuilder().addCoderUrn("beam:coder:iterable:v1").addCoderUrn("beam:coder:kv:v1").addCoderUrn("beam:coder:string_utf8:v1").addCoderUrn("beam:coder:string_utf8:v1").setPayload(ByteString.copyFrom(mapAsBytes(producerConfig))).build()).putConfiguration("key_serializer", ExternalTransforms.ConfigValue.newBuilder().addCoderUrn("beam:coder:string_utf8:v1").setPayload(ByteString.copyFrom(encodeString(keySerializer))).build()).putConfiguration("value_serializer", ExternalTransforms.ConfigValue.newBuilder().addCoderUrn("beam:coder:string_utf8:v1").setPayload(ByteString.copyFrom(encodeString(valueSerializer))).build()).build();    Pipeline p = Pipeline.create();    p.apply(Impulse.create()).apply(WithKeys.of("key"));    RunnerApi.Pipeline pipelineProto = PipelineTranslation.toProto(p);    String inputPCollection = Iterables.getOnlyElement(Iterables.getLast(pipelineProto.getComponents().getTransformsMap().values()).getOutputsMap().values());    ExpansionApi.ExpansionRequest request = ExpansionApi.ExpansionRequest.newBuilder().setComponents(pipelineProto.getComponents()).setTransform(RunnerApi.PTransform.newBuilder().setUniqueName("test").putInputs("input", inputPCollection).setSpec(RunnerApi.FunctionSpec.newBuilder().setUrn("beam:external:java:kafka:write:v1").setPayload(payload.toByteString()))).setNamespace("test_namespace").build();    ExpansionService expansionService = new ExpansionService();    TestStreamObserver<ExpansionApi.ExpansionResponse> observer = new TestStreamObserver<>();    expansionService.expand(request, observer);    ExpansionApi.ExpansionResponse result = observer.result;    RunnerApi.PTransform transform = result.getTransform();    assertThat(transform.getSubtransformsList(), Matchers.contains("test_namespacetest/Kafka ProducerRecord", "test_namespacetest/KafkaIO.WriteRecords"));    assertThat(transform.getInputsCount(), Matchers.is(1));    assertThat(transform.getOutputsCount(), Matchers.is(0));    RunnerApi.PTransform writeComposite = result.getComponents().getTransformsOrThrow(transform.getSubtransforms(1));    RunnerApi.PTransform writeParDo = result.getComponents().getTransformsOrThrow(result.getComponents().getTransformsOrThrow(writeComposite.getSubtransforms(0)).getSubtransforms(0));    RunnerApi.ParDoPayload parDoPayload = RunnerApi.ParDoPayload.parseFrom(writeParDo.getSpec().getPayload());    DoFn kafkaWriter = ParDoTranslation.getDoFn(parDoPayload);    assertThat(kafkaWriter, Matchers.instanceOf(KafkaWriter.class));    KafkaIO.WriteRecords spec = (KafkaIO.WriteRecords) Whitebox.getInternalState(kafkaWriter, "spec");    assertThat(spec.getProducerConfig(), Matchers.is(producerConfig));    assertThat(spec.getTopic(), Matchers.is(topic));    assertThat(spec.getKeySerializer().getName(), Matchers.is(keySerializer));    assertThat(spec.getValueSerializer().getName(), Matchers.is(valueSerializer));}
private Set<NamedTestResult> beam_f32377_0(PipelineResult writeResult, PipelineResult readResult)
{    BiFunction<MetricsReader, String, NamedTestResult> supplier = (reader, metricName) -> {        long start = reader.getStartTimeMetric(metricName);        long end = reader.getEndTimeMetric(metricName);        return NamedTestResult.create(TEST_ID, TIMESTAMP, metricName, (end - start) / 1e3);    };    NamedTestResult writeTime = supplier.apply(new MetricsReader(writeResult, NAMESPACE), WRITE_TIME_METRIC_NAME);    NamedTestResult readTime = supplier.apply(new MetricsReader(readResult, NAMESPACE), READ_TIME_METRIC_NAME);    NamedTestResult runTime = NamedTestResult.create(TEST_ID, TIMESTAMP, RUN_TIME_METRIC_NAME, writeTime.getValue() + readTime.getValue());    return ImmutableSet.of(readTime, writeTime, runTime);}
private void beam_f32378_0(PipelineResult readResult, PipelineResult.State readState) throws IOException
{    if (!readState.isTerminal()) {        readResult.cancel();    }}
private KafkaIO.Write<byte[], byte[]> beam_f32379_0()
{    return KafkaIO.<byte[], byte[]>write().withBootstrapServers(options.getKafkaBootstrapServerAddress()).withTopic(options.getKafkaTopic()).withKeySerializer(ByteArraySerializer.class).withValueSerializer(ByteArraySerializer.class);}
private static KafkaIO.Read<Integer, Long> beam_f32387_0(int numElements, @Nullable SerializableFunction<KV<Integer, Long>, Instant> timestampFn)
{    return mkKafkaReadTransform(numElements, numElements, timestampFn);}
private static KafkaIO.Read<Integer, Long> beam_f32388_0(int numElements, int maxNumRecords, @Nullable SerializableFunction<KV<Integer, Long>, Instant> timestampFn)
{    List<String> topics = ImmutableList.of("topic_a", "topic_b");    KafkaIO.Read<Integer, Long> reader = KafkaIO.<Integer, Long>read().withBootstrapServers("myServer1:9092,myServer2:9092").withTopics(topics).withConsumerFactoryFn(new ConsumerFactoryFn(topics, 10, numElements,     OffsetResetStrategy.EARLIEST)).withKeyDeserializer(IntegerDeserializer.class).withValueDeserializer(LongDeserializer.class).withMaxNumRecords(maxNumRecords);    if (timestampFn != null) {        return reader.withTimestampFn(timestampFn);    } else {        return reader;    }}
public Void beam_f32389_0(Iterable<Long> values)
{    for (Long v : values) {        assertEquals(0, v % num);    }    return null;}
public void beam_f32397_0()
{    int numElements = 1000;    PCollection<Long> input = p.apply(mkKafkaReadTransform(numElements, new ValueAsTimestampFn()).withoutMetadata()).apply(Values.create());    addCountingAsserts(input, numElements);    PCollection<Long> diffs = input.apply("TimestampDiff", ParDo.of(new ElementValueDiff())).apply("DistinctTimestamps", Distinct.create());        PAssert.thatSingleton(diffs).isEqualTo(0L);    p.run();}
public void beam_f32398_0()
{            int numElements = 1000;    PCollection<Long> input = p.apply(mkKafkaReadTransform(numElements, null).withLogAppendTime().withoutMetadata()).apply(Values.create());    addCountingAsserts(input, numElements);    PCollection<Long> diffs = input.apply(MapElements.into(TypeDescriptors.longs()).via(t -> LOG_APPEND_START_TIME.plus(Duration.standardSeconds(t)).getMillis())).apply("TimestampDiff", ParDo.of(new ElementValueDiff())).apply("DistinctTimestamps", Distinct.create());        PAssert.thatSingleton(diffs).isEqualTo(0L);    p.run();}
public void beam_f32399_0()
{            final int numElements = 1000;    final long customTimestampStartMillis = 80000L;    PCollection<Long> input = p.apply(mkKafkaReadTransform(numElements, null).withTimestampPolicyFactory((tp, prevWatermark) -> new CustomTimestampPolicyWithLimitedDelay<Integer, Long>((record -> new Instant(TimeUnit.SECONDS.toMillis(record.getKV().getValue()) + customTimestampStartMillis)), Duration.ZERO, prevWatermark)).withoutMetadata()).apply(Values.create());    addCountingAsserts(input, numElements);    PCollection<Long> diffs = input.apply(MapElements.into(TypeDescriptors.longs()).via(t -> TimeUnit.SECONDS.toMillis(t) + customTimestampStartMillis)).apply("TimestampDiff", ParDo.of(new ElementValueDiff())).apply("DistinctTimestamps", Distinct.create());        PAssert.thatSingleton(diffs).isEqualTo(0L);    p.run();}
public void beam_f32407_0() throws Exception
{    int numElements = 1000;    int numSplits = 10;            UnboundedSource<KafkaRecord<Integer, Long>, ?> initial = mkKafkaReadTransform(numElements, null).withKeyDeserializerAndCoder(IntegerDeserializer.class, BigEndianIntegerCoder.of()).withValueDeserializerAndCoder(LongDeserializer.class, BigEndianLongCoder.of()).makeSource();    List<? extends UnboundedSource<KafkaRecord<Integer, Long>, ?>> splits = initial.split(numSplits, p.getOptions());    assertEquals("Expected exact splitting", numSplits, splits.size());    long elementsPerSplit = numElements / numSplits;    assertEquals("Expected even splits", numElements, elementsPerSplit * numSplits);    PCollectionList<Long> pcollections = PCollectionList.empty(p);    for (int i = 0; i < splits.size(); ++i) {        pcollections = pcollections.and(p.apply("split" + i, Read.from(splits.get(i)).withMaxNumRecords(elementsPerSplit)).apply("Remove Metadata " + i, ParDo.of(new RemoveKafkaMetadata<>())).apply("collection " + i, Values.create()));    }    PCollection<Long> input = pcollections.apply(Flatten.pCollections());    addCountingAsserts(input, numElements);    p.run();}
public Instant beam_f32408_0(KV<Integer, Long> input)
{    return new Instant(input.getValue());}
private static void beam_f32409_0(UnboundedReader<?> reader, boolean isStarted) throws IOException
{    if (!isStarted && reader.start()) {        return;    }    while (!reader.advance()) {                try {            Thread.sleep(1);        } catch (InterruptedException e) {            throw new RuntimeException(e);        }    }}
public void beam_f32417_0() throws Exception
{    int numElements = 1000;    try (MockProducerWrapper producerWrapper = new MockProducerWrapper()) {        ProducerSendCompletionThread completionThread = new ProducerSendCompletionThread(producerWrapper.mockProducer).start();        final String defaultTopic = "test";        final Long ts = System.currentTimeMillis();        p.apply(mkKafkaReadTransform(numElements, new ValueAsTimestampFn()).withoutMetadata()).apply(ParDo.of(new KV2ProducerRecord(defaultTopic, ts))).setCoder(ProducerRecordCoder.of(VarIntCoder.of(), VarLongCoder.of())).apply(KafkaIO.<Integer, Long>writeRecords().withBootstrapServers("none").withKeySerializer(IntegerSerializer.class).withValueSerializer(LongSerializer.class).withProducerFactoryFn(new ProducerFactoryFn(producerWrapper.producerKey)));        p.run();        completionThread.shutdown();                List<ProducerRecord<Integer, Long>> sent = producerWrapper.mockProducer.history();        for (int i = 0; i < numElements; i++) {            ProducerRecord<Integer, Long> record = sent.get(i);            assertEquals(defaultTopic, record.topic());            assertEquals(i, record.key().intValue());            assertEquals(i, record.value().longValue());            assertEquals(ts, record.timestamp());        }    }}
public void beam_f32418_0(ProcessContext ctx)
{    KV<Integer, Long> kv = ctx.element();    if (isSingleTopic) {        ctx.output(new ProducerRecord<>(topic, null, ts, kv.getKey(), kv.getValue()));    } else {        if (kv.getKey() % 2 == 0) {            ctx.output(new ProducerRecord<>(topic + "_2", null, ts, kv.getKey(), kv.getValue()));        } else {            ctx.output(new ProducerRecord<>(topic + "_1", null, ts, kv.getKey(), kv.getValue()));        }    }}
public void beam_f32419_1()
{    if (!ProducerSpEL.supportsTransactions()) {                return;    }    int numElements = 1000;    try (MockProducerWrapper producerWrapper = new MockProducerWrapper()) {        ProducerSendCompletionThread completionThread = new ProducerSendCompletionThread(producerWrapper.mockProducer).start();        String topic = "test";        p.apply(mkKafkaReadTransform(numElements, new ValueAsTimestampFn()).withoutMetadata()).apply(KafkaIO.<Integer, Long>write().withBootstrapServers("none").withTopic(topic).withKeySerializer(IntegerSerializer.class).withValueSerializer(LongSerializer.class).withEOS(1, "test").withConsumerFactoryFn(new ConsumerFactoryFn(Lists.newArrayList(topic), 10, 10, OffsetResetStrategy.EARLIEST)).withPublishTimestampFunction((e, ts) -> ts).withProducerFactoryFn(new ProducerFactoryFn(producerWrapper.producerKey)));        p.run();        completionThread.shutdown();        verifyProducerRecords(producerWrapper.mockProducer, topic, numElements, false, true);    }}
public NonInferableObject beam_f32430_0(String topic, byte[] bytes)
{    return new NonInferableObject();}
public void beam_f32432_0()
{    CoderRegistry registry = CoderRegistry.createDefault();    assertTrue(KafkaIO.inferCoder(registry, LongDeserializer.class).getValueCoder() instanceof VarLongCoder);    assertTrue(KafkaIO.inferCoder(registry, StringDeserializer.class).getValueCoder() instanceof StringUtf8Coder);    assertTrue(KafkaIO.inferCoder(registry, InstantDeserializer.class).getValueCoder() instanceof InstantCoder);    assertTrue(KafkaIO.inferCoder(registry, DeserializerWithInterfaces.class).getValueCoder() instanceof VarLongCoder);}
public void beam_f32433_0() throws Exception
{    cannotInferException.expect(RuntimeException.class);    CoderRegistry registry = CoderRegistry.createDefault();    KafkaIO.inferCoder(registry, NonInferableObjectDeserializer.class);}
 void beam_f32441_0()
{    done.set(true);    injectorThread.shutdown();    try {        assertTrue(injectorThread.awaitTermination(10, TimeUnit.SECONDS));    } catch (InterruptedException e) {        Thread.currentThread().interrupt();        throw new RuntimeException(e);    }}
public void beam_f32442_0()
{    CoderProperties.coderSerializable(KafkaRecordCoder.of(GlobalWindow.Coder.INSTANCE, GlobalWindow.Coder.INSTANCE));}
public void beam_f32443_0() throws IOException
{    RecordHeaders headers = new RecordHeaders();    headers.add("headerKey", "headerVal".getBytes(StandardCharsets.UTF_8));    verifySerialization(headers);}
public void beam_f32451_0() throws IOException
{    long timestamp = System.currentTimeMillis();    ProducerRecord<String, String> decodedRecord = verifySerialization(1, timestamp);    assertEquals(timestamp, decodedRecord.timestamp().longValue());}
public void beam_f32452_0() throws IOException
{    ProducerRecord<String, String> decodedRecord = verifySerialization(1, null);    assertNull(decodedRecord.timestamp());}
public void beam_f32453_0() throws IOException
{    RecordHeaders headers = new RecordHeaders();    headers.add("headerKey", "headerVal".getBytes(UTF_8));    ProducerRecordCoder producerRecordCoder = ProducerRecordCoder.of(ByteArrayCoder.of(), ByteArrayCoder.of());    ProducerRecord<byte[], byte[]> producerRecord = new ProducerRecord<>("topic", 1, null, "key".getBytes(UTF_8), "value".getBytes(UTF_8), headers);    ProducerRecord testProducerRecord = (ProducerRecord) producerRecordCoder.structuralValue(producerRecord);    assertEquals(testProducerRecord.headers(), headers);}
public static CustomOptional<T> beam_f32461_0()
{    return (Absent<T>) Absent.INSTANCE;}
public static CustomOptional<T> beam_f32462_0(T v)
{    return new Present<>(v);}
public boolean beam_f32463_0()
{    return true;}
public KinesisReaderCheckpoint beam_f32471_1(SimplifiedKinesisClient kinesis) throws TransientKinesisException
{    Set<Shard> shardsAtStartingPoint = startingPointShardsFinder.findShardsAtStartingPoint(kinesis, streamName, startingPoint);        return new KinesisReaderCheckpoint(shardsAtStartingPoint.stream().map(shard -> new ShardCheckpoint(streamName, shard.getShardId(), startingPoint)).collect(Collectors.toList()));}
public String beam_f32472_0()
{    return String.format("Checkpoint generator for %s: %s", streamName, startingPoint);}
public List<KinesisRecord> beam_f32473_0()
{    return records;}
public Read beam_f32481_0(AWSClientsProvider awsClientsProvider)
{    return toBuilder().setAWSClientsProvider(awsClientsProvider).build();}
public Read beam_f32482_0(String awsAccessKey, String awsSecretKey, Regions region)
{    return withAWSClientsProvider(awsAccessKey, awsSecretKey, region, null);}
public Read beam_f32483_0(String awsAccessKey, String awsSecretKey, Regions region, String serviceEndpoint)
{    return withAWSClientsProvider(new BasicKinesisProvider(awsAccessKey, awsSecretKey, region, serviceEndpoint));}
public Read beam_f32491_0(WatermarkPolicyFactory watermarkPolicyFactory)
{    checkArgument(watermarkPolicyFactory != null, "watermarkPolicyFactory cannot be null");    return toBuilder().setWatermarkPolicyFactory(watermarkPolicyFactory).build();}
public PCollection<KinesisRecord> beam_f32492_0(PBegin input)
{    Unbounded<KinesisRecord> unbounded = org.apache.beam.sdk.io.Read.from(new KinesisSource(getAWSClientsProvider(), getStreamName(), getInitialPosition(), getUpToDateThreshold(), getWatermarkPolicyFactory(), getRequestRecordsLimit()));    PTransform<PBegin, PCollection<KinesisRecord>> transform = unbounded;    if (getMaxNumRecords() < Long.MAX_VALUE || getMaxReadTime() != null) {        transform = unbounded.withMaxReadTime(getMaxReadTime()).withMaxNumRecords(getMaxNumRecords());    }    return input.apply(transform);}
public Write beam_f32493_0(String streamName)
{    return builder().setStreamName(streamName).build();}
public PDone beam_f32501_0(PCollection<byte[]> input)
{    checkArgument(getStreamName() != null, "withStreamName() is required");    checkArgument((getPartitionKey() != null) || (getPartitioner() != null), "withPartitionKey() or withPartitioner() is required");    checkArgument(getPartitionKey() == null || (getPartitioner() == null), "only one of either withPartitionKey() or withPartitioner() is possible");    checkArgument(getAWSClientsProvider() != null, "withAWSClientsProvider() is required");    input.apply(ParDo.of(new KinesisWriterFn(this)));    return PDone.in(input.getPipeline());}
public void beam_f32502_0()
{        if (spec.getPartitioner() != null) {        partitioner = spec.getPartitioner();    }}
public void beam_f32503_0()
{    putFutures = Collections.synchronizedList(new ArrayList<>());    /**     * Keep only the first {@link MAX_NUM_FAILURES} occurred exceptions     */    failures = new LinkedBlockingDeque<>(MAX_NUM_FAILURES);}
public byte[] beam_f32511_0() throws NoSuchElementException
{    return currentRecord.get().getUniqueId();}
public KinesisRecord beam_f32512_0() throws NoSuchElementException
{    return currentRecord.get();}
public Instant beam_f32513_0() throws NoSuchElementException
{    return currentRecord.get().getApproximateArrivalTimestamp();}
private int beam_f32521_0(int nominator, int denominator)
{    return (nominator + denominator - 1) / denominator;}
public String beam_f32523_0()
{    return shardCheckpoints.toString();}
public Iterator<ShardCheckpoint> beam_f32524_0()
{    return shardCheckpoints.iterator();}
public int beam_f32532_0()
{    return reflectionHashCode(this);}
public long beam_f32533_0()
{    return subSequenceNumber;}
public String beam_f32534_0()
{    return sequenceNumber;}
public UnboundedReader<KinesisRecord> beam_f32542_1(PipelineOptions options, KinesisReaderCheckpoint checkpointMark)
{    CheckpointGenerator checkpointGenerator = initialCheckpointGenerator;    if (checkpointMark != null) {        checkpointGenerator = new StaticCheckpointGenerator(checkpointMark);    }        return new KinesisReader(SimplifiedKinesisClient.from(awsClientsProvider, limit), checkpointGenerator, this, watermarkPolicyFactory, upToDateThreshold);}
public Coder<KinesisReaderCheckpoint> beam_f32543_0()
{    return SerializableCoder.of(KinesisReaderCheckpoint.class);}
public void beam_f32544_0()
{    checkNotNull(awsClientsProvider);    checkNotNull(initialCheckpointGenerator);}
private boolean beam_f32552_0()
{    return shardIteratorType == AFTER_SEQUENCE_NUMBER && subSequenceNumber != null;}
public ShardCheckpoint beam_f32553_0(KinesisRecord record)
{    return new ShardCheckpoint(streamName, shardId, AFTER_SEQUENCE_NUMBER, record.getSequenceNumber(), record.getSubSequenceNumber());}
public String beam_f32554_0()
{    return streamName;}
 Instant beam_f32562_0()
{    return shardIteratorsMap.get().values().stream().map(ShardRecordsIterator::getShardWatermark).min(Comparator.naturalOrder()).orElse(BoundedWindow.TIMESTAMP_MAX_VALUE);}
 KinesisReaderCheckpoint beam_f32563_0()
{    ImmutableMap<String, ShardRecordsIterator> currentShardIterators = shardIteratorsMap.get();    return new KinesisReaderCheckpoint(currentShardIterators.values().stream().map(shardRecordsIterator -> {        checkArgument(shardRecordsIterator != null, "shardRecordsIterator can not be null");        return shardRecordsIterator.getCheckpoint();    }).collect(Collectors.toList()));}
 ShardRecordsIterator beam_f32564_0(SimplifiedKinesisClient kinesis, ShardCheckpoint checkpoint) throws TransientKinesisException
{    return new ShardRecordsIterator(checkpoint, kinesis, watermarkPolicyFactory);}
 Instant beam_f32572_0()
{    return watermarkPolicy.get().getWatermark();}
 String beam_f32573_0()
{    return shardId;}
 List<ShardRecordsIterator> beam_f32574_0() throws TransientKinesisException
{    List<Shard> shards = kinesis.listShards(streamName);    List<ShardRecordsIterator> successiveShardRecordIterators = new ArrayList<>();    for (Shard shard : shards) {        if (shardId.equals(shard.getParentShardId())) {            ShardCheckpoint shardCheckpoint = new ShardCheckpoint(streamName, shard.getShardId(), new StartingPoint(InitialPositionInStream.TRIM_HORIZON));            successiveShardRecordIterators.add(new ShardRecordsIterator(shardCheckpoint, kinesis, watermarkPolicyFactory));        }    }    return successiveShardRecordIterators;}
 GetMetricStatisticsRequest beam_f32582_0(String streamName, Instant countSince, Instant countTo, Minutes period)
{    return new GetMetricStatisticsRequest().withNamespace(KINESIS_NAMESPACE).withMetricName(INCOMING_RECORDS_METRIC).withPeriod(period.getMinutes() * PERIOD_GRANULARITY_IN_SECONDS).withStartTime(countSince.toDate()).withEndTime(countTo.toDate()).withStatistics(Collections.singletonList(SUM_STATISTIC)).withDimensions(Collections.singletonList(new Dimension().withName(STREAM_NAME_DIMENSION).withValue(streamName)));}
private T beam_f32583_0(Callable<T> callable) throws TransientKinesisException
{    try {        return callable.call();    } catch (ExpiredIteratorException e) {        throw e;    } catch (LimitExceededException | ProvisionedThroughputExceededException e) {        throw new TransientKinesisException("Too many requests to Kinesis. Wait some time and retry.", e);    } catch (AmazonServiceException e) {        if (e.getErrorType() == AmazonServiceException.ErrorType.Service) {            throw new TransientKinesisException("Kinesis backend failed. Wait some time and retry.", e);        }        throw new RuntimeException("Kinesis client side failure", e);    } catch (AmazonClientException e) {        if (e.isRetryable()) {            throw new TransientKinesisException("Retryable client failure", e);        }        throw new RuntimeException("Not retryable client failure", e);    } catch (Exception e) {        throw new RuntimeException("Unknown kinesis failure, when trying to reach kinesis", e);    }}
public InitialPositionInStream beam_f32584_0()
{    return position;}
private Set<Shard> beam_f32592_1(String streamName, List<Shard> allShards)
{    Set<String> shardIds = new HashSet<>();    for (Shard shard : allShards) {        shardIds.add(shard.getShardId());    }        Set<Shard> shardsWithoutParents = new HashSet<>();    for (Shard shard : allShards) {        if (!shardIds.contains(shard.getParentShardId())) {            shardsWithoutParents.add(shard);        }    }    return shardsWithoutParents;}
private Set<Shard> beam_f32593_0(SimplifiedKinesisClient kinesis, Iterable<Shard> rootShards, String streamName, StartingPoint startingPoint) throws TransientKinesisException
{    Set<Shard> validShards = new HashSet<>();    ShardIteratorType shardIteratorType = ShardIteratorType.fromValue(startingPoint.getPositionName());    for (Shard shard : rootShards) {        String shardIterator = kinesis.getShardIterator(streamName, shard.getShardId(), shardIteratorType, null, startingPoint.getTimestamp());        GetKinesisRecordsResult records = kinesis.getRecords(shardIterator, streamName, shard.getShardId());        if (records.getNextShardIterator() != null || !records.getRecords().isEmpty()) {            validShards.add(shard);        }    }    return validShards;}
public KinesisReaderCheckpoint beam_f32594_0(SimplifiedKinesisClient client)
{    return checkpoint;}
 static WatermarkPolicyFactory beam_f32602_0()
{    return ProcessingTimeWatermarkPolicy::new;}
 static WatermarkPolicyFactory beam_f32603_0(WatermarkParameters watermarkParameters)
{    return () -> new CustomWatermarkPolicy(watermarkParameters);}
public Instant beam_f32604_0()
{    return watermarkPolicy.getWatermark();}
public String beam_f32613_0()
{    return "TestData{" + "data='" + data + '\'' + ", arrivalTimestamp=" + arrivalTimestamp + ", sequenceNumber='" + sequenceNumber + '\'' + '}';}
public AmazonKinesis beam_f32614_0()
{    return new AmazonKinesisMock(shardedData.stream().map(testDatas -> transform(testDatas, TestData::convertToRecord)).collect(Collectors.toList()), numberOfRecordsPerGet);}
public AmazonCloudWatch beam_f32615_0()
{    return Mockito.mock(AmazonCloudWatch.class);}
public DecreaseStreamRetentionPeriodResult beam_f32625_0(DecreaseStreamRetentionPeriodRequest decreaseStreamRetentionPeriodRequest)
{    throw new RuntimeException("Not implemented");}
public DeleteStreamResult beam_f32626_0(DeleteStreamRequest deleteStreamRequest)
{    throw new RuntimeException("Not implemented");}
public DeleteStreamResult beam_f32627_0(String streamName)
{    throw new RuntimeException("Not implemented");}
public DisableEnhancedMonitoringResult beam_f32635_0(DisableEnhancedMonitoringRequest disableEnhancedMonitoringRequest)
{    throw new RuntimeException("Not implemented");}
public EnableEnhancedMonitoringResult beam_f32636_0(EnableEnhancedMonitoringRequest enableEnhancedMonitoringRequest)
{    throw new RuntimeException("Not implemented");}
public GetShardIteratorResult beam_f32637_0(String streamName, String shardId, String shardIteratorType)
{    throw new RuntimeException("Not implemented");}
public ListStreamsResult beam_f32645_0(Integer limit, String exclusiveStartStreamName)
{    throw new RuntimeException("Not implemented");}
public ListTagsForStreamResult beam_f32646_0(ListTagsForStreamRequest listTagsForStreamRequest)
{    throw new RuntimeException("Not implemented");}
public MergeShardsResult beam_f32647_0(MergeShardsRequest mergeShardsRequest)
{    throw new RuntimeException("Not implemented");}
public SplitShardResult beam_f32655_0(SplitShardRequest splitShardRequest)
{    throw new RuntimeException("Not implemented");}
public SplitShardResult beam_f32656_0(String streamName, String shardToSplit, String newStartingHashKey)
{    throw new RuntimeException("Not implemented");}
public StartStreamEncryptionResult beam_f32657_0(StartStreamEncryptionRequest startStreamEncryptionRequest)
{    throw new RuntimeException("Not implemented");}
public void beam_f32666_0() throws Exception
{    when(shard1.getShardId()).thenReturn("shard-01");    when(shard2.getShardId()).thenReturn("shard-02");    when(shard3.getShardId()).thenReturn("shard-03");    String streamName = "stream";    Set<Shard> shards = Sets.newHashSet(shard1, shard2);    StartingPoint startingPoint = new StartingPoint(InitialPositionInStream.LATEST);    when(startingPointShardsFinder.findShardsAtStartingPoint(kinesisClient, "stream", startingPoint)).thenReturn(shards);    DynamicCheckpointGenerator underTest = new DynamicCheckpointGenerator(streamName, startingPoint, startingPointShardsFinder);    KinesisReaderCheckpoint checkpoint = underTest.generate(kinesisClient);    assertThat(checkpoint).hasSize(2).doesNotContain(new ShardCheckpoint(streamName, shard3.getShardId(), startingPoint));}
public static void beam_f32667_0()
{    PipelineOptionsFactory.register(KinesisTestOptions.class);    options = TestPipeline.testingPipelineOptions().as(KinesisTestOptions.class);    numberOfShards = options.getNumberOfShards();    numberOfRows = options.getNumberOfRecords();}
public void beam_f32668_0()
{    runWrite();    runRead();}
public void beam_f32676_0(ProcessContext c) throws Exception
{    c.output(new AmazonKinesisMock.TestData(c.element()));}
private List<List<AmazonKinesisMock.TestData>> beam_f32677_0(int noOfShards, int noOfEventsPerShard)
{    int seqNumber = 0;    List<List<AmazonKinesisMock.TestData>> shardedData = newArrayList();    for (int i = 0; i < noOfShards; ++i) {        List<AmazonKinesisMock.TestData> shardData = newArrayList();        shardedData.add(shardData);        DateTime arrival = DateTime.now();        for (int j = 0; j < noOfEventsPerShard; ++j) {            arrival = arrival.plusSeconds(1);            seqNumber++;            shardData.add(new AmazonKinesisMock.TestData(Integer.toString(seqNumber), arrival.toInstant(), Integer.toString(seqNumber)));        }    }    return shardedData;}
public void beam_f32678_0()
{    KinesisServiceMock kinesisService = KinesisServiceMock.getInstance();    kinesisService.init(STREAM, 1);}
public void beam_f32686_0()
{    Iterable<byte[]> data = ImmutableList.of("1".getBytes(StandardCharsets.UTF_8));    p.apply(Create.of(data)).apply(KinesisIO.write().withStreamName(STREAM).withPartitionKey(PARTITION_KEY).withAWSClientsProvider(new FakeKinesisProvider().setFailedFlush(true)).withRetries(2));    thrown.expect(RuntimeException.class);    p.run().waitUntilFinish();}
public void beam_f32687_0()
{    KinesisServiceMock kinesisService = KinesisServiceMock.getInstance();    Iterable<byte[]> data = ImmutableList.of("1".getBytes(StandardCharsets.UTF_8), "2".getBytes(StandardCharsets.UTF_8));    p.apply(Create.of(data)).apply(KinesisIO.write().withStreamName(STREAM).withPartitionKey(PARTITION_KEY).withAWSClientsProvider(new FakeKinesisProvider()));    p.run().waitUntilFinish();    assertEquals(2, kinesisService.getAddedRecords().get());    List<List<AmazonKinesisMock.TestData>> testData = kinesisService.getShardedData();    int noOfShards = 1;    int noOfEventsPerShard = 2;    PCollection<AmazonKinesisMock.TestData> result = p2.apply(KinesisIO.read().withStreamName(STREAM).withInitialPositionInStream(InitialPositionInStream.TRIM_HORIZON).withAWSClientsProvider(new AmazonKinesisMock.Provider(testData, 10)).withMaxNumRecords(noOfShards * noOfEventsPerShard)).apply(ParDo.of(new KinesisMockReadTest.KinesisRecordToTestData()));    PAssert.that(result).containsInAnyOrder(Iterables.concat(testData));    p2.run().waitUntilFinish();}
public String beam_f32688_0(byte[] value)
{    return String.valueOf(value.length);}
public ListenableFuture<UserRecordResult> beam_f32696_0(UserRecord userRecord)
{    throw new UnsupportedOperationException("Not implemented");}
public synchronized ListenableFuture<UserRecordResult> beam_f32697_0(String stream, String partitionKey, String explicitHashKey, ByteBuffer data)
{    seqNumber.incrementAndGet();    SettableFuture<UserRecordResult> f = SettableFuture.create();    f.set(new UserRecordResult(new ArrayList<>(), String.valueOf(seqNumber.get()), explicitHashKey, !isFailedFlush));    if (kinesisService.getExistedStream().equals(stream)) {        addedRecords.add(new UserRecord(stream, partitionKey, explicitHashKey, data));    }    return f;}
public int beam_f32698_0()
{    return addedRecords.size();}
public void beam_f32707_0()
{    checkpoint = new KinesisReaderCheckpoint(asList(a, b, c));}
public void beam_f32708_0()
{    verifySplitInto(1);    verifySplitInto(2);    verifySplitInto(3);    verifySplitInto(4);}
public void beam_f32709_0()
{    Iterator<ShardCheckpoint> iterator = checkpoint.iterator();    iterator.remove();}
public void beam_f32717_0() throws IOException
{    when(shardReadersPool.nextRecord()).thenReturn(CustomOptional.of(c)).thenReturn(CustomOptional.absent()).thenReturn(CustomOptional.of(a)).thenReturn(CustomOptional.absent()).thenReturn(CustomOptional.of(d)).thenReturn(CustomOptional.of(b)).thenReturn(CustomOptional.absent());    assertThat(reader.start()).isTrue();    assertThat(reader.getCurrent()).isEqualTo(c);    assertThat(reader.advance()).isFalse();    assertThat(reader.advance()).isTrue();    assertThat(reader.getCurrent()).isEqualTo(a);    assertThat(reader.advance()).isFalse();    assertThat(reader.advance()).isTrue();    assertThat(reader.getCurrent()).isEqualTo(d);    assertThat(reader.advance()).isTrue();    assertThat(reader.getCurrent()).isEqualTo(b);    assertThat(reader.advance()).isFalse();}
public void beam_f32718_0() throws IOException
{    Instant expectedWatermark = new Instant(123456L);    when(shardReadersPool.getWatermark()).thenReturn(expectedWatermark);    reader.start();    Instant currentWatermark = reader.getWatermark();    assertThat(currentWatermark).isEqualTo(expectedWatermark);}
public void beam_f32719_0() throws TransientKinesisException, IOException
{    reader.start();    when(kinesisSource.getStreamName()).thenReturn("stream1");    doReturn(Instant.now().minus(Duration.standardMinutes(1))).when(reader).getWatermark();    when(kinesis.getBacklogBytes(eq("stream1"), any(Instant.class))).thenReturn(10L).thenThrow(TransientKinesisException.class).thenReturn(20L);    assertThat(reader.getTotalBacklogBytes()).isEqualTo(10);    assertThat(reader.getTotalBacklogBytes()).isEqualTo(10);    assertThat(reader.getTotalBacklogBytes()).isEqualTo(20);}
public synchronized void beam_f32727_0(ByteBuffer data, DateTime arrival)
{    String dataString = StandardCharsets.UTF_8.decode(data).toString();    List<AmazonKinesisMock.TestData> shardData = shardedData.get(0);    seqNumber.incrementAndGet();    AmazonKinesisMock.TestData testData = new AmazonKinesisMock.TestData(dataString, arrival.toInstant(), Integer.toString(seqNumber.get()));    shardData.add(testData);    addedRecords.incrementAndGet();}
public synchronized List<List<AmazonKinesisMock.TestData>> beam_f32728_0()
{    return shardedData;}
public void beam_f32729_0()
{    when(checkpoint.isBeforeOrAt(record1)).thenReturn(false);    when(checkpoint.isBeforeOrAt(record2)).thenReturn(true);    when(checkpoint.isBeforeOrAt(record3)).thenReturn(true);    when(checkpoint.isBeforeOrAt(record4)).thenReturn(false);    when(checkpoint.isBeforeOrAt(record5)).thenReturn(true);    List<KinesisRecord> records = Lists.newArrayList(record1, record2, record3, record4, record5);    RecordFilter underTest = new RecordFilter();    List<KinesisRecord> retainedRecords = underTest.apply(records, checkpoint);    Assertions.assertThat(retainedRecords).containsOnly(record2, record3, record5);}
private KinesisRecord beam_f32737_0(Instant approximateArrivalTimestamp)
{    KinesisRecord record = mock(KinesisRecord.class);    when(record.getApproximateArrivalTimestamp()).thenReturn(approximateArrivalTimestamp);    return record;}
private ShardCheckpoint beam_f32738_0(ShardIteratorType iteratorType, Instant timestamp)
{    return new ShardCheckpoint(STREAM_NAME, SHARD_ID, iteratorType, timestamp);}
public void beam_f32739_0() throws TransientKinesisException
{    when(a.getShardId()).thenReturn("shard1");    when(b.getShardId()).thenReturn("shard1");    when(c.getShardId()).thenReturn("shard2");    when(d.getShardId()).thenReturn("shard2");    when(firstCheckpoint.getShardId()).thenReturn("shard1");    when(secondCheckpoint.getShardId()).thenReturn("shard2");    when(firstIterator.getShardId()).thenReturn("shard1");    when(firstIterator.getCheckpoint()).thenReturn(firstCheckpoint);    when(secondIterator.getShardId()).thenReturn("shard2");    when(secondIterator.getCheckpoint()).thenReturn(secondCheckpoint);    when(thirdIterator.getShardId()).thenReturn("shard3");    when(fourthIterator.getShardId()).thenReturn("shard4");    WatermarkPolicy policy = WatermarkPolicyFactory.withArrivalTimePolicy().createWatermarkPolicy();    checkpoint = new KinesisReaderCheckpoint(ImmutableList.of(firstCheckpoint, secondCheckpoint));    shardReadersPool = Mockito.spy(new ShardReadersPool(kinesis, checkpoint, factory));    when(factory.createWatermarkPolicy()).thenReturn(policy);    doReturn(firstIterator).when(shardReadersPool).createShardIterator(kinesis, firstCheckpoint);    doReturn(secondIterator).when(shardReadersPool).createShardIterator(kinesis, secondCheckpoint);}
public void beam_f32747_0() throws Exception
{    when(firstIterator.readNextBatch()).thenThrow(KinesisShardClosedException.class);    when(firstIterator.findSuccessiveShardRecordIterators()).thenReturn(ImmutableList.of(thirdIterator, fourthIterator));    shardReadersPool.start();    verify(thirdIterator, timeout(TIMEOUT_IN_MILLIS).atLeast(2)).readNextBatch();    verify(fourthIterator, timeout(TIMEOUT_IN_MILLIS).atLeast(2)).readNextBatch();}
public void beam_f32748_0() throws Exception
{    when(firstIterator.readNextBatch()).thenThrow(KinesisShardClosedException.class);    when(firstIterator.findSuccessiveShardRecordIterators()).thenReturn(Collections.emptyList());    shardReadersPool.start();    verify(firstIterator, timeout(TIMEOUT_IN_MILLIS).times(1)).readNextBatch();}
public void beam_f32749_0() throws Exception
{    when(firstIterator.readNextBatch()).thenThrow(KinesisShardClosedException.class);    when(firstIterator.findSuccessiveShardRecordIterators()).thenThrow(TransientKinesisException.class).thenReturn(Collections.emptyList());    shardReadersPool.start();    verify(firstIterator, timeout(TIMEOUT_IN_MILLIS).times(2)).readNextBatch();}
public Object beam_f32757_0(InvocationOnMock invocation) throws Throwable
{    return invocation.getArguments()[0];}
public void beam_f32758_0() throws Exception
{    when(kinesis.getShardIterator(new GetShardIteratorRequest().withStreamName(STREAM).withShardId(SHARD_1).withShardIteratorType(ShardIteratorType.AT_SEQUENCE_NUMBER).withStartingSequenceNumber(SEQUENCE_NUMBER))).thenReturn(new GetShardIteratorResult().withShardIterator(SHARD_ITERATOR));    String stream = underTest.getShardIterator(STREAM, SHARD_1, ShardIteratorType.AT_SEQUENCE_NUMBER, SEQUENCE_NUMBER, null);    assertThat(stream).isEqualTo(SHARD_ITERATOR);}
public void beam_f32759_0() throws Exception
{    Instant timestamp = Instant.now();    when(kinesis.getShardIterator(new GetShardIteratorRequest().withStreamName(STREAM).withShardId(SHARD_1).withShardIteratorType(ShardIteratorType.AT_SEQUENCE_NUMBER).withTimestamp(timestamp.toDate()))).thenReturn(new GetShardIteratorResult().withShardIterator(SHARD_ITERATOR));    String stream = underTest.getShardIterator(STREAM, SHARD_1, ShardIteratorType.AT_SEQUENCE_NUMBER, null, timestamp);    assertThat(stream).isEqualTo(SHARD_ITERATOR);}
public void beam_f32767_0() throws Exception
{    Shard shard1 = new Shard().withShardId(SHARD_1);    Shard shard2 = new Shard().withShardId(SHARD_2);    Shard shard3 = new Shard().withShardId(SHARD_3);    when(kinesis.describeStream(STREAM, null)).thenReturn(new DescribeStreamResult().withStreamDescription(new StreamDescription().withShards(shard1, shard2).withHasMoreShards(true)));    when(kinesis.describeStream(STREAM, SHARD_2)).thenReturn(new DescribeStreamResult().withStreamDescription(new StreamDescription().withShards(shard3).withHasMoreShards(false)));    List<Shard> shards = underTest.listShards(STREAM);    assertThat(shards).containsOnly(shard1, shard2, shard3);}
public void beam_f32768_0()
{    shouldHandleShardListingError(new ExpiredIteratorException(""), ExpiredIteratorException.class);}
public void beam_f32769_0()
{    shouldHandleShardListingError(new LimitExceededException(""), TransientKinesisException.class);}
public void beam_f32777_0() throws Exception
{    Instant countSince = new Instant("2017-04-06T10:00:00.000Z");    Instant countTo = new Instant("2017-04-06T10:00:02.000Z");    long backlogBytes = underTest.getBacklogBytes(STREAM, countSince, countTo);    assertThat(backlogBytes).isEqualTo(0L);    verifyZeroInteractions(cloudWatch);}
public void beam_f32778_0()
{    shouldHandleGetBacklogBytesError(new LimitExceededException(""), TransientKinesisException.class);}
public void beam_f32779_0()
{    shouldHandleGetBacklogBytesError(new ProvisionedThroughputExceededException(""), TransientKinesisException.class);}
public void beam_f32787_0() throws Exception
{        Instant timestampAtTheBeginning = new Instant();    StartingPoint startingPointAtTheBeginning = new StartingPoint(timestampAtTheBeginning);    for (Shard shard : allShards) {        activeAtTimestamp(shard, timestampAtTheBeginning);    }    when(kinesis.listShards(STREAM_NAME)).thenReturn(allShards);        Iterable<Shard> shardsAtStartingPoint = underTest.findShardsAtStartingPoint(kinesis, STREAM_NAME, startingPointAtTheBeginning);        assertThat(shardsAtStartingPoint).containsExactlyInAnyOrder(shard00, shard01);}
public void beam_f32788_0() throws Exception
{        Instant timestampAfterShards3And4Merge = new Instant();    StartingPoint startingPointAfterFirstSplitsAndMerge = new StartingPoint(timestampAfterShards3And4Merge);    expiredAtTimestamp(shard00, timestampAfterShards3And4Merge);    expiredAtTimestamp(shard01, timestampAfterShards3And4Merge);    activeAtTimestamp(shard02, timestampAfterShards3And4Merge);    expiredAtTimestamp(shard03, timestampAfterShards3And4Merge);    expiredAtTimestamp(shard04, timestampAfterShards3And4Merge);    activeAtTimestamp(shard05, timestampAfterShards3And4Merge);    activeAtTimestamp(shard06, timestampAfterShards3And4Merge);    activeAtTimestamp(shard07, timestampAfterShards3And4Merge);    activeAtTimestamp(shard08, timestampAfterShards3And4Merge);    activeAtTimestamp(shard09, timestampAfterShards3And4Merge);    activeAtTimestamp(shard10, timestampAfterShards3And4Merge);    when(kinesis.listShards(STREAM_NAME)).thenReturn(allShards);        Iterable<Shard> shardsAtStartingPoint = underTest.findShardsAtStartingPoint(kinesis, STREAM_NAME, startingPointAfterFirstSplitsAndMerge);        assertThat(shardsAtStartingPoint).containsExactlyInAnyOrder(shard02, shard05, shard06);}
public void beam_f32789_0() throws Exception
{        Instant timestampAtTheEnd = new Instant();    StartingPoint startingPointAtTheEnd = new StartingPoint(timestampAtTheEnd);    expiredAtTimestamp(shard00, timestampAtTheEnd);    expiredAtTimestamp(shard01, timestampAtTheEnd);    expiredAtTimestamp(shard02, timestampAtTheEnd);    expiredAtTimestamp(shard03, timestampAtTheEnd);    expiredAtTimestamp(shard04, timestampAtTheEnd);    expiredAtTimestamp(shard05, timestampAtTheEnd);    expiredAtTimestamp(shard06, timestampAtTheEnd);    expiredAtTimestamp(shard07, timestampAtTheEnd);    expiredAtTimestamp(shard08, timestampAtTheEnd);    activeAtTimestamp(shard09, timestampAtTheEnd);    activeAtTimestamp(shard10, timestampAtTheEnd);    when(kinesis.listShards(STREAM_NAME)).thenReturn(allShards);        Iterable<Shard> shardsAtStartingPoint = underTest.findShardsAtStartingPoint(kinesis, STREAM_NAME, startingPointAtTheEnd);        assertThat(shardsAtStartingPoint).containsExactlyInAnyOrder(shard09, shard10);}
private void beam_f32797_0(Shard shard, Instant startTimestamp)
{    prepareShard(shard, "timestampIterator-" + shard.getShardId(), ShardIteratorType.AT_TIMESTAMP, startTimestamp);}
private void beam_f32798_0(Shard shard, ShardIteratorType shardIteratorType)
{    prepareShard(shard, shardIteratorType.toString() + shard.getShardId(), shardIteratorType, null);}
private void beam_f32799_0(Shard shard, String nextIterator, ShardIteratorType shardIteratorType, Instant startTimestamp)
{    try {        String shardIterator = shardIteratorType + shard.getShardId() + "-current";        if (shardIteratorType == ShardIteratorType.AT_TIMESTAMP) {            when(kinesis.getShardIterator(STREAM_NAME, shard.getShardId(), ShardIteratorType.AT_TIMESTAMP, null, startTimestamp)).thenReturn(shardIterator);        } else {            when(kinesis.getShardIterator(STREAM_NAME, shard.getShardId(), shardIteratorType, null, null)).thenReturn(shardIterator);        }        GetKinesisRecordsResult result = new GetKinesisRecordsResult(Collections.<UserRecord>emptyList(), nextIterator, 0, STREAM_NAME, shard.getShardId());        when(kinesis.getRecords(shardIterator, STREAM_NAME, shard.getShardId())).thenReturn(result);    } catch (TransientKinesisException e) {        throw new RuntimeException(e);    }}
 Coder<T> beam_f32807_0(CoderRegistry coderRegistry)
{    try {        return getCoder() != null ? getCoder() : coderRegistry.getCoder(TypeDescriptors.outputOf(getParseFn()));    } catch (CannotProvideCoderException e) {        throw new IllegalArgumentException("Unable to infer coder for output of parseFn (" + TypeDescriptors.outputOf(getParseFn()) + "). Specify it explicitly using withCoder().", e);    }}
public Read<T> beam_f32808_0(String masterAddresses)
{    checkArgument(masterAddresses != null, "masterAddresses cannot be null or empty");    return builder().setMasterAddresses(Splitter.on(",").splitToList(masterAddresses)).build();}
public Read<T> beam_f32809_0(String table)
{    checkArgument(table != null, "table cannot be null");    return builder().setTable(table).build();}
public PCollection<T> beam_f32817_0(PBegin input)
{    Pipeline p = input.getPipeline();    final Coder<T> coder = inferCoder(p.getCoderRegistry());    return input.apply(org.apache.beam.sdk.io.Read.from(new KuduSource<>(this, coder, null)));}
public void beam_f32818_0(PipelineOptions pipelineOptions)
{    checkState(getMasterAddresses() != null, "KuduIO.read() requires a list of master addresses to be set via withMasterAddresses(masterAddresses)");    checkState(getTable() != null, "KuduIO.read() requires a table name to be set via withTableName(tableName)");    checkState(getParseFn() != null, "KuduIO.read() requires a parse function to be set via withParseFn(parseFn)");}
public void beam_f32819_0(DisplayData.Builder builder)
{    super.populateDisplayData(builder);    builder.add(DisplayData.item("masterAddresses", getMasterAddresses().toString()));    builder.add(DisplayData.item("table", getTable()));}
 Write<T> beam_f32827_0(KuduService<T> kuduService)
{    checkArgument(kuduService != null, "kuduService cannot be null");    return builder().setKuduService(kuduService).build();}
public PDone beam_f32828_0(PCollection<T> input)
{    input.apply(ParDo.of(new WriteFn(this)));    return PDone.in(input.getPipeline());}
public void beam_f32829_0(PipelineOptions pipelineOptions)
{    checkState(masterAddresses() != null, "KuduIO.write() requires a list of master addresses to be set via withMasterAddresses(masterAddresses)");    checkState(table() != null, "KuduIO.write() requires a table name to be set via withTable(table)");    checkState(formatFn() != null, "KuduIO.write() requires a format function to be set via withFormatFn(formatFn)");}
public Writer beam_f32837_0(KuduIO.Write<T> spec) throws KuduException
{    return new WriterImpl(spec);}
public BoundedSource.BoundedReader beam_f32838_0(KuduIO.KuduSource source)
{    return new ReaderImpl(source);}
public List<byte[]> beam_f32839_0(KuduIO.Read spec) throws KuduException
{    try (KuduClient client = getKuduClient(spec.getMasterAddresses())) {        KuduTable table = client.openTable(spec.getTable());        KuduScanToken.KuduScanTokenBuilder builder = client.newScanTokenBuilder(table);        configureBuilder(spec, table.getSchema(), builder);        List<KuduScanToken> tokens = builder.build();        return tokens.stream().map(t -> uncheckCall(t::serialize)).collect(Collectors.toList());    }}
public void beam_f32847_1() throws IOException
{        if (scanner != null) {        scanner.close();        scanner = null;    }    if (client != null) {        client.close();        client = null;    }}
public synchronized KuduIO.KuduSource beam_f32848_0()
{    return source;}
private synchronized KuduClient beam_f32849_0(List<String> masterAddresses)
{    return new AsyncKuduClient.AsyncKuduClientBuilder(masterAddresses).build().syncClient();}
private void beam_f32857_0()
{        PCollection<String> output = readPipeline.apply(KuduIO.<String>read().withMasterAddresses(options.getKuduMasterAddresses()).withTable(options.getKuduTable()).withParseFn((SerializableFunction<RowResult, String>) input -> input.getString(COL_NAME)).withCoder(StringUtf8Coder.of()));    PAssert.thatSingleton(output.apply("Count", Count.globally())).isEqualTo((long) options.getNumberOfRecords());    readPipeline.run().waitUntilFinish();}
private void beam_f32858_0()
{    PCollection<String> output = readPipeline.apply("Read with predicates", KuduIO.<String>read().withMasterAddresses(options.getKuduMasterAddresses()).withTable(options.getKuduTable()).withParseFn((SerializableFunction<RowResult, String>) input -> input.getString(COL_NAME)).withPredicates(Arrays.asList(KuduPredicate.newComparisonPredicate(SCHEMA.getColumn(COL_ID), KuduPredicate.ComparisonOp.GREATER_EQUAL, 2), KuduPredicate.newComparisonPredicate(SCHEMA.getColumn(COL_ID), KuduPredicate.ComparisonOp.LESS, 7))).withCoder(StringUtf8Coder.of()));    output.apply(Count.globally());    PAssert.thatSingleton(output.apply("Count", Count.globally())).isEqualTo((long) 5);    readPipeline.run().waitUntilFinish();}
private void beam_f32859_0()
{    thrown.expect(IllegalArgumentException.class);    readPipeline.apply("Read with projected columns", KuduIO.<String>read().withMasterAddresses(options.getKuduMasterAddresses()).withTable(options.getKuduTable()).withParseFn((SerializableFunction<RowResult, String>) input -> input.getString(COL_NAME)).withProjectedColumns(    Collections.singletonList(COL_ID))).setCoder(StringUtf8Coder.of());    readPipeline.run().waitUntilFinish();}
public void beam_f32867_1()
{        }
private void beam_f32868_0(ObjectInputStream in) throws IOException, ClassNotFoundException
{    in.defaultReadObject();    id = counter.incrementAndGet();}
public boolean beam_f32869_1()
{        if (source.serializedToken != null) {        ByteBuffer bb = ByteBuffer.wrap(source.serializedToken);        lowerInclusive = bb.getInt();        upperExclusive = bb.getInt();            }    current = lowerInclusive;    return true;}
private static Builder beam_f32878_0()
{    return new AutoValue_AggregationQuery.Builder().setMongoDbPipeline(new ArrayList<>());}
public static AggregationQuery beam_f32879_0()
{    return builder().build();}
public AggregationQuery beam_f32880_0(List<BsonDocument> mongoDbPipeline)
{    return toBuilder().setMongoDbPipeline(mongoDbPipeline).build();}
public FindQuery beam_f32888_0(List<String> projection)
{    checkArgument(projection != null, "projection can not be null");    return toBuilder().setProjection(projection).build();}
public MongoCursor<Document> beam_f32889_0(MongoCollection<Document> collection)
{    return collection.find().filter(filters()).limit(limit()).projection(Projections.include(projection())).iterator();}
public static Read<String> beam_f32890_0()
{    return new AutoValue_MongoDbGridFSIO_Read.Builder<String>().setParser(TEXT_PARSER).setCoder(StringUtf8Coder.of()).setConnectionConfiguration(ConnectionConfiguration.create()).setSkew(Duration.ZERO).build();}
public Read<T> beam_f32898_0(String database)
{    checkNotNull(database);    ConnectionConfiguration config = ConnectionConfiguration.create(connectionConfiguration().uri(), database, connectionConfiguration().bucket());    return toBuilder().setConnectionConfiguration(config).build();}
public Read<T> beam_f32899_0(String bucket)
{    checkNotNull(bucket);    ConnectionConfiguration config = ConnectionConfiguration.create(connectionConfiguration().uri(), connectionConfiguration().database(), bucket);    return toBuilder().setConnectionConfiguration(config).build();}
public Read<X> beam_f32900_0(Parser<X> parser)
{    checkNotNull(parser);    Builder<X> builder = (Builder<X>) toBuilder();    return builder.setParser(parser).setCoder(null).build();}
public void beam_f32908_0(final ProcessContext c) throws IOException
{    ObjectId oid = c.element();    GridFSDBFile file = gridfs.find(oid);    parser().parse(file, new ParserCallback<T>() {        @Override        public void output(T output, Instant timestamp) {            checkNotNull(timestamp);            c.outputWithTimestamp(output, timestamp);        }        @Override        public void output(T output) {            c.output(output);        }    });}
public void beam_f32909_0(T output, Instant timestamp)
{    checkNotNull(timestamp);    c.outputWithTimestamp(output, timestamp);}
public void beam_f32910_0(T output)
{    c.output(output);}
public BoundedSource<ObjectId> beam_f32918_0()
{    return source;}
public boolean beam_f32919_0() throws IOException
{    if (objects == null) {        mongo = source.spec.connectionConfiguration().setupMongo();        GridFS gridfs = source.spec.connectionConfiguration().setupGridFS(mongo);        cursor = source.createCursor(gridfs);    } else {        iterator = objects.iterator();    }    return advance();}
public boolean beam_f32920_0() throws IOException
{    if (iterator != null && iterator.hasNext()) {        current = iterator.next();        return true;    } else if (cursor != null && cursor.hasNext()) {        GridFSDBFile file = (GridFSDBFile) cursor.next();        current = (ObjectId) file.getId();        return true;    }    current = null;    return false;}
public Write<T> beam_f32928_0(Long chunkSize)
{    checkNotNull(chunkSize);    checkArgument(chunkSize > 1, "Chunk Size must be greater than 1", chunkSize);    return toBuilder().setChunkSize(chunkSize).build();}
public void beam_f32929_0(T input)
{    checkNotNull(filename(), "filename");    checkNotNull(writeFn(), "writeFn");}
public void beam_f32930_0(DisplayData.Builder builder)
{    super.populateDisplayData(builder);    builder.addIfNotNull(DisplayData.item("uri", connectionConfiguration().uri()));    builder.addIfNotNull(DisplayData.item("database", connectionConfiguration().database()));    builder.addIfNotNull(DisplayData.item("bucket", connectionConfiguration().bucket()));    builder.addIfNotNull(DisplayData.item("chunkSize", chunkSize()));    builder.addIfNotNull(DisplayData.item("filename", filename()));}
public static Write beam_f32938_0()
{    return new AutoValue_MongoDbIO_Write.Builder().setMaxConnectionIdleTime(60000).setBatchSize(1024L).setSslEnabled(false).setIgnoreSSLCertificate(false).setSslInvalidHostNameAllowed(false).setOrdered(true).build();}
public Read beam_f32939_0(String uri)
{    checkArgument(uri != null, "MongoDbIO.read().withUri(uri) called with null uri");    return builder().setUri(uri).build();}
public Read beam_f32940_0(int maxConnectionIdleTime)
{    return builder().setMaxConnectionIdleTime(maxConnectionIdleTime).build();}
public Read beam_f32948_0(int numSplits)
{    checkArgument(numSplits >= 0, "invalid num_splits: must be >= 0, but was %s", numSplits);    return builder().setNumSplits(numSplits).build();}
public Read beam_f32949_0(boolean bucketAuto)
{    return builder().setBucketAuto(bucketAuto).build();}
public Read beam_f32950_0(SerializableFunction<MongoCollection<Document>, MongoCursor<Document>> queryBuilderFn)
{    return builder().setQueryFn(queryBuilderFn).build();}
private long beam_f32958_0(MongoClient mongoClient, String database, String collection)
{    MongoDatabase mongoDatabase = mongoClient.getDatabase(database);            BasicDBObject stat = new BasicDBObject();    stat.append("collStats", collection);    Document stats = mongoDatabase.runCommand(stat);    return stats.get("size", Number.class).longValue();}
public List<BoundedSource<Document>> beam_f32959_1(long desiredBundleSizeBytes, PipelineOptions options)
{    try (MongoClient mongoClient = new MongoClient(new MongoClientURI(spec.uri(), getOptions(spec.maxConnectionIdleTime(), spec.sslEnabled(), spec.sslInvalidHostNameAllowed())))) {        MongoDatabase mongoDatabase = mongoClient.getDatabase(spec.database());        List<Document> splitKeys;        List<BoundedSource<Document>> sources = new ArrayList<>();        if (spec.queryFn().getClass() == AutoValue_FindQuery.class) {            if (spec.bucketAuto()) {                splitKeys = buildAutoBuckets(mongoDatabase, spec);            } else {                if (spec.numSplits() > 0) {                                                            long estimatedSizeBytes = getEstimatedSizeBytes(mongoClient, spec.database(), spec.collection());                    desiredBundleSizeBytes = estimatedSizeBytes / spec.numSplits();                }                                if (desiredBundleSizeBytes < 1024L * 1024L) {                    desiredBundleSizeBytes = 1024L * 1024L;                }                                                BasicDBObject splitVectorCommand = new BasicDBObject();                splitVectorCommand.append("splitVector", spec.database() + "." + spec.collection());                splitVectorCommand.append("keyPattern", new BasicDBObject().append("_id", 1));                splitVectorCommand.append("force", false);                                                splitVectorCommand.append("maxChunkSize", desiredBundleSizeBytes / 1024 / 1024);                Document splitVectorCommandResult = mongoDatabase.runCommand(splitVectorCommand);                splitKeys = (List<Document>) splitVectorCommandResult.get("splitKeys");            }            if (splitKeys.size() < 1) {                                return Collections.singletonList(this);            }            for (String shardFilter : splitKeysToFilters(splitKeys)) {                SerializableFunction<MongoCollection<Document>, MongoCursor<Document>> queryFn = spec.queryFn();                BsonDocument filters = bson2BsonDocument(Document.parse(shardFilter));                FindQuery findQuery = (FindQuery) queryFn;                final BsonDocument allFilters = bson2BsonDocument(findQuery.filters() != null ? Filters.and(findQuery.filters(), filters) : filters);                FindQuery queryWithFilter = findQuery.toBuilder().setFilters(allFilters).build();                                sources.add(new BoundedMongoDbSource(spec.withQueryFn(queryWithFilter)));            }        } else {            SerializableFunction<MongoCollection<Document>, MongoCursor<Document>> queryFn = spec.queryFn();            AggregationQuery aggregationQuery = (AggregationQuery) queryFn;            if (aggregationQuery.mongoDbPipeline().stream().anyMatch(s -> s.keySet().contains("$limit"))) {                return Collections.singletonList(this);            }            splitKeys = buildAutoBuckets(mongoDatabase, spec);            for (BsonDocument shardFilter : splitKeysToMatch(splitKeys)) {                AggregationQuery queryWithBucket = aggregationQuery.toBuilder().setBucket(shardFilter).build();                sources.add(new BoundedMongoDbSource(spec.withQueryFn(queryWithBucket)));            }        }        return sources;    }}
 static List<String> beam_f32960_0(List<Document> splitKeys)
{    ArrayList<String> filters = new ArrayList<>();        String lowestBound = null;    for (int i = 0; i < splitKeys.size(); i++) {        String splitKey = splitKeys.get(i).get("_id").toString();        String rangeFilter;        if (i == 0) {                                    rangeFilter = String.format("{ $and: [ {\"_id\":{$lte:ObjectId(\"%s\")}}", splitKey);            filters.add(String.format("%s ]}", rangeFilter));                        if (splitKeys.size() == 1) {                rangeFilter = String.format("{ $and: [ {\"_id\":{$gt:ObjectId(\"%s\")}}", splitKey);                filters.add(String.format("%s ]}", rangeFilter));            }        } else if (i == splitKeys.size() - 1) {                                                rangeFilter = String.format("{ $and: [ {\"_id\":{$gt:ObjectId(\"%s\")," + "$lte:ObjectId(\"%s\")}}", lowestBound, splitKey);            filters.add(String.format("%s ]}", rangeFilter));            rangeFilter = String.format("{ $and: [ {\"_id\":{$gt:ObjectId(\"%s\")}}", splitKey);            filters.add(String.format("%s ]}", rangeFilter));        } else {                        rangeFilter = String.format("{ $and: [ {\"_id\":{$gt:ObjectId(\"%s\")," + "$lte:ObjectId(\"%s\")}}", lowestBound, splitKey);            filters.add(String.format("%s ]}", rangeFilter));        }        lowestBound = splitKey;    }    return filters;}
private MongoClient beam_f32968_0(Read spec)
{    return new MongoClient(new MongoClientURI(spec.uri(), getOptions(spec.maxConnectionIdleTime(), spec.sslEnabled(), spec.sslInvalidHostNameAllowed())));}
public Write beam_f32969_0(String uri)
{    checkArgument(uri != null, "uri can not be null");    return builder().setUri(uri).build();}
public Write beam_f32970_0(int maxConnectionIdleTime)
{    return builder().setMaxConnectionIdleTime(maxConnectionIdleTime).build();}
public PDone beam_f32978_0(PCollection<Document> input)
{    checkArgument(uri() != null, "withUri() is required");    checkArgument(database() != null, "withDatabase() is required");    checkArgument(collection() != null, "withCollection() is required");    input.apply(ParDo.of(new WriteFn(this)));    return PDone.in(input.getPipeline());}
public void beam_f32979_0(DisplayData.Builder builder)
{    builder.add(DisplayData.item("uri", uri()));    builder.add(DisplayData.item("maxConnectionIdleTime", maxConnectionIdleTime()));    builder.add(DisplayData.item("sslEnable", sslEnabled()));    builder.add(DisplayData.item("sslInvalidHostNameAllowed", sslInvalidHostNameAllowed()));    builder.add(DisplayData.item("ignoreSSLCertificate", ignoreSSLCertificate()));    builder.add(DisplayData.item("ordered", ordered()));    builder.add(DisplayData.item("database", database()));    builder.add(DisplayData.item("collection", collection()));    builder.add(DisplayData.item("batchSize", batchSize()));}
public void beam_f32980_0()
{    client = new MongoClient(new MongoClientURI(spec.uri(), getOptions(spec.maxConnectionIdleTime(), spec.sslEnabled(), spec.sslInvalidHostNameAllowed())));}
public static void beam_f32990_1() throws Exception
{    port = NetworkTestHelper.getAvailableLocalPort();        IMongodConfig mongodConfig = new MongodConfigBuilder().version(Version.Main.PRODUCTION).configServer(false).replication(new Storage(MONGODB_LOCATION.getRoot().getPath(), null, 0)).net(new Net("localhost", port, Network.localhostIsIPv6())).cmdOptions(new MongoCmdOptionsBuilder().syncDelay(10).useNoPrealloc(true).useSmallFiles(true).useNoJournal(true).verbose(false).build()).build();    mongodExecutable = mongodStarter.prepare(mongodConfig);    mongodProcess = mongodExecutable.start();        Mongo client = new Mongo("localhost", port);    DB database = client.getDB(DATABASE);    GridFS gridfs = new GridFS(database);    ByteArrayOutputStream out = new ByteArrayOutputStream();    for (int x = 0; x < 100; x++) {        out.write(("Einstein\nDarwin\nCopernicus\nPasteur\n" + "Curie\nFaraday\nNewton\nBohr\nGalilei\nMaxwell\n").getBytes(StandardCharsets.UTF_8));    }    for (int x = 0; x < 5; x++) {        gridfs.createFile(new ByteArrayInputStream(out.toByteArray()), "file" + x).save();    }    gridfs = new GridFS(database, "mapBucket");    long now = System.currentTimeMillis();    Random random = new Random();    String[] scientists = { "Einstein", "Darwin", "Copernicus", "Pasteur", "Curie", "Faraday", "Newton", "Bohr", "Galilei", "Maxwell" };    for (int x = 0; x < 10; x++) {        GridFSInputFile file = gridfs.createFile("file_" + x);        OutputStream outf = file.getOutputStream();        OutputStreamWriter writer = new OutputStreamWriter(outf, StandardCharsets.UTF_8);        for (int y = 0; y < 5000; y++) {            long time = now - random.nextInt(3600000);            String name = scientists[y % scientists.length];            writer.write(Long.toString(time) + "\t");            writer.write(name + "\t");            writer.write(Integer.toString(random.nextInt(100)));            writer.write("\n");        }        for (int y = 0; y < scientists.length; y++) {            String name = scientists[y % scientists.length];            writer.write(Long.toString(now) + "\t");            writer.write(name + "\t");            writer.write("101");            writer.write("\n");        }        writer.flush();        writer.close();    }    client.close();}
public static void beam_f32991_1()
{        mongodProcess.stop();    mongodExecutable.stop();}
public void beam_f32992_0()
{    PCollection<String> output = pipeline.apply(MongoDbGridFSIO.read().withUri("mongodb://localhost:" + port).withDatabase(DATABASE));    PAssert.thatSingleton(output.apply("Count All", Count.globally())).isEqualTo(5000L);    PAssert.that(output.apply("Count PerElement", Count.perElement())).satisfies(input -> {        for (KV<String, Long> element : input) {            assertEquals(500L, element.getValue().longValue());        }        return null;    });    pipeline.run();}
private void beam_f33000_0(PipelineResult writeResult, PipelineResult readResult)
{    String uuid = UUID.randomUUID().toString();    String timestamp = Timestamp.now().toString();    Set<Function<MetricsReader, NamedTestResult>> readSuppliers = getReadSuppliers(uuid, timestamp);    Set<Function<MetricsReader, NamedTestResult>> writeSuppliers = getWriteSuppliers(uuid, timestamp);    IOITMetrics readMetrics = new IOITMetrics(readSuppliers, readResult, NAMESPACE, uuid, timestamp);    IOITMetrics writeMetrics = new IOITMetrics(writeSuppliers, writeResult, NAMESPACE, uuid, timestamp);    readMetrics.publish(bigQueryDataset, bigQueryTable);    writeMetrics.publish(bigQueryDataset, bigQueryTable);}
private Set<Function<MetricsReader, NamedTestResult>> beam_f33001_0(String uuid, String timestamp)
{    Set<Function<MetricsReader, NamedTestResult>> suppliers = new HashSet<>();    suppliers.add(reader -> {        long writeStart = reader.getStartTimeMetric("write_time");        long writeEnd = reader.getEndTimeMetric("write_time");        return NamedTestResult.create(uuid, timestamp, "write_time", (writeEnd - writeStart) / 1e3);    });    return suppliers;}
private Set<Function<MetricsReader, NamedTestResult>> beam_f33002_0(String uuid, String timestamp)
{    Set<Function<MetricsReader, NamedTestResult>> suppliers = new HashSet<>();    suppliers.add(reader -> {        long readStart = reader.getStartTimeMetric("read_time");        long readEnd = reader.getEndTimeMetric("read_time");        return NamedTestResult.create(uuid, timestamp, "read_time", (readEnd - readStart) / 1e3);    });    return suppliers;}
public void beam_f33010_0()
{    PCollection<Document> output = pipeline.apply(MongoDbIO.read().withUri("mongodb://localhost:" + port).withDatabase(DATABASE).withCollection(COLLECTION));    PAssert.thatSingleton(output.apply("Count All", Count.globally())).isEqualTo(1000L);    PAssert.that(output.apply("Map Scientist", MapElements.via(new DocumentToKVFn())).apply("Count Scientist", Count.perKey())).satisfies(input -> {        for (KV<String, Long> element : input) {            assertEquals(100L, element.getValue().longValue());        }        return null;    });    pipeline.run();}
public void beam_f33011_0()
{    MongoDbIO.Read read = MongoDbIO.read().withUri("mongodb://localhost:" + port).withMaxConnectionIdleTime(10).withDatabase(DATABASE).withCollection(COLLECTION);    assertEquals(10, read.maxConnectionIdleTime());    PCollection<Document> documents = pipeline.apply(read);    PAssert.thatSingleton(documents.apply("Count All", Count.globally())).isEqualTo(1000L);    PAssert.that(documents.apply("Map Scientist", MapElements.via(new DocumentToKVFn())).apply("Count Scientist", Count.perKey())).satisfies(input -> {        for (KV<String, Long> element : input) {            assertEquals(100L, element.getValue().longValue());        }        return null;    });    pipeline.run();}
public void beam_f33012_0()
{    PCollection<Document> output = pipeline.apply(MongoDbIO.read().withUri("mongodb://localhost:" + port).withDatabase(DATABASE).withCollection(COLLECTION).withQueryFn(FindQuery.create().withFilters(Filters.eq("scientist", "Einstein"))));    PAssert.thatSingleton(output.apply("Count", Count.globally())).isEqualTo(100L);    pipeline.run();}
private static MongoCollection<Document> beam_f33020_0(final String collectionName)
{    MongoDatabase database = client.getDatabase(DATABASE);    return database.getCollection(collectionName);}
public KV<String, Void> beam_f33021_0(Document input)
{    return KV.of(input.getString("scientist"), null);}
public static Read beam_f33022_0()
{    return new AutoValue_MqttIO_Read.Builder().setMaxReadTime(null).setMaxNumRecords(Long.MAX_VALUE).build();}
public ConnectionConfiguration beam_f33030_0(String password)
{    checkArgument(password != null, "password can not be null");    return builder().setPassword(password).build();}
private void beam_f33031_0(DisplayData.Builder builder)
{    builder.add(DisplayData.item("serverUri", getServerUri()));    builder.add(DisplayData.item("topic", getTopic()));    builder.addIfNotNull(DisplayData.item("clientId", getClientId()));    builder.addIfNotNull(DisplayData.item("username", getUsername()));}
private MQTT beam_f33032_1() throws Exception
{        MQTT client = new MQTT();    client.setHost(getServerUri());    if (getUsername() != null) {                client.setUserName(getUsername());        client.setPassword(getPassword());    }    if (getClientId() != null) {        String clientId = getClientId() + "-" + UUID.randomUUID().toString();                client.setClientId(clientId);    } else {        String clientId = UUID.randomUUID().toString();                client.setClientId(clientId);    }    return client;}
private void beam_f33040_0(java.io.ObjectInputStream stream) throws IOException, ClassNotFoundException
{    stream.defaultReadObject();    messages = new ArrayList<>();}
public boolean beam_f33041_0(Object other)
{    if (other instanceof MqttCheckpointMark) {        MqttCheckpointMark that = (MqttCheckpointMark) other;        return Objects.equals(this.clientId, that.clientId) && Objects.equals(this.oldestMessageTimestamp, that.oldestMessageTimestamp) && Objects.deepEquals(this.messages, that.messages);    } else {        return false;    }}
public int beam_f33042_0()
{    return Objects.hash(clientId, oldestMessageTimestamp, messages);}
public void beam_f33050_1() throws IOException
{        try {        if (connection != null) {            connection.disconnect();        }    } catch (Exception e) {        throw new IOException(e);    }}
public Instant beam_f33051_0()
{    return checkpointMark.oldestMessageTimestamp;}
public UnboundedSource.CheckpointMark beam_f33052_0()
{    return checkpointMark;}
public void beam_f33060_1() throws Exception
{        client = spec.connectionConfiguration().createClient();        connection = client.blockingConnection();    connection.connect();}
public void beam_f33061_1(ProcessContext context) throws Exception
{    byte[] payload = context.element();        connection.publish(spec.connectionConfiguration().getTopic(), payload, QoS.AT_LEAST_ONCE, false);}
public void beam_f33062_1() throws Exception
{    if (connection != null) {                connection.disconnect();    }}
public static Read beam_f33070_0(Schema schema)
{    return new AutoValue_ParquetIO_Read.Builder().setSchema(schema).build();}
public static ReadFiles beam_f33071_0(Schema schema)
{    return new AutoValue_ParquetIO_ReadFiles.Builder().setSchema(schema).build();}
public Read beam_f33072_0(ValueProvider<String> filepattern)
{    return toBuilder().setFilepattern(filepattern).build();}
public long beam_f33080_0() throws IOException
{    return seekableByteChannel.position();}
public void beam_f33081_0(long newPos) throws IOException
{    seekableByteChannel.position(newPos);}
public static Sink beam_f33082_0(Schema schema)
{    return new AutoValue_ParquetIO_Sink.Builder().setJsonSchema(schema.toString()).setCompressionCodec(CompressionCodecName.SNAPPY).build();}
public long beam_f33090_0()
{    return 0;}
public long beam_f33091_0() throws IOException
{    return position;}
public void beam_f33092_0(int b) throws IOException
{    position++;    outputStream.write(b);}
public void beam_f33100_0()
{    DisplayData displayData = DisplayData.from(ParquetIO.read(SCHEMA).from("foo.parquet"));    Assert.assertThat(displayData, hasDisplayItem("filePattern", "foo.parquet"));}
public static Read beam_f33101_0()
{    return new AutoValue_RabbitMqIO_Read.Builder().setQueueDeclare(false).setMaxReadTime(null).setMaxNumRecords(Long.MAX_VALUE).setUseCorrelationId(false).build();}
public static Write beam_f33102_0()
{    return new AutoValue_RabbitMqIO_Write.Builder().setExchangeDeclare(false).setQueueDeclare(false).build();}
public Read beam_f33110_0(long maxNumRecords)
{    checkArgument(maxReadTime() == null, "maxNumRecord and maxReadTime are exclusive");    return builder().setMaxNumRecords(maxNumRecords).build();}
public Read beam_f33111_0(Duration maxReadTime)
{    checkArgument(maxNumRecords() == Long.MAX_VALUE, "maxNumRecord and maxReadTime are exclusive");    return builder().setMaxReadTime(maxReadTime).build();}
public PCollection<RabbitMqMessage> beam_f33112_0(PBegin input)
{    org.apache.beam.sdk.io.Read.Unbounded<RabbitMqMessage> unbounded = org.apache.beam.sdk.io.Read.from(new RabbitMQSource(this));    PTransform<PBegin, PCollection<RabbitMqMessage>> transform = unbounded;    if (maxNumRecords() < Long.MAX_VALUE || maxReadTime() != null) {        transform = unbounded.withMaxReadTime(maxReadTime()).withMaxNumRecords(maxNumRecords());    }    return input.getPipeline().apply(transform);}
public UnboundedSource.CheckpointMark beam_f33120_0()
{    return checkpointMark;}
public RabbitMQSource beam_f33121_0()
{    return source;}
public byte[] beam_f33122_0()
{    if (current == null) {        throw new NoSuchElementException();    }    if (currentRecordId != null) {        return currentRecordId;    } else {        return "".getBytes(StandardCharsets.UTF_8);    }}
public Write beam_f33130_0(boolean exchangeDeclare)
{    return builder().setExchangeDeclare(exchangeDeclare).build();}
public Write beam_f33131_0(String queue)
{    checkArgument(queue != null, "queue can not be null");    return builder().setQueue(queue).build();}
public Write beam_f33132_0(boolean queueDeclare)
{    return builder().setQueueDeclare(queueDeclare).build();}
public String beam_f33140_0()
{    return contentEncoding;}
public Map<String, Object> beam_f33141_0()
{    return headers;}
public Integer beam_f33142_0()
{    return deliveryMode;}
public String beam_f33150_0()
{    return userId;}
public String beam_f33151_0()
{    return appId;}
public String beam_f33152_0()
{    return clusterId;}
public void beam_f33160_0() throws Exception
{    final int maxNumRecords = 1000;    List<RabbitMqMessage> data = generateRecords(maxNumRecords).stream().map(bytes -> new RabbitMqMessage(bytes)).collect(Collectors.toList());    p.apply(Create.of(data)).apply(RabbitMqIO.write().withUri("amqp://guest:guest@localhost:" + port).withQueue("TEST"));    final List<String> received = new ArrayList<>();    ConnectionFactory connectionFactory = new ConnectionFactory();    connectionFactory.setUri("amqp://guest:guest@localhost:" + port);    Connection connection = null;    Channel channel = null;    try {        connection = connectionFactory.newConnection();        channel = connection.createChannel();        channel.queueDeclare("TEST", true, false, false, null);        Consumer consumer = new TestConsumer(channel, received);        channel.basicConsume("TEST", true, consumer);        p.run();        while (received.size() < maxNumRecords) {            Thread.sleep(500);        }        assertEquals(maxNumRecords, received.size());        for (int i = 0; i < maxNumRecords; i++) {            assertTrue(received.contains("Test " + i));        }    } finally {        if (channel != null) {            channel.close();        }        if (connection != null) {            connection.close();        }    }}
public void beam_f33161_0() throws Exception
{    final int maxNumRecords = 1000;    List<RabbitMqMessage> data = generateRecords(maxNumRecords).stream().map(bytes -> new RabbitMqMessage(bytes)).collect(Collectors.toList());    p.apply(Create.of(data)).apply(RabbitMqIO.write().withUri("amqp://guest:guest@localhost:" + port).withExchange("WRITE", "fanout"));    final List<String> received = new ArrayList<>();    ConnectionFactory connectionFactory = new ConnectionFactory();    connectionFactory.setUri("amqp://guest:guest@localhost:" + port);    Connection connection = null;    Channel channel = null;    try {        connection = connectionFactory.newConnection();        channel = connection.createChannel();        channel.exchangeDeclare("WRITE", "fanout");        String queueName = channel.queueDeclare().getQueue();        channel.queueBind(queueName, "WRITE", "");        Consumer consumer = new TestConsumer(channel, received);        channel.basicConsume(queueName, true, consumer);        p.run();        while (received.size() < maxNumRecords) {            Thread.sleep(500);        }        assertEquals(maxNumRecords, received.size());        for (int i = 0; i < maxNumRecords; i++) {            assertTrue(received.contains("Test " + i));        }    } finally {        if (channel != null) {            channel.close();        }        if (connection != null) {            connection.close();        }    }}
private static List<byte[]> beam_f33162_0(int maxNumRecords)
{    return IntStream.range(0, maxNumRecords).mapToObj(i -> ("Test " + i).getBytes(StandardCharsets.UTF_8)).collect(Collectors.toList());}
public RedisConnectionConfiguration beam_f33170_0()
{    return builder().setSsl(true).build();}
public Jedis beam_f33171_0()
{    Jedis jedis = new Jedis(host(), port(), timeout(), ssl());    if (auth() != null) {        jedis.auth(auth());    }    return jedis;}
public void beam_f33172_0(DisplayData.Builder builder)
{    builder.add(DisplayData.item("host", host()));    builder.add(DisplayData.item("port", port()));    builder.addIfNotNull(DisplayData.item("timeout", timeout()));    builder.add(DisplayData.item("ssl", ssl()));}
public Read beam_f33180_0(RedisConnectionConfiguration connection)
{    checkArgument(connection != null, "connection can not be null");    return builder().setConnectionConfiguration(connection).build();}
public Read beam_f33181_0(int batchSize)
{    return builder().setBatchSize(batchSize).build();}
public void beam_f33182_0(DisplayData.Builder builder)
{    connectionConfiguration().populateDisplayData(builder);}
public void beam_f33190_0()
{    jedis = connectionConfiguration.connect();}
public void beam_f33191_0()
{    jedis.close();}
public void beam_f33192_0(ProcessContext processContext) throws Exception
{    ScanParams scanParams = new ScanParams();    scanParams.match(processContext.element());    String cursor = ScanParams.SCAN_POINTER_START;    boolean finished = false;    while (!finished) {        ScanResult<String> scanResult = jedis.scan(cursor, scanParams);        List<String> keys = scanResult.getResult();        for (String k : keys) {            processContext.output(k);        }        cursor = scanResult.getCursor();        if (cursor.equals(ScanParams.SCAN_POINTER_START)) {            finished = true;        }    }}
public Write beam_f33200_0(String host, int port)
{    checkArgument(host != null, "host can not be null");    checkArgument(port > 0, "port can not be negative or 0");    return builder().setConnectionConfiguration(connectionConfiguration().withHost(host).withPort(port)).build();}
public Write beam_f33201_0(String auth)
{    checkArgument(auth != null, "auth can not be null");    return builder().setConnectionConfiguration(connectionConfiguration().withAuth(auth)).build();}
public Write beam_f33202_0(int timeout)
{    checkArgument(timeout >= 0, "timeout can not be negative");    return builder().setConnectionConfiguration(connectionConfiguration().withTimeout(timeout)).build();}
private void beam_f33210_0(KV<String, String> record)
{    Method method = spec.method();    Long expireTime = spec.expireTime();    if (Method.APPEND == method) {        writeUsingAppendCommand(record, expireTime);    } else if (Method.SET == method) {        writeUsingSetCommand(record, expireTime);    } else if (Method.LPUSH == method || Method.RPUSH == method) {        writeUsingListCommand(record, method, expireTime);    } else if (Method.SADD == method) {        writeUsingSaddCommand(record, expireTime);    } else if (Method.PFADD == method) {        writeUsingHLLCommand(record, expireTime);    } else if (Method.INCRBY == method) {        writeUsingIncrBy(record);    } else if (Method.DECRBY == method) {        writeUsingDecrBy(record);    }}
private void beam_f33211_0(KV<String, String> record, Long expireTime)
{    String key = record.getKey();    String value = record.getValue();    pipeline.append(key, value);    setExpireTimeWhenRequired(key, expireTime);}
private void beam_f33212_0(KV<String, String> record, Long expireTime)
{    String key = record.getKey();    String value = record.getValue();    if (expireTime != null) {        pipeline.psetex(key, expireTime, value);    } else {        pipeline.set(key, value);    }}
public void beam_f33220_0()
{    jedis.close();}
public static void beam_f33221_0() throws Exception
{    port = NetworkTestHelper.getAvailableLocalPort();    server = new RedisServer(port);    server.start();    client = RedisConnectionConfiguration.create(REDIS_HOST, port).connect();}
public static void beam_f33222_0()
{    client.close();    server.stop();}
public void beam_f33230_0() throws Exception
{    String key = "key_incr";    List<String> values = Arrays.asList("0", "1", "2", "-3", "2", "4", "0", "5");    List<KV<String, String>> data = buildConstantKeyList(key, values);    PCollection<KV<String, String>> write = p.apply(Create.of(data));    write.apply(RedisIO.write().withEndpoint(REDIS_HOST, port).withMethod(Method.INCRBY));    p.run();    long count = Long.parseLong(client.get(key));    assertEquals(11, count);}
public void beam_f33231_0() throws Exception
{    String key = "key_decr";    List<String> values = Arrays.asList("-10", "1", "2", "-3", "2", "4", "0", "5");    List<KV<String, String>> data = buildConstantKeyList(key, values);    PCollection<KV<String, String>> write = p.apply(Create.of(data));    write.apply(RedisIO.write().withEndpoint(REDIS_HOST, port).withMethod(Method.DECRBY));    p.run();    long count = Long.parseLong(client.get(key));    assertEquals(-1, count);}
private static List<KV<String, String>> beam_f33232_0(String key, List<String> values)
{    List<KV<String, String>> data = new ArrayList<>();    for (String value : values) {        data.add(KV.of(key, value));    }    return data;}
public static JavaBinCodecCoder<T> beam_f33240_0(Class<T> clazz)
{    return new JavaBinCodecCoder<>(clazz);}
public void beam_f33241_0(T value, OutputStream outStream) throws IOException
{    if (value == null) {        throw new CoderException("cannot encode a null SolrDocument");    }    ByteArrayOutputStream baos = new ByteArrayOutputStream();    JavaBinCodec codec = new JavaBinCodec();    codec.marshal(value, baos);    byte[] bytes = baos.toByteArray();    VarInt.encode(bytes.length, outStream);    outStream.write(bytes);}
public T beam_f33242_0(InputStream inStream) throws IOException
{    DataInputStream in = new DataInputStream(inStream);    int len = VarInt.decodeInt(in);    if (len < 0) {        throw new CoderException("Invalid encoded SolrDocument length: " + len);    }    JavaBinCodec codec = new JavaBinCodec();    return (T) codec.unmarshal(new BoundedInputStream(in, len));}
private HttpClient beam_f33250_0()
{            ModifiableSolrParams params = new ModifiableSolrParams();    params.set(HttpClientUtil.PROP_BASIC_AUTH_USER, getUsername());    params.set(HttpClientUtil.PROP_BASIC_AUTH_PASS, getPassword());    return HttpClientUtil.createClient(params);}
 AuthorizedSolrClient<CloudSolrClient> beam_f33251_0()
{    CloudSolrClient solrClient = new CloudSolrClient(getZkHost(), createHttpClient());    return new AuthorizedSolrClient<>(solrClient, this);}
 AuthorizedSolrClient<HttpSolrClient> beam_f33252_0(String shardUrl)
{    HttpSolrClient solrClient = new HttpSolrClient(shardUrl, createHttpClient());    return new AuthorizedSolrClient<>(solrClient, this);}
public PCollection<SolrDocument> beam_f33260_0(PBegin input)
{    checkArgument(getConnectionConfiguration() != null, "withConnectionConfiguration() is required");    checkArgument(getCollection() != null, "from() is required");    return input.apply("Create", Create.of(this)).apply("Split", ParDo.of(new SplitFn())).apply("Reshuffle", Reshuffle.viaRandomKey()).apply("Read", ParDo.of(new ReadFn()));}
public void beam_f33261_0(DisplayData.Builder builder)
{    super.populateDisplayData(builder);    builder.addIfNotNull(DisplayData.item("query", getQuery()));    getConnectionConfiguration().populateDisplayData(builder);}
 static ReplicaInfo beam_f33262_0(Replica replica)
{    return new AutoValue_SolrIO_ReplicaInfo(replica.getStr(ZkStateReader.CORE_NAME_PROP), replica.getCoreUrl(), replica.getStr(ZkStateReader.BASE_URL_PROP));}
public void beam_f33270_0() throws Exception
{    solrClient = spec.getConnectionConfiguration().createClient();    retryBackoff = FluentBackoff.DEFAULT.withMaxRetries(    0).withInitialBackoff(RETRY_INITIAL_BACKOFF);    if (spec.getRetryConfiguration() != null) {                        retryBackoff = retryBackoff.withMaxRetries(spec.getRetryConfiguration().getMaxAttempts() - 1).withMaxCumulativeBackoff(spec.getRetryConfiguration().getMaxDuration());    }}
public void beam_f33271_0(StartBundleContext context)
{    batch = new ArrayList<>();}
public void beam_f33272_0(ProcessContext context) throws Exception
{    SolrInputDocument document = context.element();    batch.add(document);    if (batch.size() >= spec.getMaxBatchSize()) {        flushBatch();    }}
public static void beam_f33280_0() throws Exception
{    solrClient.close();}
public void beam_f33281_0() throws Exception
{    SolrIOTestUtils.clearCollection(SOLR_COLLECTION, solrClient);}
public void beam_f33282_0() throws IOException
{    thrown.expect(SolrException.class);    String zkAddress = cluster.getZkServer().getZkAddress();    SolrIO.ConnectionConfiguration connectionConfiguration = SolrIO.ConnectionConfiguration.create(zkAddress).withBasicCredentials("solr", "wrongpassword");    try (AuthorizedSolrClient solrClient = connectionConfiguration.createClient()) {        SolrIOTestUtils.insertTestDocuments(SOLR_COLLECTION, NUM_DOCS, solrClient);    }}
 static void beam_f33290_0(String collection, int numShards, int replicationFactor, AuthorizedSolrClient client) throws Exception
{    CollectionAdminRequest.Create create = new CollectionAdminRequest.Create().setCollectionName(collection).setNumShards(numShards).setReplicationFactor(replicationFactor).setMaxShardsPerNode(2);    client.process(create);}
 static void beam_f33291_0(String collection, long numDocs, AuthorizedSolrClient client) throws IOException
{    List<SolrInputDocument> data = createDocuments(numDocs);    try {        UpdateRequest updateRequest = new UpdateRequest();        updateRequest.setAction(UpdateRequest.ACTION.COMMIT, true, true);        updateRequest.add(data);        client.process(collection, updateRequest);    } catch (SolrServerException e) {        throw new IOException("Failed to insert test documents to collection", e);    }}
 static void beam_f33292_0(String collection, AuthorizedSolrClient client) throws IOException
{    try {        UpdateRequest updateRequest = new UpdateRequest();        updateRequest.setAction(UpdateRequest.ACTION.COMMIT, true, true);        updateRequest.deleteByQuery("*:*");        client.process(collection, updateRequest);    } catch (SolrServerException e) {        throw new IOException(e);    }}
public void beam_f33300_0(long currentElement)
{    SyntheticDelay.delay(options.nextInitializeDelay(currentElement), options.cpuUtilizationInMixedDelay, options.delayType, new Random(currentElement));}
public static long beam_f33301_0(Duration delay, double cpuUtilizationInMixedDelay, SyntheticOptions.DelayType delayType, Random rnd)
{    switch(delayType) {        case CPU:            cpuDelay(delay.getMillis());            return 0;        case SLEEP:            Uninterruptibles.sleepUninterruptibly(Math.max(0L, delay.getMillis()), TimeUnit.MILLISECONDS);            return delay.getMillis();        case MIXED:                                                long sleepMillis = 0;            for (long i = 0; i < delay.getMillis(); i++) {                if (rnd.nextDouble() < cpuUtilizationInMixedDelay) {                    delay(new Duration(1), 0.0, SyntheticOptions.DelayType.CPU, rnd);                } else {                    sleepMillis += delay(new Duration(1), 0.0, SyntheticOptions.DelayType.SLEEP, rnd);                }            }            return sleepMillis;        default:            throw new IllegalArgumentException("Unknown delay type " + delayType);    }}
private static void beam_f33302_0(long delayMillis)
{                    long cpuMicros = delayMillis * 1000;    Stopwatch timer = Stopwatch.createUnstarted();    while (timer.elapsed(TimeUnit.MICROSECONDS) < cpuMicros) {                        timer.start();        long p = INIT_PLAINTEXT;        while (true) {            long t = Hashing.murmur3_128().hashLong(p).asLong();            if ((t & MASK) == (HASH & MASK)) {                break;            }            p++;        }        timer.stop();    }}
public List<SyntheticBoundedSource> beam_f33310_1(long desiredBundleSizeBytes, PipelineOptions options) throws Exception
{            int desiredNumBundles = (sourceOptions.forceNumInitialBundles == null) ? ((int) Math.ceil(1.0 * getEstimatedSizeBytes(options) / desiredBundleSizeBytes)) : sourceOptions.forceNumInitialBundles;    List<SyntheticBoundedSource> res = bundleSplitter.getBundleSizes(desiredNumBundles, this.getStartOffset(), this.getEndOffset()).stream().map(offsetRange -> createSourceForSubrange(offsetRange.getFrom(), offsetRange.getTo())).collect(Collectors.toList());        return res;}
public synchronized SyntheticBoundedSource beam_f33311_0()
{    return (SyntheticBoundedSource) super.getCurrentSource();}
protected long beam_f33312_0() throws IllegalStateException
{    return currentOffset;}
public static Sampler beam_f33321_0(final RealDistribution dist)
{    return new Sampler() {        private static final long serialVersionUID = 0L;        @Override        public double sample(long seed) {            dist.reseedRandomGenerator(seed);            return dist.sample();        }        @Override        public Object getDistribution() {            return dist;        }    };}
public double beam_f33322_0(long seed)
{    dist.reseedRandomGenerator(seed);    return dist.sample();}
public Object beam_f33323_0()
{    return dist;}
public HashFunction beam_f33331_0()
{        if (hashFunction == null) {        this.hashFunction = Hashing.murmur3_128(seed);    }    return hashFunction;}
public Sampler beam_f33332_0(JsonParser jp, DeserializationContext ctxt) throws IOException
{    JsonNode node = jp.getCodec().readTree(jp);    String type = node.get("type").asText();    switch(type) {        case "uniform":            {                double lowerBound = node.get("lower").asDouble();                double upperBound = node.get("upper").asDouble();                checkArgument(lowerBound >= 0, "The lower bound of uniform distribution should be a non-negative number, " + "but found %s.", lowerBound);                return fromRealDistribution(new UniformRealDistribution(lowerBound, upperBound));            }        case "exp":            {                double mean = node.get("mean").asDouble();                return fromRealDistribution(new ExponentialDistribution(mean));            }        case "normal":            {                double mean = node.get("mean").asDouble();                double stddev = node.get("stddev").asDouble();                checkArgument(mean >= 0, "The mean of normal distribution should be a non-negative number, but found %s.", mean);                return fromRealDistribution(new NormalDistribution(mean, stddev));            }        case "const":            {                double constant = node.get("const").asDouble();                checkArgument(constant >= 0, "The value of constant distribution should be a non-negative number, but found %s.", constant);                return fromRealDistribution(new ConstantRealDistribution(constant));            }        case "zipf":            {                double param = node.get("param").asDouble();                final double multiplier = node.has("multiplier") ? node.get("multiplier").asDouble() : 1.0;                checkArgument(param > 1, "The parameter of the Zipf distribution should be > 1, but found %s.", param);                checkArgument(multiplier >= 0, "The multiplier of the Zipf distribution should be >= 0, but found %s.", multiplier);                final ZipfDistribution dist = new ZipfDistribution(100, param);                return scaledSampler(fromIntegerDistribution(dist), multiplier);            }        default:            {                throw new IllegalArgumentException("Unknown distribution type: " + type);            }    }}
public void beam_f33333_0()
{    checkArgument(keySizeBytes > 0, "keySizeBytes should be a positive number, but found %s", keySizeBytes);    checkArgument(valueSizeBytes >= 0, "valueSizeBytes should be a non-negative number, but found %s", valueSizeBytes);    checkArgument(numHotKeys >= 0, "numHotKeys should be a non-negative number, but found %s", numHotKeys);    checkArgument(hotKeyFraction >= 0, "hotKeyFraction should be a non-negative number, but found %s", hotKeyFraction);    if (hotKeyFraction > 0) {        int intBytes = Integer.SIZE / 8;        checkArgument(keySizeBytes >= intBytes, "Allowing hot keys (hotKeyFraction=%s) requires keySizeBytes " + "to be at least %s, but found %s", hotKeyFraction, intBytes, keySizeBytes);    }}
public long beam_f33342_0()
{    return endPosition;}
public Duration beam_f33343_0(long seed)
{    return Duration.millis((long) initializeDelayDistribution.sample(seed));}
public Duration beam_f33344_0(long seed)
{    return Duration.millis((long) processingTimeDelayDistribution.sample(seed));}
public void beam_f33352_0() throws Exception
{    if (options.perBundleDelay > 0) {        SyntheticDelay.delay(Duration.millis(options.perBundleDelay), options.cpuUtilizationInMixedDelay, options.perBundleDelayType, new Random());    }}
public void beam_f33353_0()
{    super.validate();    checkArgument(outputRecordsPerInputRecord >= 0, "outputRecordsPerInputRecord should be a non-negative number, but found %s.", outputRecordsPerInputRecord);    checkArgument(perBundleDelay >= 0, "perBundleDelay should be a non-negative number, but found %s.", perBundleDelay);    if (maxWorkerThroughput >= 0) {        checkArgument(perBundleDelay == 0, "maxWorkerThroughput and perBundleDelay cannot be enabled simultaneously.");    }}
public Coder<KV<byte[], byte[]>> beam_f33354_0()
{    return KvCoder.of(ByteArrayCoder.of(), ByteArrayCoder.of());}
public boolean beam_f33362_0()
{    currentOffset = SyntheticUnboundedSource.this.startOffset;    delay.delayStart(currentOffset);    return advance();}
public boolean beam_f33363_0()
{    currentOffset++;    processingTime = new Instant();    eventTime = processingTime.minus(sourceOptions.nextProcessingTimeDelay(currentOffset));    SyntheticSourceOptions.Record record = getCurrentSource().sourceOptions.genRecord(currentOffset);    currentKVPair = record.kv;    delay.delayRecord(record);    return currentOffset < source.endOffset;}
public Instant beam_f33364_0()
{    return syntheticWatermark.calculateNew(currentOffset, processingTime);}
public void beam_f33373_0()
{    long expectedBundleSize = 4;    options.bundleSizeDistribution = fromRealDistribution(new ConstantRealDistribution(2));    options.numRecords = 16;    splitter = new BundleSplitter(options);    List<OffsetRange> bundleSizes = splitter.getBundleSizes(4, 0, options.numRecords);    bundleSizes.stream().map(range -> range.getTo() - range.getFrom()).forEach(size -> assertEquals(expectedBundleSize, size.intValue()));}
public void beam_f33374_0()
{    int desiredNumberOfBundles = 2;    options.bundleSizeDistribution = fromRealDistribution(new ConstantRealDistribution(2));    splitter = new BundleSplitter(options);    List<OffsetRange> bundleSizes = splitter.getBundleSizes(desiredNumberOfBundles, 0, options.numRecords);    assertEquals(bundleSizes.get(0).getTo(), bundleSizes.get(1).getFrom());    assertEquals(bundleSizes.get(0).getTo(), bundleSizes.get(1).getFrom());    assertEquals(desiredNumberOfBundles, bundleSizes.size());}
public void beam_f33375_0()
{    testSourceOptions.splitPointFrequencyRecords = 1;    testSourceOptions.numRecords = 10;    testSourceOptions.keySizeBytes = 10;    testSourceOptions.valueSizeBytes = 20;    testSourceOptions.numHotKeys = 3;    testSourceOptions.hotKeyFraction = 0.3;    testSourceOptions.setSeed(123456);    testSourceOptions.bundleSizeDistribution = fromIntegerDistribution(new ZipfDistribution(100, 2.5));    testSourceOptions.forceNumInitialBundles = null;}
private void beam_f33383_0(long splitPointFrequency) throws Exception
{    PipelineOptions options = PipelineOptionsFactory.create();    testSourceOptions.splitPointFrequencyRecords = splitPointFrequency;    SyntheticBoundedSource source = new SyntheticBoundedSource(testSourceOptions);    SourceTestUtils.assertSplitAtFractionExhaustive(source, options);        SourceTestUtils.assertSplitAtFractionFails(source, 5, 0.3, options);    SourceTestUtils.assertSplitAtFractionSucceedsAndConsistent(source, 1, 0.3, options);}
public void beam_f33384_0() throws Exception
{    testSplitIntoBundlesP(1);    testSplitIntoBundlesP(-1);    testSplitIntoBundlesP(5);    PipelineOptions options = PipelineOptionsFactory.create();    testSourceOptions.forceNumInitialBundles = 37;    assertEquals(37, new SyntheticBoundedSource(testSourceOptions).split(42, options).size());}
private void beam_f33385_0(long splitPointFrequency) throws Exception
{    PipelineOptions options = PipelineOptionsFactory.create();    testSourceOptions.splitPointFrequencyRecords = splitPointFrequency;    testSourceOptions.numRecords = 100;    SyntheticBoundedSource source = new SyntheticBoundedSource(testSourceOptions);    SourceTestUtils.assertSourcesEqualReferenceSource(source, source.split(10, options), options);    SourceTestUtils.assertSourcesEqualReferenceSource(source, source.split(40, options), options);    SourceTestUtils.assertSourcesEqualReferenceSource(source, source.split(100, options), options);}
public void beam_f33393_0() throws Exception
{    String syntheticOptions = "{\"seed\":12345," + "\"delayDistribution\":{\"type\":\"uniform\",\"lower\":0,\"upper\":100}}";    SyntheticOptions sourceOptions = optionsFromString(syntheticOptions, SyntheticOptions.class);    assertEquals(0, (long) ((UniformRealDistribution) (sourceOptions.delayDistribution.getDistribution())).getSupportLowerBound());    assertEquals(100, (long) ((UniformRealDistribution) (sourceOptions.delayDistribution.getDistribution())).getSupportUpperBound());}
public void beam_f33394_0() throws Exception
{    String syntheticOptions = "{\"seed\":12345," + "\"delayDistribution\":{\"type\":\"normal\",\"mean\":100,\"stddev\":50}}";    SyntheticOptions sourceOptions = optionsFromString(syntheticOptions, SyntheticOptions.class);    assertEquals(100, (long) ((NormalDistribution) (sourceOptions.delayDistribution.getDistribution())).getMean());    assertEquals(50, (long) ((NormalDistribution) (sourceOptions.delayDistribution.getDistribution())).getStandardDeviation());}
public void beam_f33395_0() throws Exception
{    String syntheticOptions = "{\"seed\":12345," + "\"delayDistribution\":{\"type\":\"exp\",\"mean\":10}}";    SyntheticOptions sourceOptions = optionsFromString(syntheticOptions, SyntheticOptions.class);    assertEquals(10, (long) ((ExponentialDistribution) (sourceOptions.delayDistribution.getDistribution())).getMean());}
public void beam_f33403_0() throws IOException
{    String optionsJson = "{\"numRecords\":100,\"splitPointFrequencyRecords\":10,\"keySizeBytes\":10," + "\"valueSizeBytes\":20,\"numHotKeys\":3," + "\"hotKeyFraction\":0.3,\"seed\":123456," + "\"bundleSizeDistribution\":{\"type\":\"const\",\"const\":42}," + "\"forceNumInitialBundles\":10}";    sourceOptions = SyntheticTestUtils.optionsFromString(optionsJson, SyntheticSourceOptions.class);    source = new SyntheticUnboundedSource(sourceOptions);    checkpoint = new SyntheticRecordsCheckpoint(0, sourceOptions.numRecords);}
public void beam_f33404_0()
{    CoderProperties.coderSerializable(source.getCheckpointMarkCoder());}
public void beam_f33405_0()
{    CoderProperties.coderSerializable(source.getOutputCoder());}
public String beam_f33413_0()
{    return fileLocation;}
public boolean beam_f33414_0()
{    return error == null;}
public Throwable beam_f33415_0()
{    checkState(error != null, "This is a successful ParseResult");    return error.getThrowable();}
public static Parse beam_f33423_0()
{    return new AutoValue_TikaIO_Parse.Builder().build();}
public static ParseFiles beam_f33424_0()
{    return new AutoValue_TikaIO_ParseFiles.Builder().build();}
public Parse beam_f33425_0(String filepattern)
{    return this.filepattern(ValueProvider.StaticValueProvider.of(filepattern));}
public PCollection<ParseResult> beam_f33433_0(PCollection<ReadableFile> input)
{    return input.apply(ParDo.of(new ParseToStringFn(this)));}
public void beam_f33434_0(DisplayData.Builder builder)
{    super.populateDisplayData(builder);    if (getTikaConfigPath() != null) {        builder.add(DisplayData.item("tikaConfigPath", getTikaConfigPath()).withLabel("TikaConfig Path"));    }    Metadata metadata = getInputMetadata();    if (metadata != null) {                builder.add(DisplayData.item("inputMetadata", metadata.toString().trim()).withLabel("Input Metadata"));    }    builder.addIfNotNull(DisplayData.item("contentTypeHint", getContentTypeHint()).withLabel("Content type hint"));}
public void beam_f33435_0() throws Exception
{    if (spec.getTikaConfigPath() != null) {        ResourceId configResource = FileSystems.matchSingleFileSpec(spec.getTikaConfigPath().get()).resourceId();        tikaConfig = new TikaConfig(Channels.newInputStream(FileSystems.open(configResource)));    }}
public void beam_f33443_0()
{    TikaIO.Parse parse = TikaIO.parse().filepattern("file.pdf");    DisplayData displayData = DisplayData.from(parse);    assertThat(displayData, hasDisplayItem("filePattern", "file.pdf"));    assertEquals(1, displayData.items().size());}
public void beam_f33444_0()
{    TikaIO.ParseFiles parseFiles = TikaIO.parseFiles().withTikaConfigPath("/tikaConfigPath").withContentTypeHint("application/pdf");    DisplayData displayData = DisplayData.from(parseFiles);    assertThat(displayData, hasDisplayItem("tikaConfigPath", "/tikaConfigPath"));    assertThat(displayData, hasDisplayItem("contentTypeHint", "application/pdf"));}
public Class<T> beam_f33445_0()
{    return jaxbClass;}
private JAXBContext beam_f33453_0() throws JAXBException
{    if (jaxbContext == null) {        synchronized (this) {            if (jaxbContext == null) {                jaxbContext = JAXBContext.newInstance(jaxbClass);            }        }    }    return jaxbContext;}
public TypeDescriptor<T> beam_f33454_0()
{    return TypeDescriptor.of(jaxbClass);}
public boolean beam_f33455_0(Object other)
{    if (other == this) {        return true;    }    if (!(other instanceof JAXBCoder)) {        return false;    }    JAXBCoder<?> that = (JAXBCoder<?>) other;    return Objects.equals(this.jaxbClass, that.jaxbClass);}
private MappingConfiguration<T> beam_f33465_0(Charset charset)
{    return toBuilder().setCharset(charset.name()).build();}
private MappingConfiguration<T> beam_f33466_0(ValidationEventHandler validationEventHandler)
{    return toBuilder().setValidationEventHandler(validationEventHandler).build();}
private void beam_f33467_0()
{    checkArgument(getRootElement() != null, "withRootElement() is required");    checkArgument(getRecordElement() != null, "withRecordElement() is required");    checkArgument(getRecordClass() != null, "withRecordClass() is required");    checkArgument(getCharset() != null, "withCharset() is required");}
public Read<T> beam_f33475_0(Class<T> recordClass)
{    return withConfiguration(getConfiguration().withRecordClass(recordClass));}
public Read<T> beam_f33476_0(long minBundleSize)
{    return toBuilder().setMinBundleSize(minBundleSize).build();}
public Read<T> beam_f33477_0(CompressionType compressionType)
{    return withCompression(compressionType.canonical);}
public ReadFiles<T> beam_f33485_0(String rootElement)
{    return withConfiguration(getConfiguration().withRootElement(rootElement));}
public ReadFiles<T> beam_f33486_0(String recordElement)
{    return withConfiguration(getConfiguration().withRecordElement(recordElement));}
public ReadFiles<T> beam_f33487_0(Class<T> recordClass)
{    return withConfiguration(getConfiguration().withRecordClass(recordClass));}
public Write<T> beam_f33495_0(Charset charset)
{    return toBuilder().setCharset(charset.name()).build();}
public PDone beam_f33496_0(PCollection<T> input)
{    checkArgument(getRecordClass() != null, "withRecordClass() is required");    checkArgument(getRootElement() != null, "withRootElement() is required");    checkArgument(getFilenamePrefix() != null, "to() is required");    checkArgument(getCharset() != null, "withCharset() is required");    try {        JAXBContext.newInstance(getRecordClass());    } catch (JAXBException e) {        throw new RuntimeException("Error binding classes to a JAXB Context.", e);    }    ResourceId prefix = FileSystems.matchNewResource(getFilenamePrefix(), false);    input.apply(FileIO.<T>write().via(sink(getRecordClass()).withCharset(Charset.forName(getCharset())).withRootElement(getRootElement())).to(prefix.getCurrentDirectory().toString()).withPrefix(prefix.getFilename()).withSuffix(".xml").withIgnoreWindowing());    return PDone.in(input.getPipeline());}
public void beam_f33497_0(DisplayData.Builder builder)
{    builder.addIfNotNull(DisplayData.item("rootElement", getRootElement()).withLabel("XML Root Element")).addIfNotNull(DisplayData.item("recordClass", getRecordClass()).withLabel("XML Record Class")).addIfNotNull(DisplayData.item("charset", getCharset()).withLabel("Charset"));}
protected FileBasedReader<T> beam_f33505_0(PipelineOptions options)
{    return new XMLReader<>(this);}
public Coder<T> beam_f33506_0()
{    return JAXBCoder.of(configuration.getRecordClass());}
public synchronized XmlSource<T> beam_f33507_0()
{    return (XmlSource<T>) super.getCurrentSource();}
public String beam_f33515_0()
{    return testString;}
public void beam_f33516_0(String testString)
{    this.testString = testString;}
public int beam_f33517_0()
{    return testInt;}
public void beam_f33525_0(TestType value, OutputStream outStream) throws IOException
{    encode(value, outStream, Context.NESTED);}
public void beam_f33526_0(TestType value, OutputStream outStream, Context context) throws IOException
{    VarIntCoder.of().encode(3, outStream);    jaxbCoder.encode(value, outStream);    VarLongCoder.of().encode(22L, outStream, context);}
public TestType beam_f33527_0(InputStream inStream) throws IOException
{    return decode(inStream, Context.NESTED);}
public void beam_f33535_0()
{    testWriteThenRead(Method.WRITE_AND_READ, BIRDS, StandardCharsets.UTF_8);}
public void beam_f33536_0()
{    testWriteThenRead(Method.WRITE_AND_READ, BIRDS, StandardCharsets.ISO_8859_1);}
private void beam_f33537_0(Method method, List<Bird> birds, Charset charset)
{    switch(method) {        case SINK_AND_READ_FILES:            PCollection<Bird> writeThenRead = mainPipeline.apply(Create.of(birds)).apply(FileIO.<Bird>write().via(XmlIO.sink(Bird.class).withRootElement("birds").withCharset(charset)).to(tmpFolder.getRoot().getAbsolutePath()).withPrefix("birds").withSuffix(".xml")).getPerDestinationOutputFilenames().apply(Values.create()).apply(FileIO.matchAll()).apply(FileIO.readMatches()).apply(XmlIO.<Bird>readFiles().withRecordClass(Bird.class).withRootElement("birds").withRecordElement("bird").withCharset(charset));            PAssert.that(writeThenRead).containsInAnyOrder(birds);            mainPipeline.run();            break;        case WRITE_AND_READ:            mainPipeline.apply(Create.of(birds)).apply(XmlIO.<Bird>write().to(new File(tmpFolder.getRoot(), "birds").getAbsolutePath()).withRecordClass(Bird.class).withRootElement("birds").withCharset(charset));            mainPipeline.run();            PCollection<Bird> readBack = readPipeline.apply(XmlIO.<Bird>read().from(readPipeline.newProvider(new File(tmpFolder.getRoot(), "birds").getAbsolutePath() + "*")).withRecordClass(Bird.class).withRootElement("birds").withRecordElement("bird").withCharset(charset));            PAssert.that(readBack).containsInAnyOrder(birds);            readPipeline.run();            break;    }}
public boolean beam_f33545_0(Object o)
{    if (this == o) {        return true;    }    if (o == null || getClass() != o.getClass()) {        return false;    }    Bird bird = (Bird) o;    if (!name.equals(bird.name)) {        return false;    }    return adjective.equals(bird.adjective);}
public int beam_f33546_0()
{    int result = name.hashCode();    result = 31 * result + adjective.hashCode();    return result;}
public String beam_f33547_0()
{    return String.format("Bird: %s, %s", name, adjective);}
private List<T> beam_f33555_0(Reader<T> reader) throws IOException
{    List<T> results = new ArrayList<>();    for (boolean available = reader.start(); available; available = reader.advance()) {        T train = reader.getCurrent();        results.add(train);    }    return results;}
public void beam_f33556_0() throws IOException
{    File file = tempFolder.newFile("trainXMLTiny");    Files.write(file.toPath(), tinyXML.getBytes(StandardCharsets.UTF_8));    BoundedSource<Train> source = XmlIO.<Train>read().from(file.toPath().toString()).withRootElement("trains").withRecordElement("train").withRecordClass(Train.class).withMinBundleSize(1024).createSource();    List<Train> expectedResults = ImmutableList.of(new Train("Thomas", Train.TRAIN_NUMBER_UNDEFINED, null, null), new Train("Henry", Train.TRAIN_NUMBER_UNDEFINED, null, null), new Train("James", Train.TRAIN_NUMBER_UNDEFINED, null, null));    assertThat(trainsToStrings(expectedResults), containsInAnyOrder(trainsToStrings(readEverythingFromReader(source.createReader(null))).toArray()));}
public void beam_f33557_0() throws IOException
{    File file = tempFolder.newFile("trainXMLTiny");    Files.write(file.toPath(), xmlWithMultiByteChars.getBytes(StandardCharsets.UTF_8));    BoundedSource<Train> source = XmlIO.<Train>read().from(file.toPath().toString()).withRootElement("trains").withRecordElement("train").withRecordClass(Train.class).withMinBundleSize(1024).createSource();    List<Train> expectedResults = ImmutableList.of(new Train("Thomas¥", Train.TRAIN_NUMBER_UNDEFINED, null, null), new Train("Hen¶ry", Train.TRAIN_NUMBER_UNDEFINED, null, null), new Train("Jamßes", Train.TRAIN_NUMBER_UNDEFINED, null, null));    assertThat(trainsToStrings(expectedResults), containsInAnyOrder(trainsToStrings(readEverythingFromReader(source.createReader(null))).toArray()));}
public void beam_f33565_0() throws IOException
{    File file = tempFolder.newFile("trainXMLSmall");    Files.write(file.toPath(), trainXML.getBytes(StandardCharsets.UTF_8));    ValidationEventHandler validationEventHandler = event -> {        throw new RuntimeException("MyCustomValidationEventHandler failure mesage");    };    BoundedSource<WrongTrainType> source = XmlIO.<WrongTrainType>read().from(file.toPath().toString()).withRootElement("trains").withRecordElement("train").withRecordClass(WrongTrainType.class).withValidationEventHandler(validationEventHandler).createSource();    exception.expect(RuntimeException.class);        exception.expectMessage("MyCustomValidationEventHandler failure mesage");    try (Reader<WrongTrainType> reader = source.createReader(null)) {        List<WrongTrainType> results = new ArrayList<>();        for (boolean available = reader.start(); available; available = reader.advance()) {            WrongTrainType train = reader.getCurrent();            results.add(train);        }    }}
public void beam_f33566_0() throws IOException
{    File file = tempFolder.newFile("trainXMLSmall");    Files.write(file.toPath(), trainXML.getBytes(StandardCharsets.UTF_8));    BoundedSource<TinyTrain> source = XmlIO.<TinyTrain>read().from(file.toPath().toString()).withRootElement("trains").withRecordElement("train").withRecordClass(TinyTrain.class).createSource();    List<TinyTrain> expectedResults = ImmutableList.of(new TinyTrain("Thomas"), new TinyTrain("Henry"), new TinyTrain("Toby"), new TinyTrain("Gordon"), new TinyTrain("Emily"), new TinyTrain("Percy"));    assertThat(tinyTrainsToStrings(expectedResults), containsInAnyOrder(tinyTrainsToStrings(readEverythingFromReader(source.createReader(null))).toArray()));}
public void beam_f33567_0() throws IOException
{    File file = tempFolder.newFile("trainXMLSmall");    Files.write(file.toPath(), trainXML.getBytes(StandardCharsets.UTF_8));    BoundedSource<Train> source = XmlIO.<Train>read().from(file.toPath().toString()).withRootElement("trains").withRecordElement("train").withRecordClass(Train.class).createSource();    List<Train> expectedResults = ImmutableList.of(new Train("Thomas", 1, "blue", null), new Train("Henry", 3, "green", null), new Train("Toby", 7, "brown", null), new Train("Gordon", 4, "blue", null), new Train("Emily", -1, "red", null), new Train("Percy", 6, "green", null));    assertThat(trainsToStrings(expectedResults), containsInAnyOrder(trainsToStrings(readEverythingFromReader(source.createReader(null))).toArray()));}
public void beam_f33575_0() throws Exception
{    String fileName = "temp.xml";    List<Train> trains = generateRandomTrainList(100);    File file = createRandomTrainXML(fileName, trains);    BoundedSource<Train> source = XmlIO.<Train>read().from(file.toPath().toString()).withRootElement("trains").withRecordElement("train").withRecordClass(Train.class).withMinBundleSize(10).createSource();    List<? extends BoundedSource<Train>> splits = source.split(256, null);        assertTrue(splits.size() > 2);    List<Train> results = new ArrayList<>();    for (BoundedSource<Train> split : splits) {        results.addAll(readEverythingFromReader(split.createReader(null)));    }    assertThat(trainsToStrings(trains), containsInAnyOrder(trainsToStrings(results).toArray()));}
public void beam_f33576_0() throws Exception
{    PipelineOptions options = PipelineOptionsFactory.create();    String fileName = "temp.xml";    List<Train> trains = generateRandomTrainList(100);    File file = createRandomTrainXML(fileName, trains);    BoundedSource<Train> fileSource = XmlIO.<Train>read().from(file.toPath().toString()).withRootElement("trains").withRecordElement("train").withRecordClass(Train.class).withMinBundleSize(10).createSource();    List<? extends BoundedSource<Train>> splits = fileSource.split(file.length() / 3, null);    for (BoundedSource<Train> splitSource : splits) {        int numItems = readEverythingFromReader(splitSource.createReader(null)).size();                assertSplitAtFractionFails(splitSource, 0, 0.7, options);        assertSplitAtFractionSucceedsAndConsistent(splitSource, 1, 0.7, options);        assertSplitAtFractionSucceedsAndConsistent(splitSource, 15, 0.7, options);        assertSplitAtFractionFails(splitSource, 0, 0.0, options);        assertSplitAtFractionFails(splitSource, 20, 0.3, options);        assertSplitAtFractionFails(splitSource, numItems, 1.0, options);                                assertSplitAtFractionFails(splitSource, numItems, 0.9, options);                                                                        assertSplitAtFractionSucceedsAndConsistent(splitSource, numItems - 1, 0.999, options);    }}
public void beam_f33577_0() throws Exception
{    PipelineOptions options = PipelineOptionsFactory.create();    File file = tempFolder.newFile("trainXMLSmall");    Files.write(file.toPath(), trainXMLWithAllFeaturesSingleByte.getBytes(StandardCharsets.UTF_8));    BoundedSource<Train> source = XmlIO.<Train>read().from(file.toPath().toString()).withRootElement("trains").withRecordElement("train").withRecordClass(Train.class).createSource();    assertSplitAtFractionExhaustive(source, options);}
public static void beam_f33585_0(String[] args) throws Exception
{    int port = Integer.parseInt(args[0]);    System.out.println("Starting expansion service at localhost:" + port);    Server server = ServerBuilder.forPort(port).addService(new ExpansionService()).build();    server.start();    server.awaitTermination();}
 void beam_f33586_0() throws IOException
{    SyntheticSourceOptions coSourceOptions = fromJsonString(options.getCoSourceOptions(), SyntheticSourceOptions.class);    Optional<SyntheticStep> syntheticStep = createStep(options.getStepOptions());    PCollection<KV<byte[], byte[]>> input = pipeline.apply("Read input", readFromSource(sourceOptions));    input = input.apply("Collect start time metrics (input)", ParDo.of(runtimeMonitor));    input = applyWindowing(input);    input = applyStepIfPresent(input, "Synthetic step for input", syntheticStep);    PCollection<KV<byte[], byte[]>> coInput = pipeline.apply("Read co-input", readFromSource(coSourceOptions));    coInput = coInput.apply("Collect start time metrics (co-input)", ParDo.of(runtimeMonitor));    coInput = applyWindowing(coInput, options.getCoInputWindowDurationSec());    coInput = applyStepIfPresent(coInput, "Synthetic step for co-input", syntheticStep);    KeyedPCollectionTuple.of(INPUT_TAG, input).and(CO_INPUT_TAG, coInput).apply("CoGroupByKey", CoGroupByKey.create()).apply("Ungroup and reiterate", ParDo.of(new UngroupAndReiterate(options.getIterations()))).apply("Collect total bytes", ParDo.of(new ByteMonitor(METRICS_NAMESPACE, "totalBytes.count"))).apply("Collect end time metrics", ParDo.of(runtimeMonitor));}
public void beam_f33587_0(ProcessContext c)
{    byte[] key = c.element().getKey();    CoGbkResult elementValue = c.element().getValue();    Iterable<byte[]> inputs = elementValue.getAll(INPUT_TAG);    Iterable<byte[]> coInputs = elementValue.getAll(CO_INPUT_TAG);        for (int i = 0; i < iterations; i++) {        for (byte[] value : inputs) {            if (i == iterations - 1) {                c.output(KV.of(key, value));            }        }        for (byte[] value : coInputs) {            if (i == iterations - 1) {                c.output(KV.of(key, value));            }        }    }}
public static void beam_f33595_0(String[] args) throws IOException
{    new GroupByKeyLoadTest(args).run();}
 static void beam_f33596_0(final PipelineResult pipelineResult, final List<NamedTestResult> testResults) throws IOException
{    Optional<JobFailure> failure = lookForFailure(pipelineResult, testResults);    if (failure.isPresent()) {        JobFailure jobFailure = failure.get();        if (jobFailure.requiresCancelling) {            pipelineResult.cancel();        }        throw new RuntimeException(jobFailure.cause);    }}
private static Optional<JobFailure> beam_f33597_0(PipelineResult pipelineResult, List<NamedTestResult> testResults)
{    PipelineResult.State state = pipelineResult.getState();    Optional<JobFailure> stateRelatedFailure = lookForInvalidState(state);    if (stateRelatedFailure.isPresent()) {        return stateRelatedFailure;    } else {        return lookForMetricResultFailure(testResults);    }}
 Optional<SyntheticStep> beam_f33605_0(String stepOptions) throws IOException
{    if (stepOptions != null && !stepOptions.isEmpty()) {        return Optional.of(new SyntheticStep(fromJsonString(stepOptions, SyntheticStep.Options.class)));    } else {        return Optional.empty();    }}
 PCollection<KV<byte[], byte[]>> beam_f33606_0(PCollection<KV<byte[], byte[]>> input, String name, Optional<SyntheticStep> syntheticStep)
{    if (syntheticStep.isPresent()) {        return input.apply(name, ParDo.of(syntheticStep.get()));    } else {        return input;    }}
 PCollection<T> beam_f33607_0(PCollection<T> input)
{    return applyWindowing(input, options.getInputWindowDurationSec());}
public static void beam_f33615_0(String[] args) throws IOException
{    new ParDoLoadTest(args).run();}
public void beam_f33616_0(ProcessContext processContext)
{    for (int i = 0; i < numberOfOperations; i++) {        for (Counter counter : counters) {            counter.inc();        }    }    processContext.output(processContext.element());}
public static void beam_f33617_0(String[] args) throws IOException
{    options = PipelineOptionsFactory.fromArgs(args).withValidation().as(Options.class);    SyntheticSourceOptions sourceOptions = SyntheticOptions.fromJsonString(options.getSourceOptions(), SyntheticSourceOptions.class);    Pipeline pipeline = Pipeline.create(options);    PCollection<KV<byte[], byte[]>> syntheticData = pipeline.apply("Read synthetic data", Read.from(new SyntheticBoundedSource(sourceOptions)));    if (options.getKafkaBootstrapServerAddress() != null && options.getKafkaTopic() != null) {        writeToKafka(syntheticData);    }    if (options.getPubSubTopic() != null) {        writeToPubSub(syntheticData);    }    if (allKinesisOptionsConfigured()) {        writeToKinesis(syntheticData);    }    pipeline.run().waitUntilFinish();}
private static byte[] beam_f33625_0(KV<byte[], byte[]> input)
{    try {        return encodeToByteArray(RECORD_CODER, input);    } catch (CoderException e) {        throw new RuntimeException(String.format("Couldn't encode element. Exception: %s", e));    }}
private static Map<String, String> beam_f33626_0(KV<byte[], byte[]> input)
{    String key = new String(input.getKey(), UTF_8);    String value = new String(input.getValue(), UTF_8);    HashMap<String, String> map = new HashMap<>();    map.put(key, value);    return map;}
public Result beam_f33627_0() throws IOException
{    NexmarkPerf perf = nexmarkLauncher.run();    return new Result(configuration, perf);}
public Auction beam_f33635_0(InputStream inStream) throws CoderException, IOException
{    long id = LONG_CODER.decode(inStream);    String itemName = STRING_CODER.decode(inStream);    String description = STRING_CODER.decode(inStream);    long initialBid = LONG_CODER.decode(inStream);    long reserve = LONG_CODER.decode(inStream);    Instant dateTime = INSTANT_CODER.decode(inStream);    Instant expires = INSTANT_CODER.decode(inStream);    long seller = LONG_CODER.decode(inStream);    long category = LONG_CODER.decode(inStream);    String extra = STRING_CODER.decode(inStream);    return new Auction(id, itemName, description, initialBid, reserve, dateTime, expires, seller, category, extra);}
public Object beam_f33636_0(Auction v)
{    return v;}
public Auction beam_f33637_0(String annotation)
{    return new Auction(id, itemName, description, initialBid, reserve, dateTime, expires, seller, category, annotation + ": " + extra);}
public AuctionBid beam_f33645_0(InputStream inStream) throws CoderException, IOException
{    Auction auction = Auction.CODER.decode(inStream);    Bid bid = Bid.CODER.decode(inStream);    return new AuctionBid(auction, bid);}
public Object beam_f33646_0(AuctionBid v)
{    return v;}
public long beam_f33647_0()
{    return auction.sizeInBytes() + bid.sizeInBytes();}
public int beam_f33655_0()
{    return Objects.hash(auction, num);}
public long beam_f33656_0()
{    return 8L + 8L;}
public String beam_f33657_0()
{    try {        return NexmarkUtils.MAPPER.writeValueAsString(this);    } catch (JsonProcessingException e) {        throw new RuntimeException(e);    }}
public void beam_f33665_0(Bid value, OutputStream outStream) throws CoderException, IOException
{    LONG_CODER.encode(value.auction, outStream);    LONG_CODER.encode(value.bidder, outStream);    LONG_CODER.encode(value.price, outStream);    INSTANT_CODER.encode(value.dateTime, outStream);    STRING_CODER.encode(value.extra, outStream);}
public Bid beam_f33666_0(InputStream inStream) throws CoderException, IOException
{    long auction = LONG_CODER.decode(inStream);    long bidder = LONG_CODER.decode(inStream);    long price = LONG_CODER.decode(inStream);    Instant dateTime = INSTANT_CODER.decode(inStream);    String extra = STRING_CODER.decode(inStream);    return new Bid(auction, bidder, price, dateTime, extra);}
public Object beam_f33668_0(Bid v)
{    return v;}
public void beam_f33676_0(BidsPerSession value, OutputStream outStream) throws CoderException, IOException
{    LONG_CODER.encode(value.personId, outStream);    LONG_CODER.encode(value.bidsPerSession, outStream);}
public BidsPerSession beam_f33677_0(InputStream inStream) throws CoderException, IOException
{    long personId = LONG_CODER.decode(inStream);    long bidsPerSession = LONG_CODER.decode(inStream);    return new BidsPerSession(personId, bidsPerSession);}
public Object beam_f33679_0(BidsPerSession v)
{    return v;}
public long beam_f33688_0()
{    return 8L + 8L + 1L;}
public String beam_f33689_0()
{    try {        return NexmarkUtils.MAPPER.writeValueAsString(this);    } catch (JsonProcessingException e) {        throw new RuntimeException(e);    }}
public boolean beam_f33690_0(Object o)
{    if (this == o) {        return true;    }    if (o == null || getClass() != o.getClass()) {        return false;    }    CategoryPrice that = (CategoryPrice) o;    return category == that.category && price == that.price && isLast == that.isLast;}
public int beam_f33699_0()
{    return Objects.hashCode(message);}
public boolean beam_f33700_0(Object o)
{    if (this == o) {        return true;    }    if (o == null || getClass() != o.getClass()) {        return false;    }    Event event = (Event) o;    return Objects.equal(newPerson, event.newPerson) && Objects.equal(newAuction, event.newAuction) && Objects.equal(bid, event.bid);}
public int beam_f33701_0()
{    return Objects.hashCode(newPerson, newAuction, bid);}
public void beam_f33710_0(IdNameReserve value, OutputStream outStream) throws CoderException, IOException
{    LONG_CODER.encode(value.id, outStream);    STRING_CODER.encode(value.name, outStream);    LONG_CODER.encode(value.reserve, outStream);}
public IdNameReserve beam_f33711_0(InputStream inStream) throws CoderException, IOException
{    long id = LONG_CODER.decode(inStream);    String name = STRING_CODER.decode(inStream);    long reserve = LONG_CODER.decode(inStream);    return new IdNameReserve(id, name, reserve);}
public Object beam_f33713_0(IdNameReserve v)
{    return v;}
public boolean beam_f33722_0(Object otherObject)
{    if (this == otherObject) {        return true;    }    if (otherObject == null || getClass() != otherObject.getClass()) {        return false;    }    NameCityStateId other = (NameCityStateId) otherObject;    return Objects.equals(name, other.name) && Objects.equals(city, other.city) && Objects.equals(state, other.state) && Objects.equals(id, other.id);}
public int beam_f33723_0()
{    return Objects.hash(name, city, state, id);}
public long beam_f33724_0()
{    return name.length() + 1L + city.length() + 1L + state.length() + 1L + 8L;}
public long beam_f33733_0()
{    return 8L + name.length() + 1L + emailAddress.length() + 1L + creditCard.length() + 1L + city.length() + 1L + state.length() + 8L + 1L + extra.length() + 1L;}
public String beam_f33734_0()
{    try {        return NexmarkUtils.MAPPER.writeValueAsString(this);    } catch (JsonProcessingException e) {        throw new RuntimeException(e);    }}
public boolean beam_f33735_0(Object o)
{    if (this == o) {        return true;    }    if (o == null || getClass() != o.getClass()) {        return false;    }    Person person = (Person) o;    return id == person.id && Objects.equal(dateTime, person.dateTime) && Objects.equal(name, person.name) && Objects.equal(emailAddress, person.emailAddress) && Objects.equal(creditCard, person.creditCard) && Objects.equal(city, person.city) && Objects.equal(state, person.state) && Objects.equal(extra, person.extra);}
public int beam_f33744_0()
{    return Objects.hashCode(seller, price);}
public void beam_f33745_0(RowSize rowSize, OutputStream outStream) throws IOException
{    LONG_CODER.encode(rowSize.sizeInBytes(), outStream);}
public RowSize beam_f33746_0(InputStream inStream) throws IOException
{    return new RowSize(LONG_CODER.decode(inStream));}
public void beam_f33754_0(@Element Row row, OutputReceiver<Row> o)
{    o.output(row.getRow(index));}
public void beam_f33755_0(ProcessContext c)
{    elementCounter.inc();    bytesCounter.inc(c.element().sizeInBytes());    long now = System.currentTimeMillis();    startTime.update(now);    endTime.update(now);    startTimestamp.update(c.timestamp().getMillis());    endTimestamp.update(c.timestamp().getMillis());    c.output(c.element());}
public PTransform<PCollection<? extends T>, PCollection<T>> beam_f33756_0()
{    return transform;}
private boolean beam_f33764_0()
{    return options.isStreaming();}
private int beam_f33765_0()
{    return 5;}
private void beam_f33766_0(NexmarkPerf perf, List<NexmarkPerf.ProgressSnapshot> snapshots)
{    if (!options.isStreaming()) {        return;    }        int dataStart = 0;    for (; dataStart < snapshots.size(); dataStart++) {        if (snapshots.get(dataStart).numEvents >= 0 && snapshots.get(dataStart).numResults >= 0) {            break;        }    }        int dataEnd = snapshots.size() - 1;    for (; dataEnd > dataStart; dataEnd--) {        if (snapshots.get(dataEnd).anyActivity(snapshots.get(dataEnd - 1))) {            break;        }    }    int numSamples = dataEnd - dataStart + 1;    if (numSamples < MIN_SAMPLES) {                NexmarkUtils.console("%d samples not enough to calculate steady-state event rate", numSamples);        return;    }        int sampleStart = dataStart + numSamples / 3;    int sampleEnd = dataEnd - numSamples / 3;    double sampleSec = snapshots.get(sampleEnd).secSinceStart - snapshots.get(sampleStart).secSinceStart;    if (sampleSec < MIN_WINDOW.getStandardSeconds()) {                NexmarkUtils.console("sample of %.1f sec not long enough to calculate steady-state event rate", sampleSec);        return;    }        double sumxx = 0.0;    double sumxy = 0.0;    long prevNumEvents = -1;    for (int i = sampleStart; i <= sampleEnd; i++) {        if (prevNumEvents == snapshots.get(i).numEvents) {                        continue;        }                        double x = snapshots.get(i).runtimeSec;        prevNumEvents = snapshots.get(i).numEvents;        double y = prevNumEvents;        sumxx += x * x;        sumxy += x * y;    }    double eventsPerSec = sumxy / sumxx;    NexmarkUtils.console("revising events/sec from %.1f to %.1f", perf.eventsPerSec, eventsPerSec);    perf.eventsPerSec = eventsPerSec;}
private PCollection<Event> beam_f33774_0(Pipeline p)
{    if (isStreaming()) {        NexmarkUtils.console("Generating %d events in streaming mode", configuration.numEvents);        return p.apply(queryName + ".ReadUnbounded", NexmarkUtils.streamEventsSource(configuration));    } else {        NexmarkUtils.console("Generating %d events in batch mode", configuration.numEvents);        return p.apply(queryName + ".ReadBounded", NexmarkUtils.batchEventsSource(configuration));    }}
private PCollection<Event> beam_f33775_0(Pipeline p)
{    NexmarkUtils.console("Reading events from Pubsub %s", pubsubSubscription);    PubsubIO.Read<PubsubMessage> io = PubsubIO.readMessagesWithAttributes().fromSubscription(pubsubSubscription).withIdAttribute(NexmarkUtils.PUBSUB_ID);    if (!configuration.usePubsubPublishTime) {        io = io.withTimestampAttribute(NexmarkUtils.PUBSUB_TIMESTAMP);    }    return p.apply(queryName + ".ReadPubsubEvents", io).apply(queryName + ".PubsubMessageToEvent", ParDo.of(new PubsubMessageEventDoFn()));}
public void beam_f33776_0(ProcessContext c) throws IOException
{    byte[] encodedEvent = CoderUtils.encodeToByteArray(Event.CODER, c.element());    c.output(encodedEvent);}
private void beam_f33784_0(PCollection<Event> source)
{    String filename = options.getOutputPath();    if (Strings.isNullOrEmpty(filename)) {        throw new RuntimeException("Missing --outputPath");    }    NexmarkUtils.console("Writing events to Avro files at %s", filename);    source.apply(queryName + ".WriteAvroEvents", AvroIO.write(Event.class).to(filename + "/event").withSuffix(".avro"));    source.apply(NexmarkQueryUtil.JUST_BIDS).apply(queryName + ".WriteAvroBids", AvroIO.write(Bid.class).to(filename + "/bid").withSuffix(".avro"));    source.apply(NexmarkQueryUtil.JUST_NEW_AUCTIONS).apply(queryName + ".WriteAvroAuctions", AvroIO.write(Auction.class).to(filename + "/auction").withSuffix(".avro"));    source.apply(NexmarkQueryUtil.JUST_NEW_PERSONS).apply(queryName + ".WriteAvroPeople", AvroIO.write(Person.class).to(filename + "/person").withSuffix(".avro"));}
private void beam_f33785_0(PCollection<String> formattedResults, long now)
{    String filename = textFilename(now);    NexmarkUtils.console("Writing results to text files at %s", filename);    formattedResults.apply(queryName + ".WriteTextResults", TextIO.write().to(filename));}
public void beam_f33786_0(ProcessContext c)
{    int n = ThreadLocalRandom.current().nextInt(10);    List<TableRow> records = new ArrayList<>(n);    for (int i = 0; i < n; i++) {        records.add(new TableRow().set("index", i).set("value", Integer.toString(i)));    }    c.output(new TableRow().set("result", c.element()).set("records", records));}
private boolean beam_f33794_0()
{    return SQL.equalsIgnoreCase(options.getQueryLanguage());}
private NexmarkQueryModel beam_f33795_0()
{    Map<NexmarkQueryName, NexmarkQueryModel> models = createQueryModels();    return models.get(configuration.query);}
private NexmarkQuery<?> beam_f33796_0()
{    Map<NexmarkQueryName, NexmarkQuery> queries = createQueries();    return queries.get(configuration.query);}
public void beam_f33804_0(ProcessContext c) throws IOException
{    byte[] payload = CoderUtils.encodeToByteArray(Event.CODER, c.element());    c.output(new PubsubMessage(payload, Collections.emptyMap()));}
public boolean beam_f33805_0(ProgressSnapshot that)
{    if (runtimeSec != that.runtimeSec) {                return true;    }    if (numEvents != that.numEvents) {                return true;    }    if (numResults != that.numResults) {                return true;    }    return false;}
public String beam_f33806_0()
{    try {        return NexmarkUtils.MAPPER.writeValueAsString(this);    } catch (JsonProcessingException e) {        throw new RuntimeException(e);    }}
private static List<NexmarkConfiguration> beam_f33814_0()
{    List<NexmarkConfiguration> configurations = smoke();    for (NexmarkConfiguration configuration : configurations) {        if (configuration.numEvents >= 0) {            configuration.numEvents *= 100;        }    }    return configurations;}
private static List<NexmarkConfiguration> beam_f33815_0()
{    List<NexmarkConfiguration> configurations = smoke();    for (NexmarkConfiguration configuration : configurations) {        if (configuration.numEvents >= 0) {            configuration.numEvents *= 10000;        }    }    return configurations;}
private static List<NexmarkConfiguration> beam_f33816_0()
{    NexmarkConfiguration configuration = NexmarkConfiguration.DEFAULT.copy();    configuration.numEventGenerators = 1;    configuration.query = NexmarkQueryName.LOG_TO_SHARDED_FILES;    configuration.isRateLimited = true;    configuration.sourceType = NexmarkUtils.SourceType.PUBSUB;        configuration.numEvents = 0;    configuration.avgPersonByteSize = 500;    configuration.avgAuctionByteSize = 500;    configuration.avgBidByteSize = 500;    configuration.windowSizeSec = 30;    configuration.occasionalDelaySec = 360;    configuration.probDelayedEvent = 0.001;    configuration.useWallclockEventTime = true;    configuration.firstEventRate = 100;    configuration.nextEventRate = 100;    configuration.maxLogEvents = 15000;    List<NexmarkConfiguration> configurations = new ArrayList<>();    configurations.add(configuration);    return configurations;}
public long beam_f33824_0(int rate, RateUnit unit, int numGenerators)
{    return unit.rateToPeriodUs(rate) * numGenerators;}
public long[] beam_f33825_0(int firstRate, int nextRate, RateUnit unit, int numGenerators)
{    if (firstRate == nextRate) {        long[] interEventDelayUs = new long[1];        interEventDelayUs[0] = unit.rateToPeriodUs(firstRate) * numGenerators;        return interEventDelayUs;    }    switch(this) {        case SQUARE:            {                long[] interEventDelayUs = new long[2];                interEventDelayUs[0] = unit.rateToPeriodUs(firstRate) * numGenerators;                interEventDelayUs[1] = unit.rateToPeriodUs(nextRate) * numGenerators;                return interEventDelayUs;            }        case SINE:            {                double mid = (firstRate + nextRate) / 2.0;                                double amp = (firstRate - nextRate) / 2.0;                long[] interEventDelayUs = new long[N];                for (int i = 0; i < N; i++) {                    double r = (2.0 * Math.PI * i) / N;                    double rate = mid + amp * Math.cos(r);                    interEventDelayUs[i] = unit.rateToPeriodUs(Math.round(rate)) * numGenerators;                }                return interEventDelayUs;            }    }        throw new RuntimeException();}
public int beam_f33826_0(int ratePeriodSec)
{    int n = 0;    switch(this) {        case SQUARE:            n = 2;            break;        case SINE:            n = N;            break;    }    return (ratePeriodSec + n - 1) / n;}
public static ParDo.SingleOutput<Event, Event> beam_f33834_0(final String name)
{    return ParDo.of(new DoFn<Event, Event>() {        final Counter eventCounter = Metrics.counter(name, "events");        final Counter newPersonCounter = Metrics.counter(name, "newPersons");        final Counter newAuctionCounter = Metrics.counter(name, "newAuctions");        final Counter bidCounter = Metrics.counter(name, "bids");        final Counter endOfStreamCounter = Metrics.counter(name, "endOfStream");        @ProcessElement        public void processElement(ProcessContext c) {            eventCounter.inc();            if (c.element().newPerson != null) {                newPersonCounter.inc();            } else if (c.element().newAuction != null) {                newAuctionCounter.inc();            } else if (c.element().bid != null) {                bidCounter.inc();            } else {                endOfStreamCounter.inc();            }            info("%s snooping element %s", name, c.element());            c.output(c.element());        }    });}
public void beam_f33835_0(ProcessContext c)
{    eventCounter.inc();    if (c.element().newPerson != null) {        newPersonCounter.inc();    } else if (c.element().newAuction != null) {        newAuctionCounter.inc();    } else if (c.element().bid != null) {        bidCounter.inc();    } else {        endOfStreamCounter.inc();    }    info("%s snooping element %s", name, c.element());    c.output(c.element());}
public static ParDo.SingleOutput<T, Void> beam_f33836_0(final String name)
{    return ParDo.of(new DoFn<T, Void>() {        final Counter discardedCounterMetric = Metrics.counter(name, "discarded");        @ProcessElement        public void processElement(ProcessContext c) {            discardedCounterMetric.inc();        }    });}
public static PTransform<PCollection<T>, PCollection<Long>> beam_f33844_0(final long numEvents, String name)
{    return new PTransform<PCollection<T>, PCollection<Long>>(name) {        @Override        public PCollection<Long> expand(PCollection<T> input) {            return input.apply(Window.<T>into(new GlobalWindows()).triggering(AfterPane.elementCountAtLeast((int) numEvents)).withAllowedLateness(Duration.standardDays(1)).discardingFiredPanes()).apply(name + ".Hash", ParDo.of(new DoFn<T, Long>() {                @ProcessElement                public void processElement(ProcessContext c) {                    long hash = Hashing.murmur3_128().newHasher().putLong(c.timestamp().getMillis()).putString(c.element().toString(), StandardCharsets.UTF_8).hash().asLong();                    c.output(hash);                }            })).apply(Combine.globally(new Combine.BinaryCombineFn<Long>() {                @Override                public Long apply(Long left, Long right) {                    return left ^ right;                }            }));        }    };}
public PCollection<Long> beam_f33845_0(PCollection<T> input)
{    return input.apply(Window.<T>into(new GlobalWindows()).triggering(AfterPane.elementCountAtLeast((int) numEvents)).withAllowedLateness(Duration.standardDays(1)).discardingFiredPanes()).apply(name + ".Hash", ParDo.of(new DoFn<T, Long>() {        @ProcessElement        public void processElement(ProcessContext c) {            long hash = Hashing.murmur3_128().newHasher().putLong(c.timestamp().getMillis()).putString(c.element().toString(), StandardCharsets.UTF_8).hash().asLong();            c.output(hash);        }    })).apply(Combine.globally(new Combine.BinaryCombineFn<Long>() {        @Override        public Long apply(Long left, Long right) {            return left ^ right;        }    }));}
public void beam_f33846_0(ProcessContext c)
{    long hash = Hashing.murmur3_128().newHasher().putLong(c.timestamp().getMillis()).putString(c.element().toString(), StandardCharsets.UTF_8).hash().asLong();    c.output(hash);}
private static ParDo.SingleOutput<T, KnownSize> beam_f33854_0()
{    return ParDo.of(new DoFn<T, KnownSize>() {        @ProcessElement        public void processElement(ProcessContext c) {            c.output(c.element());        }    });}
public void beam_f33855_0(ProcessContext c)
{    c.output(c.element());}
public PCollection<KV<Long, String>> beam_f33856_0(PBegin input)
{    return input.apply(GenerateSequence.from(0).to(config.sideInputRowCount)).apply(MapElements.via(new SimpleFunction<Long, KV<Long, String>>() {        @Override        public KV<Long, String> apply(Long input) {            return KV.of(input, String.valueOf(input));        }    }));}
public static PubsubHelper beam_f33864_0(PubsubOptions options) throws IOException
{    return new PubsubHelper(PubsubJsonClient.FACTORY.newClient(null, null, options), options.getProject());}
public TopicPath beam_f33865_0(String shortTopic) throws IOException
{    TopicPath topic = PubsubClient.topicPathFromName(project, shortTopic);    while (true) {        try {            NexmarkUtils.console("create topic %s", topic);            pubsubClient.createTopic(topic);            createdTopics.add(topic);            return topic;        } catch (GoogleJsonResponseException ex) {            NexmarkUtils.console("attempting to cleanup topic %s", topic);            pubsubClient.deleteTopic(topic);            try {                if (!BackOffUtils.next(sleeper, backOff)) {                    NexmarkUtils.console("too many retries for creating topic %s", topic);                    throw ex;                }            } catch (InterruptedException in) {                throw new IOException(in);            }        }    }}
public TopicPath beam_f33866_0(String shortTopic) throws IOException
{    TopicPath topic = PubsubClient.topicPathFromName(project, shortTopic);    while (true) {        try {            NexmarkUtils.console("create topic %s", topic);            pubsubClient.createTopic(topic);            return topic;        } catch (GoogleJsonResponseException ex) {            if (topicExists(shortTopic)) {                NexmarkUtils.console("topic %s already exists", topic);                return topic;            }            try {                if (!BackOffUtils.next(sleeper, backOff)) {                    NexmarkUtils.console("too many retries for creating/reusing topic %s", topic);                    throw ex;                }            } catch (InterruptedException in) {                throw new IOException(in);            }        }    }}
 void beam_f33874_0(TimestampedValue<OutputT> result)
{    NexmarkUtils.info("intermediate result: %s", result);    updateCounts(result.getTimestamp());}
 void beam_f33875_0(TimestampedValue<OutputT> result)
{    NexmarkUtils.info("result: %s", result);    pendingResults.add(result);    updateCounts(result.getTimestamp());}
private void beam_f33876_0(Instant timestamp)
{    long window = timestamp.getMillis() - timestamp.getMillis() % WINDOW_SIZE.getMillis();    if (window > currentWindow) {        if (currentWindow > BoundedWindow.TIMESTAMP_MIN_VALUE.getMillis()) {            pendingCounts.add(currentCount);        }        currentCount = 0;        currentWindow = window;    }    currentCount++;}
public Long beam_f33884_0()
{    Long result = pendingCounts.get(0);    pendingCounts.remove(0);    return result;}
public void beam_f33885_0()
{    throw new UnsupportedOperationException();}
public Boolean beam_f33886_0(Event input)
{    return input.bid != null || input.newAuction != null;}
public PCollection<TimestampedValue<T>> beam_f33894_0(PCollection<Event> events)
{    if (configuration.debug) {        events = events.apply(name + ".Monitor", eventMonitor.getTransform()).apply(name + ".Snoop", NexmarkUtils.snoop(name));    }    if (configuration.cpuDelayMs > 0) {                events = events.apply(name + ".CpuDelay", NexmarkUtils.cpuDelay(name, configuration.cpuDelayMs));    }    if (configuration.diskBusyBytes > 0) {                events = events.apply(name + ".DiskBusy", NexmarkUtils.diskBusy(configuration.diskBusyBytes));    }        PCollection<T> queryResults = events.apply(transform);    if (configuration.debug) {                queryResults = queryResults.apply(name + ".Debug", resultMonitor.getTransform());    }        return queryResults.apply(name + ".Stamp", NexmarkUtils.stamp(name));}
 static Instant beam_f33895_0(Duration size, Duration period, Instant timestamp)
{    long ts = timestamp.getMillis();    long p = period.getMillis();    long lim = ts - ts % p;    long s = size.getMillis();    return new Instant(lim - s);}
 static Set<String> beam_f33896_0(Iterator<TimestampedValue<T>> itr)
{    Set<String> strings = new HashSet<>();    while (itr.hasNext()) {        strings.add(itr.next().toString());    }    return strings;}
public void beam_f33904_0(ProcessContext c)
{    c.output(c.element().newPerson);}
public void beam_f33905_0(ProcessContext c)
{    c.output(c.element().newAuction);}
public void beam_f33906_0(ProcessContext c)
{    c.output(c.element().bid);}
public PCollection<Auction> beam_f33914_0(PCollection<Event> input)
{    return input.apply("IsNewAuction", Filter.by(IS_NEW_AUCTION)).apply("AsAuction", ParDo.of(AS_AUCTION));}
public PCollection<Person> beam_f33915_0(PCollection<Event> input)
{    return input.apply("IsNewPerson", Filter.by(IS_NEW_PERSON)).apply("AsPerson", ParDo.of(AS_PERSON));}
public PCollection<Bid> beam_f33916_0(PCollection<Event> input)
{    return input.apply("IsBid", Filter.by(IS_BID)).apply("AsBid", ParDo.of(AS_BID));}
public String beam_f33924_0()
{    return String.format("%s %s %d %s %s%n", maxTimestamp, shard, index, timing, filename);}
public void beam_f33925_0(@Nullable String outputPath)
{    this.outputPath = outputPath;}
public void beam_f33926_0(int maxNumWorkers)
{    this.maxNumWorkers = maxNumWorkers;}
public void beam_f33934_1(ProcessContext c, BoundedWindow window) throws IOException
{    String shard = c.element().getKey();    GcsOptions options = c.getPipelineOptions().as(GcsOptions.class);    OutputFile outputFile = outputFileFor(window, shard, c.pane());        if (outputFile.filename != null) {                int n = 0;        try (OutputStream output = Channels.newOutputStream(openWritableGcsFile(options, outputFile.filename))) {            for (Event event : c.element().getValue()) {                Event.CODER.encode(event, output, Coder.Context.OUTER);                writtenRecordsCounter.inc();                if (++n % 10000 == 0) {                                    }            }        }            }    savedFileCounter.inc();    c.output(KV.of(null, outputFile));}
public void beam_f33935_1(ProcessContext c, BoundedWindow window) throws IOException
{    if (c.pane().getTiming() == Timing.LATE) {        unexpectedLateCounter.inc();            } else if (c.pane().getTiming() == Timing.EARLY) {        unexpectedEarlyCounter.inc();            } else if (c.pane().getTiming() == Timing.ON_TIME && c.pane().getIndex() != 0) {        unexpectedIndexCounter.inc();            } else {        GcsOptions options = c.getPipelineOptions().as(GcsOptions.class);                @Nullable        String filename = indexPathFor(window);        if (filename != null) {                        int n = 0;            try (OutputStream output = Channels.newOutputStream(openWritableGcsFile(options, filename))) {                for (OutputFile outputFile : c.element().getValue()) {                    output.write(outputFile.toString().getBytes(StandardCharsets.UTF_8));                    n++;                }            }                    }        c.output(new Done("written for timestamp " + window.maxTimestamp()));        finalizedCounter.inc();    }}
public PCollection<BidsPerSession> beam_f33936_0(PCollection<Event> events)
{    PCollection<Long> bidders = events.apply(NexmarkQueryUtil.JUST_BIDS).apply(name + ".Rekey", ParDo.of(new DoFn<Bid, Long>() {        @ProcessElement        public void processElement(ProcessContext c) {            Bid bid = c.element();            c.output(bid.bidder);        }    }));    PCollection<Long> biddersWindowed = bidders.apply(Window.<Long>into(Sessions.withGapDuration(Duration.standardSeconds(configuration.windowSizeSec))).triggering(Repeatedly.forever(AfterPane.elementCountAtLeast(configuration.maxLogEvents))).discardingFiredPanes().withAllowedLateness(Duration.standardSeconds(configuration.occasionalDelaySec / 2)));    return biddersWindowed.apply(Count.perElement()).apply(name + ".ToResult", ParDo.of(new DoFn<KV<Long, Long>, BidsPerSession>() {        @ProcessElement        public void processElement(ProcessContext c) {            c.output(new BidsPerSession(c.element().getKey(), c.element().getValue()));        }    }));}
protected Collection<String> beam_f33944_0(Iterator<TimestampedValue<Bid>> itr)
{    return toValueTimestamp(itr);}
public PCollection<AuctionPrice> beam_f33945_0(PCollection<Event> events)
{    return events.apply(NexmarkQueryUtil.JUST_BIDS).apply(Filter.by(bid -> bid.auction % this.auctionSkip == 0)).apply(name + ".Project", ParDo.of(new DoFn<Bid, AuctionPrice>() {        @ProcessElement        public void processElement(ProcessContext c) {            Bid bid = c.element();            c.output(new AuctionPrice(bid.auction, bid.price));        }    }));}
public void beam_f33946_0(ProcessContext c)
{    Bid bid = c.element();    c.output(new AuctionPrice(bid.auction, bid.price));}
private void beam_f33954_0(Auction auction, Person person, Instant timestamp)
{    TimestampedValue<NameCityStateId> result = TimestampedValue.of(new NameCityStateId(person.name, person.city, person.state, auction.id), timestamp);    addResult(result);}
protected void beam_f33955_0()
{    TimestampedValue<Event> timestampedEvent = nextInput();    if (timestampedEvent == null) {        allDone();        return;    }    Event event = timestampedEvent.getValue();    if (event.bid != null) {                return;    }    Instant timestamp = timestampedEvent.getTimestamp();    if (event.newAuction != null) {                if (event.newAuction.category == 10) {                        Person person = newPersons.get(event.newAuction.seller);            if (person != null) {                addResult(event.newAuction, person, timestamp);            } else {                                newAuctions.put(event.newAuction.seller, event.newAuction);            }        }    } else {                if ("OR".equals(event.newPerson.state) || "ID".equals(event.newPerson.state) || "CA".equals(event.newPerson.state)) {                        for (Auction auction : newAuctions.get(event.newPerson.id)) {                addResult(auction, event.newPerson, timestamp);            }                        newAuctions.removeAll(event.newPerson.id);                        newPersons.put(event.newPerson.id, event.newPerson);        }    }}
public AbstractSimulator<?, NameCityStateId> beam_f33956_0()
{    return new Simulator(configuration);}
protected void beam_f33964_0()
{    TimestampedValue<AuctionBid> timestampedWinningBid = nextInput();    if (timestampedWinningBid == null) {        prune(NexmarkUtils.END_OF_TIME);        for (TimestampedValue<CategoryPrice> result : lastSeenResults.values()) {            addResult(result);        }        allDone();        return;    }    lastTimestamp = timestampedWinningBid.getTimestamp();    Instant newWindowStart = windowStart(Duration.standardSeconds(configuration.windowSizeSec), Duration.standardSeconds(configuration.windowPeriodSec), lastTimestamp);    prune(newWindowStart);    captureWinningBid(timestampedWinningBid.getValue().auction, timestampedWinningBid.getValue().bid, lastTimestamp);}
public AbstractSimulator<?, CategoryPrice> beam_f33965_0()
{    return new Simulator(configuration);}
protected Iterable<TimestampedValue<CategoryPrice>> beam_f33966_0(Iterable<TimestampedValue<CategoryPrice>> results)
{        Map<Long, TimestampedValue<CategoryPrice>> finalAverages = new TreeMap<>();    for (TimestampedValue<CategoryPrice> obj : results) {        Assert.assertTrue("have CategoryPrice", obj.getValue() instanceof CategoryPrice);        CategoryPrice categoryPrice = (CategoryPrice) obj.getValue();        if (categoryPrice.isLast) {            finalAverages.put(categoryPrice.category, TimestampedValue.of(categoryPrice, obj.getTimestamp()));        }    }    return finalAverages.values();}
private void beam_f33974_0(Instant newWindowStart)
{    while (!newWindowStart.equals(windowStart)) {        NexmarkUtils.info("retiring window %s, aiming for %s", windowStart, newWindowStart);                countBids(windowStart.plus(Duration.standardSeconds(configuration.windowSizeSec)));                windowStart = windowStart.plus(Duration.standardSeconds(configuration.windowPeriodSec));                if (!retireBids(windowStart)) {                        windowStart = newWindowStart;        }    }}
private void beam_f33975_0(Bid bid, Instant timestamp)
{    List<Instant> existing = bids.computeIfAbsent(bid.auction, k -> new ArrayList<>());    existing.add(timestamp);}
public void beam_f33976_0()
{    TimestampedValue<Event> timestampedEvent = nextInput();    if (timestampedEvent == null) {                retireWindows(NexmarkUtils.END_OF_TIME);        allDone();        return;    }    Event event = timestampedEvent.getValue();    if (event.bid == null) {                return;    }    Instant timestamp = timestampedEvent.getTimestamp();    Instant newWindowStart = windowStart(Duration.standardSeconds(configuration.windowSizeSec), Duration.standardSeconds(configuration.windowPeriodSec), timestamp);        retireWindows(newWindowStart);        captureBid(event.bid, timestamp);}
public void beam_f33984_0(ProcessContext c)
{    Auction auction = c.element().auction;    Bid bid = c.element().bid;    c.output(KV.of(auction.seller, bid));}
public void beam_f33985_0(ProcessContext c)
{    c.output(new SellerPrice(c.element().getKey(), c.element().getValue()));}
private void beam_f33986_0(Auction auction, Bid bid, Instant timestamp)
{    NexmarkUtils.info("winning auction, bid: %s, %s", auction, bid);    Queue<Bid> queue = winningBidsPerSeller.get(auction.seller);    if (queue == null) {        queue = new PriorityQueue<>(10, (Bid b1, Bid b2) -> b1.dateTime.compareTo(b2.dateTime));    }    Long total = totalWinningBidPricesPerSeller.get(auction.seller);    if (total == null) {        total = 0L;    }    int count = queue.size();    if (count == 10) {        total -= queue.remove().price;    } else {        count += 1;    }    queue.add(bid);    total += bid.price;    winningBidsPerSeller.put(auction.seller, queue);    totalWinningBidPricesPerSeller.put(auction.seller, total);    TimestampedValue<SellerPrice> intermediateResult = TimestampedValue.of(new SellerPrice(auction.seller, Math.round((double) total / count)), timestamp);    addIntermediateResult(intermediateResult);}
private void beam_f33994_0(Bid bid)
{    Iterator<Bid> itr = highestBids.iterator();    boolean isWinning = true;    while (itr.hasNext()) {        Bid existingBid = itr.next();        if (existingBid.price > bid.price) {            isWinning = false;            break;        }        NexmarkUtils.info("smaller price: %s", existingBid);        itr.remove();    }    if (isWinning) {        NexmarkUtils.info("larger price: %s", bid);        highestBids.add(bid);    }}
protected void beam_f33995_0()
{    TimestampedValue<Event> timestampedEvent = nextInput();    if (timestampedEvent == null) {                retireWindow(lastTimestamp);        allDone();        return;    }    Event event = timestampedEvent.getValue();    if (event.bid == null) {                return;    }    lastTimestamp = timestampedEvent.getTimestamp();    Instant newWindowStart = windowStart(Duration.standardSeconds(configuration.windowSizeSec), Duration.standardSeconds(configuration.windowSizeSec), lastTimestamp);    if (!newWindowStart.equals(windowStart)) {                retireWindow(lastTimestamp);        windowStart = newWindowStart;    }        captureBid(event.bid);}
public AbstractSimulator<?, Bid> beam_f33996_0()
{    return new Simulator(configuration);}
public AbstractSimulator<?, IdNameReserve> beam_f34004_0()
{    return new Simulator(configuration);}
protected Collection<String> beam_f34005_0(Iterator<TimestampedValue<IdNameReserve>> itr)
{    return toValue(itr);}
public PCollection<AuctionBid> beam_f34006_0(PCollection<Event> events)
{    return events.apply(Filter.by(new AuctionOrBid())).apply(new WinningBids(name, configuration));}
private void beam_f34014_0(long bidder)
{    List<TimestampedValue<Event>> session = activeSessions.get(bidder);    checkState(session != null);    Instant sessionStart = Ordering.<Instant>natural().min(session.stream().<Instant>map(tsv -> tsv.getTimestamp()).collect(Collectors.toList()));    Instant sessionEnd = Ordering.<Instant>natural().max(session.stream().<Instant>map(tsv -> tsv.getTimestamp()).collect(Collectors.toList())).plus(configuration.sessionGap);    for (TimestampedValue<Event> timestampedEvent : session) {                Bid bid = timestampedEvent.getValue().bid;        Bid resultBid = new Bid(bid.auction, bid.bidder, bid.price, bid.dateTime, String.format("%d:%s:%s", bid.bidder % configuration.sideInputRowCount, sessionStart, sessionEnd));        TimestampedValue<Bid> result = TimestampedValue.of(resultBid, timestampedEvent.getTimestamp());        addResult(result);    }    activeSessions.remove(bidder);}
public AbstractSimulator<?, Bid> beam_f34015_0()
{    return new Simulator(configuration);}
protected Collection<String> beam_f34016_0(Iterator<TimestampedValue<Bid>> itr)
{    return toValue(itr);}
public PCollection<AuctionPrice> beam_f34024_0(PCollection<Event> allEvents)
{    return allEvents.apply(Filter.by(NexmarkQueryUtil.IS_BID)).apply(getName() + ".SelectEvent", new SelectEvent(Type.BID)).apply(query).apply(Convert.fromRows(AuctionPrice.class));}
public PCollection<NameCityStateId> beam_f34025_0(PCollection<Event> allEvents)
{    PCollection<Event> windowed = fixedWindows(allEvents);    PCollection<Row> auctions = filter(windowed, e -> e.newAuction != null, Auction.class, Type.AUCTION);    PCollection<Row> people = filter(windowed, e -> e.newPerson != null, Person.class, Type.PERSON);    PCollectionTuple inputStreams = createStreamsTuple(auctions, people);    return inputStreams.apply(SqlTransform.query(QUERY_STRING)).apply(Convert.fromRows(NameCityStateId.class));}
private PCollection<Event> beam_f34026_0(PCollection<Event> events)
{    return events.apply(Window.into(FixedWindows.of(Duration.standardSeconds(configuration.windowSizeSec))));}
public String beam_f34034_0()
{    return String.format("AuctionOrBidWindow{start:%s; end:%s; auction:%d; isAuctionWindow:%s}", start(), end(), auction, isAuctionWindow);}
public boolean beam_f34035_0(Object o)
{    if (this == o) {        return true;    }    if (o == null || getClass() != o.getClass()) {        return false;    }    if (!super.equals(o)) {        return false;    }    AuctionOrBidWindow that = (AuctionOrBidWindow) o;    return (isAuctionWindow == that.isAuctionWindow) && (auction == that.auction);}
public int beam_f34036_0()
{    return Objects.hash(super.hashCode(), isAuctionWindow, auction);}
public Coder<AuctionOrBidWindow> beam_f34045_0()
{    return AuctionOrBidWindowCoder.of();}
public WindowMappingFn<AuctionOrBidWindow> beam_f34046_0()
{    throw new UnsupportedOperationException("AuctionWindowFn not supported for side inputs");}
public Instant beam_f34047_0(Instant inputTimestamp, AuctionOrBidWindow window)
{    return window.maxTimestamp();}
protected void beam_f34055_0()
{    if (lastTimestamp.compareTo(BoundedWindow.TIMESTAMP_MIN_VALUE) > 0) {                flushBidsWithoutAuctions();        TimestampedValue<AuctionBid> result = nextWinningBid(lastTimestamp);        if (result != null) {            addResult(result);            return;        }    }    TimestampedValue<Event> timestampedEvent = nextInput();    if (timestampedEvent == null) {                TimestampedValue<AuctionBid> result = nextWinningBid(BoundedWindow.TIMESTAMP_MAX_VALUE);        if (result == null) {                        allDone();            return;        }        addResult(result);        return;    }    Event event = timestampedEvent.getValue();    if (event.newPerson != null) {                return;    }    lastTimestamp = timestampedEvent.getTimestamp();    if (event.newAuction != null) {                openAuctions.put(event.newAuction.id, event.newAuction);    } else {        if (!captureBestBid(event.bid, true)) {                        NexmarkUtils.info("bid not yet accounted for: %s", event.bid);            bidsWithoutAuctions.add(event.bid);        }    }}
public synchronized boolean beam_f34056_0()
{    NexmarkUtils.info("starting bounded generator %s", generator);    return advance();}
public synchronized boolean beam_f34057_0()
{    if (!generator.hasNext()) {                if (!reportedStop) {            reportedStop = true;            NexmarkUtils.info("stopped bounded generator %s", generator);        }        return false;    }    currentEvent = generator.next();    return true;}
public EventReader beam_f34066_0(PipelineOptions options)
{    NexmarkUtils.info("creating initial bounded reader for %s", config);    return new EventReader(this, config);}
public Coder<Event> beam_f34068_0()
{    return Event.CODER;}
public NextEvent beam_f34069_0(long delayMs)
{    return new NextEvent(wallclockTimestamp + delayMs, eventTimestamp, event.withAnnotation("LATE"), watermark);}
public long beam_f34077_0()
{    return config.firstEventId + config.nextAdjustedEventNumber(eventsCountSoFar);}
public boolean beam_f34078_0()
{    return eventsCountSoFar < config.maxEvents;}
public NextEvent beam_f34079_0()
{    if (wallclockBaseTime < 0) {        wallclockBaseTime = System.currentTimeMillis();    }        long eventTimestamp = config.timestampAndInterEventDelayUsForEvent(config.nextEventNumber(eventsCountSoFar)).getKey();            long adjustedEventTimestamp = config.timestampAndInterEventDelayUsForEvent(config.nextAdjustedEventNumber(eventsCountSoFar)).getKey();            long watermark = config.timestampAndInterEventDelayUsForEvent(config.nextEventNumberForWatermark(eventsCountSoFar)).getKey();        long wallclockTimestamp = wallclockBaseTime + (eventTimestamp - getCurrentConfig().baseTime);        Random random = new Random(getNextEventId());    long newEventId = getNextEventId();    long rem = newEventId % GeneratorConfig.PROPORTION_DENOMINATOR;    Event event;    if (rem < GeneratorConfig.PERSON_PROPORTION) {        event = new Event(nextPerson(newEventId, random, new DateTime(adjustedEventTimestamp), config));    } else if (rem < GeneratorConfig.PERSON_PROPORTION + GeneratorConfig.AUCTION_PROPORTION) {        event = new Event(nextAuction(eventsCountSoFar, newEventId, random, adjustedEventTimestamp, config));    } else {        event = new Event(nextBid(newEventId, random, adjustedEventTimestamp, config));    }    eventsCountSoFar++;    return new NextEvent(wallclockTimestamp, adjustedEventTimestamp, event, watermark);}
public Generator beam_f34088_0(GeneratorConfig config)
{    return new Generator(config, numEvents, wallclockBaseTime);}
public String beam_f34090_0()
{    return toStringHelper(this).add("numEvents", numEvents).add("wallclockBaseTime", wallclockBaseTime).toString();}
public GeneratorConfig beam_f34091_0()
{    GeneratorConfig result;    result = new GeneratorConfig(configuration, baseTime, firstEventId, maxEvents, firstEventNumber);    return result;}
public int beam_f34099_0()
{    return configuration.hotAuctionRatio;}
public int beam_f34100_0()
{    return configuration.hotBiddersRatio;}
public int beam_f34101_0()
{    return configuration.avgBidByteSize;}
public long beam_f34109_0(long numEvents)
{    long n = configuration.outOfOrderGroupSize;    long eventNumber = nextEventNumber(numEvents);    long base = (eventNumber / n) * n;    long offset = (eventNumber * 953) % n;    return base + offset;}
public long beam_f34110_0(long numEvents)
{    long n = configuration.outOfOrderGroupSize;    long eventNumber = nextEventNumber(numEvents);    return (eventNumber / n) * n;}
public KV<Long, Long> beam_f34111_0(long eventNumber)
{    if (interEventDelayUs.length == 1) {        long timestamp = baseTime + (eventNumber * interEventDelayUs[0]) / 1000L;        return KV.of(timestamp, interEventDelayUs[0]);    }    long epoch = eventNumber / eventsPerEpoch;    long n = eventNumber % eventsPerEpoch;    long offsetInEpochMs = 0;    for (long interEventDelayU : interEventDelayUs) {        long numEventsForThisCycle = (stepLengthSec * 1_000_000L) / interEventDelayU;        if (n < numEventsForThisCycle) {            long offsetInCycleUs = n * interEventDelayU;            long timestamp = baseTime + epoch * epochPeriodMs + offsetInEpochMs + (offsetInCycleUs / 1000L);            return KV.of(timestamp, interEventDelayU);        }        n -= numEventsForThisCycle;        offsetInEpochMs += (numEventsForThisCycle * interEventDelayU) / 1000L;    }        throw new RuntimeException("internal eventsPerEpoch incorrect");}
public static Person beam_f34119_0(long nextEventId, Random random, DateTime timestamp, GeneratorConfig config)
{    long id = lastBase0PersonId(nextEventId) + GeneratorConfig.FIRST_PERSON_ID;    String name = nextPersonName(random);    String email = nextEmail(random);    String creditCard = nextCreditCard(random);    String city = nextUSCity(random);    String state = nextUSState(random);    int currentSize = 8 + name.length() + email.length() + creditCard.length() + city.length() + state.length();    String extra = nextExtra(random, currentSize, config.getAvgPersonByteSize());    return new Person(id, name, email, creditCard, city, state, timestamp.toInstant(), extra);}
public static long beam_f34120_0(long eventId, Random random, GeneratorConfig config)
{                        long numPeople = lastBase0PersonId(eventId) + 1;    long activePeople = Math.min(numPeople, config.getNumActivePeople());    long n = nextLong(random, activePeople + PERSON_ID_LEAD);    return numPeople - activePeople + n;}
public static long beam_f34121_0(long eventId)
{    long epoch = eventId / GeneratorConfig.PROPORTION_DENOMINATOR;    long offset = eventId % GeneratorConfig.PROPORTION_DENOMINATOR;    if (offset >= GeneratorConfig.PERSON_PROPORTION) {                        offset = GeneratorConfig.PERSON_PROPORTION - 1;    }        return epoch * GeneratorConfig.PERSON_PROPORTION + offset;}
public static String beam_f34129_0(Random random, int length)
{    StringBuilder sb = new StringBuilder();    int rnd = 0;        int n = 0;    while (length-- > 0) {        if (n == 0) {            rnd = random.nextInt();                        n = 6;        }        sb.append((char) ('a' + rnd % 26));        rnd /= 26;        n--;    }    return sb.toString();}
public static String beam_f34130_0(Random random, int currentSize, int desiredAverageSize)
{    if (currentSize > desiredAverageSize) {        return "";    }    desiredAverageSize -= currentSize;    int delta = (int) Math.round(desiredAverageSize * 0.2);    int minSize = desiredAverageSize - delta;    int desiredSize = minSize + (delta == 0 ? 0 : random.nextInt(2 * delta));    return nextExactString(random, desiredSize);}
public boolean beam_f34131_0()
{    LOG.trace("starting unbounded generator {}", generator);    return advance();}
public long beam_f34140_0()
{    return backlogBytes == null ? BACKLOG_UNKNOWN : backlogBytes;}
public String beam_f34141_0()
{    return String.format("EventReader(%d, %d, %d)", generator.getCurrentConfig().getStartEventId(), generator.getNextEventId(), generator.getCurrentConfig().getStopEventId());}
public Coder<GeneratorCheckpoint> beam_f34142_0()
{    return GeneratorCheckpoint.CODER_INSTANCE;}
public Void beam_f34151_0(Iterable<Row> input)
{    RowSize recordSize = RowSize.of(Iterables.getOnlyElement(input));    assertThat(recordSize.sizeInBytes(), equalTo(ROW_SIZE));    return null;}
public void beam_f34152_0()
{    NexmarkConfiguration configuration = NexmarkConfiguration.DEFAULT;    configuration.overrideFromOptions(PipelineOptionsFactory.fromArgs("--query=3").as(NexmarkOptions.class));    assertThat(configuration.query, equalTo(NexmarkQueryName.LOCAL_ITEM_SUGGESTION));}
public void beam_f34153_0()
{    NexmarkConfiguration configuration = NexmarkConfiguration.DEFAULT;    configuration.overrideFromOptions(PipelineOptionsFactory.fromArgs("--query=HIGHEST_BID").as(NexmarkOptions.class));    assertThat(configuration.query, equalTo(NexmarkQueryName.HIGHEST_BID));}
public void beam_f34161_0()
{    NexmarkConfiguration nexmarkConfiguration1 = new NexmarkConfiguration();    nexmarkConfiguration1.query = QUERY;        nexmarkConfiguration1.cpuDelayMs = 100L;    NexmarkPerf nexmarkPerf1 = new NexmarkPerf();    nexmarkPerf1.numResults = 1000L;    nexmarkPerf1.eventsPerSec = 0.5F;    nexmarkPerf1.runtimeSec = 0.325F;    NexmarkConfiguration nexmarkConfiguration2 = new NexmarkConfiguration();    nexmarkConfiguration2.query = QUERY;        nexmarkConfiguration1.cpuDelayMs = 200L;    NexmarkPerf nexmarkPerf2 = new NexmarkPerf();    nexmarkPerf2.numResults = 1001L;    nexmarkPerf2.eventsPerSec = 1.5F;    nexmarkPerf2.runtimeSec = 1.325F;        HashMap<NexmarkConfiguration, NexmarkPerf> perfs = new HashMap<>(2);    perfs.put(nexmarkConfiguration1, nexmarkPerf1);    perfs.put(nexmarkConfiguration2, nexmarkPerf2);    long startTimestampMilliseconds = 1454284800000L;    Main.savePerfsToBigQuery(publisher, options, perfs, new Instant(startTimestampMilliseconds));    String tableName = NexmarkUtils.tableName(options, QUERY.getNumberOrName(), 0L, null);    List<TestResult> rows = publisher.getRecords(tableName);    assertThat(rows, hasItems(nexmarkPerf1, nexmarkPerf2));}
public void beam_f34162_0()
{    NexmarkUtils.setupPipeline(NexmarkUtils.CoderStrategy.HAND, p);}
private void beam_f34163_0(String name, NexmarkConfiguration config, NexmarkQueryTransform<T> query, NexmarkQueryModel<T> model, boolean streamingMode) throws Exception
{    ResourceId sideInputResourceId = FileSystems.matchNewResource(String.format("%s/BoundedSideInputJoin-%s", p.getOptions().getTempLocation(), new Random().nextInt()), false);    config.sideInputUrl = sideInputResourceId.toString();    try {        PCollection<KV<Long, String>> sideInput = NexmarkUtils.prepareSideInput(p, config);        query.setSideInput(sideInput);        PCollection<Event> events = p.apply(name + ".Read", streamingMode ? NexmarkUtils.streamEventsSource(config) : NexmarkUtils.batchEventsSource(config));        PCollection<TimestampedValue<T>> results = (PCollection<TimestampedValue<T>>) events.apply(new NexmarkQuery<>(config, query));        PAssert.that(results).satisfies(model.assertionFor());        PipelineResult result = p.run();        result.waitUntilFinish();    } finally {        NexmarkUtils.cleanUpSideInput(config);    }}
public void beam_f34171_0()
{    queryMatchesModel("Query0TestStreaming", new Query0(), new Query0Model(CONFIG), true);}
public void beam_f34172_0()
{    queryMatchesModel("Query1TestBatch", new Query1(CONFIG), new Query1Model(CONFIG), false);}
public void beam_f34173_0()
{    queryMatchesModel("Query1TestStreaming", new Query1(CONFIG), new Query1Model(CONFIG), true);}
public void beam_f34181_0()
{    queryMatchesModel("Query3TestStreaming", new Query3(CONFIG), new Query3Model(CONFIG), true);}
public void beam_f34182_0()
{    queryMatchesModel("SqlQuery3TestBatch", new SqlQuery3(CONFIG), new Query3Model(CONFIG), false);}
public void beam_f34183_0()
{    queryMatchesModel("SqlQuery3TestStreaming", new SqlQuery3(CONFIG), new Query3Model(CONFIG), true);}
public void beam_f34191_0()
{    queryMatchesModel("Query6TestStreaming", new Query6(CONFIG), new Query6Model(CONFIG), true);}
public void beam_f34192_0()
{    queryMatchesModel("Query7TestBatch", new Query7(CONFIG), new Query7Model(CONFIG), false);}
public void beam_f34193_0()
{    queryMatchesModel("Query7TestStreaming", new Query7(CONFIG), new Query7Model(CONFIG), true);}
private void beam_f34201_0(String name, NexmarkConfiguration config, NexmarkQueryTransform<T> query, NexmarkQueryModel<T> model, boolean streamingMode) throws Exception
{    ResourceId sideInputResourceId = FileSystems.matchNewResource(String.format("%s/SessionSideInputJoin-%s", p.getOptions().getTempLocation(), new Random().nextInt()), false);    config.sideInputUrl = sideInputResourceId.toString();    try {        PCollection<KV<Long, String>> sideInput = NexmarkUtils.prepareSideInput(p, config);        query.setSideInput(sideInput);        PCollection<Event> events = p.apply(name + ".Read", streamingMode ? NexmarkUtils.streamEventsSource(config) : NexmarkUtils.batchEventsSource(config));        PCollection<TimestampedValue<T>> results = (PCollection<TimestampedValue<T>>) events.apply(new NexmarkQuery<>(config, query));        PAssert.that(results).satisfies(model.assertionFor());        PipelineResult result = p.run();        result.waitUntilFinish();    } finally {        NexmarkUtils.cleanUpSideInput(config);    }}
public void beam_f34202_0() throws Exception
{    NexmarkConfiguration config = NexmarkConfiguration.DEFAULT.copy();    config.sideInputType = NexmarkUtils.SideInputType.DIRECT;    config.numEventGenerators = 1;    config.numEvents = 5000;    config.sideInputRowCount = 10;    config.sideInputNumShards = 3;    PCollection<KV<Long, String>> sideInput = NexmarkUtils.prepareSideInput(p, config);    try {        PCollection<Event> input = p.apply(NexmarkUtils.batchEventsSource(config));        PCollection<Bid> justBids = input.apply(NexmarkQueryUtil.JUST_BIDS);        PCollection<Long> bidCount = justBids.apply("Count Bids", Count.globally());        NexmarkQueryTransform<Bid> query = new SessionSideInputJoin(config);        query.setSideInput(sideInput);        PCollection<TimestampedValue<Bid>> output = (PCollection<TimestampedValue<Bid>>) input.apply(new NexmarkQuery(config, query));        PCollection<Long> outputCount = output.apply(Window.into(new GlobalWindows())).apply("Count outputs", Count.globally());        PAssert.that(PCollectionList.of(bidCount).and(outputCount).apply(Flatten.pCollections())).satisfies(counts -> {            assertThat(Iterables.size(counts), equalTo(2));            assertThat(Iterables.get(counts, 0), greaterThan(0L));            assertThat(Iterables.get(counts, 0), equalTo(Iterables.get(counts, 1)));            return null;        });        p.run();    } finally {        NexmarkUtils.cleanUpSideInput(config);    }}
public void beam_f34203_0() throws Exception
{    NexmarkConfiguration config = NexmarkConfiguration.DEFAULT.copy();    config.sideInputType = NexmarkUtils.SideInputType.DIRECT;    config.numEventGenerators = 1;    config.numEvents = 5000;    config.sideInputRowCount = 10;    config.sideInputNumShards = 3;    queryMatchesModel("SessionSideInputJoinTestBatch", config, new SessionSideInputJoin(config), new SessionSideInputJoinModel(config), false);}
public void beam_f34211_0() throws Exception
{    NexmarkConfiguration config = NexmarkConfiguration.DEFAULT.copy();    config.sideInputType = NexmarkUtils.SideInputType.DIRECT;    config.numEventGenerators = 1;    config.numEvents = 5000;    config.sideInputRowCount = 10;    config.sideInputNumShards = 3;    queryMatchesModel("SqlBoundedSideInputJoinTestStreaming", config, new SqlBoundedSideInputJoin(config), new BoundedSideInputJoinModel(config), true);}
public void beam_f34212_0() throws Exception
{    NexmarkConfiguration config = NexmarkConfiguration.DEFAULT.copy();    config.sideInputType = NexmarkUtils.SideInputType.CSV;    config.numEventGenerators = 1;    config.numEvents = 5000;    config.sideInputRowCount = 10;    config.sideInputNumShards = 3;    queryMatchesModel("SqlBoundedSideInputJoinTestBatch", config, new SqlBoundedSideInputJoin(config), new BoundedSideInputJoinModel(config), false);}
public void beam_f34213_0() throws Exception
{    NexmarkConfiguration config = NexmarkConfiguration.DEFAULT.copy();    config.sideInputType = NexmarkUtils.SideInputType.CSV;    config.numEventGenerators = 1;    config.numEvents = 5000;    config.sideInputRowCount = 10;    config.sideInputNumShards = 3;    queryMatchesModel("SqlBoundedSideInputJoinTestStreaming", config, new SqlBoundedSideInputJoin(config), new BoundedSideInputJoinModel(config), true);}
public void beam_f34221_0() throws Exception
{    PCollection<Event> events = testPipeline.apply(Create.of(PEOPLE_AND_AUCTIONS_EVENTS));    PAssert.that(events.apply(new SqlQuery3(new NexmarkConfiguration()))).containsInAnyOrder(RESULTS);    testPipeline.run();}
private static Person beam_f34222_0(long id, String state)
{    return new Person(id, "name_" + id, "email_" + id, "cc_" + id, "city_" + id, state, new Instant(123123L + id), "extra_" + id);}
private static Auction beam_f34223_0(long id, long seller, long category)
{    return new Auction(id, "item_" + id, "desc_" + id, 123 + id, 200 + id, new Instant(123123L + id), new Instant(223123 + id), seller, category, "extra_" + id);}
public void beam_f34231_0() throws Exception
{    NexmarkOptions options = PipelineOptionsFactory.as(NexmarkOptions.class);    long n = 200L;    BoundedEventSource source = new BoundedEventSource(makeConfig(n), 1);    SourceTestUtils.assertSourcesEqualReferenceSource(source, source.split(10, options), options);}
private GeneratorConfig beam_f34232_0(long n)
{    return new GeneratorConfig(NexmarkConfiguration.DEFAULT, System.currentTimeMillis(), 0, n, 0);}
private long beam_f34233_0(long n, Iterator<T> itr)
{    for (long i = 0; i < n; i++) {        assertTrue(itr.hasNext());        itr.next();    }    return n;}
public void beam_f34241_0(ProcessContext c)
{    totalBytes.inc(c.element().getKey().length + c.element().getValue().length);    c.output(c.element());}
public void beam_f34242_0(String bigQueryDataset, String bigQueryTable)
{    MetricsReader reader = new MetricsReader(result, namespace);    Collection<NamedTestResult> namedTestResults = reader.readAll(metricSuppliers);    publish(uuid, timestamp, bigQueryDataset, bigQueryTable, namedTestResults);}
public static void beam_f34243_0(String uuid, String timestamp, String bigQueryDataset, String bigQueryTable, Collection<NamedTestResult> results)
{    if (bigQueryDataset != null && bigQueryTable != null) {        BigQueryResultsPublisher.create(bigQueryDataset, NamedTestResult.getSchema()).publish(results, bigQueryTable);    }    ConsoleResultPublisher.publish(results, uuid, timestamp);}
private Iterable<MetricResult<DistributionResult>> beam_f34251_0(String name)
{    MetricQueryResults metrics = result.metrics().queryMetrics(MetricsFilter.builder().addNameFilter(MetricNameFilter.named(namespace, name)).build());    return metrics.getDistributions();}
private void beam_f34252_0(String name, Iterable<MetricResult<T>> metricResult) throws IllegalStateException
{    int resultCount = Iterables.size(metricResult);    Preconditions.checkState(resultCount <= 1, "More than one metric result matches name: %s in namespace %s. Metric results count: %s", name, namespace, resultCount);}
private boolean beam_f34253_0(long value)
{    return (Math.abs(value - now) <= Duration.standardDays(10000).getMillis());}
private void beam_f34261_0(TableId tableId, Schema schema)
{    TableInfo tableInfo = TableInfo.newBuilder(tableId, StandardTableDefinition.of(schema)).setFriendlyName(tableId.getTable()).build();    client.create(tableInfo, FIELD_OPTIONS);}
public void beam_f34262_0(Map<String, ?> row, Map<String, String> schema, String table)
{    createTableIfNotExists(table, schema);    insertRow(row, table);}
public void beam_f34263_0(Map<String, ?> row, String table)
{    insertAll(Collections.singletonList(row), table);}
public void beam_f34271_0(Collection<? extends TestResult> results, String tableName)
{    List<Map<String, ?>> records = results.stream().map(this::getRowOfSchema).collect(Collectors.toList());    client.insertAll(records, schema, tableName);}
private Map<String, Object> beam_f34272_0(TestResult result)
{    return result.toMap().entrySet().stream().filter(element -> schema.containsKey(element.getKey())).collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));}
public static void beam_f34273_0(Collection<NamedTestResult> results, String testId, String timestamp)
{    final String textTemplate = "%24s  %24s";    System.out.println(format("Load test results for test (ID): %s and timestamp: %s:", testId, timestamp));    System.out.println(format(textTemplate, "Metric:", "Value:"));    results.forEach((result) -> System.out.println(format(textTemplate, result.getMetric(), result.getValue())));}
public void beam_f34282_0()
{    List<Integer> sampleInputData = Arrays.asList(1, 2, 3, 4, 5);    createTestPipelineWithBranches(sampleInputData);    PipelineResult result = testPipeline.run();    MetricsReader reader = new MetricsReader(result, NAMESPACE, 0);    assertEquals(10, reader.getEndTimeMetric("timeDist"));}
private void beam_f34283_0(List<Integer> sampleInputData)
{    PCollection<Integer> inputData = testPipeline.apply(Create.of(sampleInputData));    inputData.apply("Monitor #1", ParDo.of(new MonitorWithTimeDistribution()));    inputData.apply("Multiply input", MapElements.via(new MultiplyElements())).apply("Monitor #2", ParDo.of(new MonitorWithTimeDistribution()));}
public void beam_f34284_0()
{    PipelineResult result = testPipeline.run();    MetricsReader reader = new MetricsReader(result, NAMESPACE);    reader.getCounterMetric("nonexistent");}
public void beam_f34292_0()
{    publisher.publish(new SampleTestResult("a", "b"), TABLE_NAME);    Map<String, ?> rowInTable = bigQueryClient.getRows(TABLE_NAME).get(0);    assertEquals(1, rowInTable.entrySet().size());    assertEquals("a", rowInTable.get("field1"));}
public void beam_f34293_0()
{    publisher.publish(new SampleTestResult("a", "b"), TABLE_NAME);    Map<String, ?> rowInTable = bigQueryClient.getRows(TABLE_NAME).get(0);    assertNull(rowInTable.get("field2"));}
public void beam_f34294_0()
{    List<SampleTestResult> results = Arrays.asList(new SampleTestResult("a", "b"), new SampleTestResult("a", "b"));    publisher.publish(results, TABLE_NAME);    assertEquals(2, bigQueryClient.getRows(TABLE_NAME).size());}
