public void looseUnmarshal(OpenWireFormat wireFormat, Object o, DataInput dataIn) throws IOException
{    super.looseUnmarshal(wireFormat, o, dataIn);    ConnectionId info = (ConnectionId) o;    info.setValue(looseUnmarshalString(dataIn));}
0
public void write(int b) throws IOException
{    out.writeByte((byte) b);}
0
public BigDecimal getAmount()
{    return amount;}
0
public short readShort(long pos)
{    long absPos = offset + pos;    int blockOffset = blockOffset(absPos);    if (blockOffset + Short.BYTES <= blockMask) {        return blocks[blockIndex(absPos)].getShort(blockOffset);    } else {        return (short) ((readByte(pos) & 0xFF) << 8 | (readByte(pos + 1) & 0xFF));    }}
0
public void testSequenceId() throws IOException
{    region = initHRegion(tableName, method, CONF, COLUMN_FAMILY_BYTES);    assertEquals(HConstants.NO_SEQNUM, region.getMaxFlushedSeqId());        assertEquals(0, (long) region.getMaxStoreSeqId().get(COLUMN_FAMILY_BYTES));    HBaseTestingUtility.closeRegionAndWAL(this.region);    assertEquals(HConstants.NO_SEQNUM, region.getMaxFlushedSeqId());    assertEquals(0, (long) region.getMaxStoreSeqId().get(COLUMN_FAMILY_BYTES));        region = initHRegion(tableName, method, CONF, COLUMN_FAMILY_BYTES);    byte[] value = Bytes.toBytes(method);        Put put = new Put(value);    put.addColumn(COLUMN_FAMILY_BYTES, null, value);    region.put(put);        assertEquals(HConstants.NO_SEQNUM, region.getMaxFlushedSeqId());    assertEquals(0, (long) region.getMaxStoreSeqId().get(COLUMN_FAMILY_BYTES));    region.flush(true);    long max = region.getMaxFlushedSeqId();    HBaseTestingUtility.closeRegionAndWAL(this.region);    assertEquals(max, region.getMaxFlushedSeqId());    this.region = null;}
0
public void processVersionTag(final InternalRegion region, final VersionTag versionTag, final boolean isTombstoneFromGII, final boolean hasDelta, final VersionSource versionSource, final InternalDistributedMember sender, final boolean checkForConflicts)
{    basicProcessVersionTag(region, versionTag, isTombstoneFromGII, hasDelta, versionSource, sender, checkForConflicts);}
0
private void putConflictPipeline() throws IOException
{    final XContentBuilder pipelineBuilder = jsonBuilder().startObject().startArray("processors").startObject().startObject("set").field("field", "_version").field("value", 1).endObject().endObject().startObject().startObject("set").field("field", "_id").field("value", "1").endObject().endObject().endArray().endObject();    final PutPipelineRequest putPipelineRequest = new PutPipelineRequest(CONFLICT_PIPELINE_ID, BytesReference.bytes(pipelineBuilder), pipelineBuilder.contentType());    assertTrue(highLevelClient().ingest().putPipeline(putPipelineRequest, RequestOptions.DEFAULT).isAcknowledged());}
0
public void validate() throws org.apache.thrift.TException
{}
0
public Map<String, WebSocketFactory> getSocketFactory()
{    return socketFactory;}
0
protected CamelContext createCamelContext() throws Exception
{    MicroProfileMetricsRouteEventNotifier eventNotifier = new MicroProfileMetricsRouteEventNotifier();    eventNotifier.setMetricRegistry(metricRegistry);    CamelContext camelContext = super.createCamelContext();    camelContext.getManagementStrategy().addEventNotifier(eventNotifier);    return camelContext;}
0
private boolean waitForFetchRemoteEntriesRetry(RetryTimeKeeper retryTime)
{    if (retryTime.overMaximum()) {        return false;    }    retryTime.waitToRetryNode();    return true;}
0
public boolean referencesUserType(ByteBuffer name)
{    return elements.referencesUserType(name);}
0
public long getLocalValuePrimitive()
{    return this.localValue;}
0
public void testEmpty()
{}
0
private LogWriter getLogger()
{    if (cache == null) {        return null;    }    return cache.getLogger();}
0
protected void fromData(OldValueImporter ovi, byte[] bytes) throws IOException, ClassNotFoundException
{    ((PutReplyMessage) ovi).fromData(new DataInputStream(new ByteArrayInputStream(bytes)), mock(DeserializationContext.class));}
0
public static long copy(InputStream in, OutputStream out) throws IOException
{    return copy(in, out, new byte[BUFFER_SIZE]);}
0
public String nodeName()
{    return getClass().getSimpleName();}
0
public List<RebalanceRegionResult> getRebalanceRegionResults()
{    return rebalanceSummary;}
0
public Collection<DataSet<?>> getDanglingDataSets()
{    return danglingDataSets.stream().map(id -> dataSets.get(id)).collect(Collectors.toList());}
0
public long minDocCount()
{    return minDocCount;}
0
public SqlNode field(int ordinal)
{    return field.apply(ordinal);}
0
private boolean isDefaultActionUnavailable()
{    return suspendedDefaultAction != null;}
0
public static _Fields findByThriftIdOrThrow(int fieldId)
{    _Fields fields = findByThriftId(fieldId);    if (fields == null)        throw new IllegalArgumentException("Field " + fieldId + " doesn't exist!");    return fields;}
0
public String getFieldName()
{    return _fieldName;}
0
public void setAutoCommit(final boolean autoCommit) throws SQLException
{    if (transactionContext != null) {        throw new SQLException("Auto-commit can not be set while enrolled in a transaction");    }    super.setAutoCommit(autoCommit);}
0
public List<MetricConfig> getMetricsConfig()
{    return metricsConfig;}
0
public void testTableWithOneClustering() throws Throwable
{    createTable("CREATE TABLE %s (k text, t int, v1 text, v2 text, PRIMARY KEY (k, t));");    execute("INSERT INTO %s (k, t, v1, v2) values (?, ?, ?, ?)", "key", 1, "v11", "v21");    execute("INSERT INTO %s (k, t, v1, v2) values (?, ?, ?, ?)", "key", 2, "v12", "v22");    execute("INSERT INTO %s (k, t, v1, v2) values (?, ?, ?, ?)", "key", 3, "v13", "v23");    flush();    execute("INSERT INTO %s (k, t, v1, v2) values (?, ?, ?, ?)", "key", 4, "v14", "v24");    execute("INSERT INTO %s (k, t, v1, v2) values (?, ?, ?, ?)", "key", 5, "v15", "v25");    assertRows(execute("SELECT * FROM %s"), row("key", 1, "v11", "v21"), row("key", 2, "v12", "v22"), row("key", 3, "v13", "v23"), row("key", 4, "v14", "v24"), row("key", 5, "v15", "v25"));    assertRows(execute("SELECT * FROM %s WHERE k = ? AND t > ?", "key", 3), row("key", 4, "v14", "v24"), row("key", 5, "v15", "v25"));    assertRows(execute("SELECT * FROM %s WHERE k = ? AND t >= ? AND t < ?", "key", 2, 4), row("key", 2, "v12", "v22"), row("key", 3, "v13", "v23"));        assertRows(execute("SELECT * FROM %s WHERE k = ? ORDER BY t DESC", "key"), row("key", 5, "v15", "v25"), row("key", 4, "v14", "v24"), row("key", 3, "v13", "v23"), row("key", 2, "v12", "v22"), row("key", 1, "v11", "v21"));    assertRows(execute("SELECT * FROM %s WHERE k = ? AND t > ? ORDER BY t DESC", "key", 3), row("key", 5, "v15", "v25"), row("key", 4, "v14", "v24"));    assertRows(execute("SELECT * FROM %s WHERE k = ? AND t >= ? AND t < ? ORDER BY t DESC", "key", 2, 4), row("key", 3, "v13", "v23"), row("key", 2, "v12", "v22"));}
0
private PCollectionSingletonIterableAssert<T> satisfies(AssertRelation<Iterable<T>, Iterable<T>> relation, Iterable<T> expectedElements)
{    return satisfies(new CheckRelationAgainstExpected<>(relation, expectedElements, IterableCoder.of(elementCoder)));}
0
public static Object instantiate(Constructor ctor, Object... args)
{    try {        return ctor.newInstance(args);    } catch (Exception e) {        String msg = "Unable to instantiate Permission instance with constructor [" + ctor + "]";        throw new InstantiationException(msg, e);    }}
0
public void setProtocolConfig(String config)
{    this.protocolConfig = config;}
0
public void testNonExistingGroupAttribute() throws InterruptedException
{    final TestRunner runner = TestRunners.newTestRunner(new ControlRate());    runner.setProperty(ControlRate.RATE_CONTROL_CRITERIA, ControlRate.FLOWFILE_RATE);    runner.setProperty(ControlRate.MAX_RATE, "2");    runner.setProperty(ControlRate.TIME_PERIOD, "1 sec");    runner.setProperty(ControlRate.GROUPING_ATTRIBUTE_NAME, "group");    createFlowFileWithGroup(runner, "one");        createFlowFile(runner, 1);    createFlowFileWithGroup(runner, "one");        createFlowFile(runner, 2);    runner.run(4, false);    runner.assertAllFlowFilesTransferred(ControlRate.REL_SUCCESS, 4);    runner.assertQueueEmpty();}
0
public String name()
{        return this.name.substring(1);}
0
public int read(byte[] b) throws IOException
{    if (inputIndex < inputEnd) {        int minimum = Math.min(b.length, inputEnd - inputIndex);        for (int i = 0; i < minimum; i++) {            b[i] = fourByteBuffer[inputIndex];            inputIndex++;        }        int rest = b.length - minimum;        if (rest == 0) {            return minimum;        }        try {            int additionalRead = in.read(b, minimum, rest);            if (additionalRead < 0) {                return minimum;            } else {                return minimum + additionalRead;            }        } catch (java.io.EOFException ex) {            return minimum;        }    } else {        return read(b, 0, b.length);    }}
0
public TEXTAREA<KBD<T>> textarea()
{    closeAttrs();    return textarea_(this, true);}
0
public KubernetesServiceCallServiceDiscoveryConfiguration clientKeyPassphrase(String clientKeyPassphrase)
{    setClientKeyPassphrase(clientKeyPassphrase);    return this;}
0
public void testBinaryDecimalPruning() throws Exception
{    String dataTable = createTable("data_table for_binary_decimal_pruning_no_meta", true);    String newTable = "dfs.`tmp`.binary_decimal_pruning";    tablesToDrop.add(newTable);    client.alterSession(ExecConstants.PARQUET_WRITER_USE_PRIMITIVE_TYPES_FOR_DECIMALS, false);    client.alterSession(ExecConstants.PARQUET_WRITER_LOGICAL_TYPE_FOR_DECIMALS, "binary");    queryBuilder().sql(String.format("create table %s partition by (part_binary) " + "as select part_int_32 as part_binary, val_int_32 as val_binary from %s", newTable, dataTable)).run();    String query = String.format("select part_binary, val_binary from %s where part_binary = cast(1.00 as decimal(5, 2))", newTable);    String plan = client.queryBuilder().sql(query).explainText();    assertThat(plan, containsString("numRowGroups=1"));    assertThat(plan, containsString("usedMetadataFile=false"));    assertThat(plan, not(containsString("Filter")));    client.testBuilder().sqlQuery(query).unOrdered().baselineColumns("part_binary", "val_binary").baselineValues(new BigDecimal("1.00"), new BigDecimal("1.05")).baselineValues(new BigDecimal("1.00"), new BigDecimal("10.00")).baselineValues(new BigDecimal("1.00"), new BigDecimal("10.25")).go();    queryBuilder().sql(String.format("refresh table metadata %s", newTable)).run();    plan = client.queryBuilder().sql(query).explainText();    assertThat(plan, containsString("numRowGroups=1"));    assertThat(plan, containsString("usedMetadataFile=true"));    assertThat(plan, not(containsString("Filter")));    client.testBuilder().sqlQuery(query).unOrdered().baselineColumns("part_binary", "val_binary").baselineValues(new BigDecimal("1.00"), new BigDecimal("1.05")).baselineValues(new BigDecimal("1.00"), new BigDecimal("10.00")).baselineValues(new BigDecimal("1.00"), new BigDecimal("10.25")).go();}
0
protected void addAuthorizationToRole(String roleName, String resourceType, String roleAuthorizationID)
{    if (!StringUtils.isEmpty(roleAuthorizationID)) {        RoleAuthorizationDAO roleAuthorizationDAO = injector.getInstance(RoleAuthorizationDAO.class);        RoleAuthorizationEntity roleAuthorization = roleAuthorizationDAO.findById(roleAuthorizationID);        if (roleAuthorization != null) {            addAuthorizationToRole(roleName, resourceType, roleAuthorization);        }    }}
0
public void testPercentageStorageTypeWithMixedTopology() throws Exception
{    double percentage = 0.9;    for (int i = 0; i < NODE_NUM; i++) {        if (coinFlip(percentage)) {            types[i] = StorageType.ARCHIVE;        } else {            types[i] = StorageType.DISK;        }    }    addNodeByTypes(types);        Thread.sleep(1000);    printMemUsage("before test1");    totalStart = System.nanoTime();    totalTrials = 0;    for (int i = 0; i < OP_NUM; i++) {                                        localStart = System.nanoTime();        totalTrials += 1;        node = cluster.chooseRandom("", excluded);        assertNotNull(node);        if (!isType(node, StorageType.ARCHIVE)) {            totalTrials += 1;            excluded.add(node);            node = dfscluster.chooseRandomWithStorageType("", excluded, StorageType.ARCHIVE);        }        assertTrue(isType(node, StorageType.ARCHIVE));        excluded.clear();        localEnd = System.nanoTime();        records[i] = localEnd - localStart;    }    totalEnd = System.nanoTime();    totalMs = (totalEnd - totalStart) / NS_TO_MS;            Thread.sleep(1000);    printMemUsage("test StorageType with mixed topology.");}
1
public void processElement(WindowedValue<InputT> element)
{    try {        Iterable<WindowedValue<InputT>> unprocessed = fnRunner.processElementInReadyWindows(element);        unprocessedElements.addAll(unprocessed);    } catch (Exception e) {        throw UserCodeException.wrap(e);    }}
0
public List<FieldSchema> getSchema(String catName, String db, String tableName) throws TException
{    EnvironmentContext envCxt = null;    String addedJars = MetastoreConf.getVar(conf, ConfVars.ADDED_JARS);    if (org.apache.commons.lang.StringUtils.isNotBlank(addedJars)) {        Map<String, String> props = new HashMap<>();        props.put("hive.added.jars.path", addedJars);        envCxt = new EnvironmentContext(props);    }    List<FieldSchema> fields = client.get_schema_with_environment_context(prependCatalogToDbName(catName, db, conf), tableName, envCxt);    return deepCopyFieldSchemas(fields);}
0
public Set<Relationship> getRelationships()
{    return relationships;}
0
private static Path getPathAndAssertUpload(String fileName, String type, Map<String, Path> uploadedFiles) throws MissingFileException
{    final Path file = uploadedFiles.get(fileName);    if (file == null) {        throw new MissingFileException(type, fileName);    }    return file;}
0
public int hashCode()
{    return source.hashCode() + name().hashCode();}
0
protected String getProtocol()
{    /*         * The tests are all setup for HTTP so need to convert the protocol         * values to AJP.         */        String protocol = System.getProperty("tomcat.test.protocol");        if (protocol == null) {        protocol = "org.apache.coyote.ajp.AjpNioProtocol";    } else if (protocol.contains("Nio2")) {        protocol = "org.apache.coyote.ajp.AjpNio2Protocol";    } else if (protocol.contains("Apr")) {        protocol = "org.apache.coyote.ajp.AjpAprProtocol";    } else {        protocol = "org.apache.coyote.ajp.AjpNioProtocol";    }    return protocol;}
0
public BucketingSink<T> setWriter(Writer<T> writer)
{    this.writerTemplate = writer;    return this;}
0
protected Expression zeroLiteral()
{    return literal(0L);}
0
public boolean advance() throws IOException
{    if (iterator.hasNext()) {        current = iterator.next();        return true;    } else {        return false;    }}
0
public void configureServer(Server server)
{    parent.configureServer(server);    child.configureServer(server);}
0
public File getKerberosServiceKeytab()
{    return null;}
0
public void run()
{    action.start();}
0
private void chooseSchema(SupportedTypes supportedTypes, int maxComplexDepth)
{    Set<Integer> hashSet = null;    final boolean allTypes;    final boolean onlyOne = (r.nextInt(100) == 7);    if (onlyOne) {        columnCount = 1;        allTypes = false;    } else {        allTypes = r.nextBoolean();        if (allTypes) {            switch(supportedTypes) {                case ALL:                    columnCount = possibleHivePrimitiveTypeNames.length + possibleHiveComplexTypeNames.length;                    break;                case ALL_EXCEPT_MAP:                    columnCount = possibleHivePrimitiveTypeNames.length + possibleHiveComplexTypeNames.length - 1;                    break;                case PRIMITIVE:                    columnCount = possibleHivePrimitiveTypeNames.length;                    break;            }            hashSet = new HashSet<Integer>();        } else {            columnCount = 1 + r.nextInt(20);        }    }    typeNames = new ArrayList<String>(columnCount);    categories = new Category[columnCount];    typeInfos = new TypeInfo[columnCount];    objectInspectorList = new ArrayList<ObjectInspector>(columnCount);    primitiveCategories = new PrimitiveCategory[columnCount];    primitiveTypeInfos = new PrimitiveTypeInfo[columnCount];    primitiveObjectInspectorList = new ArrayList<ObjectInspector>(columnCount);    final List<String> columnNames = new ArrayList<String>(columnCount);    for (int c = 0; c < columnCount; c++) {        columnNames.add(String.format("col%d", c));        String typeName;        if (onlyOne) {            typeName = getRandomTypeName(supportedTypes);        } else {            int typeNum;            if (allTypes) {                int maxTypeNum = 0;                switch(supportedTypes) {                    case PRIMITIVE:                        maxTypeNum = possibleHivePrimitiveTypeNames.length;                        break;                    case ALL_EXCEPT_MAP:                        maxTypeNum = possibleHivePrimitiveTypeNames.length + possibleHiveComplexTypeNames.length - 1;                        break;                    case ALL:                        maxTypeNum = possibleHivePrimitiveTypeNames.length + possibleHiveComplexTypeNames.length;                        break;                }                while (true) {                    typeNum = r.nextInt(maxTypeNum);                    final Integer typeNumInteger = Integer.valueOf(typeNum);                    if (!hashSet.contains(typeNumInteger)) {                        hashSet.add(typeNumInteger);                        break;                    }                }            } else {                if (supportedTypes == SupportedTypes.PRIMITIVE || r.nextInt(10) != 0) {                    typeNum = r.nextInt(possibleHivePrimitiveTypeNames.length);                } else {                    typeNum = possibleHivePrimitiveTypeNames.length + r.nextInt(possibleHiveComplexTypeNames.length);                    if (supportedTypes == SupportedTypes.ALL_EXCEPT_MAP) {                        typeNum--;                    }                }            }            if (typeNum < possibleHivePrimitiveTypeNames.length) {                typeName = possibleHivePrimitiveTypeNames[typeNum];            } else {                typeName = possibleHiveComplexTypeNames[typeNum - possibleHivePrimitiveTypeNames.length];            }        }        final String decoratedTypeName = getDecoratedTypeName(typeName, supportedTypes, 0, maxComplexDepth);        final TypeInfo typeInfo;        try {            typeInfo = TypeInfoUtils.getTypeInfoFromTypeString(decoratedTypeName);        } catch (Exception e) {            throw new RuntimeException("Cannot convert type name " + decoratedTypeName + " to a type " + e);        }        typeInfos[c] = typeInfo;        final Category category = typeInfo.getCategory();        categories[c] = category;        ObjectInspector objectInspector = getObjectInspector(typeInfo);        switch(category) {            case PRIMITIVE:                {                    final PrimitiveTypeInfo primitiveTypeInfo = (PrimitiveTypeInfo) typeInfo;                    final PrimitiveCategory primitiveCategory = primitiveTypeInfo.getPrimitiveCategory();                    objectInspector = PrimitiveObjectInspectorFactory.getPrimitiveWritableObjectInspector(primitiveTypeInfo);                    primitiveTypeInfos[c] = primitiveTypeInfo;                    primitiveCategories[c] = primitiveCategory;                    primitiveObjectInspectorList.add(objectInspector);                }                break;            case LIST:            case MAP:            case STRUCT:            case UNION:                primitiveObjectInspectorList.add(null);                break;            default:                throw new RuntimeException("Unexpected catagory " + category);        }        objectInspectorList.add(objectInspector);        if (category == Category.PRIMITIVE) {        }        typeNames.add(decoratedTypeName);    }    rowStructObjectInspector = ObjectInspectorFactory.getStandardStructObjectInspector(columnNames, objectInspectorList);    alphabets = new String[columnCount];}
0
public synchronized void close()
{    if (_key != null) {        _connection.close(_key);    }}
0
public static boolean hasNonNullUnion(org.apache.avro.Schema schema)
{    if (schema.getType() == Type.UNION) {        final List<org.apache.avro.Schema> types = schema.getTypes();        if (types.size() == 2) {            return !types.contains(NULL_SCHEMA);        } else {            return true;        }    }    return false;}
0
public void append(LoggingEvent event)
{}
0
 static TimeDuration getClientRequestTimeout(Configuration conf)
{        final TimeUnit timeUnit = OzoneConfigKeys.DFS_RATIS_CLIENT_REQUEST_TIMEOUT_DURATION_DEFAULT.getUnit();    final long duration = conf.getTimeDuration(OzoneConfigKeys.DFS_RATIS_CLIENT_REQUEST_TIMEOUT_DURATION_KEY, OzoneConfigKeys.DFS_RATIS_CLIENT_REQUEST_TIMEOUT_DURATION_DEFAULT.getDuration(), timeUnit);    final TimeDuration clientRequestTimeout = TimeDuration.valueOf(duration, timeUnit);    return clientRequestTimeout;}
0
private static void assertSyncMethod(Method method, String apiName, List<String> booleanReturnMethods)
{        if (apiName.equals("ping") || apiName.contains("exist") || booleanReturnMethods.contains(apiName)) {        assertThat("the return type for method [" + method + "] is incorrect", method.getReturnType().getSimpleName(), equalTo("boolean"));    } else {                if (!method.getReturnType().isAssignableFrom(Optional.class)) {            assertThat("the return type for method [" + method + "] is incorrect", method.getReturnType().getSimpleName(), endsWith("Response"));        }    }    assertEquals("incorrect number of exceptions for method [" + method + "]", 1, method.getExceptionTypes().length);        if (APIS_WITHOUT_REQUEST_OBJECT.contains(apiName)) {        assertEquals("incorrect number of arguments for method [" + method + "]", 1, method.getParameterTypes().length);        assertThat("the parameter to method [" + method + "] is the wrong type", method.getParameterTypes()[0], equalTo(RequestOptions.class));    } else {        assertEquals("incorrect number of arguments for method [" + method + "]", 2, method.getParameterTypes().length);                if (method.getParameterTypes()[0].equals(RequestOptions.class)) {            assertThat("the first parameter to method [" + method + "] is the wrong type", method.getParameterTypes()[0], equalTo(RequestOptions.class));            assertThat("the second parameter to method [" + method + "] is the wrong type", method.getParameterTypes()[1].getSimpleName(), endsWith("Request"));        } else {            assertThat("the first parameter to method [" + method + "] is the wrong type", method.getParameterTypes()[0].getSimpleName(), endsWith("Request"));            assertThat("the second parameter to method [" + method + "] is the wrong type", method.getParameterTypes()[1], equalTo(RequestOptions.class));        }    }}
0
public String getRedirectLocation()
{    return redirectLocation;}
0
public QuerySearchResult serviceTimeEWMA(long serviceTimeEWMA)
{    this.serviceTimeEWMA = serviceTimeEWMA;    return this;}
0
public void updateFloat(final String columnName, final float x) throws SQLException
{    try {        resultSet.updateFloat(columnName, x);    } catch (final SQLException e) {        handleException(e);    }}
0
public void collect(IT record)
{    try {        this.numRecordsIn.inc();        this.outputCollector.collect(record);    } catch (Exception ex) {        throw new ExceptionInChainedStubException(this.taskName, ex);    }}
0
public static Collection sort(List<Object> memberInfo)
{    Collections.sort(memberInfo, new MemberComparator());    return memberInfo;}
0
public int getOutgoingRecoveries(String nodeId)
{    return recoveriesPerNode.getOrDefault(nodeId, Recoveries.EMPTY).getOutgoing();}
0
public boolean next() throws IOException
{    if (upto-- > 0) {        pos = pe.nextPosition();        return true;    }    return false;}
0
private void addHdfsResource(Configuration conf, List<LocalResource> tmpResources, LocalResourceType type, String[] files) throws IOException
{    for (String file : files) {        if (StringUtils.isNotBlank(file)) {            Path dest = new Path(file);            FileSystem destFS = dest.getFileSystem(conf);            LocalResource localResource = createLocalResource(destFS, dest, type, LocalResourceVisibility.PRIVATE);            tmpResources.add(localResource);        }    }}
0
public DataStructure createObject()
{    return new ActiveMQTempQueue();}
0
private void assignQueueSignal()
{    assignQueueLock.lock();    try {        assignQueueFullCond.signal();    } finally {        assignQueueLock.unlock();    }}
0
public static void beforeTest() throws Exception
{    createAndStartJetty(legacyExampleCollection1SolrHome());}
0
public String getRecordIndex()
{    return recordIndex;}
0
private boolean hasRegisteredState()
{    return !(registeredKVStates.isEmpty() && registeredPQStates.isEmpty());}
0
private List<LoggingEvent> getLogEvents()
{    return logEvents;}
0
protected final T transformNodeProps(Function<? super E, ? extends E> rule, Class<E> typeToken)
{    return info().transform(rule, typeToken);}
0
public Answer execute(CreateObjectCommand cmd)
{        DataTO data = cmd.getData();    if (data.getObjectType() == DataObjectType.VOLUME) {        return createVolume(cmd);    } else if (data.getObjectType() == DataObjectType.SNAPSHOT) {        return createSnapshot(cmd);    } else if (data.getObjectType() == DataObjectType.TEMPLATE) {            }    return new CreateObjectAnswer(data.getObjectType() + " object creation not supported");}
1
public String toString()
{    return String.format("The status of listener %s is %s", new Object[] { Integer.valueOf(this.listenerId), this.status });}
0
public void testReadOnly() throws Exception
{    Path tempDir = createTempDir();    Path path = tempDir.resolve("bar");    try (FsBlobStore store = new FsBlobStore(Settings.EMPTY, path, true)) {        assertFalse(Files.exists(path));        BlobPath blobPath = BlobPath.cleanPath().add("foo");        store.blobContainer(blobPath);        Path storePath = store.path();        for (String d : blobPath) {            storePath = storePath.resolve(d);        }        assertFalse(Files.exists(storePath));    }    try (FsBlobStore store = new FsBlobStore(Settings.EMPTY, path, false)) {        assertTrue(Files.exists(path));        BlobPath blobPath = BlobPath.cleanPath().add("foo");        BlobContainer container = store.blobContainer(blobPath);        Path storePath = store.path();        for (String d : blobPath) {            storePath = storePath.resolve(d);        }        assertTrue(Files.exists(storePath));        assertTrue(Files.isDirectory(storePath));        byte[] data = randomBytes(randomIntBetween(10, scaledRandomIntBetween(1024, 1 << 16)));        writeBlob(container, "test", new BytesArray(data));        assertArrayEquals(readBlobFully(container, "test", data.length), data);        assertTrue(BlobStoreTestUtil.blobExists(container, "test"));    }}
0
public void start(InternalManagedEntity entity)
{    if (logger.isTraceEnabled(LogMarker.MANAGED_ENTITY_VERBOSE)) {        logger.trace(LogMarker.MANAGED_ENTITY_VERBOSE, "DisabledManagedEntityController#start {}", EXCEPTION_MESSAGE);    }    throw new UnsupportedOperationException(EXCEPTION_MESSAGE);}
0
public void setup()
{    ThreadContext.remove();}
0
public void severe(final Marker marker, final Object message, final Throwable throwable)
{    logWrapper.logIfEnabled(loggerName, Level.FATAL, marker, message, throwable);}
0
public boolean isMapTask()
{    return isMap;}
0
public boolean destroy(final InternalRegion region, final EntryEventImpl event, final boolean inTokenMode, final boolean cacheWrite, final Object expectedOldValue, final boolean forceDestroy, final boolean removeRecoveredEntry) throws CacheWriterException, EntryNotFoundException, TimeoutException, RegionClearedException
{    return false;}
0
protected String getDefaultWireFormatType()
{    return "stomp";}
0
public void testInternalSSLWrongKeystorePassword() throws Exception
{    final Configuration config = createInternalSslConfigWithKeyAndTrustStores();    config.setString(SecurityOptions.SSL_INTERNAL_KEYSTORE_PASSWORD, "badpw");    try {        SSLUtils.createInternalServerSSLEngineFactory(config);        fail("exception expected");    } catch (Exception ignored) {    }    try {        SSLUtils.createInternalClientSSLEngineFactory(config);        fail("exception expected");    } catch (Exception ignored) {    }}
0
public NetworkPolicyModel getNetworkPolicyModel()
{    return _policyModel;}
0
public void testParseDoSectionMultivaluedField() throws Exception
{    parser = createParser(YamlXContent.yamlXContent, "indices.get_field_mapping:\n" + "        index: test_index\n" + "        type: test_type\n" + "        field: [ text , text1 ]");    DoSection doSection = DoSection.parse(parser);    assertThat(doSection.getCatch(), nullValue());    assertThat(doSection.getApiCallSection(), notNullValue());    assertThat(doSection.getApiCallSection().getApi(), equalTo("indices.get_field_mapping"));    assertThat(doSection.getApiCallSection().getParams().size(), equalTo(3));    assertThat(doSection.getApiCallSection().getParams().get("index"), equalTo("test_index"));    assertThat(doSection.getApiCallSection().getParams().get("type"), equalTo("test_type"));    assertThat(doSection.getApiCallSection().getParams().get("field"), equalTo("text,text1"));    assertThat(doSection.getApiCallSection().hasBody(), equalTo(false));    assertThat(doSection.getApiCallSection().getBodies().size(), equalTo(0));}
0
public void destroyIndexOnRegionNotInClusterConfig()
{    IgnoredException.addIgnoredException("failed to update cluster config for cluster");    IgnoredException.addIgnoredException("org.apache.geode.management.internal.exceptions.EntityNotFoundException");    server1.invoke(() -> {        Cache cache = ClusterStartupRule.getCache();        RegionFactory factory = cache.createRegionFactory(RegionShortcut.REPLICATE);        factory.create("REGION3");        cache.getQueryService().createIndex("INDEX3", "key", "/REGION3");    });    gfsh.executeAndAssertThat("destroy index --name=INDEX3" + " --region=REGION3").statusIsSuccess().tableHasColumnWithExactValuesInAnyOrder("Status", "OK", "ERROR").tableHasColumnWithExactValuesInAnyOrder("Message", "Destroyed index INDEX3 on region REGION3", "Region \"REGION3\" not found");    assertIndexCount(REGION_1, 2);}
0
 AdvancedSalesforceEndpointConsumerBuilder basicPropertyBinding(boolean basicPropertyBinding)
{    doSetProperty("basicPropertyBinding", basicPropertyBinding);    return this;}
0
public void setLbForceSession(Boolean lbForceSession)
{    this.lbForceSession = lbForceSession;}
0
public Result implement(EnumerableRelImplementor implementor, Prefer pref)
{    final BlockBuilder builder = new BlockBuilder();    final Result leftResult = implementor.visitChild(this, 0, (EnumerableRel) left, pref);    Expression leftExpression = builder.append("left", leftResult.block);    final Result rightResult = implementor.visitChild(this, 1, (EnumerableRel) right, pref);    Expression rightExpression = builder.append("right", rightResult.block);    final PhysType physType = PhysTypeImpl.of(implementor.getTypeFactory(), getRowType(), pref.preferArray());    final Expression predicate = EnumUtils.generatePredicate(implementor, getCluster().getRexBuilder(), left, right, leftResult.physType, rightResult.physType, condition);    return implementor.result(physType, builder.append(Expressions.call(BuiltInMethod.NESTED_LOOP_JOIN.method, leftExpression, rightExpression, predicate, EnumUtils.joinSelector(joinType, physType, ImmutableList.of(leftResult.physType, rightResult.physType)), Expressions.constant(EnumUtils.toLinq4jJoinType(joinType)))).toBlock());}
0
public alter_partition_with_environment_context_argsTupleScheme getScheme()
{    return new alter_partition_with_environment_context_argsTupleScheme();}
0
public java.lang.Class<org.apache.drill.exec.proto.ExecProtos.ServerPreparedStatementState.Builder> typeClass()
{    return org.apache.drill.exec.proto.ExecProtos.ServerPreparedStatementState.Builder.class;}
0
protected RouteBuilder createRouteBuilder() throws Exception
{    return new RouteBuilder() {        @Override        public void configure() throws Exception {                        from("direct:start").policy("foo").setBody().constant("body not altered").to("mock:foo").end();        }    };}
0
private static void ensureInitialized()
{    if (!isInitialized()) {        synchronized (UserGroupInformation.class) {            if (!isInitialized()) {                                initialize(new Configuration(), false);            }        }    }}
0
private static ClusterState.Builder createClusterState(String indexName, String aliasName, String templateName, String buildMappingFrom, int format, IndexMetaData.State state) throws IOException
{    IndexTemplateMetaData.Builder templateBuilder = getIndexTemplateMetaData(templateName);    IndexMetaData.Builder indexMeta = getIndexMetadata(indexName, aliasName, buildMappingFrom, format, state);    MetaData.Builder metaDataBuilder = new MetaData.Builder();    metaDataBuilder.put(templateBuilder);    metaDataBuilder.put(indexMeta);    return ClusterState.builder(state()).metaData(metaDataBuilder.build());}
0
public JobACL getJobACLNeeded()
{    return jobACLNeeded;}
0
public void configure(HashTree testTree)
{        SearchByClass<TestPlan> testPlan = new SearchByClass<>(TestPlan.class);    testTree.traverse(testPlan);    Object[] plan = testPlan.getSearchResults().toArray();    if (plan.length == 0) {        throw new IllegalStateException("Could not find the TestPlan class!");    }    TestPlan tp = (TestPlan) plan[0];    serialized = tp.isSerialized();    tearDownOnShutdown = tp.isTearDownOnShutdown();    active = true;    test = testTree;}
0
public void testColumnsIndexTooLarge()
{    try {        ScanLevelProjection.build(RowSetTestUtils.projectCols(SchemaPath.parseFromString("columns[70000]")), ScanTestUtils.parsers(new ColumnsArrayParser(true)));        fail();    } catch (UserException e) {        }}
0
public void update(long value)
{    histogram.update(value);}
0
private String formattedName(String counterName, String vertexName)
{    return String.format("%s_", counterName) + vertexName.replace(" ", "_");}
0
public V putIfAbsent(final String key, final V value)
{    final V existing = delegate.putIfAbsent(key, value);    if (existing == null) {        fireListeners(TransientStoreEvent.of(TransientStoreEventType.CREATE, key, value));    }    return existing;}
0
 static Map<Long, Set<Long>> checkConfigGroupHostMapping(boolean warnIfFound)
{        Map<Long, Set<Long>> nonMappedHostIds = new HashMap<>();    Clusters clusters = injector.getInstance(Clusters.class);    Map<String, Cluster> clusterMap = clusters.getClusters();    StringBuilder output = new StringBuilder("[(ConfigGroup, Service, HostCount) => ");    if (!MapUtils.isEmpty(clusterMap)) {        for (Cluster cluster : clusterMap.values()) {            Map<Long, ConfigGroup> configGroups = cluster.getConfigGroups();            Map<String, Host> clusterHosts = clusters.getHostsForCluster(cluster.getClusterName());            if (!MapUtils.isEmpty(configGroups) && !MapUtils.isEmpty(clusterHosts)) {                for (ConfigGroup configGroup : configGroups.values()) {                                                                                Map<Long, Host> hosts = configGroup.getHosts();                    boolean addToOutput = false;                    Set<String> hostnames = new HashSet<>();                    if (!MapUtils.isEmpty(hosts)) {                        for (Host host : hosts.values()) {                                                        if (!clusterHosts.containsKey(host.getHostName())) {                                Set<Long> hostIds = nonMappedHostIds.computeIfAbsent(configGroup.getId(), configGroupId -> new HashSet<>());                                hostIds.add(host.getHostId());                                hostnames.add(host.getHostName());                                addToOutput = true;                            }                        }                    }                    if (addToOutput) {                        output.append("( ");                        output.append(configGroup.getName());                        output.append(", ");                        output.append(configGroup.getTag());                        output.append(", ");                        output.append(hostnames);                        output.append(" ), ");                    }                }            }        }    }    if (!MapUtils.isEmpty(nonMappedHostIds) && warnIfFound) {        output.replace(output.lastIndexOf(","), output.length(), "]");        warning("You have config group host mappings with hosts that are no " + "longer associated with the cluster, {}. Run --auto-fix-database to " + "fix this automatically. Alternatively, you can remove this mapping " + "from the UI. Please backup Ambari Server database before running --auto-fix-database.", output.toString());    }    return nonMappedHostIds;}
1
public void testDiscardPreallocatedBlocks() throws Exception
{    String keyName = getKeyName();    OzoneOutputStream key = createKey(keyName, ReplicationType.RATIS, 2 * blockSize);    KeyOutputStream keyOutputStream = (KeyOutputStream) key.getOutputStream();    Assert.assertTrue(key.getOutputStream() instanceof KeyOutputStream);        Assert.assertEquals(2, keyOutputStream.getStreamEntries().size());    String dataString = ContainerTestHelper.getFixedLengthString(keyString, (1 * blockSize));    byte[] data = dataString.getBytes(UTF_8);    key.write(data);    List<OmKeyLocationInfo> locationInfos = new ArrayList<>(keyOutputStream.getLocationInfoList());    long containerID = locationInfos.get(0).getContainerID();    ContainerInfo container = cluster.getStorageContainerManager().getContainerManager().getContainer(ContainerID.valueof(containerID));    Pipeline pipeline = cluster.getStorageContainerManager().getPipelineManager().getPipeline(container.getPipelineID());    List<DatanodeDetails> datanodes = pipeline.getNodes();    Assert.assertEquals(1, datanodes.size());    waitForContainerClose(key);    dataString = ContainerTestHelper.getFixedLengthString(keyString, (1 * blockSize));    data = dataString.getBytes(UTF_8);    key.write(data);    Assert.assertEquals(2, keyOutputStream.getStreamEntries().size());                Assert.assertTrue(keyOutputStream.getLocationInfoList().get(0).getBlockID().equals(locationInfos.get(0).getBlockID()));    Assert.assertFalse(keyOutputStream.getLocationInfoList().get(1).getBlockID().equals(locationInfos.get(1).getBlockID()));    key.close();}
0
public void testIllegalExpressionsInvalidLimitPre()
{    thrown.expect(IllegalArgumentException.class);    ExpressionFactory.fromString("BYTES_READ > foo1024");}
0
 CouchbaseEndpointConsumerBuilder schedulerProperties(Map<String, Object> schedulerProperties)
{    doSetProperty("schedulerProperties", schedulerProperties);    return this;}
0
public boolean equals(Object that)
{    throw new RuntimeException("Not implemented");}
0
public void setHeader(List<String> header)
{    this.header = header;}
0
public void write(DataOutput out) throws IOException
{    out.writeUTF(name);    out.writeLong(this.index.getPageId());}
0
private void deleteAttemptTempFiles(Path targetWorkPath, FileSystem targetFS, String jobId) throws IOException
{    if (targetWorkPath == null) {        return;    }    FileStatus[] tempFiles = targetFS.globStatus(new Path(targetWorkPath, ".distcp.tmp." + jobId.replaceAll("job", "attempt") + "*"));    if (tempFiles != null && tempFiles.length > 0) {        for (FileStatus file : tempFiles) {                        targetFS.delete(file.getPath(), false);        }    }}
1
public AlertingService getAlertingService()
{    return NullAlertingService.get();}
0
 ResourceInstance createClusterResource(String clusterName)
{    return createResource(Resource.Type.Cluster, Collections.singletonMap(Resource.Type.Cluster, clusterName));}
0
public long getPeriod()
{    return this.jobLocation.getPeriod();}
0
private void testCreateResourcesMixed(Authentication authentication) throws Exception
{    Cluster cluster = createNiceMock(Cluster.class);    Map<String, String> hostLevelParams = new HashMap<>();    StackId stackId = new StackId("HDP", "2.0.1");    File f = new File("src/test/resources/hbase_version_test.xml");    String xml = IOUtils.toString(new FileInputStream(f));        xml = xml.replace("<package-version>2_3_4_0_3396</package-version>", "");    StackEntity stack = new StackEntity();    stack.setStackName("HDP");    RepositoryVersionEntity repoVersion = new RepositoryVersionEntity();    repoVersion.setStack(stack);    repoVersion.setId(1l);    repoVersion.addRepoOsEntities(REPO_OS_ENTITIES);    repoVersion.setVersionXml(xml);    repoVersion.setVersionXsd("version_definition.xsd");    repoVersion.setType(RepositoryType.STANDARD);    Map<String, Host> hostsForCluster = new HashMap<>();    int hostCount = 10;    for (int i = 0; i < hostCount; i++) {        String hostname = "host" + i;        List<HostVersionEntity> hostVersions = new ArrayList<>();        HostVersionEntity hostVersion = createNiceMock(HostVersionEntity.class);        expect(hostVersion.getRepositoryVersion()).andReturn(repoVersion);        hostVersions.add(hostVersion);        if (i == 2) {                        RepositoryVersionEntity badRve = new RepositoryVersionEntity();            badRve.setStack(stack);            badRve.setVersion("2.2.1.0-1000");            HostVersionEntity badHostVersion = createNiceMock(HostVersionEntity.class);            expect(badHostVersion.getRepositoryVersion()).andReturn(badRve);            hostVersions.add(badHostVersion);            replay(badHostVersion);        }        Host host = createNiceMock(hostname, Host.class);        expect(host.getHostName()).andReturn(hostname).anyTimes();        expect(host.getOsFamily()).andReturn("redhat6").anyTimes();        expect(host.getMaintenanceState(EasyMock.anyLong())).andReturn(MaintenanceState.OFF).anyTimes();        expect(host.getAllHostVersions()).andReturn(hostVersions).anyTimes();        replay(host, hostVersion);        hostsForCluster.put(hostname, host);    }    final ServiceComponentHost schDatanode = createMock(ServiceComponentHost.class);    expect(schDatanode.getServiceName()).andReturn("HDFS").anyTimes();    expect(schDatanode.getServiceComponentName()).andReturn("DATANODE").anyTimes();    final ServiceComponentHost schNamenode = createMock(ServiceComponentHost.class);    expect(schNamenode.getServiceName()).andReturn("HDFS").anyTimes();    expect(schNamenode.getServiceComponentName()).andReturn("NAMENODE").anyTimes();    final ServiceComponentHost schAMS = createMock(ServiceComponentHost.class);    expect(schAMS.getServiceName()).andReturn("AMBARI_METRICS").anyTimes();    expect(schAMS.getServiceComponentName()).andReturn("METRICS_COLLECTOR").anyTimes();        final List<ServiceComponentHost> schsH1 = Lists.newArrayList(schDatanode, schNamenode, schAMS);        final List<ServiceComponentHost> schsH2 = Lists.newArrayList(schAMS);    ServiceOsSpecific.Package hdfsPackage = new ServiceOsSpecific.Package();    hdfsPackage.setName("hdfs");    List<ServiceOsSpecific.Package> packages = Collections.singletonList(hdfsPackage);    ActionManager actionManager = createNiceMock(ActionManager.class);    RequestStatusResponse response = createNiceMock(RequestStatusResponse.class);    Map<String, Map<String, String>> hostConfigTags = new HashMap<>();    expect(configHelper.getEffectiveDesiredTags(anyObject(ClusterImpl.class), anyObject(String.class))).andReturn(hostConfigTags);    expect(managementController.getClusters()).andReturn(clusters).anyTimes();    expect(managementController.getAmbariMetaInfo()).andReturn(ambariMetaInfo).anyTimes();    expect(managementController.getAuthName()).andReturn("admin").anyTimes();    expect(managementController.getActionManager()).andReturn(actionManager).anyTimes();    expect(managementController.getJdkResourceUrl()).andReturn("/JdkResourceUrl").anyTimes();    expect(managementController.getPackagesForServiceHost(anyObject(ServiceInfo.class), EasyMock.<Map<String, String>>anyObject(), anyObject(String.class))).andReturn(packages).anyTimes();    expect(managementController.findConfigurationTagsWithOverrides(anyObject(Cluster.class), EasyMock.anyString())).andReturn(new HashMap<String, Map<String, String>>()).anyTimes();    expect(clusters.getCluster(anyObject(String.class))).andReturn(cluster);    expect(clusters.getHostsForCluster(anyObject(String.class))).andReturn(hostsForCluster).anyTimes();    String clusterName = "Cluster100";    expect(cluster.getClusterId()).andReturn(1L).anyTimes();    expect(cluster.getHosts()).andReturn(hostsForCluster.values()).atLeastOnce();    expect(cluster.getServices()).andReturn(new HashMap<>()).anyTimes();    expect(cluster.getCurrentStackVersion()).andReturn(stackId);    expect(cluster.getServiceComponentHosts(anyObject(String.class))).andAnswer(new IAnswer<List<ServiceComponentHost>>() {        @Override        public List<ServiceComponentHost> answer() throws Throwable {            String hostname = (String) EasyMock.getCurrentArguments()[0];            if (hostname.equals("host2")) {                return schsH2;            } else {                return schsH1;            }        }    }).anyTimes();            expect(cluster.transitionHostsToInstalling(anyObject(RepositoryVersionEntity.class), anyObject(VersionDefinitionXml.class), eq(false))).andReturn(Collections.emptyList()).anyTimes();    ExecutionCommand executionCommand = createNiceMock(ExecutionCommand.class);    ExecutionCommandWrapper executionCommandWrapper = createNiceMock(ExecutionCommandWrapper.class);    expect(executionCommandWrapper.getExecutionCommand()).andReturn(executionCommand).anyTimes();    Stage stage = createNiceMock(Stage.class);    expect(stage.getExecutionCommandWrapper(anyObject(String.class), anyObject(String.class))).andReturn(executionCommandWrapper).anyTimes();    expect(executionCommand.getHostLevelParams()).andReturn(hostLevelParams).anyTimes();    Map<Role, Float> successFactors = new HashMap<>();    expect(stage.getSuccessFactors()).andReturn(successFactors).atLeastOnce();        expect(stageFactory.createNew(anyLong(), anyObject(String.class), anyObject(String.class), anyLong(), anyObject(String.class), anyObject(String.class), anyObject(String.class))).andReturn(stage).times((int) Math.ceil(hostCount / MAX_TASKS_PER_STAGE));    expect(repositoryVersionDAOMock.findByStackAndVersion(anyObject(StackId.class), anyObject(String.class))).andReturn(repoVersion);    expect(actionManager.getRequestTasks(anyLong())).andReturn(Collections.emptyList()).anyTimes();    ClusterEntity clusterEntity = new ClusterEntity();    clusterEntity.setClusterId(1l);    clusterEntity.setClusterName(clusterName);    TopologyManager topologyManager = injector.getInstance(TopologyManager.class);    StageUtils.setTopologyManager(topologyManager);    StageUtils.setConfiguration(injector.getInstance(Configuration.class));        replay(managementController, response, clusters, cluster, repositoryVersionDAOMock, configHelper, schDatanode, schNamenode, schAMS, actionManager, executionCommand, executionCommandWrapper, stage, stageFactory);    ResourceProvider provider = createProvider(managementController);    injector.injectMembers(provider);        Set<Map<String, Object>> propertySet = new LinkedHashSet<>();    Map<String, Object> properties = new LinkedHashMap<>();        properties.put(ClusterStackVersionResourceProvider.CLUSTER_STACK_VERSION_CLUSTER_NAME_PROPERTY_ID, "Cluster100");    properties.put(ClusterStackVersionResourceProvider.CLUSTER_STACK_VERSION_REPOSITORY_VERSION_PROPERTY_ID, "2.2.0.1-885");    properties.put(ClusterStackVersionResourceProvider.CLUSTER_STACK_VERSION_STACK_PROPERTY_ID, "HDP");    properties.put(ClusterStackVersionResourceProvider.CLUSTER_STACK_VERSION_VERSION_PROPERTY_ID, "2.1.1");    propertySet.add(properties);        Request request = PropertyHelper.getCreateRequest(propertySet, null);    SecurityContextHolder.getContext().setAuthentication(authentication);    try {        provider.createResources(request);        Assert.fail("Expecting the create to fail due to an already installed version");    } catch (IllegalArgumentException iae) {        }}
0
public void createMappingWithSynchronousReturnsStatusOKWhenAsycnEventQueueAlreadyExists() throws IOException
{    results.add(successFunctionResult);    ConfigurationPersistenceService configurationPersistenceService = mock(ConfigurationPersistenceService.class);    doReturn(configurationPersistenceService).when(createRegionMappingCommand).getConfigurationPersistenceService();    when(configurationPersistenceService.getCacheConfig(ConfigurationPersistenceService.CLUSTER_CONFIG)).thenReturn(cacheConfig);    List<RegionConfig> list = new ArrayList<>();    list.add(matchingRegion);    when(cacheConfig.getRegions()).thenReturn(list);    RegionAttributesType loaderAttribute = mock(RegionAttributesType.class);    when(loaderAttribute.getCacheLoader()).thenReturn(null);    when(matchingRegion.getRegionAttributes()).thenReturn(loaderAttribute);    List<AsyncEventQueue> asyncEventQueues = new ArrayList<>();    AsyncEventQueue matchingQueue = mock(AsyncEventQueue.class);    String queueName = MappingCommandUtils.createAsyncEventQueueName(regionName);    when(matchingQueue.getId()).thenReturn(queueName);    asyncEventQueues.add(matchingQueue);    when(cacheConfig.getAsyncEventQueues()).thenReturn(asyncEventQueues);    ResultModel result = createRegionMappingCommand.createMapping(regionName, dataSourceName, tableName, pdxClass, pdxClassFile, true, null, null, null, false, null);    assertThat(result.getStatus()).isSameAs(Result.Status.OK);}
0
 AdvancedReactiveStreamsEndpointBuilder basicPropertyBinding(boolean basicPropertyBinding)
{    doSetProperty("basicPropertyBinding", basicPropertyBinding);    return this;}
0
protected void channelRead0(ChannelHandlerContext ctx, ByteBuf msg) throws Exception
{    byte[] bytes = new byte[msg.readableBytes()];    msg.readBytes(bytes);    ctx.fireChannelRead(Unpooled.wrappedBuffer(cryptoAES.unwrap(bytes, 0, bytes.length)));}
0
public void testAbortPendingCheckpointsWithTriggerValidation()
{    final int maxConcurrentCheckpoints = ThreadLocalRandom.current().nextInt(10) + 1;    ExecutionVertex executionVertex = mockExecutionVertex();    CheckpointCoordinatorConfiguration checkpointCoordinatorConfiguration = new CheckpointCoordinatorConfiguration(Integer.MAX_VALUE, Integer.MAX_VALUE, 0, maxConcurrentCheckpoints, CheckpointRetentionPolicy.NEVER_RETAIN_AFTER_TERMINATION, true, false, 0);    CheckpointCoordinator checkpointCoordinator = new CheckpointCoordinator(new JobID(), checkpointCoordinatorConfiguration, new ExecutionVertex[] { executionVertex }, new ExecutionVertex[] { executionVertex }, new ExecutionVertex[] { executionVertex }, new StandaloneCheckpointIDCounter(), new StandaloneCompletedCheckpointStore(1), new MemoryStateBackend(), Executors.directExecutor(), SharedStateRegistry.DEFAULT_FACTORY, mock(CheckpointFailureManager.class));        mockExecutionRunning(executionVertex);        ManualCheckpointTimer manualCheckpointTimer = new ManualCheckpointTimer(manualThreadExecutor);            manualCheckpointTimer.setManualDelay(0L);    Whitebox.setInternalState(checkpointCoordinator, "timer", manualCheckpointTimer);    checkpointCoordinator.startCheckpointScheduler();    assertTrue(checkpointCoordinator.isCurrentPeriodicTriggerAvailable());    manualThreadExecutor.triggerAll();    manualThreadExecutor.triggerScheduledTasks();    assertEquals(1, checkpointCoordinator.getNumberOfPendingCheckpoints());    for (int i = 1; i < maxConcurrentCheckpoints; i++) {        checkpointCoordinator.triggerCheckpoint(System.currentTimeMillis(), false);        assertEquals(i + 1, checkpointCoordinator.getNumberOfPendingCheckpoints());        assertTrue(checkpointCoordinator.isCurrentPeriodicTriggerAvailable());    }            checkpointCoordinator.triggerCheckpoint(System.currentTimeMillis(), false);    assertFalse(checkpointCoordinator.isCurrentPeriodicTriggerAvailable());    assertEquals(maxConcurrentCheckpoints, checkpointCoordinator.getNumberOfPendingCheckpoints());    checkpointCoordinator.abortPendingCheckpoints(new CheckpointException(CheckpointFailureReason.JOB_FAILOVER_REGION));        assertTrue(checkpointCoordinator.isCurrentPeriodicTriggerAvailable());    assertEquals(0, checkpointCoordinator.getNumberOfPendingCheckpoints());}
0
public SUP<FIELDSET<T>> sup()
{    closeAttrs();    return sup_(this, true);}
0
public String toString()
{    return "NestedPdx [myString1=" + myString1 + ", myLong=" + myLong + ", myHashMap=" + myHashMap + ", myString2=" + myString2 + ", myFloat=" + myFloat + "]";}
0
public Y getY()
{    return y;}
0
public java.util.List<java.lang.String> getNicknames()
{    return nicknames;}
0
public boolean equals(Object obj)
{    if (sameClassAs(obj) == false) {        return false;    }    IntRangeSlowRangeQuery that = (IntRangeSlowRangeQuery) obj;    return Objects.equals(field, that.field) && Arrays.equals(min, that.min) && Arrays.equals(max, that.max);}
0
public long getTransactionCapacity()
{    return transCapacity;}
0
public XShapeCollection<Point> buildS4J()
{            List<Point> shapes = new ArrayList<>(coordinates.size());    for (Coordinate coord : coordinates) {        shapes.add(SPATIAL_CONTEXT.makePoint(coord.x, coord.y));    }    XShapeCollection<Point> multiPoints = new XShapeCollection<>(shapes, SPATIAL_CONTEXT);    multiPoints.setPointsOnly(true);    return multiPoints;}
0
protected Object processResponse(Message msg) throws Exception
{    Part part = msg.getPart(0);    final int msgType = msg.getMessageType();    if (msgType == MessageType.RESPONSE) {        return Integer.valueOf(part.getInt());    } else {        if (msgType == MessageType.EXCEPTION) {            String s = "While performing a remote " + "getPdxIdForEnum";            throw new ServerOperationException(s, (Throwable) part.getObject());                        } else if (isErrorResponse(msgType)) {            throw new ServerOperationException(part.getString());        } else {            throw new InternalGemFireError("Unexpected message type " + MessageType.getString(msgType));        }    }}
0
public boolean inStates(State... states)
{    for (State s : states) {        if (s.equals(this.state))            return true;    }    return false;}
0
public void testE164NumberWithPlusSignIsInvalid() throws Exception
{    final String phoneNumber = "+34 600 00 00 00";    final SMSMessage m = new SMSMessage("Hello world!", phoneNumber);    final Set<ConstraintViolation<SMSMessage>> constraintViolations = validator.validate(m);    Assert.isTrue(1 == constraintViolations.size());}
0
private static String encodeName(String s) throws UnsupportedEncodingException
{    return URLEncoder.encode(s, "UTF-8");}
0
public boolean equals(Object o)
{    return delegate.equals(o);}
0
private static final void loadRealmPackage(ClassLoader loader) throws Exception
{    final String basePackage = "org.apache.catalina.realm.";    loader.loadClass(basePackage + "LockOutRealm$LockRecord");}
0
public void setAccountName(String accountName)
{    this.accountName = accountName;}
0
public static ConfigDef configDef()
{    return CONFIG;}
0
public void testExpire() throws Exception
{    stompConnection.connect("system", "manager");    HashMap<String, String> headers = new HashMap<String, String>();    long timestamp = System.currentTimeMillis() - 100;    headers.put(Stomp.Headers.Message.EXPIRATION_TIME, String.valueOf(timestamp));    headers.put(Stomp.Headers.Send.PERSISTENT, "true");    stompConnection.send("/queue/" + getQueueName(), "msg", null, headers);    stompConnection.subscribe("/queue/ActiveMQ.DLQ");    StompFrame stompMessage = stompConnection.receive(35000);    assertNotNull(stompMessage);    assertEquals(stompMessage.getHeaders().get(Stomp.Headers.Message.ORIGINAL_DESTINATION), "/queue/" + getQueueName());}
0
public void setVersionXml(String xml)
{    versionXml = xml;}
0
private void cancelCubingJobInner(CubingJob cubingJob) throws IOException
{    CubeInstance cubeInstance = getCubeManager().getCube(CubingExecutableUtil.getCubeName(cubingJob.getParams()));        final String segmentIds = CubingExecutableUtil.getSegmentId(cubingJob.getParams());    if (!StringUtils.isEmpty(segmentIds)) {        for (String segmentId : StringUtils.split(segmentIds)) {            final CubeSegment segment = cubeInstance.getSegmentById(segmentId);            if (segment != null && (segment.getStatus() == SegmentStatusEnum.NEW || segment.getTSRange().end.v == 0)) {                                getCubeManager().updateCubeDropSegments(cubeInstance, segment);            }        }    }    getExecutableManager().discardJob(cubingJob.getId());}
0
private String combineStrings(String... strings)
{    StringBuilder stringBuilder = new StringBuilder();    for (String string : strings) {        stringBuilder.append(string).append(" ");    }    return stringBuilder.toString().trim();}
0
protected boolean supportsMinVersion()
{    return true;}
0
public void publish(final CoreDescriptor cd, final Replica.State state) throws Exception
{    publish(cd, state, true, false);}
0
public final int hashCode()
{    int hash = classHash();    hash = 31 * hash + scoreMode.hashCode();    hash = 31 * hash + field.hashCode();    hash = 31 * hash + originalQuery.hashCode();    hash = 31 * hash + sortedPackedPointsHashCode;    hash = 31 * hash + bytesPerDim;    return hash;}
0
protected void assertTextMessagesEqual(Message[] firstSet, Message[] secondSet) throws JMSException
{    assertTextMessagesEqual("", firstSet, secondSet);}
0
public void validate(UserEntity userEntity, String key) throws AmbariException
{    List<UserAuthenticationEntity> authenticationEntities = userEntity.getAuthenticationEntities();        for (UserAuthenticationEntity entity : authenticationEntities) {        if (entity.getAuthenticationType() == UserAuthenticationType.LOCAL) {            throw new AmbariException("The authentication type already exists for this user");        }    }}
0
public boolean equals(Object o)
{    if (this == o) {        return true;    }    if (o == null || getClass() != o.getClass()) {        return false;    }    GetSnapshotLifecycleStatsResponse other = (GetSnapshotLifecycleStatsResponse) o;    return Objects.equals(this.stats, other.stats);}
0
private void accountForTimeSpentWorking(long nanosSpentWorking, long nanosSpentSleeping)
{    this.samplerStats.tookSample(nanosSpentWorking, getStatisticsManager().getStatisticsCount(), nanosSpentSleeping);}
0
public static long timestampFloor(TimeUnitRange range, long ts)
{    return timestampFloor(range, ts, UTC_ZONE);}
0
private LuceneIndexInfo getMockLuceneInfo(final String index1)
{    LuceneIndexInfo mockInfo = mock(LuceneIndexInfo.class);    doReturn(index1).when(mockInfo).getIndexName();    doReturn("/region").when(mockInfo).getRegionPath();    return mockInfo;}
0
public void describeTo(Description description)
{    description.appendText(String.format("log message containing message [%s]", substring));}
0
private TypeInformation<?> convertObjectArray()
{    nextToken(TokenType.BEGIN);    nextToken(TokenType.LITERAL);    final TypeInformation<?> elementTypeInfo = convertType();    nextToken(TokenType.END);    return Types.OBJECT_ARRAY(elementTypeInfo);}
0
public static String getExprListString(Collection<? extends ExprNodeDesc> exprs, boolean userLevelExplain)
{    return getExprListString(exprs, userLevelExplain, false);}
0
public void testFilterExprOrExpr() throws HiveException
{    VectorizedRowBatch batch1 = getBatchThreeBooleanCols();    VectorizedRowBatch batch2 = getBatchThreeBooleanCols();    SelectColumnIsTrue expr1 = new SelectColumnIsTrue(0);    SelectColumnIsFalse expr2 = new SelectColumnIsFalse(1);    FilterExprOrExpr orExpr = new FilterExprOrExpr();    orExpr.setChildExpressions(new VectorExpression[] { expr1, expr2 });    orExpr.evaluate(batch1);    orExpr.evaluate(batch2);    assertEquals(batch1.size, batch2.size);    for (int j = 0; j < batch1.size; j++) {        assertEquals(batch1.selected[j], batch2.selected[j]);        int i = j;        assertEquals((((LongColumnVector) batch1.cols[0]).vector[i]), (((LongColumnVector) batch2.cols[0]).vector[i]));    }    assertEquals(5, batch1.size);    assertEquals(0, batch1.selected[0]);    assertEquals(2, batch1.selected[1]);    assertEquals(3, batch1.selected[2]);    assertEquals(4, batch1.selected[3]);    assertEquals(7, batch1.selected[4]);            orExpr.evaluate(batch1);    assertEquals(5, batch1.size);    assertEquals(0, batch1.selected[0]);    assertEquals(2, batch1.selected[1]);    assertEquals(3, batch1.selected[2]);    assertEquals(4, batch1.selected[3]);    assertEquals(7, batch1.selected[4]);}
0
public void flatMap(Edge<K, EV> edge, Collector<Edge<K, EV>> out) throws Exception
{    out.collect(edge);    output.setFields(edge.f1, edge.f0, edge.f2);    out.collect(output);}
0
public void testIllegalSettingsMissingProject()
{    Settings nodeSettings = Settings.builder().put(GceMetadataService.GCE_HOST.getKey(), "http://internal").putList(GceInstancesServiceImpl.ZONE_SETTING.getKey(), "us-central1-a", "us-central1-b").build();    mock = new GceInstancesServiceMock(nodeSettings);    try {        buildDynamicNodes(mock, nodeSettings);        fail("We expect an IllegalArgumentException for incomplete settings");    } catch (IllegalArgumentException expected) {        assertThat(expected.getMessage(), containsString("one or more gce discovery settings are missing."));    }}
0
public static void printHelpForCancel(Collection<CustomCommandLine<?>> customCommandLines)
{    HelpFormatter formatter = new HelpFormatter();    formatter.setLeftPadding(5);    formatter.setWidth(80);    System.out.println("\nAction \"cancel\" cancels a running program.");    System.out.println("\n  Syntax: cancel [OPTIONS] <Job ID>");    formatter.setSyntaxPrefix("  \"cancel\" action options:");    formatter.printHelp(" ", getCancelOptionsWithoutDeprecatedOptions(new Options()));    printCustomCliOptions(customCommandLines, formatter, false);    System.out.println();}
0
private static void printRootIndexPage(Path rootDir, List<PainlessContextInfo> contextInfos, Set<PainlessContextInfo> isSpecialized) throws IOException
{    Path rootIndexPath = rootDir.resolve("index.asciidoc");    try (PrintStream rootIndexStream = new PrintStream(Files.newOutputStream(rootIndexPath, StandardOpenOption.CREATE_NEW, StandardOpenOption.WRITE), false, StandardCharsets.UTF_8.name())) {        printAutomatedMessage(rootIndexStream);        rootIndexStream.println("[cols=\"<3,^3,^3\"]");        rootIndexStream.println("|====");        for (PainlessContextInfo contextInfo : contextInfos) {            String contextName = getContextName(contextInfo);            String contextHeader = getContextHeader(contextInfo);            rootIndexStream.print("|" + contextName + " ");            rootIndexStream.print("| <<" + SHARED_HEADER + ", " + SHARED_NAME + " API>> ");            if (isSpecialized.contains(contextInfo)) {                rootIndexStream.println("| <<" + contextHeader + ", Specialized API>>");            } else {                rootIndexStream.println("| ");            }        }        rootIndexStream.println("|====");        rootIndexStream.println();        rootIndexStream.println("include::" + SHARED_HEADER + "/index.asciidoc[]");        for (PainlessContextInfo contextInfo : contextInfos) {            if (isSpecialized.contains(contextInfo)) {                rootIndexStream.println("include::" + getContextHeader(contextInfo) + "/index.asciidoc[]");            }        }    }}
0
public void initialImagePut_givenPutIfAbsentReturningNonTombstone_callsUpdateSizeOnRemove() throws RegionClearedException
{    ConcurrentMapWithReusableEntries map = mock(ConcurrentMapWithReusableEntries.class);    RegionEntry entry = mock(RegionEntry.class);    when(entry.isTombstone()).thenReturn(false);    when(entry.isDestroyedOrRemoved()).thenReturn(false);    when(entry.initialImagePut(any(), anyLong(), any(), anyBoolean(), anyBoolean())).thenReturn(true);    VersionStamp versionStamp = mock(VersionStamp.class);    when(entry.getVersionStamp()).thenReturn(versionStamp);    when(versionStamp.asVersionTag()).thenReturn(mock(VersionTag.class));    when(map.putIfAbsent(eq(KEY), any())).thenReturn(entry).thenReturn(null);    TestableAbstractRegionMap arm = new TestableAbstractRegionMap(false, map, null);    when(arm._getOwner().getConcurrencyChecksEnabled()).thenReturn(true);    when(arm._getOwner().getServerProxy()).thenReturn(mock(ServerRegionProxy.class));    VersionTag versionTag = mock(VersionTag.class);    when(versionTag.getMemberID()).thenReturn(mock(VersionSource.class));    arm.initialImagePut(KEY, 0, Token.TOMBSTONE, false, false, versionTag, null, false);    verify(arm._getOwner(), times(1)).updateSizeOnRemove(eq(KEY), anyInt());}
0
private static boolean isRunningOrFinal(SqlKind kind)
{    return kind == SqlKind.RUNNING || kind == SqlKind.FINAL;}
0
public static long getTotalRequestsCount(RegionLoad rl)
{    if (rl == null) {        return 0;    }    return rl.getReadRequestsCount() + rl.getWriteRequestsCount();}
0
public TypedFieldId getFieldId()
{    return fieldId;}
0
private void cancelTasks(int cancelCount)
{    int cancelled = state.getCancelledTaskCount();    int cancellable = cancelCount - cancelled;    int n = cancellable - quantity;        if (n <= 0) {        return;    }    for (Task task : state.getStartingTasks()) {        state.cancel(task);        if (--n == 0) {            return;        }    }    for (Task task : state.getActiveTasks()) {        state.cancel(task);        if (--n == 0) {            return;        }    }            assert false;}
1
public void setTargetRunning(Boolean targetRunning)
{    this.targetRunning = targetRunning;}
0
public void writeField(FieldInfo info, IndexableField field) throws IOException
{    assert docStatus == Status.STARTED;    in.writeField(info, field);}
0
protected static Response errorEntity(String name)
{    HashMap<String, Object> response = new HashMap<String, Object>();    response.put("message", String.format(message, name));    response.put("trace", null);    response.put("status", STATUS);    return Response.status(STATUS).entity(new JSONObject(response)).type(MediaType.APPLICATION_JSON).build();}
0
 String getEndpointId()
{    return Dispatcher.DISPATCHER_NAME;}
0
private static String threadProcessAndHost()
{    return Thread.currentThread().getId() + "-" + processAndHost();}
0
public boolean isFailed()
{    return (saslState == SaslState.FAILED);}
0
public SSLConfigBuilder initializeSSLContext(boolean initializeSSLContext)
{    this.initializeSSLContext = initializeSSLContext;    return this;}
0
public LoggingLevel getLoggingLevel()
{    return loggingLevel;}
0
public OperatorType getType()
{    return TOPNKEY;}
0
public long getValue()
{    CounterProtoOrBuilder p = viaProto ? proto : builder;    return (p.getValue());}
0
public void grant_role(String role_name, String principal_name, PrincipalType principal_type, String grantor, PrincipalType grantorType, boolean grant_option, org.apache.thrift.async.AsyncMethodCallback resultHandler) throws org.apache.thrift.TException
{    checkReady();    grant_role_call method_call = new grant_role_call(role_name, principal_name, principal_type, grantor, grantorType, grant_option, resultHandler, this, ___protocolFactory, ___transport);    this.___currentMethod = method_call;    ___manager.call(method_call);}
0
public DiskId getDiskId()
{    return this.id;}
0
public void shutdownThreadPool()
{    terminate(threadPool);}
0
public void setLoginTimeout(int seconds) throws SQLException
{    throw new UnsupportedOperationException();}
0
public void testRemove() throws InterruptedException
{    MockEndpoint out = getMockEndpoint("mock:removed");    out.expectedMessageCount(1);    verify(map).addEntryListener(argument.capture(), eq(true));    EntryEvent<Object, Object> event = new EntryEvent<>("foo", null, EntryEventType.REMOVED.getType(), "4711", "my-foo");    argument.getValue().entryRemoved(event);    assertMockEndpointsSatisfied(5000, TimeUnit.MILLISECONDS);    this.checkHeaders(out.getExchanges().get(0).getIn().getHeaders(), HazelcastConstants.REMOVED);}
0
public String getShortName()
{    return this.getClass().getSimpleName();}
0
public Optional<EvaluationMetricResult> getResult()
{    return Optional.ofNullable(result);}
0
private Value decodeStringLax()
{    int index = __index;    char currentChar = charArray[__index];    final int startIndex = __index;    boolean encoded = false;    char[] charArray = this.charArray;    for (; index < charArray.length; index++) {        currentChar = charArray[index];        if (isDelimiter(currentChar))            break;        else if (currentChar == '\\')            break;    }    Value value = this.extractLaxString(startIndex, index, encoded, defaultCheckDates);    __index = index;    return value;}
0
public void testLUCENE8451()
{        final List<GeoPoint> points = new ArrayList<>();    points.add(new GeoPoint(PlanetModel.SPHERE, Geo3DUtil.fromDegrees(-24.093993739745027), Geo3DUtil.fromDegrees(-2.5185339401969213)));    points.add(new GeoPoint(PlanetModel.SPHERE, Geo3DUtil.fromDegrees(8.828539494442529E-27), Geo3DUtil.fromDegrees(0.0)));    points.add(new GeoPoint(PlanetModel.SPHERE, Geo3DUtil.fromDegrees(-8.321407453133E-11), Geo3DUtil.fromDegrees(5.495998489568957E-11)));    points.add(new GeoPoint(PlanetModel.SPHERE, Geo3DUtil.fromDegrees(1.0260761462208114E-10), Geo3DUtil.fromDegrees(2.7174659198424288E-11)));    points.add(new GeoPoint(PlanetModel.SPHERE, Geo3DUtil.fromDegrees(16.934529875343244), Geo3DUtil.fromDegrees(88.32137548549387)));    points.add(new GeoPoint(PlanetModel.SPHERE, Geo3DUtil.fromDegrees(39.919704493657484), Geo3DUtil.fromDegrees(-87.97237709688223)));    points.add(new GeoPoint(PlanetModel.SPHERE, Geo3DUtil.fromDegrees(34.91204903885665), Geo3DUtil.fromDegrees(-88.0876897472551)));    final GeoPolygonFactory.PolygonDescription description = new GeoPolygonFactory.PolygonDescription(points);    final GeoPolygon polygon = GeoPolygonFactory.makeGeoPolygon(PlanetModel.SPHERE, description);    final GeoPolygon largePolygon = GeoPolygonFactory.makeLargeGeoPolygon(PlanetModel.SPHERE, Collections.singletonList(description));        final GeoPoint point = new GeoPoint(PlanetModel.SPHERE, Geo3DUtil.fromDegrees(-5.021400461974724E-11), Geo3DUtil.fromDegrees(179.99999999999983));    assertTrue(polygon.isWithin(point) == largePolygon.isWithin(point));}
0
public GroupKey groupKey(Iterable<? extends RexNode> nodes, Iterable<? extends Iterable<? extends RexNode>> nodeLists)
{    return groupKey_(nodes, nodeLists);}
0
private void generateInit()
{    if (ctxt.isTagFile()) {        out.printil("private void _jspInit(javax.servlet.ServletConfig config) {");    } else {        out.printil("public void _jspInit() {");    }    out.pushIndent();    if (isPoolingEnabled) {        for (int i = 0; i < tagHandlerPoolNames.size(); i++) {            out.printin(tagHandlerPoolNames.elementAt(i));            out.print(" = org.apache.jasper.runtime.TagHandlerPool.getTagHandlerPool(");            if (ctxt.isTagFile()) {                out.print("config");            } else {                out.print("getServletConfig()");            }            out.println(");");        }    }        if (ctxt.isTagFile()) {        out.printin(VAR_EXPRESSIONFACTORY);        out.println(" = _jspxFactory.getJspApplicationContext(config.getServletContext()).getExpressionFactory();");        out.printin(VAR_INSTANCEMANAGER);        out.println(" = org.apache.jasper.runtime.InstanceManagerFactory.getInstanceManager(config);");    }    out.popIndent();    out.printil("}");    out.println();}
0
public void copyFrom(Directory from, String src, String dest, IOContext context) throws IOException
{    final long l = from.fileLength(src);    final AtomicBoolean copies = new AtomicBoolean(false);                in.copyFrom(new FilterDirectory(from) {        @Override        public IndexInput openInput(String name, IOContext context) throws IOException {            index.addFileDetail(dest, l, false);            copies.set(true);            final IndexInput input = in.openInput(name, context);            return new IndexInput("StatsDirectoryWrapper(" + input.toString() + ")") {                @Override                public void close() throws IOException {                    input.close();                }                @Override                public long getFilePointer() {                    throw new UnsupportedOperationException("only straight copies are supported");                }                @Override                public void seek(long pos) throws IOException {                    throw new UnsupportedOperationException("seeks are not supported");                }                @Override                public long length() {                    return input.length();                }                @Override                public IndexInput slice(String sliceDescription, long offset, long length) throws IOException {                    throw new UnsupportedOperationException("slices are not supported");                }                @Override                public byte readByte() throws IOException {                    throw new UnsupportedOperationException("use a buffer if you wanna perform well");                }                @Override                public void readBytes(byte[] b, int offset, int len) throws IOException {                                        input.readBytes(b, offset, len);                    index.addRecoveredBytesToFile(dest, len);                }            };        }    }, src, dest, context);    if (copies.get() == false) {                index.addFileDetail(dest, l, true);    } else {        assert index.getFileDetails(dest) != null : "File [" + dest + "] has no file details";        assert index.getFileDetails(dest).recovered() == l : index.getFileDetails(dest).toString();    }}
0
public void setCacheKeysToSave(int keyCacheKeysToSave, int rowCacheKeysToSave, int counterCacheKeysToSave)
{    try {        String keyCachePath = "org.apache.cassandra.db:type=Caches";        CacheServiceMBean cacheMBean = JMX.newMBeanProxy(mbeanServerConn, new ObjectName(keyCachePath), CacheServiceMBean.class);        cacheMBean.setKeyCacheKeysToSave(keyCacheKeysToSave);        cacheMBean.setRowCacheKeysToSave(rowCacheKeysToSave);        cacheMBean.setCounterCacheKeysToSave(counterCacheKeysToSave);    } catch (MalformedObjectNameException e) {        throw new RuntimeException(e);    }}
0
protected int doPoll() throws Exception
{    class ExcludePathFilter implements PathFilter {        @Override        public boolean accept(Path path) {            return !(path.toString().endsWith(config.getOpenedSuffix()) || path.toString().endsWith(config.getReadSuffix()));        }    }    int numMessages = 0;    HdfsInfo info = setupHdfs(false);    FileStatus[] fileStatuses;    if (info.getFileSystem().isFile(info.getPath())) {        fileStatuses = info.getFileSystem().globStatus(info.getPath());    } else {        Path pattern = info.getPath().suffix("/" + this.config.getPattern());        fileStatuses = info.getFileSystem().globStatus(pattern, new ExcludePathFilter());    }    for (FileStatus status : fileStatuses) {        if (normalFileIsDirectoryNoSuccessFile(status, info)) {            continue;        }        if (config.getOwner() != null) {                        if (!config.getOwner().equals(status.getOwner())) {                if (log.isDebugEnabled()) {                                    }                continue;            }        }        try {            this.rwlock.writeLock().lock();            this.istream = HdfsInputStream.createInputStream(status.getPath().toString(), this.config);            if (!this.istream.isOpened()) {                if (log.isDebugEnabled()) {                                    }                continue;            }        } finally {            this.rwlock.writeLock().unlock();        }        try {            Holder<Object> key = new Holder<>();            Holder<Object> value = new Holder<>();            while (this.istream.next(key, value) >= 0) {                Exchange exchange = this.getEndpoint().createExchange();                Message message = new DefaultMessage(this.getEndpoint().getCamelContext());                String fileName = StringUtils.substringAfterLast(status.getPath().toString(), "/");                message.setHeader(Exchange.FILE_NAME, fileName);                if (key.value != null) {                    message.setHeader(HdfsHeader.KEY.name(), key.value);                }                message.setBody(value.value);                exchange.setIn(message);                                try {                    processor.process(exchange);                } catch (Exception e) {                    exchange.setException(e);                }                                if (exchange.getException() != null) {                    getExceptionHandler().handleException(exchange.getException());                }                numMessages++;            }        } finally {            IOHelper.close(istream, "input stream", log);        }    }    return numMessages;}
1
public QueueSender createSender(Queue queue) throws JMSException
{    checkClosed();    if (queue instanceof CustomDestination) {        CustomDestination customDestination = (CustomDestination) queue;        return customDestination.createSender(this);    }    int timeSendOut = connection.getSendTimeout();    return new ActiveMQQueueSender(this, ActiveMQMessageTransformation.transformDestination(queue), timeSendOut);}
0
public int getNumFeatures()
{    return numFeatures;}
0
public SUB<LABEL<T>> sub()
{    closeAttrs();    return sub_(this, true);}
0
public void teardownTest()
{    deleteTestGroup();    deleteTestUser();}
0
public synchronized boolean getUnmanagedAM()
{    ApplicationSubmissionContextProtoOrBuilder p = viaProto ? proto : builder;    return p.getUnmanagedAm();}
0
public void testSingleFailure() throws IOException
{    final MockFlowFile flowFile = runner.enqueue("hello world");    when(mockLease.complete()).thenReturn(createFailurePublishResult(flowFile));    runner.run();    runner.assertAllFlowFilesTransferred(PublishKafka_0_11.REL_FAILURE, 1);    verify(mockLease, times(1)).publish(any(FlowFile.class), any(InputStream.class), eq(null), eq(null), eq(TOPIC_NAME));    verify(mockLease, times(1)).complete();    verify(mockLease, times(1)).close();}
0
public void testCreate() throws Exception
{        Assert.assertEquals(StringSummaryAggregator.class, SummaryAggregatorFactory.create(String.class).getClass());    Assert.assertEquals(ShortSummaryAggregator.class, SummaryAggregatorFactory.create(Short.class).getClass());    Assert.assertEquals(IntegerSummaryAggregator.class, SummaryAggregatorFactory.create(Integer.class).getClass());    Assert.assertEquals(LongSummaryAggregator.class, SummaryAggregatorFactory.create(Long.class).getClass());    Assert.assertEquals(FloatSummaryAggregator.class, SummaryAggregatorFactory.create(Float.class).getClass());    Assert.assertEquals(DoubleSummaryAggregator.class, SummaryAggregatorFactory.create(Double.class).getClass());    Assert.assertEquals(BooleanSummaryAggregator.class, SummaryAggregatorFactory.create(Boolean.class).getClass());        Assert.assertEquals(ValueSummaryAggregator.StringValueSummaryAggregator.class, SummaryAggregatorFactory.create(StringValue.class).getClass());    Assert.assertEquals(ValueSummaryAggregator.ShortValueSummaryAggregator.class, SummaryAggregatorFactory.create(ShortValue.class).getClass());    Assert.assertEquals(ValueSummaryAggregator.IntegerValueSummaryAggregator.class, SummaryAggregatorFactory.create(IntValue.class).getClass());    Assert.assertEquals(ValueSummaryAggregator.LongValueSummaryAggregator.class, SummaryAggregatorFactory.create(LongValue.class).getClass());    Assert.assertEquals(ValueSummaryAggregator.FloatValueSummaryAggregator.class, SummaryAggregatorFactory.create(FloatValue.class).getClass());    Assert.assertEquals(ValueSummaryAggregator.DoubleValueSummaryAggregator.class, SummaryAggregatorFactory.create(DoubleValue.class).getClass());    Assert.assertEquals(ValueSummaryAggregator.BooleanValueSummaryAggregator.class, SummaryAggregatorFactory.create(BooleanValue.class).getClass());        Assert.assertEquals(ObjectSummaryAggregator.class, SummaryAggregatorFactory.create(Object.class).getClass());    Assert.assertEquals(ObjectSummaryAggregator.class, SummaryAggregatorFactory.create(List.class).getClass());}
0
private void writeObject(java.io.ObjectOutputStream out) throws java.io.IOException
{    try {        write(new org.apache.thrift.protocol.TCompactProtocol(new org.apache.thrift.transport.TIOStreamTransport(out)));    } catch (org.apache.thrift.TException te) {        throw new java.io.IOException(te);    }}
0
public boolean getClassDebugInfo()
{    return classDebugInfo;}
0
public void destroyInTokenModeWithConcurrentChangeFromNullToRemovePhase2RetriesAndDoesDestroy() throws RegionClearedException
{    CustomEntryConcurrentHashMap<Object, EvictableEntry> map = mock(CustomEntryConcurrentHashMap.class);    EvictableEntry entry = mock(EvictableEntry.class);    when(entry.isRemovedPhase2()).thenReturn(true);    when(entry.destroy(any(), any(), anyBoolean(), anyBoolean(), any(), anyBoolean(), anyBoolean())).thenReturn(true);    when(map.get(KEY)).thenReturn(null);    when(map.putIfAbsent(eq(KEY), any())).thenReturn(entry).thenReturn(null);    final TestableVMLRURegionMap arm = new TestableVMLRURegionMap(true, map);    final EntryEventImpl event = createEventForDestroy(arm._getOwner());    final Object expectedOldValue = null;    final boolean inTokenMode = true;    final boolean duringRI = false;    final boolean evict = false;    assertThat(arm.destroy(event, inTokenMode, duringRI, false, evict, expectedOldValue, false)).isTrue();    verify(map).remove(eq(KEY), eq(entry));    verify(map, times(2)).putIfAbsent(eq(KEY), any());    verify(entry, never()).destroy(eq(arm._getOwner()), eq(event), eq(false), anyBoolean(), eq(expectedOldValue), anyBoolean(), anyBoolean());    boolean invokeCallbacks = true;    verify(arm._getOwner(), times(1)).basicDestroyPart2(any(), eq(event), eq(inTokenMode), eq(false), eq(duringRI), eq(invokeCallbacks));    verify(arm._getOwner(), times(1)).basicDestroyPart3(any(), eq(event), eq(inTokenMode), eq(duringRI), eq(invokeCallbacks), eq(expectedOldValue));}
0
public void testGetType01()
{    BeanNameELResolver resolver = createBeanNameELResolver();    resolver.getType(null, new Object(), new Object());}
0
public void setProperties(Properties properties)
{    buildFromProperties(properties);}
0
public int compare(Map.Entry<String, DoubleColumnStatsData> o1, Map.Entry<String, DoubleColumnStatsData> o2)
{    return Double.compare(o1.getValue().getLowValue(), o2.getValue().getLowValue());}
0
public Map<String, String> getParams()
{    return params;}
0
protected ReloadResult readShardResult(StreamInput in) throws IOException
{    return new ReloadResult(in);}
0
public DiskRangeList getTail()
{    return tail;}
0
public String toString()
{    return snapshot.toString();}
0
public void primaryKeyAndForeignKey() throws TException
{    Table parentTable = testTables[2];    Table table = testTables[3];    String constraintName = "othercatfk";        List<SQLPrimaryKey> pk = new SQLPrimaryKeyBuilder().onTable(parentTable).addColumn("test_col1").build(conf);    client.addPrimaryKey(pk);    List<SQLForeignKey> fk = new SQLForeignKeyBuilder().fromPrimaryKey(pk).onTable(table).addColumn("test_col1").setConstraintName(constraintName).build(conf);    client.addForeignKey(fk);    PrimaryKeysRequest pkRqst = new PrimaryKeysRequest(parentTable.getDbName(), parentTable.getTableName());    pkRqst.setCatName(parentTable.getCatName());    List<SQLPrimaryKey> pkFetched = client.getPrimaryKeys(pkRqst);    Assert.assertEquals(1, pkFetched.size());    Assert.assertEquals(expectedCatalog(), pkFetched.get(0).getCatName());    Assert.assertEquals(parentTable.getDbName(), pkFetched.get(0).getTable_db());    Assert.assertEquals(parentTable.getTableName(), pkFetched.get(0).getTable_name());    Assert.assertEquals("test_col1", pkFetched.get(0).getColumn_name());    Assert.assertEquals(1, pkFetched.get(0).getKey_seq());    Assert.assertTrue(pkFetched.get(0).isEnable_cstr());    Assert.assertFalse(pkFetched.get(0).isValidate_cstr());    Assert.assertFalse(pkFetched.get(0).isRely_cstr());    Assert.assertEquals(parentTable.getCatName(), pkFetched.get(0).getCatName());    ForeignKeysRequest rqst = new ForeignKeysRequest(parentTable.getDbName(), parentTable.getTableName(), table.getDbName(), table.getTableName());    rqst.setCatName(table.getCatName());    List<SQLForeignKey> fetched = client.getForeignKeys(rqst);    Assert.assertEquals(1, fetched.size());    Assert.assertEquals(table.getDbName(), fetched.get(0).getFktable_db());    Assert.assertEquals(table.getTableName(), fetched.get(0).getFktable_name());    Assert.assertEquals(expectedCatalog(), fetched.get(0).getCatName());    Assert.assertEquals("test_col1", fetched.get(0).getFkcolumn_name());    Assert.assertEquals(parentTable.getDbName(), fetched.get(0).getPktable_db());    Assert.assertEquals(parentTable.getTableName(), fetched.get(0).getPktable_name());    Assert.assertEquals("test_col1", fetched.get(0).getFkcolumn_name());    Assert.assertEquals(1, fetched.get(0).getKey_seq());    Assert.assertEquals(parentTable.getTableName() + "_primary_key", fetched.get(0).getPk_name());    Assert.assertEquals(constraintName, fetched.get(0).getFk_name());    String table0FkName = fetched.get(0).getFk_name();    Assert.assertTrue(fetched.get(0).isEnable_cstr());    Assert.assertFalse(fetched.get(0).isValidate_cstr());    Assert.assertFalse(fetched.get(0).isRely_cstr());    Assert.assertEquals(table.getCatName(), fetched.get(0).getCatName());        client.dropConstraint(table.getDbName(), table.getTableName(), table0FkName);    rqst = new ForeignKeysRequest(parentTable.getDbName(), parentTable.getTableName(), table.getDbName(), table.getTableName());    rqst.setCatName(table.getCatName());    fetched = client.getForeignKeys(rqst);    Assert.assertTrue(fetched.isEmpty());}
0
public void transition(RMAppAttemptImpl appAttempt, RMAppAttemptEvent event)
{    RMAppAttemptContainerFinishedEvent containerFinishedEvent = (RMAppAttemptContainerFinishedEvent) event;    ContainerStatus containerStatus = containerFinishedEvent.getContainerStatus();        if (appAttempt.masterContainer.getId().equals(containerStatus.getContainerId())) {        appAttempt.amContainerFinished(appAttempt, containerFinishedEvent);        if (appAttempt.targetedFinalState.equals(RMAppAttemptState.FAILED) || appAttempt.targetedFinalState.equals(RMAppAttemptState.KILLED)) {                        return;        }                        appAttempt.rememberTargetTransitions(event, new AMFinishedAfterFinalSavingTransition(appAttempt.eventCausingFinalSaving), RMAppAttemptState.FINISHED);        return;    }        addJustFinishedContainer(appAttempt, containerFinishedEvent);}
0
public MapRDBCost getPluginCostModel()
{    return pluginCostModel;}
0
private static IndexWriterConfig newIndexWriterConfig()
{    return new IndexWriterConfig(null).setSoftDeletesField(Lucene.SOFT_DELETES_FIELD).setCommitOnClose(false).setMergePolicy(NoMergePolicy.INSTANCE);}
0
public void setPositionIsSet(boolean value)
{    __isset_bitfield = EncodingUtils.setBit(__isset_bitfield, __POSITION_ISSET_ID, value);}
0
public DataStructure createObject()
{    return new ActiveMQQueue();}
0
private static final void sortByMortonWithReset(Node start)
{    Node next = start;    do {        next.previousZ = next.previous;        next.nextZ = next.next;        next = next.next;    } while (next != start);    sortByMorton(start);}
0
public void unsetSuccess()
{    this.success = null;}
0
public static int getPrecision(MajorType majorType)
{    if (majorType.hasPrecision()) {        return majorType.getPrecision();    }    return isScalarStringType(majorType) ? MAX_VARCHAR_LENGTH : UNDEFINED;}
0
 boolean areSerializedFieldsEqual(Filter o)
{    if (o == this)        return true;    if (!(o instanceof SingleColumnValueFilter))        return false;    SingleColumnValueFilter other = (SingleColumnValueFilter) o;    return Bytes.equals(this.getFamily(), other.getFamily()) && Bytes.equals(this.getQualifier(), other.getQualifier()) && this.op.equals(other.op) && this.getComparator().areSerializedFieldsEqual(other.getComparator()) && this.getFilterIfMissing() == other.getFilterIfMissing() && this.getLatestVersionOnly() == other.getLatestVersionOnly();}
0
public ResourceScheduler createScheduler()
{    CapacityScheduler cs = new MyScheduler();    cs.setConf(conf);    return cs;}
0
protected boolean rethrowUnhandledExceptions()
{    return false;}
0
public void copyBytes(DataInput input, long numBytes) throws IOException
{    delegate.copyBytes(input, numBytes);}
0
public void testGetIndexMappingsIsFiltered()
{    assertAcked(client().admin().indices().prepareCreate("test").addMapping("type1", "field1", "type=text", "field2", "type=text"));    client().prepareIndex("test", "type1", "1").setSource("field1", "value1").setRefreshPolicy(IMMEDIATE).get();    client().prepareIndex("test", "type1", "2").setSource("field2", "value2").setRefreshPolicy(IMMEDIATE).get();    {        GetIndexResponse getIndexResponse = client().filterWithHeader(Collections.singletonMap(BASIC_AUTH_HEADER, basicAuthHeaderValue("user1", USERS_PASSWD))).admin().indices().prepareGetIndex().setIndices("test").get();        assertExpectedFields(getIndexResponse.getMappings(), "field1");    }    {        GetIndexResponse getIndexResponse = client().filterWithHeader(Collections.singletonMap(BASIC_AUTH_HEADER, basicAuthHeaderValue("user2", USERS_PASSWD))).admin().indices().prepareGetIndex().setIndices("test").get();        assertExpectedFields(getIndexResponse.getMappings(), "field2");    }    {        GetIndexResponse getIndexResponse = client().filterWithHeader(Collections.singletonMap(BASIC_AUTH_HEADER, basicAuthHeaderValue("user3", USERS_PASSWD))).admin().indices().prepareGetIndex().setIndices("test").get();        assertExpectedFields(getIndexResponse.getMappings(), "field1");    }    {        GetIndexResponse getIndexResponse = client().filterWithHeader(Collections.singletonMap(BASIC_AUTH_HEADER, basicAuthHeaderValue("user4", USERS_PASSWD))).admin().indices().prepareGetIndex().setIndices("test").get();        assertExpectedFields(getIndexResponse.getMappings(), "field1", "field2");    }}
0
public static void addSSLSettingsForStore(Settings.Builder builder, String resourcePathToStore, String password, String prefix)
{    addSSLSettingsForStore(builder, prefix, resourcePathToStore, password, true);}
0
public void beforeCreate(EntryEvent<Object, Object> event) throws CacheWriterException
{    Object newValue = event.getNewValue();    if (newValue instanceof PdxType) {            } else {            }}
1
public java.lang.String toString()
{    java.lang.StringBuilder sb = new java.lang.StringBuilder("shutdown_args(");    boolean first = true;    sb.append(")");    return sb.toString();}
0
public void testTieBreakByDocID() throws Exception
{    Directory dir = newFSDirectory(createTempDir());    IndexWriterConfig iwc = newIndexWriterConfig();    IndexWriter w = new IndexWriter(dir, iwc);    Document doc = new Document();    doc.add(new IntPoint("int", 17));    for (int i = 0; i < 300000; i++) {        w.addDocument(doc);        if (random().nextInt(1000) == 17) {            w.commit();        }    }    IndexReader r = DirectoryReader.open(w);    for (LeafReaderContext ctx : r.leaves()) {        PointValues points = ctx.reader().getPointValues("int");        points.intersect(new IntersectVisitor() {            int lastDocID = -1;            @Override            public void visit(int docID) {                if (docID < lastDocID) {                    fail("docs out of order: docID=" + docID + " but lastDocID=" + lastDocID);                }                lastDocID = docID;            }            @Override            public void visit(int docID, byte[] packedValue) {                visit(docID);            }            @Override            public Relation compare(byte[] minPackedValue, byte[] maxPackedValue) {                if (random().nextBoolean()) {                    return Relation.CELL_CROSSES_QUERY;                } else {                    return Relation.CELL_INSIDE_QUERY;                }            }        });    }    r.close();    w.close();    dir.close();}
0
public static GotoStatement goto_(LabelTarget labelTarget, Expression expression, Type type)
{    throw Extensions.todo();}
0
public void setOrderDate(Date orderDate)
{    this.orderDate = orderDate;}
0
public boolean equals(Object other)
{    return super.equals(other) && Objects.equals(skipDuplicates, ((CompletionSuggestion) other).skipDuplicates);}
0
public CreateSnapshotRequestBuilder setSettings(String source, XContentType xContentType)
{    request.settings(source, xContentType);    return this;}
0
public void appendInteger(final long l, Map<String, String> attributeMap)
{    appendCell(Long.toString(l), NO_BGCOLOR, attributeMap);}
0
private SCMBlockLocationResponse submitRequest(SCMBlockLocationRequest req) throws IOException
{    try {        SCMBlockLocationResponse response = rpcProxy.send(NULL_RPC_CONTROLLER, req);        return response;    } catch (ServiceException e) {        throw ProtobufHelper.getRemoteException(e);    }}
0
public Object getResult(Exchange exchange)
{    return ExchangeHelper.isOutCapable(exchange) ? exchange.getOut().getBody() : exchange.getIn().getBody();}
0
public void setFieldValue(_Fields field, Object value)
{    switch(field) {        case SUCCESS:            if (value == null) {                unsetSuccess();            } else {                setSuccess((GetValidWriteIdsResponse) value);            }            break;        case O1:            if (value == null) {                unsetO1();            } else {                setO1((NoSuchTxnException) value);            }            break;        case O2:            if (value == null) {                unsetO2();            } else {                setO2((MetaException) value);            }            break;    }}
0
public void killActiveTopologies() throws Exception
{    List<TopologySummary> activeTopologies = getActive();    for (TopologySummary activeTopology : activeTopologies) {        killOrThrow(activeTopology.get_name());    }    AssertUtil.empty(getActive());}
0
public IPersistentMap getMap()
{    return map;}
0
public Set<SchemaField> getRequiredSchemaFields()
{    return requiredSchemaFields;}
0
public void setLargeRequestMaxBytes(int bytes)
{    zks.setLargeRequestMaxBytes(bytes);}
0
public boolean equals(Object obj)
{    if (obj == null) {        return false;    }    if (obj.getClass() != getClass()) {        return false;    }    Request other = (Request) obj;    return Objects.equals(index, other.index) && Objects.equals(currentStepKey, other.currentStepKey) && Objects.equals(nextStepKey, other.nextStepKey);}
0
public MembershipAttributes getMembershipAttributes()
{    return membershipAttributes;}
0
public void run()
{    try {        Thread.sleep(timeout);        Threads.printThreadInfo(System.err, "TEST TIMEOUT STACK DUMP");                System.exit(1);    } catch (InterruptedException e) {        }}
0
public synchronized void addResult(DistributedMember distributedMember, Object resultOfSingleExecution)
{    this.resultList.add(resultOfSingleExecution);}
0
public Map<String, List<Partition>> getAssignmentsByReplicaSet(int replicaSetID)
{    return null;}
0
public void testResolveAliasesWildcardsIndicesAliasesRequestDeleteActions()
{    IndicesAliasesRequest request = new IndicesAliasesRequest();    request.addAliasAction(AliasActions.remove().index("*").alias("foo*"));    request.addAliasAction(AliasActions.remove().index("*bar").alias("foo*"));    final List<String> authorizedIndices = buildAuthorizedIndices(user, IndicesAliasesAction.NAME);    List<String> indices = resolveIndices(request, authorizedIndices).getLocal();                String[] expectedIndices = new String[] { "bar", "foofoobar", "foobarfoo", "foofoo" };    assertSameValues(indices, expectedIndices);        assertThat(request.getAliasActions().get(0).indices(), arrayContainingInAnyOrder("bar", "foofoo"));    assertThat(request.getAliasActions().get(0).aliases(), arrayContainingInAnyOrder("foofoobar", "foobarfoo"));    assertThat(request.getAliasActions().get(1).indices(), arrayContainingInAnyOrder("bar"));    assertThat(request.getAliasActions().get(1).aliases(), arrayContainingInAnyOrder("foofoobar", "foobarfoo"));}
0
public static ApplicationId toAppID(String prefix, String s, Iterator<String> it)
{    if (!it.hasNext() || !it.next().equals(prefix)) {        throwParseException(sjoin(prefix, ID), s);    }    shouldHaveNext(prefix, s, it);    ApplicationId appId = ApplicationId.newInstance(Long.parseLong(it.next()), Integer.parseInt(it.next()));    return appId;}
0
private LightWeightHashSet<Block> getECBlocksSet(final DatanodeInfo dn)
{    return nodeToECBlocks.get(dn);}
0
public ObjectMapper getContext(Class<?> objectType)
{    return mapper;}
0
public void testInterpolateStringArray() throws Exception
{    Model model = new Model();    Properties p = new Properties();    p.setProperty("key", "value");    p.setProperty("key2", "value2");    String[] values = { "${key}", "${key2}" };    StringSearchModelInterpolator interpolator = (StringSearchModelInterpolator) createInterpolator();    ModelBuildingRequest config = createModelBuildingRequest(p);    final SimpleProblemCollector collector = new SimpleProblemCollector();    interpolator.interpolateObject(values, model, new File("."), config, collector);    assertProblemFree(collector);    assertEquals("value", values[0]);    assertEquals("value2", values[1]);}
0
public void testMerge() throws Exception
{        final TableName tableName = TableName.valueOf(name.getMethodName());    final Admin admin = TEST_UTIL.getAdmin();    try {                Table table = createTableAndLoadData(MASTER, tableName);        AssignmentManager am = MASTER.getAssignmentManager();        List<RegionInfo> regions = am.getRegionStates().getRegionsOfTable(tableName);                RegionInfo a = regions.get(0);        RegionInfo b = regions.get(1);        am.unassign(b);        am.offlineRegion(b);        try {                        FutureUtils.get(admin.mergeRegionsAsync(a.getEncodedNameAsBytes(), b.getEncodedNameAsBytes(), false));            fail("Offline regions should not be able to merge");        } catch (DoNotRetryRegionException ie) {            System.out.println(ie);            assertTrue(ie instanceof MergeRegionException);        }        try {                        FutureUtils.get(admin.mergeRegionsAsync(b.getEncodedNameAsBytes(), b.getEncodedNameAsBytes(), true));            fail("A region should not be able to merge with itself, even forcfully");        } catch (IOException ie) {            assertTrue("Exception should mention regions not online", StringUtils.stringifyException(ie).contains("region to itself") && ie instanceof MergeRegionException);        }        try {                        FutureUtils.get(admin.mergeRegionsAsync(Bytes.toBytes("-f1"), Bytes.toBytes("-f2"), true));            fail("Unknown region could not be merged");        } catch (IOException ie) {            assertTrue("UnknownRegionException should be thrown", ie instanceof UnknownRegionException);        }        table.close();    } finally {        TEST_UTIL.deleteTable(tableName);    }}
1
public boolean isRely_cstr()
{    return this.rely_cstr;}
0
protected boolean abort(final TEnvironment env)
{    throw new UnsupportedOperationException();}
0
private static Type getType(String name)
{    return types.get(name);}
0
public void test2() throws Exception
{    Thread t = new Thread() {        @Override        public void run() {            throw new RuntimeException("foobar2");        }    };    t.start();    t.join();}
0
public String toString()
{    return "Bound{" + "id=" + ID + '}';}
0
public boolean match(RelOptRuleCall call)
{    final DrillScanRel scan = call.rel(3);    return checkScan(scan);}
0
protected RecoveryState shardOperation(RecoveryRequest request, ShardRouting shardRouting)
{    IndexService indexService = indicesService.indexServiceSafe(shardRouting.shardId().getIndex());    IndexShard indexShard = indexService.getShard(shardRouting.shardId().id());    return indexShard.recoveryState();}
0
public String getMethodName()
{    return methodName;}
0
protected RouteBuilder createRouteBuilder() throws Exception
{    return new RouteBuilder() {        @Override        public void configure() throws Exception {            from("vm:foo?size=100").routeId("foo").noAutoStartup().to("mock:foo");            from("vm:bar").routeId("bar").noAutoStartup().to("mock:bar");        }    };}
0
public void collect(int doc, long bucket) throws IOException
{    collectTimer.start();    try {        delegate.collect(doc, bucket);    } finally {        collectTimer.stop();    }}
0
public void init(FilterConfig filterConfig) throws ServletException
{    filterConfig.getServletContext().setAttribute(DelegationTokenAuthenticationFilter.DELEGATION_TOKEN_SECRET_MANAGER_ATTR, manager);    super.init(filterConfig);}
0
public List<List<Integer>> allSplitVectors()
{    List<List<Integer>> result = new ArrayList<List<Integer>>(SplitVectorKind.values().length);    for (SplitVectorKind kind : SplitVectorKind.values()) {        result.add(kind.get(this));    }    return result;}
0
public Long getClusterId()
{    return clusterId;}
0
public RelBuilder filter(Iterable<CorrelationId> variablesSet, RexNode... predicates)
{    return filter(variablesSet, ImmutableList.copyOf(predicates));}
0
public String getHostname()
{    return hostname;}
0
public void testIsOriginal() throws IOException
{    assertIsOriginal(new Path(TEST_WAREHOUSE_DIR, Table.ACIDTBL.toString().toLowerCase()), false);    assertIsOriginal(new Path(TEST_WAREHOUSE_DIR, Table.NONACIDORCTBL.toString().toLowerCase()), true);}
0
public List<NamedExpression> getSelections()
{    return selections;}
0
public void testAllInNeighborsWithValueGreaterThanTwo() throws Exception
{    /*		 * Get the all the in-neighbors for each vertex that have a value greater than two.         */    final ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();    Graph<Long, Long, Long> graph = Graph.fromDataSet(TestGraphUtils.getLongLongVertexData(env), TestGraphUtils.getLongLongEdgeData(env), env);    DataSet<Tuple2<Long, Long>> verticesWithAllInNeighbors = graph.groupReduceOnEdges(new SelectInNeighborsValueGreaterThanTwo(), EdgeDirection.IN);    List<Tuple2<Long, Long>> result = verticesWithAllInNeighbors.collect();    expectedResult = "3,1\n" + "3,2\n" + "4,3\n" + "5,3\n" + "5,4";    compareResultAsTuples(result, expectedResult);}
0
public void runCancelingOnFullInputTest() throws Exception
{    final String topic = "cancelingOnFullTopic";    final int parallelism = 3;    createTestTopic(topic, parallelism, 1);        DataGenerators.InfiniteStringsGenerator generator = new DataGenerators.InfiniteStringsGenerator(kafkaServer, topic);    generator.start();        final AtomicReference<Throwable> jobError = new AtomicReference<>();    final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();    env.setParallelism(parallelism);    env.enableCheckpointing(100);    Properties props = new Properties();    props.putAll(standardProps);    props.putAll(secureProps);    FlinkKafkaConsumerBase<String> source = kafkaServer.getConsumer(topic, new SimpleStringSchema(), props);    env.addSource(source).addSink(new DiscardingSink<String>());    JobGraph jobGraph = StreamingJobGraphGenerator.createJobGraph(env.getStreamGraph());    final JobID jobId = jobGraph.getJobID();    final Runnable jobRunner = new Runnable() {        @Override        public void run() {            try {                client.setDetached(false);                client.submitJob(jobGraph, KafkaConsumerTestBase.class.getClassLoader());            } catch (Throwable t) {                jobError.set(t);            }        }    };    Thread runnerThread = new Thread(jobRunner, "program runner thread");    runnerThread.start();        Thread.sleep(2000);    Throwable failueCause = jobError.get();    if (failueCause != null) {        failueCause.printStackTrace();        Assert.fail("Test failed prematurely with: " + failueCause.getMessage());    }        client.cancel(jobId);        runnerThread.join();    assertEquals(JobStatus.CANCELED, client.getJobStatus(jobId).get());    if (generator.isAlive()) {        generator.shutdown();        generator.join();    } else {        Throwable t = generator.getError();        if (t != null) {            t.printStackTrace();            fail("Generator failed: " + t.getMessage());        } else {            fail("Generator failed with no exception");        }    }    deleteTestTopic(topic);}
0
public static ADT_A14 toAdtA14(byte[] body, Exchange exchange) throws HL7Exception, IOException
{    return toMessage(ADT_A14.class, body, exchange);}
0
public String getValidationFailedHandlerName()
{    return validationFailedHandlerName;}
0
public Sequence removeFirstSequence(long count)
{    if (isEmpty()) {        return null;    }    Sequence sequence = getHead();    while (sequence != null) {        if (sequence.range() == count) {            sequence.unlink();            return sequence;        }        if (sequence.range() > count) {            Sequence rc = new Sequence(sequence.first, sequence.first + count - 1);            sequence.first += count;            return rc;        }        sequence = sequence.getNext();    }    return null;}
0
public static ADT_A32 toAdtA32(byte[] body, Exchange exchange) throws HL7Exception, IOException
{    return toMessage(ADT_A32.class, body, exchange);}
0
public void setNamespace(String namespace)
{    this.namespace = namespace;}
0
public AutoFollowStats getAutoFollowStats()
{    return autoFollowStats;}
0
public void setReceiveBufferSize(Integer receiveBufferSize)
{    this.receiveBufferSize = receiveBufferSize;}
0
public void testNoAutoStart() throws Exception
{    UpgradeCheckRequest request = new UpgradeCheckRequest(clusterInformation, UpgradeType.ROLLING, repositoryVersion, null, null);    UpgradeCheckResult check = m_check.perform(request);    Assert.assertEquals(UpgradeCheckStatus.PASS, check.getStatus());    Assert.assertTrue(StringUtils.isBlank(check.getFailReason()));}
0
private void addImports(Set<String> imports)
{    annotations.forEach(ann -> addImports(imports, ann));    fields.forEach(f -> addImports(imports, f));    methods.forEach(m -> addImports(imports, m));    properties.forEach(p -> addImports(imports, p));}
0
 GoogleCalendarStreamEndpointBuilder consumeFromNow(boolean consumeFromNow)
{    doSetProperty("consumeFromNow", consumeFromNow);    return this;}
0
public void handleEvent(ResourceEvent event, Object resource)
{    if (event.equals(ResourceEvent.CACHE_REMOVE)) {        try {            latch.await();        } catch (InterruptedException e) {            e.printStackTrace();        }    }}
0
protected void checkHighlightingResponseMapElement(Map<String, List<String>> docHighlights, String highlightedField, String preHighlightText, String highlightedText) throws Exception
{    if (highlightedField == null) {        assertEquals(0, docHighlights.size());    } else {        List<String> docHighlightsList = docHighlights.get(highlightedField);        assertEquals(1, docHighlightsList.size());        assertEquals(preHighlightText + SolrFragmentsBuilder.DEFAULT_PRE_TAGS + highlightedText + SolrFragmentsBuilder.DEFAULT_POST_TAGS, docHighlightsList.get(0));    }}
0
protected void waitForConsumerToLeave(AtomicInteger consumerCounter) throws InterruptedException
{    for (int i = 0; i < 200; i++) {        if (consumerCounter.get() == 0) {            return;        }        Thread.sleep(50);    }    fail("The consumer did not leave.");}
0
private void assertParentTask(List<TaskInfo> tasks, TaskInfo parentTask)
{    for (TaskInfo task : tasks) {        assertParentTask(task, parentTask);    }}
0
private void saveOrUpdateRemoteAmbariClusterEntity(Map<String, Object> properties, boolean update) throws AmbariException
{    String name = (String) properties.get(CLUSTER_NAME_PROPERTY_ID);    String url = (String) properties.get(CLUSTER_URL_PROPERTY_ID);    String username = (String) properties.get(USERNAME_PROPERTY_ID);    String password = (String) properties.get(PASSWORD_PROPERTY_ID);    if (StringUtils.isEmpty(url) && StringUtils.isEmpty(username)) {        throw new IllegalArgumentException("Url or username cannot be null");    }    RemoteAmbariClusterEntity entity;    if (update) {        Long id = Long.valueOf((String) properties.get(CLUSTER_ID_PROPERTY_ID));        entity = remoteAmbariClusterDAO.findById(id);        if (entity == null) {            throw new IllegalArgumentException(String.format("Cannot find cluster with Id : \"%s\"", id));        }    } else {        entity = remoteAmbariClusterDAO.findByName(name);        if (entity != null) {            throw new DuplicateResourceException(String.format("Cluster with name : \"%s\" already exists", name));        }    }        if (StringUtils.isBlank(password) && !update) {        throw new IllegalArgumentException("Password cannot be null");    } else if (StringUtils.isBlank(password) && update && !username.equals(entity.getUsername())) {        throw new IllegalArgumentException("Failed to update. Username does not match.");    }    if (entity == null) {        entity = new RemoteAmbariClusterEntity();    }    entity.setName(name);    entity.setUrl(url);    try {        if (password != null) {            entity.setUsername(username);            entity.setPassword(password);        }    } catch (MaskException e) {        throw new IllegalArgumentException("Failed to create new Remote Cluster " + name + ". Illegal Password");    }    try {        remoteAmbariClusterRegistry.saveOrUpdate(entity, update);    } catch (Exception e) {        throw new IllegalArgumentException("Failed to create new Remote Cluster " + name + ". " + e.getMessage(), e);    }}
0
public String getDatanodeUuid()
{    return datanodeUuid;}
0
private boolean delete(ChannelSftp channel, Path file, boolean recursive) throws IOException
{    Path workDir;    try {        workDir = new Path(channel.pwd());    } catch (SftpException e) {        throw new IOException(e);    }    Path absolute = makeAbsolute(workDir, file);    String pathName = absolute.toUri().getPath();    FileStatus fileStat = null;    try {        fileStat = getFileStatus(channel, absolute);    } catch (FileNotFoundException e) {                return false;    }    if (!fileStat.isDirectory()) {        boolean status = true;        try {            channel.rm(pathName);        } catch (SftpException e) {            status = false;        }        return status;    } else {        boolean status = true;        FileStatus[] dirEntries = listStatus(channel, absolute);        if (dirEntries != null && dirEntries.length > 0) {            if (!recursive) {                throw new IOException(String.format(E_DIR_NOTEMPTY, file));            }            for (int i = 0; i < dirEntries.length; ++i) {                delete(channel, new Path(absolute, dirEntries[i].getPath()), recursive);            }        }        try {            channel.rmdir(pathName);        } catch (SftpException e) {            status = false;        }        return status;    }}
0
public void testBrowseNoPrefetch() throws Exception
{    runTest(brokerUriNoPrefetch);}
0
private void processBlocksInternal(final DatanodeDescriptor datanode, final Iterator<BlockInfo> it, final List<BlockInfo> insufficientList, boolean pruneReliableBlocks)
{    boolean firstReplicationLog = true;        int lowRedundancyBlocksInOpenFiles = 0;    LightWeightHashSet<Long> lowRedundancyOpenFiles = new LightWeightLinkedSet<>();        int lowRedundancyBlocks = 0;        int outOfServiceOnlyReplicas = 0;    while (it.hasNext()) {        if (insufficientList == null && numBlocksCheckedPerLock >= numBlocksPerCheck) {                                                                                                namesystem.writeUnlock();            try {                                Thread.sleep(0, 500);            } catch (InterruptedException ignored) {                return;            }                        numBlocksCheckedPerLock = 0;            namesystem.writeLock();        }        numBlocksChecked++;        numBlocksCheckedPerLock++;        final BlockInfo block = it.next();                if (blockManager.blocksMap.getStoredBlock(block) == null) {            LOG.trace("Removing unknown block {}", block);            it.remove();            continue;        }        long bcId = block.getBlockCollectionId();        if (bcId == INodeId.INVALID_INODE_ID) {                        continue;        }        final BlockCollection bc = blockManager.getBlockCollection(block);        final NumberReplicas num = blockManager.countNodes(block);        final int liveReplicas = num.liveReplicas();                        boolean isDecommission = datanode.isDecommissionInProgress();        boolean isMaintenance = datanode.isEnteringMaintenance();        boolean neededReconstruction = isDecommission ? blockManager.isNeededReconstruction(block, num) : blockManager.isNeededReconstructionForMaintenance(block, num);        if (neededReconstruction) {            if (!blockManager.neededReconstruction.contains(block) && blockManager.pendingReconstruction.getNumReplicas(block) == 0 && blockManager.isPopulatingReplQueues()) {                                blockManager.neededReconstruction.add(block, liveReplicas, num.readOnlyReplicas(), num.outOfServiceReplicas(), blockManager.getExpectedRedundancyNum(block));            }        }                if (isSufficient(block, bc, num, isDecommission, isMaintenance)) {            if (pruneReliableBlocks) {                it.remove();            }            continue;        }                if (insufficientList != null) {            insufficientList.add(block);        }                if (firstReplicationLog) {            logBlockReplicationInfo(block, bc, datanode, num, blockManager.blocksMap.getStorages(block));            firstReplicationLog = false;        }                lowRedundancyBlocks++;        if (bc.isUnderConstruction()) {            INode ucFile = namesystem.getFSDirectory().getInode(bc.getId());            if (!(ucFile instanceof INodeFile) || !ucFile.asFile().isUnderConstruction()) {                            } else {                lowRedundancyBlocksInOpenFiles++;                lowRedundancyOpenFiles.add(ucFile.getId());            }        }        if ((liveReplicas == 0) && (num.outOfServiceReplicas() > 0)) {            outOfServiceOnlyReplicas++;        }    }    datanode.getLeavingServiceStatus().set(lowRedundancyBlocksInOpenFiles, lowRedundancyOpenFiles, lowRedundancyBlocks, outOfServiceOnlyReplicas);}
1
public void testDefaultNone() throws Exception
{    long before = System.currentTimeMillis();    value = variable.execute(result, null);    long now = Long.parseLong(value);    long after = System.currentTimeMillis();    assertTrue(now >= before && now <= after);}
0
private long computeVectorMemory(ColumnMemoryInfo columnInfo, int numValues)
{    if (columnInfo.columnMeta.isFixedLength()) {        return BatchSizingMemoryUtil.computeFixedLengthVectorMemory(columnInfo.columnMeta, numValues);    }    return BatchSizingMemoryUtil.computeVariableLengthVectorMemory(columnInfo.columnMeta, columnInfo.columnPrecision, numValues);}
0
protected void execute(Terminal terminal, OptionSet options) throws Exception
{    Path publicKeyPath = parsePath(publicKeyPathOption.value(options));    if (Files.exists(publicKeyPath) == false) {        throw new UserException(ExitCodes.USAGE, publicKeyPath + " does not exist");    }    final License licenseSpec;    if (options.has(licenseOption)) {        final BytesArray bytes = new BytesArray(licenseOption.value(options).getBytes(StandardCharsets.UTF_8));        licenseSpec = License.fromSource(bytes, XContentType.JSON);    } else if (options.has(licenseFileOption)) {        Path licenseSpecPath = parsePath(licenseFileOption.value(options));        if (Files.exists(licenseSpecPath) == false) {            throw new UserException(ExitCodes.USAGE, licenseSpecPath + " does not exist");        }        final BytesArray bytes = new BytesArray(Files.readAllBytes(licenseSpecPath));        licenseSpec = License.fromSource(bytes, XContentType.JSON);    } else {        throw new UserException(ExitCodes.USAGE, "Must specify either --license or --licenseFile");    }        if (!LicenseVerifier.verifyLicense(licenseSpec, Files.readAllBytes(publicKeyPath))) {        throw new UserException(ExitCodes.DATA_ERROR, "Invalid License!");    }    XContentBuilder builder = XContentFactory.contentBuilder(XContentType.JSON);    builder.startObject();    builder.startObject("license");    licenseSpec.toInnerXContent(builder, ToXContent.EMPTY_PARAMS);    builder.endObject();    builder.endObject();    builder.flush();    terminal.println(Strings.toString(builder));}
0
public static void refresh(Configuration conf)
{    Collection<String> tempServers = new HashSet<String>();        for (String host : conf.getTrimmedStrings(CONF_HADOOP_PROXYSERVERS)) {        InetSocketAddress addr = new InetSocketAddress(host, 0);        if (!addr.isUnresolved()) {            tempServers.add(addr.getAddress().getHostAddress());        }    }    proxyServers = tempServers;}
0
public String toString()
{    return principalType + ":" + name;}
0
public ObjectInspector initialize(ObjectInspector[] arguments) throws UDFArgumentException
{    if (arguments.length != 1) {        throw new UDFArgumentLengthException("CHARACTER_LENGTH requires 1 argument, got " + arguments.length);    }    if (arguments[0].getCategory() != ObjectInspector.Category.PRIMITIVE) {        throw new UDFArgumentException("CHARACTER_LENGTH only takes primitive types, got " + argumentOI.getTypeName());    }    argumentOI = (PrimitiveObjectInspector) arguments[0];    stringConverter = new PrimitiveObjectInspectorConverter.StringConverter(argumentOI);    PrimitiveObjectInspector.PrimitiveCategory inputType = argumentOI.getPrimitiveCategory();    ObjectInspector outputOI = null;    switch(inputType) {        case CHAR:        case VARCHAR:        case STRING:            isInputString = true;            break;        case BINARY:            isInputString = false;            break;        default:            throw new UDFArgumentException(" CHARACTER_LENGTH() only takes STRING/CHAR/VARCHAR/BINARY types as first argument, got " + inputType);    }    outputOI = PrimitiveObjectInspectorFactory.writableIntObjectInspector;    return outputOI;}
0
public void setStoreAppender(StoreAppender storeAppender)
{    this.storeAppender = storeAppender;}
0
protected int oldestUnrepairedTombstone()
{    return oldestUnrepairedTombstone;}
0
public void write(Directory dir, SegmentInfo info, IOContext ioContext) throws IOException
{    if (random.nextInt(100) == 0) {        throw new IOException("Fake IOException from SegmentInfoFormat.write()");    }    delegate.write(dir, info, ioContext);}
0
public DocAttributeSet getAttributes()
{    return null;}
0
public static void createUser(ConnectionParams connectionParams, String clusterName, String userName, String password, AmbariUserRole userRole) throws Exception
{    JsonElement jsonResponse;    jsonResponse = RestApiUtils.executeRequest(new CreateUserWebRequest(connectionParams, userName, password, CreateUserWebRequest.ActiveUser.TRUE, CreateUserWebRequest.AdminUser.FALSE));        if (userRole != AmbariUserRole.NONE) {        jsonResponse = RestApiUtils.executeRequest(new SetUserPrivilegeWebRequest(connectionParams, clusterName, userName, userRole, PrincipalTypeEntity.USER_PRINCIPAL_TYPE_NAME));            }}
1
private String getQueryHash(String queryStr)
{    Hasher hasher = Hashing.md5().newHasher();    hasher.putBytes(queryStr.getBytes(Charset.defaultCharset()));    return hasher.hash().toString();}
0
private List<RecordField> getDefaultFields()
{    final List<RecordField> fields = new ArrayList<>();    fields.add(new RecordField("id", RecordFieldType.INT.getDataType()));    fields.add(new RecordField("name", RecordFieldType.STRING.getDataType()));    fields.add(new RecordField("address", RecordFieldType.STRING.getDataType()));    fields.add(new RecordField("city", RecordFieldType.STRING.getDataType()));    fields.add(new RecordField("state", RecordFieldType.STRING.getDataType()));    fields.add(new RecordField("zipCode", RecordFieldType.STRING.getDataType()));    fields.add(new RecordField("country", RecordFieldType.STRING.getDataType()));    return fields;}
0
public void setFrom(Integer from)
{    this.from = from;}
0
private void checkStream()
{    if (stream == 0)        throw new NullPointerException();}
0
public InternalDistributedSystem getSystem()
{    return system;}
0
public void testParseTimestamp()
{    String thisYearString = String.valueOf(LocalDateTime.now().getYear());    int firstTwoDigits = getFirstTwoDigits();        checkParseTimestamp("y-mm-dd", "0-02-03", thisYearString.substring(0, 3) + "0-02-03 00:00:00");    checkParseTimestamp("yy-mm-dd", "00-02-03", thisYearString.substring(0, 2) + "00-02-03 00:00:00");    checkParseTimestamp("yyy-mm-dd", "000-02-03", thisYearString.substring(0, 1) + "000-02-03 00:00:00");    checkParseTimestamp("yyyy-mm-dd", "000-02-03", thisYearString.substring(0, 1) + "000-02-03 00:00:00");    checkParseTimestamp("rr-mm-dd", "0-02-03", thisYearString.substring(0, 3) + "0-02-03 00:00:00");    checkParseTimestamp("rrrr-mm-dd", "000-02-03", thisYearString.substring(0, 1) + "000-02-03 00:00:00");        checkParseTimestamp("rr-mm-dd", "00-02-03", firstTwoDigits + 1 + "00-02-03 00:00:00");    checkParseTimestamp("rr-mm-dd", "49-02-03", firstTwoDigits + 1 + "49-02-03 00:00:00");    checkParseTimestamp("rr-mm-dd", "50-02-03", firstTwoDigits + "50-02-03 00:00:00");    checkParseTimestamp("rr-mm-dd", "99-02-03", firstTwoDigits + "99-02-03 00:00:00");    checkParseTimestamp("rrrr-mm-dd", "00-02-03", firstTwoDigits + 1 + "00-02-03 00:00:00");    checkParseTimestamp("rrrr-mm-dd", "49-02-03", firstTwoDigits + 1 + "49-02-03 00:00:00");    checkParseTimestamp("rrrr-mm-dd", "50-02-03", firstTwoDigits + "50-02-03 00:00:00");    checkParseTimestamp("rrrr-mm-dd", "99-02-03", firstTwoDigits + "99-02-03 00:00:00");        checkParseTimestamp("yyyy-mm-ddThh24:mi:ss.ff8z", "2018-02-03T04:05:06.5665Z", "2018-02-03 04:05:06.5665");    checkParseTimestamp("yyyy-mm-dd hh24:mi:ss.ff", "2018-02-03 04:05:06.555555555", "2018-02-03 04:05:06.555555555");    checkParseTimestamp("yyyy-mm-dd hh12:mi:ss", "2099-2-03 04:05:06", "2099-02-03 04:05:06");    checkParseTimestamp("yyyyddd", "2018284", "2018-10-11 00:00:00");    checkParseTimestamp("yyyyddd", "20184", "2018-01-04 00:00:00");    checkParseTimestamp("yyyy-mm-ddThh24:mi:ss.ffz", "2018-02-03t04:05:06.444Z", "2018-02-03 04:05:06.444");    checkParseTimestamp("yyyy-mm-dd hh:mi:ss A.M.", "2018-02-03 04:05:06 P.M.", "2018-02-03 16:05:06");    checkParseTimestamp("YYYY-MM-DD HH24:MI TZH:TZM", "2019-1-1 14:00--1:-30", "2019-01-01 14:00:00");    checkParseTimestamp("YYYY-MM-DD HH24:MI TZH:TZM", "2019-1-1 14:00-1:30", "2019-01-01 14:00:00");    checkParseTimestamp("yyyy-mm-dd TZM:TZH", "2019-01-01 1 -3", "2019-01-01 00:00:00");    checkParseTimestamp("yyyy-mm-dd TZH:TZM", "2019-01-01 -0:30", "2019-01-01 00:00:00");    checkParseTimestamp("TZM/YYY-MM-TZH/DD", "0/333-01-11/02", "2333-01-02 00:00:00");    checkParseTimestamp("YYYY-MM-DD HH12:MI AM", "2019-01-01 11:00 p.m.", "2019-01-01 23:00:00");    checkParseTimestamp("YYYY-MM-DD HH12:MI A.M..", "2019-01-01 11:00 pm.", "2019-01-01 23:00:00");    checkParseTimestamp("MI DD-TZM-YYYY-MM TZHPM SS:HH12.FF9", "59 03-30-2017-05 01PM 01:08.123456789", "2017-05-03 20:59:01.123456789");    checkParseTimestamp("YYYYDDMMHH12MISSFFAMTZHTZM", "20170501123159123456789AM-0130", "2017-01-05 00:31:59.123456789");    checkParseTimestamp("YYYY-MM-DD AMHH12", "2017-05-06 P.M.12", "2017-05-06 12:00:00");    checkParseTimestamp("YYYY-MM-DD HH12PM", "2017-05-05 12AM", "2017-05-05 00:00:00");    checkParseTimestamp("YYYY-MM-DD HH12:MI:SS.FF9PM TZH:TZM", "2017-05-03 08:59:01.123456789PM 01:30", "2017-05-03 20:59:01.123456789");    checkParseTimestamp("YYYYDDMMHH12MISSFFAMTZHTZM", "20170501120159123456789AM-0130", "2017-01-05 00:01:59.123456789");        checkParseTimestamp("YYYY DDD", "2000 60", "2000-02-29 00:00:00");    checkParseTimestamp("YYYY DDD", "2000 61", "2000-03-01 00:00:00");    checkParseTimestamp("YYYY DDD", "2000 366", "2000-12-31 00:00:00");        checkParseTimestamp("YYYYMMDDHH12MIA.M.TZHTZM", "201812310800AM+0515", "2018-12-31 08:00:00");    checkParseTimestamp("YYYYMMDDHH12MIA.M.TZHTZM", "201812310800AM0515", "2018-12-31 08:00:00");    checkParseTimestamp("YYYYMMDDHH12MIA.M.TZHTZM", "201812310800AM-0515", "2018-12-31 08:00:00");        checkParseTimestamp("yyyy-MONTH-dd", "2018-FEBRUARY-28", "2018-02-28 00:00:00");    checkParseTimestamp("yyyy-Month-dd", "2018-february-28", "2018-02-28 00:00:00");    checkParseTimestamp("yyyy-month-dd", "2018-FEBRUARY-28", "2018-02-28 00:00:00");    checkParseTimestamp("yyyy-montH-dd", "2018-febRuary-28", "2018-02-28 00:00:00");    checkParseTimestamp("yyyy-MON-dd", "2018-FEB-28", "2018-02-28 00:00:00");    checkParseTimestamp("yyyy-moN-dd", "2018-FeB-28", "2018-02-28 00:00:00");    checkParseTimestamp("yyyy-mon-dd", "2018-FEB-28", "2018-02-28 00:00:00");    verifyBadParseString("yyyy-MON-dd", "2018-FEBRUARY-28");    verifyBadParseString("yyyy-MON-dd", "2018-FEBR-28");    verifyBadParseString("yyyy-MONTH-dd", "2018-FEB-28");        checkParseDate("yyyy-ddMONTH", "2018-4March", "2018-03-04");    checkParseDate("yyyy-MONTHdd", "2018-March4", "2018-03-04");        checkParseTimestamp("IYYY-IW-ID", "2019-01-01", "2018-12-31 00:00:00");    checkParseTimestamp("IYYY-IW-ID", "2019-01-07", "2019-01-06 00:00:00");    checkParseTimestamp("IYYY-IW-ID", "2019-02-01", "2019-01-07 00:00:00");    checkParseTimestamp("IYYY-IW-ID", "2019-52-07", "2019-12-29 00:00:00");    checkParseTimestamp("IYYY-IW-ID", "2020-01-01", "2019-12-30 00:00:00");    checkParseTimestamp("IYYY-IW-ID", "020-01-04", thisYearString.substring(0, 1) + "020-01-02 00:00:00");    checkParseTimestamp("IYY-IW-ID", "020-01-04", thisYearString.substring(0, 1) + "020-01-02 00:00:00");    checkParseTimestamp("IYY-IW-ID", "20-01-04", thisYearString.substring(0, 2) + "20-01-02 00:00:00");    checkParseTimestamp("IY-IW-ID", "20-01-04", thisYearString.substring(0, 2) + "20-01-02 00:00:00");    checkParseTimestamp("IYYY-IW-DAY", "2019-01-monday", "2018-12-31 00:00:00");    checkParseTimestamp("IYYY-IW-Day", "2019-01-Sunday", "2019-01-06 00:00:00");    checkParseTimestamp("IYYY-IW-Dy", "2019-02-MON", "2019-01-07 00:00:00");    checkParseTimestamp("IYYY-IW-DY", "2019-52-sun", "2019-12-29 00:00:00");    checkParseTimestamp("IYYY-IW-dy", "2020-01-Mon", "2019-12-30 00:00:00");            checkParseTimestampIso("IY-IW-ID", "0-01-04", "iw, yyyy", "01, " + thisYearString.substring(0, 3) + "0");    checkParseTimestampIso("I-IW-ID", "0-01-04", "iw, yyyy", "01, " + thisYearString.substring(0, 3) + "0");        checkParseTimestamp("IYYY-IW-ID hh24:mi:ss", "2019-01-01 01:02:03", "2018-12-31 01:02:03");}
0
public boolean containsKey(Object key)
{    checkReadiness();    validateKey(key);    return getDataView().containsKey(getKeyInfo(key), this);}
0
public void updateStatsForGet(final boolean isHit, final long time)
{    setLastAccessed(time);    if (isHit) {        incrementHitCount();    } else {        incrementMissCount();    }}
0
public Boolean getMultipleConsumers()
{    return multipleConsumers;}
0
public String toString()
{    return "Page [bytes.size=" + bytes.size() + ", entryCount=" + dictionarySize + ", uncompressedSize=" + getUncompressedSize() + ", encoding=" + encoding + "]";}
0
public void testExchange() throws Exception
{    getMockEndpoint("mock:result").expectedBodiesReceived("You said: Hello Claus");    template.sendBody("direct:a", "Claus");    assertMockEndpointsSatisfied();}
0
public String toString()
{    return "NagiosConfiguration[host=" + host + ":" + port + ", connectionTimeout=" + connectionTimeout + ", timeout=" + timeout + ", encryptionMethod=" + encryptionMethod + ", encryption=" + encryption + "]";}
0
protected void doInitialize(final Context context)
{    addOption(CommandOption.TEMPLATE_ID.createOption());    addOption(CommandOption.OUTPUT_FILE.createOption());}
0
public ObjectMapper getObjectMapper(String name)
{    return in.getObjectMapper(name);}
0
public static Sensor lateRecordDropSensor(final InternalProcessorContext context)
{    final StreamsMetricsImpl metrics = context.metrics();    final Sensor sensor = metrics.nodeLevelSensor(context.taskId().toString(), context.currentNode().name(), LATE_RECORD_DROP, Sensor.RecordingLevel.INFO);    StreamsMetricsImpl.addInvocationRateAndCountToSensor(sensor, PROCESSOR_NODE_METRICS_GROUP, metrics.tagMap("task-id", context.taskId().toString(), PROCESSOR_NODE_ID_TAG, context.currentNode().name()), LATE_RECORD_DROP);    return sensor;}
0
public Response getVersions(String body, @Context HttpHeaders headers, @Context UriInfo ui, @ApiParam(value = "view name") @PathParam("viewName") String viewName)
{    return handleRequest(headers, body, ui, Request.Type.GET, createResource(viewName, null));}
0
public short getThriftFieldId()
{    return _thriftId;}
0
public void setSortColumnsIsSet(boolean value)
{    __isset_bitfield = org.apache.thrift.EncodingUtils.setBit(__isset_bitfield, __SORTCOLUMNS_ISSET_ID, value);}
0
public String getDescription()
{    return "StreamHandler";}
0
private static Predicate<T> not(Predicate<T> t)
{    return t.negate();}
0
public INPUT<SMALL<T>> input(String selector)
{    return setSelector(input(), selector);}
0
 void setChild(int idx, TrieNode child)
{    this.child[idx] = child;}
0
public void close() throws JMSException
{    if (!closed) {        closed = true;        try {            if (consumer != null) {                consumer.stop();            }            if (pollingConsumer != null) {                pollingConsumer.stop();            }        } catch (JMSException e) {            throw e;        } catch (Exception e) {            throw JMSExceptionSupport.create(e);        }    }}
0
public void setTracePattern(String tracePattern)
{    this.tracePattern = tracePattern;    if (tracePattern != null) {                this.patterns = tracePattern.split(",");    } else {        this.patterns = null;    }}
0
public MulticastDefinition streaming()
{    setStreaming(true);    return this;}
0
public void popScope()
{    scopes.pop();}
0
public void setup() throws Exception
{    connection = factory.createActiveMQConnection();    underTest = new TransactionContext(connection);}
0
public void unset_action()
{    this.action = null;}
0
public void testTranslation() throws Exception
{    LongValue reuse = new LongValue();    assertEquals(new LongValue(0), new LongValueAddOffset(0).translate(new LongValue(0), reuse));    assertEquals(new LongValue(3), new LongValueAddOffset(1).translate(new LongValue(2), reuse));    assertEquals(new LongValue(1), new LongValueAddOffset(-1).translate(new LongValue(2), reuse));    assertEquals(new LongValue(-1), new LongValueAddOffset(Long.MIN_VALUE).translate(new LongValue(Long.MAX_VALUE), reuse));    assertEquals(new LongValue(-1), new LongValueAddOffset(Long.MAX_VALUE).translate(new LongValue(Long.MIN_VALUE), reuse));        assertEquals(new LongValue(Long.MAX_VALUE), new LongValueAddOffset(-1).translate(new LongValue(Long.MIN_VALUE), reuse));    assertEquals(new LongValue(0), new LongValueAddOffset(Long.MIN_VALUE).translate(new LongValue(Long.MIN_VALUE), reuse));        assertEquals(new LongValue(Long.MIN_VALUE), new LongValueAddOffset(1).translate(new LongValue(Long.MAX_VALUE), reuse));    assertEquals(new LongValue(-2), new LongValueAddOffset(Long.MAX_VALUE).translate(new LongValue(Long.MAX_VALUE), reuse));}
0
public void before() throws Exception
{    m_dao = createMock(AlertDispatchDAO.class);    m_definitionDao = createMock(AlertDefinitionDAO.class);    m_amc = createMock(AmbariManagementController.class);    m_clusters = createMock(Clusters.class);    m_cluster = createMock(Cluster.class);        m_injector = Guice.createInjector(Modules.override(new InMemoryDefaultTestModule()).with(new MockModule()));    assertNotNull(m_injector);    expect(m_amc.getClusters()).andReturn(m_clusters).anyTimes();    expect(m_clusters.getCluster((String) anyObject())).andReturn(m_cluster).anyTimes();    expect(m_clusters.getClusterById(1L)).andReturn(m_cluster).anyTimes();    expect(m_cluster.getClusterId()).andReturn(1L).anyTimes();    expect(m_cluster.getResourceId()).andReturn(4L).anyTimes();    AuthorizationHelperInitializer.viewInstanceDAOReturningNull();}
0
private Collection<NumaNodeResource> getExpectedNumaNodesList()
{    Collection<NumaNodeResource> expectedNodesList = new ArrayList<>(2);    expectedNodesList.add(new NumaNodeResource("0", 73717, 4));    expectedNodesList.add(new NumaNodeResource("1", 73727, 4));    return expectedNodesList;}
0
 AbstractGroupScan getGroupScan(String userName, FileSelection selection, List<SchemaPath> columns, OptionManager options, MetadataProviderManager metadataProvider) throws IOException
{    return getGroupScan(userName, selection, columns, metadataProvider);}
0
public String getError()
{    if (error != null) {        return error;    }    String processResponse = getProcessResponse(process.getErrorStream());    if (!isRunning()) {        error = processResponse;    }    return processResponse;}
0
public void shouldResumeMatchingSuspendedTasks()
{    mockRunningTaskSuspension();    t1.resume();    EasyMock.expectLastCall();    t1.initializeTaskTime();    t1.initializeTopology();    EasyMock.expectLastCall().once();    EasyMock.replay(t1);    assertThat(suspendTask(), nullValue());    assertTrue(assignedTasks.maybeResumeSuspendedTask(taskId1, Collections.singleton(tp1)));    assertThat(assignedTasks.runningTaskIds(), equalTo(Collections.singleton(taskId1)));    EasyMock.verify(t1);}
0
public void start(I iface, setLogConfig_args args, org.apache.storm.thrift.async.AsyncMethodCallback<Void> resultHandler) throws org.apache.storm.thrift.TException
{    iface.setLogConfig(args.name, args.config, resultHandler);}
0
public void addStringField(String fieldName, String value)
{    if (logger.isTraceEnabled()) {        logger.trace("addStringField fieldName: {}; value: {}", fieldName, value);    }    m_pdxInstanceFactory.writeObject(fieldName, value);    addIdentityField(fieldName);}
0
public List<String> getEnabledServices()
{    return enabledServices;}
0
public void write(Kryo kryo, Output output, Dog object)
{    output.writeString(object.getName());}
0
public final boolean isDistinct()
{    return distinct;}
0
public boolean addAll(Collection<? extends Filter> c)
{    return this.backingList.addAll(c);}
0
public void updateNumTableSpaceQuotaSnapshots(long numSnapshots)
{    this.spaceQuotaSnapshotsReceived.mark(numSnapshots);}
0
public void runAsyncFailure() throws Exception
{    final String testMessage = "this is just a test";    CompletionStage<Void> sideEffectFuture = MoreFutures.runAsync(() -> {        throw new IllegalStateException(testMessage);    });    thrown.expect(ExecutionException.class);    thrown.expectCause(isA(IllegalStateException.class));    thrown.expectMessage(testMessage);    MoreFutures.get(sideEffectFuture);}
0
public GroovyClassDoc[] specifiedClasses()
{    /*todo*/    return null;}
0
private void initFields()
{}
0
private Response createDelegationToken(DelegationToken tokenData, HttpServletRequest hsr, UserGroupInformation callerUGI) throws AuthorizationException, IOException, InterruptedException, Exception
{    final String renewer = tokenData.getRenewer();    GetDelegationTokenResponse resp;    try {        resp = callerUGI.doAs(new PrivilegedExceptionAction<GetDelegationTokenResponse>() {            @Override            public GetDelegationTokenResponse run() throws IOException, YarnException {                GetDelegationTokenRequest createReq = GetDelegationTokenRequest.newInstance(renewer);                return rm.getClientRMService().getDelegationToken(createReq);            }        });    } catch (Exception e) {                throw e;    }    Token<RMDelegationTokenIdentifier> tk = new Token<RMDelegationTokenIdentifier>(resp.getRMDelegationToken().getIdentifier().array(), resp.getRMDelegationToken().getPassword().array(), new Text(resp.getRMDelegationToken().getKind()), new Text(resp.getRMDelegationToken().getService()));    RMDelegationTokenIdentifier identifier = tk.decodeIdentifier();    long currentExpiration = rm.getRMContext().getRMDelegationTokenSecretManager().getRenewDate(identifier);    DelegationToken respToken = new DelegationToken(tk.encodeToUrlString(), renewer, identifier.getOwner().toString(), tk.getKind().toString(), currentExpiration, identifier.getMaxDate());    return Response.status(Status.OK).entity(respToken).build();}
1
public KeyValue<Bytes, byte[]> makeNext()
{    final KeyValue<Bytes, byte[]> next = super.makeNext();    if (next == null) {        return allDone();    } else {        if (comparator.compare(next.key.get(), rawToKey) <= 0) {            return next;        } else {            return allDone();        }    }}
0
public void setClusterNodeId(String clusterNodeId)
{    this.clusterNodeId = clusterNodeId;}
0
public Cell current()
{        return new Cell() {        @Override        public long heapSize() {            return 0;        }        private final int i = index;        @Override        public byte[] getRowArray() {            return null;        }        @Override        public int getRowOffset() {            return 0;        }        @Override        public short getRowLength() {            return 0;        }        @Override        public byte[] getFamilyArray() {            return null;        }        @Override        public int getFamilyOffset() {            return 0;        }        @Override        public byte getFamilyLength() {            return 0;        }        @Override        public byte[] getQualifierArray() {            return null;        }        @Override        public int getQualifierOffset() {            return 0;        }        @Override        public int getQualifierLength() {            return 0;        }        @Override        public long getTimestamp() {            return 0;        }        @Override        public byte getTypeByte() {            return 0;        }        @Override        public long getSequenceId() {            return 0;        }        @Override        public byte[] getValueArray() {            return Bytes.toBytes(this.i);        }        @Override        public int getValueOffset() {            return 0;        }        @Override        public int getValueLength() {            return Bytes.SIZEOF_INT;        }        @Override        public int getSerializedSize() {            return 0;        }        @Override        public int getTagsOffset() {            return 0;        }        @Override        public int getTagsLength() {            return 0;        }        @Override        public byte[] getTagsArray() {            return null;        }        @Override        public Type getType() {            return null;        }    };}
0
protected void serviceStop() throws Exception
{    if (this.webApp != null) {                this.webApp.stop();    }    super.serviceStop();}
1
public void disableErasureCodingPolicy(String ecPolicyName) throws IOException
{    dfs.disableErasureCodingPolicy(ecPolicyName);}
0
public void setExpression(ExpressionDefinition expression)
{        super.setExpression(expression);}
0
public String toString()
{    StringBuilder sb = new StringBuilder("grant_revoke_privileges_result(");    boolean first = true;    sb.append("success:");    if (this.success == null) {        sb.append("null");    } else {        sb.append(this.success);    }    first = false;    if (!first)        sb.append(", ");    sb.append("o1:");    if (this.o1 == null) {        sb.append("null");    } else {        sb.append(this.o1);    }    first = false;    sb.append(")");    return sb.toString();}
0
public void sort(WebResource[] resources, String order)
{    Comparator<WebResource> comparator = getComparator(order);    if (null != comparator)        Arrays.sort(resources, comparator);}
0
private void applyChildDefaults()
{    Collection<FSQueue> queues = queueMgr.getQueues();    Set<String> configuredLeafQueues = allocConf.getConfiguredQueues().get(FSQueueType.LEAF);    Set<String> configuredParentQueues = allocConf.getConfiguredQueues().get(FSQueueType.PARENT);    for (FSQueue queue : queues) {                if ((queue.getParent() != null) && !configuredLeafQueues.contains(queue.getName()) && !configuredParentQueues.contains(queue.getName())) {            ConfigurableResource max = queue.getParent().getMaxChildQueueResource();            if (max != null) {                queue.setMaxShare(max);            }        }    }}
0
public long getTime(TimeUnit unit)
{    return unit.convert(timeMS, TimeUnit.MILLISECONDS);}
0
private URL[] recursiveBuildLibList(File path) throws MalformedURLException
{    URL[] urls = new URL[0];    if (path == null || !path.exists()) {        return urls;    } else if (path.getName().startsWith(".")) {        return urls;    } else if (path.isDirectory()) {        File[] files = path.listFiles();        if (files != null) {            for (File f : files) {                urls = (URL[]) ArrayUtils.addAll(urls, recursiveBuildLibList(f));            }        }        return urls;    } else {        return new URL[] { path.toURI().toURL() };    }}
0
public void start(I iface, getProgress_args args, org.apache.thrift.async.AsyncMethodCallback<java.lang.Integer> resultHandler) throws org.apache.thrift.TException
{    iface.getProgress(args.sessionId, args.className, args.interpreterContext, resultHandler);}
0
public int getFailureCount()
{    return failureCount;}
0
public boolean isSetTxn_high_water_mark()
{    return EncodingUtils.testBit(__isset_bitfield, __TXN_HIGH_WATER_MARK_ISSET_ID);}
0
public void initializeBindings()
{    for (Runnable initializer : uninitializedBindings) {        initializer.run();    }}
0
private FieldMapping createFieldMapping(String jdbcName, String jdbcType, boolean jdbcNullable, List<PdxField> pdxFields)
{    String pdxName = null;    String pdxType = null;    for (PdxField pdxField : pdxFields) {        if (pdxField.getFieldName().equals(jdbcName)) {            pdxName = pdxField.getFieldName();            pdxType = pdxField.getFieldType().name();            break;        }    }    if (pdxName == null) {                for (PdxField pdxField : pdxFields) {            if (pdxField.getFieldName().equalsIgnoreCase(jdbcName)) {                if (pdxName != null) {                    throw new JdbcConnectorException("More than one PDX field name matched the column name \"" + jdbcName + "\"");                }                pdxName = pdxField.getFieldName();                pdxType = pdxField.getFieldType().name();            }        }    }    if (pdxName == null) {        throw new JdbcConnectorException("No PDX field name matched the column name \"" + jdbcName + "\"");    }    return new FieldMapping(pdxName, pdxType, jdbcName, jdbcType, jdbcNullable);}
0
public boolean isUserInRole(String s)
{    return false;}
0
protected RouteBuilder doCreateRouteBuilder() throws Exception
{    return new RouteBuilder() {        @Override        public void configure() throws Exception {            from("direct:deleteBatchAccounts").to("salesforce:query?sObjectClass=" + Accounts.class.getName() + "&sObjectQuery=SELECT Id FROM Account WHERE Name = 'Account created from Composite batch API'").split(simple("${body.records}")).setHeader("sObjectId", simple("${body.id}")).to("salesforce:deleteSObject?sObjectName=Account").end();        }    };}
0
public ReplicaListTransformer getBaseReplicaListTransformer()
{    return baseReplicaListTransformer;}
0
private void runReadTest(BigtableIO.Read read, List<Row> expected)
{    PCollection<Row> rows = p.apply(read.getTableId() + "_" + read.getKeyRanges(), read);    PAssert.that(rows).containsInAnyOrder(expected);    p.run();}
0
public synchronized boolean putProfile(Profile p)
{    CacheProfile profile = (CacheProfile) p;    PartitionedRegion pr = getPartitionedRegion();    if (profile.hasCacheLoader) {        pr.setHaveCacheLoader();    }        if (profile.filterProfile != null) {        if (!pr.isDataStore()) {            profile.filterProfile = null;        }    }    return super.putProfile(profile);}
0
public String getShortString()
{    switch(value) {        case DIRECTED:            return "d";        case UNDIRECTED:            return "u";        case UNDIRECTED_CLIP_AND_FLIP:            return "";        default:            return "";    }}
0
public void set_resources_map(@org.apache.storm.thrift.annotation.Nullable java.util.Map<java.lang.String, java.lang.Double> resources_map)
{    this.resources_map = resources_map;}
0
public boolean isTupleType()
{    return false;}
0
public int getMinWordLength()
{    return minWordLength;}
0
private HRegion getRegion(final Configuration conf, final String tableName) throws IOException
{    FSHLog wal = new FSHLog(FileSystem.get(conf), TEST_UTIL.getDataTestDir(), TEST_UTIL.getDataTestDir().toString(), conf);    wal.init();    ChunkCreator.initialize(MemStoreLABImpl.CHUNK_SIZE_DEFAULT, false, 0, 0, 0, null);    return TEST_UTIL.createLocalHRegion(TableName.valueOf(tableName), HConstants.EMPTY_BYTE_ARRAY, HConstants.EMPTY_BYTE_ARRAY, false, Durability.SKIP_WAL, wal, INCREMENT_BYTES);}
0
public void setUp() throws IOException
{    this.mockGcsUtil = mock(GcsUtil.class);    when(mockGcsUtil.create(any(GcsPath.class), anyString())).then(invocation -> FileChannel.open(Files.createTempFile("channel-", ".tmp"), StandardOpenOption.CREATE, StandardOpenOption.WRITE, StandardOpenOption.DELETE_ON_CLOSE));    when(mockGcsUtil.create(any(GcsPath.class), anyString(), anyInt())).then(invocation -> FileChannel.open(Files.createTempFile("channel-", ".tmp"), StandardOpenOption.CREATE, StandardOpenOption.WRITE, StandardOpenOption.DELETE_ON_CLOSE));    when(mockGcsUtil.expand(any(GcsPath.class))).then(invocation -> ImmutableList.of((GcsPath) invocation.getArguments()[0]));    when(mockGcsUtil.bucketAccessible(GcsPath.fromUri(VALID_STAGING_BUCKET))).thenReturn(true);    when(mockGcsUtil.bucketAccessible(GcsPath.fromUri(VALID_TEMP_BUCKET))).thenReturn(true);    when(mockGcsUtil.bucketAccessible(GcsPath.fromUri(VALID_TEMP_BUCKET + "/staging/"))).thenReturn(true);    when(mockGcsUtil.bucketAccessible(GcsPath.fromUri(VALID_PROFILE_BUCKET))).thenReturn(true);    when(mockGcsUtil.bucketAccessible(GcsPath.fromUri(NON_EXISTENT_BUCKET))).thenReturn(false);        when(mockGcsUtil.getObjects(anyListOf(GcsPath.class))).thenAnswer(invocationOnMock -> {        List<GcsPath> gcsPaths = (List<GcsPath>) invocationOnMock.getArguments()[0];        List<GcsUtil.StorageObjectOrIOException> results = new ArrayList<>();        for (GcsPath gcsPath : gcsPaths) {            if (gcsPath.getBucket().equals(VALID_BUCKET)) {                StorageObject resultObject = new StorageObject();                resultObject.setBucket(gcsPath.getBucket());                resultObject.setName(gcsPath.getObject());                results.add(GcsUtil.StorageObjectOrIOException.create(resultObject));            }        }        return results;    });        when(mockGcsUtil.bucketAccessible(GcsPath.fromUri("gs://bucket/object"))).thenReturn(true);    mockJobs = mock(Dataflow.Projects.Locations.Jobs.class);}
0
public void setValue(PartitionFieldNode root, TreeMap<Long, Partition> partitions, TreeMap<Long, StorageDescriptor> sds, TreeMap<Long, SerDeInfo> serdes, TreeMap<Long, List<FieldSchema>> cds) throws MetaException
{            List<String> childFields = getChildrenFieldNames(root);    final String tableName = fieldNameToTableName.containsKey("COLUMNS_V2") ? fieldNameToTableName.get("COLUMNS_V2") : "COLUMNS_V2";    MetastoreDirectSqlUtils.setSDCols(tableName, childFields, pm, cds, Joiner.on(',').join(cds.keySet()));}
0
public void setKey(ObjectType value)
{    this.key = value;}
0
public Map<String, Map<Duration, Long>> getTimerSnapshots()
{    return providerInstance().getSnapshots(NanosecondTimer.class);}
0
public void setForceResourceId(Object forceResourceId)
{    this.forceResourceId = forceResourceId;}
0
public List<FileStatus> listFiles()
{    return Collections.unmodifiableList(files);}
0
public void setAttachments(List<Attachment> attachments)
{    this.attachments = attachments;}
0
public void testProcessRpcMetricDefinition()
{        org.apache.ambari.server.state.stack.Metric metric = new org.apache.ambari.server.state.stack.Metric("rpcdetailed.rpcdetailed.FsyncAvgTime", false, true, false, "unitless");    Map<String, Metric> replacementMap = PropertyHelper.processRpcMetricDefinition("ganglia", "NAMENODE", "metrics/rpcdetailed/fsync_avg_time", metric);    Assert.assertNotNull(replacementMap);    Assert.assertEquals(3, replacementMap.size());    Assert.assertTrue(replacementMap.containsKey("metrics/rpcdetailed/client/fsync_avg_time"));    Assert.assertTrue(replacementMap.containsKey("metrics/rpcdetailed/datanode/fsync_avg_time"));    Assert.assertTrue(replacementMap.containsKey("metrics/rpcdetailed/healthcheck/fsync_avg_time"));    Assert.assertEquals("rpcdetailed.rpcdetailed.client.FsyncAvgTime", replacementMap.get("metrics/rpcdetailed/client/fsync_avg_time").getName());    Assert.assertEquals("rpcdetailed.rpcdetailed.datanode.FsyncAvgTime", replacementMap.get("metrics/rpcdetailed/datanode/fsync_avg_time").getName());    Assert.assertEquals("rpcdetailed.rpcdetailed.healthcheck.FsyncAvgTime", replacementMap.get("metrics/rpcdetailed/healthcheck/fsync_avg_time").getName());        metric = new org.apache.ambari.server.state.stack.Metric("Hadoop:service=NameNode,name=RpcDetailedActivity.FsyncAvgTime", true, false, false, "unitless");    replacementMap = PropertyHelper.processRpcMetricDefinition("jmx", "NAMENODE", "metrics/rpcdetailed/fsync_avg_time", metric);    Assert.assertNotNull(replacementMap);    Assert.assertEquals(3, replacementMap.size());    Assert.assertTrue(replacementMap.containsKey("metrics/rpcdetailed/client/fsync_avg_time"));    Assert.assertTrue(replacementMap.containsKey("metrics/rpcdetailed/datanode/fsync_avg_time"));    Assert.assertTrue(replacementMap.containsKey("metrics/rpcdetailed/healthcheck/fsync_avg_time"));    Assert.assertEquals("Hadoop:service=NameNode,name=RpcDetailedActivity,tag=client.FsyncAvgTime", replacementMap.get("metrics/rpcdetailed/client/fsync_avg_time").getName());    Assert.assertEquals("Hadoop:service=NameNode,name=RpcDetailedActivity,tag=datanode.FsyncAvgTime", replacementMap.get("metrics/rpcdetailed/datanode/fsync_avg_time").getName());    Assert.assertEquals("Hadoop:service=NameNode,name=RpcDetailedActivity,tag=healthcheck.FsyncAvgTime", replacementMap.get("metrics/rpcdetailed/healthcheck/fsync_avg_time").getName());        metric = new org.apache.ambari.server.state.stack.Metric("Hadoop:service=NameNode,name=RpcActivity.RpcQueueTime_avg_time", true, false, false, "unitless");    replacementMap = PropertyHelper.processRpcMetricDefinition("jmx", "NAMENODE", "metrics/rpc/RpcQueueTime_avg_time", metric);    Assert.assertNotNull(replacementMap);    Assert.assertEquals(3, replacementMap.size());    Assert.assertTrue(replacementMap.containsKey("metrics/rpc/client/RpcQueueTime_avg_time"));    Assert.assertTrue(replacementMap.containsKey("metrics/rpc/datanode/RpcQueueTime_avg_time"));    Assert.assertTrue(replacementMap.containsKey("metrics/rpc/healthcheck/RpcQueueTime_avg_time"));    Assert.assertEquals("Hadoop:service=NameNode,name=RpcActivity,tag=client.RpcQueueTime_avg_time", replacementMap.get("metrics/rpc/client/RpcQueueTime_avg_time").getName());    Assert.assertEquals("Hadoop:service=NameNode,name=RpcActivity,tag=datanode.RpcQueueTime_avg_time", replacementMap.get("metrics/rpc/datanode/RpcQueueTime_avg_time").getName());    Assert.assertEquals("Hadoop:service=NameNode,name=RpcActivity,tag=healthcheck.RpcQueueTime_avg_time", replacementMap.get("metrics/rpc/healthcheck/RpcQueueTime_avg_time").getName());}
0
public static void setUp() throws Exception
{}
0
 AdvancedSjmsEndpointConsumerBuilder exceptionHandler(ExceptionHandler exceptionHandler)
{    doSetProperty("exceptionHandler", exceptionHandler);    return this;}
0
public String toString()
{    return "mapRefRelToCorRef=" + mapRefRelToCorRef + "\nmapCorToCorRel=" + mapCorToCorRel + "\nmapFieldAccessToCorRef=" + mapFieldAccessToCorRef + "\n";}
0
public void txApplyDestroyCallsRemoveEntry_givenOldRegionEntry() throws Exception
{    givenLocalRegion();    givenConcurrencyChecks();    givenOldRegionEntry();    doTxApplyDestroy();    verify(oldRegionEntry, never()).makeTombstone(any(), any());    verify(oldRegionEntry, times(1)).removePhase1(same(owner), eq(false));    verify(oldRegionEntry, times(1)).removePhase2();    verify(regionMap, times(1)).removeEntry(eq(key), same(oldRegionEntry), eq(false));}
0
public T oneByType(Class<T> type)
{    if (CollectionUtils.isEmpty(this.combinedPrincipals)) {        return null;    }    for (Object value : this.combinedPrincipals.values()) {        if (type.isInstance(value)) {            return type.cast(value);        }    }    return null;}
0
public BDO<H3<T>> bdo()
{    closeAttrs();    return bdo_(this, true);}
0
public void testNotFollowedByAnyBeforeOneOrMoreCombinations() throws Exception
{    final List<List<Event>> matches = testNotFollowedByBeforeOneOrMore(false, true);    compareMaps(matches, Lists.<List<Event>>newArrayList(Lists.newArrayList(NotFollowByData.A_1, NotFollowByData.B_1, NotFollowByData.B_4, NotFollowByData.B_5, NotFollowByData.B_6, NotFollowByData.D_1), Lists.newArrayList(NotFollowByData.A_1, NotFollowByData.B_1, NotFollowByData.B_4, NotFollowByData.B_6, NotFollowByData.D_1), Lists.newArrayList(NotFollowByData.A_1, NotFollowByData.B_1, NotFollowByData.B_4, NotFollowByData.B_5, NotFollowByData.D_1), Lists.newArrayList(NotFollowByData.A_1, NotFollowByData.B_1, NotFollowByData.B_4, NotFollowByData.D_1), Lists.newArrayList(NotFollowByData.A_1, NotFollowByData.B_1, NotFollowByData.B_5, NotFollowByData.B_6, NotFollowByData.D_1), Lists.newArrayList(NotFollowByData.A_1, NotFollowByData.B_1, NotFollowByData.B_5, NotFollowByData.D_1), Lists.newArrayList(NotFollowByData.A_1, NotFollowByData.B_1, NotFollowByData.B_6, NotFollowByData.D_1), Lists.newArrayList(NotFollowByData.A_1, NotFollowByData.B_1, NotFollowByData.D_1)));}
0
public void testRankingSolrFeature() throws Exception
{        loadFeature("powpularityS", SolrFeature.class.getName(), "{\"q\":\"{!func}pow(popularity,2)\"}");    loadFeature("unpopularityS", SolrFeature.class.getName(), "{\"q\":\"{!func}div(1,popularity)\"}");    loadModel("powpularityS-model", LinearModel.class.getName(), new String[] { "powpularityS" }, "{\"weights\":{\"powpularityS\":1.0}}");    loadModel("unpopularityS-model", LinearModel.class.getName(), new String[] { "unpopularityS" }, "{\"weights\":{\"unpopularityS\":1.0}}");    final SolrQuery query = new SolrQuery();    query.setQuery("title:w1");    query.add("fl", "*, score");    query.add("rows", "4");    assertJQ("/query" + query.toQueryString(), "/response/numFound/==4");    assertJQ("/query" + query.toQueryString(), "/response/docs/[0]/id=='1'");    assertJQ("/query" + query.toQueryString(), "/response/docs/[1]/id=='8'");    assertJQ("/query" + query.toQueryString(), "/response/docs/[2]/id=='6'");    assertJQ("/query" + query.toQueryString(), "/response/docs/[3]/id=='7'");        query.add("rq", "{!ltr model=powpularityS-model reRankDocs=4}");    query.set("debugQuery", "on");    assertJQ("/query" + query.toQueryString(), "/response/numFound/==4");    assertJQ("/query" + query.toQueryString(), "/response/docs/[0]/id=='8'");    assertJQ("/query" + query.toQueryString(), "/response/docs/[0]/score==64.0");    assertJQ("/query" + query.toQueryString(), "/response/docs/[1]/id=='7'");    assertJQ("/query" + query.toQueryString(), "/response/docs/[1]/score==49.0");    assertJQ("/query" + query.toQueryString(), "/response/docs/[2]/id=='6'");    assertJQ("/query" + query.toQueryString(), "/response/docs/[2]/score==36.0");    assertJQ("/query" + query.toQueryString(), "/response/docs/[3]/id=='1'");    assertJQ("/query" + query.toQueryString(), "/response/docs/[3]/score==1.0");    query.remove("rq");    query.add("rq", "{!ltr model=unpopularityS-model reRankDocs=4}");    assertJQ("/query" + query.toQueryString(), "/response/numFound/==4");    assertJQ("/query" + query.toQueryString(), "/response/docs/[0]/id=='1'");    assertJQ("/query" + query.toQueryString(), "/response/docs/[0]/score==1.0");    assertJQ("/query" + query.toQueryString(), "/response/docs/[1]/id=='6'");    assertJQ("/query" + query.toQueryString(), "/response/docs/[2]/id=='7'");    assertJQ("/query" + query.toQueryString(), "/response/docs/[3]/id=='8'");        loadFeature("powdesS", SolrFeature.class.getName(), "{\"q\":\"{!func}pow(description,2)\"}");    loadModel("powdesS-model", LinearModel.class.getName(), new String[] { "powdesS" }, "{\"weights\":{\"powdesS\":1.0}}");    query.remove("rq");    query.add("rq", "{!ltr model=powdesS-model reRankDocs=4}");    assertJQ("/query" + query.toQueryString(), "/error/msg/=='" + FeatureException.class.getName() + ": " + "java.lang.UnsupportedOperationException: " + "Unable to extract feature for powdesS'");}
0
public void setUuid(String uuid)
{    this.uuid = uuid;}
0
public void tightUnmarshal(OpenWireFormat wireFormat, Object o, DataInput dataIn, BooleanStream bs) throws IOException
{    super.tightUnmarshal(wireFormat, o, dataIn, bs);    ActiveMQBlobMessage info = (ActiveMQBlobMessage) o;    info.setRemoteBlobUrl(tightUnmarshalString(dataIn, bs));    info.setMimeType(tightUnmarshalString(dataIn, bs));    info.setDeletedByBroker(bs.readBoolean());}
0
private void logFSState()
{    }
1
 CharSequence[] getValuesByField()
{    return this.values;}
0
public void test_INTERVAL_PRECISION_hasRightValue_mdrReqINTERVAL_Y() throws SQLException
{        assertThat(getIntOrNull(mdrReqINTERVAL_Y, "INTERVAL_PRECISION"), equalTo(2));}
0
public void testAddingNewAlertDefWithoutChangingExisting() throws Exception
{        AlertDefinition existing1 = anAlertDefinition(1l);    AlertDefinition existing2 = anAlertDefinition(2l);    alertCluster = newAlertCluster(existing1, existing2);        AlertDefinition newDef = anAlertDefinition(3l);    AlertCluster result = update(alertCluster, newDef);        assertHasAlerts(result, existing1, existing2, newDef);}
0
private void toXContent(WatcherStatsResponse.Node node, XContentBuilder builder) throws IOException
{    builder.startObject();    builder.field("node_id", node.getNodeId());    builder.field("watcher_state", node.getWatcherState().toString().toLowerCase(Locale.ROOT));    builder.field("watch_count", node.getWatchesCount());    builder.startObject("execution_thread_pool");    builder.field("queue_size", node.getThreadPoolQueueSize());    builder.field("max_size", node.getThreadPoolMaxSize());    builder.endObject();    if (node.getSnapshots() != null) {        builder.startArray("current_watches");        for (WatchExecutionSnapshot snapshot : node.getSnapshots()) {            toXContent(snapshot, builder);        }        builder.endArray();    }    if (node.getQueuedWatches() != null) {        builder.startArray("queued_watches");        for (QueuedWatch queuedWatch : node.getQueuedWatches()) {            toXContent(queuedWatch, builder);        }        builder.endArray();    }    if (node.getStats() != null) {        builder.field("stats", node.getStats());    }    builder.endObject();}
0
public int partition(Long key, int numPartitions)
{    return 0;}
0
public void onConnected()
{}
0
public int getMaxResults()
{    return maxResults;}
0
public void verifyThatPdxFieldNamedSameAsPrimaryKeyIsIgnored() throws Exception
{    setupRegion("id");    PdxInstance pdx1 = cache.createPdxInstanceFactory("Employee").writeString("name", "Emp1").writeObject("age", 55).writeInt("id", 3).create();    employees.put("1", pdx1);    awaitUntil(() -> assertThat(jdbcWriter.getSuccessfulEvents()).isEqualTo(1));    ResultSet resultSet = statement.executeQuery("select * from " + REGION_TABLE_NAME + " order by id asc");    assertRecordMatchesEmployee(resultSet, "1", employee1);    assertThat(resultSet.next()).isFalse();}
0
public void configure() throws Exception
{        from("direct:start").idempotentConsumer(header("messageId"), jpaMessageIdRepository(lookup(EntityManagerFactory.class), PROCESSOR_NAME)).to("mock:result");}
0
 void pollSelectionKeys(Set<SelectionKey> selectionKeys, boolean isImmediatelyConnected, long currentTimeNanos)
{    for (SelectionKey key : selectionKeys) {        KafkaChannel channel = (KafkaChannel) key.attachment();        if (channel != null && channel.id().equals(node1))            node1Polls.incrementAndGet();    }    super.pollSelectionKeys(selectionKeys, isImmediatelyConnected, currentTimeNanos);}
0
private Path getLatestConfigPath() throws IOException
{    FileStatus[] fileStatuses = fileSystem.listStatus(this.schedulerConfDir, this.configFilePathFilter);    if (fileStatuses == null || fileStatuses.length == 0) {        return null;    }    Arrays.sort(fileStatuses);    return fileStatuses[fileStatuses.length - 1].getPath();}
0
public boolean isEmpty()
{    return map.isEmpty();}
0
 List<SchedulingResultPartition> markSchedulingResultPartitionFinished(SchedulingResultPartition srp)
{    return intermediateDataSetManager.markSchedulingResultPartitionFinished(srp);}
0
private void addContainersToDecreaseToProto()
{    maybeInitBuilder();    builder.clearContainersToDecrease();    if (this.containersToDecrease == null) {        return;    }    Iterable<ContainerProto> iterable = new Iterable<ContainerProto>() {        @Override        public Iterator<ContainerProto> iterator() {            return new Iterator<ContainerProto>() {                private Iterator<Container> iter = containersToDecrease.iterator();                @Override                public boolean hasNext() {                    return iter.hasNext();                }                @Override                public ContainerProto next() {                    return convertToProtoFormat(iter.next());                }                @Override                public void remove() {                    throw new UnsupportedOperationException();                }            };        }    };    builder.addAllContainersToDecrease(iterable);}
0
public void setContractStateClass(Class<ContractState> contractStateClass)
{    this.contractStateClass = contractStateClass;}
0
public static org.apache.drill.exec.proto.UserProtos.Property parseDelimitedFrom(java.io.InputStream input) throws java.io.IOException
{    return com.google.protobuf.GeneratedMessageV3.parseDelimitedWithIOException(PARSER, input);}
0
public void validate() throws org.apache.thrift.TException
{        if (success != null) {        success.validate();    }}
0
public Cancellable schedule(final PunctuationSchedule sched)
{    synchronized (pq) {        pq.add(sched);    }    return sched.cancellable();}
0
public Producer getFieldInjectedProducer()
{    return fieldInjectedProducer;}
0
public static T withStreams(Socket socket, @ClosureParams(value = SimpleType.class, options = { "java.io.InputStream", "java.io.OutputStream" }) Closure<T> closure) throws IOException
{    InputStream input = socket.getInputStream();    OutputStream output = socket.getOutputStream();    try {        T result = closure.call(input, output);        InputStream temp1 = input;        input = null;        temp1.close();        OutputStream temp2 = output;        output = null;        temp2.close();        return result;    } finally {        closeWithWarning(input);        closeWithWarning(output);    }}
0
public void setSpouts(List<SpoutDef> spouts)
{    this.spoutMap = new LinkedHashMap<String, SpoutDef>();    for (SpoutDef spout : spouts) {        this.spoutMap.put(spout.getId(), spout);    }}
0
public Set<ServerConnection> getAllServerConnections()
{    return Collections.unmodifiableSet(allSCs);}
0
public void testApplyAttemptedCountersQueryOneNamespace()
{    metrics.updatePhysical(bundle1, MetricUpdates.create(ImmutableList.of(MetricUpdate.create(MetricKey.create("step1", NAME1), 5L), MetricUpdate.create(MetricKey.create("step1", NAME3), 8L)), ImmutableList.of(), ImmutableList.of()));    metrics.updatePhysical(bundle1, MetricUpdates.create(ImmutableList.of(MetricUpdate.create(MetricKey.create("step2", NAME1), 7L), MetricUpdate.create(MetricKey.create("step1", NAME3), 4L)), ImmutableList.of(), ImmutableList.of()));    MetricQueryResults results = metrics.queryMetrics(MetricsFilter.builder().addNameFilter(inNamespace("ns1")).build());    assertThat(results.getCounters(), containsInAnyOrder(attemptedMetricsResult("ns1", "name1", "step1", 5L), attemptedMetricsResult("ns1", "name1", "step2", 7L)));    assertThat(results.getCounters(), containsInAnyOrder(committedMetricsResult("ns1", "name1", "step1", 0L), committedMetricsResult("ns1", "name1", "step2", 0L)));}
0
private void getDroplets(Exchange exchange) throws Exception
{    Droplets droplets = getEndpoint().getDigitalOceanClient().getAvailableDroplets(configuration.getPage(), configuration.getPerPage());    LOG.trace("All Droplets : page {} / {} per page [{}] ", configuration.getPage(), configuration.getPerPage(), droplets.getDroplets());    exchange.getOut().setBody(droplets.getDroplets());}
0
public void writeTo(StreamOutput out) throws IOException
{    ExpressionParser.writeExpressionList(elements, out);}
0
public void clear()
{    this.io = null;}
0
public void schedule(Topologies topologies, Cluster cluster)
{    List<TopologyDetails> isoTopologies = isolatedTopologies(topologies.getTopologies());    Set<String> isoIds = extractTopologyIds(isoTopologies);    Map<String, Set<Set<ExecutorDetails>>> topologyWorkerSpecs = topologyWorkerSpecs(isoTopologies);    Map<String, Map<Integer, Integer>> topologyMachineDistributions = topologyMachineDistributions(isoTopologies);    Map<String, List<AssignmentInfo>> hostAssignments = hostAssignments(cluster);    for (Map.Entry<String, List<AssignmentInfo>> entry : hostAssignments.entrySet()) {        List<AssignmentInfo> assignments = entry.getValue();        String topologyId = assignments.get(0).getTopologyId();        Map<Integer, Integer> distribution = topologyMachineDistributions.get(topologyId);        Set<Set<ExecutorDetails>> workerSpecs = topologyWorkerSpecs.get(topologyId);        int numWorkers = assignments.size();        if (isoIds.contains(topologyId) && checkAssignmentTopology(assignments, topologyId) && distribution.containsKey(numWorkers) && checkAssignmentWorkerSpecs(assignments, workerSpecs)) {            decrementDistribution(distribution, numWorkers);            for (AssignmentInfo ass : assignments) {                workerSpecs.remove(ass.getExecutors());            }            cluster.blacklistHost(entry.getKey());        } else {            for (AssignmentInfo ass : assignments) {                if (isoIds.contains(ass.getTopologyId())) {                    cluster.freeSlot(ass.getWorkerSlot());                }            }        }    }    Map<String, Set<WorkerSlot>> hostUsedSlots = hostToUsedSlots(cluster);    LinkedList<HostAssignableSlots> hss = hostAssignableSlots(cluster);    for (Map.Entry<String, Set<Set<ExecutorDetails>>> entry : topologyWorkerSpecs.entrySet()) {        String topologyId = entry.getKey();        Set<Set<ExecutorDetails>> executorSet = entry.getValue();        List<Integer> workerNum = distributionToSortedAmounts(topologyMachineDistributions.get(topologyId));        for (Integer num : workerNum) {            HostAssignableSlots hostSlots = hss.peek();            List<WorkerSlot> slot = hostSlots != null ? hostSlots.getWorkerSlots() : null;            if (slot != null && slot.size() >= num) {                hss.poll();                cluster.freeSlots(hostUsedSlots.get(hostSlots.getHostName()));                for (WorkerSlot tmpSlot : slot.subList(0, num)) {                    Set<ExecutorDetails> executor = removeElemFromExecutorsSet(executorSet);                    cluster.assign(tmpSlot, topologyId, executor);                }                cluster.blacklistHost(hostSlots.getHostName());            }        }    }    List<String> failedTopologyIds = extractFailedTopologyIds(topologyWorkerSpecs);    if (failedTopologyIds.size() > 0) {                        Map<String, Set<WorkerSlot>> usedSlots = hostToUsedSlots(cluster);        Set<Map.Entry<String, Set<WorkerSlot>>> entries = usedSlots.entrySet();        for (Map.Entry<String, Set<WorkerSlot>> entry : entries) {            if (!cluster.isBlacklistedHost(entry.getKey())) {                cluster.freeSlots(entry.getValue());            }        }    } else {                Set<String> allocatedTopologies = allocatedTopologies(topologyWorkerSpecs);        Topologies leftOverTopologies = leftoverTopologies(topologies, allocatedTopologies);        DefaultScheduler.defaultSchedule(leftOverTopologies, cluster);    }    Set<String> origBlacklist = cluster.getBlacklistedHosts();    cluster.setBlacklistedHosts(origBlacklist);}
1
public void startupConfigurationIsLoggedToFileOnlyOnce()
{    LogFileAssert.assertThat(mainLogFile).containsOnlyOnce(startupConfiguration);}
0
public String getIp6Dns2()
{    return ip6Dns2;}
0
public void deleteTestTopic(String topic)
{    ZkUtils zkUtils = getZkUtils();    try {                ZkClient zk = new ZkClient(zookeeperConnectionString, Integer.valueOf(standardProps.getProperty("zookeeper.session.timeout.ms")), Integer.valueOf(standardProps.getProperty("zookeeper.connection.timeout.ms")), new ZooKeeperStringSerializer());        AdminUtils.deleteTopic(zkUtils, topic);        zk.close();    } finally {        zkUtils.close();    }}
1
public static void grant(Connection connection, final String namespace, final String userName, final Permission.Action... actions) throws Throwable
{    grant(connection, namespace, userName, true, actions);}
0
 void configure(Map<String, ?> configs, boolean isKey)
{}
0
public void testRetentionPolicyChangeDuringRecovery() throws Exception
{    try (ReplicationGroup shards = createGroup(0)) {        shards.startPrimary();        shards.indexDocs(10);        getTranslog(shards.getPrimary()).rollGeneration();        shards.flush();        shards.indexDocs(10);        final IndexShard replica = shards.addReplica();        final CountDownLatch recoveryBlocked = new CountDownLatch(1);        final CountDownLatch releaseRecovery = new CountDownLatch(1);        Future<Void> future = shards.asyncRecoverReplica(replica, (indexShard, node) -> new RecoveryDuringReplicationTests.BlockingTarget(RecoveryState.Stage.TRANSLOG, recoveryBlocked, releaseRecovery, indexShard, node, recoveryListener, logger));        recoveryBlocked.await();        IndexMetaData.Builder builder = IndexMetaData.builder(replica.indexSettings().getIndexMetaData());        builder.settings(Settings.builder().put(replica.indexSettings().getSettings()).put(IndexSettings.INDEX_TRANSLOG_RETENTION_AGE_SETTING.getKey(), "-1").put(IndexSettings.INDEX_TRANSLOG_RETENTION_SIZE_SETTING.getKey(), "-1").put(IndexSettings.INDEX_TRANSLOG_FLUSH_THRESHOLD_SIZE_SETTING.getKey(), "100b"));        replica.indexSettings().updateIndexMetaData(builder.build());        replica.onSettingsChanged();        releaseRecovery.countDown();        future.get();                assertBusy(() -> {            assertThat(replica.getLastSyncedGlobalCheckpoint(), equalTo(19L));            assertThat(getTranslog(replica).totalOperations(), equalTo(0));        });    }}
0
public static CumulativeSumPipelineAggregationBuilder parse(String pipelineAggregatorName, XContentParser parser) throws IOException
{    XContentParser.Token token;    String currentFieldName = null;    String[] bucketsPaths = null;    String format = null;    while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {        if (token == XContentParser.Token.FIELD_NAME) {            currentFieldName = parser.currentName();        } else if (token == XContentParser.Token.VALUE_STRING) {            if (FORMAT.match(currentFieldName, parser.getDeprecationHandler())) {                format = parser.text();            } else if (BUCKETS_PATH.match(currentFieldName, parser.getDeprecationHandler())) {                bucketsPaths = new String[] { parser.text() };            } else {                throw new ParsingException(parser.getTokenLocation(), "Unknown key for a " + token + " in [" + pipelineAggregatorName + "]: [" + currentFieldName + "].");            }        } else if (token == XContentParser.Token.START_ARRAY) {            if (BUCKETS_PATH.match(currentFieldName, parser.getDeprecationHandler())) {                List<String> paths = new ArrayList<>();                while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {                    String path = parser.text();                    paths.add(path);                }                bucketsPaths = paths.toArray(new String[paths.size()]);            } else {                throw new ParsingException(parser.getTokenLocation(), "Unknown key for a " + token + " in [" + pipelineAggregatorName + "]: [" + currentFieldName + "].");            }        } else {            throw new ParsingException(parser.getTokenLocation(), "Unexpected token " + token + " in [" + pipelineAggregatorName + "].");        }    }    if (bucketsPaths == null) {        throw new ParsingException(parser.getTokenLocation(), "Missing required field [" + BUCKETS_PATH.getPreferredName() + "] for derivative aggregation [" + pipelineAggregatorName + "]");    }    CumulativeSumPipelineAggregationBuilder factory = new CumulativeSumPipelineAggregationBuilder(pipelineAggregatorName, bucketsPaths[0]);    if (format != null) {        factory.format(format);    }    return factory;}
0
public void setPassConnectAttempt(int num)
{    passSocketConnectOnAttempt = num;}
0
public boolean consistentWithEquals()
{    for (Coder<?> keyComponentCoder : keyComponentCoders) {        if (!keyComponentCoder.consistentWithEquals()) {            return false;        }    }    return valueCoder.consistentWithEquals();}
0
 Index getIndex()
{    return index;}
0
public void testGet()
{    DoubleMaximum max = new DoubleMaximum();    assertEquals(Double.NEGATIVE_INFINITY, max.getLocalValue(), 0.0);}
0
public Object visitLogicalNot(LogicalNotContext ctx)
{    return new Not(source(ctx), expression(ctx.booleanExpression()));}
0
public Iterator<Entry<Key, Value>> iterator(Transaction tx) throws IOException, UnsupportedOperationException
{    throw new UnsupportedOperationException();}
0
public static SocketInputWrapper getInputStream(Socket socket, long timeout) throws IOException
{    InputStream stm = (socket.getChannel() == null) ? socket.getInputStream() : new SocketInputStream(socket);    SocketInputWrapper w = new SocketInputWrapper(socket, stm);    w.setTimeout(timeout);    return w;}
0
 void setSharedCacheUploadPolicies(Map<String, Boolean> policies)
{    this.sharedCacheUploadPolicies = policies;}
0
public void setMockedAbstractBucketRegionQueue(BucketRegionQueue mocked)
{    this.mockedAbstractBucketRegionQueue = mocked;}
0
public static TestUtil getInstance()
{    return instance;}
0
public static byte[] serializeJobConf(JobConf jobConf)
{    ByteArrayOutputStream out = new ByteArrayOutputStream();    try {        jobConf.write(new DataOutputStream(out));    } catch (IOException e) {                return null;    } finally {        try {            out.close();        } catch (IOException e) {                    }    }    return out.toByteArray();}
1
public MaterializedField schema()
{    MaterializedField field = emptySchema();    for (MaterializedField member : schema.toFieldList()) {        field.addChild(member);    }    return field;}
0
public T first(FunctionExpression<Predicate1<T>> predicate)
{    return EnumerableDefaults.first(getThis(), predicate.getFunction());}
0
 void connect() throws Exception
{    final Promise<Object> promise = new Promise<>();    connection.connect(new Callback<Void>() {        public void onSuccess(Void value) {                        Topic[] topics = createSubscribeTopics();            if (topics != null && topics.length > 0) {                connection.subscribe(topics, new Callback<byte[]>() {                    public void onSuccess(byte[] value) {                        promise.onSuccess(value);                        connected = true;                    }                    public void onFailure(Throwable value) {                                                promise.onFailure(value);                        connection.disconnect(null);                        connected = false;                    }                });            } else {                promise.onSuccess(value);                connected = true;            }        }        public void onFailure(Throwable value) {                                    promise.onFailure(value);            connection.disconnect(null);            connected = false;        }    });        promise.await(configuration.getConnectWaitInSeconds(), TimeUnit.SECONDS);}
1
private static DatanodeDetails randomDatanodeDetails()
{    String ipAddress = "127.0.0.1";    DatanodeDetails.Port containerPort = DatanodeDetails.newPort(DatanodeDetails.Port.Name.STANDALONE, 0);    DatanodeDetails.Port ratisPort = DatanodeDetails.newPort(DatanodeDetails.Port.Name.RATIS, 0);    DatanodeDetails.Port restPort = DatanodeDetails.newPort(DatanodeDetails.Port.Name.REST, 0);    DatanodeDetails.Builder builder = DatanodeDetails.newBuilder();    builder.setUuid(UUID.randomUUID().toString()).setHostName("localhost").setIpAddress(ipAddress).addPort(containerPort).addPort(ratisPort).addPort(restPort);    return builder.build();}
0
public void setMemoryUsageThreshold(final Long memUsed, final Long memTotal, final Double threshold)
{    if (memUsed != null && memTotal != null && threshold != null && memTotal != 0) {        this.memoryThresholdExceeded = (1.0 * memUsed / memTotal) > threshold;    }}
0
public void testSubstractFrom()
{    assertEquals(toMillis("2016-09-21T00:00:00"), Duration.from("0m").substractFrom(toMillis("2016-09-21T00:00:00")));    assertEquals(toMillis("2016-09-21T00:00:00"), Duration.from("10us").substractFrom(toMillis("2016-09-21T00:00:00")));    assertEquals(toMillis("2016-09-20T23:50:00"), Duration.from("10m").substractFrom(toMillis("2016-09-21T00:00:00")));    assertEquals(toMillis("2016-09-20T22:30:00"), Duration.from("90m").substractFrom(toMillis("2016-09-21T00:00:00")));    assertEquals(toMillis("2016-09-20T21:50:00"), Duration.from("2h10m").substractFrom(toMillis("2016-09-21T00:00:00")));    assertEquals(toMillis("2016-09-18T23:50:00"), Duration.from("2d10m").substractFrom(toMillis("2016-09-21T00:00:00")));    assertEquals(toMillis("2016-09-17T23:00:00"), Duration.from("2d25h").substractFrom(toMillis("2016-09-21T00:00:00")));    assertEquals(toMillis("2016-08-21T00:00:00"), Duration.from("1mo").substractFrom(toMillis("2016-09-21T00:00:00")));    assertEquals(toMillis("2015-07-21T00:00:00"), Duration.from("14mo").substractFrom(toMillis("2016-09-21T00:00:00")));    assertEquals(toMillis("2015-02-28T00:00:00"), Duration.from("12mo").substractFrom(toMillis("2016-02-29T00:00:00")));}
0
public void returnComponentLogger(String role)
{}
0
public void handleException(TransportException e)
{    if (isDone.compareAndSet(false, true)) {        listener.onFailure(new IllegalStateException("handshake failed", e));    }}
0
public void testCommitOffsetRequestAsyncWithFencedInstanceIdException()
{    receiveFencedInstanceIdException();}
0
public void testAppsQueryQueueOneFinishedApp() throws Exception
{    rm.start();    MockNM amNodeManager = rm.registerNode("127.0.0.1:1234", 2048);    RMApp finishedApp = rm.submitApp(CONTAINER_MB);    RMApp runningApp = rm.submitApp(CONTAINER_MB);    amNodeManager.nodeHeartbeat(true);    finishApp(amNodeManager, finishedApp);    WebResource r = resource();    ClientResponse response = r.path("ws").path("v1").path("cluster").path("apps").queryParam("queue", "default").accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);    assertEquals(MediaType.APPLICATION_JSON_TYPE + "; " + JettyUtils.UTF_8, response.getType().toString());    JSONObject json = response.getEntity(JSONObject.class);    assertEquals("incorrect number of elements", 1, json.length());    JSONObject apps = json.getJSONObject("apps");    assertEquals("incorrect number of elements", 1, apps.length());    JSONArray array = apps.getJSONArray("app");    Set<String> appIds = getApplicationIds(array);    assertTrue("Running app should be in the result list!", appIds.contains(runningApp.getApplicationId().toString()));    assertTrue("Finished app should be in the result list!", appIds.contains(finishedApp.getApplicationId().toString()));    assertEquals("incorrect number of elements", 2, array.length());    rm.stop();}
0
public boolean isSatisified() throws Exception
{    return consumed.get() == numMessagesToSend;}
0
public static void setUpBeforeClass() throws Exception
{    UTIL.startMiniCluster(2);}
0
public void postAdded()
{    ObservingContext context = observingContext.get();    if (context == null) {                return;    }    ClusterState newState = clusterApplierService.state();    if (lastObservedState.get().isOlderOrDifferentMaster(newState) && context.statePredicate.test(newState)) {                if (observingContext.compareAndSet(context, null)) {            logger.trace("observer: post adding listener: accepting current cluster state ({})", newState);            clusterApplierService.removeTimeoutListener(this);            lastObservedState.set(new StoredState(newState));            context.listener.onNewClusterState(newState);        } else {            logger.trace("observer: postAdded - predicate approved state but observing context has changed - ignoring ({})", newState);        }    } else {        logger.trace("observer: postAdded - predicate rejected state ({})", newState);    }}
0
public int depth()
{    return pos;}
0
public static InputStream genericFileToInputStream(GenericFile<?> file, Exchange exchange) throws IOException
{    if (file.getFile() instanceof File) {                File f = (File) file.getFile();                if (f.exists()) {                        String charset = file.getCharset();            if (charset != null) {                            } else {                            }            return IOHelper.toInputStream(f, charset);        }    }    if (exchange != null) {                file.getBinding().loadContent(exchange, file);        return exchange.getContext().getTypeConverter().convertTo(InputStream.class, exchange, file.getBody());    } else {                return null;    }}
1
private void readObject(java.io.ObjectInputStream in) throws java.io.IOException, ClassNotFoundException
{    try {                __isset_bitfield = 0;        read(new org.apache.thrift.protocol.TCompactProtocol(new org.apache.thrift.transport.TIOStreamTransport(in)));    } catch (org.apache.thrift.TException te) {        throw new java.io.IOException(te);    }}
0
public void checkCastShouldReturnCorrectlyWhenTargetObjectIsNotNullAndCanBeTypeCasted()
{    Object stringCastTarget = "SomeCharacters";    Object stringCastResult = TypeUtils.checkCast(stringCastTarget, String.class);    assertThat(stringCastResult).isNotNull();    assertThat(stringCastResult).isInstanceOf(String.class);    assertThat(stringCastResult).isSameAs(stringCastTarget);    Object integerCastTarget = 20;    Object integerCastResult = TypeUtils.checkCast(integerCastTarget, Integer.class);    assertThat(integerCastResult).isNotNull();    assertThat(integerCastResult).isInstanceOf(Integer.class);    assertThat(integerCastResult).isSameAs(integerCastResult);    Object numberCastResult = TypeUtils.checkCast(integerCastTarget, Number.class);    assertThat(numberCastResult).isNotNull();    assertThat(numberCastResult).isInstanceOf(Integer.class);    assertThat(numberCastResult).isSameAs(numberCastResult);}
0
 static String resolveFailureReason(HttpResponse response)
{    int status = response.status();    if (status < 300) {        return null;    }    StringBuilder message = new StringBuilder();    switch(status) {        case HttpStatus.SC_BAD_REQUEST:            message.append("Bad Request");            break;        case HttpStatus.SC_UNAUTHORIZED:            message.append("Unauthorized (authentication credentials are invalid)");            break;        case HttpStatus.SC_FORBIDDEN:            message.append("Forbidden (account doesn't have permission to create this issue)");            break;        case HttpStatus.SC_NOT_FOUND:            message.append("Not Found (account uses invalid JIRA REST APIs)");            break;        case HttpStatus.SC_REQUEST_TIMEOUT:            message.append("Request Timeout (request took too long to process)");            break;        case HttpStatus.SC_INTERNAL_SERVER_ERROR:            message.append("JIRA Server Error (internal error occurred while processing request)");            break;        default:            message.append("Unknown Error");            break;    }    if (response.hasContent()) {        final List<String> errors = new ArrayList<>();                try (InputStream stream = response.body().streamInput();            XContentParser parser = JsonXContent.jsonXContent.createParser(NamedXContentRegistry.EMPTY, LoggingDeprecationHandler.INSTANCE, stream)) {            XContentParser.Token token = parser.currentToken();            if (token == null) {                token = parser.nextToken();            }            if (token != XContentParser.Token.START_OBJECT) {                throw new ElasticsearchParseException("failed to parse jira project. expected an object, but found [{}] instead", token);            }            String currentFieldName = null;            while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {                if (token == XContentParser.Token.FIELD_NAME) {                    currentFieldName = parser.currentName();                } else if (Field.ERRORS.match(currentFieldName, parser.getDeprecationHandler())) {                    Map<String, Object> fieldErrors = parser.mapOrdered();                    for (Map.Entry<String, Object> entry : fieldErrors.entrySet()) {                        errors.add("Field [" + entry.getKey() + "] has error [" + String.valueOf(entry.getValue()) + "]");                    }                } else if (Field.ERROR_MESSAGES.match(currentFieldName, parser.getDeprecationHandler())) {                    while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {                        errors.add(parser.text());                    }                } else {                    throw new ElasticsearchParseException("could not parse jira response. unexpected field [{}]", currentFieldName);                }            }        } catch (Exception e) {            errors.add("Exception when parsing jira response [" + String.valueOf(e) + "]");        }        if (errors.isEmpty() == false) {            message.append(" - ");            for (String error : errors) {                message.append(error).append('\n');            }        }    }    return message.toString();}
0
 ArrayList<Path> getRegionList(Path tableArchivePath) throws IOException
{    ArrayList<Path> regionDirList = new ArrayList<>();    FileStatus[] children = fs.listStatus(tableArchivePath);    for (FileStatus childStatus : children) {                Path child = childStatus.getPath();        regionDirList.add(child);    }    return regionDirList;}
0
public void setNext(final EvictionNode nextEvictionNode)
{    this.nextEvictionNode = nextEvictionNode;}
0
public static void tearDown()
{    jedis.close();    cache.close();    server.shutdown();}
0
private boolean isSeparator(final char ch, final char[] separators)
{    for (char separator : separators) {        if (ch == separator) {            return true;        }    }    return false;}
0
public void testCoderContext() throws Exception
{    String input = "hello world";    Coder<String> coder = StringUtf8Coder.of();    ByteBuffer encoded = FlinkKeyUtils.encodeKey(input, coder);        assertThat(encoded.array(), is(CoderUtils.encodeToByteArray(coder, input, Coder.Context.NESTED)));}
0
public boolean equalToReference(IntPair candidate)
{    return candidate.getKey() == this.reference;}
0
public int hashCode()
{    List<Object> list = new ArrayList<Object>();    boolean present_db_name = true && (isSetDb_name());    list.add(present_db_name);    if (present_db_name)        list.add(db_name);    boolean present_tbl_name = true && (isSetTbl_name());    list.add(present_tbl_name);    if (present_tbl_name)        list.add(tbl_name);    boolean present_part_vals = true && (isSetPart_vals());    list.add(present_part_vals);    if (present_part_vals)        list.add(part_vals);    boolean present_eventType = true && (isSetEventType());    list.add(present_eventType);    if (present_eventType)        list.add(eventType.getValue());    return list.hashCode();}
0
public void unregisterInstallHandler()
{    this.installHandler = null;}
0
public void setListener(AsyncListener listener)
{    this.listener = listener;}
0
private synchronized void init()
{    if (openFiles == null) {        openFiles = new HashMap<>();        openFilesDeleted = new HashSet<>();    }    if (createdFiles == null)        createdFiles = new HashSet<>();    if (unSyncedFiles == null)        unSyncedFiles = new HashSet<>();}
0
public CompletableFuture<Void> getTerminationFuture()
{    return terminationFuture;}
0
private void configureAndTestInternalCanHandle(boolean sPool0IsManaged, boolean sPool1IsManaged, StrategyPriority expectedStrategyPriority)
{    VolumeObject volumeInfo = Mockito.spy(new VolumeObject());    Mockito.doReturn(0l).when(volumeInfo).getPoolId();    DataStore ds = Mockito.spy(new PrimaryDataStoreImpl());    Mockito.doReturn(1l).when(ds).getId();    Map<VolumeInfo, DataStore> volumeMap = new HashMap<>();    volumeMap.put(volumeInfo, ds);    StoragePoolVO storagePool0 = Mockito.spy(new StoragePoolVO());    Mockito.doReturn(sPool0IsManaged).when(storagePool0).isManaged();    StoragePoolVO storagePool1 = Mockito.spy(new StoragePoolVO());    Mockito.doReturn(sPool1IsManaged).when(storagePool1).isManaged();    Mockito.doReturn(storagePool0).when(primaryDataStoreDao).findById(0l);    Mockito.doReturn(storagePool1).when(primaryDataStoreDao).findById(1l);    StrategyPriority strategyPriority = strategy.internalCanHandle(volumeMap, new HostVO("srcHostUuid"), new HostVO("destHostUuid"));    Assert.assertEquals(expectedStrategyPriority, strategyPriority);}
0
public long writeCommit(CommitUpdateCommand cmd)
{    LogCodec codec = new LogCodec(resolver);    synchronized (this) {        try {                        long pos = fos.size();            if (pos == 0) {                writeLogHeader(codec);                pos = fos.size();            }            codec.init(fos);            codec.writeTag(JavaBinCodec.ARR, 3);                        codec.writeInt(UpdateLog.COMMIT);            codec.writeLong(cmd.getVersion());                        codec.writeStr(END_MESSAGE);            endRecord(pos);                        ensureFlushed();                        closeOutput();            return pos;        } catch (IOException e) {            throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);        }    }}
0
protected void setUserInformation(String userInfo)
{    if (userInfo != null) {        String[] userPass = userInfo.split(":");        if (userPass.length > 0)            this.ftpUser = userPass[0];        if (userPass.length > 1)            this.ftpPass = userPass[1];    } else {        this.ftpUser = "anonymous";        this.ftpPass = "anonymous";    }}
0
public void setAutoStartup(Boolean autoStartup)
{    this.autoStartup = autoStartup;}
0
protected void afterPipelineExecution()
{    runVisitedNodes = recordPipelineNodes(pipeline);    super.afterPipelineExecution();}
0
public void checkIntegrity()
{}
0
public static javax.security.auth.login.Configuration createServerConfig(String principal, File keytab)
{    return new KerberosConfiguration(principal, keytab, false);}
0
public JobConf obtainJobConf(MiniMRCluster cluster)
{    try {        Method meth = MiniMRCluster.class.getMethod("getJobTrackerConf", emptyParam);        return (JobConf) meth.invoke(cluster, new Object[] {});    } catch (NoSuchMethodException nsme) {        return null;    } catch (InvocationTargetException ite) {        return null;    } catch (IllegalAccessException iae) {        return null;    }}
0
private NodeAttributeKeyPBImpl convertFromProtoFormat(NodeAttributeKeyProto attributeKeyProto)
{    return new NodeAttributeKeyPBImpl(attributeKeyProto);}
0
 AclStatus getAclStatus(String src) throws IOException
{    final String operationName = "getAclStatus";    checkOperation(OperationCategory.READ);    final AclStatus ret;    final FSPermissionChecker pc = getPermissionChecker();    try {        readLock();        try {            checkOperation(OperationCategory.READ);            ret = FSDirAclOp.getAclStatus(dir, pc, src);        } finally {            readUnlock(operationName);        }    } catch (AccessControlException ace) {        logAuditEvent(false, operationName, src);        throw ace;    }    logAuditEvent(true, operationName, src);    return ret;}
0
 static List<String> concat(List<String> l1, List<String> l2)
{    List<String> ret = new ArrayList<String>(l1.size() + l2.size());    ret.addAll(l1);    ret.addAll(l2);    return ret;}
0
public String toString()
{    StringBuilder sb = new StringBuilder("get_table_column_statistics_result(");    boolean first = true;    sb.append("success:");    if (this.success == null) {        sb.append("null");    } else {        sb.append(this.success);    }    first = false;    if (!first)        sb.append(", ");    sb.append("o1:");    if (this.o1 == null) {        sb.append("null");    } else {        sb.append(this.o1);    }    first = false;    if (!first)        sb.append(", ");    sb.append("o2:");    if (this.o2 == null) {        sb.append("null");    } else {        sb.append(this.o2);    }    first = false;    if (!first)        sb.append(", ");    sb.append("o3:");    if (this.o3 == null) {        sb.append("null");    } else {        sb.append(this.o3);    }    first = false;    if (!first)        sb.append(", ");    sb.append("o4:");    if (this.o4 == null) {        sb.append("null");    } else {        sb.append(this.o4);    }    first = false;    sb.append(")");    return sb.toString();}
0
public void testSkipTo() throws IOException
{    Directory dir = newDirectory();    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy()));    Term ta = new Term("content", "aaa");    for (int i = 0; i < 10; i++) addDoc(writer, "aaa aaa aaa aaa");    Term tb = new Term("content", "bbb");    for (int i = 0; i < 16; i++) addDoc(writer, "bbb bbb bbb bbb");    Term tc = new Term("content", "ccc");    for (int i = 0; i < 50; i++) addDoc(writer, "ccc ccc ccc ccc");        writer.forceMerge(1);    writer.close();    IndexReader reader = DirectoryReader.open(dir);    PostingsEnum tdocs = TestUtil.docs(random(), reader, ta.field(), new BytesRef(ta.text()), null, PostingsEnum.FREQS);            assertTrue(tdocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);    assertEquals(0, tdocs.docID());    assertEquals(4, tdocs.freq());    assertTrue(tdocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);    assertEquals(1, tdocs.docID());    assertEquals(4, tdocs.freq());    assertTrue(tdocs.advance(2) != DocIdSetIterator.NO_MORE_DOCS);    assertEquals(2, tdocs.docID());    assertTrue(tdocs.advance(4) != DocIdSetIterator.NO_MORE_DOCS);    assertEquals(4, tdocs.docID());    assertTrue(tdocs.advance(9) != DocIdSetIterator.NO_MORE_DOCS);    assertEquals(9, tdocs.docID());    assertFalse(tdocs.advance(10) != DocIdSetIterator.NO_MORE_DOCS);        tdocs = TestUtil.docs(random(), reader, ta.field(), new BytesRef(ta.text()), null, 0);    assertTrue(tdocs.advance(0) != DocIdSetIterator.NO_MORE_DOCS);    assertEquals(0, tdocs.docID());    assertTrue(tdocs.advance(4) != DocIdSetIterator.NO_MORE_DOCS);    assertEquals(4, tdocs.docID());    assertTrue(tdocs.advance(9) != DocIdSetIterator.NO_MORE_DOCS);    assertEquals(9, tdocs.docID());    assertFalse(tdocs.advance(10) != DocIdSetIterator.NO_MORE_DOCS);            tdocs = TestUtil.docs(random(), reader, tb.field(), new BytesRef(tb.text()), null, PostingsEnum.FREQS);    assertTrue(tdocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);    assertEquals(10, tdocs.docID());    assertEquals(4, tdocs.freq());    assertTrue(tdocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);    assertEquals(11, tdocs.docID());    assertEquals(4, tdocs.freq());    assertTrue(tdocs.advance(12) != DocIdSetIterator.NO_MORE_DOCS);    assertEquals(12, tdocs.docID());    assertTrue(tdocs.advance(15) != DocIdSetIterator.NO_MORE_DOCS);    assertEquals(15, tdocs.docID());    assertTrue(tdocs.advance(24) != DocIdSetIterator.NO_MORE_DOCS);    assertEquals(24, tdocs.docID());    assertTrue(tdocs.advance(25) != DocIdSetIterator.NO_MORE_DOCS);    assertEquals(25, tdocs.docID());    assertFalse(tdocs.advance(26) != DocIdSetIterator.NO_MORE_DOCS);        tdocs = TestUtil.docs(random(), reader, tb.field(), new BytesRef(tb.text()), null, PostingsEnum.FREQS);    assertTrue(tdocs.advance(5) != DocIdSetIterator.NO_MORE_DOCS);    assertEquals(10, tdocs.docID());    assertTrue(tdocs.advance(15) != DocIdSetIterator.NO_MORE_DOCS);    assertEquals(15, tdocs.docID());    assertTrue(tdocs.advance(24) != DocIdSetIterator.NO_MORE_DOCS);    assertEquals(24, tdocs.docID());    assertTrue(tdocs.advance(25) != DocIdSetIterator.NO_MORE_DOCS);    assertEquals(25, tdocs.docID());    assertFalse(tdocs.advance(26) != DocIdSetIterator.NO_MORE_DOCS);            tdocs = TestUtil.docs(random(), reader, tc.field(), new BytesRef(tc.text()), null, PostingsEnum.FREQS);    assertTrue(tdocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);    assertEquals(26, tdocs.docID());    assertEquals(4, tdocs.freq());    assertTrue(tdocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);    assertEquals(27, tdocs.docID());    assertEquals(4, tdocs.freq());    assertTrue(tdocs.advance(28) != DocIdSetIterator.NO_MORE_DOCS);    assertEquals(28, tdocs.docID());    assertTrue(tdocs.advance(40) != DocIdSetIterator.NO_MORE_DOCS);    assertEquals(40, tdocs.docID());    assertTrue(tdocs.advance(57) != DocIdSetIterator.NO_MORE_DOCS);    assertEquals(57, tdocs.docID());    assertTrue(tdocs.advance(74) != DocIdSetIterator.NO_MORE_DOCS);    assertEquals(74, tdocs.docID());    assertTrue(tdocs.advance(75) != DocIdSetIterator.NO_MORE_DOCS);    assertEquals(75, tdocs.docID());    assertFalse(tdocs.advance(76) != DocIdSetIterator.NO_MORE_DOCS);        tdocs = TestUtil.docs(random(), reader, tc.field(), new BytesRef(tc.text()), null, 0);    assertTrue(tdocs.advance(5) != DocIdSetIterator.NO_MORE_DOCS);    assertEquals(26, tdocs.docID());    assertTrue(tdocs.advance(40) != DocIdSetIterator.NO_MORE_DOCS);    assertEquals(40, tdocs.docID());    assertTrue(tdocs.advance(57) != DocIdSetIterator.NO_MORE_DOCS);    assertEquals(57, tdocs.docID());    assertTrue(tdocs.advance(74) != DocIdSetIterator.NO_MORE_DOCS);    assertEquals(74, tdocs.docID());    assertTrue(tdocs.advance(75) != DocIdSetIterator.NO_MORE_DOCS);    assertEquals(75, tdocs.docID());    assertFalse(tdocs.advance(76) != DocIdSetIterator.NO_MORE_DOCS);    reader.close();    dir.close();}
0
public Statistic getStatistic()
{    List<RelFieldCollation> collationFields = new ArrayList<>();    for (Integer key : pkColumns) {        collationFields.add(new RelFieldCollation(key, RelFieldCollation.Direction.ASCENDING, RelFieldCollation.NullDirection.LAST));    }    return Statistics.of(data.size(), ImmutableList.of(pkColumns), ImmutableList.of(RelCollations.of(collationFields)));}
0
public static RPR_I03 toRprI03(String body) throws HL7Exception
{    return toMessage(RPR_I03.class, body);}
0
public SqlServerSinkHadoop1 createInstance() throws InstantiationException, IllegalAccessException
{    return new SqlServerSinkHadoop1();}
0
public static DatanodeRegistrationProto convert(DatanodeRegistration registration)
{    DatanodeRegistrationProto.Builder builder = DatanodeRegistrationProto.newBuilder();    return builder.setDatanodeID(PBHelperClient.convert((DatanodeID) registration)).setStorageInfo(convert(registration.getStorageInfo())).setKeys(convert(registration.getExportedKeys())).setSoftwareVersion(registration.getSoftwareVersion()).build();}
0
public deleteBlob_args deepCopy()
{    return new deleteBlob_args(this);}
0
public String toString()
{    return ReflectionToStringBuilder.reflectionToString(this, ToStringStyle.DEFAULT_STYLE, false);}
0
public void replay(QueryableFactory<T> factory)
{    factory.distinct(source, comparer);}
0
public void testBitResize()
{    TupleMetadata schema = new SchemaBuilder().add("col", MinorType.BIT).buildSchema();    RowSetBuilder rsb = new RowSetBuilder(fixture.allocator(), schema, 100);    ScalarWriter bitWriter = rsb.writer().scalar(0);    for (int i = 0; i < ValueVector.MAX_ROW_COUNT; i++) {        bitWriter.setBoolean((i % 5) == 0);        rsb.writer().save();    }    SingleRowSet rs = rsb.build();    RowSetReader reader = rs.reader();    ScalarReader bitReader = reader.scalar(0);    for (int i = 0; i < ValueVector.MAX_ROW_COUNT; i++) {        reader.next();        assertEquals((i % 5) == 0, bitReader.getBoolean());    }    rs.clear();}
0
private JPanel initMainPanel()
{    JPanel panel = new JPanel(new GridLayout(1, 1));    tabbedPane.setEnabledAt(TabbedPaneProvider.Tab.OVERVIEW.index(), false);    tabbedPane.setEnabledAt(TabbedPaneProvider.Tab.DOCUMENTS.index(), false);    tabbedPane.setEnabledAt(TabbedPaneProvider.Tab.SEARCH.index(), false);    tabbedPane.setEnabledAt(TabbedPaneProvider.Tab.COMMITS.index(), false);    panel.add(tabbedPane);    panel.setOpaque(false);    return panel;}
0
public SqlCall createCall(SqlLiteral functionQualifier, SqlParserPos pos, SqlNode... operands)
{    assert functionQualifier == null;    assert operands.length == 4;    return new SqlCase(pos, operands[0], (SqlNodeList) operands[1], (SqlNodeList) operands[2], operands[3]);}
0
public void testNullTrackingUrlUnmanagedAM()
{    testUnmanagedAMSuccess(null);}
0
protected void setUp() throws Exception
{    setAutoFail(true);    super.setUp();}
0
public void process(WatchedEvent event)
{    lastEvent = event;    if (latch != null) {        latch.countDown();    }    }
1
protected NodeInfo<Floor> info()
{    return NodeInfo.create(this, Floor::new, field());}
0
public void testBasicReplication() throws Exception
{    addNodes(nodes);    for (int i = 0; i < NUM_TEST_ITERS; i++) {        doBasicTest(i);    }}
0
private static Matcher<Iterable<E>> isIterable(final Iterable<E> iterable)
{    final List<E> list = toList(iterable);    return new TypeSafeMatcher<Iterable<E>>() {        protected boolean matchesSafely(Iterable<E> iterable) {            return list.equals(toList(iterable));        }        public void describeTo(Description description) {            description.appendText("is iterable ").appendValue(list);        }    };}
0
public void txDidDestroy(long timeStamp)
{    setLastModified(timeStamp);    setLastAccessed(timeStamp);    this.hitCount = 0;    this.missCount = 0;}
0
public Map<String, String> getAttributes()
{    return flowFile.getAttributes();}
0
public GetSubClusterPoliciesConfigurationsResponse getPoliciesConfigurations(GetSubClusterPoliciesConfigurationsRequest request) throws YarnException
{    ArrayList<SubClusterPolicyConfiguration> result = new ArrayList<SubClusterPolicyConfiguration>();    for (SubClusterPolicyConfiguration policy : policies.values()) {        result.add(policy);    }    return GetSubClusterPoliciesConfigurationsResponse.newInstance(result);}
0
public void testCAMELLIA192() throws Exception
{    if (!TestHelper.UNRESTRICTED_POLICIES_INSTALLED) {        return;    }        KeyGenerator keygen = KeyGenerator.getInstance("CAMELLIA");    keygen.init(192);    SecretKey key = keygen.generateKey();    final XMLSecurityDataFormat xmlEncDataFormat = new XMLSecurityDataFormat();    xmlEncDataFormat.setPassPhrase(key.getEncoded());    xmlEncDataFormat.setSecureTagContents(true);    xmlEncDataFormat.setSecureTag("//cheesesites/italy/cheese");    xmlEncDataFormat.setXmlCipherAlgorithm(XMLCipher.CAMELLIA_192);    context.addRoutes(new RouteBuilder() {        public void configure() {            from("direct:start").marshal(xmlEncDataFormat).to("mock:encrypted").log("Body: + ${body}").unmarshal(xmlEncDataFormat).to("mock:decrypted");        }    });    xmlsecTestHelper.testDecryption(context);}
0
protected void handleMergeException(Directory dir, Throwable exc)
{    throw new MergePolicy.MergeException(exc, dir);}
0
protected String getMethodName()
{    return suitename() + "-" + super.getMethodName();}
0
public void testValidSchemaWithELShouldBeSuccessful() throws InitializationException, IOException
{    configure(proc, 10);    final String filename = "testValidSchemaWithELShouldBeSuccessful-" + System.currentTimeMillis();        final Map<String, String> flowFileAttributes = new HashMap<>();    flowFileAttributes.put(CoreAttributes.FILENAME.key(), filename);    flowFileAttributes.put("my.schema", schema.toString());    testRunner.enqueue("trigger", flowFileAttributes);    testRunner.run();    testRunner.assertAllFlowFilesTransferred(PutParquet.REL_SUCCESS, 1);}
0
public void testSelectOptionsOnlyOnSelectCalls() throws Throwable
{    describe("Secondary select options are only valid on select" + " queries");    String key = CSV_INPUT_HEADER;    intercept(IllegalArgumentException.class, key, () -> getFileSystem().openFile(csvPath).must(key, CSV_HEADER_OPT_USE).build());}
0
protected RouteBuilder createRouteBuilder()
{    return new RouteBuilder() {        public void configure() {                        errorHandler(defaultErrorHandler().maximumRedeliveries(2));            from(String.format("mina:tcp://localhost:%1$s?textline=true&minaLogger=true&textlineDelimiter=MAC&sync=true", getPort())).process(new Processor() {                public void process(Exchange e) {                    String in = e.getIn().getBody(String.class);                    if ("force-null-out-body".equals(in)) {                                                e.getOut().setBody(null);                    } else if ("force-exception".equals(in)) {                                                e.getOut().setBody(null);                        throw new IllegalArgumentException("Forced exception");                    } else {                        e.getOut().setBody("Hello " + in);                    }                }            });        }    };}
0
public long getRequestSubmissionTime()
{    return submissionTime;}
0
public ByteBuffer getFinalResult(ByteBuffer in)
{    throw new UnsupportedOperationException();}
0
 static Operand checkForRangeOperand(Operand expected, Object strVal, ComputedType computedType)
{    if (strVal instanceof String) {        String s = ((String) strVal).trim();        int hyphenIdx = s.indexOf('-');        if (hyphenIdx > 0) {            if (hyphenIdx == s.length() - 1) {                throw new IllegalArgumentException("bad range input :" + expected);            }            if (expected == Operand.EQUAL)                return Operand.RANGE_EQUAL;            if (expected == Operand.NOT_EQUAL)                return Operand.RANGE_NOT_EQUAL;        }    }    if (expected == Operand.EQUAL && (computedType != null || !isIntegerEquivalent(strVal))) {        return Operand.RANGE_EQUAL;    }    if (expected == Operand.NOT_EQUAL && (computedType != null || !isIntegerEquivalent(strVal)))        return Operand.RANGE_NOT_EQUAL;    return expected;}
0
public void resetEvaluator()
{    isNull = true;    max = 0.0;}
0
public Map<EventType, Set<Watcher>> unregister(int rc) throws KeeperException
{    return zkManager.removeWatcher(clientPath, watcher, watcherType, local, rc);}
0
public void setUp() throws Exception
{    Properties configProperties = new Properties();    configProperties.setProperty(MCAST_PORT, "0");    configProperties.setProperty(ENABLE_TIME_STATISTICS, "true");    configProperties.setProperty(STATISTIC_SAMPLING_ENABLED, "false");    configProperties.setProperty(STATISTIC_SAMPLE_RATE, "60000");    system = (InternalDistributedSystem) DistributedSystem.connect(configProperties);    assertNotNull(system.getStatSampler());    assertNotNull(system.getStatSampler().waitForSampleCollector(TIMEOUT));    new CacheFactory().create();    StatisticsFactory statisticsFactory = system.getStatisticsManager();    SimpleMeterRegistry meterRegistry = new SimpleMeterRegistry();    receiverStats = createGatewayReceiverStats(statisticsFactory, "Test Sock Name", meterRegistry);    GatewayReceiver gatewayReceiver = mock(GatewayReceiver.class);    InternalCacheServer receiverServer = mock(InternalCacheServer.class);    when(gatewayReceiver.getServer()).thenReturn(receiverServer);    bridge = new GatewayReceiverMBeanBridge(gatewayReceiver);    bridge.addGatewayReceiverStats(receiverStats);    sample();}
0
public void tightUnmarshal(OpenWireFormat wireFormat, Object o, DataInput dataIn, BooleanStream bs) throws IOException
{    super.tightUnmarshal(wireFormat, o, dataIn, bs);    ConnectionInfo info = (ConnectionInfo) o;    info.setConnectionId((org.apache.activemq.command.ConnectionId) tightUnmarsalCachedObject(wireFormat, dataIn, bs));    info.setClientId(tightUnmarshalString(dataIn, bs));    info.setPassword(tightUnmarshalString(dataIn, bs));    info.setUserName(tightUnmarshalString(dataIn, bs));    if (bs.readBoolean()) {        short size = dataIn.readShort();        org.apache.activemq.command.BrokerId[] value = new org.apache.activemq.command.BrokerId[size];        for (int i = 0; i < size; i++) {            value[i] = (org.apache.activemq.command.BrokerId) tightUnmarsalNestedObject(wireFormat, dataIn, bs);        }        info.setBrokerPath(value);    } else {        info.setBrokerPath(null);    }    info.setBrokerMasterConnector(bs.readBoolean());    info.setManageable(bs.readBoolean());    info.setClientMaster(bs.readBoolean());    info.setFaultTolerant(bs.readBoolean());    info.setFailoverReconnect(bs.readBoolean());    info.setClientIp(tightUnmarshalString(dataIn, bs));}
0
public TermsEnum termsEnum() throws IOException
{    return new SortedSetDocValuesTermsEnum(this);}
0
 RobotFrameworkEndpointProducerBuilder warnOnSkippedFiles(boolean warnOnSkippedFiles)
{    doSetProperty("warnOnSkippedFiles", warnOnSkippedFiles);    return this;}
0
private void configure()
{    if (!isConfigured) {        ThreadGroup group = Thread.currentThread().getThreadGroup();        Thread[] threads = new Thread[group.activeCount() * 2];        group.enumerate(threads, true);        for (Thread t : threads) {            if (t != null && (t instanceof ResourceUsageMatcherRunner)) {                this.matcher = (ResourceUsageMatcherRunner) t;                isConfigured = true;                break;            }        }    }}
0
public void checkFailComplexUsage1(boolean b)
{    String message = "Hello {}, {}";    Object[] args = new Object[] { "world", 42 };    if (b) {        message = "just one arg {}";        args = new Object[] { "world", 43 };    }    }
1
public int getMaxParallelism()
{    return maxParallelism;}
0
public java.lang.String toString()
{    java.lang.StringBuilder sb = new java.lang.StringBuilder("getNimbusConf_result(");    boolean first = true;    sb.append("success:");    if (this.success == null) {        sb.append("null");    } else {        sb.append(this.success);    }    first = false;    if (!first)        sb.append(", ");    sb.append("aze:");    if (this.aze == null) {        sb.append("null");    } else {        sb.append(this.aze);    }    first = false;    sb.append(")");    return sb.toString();}
0
public ScheduledFuture<?> scheduleWithFixedDelay(@Nonnull Runnable command, long initialDelay, long delay, @Nonnull TimeUnit unit)
{    ScheduledFutureTask<Void> scheduledFutureTask = new ScheduledFutureTask<>(command, triggerTime(unit.toNanos(initialDelay)), unit.toNanos(-delay));    Cancellable cancellable = internalSchedule(scheduledFutureTask, initialDelay, unit);    scheduledFutureTask.setCancellable(cancellable);    return scheduledFutureTask;}
0
protected void after()
{    DataflowWorkerLoggingMDC.setJobId(previousJobId);    DataflowWorkerLoggingMDC.setStageName(previousStageName);    DataflowWorkerLoggingMDC.setWorkerId(previousWorkerId);    DataflowWorkerLoggingMDC.setWorkId(previousWorkId);}
0
public String getBeanClassName()
{    if (beanClassName != null) {        return beanClassName;    }    try {        Object bean = getProcessor().getBean();        if (bean != null) {            beanClassName = ObjectHelper.className(bean);        }    } catch (NoSuchBeanException e) {        }    return beanClassName;}
0
public byte[] bytes()
{    return buf;}
0
public void serialize(Object code, ByteBuffer buf)
{    throw new UnsupportedOperationException();}
0
public Void call() throws Exception
{    Stopwatch timeRun = duration != DEFAULT_DURATION ? Stopwatch.createStarted() : null;    int numRuns = 0;    do {        numRuns++;        callAndValidate();    } while ((iterations != DEFAULT_ITERATIONS && numRuns < iterations) || (duration != DEFAULT_DURATION && timeRun.elapsed(SECONDS) <= duration.getSeconds()));    return null;}
0
public static LocalDate leftShift(final MonthDay self, Year year)
{    return year.atMonthDay(self);}
0
protected String executor()
{    return ThreadPool.Names.SAME;}
0
public void testJmsTemplateUsesPoolingConnectionFactory() throws Exception
{    JmsEndpoint endpoint = resolveMandatoryEndpoint("activemq:test.foo");    JmsProducer producer = (JmsProducer) endpoint.createProducer();    JmsTemplate template = assertIsInstanceOf(JmsTemplate.class, producer.getInOutTemplate());    assertEquals("pubSubDomain", false, template.isPubSubDomain());    assertIsInstanceOf(PooledConnectionFactory.class, template.getConnectionFactory());}
0
public void setStatisticsEnabled(Boolean value)
{    this.statisticsEnabled = value;}
0
public void modifyAckDeadline(SubscriptionPath subscription, List<String> ackIds, int deadlineSeconds) throws IOException
{    ModifyAckDeadlineRequest request = ModifyAckDeadlineRequest.newBuilder().setSubscription(subscription.getPath()).addAllAckIds(ackIds).setAckDeadlineSeconds(deadlineSeconds).build();        subscriberStub().modifyAckDeadline(request);}
0
 AdvancedWebsocketEndpointProducerBuilder maxTextMessageSize(String maxTextMessageSize)
{    doSetProperty("maxTextMessageSize", maxTextMessageSize);    return this;}
0
private void touchz(PathData item) throws IOException
{    item.fs.create(item.path).close();}
0
public int hashCode()
{    return Objects.hash(super.hashCode(), attributes, isInstantiable, comparision, superType, implementationClass);}
0
private void sendAckUpstream(PipelineAck ack, long seqno, long totalAckTimeNanos, long offsetInBlock, int myHeader) throws IOException
{    try {                synchronized (this) {            while (sending) {                wait();            }            sending = true;        }        try {            if (!running)                return;            sendAckUpstreamUnprotected(ack, seqno, totalAckTimeNanos, offsetInBlock, myHeader);        } finally {            synchronized (this) {                sending = false;                notify();            }        }    } catch (InterruptedException ie) {                        running = false;    }}
0
public void testSetDefaultUnqualifiedSchema()
{    PCollection<Row> inputMain = pipeline.apply("mainInput", create(row(1, "pcollection_1"), row(2, "pcollection_2")));    PCollection<Row> inputExtra = pipeline.apply("extraInput", create(row(1, "_extra_table_1"), row(2, "_extra_table_2")));    TableProvider extraInputProvider = extraTableProvider("extraTable", inputExtra);    PCollection<Row> result = inputMain.apply(SqlTransform.query("SELECT f_int, f_string FROM extraTable").withDefaultTableProvider("extraSchema", extraInputProvider));    PAssert.that(result).containsInAnyOrder(row(1, "_extra_table_1"), row(2, "_extra_table_2"));    pipeline.run();}
0
public Response visit(CommandVisitor visitor) throws Exception
{    switch(type) {        case TransactionInfo.BEGIN:            return visitor.processBeginTransaction(this);        case TransactionInfo.END:            return visitor.processEndTransaction(this);        case TransactionInfo.PREPARE:            return visitor.processPrepareTransaction(this);        case TransactionInfo.COMMIT_ONE_PHASE:            return visitor.processCommitTransactionOnePhase(this);        case TransactionInfo.COMMIT_TWO_PHASE:            return visitor.processCommitTransactionTwoPhase(this);        case TransactionInfo.ROLLBACK:            return visitor.processRollbackTransaction(this);        case TransactionInfo.RECOVER:            return visitor.processRecoverTransactions(this);        case TransactionInfo.FORGET:            return visitor.processForgetTransaction(this);        default:            throw new IOException("Transaction info type unknown: " + type);    }}
0
 void checkInputBuffers(byte[][] buffers)
{    int validInputs = 0;    for (byte[] buffer : buffers) {        if (buffer == null) {            continue;        }        if (buffer.length != decodeLength) {            throw new HadoopIllegalArgumentException("Invalid buffer, not of length " + decodeLength);        }        validInputs++;    }    if (validInputs < decoder.getNumDataUnits()) {        throw new HadoopIllegalArgumentException("No enough valid inputs are provided, not recoverable");    }}
0
public byte[] take() throws Exception
{    Timer.Context timer = stats.time(dir + "_take");    updateLock.lockInterruptibly();    try {        while (true) {            byte[] result = poll();            if (result != null) {                return result;            }            changed.await();        }    } finally {        updateLock.unlock();        timer.stop();    }}
0
public void testParse_OptionalFieldsAbsent() throws IOException
{    String json = "{\"job_id\": \"my-job-id\"}";    try (XContentParser parser = XContentFactory.xContent(XContentType.JSON).createParser(xContentRegistry(), DeprecationHandler.THROW_UNSUPPORTED_OPERATION, json)) {        TimingStats stats = TimingStats.PARSER.apply(parser, null);        assertThat(stats.getJobId(), equalTo(JOB_ID));        assertThat(stats.getBucketCount(), equalTo(0L));        assertThat(stats.getTotalBucketProcessingTimeMs(), equalTo(0.0));        assertThat(stats.getMinBucketProcessingTimeMs(), nullValue());        assertThat(stats.getMaxBucketProcessingTimeMs(), nullValue());        assertThat(stats.getAvgBucketProcessingTimeMs(), nullValue());        assertThat(stats.getExponentialAvgBucketProcessingTimeMs(), nullValue());        assertThat(stats.getExponentialAvgBucketProcessingTimePerHourMs(), nullValue());    }}
0
public void setInterpreterDir(String interpreterDir)
{    this.interpreterDir = interpreterDir;}
0
public void testGetStagingAreaDir() throws IOException, InterruptedException
{    Configuration conf = new Configuration();    JobClient client = new JobClient(conf);    Assert.assertTrue("Mismatch in paths", client.getClusterHandle().getStagingAreaDir().toString().equals(client.getStagingAreaDir().toString()));}
0
public Ord<E> next()
{    return Ord.of(i, elementList.get(i--));}
0
public static QuotaSettings unthrottleUserByThrottleType(final String userName, final ThrottleType type)
{    return throttle(userName, null, null, null, type, 0, null, QuotaScope.MACHINE);}
0
 AdvancedLambdaEndpointBuilder awsLambdaClient(Object awsLambdaClient)
{    doSetProperty("awsLambdaClient", awsLambdaClient);    return this;}
0
public final void flushBatch() throws IOException
{    base.flushBatch();}
0
private void op(String path, ServiceRecord record, RegistryCommand command) throws IOException
{    ServiceRecordProcessor processor;    try {        String yarnPersistanceValue = record.get(YarnRegistryAttributes.YARN_PERSISTENCE);        if (yarnPersistanceValue != null) {            if (yarnPersistanceValue.equals(CONTAINER)) {                                                                processor = new ContainerServiceRecordProcessor(record, path, domainName, this);            } else {                                processor = new ApplicationServiceRecordProcessor(record, path, domainName, this);            }            processor.manageDNSRecords(command);        } else {                    }    } catch (Exception e) {        throw new IOException(e);    }}
1
public LABEL<T> code(String cdata)
{    return code()._(cdata)._();}
0
public static Version randomIndexCompatibleVersion(Random random)
{    return randomVersionBetween(random, Version.CURRENT.minimumIndexCompatibilityVersion(), Version.CURRENT);}
0
public void subTestIntervalDayToMinuteFailsValidation()
{        expr("INTERVAL ' :' DAY TO MINUTE").ok("INTERVAL ' :' DAY TO MINUTE");    expr("INTERVAL '1' DAY TO MINUTE").ok("INTERVAL '1' DAY TO MINUTE");    expr("INTERVAL '1 2' DAY TO MINUTE").ok("INTERVAL '1 2' DAY TO MINUTE");    expr("INTERVAL '1:2' DAY TO MINUTE").ok("INTERVAL '1:2' DAY TO MINUTE");    expr("INTERVAL '1.2' DAY TO MINUTE").ok("INTERVAL '1.2' DAY TO MINUTE");    expr("INTERVAL 'x 1:1' DAY TO MINUTE").ok("INTERVAL 'x 1:1' DAY TO MINUTE");    expr("INTERVAL '1 x:1' DAY TO MINUTE").ok("INTERVAL '1 x:1' DAY TO MINUTE");    expr("INTERVAL '1 1:x' DAY TO MINUTE").ok("INTERVAL '1 1:x' DAY TO MINUTE");    expr("INTERVAL '1 1:2:3' DAY TO MINUTE").ok("INTERVAL '1 1:2:3' DAY TO MINUTE");    expr("INTERVAL '1 1:1:1.2' DAY TO MINUTE").ok("INTERVAL '1 1:1:1.2' DAY TO MINUTE");    expr("INTERVAL '1 1:2:3' DAY(2) TO MINUTE").ok("INTERVAL '1 1:2:3' DAY(2) TO MINUTE");    expr("INTERVAL '1 1' DAY(2) TO MINUTE").ok("INTERVAL '1 1' DAY(2) TO MINUTE");    expr("INTERVAL 'bogus text' DAY TO MINUTE").ok("INTERVAL 'bogus text' DAY TO MINUTE");        expr("INTERVAL '--1 1:1' DAY TO MINUTE").ok("INTERVAL '--1 1:1' DAY TO MINUTE");    expr("INTERVAL '1 -1:1' DAY TO MINUTE").ok("INTERVAL '1 -1:1' DAY TO MINUTE");    expr("INTERVAL '1 1:-1' DAY TO MINUTE").ok("INTERVAL '1 1:-1' DAY TO MINUTE");                expr("INTERVAL '100 0' DAY TO MINUTE").ok("INTERVAL '100 0' DAY TO MINUTE");    expr("INTERVAL '100 0' DAY(2) TO MINUTE").ok("INTERVAL '100 0' DAY(2) TO MINUTE");    expr("INTERVAL '1000 0' DAY(3) TO MINUTE").ok("INTERVAL '1000 0' DAY(3) TO MINUTE");    expr("INTERVAL '-1000 0' DAY(3) TO MINUTE").ok("INTERVAL '-1000 0' DAY(3) TO MINUTE");    expr("INTERVAL '2147483648 0' DAY(10) TO MINUTE").ok("INTERVAL '2147483648 0' DAY(10) TO MINUTE");    expr("INTERVAL '-2147483648 0' DAY(10) TO MINUTE").ok("INTERVAL '-2147483648 0' DAY(10) TO MINUTE");    expr("INTERVAL '1 24:1' DAY TO MINUTE").ok("INTERVAL '1 24:1' DAY TO MINUTE");    expr("INTERVAL '1 1:60' DAY TO MINUTE").ok("INTERVAL '1 1:60' DAY TO MINUTE");        expr("INTERVAL '1 1' DAY(11) TO MINUTE").ok("INTERVAL '1 1' DAY(11) TO MINUTE");                expr("INTERVAL '0 0' DAY(0) TO MINUTE").ok("INTERVAL '0 0' DAY(0) TO MINUTE");}
0
private void declareField(VariableDeclarationContext ctx, ModifierManager modifierManager, ClassNode variableType, ClassNode classNode, int i, VariableExpression variableExpression, String fieldName, int modifiers, Expression initialValue)
{    FieldNode existingFieldNode = classNode.getDeclaredField(fieldName);    if (null != existingFieldNode && !existingFieldNode.isSynthetic()) {        throw createParsingFailedException("The field '" + fieldName + "' is declared multiple times", ctx);    }    FieldNode fieldNode;    PropertyNode propertyNode = classNode.getProperty(fieldName);    if (null != propertyNode && propertyNode.getField().isSynthetic()) {        classNode.getFields().remove(propertyNode.getField());        fieldNode = new FieldNode(fieldName, modifiers, variableType, classNode.redirect(), initialValue);        propertyNode.setField(fieldNode);        classNode.addField(fieldNode);    } else {        fieldNode = classNode.addField(fieldName, modifiers, variableType, initialValue);    }    modifierManager.attachAnnotations(fieldNode);    groovydocManager.handle(fieldNode, ctx);    if (0 == i) {        configureAST(fieldNode, ctx, initialValue);    } else {        configureAST(fieldNode, variableExpression, initialValue);    }}
0
public void remoteAddress(TransportAddress remoteAddress)
{    this.remoteAddress = remoteAddress;}
0
public Tuple5Builder<T0, T1, T2, T3, T4> add(T0 value0, T1 value1, T2 value2, T3 value3, T4 value4)
{    tuples.add(new Tuple5<>(value0, value1, value2, value3, value4));    return this;}
0
public void readAllFiles(CommitLogReadHandler handler, File[] files, CommitLogPosition minPosition) throws IOException
{    List<File> filteredLogs = filterCommitLogFiles(files);    int i = 0;    for (File file : filteredLogs) {        i++;        readCommitLogSegment(handler, file, minPosition, ALL_MUTATIONS, i == filteredLogs.size());    }}
0
public List<NicVO> listPlaceholderNicsByNetworkIdAndVmType(long networkId, VirtualMachine.Type vmType)
{    SearchCriteria<NicVO> sc = AllFieldsSearch.create();    sc.setParameters("network", networkId);    sc.setParameters("strategy", Nic.ReservationStrategy.PlaceHolder.toString());    sc.setParameters("vmType", vmType);    return listBy(sc);}
0
 RangeQueryBuilder lt(Object value)
{    return to(value, false);}
0
private long writeRows(String tableId, List<TableRow> rows, TableSchema schema, String destinationPattern) throws IOException
{    Schema avroSchema = BigQueryUtils.toGenericAvroSchema(tableId, schema.getFields());    List<TableRow> rowsToWrite = Lists.newArrayList();    int shard = 0;    for (TableRow row : rows) {        rowsToWrite.add(row);        if (rowsToWrite.size() == 5) {            writeRowsHelper(rowsToWrite, avroSchema, destinationPattern, shard++);            rowsToWrite.clear();        }    }    if (!rowsToWrite.isEmpty()) {        writeRowsHelper(rowsToWrite, avroSchema, destinationPattern, shard++);    }    return shard;}
0
public void connect() throws IOException
{    connected = true;}
0
public void testSerializeDeserialize() throws Exception
{    testNullableField(Types.LONG, "null", null);    testNullableField(Types.STRING, "null", null);    testNullableField(Types.VOID, "null", null);    testNullableField(Types.STRING, "\"This is a test.\"", "This is a test.");    testNullableField(Types.STRING, "\"This is a test\n\r.\"", "This is a test\n\r.");    testNullableField(Types.BOOLEAN, "true", true);    testNullableField(Types.BOOLEAN, "null", null);    testNullableField(Types.BYTE, "124", (byte) 124);    testNullableField(Types.SHORT, "10000", (short) 10000);    testNullableField(Types.INT, "1234567", 1234567);    testNullableField(Types.LONG, "12345678910", 12345678910L);    testNullableField(Types.FLOAT, "0.33333334", 0.33333334f);    testNullableField(Types.DOUBLE, "0.33333333332", 0.33333333332d);    testNullableField(Types.BIG_DEC, "\"1234.0000000000000000000000001\"", new BigDecimal("1234.0000000000000000000000001"));    testNullableField(Types.BIG_INT, "\"123400000000000000000000000000\"", new BigInteger("123400000000000000000000000000"));    testNullableField(Types.SQL_DATE, "2018-10-12", Date.valueOf("2018-10-12"));    testNullableField(Types.SQL_TIME, "12:12:12", Time.valueOf("12:12:12"));    testNullableField(Types.SQL_TIMESTAMP, "\"2018-10-12 12:12:12.0\"", Timestamp.valueOf("2018-10-12 12:12:12"));    testNullableField(Types.ROW(Types.STRING, Types.INT, Types.BOOLEAN), "Hello;42;false", Row.of("Hello", 42, false));    testNullableField(Types.OBJECT_ARRAY(Types.STRING), "a;b;c", new String[] { "a", "b", "c" });    testNullableField(Types.OBJECT_ARRAY(Types.BYTE), "12;4;null", new Byte[] { 12, 4, null });    testNullableField((TypeInformation<byte[]>) Types.PRIMITIVE_ARRAY(Types.BYTE), "awML", new byte[] { 107, 3, 11 });}
0
private void dumpINodeDirectorySection(InputStream in) throws IOException
{    out.print("<" + INODE_DIRECTORY_SECTION_NAME + ">");    while (true) {        INodeDirectorySection.DirEntry e = INodeDirectorySection.DirEntry.parseDelimitedFrom(in);                if (e == null) {            break;        }        out.print("<" + INODE_DIRECTORY_SECTION_DIRECTORY + ">");        o(INODE_DIRECTORY_SECTION_PARENT, e.getParent());        for (long id : e.getChildrenList()) {            o(INODE_DIRECTORY_SECTION_CHILD, id);        }        for (int refId : e.getRefChildrenList()) {            o(INODE_DIRECTORY_SECTION_REF_CHILD, refId);        }        out.print("</" + INODE_DIRECTORY_SECTION_DIRECTORY + ">\n");    }    out.print("</" + INODE_DIRECTORY_SECTION_NAME + ">\n");}
0
public boolean setInProgress(boolean progress)
{    boolean retVal = false;    TXStateProxy tsp = txContext.get();    assert tsp != PAUSED;    if (tsp != null) {        retVal = tsp.isInProgress();        tsp.setInProgress(progress);    }    return retVal;}
0
private boolean checkForBadCharsAt(String filename)
{    final int len = filename.length();    for (int i = 0; i < len; i++) {        char c = filename.charAt(i);        if (c == '\"' || c == '<' || c == '>') {            return false;        }    }    if (filename.charAt(len - 1) == ' ') {        return false;    }    return true;}
0
public static List<DotDrillFile> getDotDrills(DrillFileSystem fs, Path root, DotDrillType... types) throws IOException
{    return getDrillFiles(fs, getDrillFileStatus(fs, root, "*.drill"), types);}
0
public void createRegionWithGroup()
{    Region region = new Region();    region.setName("company");    region.setType(RegionType.PARTITION);    region.setGroup(groupA);    ClusterManagementRealizationResult result = cmsClient.create(region);    assertThat(result.isSuccessful()).isTrue();    assertThat(result.getStatusCode()).isEqualTo(ClusterManagementResult.StatusCode.OK);        assertThat(result.getMemberStatuses()).extracting(RealizationResult::getMemberName).containsExactlyInAnyOrder("server-2");    locator.invoke(() -> {        InternalConfigurationPersistenceService persistenceService = ClusterStartupRule.getLocator().getConfigurationPersistenceService();        CacheConfig clusterCacheConfig = persistenceService.getCacheConfig("cluster", true);        CacheConfig groupACacheConfig = persistenceService.getCacheConfig("group-a");        assertThat(find(clusterCacheConfig.getRegions(), "company")).isNull();        assertThat(find(groupACacheConfig.getRegions(), "company")).isNotNull();    });}
0
public void testSelectDistinctFails() throws Exception
{    index("{\"name\":\"test\"}");    expectBadRequest(() -> runSql(randomMode(), "SELECT DISTINCT name FROM test"), containsString("line 1:8: SELECT DISTINCT is not yet supported"));}
0
public void testWhereInJoinCorrelated()
{    final String sql = "select empno from emp as e\n" + "join dept as d using (deptno)\n" + "where e.sal in (\n" + "  select e2.sal from emp as e2 where e2.deptno > e.deptno)";    checkSubQuery(sql).check();}
0
public void setSuccessIsSet(boolean value)
{    if (!value) {        this.success = null;    }}
0
public void tearDown() throws Exception
{    cache.close();}
0
public void incNumBucketDeletes()
{    numBucketOps.incr();    numBucketDeletes.incr();}
0
public boolean equals(final Object other)
{    if (other == null || !(other instanceof FileSystemCounterLimit)) {        return false;    }    if (other == this) {        return true;    }    FileSystemCounterLimit otherFscl = (FileSystemCounterLimit) other;    return scheme.equals(otherFscl.scheme) && fsCounter.equals(otherFscl.fsCounter) && limit == otherFscl.limit;}
0
public static void setupDD()
{    DatabaseDescriptor.daemonInitialization();}
0
public void testIterativeWithABACPattern() throws Exception
{    List<StreamRecord<Event>> inputEvents = new ArrayList<>();    final Event startEvent1 = new Event(40, "start", 1.0);    final Event startEvent2 = new Event(40, "start", 2.0);    final Event startEvent3 = new Event(40, "start", 3.0);    final Event startEvent4 = new Event(40, "start", 4.0);    final SubEvent middleEvent1 = new SubEvent(41, "foo1", 1.0, 10);    final SubEvent middleEvent2 = new SubEvent(42, "foo2", 2.0, 10);    final SubEvent middleEvent3 = new SubEvent(43, "foo3", 3.0, 10);    final SubEvent middleEvent4 = new SubEvent(43, "foo4", 1.0, 10);    final Event endEvent = new Event(46, "end", 1.0);        inputEvents.add(new StreamRecord<>(startEvent1, 1L));        inputEvents.add(new StreamRecord<Event>(middleEvent1, 2L));        inputEvents.add(new StreamRecord<>(startEvent2, 2L));        inputEvents.add(new StreamRecord<>(startEvent3, 2L));        inputEvents.add(new StreamRecord<Event>(middleEvent2, 2L));        inputEvents.add(new StreamRecord<>(startEvent4, 2L));        inputEvents.add(new StreamRecord<Event>(middleEvent3, 2L));        inputEvents.add(new StreamRecord<Event>(middleEvent4, 2L));    inputEvents.add(new StreamRecord<>(endEvent, 4L));    Pattern<Event, ?> pattern = Pattern.<Event>begin("start").where(new SimpleCondition<Event>() {        private static final long serialVersionUID = 6215754202506583964L;        @Override        public boolean filter(Event value) throws Exception {            return value.getName().equals("start");        }    }).followedByAny("middle1").subtype(SubEvent.class).where(new SimpleCondition<SubEvent>() {        private static final long serialVersionUID = 2178338526904474690L;        @Override        public boolean filter(SubEvent value) throws Exception {            return value.getName().startsWith("foo");        }    }).followedBy("middle2").where(new IterativeCondition<Event>() {        private static final long serialVersionUID = -1223388426808292695L;        @Override        public boolean filter(Event value, Context<Event> ctx) throws Exception {            if (!value.getName().equals("start")) {                return false;            }            double sum = 0.0;            for (Event e : ctx.getEventsForPattern("middle2")) {                sum += e.getPrice();            }            sum += value.getPrice();            return Double.compare(sum, 5.0) <= 0;        }    }).oneOrMore().followedBy("end").where(new SimpleCondition<Event>() {        private static final long serialVersionUID = 562590474115118323L;        @Override        public boolean filter(Event value) throws Exception {            return value.getName().equals("end");        }    });    TestSharedBuffer<Event> sharedBuffer = TestSharedBuffer.createTestBuffer(Event.createTypeSerializer());    NFATestHarness nfaTestHarness = NFATestHarness.forPattern(pattern).withSharedBuffer(sharedBuffer).build();    nfaTestHarness.consumeRecords(inputEvents);    assertEquals(90, sharedBuffer.getStateReads());    assertEquals(31, sharedBuffer.getStateWrites());    assertEquals(121, sharedBuffer.getStateAccesses());}
0
public int getSizeEstimate(Object key, int operator, int matchLevel) throws TypeMismatchException
{        int size = 0;    long start = updateIndexUseStats(false);    try {        switch(operator) {            case OQLLexerTokenTypes.TOK_EQ:                {                    key = TypeUtils.indexKeyFor(key);                    size = this.entriesSet.size(key);                }                break;            case OQLLexerTokenTypes.TOK_NE_ALT:            case OQLLexerTokenTypes.TOK_NE:                size = this.region.size();                key = TypeUtils.indexKeyFor(key);                size = this.entriesSet.size(key);                break;        }    } finally {        updateIndexUseEndStats(start, false);    }    return size;}
0
public static HBaseProtos.RegionLocation toRegionLocation(HRegionLocation loc)
{    HBaseProtos.RegionLocation.Builder builder = HBaseProtos.RegionLocation.newBuilder();    builder.setRegionInfo(toRegionInfo(loc.getRegion()));    if (loc.getServerName() != null) {        builder.setServerName(toServerName(loc.getServerName()));    }    builder.setSeqNum(loc.getSeqNum());    return builder.build();}
0
public Type getType()
{    return type;}
0
public void put(final byte[] key, final byte[] value)
{    if (value == null) {        try {            db.delete(columnFamily, wOptions, key);        } catch (final RocksDBException e) {                        throw new ProcessorStateException("Error while removing key from store " + name, e);        }    } else {        try {            db.put(columnFamily, wOptions, key, value);        } catch (final RocksDBException e) {                        throw new ProcessorStateException("Error while putting key/value into store " + name, e);        }    }}
0
public String getBaseUri()
{    return baseUri;}
0
public StreamStatus getStreamStatus()
{    return streamStatus;}
0
public StateIncrementalVisitor<K, N, SV> getStateIncrementalVisitor(int recommendedMaxNumberOfReturnedRecords)
{    return stateTable.getStateIncrementalVisitor(recommendedMaxNumberOfReturnedRecords);}
0
public void setO1IsSet(boolean value)
{    if (!value) {        this.o1 = null;    }}
0
public void setRecreatable(boolean recreatable)
{    this.recreatable = recreatable;}
0
public AcknowledgedResponse putMapping(PutMappingRequest putMappingRequest, RequestOptions options) throws IOException
{    return restHighLevelClient.performRequestAndParseEntity(putMappingRequest, IndicesRequestConverters::putMapping, options, AcknowledgedResponse::fromXContent, emptySet());}
0
public void run()
{    for (int i = 0; i < ITERATIONS; i++) {        Object obj = QUEUE.poll();        if (obj == null) {            obj = new Object();        }        QUEUE.offer(obj);    }    super.run();}
0
public void eval()
{    out.value = rand.nextDouble();}
0
protected void setErrorState(ErrorState errorState, Throwable t)
{    response.setError();    boolean blockIo = this.errorState.isIoAllowed() && !errorState.isIoAllowed();    this.errorState = this.errorState.getMostSevere(errorState);        if (response.getStatus() < 400 && !(t instanceof IOException)) {        response.setStatus(500);    }    if (t != null) {        request.setAttribute(RequestDispatcher.ERROR_EXCEPTION, t);    }    if (blockIo && !ContainerThreadMarker.isContainerThread() && isAsync()) {                                                asyncStateMachine.asyncMustError();        if (getLog().isDebugEnabled()) {            getLog().debug(sm.getString("abstractProcessor.nonContainerThreadError"), t);        }        processSocketEvent(SocketEvent.ERROR, true);    }}
0
public void testDeleteFails()
{    final TestableFetchFileTransfer proc = new TestableFetchFileTransfer();    final TestRunner runner = TestRunners.newTestRunner(proc);    runner.setProperty(FetchFileTransfer.HOSTNAME, "localhost");    runner.setProperty(FetchFileTransfer.UNDEFAULTED_PORT, "11");    runner.setProperty(FetchFileTransfer.REMOTE_FILENAME, "${filename}");    runner.setProperty(FetchFileTransfer.COMPLETION_STRATEGY, FetchFileTransfer.COMPLETION_DELETE.getValue());    proc.addContent("hello.txt", "world".getBytes());    final Map<String, String> attrs = new HashMap<>();    attrs.put("filename", "hello.txt");    runner.enqueue(new byte[0], attrs);    proc.allowDelete = false;    runner.run(1, false, false);    runner.assertAllFlowFilesTransferred(FetchFileTransfer.REL_SUCCESS, 1);    assertFalse(proc.fileContents.isEmpty());}
0
public static INU_U05 toInuU05(String body) throws HL7Exception
{    return toMessage(INU_U05.class, body);}
0
public void testRpcWithChaosMonkeyWithSyncClient() throws Throwable
{    for (int i = 0; i < numIterations; i++) {        TimeoutThread.runWithTimeout(new Callable<Void>() {            @Override            public Void call() throws Exception {                try {                    testRpcWithChaosMonkey(true);                } catch (Throwable e) {                    if (e instanceof Exception) {                        throw (Exception) e;                    } else {                        throw new Exception(e);                    }                }                return null;            }        }, 180000);    }}
0
public int hashCode()
{    return Objects.hash(id, jobId, frequency, queryDelay, indices, asMap(query), scrollSize, asMap(aggregations), scriptFields, chunkingConfig, delayedDataCheckConfig, maxEmptySearches);}
0
public String toString()
{    return toString(":");}
0
private void syncRange(Context context, ImmutableBytesWritable startRow, ImmutableBytesWritable stopRow) throws IOException, InterruptedException
{    Scan scan = sourceTableHash.initScan();    scan.setStartRow(startRow.copyBytes());    scan.setStopRow(stopRow.copyBytes());    ResultScanner sourceScanner = sourceTable.getScanner(scan);    CellScanner sourceCells = new CellScanner(sourceScanner.iterator());    ResultScanner targetScanner = targetTable.getScanner(new Scan(scan));    CellScanner targetCells = new CellScanner(targetScanner.iterator());    boolean rangeMatched = true;    byte[] nextSourceRow = sourceCells.nextRow();    byte[] nextTargetRow = targetCells.nextRow();    while (nextSourceRow != null || nextTargetRow != null) {        boolean rowMatched;        int rowComparison = compareRowKeys(nextSourceRow, nextTargetRow);        if (rowComparison < 0) {            if (LOG.isInfoEnabled()) {                            }            context.getCounter(Counter.TARGETMISSINGROWS).increment(1);            rowMatched = syncRowCells(context, nextSourceRow, sourceCells, EMPTY_CELL_SCANNER);                        nextSourceRow = sourceCells.nextRow();        } else if (rowComparison > 0) {            if (LOG.isInfoEnabled()) {                            }            context.getCounter(Counter.SOURCEMISSINGROWS).increment(1);            rowMatched = syncRowCells(context, nextTargetRow, EMPTY_CELL_SCANNER, targetCells);                        nextTargetRow = targetCells.nextRow();        } else {                        rowMatched = syncRowCells(context, nextSourceRow, sourceCells, targetCells);            nextSourceRow = sourceCells.nextRow();            nextTargetRow = targetCells.nextRow();        }        if (!rowMatched) {            rangeMatched = false;        }    }    sourceScanner.close();    targetScanner.close();    context.getCounter(rangeMatched ? Counter.RANGESMATCHED : Counter.RANGESNOTMATCHED).increment(1);}
1
protected boolean isYieldAfterExecutionStep(final TEnvironment env)
{    return isYieldBeforeExecuteFromState(env, getCurrentState());}
0
private static LineStringBuilder parseLinearRing(StreamTokenizer stream, final boolean ignoreZValue, final boolean coerce) throws IOException, ElasticsearchParseException
{    String token = nextEmptyOrOpen(stream);    if (token.equals(EMPTY)) {        return null;    }    List<Coordinate> coordinates = parseCoordinateList(stream, ignoreZValue, coerce);    int coordinatesNeeded = coerce ? 3 : 4;    if (coordinates.size() >= coordinatesNeeded) {        if (!coordinates.get(0).equals(coordinates.get(coordinates.size() - 1))) {            if (coerce == true) {                coordinates.add(coordinates.get(0));            } else {                throw new ElasticsearchParseException("invalid LinearRing found (coordinates are not closed)");            }        }    }    if (coordinates.size() < 4) {        throw new ElasticsearchParseException("invalid number of points in LinearRing (found [{}] - must be >= 4)", coordinates.size());    }    return new LineStringBuilder(coordinates);}
0
public void delete()
{}
0
public void testIsLinux()
{    final boolean expected = ManagementFactory.getOperatingSystemMXBean().getName().contains(LINUX_OS_NAME);    assertEquals(expected, isLinux());}
0
public boolean isProtocol(byte[] value)
{    for (int i = 0; i < PREFIX.length; i++) {        if (value[i] != PREFIX[i]) {            return false;        }    }    return true;}
0
public String getPlanner()
{    return planner;}
0
public static TypeDescriptor<KV<K, V>> keyValues(TypeDescriptor<K> key, TypeDescriptor<V> value)
{    if (Objects.isNull(key) || Objects.isNull(value)) {        return null;    }    return new TypeDescriptor<KV<K, V>>() {    }.where(new TypeParameter<K>() {    }, key).where(new TypeParameter<V>() {    }, value);}
0
public void run()
{    try {        lockerA.start();        lockedSet.add(lockerA);        printLockTable(connection);    } catch (Exception e) {        e.printStackTrace();    }}
0
public static void main(String[] args) throws Exception
{    ToolRunner.run(new Configuration(), new FuzzyKMeansDriver(), args);}
0
public void doSomething()
{}
0
public final boolean incrementToken() throws java.io.IOException
{    boolean isTokenAvailable = false;    while (input.incrementToken()) {        if (typeAtt.type() == UAX29URLEmailTokenizer.TOKEN_TYPES[UAX29URLEmailTokenizer.EMAIL]) {            isTokenAvailable = true;            break;        }    }    return isTokenAvailable;}
0
public void getSslContext_OpenSSL() throws IOException
{        if (!OpenSsl.isAvailable()) {                return;    }    EncryptionOptions options = addKeystoreOptions(encryptionOptions);    SslContext sslContext = SSLFactory.getOrCreateSslContext(options, true, SSLFactory.SocketType.CLIENT, true);    Assert.assertNotNull(sslContext);    Assert.assertTrue(sslContext instanceof OpenSslContext);}
1
public static ClientAuth fromPropertyValue(String prop)
{    if (prop == null || prop.length() == 0) {        return NEED;    }    return ClientAuth.valueOf(prop.toUpperCase());}
0
public static String getMessage(final String key)
{    return getMessage(key, (Object[]) null);}
0
public static ArrayList runQueryOnServerLocalDataSet(String query, Set filter)
{    return new QueryUsingFunctionContextDUnitTest().runQueryOnServerLDS(query, filter);}
0
private void registerPulsarBeans(final JndiRegistry jndi) throws PulsarClientException
{    PulsarClient pulsarClient = givenPulsarClient();    AutoConfiguration autoConfiguration = new AutoConfiguration(null, null);    jndi.bind("pulsarClient", pulsarClient);    PulsarComponent comp = new PulsarComponent(context);    comp.setAutoConfiguration(autoConfiguration);    comp.setPulsarClient(pulsarClient);        comp.setAllowManualAcknowledgement(true);            jndi.bind("pulsar", comp);}
0
public String getCipher()
{    return cipher;}
0
protected Class getValueType(String name)
{    switch(name) {        case KEY_TABLE:            return OmKeyInfo.class;        case VOLUME_TABLE:            return OmVolumeArgs.class;        case BUCKET_TABLE:            return OmBucketInfo.class;        default:            return null;    }}
0
public int hashCode()
{    final int prime = 31;    int result = 1;    result = prime * result + id;    result = prime * result + Arrays.hashCode(relatedIds);    return result;}
0
public String getMetaStoreSchemaVersion() throws MetaException
{    return objectStore.getMetaStoreSchemaVersion();}
0
public PredicateBuilder and()
{    if (operator != Operator.And) {        handleComparator();        operator = Operator.And;    }    return PredicateBuilder.this;}
0
public int getCommandCount(DatanodeDetails dd)
{    List<SCMCommand> list = commandMap.get(dd.getUuid());    return (list == null) ? 0 : list.size();}
0
public void shutdown()
{    if (cluster != null) {        cluster.shutdown();    }}
0
public String verifyRMRegistrationResponseForNodeLabels(RegisterNodeManagerResponse regNMResponse)
{    return "";}
0
public void start() throws Exception
{}
0
public long getPreferredBlockSize(String src) throws IOException
{    rpcServer.checkOperation(NameNode.OperationCategory.READ);    final List<RemoteLocation> locations = rpcServer.getLocationsForPath(src, true, false);    RemoteMethod method = new RemoteMethod("getPreferredBlockSize", new Class<?>[] { String.class }, new RemoteParam());    return rpcClient.invokeSequential(locations, method, Long.class, null);}
0
public boolean isSetTableName()
{    return this.tableName != null;}
0
public void testStandardResponse() throws IOException
{    ByteArrayOutputStream baos = new ByteArrayOutputStream();    OutputStreamWriter osw = new OutputStreamWriter(baos, StandardCharsets.UTF_8);    PushWriter pw = new JSONWriter(osw, new LocalSolrQueryRequest(null, new ModifiableSolrParams()), new SolrQueryResponse());    writeData(pw);    osw.flush();        Map m = (Map) Utils.fromJSON(baos.toByteArray());    checkValues(m);    try (JavaBinCodec jbc = new JavaBinCodec(baos = new ByteArrayOutputStream(), null)) {        writeData(jbc);        try (JavaBinCodec jbcUn = new JavaBinCodec()) {            m = (Map) jbcUn.unmarshal(new ByteArrayInputStream(baos.toByteArray()));        }    }    checkValues(m);}
1
public void checkArgs(String argStr) throws BadFencingConfigurationException
{    if (argStr != null) {        new Args(argStr);    }}
0
public EverythingEnum reset(IntBlockTermState termState, int flags) throws IOException
{    docFreq = termState.docFreq;    docTermStartFP = termState.docStartFP;    posTermStartFP = termState.posStartFP;    payTermStartFP = termState.payStartFP;    skipOffset = termState.skipOffset;    totalTermFreq = termState.totalTermFreq;    singletonDocID = termState.singletonDocID;    if (docFreq > 1) {        if (docIn == null) {                        docIn = startDocIn.clone();        }        docIn.seek(docTermStartFP);    }    posPendingFP = posTermStartFP;    payPendingFP = payTermStartFP;    posPendingCount = 0;    if (termState.totalTermFreq < BLOCK_SIZE) {        lastPosBlockFP = posTermStartFP;    } else if (termState.totalTermFreq == BLOCK_SIZE) {        lastPosBlockFP = -1;    } else {        lastPosBlockFP = posTermStartFP + termState.lastPosBlockOffset;    }    this.needsOffsets = PostingsEnum.featureRequested(flags, PostingsEnum.OFFSETS);    this.needsPayloads = PostingsEnum.featureRequested(flags, PostingsEnum.PAYLOADS);    doc = -1;    accum = 0;    docUpto = 0;    if (docFreq > BLOCK_SIZE) {                nextSkipDoc = BLOCK_SIZE - 1;    } else {                nextSkipDoc = NO_MORE_DOCS;    }    docBufferUpto = BLOCK_SIZE;    skipped = false;    return this;}
0
public Map<String, Byte> getReversedExprs()
{    return reversedExprs;}
0
public Bits getLiveDocs()
{    final Bits actualLiveDocs = in.getLiveDocs();    if (roleQueryBits == null) {                return new Bits.MatchNoBits(in.maxDoc());    } else if (actualLiveDocs == null) {        return roleQueryBits;    } else {                return new CombinedBitSet(roleQueryBits, actualLiveDocs);    }}
0
public java.util.Map<java.lang.String, java.lang.Long> get_window_to_acked()
{    return this.window_to_acked;}
0
public void assignRowColumn(VectorizedRowBatch batch, int batchIndex, int columnNum, AggregationBuffer agg) throws HiveException
{    Decimal64ColumnVector outputColVector = (Decimal64ColumnVector) batch.cols[columnNum];    Aggregation myagg = (Aggregation) agg;    if (myagg.isNull || myagg.isOverflowed) {        outputColVector.noNulls = false;        outputColVector.isNull[batchIndex] = true;        return;    }    outputColVector.isNull[batchIndex] = false;    outputColVector.vector[batchIndex] = myagg.sum;}
0
public Integer getReplicationFactor()
{    return getNumNrtReplicas();}
0
public void remove()
{    throw new UnsupportedOperationException();}
0
private DisplayData build()
{    return new DisplayData(this.entries);}
0
public int hashCode()
{    List<Object> list = new ArrayList<Object>();    return list.hashCode();}
0
private static Collection<UDFunction> fetchUDFs(String keyspaceName, Types types)
{    String query = format("SELECT * FROM %s.%s WHERE keyspace_name = ?", SchemaConstants.SCHEMA_KEYSPACE_NAME, FUNCTIONS);    Collection<UDFunction> functions = new ArrayList<>();    for (UntypedResultSet.Row row : query(query, keyspaceName)) functions.add(createUDFFromRow(row, types));    return functions;}
0
public Boolean isEager()
{    return getProcessor().isEager();}
0
protected void initChannel(final Channel ch) throws Exception
{    super.initChannel(ch);    maybeAddIPFilter(ch, name);}
0
public Long getExchangesCompleted()
{    return exchangesCompleted;}
0
public DeleteEntity deleteEntity()
{    return new DeleteEntity(null, null);}
0
public void write(org.apache.storm.thrift.protocol.TProtocol oprot, TopologyInfo struct) throws org.apache.storm.thrift.TException
{    struct.validate();    oprot.writeStructBegin(STRUCT_DESC);    if (struct.id != null) {        oprot.writeFieldBegin(ID_FIELD_DESC);        oprot.writeString(struct.id);        oprot.writeFieldEnd();    }    if (struct.name != null) {        oprot.writeFieldBegin(NAME_FIELD_DESC);        oprot.writeString(struct.name);        oprot.writeFieldEnd();    }    oprot.writeFieldBegin(UPTIME_SECS_FIELD_DESC);    oprot.writeI32(struct.uptime_secs);    oprot.writeFieldEnd();    if (struct.executors != null) {        oprot.writeFieldBegin(EXECUTORS_FIELD_DESC);        {            oprot.writeListBegin(new org.apache.storm.thrift.protocol.TList(org.apache.storm.thrift.protocol.TType.STRUCT, struct.executors.size()));            for (ExecutorSummary _iter374 : struct.executors) {                _iter374.write(oprot);            }            oprot.writeListEnd();        }        oprot.writeFieldEnd();    }    if (struct.status != null) {        oprot.writeFieldBegin(STATUS_FIELD_DESC);        oprot.writeString(struct.status);        oprot.writeFieldEnd();    }    if (struct.errors != null) {        oprot.writeFieldBegin(ERRORS_FIELD_DESC);        {            oprot.writeMapBegin(new org.apache.storm.thrift.protocol.TMap(org.apache.storm.thrift.protocol.TType.STRING, org.apache.storm.thrift.protocol.TType.LIST, struct.errors.size()));            for (java.util.Map.Entry<java.lang.String, java.util.List<ErrorInfo>> _iter375 : struct.errors.entrySet()) {                oprot.writeString(_iter375.getKey());                {                    oprot.writeListBegin(new org.apache.storm.thrift.protocol.TList(org.apache.storm.thrift.protocol.TType.STRUCT, _iter375.getValue().size()));                    for (ErrorInfo _iter376 : _iter375.getValue()) {                        _iter376.write(oprot);                    }                    oprot.writeListEnd();                }            }            oprot.writeMapEnd();        }        oprot.writeFieldEnd();    }    if (struct.component_debug != null) {        if (struct.is_set_component_debug()) {            oprot.writeFieldBegin(COMPONENT_DEBUG_FIELD_DESC);            {                oprot.writeMapBegin(new org.apache.storm.thrift.protocol.TMap(org.apache.storm.thrift.protocol.TType.STRING, org.apache.storm.thrift.protocol.TType.STRUCT, struct.component_debug.size()));                for (java.util.Map.Entry<java.lang.String, DebugOptions> _iter377 : struct.component_debug.entrySet()) {                    oprot.writeString(_iter377.getKey());                    _iter377.getValue().write(oprot);                }                oprot.writeMapEnd();            }            oprot.writeFieldEnd();        }    }    if (struct.storm_version != null) {        if (struct.is_set_storm_version()) {            oprot.writeFieldBegin(STORM_VERSION_FIELD_DESC);            oprot.writeString(struct.storm_version);            oprot.writeFieldEnd();        }    }    if (struct.sched_status != null) {        if (struct.is_set_sched_status()) {            oprot.writeFieldBegin(SCHED_STATUS_FIELD_DESC);            oprot.writeString(struct.sched_status);            oprot.writeFieldEnd();        }    }    if (struct.owner != null) {        if (struct.is_set_owner()) {            oprot.writeFieldBegin(OWNER_FIELD_DESC);            oprot.writeString(struct.owner);            oprot.writeFieldEnd();        }    }    if (struct.is_set_replication_count()) {        oprot.writeFieldBegin(REPLICATION_COUNT_FIELD_DESC);        oprot.writeI32(struct.replication_count);        oprot.writeFieldEnd();    }    if (struct.is_set_requested_memonheap()) {        oprot.writeFieldBegin(REQUESTED_MEMONHEAP_FIELD_DESC);        oprot.writeDouble(struct.requested_memonheap);        oprot.writeFieldEnd();    }    if (struct.is_set_requested_memoffheap()) {        oprot.writeFieldBegin(REQUESTED_MEMOFFHEAP_FIELD_DESC);        oprot.writeDouble(struct.requested_memoffheap);        oprot.writeFieldEnd();    }    if (struct.is_set_requested_cpu()) {        oprot.writeFieldBegin(REQUESTED_CPU_FIELD_DESC);        oprot.writeDouble(struct.requested_cpu);        oprot.writeFieldEnd();    }    if (struct.is_set_assigned_memonheap()) {        oprot.writeFieldBegin(ASSIGNED_MEMONHEAP_FIELD_DESC);        oprot.writeDouble(struct.assigned_memonheap);        oprot.writeFieldEnd();    }    if (struct.is_set_assigned_memoffheap()) {        oprot.writeFieldBegin(ASSIGNED_MEMOFFHEAP_FIELD_DESC);        oprot.writeDouble(struct.assigned_memoffheap);        oprot.writeFieldEnd();    }    if (struct.is_set_assigned_cpu()) {        oprot.writeFieldBegin(ASSIGNED_CPU_FIELD_DESC);        oprot.writeDouble(struct.assigned_cpu);        oprot.writeFieldEnd();    }    oprot.writeFieldStop();    oprot.writeStructEnd();}
0
public static void beforeClass()
{    SecurityContextHolder.setContext(new SecurityContext() {        @Override        public Authentication getAuthentication() {            return new Authentication() {                @Override                public Collection<? extends GrantedAuthority> getAuthorities() {                    return null;                }                @Override                public Object getCredentials() {                    return null;                }                @Override                public Object getDetails() {                    return null;                }                @Override                public Object getPrincipal() {                    return new User(userName, "password", Collections.emptyList());                }                @Override                public boolean isAuthenticated() {                    return true;                }                @Override                public void setAuthenticated(boolean b) throws IllegalArgumentException {                }                @Override                public String getName() {                    return ((User) getPrincipal()).getUsername();                }            };        }        @Override        public void setAuthentication(Authentication authentication) {        }    });}
0
public void isLocatorReturnsFalseForNonLocatorMember()
{    BootstrappingFunction bootstrappingFunction = new BootstrappingFunction();    GemFireCacheImpl gemFireCacheImpl = mock(GemFireCacheImpl.class);    InternalDistributedSystem distributedSystem = mock(InternalDistributedSystem.class);    InternalDistributedMember internalDistributedMember = mock(InternalDistributedMember.class);    DistributionManager distributionManager = mock(DistributionManager.class);    when(gemFireCacheImpl.getDistributedSystem()).thenReturn(distributedSystem);    when(distributedSystem.getDistributionManager()).thenReturn(distributionManager);    when(distributedSystem.getDistributedMember()).thenReturn(internalDistributedMember);    when(internalDistributedMember.getVmKind()).thenReturn(ClusterDistributionManager.NORMAL_DM_TYPE);    assertThat(bootstrappingFunction.isLocator(gemFireCacheImpl)).isFalse();}
0
private void parseRules(List<String> rules, NormalizeCharMap.Builder map)
{    for (String rule : rules) {        Matcher m = rulePattern.matcher(rule);        if (!m.find())            throw new RuntimeException("Invalid Mapping Rule : [" + rule + "]");        String lhs = parseString(m.group(1).trim());        String rhs = parseString(m.group(2).trim());        if (lhs == null || rhs == null)            throw new RuntimeException("Invalid Mapping Rule : [" + rule + "]. Illegal mapping.");        map.add(lhs, rhs);    }}
0
public TermVectorsRequestBuilder setFieldStatistics(boolean fieldStatistics)
{    request.fieldStatistics(fieldStatistics);    return this;}
0
private float runThrottleTest(int blocks) throws IOException, InterruptedException, TimeoutException
{    scanner.setRetainDiffs(true);    scan(blocks, 0, 0, 0, 0, 0);    scanner.shutdown();    assertFalse(scanner.getRunStatus());    return (float) scanner.timeWaitingMs.get() / scanner.timeRunningMs.get();}
0
public void append(BytesWritable key, BytesWritable value) throws IOException
{    writer.append(key, value);}
0
private static S scheme(org.apache.thrift.protocol.TProtocol proto)
{    return (org.apache.thrift.scheme.StandardScheme.class.equals(proto.getScheme()) ? STANDARD_SCHEME_FACTORY : TUPLE_SCHEME_FACTORY).getScheme();}
0
private static LinearizabilityChecker.History readHistory(StreamInput input) throws IOException
{    int size = input.readInt();    List<LinearizabilityChecker.Event> events = new ArrayList<>(size);    for (int i = 0; i < size; ++i) {        events.add(readEvent(input));    }    return new LinearizabilityChecker.History(events);}
0
public boolean isCallerClosed()
{    return false;}
0
public RMContainer getRMContainer()
{    return this.rmContainer;}
0
public Partition appendPartition(String catName, String dbName, String tableName, String name) throws TException
{    Partition p = client.append_partition_by_name(prependCatalogToDbName(catName, dbName, conf), tableName, name);    return deepCopy(p);}
0
private boolean getPartitionVectors()
{    try {        if (!saveSamples()) {            return false;        }        CachedVectorContainer finalTable = null;        long val = minorFragmentSampleCount.incrementAndGet();                final long fragmentsBeforeProceed = (long) Math.ceil(sendingMajorFragmentWidth * completionFactor);        final String finalTableKey = mapKey + "final";        if (val == fragmentsBeforeProceed) {                        buildTable();            finalTable = tableMap.get(finalTableKey);        } else {            if (val < fragmentsBeforeProceed) {                if (!waitUntilTimeOut(10)) {                    return false;                }            }            for (int i = 0; i < 100 && finalTable == null; i++) {                finalTable = tableMap.get(finalTableKey);                if (finalTable != null) {                    break;                }                if (!waitUntilTimeOut(10)) {                    return false;                }            }            if (finalTable == null) {                buildTable();            }            finalTable = tableMap.get(finalTableKey);        }        Preconditions.checkState(finalTable != null);                for (VectorWrapper<?> w : finalTable.get()) {            partitionVectors.add(w.getValueVector());        }    } catch (final ClassTransformationException | IOException | SchemaChangeException ex) {        kill(false);        context.getExecutorState().fail(ex);        return false;        }    return true;}
1
public List<ExternalFirewallDeviceVO> listByProviderAndDeviceStaus(long physicalNetworkId, String providerName, FirewallDeviceState state)
{    SearchCriteria<ExternalFirewallDeviceVO> sc = deviceStatusSearch.create();    sc.setParameters("physicalNetworkId", physicalNetworkId);    sc.setParameters("providerName", providerName);    sc.setParameters("deviceState", state);    return search(sc, null);}
0
public String getLabelResource()
{        return "workbench_title";}
0
public long bumpCount(long refWord, WriteBuffers.Position unsafeReadPos)
{        /*     * Extract information from the reference word.     */    final long countAbsoluteOffset = KeyRef.getAbsoluteOffset(refWord) - MultiSetCount.byteLength;    final int currentCount = writeBuffers.readInt(countAbsoluteOffset, unsafeReadPos);        refWord &= KeyRef.IsSingleFlag.flagOffMask;        final long saveAbsoluteOffset = writeBuffers.getWritePoint();    writeBuffers.setWritePoint(countAbsoluteOffset);    writeBuffers.writeInt(countAbsoluteOffset, currentCount + 1);        writeBuffers.setWritePoint(saveAbsoluteOffset);    return refWord;}
0
public static ConnectionResponse createConflictingNodeIdResponse(final String otherNode)
{    return new ConnectionResponse("The Node Identifier provided already belongs to node " + otherNode);}
0
public void map(IntWritable key, IntWritable val, Context context) throws IOException
{        Configuration tlConf = new YarnConfiguration();    TimelineCollectorManager manager = new TimelineCollectorManager("test");    manager.init(tlConf);    manager.start();    try {                writeEntities(tlConf, manager, context);    } finally {        manager.close();    }}
0
public Long getDiskOfferingId()
{    return diskOfferingId;}
0
public void run()
{    route.setId(null);    route.setCustomId(false);}
0
public void startGroup() throws HiveException
{        defaultStartGroup();}
0
public long cost()
{    return docs.length;}
0
public BytesRef encodeRanges(Set<RangeFieldMapper.Range> ranges) throws IOException
{    return BinaryRangeUtil.encodeFloatRanges(ranges);}
0
public Connection connect(String url, Properties info) throws SQLException
{    if (!this.acceptsURL(url)) {        return null;    }    Connection connection = super.connect(url, info);    CalciteConnection calciteConnection = (CalciteConnection) connection;    final SchemaPlus rootSchema = calciteConnection.getRootSchema();    String schemaName = info.getProperty("zk");    if (schemaName == null) {        throw new SQLException("zk must be set");    }    rootSchema.add(schemaName, new SolrSchema(info));        calciteConnection.setSchema(schemaName);    return connection;}
0
public Counters getCounters()
{    return this.counters;}
0
public void testRemoteCreateInReplicatedRegion()
{    Host host = Host.getHost(0);    VM accessor = host.getVM(0);    VM datastore = host.getVM(1);    initAccessorAndDataStore(accessor, datastore, 0);    accessor.invoke(new DoOpsInTX(OP.PUT));    accessor.invoke(new SerializableCallable() {        @Override        public Object call() throws Exception {            Region refRegion = getCache().getRegion(D_REFERENCE);            refRegion.create("sup", "dawg");            return null;        }    });    accessor.invoke(new SerializableCallable() {        @Override        public Object call() throws Exception {            TXManagerImpl mgr = getGemfireCache().getTxManager();            TXStateProxy tx = mgr.pauseTransaction();            assertNotNull(tx);            mgr.unpauseTransaction(tx);            mgr.commit();            return null;        }    });    accessor.invoke(new SerializableCallable() {        @Override        public Object call() throws Exception {            Region refRegion = getCache().getRegion(D_REFERENCE);            assertEquals("dawg", refRegion.get("sup"));            return null;        }    });}
0
public MetaException getO1()
{    return this.o1;}
0
public String[] indices()
{    return indices;}
0
public long getLastRead()
{    return lastRead;}
0
public void setCurrentTransform(AppliedPTransform<?, ?, ?> transform)
{    this.currentTransform = transform;}
0
public boolean isShowCloud()
{    return showCloud;}
0
public void eval()
{    com.esri.core.geometry.ogc.OGCGeometry geom1;    geom1 = com.esri.core.geometry.ogc.OGCGeometry.fromBinary(geom1Param.buffer.nioBuffer(geom1Param.start, geom1Param.end - geom1Param.start));    if (geom1.geometryType().equals("Point")) {        out.value = ((com.esri.core.geometry.ogc.OGCPoint) geom1).Y();    } else {        com.esri.core.geometry.Envelope envelope = new com.esri.core.geometry.Envelope();        geom1.getEsriGeometry().queryEnvelope(envelope);        out.value = envelope.getYMin();    }}
0
public String toString()
{    return keyId + "/" + blockPoolId + "/" + nonce.length + "/" + encryptionKey.length;}
0
public H1<T> button(String selector, String cdata)
{    return setSelector(button(), selector)._(cdata)._();}
0
public void updateState(List<TridentTuple> tuples, TridentCollector collector)
{    List<List<Column>> columnsLists = new ArrayList<List<Column>>();    for (TridentTuple tuple : tuples) {        columnsLists.add(options.mapper.getColumns(tuple));    }    try {        if (!StringUtils.isBlank(options.tableName)) {            jdbcClient.insert(options.tableName, columnsLists);        } else {            jdbcClient.executeInsertQuery(options.insertQuery, columnsLists);        }    } catch (Exception e) {                throw new FailedException(e);    }}
1
private Object executeAggregateAsync(ProtocolVersion protocolVersion, Object firstParam, List<ByteBuffer> parameters)
{    ThreadIdAndCpuTime threadIdAndCpuTime = new ThreadIdAndCpuTime();    return async(threadIdAndCpuTime, () -> {        threadIdAndCpuTime.setup();        return executeAggregateUserDefined(protocolVersion, firstParam, parameters);    });}
0
protected void testProgram() throws Exception
{    final ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();    DataSet<String> text = env.readTextFile(textPath);    DataSet<WC> counts = text.flatMap(new Tokenizer()).groupBy("word").reduce(new ReduceFunction<WC>() {        private static final long serialVersionUID = 1L;        public WC reduce(WC value1, WC value2) {            return new WC(value1.word, value1.count + value2.count);        }    });    counts.writeAsText(resultPath);    env.execute("WordCount with custom data types example");}
0
public ImmutableList<RexNode> fields(Mappings.TargetMapping mapping)
{    return fields(Mappings.asList(mapping));}
0
public void resetStateIfNecessary(final ProcessContext context) throws IOException
{    if (resetState) {        getLogger().debug("Property has been modified. Resetting the state values - listing.timestamp and emitted.timestamp to -1L");        context.getStateManager().clear(Scope.CLUSTER);        this.resetState = false;    }}
0
public void testGoodDesc() throws IOException
{    AggregationGroupRule rule = getAggregationGroupRule();    for (File f : new File(LocalFileMetadataTestCase.LOCALMETA_TEMP_DATA + "/cube_desc/").listFiles()) {        if (!f.getName().endsWith("json")) {            continue;        }        CubeDesc desc = JsonUtil.readValue(new FileInputStream(f), CubeDesc.class);        desc.init(getTestConfig());        ValidateContext vContext = new ValidateContext();        rule.validate(desc, vContext);                assertTrue(vContext.getResults().length == 0);    }}
0
public String getClientFactory()
{    return clientFactory;}
0
protected InetSocketAddress getProtocolAddress(Configuration conf) throws IOException
{    return conf.getSocketAddr(YarnConfiguration.RM_ADMIN_ADDRESS, YarnConfiguration.DEFAULT_RM_ADMIN_ADDRESS, YarnConfiguration.DEFAULT_RM_ADMIN_PORT);}
0
public void subTestIntervalMonthPositive()
{        expr("INTERVAL '1' MONTH").columnType("INTERVAL MONTH NOT NULL");    expr("INTERVAL '99' MONTH").columnType("INTERVAL MONTH NOT NULL");        expr("INTERVAL '1' MONTH(2)").columnType("INTERVAL MONTH(2) NOT NULL");    expr("INTERVAL '99' MONTH(2)").columnType("INTERVAL MONTH(2) NOT NULL");        expr("INTERVAL '2147483647' MONTH(10)").columnType("INTERVAL MONTH(10) NOT NULL");        expr("INTERVAL '0' MONTH(1)").columnType("INTERVAL MONTH(1) NOT NULL");        expr("INTERVAL '1234' MONTH(4)").columnType("INTERVAL MONTH(4) NOT NULL");        expr("INTERVAL '+1' MONTH").columnType("INTERVAL MONTH NOT NULL");    expr("INTERVAL '-1' MONTH").columnType("INTERVAL MONTH NOT NULL");    expr("INTERVAL +'1' MONTH").columnType("INTERVAL MONTH NOT NULL");    expr("INTERVAL +'+1' MONTH").columnType("INTERVAL MONTH NOT NULL");    expr("INTERVAL +'-1' MONTH").columnType("INTERVAL MONTH NOT NULL");    expr("INTERVAL -'1' MONTH").columnType("INTERVAL MONTH NOT NULL");    expr("INTERVAL -'+1' MONTH").columnType("INTERVAL MONTH NOT NULL");    expr("INTERVAL -'-1' MONTH").columnType("INTERVAL MONTH NOT NULL");}
0
 void dispatch(final BatchedRequests batchedRequests)
{    final ConstraintPlacementAlgorithmOutputCollector collector = this;    Runnable placingTask = () -> {                algorithm.place(batchedRequests, collector);    };    this.algorithmThreadPool.submit(placingTask);}
1
 AdvancedInfinispanEndpointConsumerBuilder remappingFunction(String remappingFunction)
{    doSetProperty("remappingFunction", remappingFunction);    return this;}
0
public static void setup()
{    PipelineOptionsFactory.register(TestPipelineOptions.class);}
0
public float getLabeledQueueCapacity(String queue, String label)
{    return internalGetLabeledQueueCapacity(queue, label, CAPACITY, 0f);}
0
public void testIndexJoinValidator_FullyCurrentCluster() throws Exception
{    createComponents(Settings.EMPTY);    BiConsumer<DiscoveryNode, ClusterState> joinValidator = security.getJoinValidator();    assertNotNull(joinValidator);    DiscoveryNode node = new DiscoveryNode("foo", buildNewFakeTransportAddress(), Version.CURRENT);    int indexFormat = randomBoolean() ? INTERNAL_MAIN_INDEX_FORMAT : INTERNAL_MAIN_INDEX_FORMAT - 1;    IndexMetaData indexMetaData = IndexMetaData.builder(SECURITY_MAIN_ALIAS).settings(settings(VersionUtils.randomIndexCompatibleVersion(random())).put(INDEX_FORMAT_SETTING.getKey(), indexFormat)).numberOfShards(1).numberOfReplicas(0).build();    DiscoveryNode existingOtherNode = new DiscoveryNode("bar", buildNewFakeTransportAddress(), Version.CURRENT);    DiscoveryNodes discoveryNodes = DiscoveryNodes.builder().add(existingOtherNode).build();    ClusterState clusterState = ClusterState.builder(ClusterName.DEFAULT).nodes(discoveryNodes).metaData(MetaData.builder().put(indexMetaData, true).build()).build();    joinValidator.accept(node, clusterState);}
0
public void testTableQueryExtractYearQuarter()
{    final String sql = "SELECT * FROM (SELECT CAST((MONTH(\"timestamp\") - 1) / 3 + 1 AS BIGINT)" + "AS qr_timestamp_ok,  SUM(\"store_sales\") AS sum_store_sales, YEAR(\"timestamp\") AS yr_timestamp_ok" + " FROM \"foodmart\" GROUP BY CAST((MONTH(\"timestamp\") - 1) / 3 + 1 AS BIGINT)," + " YEAR(\"timestamp\")) LIMIT_ZERO LIMIT 1";    final String extract_year = "{\"type\":\"extraction\",\"dimension\":\"__time\",\"outputName\":" + "\"extract_year\",\"extractionFn\":{\"type\":\"timeFormat\",\"format\":\"yyyy\"," + "\"timeZone\":\"UTC\",\"locale\":\"en-US\"}}";    final String extract_expression = "\"expression\":\"(((timestamp_extract(\\\"__time\\\",";    sql(sql, FOODMART).returnsOrdered("QR_TIMESTAMP_OK=1; SUM_STORE_SALES=139628.34999999971; YR_TIMESTAMP_OK=1997").queryContains(new DruidChecker("\"queryType\":\"groupBy\"", extract_year, extract_expression)).explainContains("PLAN=EnumerableInterpreter\n" + "  BindableProject(QR_TIMESTAMP_OK=[$0], SUM_STORE_SALES=[$2], YR_TIMESTAMP_OK=[$1])\n" + "    DruidQuery(table=[[foodmart, foodmart]], intervals=[[1900-01-09T00:00:00.000Z/" + "2992-01-10T00:00:00.000Z]], projects=[[+(/(-(EXTRACT(FLAG(MONTH), $0), 1), 3), 1), " + "EXTRACT(FLAG(YEAR), $0), $90]], groups=[{0, 1}], aggs=[[SUM($2)]], fetch=[1])");}
0
public void verifyDeterministic()
{}
0
private EnvironmentContext initializeEnvironmentContext(EnvironmentContext environmentContext)
{    EnvironmentContext result = environmentContext == null ? new EnvironmentContext() : environmentContext;        if (result.getProperties() == null || result.getProperties().get(StatsSetupConst.DO_NOT_UPDATE_STATS) == null) {        result.putToProperties(StatsSetupConst.DO_NOT_UPDATE_STATS, StatsSetupConst.TRUE);    }    return result;}
0
public void testConcurrentWriteReplicaResultCompletion() throws InterruptedException
{    IndexShard replica = mock(IndexShard.class);    when(replica.getTranslogDurability()).thenReturn(Translog.Durability.ASYNC);    TestRequest request = new TestRequest();    request.setRefreshPolicy(RefreshPolicy.WAIT_UNTIL);    TransportWriteAction.WriteReplicaResult<TestRequest> replicaResult = new TransportWriteAction.WriteReplicaResult<>(request, new Translog.Location(0, 0, 0), null, replica, logger);    CyclicBarrier barrier = new CyclicBarrier(2);    Runnable waitForBarrier = () -> {        try {            barrier.await();        } catch (InterruptedException | BrokenBarrierException e) {            throw new AssertionError(e);        }    };    CountDownLatch completionLatch = new CountDownLatch(1);    threadPool.generic().execute(() -> {        waitForBarrier.run();        replicaResult.respond(ActionListener.wrap(completionLatch::countDown));    });    if (randomBoolean()) {        threadPool.generic().execute(() -> {            waitForBarrier.run();            replicaResult.onFailure(null);        });    } else {        threadPool.generic().execute(() -> {            waitForBarrier.run();            replicaResult.onSuccess(false);        });    }    assertTrue(completionLatch.await(30, TimeUnit.SECONDS));}
0
protected RequestStatus updateResourcesAuthorized(final Request request, Predicate predicate) throws SystemException, UnsupportedPropertyException, NoSuchResourceException, NoSuchParentResourceException
{    throw new SystemException("Cannot update Version Definitions");}
0
public DataStructure createObject()
{    return new DestinationInfo();}
0
protected RouteBuilder createRouteBuilder()
{    return new RouteBuilder() {        public void configure() {            from("direct:start").script(body().append(" World!")).to("mock:result");        }    };}
0
public String getCommandName()
{    return s_name;}
0
public void onResponse(InputStream response, Map<String, String> headers, SalesforceException ex)
{    SyncReportResults reportResults = null;    try {        reportResults = unmarshalResponse(response, request, SyncReportResults.class);    } catch (SalesforceException e) {        ex = e;    }    callback.onResponse(reportResults, headers, ex);}
0
public Scorer getFragmentScorer()
{    return fragmentScorer;}
0
public void commit() throws IOException
{    throw new IOException("Input/output error");}
0
public Map<String, String> getKafkaConfigOverride()
{    return getPropertiesByPrefix("kylin.source.kafka.config-override.");}
0
private Map<TblColRef, Dictionary<String>> buildDictionaryMap(List<TblColRef> columnsNeedDictionary)
{    Map<TblColRef, Dictionary<String>> result = Maps.newHashMap();    for (TblColRef col : columnsNeedDictionary) {        result.put(col, cubeSeg.getDictionary(col));    }    return result;}
0
public boolean isSkipLockedEntity()
{    return skipLockedEntity;}
0
private long skipToStartTime(long startTime)
{    if (lastEndTimeMs != null) {        if (lastEndTimeMs + 1 > startTime) {                        return lastEndTimeMs + 1;        }                FlushJobAction.Request request = new FlushJobAction.Request(jobId);        request.setSkipTime(String.valueOf(startTime));        FlushJobAction.Response flushResponse = flushJob(request);                return flushResponse.getLastFinalizedBucketEnd().getTime();    }    return startTime;}
1
public String logPrefix()
{    return logPrefix;}
0
private void testCreateLdapContextSSLSocketFactory(boolean trusting) throws Exception
{    Injector injector = getInjector();    Configuration configuration = injector.getInstance(Configuration.class);    expect(configuration.validateKerberosOperationSSLCertTrust()).andReturn(!trusting).once();    LdapContext initialContext = createNiceMock(LdapContext.class);    Capture<? extends Properties> capturedProperties = newCapture(CaptureType.FIRST);    ADKerberosOperationHandler handler = createMockBuilder(ADKerberosOperationHandler.class).addMockedMethod(ADKerberosOperationHandler.class.getDeclaredMethod("createInitialLdapContext", Properties.class, Control[].class)).createNiceMock();    injector.injectMembers(handler);    expect(handler.createInitialLdapContext(capture(capturedProperties), anyObject(Control[].class))).andReturn(initialContext).once();    replayAll();    handler.open(new PrincipalKeyCredential("principal", "key"), "EXAMPLE.COM", getKerberosEnv());    Properties properties = capturedProperties.getValue();    Assert.assertNotNull(properties);    String socketFactoryClassName = properties.getProperty("java.naming.ldap.factory.socket");    if (trusting) {        Assert.assertEquals(InternalSSLSocketFactoryTrusting.class.getName(), socketFactoryClassName);    } else {        Assert.assertEquals(InternalSSLSocketFactoryNonTrusting.class.getName(), socketFactoryClassName);    }}
0
public static void doTestTableMutations(Hbase.Iface handler) throws Exception
{        handler.createTable(tableAname, getColumnDescriptors());                handler.mutateRow(tableAname, rowAname, getMutations(), null);        assertEquals(valueAname, handler.get(tableAname, rowAname, columnAname, null).get(0).value);    TRowResult rowResult1 = handler.getRow(tableAname, rowAname, null).get(0);    assertEquals(rowAname, rowResult1.row);    assertEquals(valueBname, rowResult1.columns.get(columnBname).value);                                    handler.mutateRows(tableAname, getBatchMutations(), null);        List<TCell> cells = handler.get(tableAname, rowAname, columnAname, null);    assertFalse(cells.size() > 0);    assertEquals(valueCname, handler.get(tableAname, rowAname, columnBname, null).get(0).value);    List<TCell> versions = handler.getVer(tableAname, rowAname, columnBname, MAXVERSIONS, null);    assertEquals(valueCname, versions.get(0).value);    assertEquals(valueBname, versions.get(1).value);        TRowResult rowResult2 = handler.getRow(tableAname, rowBname, null).get(0);    assertEquals(rowBname, rowResult2.row);    assertEquals(valueCname, rowResult2.columns.get(columnAname).value);    assertEquals(valueDname, rowResult2.columns.get(columnBname).value);        handler.deleteAll(tableAname, rowAname, columnBname, null);    handler.deleteAllRow(tableAname, rowBname, null);        int size = handler.get(tableAname, rowAname, columnBname, null).size();    assertEquals(0, size);    size = handler.getRow(tableAname, rowBname, null).size();    assertEquals(0, size);        List<Mutation> mutations = new ArrayList<>(1);    mutations.add(new Mutation(false, columnAname, null, true));    handler.mutateRow(tableAname, rowAname, mutations, null);    TRowResult rowResult3 = handler.getRow(tableAname, rowAname, null).get(0);    assertEquals(rowAname, rowResult3.row);    assertEquals(0, rowResult3.columns.get(columnAname).value.remaining());        handler.disableTable(tableAname);    handler.deleteTable(tableAname);}
0
public void doFillReplay(Random random, int maxCount) throws Exception
{    RandomByteArrayStream randomByteArrayStream = new RandomByteArrayStream(random);    VectorRowBytesContainer vectorMapJoinRowBytesContainer = new VectorRowBytesContainer(null);    int count = Math.min(maxCount, random.nextInt(500));    for (int i = 0; i < count; i++) {        byte[] bytes = randomByteArrayStream.next();        Output output = vectorMapJoinRowBytesContainer.getOuputForRowBytes();        output.write(bytes);        vectorMapJoinRowBytesContainer.finishRow();    }    vectorMapJoinRowBytesContainer.prepareForReading();    for (int i = 0; i < count; i++) {        if (!vectorMapJoinRowBytesContainer.readNext()) {            assertTrue(false);        }        byte[] readBytes = vectorMapJoinRowBytesContainer.currentBytes();        int readOffset = vectorMapJoinRowBytesContainer.currentOffset();        int readLength = vectorMapJoinRowBytesContainer.currentLength();        byte[] expectedBytes = randomByteArrayStream.get(i);        if (readLength != expectedBytes.length) {            assertTrue(false);        }        for (int j = 0; j < readLength; j++) {            byte readByte = readBytes[readOffset + j];            byte expectedByte = expectedBytes[j];            if (readByte != expectedByte) {                assertTrue(false);            }        }    }}
0
public java.lang.Object getFieldValue(_Fields field)
{    switch(field) {        case AZE:            return get_aze();    }    throw new java.lang.IllegalStateException();}
0
private SELECT<T> select_(T e, boolean inline)
{    return new SELECT<T>("select", e, opt(true, inline, false));}
0
public void testMessagesThroughDifferentRoutes() throws Exception
{    resultEndpoint.expectedBodiesReceived("one", "two", "three");    sendMessage("bar", "one");    sendMessage("cheese", "two");    sendMessage("somethingUndefined", "three");    resultEndpoint.assertIsSatisfied();}
0
public boolean canUndo()
{    return undoHistory.canUndo();}
0
protected void doXContent(XContentBuilder builder, Params params) throws IOException
{    builder.startObject(NAME);    builder.field(FIELD_FIELD.getPreferredName(), field);    builder.field(ORIGIN_FIELD.getPreferredName(), origin.origin);    builder.field(PIVOT_FIELD.getPreferredName(), pivot);    printBoostAndQueryName(builder);    builder.endObject();}
0
public void testExclude() throws IOException
{    CharArraySet exclusionSet = new CharArraySet(asSet("zaldiak"), false);    Analyzer a = new BasqueAnalyzer(BasqueAnalyzer.getDefaultStopSet(), exclusionSet);    checkOneTerm(a, "zaldiak", "zaldiak");    checkOneTerm(a, "mendiari", "mendi");    a.close();}
0
private void assertResponseMessage(Message message)
{    assertEquals(204, message.getHeader(HipchatConstants.TO_ROOM_RESPONSE_STATUS, StatusLine.class).getStatusCode());    assertEquals(204, message.getHeader(HipchatConstants.TO_USER_RESPONSE_STATUS, StatusLine.class).getStatusCode());}
0
public int getIndex()
{    return 1;}
0
public CacheTransactionManager getCacheTransactionManager()
{    return transactionManager;}
0
public int handshake(boolean read, boolean write) throws IOException
{    if (handshakeComplete) {                return 0;    }    if (!sniComplete) {        int sniResult = processSNI();        if (sniResult == 0) {            sniComplete = true;        } else {            return sniResult;        }    }    if (!flush(netOutBuffer)) {                return SelectionKey.OP_WRITE;    }    SSLEngineResult handshake = null;    while (!handshakeComplete) {        switch(handshakeStatus) {            case NOT_HANDSHAKING:                                throw new IOException(sm.getString("channel.nio.ssl.notHandshaking"));            case FINISHED:                if (endpoint.hasNegotiableProtocols()) {                    if (sslEngine instanceof SSLUtil.ProtocolInfo) {                        socketWrapper.setNegotiatedProtocol(((SSLUtil.ProtocolInfo) sslEngine).getNegotiatedProtocol());                    } else if (JreCompat.isJre9Available()) {                        socketWrapper.setNegotiatedProtocol(JreCompat.getInstance().getApplicationProtocol(sslEngine));                    }                }                                handshakeComplete = !netOutBuffer.hasRemaining();                                return handshakeComplete ? 0 : SelectionKey.OP_WRITE;            case NEED_WRAP:                                try {                    handshake = handshakeWrap(write);                } catch (SSLException e) {                    if (log.isDebugEnabled()) {                                            }                    handshake = handshakeWrap(write);                }                if (handshake.getStatus() == Status.OK) {                    if (handshakeStatus == HandshakeStatus.NEED_TASK) {                        handshakeStatus = tasks();                    }                } else if (handshake.getStatus() == Status.CLOSED) {                    flush(netOutBuffer);                    return -1;                } else {                                        throw new IOException(sm.getString("channel.nio.ssl.unexpectedStatusDuringWrap", handshake.getStatus()));                }                if (handshakeStatus != HandshakeStatus.NEED_UNWRAP || (!flush(netOutBuffer))) {                                        return SelectionKey.OP_WRITE;                }                        case NEED_UNWRAP:                                handshake = handshakeUnwrap(read);                if (handshake.getStatus() == Status.OK) {                    if (handshakeStatus == HandshakeStatus.NEED_TASK) {                        handshakeStatus = tasks();                    }                } else if (handshake.getStatus() == Status.BUFFER_UNDERFLOW) {                                        return SelectionKey.OP_READ;                } else {                    throw new IOException(sm.getString("channel.nio.ssl.unexpectedStatusDuringWrap", handshake.getStatus()));                }                break;            case NEED_TASK:                handshakeStatus = tasks();                break;            default:                throw new IllegalStateException(sm.getString("channel.nio.ssl.invalidStatus", handshakeStatus));        }    }        return 0;}
1
private T validate(T array)
{    boolean success = false;    try {        adjustBreaker(array.ramBytesUsed(), true);        success = true;    } finally {        if (!success) {            Releasables.closeWhileHandlingException(array);        }    }    return array;}
0
public void testInvokingServiceFromCXFClient() throws Exception
{    URL wsdlURL = getClass().getClassLoader().getResource("person-non-wrapper.wsdl");    PersonService ss = new PersonService(wsdlURL, new QName("http://camel.apache.org/non-wrapper", "PersonService"));    Person client = ss.getSoap();    ((BindingProvider) client).getRequestContext().put(BindingProvider.ENDPOINT_ADDRESS_PROPERTY, "http://localhost:" + port1 + "/CxfNonWrapperTest/PersonService/");    GetPerson request = new GetPerson();    request.setPersonId("hello");    GetPersonResponse response = client.getPerson(request);    assertEquals("we should get the right answer from router", "Bonjour", response.getName());    request.setPersonId("");    try {        client.getPerson(request);        fail("We expect to get the UnknowPersonFault here");    } catch (UnknownPersonFault fault) {        }}
0
public String getErrorMessage()
{    return errorMessage;}
0
public boolean isEvicted()
{    return areAnyBitsSet(EVICTED);}
0
public void testMultipleSchemaParameters()
{    DoFnSignature sig = DoFnSignatures.getSignature(new DoFn<String, String>() {        @ProcessElement        public void process(@Element Row row1, @Timestamp Instant ts, @Element Row row2, OutputReceiver<String> o, @Element Integer intParameter) {        }    }.getClass());    assertEquals(3, sig.processElement().getSchemaElementParameters().size());    assertEquals(0, sig.processElement().getSchemaElementParameters().get(0).index());    assertEquals(TypeDescriptors.rows(), sig.processElement().getSchemaElementParameters().get(0).elementT());    assertEquals(1, sig.processElement().getSchemaElementParameters().get(1).index());    assertEquals(TypeDescriptors.rows(), sig.processElement().getSchemaElementParameters().get(1).elementT());    assertEquals(2, sig.processElement().getSchemaElementParameters().get(2).index());    assertEquals(TypeDescriptors.integers(), sig.processElement().getSchemaElementParameters().get(2).elementT());}
0
public void onCommand(Object command)
{    if (command instanceof BrokerInfo) {        brokerInfo = (BrokerInfo) command;    }}
0
public boolean isMoreSpecificThanWarn()
{    if (level != null) {        return level.isMoreSpecificThan(Level.WARN);    }    return false;}
0
public void testGlobExpansion() throws IOException
{    Objects modelObjects = new Objects();    List<StorageObject> items = new ArrayList<>();        items.add(new StorageObject().setBucket("testbucket").setName("testdirectory/"));        items.add(createStorageObject("gs://testbucket/testdirectory/file1name", 1L));    items.add(createStorageObject("gs://testbucket/testdirectory/file2name", 2L));    items.add(createStorageObject("gs://testbucket/testdirectory/file3name", 3L));    items.add(createStorageObject("gs://testbucket/testdirectory/otherfile", 4L));    items.add(createStorageObject("gs://testbucket/testdirectory/anotherfile", 5L));    items.add(createStorageObject("gs://testbucket/testotherdirectory/file4name", 6L));    modelObjects.setItems(items);    when(mockGcsUtil.listObjects(eq("testbucket"), anyString(), isNull(String.class))).thenReturn(modelObjects);        {        GcsPath pattern = GcsPath.fromUri("gs://testbucket/testdirectory/file*");        List<String> expectedFiles = ImmutableList.of("gs://testbucket/testdirectory/file1name", "gs://testbucket/testdirectory/file2name", "gs://testbucket/testdirectory/file3name");        assertThat(expectedFiles, contains(toFilenames(gcsFileSystem.expand(pattern)).toArray()));    }    {        GcsPath pattern = GcsPath.fromUri("gs://testbucket/testdirectory/file*");        List<String> expectedFiles = ImmutableList.of("gs://testbucket/testdirectory/file1name", "gs://testbucket/testdirectory/file2name", "gs://testbucket/testdirectory/file3name");        assertThat(expectedFiles, contains(toFilenames(gcsFileSystem.expand(pattern)).toArray()));    }    {        GcsPath pattern = GcsPath.fromUri("gs://testbucket/testdirectory/file[1-3]*");        List<String> expectedFiles = ImmutableList.of("gs://testbucket/testdirectory/file1name", "gs://testbucket/testdirectory/file2name", "gs://testbucket/testdirectory/file3name");        assertThat(expectedFiles, contains(toFilenames(gcsFileSystem.expand(pattern)).toArray()));    }    {        GcsPath pattern = GcsPath.fromUri("gs://testbucket/testdirectory/file?name");        List<String> expectedFiles = ImmutableList.of("gs://testbucket/testdirectory/file1name", "gs://testbucket/testdirectory/file2name", "gs://testbucket/testdirectory/file3name");        assertThat(expectedFiles, contains(toFilenames(gcsFileSystem.expand(pattern)).toArray()));    }    {        GcsPath pattern = GcsPath.fromUri("gs://testbucket/test*ectory/fi*name");        List<String> expectedFiles = ImmutableList.of("gs://testbucket/testdirectory/file1name", "gs://testbucket/testdirectory/file2name", "gs://testbucket/testdirectory/file3name", "gs://testbucket/testotherdirectory/file4name");        assertThat(expectedFiles, contains(toFilenames(gcsFileSystem.expand(pattern)).toArray()));    }}
0
public void testWithoutGenerateRetraction() throws Exception
{    MiniBatchDeduplicateKeepLastRowFunction func = createFunction(false);    OneInputStreamOperatorTestHarness<BaseRow, BaseRow> testHarness = createTestHarness(func);    testHarness.open();    testHarness.processElement(record("book", 1L, 10));    testHarness.processElement(record("book", 2L, 11));        Assert.assertTrue(testHarness.getOutput().isEmpty());    testHarness.processElement(record("book", 1L, 13));    List<Object> expectedOutput = new ArrayList<>();    expectedOutput.add(record("book", 2L, 11));    expectedOutput.add(record("book", 1L, 13));    assertor.assertOutputEqualsSorted("output wrong.", expectedOutput, testHarness.getOutput());    testHarness.processElement(record("book", 1L, 12));    testHarness.processElement(record("book", 2L, 11));    testHarness.processElement(record("book", 3L, 11));    expectedOutput.add(record("book", 1L, 12));    expectedOutput.add(record("book", 2L, 11));    expectedOutput.add(record("book", 3L, 11));    testHarness.close();    assertor.assertOutputEqualsSorted("output wrong.", expectedOutput, testHarness.getOutput());}
0
public static final com.google.protobuf.Descriptors.Descriptor getDescriptor()
{    return org.apache.drill.exec.proto.UserProtos.internal_static_exec_user_LikeFilter_descriptor;}
0
public void addRegion(HRegion r)
{}
0
public void setValue(Object value)
{    if (value == null) {        setNull();    } else {        setInt((int) value);    }}
0
public void testCreateStateMultipleInvocations() throws Exception
{    final OidcService service = getServiceWithOidcSupport();    service.createState(TEST_REQUEST_IDENTIFIER);    service.createState(TEST_REQUEST_IDENTIFIER);}
0
public void testConnectionFactoryReturnsCorrectLDAPConnectionOptionsWithDefaultSettings() throws Exception
{    final Environment environment = TestEnvironment.newEnvironment(Settings.builder().put("path.home", createTempDir()).build());    RealmConfig realmConfig = new RealmConfig(new RealmConfig.RealmIdentifier("ldap", "conn_settings"), environment.settings(), environment, new ThreadContext(Settings.EMPTY));    LDAPConnectionOptions options = SessionFactory.connectionOptions(realmConfig, new SSLService(environment.settings(), environment), logger);    assertThat(options.followReferrals(), is(equalTo(true)));    assertThat(options.allowConcurrentSocketFactoryUse(), is(equalTo(true)));    assertThat(options.getConnectTimeoutMillis(), is(equalTo(5000)));    assertThat(options.getResponseTimeoutMillis(), is(equalTo(5000L)));    assertThat(options.getSSLSocketVerifier(), is(instanceOf(HostNameSSLSocketVerifier.class)));}
0
public ServerSocket createServerSocket(int port) throws IOException
{    SSLServerSocket socket = (SSLServerSocket) sslServerSocketFactory.createServerSocket(port);    configureServerSocket(socket);    return socket;}
0
public boolean getSendJvmMetrics()
{    return this.sendJvmMetrics;}
0
public void testServerLocationUsedInListenerNotification() throws Exception
{    final ClientMembershipEvent[] listenerEvents = new ClientMembershipEvent[1];    ClientMembershipListener listener = new ClientMembershipListener() {        @Override        public void memberJoined(final ClientMembershipEvent event) {            synchronized (listenerEvents) {                listenerEvents[0] = event;            }        }        @Override        public void memberLeft(final ClientMembershipEvent event) {        }        @Override        public void memberCrashed(final ClientMembershipEvent event) {        }    };    InternalClientMembership.registerClientMembershipListener(listener);    ServerLocation location = new ServerLocation("1.1.1.1", 0);    InternalClientMembership.notifyServerJoined(location);    await("wait for listener notification").until(() -> {        synchronized (listenerEvents) {            return listenerEvents[0] != null;        }    });    assertNotNull(listenerEvents[0].getMember().getHost());    InetAddress addr = InetAddress.getLocalHost();    location = new ServerLocation(addr.getHostAddress(), 0);    listenerEvents[0] = null;    InternalClientMembership.notifyServerJoined(location);    await("wait for listener notification").until(() -> {        synchronized (listenerEvents) {            return listenerEvents[0] != null;        }    });    assertEquals(addr.getCanonicalHostName(), listenerEvents[0].getMember().getHost());}
0
public void initialize()
{}
0
public HashEntry<Object, Object> getNextEntry()
{    return this.nextEntry;}
0
public TermQuery build(QueryNode queryNode) throws QueryNodeException
{    return null;}
0
public BufferedUpdates resetForRecovery()
{    synchronized (this) {                                BufferedUpdates bufferedUpdates = new BufferedUpdates();        if (state == State.BUFFERING && tlog != null) {                        bufferedUpdates.tlog = tlog.tlogFile;            bufferedUpdates.offset = this.recoveryInfo.positionOfStart;        }                for (CdcrLogReader reader : logPointers.keySet()) {            reader.close();        }        logPointers.clear();                doClose(prevTlog);        doClose(tlog);        for (TransactionLog log : logs) {            if (log == prevTlog || log == tlog)                continue;            doClose(log);        }        logs.clear();        newestLogsOnStartup.clear();        tlog = prevTlog = null;        prevMapLog = prevMapLog2 = null;        map.clear();        if (prevMap != null)            prevMap.clear();        if (prevMap2 != null)            prevMap2.clear();        tlogFiles = null;        numOldRecords = 0;        oldDeletes.clear();        deleteByQueries.clear();        return bufferedUpdates;    }}
0
private static void addPartitionedRegion()
{    cache.createRegionFactory(RegionShortcut.PARTITION_REDUNDANT).create(REGION_NAME);}
0
public void setHostId(long hostId)
{    this.hostId = hostId;}
0
private String concatQueryParams(final Map<String, String> queryParams)
{    final StringBuilder concatQuery = new StringBuilder("");    if (queryParams != null && !queryParams.isEmpty()) {        int nParams = queryParams.size();        int index = 0;        for (Map.Entry<String, String> entry : queryParams.entrySet()) {            concatQuery.append(entry.getKey()).append('=').append(entry.getValue());            if (++index < nParams) {                concatQuery.append('&');            }        }    }    return concatQuery.toString().replaceAll("  *", "%20");}
0
public List<TransportInterceptor> getTransportInterceptors(NamedWriteableRegistry namedWriteableRegistry, ThreadContext threadContext)
{    return Collections.singletonList(new OriginAssertingInterceptor(threadContext));}
0
private static boolean lte(boolean a, boolean b)
{    throw new ClassCastException("Cannot apply [<=] operation to type [boolean]");}
0
public org.apache.drill.exec.proto.BitControl.PlanFragment newMessage()
{    return null;}
0
public Long getZoneId()
{    return zoneId;}
0
public Builder mergeFrom(org.apache.drill.exec.proto.UserProtos.QueryPlanFragments other)
{    if (other == org.apache.drill.exec.proto.UserProtos.QueryPlanFragments.getDefaultInstance())        return this;    if (other.hasStatus()) {        setStatus(other.getStatus());    }    if (other.hasQueryId()) {        mergeQueryId(other.getQueryId());    }    if (fragmentsBuilder_ == null) {        if (!other.fragments_.isEmpty()) {            if (fragments_.isEmpty()) {                fragments_ = other.fragments_;                bitField0_ = (bitField0_ & ~0x00000004);            } else {                ensureFragmentsIsMutable();                fragments_.addAll(other.fragments_);            }            onChanged();        }    } else {        if (!other.fragments_.isEmpty()) {            if (fragmentsBuilder_.isEmpty()) {                fragmentsBuilder_.dispose();                fragmentsBuilder_ = null;                fragments_ = other.fragments_;                bitField0_ = (bitField0_ & ~0x00000004);                fragmentsBuilder_ = com.google.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ? getFragmentsFieldBuilder() : null;            } else {                fragmentsBuilder_.addAllMessages(other.fragments_);            }        }    }    if (other.hasError()) {        mergeError(other.getError());    }    this.mergeUnknownFields(other.unknownFields);    onChanged();    return this;}
0
public Builder setNumMessagesAnalyzed(int numMessagesAnalyzed)
{    this.numMessagesAnalyzed = numMessagesAnalyzed;    return this;}
0
public ContentPath path()
{    return path;}
0
public Builder setOffset(int value)
{    bitField0_ |= 0x00000002;    offset_ = value;    onChanged();    return this;}
0
protected TcpTransportServer createTcpTransportServer(URI location, ServerSocketFactory serverSocketFactory) throws IOException, URISyntaxException
{    return new NIOSSLTransportServer(context, this, location, serverSocketFactory) {        @Override        protected Transport createTransport(Socket socket, WireFormat format) throws IOException {            StompNIOSSLTransport transport = new StompNIOSSLTransport(format, socket);            if (context != null) {                transport.setSslContext(context);            }            transport.setNeedClientAuth(isNeedClientAuth());            transport.setWantClientAuth(isWantClientAuth());            return transport;        }    };}
0
public void setHTTPFiles(HTTPFileArg[] files)
{    HTTPFileArgs fileArgs = new HTTPFileArgs();        if (files.length > 0) {        for (HTTPFileArg file : files) {            if (file.isNotEmpty()) {                fileArgs.addHTTPFileArg(file);            }        }    }    setHTTPFileArgs(fileArgs);}
0
public double runAsDouble()
{    return ((Number) execute()).doubleValue();}
0
public java.lang.String messageName()
{    return org.apache.drill.exec.proto.UserProtos.GetColumnsResp.class.getSimpleName();}
0
public NamedList<Object> processResponse(Reader reader)
{    throw new RuntimeException("Cannot handle character stream");}
0
public void configure() throws Exception
{    from("file://target/data/premove?preMove=work/work-${file:name}&initialDelay=0&delay=10&copyAndDeleteOnRenameFail=false").process(new MyPreMoveCheckerProcessor()).to("mock:result");}
0
public void testQueryProvider() throws Exception
{    Connection connection = CalciteAssert.that(CalciteAssert.Config.REGULAR).connect();    QueryProvider queryProvider = connection.unwrap(QueryProvider.class);    ParameterExpression e = Expressions.parameter(Employee.class, "e");        List<Object[]> list = queryProvider.createQuery(Expressions.call(Expressions.call(Types.of(Enumerable.class, Employee.class), null, LINQ4J_AS_ENUMERABLE_METHOD, Expressions.constant(new JdbcTest.HrSchema().emps)), "asQueryable"), Employee.class).where(Expressions.lambda(Expressions.lessThan(Expressions.field(e, "empid"), Expressions.constant(160)), e)).where(Expressions.lambda(Expressions.greaterThan(Expressions.field(e, "empid"), Expressions.constant(140)), e)).select(Expressions.<Function1<Employee, Object[]>>lambda(Expressions.new_(Object[].class, Expressions.field(e, "empid"), Expressions.call(Expressions.field(e, "name"), "toUpperCase")), e)).toList();    assertEquals(1, list.size());    assertEquals(2, list.get(0).length);    assertEquals(150, list.get(0)[0]);    assertEquals("SEBASTIAN", list.get(0)[1]);}
0
public void setJavaType(String javaType)
{    this.javaType = javaType;}
0
private void setResourceAndNodeDetails()
{    when(mCS.getClusterResource()).thenReturn(clusterResources);    when(lm.getResourceByLabel(anyString(), any(Resource.class))).thenReturn(clusterResources);    FiCaSchedulerNode mNode = mock(FiCaSchedulerNode.class);    when(mNode.getPartition()).thenReturn(RMNodeLabelsManager.NO_LABEL);    when(mCS.getSchedulerNode(any())).thenReturn(mNode);}
0
public int getErrorCode()
{    return errorCode;}
0
public void setFieldValue(_Fields field, @org.apache.storm.thrift.annotation.Nullable java.lang.Object value)
{    switch(field) {        case TOPOLOGY_ID:            if (value == null) {                unset_topologyId();            } else {                set_topologyId((java.lang.String) value);            }            break;        case PORT:            if (value == null) {                unset_port();            } else {                set_port((java.lang.Integer) value);            }            break;        case HOSTNAME:            if (value == null) {                unset_hostname();            } else {                set_hostname((java.lang.String) value);            }            break;        case METRIC_LIST:            if (value == null) {                unset_metricList();            } else {                set_metricList((WorkerMetricList) value);            }            break;    }}
0
public synchronized void close() throws IOException
{    if (closed.compareAndSet(false, true)) {        dropAll();    }}
0
public void run2() throws CacheException
{    configAndStartBridgeServer(false, false, false);    Region region = getRootRegion().getSubregion(regionName);}
0
public static org.apache.drill.exec.proto.UserBitShared.NodeStatus parseFrom(com.google.protobuf.CodedInputStream input) throws java.io.IOException
{    return com.google.protobuf.GeneratedMessageV3.parseWithIOException(PARSER, input);}
0
public void setParserInfo(MessageParserInfo parserInfo)
{    this.parserInfo = parserInfo;}
0
 void addContainerToPipeline(PipelineID pipelineID, ContainerID containerID) throws IOException
{    Preconditions.checkNotNull(pipelineID, "Pipeline Id cannot be null");    Preconditions.checkNotNull(containerID, "Container Id cannot be null");    Pipeline pipeline = getPipeline(pipelineID);    if (pipeline.isClosed()) {        throw new IOException(String.format("Cannot add container to pipeline=%s in closed state", pipelineID));    }    pipeline2container.get(pipelineID).add(containerID);}
0
public remove_token_argsStandardScheme getScheme()
{    return new remove_token_argsStandardScheme();}
0
public static Map<K, V> of(K k1, V v1, K k2, V v2, K k3, V v3, K k4, V v4, K k5, V v5, K k6, V v6, K k7, V v7, K k8, V v8, K k9, V v9, K k10, V v10, K k11, V v11, K k12, V v12, K k13, V v13, K k14, V v14, K k15, V v15, K k16, V v16, K k17, V v17, K k18, V v18, K k19, V v19, K k20, V v20, K k21, V v21, K k22, V v22, K k23, V v23, K k24, V v24, K k25, V v25, K k26, V v26, K k27, V v27, K k28, V v28, K k29, V v29, K k30, V v30, K k31, V v31, K k32, V v32, K k33, V v33, K k34, V v34, K k35, V v35, K k36, V v36, K k37, V v37, K k38, V v38, K k39, V v39, K k40, V v40, K k41, V v41, K k42, V v42, K k43, V v43, K k44, V v44, K k45, V v45, K k46, V v46, K k47, V v47, K k48, V v48, K k49, V v49, K k50, V v50, K k51, V v51, K k52, V v52, K k53, V v53, K k54, V v54, K k55, V v55, K k56, V v56, K k57, V v57, K k58, V v58, K k59, V v59, K k60, V v60, K k61, V v61, K k62, V v62, K k63, V v63, K k64, V v64, K k65, V v65, K k66, V v66, K k67, V v67, K k68, V v68, K k69, V v69, K k70, V v70)
{    Map<K, V> map = new LinkedHashMap<K, V>();    map.put(k1, v1);    map.put(k2, v2);    map.put(k3, v3);    map.put(k4, v4);    map.put(k5, v5);    map.put(k6, v6);    map.put(k7, v7);    map.put(k8, v8);    map.put(k9, v9);    map.put(k10, v10);    map.put(k11, v11);    map.put(k12, v12);    map.put(k13, v13);    map.put(k14, v14);    map.put(k15, v15);    map.put(k16, v16);    map.put(k17, v17);    map.put(k18, v18);    map.put(k19, v19);    map.put(k20, v20);    map.put(k21, v21);    map.put(k22, v22);    map.put(k23, v23);    map.put(k24, v24);    map.put(k25, v25);    map.put(k26, v26);    map.put(k27, v27);    map.put(k28, v28);    map.put(k29, v29);    map.put(k30, v30);    map.put(k31, v31);    map.put(k32, v32);    map.put(k33, v33);    map.put(k34, v34);    map.put(k35, v35);    map.put(k36, v36);    map.put(k37, v37);    map.put(k38, v38);    map.put(k39, v39);    map.put(k40, v40);    map.put(k41, v41);    map.put(k42, v42);    map.put(k43, v43);    map.put(k44, v44);    map.put(k45, v45);    map.put(k46, v46);    map.put(k47, v47);    map.put(k48, v48);    map.put(k49, v49);    map.put(k50, v50);    map.put(k51, v51);    map.put(k52, v52);    map.put(k53, v53);    map.put(k54, v54);    map.put(k55, v55);    map.put(k56, v56);    map.put(k57, v57);    map.put(k58, v58);    map.put(k59, v59);    map.put(k60, v60);    map.put(k61, v61);    map.put(k62, v62);    map.put(k63, v63);    map.put(k64, v64);    map.put(k65, v65);    map.put(k66, v66);    map.put(k67, v67);    map.put(k68, v68);    map.put(k69, v69);    map.put(k70, v70);    return Collections.unmodifiableMap(map);}
0
public get_type_args getEmptyArgsInstance()
{    return new get_type_args();}
0
public Builder loadFromPath(Path path) throws IOException
{        return loadFromStream(path.getFileName().toString(), Files.newInputStream(path), false);}
0
public String toString()
{    return TextFormat.shortDebugString(getProto());}
0
public void ethNetWeb3Sha3Test() throws Exception
{    Exchange exchange = createExchangeWithBodyAndHeader(null, OPERATION, WEB3_SHA3);    exchange.getIn().setBody("0x68656c6c6f20776f726c64");    template.send(exchange);    String body = exchange.getIn().getBody(String.class);    assertTrue(body.equals("0x47173285a8d7341e5e972fc677286384f802f8ef42a5ec5f03bbfa254cb01fad"));}
0
protected void executeDDLUpdates() throws AmbariException, SQLException
{    dropBrokenFKs();    updateStageTable();    updateRequestTable();    addOpsDisplayNameColumnToHostRoleCommand();    removeSecurityState();    addAmbariConfigurationTable();    addHostComponentLastStateTable();    upgradeUserTables();    upgradeKerberosTables();    upgradeRepoTables();    upgradeWidgetTable();}
0
private byte[] extractMessage(FlowFile flowFile, ProcessSession session)
{    final byte[] messageContent = new byte[(int) flowFile.getSize()];    session.read(flowFile, new InputStreamCallback() {        @Override        public void process(final InputStream in) throws IOException {            StreamUtils.fillBuffer(in, messageContent, true);        }    });    return messageContent;}
0
protected void closeCurrentStream()
{    if (closableRegistry.unregisterCloseable(currentStream)) {        IOUtils.closeQuietly(currentStream);    }    currentStream = null;}
0
private boolean isDbLock()
{    return db != null && table == null && partition == null;}
0
public static List<E> of(E e1, E e2, E e3, E e4, E e5, E e6, E e7)
{        return UnmodifiableArrayList.of(e1, e2, e3, e4, e5, e6, e7);}
0
public void testRepeatedlyTranslation()
{    RunnerApi.Trigger trigger = RunnerApi.Trigger.newBuilder().setRepeat(RunnerApi.Trigger.Repeat.newBuilder().setSubtrigger(subtrigger1)).build();    RepeatedlyStateMachine machine = (RepeatedlyStateMachine) TriggerStateMachines.stateMachineForTrigger(trigger);    assertThat(machine, equalTo(RepeatedlyStateMachine.forever(submachine1)));}
0
public void checkServerTrusted(X509Certificate[] certificates, String authType) throws CertificateException
{    if ((certificates != null) && LOG.isDebugEnabled()) {                for (int i = 0; i < certificates.length; i++) {                    }    }    if ((certificates != null) && (certificates.length == 1)) {        certificates[0].checkValidity();    } else {        standardTrustManager.checkServerTrusted(certificates, authType);    }}
1
public void testRecoveryCommit() throws Exception
{    java.sql.Connection jdbcConn = initDb();    sendJMSMessageToKickOffRoute();            broker.waitUntilStopped();    assertEquals("message in db, commit to db worked", 1, dumpDb(jdbcConn));        broker = createBroker(false);    broker.start();    broker.waitUntilStarted();    assertEquals("pending transactions", 1, broker.getBroker().getPreparedTransactions(null).length);            assertTrue("recovery complete in time", Wait.waitFor(new Wait.Condition() {        @Override        public boolean isSatisified() throws Exception {            return broker.getBroker().getPreparedTransactions(null).length == 0;        }    }));        assertEquals("recovery complete", 0, broker.getBroker().getPreparedTransactions(null).length);    final java.sql.Connection freshConnection = getJDBCConnection();    assertTrue("did not get replay", Wait.waitFor(new Wait.Condition() {        @Override        public boolean isSatisified() throws Exception {            return 1 == dumpDb(freshConnection);        }    }));    assertEquals("still one message in db", 1, dumpDb(freshConnection));        sendJMSMessageToKickOffRoute();    assertTrue("got second message", Wait.waitFor(new Wait.Condition() {        @Override        public boolean isSatisified() throws Exception {            return 2 == dumpDb(freshConnection);        }    }));    assertEquals("two messages in db", 2, dumpDb(freshConnection));}
1
public Status getState()
{    return state;}
0
public static NodeProcessor getGroupByProc()
{    return new GroupByInferrer();}
0
public static DFSClientFaultInjector get()
{    return instance;}
0
protected StringBuilder contentToWKT()
{    StringBuilder sb = new StringBuilder();    sb.append('(');    sb.append(ShapeBuilder.coordinateListToWKT(shell.coordinates));    for (LineStringBuilder hole : holes) {        sb.append(", ");        sb.append(ShapeBuilder.coordinateListToWKT(hole.coordinates));    }    sb.append(')');    return sb;}
0
public static Object[] createArray(Object arg0, Object arg1, Object arg2, Object arg3, Object arg4, Object arg5, Object arg6, Object arg7, Object arg8, Object arg9, Object arg10, Object arg11, Object arg12, Object arg13, Object arg14, Object arg15, Object arg16, Object arg17, Object arg18, Object arg19, Object arg20, Object arg21, Object arg22, Object arg23, Object arg24, Object arg25, Object arg26, Object arg27, Object arg28, Object arg29, Object arg30, Object arg31, Object arg32, Object arg33, Object arg34, Object arg35, Object arg36, Object arg37, Object arg38, Object arg39, Object arg40, Object arg41, Object arg42, Object arg43, Object arg44, Object arg45, Object arg46, Object arg47, Object arg48, Object arg49, Object arg50, Object arg51, Object arg52, Object arg53, Object arg54, Object arg55, Object arg56, Object arg57, Object arg58, Object arg59, Object arg60, Object arg61, Object arg62, Object arg63, Object arg64, Object arg65, Object arg66, Object arg67, Object arg68, Object arg69, Object arg70, Object arg71, Object arg72, Object arg73, Object arg74, Object arg75, Object arg76, Object arg77, Object arg78, Object arg79, Object arg80, Object arg81, Object arg82, Object arg83, Object arg84, Object arg85, Object arg86, Object arg87, Object arg88, Object arg89, Object arg90, Object arg91, Object arg92, Object arg93, Object arg94, Object arg95, Object arg96, Object arg97, Object arg98, Object arg99, Object arg100, Object arg101, Object arg102, Object arg103, Object arg104, Object arg105, Object arg106, Object arg107, Object arg108, Object arg109, Object arg110, Object arg111, Object arg112, Object arg113, Object arg114, Object arg115, Object arg116, Object arg117, Object arg118, Object arg119, Object arg120, Object arg121, Object arg122, Object arg123, Object arg124, Object arg125, Object arg126, Object arg127, Object arg128, Object arg129, Object arg130, Object arg131, Object arg132, Object arg133, Object arg134, Object arg135, Object arg136, Object arg137, Object arg138, Object arg139, Object arg140, Object arg141, Object arg142, Object arg143, Object arg144, Object arg145, Object arg146)
{    return new Object[] { arg0, arg1, arg2, arg3, arg4, arg5, arg6, arg7, arg8, arg9, arg10, arg11, arg12, arg13, arg14, arg15, arg16, arg17, arg18, arg19, arg20, arg21, arg22, arg23, arg24, arg25, arg26, arg27, arg28, arg29, arg30, arg31, arg32, arg33, arg34, arg35, arg36, arg37, arg38, arg39, arg40, arg41, arg42, arg43, arg44, arg45, arg46, arg47, arg48, arg49, arg50, arg51, arg52, arg53, arg54, arg55, arg56, arg57, arg58, arg59, arg60, arg61, arg62, arg63, arg64, arg65, arg66, arg67, arg68, arg69, arg70, arg71, arg72, arg73, arg74, arg75, arg76, arg77, arg78, arg79, arg80, arg81, arg82, arg83, arg84, arg85, arg86, arg87, arg88, arg89, arg90, arg91, arg92, arg93, arg94, arg95, arg96, arg97, arg98, arg99, arg100, arg101, arg102, arg103, arg104, arg105, arg106, arg107, arg108, arg109, arg110, arg111, arg112, arg113, arg114, arg115, arg116, arg117, arg118, arg119, arg120, arg121, arg122, arg123, arg124, arg125, arg126, arg127, arg128, arg129, arg130, arg131, arg132, arg133, arg134, arg135, arg136, arg137, arg138, arg139, arg140, arg141, arg142, arg143, arg144, arg145, arg146 };}
0
public Subscription answer(InvocationOnMock invocation)
{    Object[] args = invocation.getArguments();    ((Action1<EthBlock>) args[0]).call(new EthBlock());    return subscription;}
0
public boolean equals(Object other)
{    if (this == other) {        return true;    }    if (other == null || getClass() != other.getClass()) {        return false;    }    DeleteJobResponse that = (DeleteJobResponse) other;    return Objects.equals(acknowledged, that.acknowledged) && Objects.equals(task, that.task);}
0
public boolean equals(Object that)
{    if (that == null)        return false;    if (that instanceof list_privileges_args)        return this.equals((list_privileges_args) that);    return false;}
0
protected Map<String, String> setupIPythonEnv() throws IOException
{    Map<String, String> envs = EnvironmentUtils.getProcEnvironment();    if (envs.containsKey("PYTHONPATH")) {        if (additionalPythonPath != null) {            envs.put("PYTHONPATH", additionalPythonPath + ":" + envs.get("PYTHONPATH"));        }    } else {        envs.put("PYTHONPATH", additionalPythonPath);    }    if (usePy4JAuth) {        envs.put("PY4J_GATEWAY_SECRET", secret);    }        return envs;}
1
public synchronized void activateWithPrimaryContext(PrimaryContext primaryContext)
{    assert invariant();    assert primaryMode == false;    final Runnable runAfter = getMasterUpdateOperationFromCurrentState();    primaryMode = true;        appliedClusterStateVersion = primaryContext.clusterStateVersion();    checkpoints.clear();    for (Map.Entry<String, CheckpointState> entry : primaryContext.checkpoints.entrySet()) {        checkpoints.put(entry.getKey(), entry.getValue().copy());    }    routingTable = primaryContext.getRoutingTable();    replicationGroup = calculateReplicationGroup();    updateGlobalCheckpointOnPrimary();                runAfter.run();    if (indexSettings.isSoftDeleteEnabled()) {        addPeerRecoveryRetentionLeaseForSolePrimary();    }    assert invariant();}
0
private Map<String, Map<String, ?>> getClientDetails(GemFireMemberStatus snapshot)
{    Map<String, Map<String, ?>> clientsInfo = new LinkedHashMap<String, Map<String, ?>>();    Set connectedClients = snapshot.getConnectedClients();    if (!connectedClients.isEmpty()) {        Map clientHealthStatsMap = snapshot.getClientHealthStats();        for (Iterator iterator = connectedClients.iterator(); iterator.hasNext(); ) {            Map<String, Object> clientData = new HashMap<String, Object>();            String clientId = (String) iterator.next();            String host = snapshot.getClientHostName(clientId);            clientData.put(CLIENT_ID, clientId);            clientData.put(CLIENT_NAME, extractClientName(clientId, host));            clientData.put(CLIENT_HOST, host);            clientData.put(CLIENT_QUEUESIZE, snapshot.getClientQueueSize(clientId));            ClientHealthStats clientHealthStats = (ClientHealthStats) clientHealthStatsMap.get(clientId);            if (clientHealthStats != null) {                clientData.put(CLIENT_STATS_GETS, clientHealthStats.getNumOfGets());                clientData.put(CLIENT_STATS_PUTS, clientHealthStats.getNumOfPuts());                clientData.put(CLIENT_STATS_CACHEMISSES, clientHealthStats.getNumOfMisses());                clientData.put(CLIENT_STATS_CPUUSAGE, clientHealthStats.getProcessCpuTime());                clientData.put(CLIENT_STATS_CPUS, clientHealthStats.getCpus());                clientData.put(CLIENT_STATS_UPDATETIME, clientHealthStats.getUpdateTime().getTime());                clientData.put(CLIENT_STATS_THREADS, clientHealthStats.getNumOfThreads());            } else {                clientData.put(CLIENT_STATS_GETS, Integer.valueOf(0));                clientData.put(CLIENT_STATS_PUTS, Integer.valueOf(0));                clientData.put(CLIENT_STATS_CACHEMISSES, Integer.valueOf(0));                clientData.put(CLIENT_STATS_CPUUSAGE, Long.valueOf(0));                clientData.put(CLIENT_STATS_CPUS, Integer.valueOf(0));                clientData.put(CLIENT_STATS_UPDATETIME, Long.valueOf(0));                clientData.put(CLIENT_STATS_THREADS, Integer.valueOf(0));            }            clientsInfo.put(clientId, clientData);        }    }    return clientsInfo;}
0
public String toString()
{    return "EventWrapper: event=" + event + ", type=" + type;}
0
private static Object convertDateTimeStrict(Long value, Schema.FieldType fieldType)
{    checkTypeName(fieldType.getTypeName(), TypeName.DATETIME, "dateTime");    return new Instant(value);}
0
public boolean equals(Object o)
{    if (this == o)        return true;    if (o == null || getClass() != o.getClass())        return false;    ZooKeeperNode node = (ZooKeeperNode) o;    return Objects.equals(path, node.path);}
0
public void testWrappedExceptionOnBuildSchema()
{    MockOperatorExec opExec = new MockOperatorExec() {        @Override        public boolean buildSchema() {            throw new IllegalStateException(ERROR_MSG);        }    };    try (OperatorRecordBatch opBatch = makeOpBatch(opExec)) {        opBatch.next();        fail();    } catch (UserException e) {        assertTrue(e.getMessage().contains(ERROR_MSG));        assertTrue(e.getCause() instanceof IllegalStateException);    } catch (Throwable t) {        fail();    }    assertTrue(opExec.cancelCalled);    assertTrue(opExec.closeCalled);}
0
public boolean isUpdatedCapability()
{    return false;}
0
public static DataSet<Tuple2<Long, DummyCustomParameterizedType<Float>>> getLongCustomTuple2Data(ExecutionEnvironment env)
{    List<Tuple2<Long, DummyCustomParameterizedType<Float>>> tuples = new ArrayList<>();    tuples.add(new Tuple2<>(1L, new DummyCustomParameterizedType<>(10, 10f)));    tuples.add(new Tuple2<>(2L, new DummyCustomParameterizedType<>(20, 20f)));    tuples.add(new Tuple2<>(3L, new DummyCustomParameterizedType<>(30, 30f)));    tuples.add(new Tuple2<>(4L, new DummyCustomParameterizedType<>(40, 40f)));    return env.fromCollection(tuples);}
0
public void readFields(DataInput in) throws IOException
{    inputSplitClass = (Class<? extends InputSplit>) readClass(in);    inputSplit = ReflectionUtils.newInstance(inputSplitClass, conf);    inputSplit.readFields(in);    inputFormatClass = (Class<? extends InputFormat>) readClass(in);    mapperClass = (Class<? extends AvroMapper>) readClass(in);    String schemaString = Text.readString(in);    schema = schemaParser.parse(schemaString);}
0
public void write(int b) throws IOException
{    if (this.closed) {        throw new IOException("block stream has been closed.");    }    byte[] singleBytes = new byte[1];    singleBytes[0] = (byte) b;    this.currentBlockOutputStream.write(singleBytes, 0, 1);    this.blockWritten += 1;    if (this.blockWritten >= this.blockSize) {        this.uploadPart();        this.blockWritten = 0;    }}
0
public void shouldCheckpointOffsetsWhenStateIsFlushed()
{    final Map<TopicPartition, Long> expectedOffsets = new HashMap<>();    expectedOffsets.put(t1, 102L);    expectedOffsets.put(t2, 100L);    globalStateTask.initialize();    globalStateTask.update(new ConsumerRecord<>(topic1, 1, 101, "foo".getBytes(), "foo".getBytes()));    globalStateTask.flushState();    assertThat(stateMgr.checkpointed(), equalTo(expectedOffsets));}
0
public static OUL_R21 toOulR21(String body) throws HL7Exception
{    return toMessage(OUL_R21.class, body);}
0
protected double weight(byte[] originalForm)
{    double weight = missingValueWeight;    if (dictionary != null) {        String s = new String(originalForm, Charsets.UTF_8);        if (dictionary.containsKey(s)) {            weight = dictionary.get(s);        }    }    return weight;}
0
public void updateStatsForGet(boolean hit, long time)
{}
0
public StorageType getTargetType()
{    return targetType;}
0
private int compareValues(long v1, long v2)
{    return Long.compare(v1, v2) * reverseMul;}
0
 static CheckResult invalidOnIncompatibleSchema(String tableName, List<String> reasons)
{    StringBuilder buf = new StringBuilder();    for (String reason : reasons) {        buf.append("- ").append(reason).append("\n");    }    return new CheckResult(false, format(Locale.ROOT, "Found %d issue(s) with '%s':%n%s Please disable and " + "purge related " + "cube(s) first", reasons.size(), tableName, buf.toString()));}
0
protected Statement createSetterBlock(PropertyNode propertyNode, final FieldNode field)
{    return new BytecodeSequence(new BytecodeInstruction() {        @Override        public void visit(MethodVisitor mv) {            if (field.isStatic()) {                BytecodeHelper.load(mv, field.getType(), 0);                mv.visitFieldInsn(PUTSTATIC, BytecodeHelper.getClassInternalName(classNode), field.getName(), BytecodeHelper.getTypeDescription(field.getType()));            } else {                mv.visitVarInsn(ALOAD, 0);                BytecodeHelper.load(mv, field.getType(), 1);                mv.visitFieldInsn(PUTFIELD, BytecodeHelper.getClassInternalName(classNode), field.getName(), BytecodeHelper.getTypeDescription(field.getType()));            }            mv.visitInsn(RETURN);        }    });}
0
public void testAbortViaQueuedBarriers() throws Exception
{    BufferOrEvent[] sequence = { /* 0 */    createBuffer(1), /* 1 */    createBarrier(1, 1), createBarrier(1, 2), /* 3 */    createBuffer(2), createBuffer(0), createBuffer(1), /* 6 */    createCancellationBarrier(2, 2), /* 7 */    createBarrier(2, 1), /* 8 */    createBuffer(0), createBuffer(1), createBuffer(2), /* 11 */    createBarrier(1, 0), /* 12 */    createBuffer(2), createBuffer(1), createBuffer(0), /* 15 */    createBarrier(2, 0), /* 16 */    createBuffer(0), createBuffer(1), createBuffer(2) };    AbstractInvokable toNotify = mock(AbstractInvokable.class);    inputGate = createBarrierBuffer(3, sequence, toNotify);    long startTs;    check(sequence[0], inputGate.pollNext().get(), PAGE_SIZE);        startTs = System.nanoTime();    check(sequence[4], inputGate.pollNext().get(), PAGE_SIZE);    check(sequence[8], inputGate.pollNext().get(), PAGE_SIZE);        check(sequence[3], inputGate.pollNext().get(), PAGE_SIZE);    verify(toNotify, times(1)).triggerCheckpointOnBarrier(argThat(new CheckpointMatcher(1L)), any(CheckpointOptions.class), any(CheckpointMetrics.class));    validateAlignmentTime(startTs, inputGate.getAlignmentDurationNanos());    check(sequence[5], inputGate.pollNext().get(), PAGE_SIZE);        check(sequence[9], inputGate.pollNext().get(), PAGE_SIZE);    verify(toNotify, times(1)).abortCheckpointOnBarrier(eq(2L), argThat(new CheckpointExceptionMatcher(CheckpointFailureReason.CHECKPOINT_DECLINED_ON_CANCELLATION_BARRIER)));    assertEquals(0L, inputGate.getAlignmentDurationNanos());    check(sequence[10], inputGate.pollNext().get(), PAGE_SIZE);    check(sequence[12], inputGate.pollNext().get(), PAGE_SIZE);    check(sequence[13], inputGate.pollNext().get(), PAGE_SIZE);    check(sequence[14], inputGate.pollNext().get(), PAGE_SIZE);    check(sequence[16], inputGate.pollNext().get(), PAGE_SIZE);    check(sequence[17], inputGate.pollNext().get(), PAGE_SIZE);    check(sequence[18], inputGate.pollNext().get(), PAGE_SIZE);        assertEquals(0L, inputGate.getAlignmentDurationNanos());        verify(toNotify, times(1)).triggerCheckpointOnBarrier(any(CheckpointMetaData.class), any(CheckpointOptions.class), any(CheckpointMetrics.class));    verify(toNotify, times(1)).abortCheckpointOnBarrier(anyLong(), argThat(new CheckpointExceptionMatcher(CheckpointFailureReason.CHECKPOINT_DECLINED_ON_CANCELLATION_BARRIER)));}
0
public boolean equals(get_schema_with_environment_context_args that)
{    if (that == null)        return false;    boolean this_present_db_name = true && this.isSetDb_name();    boolean that_present_db_name = true && that.isSetDb_name();    if (this_present_db_name || that_present_db_name) {        if (!(this_present_db_name && that_present_db_name))            return false;        if (!this.db_name.equals(that.db_name))            return false;    }    boolean this_present_table_name = true && this.isSetTable_name();    boolean that_present_table_name = true && that.isSetTable_name();    if (this_present_table_name || that_present_table_name) {        if (!(this_present_table_name && that_present_table_name))            return false;        if (!this.table_name.equals(that.table_name))            return false;    }    boolean this_present_environment_context = true && this.isSetEnvironment_context();    boolean that_present_environment_context = true && that.isSetEnvironment_context();    if (this_present_environment_context || that_present_environment_context) {        if (!(this_present_environment_context && that_present_environment_context))            return false;        if (!this.environment_context.equals(that.environment_context))            return false;    }    return true;}
0
public void setStatus(Status status) throws ServerException
{    Check.notNull(status, "status");    if (status.settable) {        if (status != this.status) {            Status oldStatus = this.status;            this.status = status;            for (Service service : services.values()) {                try {                    service.serverStatusChange(oldStatus, status);                } catch (Exception ex) {                                        destroy();                    throw new ServerException(ServerException.ERROR.S11, service.getInterface().getSimpleName(), status, ex.getMessage(), ex);                }            }        }    } else {        throw new IllegalArgumentException("Status [" + status + " is not settable");    }}
1
public void onMatch(RelOptRuleCall call)
{    final Aggregate aggregate = call.rel(0);    if (!AggregateUtil.containsAccurateDistinctCall(aggregate.getAggCallList())) {        return;    }        if (AggregateUtil.containsApproximateDistinctCall(aggregate.getAggCallList())) {        throw new TableException("There are both Distinct AggCall and Approximate Distinct AggCall in one sql statement, " + "it is not supported yet.\nPlease choose one of them.");    }        if (aggregate.getGroupSets().size() > 1) {        return;    }            int nonDistinctAggCallCount = 0;    int filterCount = 0;    int unsupportedNonDistinctAggCallCount = 0;    final Set<Pair<List<Integer>, Integer>> argLists = new LinkedHashSet<>();    for (AggregateCall aggCall : aggregate.getAggCallList()) {        if (aggCall.filterArg >= 0) {            ++filterCount;        }        if (!aggCall.isDistinct()) {            ++nonDistinctAggCallCount;            final SqlKind aggCallKind = aggCall.getAggregation().getKind();                        switch(aggCallKind) {                case COUNT:                case SUM:                case SUM0:                case MIN:                case MAX:                    break;                default:                    ++unsupportedNonDistinctAggCallCount;            }        } else {            argLists.add(Pair.of(aggCall.getArgList(), aggCall.filterArg));        }    }    final int distinctAggCallCount = aggregate.getAggCallList().size() - nonDistinctAggCallCount;    Preconditions.checkState(argLists.size() > 0, "containsDistinctCall lied");        if (nonDistinctAggCallCount == 0 && argLists.size() == 1 && aggregate.getGroupType() == Group.SIMPLE) {        final Pair<List<Integer>, Integer> pair = com.google.common.collect.Iterables.getOnlyElement(argLists);        final RelBuilder relBuilder = call.builder();        convertMonopole(relBuilder, aggregate, pair.left, pair.right);        call.transformTo(relBuilder.build());        return;    }    if (useGroupingSets) {        rewriteUsingGroupingSets(call, aggregate);        return;    }        if (    distinctAggCallCount == 1 &&     filterCount == 0 &&     unsupportedNonDistinctAggCallCount == 0 && nonDistinctAggCallCount > 0) {                final RelBuilder relBuilder = call.builder();        convertSingletonDistinct(relBuilder, aggregate, argLists);        call.transformTo(relBuilder.build());        return;    }            final List<RelDataTypeField> aggFields = aggregate.getRowType().getFieldList();    final List<RexInputRef> refs = new ArrayList<>();    final List<String> fieldNames = aggregate.getRowType().getFieldNames();    final ImmutableBitSet groupSet = aggregate.getGroupSet();    final int groupAndIndicatorCount = aggregate.getGroupCount() + aggregate.getIndicatorCount();    for (int i : Util.range(groupAndIndicatorCount)) {        refs.add(RexInputRef.of(i, aggFields));    }        final List<AggregateCall> newAggCallList = new ArrayList<>();    int i = -1;    for (AggregateCall aggCall : aggregate.getAggCallList()) {        ++i;        if (aggCall.isDistinct()) {            refs.add(null);            continue;        }        refs.add(new RexInputRef(groupAndIndicatorCount + newAggCallList.size(), aggFields.get(groupAndIndicatorCount + i).getType()));        newAggCallList.add(aggCall);    }                final RelBuilder relBuilder = call.builder();    relBuilder.push(aggregate.getInput());    int n = 0;    if (!newAggCallList.isEmpty()) {        final RelBuilder.GroupKey groupKey = relBuilder.groupKey(groupSet, aggregate.getGroupSets());        relBuilder.aggregate(groupKey, newAggCallList);        ++n;    }        for (Pair<List<Integer>, Integer> argList : argLists) {        doRewrite(relBuilder, aggregate, n++, argList.left, argList.right, refs);    }    relBuilder.project(refs, fieldNames);    call.transformTo(relBuilder.build());}
0
public void test() throws Exception
{    Random random = random();    del("*:*");    index(id, random.nextInt(), "b_t", "snake a,b spider shark snail slug seal", "foo_i", "1");    index(id, random.nextInt(), "b_t", "snake spider shark snail slug", "foo_i", "2");    index(id, random.nextInt(), "b_t", "snake spider shark snail", "foo_i", "3");    index(id, random.nextInt(), "b_t", "snake spider shark", "foo_i", "2");    index(id, random.nextInt(), "b_t", "snake spider", "c_t", "snake spider");    index(id, random.nextInt(), "b_t", "snake", "c_t", "snake");    index(id, random.nextInt(), "b_t", "ant zebra", "c_t", "ant zebra");    index(id, random.nextInt(), "b_t", "zebra", "c_t", "zebra");    commit();    handle.clear();    handle.put("terms", UNORDERED);    query("qt", "/terms", "shards.qt", "/terms", "terms", "true", "terms.fl", "b_t");    query("qt", "/terms", "shards.qt", "/terms", "terms.limit", 5, "terms", "true", "terms.fl", "b_t", "terms.lower", "s");    query("qt", "/terms", "shards.qt", "/terms", "terms.limit", 5, "terms", "true", "terms.fl", "b_t", "terms.prefix", "sn", "terms.lower", "sn");    query("qt", "/terms", "shards.qt", "/terms", "terms.limit", 5, "terms", "true", "terms.fl", "b_t", "terms.prefix", "s", "terms.lower", "s", "terms.upper", "sn");        query("qt", "/terms", "shards.qt", "/terms", "terms.limit", 5, "terms", "true", "terms.fl", "b_t", "terms.prefix", "s", "terms.lower", "s", "terms.sort", "index");    query("qt", "/terms", "shards.qt", "/terms", "terms.limit", 5, "terms", "true", "terms.fl", "b_t", "terms.prefix", "s", "terms.lower", "s", "terms.upper", "sn", "terms.sort", "index");    query("qt", "/terms", "shards.qt", "/terms", "terms", "true", "terms.fl", "b_t", "terms.sort", "index");        query("qt", "/terms", "shards.qt", "/terms", "terms", "true", "terms.fl", "b_t", "terms.list", "snake,zebra,ant,bad");    query("qt", "/terms", "shards.qt", "/terms", "terms", "true", "terms.fl", "foo_i", "terms.list", "2,3,1");    query("qt", "/terms", "shards.qt", "/terms", "terms", "true", "terms.fl", "foo_i", "terms.stats", "true", "terms.list", "2,3,1");    query("qt", "/terms", "shards.qt", "/terms", "terms", "true", "terms.fl", "b_t", "terms.list", "snake,zebra", "terms.ttf", "true");    query("qt", "/terms", "shards.qt", "/terms", "terms", "true", "terms.fl", "b_t", "terms.fl", "c_t", "terms.list", "snake,ant,zebra", "terms.ttf", "true");}
0
public long getEntityOwnerId()
{    return CallContext.current().getCallingAccount().getId();}
0
public void testMultiArgsPopulateCorrectEndpointProperties() throws Exception
{    RabbitMQEndpoint endpoint = context.getEndpoint("rabbitmq:localhost/exchange?arg.exchange.e1=v1&arg.exchange.e2=v2&arg.queue.q1=v3&arg.binding.b1=v4", RabbitMQEndpoint.class);    assertEquals("Wrong number of args", 4, endpoint.getArgs().size());    assertEquals("Wrong number of args", 1, endpoint.getBindingArgs().size());    assertEquals("Wrong number of args", 2, endpoint.getExchangeArgs().size());    assertEquals("Wrong number of args", 1, endpoint.getQueueArgs().size());}
0
public int getSize()
{    StorageInstance storageInstance = storageInstances.get(DEFAULT_STORAGE_NAME);    return storageInstance.getSize();}
0
public int getSocketTimeout()
{    return delegate.getSocketTimeout();}
0
public final int readInt() throws IOException
{    final int v = this.segment.getIntBigEndian(this.position);    this.position += 4;    return v;}
0
public org.apache.lucene.search.ScoreMode scoreMode()
{    return org.apache.lucene.search.ScoreMode.COMPLETE_NO_SCORES;}
0
protected org.apache.storm.thrift.protocol.TField getFieldDesc(_Fields setField)
{    switch(setField) {        case BOLT:            return BOLT_FIELD_DESC;        case SPOUT:            return SPOUT_FIELD_DESC;        default:            throw new java.lang.IllegalArgumentException("Unknown field id " + setField);    }}
0
protected StandardVersionNegotiator getVersionNegotiator()
{    return new StandardVersionNegotiator(1);}
0
public void setUp() throws Exception
{    if (isJava16()) {                return;    }    super.setUp();}
0
public boolean canAccess(String dc)
{    return subset.contains(dc);}
0
public ValueSource parse(FunctionQParser fp) throws SyntaxError
{    List<ValueSource> sources = fp.parseValueSourceList();    return new ProductFloatFunction(sources.toArray(new ValueSource[sources.size()]));}
0
public List<String> getServicesWithNoConfigs()
{    return servicesWithNoConfigs;}
0
public void testBlockingQueueIteratorWithBlocking() throws Exception
{            final BlockingQueueIterator<String> iterator = new BlockingQueueIterator<>(new SynchronousQueue<String>());    final SettableFuture<List<String>> valuesFuture = SettableFuture.create();    Thread appender = new Thread(() -> valuesFuture.set(Arrays.asList(Iterators.toArray(iterator, String.class))));    appender.start();    iterator.accept("A");    iterator.accept("B");    iterator.close();    assertEquals(Arrays.asList("A", "B"), valuesFuture.get());    appender.join();}
0
protected AsyncHttpClient createAsyncHttpSSLClient() throws IOException, GeneralSecurityException
{    AsyncHttpClient c;    AsyncHttpClientConfig config;    DefaultAsyncHttpClientConfig.Builder builder = new DefaultAsyncHttpClientConfig.Builder();    SSLContextParameters sslContextParameters = new SSLContextParameters();    KeyStoreParameters truststoreParameters = new KeyStoreParameters();    truststoreParameters.setResource("jsse/localhost.p12");    truststoreParameters.setPassword(pwd);    TrustManagersParameters clientSSLTrustManagers = new TrustManagersParameters();    clientSSLTrustManagers.setKeyStore(truststoreParameters);    sslContextParameters.setTrustManagers(clientSSLTrustManagers);    KeyStoreParameters keystoreParameters = new KeyStoreParameters();    keystoreParameters.setResource("jsse/localhost.p12");    keystoreParameters.setPassword(pwd);    KeyManagersParameters clientAuthClientSSLKeyManagers = new KeyManagersParameters();    clientAuthClientSSLKeyManagers.setKeyPassword(pwd);    clientAuthClientSSLKeyManagers.setKeyStore(keystoreParameters);    sslContextParameters.setKeyManagers(clientAuthClientSSLKeyManagers);    SSLContext sslContext = sslContextParameters.createSSLContext(context());    JdkSslContext ssl = new JdkSslContext(sslContext, true, ClientAuth.REQUIRE);    builder.setSslContext(ssl);    builder.setAcceptAnyCertificate(true);    config = builder.build();    c = new DefaultAsyncHttpClient(config);    return c;}
0
public List<Attribute> output()
{    return command.output();}
0
public ApplicationSubmissionContext getApplicationSubmissionContext()
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public short getRegionVersionHighBytes()
{    return this.regionVersionHighBytes;}
0
public String getEntityId()
{    return entityId;}
0
public List<IdentifierContext> identifier()
{    return getRuleContexts(IdentifierContext.class);}
0
public boolean getOffHeap()
{    return this.offHeap;}
0
public void shouldDeserializeEmptyByteArrayToNull()
{    assertNull(keySerde.deserializer().deserialize(topic, new byte[0]));}
0
public void testBlockReplacement() throws Exception
{    final Configuration CONF = new HdfsConfiguration();    final String[] INITIAL_RACKS = { "/RACK0", "/RACK1", "/RACK2" };    final String[] NEW_RACKS = { "/RACK2" };    final short REPLICATION_FACTOR = (short) 3;    final int DEFAULT_BLOCK_SIZE = 1024;    final Random r = new Random();    CONF.setLong(DFSConfigKeys.DFS_BLOCK_SIZE_KEY, DEFAULT_BLOCK_SIZE);    CONF.setInt(HdfsClientConfigKeys.DFS_BYTES_PER_CHECKSUM_KEY, DEFAULT_BLOCK_SIZE / 2);    CONF.setLong(DFSConfigKeys.DFS_BLOCKREPORT_INTERVAL_MSEC_KEY, 500);    cluster = new MiniDFSCluster.Builder(CONF).numDataNodes(REPLICATION_FACTOR).racks(INITIAL_RACKS).build();    try {        cluster.waitActive();        FileSystem fs = cluster.getFileSystem();        Path fileName = new Path("/tmp.txt");                DFSTestUtil.createFile(fs, fileName, DEFAULT_BLOCK_SIZE, REPLICATION_FACTOR, r.nextLong());        DFSTestUtil.waitReplication(fs, fileName, REPLICATION_FACTOR);                InetSocketAddress addr = new InetSocketAddress("localhost", cluster.getNameNodePort());        DFSClient client = new DFSClient(addr, CONF);        List<LocatedBlock> locatedBlocks = client.getNamenode().getBlockLocations("/tmp.txt", 0, DEFAULT_BLOCK_SIZE).getLocatedBlocks();        assertEquals(1, locatedBlocks.size());        LocatedBlock block = locatedBlocks.get(0);        DatanodeInfo[] oldNodes = block.getLocations();        assertEquals(oldNodes.length, 3);        ExtendedBlock b = block.getBlock();                cluster.startDataNodes(CONF, 1, true, null, NEW_RACKS);        cluster.waitActive();        DatanodeInfo[] datanodes = client.datanodeReport(DatanodeReportType.ALL);                DatanodeInfo newNode = null;        for (DatanodeInfo node : datanodes) {            Boolean isNewNode = true;            for (DatanodeInfo oldNode : oldNodes) {                if (node.equals(oldNode)) {                    isNewNode = false;                    break;                }            }            if (isNewNode) {                newNode = node;                break;            }        }        assertTrue(newNode != null);        DatanodeInfo source = null;        ArrayList<DatanodeInfo> proxies = new ArrayList<DatanodeInfo>(2);        for (DatanodeInfo node : datanodes) {            if (node != newNode) {                if (node.getNetworkLocation().equals(newNode.getNetworkLocation())) {                    source = node;                } else {                    proxies.add(node);                }            }        }                                assertTrue(source != null && proxies.size() == 2);                                assertFalse(replaceBlock(b, source, newNode, proxies.get(0)));                        assertFalse(replaceBlock(b, source, proxies.get(0), proxies.get(1)));                        assertTrue(replaceBlock(b, source, proxies.get(0), newNode));                                checkBlocks(new DatanodeInfo[] { newNode, proxies.get(0), proxies.get(1) }, fileName.toString(), DEFAULT_BLOCK_SIZE, REPLICATION_FACTOR, client);                                assertTrue(replaceBlock(b, proxies.get(0), proxies.get(1), source));                                        checkBlocks(new DatanodeInfo[] {}, fileName.toString(), DEFAULT_BLOCK_SIZE, REPLICATION_FACTOR, client);    } finally {        cluster.shutdown();    }}
1
public void testCleanedNonSerializable() throws Exception
{    MapCreator creator = new NonSerializableMapCreator();    MapFunction<Integer, Integer> map = creator.getMap();    ClosureCleaner.clean(map, ExecutionConfig.ClosureCleanerLevel.RECURSIVE, true);    int result = map.map(3);    Assert.assertEquals(result, 4);}
0
protected RouteBuilder createRouteBuilder() throws Exception
{    return new RouteBuilder() {        @Override        public void configure() throws Exception {            from("direct:send").to("nats://" + getNatsUrl() + "?topic=test");        }    };}
0
public boolean hasTableQuota(TableName tn)
{    return tablesWithTableQuotas.contains(tn);}
0
private void readObject(java.io.ObjectInputStream in) throws java.io.IOException, ClassNotFoundException
{    try {        read(new org.apache.thrift.protocol.TCompactProtocol(new org.apache.thrift.transport.TIOStreamTransport(in)));    } catch (org.apache.thrift.TException te) {        throw new java.io.IOException(te);    }}
0
public boolean excludeEndOfBlock(int messageCount)
{    return evaluateModulus(messageCount, excludeEndOfBlockModulus);}
0
public static void resetFlags()
{    DeltaTestImpl.resetDeltaInvokationCounters();    numOfUpdates = 0;    hasDeltaBytes = 0;    check = false;}
0
public short getThriftFieldId()
{    return _thriftId;}
0
public void setOutputType(TypeInformation<OUT> type, ExecutionConfig executionConfig)
{    ((OutputTypeConfigurable<OUT>) operator).setOutputType(type, executionConfig);}
0
public static StoreSysCfgResponse create(DistributionManager dm, InternalDistributedMember recipient, Config sc)
{    StoreSysCfgResponse m = new StoreSysCfgResponse();    m.setRecipient(recipient);    InternalDistributedSystem sys = dm.getSystem();    Config conf = sys.getConfig();    String[] names = conf.getAttributeNames();    for (int i = 0; i < names.length; i++) {        if (conf.isAttributeModifiable(names[i])) {            conf.setAttributeObject(names[i], sc.getAttributeObject(names[i]), ConfigSource.runtime());        }    }    return m;}
0
public void waitActive(RouterContext router) throws InterruptedException
{    for (int loopCount = 0; loopCount < 20; loopCount++) {                if (router.router.getServiceState() == STATE.STARTED) {            return;        }        Thread.sleep(1000);    }    fail("Timeout waiting for " + router.router + " to activate");}
0
public void showScreenDescription(String sortKeyHeader)
{    TerminalPrinter printer = getTerminalPrinter(SCREEN_DESCRIPTION_START_ROW);    printer.startBold().print("Fields Management").stopBold().endOfLine();    printer.print("Current Sort Field: ").startBold().print(sortKeyHeader).stopBold().endOfLine();    printer.print("Navigate with up/down, Right selects for move then <Enter> or Left commits,").endOfLine();    printer.print("'d' or <Space> toggles display, 's' sets sort. Use 'q' or <Esc> to end!").endOfLine();}
0
public void setupCluster() throws IOException
{    conf = new Configuration();    conf.setInt(DFSConfigKeys.DFS_HA_LOGROLL_PERIOD_KEY, 1);    conf.setInt(DFSConfigKeys.DFS_HA_TAILEDITS_PERIOD_KEY, 1);    HAUtil.setAllowStandbyReads(conf, true);    MiniDFSNNTopology topology = MiniDFSNNTopology.simpleHATopology();    cluster = new MiniDFSCluster.Builder(conf).nnTopology(topology).numDataNodes(0).build();    cluster.waitActive();    shutdownClusterAndRemoveSharedEditsDir();}
0
protected Collection<Class<? extends Plugin>> nodePlugins()
{    return Collections.singleton(CustomScriptPlugin.class);}
0
public TableSchema getTableSchema()
{    return tableSchema;}
0
public boolean is_set_total_resources()
{    return this.total_resources != null;}
0
public void setDistributeByExprForClause(String clause, ASTNode ast)
{    destToDistributeby.put(clause, ast);}
0
public static void afterSuperClass()
{    System.clearProperty("solr.autoCommit.maxTime");    clearErrorHook();}
0
public void unsetSuccess()
{    this.success = null;}
0
public void rebuildNode(String fullPath) throws Exception
{    Preconditions.checkArgument(ZKPaths.getPathAndNode(fullPath).getPath().equals(path), "Node is not part of this cache: " + fullPath);    Preconditions.checkState(state.get() == State.STARTED, "cache has been closed");    ensurePath();    internalRebuildNode(fullPath);            offerOperation(new RefreshOperation(this, RefreshMode.FORCE_GET_DATA_AND_STAT));}
0
public static SimplePushbackSideInputDoFnRunner<InputT, OutputT> create(DoFnRunner<InputT, OutputT> underlying, Collection<PCollectionView<?>> views, ReadyCheckingSideInputReader sideInputReader)
{    return new SimplePushbackSideInputDoFnRunner<>(underlying, views, sideInputReader);}
0
public StringBuffer getRequestURL()
{    return RequestUtil.getRequestURL(this);}
0
public AsyncClient getAsyncClient(org.apache.thrift.transport.TNonblockingTransport transport)
{    return new AsyncClient(protocolFactory, clientManager, transport);}
0
public WriteContext init(Configuration configuration)
{    String schema = configuration.get(PARQUET_CASCADING_SCHEMA);    rootSchema = MessageTypeParser.parseMessageType(schema);    return new WriteContext(rootSchema, new HashMap<String, String>());}
0
public String getS3Mapping()
{    return s3Mapping;}
0
private void groupAndSend(Stream<Action> actions, int tries)
{    long locateTimeoutNs;    if (operationTimeoutNs > 0) {        locateTimeoutNs = remainingTimeNs();        if (locateTimeoutNs <= 0) {            failAll(actions, tries);            return;        }    } else {        locateTimeoutNs = -1L;    }    ConcurrentMap<ServerName, ServerRequest> actionsByServer = new ConcurrentHashMap<>();    ConcurrentLinkedQueue<Action> locateFailed = new ConcurrentLinkedQueue<>();    addListener(CompletableFuture.allOf(actions.map(action -> conn.getLocator().getRegionLocation(tableName, action.getAction().getRow(), RegionLocateType.CURRENT, locateTimeoutNs).whenComplete((loc, error) -> {        if (error != null) {            error = unwrapCompletionException(translateException(error));            if (error instanceof DoNotRetryIOException) {                failOne(action, tries, error, EnvironmentEdgeManager.currentTime(), "");                return;            }            addError(action, error, null);            locateFailed.add(action);        } else {            computeIfAbsent(actionsByServer, loc.getServerName(), ServerRequest::new).addAction(loc, action);        }    })).toArray(CompletableFuture[]::new)), (v, r) -> {        if (!actionsByServer.isEmpty()) {            sendOrDelay(actionsByServer, tries);        }        if (!locateFailed.isEmpty()) {            tryResubmit(locateFailed.stream(), tries, false, false);        }    });}
0
public ContentRepository getBackupRepository()
{    return backupRepositoryRef.get();}
0
public Long getLeaseDurationMillis()
{    return leaseDurationMillis;}
0
private void checkParseTimestampIso(String parsePattern, String input, String formatPattern, String expectedOutput)
{    formatter = new HiveSqlDateTimeFormatter(parsePattern, true);    Timestamp ts = formatter.parseTimestamp(input);    formatter = new HiveSqlDateTimeFormatter(formatPattern, false);    assertEquals(expectedOutput, formatter.format(ts));}
0
public void testTCPTransport() throws Exception
{    runTest(new ActiveMQConnectionFactory(connectionUri));}
0
public static Integer createSecondRemoteLocator(int dsId, int localPort, int remoteLocPort)
{    stopOldLocator();    WANTestBase test = new WANTestBase();    int port = AvailablePortHelper.getRandomAvailablePortForDUnitSite();    test.startLocator(dsId, localPort, port, remoteLocPort, true);    return port;}
0
public void incLong(int id, long delta)
{}
0
public void testSendA01() throws Exception
{    MockEndpoint mock = getMockEndpoint("mock:a01");    mock.expectedMessageCount(1);    mock.message(0).body().isInstanceOf(Message.class);    String line1 = "MSH|^~\\&|MYSENDER|MYSENDERAPP|MYCLIENT|MYCLIENTAPP|200612211200||ADT^A01|123|P|2.4||||||UNICODE UTF-8";    String line2 = "PID|||123456||De^John";    StringBuilder in = new StringBuilder();    in.append(line1);    in.append("\r");    in.append(line2);    String out = template.requestBody("mina:tcp://127.0.0.1:" + getPort() + "?sync=true&codec=#hl7codec", in.toString(), String.class);    String[] lines = out.split("\r");    assertEquals("MSH|^~\\&|MYSENDER||||200701011539||ADT^A01||||123|||||UNICODE UTF-8", lines[0]);    assertEquals("PID|||123||De^John", lines[1]);    assertMockEndpointsSatisfied();}
0
public void flush() throws Exception
{    throw new UnsupportedOperationException("Flush is not supported");}
0
public void processTimeout(javax.sip.TimeoutEvent timeoutEvent)
{    if (LOG.isWarnEnabled()) {            }}
1
public JoinUtil.JoinResult setFromVectorNoNulls(VectorHashKeyWrapperBase kw, VectorExpressionWriter[] keyOutputWriters, VectorHashKeyWrapperBatch keyWrapperBatch, MatchTracker matchTracker) throws HiveException
{    if (nulls == null) {        nulls = new boolean[keyOutputWriters.length];        currentKey = new Object[keyOutputWriters.length];        vectorKeyOIs = new ArrayList<ObjectInspector>();        for (int i = 0; i < keyOutputWriters.length; i++) {            vectorKeyOIs.add(keyOutputWriters[i].getObjectInspector());        }    } else {        assert nulls.length == keyOutputWriters.length;    }    boolean hasNulls = false;    for (int i = 0; i < keyOutputWriters.length; i++) {        currentKey[i] = keyWrapperBatch.getWritableKeyValue(kw, i, keyOutputWriters[i]);        if (currentKey[i] == null) {            nulls[i] = true;            hasNulls = true;        } else {            nulls[i] = false;        }    }    if (hasNulls) {        currentValue.reset();        return JoinUtil.JoinResult.NOMATCH;    }    return currentValue.setFromOutput(MapJoinKey.serializeRow(output, currentKey, vectorKeyOIs, sortableSortOrders, nullMarkers, notNullMarkers), matchTracker);}
0
public HashMap<String, Serializable> toMap()
{        HashMap<String, Serializable> ret = new HashMap<>();    if (allocator != null) {        if (allocator.partitioner != null)            ret.put("partitioner", allocator.partitioner.getClass().getSimpleName());        if (allocator.strategy != null)            ret.put("strategy", allocator.strategy.getClass().getSimpleName());    }    ret.put("replicas", replicas);    ret.put("numTokens", this.numTokens);    ret.put("sortedUnits", String.valueOf(sortedUnits));    ret.put("sortedTokens", String.valueOf(sortedTokens));    ret.put("unitToTokens", String.valueOf(unitToTokens));    ret.put("tokens", String.valueOf(tokens));    ret.put("unit", String.valueOf(unit));    ret.put("tokenInfo", String.valueOf(tokenInfo));    return ret;}
0
public void evaluate() throws Throwable
{    before();    try {        base.evaluate();    } finally {        after();    }}
0
public static boolean testFunctors()
{    StatGraphVisualizer instance = new StatGraphVisualizer();    return instance.model.checkFunctors(null, instance.getClass());}
0
public String getUrl()
{    return url;}
0
 static boolean isBoolIntrinsicOp(int op)
{    return op == LOGICAL_AND || op == LOGICAL_OR || op == COMPARE_NOT_IDENTICAL || op == COMPARE_IDENTICAL || op == MATCH_REGEX || op == KEYWORD_INSTANCEOF || op == COMPARE_NOT_INSTANCEOF;}
0
protected void doWrite(SendHandler handler, long blockingWriteTimeoutExpiry, ByteBuffer... buffers)
{    if (socketWrapper.hasAsyncIO()) {        final boolean block = (blockingWriteTimeoutExpiry != -1);        long timeout = -1;        if (block) {            timeout = blockingWriteTimeoutExpiry - System.currentTimeMillis();            if (timeout <= 0) {                SendResult sr = new SendResult(new SocketTimeoutException());                handler.onResult(sr);                return;            }        } else {            this.handler = handler;            timeout = getSendTimeout();            if (timeout > 0) {                                timeoutExpiry = timeout + System.currentTimeMillis();                wsWriteTimeout.register(this);            }        }        socketWrapper.write(block ? BlockingMode.BLOCK : BlockingMode.SEMI_BLOCK, timeout, TimeUnit.MILLISECONDS, null, SocketWrapperBase.COMPLETE_WRITE_WITH_COMPLETION, new CompletionHandler<Long, Void>() {            @Override            public void completed(Long result, Void attachment) {                if (block) {                    long timeout = blockingWriteTimeoutExpiry - System.currentTimeMillis();                    if (timeout <= 0) {                        failed(new SocketTimeoutException(), null);                    } else {                        handler.onResult(SENDRESULT_OK);                    }                } else {                    wsWriteTimeout.unregister(WsRemoteEndpointImplServer.this);                    clearHandler(null, true);                    if (close) {                        close();                    }                }            }            @Override            public void failed(Throwable exc, Void attachment) {                if (block) {                    SendResult sr = new SendResult(exc);                    handler.onResult(sr);                } else {                    wsWriteTimeout.unregister(WsRemoteEndpointImplServer.this);                    clearHandler(exc, true);                    close();                }            }        }, buffers);    } else {        if (blockingWriteTimeoutExpiry == -1) {            this.handler = handler;            this.buffers = buffers;                                    onWritePossible(true);        } else {                        try {                for (ByteBuffer buffer : buffers) {                    long timeout = blockingWriteTimeoutExpiry - System.currentTimeMillis();                    if (timeout <= 0) {                        SendResult sr = new SendResult(new SocketTimeoutException());                        handler.onResult(sr);                        return;                    }                    socketWrapper.setWriteTimeout(timeout);                    socketWrapper.write(true, buffer);                }                long timeout = blockingWriteTimeoutExpiry - System.currentTimeMillis();                if (timeout <= 0) {                    SendResult sr = new SendResult(new SocketTimeoutException());                    handler.onResult(sr);                    return;                }                socketWrapper.setWriteTimeout(timeout);                socketWrapper.flush(true);                handler.onResult(SENDRESULT_OK);            } catch (IOException e) {                SendResult sr = new SendResult(e);                handler.onResult(sr);            }        }    }}
0
public YarnServerCommonServiceProtos.DistributedSchedulingAllocateResponseProto getProto()
{    mergeLocalToProto();    proto = viaProto ? proto : builder.build();    viaProto = true;    return proto;}
0
public void shouldIndexOnInheritedFields()
{    String[] fields = new String[] { "myHomePages.content" };    FlatFormatSerializer serializer = new FlatFormatSerializer();    Page[] myHomePages1 = new Page[] { new Page(131), new Page(132) };    GrandSubCustomer customer = new GrandSubCustomer("Tommy Jackson", null, null, myHomePages1);    Document doc1 = SerializerTestHelper.invokeSerializer(serializer, customer, fields);    IndexableField[] fieldsInDoc = doc1.getFields("myHomePages.content");    Collection<Object> results = getResultCollection(fieldsInDoc, false);    assertEquals(2, results.size());    Object value = results.iterator().next();    assertTrue(results.contains("Hello world no 131"));    assertTrue(results.contains("Hello world no 132"));}
0
protected void tightMarshalObjectArray2(OpenWireFormat wireFormat, DataStructure[] objects, DataOutput dataOut, BooleanStream bs) throws IOException
{    if (bs.readBoolean()) {        dataOut.writeShort(objects.length);        for (int i = 0; i < objects.length; i++) {            tightMarshalNestedObject2(wireFormat, objects[i], dataOut, bs);        }    }}
0
public WorkerMetricPointTupleScheme getScheme()
{    return new WorkerMetricPointTupleScheme();}
0
public static int sround(int b0, int b1)
{    return sround(BigDecimal.valueOf(b0), b1).intValue();}
0
public String getLabelResource()
{        return "jms_point_to_point";}
0
public String getColor()
{    return color;}
0
public List<MetaMethod> respondsTo(Object obj, String name)
{    final Object owner = getOwner();    final MetaClass ownerMetaClass = getOwnerMetaClass(owner);    return ownerMetaClass.respondsTo(owner, name);}
0
public void testCompare2()
{    final List<ImmutableBitSet> sorted = getSortedList();    sorted.sort(ImmutableBitSet.COMPARATOR);    assertThat(sorted.toString(), equalTo("[{0, 1, 3}, {0, 1}, {1, 1000}, {1}, {1}, {2, 3}, {}]"));}
0
public Socket createSocket(String host, int port) throws IOException
{    SocketCreator socketCreator;    try {        socketCreator = SocketCreatorFactory.getSocketCreatorForComponent(SecurableCommunicationChannel.JMX);        return socketCreator.connectForClient(host, port, 0);    } catch (Exception exception) {        try {                                    Properties gfProperties = new Properties();            gfProperties.load(new StringReader(System.getProperty(GEODE_SSL_CONFIG_PROPERTIES)));            SSLConfig sslConfig = SSLConfigurationFactory.getSSLConfigForComponent(gfProperties, SecurableCommunicationChannel.JMX);            socketCreator = new SocketCreator(sslConfig);            return socketCreator.connectForClient(host, port, 0);        } catch (Exception finalException) {                        return defaultFactory.createSocket(host, port);        }    }}
0
public static Builder builder()
{    return new Builder();}
0
public static void setup() throws IOException
{    final Configuration conf = new Configuration();    final Path TEST_ROOT_DIR = new Path(System.getProperty("test.build.data", "/tmp"));    testdir = new Path(TEST_ROOT_DIR, "TestMiniMRClientCluster");    inDir = new Path(testdir, "in");    outDir = new Path(testdir, "out");    FileSystem fs = FileSystem.getLocal(conf);    if (fs.exists(testdir) && !fs.delete(testdir, true)) {        throw new IOException("Could not delete " + testdir);    }    if (!fs.mkdirs(inDir)) {        throw new IOException("Mkdirs failed to create " + inDir);    }    for (int i = 0; i < inFiles.length; i++) {        inFiles[i] = new Path(inDir, "part_" + i);        createFile(inFiles[i], conf);    }        mrCluster = MiniMRClientClusterFactory.create(InternalClass.class, 1, new Configuration());}
0
public static org.slf4j.Logger getLogger()
{    return logger;}
0
private AsyncEventQueue getAsyncEventQueue()
{    AsyncEventQueue value = null;    Set<AsyncEventQueue> asyncEventQueues = getCache().getAsyncEventQueues();    for (AsyncEventQueue asyncEventQueue : asyncEventQueues) {        if (asyncEventQueueId.equals(asyncEventQueue.getId())) {            value = asyncEventQueue;        }    }    assertThat(value).isNotNull();    return value;}
0
public synchronized int available() throws IOException
{    return Math.max(0, count - pos);}
0
public Procedure<?> peek()
{    return runnables.peek();}
0
public int compareTo(MetadataGraphVertex vertex)
{    if (vertex == null || vertex.getMd() == null) {        return 1;    }    ArtifactMetadata vmd = vertex.getMd();    if (vmd == null) {        if (md == null) {            return 0;        } else {            return 1;        }    }    int g = compareStrings(md.groupId, vmd.groupId);    if (g == 0) {        int a = compareStrings(md.artifactId, vmd.artifactId);        if (a == 0) {            if (compareVersion) {                int v = compareStrings(md.version, vmd.version);                if (v == 0) {                    if (compareScope) {                        String s1 = ArtifactScopeEnum.checkScope(md.artifactScope).getScope();                        String s2 = ArtifactScopeEnum.checkScope(vmd.artifactScope).getScope();                        return s1.compareTo(s2);                    } else {                        return 0;                    }                } else {                    return v;                }            } else {                return 0;            }        } else {            return a;        }    }    return g;}
0
public void close() throws IOException
{    try {        IOUtils.close(postingsReader);    } finally {        fields.clear();    }}
0
public java.lang.String toString()
{    java.lang.StringBuilder sb = new java.lang.StringBuilder("AppOutputUpdateEvent(");    boolean first = true;    sb.append("noteId:");    if (this.noteId == null) {        sb.append("null");    } else {        sb.append(this.noteId);    }    first = false;    if (!first)        sb.append(", ");    sb.append("paragraphId:");    if (this.paragraphId == null) {        sb.append("null");    } else {        sb.append(this.paragraphId);    }    first = false;    if (!first)        sb.append(", ");    sb.append("appId:");    if (this.appId == null) {        sb.append("null");    } else {        sb.append(this.appId);    }    first = false;    if (!first)        sb.append(", ");    sb.append("index:");    sb.append(this.index);    first = false;    if (!first)        sb.append(", ");    sb.append("type:");    if (this.type == null) {        sb.append("null");    } else {        sb.append(this.type);    }    first = false;    if (!first)        sb.append(", ");    sb.append("data:");    if (this.data == null) {        sb.append("null");    } else {        sb.append(this.data);    }    first = false;    sb.append(")");    return sb.toString();}
0
public void testClientGetsInvalidEntryPR() throws Exception
{    final String regionName = getUniqueName() + "Region";    doTestClientGetsInvalidEntry(regionName, true, false);}
0
protected void handleBackwardCompatibility()
{    /**     * backward compatibility for properties which are common to both client     * and server     */    super.handleBackwardCompatibility();    /**     * backward compatibility for client specific properties     */    setProperty(ZK_SASL_CLIENT_USERNAME, System.getProperty(ZK_SASL_CLIENT_USERNAME));    setProperty(LOGIN_CONTEXT_NAME_KEY, System.getProperty(LOGIN_CONTEXT_NAME_KEY));    setProperty(ENABLE_CLIENT_SASL_KEY, System.getProperty(ENABLE_CLIENT_SASL_KEY));    setProperty(ZOOKEEPER_SERVER_REALM, System.getProperty(ZOOKEEPER_SERVER_REALM));    setProperty(DISABLE_AUTO_WATCH_RESET, System.getProperty(DISABLE_AUTO_WATCH_RESET));    setProperty(ZOOKEEPER_CLIENT_CNXN_SOCKET, System.getProperty(ZOOKEEPER_CLIENT_CNXN_SOCKET));    setProperty(SECURE_CLIENT, System.getProperty(SECURE_CLIENT));}
0
public void setBatchDelete(boolean batchDelete)
{    this.batchDelete = batchDelete;}
0
public void setTrustCertificateAlgorithm(String trustCertificateAlgorithm)
{    this.trustCertificateAlgorithm = trustCertificateAlgorithm;}
0
public void testX509Helpers() throws Exception
{    KeyPair keyPair = CertUtils.generateRandomKeyPair(4096);    String privateKeyString = SAMLUtils.encodePrivateKey(keyPair.getPrivate());    String publicKeyString = SAMLUtils.encodePublicKey(keyPair.getPublic());    PrivateKey privateKey = SAMLUtils.decodePrivateKey(privateKeyString);    PublicKey publicKey = SAMLUtils.decodePublicKey(publicKeyString);    assertNotNull(privateKey);    assertNotNull(publicKey);    assertTrue(privateKey.equals(keyPair.getPrivate()));    assertTrue(publicKey.equals(keyPair.getPublic()));}
0
public void testValidate()
{    Script script = new Script(ScriptType.STORED, null, randomAlphaOfLength(10), Collections.emptyMap());    PainlessExecuteAction.Request request = new PainlessExecuteAction.Request(script, null, null);    Exception e = request.validate();    assertNotNull(e);    assertEquals("Validation Failed: 1: only inline scripts are supported;", e.getMessage());}
0
public void setDescription(String description)
{    this.description = description;}
0
public Enumeration keys()
{        return Collections.enumeration(new TreeSet(keySet()));}
0
public AbfsRestOperation flush(final String path, final long position, boolean retainUncommittedData, boolean isClose) throws AzureBlobFileSystemException
{    final List<AbfsHttpHeader> requestHeaders = createDefaultHeaders();            requestHeaders.add(new AbfsHttpHeader(X_HTTP_METHOD_OVERRIDE, HTTP_METHOD_PATCH));    final AbfsUriQueryBuilder abfsUriQueryBuilder = createDefaultUriQueryBuilder();    abfsUriQueryBuilder.addQuery(QUERY_PARAM_ACTION, FLUSH_ACTION);    abfsUriQueryBuilder.addQuery(QUERY_PARAM_POSITION, Long.toString(position));    abfsUriQueryBuilder.addQuery(QUERY_PARAM_RETAIN_UNCOMMITTED_DATA, String.valueOf(retainUncommittedData));    abfsUriQueryBuilder.addQuery(QUERY_PARAM_CLOSE, String.valueOf(isClose));    final URL url = createRequestUrl(path, abfsUriQueryBuilder.toString());    final AbfsRestOperation op = new AbfsRestOperation(AbfsRestOperationType.Flush, this, HTTP_METHOD_PUT, url, requestHeaders);    op.execute();    return op;}
0
public Map<TableScanOperator, SampleDesc> getOpToSamplePruner()
{    return opToSamplePruner;}
0
private static boolean typeCheckMethodsWithGenerics(ClassNode receiver, ClassNode[] arguments, MethodNode candidateMethod, boolean isExtensionMethod)
{    boolean failure = false;                        boolean skipBecauseOfInnerClassNotReceiver = isOuterClassOf(receiver, candidateMethod.getDeclaringClass());    Parameter[] parameters = candidateMethod.getParameters();    Map<GenericsTypeName, GenericsType> classGTs;    if (skipBecauseOfInnerClassNotReceiver) {        classGTs = Collections.EMPTY_MAP;    } else {        classGTs = GenericsUtils.extractPlaceholders(receiver);    }    if (parameters.length > arguments.length || parameters.length == 0) {                return true;    }                    Map<GenericsTypeName, GenericsType> resolvedMethodGenerics = new HashMap<GenericsTypeName, GenericsType>();    if (!skipBecauseOfInnerClassNotReceiver) {        addMethodLevelDeclaredGenerics(candidateMethod, resolvedMethodGenerics);    }        for (GenericsTypeName key : resolvedMethodGenerics.keySet()) classGTs.remove(key);        applyGenericsConnections(classGTs, resolvedMethodGenerics);        if (!skipBecauseOfInnerClassNotReceiver) {        failure |= inferenceCheck(Collections.EMPTY_SET, resolvedMethodGenerics, candidateMethod.getDeclaringClass(), receiver, false);    }                        Set<GenericsTypeName> fixedGenericsPlaceHolders = extractResolvedPlaceHolders(resolvedMethodGenerics);    for (int i = 0; i < arguments.length; i++) {        int pindex = min(i, parameters.length - 1);        ClassNode wrappedArgument = arguments[i];        ClassNode type = parameters[pindex].getOriginType();        failure |= inferenceCheck(fixedGenericsPlaceHolders, resolvedMethodGenerics, type, wrappedArgument, i >= parameters.length - 1);                if (isExtensionMethod && i == 0)            fixedGenericsPlaceHolders = extractResolvedPlaceHolders(resolvedMethodGenerics);    }    return !failure;}
0
public static String getQuartzContextName(CamelContext camelContext)
{        if (camelContext.getManagementName() != null) {        return camelContext.getManagementName();    } else {                return camelContext.getName();    }}
0
public int read() throws IOException
{    if (bytesRead >= limit) {        return -1;    }    final int val = in.read();    if (val > -1) {        bytesRead++;    }    return val;}
0
public boolean hasWriteOrderID()
{    return ((bitField0_ & 0x00000002) == 0x00000002);}
0
public void setBasicPropertyBinding(Boolean basicPropertyBinding)
{    this.basicPropertyBinding = basicPropertyBinding;}
0
public void testStopDatafeed() throws Exception
{    RestHighLevelClient client = highLevelClient();    {                        StopDatafeedRequest request = new StopDatafeedRequest("datafeed_id1", "datafeed_id*");                request = StopDatafeedRequest.stopAllDatafeedsRequest();                        request.setAllowNoDatafeeds(true);                request.setForce(true);                request.setTimeout(TimeValue.timeValueMinutes(10));                        StopDatafeedResponse response = client.machineLearning().stopDatafeed(request, RequestOptions.DEFAULT);                                boolean stopped = response.isStopped();                assertTrue(stopped);    }    {        StopDatafeedRequest request = StopDatafeedRequest.stopAllDatafeedsRequest();                ActionListener<StopDatafeedResponse> listener = new ActionListener<StopDatafeedResponse>() {            @Override            public void onResponse(StopDatafeedResponse response) {                        }            @Override            public void onFailure(Exception e) {                        }        };                        final CountDownLatch latch = new CountDownLatch(1);        listener = new LatchedActionListener<>(listener, latch);                        client.machineLearning().stopDatafeedAsync(request, RequestOptions.DEFAULT, listener);                assertTrue(latch.await(30L, TimeUnit.SECONDS));    }}
0
public void setUp() throws Exception
{    FileSystem fs = TEST_UTIL.getTestFileSystem();    if (!fs.mkdirs(CFDIR)) {        throw new IOException("Cannot create test directory " + CFDIR);    }}
0
public Boolean getSaxon()
{    return saxon;}
0
public void checkClientTrusted(final X509Certificate[] certs, final String authType)
{}
0
public void registerInterestRegex(String regex, InterestResultPolicy policy, boolean isDurable) throws CacheWriterException
{    throw new UnsupportedOperationException();}
0
public ReadPreference getReadPreference()
{    return getMongoConnection().getReadPreference();}
0
public long getNumEvictions()
{    return numEvictions;}
0
private void createCache(Properties props) throws Exception
{    DistributedSystem ds = getSystem(props);    ds.disconnect();    ds = getSystem(props);    assertNotNull(ds);    cache = CacheFactory.create(ds);    assertNotNull(cache);}
0
public void testBuildFloatZeroNaN()
{    PrimitiveType type = Types.required(FLOAT).named("test_float");    ColumnIndexBuilder builder = ColumnIndexBuilder.getBuilder(type, Integer.MAX_VALUE);    StatsBuilder sb = new StatsBuilder();    builder.add(sb.stats(type, -1.0f, -0.0f));    builder.add(sb.stats(type, 0.0f, 1.0f));    builder.add(sb.stats(type, 1.0f, 100.0f));    ColumnIndex columnIndex = builder.build();    assertCorrectValues(columnIndex.getMinValues(), -1.0f, -0.0f, 1.0f);    assertCorrectValues(columnIndex.getMaxValues(), 0.0f, 1.0f, 100.0f);    builder = ColumnIndexBuilder.getBuilder(type, Integer.MAX_VALUE);    builder.add(sb.stats(type, -1.0f, -0.0f));    builder.add(sb.stats(type, 0.0f, Float.NaN));    builder.add(sb.stats(type, 1.0f, 100.0f));    assertNull(builder.build());}
0
public void setOperand(int i, SqlNode operand)
{    switch(i) {        case 0:            table = (SqlIdentifier) operand;            break;        case 1:            column = (SqlIdentifier) operand;            break;        default:            throw new AssertionError(i);    }}
0
private void testLockExpiration(HiveTxnManager txnMgr, int numLocksBefore, boolean shouldExpire) throws Exception
{    DbLockManager lockManager = (DbLockManager) txnMgr.getLockManager();    ShowLocksResponse resp = lockManager.getLocks();    Assert.assertEquals("Wrong number of locks before expire", numLocksBefore, resp.getLocks().size());    Thread.sleep(HiveConf.getTimeVar(conf, HiveConf.ConfVars.HIVE_TXN_TIMEOUT, TimeUnit.MILLISECONDS));    runReaper();    resp = lockManager.getLocks();    if (shouldExpire) {        Assert.assertEquals("Expected all locks to expire", 0, resp.getLocks().size());        lockManager.clearLocalLockRecords();    } else {        Assert.assertEquals("No lock should expire because there is heartbeating", numLocksBefore, resp.getLocks().size());    }}
0
public Void call() throws Exception
{    try {        AccessControlClient.grant(connection, table, user, family, qualifier, actions);    } catch (Throwable t) {            }    return null;}
1
public ADDRESS<T> b(String cdata)
{    return b().__(cdata).__();}
0
public RangeQueryBuilder to(Object to)
{    return to(to, this.includeUpper);}
0
public short getThriftFieldId()
{    return _thriftId;}
0
 static TermsValuesSourceBuilder parse(String name, XContentParser parser) throws IOException
{    return PARSER.parse(parser, new TermsValuesSourceBuilder(name), null);}
0
protected List<Decimal> getExpectedResults()
{    return Arrays.asList(Decimal.castFrom("1", precision, scale), null, Decimal.castFrom("0", precision, scale));}
0
public void shouldWriteWhenDeltaIs0ForEachBlock() throws IOException
{    long[] data = new long[5 * blockSize + 1];    for (int i = 0; i < data.length; i++) {        data[i] = (i - 1) / blockSize;    }    shouldWriteAndRead(data);}
0
public void setInput(String input)
{    this.input = input;}
0
 List<Iterable<AbstractMetric>> getMetricValues()
{    return metricValues;}
0
public int getForwardingSourceField(int input, int targetField)
{    if (input != 0) {        throw new IndexOutOfBoundsException();    }    for (Map.Entry<Integer, FieldSet> e : fieldMapping.entrySet()) {        if (e.getValue().contains(targetField)) {            return e.getKey();        }    }    return -1;}
0
public void addAvroSerializersIfRequired(ExecutionConfig reg, Class<?> type)
{    if (hasSuperclass(type, AVRO_SPECIFIC_RECORD_BASE) || hasSuperclass(type, AVRO_GENERIC_RECORD)) {        throw new RuntimeException("Could not load class '" + AVRO_KRYO_UTILS + "'. " + "You may be missing the 'flink-avro' dependency.");    }}
0
public void validateConnection(final PoolableConnection conn) throws SQLException
{    if (conn.isClosed()) {        throw new SQLException("validateConnection: connection closed");    }    conn.validate(validationQuery, validationQueryTimeoutSeconds);}
0
private void checkServer(RegionLocations locations)
{    if (this.services == null) {                return;    }    if (locations == null) {        return;    }        for (HRegionLocation location : locations.getRegionLocations()) {        ServerName sn = location.getServerName();        if (sn == null) {            continue;        }        if (location.getRegion() == null) {                                    continue;        }                if (location.getRegion().isSplitParent()) {            continue;        }                if (isTableDisabled(location.getRegion())) {            continue;        }        ServerManager.ServerLiveState state = this.services.getServerManager().isServerKnownAndOnline(sn);        switch(state) {            case UNKNOWN:                this.report.unknownServers.add(new Pair<>(location.getRegion(), sn));                break;            default:                break;        }    }}
1
public void clearGui()
{    super.clearGui();    jsonPath.setText("");    jsonValue.setText("");    jsonValidation.setSelected(false);    expectNull.setSelected(false);    invert.setSelected(false);    isRegex.setSelected(true);}
0
public QueryExpression lt(LiteralExpression literal)
{    throw new PredicateAnalyzerException("SqlOperatorImpl ['<'] " + "cannot be applied to a compound expression");}
0
public Object onDataSet(DataSet ds, Object... payloads)
{    try {        return ds.count();    } catch (Exception e) {        return null;    }}
0
public Metrics getStageMetrics(final int jobId, final int stageId)
{    return aggregate(new StageFilter(jobId, stageId));}
0
public void open()
{    if (!isOpen) {        isOpen = true;        startTime = System.currentTimeMillis();    } else {            }}
1
private void testInodeWithLeasesAtScaleImpl(FSNamesystem fsNamesystem, final LeaseManager leaseManager, final FSDirectory fsDirectory, INodeDirectory ancestorDirectory, int scale) throws IOException
{    verifyINodeLeaseCounts(fsNamesystem, leaseManager, ancestorDirectory, 0, 0, 0);    Set<Long> iNodeIds = new HashSet<>();    for (int i = 0; i < scale; i++) {        iNodeIds.add(INodeId.ROOT_INODE_ID + i);    }    for (Long iNodeId : iNodeIds) {        INodeFile iNodeFile = stubInodeFile(iNodeId);        iNodeFile.toUnderConstruction("hbase", "gce-100");        iNodeFile.setParent(ancestorDirectory);        when(fsDirectory.getInode(iNodeId)).thenReturn(iNodeFile);        leaseManager.addLease("holder_" + iNodeId, iNodeId);    }    verifyINodeLeaseCounts(fsNamesystem, leaseManager, ancestorDirectory, iNodeIds.size(), iNodeIds.size(), iNodeIds.size());    leaseManager.removeAllLeases();    verifyINodeLeaseCounts(fsNamesystem, leaseManager, ancestorDirectory, 0, 0, 0);}
0
public void testScanWithStartKeyAndStopKey() throws Exception
{        testScan(1, true, 998, false, -1);    testScan(123, true, 345, true, -1);    testScan(234, true, 456, false, -1);    testScan(345, false, 567, true, -1);    testScan(456, false, 678, false, -1);}
0
public long lengthForDoubleArrayOfSize(long length)
{    return lengthForPrimitiveArrayOfSize(primitive2(), length);}
0
public _Fields fieldForId(int fieldId)
{    return _Fields.findByThriftId(fieldId);}
0
public Category getCategory()
{    return Category.MAP;}
0
public Collection getModifiableCollection()
{    return rows;}
0
private void putEventsAndCommit(final List<Row> actions, final List<Increment> incs, Transaction txn) throws Exception
{    privilegedExecutor.execute((PrivilegedExceptionAction<Void>) () -> {        final List<Mutation> mutations = new ArrayList<>(actions.size());        for (Row r : actions) {            if (r instanceof Put) {                ((Put) r).setDurability(enableWal ? Durability.USE_DEFAULT : Durability.SKIP_WAL);            }                        if (r instanceof Increment) {                ((Increment) r).setDurability(enableWal ? Durability.USE_DEFAULT : Durability.SKIP_WAL);            }            if (r instanceof Mutation) {                mutations.add((Mutation) r);            } else {                            }        }        table.mutate(mutations);        table.flush();        return null;    });    privilegedExecutor.execute((PrivilegedExceptionAction<Void>) () -> {        List<Increment> processedIncrements;        if (batchIncrements) {            processedIncrements = coalesceIncrements(incs);        } else {            processedIncrements = incs;        }                if (debugIncrCallback != null) {            debugIncrCallback.onAfterCoalesce(processedIncrements);        }        for (final Increment i : processedIncrements) {            i.setDurability(enableWal ? Durability.USE_DEFAULT : Durability.SKIP_WAL);            table.mutate(i);        }        table.flush();        return null;    });    txn.commit();    sinkCounter.addToEventDrainSuccessCount(actions.size());}
1
public Void call() throws Exception
{        final int[] numberOfBuffersPerChannel = new int[numberOfInputChannels];    try {        Optional<BufferOrEvent> boe;        while ((boe = inputGate.getNext()).isPresent()) {            if (boe.get().isBuffer()) {                boe.get().getBuffer().recycleBuffer();                                if (++numberOfBuffersPerChannel[boe.get().getChannelIndex()] > numberOfExpectedBuffersPerChannel) {                    throw new IllegalStateException("Received more buffers than expected " + "on channel " + boe.get().getChannelIndex() + ".");                }            }        }                for (int i = 0; i < numberOfBuffersPerChannel.length; i++) {            final int actualNumberOfReceivedBuffers = numberOfBuffersPerChannel[i];            if (actualNumberOfReceivedBuffers != numberOfExpectedBuffersPerChannel) {                throw new IllegalStateException("Received unexpected number of buffers " + "on channel " + i + " (" + actualNumberOfReceivedBuffers + " instead " + "of " + numberOfExpectedBuffersPerChannel + ").");            }        }    } finally {        inputGate.close();    }    return null;}
0
public void testHardCommitWithinAndSoftCommitMaxTimeMixedAdds() throws Exception
{    doTestSoftAndHardCommitMaxTimeMixedAdds(CommitWithinType.HARD);}
0
public void write(Fields fields, NormsProducer norms) throws IOException
{    write(writeState.fieldInfos, fields);}
0
public void setPoolSize(int poolSize)
{    this.poolSize = poolSize;}
0
public List<Operation> parse(String statement)
{    throw new AssertionError("Should not be called");}
0
public StartupCommand[] initialize() throws IllegalArgumentException
{    final StartupCommand[] cmds = super.initialize();    final Connection conn = getConnection();    Pool pool;    try {        pool = Pool.getByUuid(conn, _host.getPool());        final Pool.Record poolr = pool.getRecord(conn);        final Host.Record masterRecord = poolr.master.getRecord(conn);        if (_host.getUuid().equals(masterRecord.uuid)) {            _listener = new VmEventListener(true);                                        } else {            _listener = new VmEventListener(false);        }    } catch (final XenAPIException e) {        throw new CloudRuntimeException("Unable to determine who is the master", e);    } catch (final XmlRpcException e) {        throw new CloudRuntimeException("Unable to determine who is the master", e);    }    return cmds;}
0
public void add(int docId, Iterator iterator)
{    throw new UnsupportedOperationException();}
0
public com.google.protobuf.Descriptors.Descriptor getDescriptorForType()
{    return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_RegisterDagResponseProto_descriptor;}
0
public void drop_catalog(DropCatalogRequest catName, org.apache.thrift.async.AsyncMethodCallback resultHandler) throws org.apache.thrift.TException
{    checkReady();    drop_catalog_call method_call = new drop_catalog_call(catName, resultHandler, this, ___protocolFactory, ___transport);    this.___currentMethod = method_call;    ___manager.call(method_call);}
0
public int hashCode()
{    HashCodeBuilder builder = new HashCodeBuilder();    boolean pathPresent = path != null;    builder.append(pathPresent);    if (pathPresent) {        builder.append(path.toCharArray());    }    boolean lockObjectPresent = obj != null;    builder.append(lockObjectPresent);    if (lockObjectPresent) {        builder.append(obj.hashCode());    }    boolean modePresent = mode != null;    builder.append(modePresent);    if (modePresent) {        builder.append(mode);    }    return builder.toHashCode();}
0
public URL getURL(int columnIndex) throws SQLException
{        return null;}
0
public void warning(Throwable throwable)
{    warning((String) null, throwable);}
0
public void encode(Record value, OutputStream outStream) throws IOException
{}
0
public boolean storesLowerCaseIdentifiers() throws SQLException
{    throw new SQLFeatureNotSupportedException("Method not supported");}
0
public OBJECT<H1<T>> object(String selector)
{    return setSelector(object(), selector);}
0
public SQLCheckConstraint deepCopy()
{    return new SQLCheckConstraint(this);}
0
private void stackUnitIs(String name, String unit)
{    ValueAttributesInfo propertyValueAttributes = new ValueAttributesInfo();    propertyValueAttributes.setUnit(unit);    stackConfigWithMetadata.put(name, configProperty(name, unit));}
0
public void writeProperty(String namespace, String name, String value)
{    writeElement(namespace, name, OPENING);    buffer.append(value);    writeElement(namespace, name, CLOSING);}
0
public Pair<THBaseService.Client, TTransport> getClient() throws IOException
{    TSocket sock = new TSocket(connection.getHost(), connection.getPort());    sock.setSocketTimeout(connection.getOperationTimeout());    sock.setConnectTimeout(connection.getConnectTimeout());    TTransport tTransport = sock;    if (connection.isFramed()) {        tTransport = new TFramedTransport(tTransport);    }    try {        sock.open();    } catch (TTransportException e) {        throw new IOException(e);    }    TProtocol prot;    if (connection.isCompact()) {        prot = new TCompactProtocol(tTransport);    } else {        prot = new TBinaryProtocol(tTransport);    }    THBaseService.Client client = new THBaseService.Client(prot);    return new Pair<>(client, tTransport);}
0
 InfinispanEndpointConsumerBuilder customListener(String customListener)
{    doSetProperty("customListener", customListener);    return this;}
0
private void serveFile(File file) throws IOException
{    FileInputStream fis = new FileInputStream(file);    try {        setVerificationHeadersForGet(response, file);        setFileNameHeaders(response, file);        if (!file.exists()) {                        throw new FileNotFoundException(file.toString());                                        }                DataTransferThrottler throttler = parsedParams.isBootstrapStandby ? getThrottlerForBootstrapStandby(conf) : getThrottler(conf);        TransferFsImage.copyFileToStream(response.getOutputStream(), file, fis, throttler);    } finally {        IOUtils.closeStream(fis);    }}
0
public void setDefinitionId(Long definitionId)
{    this.definitionId = definitionId;}
0
public boolean isSetPartitionKeys()
{    return this.partitionKeys != null;}
0
public MergeTask consolidateBatches(long allocMemory, int inMemCount, int spilledRunsCount)
{    assert allocMemory == 0 || inMemCount > 0;    assert inMemCount + spilledRunsCount > 0;    if (inMemCount == 0 && spilledRunsCount <= 1) {        return new MergeTask(MergeAction.NONE, 0);    }    if (allocMemory > mergeMemoryLimit) {        return new MergeTask(MergeAction.SPILL, 0);    }                        int memMergeLimit = (int) ((mergeMemoryLimit - allocMemory) / spillBatchSize.maxBufferSize);    memMergeLimit = Math.max(0, memMergeLimit);    if (inMemCount > 0 && ((inMemCount + spilledRunsCount) > config.mergeLimit() || memMergeLimit < spilledRunsCount)) {        return new MergeTask(MergeAction.SPILL, 0);    }            memMergeLimit = Math.min(memMergeLimit, config.mergeLimit());    int mergeRunCount = spilledRunsCount - memMergeLimit;    if (mergeRunCount <= 0) {        return new MergeTask(MergeAction.NONE, 0);    }                        mergeRunCount += 1;                        memMergeLimit = (int) (memoryLimit / spillBatchSize.maxBufferSize) - 1;    mergeRunCount = Math.min(mergeRunCount, memMergeLimit);            mergeRunCount = Math.max(mergeRunCount, 2);        mergeRunCount = Math.min(mergeRunCount, config.mergeLimit());    return new MergeTask(MergeAction.MERGE, mergeRunCount);}
0
public void shouldNotCreateSegmentThatIsAlreadyExpired()
{    final long streamTime = updateStreamTimeAndCreateSegment(7);    assertNull(segments.getOrCreateSegmentIfLive(0, context, streamTime));    assertFalse(new File(context.stateDir(), "test/test.0").exists());}
0
private static void doPutOnSite1()
{    Region<String, String> region = ClusterStartupRule.getCache().getRegion("region");    region.put("serverkey", "servervalue");    assertThat("servervalue").isEqualTo(region.get("serverkey"));}
0
private void maybeInitBuilder()
{    if (viaProto || builder == null) {        builder = GetTimelineCollectorContextResponseProto.newBuilder(proto);    }    viaProto = false;}
0
public void setUp() throws Exception
{    super.setUp();    jt400Configuration = new Jt400Configuration("jt400://USER:password@host/QSYS.LIB/LIBRARY.LIB/QUEUE.DTAQ", getConnectionPool());    jt400Configuration.setCcsid(37);    connection = jt400Configuration.getConnection();}
0
public RelNode visit(LogicalJoin join)
{        return fail(join);}
0
private ComponentMapHolder putAll(ComponentMapHolder holder)
{    this.componentNameMap.putAll(holder.componentNameMap);    this.componentToParentGroupMap.putAll(holder.componentToParentGroupMap);    this.sourceToConnectionParentGroupMap.putAll(holder.sourceToConnectionParentGroupMap);    this.destinationToConnectionParentGroupMap.putAll(holder.destinationToConnectionParentGroupMap);    return this;}
0
public DataStructure createObject()
{    return new FlushCommand();}
0
public NewTopicBuilder replicationFactor(short replicationFactor)
{    this.replicationFactor = replicationFactor;    return this;}
0
public DecryptResult decrypt(DecryptRequest decryptRequest)
{    throw new UnsupportedOperationException();}
0
public void testValuesIterator()
{}
0
public void aes128CbcEnvelopedAndSignedMessageTest() throws Exception
{    envelopedAndSignedMessageTest(AS2EncryptionAlgorithm.AES128_CBC);}
0
public String getCommandName()
{    return s_name;}
0
protected Collection<Class<? extends Plugin>> nodePlugins()
{    return Collections.singleton(CustomScriptPlugin.class);}
0
public String getConfigData()
{    return configData;}
0
public void test_read_03() throws Exception
{    runner.setProperty(proc.SCRIPT_BODY, "def ff = session.get(); if(!ff)return; ff.read('UTF-8'){r-> assert r.getText()=='1234' }; REL_SUCCESS << ff; ");    runner.assertValid();    runner.enqueue("1234".getBytes("UTF-8"));    runner.run();    runner.assertAllFlowFilesTransferred(proc.REL_SUCCESS.getName(), 1);}
0
public static void beforeClass()
{    count = 0;    message = null;}
0
public void testInvalidCacheSplitFactorConfig() throws IOException
{    float[] singleFactorConfigValues = { 0.2f, 0f, -0.2f, 1f };    float[] multiFactorConfigValues = { 0.4f, 0f, 1f, .05f };    float[] memoryFactorConfigValues = { 0.4f, 0f, 0.2f, .5f };        boolean[] expectedOutcomes = { true, false, false, false };    Map<String, float[]> configMappings = ImmutableMap.of(BucketCache.SINGLE_FACTOR_CONFIG_NAME, singleFactorConfigValues, BucketCache.MULTI_FACTOR_CONFIG_NAME, multiFactorConfigValues, BucketCache.MEMORY_FACTOR_CONFIG_NAME, memoryFactorConfigValues);    Configuration conf = HBaseConfiguration.create();    checkConfigValues(conf, configMappings, expectedOutcomes);}
0
protected ExecuteResult doWork(ExecutableContext context) throws ExecuteException
{    doingWork = true;    try {        for (int i = 0; i < 60; i++) {            sleepOneSecond();            if (isDiscarded())                return new ExecuteResult(ExecuteResult.State.STOPPED, "stopped");        }        return new ExecuteResult();    } finally {        doingWork = false;    }}
0
public int getProcessorType()
{    return OperationExecutors.REGION_FUNCTION_EXECUTION_EXECUTOR;}
0
protected ProcedureExecutor<MasterProcedureEnv> getProcedureExecutor()
{    return master.getMasterProcedureExecutor();}
0
public void start(CoprocessorEnvironment env) throws IOException
{    super.start(env);    if (env instanceof RegionCoprocessorEnvironment) {        MetricRegistry registry = ((RegionCoprocessorEnvironment) env).getMetricRegistryForRegionServer();        if (endpointExecution == null) {            endpointExecution = registry.timer("EndpointExecution");        }    }}
0
public static void downto(float self, Number to, @ClosureParams(FirstParam.class) Closure closure)
{    float to1 = to.floatValue();    if (self >= to1) {        for (float i = self; i >= to1; i--) {            closure.call(i);        }    } else        throw new GroovyRuntimeException("The argument (" + to + ") to downto() cannot be greater than the value (" + self + ") it's called on.");}
0
public void configure()
{    from(CXF_RS_ENDPOINT_URI).recipientList(simple("direct:${header.operationName}"));    from("direct:getCustomer").process(new Processor() {        @Override        public void process(Exchange exchange) throws Exception {            assertEquals("123", exchange.getIn().getHeader("id"));            exchange.getOut().setBody(new Customer(123, "Raul"));            exchange.getOut().setHeader(Exchange.HTTP_RESPONSE_CODE, 200);        }    });    from("direct:newCustomer").process(new Processor() {        @Override        public void process(Exchange exchange) throws Exception {            Customer c = exchange.getIn().getBody(Customer.class);            assertNotNull(c);            assertEquals(123, c.getId());            assertEquals(12, exchange.getIn().getHeader("age"));            exchange.getOut().setHeader(Exchange.HTTP_RESPONSE_CODE, 200);        }    });}
0
protected static List<String> getParameters(String path)
{    List<String> parameters = null;    int startIndex = path.indexOf('{');    while (startIndex != -1) {        int endIndex = path.indexOf('}', startIndex);        if (endIndex != -1) {            if (parameters == null) {                parameters = new ArrayList<>();            }            parameters.add(path.substring(startIndex + 1, endIndex));            startIndex = path.indexOf('{', endIndex);        } else {                        startIndex = -1;        }    }    return parameters == null ? Collections.emptyList() : parameters;}
0
public void testUnsupportedOps()
{        ConcatenatedLists<Long> c = new ConcatenatedLists<>();    c.addSublist(Arrays.asList(0L, 1L));    try {        c.add(2L);        fail("Should throw");    } catch (UnsupportedOperationException ex) {    }    try {        c.addAll(Arrays.asList(2L, 3L));        fail("Should throw");    } catch (UnsupportedOperationException ex) {    }    try {        c.remove(0L);        fail("Should throw");    } catch (UnsupportedOperationException ex) {    }    try {        c.removeAll(Arrays.asList(0L, 1L));        fail("Should throw");    } catch (UnsupportedOperationException ex) {    }    try {        c.clear();        fail("Should throw");    } catch (UnsupportedOperationException ex) {    }    try {        c.retainAll(Arrays.asList(0L, 2L));        fail("Should throw");    } catch (UnsupportedOperationException ex) {    }    Iterator<Long> iter = c.iterator();    iter.next();    try {        iter.remove();        fail("Should throw");    } catch (UnsupportedOperationException ex) {    }}
0
public String format()
{    return format;}
0
public void task() throws Exception
{    Iterator<ProcessorDefinition<?>> it = AdviceWithTasks.createMatchByIterator(route, matchBy, selectFirst, selectLast, selectFrom, selectTo, maxDeep);    boolean match = false;    while (it.hasNext()) {        ProcessorDefinition<?> output = it.next();        if (matchBy.match(output)) {            List<ProcessorDefinition<?>> outputs = getOutputs(output, route);            if (outputs != null) {                int index = outputs.indexOf(output);                if (index != -1) {                    match = true;                                                            ProcessorDefinition<?> flattern = flatternOutput(replace);                    outputs.add(index + 1, flattern);                    Object old = outputs.remove(index);                                                            ProcessorDefinition parent = output.getParent() != null ? output.getParent() : route;                    flattern.setParent(parent);                                    }            }        }    }    if (!match) {        throw new IllegalArgumentException("There are no outputs which matches: " + matchBy.getId() + " in the route: " + route);    }}
1
public void testCreateProviderToPhysicalNetworkSuccess()
{    NetworkService networkService = Mockito.mock(NetworkService.class);    addNetworkServiceProviderCmd._networkService = networkService;    PhysicalNetworkServiceProvider physicalNetworkServiceProvider = Mockito.mock(PhysicalNetworkServiceProvider.class);    Mockito.when(networkService.addProviderToPhysicalNetwork(Matchers.anyLong(), Matchers.anyString(), Matchers.anyLong(), Matchers.anyList())).thenReturn(physicalNetworkServiceProvider);    try {        addNetworkServiceProviderCmd.create();    } catch (ResourceAllocationException e) {        e.printStackTrace();    }}
0
public String toString()
{    return "[address=" + address.toString() + ", user=" + user + "]";}
0
private ExtensionRequest getRequest(Map<String, Object> properties)
{    return new ExtensionRequest((String) properties.get(EXTENSION_NAME_PROPERTY_ID));}
0
public static long getLong(Timestamp timestamp)
{    return timestamp.toEpochSecond();}
0
private FileChannel createSpillingChannel() throws IOException
{    if (spillFile != null) {        throw new IllegalStateException("Spilling file already exists.");    }        int maxAttempts = 10;    for (int attempt = 0; attempt < maxAttempts; attempt++) {        String directory = tempDirs[rnd.nextInt(tempDirs.length)];        spillFile = new File(directory, randomString(rnd) + ".inputchannel");        if (spillFile.createNewFile()) {            return new RandomAccessFile(spillFile, "rw").getChannel();        }    }    throw new IOException("Could not find a unique file channel name in '" + Arrays.toString(tempDirs) + "' for spilling large records during deserialization.");}
0
public Closure<V> trampoline(final Object... args)
{    return new TrampolineClosure<V>(original.curry(args));}
0
public void setGatewayConflictResolver(GatewayConflictResolver resolver)
{    synchronized (allGatewayHubsLock) {        gatewayConflictResolver = resolver;    }}
0
public ValueFiller getValueFiller()
{    return new ValueFiller() {        private final MutableValueInt mval = new MutableValueInt();        @Override        public MutableValue getValue() {            return mval;        }        @Override        public void fillValue(int doc) throws IOException {            if (setDoc(doc)) {                mval.exists = true;                mval.value = LegacyNumericUtils.prefixCodedToInt(view.binaryValue());            } else {                mval.exists = false;                mval.value = 0;            }        }    };}
0
public void testNullValueACL() throws Exception
{    File tmpDir = ClientBase.createTmpDir();    ClientBase.setupTestEnv();    ZooKeeperServer zks = new ZooKeeperServer(tmpDir, tmpDir, 3000);    final int PORT = Integer.parseInt(HOSTPORT.split(":")[1]);    ServerCnxnFactory f = ServerCnxnFactory.createFactory(PORT, -1);    f.startup(zks);    ZooKeeper zk = ClientBase.createZKClient(HOSTPORT);    try {        List<ACL> acls = new ArrayList<ACL>();        acls.add(null);                try {            zk.create("/foo", "foo".getBytes(), acls, CreateMode.PERSISTENT);            fail("Expected InvalidACLException for null value in ACL List");        } catch (InvalidACLException e) {                }                try {            zk.create("/foo", "foo".getBytes(), acls, CreateMode.PERSISTENT, null);            fail("Expected InvalidACLException for null value in ACL List");        } catch (InvalidACLException e) {                }                try {            zk.setACL("/foo", acls, -1);            fail("Expected InvalidACLException for null value in ACL List");        } catch (InvalidACLException e) {                }    } finally {        zk.close();        f.shutdown();        zks.shutdown();        assertTrue("waiting for server down", ClientBase.waitForServerDown(HOSTPORT, ClientBase.CONNECTION_TIMEOUT));    }}
0
public static void setProxy(final ProcessContext context, final HttpClientBuilder clientBuilder, final CredentialsProvider credentialsProvider)
{        final ProxyConfiguration proxyConfig = ProxyConfiguration.getConfiguration(context, () -> {        if (context.getProperty(PROXY_HOST).isSet() && context.getProperty(PROXY_PORT).isSet()) {            final ProxyConfiguration componentProxyConfig = new ProxyConfiguration();            final String host = context.getProperty(PROXY_HOST).getValue();            final int port = context.getProperty(PROXY_PORT).asInteger();            componentProxyConfig.setProxyType(Proxy.Type.HTTP);            componentProxyConfig.setProxyServerHost(host);            componentProxyConfig.setProxyServerPort(port);            return componentProxyConfig;        }        return ProxyConfiguration.DIRECT_CONFIGURATION;    });    if (Proxy.Type.HTTP.equals(proxyConfig.getProxyType())) {        final String host = proxyConfig.getProxyServerHost();        final int port = proxyConfig.getProxyServerPort();        clientBuilder.setProxy(new HttpHost(host, port));        if (proxyConfig.hasCredential()) {            final AuthScope proxyAuthScope = new AuthScope(host, port);            final UsernamePasswordCredentials proxyCredential = new UsernamePasswordCredentials(proxyConfig.getProxyUserName(), proxyConfig.getProxyUserPassword());            credentialsProvider.setCredentials(proxyAuthScope, proxyCredential);        }    }}
0
public boolean isSevereAlertCompatible()
{        return true;}
0
public void configure() throws Exception
{    from(getFtpUrl()).to("mock:result");}
0
public void process(Exchange exchange) throws Exception
{    processor.process(exchange);}
0
public void testNotBase64EncodedKey()
{    String key = "NotBase64EncodedKey";    ValidationResult result = strategy.validateKey(key);    assertFalse(result.isValid());}
0
private Tuple2<MutableObjectIterator<BinaryRow>, MutableObjectIterator<BinaryRow>> oneEmpty()
{    return new Tuple2<>(new ListIterator(Arrays.asList(newRow(1), newRow(2))), new ListIterator(emptyList()));}
0
private Map<String, String> queryExhibitors(Exhibitors localExhibitors)
{    Map<String, String> values = newValues();    long start = System.currentTimeMillis();    int retries = 0;    boolean done = false;    while (!done) {        List<String> hostnames = Lists.newArrayList(localExhibitors.getHostnames());        if (hostnames.size() == 0) {            done = true;        } else {            String hostname = hostnames.get(random.nextInt(hostnames.size()));            try {                String encoded = restClient.getRaw(hostname, localExhibitors.getRestPort(), restUriPath, MIME_TYPE);                values.putAll(decodeExhibitorList(encoded));                done = true;            } catch (Throwable e) {                ThreadUtils.checkInterrupted(e);                if (retryPolicy.allowRetry(retries++, System.currentTimeMillis() - start, RetryLoop.getDefaultRetrySleeper())) {                                    } else {                                        done = true;                }            }        }    }    return values;}
1
protected String getUriPrefix()
{    return "xmpp://localhost:" + embeddedXmppTestServer.getXmppPort() + "/camel?login=false&room=camel-anon";}
0
public void testQualifiedVirtualMethodReference()
{    long instant = randomLong();    assertEquals(instant, exec("List l = [params.d]; return l.stream().mapToLong(Instant::toEpochMilli).sum()", singletonMap("d", Instant.ofEpochMilli(instant)), true));}
0
public void setDate(String date)
{    this.date = date;}
0
public void create_table_req(CreateTableRequest request) throws AlreadyExistsException, InvalidObjectException, MetaException, NoSuchObjectException, org.apache.thrift.TException
{    send_create_table_req(request);    recv_create_table_req();}
0
public void testBulkLoader_NoArgs()
{    runTool(1, "org.apache.cassandra.tools.BulkLoader");    assertNoUnexpectedThreadsStarted(null, null);    assertSchemaNotLoaded();    assertCLSMNotLoaded();    assertSystemKSNotLoaded();    assertKeyspaceNotLoaded();    assertServerNotLoaded();}
0
 void setExpiryTime(long expiryTime)
{    this.expiryTime = expiryTime;}
0
public void testBogusArguments() throws Exception
{    IllegalArgumentException expected = expectThrows(IllegalArgumentException.class, () -> {        tokenFilterFactory("Length", LengthFilterFactory.MIN_KEY, "4", LengthFilterFactory.MAX_KEY, "5", "bogusArg", "bogusValue");    });    assertTrue(expected.getMessage().contains("Unknown parameters"));}
0
public int compareTo(truncate_table_args other)
{    if (!getClass().equals(other.getClass())) {        return getClass().getName().compareTo(other.getClass().getName());    }    int lastComparison = 0;    lastComparison = Boolean.valueOf(isSetDbName()).compareTo(other.isSetDbName());    if (lastComparison != 0) {        return lastComparison;    }    if (isSetDbName()) {        lastComparison = org.apache.thrift.TBaseHelper.compareTo(this.dbName, other.dbName);        if (lastComparison != 0) {            return lastComparison;        }    }    lastComparison = Boolean.valueOf(isSetTableName()).compareTo(other.isSetTableName());    if (lastComparison != 0) {        return lastComparison;    }    if (isSetTableName()) {        lastComparison = org.apache.thrift.TBaseHelper.compareTo(this.tableName, other.tableName);        if (lastComparison != 0) {            return lastComparison;        }    }    lastComparison = Boolean.valueOf(isSetPartNames()).compareTo(other.isSetPartNames());    if (lastComparison != 0) {        return lastComparison;    }    if (isSetPartNames()) {        lastComparison = org.apache.thrift.TBaseHelper.compareTo(this.partNames, other.partNames);        if (lastComparison != 0) {            return lastComparison;        }    }    return 0;}
0
public void init()
{    super.init();    nonMatchedIterator = ((MapJoinTableContainer) hashMap.originalTableContainer).createNonMatchedSmallTableIterator(matchTracker);    TypeInfo integerTypeInfo;    switch(hashMap.hashTableKeyType) {        case BOOLEAN:            integerTypeInfo = TypeInfoFactory.booleanTypeInfo;            break;        case BYTE:            integerTypeInfo = TypeInfoFactory.byteTypeInfo;            break;        case SHORT:            integerTypeInfo = TypeInfoFactory.shortTypeInfo;            break;        case INT:            integerTypeInfo = TypeInfoFactory.intTypeInfo;            break;        case LONG:            integerTypeInfo = TypeInfoFactory.longTypeInfo;            break;        default:            throw new RuntimeException("Unexpected key type " + hashMap.hashTableKeyType);    }    keyBinarySortableDeserializeRead = new BinarySortableDeserializeRead(new TypeInfo[] { integerTypeInfo }, false);}
0
public void setStreamThreadStateListener(final StreamThread.StateListener listener)
{    if (state == State.CREATED) {        for (final StreamThread thread : threads) {            thread.setStateListener(listener);        }    } else {        throw new IllegalStateException("Can only set StateListener in CREATED state. " + "Current state is: " + state);    }}
0
public void setUp() throws Exception
{    setUpInternal();}
0
public void testBeans() throws Exception
{    Map<String, HystrixConfigurationDefinition> beans = context.getBeansOfType(HystrixConfigurationDefinition.class);    Assert.assertEquals(4, beans.size());    Assert.assertEquals("global-group", beans.get(HystrixConstants.DEFAULT_HYSTRIX_CONFIGURATION_ID).getGroupKey());    Assert.assertEquals("bean-group", beans.get("bean-conf").getGroupKey());    Assert.assertEquals("conf-1-group", beans.get("conf-1").getGroupKey());    Assert.assertEquals("conf-2-group", beans.get("conf-2").getGroupKey());}
0
protected void populateObject(Object object) throws Exception
{    super.populateObject(object);    ProducerAck info = (ProducerAck) object;    info.setProducerId(createProducerId("ProducerId:1"));    info.setSize(1);}
0
public void testIndexingWhileReplicaIsDown() throws Exception
{    Path primaryPath = createTempDir("primary");    NodeProcess primary = startNode(-1, 0, primaryPath, -1, false);    Path replicaPath = createTempDir("replica");    NodeProcess replica = startNode(primary.tcpPort, 1, replicaPath, -1, true);    sendReplicasToPrimary(primary, replica);        LineFileDocs docs = new LineFileDocs(random());    try (Connection c = new Connection(primary.tcpPort)) {        c.out.writeByte(SimplePrimaryNode.CMD_INDEXING);        for (int i = 0; i < 10; i++) {            Document doc = docs.nextDoc();            primary.addOrUpdateDocument(c, doc, false);        }    }        long primaryVersion1 = primary.flush(0);    assertTrue(primaryVersion1 > 0);        waitForVersionAndHits(replica, primaryVersion1, 10);        replica.commit();    replica.crash();    sendReplicasToPrimary(primary);        try (Connection c = new Connection(primary.tcpPort)) {        c.out.writeByte(SimplePrimaryNode.CMD_INDEXING);        for (int i = 0; i < 10; i++) {            Document doc = docs.nextDoc();            primary.addOrUpdateDocument(c, doc, false);        }    }        long primaryVersion2 = primary.flush(0);    assertTrue(primaryVersion2 > primaryVersion1);        replica = startNode(primary.tcpPort, 1, replicaPath, -1, false);    sendReplicasToPrimary(primary, replica);        assertVersionAndHits(replica, primaryVersion1, 10);        replica.newNRTPoint(primaryVersion2, 0, primary.tcpPort);    waitForVersionAndHits(replica, primaryVersion2, 20);    replica.close();    primary.close();}
0
public void setId(Long id)
{    this.id = id;}
0
public Path call() throws Exception
{    final long start = Time.monotonicNow();    try {        final long seed = ran.nextLong();        final int numBlocks = ran.nextInt(MAX_BLOCK_NUM) + 1;        numBlockCreated.addAndGet(numBlocks);        return createFile(dir, numBlocks, seed, dfs);    } finally {        createFileTime.addAndGet(Time.monotonicNow() - start);    }}
0
public DefaultErrorHandlerBuilder log(org.slf4j.Logger log)
{    getLogger().setLog(log);    return this;}
0
public int getFieldNumber(java.lang.String name)
{    return org.apache.drill.exec.proto.SchemaUserBitShared.SerializedField.getFieldNumber(name);}
0
public Integer getLimit()
{    return limit;}
0
public boolean streamsResult()
{        return true;}
0
public void testPoisonOnException() throws IOException
{    final AtomicInteger poisonCount = new AtomicInteger(0);    final PublisherLease lease = new PublisherLease(producer, 1024 * 1024, 1000L, logger) {        @Override        public void poison() {            poisonCount.incrementAndGet();            super.poison();        }    };    final FlowFile flowFile = Mockito.spy(new MockFlowFile(1L));        Mockito.when(flowFile.getSize()).thenReturn(1L);    final String topic = "unit-test";    final byte[] messageKey = null;    final byte[] demarcatorBytes = null;    final InputStream failureInputStream = new InputStream() {        @Override        public int read() throws IOException {            throw new IOException("Intentional Unit Test Exception");        }    };    try {        lease.publish(flowFile, failureInputStream, messageKey, demarcatorBytes, topic);        Assert.fail("Expected IOException");    } catch (final IOException ioe) {        }    assertEquals(1, poisonCount.get());    final PublishResult result = lease.complete();    assertTrue(result.getFailedFlowFiles().contains(flowFile));    assertFalse(result.getSuccessfulFlowFiles().contains(flowFile));}
0
public Builder putAlias(AliasMetaData aliasMetaData)
{    aliases.put(aliasMetaData.alias(), aliasMetaData);    return this;}
0
public boolean validate()
{    return true;}
0
public int getTrackHeight()
{    return trackHeight;}
0
public void testToXContentWithDeniedFieldsOnly()
{    final IndicesPrivileges privileges = IndicesPrivileges.builder().indices("abc").privileges("all").deniedFields("secret.*").allowRestrictedIndices(randomBoolean()).build();    final String json = Strings.toString(privileges);    assertThat(json, containsString("field_security"));    assertThat(json, containsString("\"field_security\":{\"grant\":[\"*\"],\"except\":[\"secret.*\"]}"));}
0
 PahoEndpointProducerBuilder resolveMqttConnectOptions(String resolveMqttConnectOptions)
{    doSetProperty("resolveMqttConnectOptions", resolveMqttConnectOptions);    return this;}
0
public static SchemaUserTypeCreator createConstructorCreator(Class<T> clazz, Constructor<T> constructor, Schema schema, List<FieldValueTypeInformation> types)
{    try {        DynamicType.Builder<SchemaUserTypeCreator> builder = BYTE_BUDDY.with(new InjectPackageStrategy(clazz)).subclass(SchemaUserTypeCreator.class).method(ElementMatchers.named("create")).intercept(new ConstructorCreateInstruction(types, clazz, constructor));        return builder.make().load(ReflectHelpers.findClassLoader(clazz.getClassLoader()), ClassLoadingStrategy.Default.INJECTION).getLoaded().getDeclaredConstructor().newInstance();    } catch (InstantiationException | IllegalAccessException | NoSuchMethodException | InvocationTargetException e) {        throw new RuntimeException("Unable to generate a creator for class " + clazz + " with schema " + schema);    }}
0
public GetSafeModeResponse getSafeMode(GetSafeModeRequest request) throws IOException
{    boolean isInSafeMode = false;    RouterSafemodeService safeModeService = this.router.getSafemodeService();    if (safeModeService != null) {        isInSafeMode = safeModeService.isInSafeMode();            }    return GetSafeModeResponse.newInstance(isInSafeMode);}
1
public void testDelayedDirectConnectionListener() throws Exception
{    for (int i = 0; i < 10; i++) {        Message msga = session.createTextMessage("hello a");        msga.setStringProperty("JMSXGroupID", "A");        producer.send(msga);        Message msgb = session.createTextMessage("hello b");        msgb.setStringProperty("JMSXGroupID", "B");        producer.send(msgb);        Message msgc = session.createTextMessage("hello c");        msgc.setStringProperty("JMSXGroupID", "C");        producer.send(msgc);    }        int[] counters = { 10, 10, 10 };    CountDownLatch startSignal = new CountDownLatch(1);    CountDownLatch doneSignal = new CountDownLatch(1);    messageCount.put("worker1", 0);    messageGroups.put("worker1", new HashSet<String>());    Worker worker1 = new Worker(connection, destination, "worker1", startSignal, doneSignal, counters, messageCount, messageGroups);    messageCount.put("worker2", 0);    messageGroups.put("worker2", new HashSet<String>());    Worker worker2 = new Worker(connection, destination, "worker2", startSignal, doneSignal, counters, messageCount, messageGroups);    messageCount.put("worker3", 0);    messageGroups.put("worker3", new HashSet<String>());    Worker worker3 = new Worker(connection, destination, "worker3", startSignal, doneSignal, counters, messageCount, messageGroups);    new Thread(worker1).start();    new Thread(worker2).start();    new Thread(worker3).start();    startSignal.countDown();    doneSignal.await();        if (consumersBeforeDispatchStarts == 0 && timeBeforeDispatchStarts == 0) {                return;    }    for (String worker : messageCount.keySet()) {                assertEquals("worker " + worker + " received " + messageCount.get(worker) + " messages from groups " + messageGroups.get(worker), 10, messageCount.get(worker).intValue());        assertEquals("worker " + worker + " received " + messageCount.get(worker) + " messages from groups " + messageGroups.get(worker), 1, messageGroups.get(worker).size());    }}
1
public void setStatus(String status)
{}
0
protected DatagramHeaderMarshaller createDatagramHeaderMarshaller()
{    return new MulticastDatagramHeaderMarshaller("udp://dummyHostName:" + getPort());}
0
public void configure() throws Exception
{    from("jetty://http://localhost:{{port}}/test1?enableCORS=false").transform(simple("OK"));    from("jetty://http://localhost:{{port2}}/test2?enableCORS=true").transform(simple("OK"));}
0
public void updateVertex(Vertex<K, VV> vertex, MessageIterator<VV> messages) throws Exception
{    VV current = vertex.getValue();    VV min = current;    for (VV msg : messages) {        if (msg.compareTo(min) < 0) {            min = msg;        }    }    if (!min.equals(current)) {        setNewVertexValue(min);    }}
0
public Writable serialize(Object obj, ObjectInspector objInspector) throws SerDeException
{    throw new RuntimeException("not supported");}
0
public SpatialRelation getShapeRel()
{    return cellShapeRel;}
0
public void accumulate(GenericRow acc, Object value, Long order)
{    if (value != null && acc.getLong(1) > order) {        acc.setField(0, value);        acc.setLong(1, order);    }}
0
public static String liveMessage(String token)
{    if (StringUtils.isBlank(token)) {                return ZeppelinhubMessage.EMPTY.toJson();    }    HashMap<String, Object> data = new HashMap<>();    data.put("token", token);    return ZeppelinhubMessage.newMessage(ZeppelinHubOp.LIVE, data, new HashMap<String, String>()).toJson();}
1
public void returnConnection(Connection connection) throws Exception
{    connections.returnObject(connection);}
0
public ColumnMetadata visitInterval_year(SchemaParser.Interval_yearContext ctx)
{    return constructColumn(Types.withMode(TypeProtos.MinorType.INTERVALYEAR, mode));}
0
public void setDocument(int docid)
{    leafLookup.setDocument(docid);}
0
public static JsonNode countersToJSON(Counters counters)
{    ObjectMapper mapper = new ObjectMapper();    ArrayNode nodes = mapper.createArrayNode();    if (counters != null) {        for (CounterGroup counterGroup : counters) {            ObjectNode groupNode = nodes.addObject();            groupNode.put("NAME", counterGroup.getName());            groupNode.put("DISPLAY_NAME", counterGroup.getDisplayName());            ArrayNode countersNode = groupNode.putArray("COUNTERS");            for (Counter counter : counterGroup) {                ObjectNode counterNode = countersNode.addObject();                counterNode.put("NAME", counter.getName());                counterNode.put("DISPLAY_NAME", counter.getDisplayName());                counterNode.put("VALUE", counter.getValue());            }        }    }    return nodes;}
0
 JarEntry getMetaInfEntry()
{    return metaInfEntry;}
0
protected CamelContext createCamelContext() throws Exception
{    CamelContext camelContext = super.createCamelContext();    camelContext.addComponent("async", new MyAsyncComponent());    ConnectionFactory connectionFactory = CamelJmsTestHelper.createConnectionFactory();    camelContext.addComponent("activemq", jmsComponentAutoAcknowledge(connectionFactory));    return camelContext;}
0
public void setLocker(Locker locker) throws IOException
{    this.locker = locker;    locker.setLockable(this);    if (this instanceof PersistenceAdapter) {        this.locker.configure((PersistenceAdapter) this);    }}
0
private void testDateAddColCol(PrimitiveCategory colType1, boolean isPositive) throws HiveException
{    LongColumnVector date1 = newRandomLongColumnVector(10000, size);    LongColumnVector days2 = newRandomLongColumnVector(1000, size);    ColumnVector col1 = castTo(date1, colType1);    LongColumnVector output = new LongColumnVector(size);    VectorizedRowBatch batch = new VectorizedRowBatch(3, size);    batch.cols[0] = col1;    batch.cols[1] = days2;    batch.cols[2] = output;    validateDateAdd(batch, date1, days2, colType1, isPositive);    TestVectorizedRowBatch.addRandomNulls(date1);    batch.cols[0] = castTo(date1, colType1);    validateDateAdd(batch, date1, days2, colType1, isPositive);    TestVectorizedRowBatch.addRandomNulls(days2);    batch.cols[1] = days2;    validateDateAdd(batch, date1, days2, colType1, isPositive);}
0
public H3<T> i(String selector, String cdata)
{    return setSelector(i(), selector).__(cdata).__();}
0
protected void disallowCreateInDb(String dbName, String userName, String location) throws Exception
{    setPermissions(location, "-r--r--r--");}
0
public Void answer(InvocationOnMock invocation) throws Throwable
{    UpdateSettingsRequest request = (UpdateSettingsRequest) invocation.getArguments()[0];    @SuppressWarnings("unchecked")    ActionListener<AcknowledgedResponse> listener = (ActionListener<AcknowledgedResponse>) invocation.getArguments()[1];    assertThat(request.settings(), equalTo(step.getSettings()));    assertThat(request.indices(), equalTo(new String[] { indexMetaData.getIndex().getName() }));    listener.onFailure(exception);    return null;}
0
protected void waitForAllMessagesToBeReceived(int messageCount) throws Exception
{    allMessagesList.waitForMessagesToArrive(messageCount);}
0
public void testWSHttpCallEcho() throws Exception
{        received.clear();    latch = new CountDownLatch(2);    AsyncHttpClient c = new DefaultAsyncHttpClient();    WebSocket websocket = c.prepareGet("ws://localhost:" + port + "/bar").execute(new WebSocketUpgradeHandler.Builder().addWebSocketListener(new WebSocketTextListener() {        @Override        public void onMessage(String message) {            received.add(message);                        latch.countDown();        }        @Override        public void onOpen(WebSocket websocket) {        }        @Override        public void onClose(WebSocket websocket) {        }        @Override        public void onError(Throwable t) {            t.printStackTrace();        }    }).build()).get();    websocket.sendMessage("Beer");    assertTrue(latch.await(10, TimeUnit.SECONDS));    assertEquals(2, received.size());        assertTrue(received.contains("The bar has Beer"));    assertTrue(received.contains("Broadcasting to Bar"));    websocket.close();    c.close();}
1
protected void cancelTask()
{    if (headOperator != null) {        headOperator.cancel();    }}
0
public void setUp() throws IOException
{    procSched = new SimpleProcedureScheduler();    procSched.start();}
0
public void run()
{    try {        invoked.countDown();        block.await();    } catch (InterruptedException e) {        fail();    }}
0
public boolean isValueByteArray()
{    return rawValue instanceof byte[];}
0
private String getMetaStoreUri(Configuration configuration)
{    if (configuration.get(HiveConf.ConfVars.METASTOREURIS.varname) == null) {        return metaStoreURI;    } else {        return configuration.get(HiveConf.ConfVars.METASTOREURIS.varname);    }}
0
public ArrayList<ConfEntryInfo> getAcls()
{    return acls;}
0
public void preShutdown(ObserverContext<MasterCoprocessorEnvironment> c) throws IOException
{    requirePermission(c, "shutdown", Action.ADMIN);}
0
public int hashCode()
{    List<Object> list = new ArrayList<Object>();    boolean present_db_name = true && (isSetDb_name());    list.add(present_db_name);    if (present_db_name)        list.add(db_name);    boolean present_tbl_name = true && (isSetTbl_name());    list.add(present_tbl_name);    if (present_tbl_name)        list.add(tbl_name);    boolean present_part_vals = true && (isSetPart_vals());    list.add(present_part_vals);    if (present_part_vals)        list.add(part_vals);    boolean present_new_part = true && (isSetNew_part());    list.add(present_new_part);    if (present_new_part)        list.add(new_part);    return list.hashCode();}
0
private static void addAvgAndMaxToSensor(final Sensor sensor, final String group, final Map<String, String> tags, final String operation, final String descriptionOfAvg, final String descriptionOfMax)
{    sensor.add(new MetricName(operation + AVG_SUFFIX, group, descriptionOfAvg, tags), new Avg());    sensor.add(new MetricName(operation + MAX_SUFFIX, group, descriptionOfMax, tags), new Max());}
0
public void produceMessages() throws Exception
{    ConnectionFactory connectionFactory = new ActiveMQConnectionFactory(brokerService.getTransportConnectors().get(0).getConnectUri().toString());    Connection connection = connectionFactory.createConnection();    Session session = connection.createSession(false, Session.AUTO_ACKNOWLEDGE);    Destination destination = session.createQueue(QUEUE_NAME);    MessageProducer producer = session.createProducer(destination);    producer.setDeliveryMode(DeliveryMode.PERSISTENT);    long start = System.currentTimeMillis();    for (int priority : PRIORITIES) {        String name = null;        if (priority == 10) {            name = "high";        } else if (priority == 5) {            name = "mid";        } else {            name = "low";        }        for (int i = 1; i <= MESSAGE_COUNT_OF_ONE_GROUP; i++) {            TextMessage message = session.createTextMessage(name + "_" + i);            message.setIntProperty("priority", priority);            producer.send(message);        }    }    long end = System.currentTimeMillis();        producer.close();    session.close();    connection.close();}
1
protected void doSync() throws IOException
{    avroWriter.flush();        if (this.out instanceof HdfsDataOutputStream) {        ((HdfsDataOutputStream) this.out).hsync(EnumSet.of(HdfsDataOutputStream.SyncFlag.UPDATE_LENGTH));    } else {        this.out.hsync();    }}
1
public void setId(String id)
{    this.id = id;}
0
public void testKeepAnalyzer() throws IOException
{    Reader reader = new StringReader(input);    Analyzer analyzer = new WhitespaceAnalyzer();    TokenStream ts = analyzer.tokenStream(null, reader);    ts.reset();    TokenStream f = new BloomTokenFilter(getFilter(filterTokens), true, /* keep matching tokens */    ts);    validateTokens(expectedKeepTokens, f);    ts.end();    ts.close();}
0
 static Action fromName(String name)
{    if (ObjectHelper.isNotEmpty(name)) {        for (Action action : VALUES) {            if (action.name().equalsIgnoreCase(name)) {                return action;            }        }    }    return null;}
0
public void doRecoverNextMessages(TransactionContext c, ActiveMQDestination destination, String clientId, String subscriptionName, long seq, long priority, int maxReturned, JDBCMessageRecoveryListener listener) throws Exception
{    PreparedStatement s = null;    ResultSet rs = null;    try {        s = c.getConnection().prepareStatement(this.statements.getFindDurableSubMessagesStatement());        s.setMaxRows(Math.min(maxReturned * 2, maxRows));        s.setString(1, destination.getQualifiedName());        s.setString(2, clientId);        s.setString(3, subscriptionName);        s.setLong(4, seq);        rs = s.executeQuery();        int count = 0;        if (this.statements.isUseExternalMessageReferences()) {            while (rs.next() && count < maxReturned) {                if (listener.recoverMessageReference(rs.getString(1))) {                    count++;                }            }        } else {            while (rs.next() && count < maxReturned) {                if (listener.recoverMessage(rs.getLong(1), getBinaryData(rs, 2))) {                    count++;                }            }        }    } finally {        close(rs);        close(s);    }}
0
 void addReturnResult(SSLEngineResult... sslEngineResult)
{    for (SSLEngineResult result : sslEngineResult) {        returnResults.add(result);    }}
0
public ReconfigBuilderMain withUnhandledErrorListener(UnhandledErrorListener listener)
{    backgrounding = new Backgrounding(backgrounding, listener);    return this;}
0
public AggregateCall copy(List<Integer> args)
{        return copy(args, filterArg, collation);}
0
public void testGSM338AndGT8MAXGSMMESSAGELENGTH() throws Exception
{    StringBuffer message = new StringBuffer();    for (int index = 0; index < (8 * CMConstants.MAX_GSM_MESSAGE_LENGTH_PER_PART_IF_MULTIPART + 1); index++) {        message.append("a");    }    final CMMessage cmMessage = new CMMessage(validNumber, message.toString());    cmMessage.setUnicodeAndMultipart(CMConstants.DEFAULT_MULTIPARTS);    Assert.isTrue(cmMessage.getMultiparts() == 8);    Assert.isTrue(!cmMessage.isUnicode());}
0
public Sql unparsedTo(String expectedUnparsed)
{    getTester().checkUnparsed(this.sql, expectedUnparsed);    return this;}
0
protected List<DistributionField> getDistributionField(DrillAggregateRel rel, boolean allFields)
{    List<DistributionField> groupByFields = Lists.newArrayList();    for (int group : remapGroupSet(rel.getGroupSet())) {        DistributionField field = new DistributionField(group);        groupByFields.add(field);        if (!allFields && groupByFields.size() == 1) {                        break;        }    }    return groupByFields;}
0
public long longValue() throws IOException
{    return slice.readLong(((long) doc) << 3);}
0
public void testQuartzAutoStart() throws Exception
{    MockEndpoint mock = getMockEndpoint("mock:one");    mock.expectedMessageCount(0);    QuartzComponent quartz = context.getComponent("quartz", QuartzComponent.class);    assertFalse("Should not have started scheduler", quartz.getScheduler().isStarted());    Thread.sleep(2000);    assertMockEndpointsSatisfied();    mock.reset();    mock.expectedMinimumMessageCount(1);        quartz.getScheduler().start();    assertMockEndpointsSatisfied();}
0
public List<LogicalType> getChildren()
{    return Collections.emptyList();}
0
public void testHoldInEscrow() throws Exception
{        final com.braintreegateway.Result result = requestBody("direct://HOLDINESCROW", null);    assertNotNull("holdInEscrow result", result);    }
1
public String getHost()
{    return hostname;}
0
public void validate() throws org.apache.thrift.TException
{}
0
public int hashCode()
{    if (groupValue == null) {        return 0;    } else {        return groupValue.hashCode();    }}
0
 static String toId(Model model)
{    if (model == null) {        return "";    }    String groupId = model.getGroupId();    if (groupId == null && model.getParent() != null) {        groupId = model.getParent().getGroupId();    }    String artifactId = model.getArtifactId();    String version = model.getVersion();    if (version == null && model.getParent() != null) {        version = model.getParent().getVersion();    }    if (version == null) {        version = "[unknown-version]";    }    return toId(groupId, artifactId, version);}
0
private Writable createMap(final Object obj, final MapObjectInspector inspector) throws SerDeException
{    final Map<?, ?> sourceMap = inspector.getMap(obj);    final ObjectInspector keyInspector = inspector.getMapKeyObjectInspector();    final ObjectInspector valueInspector = inspector.getMapValueObjectInspector();    final List<ArrayWritable> array = new ArrayList<ArrayWritable>();    if (sourceMap != null) {        for (final Entry<?, ?> keyValue : sourceMap.entrySet()) {            final Writable key = createObject(keyValue.getKey(), keyInspector);            final Writable value = createObject(keyValue.getValue(), valueInspector);            if (key != null) {                Writable[] arr = new Writable[2];                arr[0] = key;                arr[1] = value;                array.add(new ArrayWritable(Writable.class, arr));            }        }    }    if (array.size() > 0) {        final ArrayWritable subArray = new ArrayWritable(ArrayWritable.class, array.toArray(new ArrayWritable[array.size()]));        return new ArrayWritable(Writable.class, new Writable[] { subArray });    } else {        return null;    }}
0
 SplunkEndpointConsumerBuilder initialDelay(String initialDelay)
{    doSetProperty("initialDelay", initialDelay);    return this;}
0
public void setup()
{}
0
public void testPopulateResources_hostNameProperty() throws Exception
{    ConnectionFactory connectionFactory = createNiceMock(ConnectionFactory.class);    Connection connection = createNiceMock(Connection.class);    Statement statement = createNiceMock(Statement.class);    ResultSet resultSet = createNiceMock(ResultSet.class);        expect(connectionFactory.getConnection()).andReturn(connection).once();    expect(connection.createStatement()).andReturn(statement).once();    expect(statement.executeQuery(anyObject(String.class))).andReturn(resultSet).once();    expect(resultSet.next()).andReturn(true);    expect(resultSet.getLong("RecordTimeStamp")).andReturn(999990L);    expect(resultSet.getNString("MetricValue")).andReturn("0");    expect(resultSet.next()).andReturn(true);    expect(resultSet.getLong("RecordTimeStamp")).andReturn(999991L);    expect(resultSet.getNString("MetricValue")).andReturn("1");    expect(resultSet.next()).andReturn(true);    expect(resultSet.getLong("RecordTimeStamp")).andReturn(999992L);    expect(resultSet.getNString("MetricValue")).andReturn("2");    expect(resultSet.next()).andReturn(true);    expect(resultSet.getLong("RecordTimeStamp")).andReturn(999993L);    expect(resultSet.getNString("MetricValue")).andReturn("3");    expect(resultSet.next()).andReturn(false);        replay(connectionFactory, connection, statement, resultSet);    TestHostInfoProvider hostProvider = new TestHostInfoProvider();    SQLPropertyProvider provider = new SQLPropertyProvider(PropertyHelper.getGangliaPropertyIds(Resource.Type.HostComponent), hostProvider, CLUSTER_NAME_PROPERTY_ID, HOST_NAME_PROPERTY_ID, COMPONENT_NAME_PROPERTY_ID, SERVICE_NAME_PROPERTY_ID, connectionFactory);        Resource resource = new ResourceImpl(Resource.Type.HostComponent);    resource.setProperty(CLUSTER_NAME_PROPERTY_ID, "c1");    resource.setProperty(HOST_NAME_PROPERTY_ID, "domU-12-31-39-0E-34-E1.compute-1.internal");    resource.setProperty(COMPONENT_NAME_PROPERTY_ID, "DATANODE");    resource.setProperty(SERVICE_NAME_PROPERTY_ID, "HDFS");        Map<String, TemporalInfo> temporalInfoMap = new HashMap<String, TemporalInfo>();    temporalInfoMap.put(PROPERTY_ID_1, new TemporalInfoImpl(10L, -1L, -1L));    Request request = PropertyHelper.getReadRequest(Collections.singleton(PROPERTY_ID_1), temporalInfoMap);    provider.populateResources(Collections.singleton(resource), request, null);    Assert.assertEquals("domU-12-31-39-0E-34-E1.compute-1.internal", hostProvider.getHostId());    Assert.assertNull(hostProvider.getClusterName());    Assert.assertNull(hostProvider.getComponentName());        verify(connectionFactory, connection, statement, resultSet);}
0
public int nextDoc() throws IOException
{    assert state != State.DOC_FINISHED : "nextDoc() called after NO_MORE_DOCS: " + in;    int nextDoc = in.nextDoc();    assert nextDoc > doc : "backwards nextDoc from " + doc + " to " + nextDoc + ": " + in;    if (nextDoc == DocIdSetIterator.NO_MORE_DOCS) {        state = State.DOC_FINISHED;    } else {        assert in.startPosition() == -1 : "invalid initial startPosition() [should be -1]: " + in;        assert in.endPosition() == -1 : "invalid initial endPosition() [should be -1]: " + in;        state = State.POS_START;    }    doc = nextDoc;    return docID();}
0
public void setRegionIdleTimeout(ExpirationAttributes idleTimeout)
{    if (idleTimeout == null) {        throw new IllegalArgumentException("idleTimeout must not be null");    }    this.regionAttributes.regionIdleTimeout = idleTimeout.getTimeout();    this.regionAttributes.regionIdleTimeoutExpirationAction = idleTimeout.getAction();    this.regionAttributes.setHasRegionIdleTimeout(true);}
0
public void testMatchOverwrite()
{    IngestDocument ingestDocument = new IngestDocument("_index", "_id", null, null, null, MapBuilder.<String, Object>newMapBuilder().put("message", "foo,bar,baz").put("a", "willgetstompped").map());    assertThat(ingestDocument.getFieldValue("a", String.class), equalTo("willgetstompped"));    DissectProcessor dissectProcessor = new DissectProcessor("", "message", "%{a},%{b},%{c}", "", true);    dissectProcessor.execute(ingestDocument);    assertThat(ingestDocument.getFieldValue("a", String.class), equalTo("foo"));    assertThat(ingestDocument.getFieldValue("b", String.class), equalTo("bar"));    assertThat(ingestDocument.getFieldValue("c", String.class), equalTo("baz"));}
0
private void verifyString(int resultIndex, String expected, BytesColumnVector resultV)
{    String result = getStringFromBytesColumnVector(resultV, resultIndex);    Assert.assertEquals(expected, result);}
0
public void testEmptyRoll() throws Exception
{    for (int i = 0; i < 10; ++i) {        procStore.periodicRollForTesting();    }    assertEquals(1, procStore.getActiveLogs().size());    FileStatus[] status = fs.listStatus(logDir);    assertEquals(1, status.length);}
0
public void merge(StackRoleCommandOrder parent)
{    merge(parent, false);}
0
public Instance get(int index)
{    return instances.get(index);}
0
 void assertSecureFile(KeyStoreWrapper keystore, String setting, Path file) throws Exception
{    byte[] expectedBytes = Files.readAllBytes(file);    try (InputStream input = keystore.getFile(setting)) {        for (int i = 0; i < expectedBytes.length; ++i) {            int got = input.read();            int expected = Byte.toUnsignedInt(expectedBytes[i]);            if (got < 0) {                fail("Got EOF from keystore stream at position " + i + " but expected 0x" + Integer.toHexString(expected));            }            assertEquals("Byte " + i, expected, got);        }        int eof = input.read();        if (eof != -1) {            fail("Found extra bytes in file stream from keystore, expected " + expectedBytes.length + " bytes but found 0x" + Integer.toHexString(eof));        }    }}
0
public void testNewStringFuncs() throws Throwable
{    final Object[] expected = new Object[] { 97, 65, -32, "A", "btrim", "Peace Peace Peace ", "    ", "katcit", "\u00C3\u00A2pple", "" };    runTest(expected, "functions/string/testStringFuncs.json");}
0
private static HStore getStore(final Configuration conf, final FileSystem fs, final Path tableDir, final TableDescriptor htd, final RegionInfo hri, final String familyName, final Path tempDir) throws IOException
{    HRegionFileSystem regionFs = new HRegionFileSystem(conf, fs, tableDir, hri) {        @Override        public Path getTempDir() {            return tempDir;        }    };    HRegion region = new HRegion(regionFs, null, conf, htd, null);    return new HStore(region, htd.getColumnFamily(Bytes.toBytes(familyName)), conf, false);}
0
public boolean getSecondaryIp()
{    return secondaryIp;}
0
public static SRR_S07 toSrrS07(byte[] body, Exchange exchange) throws HL7Exception, IOException
{    return toMessage(SRR_S07.class, body, exchange);}
0
 Optional<TableFactory> getTableFactory()
{    return Optional.empty();}
0
public CoordinatorStatusResponse status() throws Exception
{    HttpResponse<CoordinatorStatusResponse> resp = JsonRestServer.<CoordinatorStatusResponse>httpRequest(url("/coordinator/status"), "GET", null, new TypeReference<CoordinatorStatusResponse>() {    }, maxTries);    return resp.body();}
0
 void setCriticalThreshold(final float criticalThreshold)
{    synchronized (this) {                if (criticalThreshold == this.thresholds.getCriticalThreshold()) {            return;        }                if (criticalThreshold > 100.0f || criticalThreshold < 0.0f) {            throw new IllegalArgumentException("Critical percentage must be greater than 0.0 and less than or equal to 100.0.");        }        if (getTenuredMemoryPoolMXBean() == null) {            throw new IllegalStateException(String.format("No tenured pools found.  Known pools are: %s", getAllMemoryPoolNames()));        }        if (criticalThreshold != 0 && this.thresholds.isEvictionThresholdEnabled() && criticalThreshold <= this.thresholds.getEvictionThreshold()) {            throw new IllegalArgumentException("Critical percentage must be greater than the eviction percentage.");        }        this.cache.setQueryMonitorRequiredForResourceManager(criticalThreshold != 0);        this.thresholds = new MemoryThresholds(this.thresholds.getMaxMemoryBytes(), criticalThreshold, this.thresholds.getEvictionThreshold());        updateStateAndSendEvent();                if (this.thresholds.isEvictionThresholdEnabled() || this.thresholds.isCriticalThresholdEnabled()) {            startMonitoring();        } else if (!this.thresholds.isEvictionThresholdEnabled() && !this.thresholds.isCriticalThresholdEnabled()) {            stopMonitoring();        }        this.stats.changeCriticalThreshold(this.thresholds.getCriticalThresholdBytes());    }}
0
protected void mergeContributor_Organization(Contributor target, Contributor source, boolean sourceDominant, Map<Object, Object> context)
{    String src = source.getOrganization();    if (src != null) {        if (sourceDominant || target.getOrganization() == null) {            target.setOrganization(src);            target.setLocation("organization", source.getLocation("organization"));        }    }}
0
private boolean returnStaticContentOnPartitionWithNoRows()
{        return queriesFullPartitions() || table.isStaticCompactTable();}
0
private static int removeChunkLength(Map<String, String> options)
{    if (options.containsKey(CHUNK_LENGTH_IN_KB)) {        if (options.containsKey(CHUNK_LENGTH_KB)) {            throw new ConfigurationException(format("The '%s' option must not be used if the chunk length is already specified by the '%s' option", CHUNK_LENGTH_KB, CHUNK_LENGTH_IN_KB));        }        return parseChunkLength(options.remove(CHUNK_LENGTH_IN_KB));    }    if (options.containsKey(CHUNK_LENGTH_KB)) {        if (!hasLoggedChunkLengthWarning) {            hasLoggedChunkLengthWarning = true;                    }        return parseChunkLength(options.remove(CHUNK_LENGTH_KB));    }    return DEFAULT_CHUNK_LENGTH;}
1
public void testWrappers()
{        Schema wrappers = ReflectData.AllowNull.get().getSchema(Wrappers.class);    Assert.assertEquals(nullableSchema(boolean.class), wrappers.getField("aBoolean").schema());    Assert.assertEquals(nullableSchema(byte.class), wrappers.getField("aByte").schema());    Assert.assertEquals(nullableSchema(short.class), wrappers.getField("aShort").schema());    Assert.assertEquals(nullableSchema(int.class), wrappers.getField("anInt").schema());    Assert.assertEquals(nullableSchema(long.class), wrappers.getField("aLong").schema());    Assert.assertEquals(nullableSchema(float.class), wrappers.getField("aFloat").schema());    Assert.assertEquals(nullableSchema(double.class), wrappers.getField("aDouble").schema());    Assert.assertEquals(nullableSchema(Primitives.class), wrappers.getField("anObject").schema());}
0
public void onBeforeWriteResponseBody(HttpServletRequest request, XHttpServletResponse response)
{    if (!isEligibleToExpirationHeaderGeneration(request, response)) {        return;    }    Date expirationDate = getExpirationDate(response);    if (expirationDate == null) {        if (log.isDebugEnabled()) {                    }    } else {        if (log.isDebugEnabled()) {                    }        String maxAgeDirective = "max-age=" + ((expirationDate.getTime() - System.currentTimeMillis()) / 1000);        String cacheControlHeader = response.getCacheControlHeader();        String newCacheControlHeader = (cacheControlHeader == null) ? maxAgeDirective : cacheControlHeader + ", " + maxAgeDirective;        response.setHeader(HEADER_CACHE_CONTROL, newCacheControlHeader);        response.setDateHeader(HEADER_EXPIRES, expirationDate.getTime());    }}
1
public void addCommitSuccess(long latency)
{    this.commitSuccess.add(latency);}
0
public org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntry.Builder getNoMapBuilder()
{    bitField0_ |= 0x00000002;    onChanged();    return getNoMapFieldBuilder().getBuilder();}
0
 String getTargetField()
{    return targetField;}
0
private void createDefaultScriptFile(PigScript object)
{    String userScriptsPath = context.getProperties().get("scripts.dir");    if (userScriptsPath == null) {        String msg = "scripts.dir is not configured!";                throw new MisconfigurationFormattedException("scripts.dir");    }    int checkId = 0;    boolean fileCreated;    String newFilePath;    do {        String normalizedName = object.getTitle().replaceAll("[^a-zA-Z0-9 ]+", "").replaceAll(" ", "_").toLowerCase();        String timestamp = new SimpleDateFormat("yyyy-MM-dd_hh-mm").format(new Date());        newFilePath = String.format(userScriptsPath + "/%s-%s%s.pig", normalizedName, timestamp, (checkId == 0) ? "" : "_" + checkId);                try {            FSDataOutputStream stream = UserLocalObjects.getHdfsApi(context).create(newFilePath, false);            stream.close();            fileCreated = true;                    } catch (FileAlreadyExistsException e) {            fileCreated = false;                    } catch (IOException e) {            try {                delete(object.getId());            } catch (ItemNotFound itemNotFound) {                throw new ServiceFormattedException("Error in creation, during clean up: " + itemNotFound.toString(), itemNotFound);            }            throw new ServiceFormattedException("Error in creation: " + e.toString(), e);        } catch (InterruptedException e) {            try {                delete(object.getId());            } catch (ItemNotFound itemNotFound) {                throw new ServiceFormattedException("Error in creation, during clean up: " + itemNotFound.toString(), itemNotFound);            }            throw new ServiceFormattedException("Error in creation: " + e.toString(), e);        }        checkId += 1;    } while (!fileCreated);    object.setPigScript(newFilePath);    getPigStorage().store(object);}
1
public void setDataCenterId(long dcId)
{    dataCenterId = dcId;}
0
public Long getValue(byte[] colFamily, byte[] colQualifier, Cell kv) throws IOException
{    if (kv == null || kv.getValueLength() != Bytes.SIZEOF_LONG)        return null;    return PrivateCellUtil.getValueAsLong(kv);}
0
public void user1_forbidden_u0_vw_voter_all_755() throws Exception
{    updateClient(org1Users[1]);    queryHiveViewFailed(db_u0_only, u0_vw_voter_all_755);}
0
 FhirEndpointConsumerBuilder useFixedDelay(String useFixedDelay)
{    doSetProperty("useFixedDelay", useFixedDelay);    return this;}
0
private boolean mightContainHash(long hash)
{    if (isSetMode) {        return hashes.contains(hash);    }        int bucket = CuckooFilter.hashToIndex((int) hash, numBuckets);    int fingerprint = CuckooFilter.fingerprint((int) (hash >> 32), bitsPerEntry, fingerprintMask);    int alternateIndex = CuckooFilter.alternateIndex(bucket, fingerprint, numBuckets);    for (CuckooFilter filter : filters) {        if (filter.mightContainFingerprint(bucket, fingerprint, alternateIndex)) {            return true;        }    }    return false;}
0
private boolean r_case()
{            ket = cursor;        if (find_among_b(a_4, 44) == 0) {        return false;    }        bra = cursor;        if (!r_R1()) {        return false;    }        slice_del();        if (!r_v_ending()) {        return false;    }    return true;}
0
public StandardProvenanceEventRecord enrich(final ProvenanceEventRecord rawEvent, final FlowFile flowFile, final long commitNanos)
{    verifyTaskActive();    final StandardRepositoryRecord repoRecord = getRecord(flowFile);    if (repoRecord == null) {        throw new FlowFileHandlingException(flowFile + " is not known in this session (" + toString() + ")");    }    final StandardProvenanceEventRecord.Builder recordBuilder = new StandardProvenanceEventRecord.Builder().fromEvent(rawEvent);    if (repoRecord.getCurrent() != null && repoRecord.getCurrentClaim() != null) {        final ContentClaim currentClaim = repoRecord.getCurrentClaim();        final long currentOffset = repoRecord.getCurrentClaimOffset();        final long size = flowFile.getSize();        final ResourceClaim resourceClaim = currentClaim.getResourceClaim();        recordBuilder.setCurrentContentClaim(resourceClaim.getContainer(), resourceClaim.getSection(), resourceClaim.getId(), currentOffset + currentClaim.getOffset(), size);    }    if (repoRecord.getOriginal() != null && repoRecord.getOriginalClaim() != null) {        final ContentClaim originalClaim = repoRecord.getOriginalClaim();        final long originalOffset = repoRecord.getOriginal().getContentClaimOffset();        final long originalSize = repoRecord.getOriginal().getSize();        final ResourceClaim resourceClaim = originalClaim.getResourceClaim();        recordBuilder.setPreviousContentClaim(resourceClaim.getContainer(), resourceClaim.getSection(), resourceClaim.getId(), originalOffset + originalClaim.getOffset(), originalSize);    }    final FlowFileQueue originalQueue = repoRecord.getOriginalQueue();    if (originalQueue != null) {        recordBuilder.setSourceQueueIdentifier(originalQueue.getIdentifier());    }    recordBuilder.setAttributes(repoRecord.getOriginalAttributes(), repoRecord.getUpdatedAttributes());    if (rawEvent.getEventDuration() < 0) {        recordBuilder.setEventDuration(TimeUnit.NANOSECONDS.toMillis(commitNanos - repoRecord.getStartNanos()));    }    return recordBuilder.build();}
0
public boolean containsKey(Object key)
{    return NamedList.this.get((String) key) != null;}
0
public void setSourceFile(String sourceFile)
{    this.sourceFile = sourceFile;}
0
public LocalFsBlobStoreFile write(String key, boolean create) throws IOException
{    return new LocalFsBlobStoreFile(getKeyDir(key), true, create);}
0
private void assertAccessTokenWorks(String token) throws IOException
{    for (RestClient client : twoClients) {        Request request = new Request("GET", "/_security/_authenticate");        RequestOptions.Builder options = request.getOptions().toBuilder();        options.addHeader(HttpHeaders.AUTHORIZATION, "Bearer " + token);        request.setOptions(options);        Response authenticateResponse = client.performRequest(request);        assertOK(authenticateResponse);        assertEquals("test_user", entityAsMap(authenticateResponse).get("username"));    }}
0
public FieldAnalysisRequest setFieldNames(List<String> fieldNames)
{    this.fieldNames = fieldNames;    return this;}
0
public Collection<Object> createComponents(Client client, ClusterService clusterService, ThreadPool threadPool, ResourceWatcherService resourceWatcherService, ScriptService scriptService, NamedXContentRegistry xContentRegistry, Environment environment, NodeEnvironment nodeEnvironment, NamedWriteableRegistry namedWriteableRegistry)
{    return Collections.singletonList(listener);}
0
private S3AFileSystem getFileSystem()
{    return this.fileSystem;}
0
protected void reset() throws IOException
{    subIterators.get(0).nextInterval();    i = 1;    start = end = firstEnd = -1;}
0
public void testValidOperations() throws Exception
{    TestOutputReceiver receiver = new TestOutputReceiver(counterSet, NameContextsForTests.nameContextForTest());    List<Operation> operations = Arrays.<Operation>asList(new TestReadOperation(receiver, createContext("ReadOperation")));    ExecutionStateTracker stateTracker = ExecutionStateTracker.newForTest();    try (IntrinsicMapTaskExecutor executor = IntrinsicMapTaskExecutor.withSharedCounterSet(operations, counterSet, stateTracker)) {        Assert.assertEquals(operations.get(0), executor.getReadOperation());    }}
0
public void canDeserializeSingleItemUnions() throws SerDeException, IOException
{    Schema s = AvroSerdeUtils.getSchemaFor(TestAvroObjectInspectorGenerator.SINGLE_ITEM_UNION_SCHEMA);    GenericData.Record record = new GenericData.Record(s);    record.put("aUnion", "this is a string");    ResultPair result = unionTester(s, record);    assertTrue(result.value instanceof String);    assertEquals("this is a string", result.value);    UnionObjectInspector uoi = (UnionObjectInspector) result.oi;    assertEquals(0, uoi.getTag(result.unionObject));}
0
public BytesRef getTokenBytesWithLeaf(BytesRef result)
{    result = getTokenBytesNoLeaf(result);        if (isLeaf() && !(getLevel() == tree.getMaxLevels())) {                result.bytes[result.offset + result.length] = LEAF;        result.length++;    }    return result;}
0
public String getSingularName()
{    return "component";}
0
public void tearDown() throws Exception
{    for (TableDescriptor htd : ADMIN.listTableDescriptors()) {        TEST_UTIL.deleteTable(htd.getTableName());    }}
0
public void testMultipleInvocationsGetPlan()
{    try {        ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();                DataSet<String> data = env.fromElements("Some", "test", "data").name("source1");                data.output(new DiscardingOutputFormat<String>()).name("print1");        data.output(new DiscardingOutputFormat<String>()).name("output1");        {            Plan p = env.createProgramPlan();            assertEquals(2, p.getDataSinks().size());            for (GenericDataSinkBase<?> sink : p.getDataSinks()) {                assertTrue(sink.getName().equals("print1") || sink.getName().equals("output1"));                assertEquals("source1", sink.getInput().getName());            }        }                data.writeAsText("/some/file/path").name("textsink");        {            Plan p = env.createProgramPlan();            assertEquals(1, p.getDataSinks().size());            GenericDataSinkBase<?> sink = p.getDataSinks().iterator().next();            assertEquals("textsink", sink.getName());            assertEquals("source1", sink.getInput().getName());        }    } catch (Exception e) {        System.err.println(e.getMessage());        e.printStackTrace();        fail(e.getMessage());    }}
0
public void testRecoveryAfterCorruptionMiddle() throws Exception
{    startBroker();    produceMessagesToConsumeMultipleDataFiles(50);    int numFiles = getNumberOfJournalFiles();    assertTrue("more than x files: " + numFiles, numFiles > 4);    corruptBatchMiddle(3);    restartBroker();    assertEquals("missing one message", 49, broker.getAdminView().getTotalMessageCount());    assertEquals("Drain", 49, drainQueue(49));}
0
public DiskId getDiskId()
{    return this.id;}
0
public static EnumerableNestedLoopJoin create(RelNode left, RelNode right, RexNode condition, Set<CorrelationId> variablesSet, JoinRelType joinType)
{    final RelOptCluster cluster = left.getCluster();    final RelMetadataQuery mq = cluster.getMetadataQuery();    final RelTraitSet traitSet = cluster.traitSetOf(EnumerableConvention.INSTANCE).replaceIfs(RelCollationTraitDef.INSTANCE, () -> RelMdCollation.enumerableNestedLoopJoin(mq, left, right, joinType));    return new EnumerableNestedLoopJoin(cluster, traitSet, left, right, condition, variablesSet, joinType);}
0
 Class<? extends HashFormat> getHashFormatClass()
{    return this.clazz;}
0
private DestinationViewMBean getProxyToDestination(ActiveMQDestination destination) throws MalformedObjectNameException, JMSException
{    final ObjectName objectName = new ObjectName("org.apache.activemq:type=Broker,brokerName=" + broker.getBrokerName() + ",destinationType=" + JMXSupport.encodeObjectNamePart(destination.getDestinationTypeAsString()) + ",destinationName=" + JMXSupport.encodeObjectNamePart(destination.getPhysicalName()));    DestinationViewMBean proxy = (DestinationViewMBean) broker.getManagementContext().newProxyInstance(objectName, DestinationViewMBean.class, true);    return proxy;}
0
public void setLimitIsSet(boolean value)
{    __isset_bitfield = EncodingUtils.setBit(__isset_bitfield, __LIMIT_ISSET_ID, value);}
0
public void testGetIndexedScriptRequestSerialization() throws IOException
{    GetStoredScriptRequest request = new GetStoredScriptRequest("id");    BytesStreamOutput out = new BytesStreamOutput();    out.setVersion(randomVersion(random()));    request.writeTo(out);    StreamInput in = out.bytes().streamInput();    in.setVersion(out.getVersion());    GetStoredScriptRequest request2 = new GetStoredScriptRequest(in);    assertThat(request2.id(), equalTo(request.id()));}
0
public void testListToCommaDelimitedString()
{    List<String> elements = Arrays.asList("element1", "element2", "element3");    String actual = RemoteIpValve.listToCommaDelimitedString(elements);    Assert.assertEquals("element1, element2, element3", actual);}
0
public void testSelectUnsupportedOutputFormat() throws Throwable
{    describe("Request a (currently) unsupported output format");    FutureDataInputStreamBuilder builder = getFileSystem().openFile(csvPath).must(SELECT_SQL, SELECT_ODD_ENTRIES).must(SELECT_INPUT_FORMAT, "csv").must(SELECT_OUTPUT_FORMAT, "json");    interceptFuture(IllegalArgumentException.class, "json", builder.build());}
0
public void disconnectFromNode(DiscoveryNode node)
{    if (isLocalNode(node)) {        return;    }    connectionManager.disconnectFromNode(node);}
0
public String javaType(Schema schema)
{    return javaType(schema, true);}
0
public static Set<String> initSrcTables()
{    if (srcTables == null) {        initSrcTablesFromSystemProperty();        storeSrcTables();    }    return srcTables;}
0
private Log getLog()
{    if (log == null) {        synchronized (this) {            if (log == null) {                log = LogFactory.getLog(LazyReplicatedMap.class);            }        }    }    return log;}
0
public void visitLiteralNew(GroovySourceAST t, int visit)
{    visitDefault(t, visit);}
0
public List<String> getSuccess()
{    return this.success;}
0
public VirtualMachineSnapshotInfo getSnapshotInfo() throws Exception
{    return (VirtualMachineSnapshotInfo) _context.getVimClient().getDynamicProperty(_mor, "snapshot");}
0
public void testDropsStandardSuffixes()
{    assertEquals("Embedded", NameUtils.approximateSimpleName("EmbeddedDoFn", true));    assertEquals("Embedded", NameUtils.approximateSimpleName("EmbeddedFn", true));    assertEquals("Embedded", NameUtils.approximateSimpleName("EmbeddedDoFn", false));    assertEquals("Embedded", NameUtils.approximateSimpleName("EmbeddedFn", false));}
0
public void createTopics() throws Exception
{    inputStream = "input-stream-" + testCounter;    CLUSTER.createTopic(inputStream);}
0
public static RTB_Z74 toRtbZ74(String body) throws HL7Exception
{    return toMessage(RTB_Z74.class, body);}
0
public void testFailedMoveTablesAndRepair() throws Exception
{                        final RSGroupInfo newGroup = addGroup(getGroupName(name.getMethodName()), 1);    Iterator iterator = newGroup.getServers().iterator();    Address newGroupServer1 = (Address) iterator.next();            Pair<ServerName, RegionStateNode> gotPair = createTableWithRegionSplitting(newGroup, new Random().nextInt(8) + 4);    RegionStateNode rsn = gotPair.getSecond();        try {        rsGroupAdmin.moveTables(Sets.newHashSet(tableName), newGroup.getName());        fail("should get IOException when retry exhausted but there still exists failed moved " + "regions");    } catch (Exception e) {        assertTrue(e.getMessage().contains(gotPair.getSecond().getRegionInfo().getRegionNameAsString()));    }    for (RegionInfo regionInfo : master.getAssignmentManager().getAssignedRegions()) {        if (regionInfo.getTable().equals(tableName) && regionInfo.equals(rsn.getRegionInfo())) {            assertNotEquals(master.getAssignmentManager().getRegionStates().getRegionServerOfRegion(regionInfo).getAddress(), newGroupServer1);        }    }        rsn.setState(RegionState.State.OPEN);    rsGroupAdmin.moveTables(Sets.newHashSet(tableName), newGroup.getName());    for (RegionInfo regionInfo : master.getAssignmentManager().getAssignedRegions()) {        if (regionInfo.getTable().equals(tableName)) {            assertEquals(master.getAssignmentManager().getRegionStates().getRegionServerOfRegion(regionInfo).getAddress(), newGroupServer1);        }    }}
0
public void create_returnsConstructedCache_ifNoSystemExists()
{    InternalCache constructedCache = constructedCache();    InternalCacheBuilder internalCacheBuilder = new InternalCacheBuilder(new Properties(), new CacheConfig(), compositeMeterRegistryFactory, metricsSessionInitializer, nullSingletonSystemSupplier, constructorOf(constructedSystem()), nullSingletonCacheSupplier, constructorOf(constructedCache));    InternalCache result = internalCacheBuilder.create();    assertThat(result).isSameAs(constructedCache);}
0
public boolean add(T o)
{    throw new UnsupportedOperationException("cannot add to Empty Ranges");}
0
public void call(ClusterState clusterState, ZkNodeProps message, NamedList results) throws Exception
{    String extCollectionName = message.getStr(ZkStateReader.COLLECTION_PROP);    String sliceId = message.getStr(ZkStateReader.SHARD_ID_PROP);    boolean followAliases = message.getBool(FOLLOW_ALIASES, false);    String collectionName;    if (followAliases) {        collectionName = ocmh.cloudManager.getClusterStateProvider().resolveSimpleAlias(extCollectionName);    } else {        collectionName = extCollectionName;    }        Slice slice = clusterState.getCollection(collectionName).getSlice(sliceId);    if (slice == null)        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, "No shard with name " + sliceId + " exists for collection " + collectionName);            final Slice.State state = slice.getState();    if (!(slice.getRange() == null || state == Slice.State.INACTIVE || state == Slice.State.RECOVERY || state == Slice.State.CONSTRUCTION) || state == Slice.State.RECOVERY_FAILED) {        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, "The slice: " + slice.getName() + " is currently " + state + ". Only non-active (or custom-hashed) slices can be deleted.");    }    if (state == Slice.State.RECOVERY) {                        Map<String, Object> propMap = new HashMap<>();        propMap.put(Overseer.QUEUE_OPERATION, OverseerAction.UPDATESHARDSTATE.toLower());        propMap.put(sliceId, Slice.State.CONSTRUCTION.toString());        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);        ZkNodeProps m = new ZkNodeProps(propMap);        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));    }    String asyncId = message.getStr(ASYNC);    try {        List<ZkNodeProps> replicas = getReplicasForSlice(collectionName, slice);        CountDownLatch cleanupLatch = new CountDownLatch(replicas.size());        for (ZkNodeProps r : replicas) {            final ZkNodeProps replica = r.plus(message.getProperties()).plus("parallel", "true").plus(ASYNC, asyncId);                        NamedList deleteResult = new NamedList();            try {                ((DeleteReplicaCmd) ocmh.commandMap.get(DELETEREPLICA)).deleteReplica(clusterState, replica, deleteResult, () -> {                    cleanupLatch.countDown();                    if (deleteResult.get("failure") != null) {                        synchronized (results) {                            results.add("failure", String.format(Locale.ROOT, "Failed to delete replica for collection=%s shard=%s" + " on node=%s", replica.getStr(COLLECTION_PROP), replica.getStr(SHARD_ID_PROP), replica.getStr(NODE_NAME_PROP)));                        }                    }                    SimpleOrderedMap success = (SimpleOrderedMap) deleteResult.get("success");                    if (success != null) {                        synchronized (results) {                            results.add("success", success);                        }                    }                });            } catch (KeeperException e) {                                cleanupLatch.countDown();            } catch (Exception e) {                                cleanupLatch.countDown();                throw e;            }        }                cleanupLatch.await(1, TimeUnit.MINUTES);        ZkNodeProps m = new ZkNodeProps(Overseer.QUEUE_OPERATION, DELETESHARD.toLower(), ZkStateReader.COLLECTION_PROP, collectionName, ZkStateReader.SHARD_ID_PROP, sliceId);        ZkStateReader zkStateReader = ocmh.zkStateReader;        ocmh.overseer.offerStateUpdate(Utils.toJSON(m));        zkStateReader.waitForState(collectionName, 45, TimeUnit.SECONDS, (c) -> c.getSlice(sliceId) == null);            } catch (SolrException e) {        throw e;    } catch (Exception e) {        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, "Error executing delete operation for collection: " + collectionName + " shard: " + sliceId, e);    }}
1
public void valuesRepeatableReadDoesNotIncludeTombstones()
{    server1.invoke(() -> createServerRegion(1, false));    server2.invoke(() -> createServerRegion(1, true));    server2.invoke(() -> doDestroyOps());    server2.invoke(() -> doValuesTransactionWithTombstone());}
0
public synchronized boolean addServerToDrainList(final ServerName sn)
{    if (!this.isServerOnline(sn)) {                return false;    }        if (this.drainingServers.contains(sn)) {                return true;    }        return this.drainingServers.add(sn);}
1
public void testAnotherCamelProxy() throws Exception
{        AbstractApplicationContext ac = new ClassPathXmlApplicationContext("org/apache/camel/spring/config/AnotherCamelProxyTest.xml");    MyProxySender sender = ac.getBean("myProxySender", MyProxySender.class);    String reply = sender.hello("Camel");    assertEquals("Bye Camel", reply);        IOHelper.close(ac);}
0
private Object getResponseBody(HttpServerExchange httpExchange, Exchange camelExchange) throws IOException
{    Object result;    if (camelExchange.hasOut()) {        result = getEndpoint().getUndertowHttpBinding().toHttpResponse(httpExchange, camelExchange.getOut());    } else {        result = getEndpoint().getUndertowHttpBinding().toHttpResponse(httpExchange, camelExchange.getIn());    }    return result;}
0
public void testBindingMode() throws Exception
{    MockEndpoint mock = getMockEndpoint("mock:input");    mock.expectedMessageCount(1);    mock.message(0).body().isInstanceOf(UserJaxbPojo.class);    String body = "{\"id\": 123, \"name\": \"Donald Duck\"}";    template.sendBody("http://localhost:" + portNum + "/users/new", body);    assertMockEndpointsSatisfied();    UserJaxbPojo user = mock.getReceivedExchanges().get(0).getIn().getBody(UserJaxbPojo.class);    assertNotNull(user);    assertEquals(123, user.getId());    assertEquals("Donald Duck", user.getName());}
0
public void info(CharSequence content)
{    print("info", content);}
0
public Map<String, String> getConfiguration()
{    return delegatee.getValues().entrySet().stream().collect(Collectors.toMap(e -> Bytes.toString(e.getKey().get(), e.getKey().getOffset(), e.getKey().getLength()), e -> Bytes.toString(e.getValue().get(), e.getValue().getOffset(), e.getValue().getLength())));}
0
public void testValueIn() throws Exception
{    assertMatches(header("name").in("Hiram", "Jonathan", "James", "Claus"));}
0
public void testFlush() throws Exception
{    SubsetConfiguration configuration = createNiceMock(SubsetConfiguration.class);    Connection connection = createNiceMock(Connection.class);        expect(configuration.getParent()).andReturn(null);    expect(configuration.getPrefix()).andReturn("prefix");    expect(configuration.getString("databaseUrl")).andReturn("url");    connection.close();        replay(configuration, connection);    SqlServerSink sink = createInstance();    sink.init(configuration);    SQLServerDriver.setConnection(connection);    sink.ensureConnection();    sink.flush();    verify(configuration, connection);}
0
public void run()
{    try {        flush();    } catch (Exception e) {            }}
1
public short addEntry(byte[] data, int offset, int length)
{    return addEntryInternal(data, offset, length, true);}
0
public int getAmt()
{    return amt;}
0
public void increfSolrCoreState()
{    synchronized (this) {        if (solrCoreStateRefCnt == 0) {            throw new CoreIsClosedException("IndexWriter has been closed");        }        solrCoreStateRefCnt++;    }}
0
public TH<T> a(String href, String anchorText)
{    return a().$href(href)._(anchorText)._();}
0
public void parseMemberNameWithNullStringArrayResultsInNull()
{    Builder builder = new Builder();    builder.parseMemberName((String[]) null);    assertThat(builder.getMemberName()).isNull();}
0
public Builder clearField(com.google.protobuf.Descriptors.FieldDescriptor field)
{    return (Builder) super.clearField(field);}
0
public String getTemplateName()
{    return _userSpecifiedName;}
0
public void statusCommandWithIncorrectPortShouldFail()
{    int incorrectPort = AvailablePort.getRandomAvailablePort(AvailablePort.SOCKET);    String cmd = "status locator --port=" + incorrectPort;    GfshScript.of(cmd).withName("test-frame").awaitAtMost(1, MINUTES).expectExitCode(ExitCode.FATAL.getValue()).execute(gfsh);}
0
private List<String> getInterceptorClassNames(Configuration conf)
{    String configuredInterceptorClassNames = conf.get(YarnConfiguration.AMRM_PROXY_INTERCEPTOR_CLASS_PIPELINE, YarnConfiguration.DEFAULT_AMRM_PROXY_INTERCEPTOR_CLASS_PIPELINE);    List<String> interceptorClassNames = new ArrayList<String>();    Collection<String> tempList = StringUtils.getStringCollection(configuredInterceptorClassNames);    for (String item : tempList) {        interceptorClassNames.add(item.trim());    }        if (this.nmContext.isDistributedSchedulingEnabled()) {        interceptorClassNames.add(0, DistributedScheduler.class.getName());    }    return interceptorClassNames;}
0
public final com.google.protobuf.Message getRequestPrototype(com.google.protobuf.Descriptors.MethodDescriptor method)
{    if (method.getService() != getDescriptor()) {        throw new java.lang.IllegalArgumentException("Service.getRequestPrototype() given method " + "descriptor for wrong service type.");    }    switch(method.getIndex()) {        case 0:            return org.apache.kylin.storage.hbase.cube.v2.coprocessor.endpoint.generated.CubeVisitProtos.CubeVisitRequest.getDefaultInstance();        default:            throw new java.lang.AssertionError("Can't get here.");    }}
0
public State getObjectInStoreState()
{    return this.state;}
0
public int getChunkNumber()
{    return chunkNumber;}
0
protected AbstractSerDe getSerDe(String fieldNames, String fieldTypes) throws Throwable
{    Properties schema = new Properties();    schema.setProperty(serdeConstants.LIST_COLUMNS, fieldNames);    schema.setProperty(serdeConstants.LIST_COLUMN_TYPES, fieldTypes);    LazyBinarySerDe serde = new LazyBinarySerDe();    SerDeUtils.initializeSerDe(serde, new Configuration(), schema, null);    return serde;}
0
public void reset()
{    sourceEnumerator.reset();    resultEnumerator = Linq4j.emptyEnumerator();}
0
protected RouteBuilder createRouteBuilder() throws Exception
{    return new RouteBuilder() {        @Override        public void configure() throws Exception {            errorHandler(defaultErrorHandler().maximumRedeliveries(2).redeliveryDelay(0));            from("direct:start").loadBalance().failover().to("direct:a", "direct:b");            from("direct:a").errorHandler(noErrorHandler()).to("mock:a").throwException(new IllegalArgumentException("I cannot do this"));            from("direct:b").errorHandler(noErrorHandler()).to("mock:b").process(new Processor() {                public void process(Exchange exchange) throws Exception {                                        if (counter++ < 1) {                        throw new IllegalArgumentException("I can still not do this");                    }                    exchange.getIn().setBody("Bye World");                }            }).to("mock:result");        }    };}
0
private static HiveConf createHiveConf() throws MalformedURLException
{    String confDir = "../../data/conf/spark/standalone/hive-site.xml";    HiveConf.setHiveSiteLocation(new File(confDir).toURI().toURL());    HiveConf conf = new HiveConf();                    conf.set("dfs.client.datanode-restart.timeout", "30");    conf.set("spark.local.dir", Paths.get(System.getProperty("test.tmp.dir"), "TestJdbcWithLocalClusterSpark-local-dir").toString());    return conf;}
0
public void onDevicesReleased(Set<Device> allocatedDevices)
{}
0
public void cleanup()
{    indexService = null;    shardContext = null;    expectedQuery = null;    expectedPhraseQuery = null;}
0
public void testForStringIPv4Input() throws UnknownHostException
{    String ipStr = "192.168.0.1";    InetAddress ipv4Addr = null;        ipv4Addr = InetAddress.getByName(ipStr);    assertEquals(ipv4Addr, InetAddresses.forString(ipStr));    assertTrue(InetAddresses.isInetAddress(ipStr));}
0
public long approximateNumEntries() throws RocksDBException
{    return db.getLongProperty(oldColumnFamily, "rocksdb.estimate-num-keys") + db.getLongProperty(newColumnFamily, "rocksdb.estimate-num-keys");}
0
 static ByteKey next(ByteKey key)
{    return ByteKey.copyFrom(Bytes.concat(key.getBytes(), ZERO_BYTE_ARRAY));}
0
public void testEnsureSettingsAreRegistered()
{    Settings settings = Settings.builder().put("path.home", createTempDir()).build();    Watcher watcher = new Watcher(settings);    for (Setting<?> setting : HtmlSanitizer.getSettings()) {        assertThat(watcher.getSettings(), hasItem(setting));    }}
0
public void test_SCOPE_TABLE_hasRightTypeCode() throws SQLException
{    assertThat(rowsMetadata.getColumnType(21), equalTo(Types.VARCHAR));}
0
public static CumulativeSumPipelineAggregationBuilder cumulativeSum(String name, String bucketsPath)
{    return new CumulativeSumPipelineAggregationBuilder(name, bucketsPath);}
0
public static TypeInformation<T> POJO(Class<T> pojoClass)
{    final TypeInformation<T> ti = TypeExtractor.createTypeInfo(pojoClass);    if (ti instanceof PojoTypeInfo) {        return ti;    }    throw new InvalidTypesException("POJO type expected but was: " + ti);}
0
public static void main(String[] args) throws Exception
{    IntBasedBitPackingGenerator.main(args);    ByteBasedBitPackingGenerator.main(args);}
0
public ACLPathAndBytesable<String> withProtectedEphemeralSequential()
{    return CreateBuilderImpl.this.withProtectedEphemeralSequential();}
0
public boolean supportsOutputConversion(Class<?> clazz)
{    if (isNullable()) {        return NULL_OUTPUT_CONVERSION.contains(clazz.getName());    }    return NOT_NULL_INPUT_OUTPUT_CONVERSION.contains(clazz.getName());}
0
public URI getUri()
{    return uri;}
0
public String toString()
{    return getClass().getSimpleName() + "(" + in.toString() + ")";}
0
public void testUnMarshallMessage() throws Exception
{    result.expectedMessageCount(1);    result.assertIsSatisfied();    Order order = result.getReceivedExchanges().get(0).getIn().getBody(Order.class);    Assert.assertTrue(order.getTrailer().toString().contains("10: 220"));    Assert.assertTrue(order.getHeader().toString().contains("FIX.4.1, 9: 20, 34: 1 , 35: 0, 49: INVMGR, 56: BRKR"));    Assert.assertTrue(order.toString().contains("BE.CHM.001, 11: CHM0001-01, 22: 4, 48: BE0001245678, 54: 1, 58: this is a camel - bindy test"));}
0
public int hashCode()
{    int hash = 0;    K[] keys = this.keys;    int max = keys.length;    for (int i = 0; i < max; i++) {        K key = keys[i];        if (key != null && key != REMOVED) {            hash = 31 * hash + key.hashCode();            hash = 31 * hash + values[i].hashCode();        }    }    return hash;}
0
 AdvancedKubernetesServicesEndpointConsumerBuilder exchangePattern(String exchangePattern)
{    doSetProperty("exchangePattern", exchangePattern);    return this;}
0
public boolean isRequired()
{    return required;}
0
public void beforeStop()
{    uninstallDailyMaintenanceService();}
0
private boolean hideSecrets()
{    return paramAsBoolean(HIDE_SECRETS_KEY, true);}
0
public void failure(long invocationTime, long now)
{    add(invocationTime, now);    failures++;}
0
public void setStore(PersistenceAdapter store)
{    this.store = store;    if (percentLimit > 0 && store != null) {                updateLimitBasedOnPercent();    } else {        onLimitChange();    }}
0
public void testParseMixedDimensionPolyWithHole() throws IOException, ParseException
{    List<Coordinate> shellCoordinates = new ArrayList<>();    shellCoordinates.add(new Coordinate(100, 0));    shellCoordinates.add(new Coordinate(101, 0));    shellCoordinates.add(new Coordinate(101, 1));    shellCoordinates.add(new Coordinate(100, 1));    shellCoordinates.add(new Coordinate(100, 0));        List<Coordinate> holeCoordinates = new ArrayList<>();    holeCoordinates.add(new Coordinate(100.2, 0.2, 15.0));    holeCoordinates.add(new Coordinate(100.8, 0.2));    holeCoordinates.add(new Coordinate(100.8, 0.8));    holeCoordinates.add(new Coordinate(100.2, 0.8, 10.0));    holeCoordinates.add(new Coordinate(100.2, 0.2));    PolygonBuilder builder = new PolygonBuilder(new CoordinatesBuilder().coordinates(shellCoordinates));    builder.hole(new LineStringBuilder(holeCoordinates));    XContentBuilder xContentBuilder = XContentFactory.jsonBuilder().value(builder.toWKT());    XContentParser parser = createParser(xContentBuilder);    parser.nextToken();    Settings indexSettings = Settings.builder().put(IndexMetaData.SETTING_VERSION_CREATED, Version.V_7_0_0).put(IndexMetaData.SETTING_NUMBER_OF_REPLICAS, 0).put(IndexMetaData.SETTING_NUMBER_OF_SHARDS, 1).put(IndexMetaData.SETTING_INDEX_UUID, UUIDs.randomBase64UUID()).build();    Mapper.BuilderContext mockBuilderContext = new Mapper.BuilderContext(indexSettings, new ContentPath());    final GeoShapeFieldMapper mapperBuilder = (GeoShapeFieldMapper) (new GeoShapeFieldMapper.Builder("test").ignoreZValue(false).build(mockBuilderContext));        ElasticsearchParseException e = expectThrows(ElasticsearchParseException.class, () -> ShapeParser.parse(parser, mapperBuilder));    assertThat(e, hasToString(containsString("but [ignore_z_value] parameter is [false]")));}
0
public void setAttribute(String name, Object o)
{    attributes.put(name, o);}
0
public void eval(MockDirectoryWrapper dir) throws IOException
{    if (random().nextInt(10) != 0) {        return;    }    if (didFail.get()) {                return;    }    StackTraceElement[] trace = Thread.currentThread().getStackTrace();    for (int i = 0; i < trace.length; i++) {        if ("merge".equals(trace[i].getMethodName())) {            if (VERBOSE) {                System.out.println("TEST: now fail; thread=" + Thread.currentThread().getName() + " exc:");                new Throwable().printStackTrace(System.out);            }            didFail.set(true);            throw new FakeIOException();        }    }}
0
 int upperSaved3(int from, int to, int val)
{    int f = to - 1, t = to;    while (f > from) {        if (compareSaved(val, f) >= 0) {            return upperSaved(f, t, val);        }        final int delta = t - f;        t = f;        f -= delta << 1;    }    return upperSaved(from, t, val);}
0
public ConcurrentOperation repeatUntilValue(T expectedValue)
{    if (this.expectedResultIsSet) {        throw new IllegalArgumentException("Specify only one expected outcome.");    }    this.eventualExpectedValueSet = true;    this.expectedValue = expectedValue;    this.expectedResultIsSet = true;    return this;}
0
public int next()
{    try {        final Text line = lineReader.createValue();        writer.allocate();        writer.reset();        int recordCount = 0;        while (recordCount < VECTOR_MEMORY_ALLOCATION && lineReader.next(lineNumber, line)) {            writer.setPosition(recordCount);            parser.parse(line.toString());            recordCount++;        }        writer.setValueCount(recordCount);        return recordCount;    } catch (DissectionFailure | InvalidDissectorException | MissingDissectorsException | IOException e) {        throw handleAndGenerate("Failure while parsing log record.", e);    }}
0
public void setDocument(byte[] document)
{    this.document = document;}
0
public void setZooKeeperUrl(String zooKeeperUrl)
{    this.zooKeeperUrl = zooKeeperUrl;}
0
public int match(int start, int end, DrillBuf drillBuf)
{        if (patternLength != (end - start)) {        return 0;    }        for (int index = 0; index < patternLength; index++) {        if (patternByteBuffer.get(index) != drillBuf.getByte(start + index)) {            return 0;        }    }    return 1;}
0
public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheResponseProto parseDelimitedFrom(java.io.InputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws java.io.IOException
{    return PARSER.parseDelimitedFrom(input, extensionRegistry);}
0
public Map<String, String> getProperties()
{    return Collections.unmodifiableMap(properties);}
0
public QueueDiagnostics getQueueDiagnostics()
{    final FlowFileQueueSize size = new FlowFileQueueSize(size().getObjectCount(), size().getByteCount(), 0, 0, 0, 0, 0);    return new StandardQueueDiagnostics(new StandardLocalQueuePartitionDiagnostics(size, false, false), Collections.emptyList());}
0
public String toString()
{    return "StopConsumersRequest{" + "cube='" + cube + '\'' + ", removeData=" + removeData + '}';}
0
private void commitPutsToOverflow_core(Transaction overflowPutTx) throws InterruptedException
{        for (int i = 0; i < 2; ++i) {        try {            synchronized (queueLock) {                overflowPutTx.commit();                drainOrder.putOverflow(putList.size());                channelCounter.setChannelSize(memQueue.size() + drainOrder.overflowCounter);                break;            }        } catch (ChannelFullException e) {                        if (i == 0) {                Thread.sleep(overflowTimeout * 1000);            } else {                throw e;            }        }    }}
0
public Object getFieldValue(_Fields field)
{    switch(field) {        case REQUEST:            return getRequest();    }    throw new IllegalStateException();}
0
private static double getNextSafeDouble(double x)
{        long bits = Double.doubleToLongBits(x);        bits += Integer.MAX_VALUE;        bits &= ~((long) Integer.MAX_VALUE);        double result = Double.longBitsToDouble(bits);    assert result > x;    return result;}
0
public void scan(final String tableName, final byte[] startRow, final byte[] endRow, final Collection<Column> columns, List<String> authorizations, final ResultHandler handler) throws IOException
{    try (final Table table = connection.getTable(TableName.valueOf(tableName));        final ResultScanner scanner = getResults(table, startRow, endRow, columns, authorizations)) {        for (final Result result : scanner) {            final byte[] rowKey = result.getRow();            final Cell[] cells = result.rawCells();            if (cells == null) {                continue;            }                        final ResultCell[] resultCells = new ResultCell[cells.length];            for (int i = 0; i < cells.length; i++) {                final Cell cell = cells[i];                final ResultCell resultCell = getResultCell(cell);                resultCells[i] = resultCell;            }                        handler.handle(rowKey, resultCells);        }    }}
0
private static boolean isIdentifierChar(int c)
{    return (c >= '0' && c <= '9') || (c >= 'a' && c <= 'z') || (c >= 'A' && c <= 'Z') || c == '-' || c == '+' || c == '.' || c == '_' || c == '&';}
0
public void testDisconnect() throws Exception
{    BaseWork parent = nodes.get(0);    BaseWork[] children = { nodes.get(1), nodes.get(2) };    TezEdgeProperty edgeProp = new TezEdgeProperty(EdgeType.SIMPLE_EDGE);    work.connect(parent, children[0], edgeProp);    work.connect(parent, children[1], edgeProp);    work.disconnect(parent, children[0]);    Assert.assertTrue(work.getChildren(parent).contains(children[1]));    Assert.assertTrue(!work.getChildren(parent).contains(children[0]));    Assert.assertTrue(work.getRoots().contains(parent) && work.getRoots().contains(children[0]) && !work.getRoots().contains(children[1]));    Assert.assertTrue(!work.getLeaves().contains(parent) && work.getLeaves().contains(children[0]) && work.getLeaves().contains(children[1]));}
0
public String getComments()
{    return comments;}
0
private void checkRootAccess()
{    if (isRoot()) {        throw new NoSuchElementException("The root has no parent");    }}
0
public void tightUnmarshal(OpenWireFormat wireFormat, Object o, DataInput dataIn, BooleanStream bs) throws IOException
{    super.tightUnmarshal(wireFormat, o, dataIn, bs);    ProducerInfo info = (ProducerInfo) o;    info.setProducerId((org.apache.activemq.command.ProducerId) tightUnmarsalCachedObject(wireFormat, dataIn, bs));    info.setDestination((org.apache.activemq.command.ActiveMQDestination) tightUnmarsalCachedObject(wireFormat, dataIn, bs));    if (bs.readBoolean()) {        short size = dataIn.readShort();        org.apache.activemq.command.BrokerId[] value = new org.apache.activemq.command.BrokerId[size];        for (int i = 0; i < size; i++) {            value[i] = (org.apache.activemq.command.BrokerId) tightUnmarsalNestedObject(wireFormat, dataIn, bs);        }        info.setBrokerPath(value);    } else {        info.setBrokerPath(null);    }    info.setDispatchAsync(bs.readBoolean());}
0
public void removeAllQueuesAndHFileRefs(String peerId) throws ReplicationException
{    removeAllQueues(peerId);    queueStorage.removePeerFromHFileRefs(peerId);}
0
public long getAddress()
{    return OFF_HEAP_ADDRESS_UPDATER.get(this);}
0
public void testAnyMatch01()
{    ELProcessor processor = new ELProcessor();    Optional result = (Optional) processor.getValue("[1,2,3,4,5].stream().anyMatch(x->x==7)", Object.class);    Assert.assertEquals(Boolean.FALSE, result.get());}
0
public get_fields_with_environment_context_result getResult(I iface, get_fields_with_environment_context_args args) throws org.apache.thrift.TException
{    get_fields_with_environment_context_result result = new get_fields_with_environment_context_result();    try {        result.success = iface.get_fields_with_environment_context(args.db_name, args.table_name, args.environment_context);    } catch (MetaException o1) {        result.o1 = o1;    } catch (UnknownTableException o2) {        result.o2 = o2;    } catch (UnknownDBException o3) {        result.o3 = o3;    }    return result;}
0
public static void showDetails(PrettyPrintWriter out, ParquetMetadata meta)
{    showDetails(out, meta.getFileMetaData());    long i = 1;    for (BlockMetaData bmeta : meta.getBlocks()) {        out.println();        showDetails(out, bmeta, i++);    }}
0
public boolean isIsolated()
{    return perUserIsolated() || perNoteIsolated();}
0
public void testIntRows() throws Exception
{    random = new Random(927337);        VectorMapJoinFastLongHashMap map = new VectorMapJoinFastLongHashMap(false, false, HashTableKeyType.INT, LARGE_CAPACITY, LOAD_FACTOR, LARGE_WB_SIZE, -1);    VerifyFastRowHashMap verifyTable = new VerifyFastRowHashMap();    VectorRandomRowSource valueSource = new VectorRandomRowSource();    valueSource.init(random, VectorRandomRowSource.SupportedTypes.ALL, 4, /* allowNulls */    false, /* isUnicodeOk */    false);    int rowCount = 1000;    Object[][] rows = valueSource.randomRows(rowCount);    addAndVerifyRows(valueSource, rows, map, HashTableKeyType.INT, verifyTable, new String[] { "int" }, /* doClipping */    false, /* useExactBytes */    false);}
0
public final void disable_tracing()
{}
0
public boolean validate(Token.TYPE previousTokenType)
{    return previousTokenType == Token.TYPE.VALUE_OPERAND || previousTokenType == Token.TYPE.BRACKET_CLOSE;}
0
 static com.google.api.services.dataflow.model.Source translateIOToCloudSource(BoundedSource<?> io, DataflowPipelineOptions options) throws Exception
{    options.setRunner(DataflowRunner.class);    options.setProject("test-project");    options.setTempLocation("gs://test-tmp");    options.setPathValidatorClass(NoopPathValidator.class);    options.setGcpCredential(new TestCredential());    DataflowPipelineTranslator translator = DataflowPipelineTranslator.fromOptions(options);    Pipeline p = Pipeline.create(options);    p.begin().apply(Read.from(io));    DataflowRunner runner = DataflowRunner.fromOptions(options);    Job workflow = translator.translate(p, runner, new ArrayList<DataflowPackage>()).getJob();    Step step = workflow.getSteps().get(0);    return stepToCloudSource(step);}
0
public Map<String, ElasticsearchException> getFailedResponses()
{    return failedResponses;}
0
public int run(String[] args) throws Exception
{    Options options = new Options();    try {        options.addOption(OPTION_JOB_NAME);        options.addOption(OPTION_CUBE_NAME);        options.addOption(OPTION_SEGMENT_ID);        options.addOption(OPTION_INPUT_PATH);        options.addOption(OPTION_OUTPUT_PATH);        parseOptions(options, args);        String input = getOptionValue(OPTION_INPUT_PATH);        String output = getOptionValue(OPTION_OUTPUT_PATH);        String cubeName = getOptionValue(OPTION_CUBE_NAME).toUpperCase(Locale.ROOT);        String segmentID = getOptionValue(OPTION_SEGMENT_ID);        CubeManager cubeMgr = CubeManager.getInstance(KylinConfig.getInstanceFromEnv());        CubeInstance cube = cubeMgr.getCube(cubeName);        CubeSegment cubeSeg = cube.getSegmentById(segmentID);                String jobName = getOptionValue(OPTION_JOB_NAME);                job = Job.getInstance(getConf(), jobName);        setJobClasspath(job, cube.getConfig());                Segments<CubeSegment> allSegs = cube.getMergingSegments(cubeSeg);        allSegs.add(cubeSeg);        attachSegmentsMetadataWithDict(allSegs, job.getConfiguration());                job.setMapperClass(MergeCuboidMapper.class);        job.setMapOutputKeyClass(Text.class);        job.setMapOutputValueClass(Text.class);                job.setReducerClass(CuboidReducer.class);        job.setOutputKeyClass(Text.class);        job.setOutputValueClass(Text.class);                IMROutput2.IMRMergeOutputFormat outputFormat = MRUtil.getBatchMergeOutputSide2(cubeSeg).getOutputFormat();        outputFormat.configureJobInput(job, input);        addInputDirs(input, job);                outputFormat.configureJobOutput(job, output, cubeSeg);                job.getConfiguration().set(BatchConstants.CFG_CUBE_NAME, cubeName);        job.getConfiguration().set(BatchConstants.CFG_CUBE_SEGMENT_ID, segmentID);        return waitForCompletion(job);    } finally {        if (job != null)            cleanupTempConfFile(job.getConfiguration());    }}
1
 KeystoneEndpointBuilder openstackKeystone(String path)
{    class KeystoneEndpointBuilderImpl extends AbstractEndpointBuilder implements KeystoneEndpointBuilder, AdvancedKeystoneEndpointBuilder {        public KeystoneEndpointBuilderImpl(String path) {            super("openstack-keystone", path);        }    }    return new KeystoneEndpointBuilderImpl(path);}
0
private void addValuesToResultSingleKeyToRemove(Object entriesMap, Collection result, Object keyToRemove, int limit, ExecutionContext context)
{    if (entriesMap == null || result == null)        return;    QueryObserver observer = QueryObserverHolder.getInstance();    if (verifyLimit(result, limit)) {        observer.limitAppliedAtIndexLevel(this, limit, result);        return;    }    assert entriesMap instanceof SortedMap;    Iterator entriesIter = ((Map) entriesMap).entrySet().iterator();    Map.Entry entry = null;    boolean foundKeyToRemove = false;    while (entriesIter.hasNext()) {        entry = (Map.Entry) entriesIter.next();                if (foundKeyToRemove || !keyToRemove.equals(entry.getKey())) {            RegionEntryToValuesMap rvMap = (RegionEntryToValuesMap) entry.getValue();            rvMap.addValuesToCollection(result, limit, context);            if (verifyLimit(result, limit)) {                observer.limitAppliedAtIndexLevel(this, limit, result);                return;            }        } else {            foundKeyToRemove = true;        }    }}
0
public void distributionConfig_returnsConfigProperty_ifPropertyIsADistributionConfigImpl()
{    DistributionConfigImpl distributionConfigFromProperties = new DistributionConfigImpl(new Properties());    Properties properties = new Properties();    properties.put(DS_CONFIG_NAME, distributionConfigFromProperties);    ConnectionConfigImpl config = new ConnectionConfigImpl(properties);    assertThat(config.distributionConfig()).isSameAs(distributionConfigFromProperties);}
0
public void read(org.apache.storm.thrift.protocol.TProtocol iprot, WorkerMetricPoint struct) throws org.apache.storm.thrift.TException
{    org.apache.storm.thrift.protocol.TField schemeField;    iprot.readStructBegin();    while (true) {        schemeField = iprot.readFieldBegin();        if (schemeField.type == org.apache.storm.thrift.protocol.TType.STOP) {            break;        }        switch(schemeField.id) {            case             1:                if (schemeField.type == org.apache.storm.thrift.protocol.TType.STRING) {                    struct.metricName = iprot.readString();                    struct.set_metricName_isSet(true);                } else {                    org.apache.storm.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);                }                break;            case             2:                if (schemeField.type == org.apache.storm.thrift.protocol.TType.I64) {                    struct.timestamp = iprot.readI64();                    struct.set_timestamp_isSet(true);                } else {                    org.apache.storm.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);                }                break;            case             3:                if (schemeField.type == org.apache.storm.thrift.protocol.TType.DOUBLE) {                    struct.metricValue = iprot.readDouble();                    struct.set_metricValue_isSet(true);                } else {                    org.apache.storm.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);                }                break;            case             4:                if (schemeField.type == org.apache.storm.thrift.protocol.TType.STRING) {                    struct.componentId = iprot.readString();                    struct.set_componentId_isSet(true);                } else {                    org.apache.storm.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);                }                break;            case             5:                if (schemeField.type == org.apache.storm.thrift.protocol.TType.STRING) {                    struct.executorId = iprot.readString();                    struct.set_executorId_isSet(true);                } else {                    org.apache.storm.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);                }                break;            case             6:                if (schemeField.type == org.apache.storm.thrift.protocol.TType.STRING) {                    struct.streamId = iprot.readString();                    struct.set_streamId_isSet(true);                } else {                    org.apache.storm.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);                }                break;            default:                org.apache.storm.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);        }        iprot.readFieldEnd();    }    iprot.readStructEnd();    struct.validate();}
0
public OptionsBuilder useLocale(Locale locale)
{    this.locale = locale;    return this;}
0
public void start()
{            sourceCounter.start();    executor = Executors.newSingleThreadExecutor();    runner = new ExecRunnable(shell, command, getChannelProcessor(), sourceCounter, restart, restartThrottle, logStderr, bufferCount, batchTimeout, charset);        runnerFuture = executor.submit(runner);        super.start();    }
1
 Beam<Map<String, Object>> buildBeam(String dataSource, String indexService, String discoveryPath, int clusterPartitions, int clusterReplication, String segmentGranularity, String queryGranularity, String windowPeriod, String firehoseGracePeriod, String indexRetryPeriod, DruidDimensions dimensions, List<AggregatorFactory> aggregator, Timestamper<Map<String, Object>> timestamper, TimestampSpec timestampSpec)
{    return DruidBeams.builder(timestamper).curator(curator).discoveryPath(discoveryPath).location(DruidLocation.create(DruidEnvironment.create(indexService, FIREHOSE_PATTERN), dataSource)).timestampSpec(timestampSpec).rollup(DruidRollup.create(dimensions, aggregator, QueryGranularity.fromString(queryGranularity))).tuning(ClusteredBeamTuning.builder().segmentGranularity(getGranularity(segmentGranularity)).windowPeriod(new Period(windowPeriod)).partitions(clusterPartitions).replicants(clusterReplication).build()).druidBeamConfig(DruidBeamConfig.builder().indexRetryPeriod(new Period(indexRetryPeriod)).firehoseGracePeriod(new Period(firehoseGracePeriod)).build()).buildBeam();}
0
public EM<T> cite(String cdata)
{    return cite().__(cdata).__();}
0
public void encode(ResultMetadata m, ByteBuf dest, ProtocolVersion version)
{    boolean noMetadata = m.flags.contains(Flag.NO_METADATA);    boolean globalTablesSpec = m.flags.contains(Flag.GLOBAL_TABLES_SPEC);    boolean hasMorePages = m.flags.contains(Flag.HAS_MORE_PAGES);    boolean metadataChanged = m.flags.contains(Flag.METADATA_CHANGED);    assert version.isGreaterThan(ProtocolVersion.V1) || (!hasMorePages && !noMetadata) : "version = " + version + ", flags = " + m.flags;    dest.writeInt(Flag.serialize(m.flags));    dest.writeInt(m.columnCount);    if (hasMorePages)        CBUtil.writeValue(m.pagingState.serialize(version), dest);    if (version.isGreaterOrEqualTo(ProtocolVersion.V5) && metadataChanged) {        assert !noMetadata : "MetadataChanged and NoMetadata are mutually exclusive flags";        CBUtil.writeBytes(m.getResultMetadataId().bytes, dest);    }    if (!noMetadata) {        if (globalTablesSpec) {            CBUtil.writeString(m.names.get(0).ksName, dest);            CBUtil.writeString(m.names.get(0).cfName, dest);        }        for (int i = 0; i < m.columnCount; i++) {            ColumnSpecification name = m.names.get(i);            if (!globalTablesSpec) {                CBUtil.writeString(name.ksName, dest);                CBUtil.writeString(name.cfName, dest);            }            CBUtil.writeString(name.name.toString(), dest);            DataType.codec.writeOne(DataType.fromType(name.type, version), dest, version);        }    }}
0
 void lockLocallyForClear(DistributionManager dm, InternalDistributedMember locker, CacheEvent event)
{    RegionVersionVector rvv = getVersionVector();    ARMLockTestHook armLockTestHook = getRegionMap().getARMLockTestHook();    if (armLockTestHook != null) {        armLockTestHook.beforeLock(this, event);    }    if (rvv != null) {                rvv.lockForClear(getFullPath(), dm, locker);                        checkReadiness();                if (getAttributes().getScope().isDistributedNoAck()) {            Set<InternalDistributedMember> members = getDistributionAdvisor().adviseCacheOp();            StateFlushOperation.flushTo(members, this);        }    }    if (armLockTestHook != null) {        armLockTestHook.afterLock(this, null);    }}
0
public static MetricAggExtractor randomMetricAggExtractor()
{    return new MetricAggExtractor(randomAlphaOfLength(16), randomAlphaOfLength(16), randomAlphaOfLength(16), randomZone(), randomBoolean());}
0
public void teardownTest() throws Exception
{    if (standaloneHaServices != null) {        standaloneHaServices.closeAndCleanupAllData();        standaloneHaServices = null;    }}
0
private static LinkedHashSet<InetAddress> getSubinterfaceInetAddrs(NetworkInterface nif)
{    LinkedHashSet<InetAddress> addrs = new LinkedHashSet<InetAddress>();    Enumeration<NetworkInterface> subNifs = nif.getSubInterfaces();    while (subNifs.hasMoreElements()) {        NetworkInterface subNif = subNifs.nextElement();        addrs.addAll(Collections.list(subNif.getInetAddresses()));    }    return addrs;}
0
public String getTransformMethodsName()
{    return transformMethodsName;}
0
private boolean verifySanity(long compressedLength, long decompressedLength, int forReduce, Set<TaskAttemptID> remaining, TaskAttemptID mapId)
{    if (compressedLength < 0 || decompressedLength < 0) {        wrongLengthErrs.increment(1);                return false;    }    if (forReduce != reduce) {        wrongReduceErrs.increment(1);                return false;    }        if (!remaining.contains(mapId)) {        wrongMapErrs.increment(1);                return false;    }    return true;}
1
public float getNormPower()
{    return normPower;}
0
public void setMinMax(int min, int max)
{    this.max = max;    this.min = min;    this.markAsNotEmpty();}
0
public List<RegionInfo> getRegionsToAdd()
{    return this.regionsToAdd;}
0
private void executeLivelinessCheck(ContainerRuntimeContext ctx) throws ContainerExecutionException
{    String procFs = ctx.getExecutionAttribute(PROCFS);    if (procFs == null || procFs.isEmpty()) {        procFs = DEFAULT_PROCFS;    }    String pid = ctx.getExecutionAttribute(PID);    if (!new File(procFs + File.separator + pid).exists()) {        String msg = "Liveliness check failed for PID: " + pid + ". Container may have already completed.";        throw new ContainerExecutionException(msg, PrivilegedOperation.ResultCode.INVALID_CONTAINER_PID.getValue());    }}
0
public Collection<byte[]> getRowsToLock()
{    return Collections.singleton(row);}
0
public DcDetailVO findDetail(long dcId, String name)
{    SearchCriteria<DcDetailVO> sc = DetailSearch.create();    sc.setParameters("dcId", dcId);    sc.setParameters("name", name);    return findOneIncludingRemovedBy(sc);}
0
public void testStoppingConsumerShutdownExecutor() throws Exception
{    RabbitMQConsumer consumer = new RabbitMQConsumer(endpoint, processor);    ThreadPoolExecutor e = (ThreadPoolExecutor) Executors.newFixedThreadPool(3);    Mockito.when(endpoint.createExecutor()).thenReturn(e);    Mockito.when(endpoint.getConcurrentConsumers()).thenReturn(1);    Mockito.when(endpoint.connect(any(ExecutorService.class))).thenReturn(conn);    Mockito.when(conn.createChannel()).thenReturn(channel);    consumer.doStart();    assertFalse(e.isShutdown());    consumer.doStop();    assertTrue(e.isShutdown());}
0
private void updateMeta(KylinConfig config, String projectName, String cubeName, DataModelDesc model)
{    String[] nodes = config.getRestServers();    Map<String, String> tableToProjects = new HashMap<>();    for (TableRef tableRef : model.getAllTables()) {        tableToProjects.put(tableRef.getTableIdentity(), tableRef.getTableDesc().getProject());    }    for (String node : nodes) {        RestClient restClient = new RestClient(node);        try {                        restClient.clearCacheForCubeMigration(cubeName, projectName, model.getName(), tableToProjects);        } catch (IOException e) {                    }    }}
1
private SimpleBean createSimple(String name)
{    return new SimpleBean(name, (byte) 1, (short) 2, 3, 4L, true, DATE, DATE.toInstant(), BYTE_ARRAY, BigDecimal.ONE, new StringBuilder(name).append("builder"));}
0
public void testSplitterEmptyAgain() throws Exception
{    getMockEndpoint("mock:line").expectedMessageCount(0);    List<Object> data = new ArrayList<>();    data.add("A");    template.sendBody("direct:start", data);    assertMockEndpointsSatisfied();}
0
private void submitJobGraphAndWait(final JobGraph jobGraph) throws JobExecutionException, InterruptedException
{    MINI_CLUSTER_RESOURCE.getMiniCluster().executeJobBlocking(jobGraph);}
0
public void setAcceptorThreadPriority(int threadPriority)
{    endpoint.setAcceptorThreadPriority(threadPriority);}
0
public boolean fetchNext() throws IOException
{    if (recordIterator == null) {        currentCuboidId = splitResult.cuboidId;        scanner = splitResult.table.scan(new GTScanRequestBuilder().setInfo(splitResult.table.getInfo()).setRanges(null).setDimensions(null).setFilterPushDown(null).createGTScanRequest());        recordIterator = scanner.iterator();    }    if (recordIterator.hasNext()) {        currentRecord = recordIterator.next();        return true;    } else {        scanner.close();        recordIterator = null;        return false;    }}
0
public String getServerName()
{    return serverName;}
0
public Resource getFairShare()
{    return this.fairShare;}
0
public void testOffsetFetch()
{    final String sql = "select empno from emp\n" + "offset 10 rows fetch next 5 rows only";    sql(sql).ok();}
0
public BulkByScrollTask.Status getStatus()
{    return getStatus(Arrays.asList(new BulkByScrollTask.StatusOrException[results.length()]));}
0
public void testDebugCommandLoggedAtDebug()
{    DefaultAuditLogger logger = makeSpyLogger(Level.DEBUG, Optional.of(Arrays.asList(DUMMY_COMMAND_1)));    logDummyCommandToAuditLog(logger, DUMMY_COMMAND_1);    verify(logger, times(1)).logAuditMessage(anyString());}
0
protected void _testMOWithJavaSerialization(boolean withCounters) throws Exception
{    Path inDir = getDir(IN_DIR);    Path outDir = getDir(OUT_DIR);    JobConf conf = createJobConf();    FileSystem fs = FileSystem.get(conf);    DataOutputStream file = fs.create(new Path(inDir, "part-0"));    file.writeBytes("a\nb\n\nc\nd\ne");    file.close();    fs.delete(inDir, true);    fs.delete(outDir, true);    file = fs.create(new Path(inDir, "part-1"));    file.writeBytes("a\nb\n\nc\nd\ne");    file.close();    conf.setJobName("mo");    conf.set("io.serializations", "org.apache.hadoop.io.serializer.JavaSerialization," + "org.apache.hadoop.io.serializer.WritableSerialization");    conf.setInputFormat(TextInputFormat.class);    conf.setMapOutputKeyClass(Long.class);    conf.setMapOutputValueClass(String.class);    conf.setOutputKeyComparatorClass(JavaSerializationComparator.class);    conf.setOutputKeyClass(Long.class);    conf.setOutputValueClass(String.class);    conf.setOutputFormat(TextOutputFormat.class);    MultipleOutputs.addNamedOutput(conf, "text", TextOutputFormat.class, Long.class, String.class);    MultipleOutputs.setCountersEnabled(conf, withCounters);    conf.setMapperClass(MOJavaSerDeMap.class);    conf.setReducerClass(MOJavaSerDeReduce.class);    FileInputFormat.setInputPaths(conf, inDir);    FileOutputFormat.setOutputPath(conf, outDir);    JobClient jc = new JobClient(conf);    RunningJob job = jc.submitJob(conf);    while (!job.isComplete()) {        Thread.sleep(100);    }        int namedOutputCount = 0;    FileStatus[] statuses = fs.listStatus(outDir);    for (FileStatus status : statuses) {        if (status.getPath().getName().equals("text-m-00000") || status.getPath().getName().equals("text-r-00000")) {            namedOutputCount++;        }    }    assertEquals(2, namedOutputCount);        BufferedReader reader = new BufferedReader(new InputStreamReader(fs.open(new Path(FileOutputFormat.getOutputPath(conf), "text-r-00000"))));    int count = 0;    String line = reader.readLine();    while (line != null) {        assertTrue(line.endsWith("text"));        line = reader.readLine();        count++;    }    reader.close();    assertFalse(count == 0);    Counters.Group counters = job.getCounters().getGroup(MultipleOutputs.class.getName());    if (!withCounters) {        assertEquals(0, counters.size());    } else {        assertEquals(1, counters.size());        assertEquals(2, counters.getCounter("text"));    }}
0
public void testMaxLambda02()
{    ELProcessor processor = new ELProcessor();    processor.defineBean("beans", beans);    processor.setVariable("comparison", "v->(x,y)->v(x).compareTo(v(y))");    Object result = processor.getValue("beans.stream().max(comparison(x->x.name))", Object.class);    Assert.assertEquals(bean03, ((Optional) result).get());}
0
public void trace(Marker marker, Object message)
{}
0
public Iterator<DecoratedKey> iterator()
{    return Collections.emptyIterator();}
0
public void test2Nprefix01a() throws Exception
{    int[] expdnrs = { 0, 1 };    distanceTest2("N(w1, w2, w3)", expdnrs);}
0
public String getKeyPassword()
{    return keyPassword;}
0
public String getClientIP()
{    return clientIP;}
0
public String getLabelResource()
{        return "transaction_controller_title";}
0
public int compareTo(StackInfo o)
{    if (name.equals(o.name)) {        return VersionUtils.compareVersions(version, o.version);    }    return name.compareTo(o.name);}
0
public TerminalNode RETURN()
{    return getToken(PainlessParser.RETURN, 0);}
0
 int getLocalFailoverCount()
{    return localFailoverCount;}
0
public int getNumberSlots()
{    return numberSlots;}
0
public Set<RemoteProcessGroupPortDTO> getOutputPorts()
{    return outputPorts;}
0
protected void runModuleUnitTestsIfEnabled(ITestConfig config) throws Exception
{    if (config.getUnitTestEnabled()) {        this.runModuleUnitTests(config);    }}
0
public void testParseScrollFullyLoadedFrom1_7() throws Exception
{    AtomicBoolean called = new AtomicBoolean();    sourceWithMockedRemoteCall("scroll_fully_loaded_1_7.json").doStartNextScroll("", timeValueMillis(0), wrapAsListener(r -> {        assertEquals("AVToMiDL50DjIiBO3yKA", r.getHits().get(0).getId());        assertEquals("{\"test\":\"test3\"}", r.getHits().get(0).getSource().utf8ToString());        assertEquals("testrouting", r.getHits().get(0).getRouting());        called.set(true);    }));    assertTrue(called.get());}
0
public void close()
{    this.receiver.close();    this.checkpoint();}
0
private static Attachment.Template.Builder createRandomAttachmentTemplateBuilder()
{    Attachment.Template.Builder attachmentBuilder = Attachment.Template.builder();    if (randomBoolean()) {        attachmentBuilder.setAuthorName(randomAlphaOfLength(10));        if (randomBoolean()) {            attachmentBuilder.setAuthorIcon(randomAlphaOfLength(10));        }        if (randomBoolean()) {            attachmentBuilder.setAuthorLink(randomAlphaOfLength(10));        }    }    if (randomBoolean()) {        attachmentBuilder.setColor(randomAlphaOfLength(10));    }    if (randomBoolean()) {        attachmentBuilder.setFallback(randomAlphaOfLength(10));    }    if (randomBoolean()) {        attachmentBuilder.setImageUrl(randomAlphaOfLength(10));    }    if (randomBoolean()) {        attachmentBuilder.setPretext(randomAlphaOfLength(10));    }    if (randomBoolean()) {        attachmentBuilder.setThumbUrl(randomAlphaOfLength(10));    }    if (randomBoolean()) {        attachmentBuilder.setTitle(randomAlphaOfLength(10));        if (randomBoolean()) {            attachmentBuilder.setTitleLink(randomAlphaOfLength(10));        }    }    if (randomBoolean()) {        attachmentBuilder.setText(randomAlphaOfLength(10));    }    if (randomBoolean()) {        int fieldCount = randomIntBetween(0, 3);        for (int j = 0; j < fieldCount; j++) {            attachmentBuilder.addField(randomAlphaOfLength(10), randomAlphaOfLength(10), randomBoolean());        }    }    if (randomBoolean()) {        attachmentBuilder.addMarkdownField(randomAlphaOfLength(10));        if (randomBoolean()) {            attachmentBuilder.addMarkdownField(randomAlphaOfLength(10));        }    }    return attachmentBuilder;}
0
public void setName(String name)
{    this.name = name;}
0
private Long getSnapshotSize(final Connection conn, final String primaryStorageSRUuid, final String snapshotUuid, final Boolean isISCSI, final int wait)
{    final String physicalSize = hypervisorResource.callHostPluginAsync(conn, "vmopsSnapshot", "getSnapshotSize", wait, "primaryStorageSRUuid", primaryStorageSRUuid, "snapshotUuid", snapshotUuid, "isISCSI", isISCSI.toString());    if (physicalSize == null || physicalSize.isEmpty()) {        return (long) 0;    } else {        return Long.parseLong(physicalSize);    }}
0
private void reinitializeCellSet(int numOfCells, KeyValueScanner segmentScanner, CellSet oldCellSet, MemStoreSizing memstoreSizing, MemStoreCompactionStrategy.Action action)
{    Cell curCell;    Chunk[] chunks = allocIndexChunks(numOfCells);    int currentChunkIdx = 0;    int offsetInCurentChunk = ChunkCreator.SIZEOF_CHUNK_HEADER;    int numUniqueKeys = 0;    Cell prev = null;    try {        while ((curCell = segmentScanner.next()) != null) {            assert (curCell instanceof ExtendedCell);            if (((ExtendedCell) curCell).getChunkId() == ExtendedCell.CELL_NOT_BASED_ON_CHUNK) {                                                                curCell = copyCellIntoMSLAB(curCell, memstoreSizing);            }            if (offsetInCurentChunk + ClassSize.CELL_CHUNK_MAP_ENTRY > chunks[currentChunkIdx].size) {                                currentChunkIdx++;                offsetInCurentChunk = ChunkCreator.SIZEOF_CHUNK_HEADER;            }            offsetInCurentChunk = createCellReference((ByteBufferKeyValue) curCell, chunks[currentChunkIdx].getData(), offsetInCurentChunk);            if (action == MemStoreCompactionStrategy.Action.FLATTEN_COUNT_UNIQUE_KEYS) {                                if (prev != null) {                    if (!CellUtil.matchingRowColumn(prev, curCell)) {                        numUniqueKeys++;                    }                } else {                    numUniqueKeys++;                }            }            prev = curCell;        }        if (action != MemStoreCompactionStrategy.Action.FLATTEN_COUNT_UNIQUE_KEYS) {            numUniqueKeys = CellSet.UNKNOWN_NUM_UNIQUES;        }    } catch (IOException ie) {        throw new IllegalStateException(ie);    } finally {        segmentScanner.close();    }    CellChunkMap ccm = new CellChunkMap(getComparator(), chunks, 0, numOfCells, false);        this.setCellSet(oldCellSet, new CellSet(ccm, numUniqueKeys));}
0
protected List<PathData> expandArgument(String arg) throws IOException
{    List<PathData> items = new LinkedList<PathData>();    try {        items.add(new PathData(new URI(arg), getConf()));    } catch (URISyntaxException e) {        items.add(new PathData(arg, getConf()));    }    return items;}
0
public synchronized int available() throws IOException
{    getInputStream();    if (fileLength != null) {        long remaining = fileLength - currentPos;        return remaining <= Integer.MAX_VALUE ? (int) remaining : Integer.MAX_VALUE;    } else {        return Integer.MAX_VALUE;    }}
0
protected TokenStreamComponents createComponents(String fieldName)
{    Tokenizer tokenizer = new KeywordTokenizer();    return new TokenStreamComponents(tokenizer, new GalicianStemFilter(tokenizer));}
0
 HipchatEndpointBuilder hipchat(String path)
{    class HipchatEndpointBuilderImpl extends AbstractEndpointBuilder implements HipchatEndpointBuilder, AdvancedHipchatEndpointBuilder {        public HipchatEndpointBuilderImpl(String path) {            super("hipchat", path);        }    }    return new HipchatEndpointBuilderImpl(path);}
0
public List<State> getFromStates(Event event)
{    return s_fsm.getFromStates(this, event);}
0
protected StormTopology systemTopologyImpl(Map<String, Object> topoConf, StormTopology topology) throws InvalidTopologyException
{    validateBasic(topology);    StormTopology ret = topology.deepCopy();    addAcker(topoConf, ret);    if (hasEventLoggers(topoConf)) {        addEventLogger(topoConf, ret);    }    addMetricComponents(topoConf, ret);    addSystemComponents(topoConf, ret);    addMetricStreams(ret);    addSystemStreams(ret);    validateStructure(ret);    return ret;}
0
public int hashCode(Object o, Schema s)
{    if (o == null)                return 0;    int hashCode = 1;    switch(s.getType()) {        case RECORD:            for (Field f : s.getFields()) {                if (f.order() == Field.Order.IGNORE)                    continue;                hashCode = hashCodeAdd(hashCode, getField(o, f.name(), f.pos()), f.schema());            }            return hashCode;        case ARRAY:            Collection<?> a = (Collection<?>) o;            Schema elementType = s.getElementType();            for (Object e : a) hashCode = hashCodeAdd(hashCode, e, elementType);            return hashCode;        case UNION:            return hashCode(o, s.getTypes().get(resolveUnion(s, o)));        case ENUM:            return s.getEnumOrdinal(o.toString());        case NULL:            return 0;        case STRING:            return (o instanceof Utf8 ? o : new Utf8(o.toString())).hashCode();        default:            return o.hashCode();    }}
0
 static DescriptiveStatistics benchmarkListPartition(@NotNull MicroBenchmark bench, @NotNull BenchData data)
{    final HMSClient client = data.getClient();    String dbName = data.dbName;    String tableName = data.tableName;    createPartitionedTable(client, dbName, tableName);    try {        addManyPartitions(client, dbName, tableName, null, Collections.singletonList("d"), 1);        return bench.measure(() -> throwingSupplierWrapper(() -> client.listPartitions(dbName, tableName)));    } catch (TException e) {        e.printStackTrace();        return new DescriptiveStatistics();    } finally {        throwingSupplierWrapper(() -> client.dropTable(dbName, tableName));    }}
0
public void sendError(int sc, String msg) throws IOException
{    this._getHttpServletResponse().sendError(sc, msg);}
0
public void testStatefulFactory()
{    StatefulFactoryTestScript.Factory factory = scriptEngine.compile("stateful_factory_test", "test + x + y + d", StatefulFactoryTestScript.CONTEXT, Collections.emptyMap());    StatefulFactoryTestScript.StatefulFactory statefulFactory = factory.newFactory(1, 2);    StatefulFactoryTestScript script = statefulFactory.newInstance(3, 4);    assertEquals(24, script.execute(3));    statefulFactory.newInstance(5, 6);    assertEquals(28, script.execute(7));    assertEquals(true, script.needsTest());    assertEquals(false, script.needsNothing());    assertEquals(true, script.needsX());    assertEquals(false, script.needsC());    assertEquals(true, script.needsD());    assertEquals(true, statefulFactory.needsTest());    assertEquals(false, statefulFactory.needsNothing());    assertEquals(true, statefulFactory.needsX());    assertEquals(false, statefulFactory.needsC());    assertEquals(true, statefulFactory.needsD());    assertEquals(true, factory.needsTest());    assertEquals(false, factory.needsNothing());    assertEquals(true, factory.needsX());    assertEquals(false, factory.needsC());    assertEquals(true, factory.needsD());}
0
protected boolean resolveDateTimeField(String dateTimeField)
{    return Part.resolve(dateTimeField) != null;}
0
public void testConstructorFive() throws Exception
{    instance = new MllpAcknowledgementExceptionStub(TEST_EXCEPTION_MESSAGE, HL7_MESSAGE_BYTES, CAUSE);    assertSame(CAUSE, instance.getCause());    assertTrue(instance.getMessage().startsWith(TEST_EXCEPTION_MESSAGE));    assertArrayEquals(HL7_MESSAGE_BYTES, instance.hl7MessageBytes);    assertNull(instance.hl7AcknowledgementBytes);}
0
 long getOperationTimeout(TimeUnit unit)
{    throw new NotImplementedException("Add an implementation!");}
0
public static Map<String, SmapStratum> generateSmap(JspCompilationContext ctxt, Node.Nodes pageNodes) throws IOException
{    Map<String, SmapStratum> smapInfo = new HashMap<>();        PreScanVisitor psVisitor = new PreScanVisitor();    try {        pageNodes.visit(psVisitor);    } catch (JasperException ex) {    }    HashMap<String, SmapStratum> map = psVisitor.getMap();        SmapStratum s = new SmapStratum();        evaluateNodes(pageNodes, s, map, ctxt.getOptions().getMappedFile());    s.optimizeLineSection();    s.setOutputFileName(unqualify(ctxt.getServletJavaFileName()));    String classFileName = ctxt.getClassFileName();    s.setClassFileName(classFileName);    smapInfo.put(ctxt.getFQCN(), s);    if (ctxt.getOptions().isSmapDumped()) {        File outSmap = new File(classFileName + ".smap");        PrintWriter so = new PrintWriter(new OutputStreamWriter(new FileOutputStream(outSmap), SMAP_ENCODING));        so.print(s.getSmapString());        so.close();    }    for (Map.Entry<String, SmapStratum> entry : map.entrySet()) {        String innerClass = entry.getKey();        s = entry.getValue();        s.optimizeLineSection();        s.setOutputFileName(unqualify(ctxt.getServletJavaFileName()));        String innerClassFileName = classFileName.substring(0, classFileName.indexOf(".class")) + '$' + innerClass + ".class";        s.setClassFileName(innerClassFileName);        smapInfo.put(ctxt.getFQCN() + "." + innerClass, s);        if (ctxt.getOptions().isSmapDumped()) {            File outSmap = new File(innerClassFileName + ".smap");            PrintWriter so = new PrintWriter(new OutputStreamWriter(new FileOutputStream(outSmap), SMAP_ENCODING));            so.print(s.getSmapString());            so.close();        }    }    return smapInfo;}
0
public int getBucketCount()
{    return rootArcs.length;}
0
public void testDropTableWithPurge() throws Exception
{    Table table = testTables[0];    client.dropTable(table.getDbName(), table.getTableName(), true, true, true);    Assert.assertFalse("Table path should be removed", metaStore.isPathExists(new Path(table.getSd().getLocation())));    Assert.assertFalse("Table path should not be in trash", metaStore.isPathExistsInTrash(new Path(table.getSd().getLocation())));}
0
public Boolean visitRangeRef(RexRangeRef rangeRef)
{    return doUnknown(rangeRef);}
0
public void setBundleState(Bundle bundle, String contextId, State state, Throwable t)
{    if (state == State.Failure) {            } else if (LOG.isDebugEnabled()) {            }    String key = String.format("%d:%s", bundle.getBundleId(), contextId);    if (state != null) {        states.put(key, state);    } else {        states.remove(key);    }    if (t != null) {        exceptions.put(key, t);    } else {        exceptions.remove(key);    }}
1
public Object getValue()
{    return null;}
0
public int getActualShards()
{    return actualShards;}
0
public Object visitSubquery(SubqueryContext ctx)
{    return plan(ctx.queryNoWith());}
0
public void refreshEntriesFromServerKeys(Connection con, List serverKeys, InterestResultPolicy interestResultPolicy)
{    if (serverKeys == null) {        return;    }    ServerRegionProxy proxy = getServerProxy();    if (logger.isDebugEnabled()) {        logKeys(serverKeys, interestResultPolicy);    }    if (interestResultPolicy == InterestResultPolicy.NONE) {                return;    }    if (logger.isDebugEnabled()) {            }    for (Object serverKey : serverKeys) {        ArrayList keysList = (ArrayList) serverKey;                if (keysList == null) {            continue;        }        if (EntryLogger.isEnabled()) {            if (con != null) {                Endpoint endpoint = con.getEndpoint();                if (endpoint != null) {                    EntryLogger.setSource(endpoint.getMemberId(), "RIGII");                }            }        }        try {            List list = new ArrayList(keysList);            if (interestResultPolicy != InterestResultPolicy.KEYS_VALUES) {                for (Object currentKey : keysList) {                                        if (currentKey == null || getImageState().hasDestroyedEntry(currentKey)) {                        list.remove(currentKey);                    }                }            }            if (interestResultPolicy == InterestResultPolicy.KEYS) {                                if (!isProxy()) {                    for (Object currentKey : list) {                        entries.initialImagePut(currentKey, 0, Token.LOCAL_INVALID, false, false, null, null, false);                    }                }                                    } else if (!list.isEmpty()) {                Assert.assertTrue(interestResultPolicy == InterestResultPolicy.KEYS_VALUES);                VersionedObjectList values = (VersionedObjectList) list.get(0);                if (logger.isDebugEnabled()) {                                    }                VersionedObjectList.Iterator listIt = values.iterator();                while (listIt.hasNext()) {                    VersionedObjectList.Entry entry = listIt.next();                    Object currentKey = entry.getKey();                    if (currentKey == null || getImageState().hasDestroyedEntry(currentKey)) {                        continue;                    }                    Object val = entry.getObject();                    boolean isBytes = entry.isBytes();                    boolean isKeyOnServer = !entry.isKeyNotOnServer();                    boolean isTombstone = getConcurrencyChecksEnabled() && entry.isKeyNotOnServer() && entry.getVersionTag() != null;                    final VersionTag tag = entry.getVersionTag();                    if (val instanceof Throwable) {                                                localDestroyNoCallbacks(currentKey);                        continue;                    }                    if (logger.isDebugEnabled()) {                                            }                    if (tag == null) {                                                localDestroyNoCallbacks(currentKey);                    }                    if (val instanceof byte[] && !isBytes) {                        val = CachedDeserializableFactory.create((byte[]) val, getCache());                    }                    if (isTombstone) {                        assert val == null : "server returned a value for a destroyed entry";                        val = Token.TOMBSTONE;                    }                    if (val != null || isTombstone) {                                                if (!isProxy()) {                            entries.initialImagePut(currentKey, 0, val, false, false, tag, null, false);                        }                    } else {                        RegionEntry regionEntry = entries.getEntry(currentKey);                        if (!isProxy() && isKeyOnServer) {                            entries.initialImagePut(currentKey, 0, Token.LOCAL_INVALID, false, false, tag, null, false);                        } else {                            if (regionEntry != null) {                                synchronized (regionEntry) {                                    if (regionEntry.isDestroyedOrRemovedButNotTombstone()) {                                        entries.removeEntry(currentKey, regionEntry, false);                                    }                                }                            }                        }                                                            }                }            }        } catch (DiskAccessException dae) {            handleDiskAccessException(dae);            throw dae;        } finally {            EntryLogger.clearSource();        }    }}
1
public final String toString()
{    return JsonUtil.toJsonString(this);}
0
public Long getId()
{    return id;}
0
public static void assertValue()
{    try {        Region r1 = cache.getRegion(Region.SEPARATOR + REGION_NAME1);        Region r2 = cache.getRegion(Region.SEPARATOR + REGION_NAME2);        assertTrue(r1.containsKey("key-1"));        assertTrue(r1.get("key-1").equals("33"));        assertTrue(r2.containsKey("key-1"));        assertTrue(r2.get("key-1").equals("33"));    } catch (Exception e) {        fail("Exception in trying to get due to " + e);    }}
0
public static int unsignedBinarySearch(byte[] a, int fromIndex, int toIndex, byte key)
{    int unsignedKey = key & 0xff;    int low = fromIndex;    int high = toIndex - 1;    while (low <= high) {        int mid = low + ((high - low) >> 1);        int midVal = a[mid] & 0xff;        if (midVal < unsignedKey) {            low = mid + 1;        } else if (midVal > unsignedKey) {            high = mid - 1;        } else {                        return mid;        }    }        return -(low + 1);}
0
public int getCount()
{    return count;}
0
public static AvroSource<GenericRecord> from(ValueProvider<String> fileNameOrPattern)
{    return new AvroSource<>(fileNameOrPattern, EmptyMatchTreatment.DISALLOW, DEFAULT_MIN_BUNDLE_SIZE, readGenericRecordsWithSchema(null));}
0
public Integer getConnectorPort()
{    return connectorPort;}
0
public static String fileNameToConfigType(String filename)
{    int extIndex = filename.indexOf(StackDirectory.SERVICE_CONFIG_FILE_NAME_POSTFIX);    return filename.substring(0, extIndex);}
0
public void testBackupRestore() throws Exception
{    BACKUP_ROOT_DIR = util.getDataTestDirOnTestFS() + Path.SEPARATOR + BACKUP_ROOT_DIR;    createTables();    runTestMulti();}
0
public Object[] getCurrentKey()
{    return currentKey;}
0
public String signature()
{    if (signature == null)        signature = accessString() + descriptor() + " " + name() + ";";    return signature;}
0
public String toString()
{    return "RootConfig{" + "includedModules=" + includedModules + ", excludedModules=" + excludedModules + ", include=" + include + ", exclude=" + exclude + ", skipBatching=" + skipBatching + ", isolate=" + isolate + ", batchSize=" + batchSize + ", subDirForPrefix='" + subDirForPrefix + '\'' + '}';}
0
private String getColInAggrGroup(AggregationGroup g, String name)
{    for (String c : g.getIncludes()) {        if (c.toLowerCase(Locale.ROOT).contains(name.toLowerCase(Locale.ROOT)))            return c;    }    throw new IllegalStateException();}
0
private ByteBuffer computeCrc(Class<? extends Crc32> clazz, ByteBuffer dataBufs, int bytePerCrc) throws Exception
{    final Constructor<? extends Crc32> ctor = clazz.getConstructor();    final Crc32 crc = ctor.newInstance();    final int size = 4 * (dataBufs.remaining() - 1) / bytePerCrc + 1;    final ByteBuffer crcBufs = allocateByteBuffer(size);    final DataChecksum checksum = DataChecksum.newDataChecksum(crc.crcType(), bytePerCrc);    checksum.calculateChunkedSums(dataBufs, crcBufs);    return crcBufs;}
0
public getTableNamesByPattern_argsTupleScheme getScheme()
{    return new getTableNamesByPattern_argsTupleScheme();}
0
public static Map<K, V> of(K k1, V v1, K k2, V v2, K k3, V v3, K k4, V v4, K k5, V v5, K k6, V v6, K k7, V v7, K k8, V v8, K k9, V v9, K k10, V v10, K k11, V v11, K k12, V v12, K k13, V v13)
{    Map<K, V> map = new LinkedHashMap<K, V>();    map.put(k1, v1);    map.put(k2, v2);    map.put(k3, v3);    map.put(k4, v4);    map.put(k5, v5);    map.put(k6, v6);    map.put(k7, v7);    map.put(k8, v8);    map.put(k9, v9);    map.put(k10, v10);    map.put(k11, v11);    map.put(k12, v12);    map.put(k13, v13);    return Collections.unmodifiableMap(map);}
0
public void open(S split) throws IOException
{    this.replicatedIF.open(split);}
0
protected String getHostName()
{        String endPoint = abfsConfig.get(AZURE_ABFS_ENDPOINT);    return endPoint.split(":")[0];}
0
protected PublisherPool createPublisherPool(final ProcessContext context)
{    final int maxMessageSize = context.getProperty(MAX_REQUEST_SIZE).asDataSize(DataUnit.B).intValue();    final long maxAckWaitMillis = context.getProperty(ACK_WAIT_TIME).asTimePeriod(TimeUnit.MILLISECONDS).longValue();    final String attributeNameRegex = context.getProperty(ATTRIBUTE_NAME_REGEX).getValue();    final Pattern attributeNamePattern = attributeNameRegex == null ? null : Pattern.compile(attributeNameRegex);    final boolean useTransactions = context.getProperty(USE_TRANSACTIONS).asBoolean();    final String transactionalIdPrefix = context.getProperty(TRANSACTIONAL_ID_PREFIX).evaluateAttributeExpressions().getValue();    Supplier<String> transactionalIdSupplier = KafkaProcessorUtils.getTransactionalIdSupplier(transactionalIdPrefix);    final String charsetName = context.getProperty(MESSAGE_HEADER_ENCODING).evaluateAttributeExpressions().getValue();    final Charset charset = Charset.forName(charsetName);    final Map<String, Object> kafkaProperties = new HashMap<>();    KafkaProcessorUtils.buildCommonKafkaProperties(context, ProducerConfig.class, kafkaProperties);    kafkaProperties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getName());    kafkaProperties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getName());    kafkaProperties.put("max.request.size", String.valueOf(maxMessageSize));    return new PublisherPool(kafkaProperties, getLogger(), maxMessageSize, maxAckWaitMillis, useTransactions, transactionalIdSupplier, attributeNamePattern, charset);}
0
public AccessPolicy getAccessPolicy(String identifier) throws AuthorizationAccessException
{    return baseConfigurableAccessPolicyProvider.getAccessPolicy(identifier);}
0
private void initLog()
{    t_log = new float[LOG_TABLE_SIZE + 1];    for (int i = 0; i < LOG_TABLE_SIZE + 1; i++) {        float x = (float) (((float) (i) + 1e-5f) / LOG_TABLE_SIZE);        t_log[i] = (float) Math.log(x);    }}
0
public com.google.protobuf.ByteString getHistoryText()
{    return historyText_;}
0
private Map<String, Integer> calculateAverageSelectedCount(Set<PeerStatus> collection, List<PeerStatus> destinations)
{        final Map<String, Integer> hostNameCounts = collection.stream().collect(groupingBy(p -> p.getPeerDescription().getHostname(), reducing(0, p -> 1, Integer::sum)));        return destinations.stream().collect(groupingBy(p -> p.getPeerDescription().getHostname(), reducing(0, p -> 1, Integer::sum))).entrySet().stream().collect(toMap(Map.Entry::getKey, e -> {        return e.getValue() / hostNameCounts.get(e.getKey());    }));}
0
public AmbariMetaInfo get()
{    return ami;}
0
public Language resolveLanguage(String name, CamelContext context)
{        Language lang = ResolverHelper.lookupLanguageInRegistryWithFallback(context, name);    if (lang != null) {        return lang;    }    lang = getLanguage(name, context);    if (lang != null) {        return lang;    }    LanguageResolver resolver = getLanguageResolver("default", context);    if (resolver != null) {        return resolver.resolveLanguage(name, context);    }    throw new NoSuchLanguageException(name);}
0
 static Modifier fromString(String modifier)
{    return EnumSet.allOf(Modifier.class).stream().filter(km -> km.modifier.equals(modifier)).findFirst().orElseThrow(    () -> new IllegalArgumentException("Found invalid modifier."));}
0
private static synchronized AppState getAppState(ApplicationId appId, YarnClient yarnClient) throws IOException
{    AppState appState = AppState.ACTIVE;    try {        ApplicationReport report = yarnClient.getApplicationReport(appId);        YarnApplicationState yarnState = report.getYarnApplicationState();        if (APP_FINAL_STATES.contains(yarnState)) {            appState = AppState.COMPLETED;        }    } catch (ApplicationNotFoundException e) {        appState = AppState.UNKNOWN;    } catch (YarnException e) {        throw new IOException(e);    }    return appState;}
0
public void setRouterId(String router)
{    this.routerId = router;}
0
public String getScheme()
{    return "ftp";}
0
public List<String> getNeededNestedColumnPaths()
{    return conf.getNeededNestedColumnPaths();}
0
public void testUnsafeUtil()
{    new Tester().checkUnsafe();}
0
public AccessControlList getBucketAcl(String bucketName) throws AmazonClientException, AmazonServiceException
{    return delegate.getBucketAcl(bucketName);}
0
public boolean isValueNull()
{    return (null == getValueAsToken());}
0
public void testDeleteIndexDestructiveOperationsRequireName()
{    createIndex("index1");    Settings settings = Settings.builder().put(DestructiveOperations.REQUIRES_NAME_SETTING.getKey(), true).build();    assertAcked(client().admin().cluster().prepareUpdateSettings().setTransientSettings(settings));    {        IllegalArgumentException illegalArgumentException = expectThrows(IllegalArgumentException.class, () -> client().admin().indices().prepareDelete("*").get());        assertEquals("Wildcard expressions or all indices are not allowed", illegalArgumentException.getMessage());        String[] indices = client().admin().indices().prepareGetIndex().setIndices("index1").get().getIndices();        assertEquals(1, indices.length);        assertEquals("index1", indices[0]);    }    {        IllegalArgumentException illegalArgumentException = expectThrows(IllegalArgumentException.class, () -> client().admin().indices().prepareDelete("*", "-index1").get());        assertEquals("Wildcard expressions or all indices are not allowed", illegalArgumentException.getMessage());        String[] indices = client().admin().indices().prepareGetIndex().setIndices("index1").get().getIndices();        assertEquals(1, indices.length);        assertEquals("index1", indices[0]);    }    {        IllegalArgumentException illegalArgumentException = expectThrows(IllegalArgumentException.class, () -> client().admin().indices().prepareDelete("_all").get());        assertEquals("Wildcard expressions or all indices are not allowed", illegalArgumentException.getMessage());        String[] indices = client().admin().indices().prepareGetIndex().setIndices("index1").get().getIndices();        assertEquals(1, indices.length);        assertEquals("index1", indices[0]);    }    assertAcked(client().admin().indices().prepareDelete("index1"));}
0
public void toData(PdxWriter out)
{    out.writeInt("id", this.id);    out.writeString("ticker", this._ticker);    out.writeInt("price", this._price);    out.writeObject("idTickers", this.idTickers);    out.writeObject("positions", this.positions);    out.writeObject("test", this.test);}
0
public static Builder newBuilder(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsRequestProto prototype)
{    return newBuilder().mergeFrom(prototype);}
0
public void shutdown() throws InterruptedException
{    terminate(threadPool);}
0
public synchronized void stop()
{    listenerRunning = false;    if (pinger != null) {        pinger.shutdownNow();    }    if (ss != null) {        try {            ss.close();        } catch (IOException e) {            e.printStackTrace();        }    }    for (LearnerHandler lh : activeObservers) {        lh.shutdown();    }}
0
private void testSliceReleaseOriginal(boolean retainedSlice1, boolean retainedSlice2)
{    ByteBuf buf = newBuffer(8).resetWriterIndex();    ByteBuf expected1 = newBuffer(3).resetWriterIndex();    ByteBuf expected2 = newBuffer(2).resetWriterIndex();    buf.writeBytes(new byte[] { 1, 2, 3, 4, 5, 6, 7, 8 });    expected1.writeBytes(new byte[] { 6, 7, 8 });    expected2.writeBytes(new byte[] { 7, 8 });    ByteBuf slice1 = retainedSlice1 ? buf.retainedSlice(buf.readerIndex() + 5, 3) : buf.slice(buf.readerIndex() + 5, 3).retain();    assertEquals(0, slice1.compareTo(expected1));        buf.release();    ByteBuf slice2 = retainedSlice2 ? slice1.retainedSlice(slice1.readerIndex() + 1, 2) : slice1.slice(slice1.readerIndex() + 1, 2).retain();    assertEquals(0, slice2.compareTo(expected2));        assertTrue(expected1.release());    assertTrue(expected2.release());        slice2.release();        assertTrue(slice1.release());            assertEquals(0, buf.refCnt());    assertEquals(0, slice1.refCnt());    assertEquals(0, slice2.refCnt());}
0
protected RouteBuilder createRouteBuilder() throws Exception
{    return new SpringRouteBuilder() {        public void configure() throws Exception {                                    from("direct:okay").policy("PROPAGATION_REQUIRED").setBody(constant("Tiger in Action")).bean("bookService").setBody(constant("Elephant in Action")).bean("bookService");                        from("direct:fail").policy("PROPAGATION_REQUIRED").setBody(constant("Tiger in Action")).bean("bookService").setBody(constant("Donkey in Action")).bean("bookService");                }    };}
0
public addColumnFamily_args getEmptyArgsInstance()
{    return new addColumnFamily_args();}
0
public void testJavaEscaping()
{    assertEquals("\\\\hello\\\"world'space/", EncodingUtils.escapeJava("\\hello\"world'space/"));}
0
public static TypeInformation<Integer> INTERVAL_MONTHS()
{    return TimeIntervalTypeInfo.INTERVAL_MONTHS;}
0
public void testUpdateWithTtl() throws Throwable
{    createTable("CREATE TABLE %s (k int PRIMARY KEY, v int)");    execute("INSERT INTO %s (k, v) VALUES (1, 1) USING TTL ?", 3600);    execute("INSERT INTO %s (k, v) VALUES (2, 2) USING TTL ?", 3600);            execute("UPDATE %s USING TTL ? SET v = ? WHERE k = ?", unset(), 1, 1);    assertRows(execute("SELECT ttl(v) FROM %s WHERE k = 1"), row(new Object[] { null }));        execute("UPDATE %s USING TTL ? SET v = ? WHERE k = ?", unset(), 2, 2);    assertRows(execute("SELECT k, v, TTL(v) FROM %s WHERE k = 2"), row(2, 2, null));        assertInvalidMessage("A TTL must be greater or equal to 0, but was -5", "UPDATE %s USING TTL ? SET v = ? WHERE k = ?", -5, 1, 1);    assertInvalidMessage("ttl is too large.", "UPDATE %s USING TTL ? SET v = ? WHERE k = ?", Attributes.MAX_TTL + 1, 1, 1);}
0
public void enterRule(ParseTreeListener listener)
{    if (listener instanceof SqlBaseListener)        ((SqlBaseListener) listener).enterFunctionTemplate(this);}
0
public long getGranularity()
{    return granularity;}
0
public void testBooleanPrecedence() throws IllegalAccessException, IOException, InvalidConfigurationValueException
{    final String accountName = "account";    final String globalKey = "fs.azure.bool";    final String accountKey = globalKey + "." + accountName;    final Configuration conf = new Configuration();    final AbfsConfiguration abfsConf = new AbfsConfiguration(conf, accountName);    conf.setBoolean(globalKey, false);    assertEquals("Default value returned even though account-agnostic config was set", abfsConf.getBoolean(globalKey, true), false);    conf.unset(globalKey);    assertEquals("Default value not returned even though config was unset", abfsConf.getBoolean(globalKey, true), true);    conf.setBoolean(accountKey, false);    assertEquals("Default value returned even though account-specific config was set", abfsConf.getBoolean(globalKey, true), false);    conf.unset(accountKey);    assertEquals("Default value not returned even though config was unset", abfsConf.getBoolean(globalKey, true), true);    conf.setBoolean(accountKey, true);    conf.setBoolean(globalKey, false);    assertEquals("Account-agnostic or default value returned even though account-specific config was set", abfsConf.getBoolean(globalKey, false), true);}
0
private static float or(float a, float b)
{    throw new ClassCastException("Cannot apply [|] operation to type [float]");}
0
public void testLoginWithDefaults() throws LoginException
{    LoginContext context = new LoginContext("GuestLoginWithDefaults", new CallbackHandler() {        public void handle(Callback[] callbacks) throws IOException, UnsupportedCallbackException {            assertEquals("Should have no Callbacks", 0, callbacks.length);        }    });    context.login();    Subject subject = context.getSubject();    assertEquals("Should have two principals", 2, subject.getPrincipals().size());    assertEquals("Should have one user principal", 1, subject.getPrincipals(UserPrincipal.class).size());    assertTrue("User principal is 'guest'", subject.getPrincipals(UserPrincipal.class).contains(new UserPrincipal("guest")));    assertEquals("Should have one group principal", 1, subject.getPrincipals(GroupPrincipal.class).size());    assertTrue("Group principal is 'guests'", subject.getPrincipals(GroupPrincipal.class).contains(new GroupPrincipal("guests")));    context.logout();    assertEquals("Should have zero principals", 0, subject.getPrincipals().size());}
0
protected void lazyCreateProperties() throws IOException
{    if (properties == null) {        if (marshalledProperties == null) {            properties = new HashMap<String, Object>();        } else {            properties = unmarsallProperties(marshalledProperties);            marshalledProperties = null;        }    }}
0
public StreamExpressionParameter toExpression(StreamFactory factory) throws IOException
{    StreamExpression expression = new StreamExpression(factory.getFunctionName(getClass()));    expression.addParameter(from.toString().toLowerCase(Locale.ROOT));    expression.addParameter(to.toString().toLowerCase(Locale.ROOT));    for (StreamEvaluator evaluator : containedEvaluators) {        expression.addParameter(evaluator.toExpression(factory));    }    return expression;}
0
public void write(org.apache.thrift.protocol.TProtocol prot, resourcePoolGetAll_args struct) throws org.apache.thrift.TException
{    org.apache.thrift.protocol.TTupleProtocol oprot = (org.apache.thrift.protocol.TTupleProtocol) prot;}
0
public void setName(String name)
{}
0
public void doVerifyVersions(Class olderClass, Class olderVersionClass, Class newerClass, Class newerVersionClass, Map<Class, String> versionedClassToNameMap) throws IllegalAccessException
{    List<Class> olderVersionClasses = new ArrayList<Class>();    olderVersionClasses.add(olderVersionClass);    List<Class> newerVersionClasses = new ArrayList<Class>();    newerVersionClasses.add(olderVersionClass);    newerVersionClasses.add(newerVersionClass);        Method[] olderMethods = onlyPublicMethods(olderClass.getDeclaredMethods());    TreeSet<String> olderMethodSet = getMethodKeySetForAnnotation(olderMethods, olderVersionClass, versionedClassToNameMap);    TreeSet<String> olderNoMethodAnnotationsSet = getMethodKeySetExcludingAnnotations(olderMethods, olderVersionClasses, versionedClassToNameMap);    Field[] olderFields = onlyPublicFields(olderClass.getFields());    TreeSet<String> olderFieldSet = getFieldKeySetForAnnotation(olderFields, olderVersionClass, versionedClassToNameMap);    TreeSet<String> olderNoFieldAnnotationsSet = getFieldKeySetExcludingAnnotations(olderFields, olderVersionClasses, versionedClassToNameMap);        Method[] newerMethods = onlyPublicMethods(newerClass.getDeclaredMethods());    TreeSet<String> newerMethodSetV1 = getMethodKeySetForAnnotation(newerMethods, olderVersionClass, versionedClassToNameMap);    TreeSet<String> newerMethodSetV2 = getMethodKeySetForAnnotation(newerMethods, newerVersionClass, versionedClassToNameMap);    TreeSet<String> newerNoMethodAnnotationsSetV1andV2 = getMethodKeySetExcludingAnnotations(newerMethods, newerVersionClasses, versionedClassToNameMap);    Field[] newerFields = onlyPublicFields(newerClass.getFields());        TreeSet<String> newerFieldSetV1 = getFieldKeySetForAnnotation(newerFields, olderVersionClass, versionedClassToNameMap);    TreeSet<String> newerFieldSetV2 = getFieldKeySetForAnnotation(newerFields, newerVersionClass, versionedClassToNameMap);    TreeSet<String> newerNoFieldAnnotationsSetV1andV2 = getFieldKeySetExcludingAnnotations(newerFields, newerVersionClasses, versionedClassToNameMap);        if (olderNoMethodAnnotationsSet.size() != 0) {        Assert.assertTrue("Class " + olderClass.getSimpleName() + " has 1 or more public methods without a version V1 annotation " + olderNoMethodAnnotationsSet.toString(), false);    }    if (olderNoFieldAnnotationsSet.size() != 0) {        Assert.assertTrue("Class " + olderClass.getSimpleName() + " has 1 or more public fields without a version V1 annotation " + olderNoFieldAnnotationsSet.toString(), false);    }    if (newerNoMethodAnnotationsSetV1andV2.size() != 0) {        Assert.assertTrue("Class " + newerClass.getSimpleName() + " has 1 or more public methods without a version V1 or V2 annotation " + newerNoMethodAnnotationsSetV1andV2.toString(), false);    }    if (newerNoFieldAnnotationsSetV1andV2.size() != 0) {        Assert.assertTrue("Class " + newerClass.getSimpleName() + " has 1 or more public methods without a version V1 or V2 annotation " + newerNoFieldAnnotationsSetV1andV2.toString(), false);    }        if (!olderMethodSet.equals(newerMethodSetV1)) {        TreeSet<String> leftCopy = new TreeSet<String>(olderMethodSet);        leftCopy.removeAll(newerMethodSetV1);        TreeSet<String> rightCopy = new TreeSet<String>(newerMethodSetV1);        rightCopy.removeAll(olderMethodSet);        Assert.assertTrue("Class " + olderClass.getSimpleName() + " and class " + newerClass.getSimpleName() + " methods are different for V1 " + leftCopy.toString() + " " + rightCopy.toString(), false);    }        if (!olderFieldSet.equals(newerFieldSetV1)) {        TreeSet<String> leftCopy = new TreeSet<String>(olderFieldSet);        leftCopy.removeAll(newerFieldSetV1);        TreeSet<String> rightCopy = new TreeSet<String>(newerFieldSetV1);        rightCopy.removeAll(olderFieldSet);        Assert.assertTrue("Class " + olderClass.getSimpleName() + " and class " + newerClass.getSimpleName() + " fields are different for V1 " + leftCopy.toString() + " " + rightCopy.toString(), false);    }}
0
public boolean equals(Object other)
{    if (this == other) {        return true;    }    if (other == null || getClass() != other.getClass()) {        return false;    }    QueryProvider that = (QueryProvider) other;    return Objects.equals(this.query, that.query) && Objects.equals(this.parsedQuery, that.parsedQuery) && Objects.equals(this.parsingException, that.parsingException);}
0
 final DocumentsWriterPerThread checkoutLargestNonPendingWriter()
{    ThreadState largestNonPendingWriter = findLargestNonPendingWriter();    if (largestNonPendingWriter != null) {                largestNonPendingWriter.lock();        try {            synchronized (this) {                try {                    if (largestNonPendingWriter.isInitialized() == false) {                        return nextPendingFlush();                    } else {                        return checkout(largestNonPendingWriter, largestNonPendingWriter.isFlushPending() == false);                    }                } finally {                    updateStallState();                }            }        } finally {            largestNonPendingWriter.unlock();        }    }    return null;}
0
public boolean equals(Object other)
{    if (other == this) {        return true;    }    if (!(other instanceof LogicalTupleFilter)) {        return false;    }    final LogicalTupleFilter otherFilter = (LogicalTupleFilter) other;    if (otherFilter.operator != this.operator || otherFilter.children.size() != this.children.size()) {        return false;    }    for (int i = 0; i < otherFilter.children.size(); i++) {        if (!otherFilter.children.get(i).equals(this.children.get(i))) {            return false;        }    }    return true;}
0
public void removeService(Service service)
{    synchronized (servicesLock) {        int j = -1;        for (int i = 0; i < services.length; i++) {            if (service == services[i]) {                j = i;                break;            }        }        if (j < 0)            return;        try {            services[j].stop();        } catch (LifecycleException e) {                }        int k = 0;        Service[] results = new Service[services.length - 1];        for (int i = 0; i < services.length; i++) {            if (i != j)                results[k++] = services[i];        }        services = results;                support.firePropertyChange("service", service, null);    }}
0
public void add()
{    if (work.obj != null) {        com.clearspring.analytics.stream.membership.BloomFilter filter = (com.clearspring.analytics.stream.membership.BloomFilter) work.obj;        if (!filter.isPresent(String.valueOf(in.value))) {            filter.add(String.valueOf(in.value));        } else {            dups.value++;        }    }}
0
public void testExchangePartitionsBetweenNonTempAndTemp() throws Exception
{    Table nonTempTable = createNonTempTable(DB_NAME, "nonTempTable", getYearMonthAndDayPartCols(), null);    Map<String, String> partitionSpecs = new HashMap<>();    partitionSpecs.put(YEAR_COL_NAME, "2017");    partitionSpecs.put(MONTH_COL_NAME, "");    partitionSpecs.put(DAY_COL_NAME, "23");    getClient().exchange_partitions(partitionSpecs, nonTempTable.getDbName(), nonTempTable.getTableName(), getDestTable().getDbName(), getDestTable().getTableName());}
0
public void writeWithDateFieldWithDateTypeFromMetaData() throws Exception
{    Date fieldValue = new Date();    Object expectedValueWritten = new java.sql.Date(fieldValue.getTime());    JDBCType dataType = JDBCType.DATE;    when(tableMetaDataView.getColumnDataType(fieldName)).thenReturn(dataType);    when(value.getField(fieldName)).thenReturn(fieldValue);    when(statement.executeUpdate()).thenReturn(1);    Object createKey = "createKey";    handler.write(region, Operation.CREATE, createKey, value);    verify(statement).setObject(1, expectedValueWritten);    verify(statement).setObject(2, createKey);    verify(statement).close();}
0
protected String[] getSupportedState()
{    return SUPPORTED_STATE;}
0
public void run2()
{    InternalResourceManager resourceManager = (InternalResourceManager) getCache().getResourceManager();    resourceManager.getHeapMonitor().updateStateAndSendEvent(NORMAL_HEAP_USED, "test");}
0
public BigDecimal randHiveBigDecimalNormalRange(Random r, String digitAlphabet)
{    String digits = RandomTypeUtil.getRandString(r, digitAlphabet, 1 + r.nextInt(38));    BigInteger bigInteger = new BigInteger(digits);    boolean negated = false;    if (r.nextBoolean()) {        bigInteger = bigInteger.negate();        negated = true;    }    int scale = 0 + r.nextInt(38 + 1);    return new BigDecimal(bigInteger, scale);}
0
public JavaSparkContext getJavaSparkContext()
{    return sparkInterpreter.getJavaSparkContext();}
0
 Path getMemberBackupLocationDir()
{    return memberBackupLocationDir;}
0
private static void writeFile(FileSystem fs, Path name, CompressionCodec codec, String contents) throws IOException
{    OutputStream stm;    if (codec == null) {        stm = fs.create(name);    } else {        stm = codec.createOutputStream(fs.create(name));    }    stm.write(contents.getBytes());    stm.close();}
0
public Map<String, Object> getConfig()
{    return config;}
0
public void testCluster() throws JSONException, Exception
{    WebResource r = resource();    ClientResponse response = r.path("ws").path("v1").path("cluster").accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);    assertEquals(MediaType.APPLICATION_JSON + "; " + JettyUtils.UTF_8, response.getType().toString());    JSONObject json = response.getEntity(JSONObject.class);    verifyClusterInfo(json);}
0
public ServerCnxnFactory getSecureServerCnxnFactory()
{    return secureServerCnxnFactory;}
0
public List<Config> getLatestConfigsWithTypes(Collection<String> types)
{    return clusterDAO.getLatestConfigurationsWithTypes(clusterId, getDesiredStackVersion(), types).stream().map(clusterConfigEntity -> configFactory.createExisting(this, clusterConfigEntity)).collect(toList());}
0
private String plural(int arity)
{    return arity > 1 ? "s" : "";}
0
public long fileLength(String name) throws IOException
{    FileStatus fileStatus = fileSystem.getFileStatus(new Path(hdfsDirPath, name));    return fileStatus.getLen();}
0
public static void savePaxosCommit(Commit commit)
{            String cql = "UPDATE system.%s USING TIMESTAMP ? AND TTL ? SET proposal_ballot = null, proposal = null, most_recent_commit_at = ?, most_recent_commit = ?, most_recent_commit_version = ? WHERE row_key = ? AND cf_id = ?";    executeInternal(format(cql, PAXOS), UUIDGen.microsTimestamp(commit.ballot), paxosTtlSec(commit.update.metadata()), commit.ballot, PartitionUpdate.toBytes(commit.update, MessagingService.current_version), MessagingService.current_version, commit.update.partitionKey().getKey(), commit.update.metadata().id.asUUID());}
0
public void setEnabled(boolean isEnabled)
{    m_isEnabled = isEnabled;}
0
public boolean isClosed()
{    return userRegion.isDestroyed() || !writer.isOpen();}
0
private void verifyReferencesValidField(String origName, @Nullable String alias)
{    Optional<Integer> refId = referenceByName(origName, ct);    if (refId.isPresent()) {        checkRowtimeType(ct.getTypeAt(refId.get()));    } else if (alias != null) {        throw new ValidationException(String.format("Alias '%s' must reference an existing field.", alias));    }}
0
public String eval()
{    return "ftremblay@gmail.com";}
0
 FacebookEndpointBuilder metric(String metric)
{    doSetProperty("metric", metric);    return this;}
0
public void clearDirectiveStats()
{    assert namesystem.hasWriteLock();    for (CacheDirective directive : directivesById.values()) {        directive.resetStatistics();    }}
0
public void testHiveConfigClusterUpdateSpecifyingHostNamesHiveServer2HA() throws Exception
{    final String expectedHostGroupName = "host_group_1";    final String expectedMetaStoreURIs = "thrift://c6401.ambari.apache.org:9083,thrift://c6402.ambari.apache.org:9083";    Map<String, Map<String, String>> configProperties = new HashMap<>();    Map<String, String> hiveEnvProperties = new HashMap<>();    Map<String, String> hiveSiteProperties = new HashMap<>();    configProperties.put("hive-env", hiveEnvProperties);    configProperties.put("hive-site", hiveSiteProperties);        hiveSiteProperties.put("hive.server2.support.dynamic.service.discovery", "true");        hiveSiteProperties.put("hive.metastore.uris", expectedMetaStoreURIs);    Configuration clusterConfig = new Configuration(configProperties, emptyMap());    Collection<String> hgComponents = new HashSet<>();    hgComponents.add("HIVE_SERVER");    TestHostGroup group1 = new TestHostGroup(expectedHostGroupName, hgComponents, Collections.singleton("some-host"));    Collection<String> hgComponents2 = new HashSet<>();    hgComponents2.add("HIVE_SERVER");    TestHostGroup group2 = new TestHostGroup("host_group_2", hgComponents2, Collections.singleton("some-host2"));    Collection<TestHostGroup> hostGroups = new HashSet<>();    hostGroups.add(group1);    hostGroups.add(group2);    expect(stack.getCardinality("HIVE_SERVER")).andReturn(new Cardinality("1+")).anyTimes();    ClusterTopology topology = createClusterTopology(bp, clusterConfig, hostGroups);    BlueprintConfigurationProcessor updater = new BlueprintConfigurationProcessor(topology);    updater.doUpdateForClusterCreate();    assertEquals("Unexpected config update for hive.metastore.uris", expectedMetaStoreURIs, hiveSiteProperties.get("hive.metastore.uris"));}
0
public HivePartitionHolder getHivePartitionHolder()
{    return hivePartitionHolder;}
0
private long getVolumeSizeIncludingHypervisorSnapshotReserve(long volumeSize, Integer hypervisorSnapshotReserve)
{    if (hypervisorSnapshotReserve != null) {        hypervisorSnapshotReserve = Math.max(hypervisorSnapshotReserve, LOWEST_HYPERVISOR_SNAPSHOT_RESERVE);        volumeSize += volumeSize * (hypervisorSnapshotReserve / 100f);    }    return Math.max(volumeSize, SolidFireUtil.MIN_VOLUME_SIZE);}
0
public ResourceEntity findAmbariResource()
{    return findById(ResourceEntity.AMBARI_RESOURCE_ID);}
0
public void setOutInterceptors(List<Interceptor<? extends Message>> interceptors)
{    interceptorHolder.setOutInterceptors(interceptors);}
0
public void testIncompleteHeader()
{    final TestRunner runner = TestRunners.newTestRunner(new SplitText());    runner.setProperty(SplitText.HEADER_LINE_COUNT, "2");    runner.setProperty(SplitText.LINE_SPLIT_COUNT, "2");    runner.setProperty(SplitText.FRAGMENT_MAX_SIZE, "50 B");    runner.enqueue("Header Line #1");    runner.run();    runner.assertTransferCount(SplitText.REL_FAILURE, 1);    runner.assertTransferCount(SplitText.REL_ORIGINAL, 0);    runner.assertTransferCount(SplitText.REL_SPLITS, 0);}
0
public void testGetTestMethodNameInAllVMs()
{    String expected = createDefaultDiskStoreName(0, -1, "testGetTestMethodNameInAllVMs");    assertGetDefaultDiskStoreName(expected);    for (int vmIndex = 0; vmIndex < getVMCount(); vmIndex++) {        String expectedInVM = createDefaultDiskStoreName(0, vmIndex, "testGetTestMethodNameInAllVMs");        getVM(vmIndex).invoke(() -> assertGetDefaultDiskStoreName(expectedInVM));    }}
0
private void swapUuids(VpcVO oldVpc, VpcVO newVpc)
{    String realUuid = oldVpc.getUuid();    String dummyUuid = newVpc.getUuid();    oldVpc.setUuid(dummyUuid.replace("-", "+"));    newVpc.setUuid(realUuid);    _vpcDao.update(oldVpc.getId(), oldVpc);    _vpcDao.update(newVpc.getId(), newVpc);    oldVpc.setUuid(dummyUuid);    _vpcDao.update(oldVpc.getId(), oldVpc);}
0
public String fold(String acc, Integer in) throws Exception
{    if (!openCalled) {        fail("Open was not called before run.");    }    return acc + in;}
0
public long getSize()
{    return size;}
0
private synchronized TreeMap<Bytes, byte[]> toTreeMap()
{    return new TreeMap<>(this.map);}
0
public final void write(final ConfigurableComponent component) throws IOException
{    write(component, null, null);}
0
public BucketingSink<T> setValidLengthPrefix(String validLengthPrefix)
{    this.validLengthPrefix = validLengthPrefix;    return this;}
0
protected ResourceInstance createPrivilegeResource(String privilegeId)
{    return createResource(Resource.Type.AmbariPrivilege, Collections.singletonMap(Resource.Type.AmbariPrivilege, privilegeId));}
0
protected RouteBuilder createRouteBuilder() throws Exception
{    return new RouteBuilder() {        @Override        public void configure() throws Exception {            from("file:target/xquery?moveFailed=error&move=ok").onException(Exception.class).to("mock:error").end().to("xquery:org/apache/camel/component/xquery/myTransform.xquery").to("mock:result");        }    };}
0
public Resource getMax()
{    if (!effMaxRes.equals(Resources.none())) {        return Resources.clone(effMaxRes);    }    return Resources.multiply(totalPartitionResource, absMaxCapacity);}
0
private DatabaseClient getDatabaseClient()
{    return spanner.getDatabaseClient(DatabaseId.of(project, options.getInstanceId(), databaseName));}
0
public Endpoint getFrom()
{    return null;}
0
public void testExactlyOnceRegularSink() throws Exception
{}
0
public void unsetTblName()
{    this.tblName = null;}
0
public void nodeSelectionChanged(List<String> selectedNodes)
{    this.metaDataPanel.removeAll();    if (selectedNodes.size() > 0) {        this.selectedNode = selectedNodes.get(0);        SwingWorker<Map<String, String>, Void> worker = new SwingWorker<Map<String, String>, Void>() {            @Override            protected Map<String, String> doInBackground() throws Exception {                return NodeViewerMetaData.this.zooInspectorManager.getNodeMeta(NodeViewerMetaData.this.selectedNode);            }            @Override            protected void done() {                Map<String, String> data = null;                try {                    data = get();                } catch (InterruptedException e) {                    data = new HashMap<String, String>();                    LoggerFactory.getLogger().error("Error retrieving meta data for node: " + NodeViewerMetaData.this.selectedNode, e);                } catch (ExecutionException e) {                    data = new HashMap<String, String>();                    LoggerFactory.getLogger().error("Error retrieving meta data for node: " + NodeViewerMetaData.this.selectedNode, e);                }                NodeViewerMetaData.this.metaDataPanel.setLayout(new GridBagLayout());                JPanel infoPanel = new JPanel();                infoPanel.setBackground(Color.WHITE);                infoPanel.setLayout(new GridBagLayout());                int i = 0;                int rowPos = 0;                for (Map.Entry<String, String> entry : data.entrySet()) {                    rowPos = 2 * i + 1;                    JLabel label = new JLabel(entry.getKey());                    JTextField text = new JTextField(entry.getValue());                    text.setEditable(false);                    GridBagConstraints c1 = new GridBagConstraints();                    c1.gridx = 0;                    c1.gridy = rowPos;                    c1.gridwidth = 1;                    c1.gridheight = 1;                    c1.weightx = 0;                    c1.weighty = 0;                    c1.anchor = GridBagConstraints.WEST;                    c1.fill = GridBagConstraints.HORIZONTAL;                    c1.insets = new Insets(5, 5, 5, 5);                    c1.ipadx = 0;                    c1.ipady = 0;                    infoPanel.add(label, c1);                    GridBagConstraints c2 = new GridBagConstraints();                    c2.gridx = 2;                    c2.gridy = rowPos;                    c2.gridwidth = 1;                    c2.gridheight = 1;                    c2.weightx = 0;                    c2.weighty = 0;                    c2.anchor = GridBagConstraints.WEST;                    c2.fill = GridBagConstraints.HORIZONTAL;                    c2.insets = new Insets(5, 5, 5, 5);                    c2.ipadx = 0;                    c2.ipady = 0;                    infoPanel.add(text, c2);                    i++;                }                GridBagConstraints c = new GridBagConstraints();                c.gridx = 1;                c.gridy = rowPos;                c.gridwidth = 1;                c.gridheight = 1;                c.weightx = 1;                c.weighty = 1;                c.anchor = GridBagConstraints.NORTHWEST;                c.fill = GridBagConstraints.NONE;                c.insets = new Insets(5, 5, 5, 5);                c.ipadx = 0;                c.ipady = 0;                NodeViewerMetaData.this.metaDataPanel.add(infoPanel, c);                NodeViewerMetaData.this.metaDataPanel.revalidate();                NodeViewerMetaData.this.metaDataPanel.repaint();            }        };        worker.execute();    }}
0
public void testCoderIsSerializableWithWellKnownCoderType() throws Exception
{    CoderProperties.coderSerializable(ListCoder.of(GlobalWindow.Coder.INSTANCE));}
0
 Object postProcessAfterInitialization(Object bean, String beanName) throws Exception
{    return bean;}
0
 ActiveMQEndpointConsumerBuilder autoStartup(boolean autoStartup)
{    doSetProperty("autoStartup", autoStartup);    return this;}
0
public TopologyPageInfo getTopologyPageInfo(String id, String window, boolean isIncludeSys) throws NotAliveException, AuthorizationException, TException
{        throw new RuntimeException("NOT IMPLEMENTED YET");}
0
private void getLeafTasks(List<Task<? extends Serializable>> rootTasks, Set<Task<? extends Serializable>> leaves)
{    for (Task<? extends Serializable> root : rootTasks) {        getLeafTasks(root, leaves);    }}
0
public synchronized Pair<NavigableMap<byte[], List<S>>, List<S>> getMedianParams()
{    List<S> l = new ArrayList<>(2);    l.add(sumVal);    l.add(sumWeights);    Pair<NavigableMap<byte[], List<S>>, List<S>> p = new Pair<>(map, l);    return p;}
0
public void printException(Exception e)
{    out.println("ERROR: " + e);    e.printStackTrace(out);}
0
protected void executeFunctionOnGroups(Object function, Object args, String[] groups, boolean allMembers, Function functionObject, ServerToClientFunctionResultSender resultSender, boolean ignoreFailedMembers)
{    throw new InternalGemFireError();}
0
protected ObjectName assertRegisteredObjectName(String name) throws MalformedObjectNameException, NullPointerException
{    ObjectName objectName = new ObjectName(name);    if (mbeanServer.isRegistered(objectName)) {            } else {        fail("Could not find MBean!: " + objectName);    }    return objectName;}
1
private static void removeDuplicateEntries(ArrayList<LinkArgs> all, ArrayList<LinkArgs> duplicates)
{        TreeMap<Long, List<LinkArgs>> highestGenstamps = new TreeMap<Long, List<LinkArgs>>();    for (LinkArgs duplicate : duplicates) {        if (!Block.isMetaFilename(duplicate.src.getName())) {            continue;        }        long blockId = Block.getBlockId(duplicate.src.getName());        List<LinkArgs> prevHighest = highestGenstamps.get(blockId);        if (prevHighest == null) {            List<LinkArgs> highest = new LinkedList<LinkArgs>();            highest.add(duplicate);            highestGenstamps.put(blockId, highest);            continue;        }        long prevGenstamp = Block.getGenerationStamp(prevHighest.get(0).src.getName());        long genstamp = Block.getGenerationStamp(duplicate.src.getName());        if (genstamp < prevGenstamp) {            continue;        }        if (genstamp > prevGenstamp) {            prevHighest.clear();        }        prevHighest.add(duplicate);    }        for (Iterator<LinkArgs> iter = duplicates.iterator(); iter.hasNext(); ) {        LinkArgs duplicate = iter.next();        long blockId = Block.getBlockId(duplicate.src.getName());        List<LinkArgs> highest = highestGenstamps.get(blockId);        if (highest != null) {            boolean found = false;            for (LinkArgs high : highest) {                if (high.src.getParent().equals(duplicate.src.getParent())) {                    found = true;                    break;                }            }            if (!found) {                                iter.remove();            }        }    }                TreeMap<Long, LinkArgs> longestBlockFiles = new TreeMap<Long, LinkArgs>();    for (LinkArgs duplicate : duplicates) {        if (Block.isMetaFilename(duplicate.src.getName())) {            continue;        }        long blockId = Block.getBlockId(duplicate.src.getName());        LinkArgs prevLongest = longestBlockFiles.get(blockId);        if (prevLongest == null) {            longestBlockFiles.put(blockId, duplicate);            continue;        }        long blockLength = duplicate.src.length();        long prevBlockLength = prevLongest.src.length();        if (blockLength < prevBlockLength) {                        continue;        }        if (blockLength > prevBlockLength) {                    }        longestBlockFiles.put(blockId, duplicate);    }        for (Iterator<LinkArgs> iter = all.iterator(); iter.hasNext(); ) {        LinkArgs args = iter.next();        long blockId = Block.getBlockId(args.src.getName());        LinkArgs bestDuplicate = longestBlockFiles.get(blockId);        if (bestDuplicate == null) {                        continue;        }        if (!bestDuplicate.src.getParent().equals(args.src.getParent())) {                        iter.remove();        }    }}
1
public void beforeAggregationsAndGroupByShouldBeCalledForAggregateFunctions() throws Exception
{    List<String> queries = Arrays.asList("SELECT MIN(pf.ID) FROM /portfolio pf WHERE pf.ID > 0", "SELECT pf.status, MIN(pf.ID) FROM /portfolio pf WHERE pf.ID > 0 GROUP BY pf.status", "SELECT pf.status, MIN(pf.ID) FROM /portfolio pf WHERE pf.ID > 0 GROUP BY pf.status", "SELECT MAX(pf.ID) FROM /portfolio pf WHERE pf.ID > 0", "SELECT pf.status, MAX(pf.ID) FROM /portfolio pf WHERE pf.ID > 0 GROUP BY pf.status", "SELECT AVG(pf.ID) FROM /portfolio pf WHERE pf.ID > 0", "SELECT pf.status, AVG(pf.ID) FROM /portfolio pf WHERE pf.ID > 0 GROUP BY pf.status", "SELECT pf.status, AVG(DISTINCT pf.ID) FROM /portfolio pf WHERE pf.ID > 0 GROUP BY pf.status", "SELECT SUM(pf.ID) FROM /portfolio pf WHERE pf.ID > 0", "SELECT pf.status, SUM(pf.ID) FROM /portfolio pf WHERE pf.ID > 0 GROUP BY pf.status", "SELECT pf.status, SUM(DISTINCT pf.ID) FROM /portfolio pf WHERE pf.ID > 0 GROUP BY pf.status", "SELECT COUNT(pf.ID) FROM /portfolio pf WHERE pf.ID > 0", "SELECT pf.status, COUNT(pf.ID) FROM /portfolio pf WHERE pf.ID > 0 GROUP BY pf.status", "SELECT pf.status, COUNT(DISTINCT pf.ID) FROM /portfolio pf WHERE pf.ID > 0 GROUP BY pf.status", "SELECT MIN(pf.ID), MAX(pf.ID), AVG(pf.ID), SUM(pf.ID), COUNT(pf.ID) FROM /portfolio pf WHERE pf.ID > 0", "SELECT pf.status, MIN(pf.ID), MAX(pf.ID), AVG(pf.ID), SUM(pf.ID), COUNT(pf.ID) FROM /portfolio pf WHERE pf.ID > 0 GROUP BY pf.status", "SELECT pf.status, MIN(pf.ID), MAX(pf.ID), AVG(DISTINCT pf.ID), SUM(DISTINCT pf.ID), COUNT(DISTINCT pf.ID) FROM /portfolio pf WHERE pf.ID > 0 GROUP BY pf.status");    MyQueryObserverImpl myQueryObserver = spy(new MyQueryObserverImpl());    QueryObserverHolder.setInstance(myQueryObserver);    for (String queryString : queries) {        Query query = queryService.newQuery(queryString);        query.execute();    }    verify(myQueryObserver, times(queries.size())).beforeAggregationsAndGroupBy(any());}
0
public void testNoMatching() throws Exception
{    TaildirMatcher tm = new TaildirMatcher("nomatch", tmpDir.getAbsolutePath() + File.separator + "abracadabra_nonexisting", isCachingNeeded);    List<File> files = tm.getMatchingFiles();    assertNotNull(msgNoMatch, files);    assertTrue(msgNoMatch, files.isEmpty());}
0
public deleteTable_result deepCopy()
{    return new deleteTable_result(this);}
0
public Object getAnalyticsRequestManager()
{    return this._analyticsRequestManager;}
0
public boolean getPdxReadSerializedByAnyGemFireServices()
{    TypeRegistry pdxRegistry = getPdxRegistry();    boolean pdxReadSerializedOverriden = false;    if (pdxRegistry != null) {        pdxReadSerializedOverriden = pdxRegistry.getPdxReadSerializedOverride();    }    return (getPdxReadSerialized() || pdxReadSerializedOverriden) && PdxInstanceImpl.getPdxReadSerialized();}
0
private void restart() throws YarnException, IOException, InterruptedException
{        isFinished = false;    pendingFailedMaps.clear();    pendingMaps.clear();    pendingReduces.clear();    pendingFailedReduces.clear();        int added = 0;    for (ContainerSimulator cs : allMaps) {        if (added >= mapTotal - mapFinished) {            break;        }        pendingMaps.add(cs);    }        added = 0;    for (ContainerSimulator cs : allReduces) {        if (added >= reduceTotal - reduceFinished) {            break;        }        pendingReduces.add(cs);    }    amContainer = null;}
0
public Map<String, Object> next() throws IOException
{    if (arraySize == 0)        return null;    Object o = readVal(fis);    arraySize--;    if (o == END_OBJ)        return null;    return (Map<String, Object>) o;}
0
public void setNumberOfHits(int value)
{    this.numberOfHits = value;}
0
public void toData(DataOutput out)
{}
0
public T visitFilter(Filter filter, X value) throws E
{    return visitOp(filter, value);}
0
public Collection<ModuleDefinition> getChildren()
{    return children.values();}
0
public boolean equals(Object o)
{    if (!(o instanceof FieldCacheSource))        return false;    FieldCacheSource other = (FieldCacheSource) o;    return this.field.equals(other.field);}
0
public void memberCrashed(ClientMembershipEvent event)
{    Notification notification = new Notification(JMXNotificationType.CLIENT_CRASHED, serverSource, SequenceNumber.next(), System.currentTimeMillis(), ManagementConstants.CLIENT_CRASHED_PREFIX + event.getMemberId());    serverLevelNotifEmitter.sendNotification(notification);    memberLevelNotifEmitter.sendNotification(notification);}
0
public void registerJavaBean(TypeDescriptor<T> typeDescriptor)
{    registerSchemaProvider(typeDescriptor, new JavaBeanSchema());}
0
public void setStatsLastRefreshed(Date statsLastRefreshed)
{    this.statsLastRefreshed = statsLastRefreshed;}
0
public short[] getPartitionKeyBindVariableIndexes()
{    return bindVariables.getPartitionKeyBindVariableIndexes(table);}
0
public Builder clearActiveLogs()
{    if (activeLogsBuilder_ == null) {        activeLogs_ = java.util.Collections.emptyList();        bitField0_ = (bitField0_ & ~0x00000010);        onChanged();    } else {        activeLogsBuilder_.clear();    }    return this;}
0
public String getPhysicalByBridgeName(String name) throws Ovm3ResourceException
{    return getInterfaceByName(name).getPhysical();}
0
public CreateTopicsRequest build(short version)
{    if (data.validateOnly() && version == 0)        throw new UnsupportedVersionException("validateOnly is not supported in version 0 of " + "CreateTopicsRequest");    final List<String> topicsWithDefaults = data.topics().stream().filter(topic -> topic.assignments().isEmpty()).filter(topic -> topic.numPartitions() == CreateTopicsRequest.NO_NUM_PARTITIONS || topic.replicationFactor() == CreateTopicsRequest.NO_REPLICATION_FACTOR).map(CreatableTopic::name).collect(Collectors.toList());    if (!topicsWithDefaults.isEmpty() && version < 4) {        throw new UnsupportedVersionException("Creating topics with default " + "partitions/replication factor are only supported in CreateTopicRequest " + "version 4+. The following topics need values for partitions and replicas: " + topicsWithDefaults);    }    return new CreateTopicsRequest(data, version);}
0
public TokenStreamComponents createComponents(String fieldName)
{    return new TokenStreamComponents(new MockTokenizer(), tokenStream);}
0
public String getLocation()
{    return location;}
0
public boolean matches(AbstractRequest body)
{    FetchRequest fetch = (FetchRequest) body;    return fetch.fetchData().containsKey(tp) && fetch.fetchData().get(tp).fetchOffset == offset;}
0
 WeatherEndpointBuilder weatherApi(String weatherApi)
{    doSetProperty("weatherApi", weatherApi);    return this;}
0
protected boolean hasPendingRequests()
{    return !pending.isEmpty();}
0
public CamelClusterMember getLocalMember()
{    return localMember;}
0
public alltypes_args setV12(java.util.Map<java.lang.String, java.lang.Long> v12)
{    this.v12 = v12;    return this;}
0
public void teardown()
{}
0
public static _Fields findByThriftId(int fieldId)
{    switch(fieldId) {        case         0:            return SUCCESS;        case         1:            return O1;        case         2:            return O2;        default:            return null;    }}
0
public void testDataReadFromNonexistentFile()
{    String cmdLine = "curl 'https://www.w3schools.com/html/tryit.asp?filename=tryhtml_form_submit/action_page.php' " + "-H 'cache-control: no-cache' --data '@test.txt' ";    BasicCurlParser basicCurlParser = new BasicCurlParser();    basicCurlParser.parse(cmdLine);    Assert.fail("The method 'translateCommandline shouldn't run when the path of file is incorrect");}
0
public void configure(URI uri)
{    String value = uri.getHost();    if (value != null) {        setHost(value);    }    if (!isIgnoreUriScheme()) {        String scheme = uri.getScheme();        if (scheme != null) {            setProtocol(scheme);        }    }    String userInfo = uri.getUserInfo();    if (userInfo != null) {        String[] parts = uri.getUserInfo().split(":");        if (parts.length == 2) {            setUsername(parts[0]);            setPassword(parts[1]);        } else {            setUsername(userInfo);        }    }    int port = uri.getPort();    if (port > 0) {        setPort(port);    } else if (this.port <= 0) {                setPort(MailUtils.getDefaultPortForProtocol(uri.getScheme()));    }}
0
private void initializeReplicateRegion(InternalCache cache)
{    cache.createRegionFactory(RegionShortcut.REPLICATE).setStatisticsEnabled(true).setSubscriptionAttributes(new SubscriptionAttributes(InterestPolicy.ALL)).create("subscriptionRegion");}
0
public void testClear() throws IOException
{    for (int i = 0; i < 1000; i++) {        estimator.insert(i);    }    estimator.clear();    assertEquals(estimator.getCount(), 0);    assertEquals(estimator.getSampleCount(), 0);    assertNull(estimator.snapshot());}
0
public void shouldDelegateToUnderlyingStoreWhenFetchingRange()
{    EasyMock.expect(inner.fetch(bytesKey, bytesKey, 0, 1)).andReturn(KeyValueIterators.emptyIterator());    init();    store.fetch(bytesKey, bytesKey, ofEpochMilli(0), ofEpochMilli(1));    EasyMock.verify(inner);}
0
public void testAclString()
{    AccessControlList acl;    acl = new AccessControlList("*");    assertEquals("All users are allowed", acl.toString());    validateGetAclString(acl);    acl = new AccessControlList(" ");    assertEquals("No users are allowed", acl.toString());    acl = new AccessControlList("user1,user2");    assertEquals("Users [user1, user2] are allowed", acl.toString());    validateGetAclString(acl);        acl = new AccessControlList("user1,user2 ");    assertEquals("Users [user1, user2] are allowed", acl.toString());    validateGetAclString(acl);    acl = new AccessControlList(" group1,group2");    assertTrue(acl.toString().equals("Members of the groups [group1, group2] are allowed"));    validateGetAclString(acl);    acl = new AccessControlList("user1,user2 group1,group2");    assertTrue(acl.toString().equals("Users [user1, user2] and " + "members of the groups [group1, group2] are allowed"));    validateGetAclString(acl);}
0
public String asSql(boolean prefix, boolean suffix, SqlDialect dialect)
{    StringBuilder ret = new StringBuilder();    dialect.quoteStringLiteral(ret, prefix ? charsetName : null, getValue());        if (false) {        if (suffix && (null != collation)) {            ret.append(" ");            ret.append(collation.toString());        }    }    return ret.toString();}
0
public static byte[] toBytes(long value)
{    Buffer buffer = new Buffer(8);    buffer.bigEndianEditor().writeLong(value);    return buffer.data;}
0
private boolean hasErrors(List<SettingsProblem> problems)
{    if (problems != null) {        for (SettingsProblem problem : problems) {            if (SettingsProblem.Severity.ERROR.compareTo(problem.getSeverity()) >= 0) {                return true;            }        }    }    return false;}
0
 SftpEndpointConsumerBuilder idempotent(String idempotent)
{    doSetProperty("idempotent", idempotent);    return this;}
0
public Result ping()
{    return CommandResult.createInfo("pong");}
0
public void visitDivAssign(GroovySourceAST t, int visit)
{    visitDefault(t, visit);}
0
public boolean isReadyToBeClosed()
{    return true;}
0
private void putTopic(List<String> events) throws Exception
{    ConnectionFactory factory = new ActiveMQConnectionFactory(jmsUserName, jmsPassword, BROKER_BIND_URL);    Connection connection = factory.createConnection();    connection.start();    Session session = connection.createSession(true, Session.AUTO_ACKNOWLEDGE);    Destination destination = session.createTopic(DESTINATION_NAME);    MessageProducer producer = session.createProducer(destination);    for (String event : events) {        TextMessage message = session.createTextMessage();        message.setText(event);        producer.send(message);    }    session.commit();    session.close();    connection.close();}
0
public void setStatusIfAbsent(String topologyId, String statusMessage)
{    assertValidTopologyForModification(topologyId);    status.putIfAbsent(topologyId, statusMessage);}
0
public boolean isNull()
{    return false;}
0
public void handle(Event event)
{}
0
 void handleException(Throwable exception)
{        if (exception instanceof AmqpProtocolException) {        onAMQPException((IOException) exception);    } else {        try {                        sendToActiveMQ(new ShutdownInfo());            amqpTransport.stop();        } catch (Throwable e) {                    }    }}
1
protected final T initialValue()
{    return initialValue;}
0
public long getSize()
{    return size;}
0
private static Properties getClientProperties(boolean durable)
{    Properties props = new Properties();    props.setProperty(MCAST_PORT, "0");    props.setProperty(LOCATORS, "");    return props;}
0
 static String encodeToString(Cursor info, Version version, ZoneId zoneId)
{    if (info == Cursor.EMPTY) {        return StringUtils.EMPTY;    }    try (SqlStreamOutput output = new SqlStreamOutput(version, zoneId)) {        output.writeNamedWriteable(info);        output.close();                return output.streamAsString();    } catch (IOException ex) {        throw new SqlIllegalArgumentException("Unexpected failure retrieving next page", ex);    }}
0
public Builder id(final long id)
{    bId = id;    return this;}
0
private int readCompressedDim(IndexInput in) throws IOException
{    int compressedDim = in.readByte();    if (compressedDim < -2 || compressedDim >= numDataDims || (version < BKDWriter.VERSION_LOW_CARDINALITY_LEAVES && compressedDim == -2)) {        throw new CorruptIndexException("Got compressedDim=" + compressedDim, in);    }    return compressedDim;}
0
public void testPushAggregateFunctionsThroughJoin()
{    final HepProgram preProgram = new HepProgramBuilder().addRuleInstance(AggregateProjectMergeRule.INSTANCE).build();    final HepProgram program = new HepProgramBuilder().addRuleInstance(AggregateJoinTransposeRule.EXTENDED).build();    final String sql = "select e.job,\n" + "  min(sal) as min_sal, min(e.deptno) as min_deptno,\n" + "  sum(sal) + 1 as sum_sal_plus, max(sal) as max_sal,\n" + "  sum(sal) as sum_sal_2, count(sal) as count_sal,\n" + "  count(mgr) as count_mgr\n" + "from sales.emp as e\n" + "join sales.dept as d on e.job = d.name\n" + "group by e.job,d.name";    checkPlanning(tester, preProgram, new HepPlanner(program), sql);}
0
public StateMapView<N, UK, UV> getStateMapView(String stateName, MapViewTypeInfo<UK, UV> mapViewTypeInfo) throws Exception
{    MapStateDescriptor<UK, UV> mapStateDescriptor = new MapStateDescriptor<>(stateName, mapViewTypeInfo.getKeyType(), mapViewTypeInfo.getValueType());    MapState<UK, UV> mapState = ctx.getMapState(mapStateDescriptor);    if (mapViewTypeInfo.isNullAware()) {        ValueStateDescriptor<UV> nullStateDescriptor = new ValueStateDescriptor<>(stateName + NULL_STATE_POSTFIX, mapViewTypeInfo.getValueType());        ValueState<UV> nullState = ctx.getState(nullStateDescriptor);        return new StateMapView.KeyedStateMapViewWithKeysNullable<>(mapState, nullState);    } else {        return new StateMapView.KeyedStateMapViewWithKeysNotNull<>(mapState);    }}
0
private PartitionMemberInfoImpl buildDetails(InternalDistributedMember id, float weight, long localMaxMemory, long[] loads, long[] primaryLoads)
{    PRLoad prLoad = new PRLoad(loads.length, weight);    int size = 0;    int primaryCount = 0;    int bucketCount = 0;    long[] bucketSizes = new long[loads.length];    for (int bucketId = 0; bucketId < loads.length; bucketId++) {        prLoad.addBucket(bucketId, loads[bucketId], primaryLoads[bucketId]);        bucketSizes[bucketId] = loads[bucketId];        size += bucketSizes[bucketId];        if (loads[bucketId] != 0) {            bucketCount++;        }        if (primaryLoads[bucketId] != 0) {            primaryCount++;        }    }    return new PartitionMemberInfoImpl(id, localMaxMemory, size, bucketCount, primaryCount, prLoad, bucketSizes);}
0
public boolean readBoolean()
{    throw new UnsupportedOperationException();}
0
public TypeInformation<C> getComponentInfo()
{    return componentInfo;}
0
public Object next()
{    Object answer = next;    if (answer == null) {        answer = reader.read();                if (forceNext != null) {            answer = forceNext;            forceNext = null;        }    } else {        next = reader.read();                if (forceNext != null) {            next = forceNext;            forceNext = null;        }    }    return answer;}
0
public boolean recv_grant_privileges() throws MetaException, org.apache.thrift.TException
{    grant_privileges_result result = new grant_privileges_result();    receiveBase(result, "grant_privileges");    if (result.isSetSuccess()) {        return result.success;    }    if (result.o1 != null) {        throw result.o1;    }    throw new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.MISSING_RESULT, "grant_privileges failed: unknown result");}
0
public void testIsMemberAttributeBaseDn_withDifferentUserAndGroupNameAttribute()
{        final Provider<AmbariLdapConfiguration> configurationProvider = createNiceMock(Provider.class);    AmbariLdapConfiguration configuration = createNiceMock(AmbariLdapConfiguration.class);    Users users = createNiceMock(Users.class);    LdapServerProperties ldapServerProperties = createNiceMock(LdapServerProperties.class);    expect(configurationProvider.get()).andReturn(configuration).anyTimes();    expect(configuration.getLdapServerProperties()).andReturn(ldapServerProperties).anyTimes();    expect(ldapServerProperties.getUsernameAttribute()).andReturn("sAMAccountName");    expect(ldapServerProperties.getGroupNamingAttr()).andReturn("groupOfNames");    replay(configurationProvider, configuration, users, ldapServerProperties);        AmbariLdapDataPopulatorTestInstance populator = new AmbariLdapDataPopulatorTestInstance(configurationProvider, users);    boolean result = populator.isMemberAttributeBaseDn("cn=mygroupname,OU=myOrganizationUnit,DC=apache,DC=org");        assertTrue(result);}
0
public boolean expunge(Long id)
{    TransactionLegacy txn = TransactionLegacy.currentTxn();    txn.start();    SecurityGroupVO entry = findById(id);    if (entry != null) {        _tagsDao.removeByIdAndType(id, ResourceObjectType.SecurityGroup);    }    boolean result = super.expunge(id);    txn.commit();    return result;}
0
public int tightMarshal1(OpenWireFormat wireFormat, Object o, BooleanStream bs) throws IOException
{    DestinationInfo info = (DestinationInfo) o;    int rc = super.tightMarshal1(wireFormat, o, bs);    rc += tightMarshalCachedObject1(wireFormat, (DataStructure) info.getConnectionId(), bs);    rc += tightMarshalCachedObject1(wireFormat, (DataStructure) info.getDestination(), bs);    rc += tightMarshalLong1(wireFormat, info.getTimeout(), bs);    rc += tightMarshalObjectArray1(wireFormat, info.getBrokerPath(), bs);    return rc + 1;}
0
public Object convert(Object input)
{    if (input == null) {        return null;    }    Object inputFieldValue = inputOI.getField(input);    byte inputFieldTag = inputOI.getTag(input);    Object outputFieldValue = null;    int inputFieldTagIndex = ((Byte) inputFieldTag).intValue();    if (inputFieldTagIndex >= 0 && inputFieldTagIndex < fieldConverters.size()) {        outputFieldValue = fieldConverters.get(inputFieldTagIndex).convert(inputFieldValue);    }    outputOI.setFieldAndTag(output, outputFieldValue, inputFieldTag);    return output;}
0
private static RecordPathFilter createFilter(final Tree operatorTree, final RecordPathSegment parent, final boolean absolute)
{    switch(operatorTree.getType()) {        case EQUAL:            return createBinaryOperationFilter(operatorTree, parent, EqualsFilter::new, absolute);        case NOT_EQUAL:            return createBinaryOperationFilter(operatorTree, parent, NotEqualsFilter::new, absolute);        case LESS_THAN:            return createBinaryOperationFilter(operatorTree, parent, LessThanFilter::new, absolute);        case LESS_THAN_EQUAL:            return createBinaryOperationFilter(operatorTree, parent, LessThanOrEqualFilter::new, absolute);        case GREATER_THAN:            return createBinaryOperationFilter(operatorTree, parent, GreaterThanFilter::new, absolute);        case GREATER_THAN_EQUAL:            return createBinaryOperationFilter(operatorTree, parent, GreaterThanOrEqualFilter::new, absolute);        case FUNCTION:            return createFunctionFilter(operatorTree, absolute);        default:            throw new RecordPathException("Expected an Expression of form <value> <operator> <value> to follow '[' Token but found " + operatorTree);    }}
0
public ClusterManager clusterManager()
{    return Mockito.mock(ClusterManager.class);}
0
public void addRegionListener(final RegionListener regionListener)
{    throw new UnsupportedOperationException("Should not be invoked");}
0
public void setProcessorId(String processorId)
{    this.processorId = processorId;}
0
private static String getFileType(OplogFile olf)
{    String name = olf.f.getName();    int index = name.lastIndexOf('.');    return name.substring(index + 1);}
0
public void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException
{    int sum = 0;    for (IntWritable value : values) {        sum += value.get();    }    context.write(new AvroWrapper<Pair<CharSequence, Integer>>(new Pair<CharSequence, Integer>(key.toString(), sum)), NullWritable.get());}
0
public ArchivedExecutionVertexBuilder setTaskNameWithSubtask(String taskNameWithSubtask)
{    this.taskNameWithSubtask = taskNameWithSubtask;    return this;}
0
private void cleanupDetachDir(File detachDir) throws IOException
{    if (!DataNodeLayoutVersion.supports(LayoutVersion.Feature.APPEND_RBW_DIR, layoutVersion) && detachDir.exists() && detachDir.isDirectory()) {        if (FileUtil.list(detachDir).length != 0) {            throw new IOException("Detached directory " + detachDir + " is not empty. Please manually move each file under this " + "directory to the finalized directory if the finalized " + "directory tree does not have the file.");        } else if (!detachDir.delete()) {            throw new IOException("Cannot remove directory " + detachDir);        }    }}
0
public void run()
{    getCache();}
0
protected Query newTermQuery(Term term, TermStates termStates)
{    Query query = super.newTermQuery(term, termStates);    if (term.text().equals("universe")) {        query = new BoostQuery(query, 100f);    }    return query;}
0
public void verifyCanDisable()
{    verifyCanDisable(Collections.emptySet());}
0
private static int getInt(byte[] b, int offset)
{    int value = 0;    for (int i = offset, j = 24; j >= 0; i++, j -= 8) {        value |= (b[i] & 0xFF) << j;    }    return value;}
0
public Employee getCurrentValue()
{    return currentValue;}
0
public boolean equals(Object o)
{    if (this == o)        return true;    if (o == null || getClass() != o.getClass())        return false;    FixedFieldIntervalsSource that = (FixedFieldIntervalsSource) o;    return Objects.equals(field, that.field) && Objects.equals(source, that.source);}
0
public ListIterator<Object> listIterator()
{    return new UnmodifiableListIterator(data.listIterator());}
0
public UserAuthenticationEntity merge(UserAuthenticationEntity entity)
{    return entityManagerProvider.get().merge(entity);}
0
public S3ResponseMetadata getCachedResponseMetadata(AmazonWebServiceRequest request)
{    throw new UnsupportedOperationException();}
0
 static String getColumnNamesProperty()
{    return "field";}
0
public FileSystem getFileSystem() throws Exception
{    return getBasePath().getFileSystem();}
0
public boolean equals(Object obj)
{    if (obj instanceof TestNonSizerObject) {        TestNonSizerObject other = (TestNonSizerObject) obj;        if (StringUtils.equals(this.testString, other.testString)) {            return true;        }    }    return false;}
0
protected void serviceInit(Configuration conf) throws Exception
{    this.setIntervalMs(conf.getTimeDuration(RBFConfigKeys.DFS_ROUTER_CACHE_TIME_TO_LIVE_MS, RBFConfigKeys.DFS_ROUTER_CACHE_TIME_TO_LIVE_MS_DEFAULT, TimeUnit.MILLISECONDS));    super.serviceInit(conf);}
0
public boolean equals(Object o)
{    if (this == o)        return true;    if (o == null || getClass() != o.getClass())        return false;    ResolvedKerberosPrincipal principal1 = (ResolvedKerberosPrincipal) o;    return isService == principal1.isService && Objects.equal(hostId, principal1.hostId) && Objects.equal(hostName, principal1.hostName) && Objects.equal(principal, principal1.principal) && Objects.equal(cacheFile, principal1.cacheFile) && Objects.equal(serviceMapping, principal1.serviceMapping) && Objects.equal(keytabPath, principal1.keytabPath);}
0
public void testNoShardRelocationsOccurWhenElectedMasterNodeFails() throws Exception
{    Settings masterNodeSettings = Settings.builder().put(Node.NODE_DATA_SETTING.getKey(), false).build();    internalCluster().startNodes(2, masterNodeSettings);    Settings dateNodeSettings = Settings.builder().put(Node.NODE_MASTER_SETTING.getKey(), false).build();    internalCluster().startNodes(2, dateNodeSettings);    ClusterHealthResponse clusterHealthResponse = client().admin().cluster().prepareHealth().setWaitForEvents(Priority.LANGUID).setWaitForNodes("4").setWaitForNoRelocatingShards(true).get();    assertThat(clusterHealthResponse.isTimedOut(), is(false));    createIndex("test");    ensureSearchable("test");    RecoveryResponse r = client().admin().indices().prepareRecoveries("test").get();    int numRecoveriesBeforeNewMaster = r.shardRecoveryStates().get("test").size();    final String oldMaster = internalCluster().getMasterName();    internalCluster().stopCurrentMasterNode();    assertBusy(() -> {        String current = internalCluster().getMasterName();        assertThat(current, notNullValue());        assertThat(current, not(equalTo(oldMaster)));    });    ensureSearchable("test");    r = client().admin().indices().prepareRecoveries("test").get();    int numRecoveriesAfterNewMaster = r.shardRecoveryStates().get("test").size();    assertThat(numRecoveriesAfterNewMaster, equalTo(numRecoveriesBeforeNewMaster));}
0
public String getName()
{    return name;}
0
private Response createOrUpdate(final NamespacesInstanceModel model, final UriInfo uriInfo, final Admin admin, final boolean updateExisting)
{    NamespaceDescriptor.Builder builder = NamespaceDescriptor.create(namespace);    builder.addConfiguration(model.getProperties());    if (model.getProperties().size() > 0) {        builder.addConfiguration(model.getProperties());    }    NamespaceDescriptor nsd = builder.build();    try {        if (updateExisting) {            admin.modifyNamespace(nsd);        } else {            admin.createNamespace(nsd);        }    } catch (IOException e) {        servlet.getMetrics().incrementFailedPutRequests(1);        return processException(e);    }    servlet.getMetrics().incrementSucessfulPutRequests(1);    return updateExisting ? Response.ok(uriInfo.getAbsolutePath()).build() : Response.created(uriInfo.getAbsolutePath()).build();}
0
 ColRef column(int offset)
{    if (offset < leftCount) {        return left.column(offset);    } else {        return right.column(offset - leftCount);    }}
0
 AdvancedNettyEndpointProducerBuilder options(String options)
{    doSetProperty("options", options);    return this;}
0
protected Class<TestUserClassBase> getTypeClass()
{    return TestUserClassBase.class;}
0
public PollingConsumer createPollingConsumer() throws Exception
{    JpaPollingConsumer consumer = new JpaPollingConsumer(this);    consumer.setQuery(getQuery());    consumer.setNamedQuery(getNamedQuery());    consumer.setNativeQuery(getNativeQuery());    consumer.setLockModeType(getLockModeType());    consumer.setParameters(getParameters());    consumer.setResultClass(getResultClass());    return consumer;}
0
public void testUpdateResourcesAsServiceAdministrator() throws Exception
{    testUpdateResources(TestAuthenticationFactory.createServiceAdministrator());}
0
public void testBZip2CodecUsingAvroCodec()
{    CodecFactory avroBZip2Codec = CodecFactory.fromString("bzip2");    JobConf job = new JobConf();    job.set("mapred.output.compress", "true");    job.set(AvroJob.OUTPUT_CODEC, "bzip2");    CodecFactory factory = AvroOutputFormat.getCodecFactory(job);    assertNotNull(factory);    assertEquals(factory.getClass(), avroBZip2Codec.getClass());}
0
public void configure() throws Exception
{    from("direct:start").to("mock:results");}
0
private synchronized UUID prepareForReplacement() throws ConfigurationException
{    if (SystemKeyspace.bootstrapComplete())        throw new RuntimeException("Cannot replace address with a node that is already bootstrapped");    if (!joinRing)        throw new ConfigurationException("Cannot set both join_ring=false and attempt to replace a node");    if (!DatabaseDescriptor.isAutoBootstrap() && !Boolean.getBoolean("cassandra.allow_unsafe_replace"))        throw new RuntimeException("Replacing a node without bootstrapping risks invalidating consistency " + "guarantees as the expected data may not be present until repair is run. " + "To perform this operation, please restart with " + "-Dcassandra.allow_unsafe_replace=true");    InetAddressAndPort replaceAddress = DatabaseDescriptor.getReplaceAddress();        Map<InetAddressAndPort, EndpointState> epStates = Gossiper.instance.doShadowRound();        if (epStates.get(replaceAddress) == null)        throw new RuntimeException(String.format("Cannot replace_address %s because it doesn't exist in gossip", replaceAddress));    validateEndpointSnitch(epStates.values().iterator());    try {        VersionedValue tokensVersionedValue = epStates.get(replaceAddress).getApplicationState(ApplicationState.TOKENS);        if (tokensVersionedValue == null)            throw new RuntimeException(String.format("Could not find tokens for %s to replace", replaceAddress));        bootstrapTokens = TokenSerializer.deserialize(tokenMetadata.partitioner, new DataInputStream(new ByteArrayInputStream(tokensVersionedValue.toBytes())));    } catch (IOException e) {        throw new RuntimeException(e);    }    UUID localHostId = SystemKeyspace.getLocalHostId();    if (isReplacingSameAddress()) {        localHostId = Gossiper.instance.getHostId(replaceAddress, epStates);                SystemKeyspace.setLocalHostId(localHostId);    }    return localHostId;}
1
public Thread newThread(Runnable r)
{    Thread t = Executors.defaultThreadFactory().newThread(r);    t.setDaemon(true);    return t;}
0
public RelOptCost makeZeroCost()
{    return DrillCostBase.ZERO;}
0
protected RouteBuilder createRouteBuilder() throws Exception
{    return new RouteBuilder() {        @Override        public void configure() throws Exception {                        from("direct:start").to("mybatis:selectAllAccounts?statementType=SelectList").split(body()).to("mock:result");                }    };}
0
private void runTest(String testName, BloomType bloomType, byte[][][] hfileRanges) throws Exception
{    runTest(testName, bloomType, null, hfileRanges);}
0
public String getHostName()
{    return hostName;}
0
private static ClusterState addClosedIndex(final String index, final int numShards, final int numReplicas, final ClusterState state)
{    return addIndex(state, index, numShards, numReplicas, IndexMetaData.State.CLOSE, INDEX_CLOSED_BLOCK);}
0
public boolean isSet(_Fields field)
{    if (field == null) {        throw new IllegalArgumentException();    }    switch(field) {        case SUCCESS:            return isSetSuccess();    }    throw new IllegalStateException();}
0
public INPUT<T> $onmouseover(String value)
{    addAttr("onmouseover", value);    return this;}
0
public int totalOperations()
{    return totalHits;}
0
public int getNodeCount(NodeState nodestate)
{    return scmNodeManager.getNodeCount(nodestate);}
0
private static List<GeoPolygonFactory.PolygonDescription> convertToDescription(final Polygon... polygons)
{    final List<GeoPolygonFactory.PolygonDescription> descriptions = new ArrayList<>(polygons.length);    for (final Polygon polygon : polygons) {        final Polygon[] theHoles = polygon.getHoles();        final List<GeoPolygonFactory.PolygonDescription> holes = convertToDescription(theHoles);                final double[] polyLats = polygon.getPolyLats();        final double[] polyLons = polygon.getPolyLons();                final List<GeoPoint> points = new ArrayList<>(polyLats.length - 1);                for (int i = 0; i < polyLats.length - 1; i++) {            final int index = polyLats.length - 2 - i;            points.add(new GeoPoint(PlanetModel.WGS84, fromDegrees(polyLats[index]), fromDegrees(polyLons[index])));        }        descriptions.add(new GeoPolygonFactory.PolygonDescription(points, holes));    }    return descriptions;}
0
protected Snapshot retrieve()
{    return _snapshotDao.findById(_snapshotId);}
0
protected DirContext open() throws NamingException
{    if (context != null) {        return context;    }    try {        Hashtable<String, String> env = new Hashtable<String, String>();        env.put(Context.INITIAL_CONTEXT_FACTORY, initialContextFactory);        if (connectionUsername != null && !"".equals(connectionUsername)) {            env.put(Context.SECURITY_PRINCIPAL, connectionUsername);        } else {            throw new NamingException("Empty username is not allowed");        }        if (connectionPassword != null && !"".equals(connectionPassword)) {            env.put(Context.SECURITY_CREDENTIALS, connectionPassword);        } else {            throw new NamingException("Empty password is not allowed");        }        env.put(Context.SECURITY_PROTOCOL, connectionProtocol);        env.put(Context.PROVIDER_URL, connectionURL);        env.put(Context.SECURITY_AUTHENTICATION, authentication);        context = new InitialDirContext(env);    } catch (NamingException e) {                throw e;    }    return context;}
1
public Authentication getAuthentication()
{    try {        return Authentication.readFromContext(threadContext);    } catch (IOException e) {                                return null;    }}
1
private static double decodeZ(final int z)
{    return z * inverseZFactor + PlanetModel.WGS84.getMinimumZValue();}
0
private void do3SentenceTest(BreakIterator bi)
{    assertEquals(0, bi.current());    assertEquals(0, bi.first());    assertEquals(SENTENCES[0], TEXT.substring(bi.current(), bi.next()));    assertEquals(SENTENCES[1], TEXT.substring(bi.current(), bi.next()));    int current = bi.current();    assertEquals(bi.getText().getEndIndex(), bi.next());    int next = bi.current();    assertEquals(SENTENCES[2], TEXT.substring(current, next));    assertEquals(BreakIterator.DONE, bi.next());    assertEquals(TEXT.length(), bi.last());    int end = bi.current();    assertEquals(SENTENCES[2], TEXT.substring(bi.previous(), end));    end = bi.current();    assertEquals(SENTENCES[1], TEXT.substring(bi.previous(), end));    end = bi.current();    assertEquals(SENTENCES[0], TEXT.substring(bi.previous(), end));    assertEquals(BreakIterator.DONE, bi.previous());    assertEquals(0, bi.current());    assertEquals(59, bi.following(39));    assertEquals(59, bi.following(31));    assertEquals(31, bi.following(30));    assertEquals(0, bi.preceding(57));    assertEquals(0, bi.preceding(58));    assertEquals(31, bi.preceding(59));    assertEquals(0, bi.first());    assertEquals(59, bi.next(2));    assertEquals(0, bi.next(-2));}
0
public void setId(String id)
{    this.id = id;}
0
public WorkflowDefinitionFactory getWorkflowDefinitionFactory(WorkflowType workflowType)
{    WorkflowTypeRegistrationOptions registrationOptions = configuration.getWorkflowTypeRegistrationOptions() != null ? configuration.getWorkflowTypeRegistrationOptions() : new WorkflowTypeRegistrationOptions();    DataConverter dataConverter = configuration.getDataConverter() != null ? configuration.getDataConverter() : new JsonDataConverter();    return new CamelWorkflowDefinitionFactory(swfWorkflowConsumer, workflowType, registrationOptions, dataConverter);}
0
 void generateInputs(int[] opsPerThread) throws IOException
{    assert opsPerThread.length == numThreads : "Error opsPerThread.length";    clientProto.setSafeMode(HdfsConstants.SafeModeAction.SAFEMODE_LEAVE, false);            fileNames = new String[numThreads][];    for (int idx = 0; idx < numThreads; idx++) {        int threadOps = opsPerThread[idx];        fileNames[idx] = new String[threadOps];        for (int jdx = 0; jdx < threadOps; jdx++) fileNames[idx][jdx] = nameGenerator.getNextFileName("ThroughputBench");    }}
1
public long getResourceId()
{    return resourceId;}
0
public void doInTransactionWithoutResult(final TransactionStatus status)
{        final List<DataCenterIpAddressVO> privateIps = _privateIpAddressDao.listByPodIdDcId(podId, pod.getDataCenterId());    if (!privateIps.isEmpty()) {        if (!_privateIpAddressDao.deleteIpAddressByPod(podId)) {            throw new CloudRuntimeException("Failed to cleanup private ip addresses for pod " + podId);        }    }        final List<DataCenterLinkLocalIpAddressVO> localIps = _linkLocalIpAllocDao.listByPodIdDcId(podId, pod.getDataCenterId());    if (!localIps.isEmpty()) {        if (!_linkLocalIpAllocDao.deleteIpAddressByPod(podId)) {            throw new CloudRuntimeException("Failed to cleanup private ip addresses for pod " + podId);        }    }        final List<? extends Vlan> vlans = _networkModel.listPodVlans(podId);    if (vlans != null && !vlans.isEmpty()) {        for (final Vlan vlan : vlans) {            _vlanDao.remove(vlan.getId());        }    }        _capacityDao.removeBy(null, null, podId, null, null);        if (!_podDao.remove(podId)) {        throw new CloudRuntimeException("Failed to delete pod " + podId);    }        final DedicatedResourceVO dr = _dedicatedDao.findByPodId(podId);    if (dr != null) {        _dedicatedDao.remove(dr.getId());    }}
0
public void defragmentWithLargeChunkSizeReturnsFalse()
{    int SMALL_SLAB = 16;    int MEDIUM_SLAB = 128;    Slab slab = new SlabImpl(DEFAULT_SLAB_SIZE);    this.freeListManager = createFreeListManager(ma, new Slab[] { new SlabImpl(SMALL_SLAB), new SlabImpl(SMALL_SLAB), new SlabImpl(MEDIUM_SLAB), slab });    ArrayList<OffHeapStoredObject> chunks = new ArrayList<>();    chunks.add(this.freeListManager.allocate(SMALL_SLAB - 8 + 1));    chunks.add(this.freeListManager.allocate(DEFAULT_SLAB_SIZE / 2 - 8));    chunks.add(this.freeListManager.allocate(DEFAULT_SLAB_SIZE / 2 - 8));    chunks.add(this.freeListManager.allocate(SMALL_SLAB - 8 + 1));    for (OffHeapStoredObject c : chunks) {        OffHeapStoredObject.release(c.getAddress(), this.freeListManager);    }    this.freeListManager.firstDefragmentation = false;    assertThat(this.freeListManager.defragment(DEFAULT_SLAB_SIZE + 1)).isFalse();}
0
private boolean hasGrantedFields()
{    if (grantedFields != null && grantedFields.length >= 0) {                if (grantedFields.length == 1 && "*".equals(grantedFields[0])) {            return false;        } else {            return true;        }    }    return false;}
0
public void create_setsConstructedCache_onConstructedSystem_ifNoSystemExists()
{    InternalDistributedSystem constructedSystem = constructedSystem();    InternalCache constructedCache = constructedCache();    InternalCacheBuilder internalCacheBuilder = new InternalCacheBuilder(new Properties(), new CacheConfig(), compositeMeterRegistryFactory, metricsSessionInitializer, nullSingletonSystemSupplier, constructorOf(constructedSystem), nullSingletonCacheSupplier, constructorOf(constructedCache));    internalCacheBuilder.create();    verify(constructedSystem).setCache(same(constructedCache));}
0
public void run()
{    try {        Thread.sleep(2000L);    } catch (final InterruptedException ignored) {    } finally {        System.gc();    }}
0
public void testPreOffsetAndPartitionAsStringHeader()
{    String testKey = "TestKey";    String testOffset = "TestOffset";    String testPartition = "TestPartition";    String testPartitionKey = "TestPartitionKey";    Endpoint endpoint = Mockito.mock(Endpoint.class);    Exchange exchange = Mockito.mock(Exchange.class);    Message message = Mockito.mock(Message.class);    Mockito.when(endpoint.getEndpointUri()).thenReturn("test");    Mockito.when(exchange.getIn()).thenReturn(message);    Mockito.when(message.getHeader(KafkaSpanDecorator.KEY)).thenReturn(testKey);    Mockito.when(message.getHeader(KafkaSpanDecorator.OFFSET, String.class)).thenReturn(testOffset);    Mockito.when(message.getHeader(KafkaSpanDecorator.PARTITION, String.class)).thenReturn(testPartition);    Mockito.when(message.getHeader(KafkaSpanDecorator.PARTITION_KEY)).thenReturn(testPartitionKey);    SpanDecorator decorator = new KafkaSpanDecorator();    MockTracer tracer = new MockTracer();    MockSpan span = tracer.buildSpan("TestSpan").start();    decorator.pre(span, exchange, endpoint);    assertEquals(testKey, span.tags().get(KafkaSpanDecorator.KAFKA_KEY_TAG));    assertEquals(testOffset, span.tags().get(KafkaSpanDecorator.KAFKA_OFFSET_TAG));    assertEquals(testPartition, span.tags().get(KafkaSpanDecorator.KAFKA_PARTITION_TAG));    assertEquals(testPartitionKey, span.tags().get(KafkaSpanDecorator.KAFKA_PARTITION_KEY_TAG));}
0
public String toString(ColumnReader columnReader)
{    return Arrays.toString(columnReader.getBinary().getBytesUnsafe());}
0
public void testConfigurableDelimiter() throws Exception
{    Configuration config = new Configuration();    config.setString(MetricOptions.SCOPE_DELIMITER, "_");    config.setString(MetricOptions.SCOPE_NAMING_TM, "A.B.C.D.E");    MetricRegistryImpl registry = new MetricRegistryImpl(MetricRegistryConfiguration.fromConfiguration(config), ReporterSetup.fromConfiguration(config));    TaskManagerMetricGroup tmGroup = new TaskManagerMetricGroup(registry, "host", "id");    assertEquals("A_B_C_D_E_name", tmGroup.getMetricIdentifier("name"));    registry.shutdown().get();}
0
public static void mutate(List<? extends IMutation> mutations, ConsistencyLevel consistencyLevel, long queryStartNanoTime) throws UnavailableException, OverloadedException, WriteTimeoutException, WriteFailureException
{    Tracing.trace("Determining replicas for mutation");    final String localDataCenter = DatabaseDescriptor.getEndpointSnitch().getLocalDatacenter();    long startTime = System.nanoTime();    List<AbstractWriteResponseHandler<IMutation>> responseHandlers = new ArrayList<>(mutations.size());    WriteType plainWriteType = mutations.size() <= 1 ? WriteType.SIMPLE : WriteType.UNLOGGED_BATCH;    try {        for (IMutation mutation : mutations) {            if (mutation instanceof CounterMutation)                responseHandlers.add(mutateCounter((CounterMutation) mutation, localDataCenter, queryStartNanoTime));            else                responseHandlers.add(performWrite(mutation, consistencyLevel, localDataCenter, standardWritePerformer, null, plainWriteType, queryStartNanoTime));        }                for (int i = 0; i < mutations.size(); ++i) {            if (            !(mutations.get(i) instanceof CounterMutation))                responseHandlers.get(i).maybeTryAdditionalReplicas(mutations.get(i), standardWritePerformer, localDataCenter);        }                for (AbstractWriteResponseHandler<IMutation> responseHandler : responseHandlers) responseHandler.get();    } catch (WriteTimeoutException | WriteFailureException ex) {        if (consistencyLevel == ConsistencyLevel.ANY) {            hintMutations(mutations);        } else {            if (ex instanceof WriteFailureException) {                writeMetrics.failures.mark();                writeMetricsMap.get(consistencyLevel).failures.mark();                WriteFailureException fe = (WriteFailureException) ex;                Tracing.trace("Write failure; received {} of {} required replies, failed {} requests", fe.received, fe.blockFor, fe.failureReasonByEndpoint.size());            } else {                writeMetrics.timeouts.mark();                writeMetricsMap.get(consistencyLevel).timeouts.mark();                WriteTimeoutException te = (WriteTimeoutException) ex;                Tracing.trace("Write timeout; received {} of {} required replies", te.received, te.blockFor);            }            throw ex;        }    } catch (UnavailableException e) {        writeMetrics.unavailables.mark();        writeMetricsMap.get(consistencyLevel).unavailables.mark();        Tracing.trace("Unavailable");        throw e;    } catch (OverloadedException e) {        writeMetrics.unavailables.mark();        writeMetricsMap.get(consistencyLevel).unavailables.mark();        Tracing.trace("Overloaded");        throw e;    } finally {        long latency = System.nanoTime() - startTime;        writeMetrics.addNano(latency);        writeMetricsMap.get(consistencyLevel).addNano(latency);        updateCoordinatorWriteLatencyTableMetric(mutations, latency);    }}
0
public java.lang.Class<org.apache.drill.exec.proto.UserBitShared.ParsingError> typeClass()
{    return org.apache.drill.exec.proto.UserBitShared.ParsingError.class;}
0
public void testIndexCreationDeadLockForOverflowToDiskRegion()
{    this.region.destroyRegion();    AttributesFactory factory = new AttributesFactory();    factory.setScope(Scope.DISTRIBUTED_ACK);    factory.setValueConstraint(Portfolio.class);    factory.setEvictionAttributes(EvictionAttributes.createLRUEntryAttributes(1, EvictionAction.OVERFLOW_TO_DISK));    factory.setIndexMaintenanceSynchronous(true);    File dir = new File("test");    dir.mkdir();    DiskStoreFactory dsf = region.getCache().createDiskStoreFactory();    DiskStore ds1 = dsf.setDiskDirs(new File[] { dir }).create("ds1");    factory.setDiskStoreName("ds1");    dir.deleteOnExit();    region = CacheUtils.createRegion("portfolios", factory.create(), true);    simulateDeadlockScenario();    assertFalse(this.cause, this.testFailed);    assertTrue("Index creation succeeded . For diskRegion this shoudl not have happened", this.exceptionInCreatingIndex);}
0
public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmissionStateProto getSubmissionState()
{    return submissionState_;}
0
public void testReflectFieldError() throws Exception
{    Object datum = "";    try {        ReflectData.get().getField(datum, "notAFieldOfString", 0);    } catch (AvroRuntimeException e) {        assertTrue(e.getMessage().contains(datum.getClass().getName()));    }}
0
public void rename(Name oldName, Name newName) throws NamingException
{    throw new OperationNotSupportedException();}
0
public void multipleTransformsOneDangling()
{    Map<String, String> props = new HashMap<>();    props.put("name", "test");    props.put("connector.class", TestConnector.class.getName());    props.put("transforms", "a, b");    props.put("transforms.a.type", SimpleTransformation.class.getName());    props.put("transforms.a.magic.number", "42");    new ConnectorConfig(MOCK_PLUGINS, props);}
0
private BufferedRows getConfInternal(boolean call)
{    Statement stmnt = null;    BufferedRows rows = null;    ResultSet rs = null;    try {        boolean hasResults = false;        DatabaseConnection dbconn = beeLine.getDatabaseConnection();        Connection conn = null;        if (dbconn != null)            conn = dbconn.getConnection();        if (conn != null) {            if (call) {                stmnt = conn.prepareCall("set");                hasResults = ((CallableStatement) stmnt).execute();            } else {                stmnt = beeLine.createStatement();                hasResults = stmnt.execute("set");            }        }        if (hasResults) {            rs = stmnt.getResultSet();            rows = new BufferedRows(beeLine, rs);        }    } catch (SQLException e) {        beeLine.error(e);    } finally {        if (rs != null) {            try {                rs.close();            } catch (SQLException e1) {                beeLine.error(e1);            }        }        if (stmnt != null) {            try {                stmnt.close();            } catch (SQLException e2) {                beeLine.error(e2);            }        }    }    return rows;}
0
protected java.lang.Object standardSchemeReadValue(org.apache.thrift.protocol.TProtocol iprot, org.apache.thrift.protocol.TField field) throws org.apache.thrift.TException
{    _Fields setField = _Fields.findByThriftId(field.id);    if (setField != null) {        switch(setField) {            case PUT:                if (field.type == PUT_FIELD_DESC.type) {                    TPut put;                    put = new TPut();                    put.read(iprot);                    return put;                } else {                    org.apache.thrift.protocol.TProtocolUtil.skip(iprot, field.type);                    return null;                }            case DELETE_SINGLE:                if (field.type == DELETE_SINGLE_FIELD_DESC.type) {                    TDelete deleteSingle;                    deleteSingle = new TDelete();                    deleteSingle.read(iprot);                    return deleteSingle;                } else {                    org.apache.thrift.protocol.TProtocolUtil.skip(iprot, field.type);                    return null;                }            default:                throw new java.lang.IllegalStateException("setField wasn't null, but didn't match any of the case statements!");        }    } else {        org.apache.thrift.protocol.TProtocolUtil.skip(iprot, field.type);        return null;    }}
0
public short getShort(String columnLabel) throws SQLException
{    this.wasLastValueNull = false;    checkClosed();    Number number = (Number) getObject(columnLabel);    if (number == null) {        this.wasLastValueNull = true;        return 0;    } else {        return number.shortValue();    }}
0
public Boolean getUseXForwardHeaders()
{    return useXForwardHeaders;}
0
public void setNameToSplitSample(Map<String, SplitSample> nameToSplitSample)
{    this.nameToSplitSample = nameToSplitSample;}
0
 Request routedBasedOnClusterVersion(long routedBasedOnClusterVersion)
{    this.routedBasedOnClusterVersion = routedBasedOnClusterVersion;    return (Request) this;}
0
public String lpad(String originalValue, Long returnLength)
{    return lpad(originalValue, returnLength, " ");}
0
public void setSourceId(String sourceId)
{    this.sourceId = sourceId;}
0
public void setStdout(String stdout)
{    this.stdout = stdout;}
0
public void attachArtifact(MavenProject project, String artifactType, File artifactFile)
{    ArtifactHandler handler = artifactHandlerManager.getArtifactHandler(artifactType);    Artifact artifact = new AttachedArtifact(project.getArtifact(), artifactType, handler);    artifact.setFile(artifactFile);    artifact.setResolved(true);    attachArtifact(project, artifact);}
0
public TerminalNode MONTH()
{    return getToken(SqlBaseParser.MONTH, 0);}
0
public String basicConsume(String queue, boolean autoAck, Map<String, Object> arguments, DeliverCallback deliverCallback, CancelCallback cancelCallback, ConsumerShutdownSignalCallback shutdownSignalCallback) throws IOException
{    throw new UnsupportedOperationException("This method is not currently supported as it is not used by current API in testing");}
0
 AdvancedFlinkEndpointBuilder synchronous(boolean synchronous)
{    doSetProperty("synchronous", synchronous);    return this;}
0
 static Node<K> untreeify(Node<K> b)
{    Node<K> hd = null, tl = null;    for (Node<K> q = b; q != null; q = q.next) {        Node<K> p = new Node<K>(q.hash, q.key, null);        if (tl == null)            hd = p;        else            tl.next = p;        tl = p;    }    return hd;}
0
private void validateExpr(SqlNode expr, SqlValidatorScope scope)
{    if (expr instanceof SqlCall) {        final SqlOperator op = ((SqlCall) expr).getOperator();        if (op.isAggregator() && op.requiresOver()) {            throw newValidationError(expr, RESOURCE.absentOverClause());        }    }        expr.validateExpr(this, scope);            scope.validateExpr(expr);}
0
public void testStartCalledWhenCamelStarts() throws Exception
{    assertEquals(1, routePolicy.getStartCount());}
0
public synchronized void add(AutoScaling.Trigger newTrigger) throws Exception
{    if (isClosed) {        throw new AlreadyClosedException("ScheduledTriggers has been closed and cannot be used anymore");    }    TriggerWrapper st;    try {        st = new TriggerWrapper(newTrigger, cloudManager, queueStats);    } catch (Exception e) {        if (isClosed || e instanceof AlreadyClosedException) {            throw new AlreadyClosedException("ScheduledTriggers has been closed and cannot be used anymore");        }        if (cloudManager.isClosed()) {                    } else {                    }        return;    }    TriggerWrapper triggerWrapper = st;    TriggerWrapper old = scheduledTriggerWrappers.putIfAbsent(newTrigger.getName(), triggerWrapper);    if (old != null) {        if (old.trigger.equals(newTrigger)) {                        return;        }        IOUtils.closeQuietly(old);        newTrigger.restoreState(old.trigger);        triggerWrapper.setReplay(false);        scheduledTriggerWrappers.replace(newTrigger.getName(), triggerWrapper);    }    newTrigger.setProcessor(event -> {        TriggerListeners triggerListeners = listeners.copy();        if (cloudManager.isClosed()) {            String msg = String.format(Locale.ROOT, "Ignoring autoscaling event %s because Solr has been shutdown.", event.toString());                        triggerListeners.fireListeners(event.getSource(), event, TriggerEventProcessorStage.ABORTED, msg);            return false;        }        TriggerWrapper scheduledSource = scheduledTriggerWrappers.get(event.getSource());        if (scheduledSource == null) {            String msg = String.format(Locale.ROOT, "Ignoring autoscaling event %s because the source trigger: %s doesn't exist.", event.toString(), event.getSource());            triggerListeners.fireListeners(event.getSource(), event, TriggerEventProcessorStage.FAILED, msg);                        return false;        }        boolean replaying = event.getProperty(TriggerEvent.REPLAYING) != null ? (Boolean) event.getProperty(TriggerEvent.REPLAYING) : false;        AutoScaling.Trigger source = scheduledSource.trigger;        if (scheduledSource.isClosed || source.isClosed()) {            String msg = String.format(Locale.ROOT, "Ignoring autoscaling event %s because the source trigger: %s has already been closed", event.toString(), source);            triggerListeners.fireListeners(event.getSource(), event, TriggerEventProcessorStage.ABORTED, msg);                                    return false;        }        if (event.isIgnored()) {                        event.getProperties().put(TriggerEvent.IGNORED, true);            triggerListeners.fireListeners(event.getSource(), event, TriggerEventProcessorStage.IGNORED, "Event was ignored.");                        return true;        }                if (cooldownStart.get() + cooldownPeriod.get() > cloudManager.getTimeSource().getTimeNs()) {                        event.getProperties().put(TriggerEvent.COOLDOWN, true);            triggerListeners.fireListeners(event.getSource(), event, TriggerEventProcessorStage.IGNORED, "In cooldown period.");            return false;        } else {                                    cooldownStart.set(cloudManager.getTimeSource().getTimeNs());        }        if (hasPendingActions.compareAndSet(false, true)) {                        pauseTriggers();            final boolean enqueued;            if (replaying) {                enqueued = false;            } else {                enqueued = triggerWrapper.enqueue(event);            }                        triggerListeners.fireListeners(event.getSource(), event, TriggerEventProcessorStage.STARTED);            List<TriggerAction> actions = source.getActions();            if (actions != null) {                if (actionExecutor.isShutdown()) {                    String msg = String.format(Locale.ROOT, "Ignoring autoscaling event %s from trigger %s because the executor has already been closed", event.toString(), source);                    triggerListeners.fireListeners(event.getSource(), event, TriggerEventProcessorStage.ABORTED, msg);                                        hasPendingActions.set(false);                                        return false;                }                actionExecutor.submit(() -> {                    assert hasPendingActions.get();                    long eventProcessingStart = cloudManager.getTimeSource().getTimeNs();                    TriggerListeners triggerListeners1 = triggerListeners.copy();                                        try {                                                                        waitForPendingTasks(newTrigger, actions);                        ActionContext actionContext = new ActionContext(cloudManager, newTrigger, new HashMap<>());                        for (TriggerAction action : actions) {                            List<String> beforeActions = (List<String>) actionContext.getProperties().computeIfAbsent(TriggerEventProcessorStage.BEFORE_ACTION.toString(), k -> new ArrayList<String>());                            beforeActions.add(action.getName());                            triggerListeners1.fireListeners(event.getSource(), event, TriggerEventProcessorStage.BEFORE_ACTION, action.getName(), actionContext);                            try {                                action.process(event, actionContext);                            } catch (Exception e) {                                triggerListeners1.fireListeners(event.getSource(), event, TriggerEventProcessorStage.FAILED, action.getName(), actionContext, e, null);                                throw new TriggerActionException(event.getSource(), action.getName(), "Error processing action for trigger event: " + event, e);                            }                            List<String> afterActions = (List<String>) actionContext.getProperties().computeIfAbsent(TriggerEventProcessorStage.AFTER_ACTION.toString(), k -> new ArrayList<String>());                            afterActions.add(action.getName());                            triggerListeners1.fireListeners(event.getSource(), event, TriggerEventProcessorStage.AFTER_ACTION, action.getName(), actionContext);                        }                        if (enqueued) {                            TriggerEvent ev = triggerWrapper.dequeue();                            assert ev.getId().equals(event.getId());                        }                        triggerListeners1.fireListeners(event.getSource(), event, TriggerEventProcessorStage.SUCCEEDED);                    } catch (TriggerActionException e) {                                            } catch (Exception e) {                        triggerListeners1.fireListeners(event.getSource(), event, TriggerEventProcessorStage.FAILED);                                            } finally {                                                cooldownStart.set(cloudManager.getTimeSource().getTimeNs());                        hasPendingActions.set(false);                                                resumeTriggers(cloudManager.getTimeSource().convertDelay(TimeUnit.NANOSECONDS, cooldownPeriod.get(), TimeUnit.MILLISECONDS));                    }                                    });            } else {                if (enqueued) {                    TriggerEvent ev = triggerWrapper.dequeue();                    if (!ev.getId().equals(event.getId())) {                        throw new RuntimeException("Wrong event dequeued, queue of " + triggerWrapper.trigger.getName() + " is broken! Expected event=" + event + " but got " + ev);                    }                }                triggerListeners.fireListeners(event.getSource(), event, TriggerEventProcessorStage.SUCCEEDED);                hasPendingActions.set(false);                                resumeTriggers(0);            }            return true;        } else {                                    triggerListeners.fireListeners(event.getSource(), event, TriggerEventProcessorStage.IGNORED, "Already processing another event.");            return false;        }    });        newTrigger.init();    triggerWrapper.scheduledFuture = scheduledThreadPoolExecutor.scheduleWithFixedDelay(triggerWrapper, 0, cloudManager.getTimeSource().convertDelay(TimeUnit.SECONDS, triggerDelay.get(), TimeUnit.MILLISECONDS), TimeUnit.MILLISECONDS);}
1
public X509Certificate[] getPeerCertificateChain() throws IOException
{    try {                                int certLength = socketWrapper.getSSLInfoI(SSL.SSL_INFO_CLIENT_CERT_CHAIN);        byte[] clientCert = socketWrapper.getSSLInfoB(SSL.SSL_INFO_CLIENT_CERT);        X509Certificate[] certs = null;        if (clientCert != null) {            if (certLength < 0) {                certLength = 0;            }            certs = new X509Certificate[certLength + 1];            CertificateFactory cf;            if (clientCertProvider == null) {                cf = CertificateFactory.getInstance("X.509");            } else {                cf = CertificateFactory.getInstance("X.509", clientCertProvider);            }            certs[0] = (X509Certificate) cf.generateCertificate(new ByteArrayInputStream(clientCert));            for (int i = 0; i < certLength; i++) {                byte[] data = socketWrapper.getSSLInfoB(SSL.SSL_INFO_CLIENT_CERT_CHAIN + i);                certs[i + 1] = (X509Certificate) cf.generateCertificate(new ByteArrayInputStream(data));            }        }        return certs;    } catch (Exception e) {        throw new IOException(e);    }}
0
public void testException() throws Throwable
{    Exception e = new NoRouteToHostException("that box caught fire 3 years ago");    ThrowableInformation ti = new ThrowableInformation(e);    Log4Json l4j = new Log4Json();    long timeStamp = Time.now();    String outcome = l4j.toJson(new StringWriter(), "testException", timeStamp, "INFO", "quoted\"", "new line\n and {}", ti).toString();    println("testException", outcome);}
0
public DateFormatter dateTimeFormatter()
{    return dateTimeFormatter;}
0
public Tuple2<Long, Long> map(Long value)
{    return new Tuple2<Long, Long>(value, value);}
0
public void toData(DataOutput out) throws IOException
{    DataSerializer.writeHashMap(this.props, out);}
0
public String zkPathMustExist(String path) throws IOException
{    zkStat(path);    return path;}
0
public void testListDatabases() throws SQLException
{    List<String> databases = new ArrayList<>();    databases.add("DB1");    databases.add("DB2");    when(jdbcMetadata.listDatabases()).thenReturn(databases);    List<String> result = jdbcExplorer.listDatabases();    verify(jdbcMetadata, times(1)).listDatabases();    Assert.assertEquals(databases, result);}
0
public void configure() throws Exception
{    errorHandler(deadLetterChannel("mock:dead"));    from("direct:start").idempotentConsumer(header("messageId"), myRepo).to("mock:result");}
0
public long readPointer() throws IOException
{    return inView.readLong();}
0
public void stop()
{        delegate.stop();    super.stop();}
1
public void setBasePath(String basePath)
{    configuration.setBasePath(basePath);}
0
public void setStackTraceEnabled(boolean b)
{    stackTraceEnabled = b;}
0
protected boolean queryMatches(byte[] t, ShapeField.DecodedTriangle scratchTriangle, QueryRelation queryRelation)
{    ShapeField.decodeTriangle(t, scratchTriangle);    double alat = decode(scratchTriangle.aY);    double alon = decode(scratchTriangle.aX);    double blat = decode(scratchTriangle.bY);    double blon = decode(scratchTriangle.bX);    double clat = decode(scratchTriangle.cY);    double clon = decode(scratchTriangle.cX);    switch(queryRelation) {        case INTERSECTS:            return poly2D.relateTriangle(alon, alat, blon, blat, clon, clat) != Relation.CELL_OUTSIDE_QUERY;        case WITHIN:            return poly2D.relateTriangle(alon, alat, blon, blat, clon, clat) == Relation.CELL_INSIDE_QUERY;        case DISJOINT:            return poly2D.relateTriangle(alon, alat, blon, blat, clon, clat) == Relation.CELL_OUTSIDE_QUERY;        default:            throw new IllegalArgumentException("Unsupported query type :[" + queryRelation + "]");    }}
0
private static File convertGLFile(File originalFile) throws IOException
{        File resultFile = new File(new File(System.getProperty("java.io.tmpdir")), "ratings.txt");    if (resultFile.exists()) {        resultFile.delete();    }    try (Writer writer = new OutputStreamWriter(new FileOutputStream(resultFile), Charsets.UTF_8)) {        for (String line : new FileLineIterable(originalFile, false)) {            int lastDelimiterStart = line.lastIndexOf(COLON_DELIMTER);            if (lastDelimiterStart < 0) {                throw new IOException("Unexpected input format on line: " + line);            }            String subLine = line.substring(0, lastDelimiterStart);            String convertedLine = COLON_DELIMITER_PATTERN.matcher(subLine).replaceAll(",");            writer.write(convertedLine);            writer.write('\n');        }    } catch (IOException ioe) {        resultFile.delete();        throw ioe;    }    return resultFile;}
0
public static Builder newBuilder(org.apache.drill.exec.proto.UserBitShared.UserCredentials prototype)
{    return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);}
0
public void await()
{}
0
public void canDeserializeMapsWithJavaLangStringKeys() throws IOException, SerDeException
{            String schemaString = "{\n" + "  \"namespace\": \"testing\",\n" + "  \"name\": \"oneMap\",\n" + "  \"type\": \"record\",\n" + "  \"fields\": [\n" + "    {\n" + "      \"name\":\"aMap\",\n" + "      \"type\":{\"type\":\"map\",\n" + "      \"avro.java.string\":\"String\",\n" + "      \"values\":\"long\"}\n" + "\t}\n" + "  ]\n" + "}";    Schema s = AvroSerdeUtils.getSchemaFor(schemaString);    GenericData.Record record = new GenericData.Record(s);    Map<String, Long> m = new Hashtable<String, Long>();    m.put("one", 1l);    m.put("two", 2l);    m.put("three", 3l);    record.put("aMap", m);    assertTrue(GENERIC_DATA.validate(s, record));    System.out.println("record = " + record);    AvroGenericRecordWritable garw = Utils.serializeAndDeserializeRecord(record);    AvroObjectInspectorGenerator aoig = new AvroObjectInspectorGenerator(s);    AvroDeserializer de = new AvroDeserializer();    ArrayList<Object> row = (ArrayList<Object>) de.deserialize(aoig.getColumnNames(), aoig.getColumnTypes(), garw, s);    assertEquals(1, row.size());    Object theMapObject = row.get(0);    assertTrue(theMapObject instanceof Map);    Map theMap = (Map) theMapObject;        assertEquals(1l, theMap.get("one"));    assertEquals(2l, theMap.get("two"));    assertEquals(3l, theMap.get("three"));        StandardStructObjectInspector oi = (StandardStructObjectInspector) aoig.getObjectInspector();    List<Object> z = oi.getStructFieldsDataAsList(row);    assertEquals(1, z.size());    StructField fieldRef = oi.getStructFieldRef("amap");    Map theMap2 = (Map) oi.getStructFieldData(row, fieldRef);    assertEquals(1l, theMap2.get("one"));    assertEquals(2l, theMap2.get("two"));    assertEquals(3l, theMap2.get("three"));}
0
public void testClientDnsArg() throws Exception
{    String clientDn = "OU=NIFI,CN=testuser";    String clientDn2 = "OU=NIFI,CN=testuser2";    runAndAssertExitCode(ExitCode.SUCCESS, "-o", tempDir.getAbsolutePath(), "-C", clientDn, "-C", clientDn2, "-B", "pass1", "-P", "pass2");    X509Certificate x509Certificate = checkLoadCertPrivateKey(TlsConfig.DEFAULT_KEY_PAIR_ALGORITHM);    checkClientCert(clientDn, x509Certificate);    checkClientCert(clientDn2, x509Certificate);    runAndAssertExitCode(ExitCode.SUCCESS, "-o", tempDir.getAbsolutePath(), "-O", "-C", clientDn2, "-B", "pass3");    checkClientCert(clientDn2, x509Certificate);}
0
public void setPrevious(final EvictionNode previousEvictionNode)
{    this.previousEvictionNode = previousEvictionNode;}
0
public int compare(FileStatus o1, FileStatus o2)
{    return o1.getPath().getName().compareTo(o2.getPath().getName());}
0
 S3EndpointConsumerBuilder proxyPort(String proxyPort)
{    doSetProperty("proxyPort", proxyPort);    return this;}
0
public void processElement(ProcessContext ctx)
{    throw new RuntimeException("Failing here is ok.");}
0
 static String lookupPassword(Configuration conf, String key, String defVal) throws IOException
{    try {        final char[] pass = conf.getPassword(key);        return pass != null ? new String(pass).trim() : defVal;    } catch (IOException ioe) {        throw new IOException("Cannot find password option " + key, ioe);    }}
0
private Map<String, Object> getContentSummaryMap(String path) throws IOException
{    long id = lookup(path);    INode inode = fromINodeId(id);    long spaceQuota = 0;    long nsQuota = 0;    long[] data = new long[4];    FsImageProto.INodeSection.INodeFile f = inode.getFile();    switch(inode.getType()) {        case FILE:            data[0] = 0;            data[1] = 1;            data[2] = getFileSize(f);            nsQuota = -1;            data[3] = data[2] * f.getReplication();            spaceQuota = -1;            return fillSummaryMap(spaceQuota, nsQuota, data);        case DIRECTORY:            fillDirSummary(id, data);            nsQuota = inode.getDirectory().getNsQuota();            spaceQuota = inode.getDirectory().getDsQuota();            return fillSummaryMap(spaceQuota, nsQuota, data);        case SYMLINK:            data[0] = 0;            data[1] = 1;            data[2] = 0;            nsQuota = -1;            data[3] = 0;            spaceQuota = -1;            return fillSummaryMap(spaceQuota, nsQuota, data);        default:            return null;    }}
0
public void ensureValid(String name, Object value)
{    final List<String> transformAliases = (List<String>) value;    if (transformAliases.size() > new HashSet<>(transformAliases).size()) {        throw new ConfigException(name, value, "Duplicate alias provided.");    }}
0
public void run()
{    try {        for (long i = 0; i < numMessagesToSend; i++) {            producer.send(session.createTextMessage("test"));            long count = produced.incrementAndGet();            if (count % 10000 == 0) {                            }        }    } catch (Throwable ex) {        ex.printStackTrace();    } finally {        try {            producer.close();            session.close();        } catch (Exception e) {        }    }}
1
public RelNode convert(RelNode rel)
{    RelTraitSet newTraitSet = rel.getTraitSet().replace(getOutConvention());    return new SolrToEnumerableConverter(rel.getCluster(), newTraitSet, rel);}
0
public RangeIterator<Long, Token> search(Expression expression)
{    ByteBuffer min = expression.lower == null ? null : expression.lower.value;    ByteBuffer max = expression.upper == null ? null : expression.upper.value;    SortedMap<ByteBuffer, ConcurrentSkipListSet<DecoratedKey>> search;    if (min == null && max == null) {        throw new IllegalArgumentException();    }    if (min != null && max != null) {        search = index.subMap(min, expression.lower.inclusive, max, expression.upper.inclusive);    } else if (min == null) {        search = index.headMap(max, expression.upper.inclusive);    } else {        search = index.tailMap(min, expression.lower.inclusive);    }    RangeUnionIterator.Builder<Long, Token> builder = RangeUnionIterator.builder();    search.values().stream().filter(keys -> !keys.isEmpty()).forEach(keys -> builder.add(new KeyRangeIterator(keys)));    return builder.build();}
0
protected void recordSuccess(long vmId)
{    _vmFailureCounts.remove(vmId);}
0
private void cleanupUserFileCache(String user)
{    List<String> localDirs = dirsHandler.getLocalDirs();    for (String dir : localDirs) {        Path filecache = new Path(dir, ContainerLocalizer.FILECACHE);        Path filedir = new Path(filecache, user);        exec.deleteAsUser(new DeletionAsUserContext.Builder().setUser(user).setSubDir(filedir).build());    }}
0
public void fromCommandWithInconclusiveRepairedDigest()
{    ByteBuffer digest = digest();    ReadCommand command = command(key(), metadata, digest, false);    ReadResponse response = command.createResponse(EmptyIterators.unfilteredPartition(metadata));    assertFalse(response.isRepairedDigestConclusive());    assertEquals(digest, response.repairedDataDigest());    verifySerDe(response);}
0
public void start(I iface, getSupervisorPageInfo_args args, org.apache.storm.thrift.async.AsyncMethodCallback<SupervisorPageInfo> resultHandler) throws org.apache.storm.thrift.TException
{    iface.getSupervisorPageInfo(args.id, args.host, args.is_include_sys, resultHandler);}
0
public void downloadConfigDir(String configName, Path dir) throws IOException
{    zkClient.downloadFromZK(CONFIGS_ZKNODE + "/" + configName, dir);}
0
public void testWrongCharacterInToken2() throws Exception
{    String header = "Digest qop=\u044f";    StringReader input = new StringReader(header);    Map<String, String> result = Authorization.parseAuthorizationDigest(input);    Assert.assertNull(result);}
0
public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException
{    builder.startObject();    builder.startObject(PHASES_FIELD.getPreferredName());    for (Phase phase : phases.values()) {        builder.field(phase.getName(), phase);    }    builder.endObject();    builder.endObject();    return builder;}
0
public void shouldBeMockable() throws Exception
{    RegionData mockRegionData = mock(RegionData.class);    String regionNamePath = "regionNamePath";    mockRegionData.setRegionNamePath(regionNamePath);    verify(mockRegionData, times(1)).setRegionNamePath(regionNamePath);}
0
private BoxUser getCurrentUser()
{    return BoxUser.getCurrentUser(getConnection());}
0
protected Exchange sendSimpleMessage()
{    Exchange exchange = template.send(getSimpleEndpointUri(), new Processor() {        public void process(final Exchange exchange) {            final List<String> params = new ArrayList<>();            params.add(TEST_MESSAGE);            Map<String, Object> requestContext = new HashMap<>();            requestContext.put(Message.ENDPOINT_ADDRESS, getSimpleServerAddress());            exchange.getIn().setBody(params);            exchange.getIn().setHeader(Client.REQUEST_CONTEXT, requestContext);            exchange.getIn().setHeader(CxfConstants.OPERATION_NAME, ECHO_OPERATION);            exchange.getIn().setHeader(Exchange.FILE_NAME, "testFile");            exchange.getIn().setHeader("requestObject", new DefaultCxfBinding());            exchange.getProperties().put(TEST_KEY, TEST_VALUE);        }    });    return exchange;}
0
public long indexSize()
{    return (dataBlockIndexReader != null ? dataBlockIndexReader.heapSize() : 0) + ((metaBlockIndexReader != null) ? metaBlockIndexReader.heapSize() : 0);}
0
public void testExtraFieldInWriteRecord() throws IOException
{    final CSVFormat csvFormat = CSVFormat.DEFAULT.withEscape('\\').withQuoteMode(QuoteMode.NONE).withRecordSeparator("\n");    final List<RecordField> fields = new ArrayList<>();    fields.add(new RecordField("id", RecordFieldType.STRING.getDataType()));    final RecordSchema schema = new SimpleRecordSchema(fields);    final Map<String, Object> values = new HashMap<>();    values.put("id", "1");    values.put("name", "John");    final Record record = new MapRecord(schema, values);    final ByteArrayOutputStream baos = new ByteArrayOutputStream();    final String output;    try (final WriteCSVResult writer = new WriteCSVResult(csvFormat, schema, new SchemaNameAsAttribute(), baos, RecordFieldType.DATE.getDefaultFormat(), RecordFieldType.TIME.getDefaultFormat(), RecordFieldType.TIMESTAMP.getDefaultFormat(), true, "ASCII")) {        writer.beginRecordSet();        writer.write(record);        writer.finishRecordSet();        writer.flush();        output = baos.toString();    }    assertEquals("id\n1\n", output);}
0
public String getRank()
{    return _rank;}
0
public org.apache.hadoop.mapred.RecordReader<NullWritable, DruidWritable> getRecordReader(org.apache.hadoop.mapred.InputSplit split, JobConf job, Reporter reporter) throws IOException
{            final DruidQueryRecordReader<?> reader;        final String druidQueryType = job.get(Constants.DRUID_QUERY_TYPE, Query.SCAN);    reader = getDruidQueryReader(druidQueryType);    reader.initialize((HiveDruidSplit) split, job);    if (Utilities.getIsVectorized(job)) {                return (org.apache.hadoop.mapred.RecordReader) new DruidVectorizedWrapper(reader, job);    }    return reader;}
0
public Builder clearSendingMinorFragmentId()
{    bitField0_ = (bitField0_ & ~0x00000010);    sendingMinorFragmentId_ = 0;    onChanged();    return this;}
0
public void resetConsumedCount()
{}
0
public static MemoryAllocator create(OutOfOffHeapMemoryListener ooohml, OffHeapMemoryStats stats, int slabCount, long offHeapMemorySize, long maxSlabSize)
{    return create(ooohml, stats, slabCount, offHeapMemorySize, maxSlabSize, null, new SlabFactory() {        @Override        public Slab create(int size) {            return new SlabImpl(size);        }    });}
0
private Map<String, String> lookupState(String state)
{    assert state != null;    for (StateAction a : STATE_TABLE) {        if (state.equals(a.state)) {            StateDatum[] stateData = a.stateData;            Map<String, String> result = new HashMap<String, String>();            for (StateDatum datum : stateData) {                result.put(datum.x, datum.y);            }            return result;        }    }    throw new IllegalArgumentException();}
0
public Comparator comparator()
{    if (this.data instanceof NWayMergeResults.NWayMergeResultsCollection) {        return ((NWayMergeResultsCollection) this.data).comparator;    } else {        return null;    }}
0
public void testExpandFilterExists() throws Exception
{    final String sql = "select empno\n" + "from sales.emp\n" + "where exists (select deptno from sales.emp where empno < 20)\n" + "or emp.sal < 100";    checkSubQuery(sql).check();}
0
private OMRequest createVolumeRequest(String volumeName, String adminName, String ownerName)
{    VolumeInfo volumeInfo = VolumeInfo.newBuilder().setVolume(volumeName).setAdminName(adminName).setOwnerName(ownerName).build();    CreateVolumeRequest createVolumeRequest = CreateVolumeRequest.newBuilder().setVolumeInfo(volumeInfo).build();    return OMRequest.newBuilder().setClientId(UUID.randomUUID().toString()).setCmdType(OzoneManagerProtocolProtos.Type.CreateVolume).setCreateVolumeRequest(createVolumeRequest).build();}
0
public Tuple2<String, Integer> add(Tuple3<String, String, Integer> value, Tuple2<String, Integer> accumulator)
{    accumulator.f0 = value.f0;    accumulator.f1 = value.f2;    return accumulator;}
0
public void close() throws IOException
{    w.close();    if (DEBUG)        System.out.println("TEST: after close writer index=" + SegmentInfos.readLatestCommit(indexDir));    /*      DirectoryReader r = mgr.acquire();      try {        TestUtil.checkReader(r);      } finally {        mgr.release(r);      }      */    mgr.close();    pruneOldSegments(true);    assertNoExtraSegments();    indexDir.close();}
0
protected void savepoint(String[] args) throws Exception
{        final Options commandOptions = CliFrontendParser.getSavepointCommandOptions();    final Options commandLineOptions = CliFrontendParser.mergeOptions(commandOptions, customCommandLineOptions);    final CommandLine commandLine = CliFrontendParser.parse(commandLineOptions, args, false);    final SavepointOptions savepointOptions = new SavepointOptions(commandLine);        if (savepointOptions.isPrintHelp()) {        CliFrontendParser.printHelpForSavepoint(customCommandLines);        return;    }    final CustomCommandLine<?> activeCommandLine = getActiveCustomCommandLine(commandLine);    if (savepointOptions.isDispose()) {        runClusterAction(activeCommandLine, commandLine, clusterClient -> disposeSavepoint(clusterClient, savepointOptions.getSavepointPath()));    } else {        String[] cleanedArgs = savepointOptions.getArgs();        final JobID jobId;        if (cleanedArgs.length >= 1) {            String jobIdString = cleanedArgs[0];            jobId = parseJobId(jobIdString);        } else {            throw new CliArgsException("Missing JobID. " + "Specify a Job ID to trigger a savepoint.");        }        final String savepointDirectory;        if (cleanedArgs.length >= 2) {            savepointDirectory = cleanedArgs[1];        } else {            savepointDirectory = null;        }                if (cleanedArgs.length >= 3) {            logAndSysout("Provided more arguments than required. Ignoring not needed arguments.");        }        runClusterAction(activeCommandLine, commandLine, clusterClient -> triggerSavepoint(clusterClient, jobId, savepointDirectory));    }}
1
public void run()
{        final Configuration oldConf = parent.getConf();    final Configuration newConf = parent.getNewConf();    final Collection<PropertyChange> changes = parent.getChangedProperties(newConf, oldConf);    Map<PropertyChange, Optional<String>> results = Maps.newHashMap();    ConfigRedactor oldRedactor = new ConfigRedactor(oldConf);    ConfigRedactor newRedactor = new ConfigRedactor(newConf);    for (PropertyChange change : changes) {        String errorMessage = null;        String oldValRedacted = oldRedactor.redact(change.prop, change.oldVal);        String newValRedacted = newRedactor.redact(change.prop, change.newVal);        if (!parent.isPropertyReconfigurable(change.prop)) {                        continue;        }                try {            String effectiveValue = parent.reconfigurePropertyImpl(change.prop, change.newVal);            if (change.newVal != null) {                oldConf.set(change.prop, effectiveValue);            } else {                oldConf.unset(change.prop);            }        } catch (ReconfigurationException e) {            Throwable cause = e.getCause();            errorMessage = cause == null ? e.getMessage() : cause.getMessage();        }        results.put(change, Optional.ofNullable(errorMessage));    }    synchronized (parent.reconfigLock) {        parent.endTime = Time.now();        parent.status = Collections.unmodifiableMap(results);        parent.reconfigThread = null;    }}
1
protected CloudSolrClient getCommonCloudSolrClient()
{    synchronized (this) {        if (commonCloudSolrClient == null) {            commonCloudSolrClient = getCloudSolrClient(zkServer.getZkAddress(), random().nextBoolean(), 5000, 120000);            commonCloudSolrClient.setDefaultCollection(DEFAULT_COLLECTION);            commonCloudSolrClient.connect();                    }    }    return commonCloudSolrClient;}
1
public void refreshLoadedJobCache()
{    if (getServiceState() == STATE.STARTED) {        setConfig(createConf());        createLoadedJobCache(getConfig());    } else {            }}
1
public SshComponent configureSshComponent() throws Exception
{    SshComponent component = new SshComponent();    component.setCamelContext(camelContext);    Map<String, Object> parameters = new HashMap<>();    IntrospectionSupport.getProperties(configuration, parameters, null, false);    for (Map.Entry<String, Object> entry : parameters.entrySet()) {        Object value = entry.getValue();        Class<?> paramClass = value.getClass();        if (paramClass.getName().endsWith("NestedConfiguration")) {            Class nestedClass = null;            try {                nestedClass = (Class) paramClass.getDeclaredField("CAMEL_NESTED_CLASS").get(null);                HashMap<String, Object> nestedParameters = new HashMap<>();                IntrospectionSupport.getProperties(value, nestedParameters, null, false);                Object nestedProperty = nestedClass.newInstance();                CamelPropertiesHelper.setCamelProperties(camelContext, nestedProperty, nestedParameters, false);                entry.setValue(nestedProperty);            } catch (NoSuchFieldException e) {            }        }    }    CamelPropertiesHelper.setCamelProperties(camelContext, component, parameters, false);    if (ObjectHelper.isNotEmpty(customizers)) {        for (ComponentCustomizer<SshComponent> customizer : customizers) {            boolean useCustomizer = (customizer instanceof HasId) ? HierarchicalPropertiesEvaluator.evaluate(applicationContext.getEnvironment(), "camel.component.customizer", "camel.component.ssh.customizer", ((HasId) customizer).getId()) : HierarchicalPropertiesEvaluator.evaluate(applicationContext.getEnvironment(), "camel.component.customizer", "camel.component.ssh.customizer");            if (useCustomizer) {                                customizer.customize(component);            }        }    }    return component;}
1
private void executeFunctionLocally(final Function<?> fn, final FunctionContext cx, final ResultSender sender, DistributionManager dm)
{    FunctionStats stats = FunctionStats.getFunctionStats(fn.getId(), dm.getSystem());    try {        long start = stats.startTime();        stats.startFunctionExecution(fn.hasResult());        if (logger.isDebugEnabled()) {                    }        fn.execute(cx);        stats.endFunctionExecution(start, fn.hasResult());    } catch (FunctionInvocationTargetException fite) {        FunctionException functionException;        if (fn.isHA()) {            functionException = new FunctionException(new InternalFunctionInvocationTargetException(fite.getMessage()));        } else {            functionException = new FunctionException(fite);        }        handleException(functionException, fn, sender, dm);    } catch (BucketMovedException bme) {        FunctionException functionException;        if (fn.isHA()) {            functionException = new FunctionException(new InternalFunctionInvocationTargetException(bme));        } else {            functionException = new FunctionException(bme);        }        handleException(functionException, fn, sender, dm);    } catch (VirtualMachineError e) {        SystemFailure.initiateFailure(e);        throw e;    } catch (Throwable t) {        SystemFailure.checkFailure();        handleException(t, fn, sender, dm);    }}
1
public static void main(String[] args) throws Exception
{        int exitCode = ToolRunner.run(new MetadataCleanupJob(), args);    System.exit(exitCode);}
1
public void tearDown() throws Exception
{        qb.tearDown();}
1
public void testGlobTargetMissingSingleLevel()
{    try {        Path listFile = new Path("target/tmp1/listing").makeQualified(fs.getUri(), fs.getWorkingDirectory());        addEntries(listFile, "*");        createFiles("multifile/file3", "multifile/file4", "multifile/file5");        createFiles("singledir/dir2/file6");        runTest(listFile, target, false, false);        checkResult(target, 2, "multifile/file3", "multifile/file4", "multifile/file5", "singledir/dir2/file6");    } catch (IOException e) {                Assert.fail("distcp failure");    } finally {        TestDistCpUtils.delete(fs, root);        TestDistCpUtils.delete(fs, "target/tmp1");    }}
1
public ConfigDef config()
{        return new ConfigDef();}
1
public static FederationAMRMProxyPolicy loadAMRMPolicy(String queue, FederationAMRMProxyPolicy oldPolicy, Configuration conf, FederationStateStoreFacade federationFacade, SubClusterId homeSubClusterId) throws FederationPolicyInitializationException
{        SubClusterPolicyConfiguration configuration = loadPolicyConfiguration(queue, conf, federationFacade);        FederationPolicyInitializationContext context = new FederationPolicyInitializationContext(configuration, federationFacade.getSubClusterResolver(), federationFacade, homeSubClusterId);        FederationPolicyManager federationPolicyManager = instantiatePolicyManager(configuration.getType());            federationPolicyManager.setQueue(configuration.getQueue());    return federationPolicyManager.getAMRMPolicy(context, oldPolicy);}
1
public void testSendFileChunksStopOnError() throws Exception
{    final List<FileChunkResponse> unrepliedChunks = new CopyOnWriteArrayList<>();    final AtomicInteger sentChunks = new AtomicInteger();    final TestRecoveryTargetHandler recoveryTarget = new TestRecoveryTargetHandler() {        final AtomicLong chunkNumberGenerator = new AtomicLong();        @Override        public void writeFileChunk(StoreFileMetaData md, long position, BytesReference content, boolean lastChunk, int totalTranslogOps, ActionListener<Void> listener) {            final long chunkNumber = chunkNumberGenerator.getAndIncrement();                        unrepliedChunks.add(new FileChunkResponse(chunkNumber, listener));            sentChunks.incrementAndGet();        }    };    final int maxConcurrentChunks = between(1, 4);    final int chunkSize = between(1, 16);    final RecoverySourceHandler handler = new RecoverySourceHandler(null, new AsyncRecoveryTarget(recoveryTarget, recoveryExecutor), threadPool, getStartRecoveryRequest(), chunkSize, maxConcurrentChunks);    Store store = newStore(createTempDir(), false);    List<StoreFileMetaData> files = generateFiles(store, between(1, 10), () -> between(1, chunkSize * 20));    int totalChunks = files.stream().mapToInt(md -> ((int) md.length() + chunkSize - 1) / chunkSize).sum();    SetOnce<Exception> sendFilesError = new SetOnce<>();    CountDownLatch sendFilesLatch = new CountDownLatch(1);    handler.sendFiles(store, files.toArray(new StoreFileMetaData[0]), () -> 0, new LatchedActionListener<>(ActionListener.wrap(r -> sendFilesError.set(null), e -> sendFilesError.set(e)), sendFilesLatch));    assertBusy(() -> assertThat(sentChunks.get(), equalTo(Math.min(totalChunks, maxConcurrentChunks))));    List<FileChunkResponse> failedChunks = randomSubsetOf(between(1, unrepliedChunks.size()), unrepliedChunks);    CountDownLatch replyLatch = new CountDownLatch(failedChunks.size());    failedChunks.forEach(c -> {        c.listener.onFailure(new IllegalStateException("test chunk exception"));        replyLatch.countDown();    });    replyLatch.await();    unrepliedChunks.removeAll(failedChunks);    unrepliedChunks.forEach(c -> {        if (randomBoolean()) {            c.listener.onFailure(new RuntimeException("test"));        } else {            c.listener.onResponse(null);        }    });    sendFilesLatch.await();    assertThat(sendFilesError.get(), instanceOf(IllegalStateException.class));    assertThat(sendFilesError.get().getMessage(), containsString("test chunk exception"));    assertThat("no more chunks should be sent", sentChunks.get(), equalTo(Math.min(totalChunks, maxConcurrentChunks)));    store.close();}
1
private boolean syncRowCells(Context context, byte[] rowKey, CellScanner sourceCells, CellScanner targetCells) throws IOException, InterruptedException
{    Put put = null;    Delete delete = null;    long matchingCells = 0;    boolean matchingRow = true;    Cell sourceCell = sourceCells.nextCellInRow();    Cell targetCell = targetCells.nextCellInRow();    while (sourceCell != null || targetCell != null) {        int cellKeyComparison = compareCellKeysWithinRow(sourceCell, targetCell);        if (cellKeyComparison < 0) {            if (LOG.isDebugEnabled()) {                            }            context.getCounter(Counter.TARGETMISSINGCELLS).increment(1);            matchingRow = false;            if (!dryRun && doPuts) {                if (put == null) {                    put = new Put(rowKey);                }                put.add(sourceCell);            }            sourceCell = sourceCells.nextCellInRow();        } else if (cellKeyComparison > 0) {            if (LOG.isDebugEnabled()) {                            }            context.getCounter(Counter.SOURCEMISSINGCELLS).increment(1);            matchingRow = false;            if (!dryRun && doDeletes) {                if (delete == null) {                    delete = new Delete(rowKey);                }                                delete.addColumn(CellUtil.cloneFamily(targetCell), CellUtil.cloneQualifier(targetCell), targetCell.getTimestamp());            }            targetCell = targetCells.nextCellInRow();        } else {                        if (CellUtil.matchingValue(sourceCell, targetCell)) {                matchingCells++;            } else {                if (LOG.isDebugEnabled()) {                                                                            }                context.getCounter(Counter.DIFFERENTCELLVALUES).increment(1);                matchingRow = false;                if (!dryRun && doPuts) {                                        if (put == null) {                        put = new Put(rowKey);                    }                    put.add(sourceCell);                }            }            sourceCell = sourceCells.nextCellInRow();            targetCell = targetCells.nextCellInRow();        }        if (!dryRun && sourceTableHash.scanBatch > 0) {            if (put != null && put.size() >= sourceTableHash.scanBatch) {                context.write(new ImmutableBytesWritable(rowKey), put);                put = null;            }            if (delete != null && delete.size() >= sourceTableHash.scanBatch) {                context.write(new ImmutableBytesWritable(rowKey), delete);                delete = null;            }        }    }    if (!dryRun) {        if (put != null) {            context.write(new ImmutableBytesWritable(rowKey), put);        }        if (delete != null) {            context.write(new ImmutableBytesWritable(rowKey), delete);        }    }    if (matchingCells > 0) {        context.getCounter(Counter.MATCHINGCELLS).increment(matchingCells);    }    if (matchingRow) {        context.getCounter(Counter.MATCHINGROWS).increment(1);        return true;    } else {        context.getCounter(Counter.ROWSWITHDIFFS).increment(1);        return false;    }}
1
public void onRegistered()
{    super.onRegistered();    }
1
public void error(String msg, Object[] os, Throwable t)
{    os = addProcessorAndThrowable(os, t);    msg = "{} " + msg + ": {}";        if (logger.isDebugEnabled()) {            }}
1
public void run()
{        try {        while (running) {            long now = Time.now();            if (lastMasterKeyUpdate + keyUpdateInterval < now) {                try {                    rollMasterKey();                    lastMasterKeyUpdate = now;                } catch (IOException e) {                                    }            }            if (lastTokenCacheCleanup + tokenRemoverScanInterval < now) {                removeExpiredToken();                lastTokenCacheCleanup = now;            }            try {                                Thread.sleep(Math.min(5000, keyUpdateInterval));            } catch (InterruptedException ie) {                            }        }    } catch (Throwable t) {                Runtime.getRuntime().exit(-1);    }}
1
public void removeAttribute(final String name)
{    checkValid();        attributes.removeAttribute(name);}
1
public void testDatanodeCursor() throws Exception
{    Configuration conf = new Configuration();    conf.setLong(DFS_DATANODE_SCAN_PERIOD_HOURS_KEY, 100L);    conf.set(INTERNAL_VOLUME_SCANNER_SCAN_RESULT_HANDLER, TestScanResultHandler.class.getName());    conf.setLong(INTERNAL_DFS_BLOCK_SCANNER_CURSOR_SAVE_INTERVAL_MS, 0L);    final TestContext ctx = new TestContext(conf, 1);    final int NUM_EXPECTED_BLOCKS = 10;    ctx.createFiles(0, NUM_EXPECTED_BLOCKS, 1);    final TestScanResultHandler.Info info = TestScanResultHandler.getInfo(ctx.volumes.get(0));    synchronized (info) {        info.sem = new Semaphore(5);        info.shouldRun = true;        info.notify();    }        GenericTestUtils.waitFor(new Supplier<Boolean>() {        @Override        public Boolean get() {            synchronized (info) {                return info.blocksScanned == 5;            }        }    }, 3, 30000);    synchronized (info) {        assertEquals(5, info.goodBlocks.size());        assertEquals(5, info.blocksScanned);        info.shouldRun = false;    }    ctx.datanode.shutdown();    URI vURI = ctx.volumes.get(0).getStorageLocation().getUri();    File cursorPath = new File(new File(new File(new File(vURI), "current"), ctx.bpids[0]), "scanner.cursor");    assertTrue("Failed to find cursor save file in " + cursorPath.getAbsolutePath(), cursorPath.exists());    Set<ExtendedBlock> prevGoodBlocks = new HashSet<ExtendedBlock>();    synchronized (info) {        info.sem = new Semaphore(4);        prevGoodBlocks.addAll(info.goodBlocks);        info.goodBlocks.clear();    }                ctx.cluster.restartDataNode(0);    synchronized (info) {        info.shouldRun = true;        info.notify();    }    GenericTestUtils.waitFor(new Supplier<Boolean>() {        @Override        public Boolean get() {            synchronized (info) {                if (info.blocksScanned != 9) {                                    }                return info.blocksScanned == 9;            }        }    }, 3, 30000);    synchronized (info) {        assertEquals(4, info.goodBlocks.size());        info.goodBlocks.addAll(prevGoodBlocks);        assertEquals(9, info.goodBlocks.size());        assertEquals(9, info.blocksScanned);    }    ctx.datanode.shutdown();        synchronized (info) {        info.sem = null;        info.shouldRun = false;        info.goodBlocks.clear();    }    ctx.cluster.restartDataNode(0);    synchronized (info) {        info.shouldRun = true;        info.notify();    }    Thread.sleep(3000);    synchronized (info) {        assertTrue(info.goodBlocks.isEmpty());    }    ctx.close();}
1
 String generateJwtAssertion()
{    final long utcPlusWindow = Clock.systemUTC().millis() / 1000 + JWT_CLAIM_WINDOW;    final StringBuilder claim = new StringBuilder().append("{\"iss\":\"").append(config.getClientId()).append("\",\"sub\":\"").append(config.getUserName()).append("\",\"aud\":\"").append(config.getLoginUrl()).append("\",\"exp\":\"").append(utcPlusWindow).append("\"}");    final StringBuilder token = new StringBuilder(JWT_HEADER).append('.').append(Base64.getUrlEncoder().encodeToString(claim.toString().getBytes(StandardCharsets.UTF_8)));    final KeyStoreParameters keyStoreParameters = config.getKeystore();    keyStoreParameters.setCamelContext(camelContext);    try {        final KeyStore keystore = keyStoreParameters.createKeyStore();        final Enumeration<String> aliases = keystore.aliases();        String alias = null;        while (aliases.hasMoreElements()) {            String tmp = aliases.nextElement();            if (keystore.isKeyEntry(tmp)) {                if (alias == null) {                    alias = tmp;                } else {                    throw new IllegalArgumentException("The given keystore `" + keyStoreParameters.getResource() + "` contains more than one key entry, expecting only one");                }            }        }        PrivateKey key = (PrivateKey) keystore.getKey(alias, keyStoreParameters.getPassword().toCharArray());        Signature signature = Signature.getInstance(JWT_SIGNATURE_ALGORITHM);        signature.initSign(key);        signature.update(token.toString().getBytes(StandardCharsets.UTF_8));        byte[] signed = signature.sign();        token.append('.').append(Base64.getUrlEncoder().encodeToString(signed));                try {            key.destroy();        } catch (javax.security.auth.DestroyFailedException ex) {                    }    } catch (IOException | GeneralSecurityException e) {        throw new IllegalStateException(e);    }    return token.toString();}
1
public MiniHBaseCluster startMiniCluster(StartMiniClusterOption option) throws Exception
{            if (miniClusterRunning) {        throw new IllegalStateException("A mini-cluster is already running");    }    miniClusterRunning = true;    setupClusterTestDir();    System.setProperty(TEST_DIRECTORY_KEY, this.clusterTestDir.getPath());        if (dfsCluster == null) {                dfsCluster = startMiniDFSCluster(option.getNumDataNodes(), option.getDataNodeHosts());    } else {            }        if (getZkCluster() == null) {        startMiniZKCluster(option.getNumZkServers());    }        return startMiniHBaseCluster(option);}
1
public void testSnapshotWhenSnapshotHFileCleanerRunning() throws Exception
{        loadDataAndFlush();    SnapshotHFileCleaner cleaner = new SnapshotHFileCleaner();    cleaner.init(ImmutableMap.of(HMaster.MASTER, TEST_UTIL.getHBaseCluster().getMaster()));    cleaner.setConf(CONF);    FileSystem fs = FSUtils.getCurrentFileSystem(CONF);    List<Path> fileNames = listHFileNames(fs, FSUtils.getTableDir(FSUtils.getRootDir(CONF), TABLE_NAME));    List<FileStatus> files = new ArrayList<>();    for (Path fileName : fileNames) {        files.add(fs.getFileStatus(fileName));    }    TEST_UTIL.getAdmin().snapshot("snapshotName_prev", TABLE_NAME);    Assert.assertEquals(Lists.newArrayList(cleaner.getDeletableFiles(files)).size(), 0);    TEST_UTIL.getAdmin().deleteSnapshot("snapshotName_prev");    cleaner.getFileCacheForTesting().triggerCacheRefreshForTesting();    Assert.assertEquals(Lists.newArrayList(cleaner.getDeletableFiles(files)).size(), 100);    Runnable snapshotRunnable = () -> {        try {                        for (int k = 0; k < 5; k++) {                TEST_UTIL.getAdmin().snapshot("snapshotName_" + k, TABLE_NAME);            }        } catch (Exception e) {                    }    };    final AtomicBoolean success = new AtomicBoolean(true);    Runnable cleanerRunnable = () -> {        try {            while (!isAnySnapshots(fs)) {                                Thread.sleep(100);            }            for (int k = 0; k < 5; k++) {                cleaner.getFileCacheForTesting().triggerCacheRefreshForTesting();                Iterable<FileStatus> toDeleteFiles = cleaner.getDeletableFiles(files);                List<FileStatus> deletableFiles = Lists.newArrayList(toDeleteFiles);                                for (int i = 0; i < deletableFiles.size(); i++) {                                    }                if (deletableFiles.size() > 0) {                    success.set(false);                }            }        } catch (Exception e) {                    }    };    Thread t1 = new Thread(snapshotRunnable);    t1.start();    Thread t2 = new Thread(cleanerRunnable);    t2.start();    t1.join();    t2.join();    Assert.assertTrue(success.get());}
1
private void listBeans(JsonGenerator jg, ObjectName qry, String attribute, HttpServletResponse response) throws IOException
{        Set<ObjectName> names = null;    names = mBeanServer.queryNames(qry, null);    jg.writeArrayFieldStart("beans");    Iterator<ObjectName> it = names.iterator();    while (it.hasNext()) {        ObjectName oname = it.next();        MBeanInfo minfo;        String code = "";        Object attributeinfo = null;        try {            minfo = mBeanServer.getMBeanInfo(oname);            code = minfo.getClassName();            String prs = "";            try {                if ("org.apache.commons.modeler.BaseModelMBean".equals(code)) {                    prs = "modelerType";                    code = (String) mBeanServer.getAttribute(oname, prs);                }                if (attribute != null) {                    prs = attribute;                    attributeinfo = mBeanServer.getAttribute(oname, prs);                }            } catch (AttributeNotFoundException e) {                                                            } catch (MBeanException e) {                                                            } catch (RuntimeException e) {                                                                            } catch (ReflectionException e) {                                                                            }        } catch (InstanceNotFoundException e) {                        continue;        } catch (IntrospectionException e) {                                                continue;        } catch (ReflectionException e) {                                                continue;        }        jg.writeStartObject();        jg.writeStringField("name", oname.toString());        jg.writeStringField("modelerType", code);        if ((attribute != null) && (attributeinfo == null)) {            jg.writeStringField("result", "ERROR");            jg.writeStringField("message", "No attribute with name " + attribute + " was found.");            jg.writeEndObject();            jg.writeEndArray();            jg.close();            response.setStatus(HttpServletResponse.SC_NOT_FOUND);            return;        }        if (attribute != null) {            writeAttribute(jg, attribute, attributeinfo);        } else {            MBeanAttributeInfo[] attrs = minfo.getAttributes();            for (int i = 0; i < attrs.length; i++) {                writeAttribute(jg, oname, attrs[i]);            }        }        jg.writeEndObject();    }    jg.writeEndArray();}
1
private List<ConsumerRecord<K, V>> fetchRecords(CompletedFetch completedFetch, int maxRecords)
{    if (!subscriptions.isAssigned(completedFetch.partition)) {                    } else if (!subscriptions.isFetchable(completedFetch.partition)) {                            } else {        SubscriptionState.FetchPosition position = subscriptions.position(completedFetch.partition);        if (completedFetch.nextFetchOffset == position.offset) {            List<ConsumerRecord<K, V>> partRecords = completedFetch.fetchRecords(maxRecords);            if (completedFetch.nextFetchOffset > position.offset) {                SubscriptionState.FetchPosition nextPosition = new SubscriptionState.FetchPosition(completedFetch.nextFetchOffset, completedFetch.lastEpoch, position.currentLeader);                log.trace("Returning fetched records at offset {} for assigned partition {} and update " + "position to {}", position, completedFetch.partition, nextPosition);                subscriptions.position(completedFetch.partition, nextPosition);            }            Long partitionLag = subscriptions.partitionLag(completedFetch.partition, isolationLevel);            if (partitionLag != null)                this.sensors.recordPartitionLag(completedFetch.partition, partitionLag);            Long lead = subscriptions.partitionLead(completedFetch.partition);            if (lead != null) {                this.sensors.recordPartitionLead(completedFetch.partition, lead);            }            return partRecords;        } else {                                            }    }    log.trace("Draining fetched records for partition {}", completedFetch.partition);    completedFetch.drain();    return emptyList();}
1
private void cleanupGraceHashJoin()
{    for (byte pos = 0; pos < mapJoinTables.length; pos++) {        if (pos != conf.getPosBigTable()) {                        HybridHashTableContainer container = (HybridHashTableContainer) mapJoinTables[pos];            container.clear();        }    }}
1
protected QueryResponse query(boolean setDistribParams, SolrParams p) throws Exception
{    final ModifiableSolrParams params = new ModifiableSolrParams(p);        params.set("distrib", "false");    final QueryResponse controlRsp = controlClient.query(params);    validateControlData(controlRsp);    if (shardCount == 0) {                return controlRsp;    }    params.remove("distrib");    if (setDistribParams)        setDistributedParams(params);    QueryResponse rsp = queryServer(params);    compareResponses(rsp, controlRsp);    if (stress > 0) {                Thread[] threads = new Thread[nThreads];        for (int i = 0; i < threads.length; i++) {            threads[i] = new Thread() {                @Override                public void run() {                    for (int j = 0; j < stress; j++) {                        int which = r.nextInt(clients.size());                        SolrClient client = clients.get(which);                        try {                            QueryResponse rsp = client.query(new ModifiableSolrParams(params));                            if (verifyStress) {                                compareResponses(rsp, controlRsp);                            }                        } catch (SolrServerException | IOException e) {                            throw new RuntimeException(e);                        }                    }                }            };            threads[i].start();        }        for (Thread thread : threads) {            thread.join();        }    }    return rsp;}
1
 final void restoreLock(TEnvironment env)
{    if (!lockedWhenLoading) {                return;    }    if (isFinished()) {                return;    }    if (isBypass()) {                return;    }        if (getState() == ProcedureState.WAITING && !holdLock(env)) {                lockedWhenLoading = false;        return;    }        LockState state = acquireLock(env);    assert state == LockState.LOCK_ACQUIRED;}
1
protected void doPostTearDown() throws Exception
{        POST_TEAR_DOWN.incrementAndGet();    super.doPostTearDown();}
1
public void orderedEqualityLeftJoin() throws Throwable
{    final DrillbitContext bitContext = mockDrillbitContext();    final UserClientConnection connection = Mockito.mock(UserClientConnection.class);    final PhysicalPlanReader reader = PhysicalPlanReaderTestFactory.defaultPhysicalPlanReader(c, new StoragePluginRegistryImpl(bitContext));    final PhysicalPlan plan = reader.readPhysicalPlan(Files.asCharSource(DrillFileUtils.getResourceAsFile("/join/merge_single_batch.json"), Charsets.UTF_8).read().replace("#{LEFT_FILE}", DrillFileUtils.getResourceAsFile("/join/merge_single_batch.left.json").toURI().toString()).replace("#{RIGHT_FILE}", DrillFileUtils.getResourceAsFile("/join/merge_single_batch.right.json").toURI().toString()));    final FunctionImplementationRegistry registry = new FunctionImplementationRegistry(c);    final FragmentContextImpl context = new FragmentContextImpl(bitContext, PlanFragment.getDefaultInstance(), connection, registry);    final SimpleRootExec exec = new SimpleRootExec(ImplCreator.getExec(context, (FragmentRoot) plan.getSortedOperators(false).iterator().next()));    int totalRecordCount = 0;    final StringBuilder sb = new StringBuilder();    while (exec.next()) {        totalRecordCount += exec.getRecordCount();        sb.append(String.format("got next with record count: %d (total: %d):\n", exec.getRecordCount(), totalRecordCount));        sb.append("       t1                 t2\n");        for (int valueIdx = 0; valueIdx < exec.getRecordCount(); valueIdx++) {            final List<Object> row = Lists.newArrayList();            for (final ValueVector v : exec) {                row.add(v.getField().getName() + ":" + v.getAccessor().getObject(valueIdx));            }            for (final Object cell : row) {                if (cell == null) {                    sb.append("<null>    ");                    continue;                }                final int len = cell.toString().length();                sb.append(cell + " ");                for (int i = 0; i < (10 - len); ++i) {                    sb.append(" ");                }            }            sb.append('\n');        }    }        assertEquals(25, totalRecordCount);    if (context.getExecutorState().getFailureCause() != null) {        throw context.getExecutorState().getFailureCause();    }    assertTrue(!context.getExecutorState().isFailed());}
1
public synchronized void validateSafeModeExitRules(String ruleName, EventPublisher eventQueue)
{    if (exitRules.get(ruleName) != null) {        validatedRules.add(ruleName);    } else {                    }    if (validatedRules.size() == exitRules.size()) {                        exitSafeMode(eventQueue);    }}
1
public ScheduledFuture<VolumeProvisioningResults> schedule(VolumeProvisioningTask volumeProvisioningTask, int delaySecond)
{        return provisioningExecutor.schedule(volumeProvisioningTask, delaySecond, TimeUnit.SECONDS);}
1
public static void waitForJob(Job job) throws Exception
{    job.submit();    while (!job.isComplete()) {                sleep(100);    }        if (!job.isSuccessful()) {        throw new RuntimeException("job failed " + job.getJobName());    }}
1
public void testDumpModelAsXml() throws Exception
{    String xml = ModelHelper.dumpModelAsXml(context, context.getRouteDefinition("myRoute"));    assertNotNull(xml);        Document doc = new XmlConverter().toDOMDocument(xml, null);    NodeList nodes = doc.getElementsByTagName("simple");    assertEquals(1, nodes.getLength());    Element node = (Element) nodes.item(0);    assertNotNull("Node <simple> expected to be instanceof Element", node);    assertEquals("${body}", node.getTextContent());    nodes = doc.getElementsByTagName("split");    assertEquals(1, nodes.getLength());    nodes = doc.getElementsByTagName("to");    assertEquals(1, nodes.getLength());    node = (Element) nodes.item(0);    assertNotNull("Node <to> expected to be instanceof Element", node);    assertEquals("mock:sub", node.getAttribute("uri"));    assertEquals("myMock", node.getAttribute("id"));    assertEquals("true", node.getAttribute("customId"));}
1
public void memberJoined(DistributionManager distributionManager, InternalDistributedMember id)
{    if (logger.isDebugEnabled()) {            }    if (service.isManager()) {        if (logger.isDebugEnabled()) {                    }        service.getFederatingManager().addMember(id);    }    service.getUniversalListenerContainer().memberJoined(id);}
1
protected void checkUsageLimit(File dir, PercentLimitUsage<?> storeUsage, int percentLimit) throws ConfigurationException
{    if (dir != null) {        dir = StoreUtil.findParentDirectory(dir);        String storeName = storeUsage instanceof StoreUsage ? "Store" : "Temporary Store";        long storeLimit = storeUsage.getLimit();        long storeCurrent = storeUsage.getUsage();        long totalSpace = storeUsage.getTotal() > 0 ? storeUsage.getTotal() : dir.getTotalSpace();        long totalUsableSpace = (storeUsage.getTotal() > 0 ? storeUsage.getTotal() : dir.getUsableSpace()) + storeCurrent;        if (totalUsableSpace < 0 || totalSpace < 0) {            final String message = "File system space reported by: " + dir + " was negative, possibly a huge file system, set a sane usage.total to provide some guidance";                        throw new ConfigurationException(message);        }                long bytePercentLimit = totalSpace * percentLimit / 100;        int oneMeg = 1024 * 1024;                if (diskUsageCheckRegrowThreshold > -1 && percentLimit > 0 && storeUsage.getTotal() == 0 && storeLimit < bytePercentLimit && storeLimit < totalUsableSpace) {                                    long newLimit = bytePercentLimit > totalUsableSpace ? totalUsableSpace : bytePercentLimit;                        if (newLimit - storeLimit >= diskUsageCheckRegrowThreshold) {                                storeUsage.setLimit(newLimit);                            }                } else if (storeLimit > totalUsableSpace) {            final String message = storeName + " limit is " + storeLimit / oneMeg + " mb (current store usage is " + storeCurrent / oneMeg + " mb). The data directory: " + dir.getAbsolutePath() + " only has " + totalUsableSpace / oneMeg + " mb of usable space.";            if (!isAdjustUsageLimits()) {                                throw new ConfigurationException(message);            }            if (percentLimit > 0) {                            } else {                            }            storeUsage.setLimit(totalUsableSpace);        }    }}
1
public final void snapshotState(FunctionSnapshotContext context) throws Exception
{    if (!running) {            } else {        unionOffsetStates.clear();        final AbstractFetcher<?, ?> fetcher = this.kafkaFetcher;        if (fetcher == null) {                        for (Map.Entry<KafkaTopicPartition, Long> subscribedPartition : subscribedPartitionsToStartOffsets.entrySet()) {                unionOffsetStates.add(Tuple2.of(subscribedPartition.getKey(), subscribedPartition.getValue()));            }            if (offsetCommitMode == OffsetCommitMode.ON_CHECKPOINTS) {                                                pendingOffsetsToCommit.put(context.getCheckpointId(), restoredState);            }        } else {            HashMap<KafkaTopicPartition, Long> currentOffsets = fetcher.snapshotCurrentState();            if (offsetCommitMode == OffsetCommitMode.ON_CHECKPOINTS) {                                                pendingOffsetsToCommit.put(context.getCheckpointId(), currentOffsets);            }            for (Map.Entry<KafkaTopicPartition, Long> kafkaTopicPartitionLongEntry : currentOffsets.entrySet()) {                unionOffsetStates.add(Tuple2.of(kafkaTopicPartitionLongEntry.getKey(), kafkaTopicPartitionLongEntry.getValue()));            }        }        if (offsetCommitMode == OffsetCommitMode.ON_CHECKPOINTS) {                        while (pendingOffsetsToCommit.size() > MAX_NUM_PENDING_CHECKPOINTS) {                pendingOffsetsToCommit.remove(0);            }        }    }}
1
public static Map<String, byte[]> getTrustedKeys(SolrZkClient zk, String dir)
{    Map<String, byte[]> result = new HashMap<>();    try {        List<String> children = zk.getChildren("/keys/" + dir, null, true);        for (String key : children) {            if (key.endsWith(".der"))                result.put(key, zk.getData("/keys/" + dir + "/" + key, null, null, true));        }    } catch (KeeperException.NoNodeException e) {                return Collections.EMPTY_MAP;    } catch (InterruptedException e) {        Thread.currentThread().interrupt();        throw new SolrException(ErrorCode.SERVER_ERROR, "Unable to read crypto keys", e);    } catch (KeeperException e) {        throw new SolrException(ErrorCode.SERVER_ERROR, "Unable to read crypto keys", e);    }    return result;}
1
public void createCatalog(Catalog cat) throws MetaException
{        boolean committed = false;    MCatalog mCat = catToMCat(cat);    try {        openTransaction();        pm.makePersistent(mCat);        committed = commitTransaction();    } finally {        if (!committed) {            rollbackTransaction();        }    }}
1
public Response initiateRevertFlowVersion(@ApiParam("The process group id.") @PathParam("id") final String groupId, @ApiParam(value = "The controller service configuration details.", required = true) final VersionControlInformationEntity requestEntity)
{        final RevisionDTO revisionDto = requestEntity.getProcessGroupRevision();    if (revisionDto == null) {        throw new IllegalArgumentException("Process Group Revision must be specified");    }    final VersionControlInformationDTO requestVersionControlInfoDto = requestEntity.getVersionControlInformation();    if (requestVersionControlInfoDto == null) {        throw new IllegalArgumentException("Version Control Information must be supplied.");    }    if (requestVersionControlInfoDto.getGroupId() == null) {        throw new IllegalArgumentException("The Process Group ID must be supplied.");    }    if (!requestVersionControlInfoDto.getGroupId().equals(groupId)) {        throw new IllegalArgumentException("The Process Group ID in the request body does not match the Process Group ID of the requested resource.");    }    if (requestVersionControlInfoDto.getBucketId() == null) {        throw new IllegalArgumentException("The Bucket ID must be supplied.");    }    if (requestVersionControlInfoDto.getFlowId() == null) {        throw new IllegalArgumentException("The Flow ID must be supplied.");    }    if (requestVersionControlInfoDto.getRegistryId() == null) {        throw new IllegalArgumentException("The Registry ID must be supplied.");    }    if (requestVersionControlInfoDto.getVersion() == null) {        throw new IllegalArgumentException("The Version of the flow must be supplied.");    }    if (isDisconnectedFromCluster()) {        verifyDisconnectedNodeModification(requestEntity.isDisconnectedNodeAcknowledged());    }                final boolean replicateRequest = isReplicateRequest();    final ComponentLifecycle componentLifecycle = replicateRequest ? clusterComponentLifecycle : localComponentLifecycle;    final NiFiUser user = NiFiUserUtils.getNiFiUser();        final VersionedFlowSnapshot flowSnapshot = serviceFacade.getVersionedFlowSnapshot(requestEntity.getVersionControlInformation(), true);            serviceFacade.discoverCompatibleBundles(flowSnapshot.getFlowContents());        serviceFacade.resolveInheritedControllerServices(flowSnapshot, groupId, NiFiUserUtils.getNiFiUser());        final Set<AffectedComponentEntity> affectedComponents = serviceFacade.getComponentsAffectedByVersionChange(groupId, flowSnapshot);        final InitiateChangeFlowVersionRequestWrapper requestWrapper = new InitiateChangeFlowVersionRequestWrapper(requestEntity, componentLifecycle, getAbsolutePath(), affectedComponents, replicateRequest, flowSnapshot);    final Revision requestRevision = getRevision(requestEntity.getProcessGroupRevision(), groupId);    return withWriteLock(serviceFacade, requestWrapper, requestRevision, lookup -> {                final ProcessGroupAuthorizable groupAuthorizable = lookup.getProcessGroup(groupId);        authorizeProcessGroup(groupAuthorizable, authorizer, lookup, RequestAction.READ, true, false, true, true, true);        authorizeProcessGroup(groupAuthorizable, authorizer, lookup, RequestAction.WRITE, true, false, true, true, false);        final VersionedProcessGroup groupContents = flowSnapshot.getFlowContents();        final Set<ConfigurableComponent> restrictedComponents = FlowRegistryUtils.getRestrictedComponents(groupContents, serviceFacade);        restrictedComponents.forEach(restrictedComponent -> {            final ComponentAuthorizable restrictedComponentAuthorizable = lookup.getConfigurableComponent(restrictedComponent);            authorizeRestrictions(authorizer, restrictedComponentAuthorizable);        });        final Map<String, VersionedParameterContext> parameterContexts = flowSnapshot.getParameterContexts();        if (parameterContexts != null) {            parameterContexts.values().forEach(context -> AuthorizeParameterReference.authorizeParameterContextAddition(context, serviceFacade, authorizer, lookup, user));        }    }, () -> {                        serviceFacade.verifyCanRevertLocalModifications(groupId, flowSnapshot);    }, (revision, wrapper) -> {        final VersionControlInformationEntity versionControlInformationEntity = wrapper.getVersionControlInformationEntity();        final VersionControlInformationDTO versionControlInformationDTO = versionControlInformationEntity.getVersionControlInformation();                final VersionControlInformationEntity currentVersionEntity = serviceFacade.getVersionControlInformation(groupId);        if (currentVersionEntity == null) {            throw new IllegalStateException("Process Group cannot be reverted to the previous version of the flow because Process Group is not under Version Control.");        }        final VersionControlInformationDTO currentVersion = currentVersionEntity.getVersionControlInformation();        if (!currentVersion.getBucketId().equals(versionControlInformationDTO.getBucketId())) {            throw new IllegalArgumentException("The Version Control Information provided does not match the flow that the Process Group is currently synchronized with.");        }        if (!currentVersion.getFlowId().equals(versionControlInformationDTO.getFlowId())) {            throw new IllegalArgumentException("The Version Control Information provided does not match the flow that the Process Group is currently synchronized with.");        }        if (!currentVersion.getRegistryId().equals(versionControlInformationDTO.getRegistryId())) {            throw new IllegalArgumentException("The Version Control Information provided does not match the flow that the Process Group is currently synchronized with.");        }        if (!currentVersion.getVersion().equals(versionControlInformationDTO.getVersion())) {            throw new IllegalArgumentException("The Version Control Information provided does not match the flow that the Process Group is currently synchronized with.");        }        final String idGenerationSeed = getIdGenerationSeed().orElse(null);                        final String requestId = UUID.randomUUID().toString();        final AsynchronousWebRequest<VersionControlInformationEntity, VersionControlInformationEntity> request = new StandardAsynchronousWebRequest<>(requestId, requestEntity, groupId, user, getUpdateSteps());                final Consumer<AsynchronousWebRequest<VersionControlInformationEntity, VersionControlInformationEntity>> updateTask = vcur -> {            try {                final VersionControlInformationEntity updatedVersionControlEntity = updateFlowVersion(groupId, wrapper.getComponentLifecycle(), wrapper.getExampleUri(), wrapper.getAffectedComponents(), wrapper.isReplicateRequest(), revision, versionControlInformationEntity, wrapper.getFlowSnapshot(), request, idGenerationSeed, false, true);                vcur.markStepComplete(updatedVersionControlEntity);            } catch (final ResumeFlowException rfe) {                                                                vcur.fail(rfe.getMessage());            } catch (final Exception e) {                                vcur.fail("Failed to update flow to new version due to " + e.getMessage());            }        };        requestManager.submitRequest("revert-requests", requestId, request, updateTask);                final VersionedFlowUpdateRequestDTO updateRequestDto = new VersionedFlowUpdateRequestDTO();        updateRequestDto.setComplete(request.isComplete());        updateRequestDto.setFailureReason(request.getFailureReason());        updateRequestDto.setLastUpdated(request.getLastUpdated());        updateRequestDto.setProcessGroupId(groupId);        updateRequestDto.setRequestId(requestId);        updateRequestDto.setState(request.getState());        updateRequestDto.setPercentCompleted(request.getPercentComplete());        updateRequestDto.setUri(generateResourceUri("versions", "revert-requests", requestId));        final VersionedFlowUpdateRequestEntity updateRequestEntity = new VersionedFlowUpdateRequestEntity();        final RevisionDTO groupRevision = serviceFacade.getProcessGroup(groupId).getRevision();        updateRequestEntity.setProcessGroupRevision(groupRevision);        updateRequestEntity.setRequest(updateRequestDto);        return generateOkResponse(updateRequestEntity).build();    });}
1
private void createCheckpoint(Path trashRoot, Date date) throws IOException
{    if (!fs.exists(new Path(trashRoot, CURRENT))) {        return;    }    Path checkpointBase;    synchronized (CHECKPOINT) {        checkpointBase = new Path(trashRoot, CHECKPOINT.format(date));    }    Path checkpoint = checkpointBase;    Path current = new Path(trashRoot, CURRENT);    int attempt = 0;    while (true) {        try {            fs.rename(current, checkpoint, Rename.NONE);                        break;        } catch (FileAlreadyExistsException e) {            if (++attempt > 1000) {                throw new IOException("Failed to checkpoint trash: " + checkpoint);            }            checkpoint = checkpointBase.suffix("-" + attempt);        }    }}
1
public void close()
{        synchronized (this.stateLock) {        if (this.closed) {            return;        }        this.closed = true;    }            releaseTable();        clearPartitions();}
1
public void testGetDownloadURL() throws Exception
{        final java.net.URL result = requestBody("direct://GETDOWNLOADURL", testFile.getID());    assertNotNull("getDownloadURL result", result);    }
1
public void run2() throws CacheException
{    SelectResults results = null;    QueryService qService = null;    try {        qService = (PoolManager.find(poolName)).getQueryService();    } catch (Exception e) {        Assert.fail("Failed to get QueryService.", e);    }    try {                Query query = qService.newQuery(queryString[0]);        results = (SelectResults) query.execute();    } catch (Exception e) {        Assert.fail("Failed executing " + queryString[0], e);    }    assertEquals(numberOfEntries, results.size());}
1
public void stopApp(Service app) throws JsonProcessingException
{    ObjectMapper mapper = new ObjectMapper();    mapper.configure(DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES, false);    String appInstanceId = app.getName();    String yarnFile = mapper.writeValueAsString(app);    ClientResponse response;    try {        response = asc.getApiClient(asc.getServicePath(appInstanceId)).put(ClientResponse.class, yarnFile);        if (response.getStatus() >= 299) {            String message = response.getEntity(String.class);            throw new RuntimeException("Failed : HTTP error code : " + response.getStatus() + " error: " + message);        }    } catch (UniformInterfaceException | ClientHandlerException | IOException e) {            }}
1
 void addTTLExpiryTask()
{    synchronized (regionExpiryLock) {        RegionTTLExpiryTask task = regionTTLExpiryTask;        if (task != null) {            task.cancel();        }        if (regionTimeToLive > 0) {            regionTTLExpiryTask = (RegionTTLExpiryTask) cache.getExpirationScheduler().addExpiryTask(new RegionTTLExpiryTask(this));            if (regionTTLExpiryTask != null) {                if (logger.isDebugEnabled()) {                                    }            }        } else {            regionTTLExpiryTask = null;        }    }}
1
public void cmdExecute(final Message clientMessage, final ServerConnection serverConnection, final SecurityService securityService, long start) throws IOException
{    Part eventIDPart = null, valuePart = null;    EventID event = null;    Object callbackArg = null;    CachedRegionHelper crHelper = serverConnection.getCachedRegionHelper();    StringBuffer errMessage = new StringBuffer();    serverConnection.setAsTrue(REQUIRES_RESPONSE);        int parts = clientMessage.getNumberOfParts();    eventIDPart = clientMessage.getPart(0);    if (eventIDPart == null) {                errMessage.append(" The event id for the get event value request is null.");        writeErrorResponse(clientMessage, MessageType.REQUESTDATAERROR, errMessage.toString(), serverConnection);        serverConnection.setAsTrue(RESPONDED);    } else {        try {            event = (EventID) eventIDPart.getObject();        } catch (Exception e) {            writeException(clientMessage, e, false, serverConnection);            serverConnection.setAsTrue(RESPONDED);            return;        }        if (parts > 1) {            valuePart = clientMessage.getPart(1);            try {                if (valuePart != null) {                    callbackArg = valuePart.getObject();                }            } catch (Exception e) {                writeException(clientMessage, e, false, serverConnection);                serverConnection.setAsTrue(RESPONDED);                return;            }        }        if (logger.isTraceEnabled()) {            logger.trace("{}: Received get event value request ({} bytes) from {}", serverConnection.getName(), clientMessage.getPayloadLength(), serverConnection.getSocketString());        }        CacheClientNotifier ccn = serverConnection.getAcceptor().getCacheClientNotifier();                HAContainerWrapper haContainer = (HAContainerWrapper) ccn.getHaContainer();        if (haContainer == null) {            String reason = " was not found during get event value request";            writeRegionDestroyedEx(clientMessage, "ha container", reason, serverConnection);            serverConnection.setAsTrue(RESPONDED);        } else {            Object[] valueAndIsObject = new Object[2];            try {                Object data = haContainer.get(new HAEventWrapper(event));                if (data == null) {                                        String msgStr = "No value found for " + event + " in " + haContainer.getName();                    writeErrorResponse(clientMessage, MessageType.REQUEST_EVENT_VALUE_ERROR, msgStr, serverConnection);                    serverConnection.setAsTrue(RESPONDED);                    return;                } else {                    if (logger.isDebugEnabled()) {                                            }                    Object val = ((ClientUpdateMessageImpl) data).getValueToConflate();                    if (!(val instanceof byte[])) {                        if (val instanceof CachedDeserializable) {                            val = ((CachedDeserializable) val).getSerializedValue();                        } else {                            val = CacheServerHelper.serialize(val);                        }                        ((ClientUpdateMessageImpl) data).setLatestValue(val);                    }                    valueAndIsObject[0] = val;                    valueAndIsObject[1] = Boolean.valueOf(((ClientUpdateMessageImpl) data).valueIsObject());                }            } catch (Exception e) {                writeException(clientMessage, e, false, serverConnection);                serverConnection.setAsTrue(RESPONDED);                return;            }            Object data = valueAndIsObject[0];            boolean isObject = (Boolean) valueAndIsObject[1];            writeResponse(data, callbackArg, clientMessage, isObject, serverConnection);            serverConnection.setAsTrue(RESPONDED);            ccn.getClientProxy(serverConnection.getProxyID()).getStatistics().incDeltaFullMessagesSent();            if (logger.isDebugEnabled()) {                            }        }    }}
1
public static boolean injectUpdateRandomPause()
{    if (updateRandomPause != null) {        Random rand = random();        if (null == rand)            return true;        Pair<Boolean, Integer> pair = parseValue(updateRandomPause);        boolean enabled = pair.first();        int chanceIn100 = pair.second();        if (enabled && rand.nextInt(100) >= (100 - chanceIn100)) {            long rndTime;            if (rand.nextInt(10) > 2) {                rndTime = rand.nextInt(300);            } else {                rndTime = rand.nextInt(1000);            }                        try {                Thread.sleep(rndTime);            } catch (InterruptedException e) {                Thread.currentThread().interrupt();            }        }    }    return true;}
1
public void updateState(List<TridentTuple> tuples, TridentCollector collector)
{    List<Message> messages = new LinkedList<>();    for (TridentTuple tuple : tuples) {        String topic = options.selector.getTopic(tuple);        String tag = options.selector.getTag(tuple);        String key = options.mapper.getKeyFromTuple(tuple);        byte[] value = options.mapper.getValueFromTuple(tuple);        if (topic == null) {                        continue;        }        Message msg = new Message(topic, tag, key, value);        messages.add(msg);    }    try {        this.producer.send(messages);    } catch (Exception e) {                collector.reportError(e);        throw new FailedException(e);    }}
1
public void testCollapsingBool() throws Exception
{    createIndex("test");    ensureGreen();    int numDocs = randomIntBetween(100, 150);    IndexRequestBuilder[] docs = new IndexRequestBuilder[numDocs];    for (int i = 0; i < numDocs; i++) {        docs[i] = client().prepareIndex("test", "type1", String.valueOf(i)).setSource("field1", English.intToEnglish(i), "field2", i);    }    indexRandom(true, docs);    refresh();    QueryBuilder q = QueryBuilders.boolQuery().must(QueryBuilders.boolQuery().must(QueryBuilders.boolQuery().must(QueryBuilders.matchQuery("field1", "one"))));        SearchResponse resp = client().prepareSearch().setQuery(q).setProfile(true).setSearchType(SearchType.QUERY_THEN_FETCH).get();    assertNotNull("Profile response element should not be null", resp.getProfileResults());    assertThat("Profile response should not be an empty array", resp.getProfileResults().size(), not(0));    for (Map.Entry<String, ProfileShardResult> shardResult : resp.getProfileResults().entrySet()) {        for (QueryProfileShardResult searchProfiles : shardResult.getValue().getQueryProfileResults()) {            for (ProfileResult result : searchProfiles.getQueryResults()) {                assertNotNull(result.getQueryName());                assertNotNull(result.getLuceneDescription());                assertThat(result.getTime(), greaterThan(0L));                assertNotNull(result.getTimeBreakdown());            }            CollectorResult result = searchProfiles.getCollectorResult();            assertThat(result.getName(), not(isEmptyOrNullString()));            assertThat(result.getTime(), greaterThan(0L));        }    }}
1
 static void renewTokens(final Configuration conf, final Path tokenFile) throws IOException, InterruptedException
{    for (Token<?> token : readTokens(tokenFile, conf)) {        if (token.isManaged()) {            long result = token.renew(conf);            if (LOG.isDebugEnabled()) {                            }        }    }}
1
public void run() throws InterruptedException, KeeperException
{    try {                doCreate();                while (true) {                        doPopulate();            waitOutstanding();                        readAll();            waitOutstanding();        }    } finally {            }}
1
private void createNewClient() throws JMSException
{    ClientType type = ClientType.randomClientType();    Client client;    synchronized (server.sendMutex) {        client = new Client(++clientRover, type);        clients.add(client);    }    client.start();    }
1
public void testRead() throws Exception
{    PCollection<byte[]> output = pipeline.apply(MqttIO.read().withConnectionConfiguration(MqttIO.ConnectionConfiguration.create("tcp://localhost:" + port, "READ_TOPIC").withClientId("READ_PIPELINE")).withMaxReadTime(Duration.standardSeconds(3)));    PAssert.that(output).containsInAnyOrder("This is test 0".getBytes(StandardCharsets.UTF_8), "This is test 1".getBytes(StandardCharsets.UTF_8), "This is test 2".getBytes(StandardCharsets.UTF_8), "This is test 3".getBytes(StandardCharsets.UTF_8), "This is test 4".getBytes(StandardCharsets.UTF_8), "This is test 5".getBytes(StandardCharsets.UTF_8), "This is test 6".getBytes(StandardCharsets.UTF_8), "This is test 7".getBytes(StandardCharsets.UTF_8), "This is test 8".getBytes(StandardCharsets.UTF_8), "This is test 9".getBytes(StandardCharsets.UTF_8));            MQTT client = new MQTT();    client.setHost("tcp://localhost:" + port);    final BlockingConnection publishConnection = client.blockingConnection();    publishConnection.connect();    Thread publisherThread = new Thread(() -> {        try {                        boolean pipelineConnected = false;            while (!pipelineConnected) {                Thread.sleep(1000);                for (Connection connection : brokerService.getBroker().getClients()) {                    if (connection.getConnectionId().startsWith("READ_PIPELINE")) {                        pipelineConnected = true;                    }                }            }            for (int i = 0; i < 10; i++) {                publishConnection.publish("READ_TOPIC", ("This is test " + i).getBytes(StandardCharsets.UTF_8), QoS.EXACTLY_ONCE, false);            }        } catch (Exception e) {                }    });    publisherThread.start();    pipeline.run();    publisherThread.join();    publishConnection.disconnect();}
1
public void setPublicId(String publicId)
{        if (publicId == null) {        return;    }    switch(publicId) {        case XmlIdentifiers.WEB_22_PUBLIC:            majorVersion = 2;            minorVersion = 2;            this.publicId = publicId;            break;        case XmlIdentifiers.WEB_23_PUBLIC:            majorVersion = 2;            minorVersion = 3;            this.publicId = publicId;            break;        default:                        break;    }}
1
public synchronized void start()
{    startTime = System.currentTimeMillis();    ensureCurrentState(STATE.INITED);    changeState(STATE.STARTED);    }
1
public CacheData getData(Long id)
{            try {        Thread.sleep(3000);    } catch (Exception e) {    }    CacheData data = new CacheData();    data.setId(id);    data.setValue(UUID.randomUUID().toString());    data.setCreated(new Date());    return data;}
1
public void testModifyNamespaceWithInvalidTableCount() throws Exception
{    final NamespaceDescriptor nsd = NamespaceDescriptor.create("testModifyNamespaceWithInvalidTableCount").build();    final String nsKey = "hbase.namespace.quota.maxtables";    final String nsValue = "-1";    final ProcedureExecutor<MasterProcedureEnv> procExec = getMasterProcedureExecutor();    createNamespaceForTesting(nsd);        nsd.setConfiguration(nsKey, nsValue);    long procId = procExec.submitProcedure(new ModifyNamespaceProcedure(procExec.getEnvironment(), nsd));        ProcedureTestingUtility.waitProcedure(procExec, procId);    Procedure<?> result = procExec.getResult(procId);    assertTrue(result.isFailed());        assertTrue(ProcedureTestingUtility.getExceptionCause(result) instanceof ConstraintException);}
1
private boolean shouldRotate()
{    boolean doRotate = false;    if (writer.isUnderReplicated()) {        this.isUnderReplicated = true;        doRotate = true;    } else {        this.isUnderReplicated = false;    }    if ((rollCount > 0) && (rollCount <= eventCounter)) {                doRotate = true;    }    if ((rollSize > 0) && (rollSize <= processSize)) {                doRotate = true;    }    return doRotate;}
1
protected int poll() throws Exception
{        shutdownRunningTask = null;    pendingExchanges = 0;    ReceiveMessageRequest request = new ReceiveMessageRequest(getQueueUrl());    request.setMaxNumberOfMessages(getMaxMessagesPerPoll() > 0 ? getMaxMessagesPerPoll() : null);    request.setVisibilityTimeout(getConfiguration().getVisibilityTimeout());    request.setWaitTimeSeconds(getConfiguration().getWaitTimeSeconds());    if (attributeNames != null) {        request.setAttributeNames(attributeNames);    }    if (messageAttributeNames != null) {        request.setMessageAttributeNames(messageAttributeNames);    }    log.trace("Receiving messages with request [{}]...", request);    ReceiveMessageResult messageResult;    try {        messageResult = getClient().receiveMessage(request);    } catch (QueueDoesNotExistException e) {                reConnectToQueue();        messageResult = getClient().receiveMessage(request);    }    if (log.isTraceEnabled()) {        log.trace("Received {} messages", messageResult.getMessages().size());    }    Queue<Exchange> exchanges = createExchanges(messageResult.getMessages());    return processBatch(CastUtils.cast(exchanges));}
1
public TestLogResponse testLog(@RequestBody TestLogRequest logsRequest, BindingResult result)
{    String testHandle = logsRequest.getTestHandle();    Test testExecution = mTests.get(testHandle);    if (result.hasErrors() || Strings.nullToEmpty(logsRequest.getTestHandle()).trim().isEmpty() || testExecution == null || logsRequest.getLength() > MAX_READ_SIZE) {        return new TestLogResponse(Status.illegalArgument());    }    File outputFile = testExecution.getOutputFile();    if (outputFile == null || logsRequest.getOffset() > outputFile.length()) {        return new TestLogResponse(Status.illegalArgument());    }    RandomAccessFile fileHandle = null;    try {        fileHandle = new RandomAccessFile(outputFile, "r");        long offset = logsRequest.getOffset();        fileHandle.seek(offset);        int readLength = 0;        if (offset < fileHandle.length()) {            readLength = (int) Math.min(fileHandle.length() - offset, logsRequest.getLength());        }        byte[] buffer = new byte[readLength];        fileHandle.readFully(buffer);        offset += readLength;        return new TestLogResponse(Status.ok(), offset, new String(buffer, Charsets.UTF_8));    } catch (IOException e) {                return new TestLogResponse(Status.internalError(e.getMessage()));    } finally {        if (fileHandle != null) {            try {                fileHandle.close();            } catch (IOException e) {                            }        }    }}
1
protected void render(Block html)
{    String attemptid = $(APPLICATION_ATTEMPT_ID);    if (attemptid.isEmpty()) {        puts("Bad request: requires application attempt ID");        return;    }    try {        appAttemptId = ApplicationAttemptId.fromString(attemptid);    } catch (IllegalArgumentException e) {        puts("Invalid application attempt ID: " + attemptid);        return;    }    UserGroupInformation callerUGI = getCallerUGI();    ApplicationAttemptReport appAttemptReport;    try {        final GetApplicationAttemptReportRequest request = GetApplicationAttemptReportRequest.newInstance(appAttemptId);        if (callerUGI == null) {            appAttemptReport = getApplicationAttemptReport(request);        } else {            appAttemptReport = callerUGI.doAs(new PrivilegedExceptionAction<ApplicationAttemptReport>() {                @Override                public ApplicationAttemptReport run() throws Exception {                    return getApplicationAttemptReport(request);                }            });        }    } catch (Exception e) {        String message = "Failed to read the application attempt " + appAttemptId + ".";                html.p().__(message).__();        return;    }    if (appAttemptReport == null) {        puts("Application Attempt not found: " + attemptid);        return;    }    boolean exceptionWhenGetContainerReports = false;    Collection<ContainerReport> containers = null;    try {        final GetContainersRequest request = GetContainersRequest.newInstance(appAttemptId);        if (callerUGI == null) {            containers = getContainers(request);        } else {            containers = callerUGI.doAs(new PrivilegedExceptionAction<Collection<ContainerReport>>() {                @Override                public Collection<ContainerReport> run() throws Exception {                    return getContainers(request);                }            });        }    } catch (RuntimeException e) {                exceptionWhenGetContainerReports = true;    } catch (Exception e) {        exceptionWhenGetContainerReports = true;    }    AppAttemptInfo appAttempt = new AppAttemptInfo(appAttemptReport);    setTitle(join("Application Attempt ", attemptid));    String node = "N/A";    if (appAttempt.getHost() != null && appAttempt.getRpcPort() >= 0 && appAttempt.getRpcPort() < 65536) {        node = appAttempt.getHost() + ":" + appAttempt.getRpcPort();    }    generateOverview(appAttemptReport, containers, appAttempt, node);    if (exceptionWhenGetContainerReports) {        html.p().__("Sorry, Failed to get containers for application attempt" + attemptid + ".").__();        return;    }    createAttemptHeadRoomTable(html);    html.__(InfoBlock.class);    createTablesForAttemptMetrics(html);        TBODY<TABLE<Hamlet>> tbody = html.table("#containers").$style("width:100%").thead().tr().th(".id", "Container ID").th(".node", "Node").th(".exitstatus", "Container Exit Status").th(".logs", "Logs").__().__().tbody();    StringBuilder containersTableData = new StringBuilder("[\n");    for (ContainerReport containerReport : containers) {        ContainerInfo container = new ContainerInfo(containerReport);        containersTableData.append("[\"<a href='").append(url("container", container.getContainerId())).append("'>").append(container.getContainerId()).append("</a>\",\"<a ").append(container.getNodeHttpAddress() == null ? "#" : "href='" + container.getNodeHttpAddress()).append("'>").append(container.getNodeHttpAddress() == null ? "N/A" : StringEscapeUtils.escapeEcmaScript(StringEscapeUtils.escapeHtml4(container.getNodeHttpAddress()))).append("</a>\",\"").append(container.getContainerExitStatus()).append("\",\"<a href='").append(container.getLogUrl() == null ? "#" : container.getLogUrl()).append("'>").append(container.getLogUrl() == null ? "N/A" : "Logs").append("</a>\"],\n");    }    if (containersTableData.charAt(containersTableData.length() - 2) == ',') {        containersTableData.delete(containersTableData.length() - 2, containersTableData.length() - 1);    }    containersTableData.append("]");    html.script().$type("text/javascript").__("var containersTableData=" + containersTableData).__();    tbody.__().__();}
1
public void run()
{    while (running) {        try {            MetricsTask task = taskQueue.take();            metricsConsumer.handleDataPoints(task.getTaskInfo(), task.getDataPoints());        } catch (InterruptedException e) {            break;        } catch (Throwable t) {                    }    }}
1
protected void map(LongWritable key, LongWritable value, Context context) throws IOException, InterruptedException
{    long chainId = value.get();        byte[] chainIdArray = Bytes.toBytes(chainId);    long currentRow = 0;    long chainLength = context.getConfiguration().getLong(CHAIN_LENGTH_KEY, CHAIN_LENGTH);    long nextRow = getNextRow(0, chainLength);    for (long i = 0; i < chainLength; i++) {        byte[] rk = Bytes.toBytes(currentRow);                KeyValue linkKv = new KeyValue(rk, CHAIN_FAM, chainIdArray, Bytes.toBytes(nextRow));                KeyValue sortKv = new KeyValue(rk, SORT_FAM, chainIdArray, Bytes.toBytes(i));                KeyValue dataKv = new KeyValue(rk, DATA_FAM, chainIdArray, Bytes.toBytes(RandomStringUtils.randomAlphabetic(50)));                context.write(new ImmutableBytesWritable(rk), linkKv);        context.write(new ImmutableBytesWritable(rk), sortKv);        context.write(new ImmutableBytesWritable(rk), dataKv);                currentRow = nextRow;        nextRow = getNextRow(i + 1, chainLength);    }}
1
public void setUp() throws Exception
{    super.setUp();        String fileUrl = "file:///" + tmpDir.replaceAll("\\\\", "/");        bindAddress = "vm://localhost?jms.blobTransferPolicy.defaultUploadUrl=" + fileUrl;    connectionFactory = createConnectionFactory();    connection = createConnection();    connection.start();}
1
public SessionHandle openSessionWithImpersonation(TProtocolVersion protocol, String username, String password, Map<String, String> configuration, String delegationToken) throws HiveSQLException
{    SessionHandle sessionHandle = sessionManager.openSession(protocol, username, password, null, configuration, true, delegationToken);        return sessionHandle;}
1
public void setCharacterEncoding(String charset)
{    if (isCommitted()) {        return;    }        if (included) {        return;    }        if (usingWriter) {        return;    }    try {        getCoyoteResponse().setCharacterEncoding(charset);    } catch (UnsupportedEncodingException e) {                return;    }    isCharacterEncodingSet = true;}
1
public void onMessage(Message msg)
{    if (firstMessage) {        firstMessage = false;        if (getInitialDelay() > 0) {            try {                Thread.sleep(getInitialDelay());            } catch (InterruptedException e) {            }        }    }    rate.increment();    try {        if (enableAudit && !this.audit.isInOrder(msg.getJMSMessageID())) {                    }        if (enableAudit && this.audit.isDuplicate(msg)) {                    }        lastMsgId = msg.getJMSMessageID();    } catch (JMSException e1) {        e1.printStackTrace();    }    try {        if (sleepDuration != 0) {            Thread.sleep(sleepDuration);        }    } catch (InterruptedException e) {    }}
1
public void eval(Object... keys)
{    Row keyRow = Row.of(keys);    if (cache != null) {        List<Row> cachedRows = cache.getIfPresent(keyRow);        if (cachedRows != null) {            for (Row cachedRow : cachedRows) {                collect(cachedRow);            }            return;        }    }    for (int retry = 1; retry <= maxRetryTimes; retry++) {        try {            statement.clearParameters();            for (int i = 0; i < keys.length; i++) {                JDBCUtils.setField(statement, keySqlTypes[i], keys[i], i);            }            try (ResultSet resultSet = statement.executeQuery()) {                if (cache == null) {                    while (resultSet.next()) {                        collect(convertToRowFromResultSet(resultSet));                    }                } else {                    ArrayList<Row> rows = new ArrayList<>();                    while (resultSet.next()) {                        Row row = convertToRowFromResultSet(resultSet);                        rows.add(row);                        collect(row);                    }                    rows.trimToSize();                    cache.put(keyRow, rows);                }            }            break;        } catch (SQLException e) {                        if (retry >= maxRetryTimes) {                throw new RuntimeException("Execution of JDBC statement failed.", e);            }            try {                Thread.sleep(1000 * retry);            } catch (InterruptedException e1) {                throw new RuntimeException(e1);            }        }    }}
1
public static MeterRegistry getOrCreateMeterRegistry(Registry camelRegistry, String registryName)
{        MeterRegistry result = getMeterRegistryFromCamelRegistry(camelRegistry, registryName);    if (result == null) {                        result = createMeterRegistry();    }    return result;}
1
public void run()
{    try {        if (!rmWorkPreservingRestartEnabled) {                        containerManager.cleanupContainersOnNMResync();                        if (context.getKnownCollectors() != null) {                context.getKnownCollectors().clear();            }        } else {                                    reregisterCollectors();        }        ((NodeStatusUpdaterImpl) nodeStatusUpdater).rebootNodeStatusUpdaterAndRegisterWithRM();    } catch (YarnRuntimeException e) {                shutDown(NodeManagerStatus.EXCEPTION.getExitCode());    } finally {                resyncingWithRM.set(false);    }}
1
private Connection openConnection() throws IOException, TimeoutException
{    if (!isConnectionOpened()) {                connection = connectionFactory.newConnection();    }    return connection;}
1
public void visitCube(final RpcController controller, final CubeVisitProtos.CubeVisitRequest request, RpcCallback<CubeVisitProtos.CubeVisitResponse> done)
{    List<RegionScanner> regionScanners = Lists.newArrayList();    HRegion region = null;    StringBuilder sb = new StringBuilder();    byte[] allRows;    String debugGitTag = "";    CubeVisitProtos.CubeVisitResponse.ErrorInfo errorInfo = null;        KylinConfig kylinConfig = KylinConfig.createKylinConfig(request.getKylinProperties());    String queryId = request.hasQueryId() ? request.getQueryId() : "UnknownId";        try (SetAndUnsetThreadLocalConfig autoUnset = KylinConfig.setAndUnsetThreadLocalConfig(kylinConfig);        SetThreadName ignored = new SetThreadName("Query %s", queryId)) {        final long serviceStartTime = System.currentTimeMillis();        region = (HRegion) env.getRegion();        region.startRegionOperation();        debugGitTag = region.getTableDesc().getValue(IRealizationConstants.HTableGitTag);        final GTScanRequest scanReq = GTScanRequest.serializer.deserialize(ByteBuffer.wrap(HBaseZeroCopyByteString.zeroCopyGetBytes(request.getGtScanRequest())));        final long deadline = scanReq.getStartTime() + scanReq.getTimeout();        checkDeadline(deadline);        List<List<Integer>> hbaseColumnsToGT = Lists.newArrayList();        for (IntList intList : request.getHbaseColumnsToGTList()) {            hbaseColumnsToGT.add(intList.getIntsList());        }        StorageSideBehavior behavior = StorageSideBehavior.valueOf(scanReq.getStorageBehavior());        final List<RawScan> hbaseRawScans = deserializeRawScans(ByteBuffer.wrap(HBaseZeroCopyByteString.zeroCopyGetBytes(request.getHbaseRawScan())));        appendProfileInfo(sb, "start latency: " + (serviceStartTime - scanReq.getStartTime()), serviceStartTime);        final List<InnerScannerAsIterator> cellListsForeachRawScan = Lists.newArrayList();        for (RawScan hbaseRawScan : hbaseRawScans) {            if (request.getRowkeyPreambleSize() - RowConstants.ROWKEY_CUBOIDID_LEN > 0) {                                updateRawScanByCurrentRegion(hbaseRawScan, region, request.getRowkeyPreambleSize() - RowConstants.ROWKEY_CUBOIDID_LEN);            }            Scan scan = CubeHBaseRPC.buildScan(hbaseRawScan);            RegionScanner innerScanner = region.getScanner(scan);            regionScanners.add(innerScanner);            InnerScannerAsIterator cellListIterator = new InnerScannerAsIterator(innerScanner);            cellListsForeachRawScan.add(cellListIterator);        }        final Iterator<List<Cell>> allCellLists = Iterators.concat(cellListsForeachRawScan.iterator());        if (behavior.ordinal() < StorageSideBehavior.SCAN.ordinal()) {                        List<Cell> temp = Lists.newArrayList();            int counter = 0;            for (RegionScanner innerScanner : regionScanners) {                while (innerScanner.nextRaw(temp)) {                    counter++;                }            }            appendProfileInfo(sb, "scanned " + counter, serviceStartTime);        }        if (behavior.ordinal() < StorageSideBehavior.SCAN_FILTER_AGGR_CHECKMEM.ordinal()) {                        scanReq.disableAggCacheMemCheck();        }        final long storagePushDownLimit = scanReq.getStoragePushDownLimit();        ResourceTrackingCellListIterator cellListIterator = new ResourceTrackingCellListIterator(allCellLists,         scanReq.getStorageScanRowNumThreshold(),         !request.hasMaxScanBytes() ? Long.MAX_VALUE : request.getMaxScanBytes(), deadline);        IGTStore store = new HBaseReadonlyStore(cellListIterator, scanReq, hbaseRawScans.get(0).hbaseColumns, hbaseColumnsToGT, request.getRowkeyPreambleSize(), behavior.delayToggledOn(), request.getIsExactAggregate());        IGTScanner rawScanner = store.scan(scanReq);        IGTScanner finalScanner = scanReq.decorateScanner(rawScanner, behavior.filterToggledOn(), behavior.aggrToggledOn(), false, request.getSpillEnabled());        ByteBuffer buffer = ByteBuffer.allocate(BufferedMeasureCodec.DEFAULT_BUFFER_SIZE);                ByteArrayOutputStream outputStream = new ByteArrayOutputStream(BufferedMeasureCodec.DEFAULT_BUFFER_SIZE);        long finalRowCount = 0L;        try {            for (GTRecord oneRecord : finalScanner) {                buffer.clear();                try {                    oneRecord.exportColumns(scanReq.getColumns(), buffer);                } catch (BufferOverflowException boe) {                    buffer = ByteBuffer.allocate(oneRecord.sizeOf(scanReq.getColumns()) * 2);                    oneRecord.exportColumns(scanReq.getColumns(), buffer);                }                outputStream.write(buffer.array(), 0, buffer.position());                finalRowCount++;                                if (!scanReq.isDoingStorageAggregation() && (scanReq.getStorageLimitLevel() != StorageLimitLevel.NO_LIMIT && finalRowCount >= storagePushDownLimit)) {                                                            break;                }            }        } catch (KylinTimeoutException e) {                        errorInfo = CubeVisitProtos.CubeVisitResponse.ErrorInfo.newBuilder().setType(CubeVisitProtos.CubeVisitResponse.ErrorType.TIMEOUT).setMessage(e.getMessage()).build();        } catch (ResourceLimitExceededException e) {                        errorInfo = CubeVisitProtos.CubeVisitResponse.ErrorInfo.newBuilder().setType(CubeVisitProtos.CubeVisitResponse.ErrorType.RESOURCE_LIMIT_EXCEEDED).setMessage(e.getMessage()).build();        } finally {            finalScanner.close();        }        long rowCountBeforeAggr = finalScanner instanceof GTAggregateScanner ? ((GTAggregateScanner) finalScanner).getInputRowCount() : finalRowCount;        appendProfileInfo(sb, "agg done", serviceStartTime);                        byte[] compressedAllRows;        if (errorInfo == null) {            allRows = outputStream.toByteArray();        } else {            allRows = new byte[0];        }        if (!kylinConfig.getCompressionResult()) {            compressedAllRows = allRows;        } else {            compressedAllRows = CompressionUtils.compress(allRows);        }        appendProfileInfo(sb, "compress done", serviceStartTime);                OperatingSystemMXBean operatingSystemMXBean = (OperatingSystemMXBean) ManagementFactory.getOperatingSystemMXBean();        double systemCpuLoad = operatingSystemMXBean.getSystemCpuLoad();        double freePhysicalMemorySize = operatingSystemMXBean.getFreePhysicalMemorySize();        double freeSwapSpaceSize = operatingSystemMXBean.getFreeSwapSpaceSize();        appendProfileInfo(sb, "server stats done", serviceStartTime);        sb.append(" debugGitTag:" + debugGitTag);        CubeVisitProtos.CubeVisitResponse.Builder responseBuilder = CubeVisitProtos.CubeVisitResponse.newBuilder();        if (errorInfo != null) {            responseBuilder.setErrorInfo(errorInfo);        }        done.run(        responseBuilder.setCompressedRows(        HBaseZeroCopyByteString.wrap(compressedAllRows)).setStats(CubeVisitProtos.CubeVisitResponse.Stats.newBuilder().setFilteredRowCount(cellListIterator.getTotalScannedRowCount() - rowCountBeforeAggr).setAggregatedRowCount(rowCountBeforeAggr - finalRowCount).setScannedRowCount(cellListIterator.getTotalScannedRowCount()).setScannedBytes(cellListIterator.getTotalScannedRowBytes()).setServiceStartTime(serviceStartTime).setServiceEndTime(System.currentTimeMillis()).setSystemCpuLoad(systemCpuLoad).setFreePhysicalMemorySize(freePhysicalMemorySize).setFreeSwapSpaceSize(freeSwapSpaceSize).setHostname(InetAddress.getLocalHost().getHostName()).setEtcMsg(sb.toString()).setNormalComplete(errorInfo == null ? 1 : 0).build()).build());    } catch (DoNotRetryIOException e) {        ResponseConverter.setControllerException(controller, e);    } catch (IOException ioe) {                IOException wrapped = new IOException("Error in coprocessor " + debugGitTag, ioe);        ResponseConverter.setControllerException(controller, wrapped);    } finally {        for (RegionScanner innerScanner : regionScanners) {            IOUtils.closeQuietly(innerScanner);        }        if (region != null) {            try {                region.closeRegionOperation();            } catch (IOException e) {                throw new RuntimeException(e);            }        }    }}
1
private void removeFromCache(CacheValue v)
{        byDirectoryCache.remove(v.directory);    byPathCache.remove(v.path);}
1
public void reset() throws IOException
{    if (!inReset) {        if (fileCache.isActive) {            fileCache.createInDiskSegment();        } else {            memCache.createInMemorySegment();        }    }    inReset = true;        for (int i = 0; i < segmentList.size(); i++) {        Segment<K, V> s = segmentList.get(i);        if (s.inMemory()) {            int offset = (i == 0) ? firstSegmentOffset : 0;            s.getReader().reset(offset);        } else {            s.closeReader();            if (i == 0) {                s.reinitReader(firstSegmentOffset);                s.getReader().disableChecksumValidation();            }        }    }    currentKVOffset = firstSegmentOffset;    nextKVOffset = -1;    readSegmentIndex = 0;    hasMore = false;    lastSegmentEOF = false;    }
1
protected void configure()
{    bind(AuditLogger.class).to(AsyncAuditLogger.class);        bind(AuditLogger.class).annotatedWith(Names.named(AsyncAuditLogger.InnerLogger)).to(AuditLoggerDefaultImpl.class);        Multibinder<RequestAuditEventCreator> multiBinder = Multibinder.newSetBinder(binder(), RequestAuditEventCreator.class);    Set<Class<?>> bindingSet = ClasspathScannerUtils.findOnClassPath(getPackageToScan(), getExclusions(), getSelectors());    for (Class clazz : bindingSet) {                multiBinder.addBinding().to(clazz).in(Scopes.SINGLETON);    }    bind(RequestAuditLogger.class).to(RequestAuditLoggerImpl.class);}
1
public CMComponent configureCMComponent() throws Exception
{    CMComponent component = new CMComponent();    component.setCamelContext(camelContext);    Map<String, Object> parameters = new HashMap<>();    IntrospectionSupport.getProperties(configuration, parameters, null, false);    for (Map.Entry<String, Object> entry : parameters.entrySet()) {        Object value = entry.getValue();        Class<?> paramClass = value.getClass();        if (paramClass.getName().endsWith("NestedConfiguration")) {            Class nestedClass = null;            try {                nestedClass = (Class) paramClass.getDeclaredField("CAMEL_NESTED_CLASS").get(null);                HashMap<String, Object> nestedParameters = new HashMap<>();                IntrospectionSupport.getProperties(value, nestedParameters, null, false);                Object nestedProperty = nestedClass.newInstance();                CamelPropertiesHelper.setCamelProperties(camelContext, nestedProperty, nestedParameters, false);                entry.setValue(nestedProperty);            } catch (NoSuchFieldException e) {            }        }    }    CamelPropertiesHelper.setCamelProperties(camelContext, component, parameters, false);    if (ObjectHelper.isNotEmpty(customizers)) {        for (ComponentCustomizer<CMComponent> customizer : customizers) {            boolean useCustomizer = (customizer instanceof HasId) ? HierarchicalPropertiesEvaluator.evaluate(applicationContext.getEnvironment(), "camel.component.customizer", "camel.component.cm-sms.customizer", ((HasId) customizer).getId()) : HierarchicalPropertiesEvaluator.evaluate(applicationContext.getEnvironment(), "camel.component.customizer", "camel.component.cm-sms.customizer");            if (useCustomizer) {                                customizer.customize(component);            }        }    }    return component;}
1
public final long getEstimatedSizeBytes(PipelineOptions options) throws IOException
{                String fileOrPattern = fileOrPatternSpec.get();    if (mode == Mode.FILEPATTERN) {        long totalSize = 0;        List<Metadata> allMatches = FileSystems.match(fileOrPattern, emptyMatchTreatment).metadata();        for (Metadata metadata : allMatches) {            totalSize += metadata.sizeBytes();        }                return totalSize;    } else {        long start = getStartOffset();        long end = Math.min(getEndOffset(), getMaxEndOffset(options));        return end - start;    }}
1
public boolean matches(Object arg0)
{    ObjectName other = (ObjectName) arg0;    ObjectName mine = (ObjectName) name;        if (!"networkConnectors".equals(other.getKeyProperty("connector"))) {        return false;    }    return other.getKeyProperty("connector").equals(mine.getKeyProperty("connector")) && other.getKeyProperty("networkBridge") != null && mine.getKeyProperty("networkBridge") != null;}
1
public void modifyAclEntries(final Path path, final List<AclEntry> aclSpec) throws IOException
{        if (!getIsNamespaceEnabled()) {        throw new UnsupportedOperationException("modifyAclEntries is only supported by storage accounts with the " + "hierarchical namespace enabled.");    }    if (aclSpec == null || aclSpec.isEmpty()) {        throw new IllegalArgumentException("The value of the aclSpec parameter is invalid.");    }    Path qualifiedPath = makeQualified(path);    performAbfsAuthCheck(FsAction.WRITE, qualifiedPath);    try {        abfsStore.modifyAclEntries(qualifiedPath, aclSpec);    } catch (AzureBlobFileSystemException ex) {        checkException(path, ex);    }}
1
public void shutdownMessageReceived(DistributedMember id, String reason)
{    if (logger.isDebugEnabled()) {            }    synchronized (this.shutdownMembers) {        this.shutdownMembers.put(id, id);        services.getHealthMonitor().memberShutdown(((GMSMemberAdapter) ((InternalDistributedMember) id).getNetMember()).getGmsMember(), reason);        services.getJoinLeave().memberShutdown(getGMSMember((InternalDistributedMember) id), reason);    }}
1
private void releaseFetchTask()
{    try {        if (fetchTask != null) {            fetchTask.clearFetch();            fetchTask = null;        }    } catch (Exception e) {            }}
1
private void consecutivelyPutAndDestroyEntries(String regionName)
{    Region r = getCache().getRegion(regionName);    for (int i = 0, j = 0; i < 500; i++, j++) {        PortfolioData p = (PortfolioData) r.get(j);                r.destroy(j);        final int key = j;        await().untilAsserted(() -> assertEquals(null, r.get(key)));                getCache().getLogger().fine("Putting the value back" + p);        r.put(j, p);                if (j == cntDest - 1) {            j = 0;        }    }}
1
private void doBulkLoad(Configuration conf, TableName tableName, Path stagingDir, Deque<LoadQueueItem> queue, int maxRetries) throws IOException
{    BulkLoadHFilesTool loader = new BulkLoadHFilesTool(conf);        loader.setBulkToken(stagingDir.toString());        loader.setClusterIds(sourceClusterIds);    for (int count = 0; !queue.isEmpty(); count++) {        if (count != 0) {                    }        if (maxRetries != 0 && count >= maxRetries) {            throw new IOException("Retry attempted " + count + " times without completing, bailing out.");        }                loader.loadHFileQueue(connection, tableName, queue, false);    }}
1
public void onRemoval(RemovalNotification<DfsClientKey, DFSClient> notification)
{    DFSClient client = notification.getValue();    try {        client.close();    } catch (IOException e) {            }}
1
public void testMasterObserver() throws IOException
{        MetricRegistryInfo info = MetricsCoprocessor.createRegistryInfoForMasterCoprocessor(CustomMasterObserver.class.getName());    Optional<MetricRegistry> registry = MetricRegistries.global().get(info);    assertTrue(registry.isPresent());    Optional<Metric> metric = registry.get().get("CreateTable");    assertTrue(metric.isPresent());    try (Connection connection = ConnectionFactory.createConnection(UTIL.getConfiguration());        Admin admin = connection.getAdmin()) {        Timer createTableTimer = (Timer) metric.get();        long prevCount = createTableTimer.getHistogram().getCount();                admin.createTable(new HTableDescriptor(TableName.valueOf(name.getMethodName())).addFamily(new HColumnDescriptor("foo")));        assertEquals(1, createTableTimer.getHistogram().getCount() - prevCount);    }}
1
private void closeJobManagerConnection(JobID jobId, Exception cause)
{    if (log.isDebugEnabled()) {            } else {            }        Iterator<Task> tasks = taskSlotTable.getTasks(jobId);    final FlinkException failureCause = new FlinkException("JobManager responsible for " + jobId + " lost the leadership.", cause);    while (tasks.hasNext()) {        tasks.next().failExternally(failureCause);    }        Iterator<AllocationID> activeSlots = taskSlotTable.getActiveSlots(jobId);    final FlinkException freeingCause = new FlinkException("Slot could not be marked inactive.");    while (activeSlots.hasNext()) {        AllocationID activeSlot = activeSlots.next();        try {            if (!taskSlotTable.markSlotInactive(activeSlot, taskManagerConfiguration.getTimeout())) {                freeSlotInternal(activeSlot, freeingCause);            }        } catch (SlotNotFoundException e) {                    }    }        JobManagerConnection jobManagerConnection = jobManagerTable.remove(jobId);    if (jobManagerConnection != null) {        try {            jobManagerHeartbeatManager.unmonitorTarget(jobManagerConnection.getResourceID());            jobManagerConnections.remove(jobManagerConnection.getResourceID());            disassociateFromJobManager(jobManagerConnection, cause);        } catch (IOException e) {                    }    }}
1
public boolean add(String key)
{    Object[] idValues = getPKValues(key);        return isApplied(getSession().execute(insertStatement.bind(idValues)));}
1
 static void appendSecurityFlags(List<String> commands, Map<String, String> env, Path policyPath, SandboxMode sandboxMode) throws ContainerExecutionException
{    for (int i = 0; i < commands.size(); i++) {        String command = commands.get(i);        if (validateJavaHome(env.get(JAVA_HOME.name())) && command.matches(CONTAINS_JAVA_CMD) && !command.matches(MULTI_COMMAND_REGEX)) {            command = command.replaceAll(CLEAN_CMD_REGEX, "");            String securityString = JVM_SECURITY_CMD + policyPath + " ";            if (LOG.isDebugEnabled()) {                securityString += SECURITY_DEBUG;            }            commands.set(i, command.replaceFirst(JAVA_CMD, securityString));        } else if (sandboxMode == SandboxMode.enforcing) {            throw new ContainerExecutionException("Only JVM containers permitted in YARN sandbox mode (enforcing). " + "The following command can not be executed securely: " + command);        } else if (sandboxMode == SandboxMode.permissive) {                    }    }}
1
 void addMap(ContainerRequestEvent event)
{    ContainerRequest request = null;    if (event.getEarlierAttemptFailed()) {        earlierFailedMaps.add(event.getAttemptID());        request = new ContainerRequest(event, PRIORITY_FAST_FAIL_MAP, mapNodeLabelExpression);                        maps.put(event.getAttemptID(), request);        addContainerReq(request);    } else {        if (mapsMod100 < numOpportunisticMapsPercent) {            request = new ContainerRequest(event, PRIORITY_OPPORTUNISTIC_MAP, mapNodeLabelExpression);            maps.put(event.getAttemptID(), request);            addOpportunisticResourceRequest(request.priority, request.capability);        } else {            request = new ContainerRequest(event, PRIORITY_MAP, mapNodeLabelExpression);            for (String host : event.getHosts()) {                LinkedList<TaskAttemptId> list = mapsHostMapping.get(host);                if (list == null) {                    list = new LinkedList<TaskAttemptId>();                    mapsHostMapping.put(host, list);                }                list.add(event.getAttemptID());                if (LOG.isDebugEnabled()) {                                    }            }            for (String rack : event.getRacks()) {                LinkedList<TaskAttemptId> list = mapsRackMapping.get(rack);                if (list == null) {                    list = new LinkedList<TaskAttemptId>();                    mapsRackMapping.put(rack, list);                }                list.add(event.getAttemptID());                if (LOG.isDebugEnabled()) {                                    }            }            maps.put(event.getAttemptID(), request);            addContainerReq(request);        }        mapsMod100++;        mapsMod100 %= 100;    }}
1
public void onComplete(TGetQueryIdResp o)
{    GetQueryId_result result = new GetQueryId_result();    result.success = o;    try {        fcall.sendResponse(fb, result, org.apache.thrift.protocol.TMessageType.REPLY, seqid);        return;    } catch (Exception e) {            }    fb.close();}
1
public void perJobYarnCluster() throws Exception
{    runTest(() -> {                addTestAppender(CliFrontend.class, Level.INFO);        File exampleJarLocation = getTestJarPath("BatchWordCount.jar");        runWithArgs(new String[] { "run", "-m", "yarn-cluster", "-yj", flinkUberjar.getAbsolutePath(), "-yt", flinkLibFolder.getAbsolutePath(), "-yt", flinkShadedHadoopDir.getAbsolutePath(),         "-ys",         "2", "-yjm", "768m", "-ytm", "1024m", exampleJarLocation.getAbsolutePath() }, /* test succeeded after this string */        "Program execution finished",         new String[] { "DataSink \\(.*\\) \\(1/1\\) switched to FINISHED" }, RunTypes.CLI_FRONTEND, 0, true);            });}
1
private Job loadJob(JobId jobId) throws RuntimeException, IOException
{    if (LOG.isDebugEnabled()) {            }    HistoryFileInfo fileInfo;    fileInfo = hsManager.getFileInfo(jobId);    if (fileInfo == null) {        throw new HSFileRuntimeException("Unable to find job " + jobId);    }    fileInfo.waitUntilMoved();    if (fileInfo.isDeleted()) {        throw new HSFileRuntimeException("Cannot load deleted job " + jobId);    } else {        return fileInfo.loadJob();    }}
1
public void testManagedResource() throws Exception
{        if (isPlatform("aix")) {        return;    }    final ManagementAgent managementAgent = context.getManagementStrategy().getManagementAgent();    Assert.assertNotNull(managementAgent);    final MBeanServer mBeanServer = managementAgent.getMBeanServer();    Assert.assertNotNull(mBeanServer);    final String mBeanServerDefaultDomain = managementAgent.getMBeanServerDefaultDomain();    Assert.assertEquals("org.apache.camel", mBeanServerDefaultDomain);    final String managementName = context.getManagementName();    Assert.assertNotNull("CamelContext should have a management name if JMX is enabled", managementName);            ObjectName onContext = ObjectName.getInstance(mBeanServerDefaultDomain + ":context=" + managementName + ",type=context,name=\"" + context.getName() + "\"");    Assert.assertTrue("Should be registered", mBeanServer.isRegistered(onContext));        ObjectName onManagedBean = ObjectName.getInstance(mBeanServerDefaultDomain + ":context=" + managementName + ",type=processors,name=\"myManagedBean\"");        Assert.assertTrue("Should be registered", mBeanServer.isRegistered(onManagedBean));        template.sendBody("direct:start", "Hello Camel");    template.sendBody("direct:start", "Camel Rocks!");        int camelsSeenCount = (Integer) mBeanServer.getAttribute(onManagedBean, "CamelsSeenCount");    Assert.assertEquals(2, camelsSeenCount);        mBeanServer.invoke(onManagedBean, "resetCamelsSeenCount", null, null);    camelsSeenCount = (Integer) mBeanServer.getAttribute(onManagedBean, "CamelsSeenCount");    Assert.assertEquals(0, camelsSeenCount);    String camelId = (String) mBeanServer.getAttribute(onManagedBean, "CamelId");    assertEquals(context.getName(), camelId);    String state = (String) mBeanServer.getAttribute(onManagedBean, "State");    assertEquals("Started", state);    String fqn = (String) mBeanServer.getAttribute(onManagedBean, "BeanClassName");    assertEquals(MyManagedBean.class.getCanonicalName(), fqn);}
1
private NewApplication createNewApplication()
{    GetNewApplicationRequest req = recordFactory.newRecordInstance(GetNewApplicationRequest.class);    GetNewApplicationResponse resp;    try {        resp = rm.getClientRMService().getNewApplication(req);    } catch (YarnException e) {        String msg = "Unable to create new app from RM web service";                throw new YarnRuntimeException(msg, e);    }    NewApplication appId = new NewApplication(resp.getApplicationId().toString(), new ResourceInfo(resp.getMaximumResourceCapability()));    return appId;}
1
protected void serviceStart() throws Exception
{    super.serviceStart();        getConfig().set(JHAdminConfig.MR_HISTORY_ADDRESS, historyServer.getConfig().get(JHAdminConfig.MR_HISTORY_ADDRESS));    MRWebAppUtil.setJHSWebappURLWithoutScheme(getConfig(), MRWebAppUtil.getJHSWebappURLWithoutScheme(historyServer.getConfig()));                }
1
private void takeAndProcessTasks() throws Exception
{    final Future<ReencryptionTask> completed = batchService.take();    throttle();    checkPauseForTesting();    if (completed.isCancelled()) {                        return;    }    final ReencryptionTask task = completed.get();    boolean shouldRetry;    do {        dir.getFSNamesystem().writeLock();        try {            throttleTimerLocked.start();            processTask(task);            shouldRetry = false;        } catch (RetriableException | SafeModeException re) {                                    shouldRetry = true;            Thread.sleep(faultRetryInterval);        } catch (IOException ioe) {                        ++task.numFailures;            task.processed = true;            shouldRetry = false;        } finally {            dir.getFSNamesystem().writeUnlock("reencryptUpdater");            throttleTimerLocked.stop();        }                        dir.getEditLog().logSync();    } while (shouldRetry);}
1
protected void addTranportConnectors() throws Exception
{    TransportConnector connector = null;    if (isUseOpenWireConnector()) {        connector = brokerService.addConnector("tcp://0.0.0.0:" + openwirePort);        openwirePort = connector.getConnectUri().getPort();        openwireURI = connector.getPublishableConnectURI();            }    if (isUseTcpConnector()) {        connector = brokerService.addConnector("amqp://0.0.0.0:" + amqpPort + "?transport.tcpNoDelay=true&transport.transformer=" + getAmqpTransformer() + getAdditionalConfig());        amqpPort = connector.getConnectUri().getPort();        amqpURI = connector.getPublishableConnectURI();            }    if (isUseSslConnector()) {        connector = brokerService.addConnector("amqp+ssl://0.0.0.0:" + amqpSslPort + "?transport.tcpNoDelay=true&transport.transformer=" + getAmqpTransformer() + getAdditionalConfig());        amqpSslPort = connector.getConnectUri().getPort();        amqpSslURI = connector.getPublishableConnectURI();            }    if (isUseNioConnector()) {        connector = brokerService.addConnector("amqp+nio://0.0.0.0:" + amqpNioPort + "?transport.tcpNoDelay=true&transport.transformer=" + getAmqpTransformer() + getAdditionalConfig());        amqpNioPort = connector.getConnectUri().getPort();        amqpNioURI = connector.getPublishableConnectURI();            }    if (isUseNioPlusSslConnector()) {        connector = brokerService.addConnector("amqp+nio+ssl://0.0.0.0:" + amqpNioPlusSslPort + "?transport.tcpNoDelay=true&transport.transformer=" + getAmqpTransformer() + getAdditionalConfig());        amqpNioPlusSslPort = connector.getConnectUri().getPort();        amqpNioPlusSslURI = connector.getPublishableConnectURI();            }    if (isUseAutoConnector()) {        connector = brokerService.addConnector("auto://0.0.0.0:" + autoPort + getAdditionalConfig());        autoPort = connector.getConnectUri().getPort();        autoURI = connector.getPublishableConnectURI();            }    if (isUseAutoSslConnector()) {        connector = brokerService.addConnector("auto+ssl://0.0.0.0:" + autoSslPort + getAdditionalConfig());        autoSslPort = connector.getConnectUri().getPort();        autoSslURI = connector.getPublishableConnectURI();            }    if (isUseAutoNioConnector()) {        connector = brokerService.addConnector("auto+nio://0.0.0.0:" + autoNioPort + getAdditionalConfig());        autoNioPort = connector.getConnectUri().getPort();        autoNioURI = connector.getPublishableConnectURI();            }    if (isUseAutoNioPlusSslConnector()) {        connector = brokerService.addConnector("auto+nio+ssl://0.0.0.0:" + autoNioPlusSslPort + getAdditionalConfig());        autoNioPlusSslPort = connector.getConnectUri().getPort();        autoNioPlusSslURI = connector.getPublishableConnectURI();            }    if (isUseWsConnector()) {        connector = brokerService.addConnector("ws://0.0.0.0:" + getProxyPort(amqpWsPort) + "?transport.tcpNoDelay=true&transport.transformer=" + getAmqpTransformer() + getAdditionalConfig());        amqpWsPort = connector.getConnectUri().getPort();        amqpWsURI = connector.getPublishableConnectURI();            }    if (isUseWssConnector()) {        connector = brokerService.addConnector("wss://0.0.0.0:" + getProxyPort(amqpWssPort) + "?transport.tcpNoDelay=true&transport.transformer=" + getAmqpTransformer() + getAdditionalConfig());        amqpWssPort = connector.getConnectUri().getPort();        amqpWssURI = connector.getPublishableConnectURI();            }}
1
public ASTNode preAnalyze(HiveSemanticAnalyzerHookContext context, ASTNode ast) throws SemanticException
{    try {        userName = context.getUserName();        ipAddress = context.getIpAddress();        command = context.getCommand();        commandType = context.getHiveOperation();    } catch (Throwable t) {                preAnalyzeError = t;    }    return ast;}
1
private void initListeners()
{    final Class<SolrEventListener> clazz = SolrEventListener.class;    final String label = "Event Listener";    for (PluginInfo info : solrConfig.getPluginInfos(SolrEventListener.class.getName())) {        final String event = info.attributes.get("event");        if ("firstSearcher".equals(event)) {            SolrEventListener obj = createInitInstance(info, clazz, label, null);            firstSearcherListeners.add(obj);                    } else if ("newSearcher".equals(event)) {            SolrEventListener obj = createInitInstance(info, clazz, label, null);            newSearcherListeners.add(obj);                    }    }}
1
private void sendRegistration()
{    Register reg;    if (nextRegistration != null) {        reg = nextRegistration;    } else {        reg = new Register();        reg.setTimestamp(System.currentTimeMillis());        reg.setHostname(this.hostname);        reg.setAgentVersion(this.agentVersion);        reg.setPrefix(Configuration.PREFIX_DIR);    }    RegistrationResponse response;    try {        response = handler.handleRegistration(reg);    } catch (AmbariException | InvalidStateTransitionException e) {                return;    }    this.responseId = response.getResponseId();    this.lastRegistrationResponse = response;    this.shouldSendRegistration = false;    this.nextRegistration = null;}
1
public void close() throws IOException
{        consumer.close();}
1
private void setIndexReadOnly(final String destinationIndexName)
{        UpdateSettingsRequest request = new UpdateSettingsRequest(destinationIndexName).setPreserveExisting(true).settings(Settings.builder().put("index.auto_expand_replicas", "0-all").put("index.blocks.write", "true"));    client.admin().indices().updateSettings(request, new ActionListener<>() {        @Override        public void onResponse(AcknowledgedResponse acknowledgedResponse) {            waitForIndexGreen(destinationIndexName);        }        @Override        public void onFailure(Exception e) {            listener.onFailure(e);        }    });}
1
protected void serviceStart() throws Exception
{    amInfos = new LinkedList<AMInfo>();    completedTasksFromPreviousRun = new HashMap<TaskId, TaskInfo>();    processRecovery();    cleanUpPreviousJobOutput();        AMInfo amInfo = MRBuilderUtils.newAMInfo(appAttemptID, startTime, containerID, nmHost, nmPort, nmHttpPort);        job = createJob(getConfig(), forcedState, shutDownMessage);        for (AMInfo info : amInfos) {        dispatcher.getEventHandler().handle(new JobHistoryEvent(job.getID(), new AMStartedEvent(info.getAppAttemptId(), info.getStartTime(), info.getContainerId(), info.getNodeManagerHost(), info.getNodeManagerPort(), info.getNodeManagerHttpPort(), appSubmitTime)));    }        dispatcher.getEventHandler().handle(new JobHistoryEvent(job.getID(), new AMStartedEvent(amInfo.getAppAttemptId(), amInfo.getStartTime(), amInfo.getContainerId(), amInfo.getNodeManagerHost(), amInfo.getNodeManagerPort(), amInfo.getNodeManagerHttpPort(), this.forcedState == null ? null : this.forcedState.toString(), appSubmitTime)));    amInfos.add(amInfo);            DefaultMetricsSystem.initialize("MRAppMaster");    boolean initFailed = false;    if (!errorHappenedShutDown) {                JobEvent initJobEvent = new JobEvent(job.getID(), JobEventType.JOB_INIT);                                jobEventDispatcher.handle(initJobEvent);                                initFailed = (((JobImpl) job).getInternalState() != JobStateInternal.INITED);        if (job.isUber()) {            speculatorEventDispatcher.disableSpeculation();                    } else {                                    dispatcher.getEventHandler().handle(new SpeculatorEvent(job.getID(), clock.getTime()));                    }                        clientService.start();    }        super.serviceStart();        MRApps.setClassLoader(jobClassLoader, getConfig());    if (initFailed) {        JobEvent initFailedEvent = new JobEvent(job.getID(), JobEventType.JOB_INIT_FAILED);        jobEventDispatcher.handle(initFailedEvent);    } else {                startJobs();    }}
1
public void threadFinished()
{    if (channel != null) {        try {            channel.close();        } catch (IOException iex) {                    }    }    channel = null;    JOrphanUtils.closeQuietly(body);    body = null;    stringBody = null;}
1
public AsyncMethodCallback<Void> getResultHandler(final AsyncFrameBuffer fb, final int seqid)
{    final org.apache.thrift.AsyncProcessFunction fcall = this;    return new AsyncMethodCallback<Void>() {        public void onComplete(Void o) {            compact_result result = new compact_result();            try {                fcall.sendResponse(fb, result, org.apache.thrift.protocol.TMessageType.REPLY, seqid);                return;            } catch (Exception e) {                            }            fb.close();        }        public void onError(Exception e) {            byte msgType = org.apache.thrift.protocol.TMessageType.REPLY;            org.apache.thrift.TBase msg;            compact_result result = new compact_result();            {                msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;                msg = (org.apache.thrift.TBase) new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());            }            try {                fcall.sendResponse(fb, msg, msgType, seqid);                return;            } catch (Exception ex) {                            }            fb.close();        }    };}
1
public void testCrawlDbStateTransitionMatrix()
{        Reducer<Text, CrawlDatum, Text, CrawlDatum>.Context context = CrawlDBTestUtil.createContext();    Configuration conf = context.getConfiguration();    CrawlDbUpdateUtil updateDb = null;    try {        updateDb = new CrawlDbUpdateUtil(new CrawlDbReducer(), context);    } catch (IOException e) {        e.printStackTrace();    }    int retryMax = conf.getInt("db.fetch.retry.max", 3);    for (String sched : schedules) {                conf.set("db.fetch.schedule.class", "org.apache.nutch.crawl." + sched);        FetchSchedule schedule = FetchScheduleFactory.getFetchSchedule(conf);        for (int i = 0; i < fetchDbStatusPairs.length; i++) {            byte fromDbStatus = fetchDbStatusPairs[i][1];            for (int j = 0; j < fetchDbStatusPairs.length; j++) {                byte fetchStatus = fetchDbStatusPairs[j][0];                CrawlDatum fromDb = null;                if (fromDbStatus == -1) {                                                } else {                    fromDb = new CrawlDatum();                    fromDb.setStatus(fromDbStatus);                                        schedule.initializeSchedule(CrawlDbUpdateUtil.dummyURL, fromDb);                }                                byte toDbStatus = fetchDbStatusPairs[j][1];                if (fetchStatus == -1) {                    if (fromDbStatus == -1) {                                                toDbStatus = STATUS_DB_UNFETCHED;                    } else {                                                toDbStatus = fromDbStatus;                    }                } else if (fetchStatus == STATUS_FETCH_RETRY) {                                        if (fromDb == null || fromDb.getRetriesSinceFetch() < retryMax) {                        toDbStatus = STATUS_DB_UNFETCHED;                    } else {                        toDbStatus = STATUS_DB_GONE;                    }                }                String fromDbStatusName = (fromDbStatus == -1 ? "<not in CrawlDb>" : getStatusName(fromDbStatus));                String fetchStatusName = (fetchStatus == -1 ? "<only inlinks>" : CrawlDatum.getStatusName(fetchStatus));                                List<CrawlDatum> values = new ArrayList<CrawlDatum>();                for (int l = 0; l <= 2; l++) {                                        CrawlDatum fetch = null;                    if (fetchStatus == -1) {                                                if (l == 0)                            continue;                    } else {                        fetch = new CrawlDatum();                        if (fromDb != null) {                            fetch.set(fromDb);                        } else {                                                        schedule.initializeSchedule(CrawlDbUpdateUtil.dummyURL, fetch);                        }                        fetch.setStatus(fetchStatus);                        fetch.setFetchTime(System.currentTimeMillis());                    }                    if (fromDb != null)                        values.add(fromDb);                    if (fetch != null)                        values.add(fetch);                    for (int n = 0; n < l; n++) {                        values.add(linked);                    }                    List<CrawlDatum> res = updateDb.update(values);                    if (res.size() != 1) {                        fail("CrawlDb update didn't result in one single CrawlDatum per URL");                        continue;                    }                    byte status = res.get(0).getStatus();                    if (status != toDbStatus) {                        fail("CrawlDb update for " + fromDbStatusName + " and " + fetchStatusName + " and " + l + " inlinks results in " + getStatusName(status) + " (expected: " + getStatusName(toDbStatus) + ")");                    }                    values.clear();                }            }        }    }}
1
protected void doClose()
{    if (log.isDebugEnabled()) {            }    try {        getEndpoint().getHandler().release(this);    } catch (Throwable e) {        ExceptionUtils.handleThrowable(e);        if (log.isDebugEnabled()) {                    }    }    socketBufferHandler = SocketBufferHandler.EMPTY;    nonBlockingWriteBuffer.clear();    synchronized (closed) {        if (sslOutputBuffer != null) {            ByteBufferUtils.cleanDirectBuffer(sslOutputBuffer);        }        ((AprEndpoint) getEndpoint()).getPoller().close(getSocket().longValue());    }}
1
private NamespaceDescriptor getNamespaceDescriptor(String namespaceAsString)
{    try {        return this.master.getClusterSchema().getNamespace(namespaceAsString);    } catch (IOException e) {                return null;    }}
1
private ReplicaHandler recoverRbwImpl(ReplicaInPipeline rbw, ExtendedBlock b, long newGS, long minBytesRcvd, long maxBytesRcvd) throws IOException
{    try (AutoCloseableLock lock = datasetLock.acquire()) {                long replicaGenerationStamp = rbw.getGenerationStamp();        if (replicaGenerationStamp < b.getGenerationStamp() || replicaGenerationStamp > newGS) {            throw new ReplicaNotFoundException(ReplicaNotFoundException.UNEXPECTED_GS_REPLICA + b + ". Expected GS range is [" + b.getGenerationStamp() + ", " + newGS + "].");        }                long bytesAcked = rbw.getBytesAcked();        long numBytes = rbw.getNumBytes();        if (bytesAcked < minBytesRcvd || numBytes > maxBytesRcvd) {            throw new ReplicaNotFoundException("Unmatched length replica " + rbw + ": BytesAcked = " + bytesAcked + " BytesRcvd = " + numBytes + " are not in the range of [" + minBytesRcvd + ", " + maxBytesRcvd + "].");        }        long bytesOnDisk = rbw.getBytesOnDisk();        long blockDataLength = rbw.getReplicaInfo().getBlockDataLength();        if (bytesOnDisk != blockDataLength) {                        bytesOnDisk = blockDataLength;            rbw.setLastChecksumAndDataLen(bytesOnDisk, null);        }        if (bytesOnDisk < bytesAcked) {            throw new ReplicaNotFoundException("Found fewer bytesOnDisk than " + "bytesAcked for replica " + rbw);        }        FsVolumeReference ref = rbw.getReplicaInfo().getVolume().obtainReference();        try {                        if (bytesOnDisk > bytesAcked) {                rbw.getReplicaInfo().truncateBlock(bytesAcked);                rbw.setNumBytes(bytesAcked);                rbw.setLastChecksumAndDataLen(bytesAcked, null);            }                        rbw.getReplicaInfo().bumpReplicaGS(newGS);        } catch (IOException e) {            IOUtils.cleanup(null, ref);            throw e;        }        return new ReplicaHandler(rbw, ref);    }}
1
public void managerReceivesAlertsFromAllMembersAtAlertLevelAndAbove()
{    changeAlertLevel(WARNING);    for (VM memberVM : toArray(memberVM1, memberVM2, memberVM3)) {        memberVM.invoke(() -> {            try (IgnoredException ie = addIgnoredException(warningLevelMessage)) {                            }            try (IgnoredException ie = addIgnoredException(errorLevelMessage)) {                            }            try (IgnoredException ie = addIgnoredException(severeLevelMessage)) {                            }        });    }    managerVM.invoke(() -> {        assertThat(captureAllAlerts(9)).contains(warningAlert, warningAlert, warningAlert, errorAlert, errorAlert, errorAlert, severeAlert, severeAlert, severeAlert);    });}
1
public void info(String message, Throwable throwable)
{    }
1
 long modifyNamespace(final NamespaceDescriptor newNsDescriptor, final long nonceGroup, final long nonce) throws IOException
{    checkInitialized();    TableName.isLegalNamespaceName(Bytes.toBytes(newNsDescriptor.getName()));    return MasterProcedureUtil.submitProcedure(new MasterProcedureUtil.NonceProcedureRunnable(this, nonceGroup, nonce) {        @Override        protected void run() throws IOException {            NamespaceDescriptor oldNsDescriptor = getNamespace(newNsDescriptor.getName());            getMaster().getMasterCoprocessorHost().preModifyNamespace(oldNsDescriptor, newNsDescriptor);                                    ProcedurePrepareLatch latch = ProcedurePrepareLatch.createBlockingLatch();                                                setProcId(getClusterSchema().modifyNamespace(newNsDescriptor, getNonceKey(), latch));            latch.await();            getMaster().getMasterCoprocessorHost().postModifyNamespace(oldNsDescriptor, newNsDescriptor);        }        @Override        protected String getDescription() {            return "ModifyNamespaceProcedure";        }    });}
1
private ConfigOverlay updateNamedPlugin(SolrConfig.SolrPluginInfo info, CommandOperation op, ConfigOverlay overlay, boolean isCeate)
{    String name = op.getStr(NAME);    String clz = info.options.contains(REQUIRE_CLASS) ? op.getStr(CLASS_NAME) : op.getStr(CLASS_NAME, null);    op.getMap(DEFAULTS, null);    op.getMap(PluginInfo.INVARIANTS, null);    op.getMap(PluginInfo.APPENDS, null);    if (op.hasError())        return overlay;    if (info.clazz == PluginBag.RuntimeLib.class) {        if (!PluginBag.RuntimeLib.isEnabled()) {            op.addError("Solr not started with -Denable.runtime.lib=true");            return overlay;        }        try {            new PluginBag.RuntimeLib(req.getCore()).init(new PluginInfo(info.tag, op.getDataMap()));        } catch (Exception e) {            op.addError(e.getMessage());                        return overlay;        }    }    if (!verifyClass(op, clz, info.clazz))        return overlay;    if (pluginExists(info, overlay, name)) {        if (isCeate) {            op.addError(formatString(" ''{0}'' already exists . Do an ''{1}'' , if you want to change it ", name, "update-" + info.getTagCleanLower()));            return overlay;        } else {            return overlay.addNamedPlugin(op.getDataMap(), info.getCleanTag());        }    } else {        if (isCeate) {            return overlay.addNamedPlugin(op.getDataMap(), info.getCleanTag());        } else {            op.addError(formatString(" ''{0}'' does not exist . Do an ''{1}'' , if you want to create it ", name, "create-" + info.getTagCleanLower()));            return overlay;        }    }}
1
public void testHistoryEvents() throws Exception
{    Configuration conf = new Configuration();    MRApp app = new MRAppWithHistory(2, 1, true, this.getClass().getName(), true);    app.submit(conf);    Job job = app.getContext().getAllJobs().values().iterator().next();    JobId jobId = job.getID();        app.waitForState(job, JobState.SUCCEEDED);        app.waitForState(Service.STATE.STOPPED);    /*     * Use HistoryContext to read logged events and verify the number of      * completed maps     */    HistoryContext context = new JobHistory();        ((JobHistory) context).init(conf);    ((JobHistory) context).start();    Assert.assertTrue(context.getStartTime() > 0);    Assert.assertEquals(((JobHistory) context).getServiceState(), Service.STATE.STARTED);        Job parsedJob = context.getJob(jobId);        ((JobHistory) context).stop();    Assert.assertEquals(((JobHistory) context).getServiceState(), Service.STATE.STOPPED);    Assert.assertEquals("CompletedMaps not correct", 2, parsedJob.getCompletedMaps());    Assert.assertEquals(System.getProperty("user.name"), parsedJob.getUserName());    Map<TaskId, Task> tasks = parsedJob.getTasks();    Assert.assertEquals("No of tasks not correct", 3, tasks.size());    for (Task task : tasks.values()) {        verifyTask(task);    }    Map<TaskId, Task> maps = parsedJob.getTasks(TaskType.MAP);    Assert.assertEquals("No of maps not correct", 2, maps.size());    Map<TaskId, Task> reduces = parsedJob.getTasks(TaskType.REDUCE);    Assert.assertEquals("No of reduces not correct", 1, reduces.size());    Assert.assertEquals("CompletedReduce not correct", 1, parsedJob.getCompletedReduces());    Assert.assertEquals("Job state not currect", JobState.SUCCEEDED, parsedJob.getState());}
1
public void inform(ResourceLoader resourceLoader)
{    provider.inform(resourceLoader);    boolean reloaded = provider.reload();    if (!reloaded) {            }}
1
private SSLContext createContext() throws GeneralSecurityException
{    SSLContext context;    if (pro != null) {                context = SSLContext.getInstance(DEFAULT_SSL_PROTOCOL, pro);    } else {                context = SSLContext.getInstance(DEFAULT_SSL_PROTOCOL);    }    KeyManagerFactory managerFactory = KeyManagerFactory.getInstance(KeyManagerFactory.getDefaultAlgorithm());    JmeterKeyStore keys = this.getKeyStore();    managerFactory.init(null, defaultpw == null ? new char[] {} : defaultpw.toCharArray());    KeyManager[] managers = managerFactory.getKeyManagers();    KeyManager[] newManagers = new KeyManager[managers.length];    if (log.isDebugEnabled()) {            }        for (int i = 0; i < managers.length; i++) {        if (managers[i] instanceof X509KeyManager) {            X509KeyManager manager = (X509KeyManager) managers[i];            newManagers[i] = new WrappedX509KeyManager(manager, keys);        } else {            newManagers[i] = managers[i];        }    }        TrustManagerFactory tmfactory = TrustManagerFactory.getInstance(TrustManagerFactory.getDefaultAlgorithm());    tmfactory.init(this.getTrustStore());        TrustManager[] trustmanagers = tmfactory.getTrustManagers();    for (int i = 0; i < trustmanagers.length; i++) {        if (trustmanagers[i] instanceof X509TrustManager) {            trustmanagers[i] = new CustomX509TrustManager((X509TrustManager) trustmanagers[i]);        }    }    context.init(newManagers, trustmanagers, this.rand);    if (log.isDebugEnabled()) {        String[] dCiphers = context.getSocketFactory().getDefaultCipherSuites();        String[] sCiphers = context.getSocketFactory().getSupportedCipherSuites();        int len = (dCiphers.length > sCiphers.length) ? dCiphers.length : sCiphers.length;        for (int i = 0; i < len; i++) {            if (i < dCiphers.length) {                            }            if (i < sCiphers.length) {                            }        }    }    return context;}
1
protected void doStop() throws Exception
{    isMaster = false;    fireLeadershipChangedEvent(Optional.empty());        raftHandle.channel().disconnect();    if (raftHandle != null && raftHandle.log() != null) {        raftHandle.log().close();            }}
1
private static void validateAttributes(PropertyDescriptor pd, PropertyEditor pe)
{    final Object deflt = pd.getValue(DEFAULT);    if (deflt == null) {        if (notNull(pd)) {            if (log.isWarnEnabled()) {                            }        }        if (noSaveDefault(pd)) {            if (log.isWarnEnabled()) {                            }        }    } else {                final Class<?> defltClass = deflt.getClass();                final Class<?> propClass = ClassUtils.primitiveToWrapper(pd.getPropertyType());        if (!propClass.isAssignableFrom(defltClass)) {            if (log.isWarnEnabled()) {                            }        }    }    if (notOther(pd) && pd.getValue(TAGS) == null && pe.getTags() == null) {        if (log.isWarnEnabled()) {                    }    }    if (!notNull(pd)) {        Class<?> propertyType = pd.getPropertyType();        if (propertyType.isPrimitive()) {            if (log.isWarnEnabled()) {                            }        }    }    if (!pd.attributeNames().hasMoreElements()) {        if (log.isWarnEnabled()) {                    }    }}
1
public void addRuntimeStat(RuntimeStat stat) throws MetaException
{        MRuntimeStat mStat = MRuntimeStat.fromThrift(stat);    boolean committed = false;    openTransaction();    try {        pm.makePersistent(mStat);        committed = commitTransaction();    } finally {        if (!committed) {            rollbackTransaction();        }    }}
1
public void removeMap(String mapId)
{    IndexInformation info = cache.get(mapId);    if (info == null || isUnderConstruction(info)) {        return;    }    info = cache.remove(mapId);    if (info != null) {        totalMemoryUsed.addAndGet(-info.getSize());        if (!queue.remove(mapId)) {                    }    } else {            }}
1
public void error(String format, Object... arguments)
{    errorMessages.add(new LogMessage(null, format, null, arguments));    }
1
public void reInitializeContainerAsync(ContainerId containerId, ContainerLaunchContext containerLaunchContex, boolean autoCommit)
{    if (!(callbackHandler instanceof AbstractCallbackHandler)) {                return;    }    AbstractCallbackHandler handler = (AbstractCallbackHandler) callbackHandler;    if (containers.get(containerId) == null) {        handler.onContainerReInitializeError(containerId, RPCUtil.getRemoteException("Container " + containerId + " is not started"));    }    try {        events.put(new ReInitializeContainerEvevnt(containerId, client.getNodeIdOfStartedContainer(containerId), containerLaunchContex, autoCommit));    } catch (InterruptedException e) {                handler.onContainerReInitializeError(containerId, e);    }}
1
public void testJmsBrowserGetsPagedIn() throws Exception
{    final int numToSend = 10;    for (int i = 0; i < ITERATIONS; i++) {        produceMessages(numToSend, 4, "TestQ");        ArrayList<Message> browsed = browseMessages("TestQ");                assertEquals(0, browsed.size());        Message message = consumeOneMessage("TestQ", Session.CLIENT_ACKNOWLEDGE);        assertNotNull(message);        browsed = browseMessages("TestQ");                assertEquals("see only the paged in for pull", 1, browsed.size());                ArrayList<Message> consumeList = consumeMessages("TestQ");                assertEquals(numToSend, consumeList.size());    }}
1
public void connect()
{    if (isConnected()) {        throw new MetricsException("Already connected to Graphite");    }    if (tooManyConnectionFailures()) {                return;    }    try {                socket = new Socket(serverHost, serverPort);        writer = new OutputStreamWriter(socket.getOutputStream(), StandardCharsets.UTF_8);    } catch (Exception e) {        connectionFailures++;        if (tooManyConnectionFailures()) {                                }        throw new MetricsException("Error creating connection, " + serverHost + ":" + serverPort, e);    }}
1
protected void ensureExists(final String path, final byte[] data, final List<ACL> acl, final CreateMode flags)
{    try {        retryOperation(() -> {            Stat stat = zookeeper.exists(path, false);            if (stat != null) {                return true;            }            zookeeper.create(path, data, acl, flags);            return true;        });    } catch (KeeperException | InterruptedException e) {            }}
1
public AttachAnswer dettachVolume(DettachCommand cmd)
{        String vmName = cmd.getVmName();    DiskTO disk = cmd.getDisk();    return attachDetach(cmd, vmName, disk, false);}
1
public static List<HostInfo> getHostInfoList(Properties properties)
{    List<HostInfo> hosts = new ArrayList<HostInfo>();    String hostNames = properties.getProperty(RpcClientConfigurationConstants.CONFIG_HOSTS);    String[] hostList;    if (hostNames != null && !hostNames.isEmpty()) {        hostList = hostNames.split("\\s+");        for (int i = 0; i < hostList.length; i++) {            String hostAndPortStr = properties.getProperty(RpcClientConfigurationConstants.CONFIG_HOSTS_PREFIX + hostList[i]);                        if (hostAndPortStr != null) {                String[] hostAndPort = hostAndPortStr.split(":");                if (hostAndPort.length != 2) {                                        throw new FlumeException("Invalid host address" + hostAndPortStr);                }                Integer port = null;                try {                    port = Integer.parseInt(hostAndPort[1]);                } catch (NumberFormatException e) {                                        throw new FlumeException("Invalid port number" + hostAndPortStr);                }                HostInfo info = new HostInfo(hostList[i], hostAndPort[0].trim(), port);                hosts.add(info);            }        }    }    return hosts;}
1
public List<Exchange> getExchanges()
{    final List<Exchange> answer = new ArrayList<>();    GenericFileConsumer<?> consumer = null;    try {                        consumer = createConsumer(null);        consumer.setCustomProcessor(new Processor() {            @Override            public void process(Exchange exchange) throws Exception {                answer.add(exchange);            }        });                consumer.setStartScheduler(false);                ServiceHelper.startService(consumer);                consumer.poll();    } catch (Exception e) {        throw RuntimeCamelException.wrapRuntimeCamelException(e);    } finally {        try {            ServiceHelper.stopService(consumer);        } catch (Exception e) {                    }    }    return answer;}
1
public void testHelloWorld()
{    serviceA.registerRequestHandler("internal:sayHello", ThreadPool.Names.GENERIC, StringMessageRequest::new, (request, channel, task) -> {        assertThat("moshe", equalTo(request.message));        try {            channel.sendResponse(new StringMessageResponse("hello " + request.message));        } catch (IOException e) {                        fail(e.getMessage());        }    });    TransportFuture<StringMessageResponse> res = submitRequest(serviceB, nodeA, "internal:sayHello", new StringMessageRequest("moshe"), new TransportResponseHandler<StringMessageResponse>() {        @Override        public StringMessageResponse read(StreamInput in) throws IOException {            return new StringMessageResponse(in);        }        @Override        public String executor() {            return ThreadPool.Names.GENERIC;        }        @Override        public void handleResponse(StringMessageResponse response) {            assertThat("hello moshe", equalTo(response.message));        }        @Override        public void handleException(TransportException exp) {                        fail("got exception instead of a response: " + exp.getMessage());        }    });    try {        StringMessageResponse message = res.get();        assertThat("hello moshe", equalTo(message.message));    } catch (Exception e) {        assertThat(e.getMessage(), false, equalTo(true));    }    res = submitRequest(serviceB, nodeA, "internal:sayHello", new StringMessageRequest("moshe"), new TransportResponseHandler<>() {        @Override        public StringMessageResponse read(StreamInput in) throws IOException {            return new StringMessageResponse(in);        }        @Override        public String executor() {            return ThreadPool.Names.GENERIC;        }        @Override        public void handleResponse(StringMessageResponse response) {            assertThat("hello moshe", equalTo(response.message));        }        @Override        public void handleException(TransportException exp) {                        fail("got exception instead of a response: " + exp.getMessage());        }    });    try {        StringMessageResponse message = res.get();        assertThat("hello moshe", equalTo(message.message));    } catch (Exception e) {        assertThat(e.getMessage(), false, equalTo(true));    }}
1
 void downloadInterpreter(InterpreterInstallationRequest request, DependencyResolver dependencyResolver, Path interpreterDir, ServiceCallback<String> serviceCallback)
{    try {                if (null != serviceCallback) {            serviceCallback.onStart("Starting to download " + request.getName() + " interpreter", null);        }        dependencyResolver.load(request.getArtifact(), interpreterDir.toFile());        interpreterSettingManager.refreshInterpreterTemplates();                if (null != serviceCallback) {            serviceCallback.onSuccess(request.getName() + " downloaded", null);        }    } catch (RepositoryException | IOException e) {                try {            FileUtils.deleteDirectory(interpreterDir.toFile());        } catch (IOException e1) {                    }        if (null != serviceCallback) {            try {                serviceCallback.onFailure(new Exception("Error while downloading " + request.getName() + " as " + e.getMessage()), null);            } catch (IOException e1) {                            }        }    }}
1
private String scanClassPathForJobJar() throws IOException
{        JarFileWithEntryClass jobJar = JarManifestParser.findOnlyEntryClass(jarsOnClassPath.get());        return jobJar.getEntryClass();}
1
public void error(String msg, Throwable t)
{    }
1
private boolean stop(final ControlFileWatchdog fileWatchdog)
{    boolean interrupted = false;    try {        fileWatchdog.stop();    } catch (InterruptedException e) {        interrupted = true;            }    return interrupted;}
1
public void onLoginSuccess(String username, String session)
{    UserSessionContainer.instance.setSession(username, session);    /* TODO(xxx): add proper roles */    HashSet<String> userAndRoles = new HashSet<>();    userAndRoles.add(username);    ServiceContext context = new ServiceContext(new org.apache.zeppelin.user.AuthenticationInfo(username), userAndRoles);    try {                NotebookServer.getInstance().broadcastReloadedNoteList(null, context);    } catch (IOException e) {            }    ZeppelinhubUtils.userLoginRoutine(username);}
1
public void connect(String id, InetSocketAddress address, int sendBufferSize, int receiveBufferSize) throws IOException
{    ensureNotRegistered(id);    SocketChannel socketChannel = SocketChannel.open();    SelectionKey key = null;    try {        configureSocketChannel(socketChannel, sendBufferSize, receiveBufferSize);        boolean connected = doConnect(socketChannel, address);        key = registerChannel(id, socketChannel, SelectionKey.OP_CONNECT);        if (connected) {                                    immediatelyConnectedKeys.add(key);            key.interestOps(0);        }    } catch (IOException | RuntimeException e) {        if (key != null)            immediatelyConnectedKeys.remove(key);        channels.remove(id);        socketChannel.close();        throw e;    }}
1
protected boolean isQueueNameValid(Cluster cluster, Set<String> validLeafQueues, String queueNameProperty, String configType)
{    Config site = cluster.getDesiredConfigByType(configType);    Map<String, String> properties = site.getProperties();    boolean result = properties.containsKey(queueNameProperty) && validLeafQueues.contains(properties.get(queueNameProperty));    if (!result) {            }    return result;}
1
private static void checkUniqueDoc(SolrClient client, String collection, String idField, String id, boolean shouldBeThere) throws IOException, SolrServerException
{    TimeOut timeOut = new TimeOut(100, TimeUnit.SECONDS, TimeSource.NANO_TIME);    final SolrQuery solrQuery = new SolrQuery(idField + ":" + id);    while (!timeOut.hasTimedOut()) {        QueryResponse rsp = client.query(collection, solrQuery);        long found = rsp.getResults().getNumFound();        if (shouldBeThere && found == 1) {            return;        }        if (shouldBeThere == false && found == 0) {            return;        }                try {            Thread.sleep(250);        } catch (InterruptedException e) {                        return;        }    }}
1
private static Properties loadProps(File file)
{    final Properties props = new Properties();    try (FileInputStream inStream = new FileInputStream(file)) {        props.load(inStream);    } catch (IOException e) {                        System.err.println("Problem loading properties. " + e);    }    return props;}
1
private String createDbGuidAndPersist() throws MetaException
{    boolean success = false;    Query query = null;    try {        openTransaction();        MMetastoreDBProperties prop = new MMetastoreDBProperties();        prop.setPropertykey("guid");        final String guid = UUID.randomUUID().toString();                prop.setPropertyValue(guid);        prop.setDescription("Metastore DB GUID generated on " + LocalDateTime.now().format(DateTimeFormatter.ofPattern("yyyy-MM-dd HH:mm:ss.SSS")));        pm.makePersistent(prop);        success = commitTransaction();        if (success) {                        return guid;        }    } catch (Exception e) {            } finally {        rollbackAndCleanup(success, query);    }                final String guid = getGuidFromDB();    if (guid == null) {        throw new MetaException("Unable to create or fetch the metastore database uuid");    }    return guid;}
1
 void processBulkIndexIngestRequest(Task task, BulkRequest original, ActionListener<BulkResponse> listener)
{    final long ingestStartTimeInNanos = System.nanoTime();    final BulkRequestModifier bulkRequestModifier = new BulkRequestModifier(original);    ingestService.executeBulkRequest(original.numberOfActions(), () -> bulkRequestModifier, bulkRequestModifier::markItemAsFailed, (originalThread, exception) -> {        if (exception != null) {                        listener.onFailure(exception);        } else {            long ingestTookInMillis = TimeUnit.NANOSECONDS.toMillis(System.nanoTime() - ingestStartTimeInNanos);            BulkRequest bulkRequest = bulkRequestModifier.getBulkRequest();            ActionListener<BulkResponse> actionListener = bulkRequestModifier.wrapActionListenerIfNeeded(ingestTookInMillis, listener);            if (bulkRequest.requests().isEmpty()) {                                                                actionListener.onResponse(new BulkResponse(new BulkItemResponse[0], 0));            } else {                                if (originalThread == Thread.currentThread()) {                    assert Thread.currentThread().getName().contains(ThreadPool.Names.WRITE);                    doExecute(task, bulkRequest, actionListener);                } else {                    threadPool.executor(ThreadPool.Names.WRITE).execute(new AbstractRunnable() {                        @Override                        public void onFailure(Exception e) {                            listener.onFailure(e);                        }                        @Override                        protected void doRun() throws Exception {                            doExecute(task, bulkRequest, actionListener);                        }                        @Override                        public boolean isForceExecution() {                                                        return true;                        }                    });                }            }        }    }, bulkRequestModifier::markItemAsDropped);}
1
public void jpaInitialized(JpaInitializedEvent event)
{        jpaStarted.set(true);    instance = loadInstance();}
1
private Map<Node, Integer> pickNodesAtRandom(int numNodes, String excludedScope, Collection<Node> excludedNodes, int ancestorGen)
{    Map<Node, Integer> frequency = new HashMap<Node, Integer>();    for (Node dnd : dataNodes) {        frequency.put(dnd, 0);    }    for (int j = 0; j < numNodes; j++) {        Node node = cluster.chooseRandom(excludedScope, excludedNodes, ancestorGen);        if (node != null) {            frequency.put(node, frequency.get(node) + 1);        }    }        return frequency;}
1
protected void doStop() throws Exception
{        if (cs != null) {        try {            cs.stop();                    } catch (IOException e) {                    }        cs = null;    }        if (registry != null) {        try {            UnicastRemoteObject.unexportObject(registry, true);                    } catch (NoSuchObjectException e) {                    }    }    if (mbeansRegistered.isEmpty()) {        return;    }        ObjectName[] mBeans = mbeansRegistered.keySet().toArray(new ObjectName[mbeansRegistered.size()]);    int caught = 0;    for (ObjectName name : mBeans) {        try {            unregister(name);        } catch (Exception e) {                        caught++;        }    }    if (caught > 0) {            }    ServiceHelper.stopService(assembler);}
1
public void run()
{    int retries = 1;    boolean success = false;    try {        while (!closed && sendBootstrapCommand() != BootstrapStatus.SUBMITTED) {            Thread.sleep(BOOTSTRAP_RETRY_DELAY_MS);        }        TimeOut timeOut = new TimeOut(BOOTSTRAP_TIMEOUT_SECONDS, TimeUnit.SECONDS, TimeSource.NANO_TIME);        while (!timeOut.hasTimedOut()) {            if (closed) {                                state.setBootstrapInProgress(false);                break;            }            BootstrapStatus status = getBoostrapStatus();            if (status == BootstrapStatus.RUNNING) {                try {                                        timeOut.sleep(BOOTSTRAP_RETRY_DELAY_MS);                } catch (InterruptedException e) {                    Thread.currentThread().interrupt();                }            } else if (status == BootstrapStatus.COMPLETED) {                                long checkpoint = CdcrReplicatorManager.this.getCheckpoint(state);                                CdcrUpdateLog.CdcrLogReader reader1 = ulog.newLogReader();                reader1.seek(checkpoint);                success = true;                break;            } else if (status == BootstrapStatus.FAILED) {                                                if (retries >= MAX_BOOTSTRAP_ATTEMPTS) {                                        break;                } else {                                        while (!closed && sendBootstrapCommand() != BootstrapStatus.SUBMITTED) {                        Thread.sleep(BOOTSTRAP_RETRY_DELAY_MS);                    }                                        timeOut = new TimeOut(BOOTSTRAP_TIMEOUT_SECONDS, TimeUnit.SECONDS, TimeSource.NANO_TIME);                    retries++;                }            } else if (status == BootstrapStatus.NOTFOUND || status == BootstrapStatus.CANCELLED) {                                                while (!closed && sendBootstrapCommand() != BootstrapStatus.SUBMITTED) {                    Thread.sleep(BOOTSTRAP_RETRY_DELAY_MS);                }                retries = 1;                                timeOut = new TimeOut(6L * 3600L * 3600L, TimeUnit.SECONDS, TimeSource.NANO_TIME);            } else if (status == BootstrapStatus.UNKNOWN || status == BootstrapStatus.SUBMITTED) {                                                                timeOut.sleep(BOOTSTRAP_RETRY_DELAY_MS);            }        }    } catch (InterruptedException e) {                state.reportError(CdcrReplicatorState.ErrorType.INTERNAL);        Thread.currentThread().interrupt();    } catch (IOException | SolrServerException | SolrException e) {                state.reportError(CdcrReplicatorState.ErrorType.BAD_REQUEST);    } finally {        if (success) {                        state.setBootstrapInProgress(false);        }    }}
1
private void clearMBeans()
{    String query = baseOname.toString() + ",*";    try {        ObjectName name = new ObjectName(query);        Set<ObjectName> onames = mbserver.queryNames(name, null);        for (ObjectName objectName : onames) {            unregisterJmx(objectName);        }    } catch (MalformedObjectNameException e) {            }}
1
 void handleFile(FileMetadata file, FileMetadata tempFile) throws IOException
{                store.delete(file.getKey());    store.delete(tempFile.getKey());}
1
public void terminateSession()
{    try {        zk.closeSession(sessionId);    } catch (Exception e) {            }}
1
public void onMessage(Message message)
{    try {        int expectedMessageId = rollbacks.get() / deliveryCount;                rollbacks.incrementAndGet();        session.rollback();    } catch (Throwable e) {                        if (e instanceof Error) {            error[0] = (Error) e;        } else {            fail("unexpected exception: " + e);        }    }}
1
public MiloClientComponent configureMiloClientComponent() throws Exception
{    MiloClientComponent component = new MiloClientComponent();    component.setCamelContext(camelContext);    Map<String, Object> parameters = new HashMap<>();    IntrospectionSupport.getProperties(configuration, parameters, null, false);    for (Map.Entry<String, Object> entry : parameters.entrySet()) {        Object value = entry.getValue();        Class<?> paramClass = value.getClass();        if (paramClass.getName().endsWith("NestedConfiguration")) {            Class nestedClass = null;            try {                nestedClass = (Class) paramClass.getDeclaredField("CAMEL_NESTED_CLASS").get(null);                HashMap<String, Object> nestedParameters = new HashMap<>();                IntrospectionSupport.getProperties(value, nestedParameters, null, false);                Object nestedProperty = nestedClass.newInstance();                CamelPropertiesHelper.setCamelProperties(camelContext, nestedProperty, nestedParameters, false);                entry.setValue(nestedProperty);            } catch (NoSuchFieldException e) {            }        }    }    CamelPropertiesHelper.setCamelProperties(camelContext, component, parameters, false);    if (ObjectHelper.isNotEmpty(customizers)) {        for (ComponentCustomizer<MiloClientComponent> customizer : customizers) {            boolean useCustomizer = (customizer instanceof HasId) ? HierarchicalPropertiesEvaluator.evaluate(applicationContext.getEnvironment(), "camel.component.customizer", "camel.component.milo-client.customizer", ((HasId) customizer).getId()) : HierarchicalPropertiesEvaluator.evaluate(applicationContext.getEnvironment(), "camel.component.customizer", "camel.component.milo-client.customizer");            if (useCustomizer) {                                customizer.customize(component);            }        }    }    return component;}
1
private void refreshNodes(List<ZKUtil.NodeAndData> nodes)
{    for (ZKUtil.NodeAndData n : nodes) {        String path = n.getNode();        String keyId = ZKUtil.getNodeName(path);        try {            byte[] data = n.getData();            if (data == null || data.length == 0) {                                continue;            }            AuthenticationKey key = (AuthenticationKey) Writables.getWritable(data, new AuthenticationKey());            secretManager.addKey(key);        } catch (IOException ioe) {                        watcher.abort("Error deserializing key from znode " + path, ioe);        }    }}
1
public void run()
{    if (logger.isTraceEnabled()) {        logger.trace("Submitting new rescheduling cluster info update job");    }    try {        threadPool.executor(executorName()).execute(() -> {            try {                maybeRefresh();            } finally {                                if (isMaster) {                    if (logger.isTraceEnabled()) {                        logger.trace("Scheduling next run for updating cluster info in: {}", updateFrequency.toString());                    }                    threadPool.scheduleUnlessShuttingDown(updateFrequency, executorName(), this);                }            }        });    } catch (EsRejectedExecutionException ex) {        if (logger.isDebugEnabled()) {                    }    }}
1
protected void poison(final Throwable t)
{    this.poisonCause = t;        try {        if (fileOut != null) {            fileOut.close();        }        closed = true;    } catch (final IOException innerIOE) {        t.addSuppressed(innerIOE);    }}
1
public void markDead(InetAddressAndPort addr, EndpointState localState)
{    checkProperThreadForStateMutation();    if (logger.isTraceEnabled())        logger.trace("marking as down {}", addr);    localState.markDead();    liveEndpoints.remove(addr);    unreachableEndpoints.put(addr, System.nanoTime());        for (IEndpointStateChangeSubscriber subscriber : subscribers) subscriber.onDead(addr, localState);    if (logger.isTraceEnabled())        logger.trace("Notified {}", subscribers);    GossiperDiagnostics.markedDead(this, addr, localState);}
1
 static void doFinalize(StorageDirectory sd) throws IOException
{    File prevDir = sd.getPreviousDir();    if (!prevDir.exists()) {                                return;    }        Preconditions.checkState(sd.getCurrentDir().exists(), "Current directory must exist.");    final File tmpDir = sd.getFinalizedTmp();        NNStorage.rename(prevDir, tmpDir);    NNStorage.deleteDir(tmpDir);    }
1
protected void checkPermissions(final Configuration conf, final Path path, final EnumSet<FsAction> actions) throws IOException, LoginException, HiveException
{    if (path == null) {        throw new IllegalArgumentException("path is null");    }    if (userHasProxyPrivilege(authenticator.getUserName(), conf)) {                return;    }    final FileSystem fs = path.getFileSystem(conf);    FileStatus pathStatus = FileUtils.getFileStatusOrNull(fs, path);    if (pathStatus != null) {        checkPermissions(fs, pathStatus, actions, authenticator.getUserName());    } else if (path.getParent() != null) {                Path par = path.getParent();        FileStatus parStatus = null;        while (par != null) {            parStatus = FileUtils.getFileStatusOrNull(fs, par);            if (parStatus != null) {                break;            }            par = par.getParent();        }        checkPermissions(fs, parStatus, actions, authenticator.getUserName());    }}
1
public void nextTuple()
{    try {        Message msg = consumer.receive(POLL_INTERVAL_MS);        if (msg != null) {                        messageHandler.emit(msg);        }    } catch (JMSException ex) {            }}
1
protected long getStoredSequenceIdForMessage(MessageId messageId)
{    try {        return brokerService.getPersistenceAdapter().getLastProducerSequenceId(messageId.getProducerId());    } catch (IOException ignored) {            }    return -1;}
1
public void persistOutcome(Tx tx, TransactionId txid) throws IOException
{    if (haveOutcome) {        super.persistOutcome(tx, txid);    }    try {                persistenceAdapter.stop();    } catch (Exception e) {                exceptions.add(e);    }}
1
public void testIterationStart(LoopIterationEvent event)
{    JMeterVariables jMeterVariables = JMeterContextService.getContext().getVariables();    if ((getControlledByThread() && !jMeterVariables.isSameUserOnNextIteration()) || (!getControlledByThread() && getClearEachIteration())) {                        setProperty(initialCookies.clone());    }}
1
 void prepareForClose()
{    try {        finishKrf();    } catch (CancelException e) {                if (logger.isDebugEnabled()) {                    }    }}
1
public List<StreamingConfig> getStreamings(@RequestParam(value = "table", required = false) String table, @RequestParam(value = "project", required = false) String project, @RequestParam(value = "limit", required = false) Integer limit, @RequestParam(value = "offset", required = false) Integer offset)
{    try {        return streamingService.getStreamingConfigs(table, project, limit, offset);    } catch (IOException e) {                throw new InternalErrorException("Failed to deal with the request: " + e.getLocalizedMessage(), e);    }}
1
private static void invokeSetStoragePolicy(final FileSystem fs, final Path path, final String storagePolicy) throws IOException
{    Method m = null;    Exception toThrow = null;    try {        m = fs.getClass().getDeclaredMethod("setStoragePolicy", new Class<?>[] { Path.class, String.class });        m.setAccessible(true);    } catch (NoSuchMethodException e) {        toThrow = e;        final String msg = "FileSystem doesn't support setStoragePolicy; HDFS-6584, HDFS-9345 " + "not available. This is normal and expected on earlier Hadoop versions.";        if (!warningMap.containsKey(fs)) {            warningMap.put(fs, true);                    } else if (LOG.isDebugEnabled()) {                    }        m = null;    } catch (SecurityException e) {        toThrow = e;        final String msg = "No access to setStoragePolicy on FileSystem from the SecurityManager; " + "HDFS-6584, HDFS-9345 not available. This is unusual and probably warrants an email " + "to the user@hbase mailing list. Please be sure to include a link to your configs, and " + "logs that include this message and period of time before it. Logs around service " + "start up will probably be useful as well.";        if (!warningMap.containsKey(fs)) {            warningMap.put(fs, true);                    } else if (LOG.isDebugEnabled()) {                    }                m = null;    }    if (m != null) {        try {            m.invoke(fs, path, storagePolicy);            if (LOG.isDebugEnabled()) {                            }        } catch (Exception e) {            toThrow = e;                        if (!warningMap.containsKey(fs)) {                warningMap.put(fs, true);                            } else if (LOG.isDebugEnabled()) {                            }                        if (e instanceof InvocationTargetException) {                final Throwable exception = e.getCause();                if (exception instanceof RemoteException && HadoopIllegalArgumentException.class.getName().equals(((RemoteException) exception).getClassName())) {                    if (LOG.isDebugEnabled()) {                                            }                                                } else if (exception instanceof UnsupportedOperationException) {                    if (LOG.isDebugEnabled()) {                                            }                }            }        }    }    if (toThrow != null) {        throw new IOException(toThrow);    }}
1
public void onError(Exception e)
{    byte msgType = org.apache.thrift.protocol.TMessageType.REPLY;    org.apache.thrift.TBase msg;    ExecuteStatement_result result = new ExecuteStatement_result();    {        msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;        msg = (org.apache.thrift.TBase) new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());    }    try {        fcall.sendResponse(fb, msg, msgType, seqid);        return;    } catch (Exception ex) {            }    fb.close();}
1
public NodePublishVolumeResponse nodePublishVolume(NodePublishVolumeRequest request) throws YarnException, IOException
{        Csi.NodePublishVolumeRequest req = ProtoTranslatorFactory.getTranslator(NodePublishVolumeRequest.class, Csi.NodePublishVolumeRequest.class).convertTo(request);        csiClient.nodePublishVolume(req);    return NodePublishVolumeResponse.newInstance();}
1
public void close()
{    for (String path : currSpillDirs) {        try {            fileManager.deleteDir(path);        } catch (IOException e) {                                }                currSpillDirs.clear();    }}
1
public RecoveredAMRMProxyState loadAMRMProxyState() throws IOException
{    RecoveredAMRMProxyState result = new RecoveredAMRMProxyState();    Set<String> unknownKeys = new HashSet<>();    LeveldbIterator iter = null;    try {        iter = new LeveldbIterator(db);        iter.seek(bytes(AMRMPROXY_KEY_PREFIX));        while (iter.hasNext()) {            Entry<byte[], byte[]> entry = iter.peekNext();            String key = asString(entry.getKey());            if (!key.startsWith(AMRMPROXY_KEY_PREFIX)) {                break;            }            String suffix = key.substring(AMRMPROXY_KEY_PREFIX.length());            if (suffix.equals(CURRENT_MASTER_KEY_SUFFIX)) {                iter.next();                result.setCurrentMasterKey(parseMasterKey(entry.getValue()));                            } else if (suffix.equals(NEXT_MASTER_KEY_SUFFIX)) {                iter.next();                result.setNextMasterKey(parseMasterKey(entry.getValue()));                            } else {                                                int idEndPos;                ApplicationAttemptId attemptId;                try {                    idEndPos = key.indexOf('/', AMRMPROXY_KEY_PREFIX.length());                    if (idEndPos < 0) {                        throw new IOException("Unable to determine attemptId in key: " + key);                    }                    attemptId = ApplicationAttemptId.fromString(key.substring(AMRMPROXY_KEY_PREFIX.length(), idEndPos));                } catch (Exception e) {                                                                                unknownKeys.add(key);                    continue;                }                                Map<String, byte[]> appContext = loadAMRMProxyAppContextMap(iter, key.substring(0, idEndPos + 1));                result.getAppContexts().put(attemptId, appContext);                            }        }    } catch (DBException e) {        throw new IOException(e);    } finally {        if (iter != null) {            iter.close();        }    }        try {        for (String key : unknownKeys) {            db.delete(bytes(key));        }    } catch (DBException e) {        throw new IOException(e);    }    return result;}
1
public void close()
{    synchronized (this) {        if (closed.get()) {                        return;        }        int remainder = refCount.decrementAndGet();        if (remainder != 0) {                                                return;        }                        closed.set(true);    }        for (AWSCredentialsProvider p : providers) {        if (p instanceof Closeable) {            IOUtils.closeStream((Closeable) p);        } else if (p instanceof AutoCloseable) {            S3AUtils.closeAutocloseables(LOG, (AutoCloseable) p);        }    }}
1
protected void doReturnLocalStats(ResponseBuilder rb, SolrIndexSearcher searcher)
{    }
1
private void logAll()
{    StringBuilder b = new StringBuilder();    b.append(getClass().getSimpleName());    b.append(" values: ");    b.append(Utils.NL);    for (Map.Entry<String, Object> entry : new TreeMap<>(this.values).entrySet()) {        b.append('\t');        b.append(entry.getKey());        b.append(" = ");        b.append(entry.getValue());        b.append(Utils.NL);    }    }
1
private static void update(String name, SQLResponse sqlResponse)
{    try {        incrQueryCount(name, sqlResponse);        incrCacheHitCount(name, sqlResponse);        if (!sqlResponse.getIsException()) {            metrics.updateTimer(MetricsNameBuilder.buildMetricName(name, MetricsConstant.QUERY_DURATION), sqlResponse.getDuration(), TimeUnit.MILLISECONDS);            metrics.updateHistogram(MetricsNameBuilder.buildMetricName(name, MetricsConstant.QUERY_RESULT_ROWCOUNT), sqlResponse.getResults().size());            metrics.updateHistogram(MetricsNameBuilder.buildMetricName(name, MetricsConstant.QUERY_SCAN_ROWCOUNT), sqlResponse.getTotalScanCount());        }    } catch (Exception e) {            }}
1
private void terminateApplicationContext(final ConfigurableApplicationContext applicationContext, final CamelContext camelContext, int seconds)
{    ScheduledExecutorService executorService = camelContext.getExecutorServiceManager().newSingleThreadScheduledExecutor(this, "CamelSpringBootTerminateTask");    Runnable task = () -> {                        new Thread(applicationContext::close).start();    };    executorService.schedule(task, seconds, TimeUnit.SECONDS);}
1
protected void createMBeans(String name, UserDatabase database) throws Exception
{        if (log.isDebugEnabled()) {                    }    try {        MBeanUtils.createMBean(database);    } catch (Exception e) {        throw new IllegalArgumentException(sm.getString("globalResources.createError.userDatabase", name), e);    }        Iterator<Role> roles = database.getRoles();    while (roles.hasNext()) {        Role role = roles.next();        if (log.isDebugEnabled()) {                    }        try {            MBeanUtils.createMBean(role);        } catch (Exception e) {            throw new IllegalArgumentException(sm.getString("globalResources.createError.userDatabase.role", role), e);        }    }        Iterator<Group> groups = database.getGroups();    while (groups.hasNext()) {        Group group = groups.next();        if (log.isDebugEnabled()) {                    }        try {            MBeanUtils.createMBean(group);        } catch (Exception e) {            throw new IllegalArgumentException(sm.getString("globalResources.createError.userDatabase.group", group), e);        }    }        Iterator<User> users = database.getUsers();    while (users.hasNext()) {        User user = users.next();        if (log.isDebugEnabled()) {                    }        try {            MBeanUtils.createMBean(user);        } catch (Exception e) {            throw new IllegalArgumentException(sm.getString("globalResources.createError.userDatabase.user", user), e);        }    }}
1
public void testLoadMetaRegion() throws Exception
{    HRegionServer rsWithMeta = TEST_UTIL.getMiniHBaseCluster().getRegionServerThreads().stream().map(t -> t.getRegionServer()).filter(rs -> rs.getRegions(TableName.META_TABLE_NAME).size() > 0).findFirst().get();    int onlineRegions = rsWithMeta.getNumberOfOnlineRegions();    String rsName = rsWithMeta.getServerName().getAddress().toString();    try (RegionMover rm = new RegionMoverBuilder(rsName, TEST_UTIL.getConfiguration()).ack(true).build()) {                rm.unload();        assertEquals(0, rsWithMeta.getNumberOfOnlineRegions());                rm.load();        assertEquals(onlineRegions, rsWithMeta.getNumberOfOnlineRegions());    }}
1
public Callable<Long> call() throws Exception
{    init(idToTask, idToTaskBase);    return new Callable<Long>() {        int bpIdleCount = 0;        int consumeIdleCounter = 0;        private final ExitCondition tillNoPendingEmits = () -> pendingEmits.isEmpty();        @Override        public Long call() throws Exception {            boolean pendingEmitsIsEmpty = tryFlushPendingEmits();            if (pendingEmitsIsEmpty) {                if (bpIdleCount != 0) {                                    }                bpIdleCount = 0;                int consumeCount = receiveQueue.consume(BoltExecutor.this, tillNoPendingEmits);                if (consumeCount == 0) {                    if (consumeIdleCounter == 0) {                                            }                    consumeIdleCounter = consumeWaitStrategy.idle(consumeIdleCounter);                    if (Thread.interrupted()) {                        throw new InterruptedException();                    }                } else {                    if (consumeIdleCounter != 0) {                                            }                    consumeIdleCounter = 0;                }            } else {                if (bpIdleCount == 0) {                                                        }                bpIdleCount = backPressureWaitStrategy.idle(bpIdleCount);            }            return 0L;        }                private boolean tryFlushPendingEmits() {            for (AddressedTuple t = pendingEmits.peek(); t != null; t = pendingEmits.peek()) {                if (executorTransfer.tryTransfer(t, null)) {                    pendingEmits.poll();                } else {                                        return false;                }            }            return true;        }    };}
1
 static boolean removeUnfinishedLeftovers(Map.Entry<String, List<File>> entry)
{    try (LogFile txn = LogFile.make(entry.getKey(), entry.getValue())) {        if (txn.verify()) {            Throwable failure = txn.removeUnfinishedLeftovers(null);            if (failure != null) {                                return false;            }            return true;        } else {                        return false;        }    }}
1
public void run()
{    while (true) {        final SocketChannel socketChannel;        try {            socketChannel = serverSocketChannel.accept();                    } catch (final IOException e) {            if (!stopped) {                                if (logger.isDebugEnabled()) {                                    }            }            return;        }        final Runnable processInputRunnable = new Runnable() {            @Override            public void run() {                final InputStream rawInputStream;                final OutputStream rawOutputStream;                final String peer = socketChannel.socket().getInetAddress().getHostName();                try {                    if (sslContext == null) {                        rawInputStream = new SocketChannelInputStream(socketChannel);                        rawOutputStream = new SocketChannelOutputStream(socketChannel);                    } else {                        final SSLSocketChannel sslSocketChannel = new SSLSocketChannel(sslContext, socketChannel, false);                        sslSocketChannel.connect();                        rawInputStream = new SSLSocketChannelInputStream(sslSocketChannel);                        rawOutputStream = new SSLSocketChannelOutputStream(sslSocketChannel);                    }                } catch (IOException e) {                                        if (logger.isDebugEnabled()) {                                            }                    try {                        socketChannel.close();                    } catch (IOException swallow) {                    }                    return;                }                try (final InputStream in = new BufferedInputStream(rawInputStream);                    final OutputStream out = new BufferedOutputStream(rawOutputStream)) {                    final VersionNegotiator versionNegotiator = getVersionNegotiator();                    ProtocolHandshake.receiveHandshake(in, out, versionNegotiator);                    boolean continueComms = true;                    while (continueComms) {                        continueComms = listen(in, out, versionNegotiator.getVersion());                    }                                                        } catch (final SocketTimeoutException e) {                                    } catch (final IOException | HandshakeException e) {                    if (!stopped) {                                                if (logger.isDebugEnabled()) {                                                    }                    }                } finally {                    processInputThreads.remove(Thread.currentThread());                }            }        };        final Thread processInputThread = new Thread(processInputRunnable);        processInputThread.setName("Distributed Cache Server Communications Thread: " + identifier);        processInputThread.setDaemon(true);        processInputThread.start();        processInputThreads.add(processInputThread);    }}
1
public static void killProcess(String pid)
{    synchronized (lock) {                Shutdownable shutdownHandle = processMap.get(pid);        if (shutdownHandle != null) {            shutdownHandle.shutdown();        }        processMap.remove(pid);            }}
1
private boolean updateWatchedCollection(String coll, DocCollection newState)
{    if (newState == null) {                watchedCollectionStates.remove(coll);        return true;    }    boolean updated = false;        while (true) {        if (!collectionWatches.containsKey(coll)) {            break;        }        DocCollection oldState = watchedCollectionStates.get(coll);        if (oldState == null) {            if (watchedCollectionStates.putIfAbsent(coll, newState) == null) {                                updated = true;                break;            }        } else {            if (oldState.getZNodeVersion() >= newState.getZNodeVersion()) {                                                updated = true;                break;            }            if (watchedCollectionStates.replace(coll, oldState, newState)) {                                updated = true;                break;            }        }    }        if (!collectionWatches.containsKey(coll)) {        watchedCollectionStates.remove(coll);            }    return updated;}
1
 static DescriptiveStatistics benchmarkGetPartitions(@NotNull MicroBenchmark bench, @NotNull BenchData data, int howMany)
{    final HMSClient client = data.getClient();    String dbName = data.dbName;    String tableName = data.tableName;    createPartitionedTable(client, dbName, tableName);    try {        addManyPartitions(client, dbName, tableName, null, Collections.singletonList("d"), howMany);                        return bench.measure(() -> throwingSupplierWrapper(() -> client.getPartitions(dbName, tableName)));    } catch (TException e) {        e.printStackTrace();        return new DescriptiveStatistics();    } finally {        throwingSupplierWrapper(() -> client.dropTable(dbName, tableName));    }}
1
public void run()
{        if (!running) {        return;    }        final Handover handover = this.handover;        try {        this.consumer = getConsumer(kafkaProperties);    } catch (Throwable t) {        handover.reportError(t);        return;    }        try {                if (useMetrics) {                        Map<MetricName, ? extends Metric> metrics = consumer.metrics();            if (metrics == null) {                                            } else {                                for (Map.Entry<MetricName, ? extends Metric> metric : metrics.entrySet()) {                    consumerMetricGroup.gauge(metric.getKey().name(), new KafkaMetricWrapper(metric.getValue()));                                        subtaskMetricGroup.gauge(metric.getKey().name(), new KafkaMetricWrapper(metric.getValue()));                }            }        }                if (!running) {            return;        }                        ConsumerRecords<byte[], byte[]> records = null;                                List<KafkaTopicPartitionState<TopicPartition>> newPartitions;                while (running) {                        if (!commitInProgress) {                                final Tuple2<Map<TopicPartition, OffsetAndMetadata>, KafkaCommitCallback> commitOffsetsAndCallback = nextOffsetsToCommit.getAndSet(null);                if (commitOffsetsAndCallback != null) {                                                                                commitInProgress = true;                    consumer.commitAsync(commitOffsetsAndCallback.f0, new CommitCallback(commitOffsetsAndCallback.f1));                }            }            try {                if (hasAssignedPartitions) {                    newPartitions = unassignedPartitionsQueue.pollBatch();                } else {                                                                                                    newPartitions = unassignedPartitionsQueue.getBatchBlocking();                }                if (newPartitions != null) {                    reassignPartitions(newPartitions);                }            } catch (AbortedReassignmentException e) {                continue;            }            if (!hasAssignedPartitions) {                                continue;            }                        if (records == null) {                try {                    records = consumer.poll(pollTimeout);                } catch (WakeupException we) {                    continue;                }            }            try {                handover.produce(records);                records = null;            } catch (Handover.WakeupException e) {                        }        }        } catch (Throwable t) {                                handover.reportError(t);    } finally {                handover.close();                try {            consumer.close();        } catch (Throwable t) {                    }    }}
1
public Table getTableResource(String projectId, String datasetId, String tableId) throws IOException, InterruptedException
{    Sleeper sleeper = Sleeper.DEFAULT;    BackOff backoff = BackOffAdapter.toGcpBackOff(BACKOFF_FACTORY.backoff());    IOException lastException = null;    do {        if (lastException != null) {                    }        try {            Table response = this.bqClient.tables().get(projectId, datasetId, tableId).execute();            if (response != null) {                return response;            } else {                lastException = new IOException("Expected valid response from tables.get, but received null.");            }        } catch (IOException e) {                        lastException = e;        }    } while (BackOffUtils.next(sleeper, backoff));    throw new RuntimeException(String.format("Unable to get BigQuery response after retrying %d times for tables.get (%s)", MAX_QUERY_RETRIES, tableId), lastException);}
1
public void setCq(String cqName, boolean isDurable) throws Exception
{    final boolean isDebugEnabled = logger.isDebugEnabled();    if (requestMessage.isSecureMode()) {        if (isDebugEnabled) {                    }        try {            byte[] secureBytes = requestMessage.getSecureBytes();            secureBytes = handshake.getEncryptor().decryptBytes(secureBytes);            AuthIds aIds = new AuthIds(secureBytes);            long uniqueId = aIds.getUniqueId();            CacheClientProxy proxy = getAcceptor().getCacheClientNotifier().getClientProxy(proxyId);            if (proxy != null) {                proxy.setCQVsUserAuth(cqName, uniqueId, isDurable);            }        } catch (Exception ex) {            if (isDebugEnabled) {                            }            throw ex;        }    } else {        if (isDebugEnabled) {                    }    }}
1
public void run()
{    while (shouldRun) {        scan(streamTimeout);                try {            long workedTime = Time.monotonicNow() - lastWakeupTime;            if (workedTime < rotation) {                if (LOG.isTraceEnabled()) {                    LOG.trace("StreamMonitor can still have a sleep:" + ((rotation - workedTime) / 1000));                }                Thread.sleep(rotation - workedTime);            }            lastWakeupTime = Time.monotonicNow();        } catch (InterruptedException e) {                        return;        }    }}
1
public SampleResult sample(Entry e)
{    SendMailCommand sendMailCmd;    Message message;    SampleResult result = createSampleResult();    try {        sendMailCmd = createSendMailCommandFromProperties();        message = sendMailCmd.prepareMessage();        result.setBytes(calculateMessageSize(message));    } catch (Exception ex) {                result.setResponseCode("500");        result.setResponseMessage(ex.toString());        return result;    }        result.setDataType(SampleResult.TEXT);    try {        result.setRequestHeaders(getRequestHeaders(message));        result.setSamplerData(getSamplerData(message));    } catch (MessagingException | IOException ex) {        result.setSamplerData("Error occurred trying to save request info: " + ex);            }        result.sampleStart();    boolean isSuccessful = executeMessage(result, sendMailCmd, message);    result.sampleEnd();    try {        result.setResponseData(processSampler(message));    } catch (IOException | MessagingException ex) {            }    result.setSuccessful(isSuccessful);    return result;}
1
protected void doDeleteReplicationController(Exchange exchange, String operation) throws Exception
{    String rcName = exchange.getIn().getHeader(KubernetesConstants.KUBERNETES_REPLICATION_CONTROLLER_NAME, String.class);    String namespaceName = exchange.getIn().getHeader(KubernetesConstants.KUBERNETES_NAMESPACE_NAME, String.class);    if (ObjectHelper.isEmpty(rcName)) {                throw new IllegalArgumentException("Delete a specific replication controller require specify a replication controller name");    }    if (ObjectHelper.isEmpty(namespaceName)) {                throw new IllegalArgumentException("Delete a specific replication controller require specify a namespace name");    }    boolean rcDeleted = getEndpoint().getKubernetesClient().replicationControllers().inNamespace(namespaceName).withName(rcName).delete();    MessageHelper.copyHeaders(exchange.getIn(), exchange.getOut(), true);    exchange.getOut().setBody(rcDeleted);}
1
public void testAccessControlClientMultiGrantRevoke() throws Exception
{    User testGrantRevoke = User.createUserForTesting(conf, "testGrantRevoke", new String[0]);    AccessTestAction getAction = new AccessTestAction() {        @Override        public Object run() throws Exception {            try (Connection conn = ConnectionFactory.createConnection(conf);                Table t = conn.getTable(TEST_TABLE)) {                return t.get(new Get(TEST_ROW));            }        }    };    AccessTestAction putAction = new AccessTestAction() {        @Override        public Object run() throws Exception {            Put p = new Put(TEST_ROW);            p.addColumn(TEST_FAMILY, TEST_QUALIFIER, Bytes.toBytes(1));            try (Connection conn = ConnectionFactory.createConnection(conf);                Table t = conn.getTable(TEST_TABLE)) {                t.put(p);                return null;            }        }    };    verifyDenied(getAction, testGrantRevoke);    verifyDenied(putAction, testGrantRevoke);        String userName = testGrantRevoke.getShortName();    try {        grantGlobalUsingAccessControlClient(TEST_UTIL, systemUserConnection, userName, Permission.Action.READ);    } catch (Throwable e) {            }    verifyAllowed(getAction, testGrantRevoke);    verifyDenied(putAction, testGrantRevoke);        try {        grantGlobalUsingAccessControlClient(TEST_UTIL, systemUserConnection, userName, Permission.Action.WRITE);    } catch (Throwable e) {            }    verifyAllowed(getAction, testGrantRevoke);    verifyAllowed(putAction, testGrantRevoke);        try {        revokeGlobalUsingAccessControlClient(TEST_UTIL, systemUserConnection, userName, Permission.Action.READ, Permission.Action.WRITE);    } catch (Throwable e) {            }    verifyDenied(getAction, testGrantRevoke);    verifyDenied(putAction, testGrantRevoke);        try {        grantOnTableUsingAccessControlClient(TEST_UTIL, systemUserConnection, userName, TEST_TABLE, null, null, Permission.Action.READ);    } catch (Throwable e) {            }    verifyAllowed(getAction, testGrantRevoke);    verifyDenied(putAction, testGrantRevoke);        try {        grantOnTableUsingAccessControlClient(TEST_UTIL, systemUserConnection, userName, TEST_TABLE, null, null, Action.WRITE);    } catch (Throwable e) {            }    verifyAllowed(getAction, testGrantRevoke);    verifyAllowed(putAction, testGrantRevoke);        try {        revokeFromTableUsingAccessControlClient(TEST_UTIL, systemUserConnection, userName, TEST_TABLE, null, null, Permission.Action.READ, Permission.Action.WRITE);    } catch (Throwable e) {            }    verifyDenied(getAction, testGrantRevoke);    verifyDenied(putAction, testGrantRevoke);        String namespace = TEST_TABLE.getNamespaceAsString();    try {        grantOnNamespaceUsingAccessControlClient(TEST_UTIL, systemUserConnection, userName, namespace, Permission.Action.READ);    } catch (Throwable e) {            }    verifyAllowed(getAction, testGrantRevoke);    verifyDenied(putAction, testGrantRevoke);        try {        grantOnNamespaceUsingAccessControlClient(TEST_UTIL, systemUserConnection, userName, namespace, Permission.Action.WRITE);    } catch (Throwable e) {            }    verifyAllowed(getAction, testGrantRevoke);    verifyAllowed(putAction, testGrantRevoke);        try {        revokeFromNamespaceUsingAccessControlClient(TEST_UTIL, systemUserConnection, userName, TEST_TABLE.getNamespaceAsString(), Permission.Action.READ, Permission.Action.WRITE);    } catch (Throwable e) {            }    verifyDenied(getAction, testGrantRevoke);    verifyDenied(putAction, testGrantRevoke);}
1
public void clear()
{        writeLock.lock();    try {        if (this.locationCache != null) {            this.locationCache.invalidateAll();        }        this.tree.clear();    } finally {        writeLock.unlock();    }}
1
public void processElement(ProcessContext c, BoundedWindow window) throws Exception
{    getDynamicDestinations().setSideInputAccessorFromProcessContext(c);    PaneInfo paneInfo = c.pane();                        DestinationT destination = getDynamicDestinations().getDestination(c.element());    WriterKey<DestinationT> key = new WriterKey<>(window, c.pane(), destination);    Writer<DestinationT, OutputT> writer = writers.get(key);    if (writer == null) {        if (getMaxNumWritersPerBundle() < 0 || writers.size() <= getMaxNumWritersPerBundle()) {            String uuid = UUID.randomUUID().toString();                        writer = writeOperation.createWriter();            writer.setDestination(destination);            writer.open(uuid);            writers.put(key, writer);                    } else {            if (spilledShardNum == UNKNOWN_SHARDNUM) {                                spilledShardNum = ThreadLocalRandom.current().nextInt(SPILLED_RECORD_SHARDING_FACTOR);            } else {                spilledShardNum = (spilledShardNum + 1) % SPILLED_RECORD_SHARDING_FACTOR;            }            c.output(unwrittenRecordsTag, KV.of(ShardedKey.of(hashDestination(destination, destinationCoder), spilledShardNum), c.element()));            return;        }    }    writeOrClose(writer, getDynamicDestinations().formatRecord(c.element()));}
1
public void createTopic(String topic, int partitions, int replication, Map<String, String> topicConfig)
{    if (replication > brokers.length) {        throw new InvalidReplicationFactorException("Insufficient brokers (" + brokers.length + ") for desired replication (" + replication + ")");    }        final NewTopic newTopic = new NewTopic(topic, partitions, (short) replication);    newTopic.configs(topicConfig);    try (final Admin adminClient = createAdminClient()) {        adminClient.createTopics(Collections.singletonList(newTopic)).all().get();    } catch (final InterruptedException | ExecutionException e) {        throw new RuntimeException(e);    }}
1
public Void call() throws Exception
{    try {        AccessControlClient.grant(connection, user, actions);    } catch (Throwable t) {            }    return null;}
1
public void close()
{    if (threadProducer != null) {        try {            threadProducer.close();        } catch (final Throwable e) {                    }    }}
1
public void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException
{    /*            Available endpoints are:             - /v1/queries/active             - /v1/queries/historical             - /v1/sessions        */    String pathInfo = request.getPathInfo();    if (pathInfo == null || "/".equals(pathInfo)) {        sendError(response, HttpServletResponse.SC_BAD_REQUEST, "Path to the endpoint is missing");        return;    }    String[] splits = pathInfo.split("/");    if (splits.length < 3) {                sendError(response, HttpServletResponse.SC_BAD_REQUEST, "Expecting at least 2 parts in the path");        return;    }    ServletContext ctx = getServletContext();    SessionManager sessionManager = (SessionManager) ctx.getAttribute("hive.sm");    OperationManager operationManager = sessionManager.getOperationManager();    String apiVersion = splits[1];    if (apiVersion.equals(API_V1)) {        String reqType = splits[2];        if (reqType.equals(REQ_QUERIES)) {            if (splits.length != 4) {                sendError(response, HttpServletResponse.SC_NOT_FOUND, "Expecting 3 parts in the path: /v1/queries/active or /v1/queries/historical");                return;            }            String queriesType = splits[3];            if (queriesType.equals(REQ_ACTIVE)) {                Collection<QueryInfo> operations = operationManager.getLiveQueryInfos();                                sendAsJson(response, operations);            } else if (queriesType.equals(REQ_HISTORICAL)) {                Collection<QueryInfo> operations = operationManager.getHistoricalQueryInfos();                                sendAsJson(response, operations);            } else {                sendError(response, HttpServletResponse.SC_BAD_REQUEST, "Unknown query type: " + queriesType);                return;            }        } else if (reqType.equals(REQ_SESSIONS)) {            Collection<HiveSession> hiveSessions = sessionManager.getSessions();                        sendAsJson(response, hiveSessions);        } else {                        sendError(response, HttpServletResponse.SC_NOT_FOUND, "Unknown request type: " + reqType);            return;        }    } else {                sendError(response, HttpServletResponse.SC_BAD_REQUEST, "This server only handles API v1");        return;    }}
1
public void refreshAnomalyDetectorJobMemory(String jobId, ActionListener<Long> listener)
{    if (isMaster == false) {        listener.onResponse(null);        return;    }        if (stopPhaser.register() != 0) {                stopPhaser.arriveAndDeregister();        listener.onFailure(new EsRejectedExecutionException("Couldn't run ML memory update - node is shutting down"));        return;    }    ActionListener<Long> phaserListener = ActionListener.wrap(r -> {        stopPhaser.arriveAndDeregister();        listener.onResponse(r);    }, e -> {        stopPhaser.arriveAndDeregister();        listener.onFailure(e);    });    try {        jobResultsProvider.getEstablishedMemoryUsage(jobId, null, null, establishedModelMemoryBytes -> {            if (establishedModelMemoryBytes <= 0L) {                setAnomalyDetectorJobMemoryToLimit(jobId, phaserListener);            } else {                Long memoryRequirementBytes = establishedModelMemoryBytes + Job.PROCESS_MEMORY_OVERHEAD.getBytes();                memoryRequirementByAnomalyDetectorJob.put(jobId, memoryRequirementBytes);                phaserListener.onResponse(memoryRequirementBytes);            }        }, e -> {                        setAnomalyDetectorJobMemoryToLimit(jobId, phaserListener);        });    } catch (Exception e) {                setAnomalyDetectorJobMemoryToLimit(jobId, phaserListener);    }}
1
private int putUpJettyServer() throws IOException
{    if (!conf.getBoolean("hbase.master.infoserver.redirect", true)) {        return -1;    }    final int infoPort = conf.getInt("hbase.master.info.port.orig", HConstants.DEFAULT_MASTER_INFOPORT);        if (infoPort < 0 || infoServer == null) {        return -1;    }    if (infoPort == infoServer.getPort()) {        return infoPort;    }    final String addr = conf.get("hbase.master.info.bindAddress", "0.0.0.0");    if (!Addressing.isLocalAddress(InetAddress.getByName(addr))) {        String msg = "Failed to start redirecting jetty server. Address " + addr + " does not belong to this host. Correct configuration parameter: " + "hbase.master.info.bindAddress";                throw new IOException(msg);    }                masterJettyServer = new Server();    final ServerConnector connector = new ServerConnector(masterJettyServer);    connector.setHost(addr);    connector.setPort(infoPort);    masterJettyServer.addConnector(connector);    masterJettyServer.setStopAtShutdown(true);    final String redirectHostname = StringUtils.isBlank(useThisHostnameInstead) ? null : useThisHostnameInstead;    final RedirectServlet redirect = new RedirectServlet(infoServer, redirectHostname);    final WebAppContext context = new WebAppContext(null, "/", null, null, null, null, WebAppContext.NO_SESSIONS);    context.addServlet(new ServletHolder(redirect), "/*");    context.setServer(masterJettyServer);    try {        masterJettyServer.start();    } catch (Exception e) {        throw new IOException("Failed to start redirecting jetty server", e);    }    return connector.getLocalPort();}
1
protected void serviceStop()
{    if (renewalTimer != null) {        renewalTimer.cancel();    }    appTokens.clear();    allTokens.clear();    serviceStateLock.writeLock().lock();    try {        isServiceStarted = false;        this.renewerService.shutdown();    } finally {        serviceStateLock.writeLock().unlock();    }    dtCancelThread.interrupt();    try {        dtCancelThread.join(1000);    } catch (InterruptedException e) {        e.printStackTrace();    }    if (tokenKeepAliveEnabled && delayedRemovalThread != null) {        delayedRemovalThread.interrupt();        try {            delayedRemovalThread.join(1000);        } catch (InterruptedException e) {                    }    }}
1
protected void doGetConfigMap(Exchange exchange, String operation) throws Exception
{    ConfigMap configMap = null;    String cfMapName = exchange.getIn().getHeader(KubernetesConstants.KUBERNETES_CONFIGMAP_NAME, String.class);    String namespaceName = exchange.getIn().getHeader(KubernetesConstants.KUBERNETES_NAMESPACE_NAME, String.class);    if (ObjectHelper.isEmpty(cfMapName)) {                throw new IllegalArgumentException("Get a specific ConfigMap require specify a ConfigMap name");    }    if (namespaceName != null) {        configMap = getEndpoint().getKubernetesClient().configMaps().inNamespace(namespaceName).withName(cfMapName).get();    } else {        configMap = getEndpoint().getKubernetesClient().configMaps().withName(cfMapName).get();    }    MessageHelper.copyHeaders(exchange.getIn(), exchange.getOut(), true);    exchange.getOut().setBody(configMap);}
1
protected ExecutorService createDefaultExecutor()
{    ThreadPoolExecutor rc = new ThreadPoolExecutor(getDefaultCorePoolSize(), getMaxThreadPoolSize(), getDefaultKeepAliveTime(), TimeUnit.SECONDS, new SynchronousQueue<Runnable>(), new ThreadFactory() {        @Override        public Thread newThread(Runnable runnable) {            String threadName = name + "-" + id.incrementAndGet();            Thread thread = new Thread(runnable, threadName);            thread.setDaemon(daemon);            thread.setPriority(priority);            if (threadClassLoader != null) {                thread.setContextClassLoader(threadClassLoader);            }            thread.setUncaughtExceptionHandler(new Thread.UncaughtExceptionHandler() {                @Override                public void uncaughtException(final Thread t, final Throwable e) {                                    }            });            LOG.trace("Created thread[{}]: {}", threadName, thread);            return thread;        }    });    if (rejectedTaskHandler != null) {        rc.setRejectedExecutionHandler(rejectedTaskHandler);    } else {        rc.setRejectedExecutionHandler(new ThreadPoolExecutor.CallerRunsPolicy());    }    return rc;}
1
private void removeTableFromBackupImage(BackupInfo info, TableName tn, BackupSystemTable sysTable) throws IOException
{    List<TableName> tables = info.getTableNames();        if (tables.contains(tn)) {        tables.remove(tn);        if (tables.isEmpty()) {                        sysTable.deleteBackupInfo(info.getBackupId());                        BackupUtils.cleanupBackupData(info, conn.getConfiguration());        } else {            info.setTables(tables);            sysTable.updateBackupInfo(info);                        cleanupBackupDir(info, tn, conn.getConfiguration());        }    }}
1
public void circuitBreak(String fieldName, long bytesNeeded)
{    this.trippedCount.incrementAndGet();    final String message = "[" + this.name + "] Data too large, data for [" + fieldName + "]" + " would be [" + bytesNeeded + "/" + new ByteSizeValue(bytesNeeded) + "]" + ", which is larger than the limit of [" + memoryBytesLimit + "/" + new ByteSizeValue(memoryBytesLimit) + "]";        throw new CircuitBreakingException(message, bytesNeeded, memoryBytesLimit, durability);}
1
public void updateAllocationConfiguration(AllocationConfiguration queueConf)
{        synchronized (queues) {                if (!rootQueue.verifyAndSetPolicyFromConf(queueConf)) {                    }        ensureQueueExistsAndIsCompatibleAndIsStatic(queueConf, FSQueueType.LEAF);                                ensureQueueExistsAndIsCompatibleAndIsStatic(queueConf, FSQueueType.PARENT);    }        rootQueue.reinit(true);        rootQueue.recomputeSteadyShares();}
1
public void onComplete(GetPartitionsByNamesResult o)
{    get_partitions_by_names_req_result result = new get_partitions_by_names_req_result();    result.success = o;    try {        fcall.sendResponse(fb, result, org.apache.thrift.protocol.TMessageType.REPLY, seqid);        return;    } catch (Exception e) {            }    fb.close();}
1
public HistoryServerState loadState() throws IOException
{        HistoryServerState state = new HistoryServerState();    loadTokenState(state);    return state;}
1
public void stop()
{    if (jmdns != null) {        for (Iterator<ServiceInfo> iter = serviceInfos.iterator(); iter.hasNext(); ) {            ServiceInfo si = iter.next();            jmdns.unregisterService(si);        }                final JmDNS closeTarget = jmdns;        Thread thread = new Thread() {            @Override            public void run() {                try {                    if (JmDNSFactory.onClose(getLocalAddress())) {                        closeTarget.close();                    }                    ;                } catch (IOException e) {                                    }            }        };        thread.setDaemon(true);        thread.start();        jmdns = null;    }}
1
public void start()
{        if (started.compareAndSet(false, true)) {        connected.set(client.getZookeeperClient().isConnected());        if (isConnected()) {            handleStateChange(ConnectionState.CONNECTED);        }        client.getConnectionStateListenable().addListener(connectionStateListener);        executorService.execute(new Runnable() {            @Override            public void run() {                mainLoop();            }        });    }}
1
protected boolean chunkEntries(DistributedRegion rgn, int chunkSizeInBytes, boolean includeValues, RegionVersionVector versionVector, HashSet unfinishedKeys, InitialImageFlowControl flowControl, ObjectIntProcedure proc) throws IOException
{    boolean keepGoing = true;    boolean sentLastChunk = false;    int MAX_ENTRIES_PER_CHUNK = chunkSizeInBytes / 100;    if (MAX_ENTRIES_PER_CHUNK < 1000) {        MAX_ENTRIES_PER_CHUNK = 1000;    }    ByteArrayDataInput in = null;    ClusterDistributionManager dm = (ClusterDistributionManager) rgn.getDistributionManager();    List chunkEntries = null;    chunkEntries = new InitialImageVersionedEntryList(rgn.getConcurrencyChecksEnabled(), MAX_ENTRIES_PER_CHUNK);    DiskRegion dr = rgn.getDiskRegion();    if (dr != null) {        dr.setClearCountReference();        in = new ByteArrayDataInput();    }    VersionSource myId = rgn.getVersionMember();    Set<VersionSource> foundIds = new HashSet<VersionSource>();    if (internalDuringPackingImage != null && this.regionPath.endsWith(internalDuringPackingImage.getRegionName())) {        internalDuringPackingImage.run();    }    try {        Iterator it = null;        if (versionVector != null) {                        it = rgn.entries.regionEntries().iterator();        } else {            it = rgn.getBestIterator(includeValues);        }        do {            flowControl.acquirePermit();            int currentChunkSize = 0;            while (chunkEntries.size() < MAX_ENTRIES_PER_CHUNK && currentChunkSize < chunkSizeInBytes && it.hasNext()) {                RegionEntry mapEntry = (RegionEntry) it.next();                Object key = mapEntry.getKey();                if (rgn.checkEntryNotValid(mapEntry)) {                                        continue;                }                if (logger.isDebugEnabled()) {                                        Object v = mapEntry.getValueInVM(rgn);                    if (v instanceof Conflatable) {                        if (((Conflatable) v).getEventId() == null) {                                                    }                    }                }                InitialImageOperation.Entry entry = null;                if (includeValues) {                    boolean fillRes = false;                    try {                                                VersionStamp<?> stamp = mapEntry.getVersionStamp();                        if (stamp != null) {                            synchronized (mapEntry) {                                                                                                VersionSource<?> id = stamp.getMemberID();                                if (id == null) {                                    id = myId;                                }                                foundIds.add(id);                                                                if ((unfinishedKeys == null || !unfinishedKeys.contains(key)) && versionVector != null) {                                    if (versionVector.contains(id, stamp.getRegionVersion())) {                                        continue;                                    }                                }                                entry = new InitialImageOperation.Entry();                                entry.key = key;                                entry.setVersionTag(stamp.asVersionTag());                                fillRes = mapEntry.fillInValue(rgn, entry, in, rgn.getDistributionManager(), sender.getVersionObject());                                if (versionVector != null) {                                    if (logger.isTraceEnabled(LogMarker.INITIAL_IMAGE_VERBOSE)) {                                        logger.trace(LogMarker.INITIAL_IMAGE_VERBOSE, "chunkEntries:entry={},stamp={}", entry, stamp);                                    }                                }                            }                        } else {                            entry = new InitialImageOperation.Entry();                            entry.key = key;                            fillRes = mapEntry.fillInValue(rgn, entry, in, rgn.getDistributionManager(), sender.getVersionObject());                        }                    } catch (DiskAccessException dae) {                        rgn.handleDiskAccessException(dae);                        throw dae;                    }                    if (!fillRes) {                                                continue;                    }                } else {                    entry = new InitialImageOperation.Entry();                    entry.key = key;                    entry.setLocalInvalid();                    entry.setLastModified(rgn.getDistributionManager(), mapEntry.getLastModified());                }                chunkEntries.add(entry);                currentChunkSize += entry.calcSerializedSize();            }                        int lastMsg = it.hasNext() ? 0 : 1;            keepGoing = proc.executeWith(chunkEntries, lastMsg);            sentLastChunk = lastMsg == 1 && keepGoing;            chunkEntries.clear();                } while (keepGoing && it.hasNext());        if (foundIds.size() > 0) {            RegionVersionVector vv = rgn.getVersionVector();            if (vv != null) {                vv.removeOldMembers(foundIds);            }        }                return sentLastChunk;    } finally {        if (dr != null) {            dr.removeClearCountReference();        }    }}
1
protected void doSync() throws IOException
{        this.writer.hsync();}
1
private String doPreemptive(CandidateCollection targetCollectionDesc, SolrCore core, CoreContainer coreContainer)
{    if (!this.preemptiveCreateOnceAlready) {        preemptiveAsync(() -> {            try {                ensureCollection(targetCollectionDesc.creationCollection, coreContainer);            } catch (Exception e) {                            }        }, core);    }    return targetCollectionDesc.destinationCollection;}
1
private PatternRule generateRule(String patternRuleDefinition) throws FlumeException
{    patternRuleDefinition = patternRuleDefinition.trim();        int firstColonIndex = patternRuleDefinition.indexOf(":");    if (firstColonIndex == -1) {        throw new FlumeException("Invalid ipFilter patternRule '" + patternRuleDefinition + "' should look like <'allow'  or 'deny'>:<'ip' or " + "'name'>:<pattern>");    } else {        String ruleAccessFlag = patternRuleDefinition.substring(0, firstColonIndex);        int secondColonIndex = patternRuleDefinition.indexOf(":", firstColonIndex + 1);        if ((!ruleAccessFlag.equals("allow") && !ruleAccessFlag.equals("deny")) || secondColonIndex == -1) {            throw new FlumeException("Invalid ipFilter patternRule '" + patternRuleDefinition + "' should look like <'allow'  or 'deny'>:<'ip' or " + "'name'>:<pattern>");        }        String patternTypeFlag = patternRuleDefinition.substring(firstColonIndex + 1, secondColonIndex);        if ((!patternTypeFlag.equals("ip") && !patternTypeFlag.equals("name"))) {            throw new FlumeException("Invalid ipFilter patternRule '" + patternRuleDefinition + "' should look like <'allow'  or 'deny'>:<'ip' or " + "'name'>:<pattern>");        }        boolean isAllow = ruleAccessFlag.equals("allow");        String patternRuleString = (patternTypeFlag.equals("ip") ? "i" : "n") + ":" + patternRuleDefinition.substring(secondColonIndex + 1);                return new PatternRule(isAllow, patternRuleString);    }}
1
private StreamingConfig deserializeSchemalDesc(StreamingRequest streamingRequest)
{    StreamingConfig desc = null;    try {                desc = JsonUtil.readValue(streamingRequest.getStreamingConfig(), StreamingConfig.class);        updateRequest(streamingRequest, true, null);    } catch (JsonParseException e) {                updateRequest(streamingRequest, false, e.getMessage());    } catch (JsonMappingException e) {                updateRequest(streamingRequest, false, e.getMessage());    } catch (IOException e) {                throw new InternalErrorException("Failed to deal with the request:" + e.getMessage(), e);    }    return desc;}
1
private void readTokensFromFiles(Configuration conf, Credentials credentials) throws IOException
{        String binaryTokenFilename = conf.get(MRJobConfig.MAPREDUCE_JOB_CREDENTIALS_BINARY);    if (binaryTokenFilename != null) {        Credentials binary = Credentials.readTokenStorageFile(FileSystem.getLocal(conf).makeQualified(new Path(binaryTokenFilename)), conf);        credentials.addAll(binary);    }        String tokensFileName = conf.get("mapreduce.job.credentials.json");    if (tokensFileName != null) {                String localFileName = new Path(tokensFileName).toUri().getPath();        try {                        Map<String, String> nm = JsonSerialization.mapReader().readValue(new File(localFileName));            for (Map.Entry<String, String> ent : nm.entrySet()) {                credentials.addSecretKey(new Text(ent.getKey()), ent.getValue().getBytes(Charsets.UTF_8));            }        } catch (JsonMappingException | JsonParseException e) {                    }    }}
1
public void run()
{    if (LOG.isDebugEnabled()) {            }    TxnStore.MutexAPI.LockHandle handle = null;    try {        handle = txnHandler.getMutexAPI().acquireLock(TxnStore.MUTEX_KEY.MaterializationRebuild.name());        ValidTxnList validTxnList = TxnCommonUtils.createValidReadTxnList(txnHandler.getOpenTxns(), 0);        long removedCnt = txnHandler.cleanupMaterializationRebuildLocks(validTxnList, MetastoreConf.getTimeVar(conf, MetastoreConf.ConfVars.TXN_TIMEOUT, TimeUnit.MILLISECONDS));        if (removedCnt > 0) {            if (LOG.isDebugEnabled()) {                            }        }    } catch (Throwable t) {            } finally {        if (handle != null) {            handle.releaseLocks();        }    }}
1
public void onComplete(Void o)
{    mark_compacted_result result = new mark_compacted_result();    try {        fcall.sendResponse(fb, result, org.apache.thrift.protocol.TMessageType.REPLY, seqid);        return;    } catch (Exception e) {            }    fb.close();}
1
public void testMountController() throws IOException
{    File parentDir = new File(tmpPath);    File cgroup = new File(parentDir, controller.getName());    assertTrue("cgroup dir should be cerated", cgroup.mkdirs());            verifyZeroInteractions(privilegedOperationExecutorMock);    File emptyMtab = createEmptyCgroups();    try {        CGroupsHandler cGroupsHandler = new CGroupsHandlerImpl(createMountConfiguration(), privilegedOperationExecutorMock, emptyMtab.getAbsolutePath());        PrivilegedOperation expectedOp = new PrivilegedOperation(PrivilegedOperation.OperationType.MOUNT_CGROUPS);                        String controllerKV = controller.getName() + "=" + tmpPath + Path.SEPARATOR + controller.getName();        expectedOp.appendArgs(hierarchy, controllerKV);        cGroupsHandler.initializeCGroupController(controller);        try {            ArgumentCaptor<PrivilegedOperation> opCaptor = ArgumentCaptor.forClass(PrivilegedOperation.class);            verify(privilegedOperationExecutorMock).executePrivilegedOperation(opCaptor.capture(), eq(false));                                    Assert.assertEquals(expectedOp, opCaptor.getValue());            verifyNoMoreInteractions(privilegedOperationExecutorMock);                        cGroupsHandler.initializeCGroupController(controller);            verifyNoMoreInteractions(privilegedOperationExecutorMock);        } catch (PrivilegedOperationException e) {                        assertTrue("Unexpected PrivilegedOperationException from mock!", false);        }    } catch (ResourceHandlerException e) {                assertTrue("Unexpected ResourceHandler Exception!", false);    }}
1
public void run()
{        try {        consumerSession.commit();    } catch (JMSException ignored) {        ignored.printStackTrace();        gotException.countDown();    } finally {        commitDone.countDown();    }}
1
public void onError(Exception e)
{    byte msgType = org.apache.thrift.protocol.TMessageType.REPLY;    org.apache.thrift.TBase msg;    add_check_constraint_result result = new add_check_constraint_result();    if (e instanceof NoSuchObjectException) {        result.o1 = (NoSuchObjectException) e;        result.setO1IsSet(true);        msg = result;    } else if (e instanceof MetaException) {        result.o2 = (MetaException) e;        result.setO2IsSet(true);        msg = result;    } else {        msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;        msg = (org.apache.thrift.TBase) new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());    }    try {        fcall.sendResponse(fb, msg, msgType, seqid);        return;    } catch (Exception ex) {            }    fb.close();}
1
private void openDB(File dbPath, Options options) throws IOException
{    if (dbPath.getParentFile().mkdirs()) {            }    db = JniDBFactory.factory.open(dbPath, options);    if (LOG.isDebugEnabled()) {                                                            }}
1
public static TableReference executeQuery(BigQueryServices bqServices, BigQueryOptions options, AtomicReference<JobStatistics> dryRunJobStats, String stepUuid, String query, Boolean flattenResults, Boolean useLegacySql, QueryPriority priority, @Nullable String location, @Nullable String kmsKey) throws InterruptedException, IOException
{        String effectiveLocation = location;    DatasetService tableService = bqServices.getDatasetService(options);    if (effectiveLocation == null) {        List<TableReference> referencedTables = dryRunQueryIfNeeded(bqServices, options, dryRunJobStats, query, flattenResults, useLegacySql, location).getQuery().getReferencedTables();        if (referencedTables != null && !referencedTables.isEmpty()) {            TableReference referencedTable = referencedTables.get(0);            effectiveLocation = tableService.getTable(referencedTable).getLocation();        }    }        String jobIdToken = createJobIdToken(options.getJobName(), stepUuid);    TableReference queryResultTable = createTempTableReference(options.getProject(), jobIdToken);        tableService.createDataset(queryResultTable.getProjectId(), queryResultTable.getDatasetId(), effectiveLocation, "Temporary tables for query results of job " + options.getJobName(), TimeUnit.DAYS.toMillis(1));                    String queryJobId = jobIdToken + "-query-" + BigQueryHelpers.randomUUIDString();        JobReference jobReference = new JobReference().setProjectId(options.getProject()).setLocation(effectiveLocation).setJobId(queryJobId);    JobConfigurationQuery queryConfiguration = createBasicQueryConfig(query, flattenResults, useLegacySql).setAllowLargeResults(true).setDestinationTable(queryResultTable).setCreateDisposition("CREATE_IF_NEEDED").setWriteDisposition("WRITE_TRUNCATE").setPriority(priority.name());    if (kmsKey != null) {        queryConfiguration.setDestinationEncryptionConfiguration(new EncryptionConfiguration().setKmsKeyName(kmsKey));    }    JobService jobService = bqServices.getJobService(options);    jobService.startQueryJob(jobReference, queryConfiguration);    Job job = jobService.pollJob(jobReference, JOB_POLL_MAX_RETRIES);    if (BigQueryHelpers.parseStatus(job) != Status.SUCCEEDED) {        throw new IOException(String.format("Query job %s failed, status: %s", queryJobId, BigQueryHelpers.statusToPrettyString(job.getStatus())));    }        return queryResultTable;}
1
public void run()
{    try {        List<Writable> res = getMapRecords(new Path(segment, Content.DIR_NAME), key);        results.put("co", res);    } catch (Exception e) {            }}
1
protected void processAnnotationsJar(URL url, WebXml fragment, boolean handlesTypesOnly, Map<String, JavaClassCacheEntry> javaClassCache)
{    try (Jar jar = JarFactory.newInstance(url)) {        if (log.isDebugEnabled()) {                    }        jar.nextEntry();        String entryName = jar.getEntryName();        while (entryName != null) {            if (entryName.endsWith(".class")) {                try (InputStream is = jar.getEntryInputStream()) {                    processAnnotationsStream(is, fragment, handlesTypesOnly, javaClassCache);                } catch (IOException e) {                                    } catch (ClassFormatException e) {                                    }            }            jar.nextEntry();            entryName = jar.getEntryName();        }    } catch (IOException e) {            }}
1
protected boolean operateOnRegion(CacheEvent event, ClusterDistributionManager dm) throws EntryNotFoundException
{    EntryEventImpl ev = (EntryEventImpl) event;    DistributedRegion rgn = (DistributedRegion) ev.getRegion();    DistributionManager mgr = dm;        boolean sendReply = true;        if (!rgn.isCacheContentProxy()) {        basicOperateOnRegion(ev, rgn);    } else     {        if (logger.isDebugEnabled()) {                    }    }    return sendReply;}
1
private boolean shouldSkipSegmentId(File file, CommitLogDescriptor desc, CommitLogPosition minPosition)
{        if (minPosition.segmentId > desc.id) {        logger.trace("Skipping read of fully-flushed {}", file);        return true;    }    return false;}
1
public void setLoader(Loader loader)
{    Lock writeLock = loaderLock.writeLock();    writeLock.lock();    Loader oldLoader = null;    try {                oldLoader = this.loader;        if (oldLoader == loader)            return;        this.loader = loader;                if (getState().isAvailable() && (oldLoader != null) && (oldLoader instanceof Lifecycle)) {            try {                ((Lifecycle) oldLoader).stop();            } catch (LifecycleException e) {                            }        }                if (loader != null)            loader.setContext(this);        if (getState().isAvailable() && (loader != null) && (loader instanceof Lifecycle)) {            try {                ((Lifecycle) loader).start();            } catch (LifecycleException e) {                            }        }    } finally {        writeLock.unlock();    }        support.firePropertyChange("loader", oldLoader, loader);}
1
public void onProcessComplete(int exitValue)
{        if (exitValue == 0) {        transition(State.COMPLETED);    } else {        transition(State.TERMINATED);    }}
1
public void finalizeAndDestroyPipeline(Pipeline pipeline, boolean onTimeout) throws IOException
{        finalizePipeline(pipeline.getId());    if (onTimeout) {        long pipelineDestroyTimeoutInMillis = conf.getTimeDuration(ScmConfigKeys.OZONE_SCM_PIPELINE_DESTROY_TIMEOUT, ScmConfigKeys.OZONE_SCM_PIPELINE_DESTROY_TIMEOUT_DEFAULT, TimeUnit.MILLISECONDS);        scheduler.schedule(() -> destroyPipeline(pipeline), pipelineDestroyTimeoutInMillis, TimeUnit.MILLISECONDS, LOG, String.format("Destroy pipeline failed for pipeline:%s", pipeline));    } else {        destroyPipeline(pipeline);    }}
1
public void testMaxCorruptFiles() throws Exception
{    MiniDFSCluster cluster = null;    try {        Configuration conf = new HdfsConfiguration();                conf.setInt(DFSConfigKeys.DFS_BLOCKREPORT_INTERVAL_MSEC_KEY, 3 * 1000);        cluster = new MiniDFSCluster.Builder(conf).build();        FileSystem fs = cluster.getFileSystem();        final int maxCorruptFileBlocks = conf.getInt(DFSConfigKeys.DFS_NAMENODE_MAX_CORRUPT_FILE_BLOCKS_RETURNED_KEY, 100);                DFSTestUtil util = new DFSTestUtil.Builder().setName("testMaxCorruptFiles").setNumFiles(maxCorruptFileBlocks * 3).setMaxLevels(1).setMaxSize(512).build();        util.createFiles(fs, "/srcdat2", (short) 1);        util.waitReplication(fs, "/srcdat2", (short) 1);                final NameNode namenode = cluster.getNameNode();        Collection<FSNamesystem.CorruptFileBlockInfo> badFiles = namenode.getNamesystem().listCorruptFileBlocks("/srcdat2", null);        assertEquals("Namenode has " + badFiles.size() + " corrupt files. Expecting none.", 0, badFiles.size());        assertCorruptFilesCount(cluster, badFiles.size());                final String bpid = cluster.getNamesystem().getBlockPoolId();        for (int i = 0; i < 4; i++) {            for (int j = 0; j <= 1; j++) {                File storageDir = cluster.getInstanceStorageDir(i, j);                File data_dir = MiniDFSCluster.getFinalizedDir(storageDir, bpid);                                List<File> metadataFiles = MiniDFSCluster.getAllBlockMetadataFiles(data_dir);                if (metadataFiles == null)                    continue;                for (File metadataFile : metadataFiles) {                    File blockFile = Block.metaToBlockFile(metadataFile);                    assertTrue("Cannot remove file.", blockFile.delete());                    assertTrue("Cannot remove file.", metadataFile.delete());                }            }        }                DataNode dn = cluster.getDataNodes().get(0);        DataNodeTestUtils.runDirectoryScanner(dn);                                        cluster.restartDataNodes();        cluster.waitActive();        badFiles = namenode.getNamesystem().listCorruptFileBlocks("/srcdat2", null);        while (badFiles.size() < maxCorruptFileBlocks) {                        Thread.sleep(10000);            badFiles = namenode.getNamesystem().listCorruptFileBlocks("/srcdat2", null);        }        badFiles = namenode.getNamesystem().listCorruptFileBlocks("/srcdat2", null);                assertEquals("Namenode has " + badFiles.size() + " bad files. " + "Expecting " + maxCorruptFileBlocks + ".", maxCorruptFileBlocks, badFiles.size());        CorruptFileBlockIterator iter = (CorruptFileBlockIterator) fs.listCorruptFileBlocks(new Path("/srcdat2"));        int corruptPaths = countPaths(iter);        assertTrue("Expected more than " + maxCorruptFileBlocks + " corrupt file blocks but got " + corruptPaths, corruptPaths > maxCorruptFileBlocks);        assertTrue("Iterator should have made more than 1 call but made " + iter.getCallsMade(), iter.getCallsMade() > 1);        util.cleanup(fs, "/srcdat2");    } finally {        if (cluster != null) {            cluster.shutdown();        }    }}
1
public void stopRunning()
{    try {        running.set(false);                boolean tryLock = lockRunning.tryLock();        int loop = 0;        while (false == tryLock && loop++ < 100) {                        Thread.sleep(500);            tryLock = lockRunning.tryLock();        }    } catch (Exception e) {            } finally {        lockRunning.unlock();    }}
1
private void logApplicationReport(ApplicationReport appReport)
{        if (appReport.getYarnApplicationState() == YarnApplicationState.FAILED) {                    }}
1
private void processRecovery() throws IOException
{    boolean attemptRecovery = shouldAttemptRecovery();    boolean recoverySucceeded = true;    if (attemptRecovery) {                try {            parsePreviousJobHistory();        } catch (IOException e) {                        recoverySucceeded = false;        }    }    if (!isFirstAttempt() && (!attemptRecovery || !recoverySucceeded)) {        amInfos.addAll(readJustAMInfos());    }}
1
public void testNoteCreateWithParagraphs() throws IOException
{        String noteName = "test";    String jsonRequest = "{\"name\":\"" + noteName + "\", \"paragraphs\": [" + "{\"title\": \"title1\", \"text\": \"text1\"}," + "{\"title\": \"title2\", \"text\": \"text2\"}," + "{\"title\": \"titleConfig\", \"text\": \"text3\", " + "\"config\": {\"colWidth\": 9.0, \"title\": true, " + "\"results\": [{\"graph\": {\"mode\": \"pieChart\"}}] " + "}}]} ";    PostMethod post = httpPost("/notebook/", jsonRequest);        assertThat("test note create method:", post, isAllowed());    Map<String, Object> resp = gson.fromJson(post.getResponseBodyAsString(), new TypeToken<Map<String, Object>>() {    }.getType());    String newNoteId = (String) resp.get("body");        Note newNote = TestUtils.getInstance(Notebook.class).getNote(newNoteId);    assertNotNull("Can not find new note by id", newNote);        String newNoteName = newNote.getName();        String expectedNoteName = noteName;    if (noteName.isEmpty()) {        expectedNoteName = "Note " + newNoteId;    }    assertEquals("compare note name", expectedNoteName, newNoteName);    assertEquals("initial paragraph check failed", 4, newNote.getParagraphs().size());    for (Paragraph p : newNote.getParagraphs()) {        if (StringUtils.isEmpty(p.getText())) {            continue;        }        assertTrue("paragraph title check failed", p.getTitle().startsWith("title"));        assertTrue("paragraph text check failed", p.getText().startsWith("text"));        if (p.getTitle().equals("titleConfig")) {            assertEquals("paragraph col width check failed", 9.0, p.getConfig().get("colWidth"));            assertTrue("paragraph show title check failed", ((boolean) p.getConfig().get("title")));            Map graph = ((List<Map>) p.getConfig().get("results")).get(0);            String mode = ((Map) graph.get("graph")).get("mode").toString();            assertEquals("paragraph graph mode check failed", "pieChart", mode);        }    }        TestUtils.getInstance(Notebook.class).removeNote(newNoteId, anonymous);    post.releaseConnection();}
1
public void handleResult(List<ByteBuffer> responseBytes)
{    ByteBufferInputStream bbi = new ByteBufferInputStream(responseBytes);    BinaryDecoder in = DecoderFactory.get().binaryDecoder(bbi, null);    try {        if (!readHandshake(in)) {                        Request handshake = new Request(request);            getTransceiver().transceive(handshake.getBytes(), new TransceiverCallback<>(handshake, callback));            return;        }    } catch (Exception e) {            }        Response response = new Response(request, in);    Object responseObject;    try {        try {            responseObject = response.getResponse();        } catch (Exception e) {            if (callback != null) {                callback.handleError(e);            }            return;        }        if (callback != null) {            callback.handleResult((T) responseObject);        }    } catch (Throwable t) {            }}
1
protected Destination createTemporaryDestination(String destName) throws JMSException
{    String simpleName = getSimpleName(destName);    byte destinationType = getDestinationType(destName);        if (destinationType == ActiveMQDestination.TEMP_QUEUE_TYPE) {                return getSession().createQueue(simpleName);    } else if (destinationType == ActiveMQDestination.TEMP_TOPIC_TYPE) {                return getSession().createTopic(simpleName);    } else {        throw new IllegalArgumentException("Unrecognized destination type: " + destinationType);    }}
1
public AllocateResponse allocate(AllocateRequest request) throws YarnException, IOException
{    validateRunning();    ApplicationAttemptId attemptId = getAppIdentifier();        ApplicationId appId = attemptId.getApplicationId();    if (shouldReRegisterNext) {        String message = "AM is not registered, should re-register.";                throw new ApplicationMasterNotRegisteredException(message);    }        synchronized (allocateSyncObj) {        if (shouldWaitForSyncNextAllocate) {            shouldWaitForSyncNextAllocate = false;                        try {                allocateSyncObj.wait();                            } catch (InterruptedException e) {                            }        }    }    ArrayList<Container> containerList = new ArrayList<Container>();    if (request.getAskList() != null) {        for (ResourceRequest rr : request.getAskList()) {            for (int i = 0; i < rr.getNumContainers(); i++) {                ContainerId containerId = ContainerId.newInstance(getApplicationAttemptId(1), containerIndex.incrementAndGet());                Container container = Records.newRecord(Container.class);                container.setId(containerId);                container.setPriority(rr.getPriority());                                                NodeId nodeId = NodeId.newInstance(!Strings.isNullOrEmpty(rr.getResourceName()) ? rr.getResourceName() : "dummy", 1000);                container.setNodeId(nodeId);                container.setResource(rr.getCapability());                containerList.add(container);                synchronized (applicationContainerIdMap) {                                                            Assert.assertTrue("The application id is Not registered before allocate(): " + appId, applicationContainerIdMap.containsKey(appId));                    List<ContainerId> ids = applicationContainerIdMap.get(appId);                    ids.add(containerId);                }            }        }    }    List<ContainerStatus> completedList = new ArrayList<>();    if (request.getReleaseList() != null && request.getReleaseList().size() > 0) {                synchronized (applicationContainerIdMap) {            Assert.assertTrue("The application id is not registered before allocate(): " + appId, applicationContainerIdMap.containsKey(appId));            List<ContainerId> ids = applicationContainerIdMap.get(appId);            for (ContainerId id : request.getReleaseList()) {                boolean found = false;                for (ContainerId c : ids) {                    if (c.equals(id)) {                        found = true;                        break;                    }                }                Assert.assertTrue("ContainerId " + id + " being released is not valid for application: " + attemptId, found);                ids.remove(id);                completedList.add(ContainerStatus.newInstance(id, ContainerState.COMPLETE, "", 0));            }        }    }            Token newAMRMToken = Token.newInstance(new byte[0], Integer.toString(this.rmId), new byte[0], "");    return AllocateResponse.newInstance(0, completedList, containerList, new ArrayList<NodeReport>(), null, AMCommand.AM_RESYNC, 1, null, new ArrayList<NMToken>(), newAMRMToken, new ArrayList<UpdatedContainer>());}
1
public void dispatch(Notification notification)
{        try {        createTransportMapping();        snmp = new Snmp(transportMapping);        SnmpVersion snmpVersion = getSnmpVersion(notification.DispatchProperties);        sendTraps(notification, snmpVersion);        successCallback(notification);    } catch (InvalidSnmpConfigurationException ex) {                failureCallback(notification);    } catch (Exception ex) {                failureCallback(notification);        transportMapping = null;    }}
1
public void register(ServiceDefinition definition)
{    if (definition.getId() == null) {        throw new IllegalArgumentException("Service ID must be defined (definition=" + definition + ")");    }    if (definition.getName() == null) {        throw new IllegalArgumentException("Service Name must be defined (definition=" + definition + ")");    }    try {        ServiceInstance<MetaData> instance = ServiceInstance.<MetaData>builder().address(computeServiceHost(definition)).port(definition.getPort()).name(definition.getName()).id(definition.getId()).payload(new MetaData(definition.getMetadata())).build();        serviceDiscovery.registerService(instance);                serviceList.add(definition.getId());    } catch (Exception e) {            }}
1
private void sendRecoveryMsgs(final DistributionManager dm, final DLockBatch[] batches, final InternalDistributedMember owner, final DLockGrantor grantor)
{    synchronized (stateLock) {        processingDepartures = true;    }    Runnable recoverTx = () -> {        try {            for (int i = 0; i < batches.length; i++) {                TXLockBatch batch = (TXLockBatch) batches[i];                                Set participants = batch.getParticipants();                TXOriginatorRecoveryProcessor.sendMessage(participants, owner, batch.getTXLockId(), grantor, dm);            }        } finally {            clearProcessingDepartures();        }    };    try {        dm.getExecutors().getWaitingThreadPool().execute(recoverTx);    } catch (RejectedExecutionException e) {                if (!dm.getCancelCriterion().isCancelInProgress()) {                        recoverTx.run();        }    }}
1
private Map<String, String> getStorageIDToVolumeBasePathMap() throws DiskBalancerException
{    Map<String, String> storageIDToVolBasePathMap = new HashMap<>();    FsDatasetSpi.FsVolumeReferences references;    try {        try (AutoCloseableLock lock = this.dataset.acquireDatasetLock()) {            references = this.dataset.getFsVolumeReferences();            for (int ndx = 0; ndx < references.size(); ndx++) {                FsVolumeSpi vol = references.get(ndx);                storageIDToVolBasePathMap.put(vol.getStorageID(), vol.getBaseURI().getPath());            }            references.close();        }    } catch (IOException ex) {                throw new DiskBalancerException("Internal error", ex, DiskBalancerException.Result.INTERNAL_ERROR);    }    return storageIDToVolBasePathMap;}
1
public void processLogConfigChange(LogConfig logConfig)
{    if (null != logConfig) {                TreeMap<String, LogLevel> loggers = new TreeMap<>(logConfig.get_named_logger_level());        LoggerContext logContext = (LoggerContext) LogManager.getContext(false);        Map<String, LogLevel> newLogConfigs = new HashMap<>();        for (Map.Entry<String, LogLevel> entry : loggers.entrySet()) {            String msgLoggerName = entry.getKey();            msgLoggerName = ("ROOT".equalsIgnoreCase(msgLoggerName)) ? LogManager.ROOT_LOGGER_NAME : msgLoggerName;            LogLevel loggerLevel = entry.getValue();                        if (loggerLevel.is_set_reset_log_level_timeout_epoch()) {                LogLevel copy = new LogLevel(loggerLevel);                if (originalLogLevels.containsKey(msgLoggerName)) {                    copy.set_reset_log_level(originalLogLevels.get(msgLoggerName).name());                } else {                    copy.set_reset_log_level(Level.INFO.name());                }                newLogConfigs.put(msgLoggerName, copy);            }        }                TreeMap<String, LogLevel> latestConf = latestLogConfig.get();        if (latestConf != null) {            for (String loggerName : latestConf.descendingKeySet()) {                if (!newLogConfigs.containsKey(loggerName)) {                                        setLoggerLevel(logContext, loggerName, latestConf.get(loggerName).get_reset_log_level());                }            }        }                for (String loggerName : new TreeSet<>(logConfig.get_named_logger_level().keySet())) {            LogLevel logLevel = logConfig.get_named_logger_level().get(loggerName);            loggerName = ("ROOT".equalsIgnoreCase(loggerName)) ? LogManager.ROOT_LOGGER_NAME : loggerName;            LogLevelAction action = logLevel.get_action();            if (action == LogLevelAction.UPDATE) {                setLoggerLevel(logContext, loggerName, logLevel.get_target_log_level());            }        }        logContext.updateLoggers();        latestLogConfig.set(new TreeMap<>(newLogConfigs));            }}
1
public AbstractCompactionTask getNextBackgroundTask(int gcBefore)
{    Collection<SSTableReader> previousCandidate = null;    while (true) {        OperationType op;        LeveledManifest.CompactionCandidate candidate = manifest.getCompactionCandidates();        if (candidate == null) {                        SSTableReader sstable = findDroppableSSTable(gcBefore);            if (sstable == null) {                logger.trace("No compaction necessary for {}", this);                return null;            }            candidate = new LeveledManifest.CompactionCandidate(Collections.singleton(sstable), sstable.getSSTableLevel(), getMaxSSTableBytes());            op = OperationType.TOMBSTONE_COMPACTION;        } else {            op = OperationType.COMPACTION;        }                if (candidate.sstables.equals(previousCandidate)) {                        return null;        }        LifecycleTransaction txn = cfs.getTracker().tryModify(candidate.sstables, OperationType.COMPACTION);        if (txn != null) {            AbstractCompactionTask newTask;            if (!singleSSTableUplevel || op == OperationType.TOMBSTONE_COMPACTION || txn.originals().size() > 1)                newTask = new LeveledCompactionTask(cfs, txn, candidate.level, gcBefore, candidate.maxSSTableBytes, false);            else                newTask = new SingleSSTableLCSTask(cfs, txn, candidate.level);            newTask.setCompactionType(op);            return newTask;        }        previousCandidate = candidate.sstables;    }}
1
public void testContainerUpgradeSuccessExplicitRollback() throws IOException, InterruptedException, YarnException
{    Listener listener = new Listener();    ((NodeManager.DefaultContainerStateListener) containerManager.context.getContainerStateTransitionListener()).addListener(listener);    String[] pids = testContainerReInitSuccess(false);                pids[1] = doRestartTests(createContainerId(0), new File(tmpDir, "start_file_n.txt").getAbsoluteFile(), "Upgrade World!", pids[1], true);        File oldStartFile = new File(tmpDir, "start_file_o.txt").getAbsoluteFile();    oldStartFile.delete();    ContainerId cId = createContainerId(0);        containerManager.rollbackLastReInitialization(cId);    Container container = containerManager.getContext().getContainers().get(cId);    Assert.assertTrue(container.isReInitializing());        Assert.assertFalse("Original Process is still alive!", DefaultContainerExecutor.containerIsAlive(pids[0]));        int timeoutSecs = 0;    while (container.isReInitializing() && timeoutSecs++ < 20) {        Thread.sleep(1000);            }    Assert.assertFalse(container.isReInitializing());    timeoutSecs = 0;        while (!oldStartFile.exists() && timeoutSecs++ < 20) {        Thread.sleep(1000);            }        BufferedReader reader = new BufferedReader(new FileReader(oldStartFile));    Assert.assertEquals("Hello World!", reader.readLine());        String rolledBackPid = reader.readLine().trim();        Assert.assertEquals(null, reader.readLine());    Assert.assertNotEquals("The Rolled-back process should be a different pid", pids[0], rolledBackPid);    List<org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerState> containerStates = listener.states.get(createContainerId(0));    Assert.assertEquals(Arrays.asList(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerState.NEW, org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerState.LOCALIZING, org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerState.SCHEDULED, org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerState.RUNNING, org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerState.REINITIALIZING, org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerState.REINITIALIZING_AWAITING_KILL, org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerState.REINITIALIZING_AWAITING_KILL, org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerState.SCHEDULED, org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerState.RUNNING,     org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerState.REINITIALIZING_AWAITING_KILL, org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerState.REINITIALIZING_AWAITING_KILL, org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerState.SCHEDULED, org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerState.RUNNING,     org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerState.REINITIALIZING_AWAITING_KILL, org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerState.REINITIALIZING_AWAITING_KILL, org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerState.SCHEDULED, org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerState.RUNNING), containerStates);    List<ContainerEventType> containerEventTypes = listener.events.get(createContainerId(0));    Assert.assertEquals(Arrays.asList(ContainerEventType.INIT_CONTAINER, ContainerEventType.RESOURCE_LOCALIZED, ContainerEventType.CONTAINER_LAUNCHED, ContainerEventType.REINITIALIZE_CONTAINER, ContainerEventType.RESOURCE_LOCALIZED, ContainerEventType.UPDATE_DIAGNOSTICS_MSG, ContainerEventType.CONTAINER_KILLED_ON_REQUEST, ContainerEventType.CONTAINER_LAUNCHED, ContainerEventType.REINITIALIZE_CONTAINER, ContainerEventType.UPDATE_DIAGNOSTICS_MSG, ContainerEventType.CONTAINER_KILLED_ON_REQUEST, ContainerEventType.CONTAINER_LAUNCHED, ContainerEventType.ROLLBACK_REINIT, ContainerEventType.UPDATE_DIAGNOSTICS_MSG, ContainerEventType.CONTAINER_KILLED_ON_REQUEST, ContainerEventType.CONTAINER_LAUNCHED), containerEventTypes);}
1
public AmazonS3 getAmazonS3ClientForTesting(String reason)
{        return s3;}
1
public synchronized void close() throws IOException
{    if (isClosed) {        return;    }        super.close();        IOUtils.cleanupWithLogger(LOG, abfsStore, delegationTokenManager);    this.isClosed = true;}
1
 synchronized void resetGenerationOnLeaveGroup()
{        resetGeneration();}
1
private void addEntry(StreamElementQueueEntry<T> streamElementQueueEntry)
{    assert (lock.isHeldByCurrentThread());    if (streamElementQueueEntry.isWatermark()) {        lastSet = new HashSet<>(capacity);        if (firstSet.isEmpty()) {            firstSet.add(streamElementQueueEntry);        } else {            Set<StreamElementQueueEntry<?>> watermarkSet = new HashSet<>(1);            watermarkSet.add(streamElementQueueEntry);            uncompletedQueue.offer(watermarkSet);        }        uncompletedQueue.offer(lastSet);    } else {        lastSet.add(streamElementQueueEntry);    }    streamElementQueueEntry.onComplete((StreamElementQueueEntry<T> value) -> {        try {            onCompleteHandler(value);        } catch (InterruptedException e) {                                            } catch (Throwable t) {            operatorActions.failOperator(new Exception("Could not complete the " + "stream element queue entry: " + value + '.', t));        }    }, executor);    numberEntries++;}
1
public synchronized void close() throws IOException
{        super.close();    boolean[] value = socketAudits.get(System.identityHashCode(this));    value[1] = true;}
1
private void invalidateSession(SamlRealm realm, SamlInvalidateSessionRequest request, ActionListener<SamlInvalidateSessionResponse> listener)
{    try {        final SamlLogoutRequestHandler.Result result = realm.getLogoutHandler().parseFromQueryString(request.getQueryString());        findAndInvalidateTokens(realm, result, ActionListener.wrap(count -> listener.onResponse(new SamlInvalidateSessionResponse(realm.name(), count, buildLogoutResponseUrl(realm, result))), listener::onFailure));    } catch (ElasticsearchSecurityException e) {                listener.onFailure(e);    }}
1
private void handleMismatch(List<String> targetHosts, FQLQuery query, List<ResultHandler.ComparableRow> rows)
{    UUID mismatchUUID = UUID.randomUUID();    StringBuilder sb = new StringBuilder("{} - MISMATCH Query = {} ");    for (int i = 0; i < targetHosts.size(); i++) sb.append("mismatch").append(i).append('=').append('"').append(targetHosts.get(i)).append(':').append(rows.get(i)).append('"').append(',');        try {        if (mismatchListener != null)            mismatchListener.mismatch(mismatchUUID, targetHosts, query, rows);    } catch (Throwable t) {            }}
1
public void connectionSucceeded(final UserToBitConnection connection)
{    connection.getChannel().closeFuture().addListener(new GenericFutureListener<Future<? super Void>>() {        @Override        public void operationComplete(Future<? super Void> future) throws Exception {            for (final UserResultsListener listener : queryIdToResultsListenersMap.values()) {                listener.submissionFailed(UserException.connectionError().message("Connection %s closed unexpectedly. Drillbit down?", connection.getName()).build(logger));                if (listener instanceof BufferingResultsListener) {                                                        }            }        }    });    parentHandler.connectionSucceeded(connection);}
1
private void bind() throws IOException
{    this.setDaemon(true);    serverSocket = new ServerSocket();        serverSocket.setSoTimeout(acceptTimeout);    serverSocket.setReuseAddress(true);    InetSocketAddress listenAddress;    if (null != this.listenHost) {        listenAddress = new InetSocketAddress(this.listenHost, this.listenPort);    } else {        listenAddress = new InetSocketAddress(this.listenPort);    }    long startTicks = System.currentTimeMillis();    while (!serverSocket.isBound()) {        try {            serverSocket.bind(listenAddress, backlog);        } catch (BindException bindEx) {            if (System.currentTimeMillis() < startTicks + bindTimeout) {                                try {                    Thread.sleep(bindRetryDelay);                } catch (InterruptedException interruptedEx) {                                        throw bindEx;                }            }        }    }    if (0 >= this.listenPort) {        this.listenPort = serverSocket.getLocalPort();    }    }
1
protected Transport createTransport(URI location, WireFormat wf) throws UnknownHostException, IOException
{    URI localLocation = null;    String path = location.getPath();        if (path != null && path.length() > 0) {        int localPortIndex = path.indexOf(':');        try {            Integer.parseInt(path.substring(localPortIndex + 1, path.length()));            String localString = location.getScheme() + ":/" + path;            localLocation = new URI(localString);        } catch (Exception e) {                    }    }    SocketFactory socketFactory = createSocketFactory();    return createTcpFaultyTransport(wf, socketFactory, location, localLocation);}
1
private synchronized Set<String> doLookup(long freshness)
{    String url = registryURL + "?freshness=" + freshness;    try {        HttpGet method = new HttpGet(url);        ResponseHandler<String> handler = new BasicResponseHandler();        String response = httpClient.execute(method, handler);                Set<String> rc = new HashSet<String>();        Scanner scanner = new Scanner(response);        while (scanner.hasNextLine()) {            String service = scanner.nextLine();            if (service.trim().length() != 0) {                rc.add(service);            }        }        scanner.close();        return rc;    } catch (Exception e) {                return null;    }}
1
 void processUpdate(SolrQueryRequest req, UpdateRequestProcessor processor, XMLStreamReader parser) throws XMLStreamException, IOException, FactoryConfigurationError
{    AddUpdateCommand addCmd = null;    SolrParams params = req.getParams();    while (true) {        int event = parser.next();        switch(event) {            case XMLStreamConstants.END_DOCUMENT:                parser.close();                return;            case XMLStreamConstants.START_ELEMENT:                String currTag = parser.getLocalName();                if (currTag.equals(UpdateRequestHandler.ADD)) {                    log.trace("SolrCore.update(add)");                    addCmd = new AddUpdateCommand(req);                                        addCmd.commitWithin = params.getInt(UpdateParams.COMMIT_WITHIN, -1);                    addCmd.overwrite = params.getBool(UpdateParams.OVERWRITE, true);                    for (int i = 0; i < parser.getAttributeCount(); i++) {                        String attrName = parser.getAttributeLocalName(i);                        String attrVal = parser.getAttributeValue(i);                        if (UpdateRequestHandler.OVERWRITE.equals(attrName)) {                            addCmd.overwrite = StrUtils.parseBoolean(attrVal);                        } else if (UpdateRequestHandler.COMMIT_WITHIN.equals(attrName)) {                            addCmd.commitWithin = Integer.parseInt(attrVal);                        } else {                                                    }                    }                } else if ("doc".equals(currTag)) {                    if (addCmd != null) {                        log.trace("adding doc...");                        addCmd.clear();                        addCmd.solrDoc = readDoc(parser);                        processor.processAdd(addCmd);                    } else {                        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, "Unexpected <doc> tag without an <add> tag surrounding it.");                    }                } else if (UpdateRequestHandler.COMMIT.equals(currTag) || UpdateRequestHandler.OPTIMIZE.equals(currTag)) {                    log.trace("parsing " + currTag);                    CommitUpdateCommand cmd = new CommitUpdateCommand(req, UpdateRequestHandler.OPTIMIZE.equals(currTag));                    ModifiableSolrParams mp = new ModifiableSolrParams();                    for (int i = 0; i < parser.getAttributeCount(); i++) {                        String attrName = parser.getAttributeLocalName(i);                        String attrVal = parser.getAttributeValue(i);                        mp.set(attrName, attrVal);                    }                    RequestHandlerUtils.validateCommitParams(mp);                                        SolrParams p = SolrParams.wrapDefaults(mp, req.getParams());                    RequestHandlerUtils.updateCommit(cmd, p);                    processor.processCommit(cmd);                } else                 if (UpdateRequestHandler.ROLLBACK.equals(currTag)) {                    log.trace("parsing rollback");                    RollbackUpdateCommand cmd = new RollbackUpdateCommand(req);                    processor.processRollback(cmd);                } else                 if (UpdateRequestHandler.DELETE.equals(currTag)) {                    log.trace("parsing delete");                    processDelete(req, processor, parser);                }                                break;        }    }}
1
public void testFullRestoreSingleDNECommand() throws Exception
{        List<TableName> tables = Lists.newArrayList(table1);    String backupId = fullTableBackup(tables);    assertTrue(checkSucceeded(backupId));        TableName[] tableset = new TableName[] { TableName.valueOf("faketable") };    TableName[] tablemap = new TableName[] { table1_restore };    String[] args = new String[] { BACKUP_ROOT_DIR, backupId, StringUtils.join(tableset, ","), "-m", StringUtils.join(tablemap, ",") };        int ret = ToolRunner.run(conf1, new RestoreDriver(), args);    assertTrue(ret != 0);}
1
public void testJobRelocationIsMemoryAware() throws Exception
{    internalCluster().ensureAtLeastNumDataNodes(1);    ensureStableClusterOnAllNodes(1);        setupJobWithoutDatafeed("small1", new ByteSizeValue(2, ByteSizeUnit.MB));    setupJobWithoutDatafeed("small2", new ByteSizeValue(2, ByteSizeUnit.MB));    setupJobWithoutDatafeed("small3", new ByteSizeValue(2, ByteSizeUnit.MB));    setupJobWithoutDatafeed("small4", new ByteSizeValue(2, ByteSizeUnit.MB));            internalCluster().ensureAtLeastNumDataNodes(3);    ensureStableClusterOnAllNodes(3);        ensureGreen();        setupJobWithoutDatafeed("big1", new ByteSizeValue(500, ByteSizeUnit.MB));        internalCluster().stopCurrentMasterNode();    ensureStableClusterOnAllNodes(2);                            assertBusy(() -> {        GetJobsStatsAction.Response statsResponse = client().execute(GetJobsStatsAction.INSTANCE, new GetJobsStatsAction.Request(MetaData.ALL)).actionGet();        QueryPage<JobStats> jobStats = statsResponse.getResponse();        assertNotNull(jobStats);        List<String> smallJobNodes = jobStats.results().stream().filter(s -> s.getJobId().startsWith("small") && s.getNode() != null).map(s -> s.getNode().getName()).collect(Collectors.toList());        List<String> bigJobNodes = jobStats.results().stream().filter(s -> s.getJobId().startsWith("big") && s.getNode() != null).map(s -> s.getNode().getName()).collect(Collectors.toList());                assertEquals(5, jobStats.count());        assertEquals(4, smallJobNodes.size());        assertEquals(1, bigJobNodes.size());        assertEquals(1L, smallJobNodes.stream().distinct().count());        assertEquals(1L, bigJobNodes.stream().distinct().count());        assertNotEquals(smallJobNodes, bigJobNodes);    });}
1
public void testRecoveryOnDeleteFailureMetaDataOk() throws Exception
{    startBroker();    int sent = produceMessagesToConsumeMultipleDataFiles(50);    int numFilesAfterSend = getNumberOfJournalFiles();        assertTrue("more than x files: " + numFilesAfterSend, numFilesAfterSend > 4);    int received = tryConsume(destination, sent / 2);    assertEquals("all message received", sent / 2, received);    int numFilesAfterRestart = getNumberOfJournalFiles();            ((KahaDBPersistenceAdapter) broker.getPersistenceAdapter()).getStore().checkpoint(true);    int numFilesAfterGC = getNumberOfJournalFiles();    assertEquals("all message received", sent / 2, received);        assertTrue("Gc has happened", numFilesAfterGC < numFilesAfterRestart);        final MessageDatabase.Metadata[] fromDiskMetaData = new MessageDatabase.Metadata[1];    final KahaDBStore messageStore = ((KahaDBPersistenceAdapter) broker.getPersistenceAdapter()).getStore();        PageFile fromDiskPageFile = new PageFile(messageStore.getIndexDirectory(), "db");    fromDiskPageFile.setEnablePageCaching(false);    fromDiskPageFile.setEnableRecoveryFile(false);    fromDiskPageFile.load();    fromDiskPageFile.tx().execute(new Transaction.Closure<IOException>() {        @Override        public void execute(Transaction tx) throws IOException {            Page<MessageDatabase.Metadata> page = tx.load(0, messageStore.metadataMarshaller);            fromDiskMetaData[0] = page.get();        }    });    assertEquals("location is uptodate", messageStore.getMetadata().ackMessageFileMapLocation, fromDiskMetaData[0].ackMessageFileMapLocation);}
1
public void watch()
{    while (!shutdown.get()) {        WatchKey watchKey;        try {            watchKey = watchService.take();        } catch (InterruptedException e) {            if (shutdown.get()) {                                break;            } else {                                throw new RuntimeException("Watcher interrupted before being shutdown", e);            }        }        Path watchedPath = (Path) watchKey.watchable();        WatchedPathInfo parentWatchedPathInfo = watchedPaths.get(watchedPath);        boolean cancelledWatch = false;        for (WatchEvent<?> rawEvent : watchKey.pollEvents()) {            if (rawEvent.kind().equals(OVERFLOW)) {                                continue;            }            WatchEvent<Path> event = (WatchEvent<Path>) rawEvent;            WatchedPathInfo watchedPathInfo;            Path resolvedPath;            switch(parentWatchedPathInfo.type) {                case BASE:                                        if (event.context().getFileName().toString().equals(OUTPUT)) {                        resolvedPath = watchedPath.resolve(event.context());                        watchedPathInfo = new WatchedPathInfo(parentWatchedPathInfo, Type.OUTPUT, null);                        registerDir(resolvedPath, watchedPathInfo);                                                try (DirectoryStream<Path> dirStream = Files.newDirectoryStream(resolvedPath)) {                            for (Path path : dirStream) {                                                                if (path.toFile().isDirectory()) {                                    watchedPathInfo = new WatchedPathInfo(parentWatchedPathInfo, Type.FINAL, path.getFileName().toString());                                    registerDir(path, watchedPathInfo);                                    scanForFinalFiles(watchedPathInfo, path);                                } else {                                                                    }                            }                        } catch (IOException e) {                                                    }                                                cancelledWatch = true;                        watchKey.cancel();                    } else {                                            }                    break;                case OUTPUT:                                        resolvedPath = watchedPath.resolve(event.context());                                        watchedPathInfo = new WatchedPathInfo(parentWatchedPathInfo, Type.FINAL, event.context().getFileName().toString());                    registerDir(resolvedPath, watchedPathInfo);                    scanForFinalFiles(watchedPathInfo, resolvedPath);                    break;                case FINAL:                    resolvedPath = watchedPath.resolve(event.context());                    if (event.context().getFileName().toString().equals(ShuffleHandler.DATA_FILE_NAME)) {                        registerFoundAttempt(parentWatchedPathInfo.pathIdentifier, null, resolvedPath);                    } else if (event.context().getFileName().toString().equals(ShuffleHandler.INDEX_FILE_NAME)) {                        registerFoundAttempt(parentWatchedPathInfo.pathIdentifier, resolvedPath, null);                    } else {                                            }                    break;            }        }        if (!cancelledWatch) {            boolean valid = watchKey.reset();            if (!valid) {                            }        }    }}
1
public void setupServer(int i) throws IOException
{    int tickTime = 2000;    int initLimit = 3;    int syncLimit = 3;    int connectToLearnerMasterLimit = 3;    if (peers == null) {        peers = new HashMap<Long, QuorumServer>();        peers.put(Long.valueOf(1), new QuorumServer(1, new InetSocketAddress(LOCALADDR, port1), new InetSocketAddress(LOCALADDR, portLE1), new InetSocketAddress(LOCALADDR, portClient1), LearnerType.PARTICIPANT));        peers.put(Long.valueOf(2), new QuorumServer(2, new InetSocketAddress(LOCALADDR, port2), new InetSocketAddress(LOCALADDR, portLE2), new InetSocketAddress(LOCALADDR, portClient2), LearnerType.PARTICIPANT));        peers.put(Long.valueOf(3), new QuorumServer(3, new InetSocketAddress(LOCALADDR, port3), new InetSocketAddress(LOCALADDR, portLE3), new InetSocketAddress(LOCALADDR, portClient3), LearnerType.PARTICIPANT));        peers.put(Long.valueOf(4), new QuorumServer(4, new InetSocketAddress(LOCALADDR, port4), new InetSocketAddress(LOCALADDR, portLE4), new InetSocketAddress(LOCALADDR, portClient4), LearnerType.PARTICIPANT));        peers.put(Long.valueOf(5), new QuorumServer(5, new InetSocketAddress(LOCALADDR, port5), new InetSocketAddress(LOCALADDR, portLE5), new InetSocketAddress(LOCALADDR, portClient5), LearnerType.PARTICIPANT));    }    switch(i) {        case 1:                        s1 = new QuorumPeer(peers, s1dir, s1dir, portClient1, 3, 1, tickTime, initLimit, syncLimit, connectToLearnerMasterLimit);            assertEquals(portClient1, s1.getClientPort());            break;        case 2:                        s2 = new QuorumPeer(peers, s2dir, s2dir, portClient2, 3, 2, tickTime, initLimit, syncLimit, connectToLearnerMasterLimit);            assertEquals(portClient2, s2.getClientPort());            break;        case 3:                        s3 = new QuorumPeer(peers, s3dir, s3dir, portClient3, 3, 3, tickTime, initLimit, syncLimit, connectToLearnerMasterLimit);            assertEquals(portClient3, s3.getClientPort());            break;        case 4:                        s4 = new QuorumPeer(peers, s4dir, s4dir, portClient4, 3, 4, tickTime, initLimit, syncLimit, connectToLearnerMasterLimit);            assertEquals(portClient4, s4.getClientPort());            break;        case 5:                        s5 = new QuorumPeer(peers, s5dir, s5dir, portClient5, 3, 5, tickTime, initLimit, syncLimit, connectToLearnerMasterLimit);            assertEquals(portClient5, s5.getClientPort());    }}
1
protected void decode(ChannelHandlerContext ctx, AddressedEnvelope<Object, InetSocketAddress> msg, List<Object> out) throws Exception
{    if (msg.content() instanceof ByteBuf) {        ByteBuf payload = (ByteBuf) msg.content();        Object result = delegateDecoder.decode(ctx, payload);        AddressedEnvelope<Object, InetSocketAddress> addressEvelop = new DefaultAddressedEnvelope<>(result, msg.recipient(), msg.sender());        out.add(addressEvelop);    } else {            }}
1
protected void destroyServers() throws Exception
{    for (JettySolrRunner runner : jettys) {        try {            runner.stop();        } catch (Exception e) {                    }    }    jettys.clear();}
1
public SalesforceComponent configureSalesforceComponent() throws Exception
{    SalesforceComponent component = new SalesforceComponent();    component.setCamelContext(camelContext);    Map<String, Object> parameters = new HashMap<>();    IntrospectionSupport.getProperties(configuration, parameters, null, false);    for (Map.Entry<String, Object> entry : parameters.entrySet()) {        Object value = entry.getValue();        Class<?> paramClass = value.getClass();        if (paramClass.getName().endsWith("NestedConfiguration")) {            Class nestedClass = null;            try {                nestedClass = (Class) paramClass.getDeclaredField("CAMEL_NESTED_CLASS").get(null);                HashMap<String, Object> nestedParameters = new HashMap<>();                IntrospectionSupport.getProperties(value, nestedParameters, null, false);                Object nestedProperty = nestedClass.newInstance();                CamelPropertiesHelper.setCamelProperties(camelContext, nestedProperty, nestedParameters, false);                entry.setValue(nestedProperty);            } catch (NoSuchFieldException e) {            }        }    }    CamelPropertiesHelper.setCamelProperties(camelContext, component, parameters, false);    if (ObjectHelper.isNotEmpty(customizers)) {        for (ComponentCustomizer<SalesforceComponent> customizer : customizers) {            boolean useCustomizer = (customizer instanceof HasId) ? HierarchicalPropertiesEvaluator.evaluate(applicationContext.getEnvironment(), "camel.component.customizer", "camel.component.salesforce.customizer", ((HasId) customizer).getId()) : HierarchicalPropertiesEvaluator.evaluate(applicationContext.getEnvironment(), "camel.component.customizer", "camel.component.salesforce.customizer");            if (useCustomizer) {                                customizer.customize(component);            }        }    }    return component;}
1
private List<OrcSplit> generateSplitsFromPpd(SplitInfos ppdResult) throws IOException
{    OffsetAndLength current = new OffsetAndLength();    List<OrcSplit> splits = new ArrayList<>(ppdResult.getInfosCount());    int lastIdx = -1;    for (Metastore.SplitInfo si : ppdResult.getInfosList()) {        int index = si.getIndex();        if (lastIdx >= 0 && lastIdx + 1 != index && current.offset != -1) {                        splits.add(createSplit(current.offset, current.length, orcTail));            current.offset = -1;        }        lastIdx = index;        String debugStr = null;        if (LOG.isDebugEnabled()) {            debugStr = current.toString();        }        current = generateOrUpdateSplit(splits, current, si.getOffset(), si.getLength(), null);        if (LOG.isDebugEnabled()) {                    }    }    generateLastSplit(splits, current, null);    return splits;}
1
public void error(Marker marker, String string)
{    }
1
protected void onStopped()
{    try {                mqttClient.disconnect(DISCONNECT_TIMEOUT);    } catch (MqttException me) {            }    try {                mqttClient.close();        mqttClient = null;    } catch (MqttException me) {            }}
1
public Collection<Instance> instances()
{        final List<Instance> instances = zones.stream().map((zoneId) -> {        try {                                    InstanceList instanceList = Access.doPrivilegedIOException(() -> {                Compute.Instances.List list = client().instances().list(project, zoneId);                return list.execute();            });                        return instanceList.isEmpty() || instanceList.getItems() == null ? Collections.<Instance>emptyList() : instanceList.getItems();        } catch (IOException e) {                                                return Collections.<Instance>emptyList();        }    }).reduce(new ArrayList<>(), (a, b) -> {        a.addAll(b);        return a;    });    if (instances.isEmpty()) {            }    return instances;}
1
public void testCreateParentFailed() throws IOException
{    Path f = new Path("/" + name.getMethodName() + "/test");    EventLoop eventLoop = EVENT_LOOP_GROUP.next();    try {        FanOutOneBlockAsyncDFSOutputHelper.createOutput(FS, f, true, false, (short) 3, FS.getDefaultBlockSize(), eventLoop, CHANNEL_CLASS);        fail("should fail with parent does not exist");    } catch (RemoteException e) {                assertThat(e.unwrapRemoteException(), instanceOf(FileNotFoundException.class));    }}
1
private Optional<Boolean> assertConnectorAndTasksRunning(String connectorName, int numTasks)
{    try {        ConnectorStateInfo info = connect.connectorStatus(connectorName);        boolean result = info != null && info.tasks().size() == numTasks && info.connector().state().equals(AbstractStatus.State.RUNNING.toString()) && info.tasks().stream().allMatch(s -> s.state().equals(AbstractStatus.State.RUNNING.toString()));                return Optional.of(result);    } catch (Exception e) {                return Optional.empty();    }}
1
protected void chore()
{    try {        if (LOG.isTraceEnabled()) {            LOG.trace("Refreshing space quotas in RegionServer");        }        long start = System.nanoTime();        _chore();        if (metrics != null) {            metrics.incrementQuotaObserverTime((System.nanoTime() - start) / 1_000_000);        }    } catch (IOException e) {            }}
1
public static void chmod(String path, int mode) throws IOException
{    if (!Shell.WINDOWS) {        chmodImpl(path, mode);    } else {        try {            chmodImpl(path, mode);        } catch (NativeIOException nioe) {            if (nioe.getErrorCode() == 3) {                throw new NativeIOException("No such file or directory", Errno.ENOENT);            } else {                                throw new NativeIOException("Unknown error", Errno.UNKNOWN);            }        }    }}
1
public HybridInstance createHybridInstance(String hybridName, String projectName, String modelName, String[] cubeNames)
{    aclEvaluate.checkProjectWritePermission(projectName);    List<String> args = new ArrayList<String>();    args.add("-name");    args.add(hybridName);    args.add("-project");    args.add(projectName);    args.add("-model");    args.add(modelName);    args.add("-cubes");    args.add(StringUtils.join(cubeNames, ","));    args.add("-action");    args.add("create");    try {        HybridCubeCLI.main(args.toArray(new String[args.size()]));    } catch (Exception e) {                throw e;    }    return getHybridInstance(hybridName);}
1
public Answer deleteVolume(DeleteCommand cmd)
{        DataTO data = cmd.getData();    VolumeObjectTO volume = (VolumeObjectTO) data;    try {        String poolUuid = data.getDataStore().getUuid();        String uuid = volume.getUuid();        String path = getVirtualDiskPath(uuid, poolUuid);        StoragePlugin sp = new StoragePlugin(c);        sp.storagePluginDestroy(poolUuid, path);            } catch (Ovm3ResourceException e) {                return new CreateObjectAnswer(e.toString());    }    return new Answer(cmd);}
1
public static void beforeTest() throws Exception
{    hiveConf = new HiveConf();    hiveConf.setBoolVar(ConfVars.HIVE_SERVER2_SUPPORT_DYNAMIC_SERVICE_DISCOVERY, true);    hiveConf.setIntVar(ConfVars.HIVE_ZOOKEEPER_CONNECTION_MAX_RETRIES, 0);    hiveConf.setTimeVar(ConfVars.HIVE_ZOOKEEPER_CONNECTION_BASESLEEPTIME, 0, TimeUnit.MILLISECONDS);    miniHS2 = new MiniHS2(hiveConf);    Map<String, String> confOverlay = new HashMap<String, String>();    try {        miniHS2.start(confOverlay);    } catch (Exception ex) {                miniHS2.setStarted(true);    }}
1
public void prepareAndExecuteStatement()
{    int size = queue.size();    if (size > 0) {        List<Tuple> inputs = new ArrayList<>(size);        queue.drainTo(inputs);        try {            List<PairStatementTuple> psl = buildStatement(inputs);            int sinceLastModified = updateAndGetSecondsSinceLastModified();                        checkTimeElapsedSinceLastExec(sinceLastModified);            GroupingBatchBuilder batchBuilder = new GroupingBatchBuilder(cassandraConf.getBatchSizeRows(), psl);            int batchSize = 0;            for (PairBatchStatementTuples batch : batchBuilder) {                                getAsyncExecutor().execAsync(batch.getStatement(), batch.getInputs());                batchSize++;            }            int pending = getAsyncExecutor().getPendingTasksSize();            if (pending > batchSize) {                            }        } catch (Throwable r) {                        getAsyncHandler().failure(r, inputs);        }    }}
1
public void run()
{    while (monitorRunning) {        try {            blockStorageMovementReportedItemsCheck();            blocksStorageMovementUnReportedItemsCheck();            Thread.sleep(minCheckTimeout);        } catch (InterruptedException ie) {                    } catch (IOException ie) {                    }    }}
1
public static synchronized DatanodeDetails readDatanodeDetailsFrom(File path) throws IOException
{    if (!path.exists()) {        throw new IOException("Datanode ID file not found.");    }    try {        return DatanodeIdYaml.readDatanodeIdFile(path);    } catch (IOException e) {                        try (FileInputStream in = new FileInputStream(path)) {            return DatanodeDetails.getFromProtoBuf(HddsProtos.DatanodeDetailsProto.parseFrom(in));        } catch (IOException io) {            throw new IOException("Failed to parse DatanodeDetails from " + path.getAbsolutePath(), io);        }    }}
1
protected void printDestinations() throws Exception
{    ActiveMQDestination[] destinations = brokerService.getRegionBroker().getDestinations();    for (ActiveMQDestination destination : destinations) {            }}
1
protected ExecuteResult doWork(ExecutableContext context) throws ExecuteException
{    final CubeManager mgr = CubeManager.getInstance(context.getConfig());    final CubeInstance cube = mgr.getCube(CubingExecutableUtil.getCubeName(this.getParams()));    final CubeSegment optimizeSegment = cube.getSegmentById(CubingExecutableUtil.getSegmentId(this.getParams()));    CubeSegment oldSegment = optimizeSegment.getCubeInstance().getOriginalSegmentToOptimize(optimizeSegment);    Preconditions.checkNotNull(oldSegment, "cannot find the original segment to be optimized by " + optimizeSegment);    KylinConfig kylinConf = cube.getConfig();    Configuration conf = HadoopUtil.getCurrentConfiguration();    ResourceStore rs = ResourceStore.getStore(kylinConf);    int averageSamplingPercentage = 0;    try {                Path statisticsDirPath = new Path(CubingExecutableUtil.getStatisticsPath(this.getParams()));        FileSystem hdfs = FileSystem.get(conf);        if (!hdfs.exists(statisticsDirPath)) {            throw new IOException("StatisticsFilePath " + statisticsDirPath + " does not exists");        }        if (!hdfs.isDirectory(statisticsDirPath)) {            throw new IOException("StatisticsFilePath " + statisticsDirPath + " is not a directory");        }        Path[] statisticsFiles = HadoopUtil.getFilteredPath(hdfs, statisticsDirPath, BatchConstants.CFG_OUTPUT_STATISTICS);        if (statisticsFiles == null) {            throw new IOException("fail to find the statistics file in base dir: " + statisticsDirPath);        }        for (Path item : statisticsFiles) {            CubeStatsReader optimizeSegmentStatsReader = new CubeStatsReader(optimizeSegment, null, optimizeSegment.getConfig(), item);            averageSamplingPercentage += optimizeSegmentStatsReader.getSamplingPercentage();            addFromCubeStatsReader(optimizeSegmentStatsReader);        }                CubeStatsReader oldSegmentStatsReader = new CubeStatsReader(oldSegment, null, oldSegment.getConfig());        averageSamplingPercentage += oldSegmentStatsReader.getSamplingPercentage();        addFromCubeStatsReader(oldSegmentStatsReader);                        averageSamplingPercentage = averageSamplingPercentage / 2;        Set<Long> cuboidsRecommend = cube.getCuboidsRecommend();        Map<Long, HLLCounter> resultCuboidHLLMap = Maps.newHashMapWithExpectedSize(cuboidsRecommend.size());        for (Long cuboid : cuboidsRecommend) {            HLLCounter hll = cuboidHLLMap.get(cuboid);            if (hll == null) {                            } else {                resultCuboidHLLMap.put(cuboid, hll);            }        }        String resultDir = CubingExecutableUtil.getMergedStatisticsPath(this.getParams());        CubeStatsWriter.writeCuboidStatistics(conf, new Path(resultDir), resultCuboidHLLMap, averageSamplingPercentage, oldSegmentStatsReader.getSourceRowCount());        try (FSDataInputStream mergedStats = hdfs.open(new Path(resultDir, BatchConstants.CFG_STATISTICS_CUBOID_ESTIMATION_FILENAME))) {                        String statisticsFileName = optimizeSegment.getStatisticsResourcePath();            rs.putResource(statisticsFileName, mergedStats, System.currentTimeMillis());        }                CubingJob cubingJob = (CubingJob) getManager().getJob(CubingExecutableUtil.getCubingJobId(this.getParams()));        StatisticsDecisionUtil.decideCubingAlgorithm(cubingJob, optimizeSegment);        return new ExecuteResult();    } catch (IOException e) {                return ExecuteResult.createError(e);    }}
1
protected ScoreDoc[] transformToNativeShardDoc(List<NamedList<Object>> documents, Sort groupSort, String shard, IndexSchema schema)
{    ScoreDoc[] scoreDocs = new ScoreDoc[documents.size()];    int j = 0;    for (NamedList<Object> document : documents) {        Object docId = document.get(ID);        if (docId != null) {            docId = docId.toString();        } else {                    }        Float score = (Float) document.get("score");        if (score == null) {            score = Float.NaN;        }        Object[] sortValues = null;        Object sortValuesVal = document.get("sortValues");        if (sortValuesVal != null) {            sortValues = ((List) sortValuesVal).toArray();            for (int k = 0; k < sortValues.length; k++) {                SchemaField field = groupSort.getSort()[k].getField() != null ? schema.getFieldOrNull(groupSort.getSort()[k].getField()) : null;                sortValues[k] = ShardResultTransformerUtils.unmarshalSortValue(sortValues[k], field);            }        } else {                    }        scoreDocs[j++] = new ShardDoc(score, sortValues, docId, shard);    }    return scoreDocs;}
1
public void applyClusterState(ClusterChangedEvent event)
{    try {        RepositoriesMetaData oldMetaData = event.previousState().getMetaData().custom(RepositoriesMetaData.TYPE);        RepositoriesMetaData newMetaData = event.state().getMetaData().custom(RepositoriesMetaData.TYPE);                if ((oldMetaData == null && newMetaData == null) || (oldMetaData != null && oldMetaData.equals(newMetaData))) {            return;        }        logger.trace("processing new index repositories for state version [{}]", event.state().version());        Map<String, Repository> survivors = new HashMap<>();                for (Map.Entry<String, Repository> entry : repositories.entrySet()) {            if (newMetaData == null || newMetaData.repository(entry.getKey()) == null) {                                closeRepository(entry.getValue());            } else {                survivors.put(entry.getKey(), entry.getValue());            }        }        Map<String, Repository> builder = new HashMap<>();        if (newMetaData != null) {                        for (RepositoryMetaData repositoryMetaData : newMetaData.repositories()) {                Repository repository = survivors.get(repositoryMetaData.name());                if (repository != null) {                                        RepositoryMetaData previousMetadata = repository.getMetadata();                    if (previousMetadata.type().equals(repositoryMetaData.type()) == false || previousMetadata.settings().equals(repositoryMetaData.settings()) == false) {                                                                        closeRepository(repository);                        repository = null;                        try {                            repository = createRepository(repositoryMetaData, typesRegistry);                        } catch (RepositoryException ex) {                                                                                                            }                    }                } else {                    try {                        repository = createRepository(repositoryMetaData, typesRegistry);                    } catch (RepositoryException ex) {                                            }                }                if (repository != null) {                                        builder.put(repositoryMetaData.name(), repository);                }            }        }        repositories = Collections.unmodifiableMap(builder);    } catch (Exception ex) {            }}
1
private boolean calculateIsStaleConfigs(ServiceComponentHost sch, Map<String, DesiredConfig> desiredConfigs, HostComponentDesiredStateEntity hostComponentDesiredStateEntity) throws AmbariException
{    if (sch.isRestartRequired(hostComponentDesiredStateEntity)) {        return true;    }    Map<String, HostConfig> actual = sch.getActualConfigs();    if (null == actual || actual.isEmpty()) {        return false;    }    Cluster cluster = clusters.getClusterById(sch.getClusterId());    Map<String, Map<String, String>> desired = getEffectiveDesiredTags(cluster, sch.getHostName(), desiredConfigs);    Boolean stale = null;    int staleHash = 0;    if (STALE_CONFIGS_CACHE_ENABLED) {        staleHash = Objects.hashCode(actual.hashCode(), desired.hashCode(), sch.getHostName(), sch.getServiceComponentName(), sch.getServiceName());        stale = staleConfigsCache.getIfPresent(staleHash);        if (stale != null) {            return stale;        }    }    stale = false;    StackId stackId = sch.getServiceComponent().getDesiredStackId();    StackInfo stackInfo = ambariMetaInfo.getStack(stackId);    ServiceInfo serviceInfo = ambariMetaInfo.getService(stackId.getStackName(), stackId.getStackVersion(), sch.getServiceName());    ComponentInfo componentInfo = serviceInfo.getComponentByName(sch.getServiceComponentName());                                    Iterator<Entry<String, Map<String, String>>> it = desired.entrySet().iterator();    List<String> changedProperties = new LinkedList<>();    while (it.hasNext()) {        boolean staleEntry = false;        Entry<String, Map<String, String>> desiredEntry = it.next();        String type = desiredEntry.getKey();        Map<String, String> tags = desiredEntry.getValue();        if (!actual.containsKey(type)) {                        staleEntry = (serviceInfo.hasConfigDependency(type) || componentInfo.hasConfigType(type));        } else {                        HostConfig hc = actual.get(type);            Map<String, String> actualTags = buildTags(hc);            if (!isTagChanged(tags, actualTags, hasGroupSpecificConfigsForType(cluster, sch.getHostName(), type))) {                staleEntry = false;            } else {                staleEntry = (serviceInfo.hasConfigDependency(type) || componentInfo.hasConfigType(type));                if (staleEntry) {                    Collection<String> changedKeys = findChangedKeys(cluster, type, tags.values(), actualTags.values());                    changedProperties.addAll(changedKeys);                }            }        }        stale = stale | staleEntry;    }    String refreshCommand = calculateRefreshCommand(stackInfo.getRefreshCommandConfiguration(), sch, changedProperties);    if (STALE_CONFIGS_CACHE_ENABLED) {        staleConfigsCache.put(staleHash, stale);        if (refreshCommand != null) {            refreshConfigCommandCache.put(staleHash, refreshCommand);        }    }        if (LOG.isDebugEnabled()) {                for (String p : changedProperties) {                    }    }    return stale;}
1
public void importContainerData(InputStream input, ContainerPacker<KeyValueContainerData> packer) throws IOException
{    writeLock();    try {        if (getContainerFile().exists()) {            String errorMessage = String.format("Can't import container (cid=%d) data to a specific location" + " as the container descriptor (%s) has already been exist.", getContainerData().getContainerID(), getContainerFile().getAbsolutePath());            throw new IOException(errorMessage);        }                        byte[] descriptorContent = packer.unpackContainerData(this, input);        Preconditions.checkNotNull(descriptorContent, "Container descriptor is missing from the container archive: " + getContainerData().getContainerID());                                KeyValueContainerData originalContainerData = (KeyValueContainerData) ContainerDataYaml.readContainer(descriptorContent);        containerData.setState(originalContainerData.getState());        containerData.setContainerDBType(originalContainerData.getContainerDBType());        containerData.setBytesUsed(originalContainerData.getBytesUsed());                update(originalContainerData.getMetadata(), true);                KeyValueContainerUtil.parseKVContainerData(containerData, config);    } catch (Exception ex) {                try {            FileUtils.deleteDirectory(new File(containerData.getMetadataPath()));            FileUtils.deleteDirectory(new File(containerData.getChunksPath()));            FileUtils.deleteDirectory(getContainerFile());        } catch (Exception deleteex) {                    }        throw ex;    } finally {        writeUnlock();    }}
1
public void createNifiKeystoresAndTrustStores(StandaloneConfig standaloneConfig) throws GeneralSecurityException, IOException
{        File baseDir = standaloneConfig.getBaseDir();    if (!baseDir.exists() && !baseDir.mkdirs()) {        throw new IOException(baseDir + " doesn't exist and unable to create it.");    }    if (!baseDir.isDirectory()) {        throw new IOException("Expected directory to output to");    }    String signingAlgorithm = standaloneConfig.getSigningAlgorithm();    int days = standaloneConfig.getDays();    String keyPairAlgorithm = standaloneConfig.getKeyPairAlgorithm();    int keySize = standaloneConfig.getKeySize();    File nifiCert = new File(baseDir, NIFI_CERT + ".pem");    File nifiKey = new File(baseDir, NIFI_KEY + ".key");    X509Certificate certificate;    KeyPair caKeyPair;    if (logger.isInfoEnabled()) {            }    if (nifiCert.exists()) {        if (!nifiKey.exists()) {            throw new IOException(nifiCert + " exists already, but " + nifiKey + " does not, we need both certificate and key to continue with an existing CA.");        }        try (FileReader pemEncodedCertificate = new FileReader(nifiCert)) {            certificate = TlsHelper.parseCertificate(pemEncodedCertificate);        }        try (FileReader pemEncodedKeyPair = new FileReader(nifiKey)) {            caKeyPair = TlsHelper.parseKeyPairFromReader(pemEncodedKeyPair);        }                        List<X509Certificate> signingCertificates = new ArrayList<>();                if (!StringUtils.isBlank(standaloneConfig.getAdditionalCACertificate())) {            X509Certificate signingCertificate;            final File additionalCACertFile = new File(standaloneConfig.getAdditionalCACertificate());            if (!additionalCACertFile.exists()) {                throw new IOException("The additional CA certificate does not exist at " + additionalCACertFile.getAbsolutePath());            }            try (FileReader pemEncodedCACertificate = new FileReader(additionalCACertFile)) {                signingCertificate = TlsHelper.parseCertificate(pemEncodedCACertificate);            }            signingCertificates.add(signingCertificate);        }                signingCertificates.add(certificate);        boolean signatureValid = TlsHelper.verifyCertificateSignature(certificate, signingCertificates);        if (!signatureValid) {            throw new SignatureException("The signing certificate was not signed by any known certificates");        }        if (!caKeyPair.getPublic().equals(certificate.getPublicKey())) {            throw new IOException("Expected " + nifiKey + " to correspond to CA certificate at " + nifiCert);        }        if (logger.isInfoEnabled()) {                    }    } else if (nifiKey.exists()) {        throw new IOException(nifiKey + " exists already, but " + nifiCert + " does not, we need both certificate and key to continue with an existing CA.");    } else {        TlsCertificateAuthorityManager tlsCertificateAuthorityManager = new TlsCertificateAuthorityManager(standaloneConfig);        KeyStore.PrivateKeyEntry privateKeyEntry = tlsCertificateAuthorityManager.getOrGenerateCertificateAuthority();        certificate = (X509Certificate) privateKeyEntry.getCertificateChain()[0];        caKeyPair = new KeyPair(certificate.getPublicKey(), privateKeyEntry.getPrivateKey());        try (PemWriter pemWriter = new PemWriter(new OutputStreamWriter(outputStreamFactory.create(nifiCert)))) {            pemWriter.writeObject(new JcaMiscPEMGenerator(certificate));        }        try (PemWriter pemWriter = new PemWriter(new OutputStreamWriter(outputStreamFactory.create(nifiKey)))) {            pemWriter.writeObject(new JcaMiscPEMGenerator(caKeyPair));        }        if (logger.isInfoEnabled()) {                    }    }    NiFiPropertiesWriterFactory niFiPropertiesWriterFactory = standaloneConfig.getNiFiPropertiesWriterFactory();    boolean overwrite = standaloneConfig.isOverwrite();    List<InstanceDefinition> instanceDefinitions = standaloneConfig.getInstanceDefinitions();    if (instanceDefinitions.isEmpty() && logger.isInfoEnabled()) {            }    List<String> domainAlternativeNames = standaloneConfig.getDomainAlternativeNames();    for (Integer instanceIndex : IntStream.range(0, instanceDefinitions.size()).boxed().collect(Collectors.toList())) {        InstanceDefinition instanceDefinition = instanceDefinitions.get(instanceIndex);        String hostname = instanceDefinition.getHostname();        File hostDir;        int hostIdentifierNumber = instanceDefinition.getInstanceIdentifier().getNumber();        if (hostIdentifierNumber == 1) {            hostDir = new File(baseDir, hostname);        } else {            hostDir = new File(baseDir, hostname + "_" + hostIdentifierNumber);        }        TlsClientConfig tlsClientConfig = new TlsClientConfig(standaloneConfig);        File keystore = new File(hostDir, "keystore." + tlsClientConfig.getKeyStoreType().toLowerCase());        File truststore = new File(hostDir, "truststore." + tlsClientConfig.getTrustStoreType().toLowerCase());                if (domainAlternativeNames.size() == 1) {            tlsClientConfig.setDomainAlternativeNames(Collections.singletonList(domainAlternativeNames.get(0)));        } else if (domainAlternativeNames.size() == instanceDefinitions.size()) {            tlsClientConfig.setDomainAlternativeNames(Collections.singletonList(domainAlternativeNames.get(instanceIndex)));                    } else if (domainAlternativeNames.size() > 0) {                    }        if (hostDir.exists()) {            if (!hostDir.isDirectory()) {                throw new IOException(hostDir + " exists but is not a directory.");            } else if (overwrite) {                if (logger.isInfoEnabled()) {                                    }                keystore.delete();                if (keystore.exists()) {                    throw new IOException("Keystore " + keystore + " already exists and couldn't be deleted.");                }                truststore.delete();                if (truststore.exists()) {                    throw new IOException("Truststore " + truststore + " already exists and couldn't be deleted.");                }            } else {                throw new IOException(hostDir + " exists and overwrite is not set.");            }        } else if (!hostDir.mkdirs()) {            throw new IOException("Unable to make directory: " + hostDir.getAbsolutePath());        } else if (logger.isInfoEnabled()) {                    }        tlsClientConfig.setKeyStore(keystore.getAbsolutePath());        tlsClientConfig.setKeyStorePassword(instanceDefinition.getKeyStorePassword());        tlsClientConfig.setKeyPassword(instanceDefinition.getKeyPassword());        tlsClientConfig.setTrustStore(truststore.getAbsolutePath());        tlsClientConfig.setTrustStorePassword(instanceDefinition.getTrustStorePassword());        TlsClientManager tlsClientManager = new TlsClientManager(tlsClientConfig);        KeyPair keyPair = TlsHelper.generateKeyPair(keyPairAlgorithm, keySize);        Extensions sanDnsExtensions = TlsHelper.createDomainAlternativeNamesExtensions(tlsClientConfig.getDomainAlternativeNames(), tlsClientConfig.calcDefaultDn(hostname));        tlsClientManager.addPrivateKeyToKeyStore(keyPair, NIFI_KEY, CertificateUtils.generateIssuedCertificate(tlsClientConfig.calcDefaultDn(hostname), keyPair.getPublic(), sanDnsExtensions, certificate, caKeyPair, signingAlgorithm, days), certificate);        tlsClientManager.setCertificateEntry(NIFI_CERT, certificate);        tlsClientManager.addClientConfigurationWriter(new NifiPropertiesTlsClientConfigWriter(niFiPropertiesWriterFactory, new File(hostDir, "nifi.properties"), hostname, instanceDefinition.getNumber()));        tlsClientManager.write(outputStreamFactory);        if (logger.isInfoEnabled()) {                    }    }    List<String> clientDns = standaloneConfig.getClientDns();    if (standaloneConfig.getClientDns().isEmpty() && logger.isInfoEnabled()) {            }    List<String> clientPasswords = standaloneConfig.getClientPasswords();    for (int i = 0; i < clientDns.size(); i++) {        String reorderedDn = CertificateUtils.reorderDn(clientDns.get(i));        String clientDnFile = TlsHelper.escapeFilename(reorderedDn);        File clientCertFile = new File(baseDir, clientDnFile + ".p12");        if (clientCertFile.exists()) {            if (overwrite) {                if (logger.isInfoEnabled()) {                                    }            } else {                throw new IOException(clientCertFile + " exists and overwrite is not set.");            }        } else if (logger.isInfoEnabled()) {                    }        KeyPair keyPair = TlsHelper.generateKeyPair(keyPairAlgorithm, keySize);        X509Certificate clientCert = CertificateUtils.generateIssuedCertificate(reorderedDn, keyPair.getPublic(), null, certificate, caKeyPair, signingAlgorithm, days);        KeyStore keyStore = KeyStoreUtils.getKeyStore(KeystoreType.PKCS12.toString());        keyStore.load(null, null);        keyStore.setKeyEntry(NIFI_KEY, keyPair.getPrivate(), null, new Certificate[] { clientCert, certificate });        String password = TlsHelper.writeKeyStore(keyStore, outputStreamFactory, clientCertFile, clientPasswords.get(i), standaloneConfig.isClientPasswordsGenerated());        try (FileWriter fileWriter = new FileWriter(new File(baseDir, clientDnFile + ".password"))) {            fileWriter.write(password);        }        if (logger.isInfoEnabled()) {                    }    }    if (logger.isInfoEnabled()) {            }}
1
private void close()
{    done = true;    if (null != serverSocket) {                serverSocket.close();    }}
1
public void doTestFailoverConsumerAckLost(final int pauseSeconds) throws Exception
{    broker = createBroker(true);    setDefaultPersistenceAdapter(broker);    broker.setPlugins(new BrokerPlugin[] { new BrokerPluginSupport() {                @Override        public void acknowledge(ConsumerBrokerExchange consumerExchange, final MessageAck ack) throws Exception {            consumerExchange.getConnectionContext().setDontSendReponse(true);            Executors.newSingleThreadExecutor().execute(new Runnable() {                public void run() {                                        try {                        broker.stop();                    } catch (Exception e) {                        e.printStackTrace();                    }                }            });        }    } });    broker.start();    Vector<Connection> connections = new Vector<Connection>();    ActiveMQConnectionFactory cf = new ActiveMQConnectionFactory("failover:(" + url + ")");    configureConnectionFactory(cf);    Connection connection = cf.createConnection();    connection.start();    connections.add(connection);    final Session producerSession = connection.createSession(false, Session.AUTO_ACKNOWLEDGE);    final Queue destination = producerSession.createQueue(QUEUE_NAME + "?consumer.prefetchSize=1");    connection = cf.createConnection();    connection.start();    connections.add(connection);    final Session consumerSession1 = connection.createSession(true, Session.SESSION_TRANSACTED);    connection = cf.createConnection();    connection.start();    connections.add(connection);    final Session consumerSession2 = connection.createSession(true, Session.SESSION_TRANSACTED);    final MessageConsumer consumer1 = consumerSession1.createConsumer(destination);    final MessageConsumer consumer2 = consumerSession2.createConsumer(destination);    produceMessage(producerSession, destination);    produceMessage(producerSession, destination);    final Vector<Message> receivedMessages = new Vector<Message>();    final CountDownLatch commitDoneLatch = new CountDownLatch(1);    final AtomicBoolean gotTransactionRolledBackException = new AtomicBoolean(false);    Executors.newSingleThreadExecutor().execute(new Runnable() {        public void run() {                        try {                Message msg = consumer1.receive(20000);                                receivedMessages.add(msg);                                TimeUnit.SECONDS.sleep(random.nextInt(5));                                                                                                                                msg = consumer1.receive(5000);                                if (msg != null) {                    receivedMessages.add(msg);                }                                try {                    consumerSession1.commit();                } catch (TransactionRolledBackException expected) {                                        gotTransactionRolledBackException.set(true);                                }                commitDoneLatch.countDown();                            } catch (Exception e) {                e.printStackTrace();            }        }    });        broker.waitUntilStopped();    broker = createBroker(false, url);    setDefaultPersistenceAdapter(broker);    broker.start();    assertTrue("tx committed through failover", commitDoneLatch.await(30, TimeUnit.SECONDS));            for (int i = 0; i < 2; i++) {        Message msg = consumer1.receive(5000);                consumerSession1.commit();        if (msg == null) {            msg = consumer2.receive(10000);                        consumerSession2.commit();        }        assertNotNull("got message [" + i + "]", msg);    }    for (Connection c : connections) {        c.close();    }        broker.stop();    broker.waitUntilStopped();        broker = createBroker(false, url);    setDefaultPersistenceAdapter(broker);    broker.start();        cf = new ActiveMQConnectionFactory("failover:(" + url + ")");    configureConnectionFactory(cf);    connection = cf.createConnection();    connection.start();    Session sweeperSession = connection.createSession(false, Session.AUTO_ACKNOWLEDGE);    MessageConsumer sweeper = sweeperSession.createConsumer(destination);    Message msg = sweeper.receive(1000);        assertNull("no messges left dangling but got: " + msg, msg);    connection.close();}
1
public AggrStats getAggrColStatsFor(String catName, String dbName, String tblName, List<String> colNames, List<String> partNames, String engine, String writeIdList) throws NoSuchObjectException, MetaException, TException
{    if (colNames.isEmpty() || partNames.isEmpty()) {                        return new AggrStats(new ArrayList<>(), 0);    }    PartitionsStatsRequest req = new PartitionsStatsRequest(dbName, tblName, colNames, partNames, engine);    req.setCatName(catName);    req.setValidWriteIdList(writeIdList);    return client.get_aggr_stats_for(req);}
1
public void execute(FunctionContext functionContext)
{    try {        InternalCache cache = ((InternalCache) functionContext.getCache()).getCacheForProcessingClientRequests();        DataCommandRequest request = (DataCommandRequest) functionContext.getArguments();        if (logger.isDebugEnabled()) {                    }        DataCommandResult result = null;        if (request.isGet()) {            result = get(request, cache);        } else if (request.isLocateEntry()) {            result = locateEntry(request, cache);        } else if (request.isPut()) {            result = put(request, cache);        } else if (request.isRemove()) {            result = remove(request, cache);        } else if (request.isSelect()) {            result = select(request, cache);        }        if (logger.isDebugEnabled()) {                    }        functionContext.getResultSender().lastResult(result);    } catch (Exception e) {                functionContext.getResultSender().sendException(e);    }}
1
public synchronized String toString(String charsetName)
{    if (availableByteCount > 0) {        try {            if (Charset.isSupported(charsetName)) {                return toString(Charset.forName(charsetName));            }                    } catch (Exception charsetEx) {                    }        return toString(MllpComponent.getDefaultCharset());    }    return "";}
1
public synchronized boolean disablePolicy(String name)
{    ErasureCodingPolicyInfo info = policiesByName.get(name);    if (info == null) {        throw new HadoopIllegalArgumentException("The policy name " + name + " does not exist");    }    if (enabledPoliciesByName.containsKey(name)) {        enabledPoliciesByName.remove(name);        enabledPolicies = enabledPoliciesByName.values().toArray(new ErasureCodingPolicy[0]);        info.setState(ErasureCodingPolicyState.DISABLED);                allPersistedPolicies.put(info.getPolicy().getId(), createPolicyInfo(info.getPolicy(), ErasureCodingPolicyState.DISABLED));        return true;    }    return false;}
1
private void publishMessage(TypedModeledFramework2<T, Group, Priority> typedClient, Group group, T message)
{    ModeledFramework<T> resolvedClient = typedClient.resolved(client, group, message.getPriority());    resolvedClient.set(message).exceptionally(e -> {                return null;    });}
1
public void receiveEnvelopedMessageTest() throws Exception
{    AS2ClientConnection clientConnection = new AS2ClientConnection(AS2_VERSION, USER_AGENT, CLIENT_FQDN, TARGET_HOST, TARGET_PORT);    AS2ClientManager clientManager = new AS2ClientManager(clientConnection);    clientManager.send(EDI_MESSAGE, REQUEST_URI, SUBJECT, FROM, AS2_NAME, AS2_NAME, AS2MessageStructure.ENCRYPTED, ContentType.create(AS2MediaType.APPLICATION_EDIFACT, AS2Charset.US_ASCII), null, null, null, null, null, DISPOSITION_NOTIFICATION_TO, SIGNED_RECEIPT_MIC_ALGORITHMS, AS2EncryptionAlgorithm.AES128_CBC, certList.toArray(new Certificate[0]));    MockEndpoint mockEndpoint = getMockEndpoint("mock:as2RcvMsgs");    mockEndpoint.expectedMinimumMessageCount(1);    mockEndpoint.setResultWaitTime(TimeUnit.MILLISECONDS.convert(30, TimeUnit.SECONDS));    mockEndpoint.assertIsSatisfied();    final List<Exchange> exchanges = mockEndpoint.getExchanges();    assertNotNull("listen result", exchanges);    assertFalse("listen result", exchanges.isEmpty());        Exchange exchange = exchanges.get(0);    Message message = exchange.getIn();    assertNotNull("exchange message", message);    BasicHttpContext context = message.getBody(BasicHttpContext.class);    assertNotNull("context", context);    HttpCoreContext coreContext = HttpCoreContext.adapt(context);    HttpRequest request = coreContext.getRequest();    assertNotNull("request", request);    assertEquals("Unexpected method value", METHOD, request.getRequestLine().getMethod());    assertEquals("Unexpected request URI value", REQUEST_URI, request.getRequestLine().getUri());    assertEquals("Unexpected HTTP version value", HttpVersion.HTTP_1_1, request.getRequestLine().getProtocolVersion());    assertEquals("Unexpected subject value", SUBJECT, request.getFirstHeader(AS2Header.SUBJECT).getValue());    assertEquals("Unexpected from value", FROM, request.getFirstHeader(AS2Header.FROM).getValue());    assertEquals("Unexpected AS2 version value", AS2_VERSION, request.getFirstHeader(AS2Header.AS2_VERSION).getValue());    assertEquals("Unexpected AS2 from value", AS2_NAME, request.getFirstHeader(AS2Header.AS2_FROM).getValue());    assertEquals("Unexpected AS2 to value", AS2_NAME, request.getFirstHeader(AS2Header.AS2_TO).getValue());    assertTrue("Unexpected message id value", request.getFirstHeader(AS2Header.MESSAGE_ID).getValue().endsWith(CLIENT_FQDN + ">"));    assertEquals("Unexpected target host value", TARGET_HOST + ":" + TARGET_PORT, request.getFirstHeader(AS2Header.TARGET_HOST).getValue());    assertEquals("Unexpected user agent value", USER_AGENT, request.getFirstHeader(AS2Header.USER_AGENT).getValue());    assertNotNull("Date value missing", request.getFirstHeader(AS2Header.DATE));    assertNotNull("Content length value missing", request.getFirstHeader(AS2Header.CONTENT_LENGTH));    assertTrue("Unexpected content type for message", request.getFirstHeader(AS2Header.CONTENT_TYPE).getValue().startsWith(AS2MimeType.APPLICATION_PKCS7_MIME));    assertTrue("Request does not contain entity", request instanceof BasicHttpEntityEnclosingRequest);    HttpEntity entity = ((BasicHttpEntityEnclosingRequest) request).getEntity();    assertNotNull("Request does not contain entity", entity);    assertTrue("Unexpected request entity type", entity instanceof ApplicationPkcs7MimeEnvelopedDataEntity);    ApplicationPkcs7MimeEnvelopedDataEntity envelopedEntity = (ApplicationPkcs7MimeEnvelopedDataEntity) entity;    assertTrue("Entity not set as main body of request", envelopedEntity.isMainBody());        MimeEntity encryptedEntity = envelopedEntity.getEncryptedEntity(signingKP.getPrivate());    assertTrue("Enveloped mime part incorrect type ", encryptedEntity instanceof ApplicationEDIFACTEntity);    ApplicationEDIFACTEntity ediEntity = (ApplicationEDIFACTEntity) encryptedEntity;    assertTrue("Unexpected content type for enveloped mime part", ediEntity.getContentType().getValue().startsWith(AS2MediaType.APPLICATION_EDIFACT));    assertFalse("Enveloped mime type set as main body of request", ediEntity.isMainBody());    assertEquals("Unexpected content for enveloped mime part", EDI_MESSAGE.replaceAll("[\n\r]", ""), ediEntity.getEdiMessage().replaceAll("[\n\r]", ""));}
1
public void saveSourceCheckpoint(String cubeName, String segmentName, int rsID, String sourceCheckpoint)
{    try {        String path = ZKPaths.makePath(cubeRoot, cubeName, CUBE_SRC_CHECKPOINT, segmentName, String.valueOf(rsID));        if (client.checkExists().forPath(path) == null) {            client.create().creatingParentsIfNeeded().forPath(path);        } else {                    }        client.setData().forPath(path, Bytes.toBytes(sourceCheckpoint));    } catch (Exception e) {                throw new StoreException(e);    }}
1
public void testWaitForStateAfterShutDown() throws Exception
{    final String col_name = "test_col";    final MiniSolrCloudCluster cluster = new MiniSolrCloudCluster(1, createTempDir(), buildJettyConfig("/solr"));    try {                CollectionAdminRequest.createCollection(col_name, "_default", 1, 1).process(cluster.getSolrClient());                cluster.getSolrClient().waitForState(col_name, 30, TimeUnit.SECONDS, clusterShape(1, 1));                final JettySolrRunner nodeToStop = cluster.getJettySolrRunner(0);        nodeToStop.stop();                cluster.waitForJettyToStop(nodeToStop);                                cluster.getSolrClient().waitForState(col_name, 500, TimeUnit.MILLISECONDS, clusterShape(1, 0));    } finally {        cluster.shutdown();    }}
1
public void stop()
{    if (flushedSeqIdFlusher != null) {        flushedSeqIdFlusher.cancel();    }    if (persistFlushedSequenceId) {        try {            persistRegionLastFlushedSequenceIds();        } catch (IOException e) {                    }    }}
1
private void parseMpackDirectories()
{    try {        for (final File dirEntry : mpackStaging.listFiles()) {            if (dirEntry.isDirectory()) {                String mpackName = dirEntry.getName();                                if (!mpackName.equals(MPACK_TAR_LOCATION)) {                    for (final File file : dirEntry.listFiles()) {                        if (file.isDirectory()) {                            String mpackVersion = file.getName();                            List resultSet = mpackDAO.findByNameVersion(mpackName, mpackVersion);                            if (resultSet.size() > 0) {                                MpackEntity mpackEntity = (MpackEntity) resultSet.get(0);                                                                String mpackJsonContents = new String((Files.readAllBytes(Paths.get(file + "/" + MPACK_METADATA))), "UTF-8");                                Gson gson = new Gson();                                Mpack existingMpack = gson.fromJson(mpackJsonContents, Mpack.class);                                existingMpack.setResourceId(mpackEntity.getId());                                existingMpack.setMpackUri(mpackEntity.getMpackUri());                                existingMpack.setRegistryId(mpackEntity.getRegistryId());                                mpackMap.put(mpackEntity.getId(), existingMpack);                            }                        }                    }                }            }        }    } catch (IOException e) {        e.printStackTrace();    }}
1
public AsyncMethodCallback<WMDropResourcePlanResponse> getResultHandler(final AsyncFrameBuffer fb, final int seqid)
{    final org.apache.thrift.AsyncProcessFunction fcall = this;    return new AsyncMethodCallback<WMDropResourcePlanResponse>() {        public void onComplete(WMDropResourcePlanResponse o) {            drop_resource_plan_result result = new drop_resource_plan_result();            result.success = o;            try {                fcall.sendResponse(fb, result, org.apache.thrift.protocol.TMessageType.REPLY, seqid);                return;            } catch (Exception e) {                            }            fb.close();        }        public void onError(Exception e) {            byte msgType = org.apache.thrift.protocol.TMessageType.REPLY;            org.apache.thrift.TBase msg;            drop_resource_plan_result result = new drop_resource_plan_result();            if (e instanceof NoSuchObjectException) {                result.o1 = (NoSuchObjectException) e;                result.setO1IsSet(true);                msg = result;            } else if (e instanceof InvalidOperationException) {                result.o2 = (InvalidOperationException) e;                result.setO2IsSet(true);                msg = result;            } else if (e instanceof MetaException) {                result.o3 = (MetaException) e;                result.setO3IsSet(true);                msg = result;            } else {                msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;                msg = (org.apache.thrift.TBase) new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());            }            try {                fcall.sendResponse(fb, msg, msgType, seqid);                return;            } catch (Exception ex) {                            }            fb.close();        }    };}
1
public boolean interrupt()
{    if (savedBsh != null) {        try {                        savedBsh.evalNoLog("interrupt()");        } catch (JMeterException ignored) {            if (log.isDebugEnabled()) {                                            }        }        return true;    }    return false;}
1
public static ObjectName register(String serviceName, String nameName, Map<String, String> properties, Object theMbean)
{    final MBeanServer mbs = ManagementFactory.getPlatformMBeanServer();    Preconditions.checkNotNull(properties, "JMX bean properties should not be null for " + "bean registration.");    ObjectName name = getMBeanName(serviceName, nameName, properties);    if (name != null) {        try {            mbs.registerMBean(theMbean, name);                        return name;        } catch (InstanceAlreadyExistsException iaee) {            if (LOG.isTraceEnabled()) {                LOG.trace("Failed to register MBean \"" + name + "\"", iaee);            } else {                            }        } catch (Exception e) {                    }    }    return null;}
1
protected void removeReservationState(String planName, String reservationIdName) throws Exception
{    try {        WriteBatch batch = db.createWriteBatch();        try {            String reservationKey = getReservationNodeKey(planName, reservationIdName);            batch.delete(bytes(reservationKey));                        db.write(batch);        } finally {            batch.close();        }    } catch (DBException e) {        throw new IOException(e);    }}
1
public boolean waitForBridgeFormation(final BrokerService broker, final int min, final int bridgeIndex, long wait) throws Exception
{    boolean result = false;    if (!broker.getNetworkConnectors().isEmpty()) {        result = Wait.waitFor(new Wait.Condition() {            public boolean isSatisified() throws Exception {                int activeCount = 0;                for (NetworkBridge bridge : broker.getNetworkConnectors().get(bridgeIndex).activeBridges()) {                    if (bridge.getRemoteBrokerName() != null) {                                                activeCount++;                    }                }                return activeCount >= min;            }        }, wait);    }    return result;}
1
public boolean isSatisified() throws Exception
{            Thread[] threads = filterDaemonThreads(ThreadExplorer.listThreads());    threadCountAfterStop.set(threads.length);    return threadCountAfterStop.get() <= originalThreadCount;}
1
public MetricsComponent configureMetricsComponent() throws Exception
{    MetricsComponent component = new MetricsComponent();    component.setCamelContext(camelContext);    Map<String, Object> parameters = new HashMap<>();    IntrospectionSupport.getProperties(configuration, parameters, null, false);    for (Map.Entry<String, Object> entry : parameters.entrySet()) {        Object value = entry.getValue();        Class<?> paramClass = value.getClass();        if (paramClass.getName().endsWith("NestedConfiguration")) {            Class nestedClass = null;            try {                nestedClass = (Class) paramClass.getDeclaredField("CAMEL_NESTED_CLASS").get(null);                HashMap<String, Object> nestedParameters = new HashMap<>();                IntrospectionSupport.getProperties(value, nestedParameters, null, false);                Object nestedProperty = nestedClass.newInstance();                CamelPropertiesHelper.setCamelProperties(camelContext, nestedProperty, nestedParameters, false);                entry.setValue(nestedProperty);            } catch (NoSuchFieldException e) {            }        }    }    CamelPropertiesHelper.setCamelProperties(camelContext, component, parameters, false);    if (ObjectHelper.isNotEmpty(customizers)) {        for (ComponentCustomizer<MetricsComponent> customizer : customizers) {            boolean useCustomizer = (customizer instanceof HasId) ? HierarchicalPropertiesEvaluator.evaluate(applicationContext.getEnvironment(), "camel.component.customizer", "camel.component.metrics.customizer", ((HasId) customizer).getId()) : HierarchicalPropertiesEvaluator.evaluate(applicationContext.getEnvironment(), "camel.component.customizer", "camel.component.metrics.customizer");            if (useCustomizer) {                                customizer.customize(component);            }        }    }    return component;}
1
protected void publishAndConsume() throws Exception
{    createConsumers();    createProducers();    int counter = 0;    boolean resetCon = false;        for (int i = 0; i < messageCount; i++) {        if (resetCon) {            closeConsumers();            closeProducers();            createConsumers();            createProducers();            resetCon = false;        }        for (int k = 0; k < producers.length; k++) {            producers[k].sendMessage(payload, "counter", counter);            counter++;            if (resetConnection(counter)) {                resetCon = true;                break;            }        }    }}
1
public static void main(String[] args) throws Exception
{    Map<String, Object> cl = CLI.opt("l", "log-setting", null, new LogLevelsParser(LogLevelAction.UPDATE), CLI.INTO_MAP).opt("r", "remove-log-setting", null, new LogLevelsParser(LogLevelAction.REMOVE), CLI.INTO_MAP).arg("topologyName", CLI.FIRST_WINS).parse(args);    final String topologyName = (String) cl.get("topologyName");    final LogConfig logConfig = new LogConfig();    Map<String, LogLevel> logLevelMap = new HashMap<>();    Map<String, LogLevel> updateLogLevel = (Map<String, LogLevel>) cl.get("l");    if (null != updateLogLevel) {        logLevelMap.putAll(updateLogLevel);    }    Map<String, LogLevel> removeLogLevel = (Map<String, LogLevel>) cl.get("r");    if (null != removeLogLevel) {        logLevelMap.putAll(removeLogLevel);    }    for (Map.Entry<String, LogLevel> entry : logLevelMap.entrySet()) {        logConfig.put_to_named_logger_level(entry.getKey(), entry.getValue());    }    NimbusClient.withConfiguredClient(new NimbusClient.WithNimbus() {        @Override        public void run(Nimbus.Iface nimbus) throws Exception {            String topologyId = Utils.getTopologyId(topologyName, nimbus);            if (null == topologyId) {                throw new IllegalArgumentException(topologyName + " is not a running topology");            }            nimbus.setLogConfig(topologyId, logConfig);                    }    });}
1
private static void checkForDualNetworkCards(Set<String> componentHosts, NameService.NameNode nameNode)
{    if (!componentHosts.contains(nameNode.getHost())) {                    }}
1
public void exceptionCaught(ChannelHandlerContext ctx, ExceptionEvent e) throws Exception
{    Channel ch = e.getChannel();    Throwable cause = e.getCause();    if (cause instanceof TooLongFrameException) {        sendError(ctx, BAD_REQUEST);        return;    } else if (cause instanceof IOException) {        if (cause instanceof ClosedChannelException) {                        return;        }        String message = String.valueOf(cause.getMessage());        if (IGNORABLE_ERROR_MESSAGE.matcher(message).matches()) {                        return;        }    }        if (ch.isConnected()) {                sendError(ctx, INTERNAL_SERVER_ERROR);    }}
1
private void removeStandbyNameDirs()
{    for (int i = 1; i < maxNNCount; i++) {        for (URI u : cluster.getNameDirs(i)) {            assertTrue(u.getScheme().equals("file"));            File dir = new File(u.getPath());                        assertTrue(FileUtil.fullyDelete(dir));        }    }}
1
public void testHFileCleaning() throws Exception
{    final EnvironmentEdge originalEdge = EnvironmentEdgeManager.getDelegate();    String prefix = "someHFileThatWouldBeAUUID";    Configuration conf = UTIL.getConfiguration();        long ttl = 2000;    conf.set(HFileCleaner.MASTER_HFILE_CLEANER_PLUGINS, "org.apache.hadoop.hbase.master.cleaner.TimeToLiveHFileCleaner");    conf.setLong(TimeToLiveHFileCleaner.TTL_CONF_KEY, ttl);    Server server = new DummyServer();    Path archivedHfileDir = new Path(UTIL.getDataTestDirOnTestFS(), HConstants.HFILE_ARCHIVE_DIRECTORY);    FileSystem fs = FileSystem.get(conf);    HFileCleaner cleaner = new HFileCleaner(1000, server, conf, fs, archivedHfileDir, POOL);        final long createTime = System.currentTimeMillis();    fs.delete(archivedHfileDir, true);    fs.mkdirs(archivedHfileDir);        fs.createNewFile(new Path(archivedHfileDir, "dfd-dfd"));                for (int i = 1; i < 32; i++) {                        Path fileName = new Path(archivedHfileDir, (prefix + "." + (createTime + i)));        fs.createNewFile(fileName);                fs.setTimes(fileName, createTime - ttl - 1, -1);            }            Path saved = new Path(archivedHfileDir, prefix + ".00000000000");    fs.createNewFile(saved);        fs.setTimes(saved, createTime - ttl / 2, -1);        for (FileStatus stat : fs.listStatus(archivedHfileDir)) {            }    assertEquals(33, fs.listStatus(archivedHfileDir).length);        EnvironmentEdge setTime = new EnvironmentEdge() {        @Override        public long currentTime() {            return createTime;        }    };    EnvironmentEdgeManager.injectEdge(setTime);        cleaner.chore();        assertEquals(1, fs.listStatus(archivedHfileDir).length);    for (FileStatus file : fs.listStatus(archivedHfileDir)) {            }        EnvironmentEdgeManager.injectEdge(originalEdge);}
1
public void onComplete(PartitionsByExprResult o)
{    get_partitions_by_expr_result result = new get_partitions_by_expr_result();    result.success = o;    try {        fcall.sendResponse(fb, result, org.apache.thrift.protocol.TMessageType.REPLY, seqid);        return;    } catch (Exception e) {            }    fb.close();}
1
protected void setup(Context context) throws IOException, InterruptedException
{    super.setup(context);    Configuration conf = context.getConfiguration();    this.maxShingleSize = conf.getInt(MAX_SHINGLE_SIZE, DEFAULT_MAX_SHINGLE_SIZE);    this.emitUnigrams = conf.getBoolean(CollocDriver.EMIT_UNIGRAMS, CollocDriver.DEFAULT_EMIT_UNIGRAMS);    if (log.isInfoEnabled()) {                    }}
1
public synchronized void removeRMDelegationTokenState(RMDelegationTokenIdentifier identifier) throws Exception
{    Path nodeCreatePath = getNodePath(rmDTSecretManagerRoot, DELEGATION_TOKEN_PREFIX + identifier.getSequenceNumber());        deleteFileWithRetries(nodeCreatePath);}
1
public WorkItemServiceState reportWorkItemStatus(WorkItemStatus workItemStatus) throws IOException
{    DateTime endTime = DateTime.now();    workItemStatus.setFactory(Transport.getJsonFactory());            if (firstNonNull(workItemStatus.getCompleted(), Boolean.FALSE) && DataflowWorkerLoggingMDC.getStageName() != null) {        DateTime startTime = stageStartTime.get();        if (startTime != null) {                        Interval elapsed = new Interval(startTime, endTime);            int numErrors = workItemStatus.getErrors() == null ? 0 : workItemStatus.getErrors().size();                    }    }    shortIdCache.shortenIdsIfAvailable(workItemStatus.getCounterUpdates());    ReportWorkItemStatusRequest request = new ReportWorkItemStatusRequest().setWorkerId(options.getWorkerId()).setWorkItemStatuses(Collections.singletonList(workItemStatus)).setCurrentWorkerTime(toCloudTime(endTime));    ReportWorkItemStatusResponse result = dataflow.projects().locations().jobs().workItems().reportStatus(options.getProject(), options.getRegion(), options.getJobId(), request).execute();    if (result == null) {                throw new IOException("Got null work item status response");    }    if (result.getWorkItemServiceStates() == null) {                throw new IOException("Report work item status contained no work item service states");    }    if (result.getWorkItemServiceStates().size() != 1) {                throw new IOException("This version of the SDK expects exactly one work item service state from the service " + "but got " + result.getWorkItemServiceStates().size() + " states");    }    shortIdCache.storeNewShortIds(request, result);    WorkItemServiceState state = result.getWorkItemServiceStates().get(0);        return state;}
1
public void run()
{    try {        QueueBrowser browser = session.createBrowser(queue);        Enumeration<?> enumeration = browser.getEnumeration();        int received = 0;        while (enumeration.hasMoreElements()) {            Message m = (Message) enumeration.nextElement();            received++;                    }        assertEquals("Browsed all messages", messageToSend, received);    } catch (Exception e) {        e.printStackTrace();    }}
1
public void testConcurrentAppSubmit() throws IOException, InterruptedException, BrokenBarrierException, YarnException
{    ResourceScheduler scheduler = mockResourceScheduler();    RMContext rmContext = mock(RMContext.class);    mockRMContext(scheduler, rmContext);    RMStateStore stateStore = mock(RMStateStore.class);    when(rmContext.getStateStore()).thenReturn(stateStore);    RMAppManager appManager = new RMAppManager(rmContext, scheduler, null, mock(ApplicationACLsManager.class), new Configuration());    final ApplicationId appId1 = getApplicationId(100);    final ApplicationId appId2 = getApplicationId(101);    final SubmitApplicationRequest submitRequest1 = mockSubmitAppRequest(appId1, null, null);    final SubmitApplicationRequest submitRequest2 = mockSubmitAppRequest(appId2, null, null);    final CyclicBarrier startBarrier = new CyclicBarrier(2);    final CyclicBarrier endBarrier = new CyclicBarrier(2);    EventHandler<Event> eventHandler = new EventHandler<Event>() {        @Override        public void handle(Event rawEvent) {            if (rawEvent instanceof RMAppEvent) {                RMAppEvent event = (RMAppEvent) rawEvent;                if (event.getApplicationId().equals(appId1)) {                    try {                        startBarrier.await();                        endBarrier.await();                    } catch (BrokenBarrierException e) {                                            } catch (InterruptedException e) {                                            }                }            }        }    };    when(rmContext.getDispatcher().getEventHandler()).thenReturn(eventHandler);    doReturn(mock(RMTimelineCollectorManager.class)).when(rmContext).getRMTimelineCollectorManager();    final ClientRMService rmService = new ClientRMService(rmContext, scheduler, appManager, null, null, null);    rmService.init(new Configuration());        Thread t = new Thread() {        @Override        public void run() {            try {                rmService.submitApplication(submitRequest1);            } catch (YarnException | IOException e) {            }        }    };    t.start();        startBarrier.await();    rmService.submitApplication(submitRequest2);    endBarrier.await();    t.join();}
1
private void addDirectParentControllers(List<Controller> controllers, TestElement maybeController)
{    if (maybeController instanceof Controller) {                controllers.add((Controller) maybeController);    }}
1
private boolean checkCardinalityAndPending(SchedulerNode node, Optional<DiagnosticsCollector> dcOpt)
{        if (schedulingRequest.getResourceSizing().getNumAllocations() <= 0) {        return false;    }        try {        return PlacementConstraintsUtil.canSatisfyConstraints(appSchedulingInfo.getApplicationId(), schedulingRequest, node, placementConstraintManager, allocationTagsManager, dcOpt);    } catch (InvalidAllocationTagsQueryException e) {                return false;    }}
1
public void onFailure(Exception e)
{    if (e instanceof IndexNotFoundException) {        logger.trace("could not retrieve built in users since security index does not exist", e);        listener.onResponse(Collections.emptyMap());    } else {                listener.onFailure(e);    }}
1
public void accept(Object event)
{    AddressedTuple addressedTuple = (AddressedTuple) event;    int taskId = addressedTuple.getDest();    TupleImpl tuple = (TupleImpl) addressedTuple.getTuple();    if (isDebug) {            }    try {        if (taskId != AddressedTuple.BROADCAST_DEST) {            tupleActionFn(taskId, tuple);        } else {            for (Integer t : taskIds) {                tupleActionFn(t, tuple);            }        }    } catch (Exception e) {        throw new RuntimeException(e);    }}
1
public synchronized void stop()
{    started = false;    narAutoLoaderTask.stop();    }
1
public void testS3Kind() throws IOException
{    try {        Class.forName("org.apache.hadoop.fs.s3.S3FileSystem");    } catch (ClassNotFoundException ignored) {                        return;    }    final FileSystem s3 = new Path("s3://myId:mySecret@bucket/some/bucket/some/object").getFileSystem();    assertEquals(FileSystemKind.OBJECT_STORE, s3.getKind());}
1
public static MessageReader getMessageReader(String messageReaderKlass)
{    if (messageReaderKlass == null) {        throw UserException.validationError().message("Please configure message reader implementation using the property 'store.kafka.record.reader'").build(logger);    }    MessageReader messageReader = null;    try {        Class<?> klass = Class.forName(messageReaderKlass);        if (MessageReader.class.isAssignableFrom(klass)) {            messageReader = (MessageReader) klass.newInstance();                    }    } catch (ClassNotFoundException | InstantiationException | IllegalAccessException e) {        throw UserException.validationError().message("Failed to initialize message reader : %s", messageReaderKlass).build(logger);    }    if (messageReader == null) {        throw UserException.validationError().message("Message reader configured '%s' does not implement '%s'", messageReaderKlass, MessageReader.class.getName()).build(logger);    }    return messageReader;}
1
protected void addSigningCertificate(Document doc, Element signedProperties, Input input) throws Exception
{        if (getSigningCertificate() == null && (getSigningCertificateChain() == null || getSigningCertificateChain().length == 0)) {        return;    }        Element signedCertificate = createElement("SigningCertificate", doc, input);    signedProperties.appendChild(signedCertificate);    if (getSigningCertificate() != null) {                X509Certificate cert = getSigningCertificate();        addCertificate(cert, signedCertificate, doc, 0, input);    } else if (getSigningCertificateChain() != null && getSigningCertificateChain().length > 0) {        Certificate[] certs = getSigningCertificateChain();        int index = 0;        for (Certificate cert : certs) {                        X509Certificate x509Cert = (X509Certificate) cert;            addCertificate(x509Cert, signedCertificate, doc, index, input);            index++;        }    } else {                throw new IllegalStateException("Unexpected exception");    }}
1
private void snapshotActiveBuckets(final long checkpointId, final ListState<byte[]> bucketStatesContainer) throws Exception
{    for (Bucket<IN, BucketID> bucket : activeBuckets.values()) {        final BucketState<BucketID> bucketState = bucket.onReceptionOfCheckpoint(checkpointId);        final byte[] serializedBucketState = SimpleVersionedSerialization.writeVersionAndSerialize(bucketStateSerializer, bucketState);        bucketStatesContainer.add(serializedBucketState);        if (LOG.isDebugEnabled()) {                    }    }}
1
public static String exec(S3GuardTool cmd, String... args) throws Exception
{    ByteArrayOutputStream buf = new ByteArrayOutputStream();    try {        exec(0, "", cmd, buf, args);        return buf.toString();    } catch (AssertionError e) {        throw e;    } catch (Exception e) {                throw e;    }}
1
public void fail(String offset)
{        EventDataWrap eventDataWrap = pending.remove(offset);    toResend.add(eventDataWrap);}
1
public void lifecycleEvent(LifecycleEvent event)
{        try {        engine = (Engine) event.getLifecycle();    } catch (ClassCastException e) {                return;    }        if (event.getType().equals(Lifecycle.START_EVENT))        start();    else if (event.getType().equals(Lifecycle.STOP_EVENT))        stop();}
1
private void insertEntry(int incomingRowIdx, int currentIdx, int hashValue, BatchHolder lastEntryBatch, int lastEntryIdxWithinBatch) throws SchemaChangeException
{    int currentIdxWithinBatch = currentIdx & BATCH_MASK;    setValue(incomingRowIdx, currentIdxWithinBatch);        if (lastEntryBatch != null) {        lastEntryBatch.updateLinks(lastEntryIdxWithinBatch, currentIdx);    }            links.getMutator().set(currentIdxWithinBatch, EMPTY_SLOT);    hashValues.getMutator().set(currentIdxWithinBatch, hashValue);    maxOccupiedIdx = Math.max(maxOccupiedIdx, currentIdxWithinBatch);    if (EXTRA_DEBUG) {            }}
1
private NodeSchemaLoadResult loadSchemaFromYaml(InputStream schemaFile)
{        NodeSchemaLoadResult finalSchema;    try {        Yaml yaml = new Yaml();        NodeSchema nodeTree;        nodeTree = yaml.loadAs(schemaFile, NodeSchema.class);        List<NodeSchema> schemaList = new ArrayList<>();        if (nodeTree.getType() != LayerType.ROOT) {            throw new IllegalArgumentException("First layer is not a ROOT node." + " schema file.");        }        schemaList.add(nodeTree);        if (nodeTree.getSublayer() != null) {            nodeTree = nodeTree.getSublayer().get(0);        }        while (nodeTree != null) {            if (nodeTree.getType() == LayerType.LEAF_NODE && nodeTree.getSublayer() != null) {                throw new IllegalArgumentException("Leaf node in the middle of path." + " schema file.");            }            if (nodeTree.getType() == LayerType.ROOT) {                throw new IllegalArgumentException("Multiple root nodes are defined." + " schema file.");            }            schemaList.add(nodeTree);            if (nodeTree.getSublayer() != null) {                nodeTree = nodeTree.getSublayer().get(0);            } else {                break;            }        }        finalSchema = new NodeSchemaLoadResult(schemaList, true);    } catch (Exception e) {        throw new IllegalArgumentException("Fail to load network topology node" + " schema file: " + schemaFile + " , error:" + e.getMessage(), e);    }    return finalSchema;}
1
public void onComplete(Void o)
{    rename_partition_result result = new rename_partition_result();    try {        fcall.sendResponse(fb, result, org.apache.thrift.protocol.TMessageType.REPLY, seqid);        return;    } catch (Exception e) {            }    fb.close();}
1
public static Map<Long, Long> getRecommendCuboidList(CubeSegment segment) throws IOException
{    if (segment == null) {        return null;    }    CubeStatsReader cubeStatsReader = new CubeStatsReader(segment, null, segment.getConfig());    if (cubeStatsReader.getCuboidRowEstimatesHLL() == null || cubeStatsReader.getCuboidRowEstimatesHLL().isEmpty()) {                return null;    }    CubeInstance cube = segment.getCubeInstance();    long baseCuboid = cube.getCuboidScheduler().getBaseCuboidId();    if (cubeStatsReader.getCuboidRowEstimatesHLL().get(baseCuboid) == null || cubeStatsReader.getCuboidRowEstimatesHLL().get(baseCuboid) == 0L) {                return null;    }    Set<Long> mandatoryCuboids = segment.getCubeDesc().getMandatoryCuboids();    String key = cube.getName();    CuboidStats cuboidStats = new CuboidStats.Builder(key, baseCuboid, cubeStatsReader.getCuboidRowEstimatesHLL(), cubeStatsReader.getCuboidSizeMap()).setMandatoryCuboids(mandatoryCuboids).setBPUSMinBenefitRatio(segment.getConfig().getCubePlannerBPUSMinBenefitRatio()).build();    return CuboidRecommender.getInstance().getRecommendCuboidList(cuboidStats, segment.getConfig(), !mandatoryCuboids.isEmpty());}
1
public void deleteUser(String userId, boolean notifyUser, boolean force)
{    try {                if (userId == null) {            throw new IllegalArgumentException("Parameter 'fileId' can not be null");        }        BoxUser file = new BoxUser(boxConnection, userId);        file.delete(notifyUser, force);    } catch (BoxAPIException e) {        throw new RuntimeException(String.format("Box API returned the error code %d\n\n%s", e.getResponseCode(), e.getResponse()), e);    }}
1
private void saveExtSnapshotIfNeeded(CubeManager cubeManager, CubeInstance cube, CubeSegment segment) throws IOException
{    String extLookupSnapshotStr = this.getParam(BatchConstants.ARG_EXT_LOOKUP_SNAPSHOTS_INFO);    if (extLookupSnapshotStr == null || extLookupSnapshotStr.isEmpty()) {        return;    }    Map<String, String> extLookupSnapshotMap = LookupMaterializeContext.parseLookupSnapshots(extLookupSnapshotStr);        List<SnapshotTableDesc> snapshotTableDescList = cube.getDescriptor().getSnapshotTableDescList();    for (SnapshotTableDesc snapshotTableDesc : snapshotTableDescList) {        String tableName = snapshotTableDesc.getTableName();        if (snapshotTableDesc.isExtSnapshotTable()) {            String newSnapshotResPath = extLookupSnapshotMap.get(tableName);            if (newSnapshotResPath == null || newSnapshotResPath.isEmpty()) {                continue;            }            if (snapshotTableDesc.isGlobal()) {                if (!newSnapshotResPath.equals(cube.getSnapshotResPath(tableName))) {                    cubeManager.updateCubeLookupSnapshot(cube, tableName, newSnapshotResPath);                }            } else {                segment.putSnapshotResPath(tableName, newSnapshotResPath);            }        }    }}
1
protected synchronized void stopInternal() throws LifecycleException
{    if (log.isDebugEnabled()) {            }    setState(LifecycleState.STOPPING);        try {        unload();    } catch (Throwable t) {        ExceptionUtils.handleThrowable(t);            }        Session[] sessions = findSessions();    for (int i = 0; i < sessions.length; i++) {        Session session = sessions[i];        try {            if (session.isValid()) {                session.expire();            }        } catch (Throwable t) {            ExceptionUtils.handleThrowable(t);        } finally {                                    session.recycle();        }    }        super.stopInternal();}
1
public void run()
{    final int MAX_RETRIES = 3;    int remainingRetries = MAX_RETRIES;        boolean sendProgress = resetProgressFlag();    long taskProgressInterval = MRJobConfUtil.getTaskProgressReportInterval(conf);    while (!taskDone.get()) {        synchronized (lock) {            done = false;        }        try {                        boolean taskFound = true;            AMFeedback amFeedback = null;                        synchronized (lock) {                if (taskDone.get()) {                    break;                }                lock.wait(taskProgressInterval);            }            if (taskDone.get()) {                break;            }            if (sendProgress) {                                updateCounters();                checkTaskLimits();                taskStatus.statusUpdate(taskProgress.get(), taskProgress.toString(), counters);                amFeedback = umbilical.statusUpdate(taskId, taskStatus);                taskFound = amFeedback.getTaskFound();                taskStatus.clearStatus();            } else {                                amFeedback = umbilical.statusUpdate(taskId, null);                taskFound = amFeedback.getTaskFound();            }                        if (!taskFound) {                if (uberized) {                    taskDone.set(true);                    break;                } else {                                        resetDoneFlag();                    System.exit(66);                }            }                                                boolean lastPreempt = mustPreempt.get();            mustPreempt.set(mustPreempt.get() || amFeedback.getPreemption());            if (lastPreempt ^ mustPreempt.get()) {                            }            sendProgress = resetProgressFlag();            remainingRetries = MAX_RETRIES;        } catch (TaskLimitException e) {            String errMsg = "Task exceeded the limits: " + StringUtils.stringifyException(e);                        try {                umbilical.fatalError(taskId, errMsg, true);            } catch (IOException ioe) {                            }                        resetDoneFlag();            ExitUtil.terminate(69);        } catch (Throwable t) {                        remainingRetries -= 1;            if (remainingRetries == 0) {                ReflectionUtils.logThreadInfo(LOG, "Communication exception", 0);                                resetDoneFlag();                System.exit(65);            }        }    }        resetDoneFlag();}
1
public Object peek() throws InterruptedException
{    Object object = null;    while (true) {        if (takeWhenPeekInProgress) {            try {                this.take();            } catch (CacheException ce) {                throw new RuntimeException(ce) {                };            }            this.takeWhenPeekInProgress = false;        }        object = super.peek();        if (object == null) {            synchronized (forWaiting) {                object = super.peek();                if (object == null) {                    boolean interrupted = Thread.interrupted();                    try {                        forWaiting.wait();                    } catch (InterruptedException e) {                        interrupted = true;                        /**                         * ignore*                         */                        if (logger.isDebugEnabled()) {                                                    }                    } finally {                        if (interrupted) {                            Thread.currentThread().interrupt();                        }                    }                } else {                    break;                }            }        } else {            break;        }    }    return object;}
1
private void removeApplication(ApplicationId applicationId, String user)
{    writeLock.lock();    try {        --numApplications;            } finally {        writeLock.unlock();    }}
1
protected void reduce(ImmutableBytesWritable key, Iterable<ImmutableBytesWritable> values, Context context) throws IOException, InterruptedException
{    int count = 0;    for (ImmutableBytesWritable value : values) {        String val = Bytes.toStringBinary(value.get());                if (first == null)            first = val;        last = val;        count++;    }}
1
private static void ensureDestroyed() throws Exception
{    if (tpe == null) {        return;    }    int shutdownTries = SHUTDOWN_WAIT_TRIES;    tpe.shutdown();    if (!tpe.isShutdown()) {        throw new RuntimeException("Shutdown had no effect.");    }    while (!tpe.awaitTermination(SHUTDOWN_WAIT_MSEC, TimeUnit.MILLISECONDS)) {                if (shutdownTries-- <= 0) {                        break;        }    }    if (!tpe.isTerminated()) {        tpe.shutdownNow();        if (!tpe.awaitTermination(SHUTDOWN_WAIT_MSEC, TimeUnit.MILLISECONDS)) {            throw new RuntimeException("Failed to terminate thread pool in timely manner.");        }    }    tpe = null;}
1
public List<AtmosphereRequest> onBinaryStream(WebSocket webSocket, InputStream data)
{        String connectionKey = store.getConnectionKey(webSocket);    consumer.sendMessage(connectionKey, data);        return null;}
1
private void addNodeCapacityToPlan(MockRM rm, int memory, int vCores)
{    try {        rm.registerNode("127.0.0.1:1", memory, vCores);        int attempts = 10;        do {            rm1.drainEvents();            rm.getRMContext().getReservationSystem().synchronizePlan(ReservationSystemTestUtil.reservationQ, false);            if (rm.getRMContext().getReservationSystem().getPlan(ReservationSystemTestUtil.reservationQ).getTotalCapacity().getMemorySize() > 0) {                break;            }                        Thread.sleep(100);        } while (attempts-- > 0);        if (attempts <= 0) {            Assert.fail("Exhausted attempts in checking if node capacity was " + "added to the plan");        }    } catch (Exception e) {        Assert.fail(e.getMessage());    }}
1
public void testTransformationSendXMLObject() throws Exception
{    MessageConsumer consumer = session.createConsumer(queue);    String frame = "CONNECT\n" + "login:system\n" + "passcode:manager\n\n" + Stomp.NULL;    stompConnection.sendFrame(frame);    frame = stompConnection.receiveFrame();    assertTrue(frame.startsWith("CONNECTED"));    frame = "SEND\n" + "destination:/queue/" + getQueueName() + "\n" + "transformation:" + Stomp.Transformations.JMS_OBJECT_XML + "\n\n" + xmlObject + Stomp.NULL;    stompConnection.sendFrame(frame);    Message message = consumer.receive(2500);    assertNotNull(message);        assertTrue(message instanceof ObjectMessage);    ObjectMessage objectMessage = (ObjectMessage) message;    SamplePojo object = (SamplePojo) objectMessage.getObject();    assertEquals("Dejan", object.getName());}
1
public void prepare(Map<String, Object> stormConf)
{    Map<?, ?> params = (Map<?, ?>) stormConf.get(Config.STORM_GROUP_MAPPING_SERVICE_PARAMS);    Map<String, Set<String>> mapping = (Map<String, Set<String>>) params.get(STORM_FIXED_GROUP_MAPPING);    if (mapping != null) {        cachedGroups.putAll(mapping);    } else {            }}
1
public FileMetadata retrieveMetadata(String key) throws IOException
{        if (null == storageInteractionLayer) {        final String errMsg = String.format("Storage session expected for URI '%s' but does not exist.", sessionUri);        throw new AssertionError(errMsg);    }        try {        if (checkContainer(ContainerAccessType.PureRead) == ContainerState.DoesntExist) {                        return null;        }                if (key.equals("/")) {                        return new FileMetadata(key, 0, defaultPermissionNoBlobMetadata(), BlobMaterialization.Implicit, hadoopBlockSize);        }        CloudBlobWrapper blob = getBlobReference(key);                if (null != blob && blob.exists(getInstrumentedContext())) {                        try {                                                blob.downloadAttributes(getInstrumentedContext());                BlobProperties properties = blob.getProperties();                if (retrieveFolderAttribute(blob)) {                                        return new FileMetadata(key, properties.getLastModified().getTime(), getPermissionStatus(blob), BlobMaterialization.Explicit, hadoopBlockSize);                } else {                                        return new FileMetadata(                    key, getDataLength(blob, properties), properties.getLastModified().getTime(), getPermissionStatus(blob), hadoopBlockSize);                }            } catch (StorageException e) {                if (!NativeAzureFileSystemHelper.isFileNotFoundException(e)) {                    throw e;                }            }        }                                        Iterable<ListBlobItem> objects = listRootBlobs(key, true, EnumSet.of(BlobListingDetails.METADATA), null, getInstrumentedContext());                for (ListBlobItem blobItem : objects) {            if (blobItem instanceof CloudBlockBlobWrapper || blobItem instanceof CloudPageBlobWrapper) {                                blob = (CloudBlobWrapper) blobItem;                                                BlobProperties properties = blob.getProperties();                return new FileMetadata(key, properties.getLastModified().getTime(), getPermissionStatus(blob), BlobMaterialization.Implicit, hadoopBlockSize);            }        }                return null;    } catch (Exception e) {                throw new AzureException(e);    }}
1
public String[] listConfigDir()
{    List<String> list;    try {        list = zkController.getZkClient().getChildren(configSetZkPath, null, true);    } catch (InterruptedException e) {                Thread.currentThread().interrupt();                throw new ZooKeeperException(SolrException.ErrorCode.SERVER_ERROR, "", e);    } catch (KeeperException e) {                throw new ZooKeeperException(SolrException.ErrorCode.SERVER_ERROR, "", e);    }    return list.toArray(new String[0]);}
1
private static void triggerCacheRefresh(HBaseTestingUtility testUtil, boolean bypass, boolean userLimiter, boolean tableLimiter, boolean nsLimiter, boolean rsLimiter, boolean exceedThrottleQuota, final TableName... tables) throws Exception
{    envEdge.incValue(2 * REFRESH_TIME);    for (RegionServerThread rst : testUtil.getMiniHBaseCluster().getRegionServerThreads()) {        RegionServerRpcQuotaManager quotaManager = rst.getRegionServer().getRegionServerRpcQuotaManager();        QuotaCache quotaCache = quotaManager.getQuotaCache();        quotaCache.triggerCacheRefresh();                Thread.sleep(250);        for (TableName table : tables) {            quotaCache.getTableLimiter(table);        }        boolean isUpdated = false;        while (!isUpdated) {            quotaCache.triggerCacheRefresh();            isUpdated = true;            for (TableName table : tables) {                boolean isBypass = true;                if (userLimiter) {                    isBypass = quotaCache.getUserLimiter(User.getCurrent().getUGI(), table).isBypass();                }                if (tableLimiter) {                    isBypass &= quotaCache.getTableLimiter(table).isBypass();                }                if (nsLimiter) {                    isBypass &= quotaCache.getNamespaceLimiter(table.getNamespaceAsString()).isBypass();                }                if (isBypass != bypass) {                    envEdge.incValue(100);                    isUpdated = false;                    break;                }            }            if (rsLimiter) {                boolean rsIsBypass = quotaCache.getRegionServerQuotaLimiter(QuotaTableUtil.QUOTA_REGION_SERVER_ROW_KEY).isBypass();                if (rsIsBypass != bypass) {                    envEdge.incValue(100);                    isUpdated = false;                }            }            if (exceedThrottleQuota) {                if (quotaCache.isExceedThrottleQuotaEnabled() != bypass) {                    envEdge.incValue(100);                    isUpdated = false;                }            }        }                                            }}
1
public void onEvent(MemoryEvent event)
{    if (logger.isDebugEnabled()) {            }    setMemoryThresholdFlag(event);}
1
public static void createDirectoriesOnWorker(SubProcessConfiguration configuration) throws IOException
{    try {        Path path = Paths.get(configuration.getWorkerPath());        if (!path.toFile().exists()) {            Files.createDirectories(path);                    }    } catch (FileAlreadyExistsException ex) {            }}
1
public void recreateSocketAddresses()
{    if (this.addr == null) {                return;    }    if (this.electionAddr == null) {                return;    }    String host = this.addr.getHostString();    InetAddress address = null;    try {        address = InetAddress.getByName(host);    } catch (UnknownHostException ex) {                return;    }        int port = this.addr.getPort();    this.addr = new InetSocketAddress(address, port);    port = this.electionAddr.getPort();    this.electionAddr = new InetSocketAddress(address, port);}
1
public static void startPublisher() throws Exception
{    MQTT client = new MQTT();    client.setTracer(new MqttLogger());    client.setHost("tcp://localhost:1883");    client.setClientId("MqttBrokerPublisher");    connection = client.blockingConnection();    Runtime.getRuntime().addShutdownHook(new Thread() {        @Override        public void run() {            try {                                connection.disconnect();            } catch (Exception e) {                e.printStackTrace();            }        }    });    connection.connect();}
1
public ClusterTasksResult<Task> execute(final ClusterState currentState, final List<Task> tasks) throws Exception
{    final DiscoveryNodes.Builder remainingNodesBuilder = DiscoveryNodes.builder(currentState.nodes());    boolean removed = false;    for (final Task task : tasks) {        if (currentState.nodes().nodeExists(task.node())) {            remainingNodesBuilder.remove(task.node());            removed = true;        } else {                    }    }    if (!removed) {                return ClusterTasksResult.<Task>builder().successes(tasks).build(currentState);    }    final ClusterState remainingNodesClusterState = remainingNodesClusterState(currentState, remainingNodesBuilder);    return getTaskClusterTasksResult(currentState, tasks, remainingNodesClusterState);}
1
public void write(Text key, Parse parse) throws IOException
{    String fromUrl = key.toString();        String origin = null;    if (textOut != null) {        textOut.append(key, new ParseText(parse.getText()));    }    ParseData parseData = parse.getData();        String sig = parseData.getContentMeta().get(Nutch.SIGNATURE_KEY);    if (sig != null) {        byte[] signature = StringUtil.fromHexString(sig);        if (signature != null) {                        CrawlDatum d = new CrawlDatum(CrawlDatum.STATUS_SIGNATURE, 0);            d.setSignature(signature);            crawlOut.append(key, d);        }    }            CrawlDatum parseMDCrawlDatum = null;    for (String mdname : parseMDtoCrawlDB) {        String mdvalue = parse.getData().getParseMeta().get(mdname);        if (mdvalue != null) {            if (parseMDCrawlDatum == null)                parseMDCrawlDatum = new CrawlDatum(CrawlDatum.STATUS_PARSE_META, 0);            parseMDCrawlDatum.getMetaData().put(new Text(mdname), new Text(mdvalue));        }    }    if (parseMDCrawlDatum != null)        crawlOut.append(key, parseMDCrawlDatum);        if (ignoreExternalLinks || ignoreInternalLinks) {        URL originURL = new URL(fromUrl.toString());                if ("bydomain".equalsIgnoreCase(ignoreExternalLinksMode)) {            origin = URLUtil.getDomainName(originURL).toLowerCase();        } else         {            origin = originURL.getHost().toLowerCase();        }    }    ParseStatus pstatus = parseData.getStatus();    if (pstatus != null && pstatus.isSuccess() && pstatus.getMinorCode() == ParseStatus.SUCCESS_REDIRECT) {        String newUrl = pstatus.getMessage();        int refreshTime = Integer.valueOf(pstatus.getArgs()[1]);        newUrl = filterNormalize(fromUrl, newUrl, origin, ignoreInternalLinks, ignoreExternalLinks, ignoreExternalLinksMode, filters, exemptionFilters, normalizers, URLNormalizers.SCOPE_FETCHER);        if (newUrl != null) {            String reprUrl = URLUtil.chooseRepr(fromUrl, newUrl, refreshTime < Fetcher.PERM_REFRESH_TIME);            CrawlDatum newDatum = new CrawlDatum();            newDatum.setStatus(CrawlDatum.STATUS_LINKED);            if (reprUrl != null && !reprUrl.equals(newUrl)) {                newDatum.getMetaData().put(Nutch.WRITABLE_REPR_URL_KEY, new Text(reprUrl));            }            crawlOut.append(new Text(newUrl), newDatum);        }    }        Outlink[] links = parseData.getOutlinks();    int outlinksToStore = Math.min(maxOutlinks, links.length);    int validCount = 0;    CrawlDatum adjust = null;    List<Entry<Text, CrawlDatum>> targets = new ArrayList<>(outlinksToStore);    List<Outlink> outlinkList = new ArrayList<>(outlinksToStore);    for (int i = 0; i < links.length && validCount < outlinksToStore; i++) {        String toUrl = links[i].getToUrl();                if (!isParsing) {            if (toUrl.length() > maxOutlinkLength) {                continue;            }            toUrl = ParseOutputFormat.filterNormalize(fromUrl, toUrl, origin, ignoreInternalLinks, ignoreExternalLinks, ignoreExternalLinksMode, filters, exemptionFilters, normalizers);            if (toUrl == null) {                continue;            }        }        CrawlDatum target = new CrawlDatum(CrawlDatum.STATUS_LINKED, interval);        Text targetUrl = new Text(toUrl);                                MapWritable outlinkMD = links[i].getMetadata();        if (outlinkMD != null) {            target.getMetaData().putAll(outlinkMD);        }        try {            scfilters.initialScore(targetUrl, target);        } catch (ScoringFilterException e) {                        target.setScore(0.0f);        }        targets.add(new SimpleEntry(targetUrl, target));                links[i].setUrl(toUrl);        outlinkList.add(links[i]);        validCount++;    }    try {                adjust = scfilters.distributeScoreToOutlinks(key, parseData, targets, null, links.length);    } catch (ScoringFilterException e) {            }    for (Entry<Text, CrawlDatum> target : targets) {        crawlOut.append(target.getKey(), target.getValue());    }    if (adjust != null)        crawlOut.append(key, adjust);    Outlink[] filteredLinks = outlinkList.toArray(new Outlink[outlinkList.size()]);    parseData = new ParseData(parseData.getStatus(), parseData.getTitle(), filteredLinks, parseData.getContentMeta(), parseData.getParseMeta());    dataOut.append(key, parseData);    if (!parse.isCanonical()) {        CrawlDatum datum = new CrawlDatum();        datum.setStatus(CrawlDatum.STATUS_FETCH_SUCCESS);        String timeString = parse.getData().getContentMeta().get(Nutch.FETCH_TIME_KEY);        try {            datum.setFetchTime(Long.parseLong(timeString));        } catch (Exception e) {                        datum.setFetchTime(System.currentTimeMillis());        }        crawlOut.append(key, datum);    }}
1
public void printState() throws Exception
{    if (null == cluster) {                return;    }                ClusterState state = cluster.getClusterStateProvider().getClusterState();    for (String coll : cluster.getSimClusterStateProvider().simListCollections()) {            }    shutdownCluster();}
1
private void sendMessageThatFailsSelection() throws JMSException
{    for (int i = 0; i < 5; i++) {        String textOfNotSelectedMsg = "Msg_" + i;        sendMessage(textOfNotSelectedMsg, "not_value");            }}
1
private boolean visit(String resourceName)
{    if (resourceName.equals(ResourceRequest.ANY)) {        return visitAny();    }    List<FSSchedulerNode> nodes = nodeTracker.getNodesByResourceName(resourceName);    int numNodes = nodes.size();    if (numNodes == 0) {                return false;    }    if (numNodes == 1) {                        FSSchedulerNode node = nodes.get(0);        if (node.getNodeName().equals(resourceName)) {            return visitNode(node.getRackName());        }    }        return visitRack(resourceName);}
1
public void testListEmptyRootDirectory() throws IOException
{    for (int attempt = 1, maxAttempts = 10; attempt <= maxAttempts; ++attempt) {        try {            super.testListEmptyRootDirectory();            break;        } catch (AssertionError | FileNotFoundException e) {            if (attempt < maxAttempts) {                                try {                    Thread.sleep(1000);                } catch (InterruptedException e2) {                    Thread.currentThread().interrupt();                    fail("Test interrupted.");                    break;                }            } else {                                throw e;            }        }    }}
1
private void connectToResourceManager()
{    assert (resourceManagerAddress != null);    assert (establishedResourceManagerConnection == null);    assert (resourceManagerConnection == null);        resourceManagerConnection = new TaskExecutorToResourceManagerConnection(log, getRpcService(), getAddress(), getResourceID(), taskManagerConfiguration.getRetryingRegistrationConfiguration(), taskManagerLocation.dataPort(), hardwareDescription, resourceManagerAddress.getAddress(), resourceManagerAddress.getResourceManagerId(), getMainThreadExecutor(), new ResourceManagerRegistrationListener());    resourceManagerConnection.start();}
1
public void onSuccess(ClientResponse resp, RequestFuture<Void> future)
{        clearFindCoordinatorFuture();    FindCoordinatorResponse findCoordinatorResponse = (FindCoordinatorResponse) resp.responseBody();    Errors error = findCoordinatorResponse.error();    if (error == Errors.NONE) {        synchronized (AbstractCoordinator.this) {                                    int coordinatorConnectionId = Integer.MAX_VALUE - findCoordinatorResponse.data().nodeId();            AbstractCoordinator.this.coordinator = new Node(coordinatorConnectionId, findCoordinatorResponse.data().host(), findCoordinatorResponse.data().port());                        client.tryConnect(coordinator);            heartbeat.resetSessionTimeout();        }        future.complete(null);    } else if (error == Errors.GROUP_AUTHORIZATION_FAILED) {        future.raise(GroupAuthorizationException.forGroupId(rebalanceConfig.groupId));    } else {                future.raise(error);    }}
1
public boolean load() throws ExecutionException, InterruptedException, TimeoutException
{    ExecutorService loadPool = Executors.newFixedThreadPool(1);    Future<Boolean> loadTask = loadPool.submit(() -> {        try {            List<RegionInfo> regionsToMove = readRegionsFromFile(filename);            if (regionsToMove.isEmpty()) {                                return true;            }            loadRegions(regionsToMove);        } catch (Exception e) {                        return false;        }        return true;    });    return waitTaskToFinish(loadPool, loadTask, "loading");}
1
private static void doDestroysP(final int num, final int multiUserIndex, final int expectedResult)
{    assertTrue(num <= KEYS.length);    Region region = null;    try {        if (multiUserAuthMode) {            region = proxyCaches[multiUserIndex].getRegion(REGION_NAME);        } else {            region = getCache().getRegion(REGION_NAME);        }        assertNotNull(region);    } catch (Exception ex) {        if (expectedResult == OTHER_EXCEPTION) {                    } else {            fail("Got unexpected exception when doing destroys", ex);        }    }    for (int index = 0; index < num; ++index) {        try {            region.destroy(KEYS[index]);            if (expectedResult != NO_EXCEPTION) {                fail("Expected a NotAuthorizedException while doing destroys");            }        } catch (NoAvailableServersException ex) {            if (expectedResult == NO_AVAILABLE_SERVERS) {                                continue;            } else {                fail("Got unexpected exception when doing destroys", ex);            }        } catch (ServerConnectivityException ex) {            if ((expectedResult == NOTAUTHZ_EXCEPTION) && (ex.getCause() instanceof NotAuthorizedException)) {                                continue;            } else if (expectedResult == OTHER_EXCEPTION) {                            } else {                fail("Got unexpected exception when doing destroys", ex);            }        } catch (Exception ex) {            if (expectedResult == OTHER_EXCEPTION) {                            } else {                fail("Got unexpected exception when doing destroys", ex);            }        }    }}
1
public Map<TableName, SpaceQuotaSnapshot> fetchSnapshotsFromQuotaTable() throws IOException
{    try (Table quotaTable = getConnection().getTable(QuotaUtil.QUOTA_TABLE_NAME);        ResultScanner scanner = quotaTable.getScanner(QuotaTableUtil.makeQuotaSnapshotScan())) {        Map<TableName, SpaceQuotaSnapshot> snapshots = new HashMap<>();        for (Result result : scanner) {            try {                extractQuotaSnapshot(result, snapshots);            } catch (IllegalArgumentException e) {                final String msg = "Failed to parse result for row " + Bytes.toString(result.getRow());                                throw new IOException(msg, e);            }        }        return snapshots;    }}
1
private void startSimpleZKServer(CountDownLatch startupDelayLatch) throws IOException
{    zks = new SimpleZooKeeperServer(tmpDir, tmpDir, 3000, startupDelayLatch);    SyncRequestProcessor.setSnapCount(100);    final int PORT = Integer.parseInt(HOSTPORT.split(":")[1]);    servcnxnf = ServerCnxnFactory.createFactory(PORT, -1);    Thread startupThread = new Thread() {        public void run() {            try {                servcnxnf.startup(zks);            } catch (IOException e) {                                                    } catch (InterruptedException e) {                                                    }        }    };        startupThread.start();}
1
public List<byte[]> getMetaTableRows(TableName tableName) throws IOException
{        Table t = getConnection().getTable(TableName.META_TABLE_NAME);    List<byte[]> rows = new ArrayList<>();    ResultScanner s = t.getScanner(new Scan());    for (Result result : s) {        RegionInfo info = MetaTableAccessor.getRegionInfo(result);        if (info == null) {                                    continue;        }        if (info.getTable().equals(tableName)) {                        rows.add(result.getRow());        }    }    s.close();    t.close();    return rows;}
1
public void init(AMRMProxyApplicationContext appContext)
{    super.init(appContext);    try {        user = UserGroupInformation.createProxyUser(appContext.getApplicationAttemptId().toString(), UserGroupInformation.getCurrentUser());        user.addToken(appContext.getAMRMToken());        final Configuration conf = this.getConf();        rmClient = createRMClient(appContext, conf);    } catch (IOException e) {        String message = "Error while creating of RM app master service proxy for attemptId:" + appContext.getApplicationAttemptId().toString();        if (user != null) {            message += ", user: " + user;        }                throw new YarnRuntimeException(message, e);    } catch (Exception e) {        throw new YarnRuntimeException(e);    }}
1
private void validatePath(String path, long sessionId) throws BadArgumentsException
{    try {        PathUtils.validatePath(path);    } catch (IllegalArgumentException ie) {                throw new BadArgumentsException(path);    }}
1
public void processCommit(CommitUpdateCommand cmd) throws IOException
{    assert TestInjection.injectFailUpdateRequests();    if (isReadOnly()) {        throw new SolrException(ErrorCode.FORBIDDEN, "Collection " + collection + " is read-only.");    }    updateCommand = cmd;    List<SolrCmdDistributor.Node> nodes = null;    Replica leaderReplica = null;    zkCheck();    try {        leaderReplica = zkController.getZkStateReader().getLeaderRetry(collection, cloudDesc.getShardId());    } catch (InterruptedException e) {        Thread.interrupted();        throw new SolrException(SolrException.ErrorCode.SERVICE_UNAVAILABLE, "Exception finding leader for shard " + cloudDesc.getShardId(), e);    }    isLeader = leaderReplica.getName().equals(cloudDesc.getCoreNodeName());    nodes = getCollectionUrls(collection, EnumSet.of(Replica.Type.TLOG, Replica.Type.NRT), true);    if (nodes == null) {                throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, "Unable to distribute commit operation. No replicas available of types " + Replica.Type.TLOG + " or " + Replica.Type.NRT);    }    nodes.removeIf((node) -> node.getNodeProps().getNodeName().equals(zkController.getNodeName()) && node.getNodeProps().getCoreName().equals(req.getCore().getName()));    if (!isLeader && req.getParams().get(COMMIT_END_POINT, "").equals("replicas")) {        if (replicaType == Replica.Type.PULL) {                    } else if (replicaType == Replica.Type.NRT) {            doLocalCommit(cmd);        }    } else {                ModifiableSolrParams params = new ModifiableSolrParams(filterParams(req.getParams()));        List<SolrCmdDistributor.Node> useNodes = null;        if (req.getParams().get(COMMIT_END_POINT) == null) {            useNodes = nodes;            params.set(DISTRIB_UPDATE_PARAM, DistribPhase.TOLEADER.toString());            params.set(COMMIT_END_POINT, "leaders");            if (useNodes != null) {                params.set(DISTRIB_FROM, ZkCoreNodeProps.getCoreUrl(zkController.getBaseUrl(), req.getCore().getName()));                cmdDistrib.distribCommit(cmd, useNodes, params);                cmdDistrib.blockAndDoRetries();            }        }        if (isLeader) {            params.set(DISTRIB_UPDATE_PARAM, DistribPhase.FROMLEADER.toString());            params.set(COMMIT_END_POINT, "replicas");            useNodes = getReplicaNodesForLeader(cloudDesc.getShardId(), leaderReplica);            if (useNodes != null) {                params.set(DISTRIB_FROM, ZkCoreNodeProps.getCoreUrl(zkController.getBaseUrl(), req.getCore().getName()));                cmdDistrib.distribCommit(cmd, useNodes, params);            }            doLocalCommit(cmd);            if (useNodes != null) {                cmdDistrib.blockAndDoRetries();            }        }    }}
1
public void testMultipleAutoFollowPatternsDifferentClusters() throws Exception
{    if ("follow".equals(targetCluster) == false) {                return;    }    int initialNumberOfSuccessfulFollowedIndices = getNumberOfSuccessfulFollowedIndices();    Request putPatternRequest = new Request("PUT", "/_ccr/auto_follow/leader_cluster_pattern");    putPatternRequest.setJsonEntity("{\"leader_index_patterns\": [\"logs-*\"], \"remote_cluster\": \"leader_cluster\"}");    assertOK(client().performRequest(putPatternRequest));    putPatternRequest = new Request("PUT", "/_ccr/auto_follow/middle_cluster_pattern");    putPatternRequest.setJsonEntity("{\"leader_index_patterns\": [\"logs-*\"], \"remote_cluster\": \"middle_cluster\"}");    assertOK(client().performRequest(putPatternRequest));    try (RestClient leaderClient = buildLeaderClient()) {        Settings settings = Settings.builder().put("index.soft_deletes.enabled", true).build();        Request request = new Request("PUT", "/logs-20190101");        request.setJsonEntity("{\"settings\": " + Strings.toString(settings) + ", \"mappings\": {\"properties\": {\"field\": {\"type\": \"keyword\"}}}}");        assertOK(leaderClient.performRequest(request));        for (int i = 0; i < 5; i++) {            String id = Integer.toString(i);            index(leaderClient, "logs-20190101", id, "field", i, "filtered_field", "true");        }    }    try (RestClient middleClient = buildMiddleClient()) {        Settings settings = Settings.builder().put("index.soft_deletes.enabled", true).build();        Request request = new Request("PUT", "/logs-20200101");        request.setJsonEntity("{\"settings\": " + Strings.toString(settings) + ", \"mappings\": {\"properties\": {\"field\": {\"type\": \"keyword\"}}}}");        assertOK(middleClient.performRequest(request));        for (int i = 0; i < 5; i++) {            String id = Integer.toString(i);            index(middleClient, "logs-20200101", id, "field", i, "filtered_field", "true");        }    }    assertBusy(() -> {        assertThat(getNumberOfSuccessfulFollowedIndices(), equalTo(initialNumberOfSuccessfulFollowedIndices + 2));        ensureYellow("logs-20190101");        ensureYellow("logs-20200101");        verifyDocuments("logs-20190101", 5, "filtered_field:true");        verifyDocuments("logs-20200101", 5, "filtered_field:true");    });}
1
 List<SegmentJobBuildInfo> traceEarliestSegmentBuildJob()
{    List<SegmentJobBuildInfo> successJobs = Lists.newArrayList();    for (ConcurrentSkipListSet<SegmentJobBuildInfo> buildInfos : segmentBuildJobCheckList.values()) {        if (buildInfos.isEmpty()) {            continue;        }                SegmentJobBuildInfo segmentBuildJob = buildInfos.first();                try {            CubingJob cubingJob = (CubingJob) coordinator.getExecutableManager().getJob(segmentBuildJob.jobID);            if (cubingJob == null) {                                continue;            }            ExecutableState jobState = cubingJob.getStatus();            if (ExecutableState.SUCCEED.equals(jobState)) {                CubeManager cubeManager = coordinator.getCubeManager();                CubeInstance cubeInstance = cubeManager.getCube(segmentBuildJob.cubeName).latestCopyForWrite();                CubeSegment cubeSegment = cubeInstance.getSegment(segmentBuildJob.segmentName, null);                                coordinator.getClusterManager().segmentBuildComplete(cubingJob, cubeInstance, cubeSegment, segmentBuildJob);                addToCheckList(cubeInstance.getName());                successJobs.add(segmentBuildJob);            } else if (ExecutableState.ERROR.equals(jobState)) {                if (segmentBuildJob.retryCnt < 5) {                                        coordinator.getExecutableManager().resumeJob(segmentBuildJob.jobID);                    segmentBuildJob.retryCnt++;                } else {                                    }            } else {                            }        } catch (Exception e) {                    }    }    return successJobs;}
1
public Token<OzoneTokenIdentifier> getDelegationToken(Text renewer) throws OMException
{    Token<OzoneTokenIdentifier> token;    try {        if (!isAllowedDelegationTokenOp()) {            throw new OMException("Delegation Token can be issued only with " + "kerberos or web authentication", INVALID_AUTH_METHOD);        }        if (delegationTokenMgr == null || !delegationTokenMgr.isRunning()) {                        return null;        }        UserGroupInformation ugi = getRemoteUser();        String user = ugi.getUserName();        Text owner = new Text(user);        Text realUser = null;        if (ugi.getRealUser() != null) {            realUser = new Text(ugi.getRealUser().getUserName());        }        return delegationTokenMgr.createToken(owner, renewer, realUser);    } catch (OMException oex) {        throw oex;    } catch (IOException ex) {                throw new OMException("Get Delegation token failed.", ex, TOKEN_ERROR_OTHER);    }}
1
public synchronized boolean beginFlush()
{    if (flushing()) {                throw new ConnectException("OffsetStorageWriter is already flushing");    }    if (data.isEmpty())        return false;    assert !flushing();    toFlush = data;    data = new HashMap<>();    return true;}
1
private void addAuthInfoToConf(Properties authConfig, Configuration conf, String owner, String userList) throws IOException
{    List<String> users = new ArrayList<>(Arrays.asList(userList.split(",")));    users.add(owner);    for (String user : users) {        String keyTabFileConfKey = "hbase." + user + ".keytab.file";        String principalConfKey = "hbase." + user + ".kerberos.principal";        if (!authConfig.containsKey(keyTabFileConfKey) || !authConfig.containsKey(principalConfKey)) {            throw new IOException("Authentication configs missing for user : " + user);        }    }    for (String key : authConfig.stringPropertyNames()) {        conf.set(key, authConfig.getProperty(key));    }    }
1
public void stop(final String why)
{    if (refreshChore != null) {                refreshChore.cancel(true);    }    stopped = true;}
1
private boolean checkSetInputArrayIndexOutOfBoundsException(Compressor compressor)
{    try {        compressor.setInput(new byte[] { (byte) 0 }, 0, -1);    } catch (ArrayIndexOutOfBoundsException e) {        return true;    } catch (Exception e) {            }    return false;}
1
protected void logInCaseOfTemplateCopyFailure(Answer copyCommandAnswer, TemplateObjectTO sourceTemplate, DataStore destDataStore)
{    if (copyCommandAnswer != null && !copyCommandAnswer.getResult()) {        String failureDetails = StringUtils.EMPTY;        if (copyCommandAnswer.getDetails() != null) {            failureDetails = " Details: " + copyCommandAnswer.getDetails();        }            }}
1
public synchronized void parseLoginResponse(ContentResponse loginResponse, String responseContent) throws SalesforceException
{    final int responseStatus = loginResponse.getStatus();    try {        switch(responseStatus) {            case HttpStatus.OK_200:                                LoginToken token = objectMapper.readValue(responseContent, LoginToken.class);                                                accessToken = token.getAccessToken();                instanceUrl = Optional.ofNullable(config.getInstanceUrl()).orElse(token.getInstanceUrl());                                int lastChar = instanceUrl.length() - 1;                if (instanceUrl.charAt(lastChar) == '/') {                    instanceUrl = instanceUrl.substring(0, lastChar);                }                                for (SalesforceSessionListener listener : listeners) {                    try {                        listener.onLogin(accessToken, instanceUrl);                    } catch (Throwable t) {                                            }                }                break;            case HttpStatus.BAD_REQUEST_400:                                final LoginError error = objectMapper.readValue(responseContent, LoginError.class);                final String errorCode = error.getError();                final String msg = String.format("Login error code:[%s] description:[%s]", error.getError(), error.getErrorDescription());                final List<RestError> errors = new ArrayList<>();                errors.add(new RestError(errorCode, msg));                throw new SalesforceException(errors, HttpStatus.BAD_REQUEST_400);            default:                throw new SalesforceException(String.format("Login error status:[%s] reason:[%s]", responseStatus, loginResponse.getReason()), responseStatus);        }    } catch (IOException e) {        String msg = "Login error: response parse exception " + e.getMessage();        throw new SalesforceException(msg, e);    }}
1
public void testGetBookmarkReturnsBookmarkIterator()
{        assertTrue(set.addAll(list));    Iterator<Integer> bookmark = set.getBookmark();    assertEquals(bookmark.next(), list.get(0));    final int numAdvance = list.size() / 2;    for (int i = 1; i < numAdvance; i++) {        bookmark.next();    }    Iterator<Integer> bookmark2 = set.getBookmark();    assertEquals(bookmark2.next(), list.get(numAdvance));}
1
public void marshal(final CountDownLatch latch) throws Exception
{        Foo[] warmUpPayloads = createFoo(warmupCount);    for (Foo payload : warmUpPayloads) {        template.sendBody(payload);    }    final Foo[] payloads = createFoo(testCycleCount);    ExecutorService pool = Executors.newFixedThreadPool(20);    long start = System.currentTimeMillis();    for (int i = 0; i < payloads.length; i++) {        final int finalI = i;        pool.execute(new Runnable() {            public void run() {                template.sendBody(payloads[finalI]);            }        });    }    latch.await();    long end = System.currentTimeMillis();    }
1
 void updateWith(TaskHistoryLog from) throws IOException
{    if (TASKID == null)        TASKID = from.TASKID;    else if (!TASKID.equals(from.TASKID)) {        throw new IOException("Incorrect TASKID: " + from.TASKID + " expect " + TASKID);    }    if (TASK_TYPE == null)        TASK_TYPE = from.TASK_TYPE;    else if (!TASK_TYPE.equals(from.TASK_TYPE)) {                return;    }    if (from.TASK_STATUS != null)        TASK_STATUS = from.TASK_STATUS;    if (from.START_TIME > 0)        START_TIME = from.START_TIME;    if (from.FINISH_TIME > 0)        FINISH_TIME = from.FINISH_TIME;}
1
private void addSessionAttributesToMessageHeaders(ServerSession remote, Message message)
{    Set<String> attributeNames = remote.getAttributeNames();    for (String attributeName : attributeNames) {        Object attribute = remote.getAttribute(attributeName);        if (attribute instanceof Integer || attribute instanceof String || attribute instanceof Long || attribute instanceof Double || attribute instanceof Boolean) {            message.setHeader(attributeName, attribute);        } else {                                }    }}
1
protected void populateNMTokens(List<NMToken> nmTokens)
{    for (NMToken token : nmTokens) {        String nodeId = token.getNodeId().toString();        if (LOG.isDebugEnabled()) {            if (getNMTokenCache().containsToken(nodeId)) {                            } else {                            }        }        getNMTokenCache().setToken(nodeId, token.getToken());    }}
1
private List<PartETag> waitForAllPartUploads() throws IOException
{        try {        return Futures.allAsList(partETagsFutures).get();    } catch (InterruptedException ie) {                Thread.currentThread().interrupt();        return null;    } catch (ExecutionException ee) {                                        for (ListenableFuture<PartETag> future : partETagsFutures) {            future.cancel(true);        }                store.abortMultipartUpload(key, uploadId);        throw new IOException("Multi-part upload with id '" + uploadId + "' to " + key, ee);    }}
1
protected void onRegistrationFailure(Throwable failure)
{        if (Objects.equals(getTargetLeaderId(), currentJobMasterId)) {                jobLeaderListener.handleError(failure);    } else {            }}
1
private void reHash(MemorySegment[] oldBuckets, int oldNumBuckets, MemorySegment[] oldOverflowSegments) throws IOException
{    long reHashStartTime = System.currentTimeMillis();    inReHash = true;    int scanCount = -1;    while (true) {        scanCount++;        if (scanCount >= oldNumBuckets) {            break;        }                final int bucketArrayPos = scanCount >> table.bucketsPerSegmentBits;        int bucketInSegOffset = (scanCount & table.bucketsPerSegmentMask) << BUCKET_SIZE_BITS;        MemorySegment bucketSeg = oldBuckets[bucketArrayPos];        int countInBucket = bucketSeg.getShort(bucketInSegOffset + HEADER_COUNT_OFFSET);        int numInBucket = 0;        while (countInBucket != 0) {            int hashCodeOffset = bucketInSegOffset + BUCKET_HEADER_LENGTH;            int pointerOffset = bucketInSegOffset + BUCKET_POINTER_START_OFFSET;            while (numInBucket < countInBucket) {                int hashCode = bucketSeg.getInt(hashCodeOffset);                int pointer = bucketSeg.getInt(pointerOffset);                if (!insertToBucket(hashCode, pointer, true, false)) {                    buildBloomFilterAndFree(oldBuckets, oldNumBuckets, oldOverflowSegments);                    return;                }                numInBucket++;                hashCodeOffset += HASH_CODE_LEN;                pointerOffset += POINTER_LEN;            }                        int forwardPointer = bucketSeg.getInt(bucketInSegOffset + HEADER_FORWARD_OFFSET);            if (forwardPointer == BUCKET_FORWARD_POINTER_NOT_SET) {                break;            }            final int overflowSegIndex = forwardPointer >>> table.segmentSizeBits;            bucketSeg = oldOverflowSegments[overflowSegIndex];            bucketInSegOffset = forwardPointer & table.segmentSizeMask;            countInBucket = bucketSeg.getShort(bucketInSegOffset + HEADER_COUNT_OFFSET);            numInBucket = 0;        }    }    freeMemory(oldBuckets, oldOverflowSegments);    inReHash = false;    }
1
public long getPendingReconstructionBlocks()
{    try {        return getRBFMetrics().getNumOfBlocksPendingReplication();    } catch (IOException e) {            }    return 0;}
1
private boolean doVersionAdd(AddUpdateCommand cmd, long versionOnUpdate, boolean isReplayOrPeersync, boolean leaderLogic, boolean forwardedFromCollection, VersionBucket bucket) throws IOException
{    try {        BytesRef idBytes = cmd.getIndexedId();        bucket.signalAll();        if (versionsStored) {            long bucketVersion = bucket.highest;            if (leaderLogic) {                if (forwardedFromCollection && ulog.getState() == UpdateLog.State.ACTIVE) {                                                                                cmd.solrDoc.remove(CommonParams.VERSION_FIELD);                    versionOnUpdate = 0;                }                getUpdatedDocument(cmd, versionOnUpdate);                                if (forwardedFromCollection && ulog.getState() != UpdateLog.State.ACTIVE && isReplayOrPeersync == false) {                                                            cmd.setFlags(cmd.getFlags() | UpdateCommand.BUFFERING);                    ulog.add(cmd);                    return true;                }                if (versionOnUpdate != 0) {                    Long lastVersion = vinfo.lookupVersion(cmd.getIndexedId());                    long foundVersion = lastVersion == null ? -1 : lastVersion;                    if (versionOnUpdate == foundVersion || (versionOnUpdate < 0 && foundVersion < 0) || (versionOnUpdate == 1 && foundVersion > 0)) {                                                            } else {                        if (cmd.getReq().getParams().getBool(CommonParams.FAIL_ON_VERSION_CONFLICTS, true) == false) {                            return true;                        }                        throw new SolrException(ErrorCode.CONFLICT, "version conflict for " + cmd.getPrintableId() + " expected=" + versionOnUpdate + " actual=" + foundVersion);                    }                }                long version = vinfo.getNewClock();                cmd.setVersion(version);                cmd.getSolrInputDocument().setField(CommonParams.VERSION_FIELD, version);                bucket.updateHighest(version);            } else {                                cmd.setVersion(versionOnUpdate);                if (shouldBufferUpdate(cmd, isReplayOrPeersync, ulog.getState())) {                                        cmd.setFlags(cmd.getFlags() | UpdateCommand.BUFFERING);                    ulog.add(cmd);                    return true;                }                if (cmd.isInPlaceUpdate()) {                    long prev = cmd.prevVersion;                    Long lastVersion = vinfo.lookupVersion(cmd.getIndexedId());                    if (lastVersion == null || Math.abs(lastVersion) < prev) {                                                                                                                                                UpdateCommand fetchedFromLeader = fetchFullUpdateFromLeader(cmd, versionOnUpdate);                        if (fetchedFromLeader instanceof DeleteUpdateCommand) {                                                        versionDelete((DeleteUpdateCommand) fetchedFromLeader);                            return true;                        } else {                            assert fetchedFromLeader instanceof AddUpdateCommand;                                                                                                                                                                        cmd.solrDoc = ((AddUpdateCommand) fetchedFromLeader).solrDoc;                            cmd.prevVersion = -1;                            cmd.setVersion((long) cmd.solrDoc.getFieldValue(CommonParams.VERSION_FIELD));                            assert cmd.isInPlaceUpdate() == false;                        }                    } else {                        if (lastVersion != null && Math.abs(lastVersion) > prev) {                                                                                                                return true;                        } else {                                                        if (bucketVersion != 0 && bucketVersion < versionOnUpdate) {                                bucket.updateHighest(versionOnUpdate);                            }                        }                    }                } else {                                        if (bucketVersion != 0 && bucketVersion < versionOnUpdate) {                                                                        bucket.updateHighest(versionOnUpdate);                    } else {                                                                        Long lastVersion = vinfo.lookupVersion(cmd.getIndexedId());                        if (lastVersion != null && Math.abs(lastVersion) >= versionOnUpdate) {                                                                                    return true;                        }                    }                }                if (!isSubShardLeader && replicaType == Replica.Type.TLOG && (cmd.getFlags() & UpdateCommand.REPLAY) == 0) {                    cmd.setFlags(cmd.getFlags() | UpdateCommand.IGNORE_INDEXWRITER);                }            }        }        SolrInputDocument clonedDoc = shouldCloneCmdDoc() ? cmd.solrDoc.deepCopy() : null;                doLocalAdd(cmd);                if (req.getSchema().isUsableForChildDocs() && shouldRefreshUlogCaches(cmd)) {            ulog.openRealtimeSearcher();        }        if (clonedDoc != null) {            cmd.solrDoc = clonedDoc;        }    } finally {        bucket.unlock();    }    return false;}
1
public static void tearDown() throws Exception
{        HiveMetaStoreClient hmsc = new HiveMetaStoreClient(conf);    hmsc.dropDatabase(dbName, true, true, true);}
1
 void sidelineBigOverlaps(Collection<HbckRegionInfo> bigOverlap) throws IOException
{    int overlapsToSideline = bigOverlap.size() - hbck.getMaxMerge();    if (overlapsToSideline > hbck.getMaxOverlapsToSideline()) {        overlapsToSideline = hbck.getMaxOverlapsToSideline();    }    List<HbckRegionInfo> regionsToSideline = RegionSplitCalculator.findBigRanges(bigOverlap, overlapsToSideline);    FileSystem fs = FileSystem.get(conf);    for (HbckRegionInfo regionToSideline : regionsToSideline) {        try {                        hbck.closeRegion(regionToSideline);        } catch (IOException ioe) {                    } catch (InterruptedException e) {                    }        try {                        hbck.offline(regionToSideline.getRegionName());        } catch (IOException ioe) {                    }                Path sidelineRegionDir = hbck.sidelineRegionDir(fs, TO_BE_LOADED, regionToSideline);        if (sidelineRegionDir != null) {            sidelinedRegions.put(sidelineRegionDir, regionToSideline);                        hbck.fixes++;        }    }}
1
public InterpreterResult run(String st) throws IOException
{    try {        checkLocalProperties(context.getLocalProperties());        int parallelism = Integer.parseInt(context.getLocalProperties().getOrDefault("parallelism", defaultParallelism + ""));        Table table = stenv.sqlQuery(st);        this.schema = removeTimeAttributes(table.getSchema());        checkTableSchema(schema);                final RowTypeInfo outputType = new RowTypeInfo(schema.getFieldTypes(), schema.getFieldNames());                TypeInformation<Tuple2<Boolean, Row>> socketType = Types.TUPLE(Types.BOOLEAN, outputType);        TypeSerializer<Tuple2<Boolean, Row>> serializer = socketType.createSerializer(senv.getConfig());                iterator = new SocketStreamIterator<>(0, InetAddress.getByName(RemoteInterpreterUtils.findAvailableHostAddress()), serializer);                                CollectStreamTableSink collectTableSink = new CollectStreamTableSink(iterator.getBindAddress(), iterator.getPort(), serializer);        collectTableSink = collectTableSink.configure(outputType.getFieldNames(), outputType.getFieldTypes());                String originalCatalog = stenv.getCurrentCatalog();        String originalDatabase = stenv.getCurrentDatabase();        try {            stenv.useCatalog("default_catalog");            stenv.useDatabase("default_database");            stenv.registerTableSink(st, collectTableSink);            table.insertInto(new StreamQueryConfig(), st);        } finally {            stenv.useCatalog(originalCatalog);            stenv.useDatabase(originalDatabase);        }        ScheduledExecutorService refreshScheduler = Executors.newScheduledThreadPool(1);        long delay = 1000L;        long period = Long.parseLong(context.getLocalProperties().getOrDefault("refreshInterval", "3000"));        refreshScheduler.scheduleAtFixedRate(new RefreshTask(context), delay, period, MILLISECONDS);        ResultRetrievalThread retrievalThread = new ResultRetrievalThread(refreshScheduler);        retrievalThread.start();                JobExecutionResult jobExecutionResult = stenv.execute(st);                return new InterpreterResult(InterpreterResult.Code.SUCCESS);    } catch (Exception e) {                throw new IOException("Fail to run stream sql job", e);    }}
1
public InterpreterResult interpret(String st, InterpreterContext context) throws InterpreterException
{        flinkInterpreter.getZeppelinContext().setInterpreterContext(context);    flinkInterpreter.getZeppelinContext().setNoteGui(context.getNoteGui());    flinkInterpreter.getZeppelinContext().setGui(context.getGui());    checkLocalProperties(context.getLocalProperties());            ClassLoader originClassLoader = Thread.currentThread().getContextClassLoader();    try {        Thread.currentThread().setContextClassLoader(flinkInterpreter.getFlinkScalaShellLoader());        return runSqlList(st, context);    } finally {        Thread.currentThread().setContextClassLoader(originClassLoader);    }}
1
public void performBackgroundOperation(final OperationAndData<Void> operationAndData) throws Exception
{    final OperationTrace trace = client.getZookeeperClient().startAdvancedTracer("FindAndDeleteProtectedNodeInBackground");    AsyncCallback.Children2Callback callback = new AsyncCallback.Children2Callback() {        @Override        public void processResult(int rc, String path, Object o, List<String> strings, Stat stat) {            trace.setReturnCode(rc).setPath(path).setStat(stat).commit();            if (debugInsertError.compareAndSet(true, false)) {                rc = KeeperException.Code.CONNECTIONLOSS.intValue();            }            if (rc == KeeperException.Code.OK.intValue()) {                                final String node = CreateBuilderImpl.findNode(strings, "/", protectedId);                if (node != null) {                    try {                        String deletePath = client.unfixForNamespace(ZKPaths.makePath(namespaceAdjustedParentPath, node));                        client.delete().guaranteed().inBackground().forPath(deletePath);                    } catch (Exception e) {                        ThreadUtils.checkInterrupted(e);                                                rc = KeeperException.Code.CONNECTIONLOSS.intValue();                    }                }            }            if (rc != KeeperException.Code.OK.intValue()) {                CuratorEventImpl event = new CuratorEventImpl(client, CuratorEventType.CHILDREN, rc, path, null, o, stat, null, strings, null, null, null);                client.processBackgroundOperation(operationAndData, event);            }        }    };    client.getZooKeeper().getChildren(namespaceAdjustedParentPath, false, callback, null);}
1
public synchronized void beforeMerge(OnGoingMerge merge)
{    int maxNumMerges = mergeScheduler.getMaxMergeCount();    if (numMergesInFlight.incrementAndGet() > maxNumMerges) {        if (isThrottling.getAndSet(true) == false) {                        activateThrottling();        }    }}
1
 void releaseResource(Resource clusterResource, FiCaSchedulerApp application, Resource resource, String nodePartition, RMContainer rmContainer)
{    writeLock.lock();    try {        super.releaseResource(clusterResource, resource, nodePartition);                if (null != rmContainer && rmContainer.getNodeLabelExpression().equals(RMNodeLabelsManager.NO_LABEL) && !nodePartition.equals(RMNodeLabelsManager.NO_LABEL)) {            if (ignorePartitionExclusivityRMContainers.containsKey(nodePartition)) {                Set<RMContainer> rmContainers = ignorePartitionExclusivityRMContainers.get(nodePartition);                rmContainers.remove(rmContainer);                if (rmContainers.isEmpty()) {                    ignorePartitionExclusivityRMContainers.remove(nodePartition);                }            }        }                String userName = application.getUser();        User user = usersManager.updateUserResourceUsage(userName, resource, nodePartition, false);        metrics.setAvailableResourcesToUser(nodePartition, userName, application.getHeadroom());        if (LOG.isDebugEnabled()) {                    }    } finally {        writeLock.unlock();    }}
1
public void close() throws JMSException
{    if (ignoreClose) {        return;    }    if (closed.compareAndSet(false, true)) {        boolean invalidate = false;        try {                        getInternalSession().setMessageListener(null);                        for (Iterator<MessageConsumer> iter = consumers.iterator(); iter.hasNext(); ) {                MessageConsumer consumer = iter.next();                consumer.close();            }            for (Iterator<QueueBrowser> iter = browsers.iterator(); iter.hasNext(); ) {                QueueBrowser browser = iter.next();                browser.close();            }            if (transactional && !isXa) {                try {                    getInternalSession().rollback();                } catch (JMSException e) {                    invalidate = true;                                    }            }        } catch (JMSException ex) {            invalidate = true;                    } finally {            consumers.clear();            browsers.clear();            for (PooledSessionEventListener listener : this.sessionEventListeners) {                listener.onSessionClosed(this);            }            sessionEventListeners.clear();        }        if (invalidate) {                        if (sessionHolder != null) {                try {                    sessionHolder.close();                } catch (JMSException e1) {                    LOG.trace("Ignoring exception on close as discarding session: " + e1, e1);                }            }            try {                sessionPool.invalidateObject(key, sessionHolder);            } catch (Exception e) {                LOG.trace("Ignoring exception on invalidateObject as discarding session: " + e, e);            }        } else {            try {                sessionPool.returnObject(key, sessionHolder);            } catch (Exception e) {                javax.jms.IllegalStateException illegalStateException = new javax.jms.IllegalStateException(e.toString());                illegalStateException.initCause(e);                throw illegalStateException;            }        }        sessionHolder = null;    }}
1
public void onError(Exception e)
{    byte msgType = org.apache.thrift.protocol.TMessageType.REPLY;    org.apache.thrift.TBase msg;    mark_compacted_result result = new mark_compacted_result();    if (e instanceof MetaException) {        result.o1 = (MetaException) e;        result.setO1IsSet(true);        msg = result;    } else {        msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;        msg = (org.apache.thrift.TBase) new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());    }    try {        fcall.sendResponse(fb, msg, msgType, seqid);        return;    } catch (Exception ex) {            }    fb.close();}
1
private void uploadCommandAssertCount(S3AFileSystem fs, String[] options, Path path, int numUploads, int ageSeconds) throws Exception
{    List<String> allOptions = new ArrayList<>();    List<String> output = new ArrayList<>();    S3GuardTool.Uploads cmd = new S3GuardTool.Uploads(fs.getConf());    ByteArrayOutputStream buf = new ByteArrayOutputStream();    allOptions.add(cmd.getName());    allOptions.addAll(Arrays.asList(options));    if (ageSeconds > 0) {        allOptions.add("-" + Uploads.SECONDS_FLAG);        allOptions.add(String.valueOf(ageSeconds));    }    allOptions.add(path.toString());    exec(0, "", cmd, buf, allOptions.toArray(new String[0]));    try (BufferedReader reader = new BufferedReader(new InputStreamReader(new ByteArrayInputStream(buf.toByteArray())))) {        String line;        while ((line = reader.readLine()) != null) {            String[] fields = line.split("\\s");            if (fields.length == 4 && fields[0].equals(Uploads.TOTAL)) {                int parsedUploads = Integer.valueOf(fields[1]);                                assertEquals("Unexpected number of uploads", numUploads, parsedUploads);                return;            }                        output.add(line);        }    }    fail("Command output did not match: \n" + StringUtils.join("\n", output));}
1
public void run()
{    try {        Session session = producerConnection.createSession(false, Session.AUTO_ACKNOWLEDGE);        MessageProducer producer = session.createProducer(destination);        producer.setDeliveryMode(deliveryMode);        for (int idx = 0; idx < MESSAGES_COUNT; ++idx) {            Message message = session.createTextMessage(new String(buf) + idx);            producer.send(message);            messagesSent.incrementAndGet();            producerHasSentTenMessages.countDown();            Thread.sleep(10);            if (idx != 0 && idx % 100 == 0) {                                            }        }        producer.close();        session.close();    } catch (Throwable ex) {        ex.printStackTrace();    }}
1
public void testAsynchronous() throws Exception
{    MockEndpoint mock = getMockEndpoint("mock:result");    mock.expectedMessageCount(100);    mock.expectsNoDuplicates(body());    StopWatch watch = new StopWatch();    for (int i = 0; i < 100; i++) {        template.sendBody("seda:start", "" + i);    }        assertMockEndpointsSatisfied(20, TimeUnit.SECONDS);    }
1
public static void setup()
{    try {        fs = FileSystem.get(getConf());        listFile = new Path("target/tmp/listing").makeQualified(fs.getUri(), fs.getWorkingDirectory());        target = new Path("target/tmp/target").makeQualified(fs.getUri(), fs.getWorkingDirectory());        root = new Path("target/tmp").makeQualified(fs.getUri(), fs.getWorkingDirectory()).toString();        TestDistCpUtils.delete(fs, root);    } catch (IOException e) {            }}
1
public void onError(Exception e)
{    byte msgType = org.apache.thrift.protocol.TMessageType.REPLY;    org.apache.thrift.TBase msg;    update_partition_column_statistics_req_result result = new update_partition_column_statistics_req_result();    if (e instanceof NoSuchObjectException) {        result.o1 = (NoSuchObjectException) e;        result.setO1IsSet(true);        msg = result;    } else if (e instanceof InvalidObjectException) {        result.o2 = (InvalidObjectException) e;        result.setO2IsSet(true);        msg = result;    } else if (e instanceof MetaException) {        result.o3 = (MetaException) e;        result.setO3IsSet(true);        msg = result;    } else if (e instanceof InvalidInputException) {        result.o4 = (InvalidInputException) e;        result.setO4IsSet(true);        msg = result;    } else {        msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;        msg = (org.apache.thrift.TBase) new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());    }    try {        fcall.sendResponse(fb, msg, msgType, seqid);        return;    } catch (Exception ex) {            }    fb.close();}
1
public void learn(Stream<Double[]> features, Stream<Double> labels)
{    double[] labelArray = ArrayUtils.toPrimitive(labels.toArray(Double[]::new));    double[][] featuresMatrix = features.map(feature -> ArrayUtils.toPrimitive(feature)).toArray(double[][]::new);    if (!supportOnlineLearning) {        regression.clear();    }    regression.addObservations(featuresMatrix, labelArray);    results = regression.regress();    }
1
public void setRpcTimeout(long value)
{    DatabaseDescriptor.setRpcTimeout(value);    }
1
protected boolean expireOrUpdate(long expVal, ThreadIdentifier ti)
{        boolean expired = false;    synchronized (this) {        if (expVal == this.lastDispatchedSequenceId && isCountersEmpty()) {            try {                                                owningQueue.destroyFromQueue(ti);                this.lastDispatchedSequenceId = TOKEN_DESTROYED;                owningQueue.eventsMap.remove(ti);                expired = true;                this.owningQueue.getStatistics().decThreadIdentifiers();            } catch (RegionDestroyedException e) {                if (!owningQueue.destroyInProgress && logger.isDebugEnabled()) {                                    }            } catch (EntryNotFoundException enfe) {                if (!owningQueue.destroyInProgress) {                                    }            }        }    }    if (!expired) {        try {                        owningQueue.region.put(ti, this.lastDispatchedSequenceId);        } catch (CancelException e) {            throw e;        } catch (Exception e) {            if (!owningQueue.destroyInProgress) {                            }        }    }    return expired;}
1
public void testAddMergeRegions() throws IOException
{    TableName tn = TableName.valueOf(this.name.getMethodName());    Put put = new Put(Bytes.toBytes(this.name.getMethodName()));    List<RegionInfo> ris = new ArrayList<>();    int limit = 10;    byte[] previous = HConstants.EMPTY_START_ROW;    for (int i = 0; i < limit; i++) {        RegionInfo ri = RegionInfoBuilder.newBuilder(tn).setStartKey(previous).setEndKey(Bytes.toBytes(i)).build();        ris.add(ri);    }    put = MetaTableAccessor.addMergeRegions(put, ris);    List<Cell> cells = put.getFamilyCellMap().get(HConstants.CATALOG_FAMILY);    String previousQualifier = null;    assertEquals(limit, cells.size());    for (Cell cell : cells) {                String qualifier = Bytes.toString(cell.getQualifierArray());        assertTrue(qualifier.startsWith(HConstants.MERGE_QUALIFIER_PREFIX_STR));        assertNotEquals(qualifier, previousQualifier);        previousQualifier = qualifier;    }}
1
protected void sleepFor(int ms)
{                sleptFor += ms;}
1
protected static void fadviseIfAvailable(FSDataInputStream inputStream, long off, long n)
{    Method readAhead;    final Class adviceType;    try {        adviceType = Class.forName("org.apache.hadoop.fs.FSDataInputStream$FadviseType");    } catch (ClassNotFoundException e) {                readAhead = null;        return;    }    try {        Class<? extends FSDataInputStream> inputStreamClass = inputStream.getClass();        readAhead = inputStreamClass.getMethod("adviseFile", new Class[] { adviceType, long.class, long.class });    } catch (NoSuchMethodException e) {                readAhead = null;        return;    }    if (readAhead != null) {        Object[] adviceTypeValues = adviceType.getEnumConstants();        for (int idx = 0; idx < adviceTypeValues.length; idx++) {            if ((adviceTypeValues[idx]).toString().contains("SEQUENTIAL")) {                try {                    readAhead.invoke(inputStream, adviceTypeValues[idx], off, n);                } catch (IllegalAccessException e) {                                    } catch (InvocationTargetException e) {                                    }                break;            }        }    }    return;}
1
 String getHostsInformation(StackAdvisorRequest request) throws StackAdvisorException
{    List<String> hostNames = new ArrayList<>(request.getHosts());        List<JsonNode> resultInfos = new ArrayList<>();    if (hostInfoCache != null && !hostInfoCache.isEmpty()) {        Iterator<String> hostNamesIterator = hostNames.iterator();        while (hostNamesIterator.hasNext()) {            String hostName = hostNamesIterator.next();            JsonNode node = hostInfoCache.get(hostName);            if (node != null) {                resultInfos.add(node);                hostNamesIterator.remove();            }        }    }    String hostsJSON = null;        if (!hostNames.isEmpty()) {                String hostsURI = String.format(GET_HOSTS_INFO_URI, String.join(",", hostNames));        Response response = handleRequest(null, null, new LocalUriInfo(hostsURI), Request.Type.GET, createHostResource());        if (response.getStatus() != Status.OK.getStatusCode()) {            String message = String.format("Error occured during hosts information retrieving, status=%s, response=%s", response.getStatus(), (String) response.getEntity());                        throw new StackAdvisorException(message);        }        hostsJSON = (String) response.getEntity();        if (LOG.isDebugEnabled()) {                    }    }        if (hostInfoCache != null) {        if (hostsJSON != null && !hostsJSON.isEmpty()) {            try {                JsonNode root = mapper.readTree(hostsJSON);                Iterator<JsonNode> iterator = root.get("items").getElements();                while (iterator.hasNext()) {                    JsonNode next = iterator.next();                    String hostName = next.get("Hosts").get("host_name").getTextValue();                    hostInfoCache.put(hostName, next);                    resultInfos.add(next);                }            } catch (IOException e) {                throw new StackAdvisorException("Error occured during parsing result host infos", e);            }        }        String fullHostsURI = String.format(GET_HOSTS_INFO_URI, request.getHostsCommaSeparated());        JsonNodeFactory f = JsonNodeFactory.instance;        ObjectNode resultRoot = f.objectNode();        resultRoot.put("href", fullHostsURI);        ArrayNode resultArray = resultRoot.putArray("items");        resultArray.addAll(resultInfos);        hostsJSON = resultRoot.toString();    }    Collection<String> unregistered = getUnregisteredHosts(hostsJSON, request.getHosts());    if (unregistered.size() > 0) {        String message = String.format("There are unregistered hosts in the request, %s", Arrays.toString(unregistered.toArray()));                throw new StackAdvisorException(message);    }    return hostsJSON;}
1
public void applicationStarted(ApplicationStartData appStart) throws IOException
{    HistoryFileWriter hfWriter = outstandingWriters.get(appStart.getApplicationId());    if (hfWriter == null) {        Path applicationHistoryFile = new Path(rootDirPath, appStart.getApplicationId().toString());        try {            hfWriter = new HistoryFileWriter(applicationHistoryFile);                    } catch (IOException e) {                        throw e;        }        outstandingWriters.put(appStart.getApplicationId(), hfWriter);    } else {        throw new IOException("History file of application " + appStart.getApplicationId() + " is already opened");    }    assert appStart instanceof ApplicationStartDataPBImpl;    try {        hfWriter.writeHistoryData(new HistoryDataKey(appStart.getApplicationId().toString(), START_DATA_SUFFIX), ((ApplicationStartDataPBImpl) appStart).getProto().toByteArray());            } catch (IOException e) {                throw e;    }}
1
private ArrayList<Long> runJobInSequence(JobConf masterJobConf, int numRuns) throws IOException
{    Random rand = new Random();    ArrayList<Long> execTimes = new ArrayList<Long>();    for (int i = 0; i < numRuns; i++) {                JobConf jobConf = new JobConf(masterJobConf);                jobConf.setJar(masterJobConf.getJar());                FileOutputFormat.setOutputPath(jobConf, new Path(OUTPUT_DIR, "output_" + rand.nextInt()));                        long curTime = System.currentTimeMillis();        JobClient.runJob(jobConf);        execTimes.add(new Long(System.currentTimeMillis() - curTime));    }    return execTimes;}
1
public void onError(Exception e)
{    byte msgType = org.apache.thrift.protocol.TMessageType.REPLY;    org.apache.thrift.TBase msg;    revoke_privileges_result result = new revoke_privileges_result();    if (e instanceof MetaException) {        result.o1 = (MetaException) e;        result.setO1IsSet(true);        msg = result;    } else {        msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;        msg = (org.apache.thrift.TBase) new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());    }    try {        fcall.sendResponse(fb, msg, msgType, seqid);        return;    } catch (Exception ex) {            }    fb.close();}
1
private void setupRemoteResource(ApplicationId appId, DynoResource resource, Map<String, String> env, String... srcPaths) throws IOException
{    FileStatus remoteFileStatus;    Path dstPath;    Preconditions.checkArgument(srcPaths.length > 0, "Must supply at least one source path");    Preconditions.checkArgument(resource.getType() == LocalResourceType.ARCHIVE || srcPaths.length == 1, "Can only specify multiple source paths if using an ARCHIVE type");    List<URI> srcURIs = Arrays.stream(srcPaths).map(URI::create).collect(Collectors.toList());    Set<String> srcSchemes = srcURIs.stream().map(URI::getScheme).collect(Collectors.toSet());    Preconditions.checkArgument(srcSchemes.size() == 1, "All source paths must have the same scheme");    String srcScheme = srcSchemes.iterator().next();    String srcPathString = "[" + Joiner.on(",").join(srcPaths) + "]";    if (srcScheme == null || srcScheme.equals(FileSystem.getLocal(getConf()).getScheme()) || srcScheme.equals("jar")) {                List<File> srcFiles = srcURIs.stream().map(URI::getSchemeSpecificPart).map(File::new).collect(Collectors.toList());        Path dstPathBase = getRemoteStoragePath(getConf(), appId);        boolean shouldArchive = srcFiles.size() > 1 || srcFiles.get(0).isDirectory() || (resource.getType() == LocalResourceType.ARCHIVE && Arrays.stream(ARCHIVE_FILE_TYPES).noneMatch(suffix -> srcFiles.get(0).getName().endsWith(suffix)));        if (shouldArchive) {            if ("jar".equals(srcScheme)) {                throw new IllegalArgumentException(String.format("Resources in JARs " + "can't be zipped; resource %s is ARCHIVE and src is: %s", resource.getResourcePath(), srcPathString));            } else if (resource.getType() != LocalResourceType.ARCHIVE) {                throw new IllegalArgumentException(String.format("Resource type is %s but srcPaths were: %s", resource.getType(), srcPathString));            }            dstPath = new Path(dstPathBase, resource.getResourcePath()).suffix(".zip");        } else {            dstPath = new Path(dstPathBase, srcFiles.get(0).getName());        }        FileSystem remoteFS = dstPath.getFileSystem(getConf());                try (OutputStream outputStream = remoteFS.create(dstPath, true)) {            if ("jar".equals(srcScheme)) {                try (InputStream inputStream = new URL(srcPaths[0]).openStream()) {                    IOUtils.copyBytes(inputStream, outputStream, getConf());                }            } else if (shouldArchive) {                List<File> filesToZip;                if (srcFiles.size() == 1 && srcFiles.get(0).isDirectory()) {                    File[] childFiles = srcFiles.get(0).listFiles();                    if (childFiles == null || childFiles.length == 0) {                        throw new IllegalArgumentException("Specified a directory to archive with no contents");                    }                    filesToZip = Lists.newArrayList(childFiles);                } else {                    filesToZip = srcFiles;                }                ZipOutputStream zout = new ZipOutputStream(outputStream);                for (File fileToZip : filesToZip) {                    addFileToZipRecursively(fileToZip.getParentFile(), fileToZip, zout);                }                zout.close();            } else {                try (InputStream inputStream = new FileInputStream(srcFiles.get(0))) {                    IOUtils.copyBytes(inputStream, outputStream, getConf());                }            }        }        remoteFileStatus = remoteFS.getFileStatus(dstPath);    } else {        if (srcPaths.length > 1) {            throw new IllegalArgumentException("If resource is on remote, must be " + "a single file: " + srcPathString);        }                dstPath = new Path(srcPaths[0]);                remoteFileStatus = FileSystem.get(dstPath.toUri(), getConf()).getFileStatus(dstPath);        if (remoteFileStatus.isDirectory()) {            throw new IllegalArgumentException("If resource is on remote " + "filesystem, must be a file: " + srcPaths[0]);        }    }    env.put(resource.getLocationEnvVar(), dstPath.toString());    env.put(resource.getTimestampEnvVar(), String.valueOf(remoteFileStatus.getModificationTime()));    env.put(resource.getLengthEnvVar(), String.valueOf(remoteFileStatus.getLen()));}
1
public DataFormat newInstance()
{    SnakeYAMLDataFormat dataformat = new SnakeYAMLDataFormat();    if (CamelContextAware.class.isAssignableFrom(SnakeYAMLDataFormat.class)) {        CamelContextAware contextAware = CamelContextAware.class.cast(dataformat);        if (contextAware != null) {            contextAware.setCamelContext(camelContext);        }    }    try {        Map<String, Object> parameters = new HashMap<>();        IntrospectionSupport.getProperties(configuration, parameters, null, false);        CamelPropertiesHelper.setCamelProperties(camelContext, dataformat, parameters, false);    } catch (Exception e) {        throw new RuntimeCamelException(e);    }    if (ObjectHelper.isNotEmpty(customizers)) {        for (DataFormatCustomizer<SnakeYAMLDataFormat> customizer : customizers) {            boolean useCustomizer = (customizer instanceof HasId) ? HierarchicalPropertiesEvaluator.evaluate(applicationContext.getEnvironment(), "camel.dataformat.customizer", "camel.dataformat.yaml-snakeyaml.customizer", ((HasId) customizer).getId()) : HierarchicalPropertiesEvaluator.evaluate(applicationContext.getEnvironment(), "camel.dataformat.customizer", "camel.dataformat.yaml-snakeyaml.customizer");            if (useCustomizer) {                                customizer.customize(dataformat);            }        }    }    return dataformat;}
1
public void perform() throws Exception
{    ClusterMetrics status = this.cluster.getClusterMetrics();    List<ServerName> victimServers = new LinkedList<>(status.getLiveServerMetrics().keySet());    Set<ServerName> killedServers = new HashSet<>();    int liveCount = (int) Math.ceil(FRC_SERVERS_THAT_HOARD_AND_LIVE * victimServers.size());    int deadCount = (int) Math.ceil(FRC_SERVERS_THAT_HOARD_AND_DIE * victimServers.size());    Assert.assertTrue("There are not enough victim servers: " + victimServers.size(), liveCount + deadCount < victimServers.size());    List<ServerName> targetServers = new ArrayList<>(liveCount);    for (int i = 0; i < liveCount + deadCount; ++i) {        int victimIx = RandomUtils.nextInt(0, victimServers.size());        targetServers.add(victimServers.remove(victimIx));    }    unbalanceRegions(status, victimServers, targetServers, HOARD_FRC_OF_REGIONS);    Thread.sleep(waitForUnbalanceMilliSec);    ServerName metaServer = cluster.getServerHoldingMeta();    for (ServerName targetServer : targetServers) {                if (context.isStopping()) {            break;        }        if (killedServers.size() >= liveCount) {            break;        }        if (!killMetaRs && targetServer.equals(metaServer)) {                    } else {            killRs(targetServer);            killedServers.add(targetServer);        }    }    Thread.sleep(waitForKillsMilliSec);    forceBalancer();    Thread.sleep(waitAfterBalanceMilliSec);    for (ServerName server : killedServers) {        startRs(server);    }}
1
private void monitorAsyncTask(final Future<?> taskFuture, final Future<?> monitoringFuture, final long completionTimestamp)
{    if (taskFuture.isDone()) {                monitoringFuture.cancel(false);    } else if (System.currentTimeMillis() > completionTimestamp) {                taskFuture.cancel(true);                                monitoringFuture.cancel(false);        final Processor processor = processorRef.get().getProcessor();            }}
1
public void resumeTransaction(long producerId, short epoch)
{    synchronized (producerClosingLock) {        ensureNotClosed();        Preconditions.checkState(producerId >= 0 && epoch >= 0, "Incorrect values for producerId %s and epoch %s", producerId, epoch);                Object transactionManager = getValue(kafkaProducer, "transactionManager");        synchronized (transactionManager) {            Object sequenceNumbers = getValue(transactionManager, "sequenceNumbers");            invoke(transactionManager, "transitionTo", getEnum("org.apache.kafka.clients.producer.internals.TransactionManager$State.INITIALIZING"));            invoke(sequenceNumbers, "clear");            Object producerIdAndEpoch = getValue(transactionManager, "producerIdAndEpoch");            setValue(producerIdAndEpoch, "producerId", producerId);            setValue(producerIdAndEpoch, "epoch", epoch);            invoke(transactionManager, "transitionTo", getEnum("org.apache.kafka.clients.producer.internals.TransactionManager$State.READY"));            invoke(transactionManager, "transitionTo", getEnum("org.apache.kafka.clients.producer.internals.TransactionManager$State.IN_TRANSACTION"));            setValue(transactionManager, "transactionStarted", true);        }    }}
1
public void minorTableLegacy() throws Exception
{        Table t = newTable("default", "mtl", false);    addLegacyFile(t, null, 20);    addDeltaFile(t, null, 21L, 22L, 2);    addDeltaFile(t, null, 23L, 24L, 2);    burnThroughTransactions("default", "mtl", 25);    CompactionRequest rqst = new CompactionRequest("default", "mtl", CompactionType.MINOR);    txnHandler.compact(rqst);    startWorker();    ShowCompactResponse rsp = txnHandler.showCompact(new ShowCompactRequest());    List<ShowCompactResponseElement> compacts = rsp.getCompacts();    Assert.assertEquals(1, compacts.size());    Assert.assertEquals("ready for cleaning", compacts.get(0).getState());        FileSystem fs = FileSystem.get(conf);    FileStatus[] stat = fs.listStatus(new Path(t.getSd().getLocation()));        boolean sawNewDelta = false;    for (int i = 0; i < stat.length; i++) {        if (stat[i].getPath().getName().equals(makeDeltaDirNameCompacted(21, 24) + "_v0000026")) {            sawNewDelta = true;            FileStatus[] buckets = fs.listStatus(stat[i].getPath(), AcidUtils.hiddenFileFilter);            Assert.assertEquals(2, buckets.length);            Assert.assertTrue(buckets[0].getPath().getName().matches("bucket_0000[01]"));            Assert.assertTrue(buckets[1].getPath().getName().matches("bucket_0000[01]"));        } else {                    }    }    Assert.assertTrue(toString(stat), sawNewDelta);}
1
public InterpreterResult interpret(final String st, final InterpreterContext context) throws InterpreterException
{    if (LOGGER.isDebugEnabled()) {            }    final FormType form = getFormType();    RemoteInterpreterProcess interpreterProcess = null;    try {        interpreterProcess = getOrCreateInterpreterProcess();    } catch (IOException e) {        throw new InterpreterException(e);    }    if (!interpreterProcess.isRunning()) {        throw new InterpreterException("Interpreter process is not running:\n" + interpreterProcess.getErrorMessage());    }    this.lifecycleManager.onInterpreterUse(this.getInterpreterGroup(), sessionId);    return interpreterProcess.callRemoteFunction(new RemoteInterpreterProcess.RemoteFunction<InterpreterResult>() {        @Override        public InterpreterResult call(Client client) throws Exception {            RemoteInterpreterResult remoteResult = client.interpret(sessionId, className, st, convert(context));            Map<String, Object> remoteConfig = (Map<String, Object>) gson.fromJson(remoteResult.getConfig(), new TypeToken<Map<String, Object>>() {            }.getType());            context.getConfig().clear();            if (remoteConfig != null) {                context.getConfig().putAll(remoteConfig);            }            GUI currentGUI = context.getGui();            GUI currentNoteGUI = context.getNoteGui();            if (form == FormType.NATIVE) {                GUI remoteGui = GUI.fromJson(remoteResult.getGui());                GUI remoteNoteGui = GUI.fromJson(remoteResult.getNoteGui());                currentGUI.clear();                currentGUI.setParams(remoteGui.getParams());                currentGUI.setForms(remoteGui.getForms());                currentNoteGUI.setParams(remoteNoteGui.getParams());                currentNoteGUI.setForms(remoteNoteGui.getForms());            } else if (form == FormType.SIMPLE) {                final Map<String, Input> currentForms = currentGUI.getForms();                final Map<String, Object> currentParams = currentGUI.getParams();                final GUI remoteGUI = GUI.fromJson(remoteResult.getGui());                final Map<String, Input> remoteForms = remoteGUI.getForms();                final Map<String, Object> remoteParams = remoteGUI.getParams();                currentForms.putAll(remoteForms);                currentParams.putAll(remoteParams);            }            InterpreterResult result = convert(remoteResult);            return result;        }    });}
1
public void run()
{        if (numEvictionLoopsCompleted < getTestAbortAfterLoopCount()) {        try {                        if (EVICT_HIGH_ENTRY_COUNT_BUCKETS_FIRST) {                createAndSubmitWeightedRegionEvictionTasks();            } else {                for (RegionEvictorTask task : createRegionEvictionTasks()) {                    executeInThreadPool(task);                }            }                        synchronized (evictionLock) {                int delayTime = getEvictionLoopDelayTime();                if (logger.isDebugEnabled()) {                                    }                numEvictionLoopsCompleted++;                try {                                                                                evictionLock.wait(delayTime);                } catch (InterruptedException ignored) {                                }            }                        if (HeapEvictor.this.mustEvict.get()) {                                                executeInThreadPool(this);            }        } catch (RegionDestroyedException ignored) {                        if (HeapEvictor.this.mustEvict.get()) {                executeInThreadPool(this);            }        }    }}
1
public static void setKylinConfigInEnvIfMissing(Properties prop)
{    synchronized (KylinConfig.class) {        if (SYS_ENV_INSTANCE == null) {            try {                KylinConfig config = new KylinConfig();                config.reloadKylinConfig(prop);                                SYS_ENV_INSTANCE = config;            } catch (IllegalArgumentException e) {                throw new IllegalStateException("Failed to find KylinConfig ", e);            }        }    }}
1
protected Destination createDestination(String destName) throws JMSException
{    String simpleName = getSimpleName(destName);    byte destinationType = getDestinationType(destName);    if (destinationType == ActiveMQDestination.QUEUE_TYPE) {                return getSession().createQueue(simpleName);    } else if (destinationType == ActiveMQDestination.TOPIC_TYPE) {                return getSession().createTopic(simpleName);    } else {        return createTemporaryDestination(destName);    }}
1
private void deriveCapacityFromAbsoluteConfigurations(String label, Resource clusterResource, ResourceCalculator rc, CSQueue childQueue)
{    /*     * In case when queues are configured with absolute resources, it is better     * to update capacity/max-capacity etc w.r.t absolute resource as well. In     * case of computation, these values wont be used any more. However for     * metrics and UI, its better these values are pre-computed here itself.     */        childQueue.getQueueCapacities().setCapacity(label, rc.divide(clusterResource, childQueue.getQueueResourceQuotas().getEffectiveMinResource(label), getQueueResourceQuotas().getEffectiveMinResource(label)));        childQueue.getQueueCapacities().setMaximumCapacity(label, rc.divide(clusterResource, childQueue.getQueueResourceQuotas().getEffectiveMaxResource(label), getQueueResourceQuotas().getEffectiveMaxResource(label)));            childQueue.getQueueCapacities().setAbsoluteCapacity(label, childQueue.getQueueCapacities().getCapacity(label) * getQueueCapacities().getAbsoluteCapacity(label));            childQueue.getQueueCapacities().setAbsoluteMaximumCapacity(label, childQueue.getQueueCapacities().getMaximumCapacity(label) * getQueueCapacities().getAbsoluteMaximumCapacity(label));        if (childQueue instanceof LeafQueue) {        LeafQueue leafQueue = (LeafQueue) childQueue;        CapacitySchedulerConfiguration conf = csContext.getConfiguration();        int maxApplications = (int) (conf.getMaximumSystemApplications() * childQueue.getQueueCapacities().getAbsoluteCapacity(label));        leafQueue.setMaxApplications(maxApplications);        int maxApplicationsPerUser = Math.min(maxApplications, (int) (maxApplications * (leafQueue.getUsersManager().getUserLimit() / 100.0f) * leafQueue.getUsersManager().getUserLimitFactor()));        leafQueue.setMaxApplicationsPerUser(maxApplicationsPerUser);            }}
1
public void abort(String why, Throwable e)
{        this.aborted = true;    stop(why);}
1
public static void deleteRecursive(ZooKeeper zk, final String pathRoot, VoidCallback cb, Object ctx) throws InterruptedException, KeeperException
{    PathUtils.validatePath(pathRoot);    List<String> tree = listSubTreeBFS(zk, pathRoot);        for (int i = tree.size() - 1; i >= 0; --i) {                        zk.delete(tree.get(i), -1, cb, ctx);    }}
1
public Map<String, String> getPartitionLocations(String catName, String dbName, String tblName, String baseLocationToNotShow, int max)
{    catName = normalizeIdentifier(catName);    dbName = normalizeIdentifier(dbName);    tblName = normalizeIdentifier(tblName);    boolean success = false;    Query query = null;    Map<String, String> partLocations = new HashMap<>();    try {        openTransaction();                query = pm.newQuery(MPartition.class);        query.setFilter("this.table.database.catalogName == t1 && this.table.database.name == t2 " + "&& this.table.tableName == t3");        query.declareParameters("String t1, String t2, String t3");        query.setResult("this.partitionName, this.sd.location");        if (max >= 0) {                        query.setRange(0, max);        }        List<Object[]> result = (List<Object[]>) query.execute(catName, dbName, tblName);        for (Object[] row : result) {            String location = (String) row[1];            if (baseLocationToNotShow != null && location != null && FileUtils.isSubdirectory(baseLocationToNotShow, location)) {                location = null;            }            partLocations.put((String) row[0], location);        }                success = commitTransaction();    } finally {        if (!success) {            rollbackTransaction();        }        if (query != null) {            query.closeAll();        }    }    return partLocations;}
1
public void drainDestructableClaims(final Collection<ResourceClaim> destination, final int maxElements)
{    final int drainedCount = destructableClaims.drainTo(destination, maxElements);    }
1
protected void doSetup(Context context) throws IOException
{    super.bindCurrentConfiguration(context.getConfiguration());    Configuration conf = context.getConfiguration();    mos = new MultipleOutputs(context);    KylinConfig config = AbstractHadoopJob.loadKylinPropsAndMetadata();    String cubeName = conf.get(BatchConstants.CFG_CUBE_NAME);    CubeInstance cube = CubeManager.getInstance(config).getCube(cubeName);    cubeConfig = cube.getConfig();    cubeDesc = cube.getDescriptor();    taskId = context.getTaskAttemptID().getTaskID().getId();    reducerMapping = new FactDistinctColumnsReducerMapping(cube);        if (reducerMapping.isCuboidRowCounterReducer(taskId)) {                isStatistics = true;        baseCuboidId = cube.getCuboidScheduler().getBaseCuboidId();        baseCuboidRowCountInMappers = Lists.newArrayList();        cuboidHLLMap = Maps.newHashMap();        samplingPercentage = Integer.parseInt(context.getConfiguration().get(BatchConstants.CFG_STATISTICS_SAMPLING_PERCENT));            } else {                col = reducerMapping.getColForReducer(taskId);        Preconditions.checkNotNull(col);                buildDictInReducer = config.isBuildDictInReducerEnabled();        if (cubeDesc.getDictionaryBuilderClass(col) != null) {                        buildDictInReducer = false;        }        if (reducerMapping.getReducerNumForDimCol(col) > 1) {                        buildDictInReducer = false;        }        if (buildDictInReducer) {            builder = DictionaryGenerator.newDictionaryBuilder(col.getType());            builder.init(null, 0, null);        }            }}
1
private boolean doCommandLine(final String[] args)
{    if (args.length < 1) {        printUsage(null);        return false;    }    try {        for (int i = 0; i < args.length; i++) {            String cmd = args[i];            if (cmd.equals("-h") || cmd.startsWith("--h")) {                printUsage(null);                return false;            }            final String startRowArgKey = "--startrow=";            if (cmd.startsWith(startRowArgKey)) {                startRow = cmd.substring(startRowArgKey.length());                continue;            }            final String stopRowArgKey = "--stoprow=";            if (cmd.startsWith(stopRowArgKey)) {                stopRow = cmd.substring(stopRowArgKey.length());                continue;            }            final String startTimeArgKey = "--starttime=";            if (cmd.startsWith(startTimeArgKey)) {                startTime = Long.parseLong(cmd.substring(startTimeArgKey.length()));                continue;            }            final String endTimeArgKey = "--endtime=";            if (cmd.startsWith(endTimeArgKey)) {                endTime = Long.parseLong(cmd.substring(endTimeArgKey.length()));                continue;            }            final String batchArgKey = "--batch=";            if (cmd.startsWith(batchArgKey)) {                batch = Integer.parseInt(cmd.substring(batchArgKey.length()));                continue;            }            final String cacheRowArgKey = "--cacheRow=";            if (cmd.startsWith(cacheRowArgKey)) {                cacheRow = Integer.parseInt(cmd.substring(cacheRowArgKey.length()));                continue;            }            final String versionsArgKey = "--versions=";            if (cmd.startsWith(versionsArgKey)) {                versions = Integer.parseInt(cmd.substring(versionsArgKey.length()));                continue;            }            final String newNameArgKey = "--new.name=";            if (cmd.startsWith(newNameArgKey)) {                dstTableName = cmd.substring(newNameArgKey.length());                continue;            }            final String peerAdrArgKey = "--peer.adr=";            if (cmd.startsWith(peerAdrArgKey)) {                peerAddress = cmd.substring(peerAdrArgKey.length());                continue;            }            final String familiesArgKey = "--families=";            if (cmd.startsWith(familiesArgKey)) {                families = cmd.substring(familiesArgKey.length());                continue;            }            if (cmd.startsWith("--all.cells")) {                allCells = true;                continue;            }            if (cmd.startsWith("--bulkload")) {                bulkload = true;                continue;            }            if (cmd.startsWith("--shuffle")) {                shuffle = true;                continue;            }            if (cmd.startsWith("--snapshot")) {                readingSnapshot = true;                continue;            }            if (i == args.length - 1) {                if (readingSnapshot) {                    snapshot = cmd;                } else {                    tableName = cmd;                }            } else {                printUsage("Invalid argument '" + cmd + "'");                return false;            }        }        if (dstTableName == null && peerAddress == null) {            printUsage("At least a new table name or a peer address must be specified");            return false;        }        if ((endTime != 0) && (startTime > endTime)) {            printUsage("Invalid time range filter: starttime=" + startTime + " >  endtime=" + endTime);            return false;        }        if (bulkload && peerAddress != null) {            printUsage("Remote bulkload is not supported!");            return false;        }        if (readingSnapshot && peerAddress != null) {            printUsage("Loading data from snapshot to remote peer cluster is not supported.");            return false;        }        if (readingSnapshot && dstTableName == null) {            printUsage("The --new.name=<table> for destination table should be " + "provided when copying data from snapshot .");            return false;        }        if (readingSnapshot && snapshot == null) {            printUsage("Snapshot shouldn't be null when --snapshot is enabled.");            return false;        }                if (dstTableName == null) {            dstTableName = tableName;        }    } catch (Exception e) {                printUsage("Can't start because " + e.getMessage());        return false;    }    return true;}
1
public final void updateHistory(final RecurrenceId recurrenceId, final List<ResourceSkyline> resourceSkylines) throws SkylineStoreException
{    inputValidator.validate(recurrenceId, resourceSkylines);    writeLock.lock();    try {        if (skylineStore.containsKey(recurrenceId)) {                        List<ResourceSkyline> filteredInput = eliminateNull(resourceSkylines);            if (filteredInput.size() > 0) {                skylineStore.put(recurrenceId, filteredInput);                            } else {                StringBuilder errMsg = new StringBuilder();                errMsg.append("Trying to updateHistory " + recurrenceId + " with empty resource skyline");                                throw new EmptyResourceSkylineException(errMsg.toString());            }        } else {            StringBuilder errMsg = new StringBuilder();            errMsg.append("Trying to updateHistory non-existing resource skylines for " + recurrenceId);                        throw new RecurrenceIdNotFoundException(errMsg.toString());        }    } finally {        writeLock.unlock();    }}
1
public void testGetJarFilesByPath()
{    HiveConf conf = new HiveConf(this.getClass());    File tmpDir = Files.createTempDir();    String jarFileName1 = tmpDir.getAbsolutePath() + File.separator + "a.jar";    String jarFileName2 = tmpDir.getAbsolutePath() + File.separator + "b.jar";    File jarFile1 = new File(jarFileName1);    try {        org.apache.commons.io.FileUtils.touch(jarFile1);        Set<String> jars = FileUtils.getJarFilesByPath(tmpDir.getAbsolutePath(), conf);        Assert.assertEquals(Sets.newHashSet("file://" + jarFileName1), jars);        jars = FileUtils.getJarFilesByPath("/folder/not/exist", conf);        Assert.assertTrue(jars.isEmpty());        File jarFile2 = new File(jarFileName2);        org.apache.commons.io.FileUtils.touch(jarFile2);        String newPath = "file://" + jarFileName1 + "," + "file://" + jarFileName2 + ",/file/not/exist";        jars = FileUtils.getJarFilesByPath(newPath, conf);        Assert.assertEquals(Sets.newHashSet("file://" + jarFileName1, "file://" + jarFileName2), jars);    } catch (IOException e) {                Assert.fail(e.getMessage());    } finally {        org.apache.commons.io.FileUtils.deleteQuietly(tmpDir);    }}
1
public void runAndReset()
{    try {        receiver.receiveWork(computation, inputDataWatermark, synchronizedProcessingTime, Windmill.WorkItem.parseFrom(data.newInput()));    } catch (IOException e) {            }    data = ByteString.EMPTY;    bufferedSize = 0;}
1
public void processResponse(ResponseEvent responseReceivedEvent)
{        Response response = responseReceivedEvent.getResponse();    Transaction clientTransactionId = responseReceivedEvent.getClientTransaction();        if (clientTransactionId == null) {        if (LOG.isWarnEnabled()) {                    }        return;    }}
1
public Object destroyLocally(Integer bucketId, EntryEventImpl event, Object expectedOldValue) throws EntryNotFoundException, PrimaryBucketException, ForceReattemptException
{    if (logger.isDebugEnabled()) {            }    Object obj = null;    final BucketRegion bucketRegion = getInitializedBucketForId(event.getKey(), bucketId);    try {        event.setRegion(bucketRegion);                                                                                bucketRegion.basicDestroy(event, true, expectedOldValue);                bucketRegion.checkReadiness();    } catch (EntryNotFoundException enf) {        if (this.partitionedRegion.isDestroyed()) {            checkRegionDestroyedOnBucket(bucketRegion, event.isOriginRemote(), new RegionDestroyedException("Region has been destroyed", this.partitionedRegion.getFullPath()));        }                throw enf;    } catch (RegionDestroyedException rde) {        checkRegionDestroyedOnBucket(bucketRegion, event.isOriginRemote(), rde);    }    return obj;}
1
public void onTaskFailure(final Execution taskExecution, final Throwable cause)
{    if (!executionGraph.getRestartStrategy().canRestart()) {                        failGlobal(cause);        return;    }    if (!isLocalFailoverValid(executionGraph.getGlobalModVersion())) {                return;    }    final ExecutionVertexID vertexID = getExecutionVertexID(taskExecution.getVertex());    final Set<ExecutionVertexID> tasksToRestart = restartPipelinedRegionStrategy.getTasksNeedingRestart(vertexID, cause);    restartTasks(tasksToRestart);}
1
protected int poll() throws Exception
{        shutdownRunningTask = null;    pendingExchanges = 0;    final Exchange dummy = getEndpoint().createExchange();    final String preparedQuery = sqlPrepareStatementStrategy.prepareQuery(resolvedQuery, getEndpoint().isAllowNamedParameters(), dummy);    log.trace("poll: {}", preparedQuery);    final PreparedStatementCallback<Integer> callback = new PreparedStatementCallback<Integer>() {        @Override        public Integer doInPreparedStatement(PreparedStatement ps) throws SQLException, DataAccessException {            Queue<DataHolder> answer = new LinkedList<>();            int expected = parametersCount > 0 ? parametersCount : ps.getParameterMetaData().getParameterCount();                        if (alwaysPopulateStatement || expected > 0) {                Iterator<?> i = sqlPrepareStatementStrategy.createPopulateIterator(resolvedQuery, preparedQuery, expected, dummy, null);                sqlPrepareStatementStrategy.populateStatement(ps, i, expected);            }                        ResultSet rs = ps.executeQuery();            SqlOutputType outputType = getEndpoint().getOutputType();            boolean closeEager = true;            try {                log.trace("Got result list from query: {}, outputType={}", rs, outputType);                if (outputType == SqlOutputType.StreamList) {                    ResultSetIterator data = getEndpoint().queryForStreamList(ps.getConnection(), ps, rs);                                        if (data.hasNext()) {                        addListToQueue(data, answer);                        closeEager = false;                    }                } else if (outputType == SqlOutputType.SelectList) {                    List<?> data = getEndpoint().queryForList(rs, true);                    addListToQueue(data, answer);                } else if (outputType == SqlOutputType.SelectOne) {                    Object data = getEndpoint().queryForObject(rs);                    if (data != null) {                        addListToQueue(data, answer);                    }                } else {                    throw new IllegalArgumentException("Invalid outputType=" + outputType);                }            } finally {                if (closeEager) {                    closeResultSet(rs);                }            }                        try {                if (answer.isEmpty()) {                                        return 0;                } else {                    int rows = processBatch(CastUtils.cast(answer));                    return rows;                }            } catch (Exception e) {                throw RuntimeCamelException.wrapRuntimeCamelException(e);            } finally {                closeResultSet(rs);            }        }    };    Integer messagePolled;    if (namedJdbcTemplate != null) {        messagePolled = namedJdbcTemplate.execute(preparedQuery, parameterSource, callback);    } else {        messagePolled = jdbcTemplate.execute(preparedQuery, callback);    }    return messagePolled;}
1
public void connectionClosed(ConnectionEvent event)
{    if (isActive) {        try {            PooledConnection conn = (PooledConnection) event.getSource();            provider.returnConnection(conn);        } catch (Exception ex) {            String exception = "GemFireConnPooledDataSource::connectionclosed:Exception =" + ex;            if (logger.isDebugEnabled()) {                            }        }    }}
1
public void process(TriggerEvent event, ActionContext actionContext)
{        final CountDownLatch stallLatch = stall;        events.add(event);    getActionStarted().countDown();    try {        if (stallLatch.await(60, TimeUnit.SECONDS)) {                        triggerFired.set(true);        } else {                    }        getActionCompleted().countDown();    } catch (InterruptedException e) {                getActionInterrupted().countDown();    }}
1
public void publish(Object event, Configuration conf)
{    try {        RabbitMQMessage message = new RabbitMQMessage();        message.setBody(getJSONString(event).getBytes());        message.setHeaders(headersStatic);        client.publish(exchange, routingKey, message);    } catch (Exception e) {            }}
1
public void connect(InternalLogWriter logWriter)
{    try {                super.connect(logWriter);                readAlertDefinitionsAsSerializedObjects();        /* Add Cache Listener to listen to Cache & Region create/destroy events */        if (logger.isDebugEnabled()) {                    }        addCacheListener(cacheRegionListener);    } catch (RuntimeException e) {                throw e;    } catch (VirtualMachineError err) {        SystemFailure.initiateFailure(err);                throw err;    } catch (Error e) {                                                SystemFailure.checkFailure();                throw e;    }}
1
 void merge(final Bucket<IN, BucketID> bucket) throws IOException
{    checkNotNull(bucket);    checkState(Objects.equals(bucket.bucketPath, bucketPath));                        checkState(bucket.pendingPartsForCurrentCheckpoint.isEmpty());    checkState(bucket.pendingPartsPerCheckpoint.isEmpty());    CommitRecoverable committable = bucket.closePartFile();    if (committable != null) {        pendingPartsForCurrentCheckpoint.add(committable);    }    if (LOG.isDebugEnabled()) {            }}
1
public T deleteRevision(final RevisionClaim claim, final NiFiUser user, final DeleteRevisionTask<T> task) throws ExpiredRevisionClaimException
{    Objects.requireNonNull(user);        final List<Revision> revisionList = new ArrayList<>(claim.getRevisions());    revisionList.sort(new RevisionComparator());        String failedId = null;    for (final Revision revision : revisionList) {        final Revision curRevision = getRevision(revision.getComponentId());        if (!curRevision.equals(revision)) {            throw new ExpiredRevisionClaimException("Invalid Revision was given for component with ID '" + failedId + "'");        }    }        final T taskResult = task.performTask();    for (final Revision revision : revisionList) {        revisionMap.remove(revision.getComponentId());    }    return taskResult;}
1
private void rollWALAndWait(final HBaseTestingUtility utility, final TableName table, final byte[] row) throws IOException
{    final Admin admin = utility.getAdmin();    final MiniHBaseCluster cluster = utility.getMiniHBaseCluster();        HRegion region = null;    for (HRegion candidate : cluster.getRegions(table)) {        if (HRegion.rowIsInRange(candidate.getRegionInfo(), row)) {            region = candidate;            break;        }    }    assertNotNull("Couldn't find the region for row '" + Arrays.toString(row) + "'", region);    final CountDownLatch latch = new CountDownLatch(1);        final WALActionsListener listener = new WALActionsListener() {        @Override        public void postLogRoll(final Path oldPath, final Path newPath) throws IOException {            latch.countDown();        }    };    region.getWAL().registerWALActionsListener(listener);        admin.rollWALWriter(cluster.getServerHoldingRegion(region.getTableDescriptor().getTableName(), region.getRegionInfo().getRegionName()));        try {        latch.await();    } catch (InterruptedException exception) {                Thread.currentThread().interrupt();    }    region.getWAL().unregisterWALActionsListener(listener);}
1
protected void debugBefore(Exchange exchange, Processor processor, ProcessorDefinition<?> definition, String id, String shortName)
{                    }
1
public void testEditLogFailOverFromMissing() throws IOException
{    File f1 = new File(TEST_DIR + "/failover0");    File f2 = new File(TEST_DIR + "/failover1");    List<URI> editUris = ImmutableList.of(f1.toURI(), f2.toURI());    NNStorage storage = setupEdits(editUris, 3);    final long startErrorTxId = 1 * TXNS_PER_ROLL + 1;    final long endErrorTxId = 2 * TXNS_PER_ROLL;    File[] files = new File(f1, "current").listFiles(new FilenameFilter() {        @Override        public boolean accept(File dir, String name) {            if (name.startsWith(NNStorage.getFinalizedEditsFileName(startErrorTxId, endErrorTxId))) {                return true;            }            return false;        }    });    assertEquals(1, files.length);    assertTrue(files[0].delete());    FSEditLog editlog = getFSEditLog(storage);    editlog.initJournalsForWrite();    long startTxId = 1;    Collection<EditLogInputStream> streams = null;    try {        streams = editlog.selectInputStreams(startTxId, 4 * TXNS_PER_ROLL);        readAllEdits(streams, startTxId);    } catch (IOException e) {                fail("Edit log failover didn't work");    } finally {        IOUtils.cleanup(null, streams.toArray(new EditLogInputStream[0]));    }}
1
public OperationStatus getOperationStatus(OperationHandle opHandle, boolean getProgressUpdate) throws HiveSQLException
{    Operation operation = sessionManager.getOperationManager().getOperation(opHandle);    /**     * If this is a background operation run asynchronously,     * we block for a duration determined by a step function, before we return     * However, if the background operation is complete, we return immediately.     */    HiveConf conf = operation.getParentSession().getHiveConf();    if (operation.shouldRunAsync()) {        long maxTimeout = HiveConf.getTimeVar(conf, HiveConf.ConfVars.HIVE_SERVER2_LONG_POLLING_TIMEOUT, TimeUnit.MILLISECONDS);        final long elapsed = System.currentTimeMillis() - operation.getBeginTime();                        final long timeout = Math.min(maxTimeout, (elapsed / TimeUnit.SECONDS.toMillis(10) + 1) * 500);        try {            operation.getBackgroundHandle().get(timeout, TimeUnit.MILLISECONDS);        } catch (TimeoutException e) {                        LOG.trace(opHandle + ": Long polling timed out");        } catch (CancellationException e) {                        LOG.trace(opHandle + ": The background operation was cancelled", e);        } catch (ExecutionException e) {                                                        } catch (InterruptedException e) {                        }    }    OperationStatus opStatus = operation.getStatus();        long numModifiedRows = operation.getNumModifiedRows();    opStatus.setNumModifiedRows(numModifiedRows);    opStatus.setJobProgressUpdate(progressUpdateLog(getProgressUpdate, operation, conf));    return opStatus;}
1
public static File fetchHadoopTarball(File destinationDir, String version, Configuration conf, Logger log) throws IOException
{        File destinationFile = new File(destinationDir, String.format(HADOOP_TAR_FILENAME_FORMAT, version));    if (destinationFile.exists()) {                return destinationFile;    }    String apacheMirror = conf.get(APACHE_DOWNLOAD_MIRROR_KEY);    if (apacheMirror == null) {        apacheMirror = System.getProperty(APACHE_DOWNLOAD_MIRROR_KEY, APACHE_DOWNLOAD_MIRROR_DEFAULT);    }    if (!destinationDir.exists()) {        if (!destinationDir.mkdirs()) {            throw new IOException("Unable to create local dir: " + destinationDir);        }    }    URL downloadURL = new URL(apacheMirror + String.format(APACHE_DOWNLOAD_MIRROR_SUFFIX_FORMAT, version, version));        FileUtils.copyURLToFile(downloadURL, destinationFile, 10000, 60000);        return destinationFile;}
1
public void testRunThriftServer() throws Exception
{    List<String> args = new ArrayList<>();    if (implType != null) {        String serverTypeOption = implType.toString();        assertTrue(serverTypeOption.startsWith("-"));        args.add(serverTypeOption);    }    port = HBaseTestingUtility.randomFreePort();    args.add("-" + PORT_OPTION);    args.add(String.valueOf(port));    args.add("-" + INFOPORT_OPTION);    int infoPort = HBaseTestingUtility.randomFreePort();    args.add(String.valueOf(infoPort));    if (specifyFramed) {        args.add("-" + FRAMED_OPTION);    }    if (specifyBindIP) {        args.add("-" + BIND_OPTION);        args.add(InetAddress.getLocalHost().getHostName());    }    if (specifyCompact) {        args.add("-" + COMPACT_OPTION);    }    args.add("start");    thriftServer = createThriftServer();    startCmdLineThread(args.toArray(new String[args.size()]));        for (int i = 0; i < 100 && (thriftServer.tserver == null); i++) {        Thread.sleep(100);    }    Class<? extends TServer> expectedClass = implType != null ? implType.serverClass : TBoundedThreadPoolServer.class;    assertEquals(expectedClass, thriftServer.tserver.getClass());    try {        talkToThriftServer();    } catch (Exception ex) {        clientSideException = ex;    } finally {        stopCmdLineThread();    }    if (clientSideException != null) {                throw new Exception(clientSideException);    }}
1
public Object next() throws IOException, InterruptedException
{    long pos = fis.position();    synchronized (HdfsTransactionLog.this) {        if (trace) {            log.trace("Reading log record.  pos=" + pos + " currentSize=" + getLogSize());        }        if (pos >= getLogSize()) {            return null;        }    }        if (pos >= sz) {                fis.close();        initStream(pos);    }    if (pos == 0) {        readHeader(fis);                synchronized (HdfsTransactionLog.this) {            if (fis.position() >= getLogSize()) {                return null;            }            pos = fis.position();        }    }    Object o = codec.readVal(fis);        int size = fis.readInt();    assert size == fis.position() - pos - 4;    return o;}
1
public OMClientResponse validateAndUpdateCache(OzoneManager ozoneManager, long transactionLogIndex, OzoneManagerDoubleBufferHelper ozoneManagerDoubleBufferHelper)
{    OMMetrics omMetrics = ozoneManager.getMetrics();    omMetrics.incNumBucketDeletes();    OMMetadataManager omMetadataManager = ozoneManager.getMetadataManager();    OMRequest omRequest = getOmRequest();    String volumeName = omRequest.getDeleteBucketRequest().getVolumeName();    String bucketName = omRequest.getDeleteBucketRequest().getBucketName();        OMResponse.Builder omResponse = OMResponse.newBuilder().setStatus(OzoneManagerProtocolProtos.Status.OK).setCmdType(omRequest.getCmdType());    AuditLogger auditLogger = ozoneManager.getAuditLogger();    Map<String, String> auditMap = buildVolumeAuditMap(volumeName);    auditMap.put(OzoneConsts.BUCKET, bucketName);    OzoneManagerProtocolProtos.UserInfo userInfo = getOmRequest().getUserInfo();    IOException exception = null;    boolean acquiredLock = false;    OMClientResponse omClientResponse = null;    try {                if (ozoneManager.getAclsEnabled()) {            checkAcls(ozoneManager, OzoneObj.ResourceType.BUCKET, OzoneObj.StoreType.OZONE, IAccessAuthorizer.ACLType.WRITE, volumeName, bucketName, null);        }                acquiredLock = omMetadataManager.getLock().acquireLock(BUCKET_LOCK, volumeName, bucketName);                                String bucketKey = omMetadataManager.getBucketKey(volumeName, bucketName);        OmBucketInfo omBucketInfo = omMetadataManager.getBucketTable().get(bucketKey);        if (omBucketInfo == null) {                        throw new OMException("Bucket doesn't exist", OMException.ResultCodes.BUCKET_NOT_FOUND);        }                if (!omMetadataManager.isBucketEmpty(volumeName, bucketName)) {                        throw new OMException("Bucket is not empty", OMException.ResultCodes.BUCKET_NOT_EMPTY);        }        omMetrics.decNumBuckets();                omMetadataManager.getBucketTable().addCacheEntry(new CacheKey<>(bucketKey), new CacheValue<>(Optional.absent(), transactionLogIndex));        omResponse.setDeleteBucketResponse(DeleteBucketResponse.newBuilder().build());                omClientResponse = new OMBucketDeleteResponse(volumeName, bucketName, omResponse.build());    } catch (IOException ex) {        exception = ex;        omClientResponse = new OMBucketDeleteResponse(volumeName, bucketName, createErrorOMResponse(omResponse, exception));    } finally {        if (omClientResponse != null) {            omClientResponse.setFlushFuture(ozoneManagerDoubleBufferHelper.add(omClientResponse, transactionLogIndex));        }        if (acquiredLock) {            omMetadataManager.getLock().releaseLock(BUCKET_LOCK, volumeName, bucketName);        }    }        auditLog(auditLogger, buildAuditMessage(OMAction.DELETE_BUCKET, auditMap, exception, userInfo));        if (exception == null) {                return omClientResponse;    } else {        omMetrics.incNumBucketDeleteFails();                return omClientResponse;    }}
1
