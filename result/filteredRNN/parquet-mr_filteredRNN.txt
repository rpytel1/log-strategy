public GroupType getList()
{    return list;}
0
public GroupType getRepeated()
{    return repeated;}
0
public Type getElement()
{    return element;}
0
public SchemaMapping fromArrow(Schema arrowSchema)
{    List<Field> fields = arrowSchema.getFields();    List<TypeMapping> parquetFields = fromArrow(fields);    MessageType parquetType = addToBuilder(parquetFields, Types.buildMessage()).named("root");    return new SchemaMapping(arrowSchema, parquetType, parquetFields);}
0
private GroupBuilder<T> addToBuilder(List<TypeMapping> parquetFields, GroupBuilder<T> builder)
{    for (TypeMapping type : parquetFields) {        builder = builder.addField(type.getParquetType());    }    return builder;}
0
private List<TypeMapping> fromArrow(List<Field> fields)
{    List<TypeMapping> result = new ArrayList<>(fields.size());    for (Field field : fields) {        result.add(fromArrow(field));    }    return result;}
0
private TypeMapping fromArrow(final Field field)
{    return fromArrow(field, field.getName());}
0
private TypeMapping fromArrow(final Field field, final String fieldName)
{    final List<Field> children = field.getChildren();    return field.getType().accept(new ArrowTypeVisitor<TypeMapping>() {        @Override        public TypeMapping visit(Null type) {                        return primitive(BINARY);        }        @Override        public TypeMapping visit(Struct type) {            List<TypeMapping> parquetTypes = fromArrow(children);            return new StructTypeMapping(field, addToBuilder(parquetTypes, Types.buildGroup(OPTIONAL)).named(fieldName), parquetTypes);        }        @Override        public TypeMapping visit(org.apache.arrow.vector.types.pojo.ArrowType.List type) {            return createListTypeMapping();        }        @Override        public TypeMapping visit(org.apache.arrow.vector.types.pojo.ArrowType.FixedSizeList type) {            return createListTypeMapping();        }        private ListTypeMapping createListTypeMapping() {            if (children.size() != 1) {                throw new IllegalArgumentException("list fields must have exactly one child: " + field);            }            TypeMapping parquetChild = fromArrow(children.get(0), "element");            GroupType list = Types.optionalList().element(parquetChild.getParquetType()).named(fieldName);            return new ListTypeMapping(field, new List3Levels(list), parquetChild);        }        @Override        public TypeMapping visit(Union type) {                        List<TypeMapping> parquetTypes = fromArrow(children);            return new UnionTypeMapping(field, addToBuilder(parquetTypes, Types.buildGroup(OPTIONAL)).named(fieldName), parquetTypes);        }        @Override        public TypeMapping visit(Int type) {            boolean signed = type.getIsSigned();            switch(type.getBitWidth()) {                case 8:                case 16:                case 32:                    return primitive(INT32, intType(type.getBitWidth(), signed));                case 64:                    return primitive(INT64, intType(64, signed));                default:                    throw new IllegalArgumentException("Illegal int type: " + field);            }        }        @Override        public TypeMapping visit(FloatingPoint type) {            switch(type.getPrecision()) {                case HALF:                                        return primitive(FLOAT);                case SINGLE:                    return primitive(FLOAT);                case DOUBLE:                    return primitive(DOUBLE);                default:                    throw new IllegalArgumentException("Illegal float type: " + field);            }        }        @Override        public TypeMapping visit(Utf8 type) {            return primitive(BINARY, stringType());        }        @Override        public TypeMapping visit(Binary type) {            return primitive(BINARY);        }        @Override        public TypeMapping visit(Bool type) {            return primitive(BOOLEAN);        }        /**         * See https://github.com/apache/parquet-format/blob/master/LogicalTypes.md#decimal         * @param type an arrow decimal type         * @return a mapping from the arrow decimal to the Parquet type         */        @Override        public TypeMapping visit(Decimal type) {            int precision = type.getPrecision();            int scale = type.getScale();            if (1 <= precision && precision <= 9) {                return decimal(INT32, precision, scale);            } else if (1 <= precision && precision <= 18) {                return decimal(INT64, precision, scale);            } else {                                return decimal(BINARY, precision, scale);            }        }        @Override        public TypeMapping visit(Date type) {            return primitive(INT32, dateType());        }        @Override        public TypeMapping visit(Time type) {            int bitWidth = type.getBitWidth();            TimeUnit timeUnit = type.getUnit();            if (bitWidth == 32 && timeUnit == TimeUnit.MILLISECOND) {                return primitive(INT32, timeType(false, MILLIS));            } else if (bitWidth == 64 && timeUnit == TimeUnit.MICROSECOND) {                return primitive(INT64, timeType(false, MICROS));            } else if (bitWidth == 64 && timeUnit == TimeUnit.NANOSECOND) {                return primitive(INT64, timeType(false, NANOS));            }            throw new UnsupportedOperationException("Unsupported type " + type);        }        @Override        public TypeMapping visit(Timestamp type) {            TimeUnit timeUnit = type.getUnit();            if (timeUnit == TimeUnit.MILLISECOND) {                return primitive(INT64, timestampType(isUtcNormalized(type), MILLIS));            } else if (timeUnit == TimeUnit.MICROSECOND) {                return primitive(INT64, timestampType(isUtcNormalized(type), MICROS));            } else if (timeUnit == TimeUnit.NANOSECOND) {                return primitive(INT64, timestampType(isUtcNormalized(type), NANOS));            }            throw new UnsupportedOperationException("Unsupported type " + type);        }        private boolean isUtcNormalized(Timestamp timestamp) {            String timeZone = timestamp.getTimezone();            return timeZone != null && !timeZone.isEmpty();        }        /**         * See https://github.com/apache/parquet-format/blob/master/LogicalTypes.md#interval         */        @Override        public TypeMapping visit(Interval type) {                        return primitiveFLBA(12, LogicalTypeAnnotation.IntervalLogicalTypeAnnotation.getInstance());        }        @Override        public TypeMapping visit(ArrowType.FixedSizeBinary fixedSizeBinary) {            return primitive(BINARY);        }        private TypeMapping mapping(PrimitiveType parquetType) {            return new PrimitiveTypeMapping(field, parquetType);        }        private TypeMapping decimal(PrimitiveTypeName type, int precision, int scale) {            return mapping(Types.optional(type).as(decimalType(scale, precision)).named(fieldName));        }        private TypeMapping primitive(PrimitiveTypeName type) {            return mapping(Types.optional(type).named(fieldName));        }        private TypeMapping primitive(PrimitiveTypeName type, LogicalTypeAnnotation otype) {            return mapping(Types.optional(type).as(otype).named(fieldName));        }        private TypeMapping primitiveFLBA(int length, LogicalTypeAnnotation otype) {            return mapping(Types.optional(FIXED_LEN_BYTE_ARRAY).length(length).as(otype).named(fieldName));        }    });}
0
public TypeMapping visit(Null type)
{        return primitive(BINARY);}
0
public TypeMapping visit(Struct type)
{    List<TypeMapping> parquetTypes = fromArrow(children);    return new StructTypeMapping(field, addToBuilder(parquetTypes, Types.buildGroup(OPTIONAL)).named(fieldName), parquetTypes);}
0
public TypeMapping visit(org.apache.arrow.vector.types.pojo.ArrowType.List type)
{    return createListTypeMapping();}
0
public TypeMapping visit(org.apache.arrow.vector.types.pojo.ArrowType.FixedSizeList type)
{    return createListTypeMapping();}
0
private ListTypeMapping createListTypeMapping()
{    if (children.size() != 1) {        throw new IllegalArgumentException("list fields must have exactly one child: " + field);    }    TypeMapping parquetChild = fromArrow(children.get(0), "element");    GroupType list = Types.optionalList().element(parquetChild.getParquetType()).named(fieldName);    return new ListTypeMapping(field, new List3Levels(list), parquetChild);}
0
public TypeMapping visit(Union type)
{        List<TypeMapping> parquetTypes = fromArrow(children);    return new UnionTypeMapping(field, addToBuilder(parquetTypes, Types.buildGroup(OPTIONAL)).named(fieldName), parquetTypes);}
0
public TypeMapping visit(Int type)
{    boolean signed = type.getIsSigned();    switch(type.getBitWidth()) {        case 8:        case 16:        case 32:            return primitive(INT32, intType(type.getBitWidth(), signed));        case 64:            return primitive(INT64, intType(64, signed));        default:            throw new IllegalArgumentException("Illegal int type: " + field);    }}
0
public TypeMapping visit(FloatingPoint type)
{    switch(type.getPrecision()) {        case HALF:                        return primitive(FLOAT);        case SINGLE:            return primitive(FLOAT);        case DOUBLE:            return primitive(DOUBLE);        default:            throw new IllegalArgumentException("Illegal float type: " + field);    }}
0
public TypeMapping visit(Utf8 type)
{    return primitive(BINARY, stringType());}
0
public TypeMapping visit(Binary type)
{    return primitive(BINARY);}
0
public TypeMapping visit(Bool type)
{    return primitive(BOOLEAN);}
0
public TypeMapping visit(Decimal type)
{    int precision = type.getPrecision();    int scale = type.getScale();    if (1 <= precision && precision <= 9) {        return decimal(INT32, precision, scale);    } else if (1 <= precision && precision <= 18) {        return decimal(INT64, precision, scale);    } else {                return decimal(BINARY, precision, scale);    }}
0
public TypeMapping visit(Date type)
{    return primitive(INT32, dateType());}
0
public TypeMapping visit(Time type)
{    int bitWidth = type.getBitWidth();    TimeUnit timeUnit = type.getUnit();    if (bitWidth == 32 && timeUnit == TimeUnit.MILLISECOND) {        return primitive(INT32, timeType(false, MILLIS));    } else if (bitWidth == 64 && timeUnit == TimeUnit.MICROSECOND) {        return primitive(INT64, timeType(false, MICROS));    } else if (bitWidth == 64 && timeUnit == TimeUnit.NANOSECOND) {        return primitive(INT64, timeType(false, NANOS));    }    throw new UnsupportedOperationException("Unsupported type " + type);}
0
public TypeMapping visit(Timestamp type)
{    TimeUnit timeUnit = type.getUnit();    if (timeUnit == TimeUnit.MILLISECOND) {        return primitive(INT64, timestampType(isUtcNormalized(type), MILLIS));    } else if (timeUnit == TimeUnit.MICROSECOND) {        return primitive(INT64, timestampType(isUtcNormalized(type), MICROS));    } else if (timeUnit == TimeUnit.NANOSECOND) {        return primitive(INT64, timestampType(isUtcNormalized(type), NANOS));    }    throw new UnsupportedOperationException("Unsupported type " + type);}
0
private boolean isUtcNormalized(Timestamp timestamp)
{    String timeZone = timestamp.getTimezone();    return timeZone != null && !timeZone.isEmpty();}
0
public TypeMapping visit(Interval type)
{        return primitiveFLBA(12, LogicalTypeAnnotation.IntervalLogicalTypeAnnotation.getInstance());}
0
public TypeMapping visit(ArrowType.FixedSizeBinary fixedSizeBinary)
{    return primitive(BINARY);}
0
private TypeMapping mapping(PrimitiveType parquetType)
{    return new PrimitiveTypeMapping(field, parquetType);}
0
private TypeMapping decimal(PrimitiveTypeName type, int precision, int scale)
{    return mapping(Types.optional(type).as(decimalType(scale, precision)).named(fieldName));}
0
private TypeMapping primitive(PrimitiveTypeName type)
{    return mapping(Types.optional(type).named(fieldName));}
0
private TypeMapping primitive(PrimitiveTypeName type, LogicalTypeAnnotation otype)
{    return mapping(Types.optional(type).as(otype).named(fieldName));}
0
private TypeMapping primitiveFLBA(int length, LogicalTypeAnnotation otype)
{    return mapping(Types.optional(FIXED_LEN_BYTE_ARRAY).length(length).as(otype).named(fieldName));}
0
public SchemaMapping fromParquet(MessageType parquetSchema)
{    List<Type> fields = parquetSchema.getFields();    List<TypeMapping> mappings = fromParquet(fields);    List<Field> arrowFields = fields(mappings);    return new SchemaMapping(new Schema(arrowFields), parquetSchema, mappings);}
0
private List<Field> fields(List<TypeMapping> mappings)
{    List<Field> result = new ArrayList<>(mappings.size());    for (TypeMapping typeMapping : mappings) {        result.add(typeMapping.getArrowField());    }    return result;}
0
private List<TypeMapping> fromParquet(List<Type> fields)
{    List<TypeMapping> result = new ArrayList<>(fields.size());    for (Type type : fields) {        result.add(fromParquet(type));    }    return result;}
0
private TypeMapping fromParquet(Type type)
{    return fromParquet(type, type.getName(), type.getRepetition());}
0
private TypeMapping fromParquet(Type type, String name, Repetition repetition)
{    if (repetition == REPEATED) {                TypeMapping child = fromParquet(type, null, REQUIRED);        Field arrowField = new Field(name, false, new ArrowType.List(), asList(child.getArrowField()));        return new RepeatedTypeMapping(arrowField, type, child);    }    if (type.isPrimitive()) {        return fromParquetPrimitive(type.asPrimitiveType(), name);    } else {        return fromParquetGroup(type.asGroupType(), name);    }}
0
private TypeMapping fromParquetGroup(GroupType type, String name)
{    LogicalTypeAnnotation logicalType = type.getLogicalTypeAnnotation();    if (logicalType == null) {        List<TypeMapping> typeMappings = fromParquet(type.getFields());        Field arrowField = new Field(name, type.isRepetition(OPTIONAL), new Struct(), fields(typeMappings));        return new StructTypeMapping(arrowField, type, typeMappings);    } else {        return logicalType.accept(new LogicalTypeAnnotation.LogicalTypeAnnotationVisitor<TypeMapping>() {            @Override            public Optional<TypeMapping> visit(LogicalTypeAnnotation.ListLogicalTypeAnnotation listLogicalType) {                List3Levels list3Levels = new List3Levels(type);                TypeMapping child = fromParquet(list3Levels.getElement(), null, list3Levels.getElement().getRepetition());                Field arrowField = new Field(name, type.isRepetition(OPTIONAL), new ArrowType.List(), asList(child.getArrowField()));                return of(new ListTypeMapping(arrowField, list3Levels, child));            }        }).orElseThrow(() -> new UnsupportedOperationException("Unsupported type " + type));    }}
0
public Optional<TypeMapping> visit(LogicalTypeAnnotation.ListLogicalTypeAnnotation listLogicalType)
{    List3Levels list3Levels = new List3Levels(type);    TypeMapping child = fromParquet(list3Levels.getElement(), null, list3Levels.getElement().getRepetition());    Field arrowField = new Field(name, type.isRepetition(OPTIONAL), new ArrowType.List(), asList(child.getArrowField()));    return of(new ListTypeMapping(arrowField, list3Levels, child));}
0
private TypeMapping fromParquetPrimitive(final PrimitiveType type, final String name)
{    return type.getPrimitiveTypeName().convert(new PrimitiveType.PrimitiveTypeNameConverter<TypeMapping, RuntimeException>() {        private TypeMapping field(ArrowType arrowType) {            Field field = new Field(name, type.isRepetition(OPTIONAL), arrowType, null);            return new PrimitiveTypeMapping(field, type);        }        @Override        public TypeMapping convertFLOAT(PrimitiveTypeName primitiveTypeName) throws RuntimeException {            return field(new ArrowType.FloatingPoint(FloatingPointPrecision.SINGLE));        }        @Override        public TypeMapping convertDOUBLE(PrimitiveTypeName primitiveTypeName) throws RuntimeException {            return field(new ArrowType.FloatingPoint(FloatingPointPrecision.DOUBLE));        }        @Override        public TypeMapping convertINT32(PrimitiveTypeName primitiveTypeName) throws RuntimeException {            LogicalTypeAnnotation logicalTypeAnnotation = type.getLogicalTypeAnnotation();            if (logicalTypeAnnotation == null) {                return integer(32, true);            }            return logicalTypeAnnotation.accept(new LogicalTypeAnnotation.LogicalTypeAnnotationVisitor<TypeMapping>() {                @Override                public Optional<TypeMapping> visit(LogicalTypeAnnotation.DecimalLogicalTypeAnnotation decimalLogicalType) {                    return of(decimal(decimalLogicalType.getPrecision(), decimalLogicalType.getScale()));                }                @Override                public Optional<TypeMapping> visit(LogicalTypeAnnotation.DateLogicalTypeAnnotation dateLogicalType) {                    return of(field(new ArrowType.Date(DateUnit.DAY)));                }                @Override                public Optional<TypeMapping> visit(LogicalTypeAnnotation.TimeLogicalTypeAnnotation timeLogicalType) {                    return timeLogicalType.getUnit() == MILLIS ? of(field(new ArrowType.Time(TimeUnit.MILLISECOND, 32))) : empty();                }                @Override                public Optional<TypeMapping> visit(LogicalTypeAnnotation.IntLogicalTypeAnnotation intLogicalType) {                    if (intLogicalType.getBitWidth() == 64) {                        return empty();                    }                    return of(integer(intLogicalType.getBitWidth(), intLogicalType.isSigned()));                }            }).orElseThrow(() -> new IllegalArgumentException("illegal type " + type));        }        @Override        public TypeMapping convertINT64(PrimitiveTypeName primitiveTypeName) throws RuntimeException {            LogicalTypeAnnotation logicalTypeAnnotation = type.getLogicalTypeAnnotation();            if (logicalTypeAnnotation == null) {                return integer(64, true);            }            return logicalTypeAnnotation.accept(new LogicalTypeAnnotation.LogicalTypeAnnotationVisitor<TypeMapping>() {                @Override                public Optional<TypeMapping> visit(LogicalTypeAnnotation.DateLogicalTypeAnnotation dateLogicalType) {                    return of(field(new ArrowType.Date(DateUnit.DAY)));                }                @Override                public Optional<TypeMapping> visit(LogicalTypeAnnotation.DecimalLogicalTypeAnnotation decimalLogicalType) {                    return of(decimal(decimalLogicalType.getPrecision(), decimalLogicalType.getScale()));                }                @Override                public Optional<TypeMapping> visit(LogicalTypeAnnotation.IntLogicalTypeAnnotation intLogicalType) {                    return of(integer(intLogicalType.getBitWidth(), intLogicalType.isSigned()));                }                @Override                public Optional<TypeMapping> visit(LogicalTypeAnnotation.TimeLogicalTypeAnnotation timeLogicalType) {                    if (timeLogicalType.getUnit() == MICROS) {                        return of(field(new ArrowType.Time(TimeUnit.MICROSECOND, 64)));                    } else if (timeLogicalType.getUnit() == NANOS) {                        return of(field(new ArrowType.Time(TimeUnit.NANOSECOND, 64)));                    }                    return empty();                }                @Override                public Optional<TypeMapping> visit(LogicalTypeAnnotation.TimestampLogicalTypeAnnotation timestampLogicalType) {                    switch(timestampLogicalType.getUnit()) {                        case MICROS:                            return of(field(new ArrowType.Timestamp(TimeUnit.MICROSECOND, getTimeZone(timestampLogicalType))));                        case MILLIS:                            return of(field(new ArrowType.Timestamp(TimeUnit.MILLISECOND, getTimeZone(timestampLogicalType))));                        case NANOS:                            return of(field(new ArrowType.Timestamp(TimeUnit.NANOSECOND, getTimeZone(timestampLogicalType))));                    }                    return empty();                }                private String getTimeZone(LogicalTypeAnnotation.TimestampLogicalTypeAnnotation timestampLogicalType) {                    return timestampLogicalType.isAdjustedToUTC() ? "UTC" : null;                }            }).orElseThrow(() -> new IllegalArgumentException("illegal type " + type));        }        @Override        public TypeMapping convertINT96(PrimitiveTypeName primitiveTypeName) throws RuntimeException {            if (convertInt96ToArrowTimestamp) {                return field(new ArrowType.Timestamp(TimeUnit.NANOSECOND, null));            } else {                return field(new ArrowType.Binary());            }        }        @Override        public TypeMapping convertFIXED_LEN_BYTE_ARRAY(PrimitiveTypeName primitiveTypeName) throws RuntimeException {            LogicalTypeAnnotation logicalTypeAnnotation = type.getLogicalTypeAnnotation();            if (logicalTypeAnnotation == null) {                return field(new ArrowType.Binary());            }            return logicalTypeAnnotation.accept(new LogicalTypeAnnotation.LogicalTypeAnnotationVisitor<TypeMapping>() {                @Override                public Optional<TypeMapping> visit(LogicalTypeAnnotation.DecimalLogicalTypeAnnotation decimalLogicalType) {                    return of(decimal(decimalLogicalType.getPrecision(), decimalLogicalType.getScale()));                }            }).orElseThrow(() -> new IllegalArgumentException("illegal type " + type));        }        @Override        public TypeMapping convertBOOLEAN(PrimitiveTypeName primitiveTypeName) throws RuntimeException {            return field(new ArrowType.Bool());        }        @Override        public TypeMapping convertBINARY(PrimitiveTypeName primitiveTypeName) throws RuntimeException {            LogicalTypeAnnotation logicalTypeAnnotation = type.getLogicalTypeAnnotation();            if (logicalTypeAnnotation == null) {                return field(new ArrowType.Binary());            }            return logicalTypeAnnotation.accept(new LogicalTypeAnnotation.LogicalTypeAnnotationVisitor<TypeMapping>() {                @Override                public Optional<TypeMapping> visit(LogicalTypeAnnotation.StringLogicalTypeAnnotation stringLogicalType) {                    return of(field(new ArrowType.Utf8()));                }                @Override                public Optional<TypeMapping> visit(LogicalTypeAnnotation.DecimalLogicalTypeAnnotation decimalLogicalType) {                    return of(decimal(decimalLogicalType.getPrecision(), decimalLogicalType.getScale()));                }            }).orElseThrow(() -> new IllegalArgumentException("illegal type " + type));        }        private TypeMapping decimal(int precision, int scale) {            return field(new ArrowType.Decimal(precision, scale));        }        private TypeMapping integer(int width, boolean signed) {            return field(new ArrowType.Int(width, signed));        }    });}
0
private TypeMapping field(ArrowType arrowType)
{    Field field = new Field(name, type.isRepetition(OPTIONAL), arrowType, null);    return new PrimitiveTypeMapping(field, type);}
0
public TypeMapping convertFLOAT(PrimitiveTypeName primitiveTypeName) throws RuntimeException
{    return field(new ArrowType.FloatingPoint(FloatingPointPrecision.SINGLE));}
0
public TypeMapping convertDOUBLE(PrimitiveTypeName primitiveTypeName) throws RuntimeException
{    return field(new ArrowType.FloatingPoint(FloatingPointPrecision.DOUBLE));}
0
public TypeMapping convertINT32(PrimitiveTypeName primitiveTypeName) throws RuntimeException
{    LogicalTypeAnnotation logicalTypeAnnotation = type.getLogicalTypeAnnotation();    if (logicalTypeAnnotation == null) {        return integer(32, true);    }    return logicalTypeAnnotation.accept(new LogicalTypeAnnotation.LogicalTypeAnnotationVisitor<TypeMapping>() {        @Override        public Optional<TypeMapping> visit(LogicalTypeAnnotation.DecimalLogicalTypeAnnotation decimalLogicalType) {            return of(decimal(decimalLogicalType.getPrecision(), decimalLogicalType.getScale()));        }        @Override        public Optional<TypeMapping> visit(LogicalTypeAnnotation.DateLogicalTypeAnnotation dateLogicalType) {            return of(field(new ArrowType.Date(DateUnit.DAY)));        }        @Override        public Optional<TypeMapping> visit(LogicalTypeAnnotation.TimeLogicalTypeAnnotation timeLogicalType) {            return timeLogicalType.getUnit() == MILLIS ? of(field(new ArrowType.Time(TimeUnit.MILLISECOND, 32))) : empty();        }        @Override        public Optional<TypeMapping> visit(LogicalTypeAnnotation.IntLogicalTypeAnnotation intLogicalType) {            if (intLogicalType.getBitWidth() == 64) {                return empty();            }            return of(integer(intLogicalType.getBitWidth(), intLogicalType.isSigned()));        }    }).orElseThrow(() -> new IllegalArgumentException("illegal type " + type));}
0
public Optional<TypeMapping> visit(LogicalTypeAnnotation.DecimalLogicalTypeAnnotation decimalLogicalType)
{    return of(decimal(decimalLogicalType.getPrecision(), decimalLogicalType.getScale()));}
0
public Optional<TypeMapping> visit(LogicalTypeAnnotation.DateLogicalTypeAnnotation dateLogicalType)
{    return of(field(new ArrowType.Date(DateUnit.DAY)));}
0
public Optional<TypeMapping> visit(LogicalTypeAnnotation.TimeLogicalTypeAnnotation timeLogicalType)
{    return timeLogicalType.getUnit() == MILLIS ? of(field(new ArrowType.Time(TimeUnit.MILLISECOND, 32))) : empty();}
0
public Optional<TypeMapping> visit(LogicalTypeAnnotation.IntLogicalTypeAnnotation intLogicalType)
{    if (intLogicalType.getBitWidth() == 64) {        return empty();    }    return of(integer(intLogicalType.getBitWidth(), intLogicalType.isSigned()));}
0
public TypeMapping convertINT64(PrimitiveTypeName primitiveTypeName) throws RuntimeException
{    LogicalTypeAnnotation logicalTypeAnnotation = type.getLogicalTypeAnnotation();    if (logicalTypeAnnotation == null) {        return integer(64, true);    }    return logicalTypeAnnotation.accept(new LogicalTypeAnnotation.LogicalTypeAnnotationVisitor<TypeMapping>() {        @Override        public Optional<TypeMapping> visit(LogicalTypeAnnotation.DateLogicalTypeAnnotation dateLogicalType) {            return of(field(new ArrowType.Date(DateUnit.DAY)));        }        @Override        public Optional<TypeMapping> visit(LogicalTypeAnnotation.DecimalLogicalTypeAnnotation decimalLogicalType) {            return of(decimal(decimalLogicalType.getPrecision(), decimalLogicalType.getScale()));        }        @Override        public Optional<TypeMapping> visit(LogicalTypeAnnotation.IntLogicalTypeAnnotation intLogicalType) {            return of(integer(intLogicalType.getBitWidth(), intLogicalType.isSigned()));        }        @Override        public Optional<TypeMapping> visit(LogicalTypeAnnotation.TimeLogicalTypeAnnotation timeLogicalType) {            if (timeLogicalType.getUnit() == MICROS) {                return of(field(new ArrowType.Time(TimeUnit.MICROSECOND, 64)));            } else if (timeLogicalType.getUnit() == NANOS) {                return of(field(new ArrowType.Time(TimeUnit.NANOSECOND, 64)));            }            return empty();        }        @Override        public Optional<TypeMapping> visit(LogicalTypeAnnotation.TimestampLogicalTypeAnnotation timestampLogicalType) {            switch(timestampLogicalType.getUnit()) {                case MICROS:                    return of(field(new ArrowType.Timestamp(TimeUnit.MICROSECOND, getTimeZone(timestampLogicalType))));                case MILLIS:                    return of(field(new ArrowType.Timestamp(TimeUnit.MILLISECOND, getTimeZone(timestampLogicalType))));                case NANOS:                    return of(field(new ArrowType.Timestamp(TimeUnit.NANOSECOND, getTimeZone(timestampLogicalType))));            }            return empty();        }        private String getTimeZone(LogicalTypeAnnotation.TimestampLogicalTypeAnnotation timestampLogicalType) {            return timestampLogicalType.isAdjustedToUTC() ? "UTC" : null;        }    }).orElseThrow(() -> new IllegalArgumentException("illegal type " + type));}
0
public Optional<TypeMapping> visit(LogicalTypeAnnotation.DateLogicalTypeAnnotation dateLogicalType)
{    return of(field(new ArrowType.Date(DateUnit.DAY)));}
0
public Optional<TypeMapping> visit(LogicalTypeAnnotation.DecimalLogicalTypeAnnotation decimalLogicalType)
{    return of(decimal(decimalLogicalType.getPrecision(), decimalLogicalType.getScale()));}
0
public Optional<TypeMapping> visit(LogicalTypeAnnotation.IntLogicalTypeAnnotation intLogicalType)
{    return of(integer(intLogicalType.getBitWidth(), intLogicalType.isSigned()));}
0
public Optional<TypeMapping> visit(LogicalTypeAnnotation.TimeLogicalTypeAnnotation timeLogicalType)
{    if (timeLogicalType.getUnit() == MICROS) {        return of(field(new ArrowType.Time(TimeUnit.MICROSECOND, 64)));    } else if (timeLogicalType.getUnit() == NANOS) {        return of(field(new ArrowType.Time(TimeUnit.NANOSECOND, 64)));    }    return empty();}
0
public Optional<TypeMapping> visit(LogicalTypeAnnotation.TimestampLogicalTypeAnnotation timestampLogicalType)
{    switch(timestampLogicalType.getUnit()) {        case MICROS:            return of(field(new ArrowType.Timestamp(TimeUnit.MICROSECOND, getTimeZone(timestampLogicalType))));        case MILLIS:            return of(field(new ArrowType.Timestamp(TimeUnit.MILLISECOND, getTimeZone(timestampLogicalType))));        case NANOS:            return of(field(new ArrowType.Timestamp(TimeUnit.NANOSECOND, getTimeZone(timestampLogicalType))));    }    return empty();}
0
private String getTimeZone(LogicalTypeAnnotation.TimestampLogicalTypeAnnotation timestampLogicalType)
{    return timestampLogicalType.isAdjustedToUTC() ? "UTC" : null;}
0
public TypeMapping convertINT96(PrimitiveTypeName primitiveTypeName) throws RuntimeException
{    if (convertInt96ToArrowTimestamp) {        return field(new ArrowType.Timestamp(TimeUnit.NANOSECOND, null));    } else {        return field(new ArrowType.Binary());    }}
0
public TypeMapping convertFIXED_LEN_BYTE_ARRAY(PrimitiveTypeName primitiveTypeName) throws RuntimeException
{    LogicalTypeAnnotation logicalTypeAnnotation = type.getLogicalTypeAnnotation();    if (logicalTypeAnnotation == null) {        return field(new ArrowType.Binary());    }    return logicalTypeAnnotation.accept(new LogicalTypeAnnotation.LogicalTypeAnnotationVisitor<TypeMapping>() {        @Override        public Optional<TypeMapping> visit(LogicalTypeAnnotation.DecimalLogicalTypeAnnotation decimalLogicalType) {            return of(decimal(decimalLogicalType.getPrecision(), decimalLogicalType.getScale()));        }    }).orElseThrow(() -> new IllegalArgumentException("illegal type " + type));}
0
public Optional<TypeMapping> visit(LogicalTypeAnnotation.DecimalLogicalTypeAnnotation decimalLogicalType)
{    return of(decimal(decimalLogicalType.getPrecision(), decimalLogicalType.getScale()));}
0
public TypeMapping convertBOOLEAN(PrimitiveTypeName primitiveTypeName) throws RuntimeException
{    return field(new ArrowType.Bool());}
0
public TypeMapping convertBINARY(PrimitiveTypeName primitiveTypeName) throws RuntimeException
{    LogicalTypeAnnotation logicalTypeAnnotation = type.getLogicalTypeAnnotation();    if (logicalTypeAnnotation == null) {        return field(new ArrowType.Binary());    }    return logicalTypeAnnotation.accept(new LogicalTypeAnnotation.LogicalTypeAnnotationVisitor<TypeMapping>() {        @Override        public Optional<TypeMapping> visit(LogicalTypeAnnotation.StringLogicalTypeAnnotation stringLogicalType) {            return of(field(new ArrowType.Utf8()));        }        @Override        public Optional<TypeMapping> visit(LogicalTypeAnnotation.DecimalLogicalTypeAnnotation decimalLogicalType) {            return of(decimal(decimalLogicalType.getPrecision(), decimalLogicalType.getScale()));        }    }).orElseThrow(() -> new IllegalArgumentException("illegal type " + type));}
0
public Optional<TypeMapping> visit(LogicalTypeAnnotation.StringLogicalTypeAnnotation stringLogicalType)
{    return of(field(new ArrowType.Utf8()));}
0
public Optional<TypeMapping> visit(LogicalTypeAnnotation.DecimalLogicalTypeAnnotation decimalLogicalType)
{    return of(decimal(decimalLogicalType.getPrecision(), decimalLogicalType.getScale()));}
0
private TypeMapping decimal(int precision, int scale)
{    return field(new ArrowType.Decimal(precision, scale));}
0
private TypeMapping integer(int width, boolean signed)
{    return field(new ArrowType.Int(width, signed));}
0
public SchemaMapping map(Schema arrowSchema, MessageType parquetSchema)
{    List<TypeMapping> children = map(arrowSchema.getFields(), parquetSchema.getFields());    return new SchemaMapping(arrowSchema, parquetSchema, children);}
0
private List<TypeMapping> map(List<Field> arrowFields, List<Type> parquetFields)
{    if (arrowFields.size() != parquetFields.size()) {        throw new IllegalArgumentException("Can not map schemas as sizes differ: " + arrowFields + " != " + parquetFields);    }    List<TypeMapping> result = new ArrayList<>(arrowFields.size());    for (int i = 0; i < arrowFields.size(); i++) {        Field arrowField = arrowFields.get(i);        Type parquetField = parquetFields.get(i);        result.add(map(arrowField, parquetField));    }    return result;}
0
private TypeMapping map(final Field arrowField, final Type parquetField)
{    return arrowField.getType().accept(new ArrowTypeVisitor<TypeMapping>() {        @Override        public TypeMapping visit(Null type) {            if (!parquetField.isRepetition(OPTIONAL)) {                throw new IllegalArgumentException("Parquet type can't be null: " + parquetField);            }            return primitive();        }        @Override        public TypeMapping visit(Struct type) {            if (parquetField.isPrimitive()) {                throw new IllegalArgumentException("Parquet type not a group: " + parquetField);            }            GroupType groupType = parquetField.asGroupType();            return new StructTypeMapping(arrowField, groupType, map(arrowField.getChildren(), groupType.getFields()));        }        @Override        public TypeMapping visit(org.apache.arrow.vector.types.pojo.ArrowType.List type) {            return createListTypeMapping(type);        }        @Override        public TypeMapping visit(org.apache.arrow.vector.types.pojo.ArrowType.FixedSizeList type) {            return createListTypeMapping(type);        }        private TypeMapping createListTypeMapping(ArrowType.ComplexType type) {            if (arrowField.getChildren().size() != 1) {                throw new IllegalArgumentException("Invalid list type: " + type);            }            Field arrowChild = arrowField.getChildren().get(0);            if (parquetField.isRepetition(REPEATED)) {                return new RepeatedTypeMapping(arrowField, parquetField, map(arrowChild, parquetField));            }            if (parquetField.isPrimitive()) {                throw new IllegalArgumentException("Parquet type not a group: " + parquetField);            }            List3Levels list3Levels = new List3Levels(parquetField.asGroupType());            if (arrowField.getChildren().size() != 1) {                throw new IllegalArgumentException("invalid arrow list: " + arrowField);            }            return new ListTypeMapping(arrowField, list3Levels, map(arrowChild, list3Levels.getElement()));        }        @Override        public TypeMapping visit(Union type) {            if (parquetField.isPrimitive()) {                throw new IllegalArgumentException("Parquet type not a group: " + parquetField);            }            GroupType groupType = parquetField.asGroupType();            return new UnionTypeMapping(arrowField, groupType, map(arrowField.getChildren(), groupType.getFields()));        }        @Override        public TypeMapping visit(Int type) {            return primitive();        }        @Override        public TypeMapping visit(FloatingPoint type) {            return primitive();        }        @Override        public TypeMapping visit(Utf8 type) {            return primitive();        }        @Override        public TypeMapping visit(Binary type) {            return primitive();        }        @Override        public TypeMapping visit(Bool type) {            return primitive();        }        @Override        public TypeMapping visit(Decimal type) {            return primitive();        }        @Override        public TypeMapping visit(Date type) {            return primitive();        }        @Override        public TypeMapping visit(Time type) {            return primitive();        }        @Override        public TypeMapping visit(Timestamp type) {            return primitive();        }        @Override        public TypeMapping visit(Interval type) {            return primitive();        }        @Override        public TypeMapping visit(ArrowType.FixedSizeBinary fixedSizeBinary) {            return primitive();        }        private TypeMapping primitive() {            if (!parquetField.isPrimitive()) {                throw new IllegalArgumentException("Can not map schemas as one is primitive and the other is not: " + arrowField + " != " + parquetField);            }            return new PrimitiveTypeMapping(arrowField, parquetField.asPrimitiveType());        }    });}
0
public TypeMapping visit(Null type)
{    if (!parquetField.isRepetition(OPTIONAL)) {        throw new IllegalArgumentException("Parquet type can't be null: " + parquetField);    }    return primitive();}
0
public TypeMapping visit(Struct type)
{    if (parquetField.isPrimitive()) {        throw new IllegalArgumentException("Parquet type not a group: " + parquetField);    }    GroupType groupType = parquetField.asGroupType();    return new StructTypeMapping(arrowField, groupType, map(arrowField.getChildren(), groupType.getFields()));}
0
public TypeMapping visit(org.apache.arrow.vector.types.pojo.ArrowType.List type)
{    return createListTypeMapping(type);}
0
public TypeMapping visit(org.apache.arrow.vector.types.pojo.ArrowType.FixedSizeList type)
{    return createListTypeMapping(type);}
0
private TypeMapping createListTypeMapping(ArrowType.ComplexType type)
{    if (arrowField.getChildren().size() != 1) {        throw new IllegalArgumentException("Invalid list type: " + type);    }    Field arrowChild = arrowField.getChildren().get(0);    if (parquetField.isRepetition(REPEATED)) {        return new RepeatedTypeMapping(arrowField, parquetField, map(arrowChild, parquetField));    }    if (parquetField.isPrimitive()) {        throw new IllegalArgumentException("Parquet type not a group: " + parquetField);    }    List3Levels list3Levels = new List3Levels(parquetField.asGroupType());    if (arrowField.getChildren().size() != 1) {        throw new IllegalArgumentException("invalid arrow list: " + arrowField);    }    return new ListTypeMapping(arrowField, list3Levels, map(arrowChild, list3Levels.getElement()));}
0
public TypeMapping visit(Union type)
{    if (parquetField.isPrimitive()) {        throw new IllegalArgumentException("Parquet type not a group: " + parquetField);    }    GroupType groupType = parquetField.asGroupType();    return new UnionTypeMapping(arrowField, groupType, map(arrowField.getChildren(), groupType.getFields()));}
0
public TypeMapping visit(Int type)
{    return primitive();}
0
public TypeMapping visit(FloatingPoint type)
{    return primitive();}
0
public TypeMapping visit(Utf8 type)
{    return primitive();}
0
public TypeMapping visit(Binary type)
{    return primitive();}
0
public TypeMapping visit(Bool type)
{    return primitive();}
0
public TypeMapping visit(Decimal type)
{    return primitive();}
0
public TypeMapping visit(Date type)
{    return primitive();}
0
public TypeMapping visit(Time type)
{    return primitive();}
0
public TypeMapping visit(Timestamp type)
{    return primitive();}
0
public TypeMapping visit(Interval type)
{    return primitive();}
0
public TypeMapping visit(ArrowType.FixedSizeBinary fixedSizeBinary)
{    return primitive();}
0
private TypeMapping primitive()
{    if (!parquetField.isPrimitive()) {        throw new IllegalArgumentException("Can not map schemas as one is primitive and the other is not: " + arrowField + " != " + parquetField);    }    return new PrimitiveTypeMapping(arrowField, parquetField.asPrimitiveType());}
0
public Schema getArrowSchema()
{    return arrowSchema;}
0
public MessageType getParquetSchema()
{    return parquetSchema;}
0
public List<TypeMapping> getChildren()
{    return children;}
0
public Field getArrowField()
{    return arrowField;}
0
public Type getParquetType()
{    return parquetType;}
0
public List<TypeMapping> getChildren()
{    return children;}
0
public T accept(TypeMappingVisitor<T> visitor)
{    return visitor.visit(this);}
0
public T accept(TypeMappingVisitor<T> visitor)
{    return visitor.visit(this);}
0
public T accept(TypeMappingVisitor<T> visitor)
{    return visitor.visit(this);}
0
public List3Levels getList3Levels()
{    return list3Levels;}
0
public TypeMapping getChild()
{    return child;}
0
public T accept(TypeMappingVisitor<T> visitor)
{    return visitor.visit(this);}
0
public TypeMapping getChild()
{    return child;}
0
public T accept(TypeMappingVisitor<T> visitor)
{    return visitor.visit(this);}
0
private static Field field(String name, boolean nullable, ArrowType type, Field... children)
{    return new Field(name, nullable, type, asList(children));}
0
private static Field field(String name, ArrowType type, Field... children)
{    return field(name, true, type, children);}
0
public void testComplexArrowToParquet() throws IOException
{    MessageType parquet = converter.fromArrow(complexArrowSchema).getParquetSchema();        Assert.assertEquals(complexParquetSchema.toString(), parquet.toString());    Assert.assertEquals(complexParquetSchema, parquet);}
0
public void testAllArrowToParquet() throws IOException
{    MessageType parquet = converter.fromArrow(allTypesArrowSchema).getParquetSchema();        Assert.assertEquals(allTypesParquetSchema.toString(), parquet.toString());    Assert.assertEquals(allTypesParquetSchema, parquet);}
0
public void testSupportedParquetToArrow() throws IOException
{    Schema arrow = converter.fromParquet(supportedTypesParquetSchema).getArrowSchema();    assertEquals(supportedTypesArrowSchema, arrow);}
0
public void testRepeatedParquetToArrow() throws IOException
{    Schema arrow = converter.fromParquet(Paper.schema).getArrowSchema();    assertEquals(paperArrowSchema, arrow);}
0
public void assertEquals(Schema left, Schema right)
{    compareFields(left.getFields(), right.getFields());    Assert.assertEquals(left, right);}
0
private void compareFields(List<Field> left, List<Field> right)
{    Assert.assertEquals(left + "\n" + right, left.size(), right.size());    int size = left.size();    for (int i = 0; i < size; i++) {        Field expectedField = left.get(i);        Field field = right.get(i);        compareFields(expectedField.getChildren(), field.getChildren());        Assert.assertEquals(expectedField, field);    }}
0
public void testAllMap() throws IOException
{    SchemaMapping map = converter.map(allTypesArrowSchema, allTypesParquetSchema);    Assert.assertEquals("p, s<p>, l<p>, l<p>, u<p>, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p", toSummaryString(map));}
0
private String toSummaryString(SchemaMapping map)
{    List<TypeMapping> fields = map.getChildren();    return toSummaryString(fields);}
0
private String toSummaryString(List<TypeMapping> fields)
{    final StringBuilder sb = new StringBuilder();    for (TypeMapping typeMapping : fields) {        if (sb.length() != 0) {            sb.append(", ");        }        sb.append(typeMapping.accept(new TypeMappingVisitor<String>() {            @Override            public String visit(PrimitiveTypeMapping primitiveTypeMapping) {                return "p";            }            @Override            public String visit(StructTypeMapping structTypeMapping) {                return "s";            }            @Override            public String visit(UnionTypeMapping unionTypeMapping) {                return "u";            }            @Override            public String visit(ListTypeMapping listTypeMapping) {                return "l";            }            @Override            public String visit(RepeatedTypeMapping repeatedTypeMapping) {                return "r";            }        }));        if (typeMapping.getChildren() != null && !typeMapping.getChildren().isEmpty()) {            sb.append("<").append(toSummaryString(typeMapping.getChildren())).append(">");        }    }    return sb.toString();}
0
public String visit(PrimitiveTypeMapping primitiveTypeMapping)
{    return "p";}
0
public String visit(StructTypeMapping structTypeMapping)
{    return "s";}
0
public String visit(UnionTypeMapping unionTypeMapping)
{    return "u";}
0
public String visit(ListTypeMapping listTypeMapping)
{    return "l";}
0
public String visit(RepeatedTypeMapping repeatedTypeMapping)
{    return "r";}
0
public void testRepeatedMap() throws IOException
{    SchemaMapping map = converter.map(paperArrowSchema, Paper.schema);    Assert.assertEquals("p, s<r<p>, r<p>>, r<s<r<s<p, p>>, p>>", toSummaryString(map));}
0
public void testArrowTimeSecondToParquet()
{    converter.fromArrow(new Schema(asList(field("a", new ArrowType.Time(TimeUnit.SECOND, 32))))).getParquetSchema();}
0
public void testArrowTimeMillisecondToParquet()
{    MessageType expected = converter.fromArrow(new Schema(asList(field("a", new ArrowType.Time(TimeUnit.MILLISECOND, 32))))).getParquetSchema();    Assert.assertEquals(expected, Types.buildMessage().addField(Types.optional(INT32).as(timeType(false, MILLIS)).named("a")).named("root"));}
0
public void testArrowTimeMicrosecondToParquet()
{    MessageType expected = converter.fromArrow(new Schema(asList(field("a", new ArrowType.Time(TimeUnit.MICROSECOND, 64))))).getParquetSchema();    Assert.assertEquals(expected, Types.buildMessage().addField(Types.optional(INT64).as(timeType(false, MICROS)).named("a")).named("root"));}
0
public void testParquetInt32TimeMillisToArrow()
{    MessageType parquet = Types.buildMessage().addField(Types.optional(INT32).as(TIME_MILLIS).named("a")).named("root");    Schema expected = new Schema(asList(field("a", new ArrowType.Time(TimeUnit.MILLISECOND, 32))));    Assert.assertEquals(expected, converter.fromParquet(parquet).getArrowSchema());}
0
public void testParquetInt64TimeMicrosToArrow()
{    MessageType parquet = Types.buildMessage().addField(Types.optional(INT64).as(TIME_MICROS).named("a")).named("root");    Schema expected = new Schema(asList(field("a", new ArrowType.Time(TimeUnit.MICROSECOND, 64))));    Assert.assertEquals(expected, converter.fromParquet(parquet).getArrowSchema());}
0
public void testParquetFixedBinaryToArrow()
{    MessageType parquet = Types.buildMessage().addField(Types.optional(FIXED_LEN_BYTE_ARRAY).length(12).named("a")).named("root");    Schema expected = new Schema(asList(field("a", new ArrowType.Binary())));    Assert.assertEquals(expected, converter.fromParquet(parquet).getArrowSchema());}
0
public void testParquetFixedBinaryToArrowDecimal()
{    MessageType parquet = Types.buildMessage().addField(Types.optional(FIXED_LEN_BYTE_ARRAY).length(5).as(DECIMAL).precision(8).scale(2).named("a")).named("root");    Schema expected = new Schema(asList(field("a", new ArrowType.Decimal(8, 2))));    Assert.assertEquals(expected, converter.fromParquet(parquet).getArrowSchema());}
0
public void testParquetInt96ToArrowBinary()
{    MessageType parquet = Types.buildMessage().addField(Types.optional(INT96).named("a")).named("root");    Schema expected = new Schema(asList(field("a", new ArrowType.Binary())));    Assert.assertEquals(expected, converter.fromParquet(parquet).getArrowSchema());}
0
public void testParquetInt96ToArrowTimestamp()
{    final SchemaConverter converterInt96ToTimestamp = new SchemaConverter(true);    MessageType parquet = Types.buildMessage().addField(Types.optional(INT96).named("a")).named("root");    Schema expected = new Schema(asList(field("a", new ArrowType.Timestamp(TimeUnit.NANOSECOND, null))));    Assert.assertEquals(expected, converterInt96ToTimestamp.fromParquet(parquet).getArrowSchema());}
0
public void testParquetInt64TimeMillisToArrow()
{    converter.fromParquet(Types.buildMessage().addField(Types.optional(INT64).as(TIME_MILLIS).named("a")).named("root"));}
0
public void testParquetInt32TimeMicrosToArrow()
{    converter.fromParquet(Types.buildMessage().addField(Types.optional(INT32).as(TIME_MICROS).named("a")).named("root"));}
0
public void testArrowTimestampSecondToParquet()
{    converter.fromArrow(new Schema(asList(field("a", new ArrowType.Timestamp(TimeUnit.SECOND, "UTC"))))).getParquetSchema();}
0
public void testArrowTimestampMillisecondToParquet()
{    MessageType expected = converter.fromArrow(new Schema(asList(field("a", new ArrowType.Timestamp(TimeUnit.MILLISECOND, "UTC"))))).getParquetSchema();    Assert.assertEquals(expected, Types.buildMessage().addField(Types.optional(INT64).as(TIMESTAMP_MILLIS).named("a")).named("root"));}
0
public void testArrowTimestampMicrosecondToParquet()
{    MessageType expected = converter.fromArrow(new Schema(asList(field("a", new ArrowType.Timestamp(TimeUnit.MICROSECOND, "UTC"))))).getParquetSchema();    Assert.assertEquals(expected, Types.buildMessage().addField(Types.optional(INT64).as(TIMESTAMP_MICROS).named("a")).named("root"));}
0
public void testParquetInt64TimestampMillisToArrow()
{    MessageType parquet = Types.buildMessage().addField(Types.optional(INT64).as(TIMESTAMP_MILLIS).named("a")).named("root");    Schema expected = new Schema(asList(field("a", new ArrowType.Timestamp(TimeUnit.MILLISECOND, "UTC"))));    Assert.assertEquals(expected, converter.fromParquet(parquet).getArrowSchema());}
0
public void testParquetInt64TimestampMicrosToArrow()
{    MessageType parquet = Types.buildMessage().addField(Types.optional(INT64).as(TIMESTAMP_MICROS).named("a")).named("root");    Schema expected = new Schema(asList(field("a", new ArrowType.Timestamp(TimeUnit.MICROSECOND, "UTC"))));    Assert.assertEquals(expected, converter.fromParquet(parquet).getArrowSchema());}
0
public void testParquetInt32TimestampMillisToArrow()
{    converter.fromParquet(Types.buildMessage().addField(Types.optional(INT32).as(TIMESTAMP_MILLIS).named("a")).named("root"));}
0
public void testParquetInt32TimestampMicrosToArrow()
{    converter.fromParquet(Types.buildMessage().addField(Types.optional(INT32).as(TIMESTAMP_MICROS).named("a")).named("root"));}
0
public T getCurrentRecord()
{    return root.getCurrentRecord();}
0
public GroupConverter getRootConverter()
{    return root;}
0
public void addBinary(Binary value)
{    parent.add(convert(value));}
0
public boolean hasDictionarySupport()
{    return true;}
0
public void setDictionary(Dictionary dictionary)
{    dict = (T[]) new Object[dictionary.getMaxId() + 1];    for (int i = 0; i <= dictionary.getMaxId(); i++) {        dict[i] = convert(dictionary.decodeToBinary(i));    }}
0
public T prepareDictionaryValue(T value)
{    return value;}
0
public void addValueFromDictionary(int dictionaryId)
{    parent.add(prepareDictionaryValue(dict[dictionaryId]));}
0
public void addInt(int value)
{    parent.addByte((byte) value);}
0
public void addInt(int value)
{    parent.addShort((short) value);}
0
public void addInt(int value)
{    parent.addChar((char) value);}
0
public final void addBoolean(boolean value)
{    parent.addBoolean(value);}
0
public final void addInt(int value)
{    parent.addInt(value);}
0
public final void addInt(int value)
{    parent.addLong((long) value);}
0
public final void addLong(long value)
{    parent.addLong(value);}
0
public final void addInt(int value)
{    parent.addFloat((float) value);}
0
public final void addLong(long value)
{    parent.addFloat((float) value);}
0
public final void addFloat(float value)
{    parent.addFloat(value);}
0
public final void addInt(int value)
{    parent.addDouble((double) value);}
0
public final void addLong(long value)
{    parent.addDouble((double) value);}
0
public final void addFloat(float value)
{    parent.addDouble((double) value);}
0
public final void addDouble(double value)
{    parent.addDouble(value);}
0
public byte[] convert(Binary binary)
{    return binary.getBytes();}
0
public ByteBuffer convert(Binary binary)
{    return ByteBuffer.wrap(binary.getBytes());}
0
public ByteBuffer prepareDictionaryValue(ByteBuffer value)
{    return value.duplicate();}
0
public String convert(Binary binary)
{    return binary.toStringUsingUTF8();}
0
public Utf8 convert(Binary binary)
{    return new Utf8(binary.getBytes());}
0
public Object convert(Binary binary)
{    try {        return ctor.newInstance(binary.toStringUsingUTF8());    } catch (InstantiationException e) {        throw new ParquetDecodingException("Cannot convert binary to " + stringableName, e);    } catch (IllegalAccessException e) {        throw new ParquetDecodingException("Cannot convert binary to " + stringableName, e);    } catch (InvocationTargetException e) {        throw new ParquetDecodingException("Cannot convert binary to " + stringableName, e);    }}
0
public Object convert(Binary binary)
{    return model.createEnum(binary.toStringUsingUTF8(), schema);}
0
public Object convert(Binary binary)
{    return model.createFixed(null, /* reuse */    binary.getBytes(), schema);}
0
public void add(Object value)
{    AvroIndexedRecordConverter.this.set(finalAvroIndex, value);}
0
private static Class<T> getDatumClass(GenericData model, Schema schema)
{    if (model.getConversionFor(schema.getLogicalType()) != null) {                return null;    }    if (model instanceof SpecificData) {        return (Class<T>) ((SpecificData) model).getClass(schema);    }    return null;}
0
private Schema.Field getAvroField(String parquetFieldName)
{    Schema.Field avroField = avroSchema.getField(parquetFieldName);    for (Schema.Field f : avroSchema.getFields()) {        if (f.aliases().contains(parquetFieldName)) {            return f;        }    }    if (avroField == null) {        throw new InvalidRecordException(String.format("Parquet/Avro schema mismatch. Avro field '%s' not found.", parquetFieldName));    }    return avroField;}
0
private static Converter newConverter(Schema schema, Type type, GenericData model, ParentValueContainer setter)
{    LogicalType logicalType = schema.getLogicalType();                Conversion<?> conversion = model.getConversionFor(logicalType);    ParentValueContainer parent = ParentValueContainer.getConversionContainer(setter, conversion, schema);    if (schema.getType().equals(Schema.Type.BOOLEAN)) {        return new AvroConverters.FieldBooleanConverter(parent);    } else if (schema.getType().equals(Schema.Type.INT)) {        return new AvroConverters.FieldIntegerConverter(parent);    } else if (schema.getType().equals(Schema.Type.LONG)) {        return new AvroConverters.FieldLongConverter(parent);    } else if (schema.getType().equals(Schema.Type.FLOAT)) {        return new AvroConverters.FieldFloatConverter(parent);    } else if (schema.getType().equals(Schema.Type.DOUBLE)) {        return new AvroConverters.FieldDoubleConverter(parent);    } else if (schema.getType().equals(Schema.Type.BYTES)) {        return new AvroConverters.FieldByteBufferConverter(parent);    } else if (schema.getType().equals(Schema.Type.STRING)) {        return new AvroConverters.FieldStringConverter(parent);    } else if (schema.getType().equals(Schema.Type.RECORD)) {        return new AvroIndexedRecordConverter(parent, type.asGroupType(), schema, model);    } else if (schema.getType().equals(Schema.Type.ENUM)) {        return new FieldEnumConverter(parent, schema, model);    } else if (schema.getType().equals(Schema.Type.ARRAY)) {        return new AvroArrayConverter(parent, type.asGroupType(), schema, model);    } else if (schema.getType().equals(Schema.Type.MAP)) {        return new MapConverter(parent, type.asGroupType(), schema, model);    } else if (schema.getType().equals(Schema.Type.UNION)) {        return new AvroUnionConverter(parent, type, schema, model);    } else if (schema.getType().equals(Schema.Type.FIXED)) {        return new FieldFixedConverter(parent, schema, model);    }    throw new UnsupportedOperationException(String.format("Cannot convert Avro type: %s" + " (Parquet type: %s) ", schema, type));}
0
private void set(int index, Object value)
{    this.currentRecord.put(index, value);}
0
public Converter getConverter(int fieldIndex)
{    return converters[fieldIndex];}
0
public void start()
{        this.currentRecord = (T) ((this.specificClass == null) ? new GenericData.Record(avroSchema) : SpecificData.newInstance(specificClass, avroSchema));}
0
public void end()
{    fillInDefaults();    if (parent != null) {        parent.add(currentRecord);    }}
0
private void fillInDefaults()
{    for (Map.Entry<Schema.Field, Object> entry : recordDefaults.entrySet()) {        Schema.Field f = entry.getKey();                Object defaultValue = deepCopy(f.schema(), entry.getValue());        this.currentRecord.put(f.pos(), defaultValue);    }}
0
private Object deepCopy(Schema schema, Object value)
{    switch(schema.getType()) {        case BOOLEAN:        case INT:        case LONG:        case FLOAT:        case DOUBLE:            return value;        default:            return model.deepCopy(schema, value);    }}
0
 T getCurrentRecord()
{    return currentRecord;}
0
public final void addBinary(Binary value)
{    Object enumValue = value.toStringUsingUTF8();    if (enumClass != null) {        enumValue = (Enum.valueOf(enumClass, (String) enumValue));    }    parent.add(enumValue);}
0
public final void addBinary(Binary value)
{    if (fixedClass == null) {        parent.add(new GenericData.Fixed(avroSchema, value.getBytes()));    } else {        if (fixedClassCtor == null) {            throw new IllegalArgumentException("fixedClass specified but fixedClassCtor is null.");        }        try {            Object fixed = fixedClassCtor.newInstance(value.getBytes());            parent.add(fixed);        } catch (Exception e) {            throw new RuntimeException(e);        }    }}
0
public void add(Object value)
{    array.add(value);}
0
public Converter getConverter(int fieldIndex)
{    return converter;}
0
public void start()
{    array = new GenericData.Array<Object>(0, avroSchema);}
0
public void end()
{    parent.add(array);}
0
public void add(Object value)
{    ElementConverter.this.element = value;}
0
public Converter getConverter(int fieldIndex)
{    Preconditions.checkArgument(fieldIndex == 0, "Illegal field index: " + fieldIndex);    return elementConverter;}
0
public void start()
{    element = null;}
0
public void end()
{    array.add(element);}
0
public void add(Object value)
{    Preconditions.checkArgument(memberValue == null, "Union is resolving to more than one type");    memberValue = value;}
0
public Converter getConverter(int fieldIndex)
{    return memberConverters[fieldIndex];}
0
public void start()
{    memberValue = null;}
0
public void end()
{    parent.add(memberValue);}
0
public Converter getConverter(int fieldIndex)
{    return keyValueConverter;}
0
public void start()
{    this.map = new HashMap<String, V>();}
0
public void end()
{    parent.add(map);}
0
public final void addBinary(Binary value)
{    key = value.toStringUsingUTF8();}
0
public void add(Object value)
{    MapKeyValueConverter.this.value = (V) value;}
0
public Converter getConverter(int fieldIndex)
{    if (fieldIndex == 0) {        return keyConverter;    } else if (fieldIndex == 1) {        return valueConverter;    }    throw new IllegalArgumentException("only the key (0) and value (1) fields expected: " + fieldIndex);}
0
public void start()
{    key = null;    value = null;}
0
public void end()
{    map.put(key, value);}
0
public static void setRequestedProjection(Job job, Schema requestedProjection)
{    AvroReadSupport.setRequestedProjection(ContextUtil.getConfiguration(job), requestedProjection);}
0
public static void setAvroReadSchema(Job job, Schema avroReadSchema)
{    AvroReadSupport.setAvroReadSchema(ContextUtil.getConfiguration(job), avroReadSchema);}
0
public static void setAvroDataSupplier(Job job, Class<? extends AvroDataSupplier> supplierClass)
{    AvroReadSupport.setAvroDataSupplier(ContextUtil.getConfiguration(job), supplierClass);}
0
public static void setSchema(Job job, Schema schema)
{    AvroWriteSupport.setSchema(ContextUtil.getConfiguration(job), schema);}
0
public static void setAvroDataSupplier(Job job, Class<? extends AvroDataSupplier> supplierClass)
{    AvroWriteSupport.setAvroDataSupplier(ContextUtil.getConfiguration(job), supplierClass);}
0
public static Builder<T> builder(Path file)
{    return new Builder<T>(file);}
0
public static Builder<T> builder(InputFile file)
{    return new Builder<T>(file);}
0
public Builder<T> withDataModel(GenericData model)
{    this.model = model;        if (model.getClass() != GenericData.class && model.getClass() != SpecificData.class) {        isReflect = true;    }    return this;}
0
public Builder<T> disableCompatibility()
{    this.enableCompatibility = false;    return this;}
0
public Builder<T> withCompatibility(boolean enableCompatibility)
{    this.enableCompatibility = enableCompatibility;    return this;}
0
protected ReadSupport<T> getReadSupport()
{    if (isReflect) {        conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, false);    } else {        conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, enableCompatibility);    }    return new AvroReadSupport<T>(model);}
0
public static Builder<T> builder(Path file)
{    return new Builder<T>(file);}
0
public static Builder<T> builder(OutputFile file)
{    return new Builder<T>(file);}
0
private static WriteSupport<T> writeSupport(Schema avroSchema, GenericData model)
{    return new AvroWriteSupport<T>(new AvroSchemaConverter().convert(avroSchema), avroSchema, model);}
0
private static WriteSupport<T> writeSupport(Configuration conf, Schema avroSchema, GenericData model)
{    return new AvroWriteSupport<T>(new AvroSchemaConverter(conf).convert(avroSchema), avroSchema, model);}
0
public Builder<T> withSchema(Schema schema)
{    this.schema = schema;    return this;}
0
public Builder<T> withDataModel(GenericData model)
{    this.model = model;    return this;}
0
protected Builder<T> self()
{    return this;}
0
protected WriteSupport<T> getWriteSupport(Configuration conf)
{    return AvroParquetWriter.writeSupport(conf, schema, model);}
0
public static void setRequestedProjection(Configuration configuration, Schema requestedProjection)
{    configuration.set(AVRO_REQUESTED_PROJECTION, requestedProjection.toString());}
0
public static void setAvroReadSchema(Configuration configuration, Schema avroReadSchema)
{    configuration.set(AVRO_READ_SCHEMA, avroReadSchema.toString());}
0
public static void setAvroDataSupplier(Configuration configuration, Class<? extends AvroDataSupplier> clazz)
{    configuration.set(AVRO_DATA_SUPPLIER, clazz.getName());}
0
public ReadContext init(Configuration configuration, Map<String, String> keyValueMetaData, MessageType fileSchema)
{    MessageType projection = fileSchema;    Map<String, String> metadata = new LinkedHashMap<String, String>();    String requestedProjectionString = configuration.get(AVRO_REQUESTED_PROJECTION);    if (requestedProjectionString != null) {        Schema avroRequestedProjection = new Schema.Parser().parse(requestedProjectionString);        projection = new AvroSchemaConverter(configuration).convert(avroRequestedProjection);    }    String avroReadSchema = configuration.get(AVRO_READ_SCHEMA);    if (avroReadSchema != null) {        metadata.put(AVRO_READ_SCHEMA_METADATA_KEY, avroReadSchema);    }    if (configuration.getBoolean(AVRO_COMPATIBILITY, AVRO_DEFAULT_COMPATIBILITY)) {        metadata.put(AVRO_COMPATIBILITY, "true");    }    return new ReadContext(projection, metadata);}
0
public RecordMaterializer<T> prepareForRead(Configuration configuration, Map<String, String> keyValueMetaData, MessageType fileSchema, ReadContext readContext)
{    Map<String, String> metadata = readContext.getReadSupportMetadata();    MessageType parquetSchema = readContext.getRequestedSchema();    Schema avroSchema;    if (metadata.get(AVRO_READ_SCHEMA_METADATA_KEY) != null) {                avroSchema = new Schema.Parser().parse(metadata.get(AVRO_READ_SCHEMA_METADATA_KEY));    } else if (keyValueMetaData.get(AVRO_SCHEMA_METADATA_KEY) != null) {                avroSchema = new Schema.Parser().parse(keyValueMetaData.get(AVRO_SCHEMA_METADATA_KEY));    } else if (keyValueMetaData.get(OLD_AVRO_SCHEMA_METADATA_KEY) != null) {                avroSchema = new Schema.Parser().parse(keyValueMetaData.get(OLD_AVRO_SCHEMA_METADATA_KEY));    } else {                avroSchema = new AvroSchemaConverter(configuration).convert(parquetSchema);    }    GenericData model = getDataModel(configuration);    String compatEnabled = metadata.get(AvroReadSupport.AVRO_COMPATIBILITY);    if (compatEnabled != null && Boolean.valueOf(compatEnabled)) {        return newCompatMaterializer(parquetSchema, avroSchema, model);    }    return new AvroRecordMaterializer<T>(parquetSchema, avroSchema, model);}
0
private static RecordMaterializer<T> newCompatMaterializer(MessageType parquetSchema, Schema avroSchema, GenericData model)
{    return (RecordMaterializer<T>) new AvroCompatRecordMaterializer(parquetSchema, avroSchema, model);}
0
private GenericData getDataModel(Configuration conf)
{    if (model != null) {        return model;    }    Class<? extends AvroDataSupplier> suppClass = conf.getClass(AVRO_DATA_SUPPLIER, SpecificDataSupplier.class, AvroDataSupplier.class);    return ReflectionUtils.newInstance(suppClass, conf).get();}
0
public void add(Object value)
{    AvroRecordConverter.this.currentRecord = (T) value;}
0
public void add(Object value)
{    AvroRecordConverter.this.set(avroField.name(), finalAvroIndex, value);}
0
private static Map<String, Class<?>> getFieldsByName(Class<?> recordClass, boolean excludeJava)
{    Map<String, Class<?>> fields = new LinkedHashMap<String, Class<?>>();    if (recordClass != null) {        Class<?> current = recordClass;        do {            if (excludeJava && current.getPackage() != null && current.getPackage().getName().startsWith("java.")) {                                break;            }            for (Field field : current.getDeclaredFields()) {                if (field.isAnnotationPresent(AvroIgnore.class) || isTransientOrStatic(field)) {                    continue;                }                AvroName altName = field.getAnnotation(AvroName.class);                Class<?> existing = fields.put(altName != null ? altName.value() : field.getName(), field.getType());                if (existing != null) {                    throw new AvroTypeException(current + " contains two fields named: " + field.getName());                }            }            current = current.getSuperclass();        } while (current != null);    }    return fields;}
0
private static boolean isTransientOrStatic(Field field)
{    return (field.getModifiers() & (Modifier.TRANSIENT | Modifier.STATIC)) != 0;}
0
private Schema.Field getAvroField(String parquetFieldName)
{    Schema.Field avroField = avroSchema.getField(parquetFieldName);    if (avroField != null) {        return avroField;    }    for (Schema.Field f : avroSchema.getFields()) {        if (f.aliases().contains(parquetFieldName)) {            return f;        }    }    throw new InvalidRecordException(String.format("Parquet/Avro schema mismatch: Avro field '%s' not found", parquetFieldName));}
0
private static Converter newConverter(Schema schema, Type type, GenericData model, ParentValueContainer setter)
{    return newConverter(schema, type, model, null, setter);}
0
private static Converter newConverter(Schema schema, Type type, GenericData model, Class<?> knownClass, ParentValueContainer setter)
{    LogicalType logicalType = schema.getLogicalType();    Conversion<?> conversion;    if (knownClass != null) {        conversion = model.getConversionByClass(knownClass, logicalType);    } else {        conversion = model.getConversionFor(logicalType);    }    ParentValueContainer parent = ParentValueContainer.getConversionContainer(setter, conversion, schema);    if (schema.getType().equals(Schema.Type.BOOLEAN)) {        return new AvroConverters.FieldBooleanConverter(parent);    } else if (schema.getType().equals(Schema.Type.INT)) {        Class<?> datumClass = getDatumClass(conversion, knownClass, schema, model);        if (datumClass == null) {            return new AvroConverters.FieldIntegerConverter(parent);        } else if (datumClass == byte.class || datumClass == Byte.class) {            return new AvroConverters.FieldByteConverter(parent);        } else if (datumClass == short.class || datumClass == Short.class) {            return new AvroConverters.FieldShortConverter(parent);        } else if (datumClass == char.class || datumClass == Character.class) {            return new AvroConverters.FieldCharConverter(parent);        }        return new AvroConverters.FieldIntegerConverter(parent);    } else if (schema.getType().equals(Schema.Type.LONG)) {        return new AvroConverters.FieldLongConverter(parent);    } else if (schema.getType().equals(Schema.Type.FLOAT)) {        return new AvroConverters.FieldFloatConverter(parent);    } else if (schema.getType().equals(Schema.Type.DOUBLE)) {        return new AvroConverters.FieldDoubleConverter(parent);    } else if (schema.getType().equals(Schema.Type.BYTES)) {        Class<?> datumClass = getDatumClass(conversion, knownClass, schema, model);        if (datumClass == null) {            return new AvroConverters.FieldByteBufferConverter(parent);        } else if (datumClass.isArray() && datumClass.getComponentType() == byte.class) {            return new AvroConverters.FieldByteArrayConverter(parent);        }        return new AvroConverters.FieldByteBufferConverter(parent);    } else if (schema.getType().equals(Schema.Type.STRING)) {        return newStringConverter(schema, model, parent);    } else if (schema.getType().equals(Schema.Type.RECORD)) {        return new AvroRecordConverter(parent, type.asGroupType(), schema, model);    } else if (schema.getType().equals(Schema.Type.ENUM)) {        return new AvroConverters.FieldEnumConverter(parent, schema, model);    } else if (schema.getType().equals(Schema.Type.ARRAY)) {        Class<?> datumClass = getDatumClass(conversion, knownClass, schema, model);        if (datumClass != null && datumClass.isArray()) {            return new AvroArrayConverter(parent, type.asGroupType(), schema, model, datumClass);        } else {            return new AvroCollectionConverter(parent, type.asGroupType(), schema, model, datumClass);        }    } else if (schema.getType().equals(Schema.Type.MAP)) {        return new MapConverter(parent, type.asGroupType(), schema, model);    } else if (schema.getType().equals(Schema.Type.UNION)) {        return new AvroUnionConverter(parent, type, schema, model);    } else if (schema.getType().equals(Schema.Type.FIXED)) {        return new AvroConverters.FieldFixedConverter(parent, schema, model);    }    throw new UnsupportedOperationException(String.format("Cannot convert Avro type: %s to Parquet type: %s", schema, type));}
0
private static Converter newStringConverter(Schema schema, GenericData model, ParentValueContainer parent)
{    Class<?> stringableClass = getStringableClass(schema, model);    if (stringableClass == String.class) {        return new FieldStringConverter(parent);    } else if (stringableClass == CharSequence.class) {        return new AvroConverters.FieldUTF8Converter(parent);    }    return new FieldStringableConverter(parent, stringableClass);}
0
private static Class<?> getStringableClass(Schema schema, GenericData model)
{    if (model instanceof SpecificData) {                boolean isMap = (schema.getType() == Schema.Type.MAP);        String stringableClass = schema.getProp(isMap ? JAVA_KEY_CLASS_PROP : JAVA_CLASS_PROP);        if (stringableClass != null) {            try {                return ClassUtils.forName(model.getClassLoader(), stringableClass);            } catch (ClassNotFoundException e) {                        }        }    }    if (ReflectData.class.isAssignableFrom(model.getClass())) {                return String.class;    }        String name = schema.getProp(STRINGABLE_PROP);    if (name == null) {        return CharSequence.class;    }    switch(GenericData.StringType.valueOf(name)) {        case String:            return String.class;        default:                        return CharSequence.class;    }}
0
private static Class<T> getDatumClass(Schema schema, GenericData model)
{    return getDatumClass(null, null, schema, model);}
0
private static Class<T> getDatumClass(Conversion<?> conversion, Class<T> knownClass, Schema schema, GenericData model)
{    if (conversion != null) {                return null;    }        if (knownClass != null) {        return knownClass;    }    if (model instanceof SpecificData) {                return ((SpecificData) model).getClass(schema);    } else if (model.getClass() == GenericData.class) {        return null;    } else {                Class<? extends GenericData> modelClass = model.getClass();        Method getClassMethod;        try {            getClassMethod = modelClass.getMethod("getClass", Schema.class);        } catch (NoSuchMethodException e) {                        return null;        }        try {            return (Class<T>) getClassMethod.invoke(schema);        } catch (IllegalAccessException e) {            return null;        } catch (InvocationTargetException e) {            return null;        }    }}
0
protected void set(String name, int avroIndex, Object value)
{    model.setField(currentRecord, name, avroIndex, value);}
0
public Converter getConverter(int fieldIndex)
{    return converters[fieldIndex];}
0
public void start()
{    this.currentRecord = (T) model.newRecord(null, avroSchema);}
0
public void end()
{    fillInDefaults();    if (parent != null) {        parent.add(currentRecord);    } else {                rootContainer.add(currentRecord);    }}
0
private void fillInDefaults()
{    for (Map.Entry<Schema.Field, Object> entry : recordDefaults.entrySet()) {        Schema.Field f = entry.getKey();                Object defaultValue = deepCopy(f.schema(), entry.getValue());        set(f.name(), f.pos(), defaultValue);    }}
0
private Object deepCopy(Schema schema, Object value)
{    switch(schema.getType()) {        case BOOLEAN:        case INT:        case LONG:        case FLOAT:        case DOUBLE:            return value;        default:            return model.deepCopy(schema, value);    }}
0
 T getCurrentRecord()
{    return currentRecord;}
0
public void add(Object value)
{    container.add(value);}
0
public Converter getConverter(int fieldIndex)
{    return converter;}
0
public void start()
{    container = newContainer();}
0
public void end()
{    parent.add(container);}
0
private Collection<Object> newContainer()
{    if (containerClass == null) {        return new GenericData.Array<Object>(0, avroSchema);    } else if (containerClass.isAssignableFrom(ArrayList.class)) {        return new ArrayList<Object>();    } else {                return (Collection<Object>) ReflectData.newInstance(containerClass, avroSchema);    }}
0
public void add(Object value)
{    ElementConverter.this.element = value;}
0
public Converter getConverter(int fieldIndex)
{    Preconditions.checkArgument(fieldIndex == 0, "Illegal field index: " + fieldIndex);    return elementConverter;}
0
public void start()
{    element = null;}
0
public void end()
{    container.add(element);}
0
public Converter getConverter(int fieldIndex)
{    return converter;}
0
public void start()
{        container.clear();}
0
public void end()
{    if (elementClass == boolean.class) {        parent.add(((BooleanArrayList) container).toBooleanArray());    } else if (elementClass == byte.class) {        parent.add(((ByteArrayList) container).toByteArray());    } else if (elementClass == char.class) {        parent.add(((CharArrayList) container).toCharArray());    } else if (elementClass == short.class) {        parent.add(((ShortArrayList) container).toShortArray());    } else if (elementClass == int.class) {        parent.add(((IntArrayList) container).toIntArray());    } else if (elementClass == long.class) {        parent.add(((LongArrayList) container).toLongArray());    } else if (elementClass == float.class) {        parent.add(((FloatArrayList) container).toFloatArray());    } else if (elementClass == double.class) {        parent.add(((DoubleArrayList) container).toDoubleArray());    } else {        parent.add(((ArrayList) container).toArray());    }}
0
private ParentValueContainer createSetterAndContainer()
{    if (elementClass == boolean.class) {        final BooleanArrayList list = new BooleanArrayList();        this.container = list;        return new ParentValueContainer() {            @Override            public void addBoolean(boolean value) {                list.add(value);            }        };    } else if (elementClass == byte.class) {        final ByteArrayList list = new ByteArrayList();        this.container = list;        return new ParentValueContainer() {            @Override            public void addByte(byte value) {                list.add(value);            }        };    } else if (elementClass == char.class) {        final CharArrayList list = new CharArrayList();        this.container = list;        return new ParentValueContainer() {            @Override            public void addChar(char value) {                list.add(value);            }        };    } else if (elementClass == short.class) {        final ShortArrayList list = new ShortArrayList();        this.container = list;        return new ParentValueContainer() {            @Override            public void addShort(short value) {                list.add(value);            }        };    } else if (elementClass == int.class) {        final IntArrayList list = new IntArrayList();        this.container = list;        return new ParentValueContainer() {            @Override            public void addInt(int value) {                list.add(value);            }        };    } else if (elementClass == long.class) {        final LongArrayList list = new LongArrayList();        this.container = list;        return new ParentValueContainer() {            @Override            public void addLong(long value) {                list.add(value);            }        };    } else if (elementClass == float.class) {        final FloatArrayList list = new FloatArrayList();        this.container = list;        return new ParentValueContainer() {            @Override            public void addFloat(float value) {                list.add(value);            }        };    } else if (elementClass == double.class) {        final DoubleArrayList list = new DoubleArrayList();        this.container = list;        return new ParentValueContainer() {            @Override            public void addDouble(double value) {                list.add(value);            }        };    } else {                final List<Object> list = new ArrayList<Object>();        this.container = list;        return new ParentValueContainer() {            @Override            public void add(Object value) {                list.add(value);            }        };    }}
0
public void addBoolean(boolean value)
{    list.add(value);}
0
public void addByte(byte value)
{    list.add(value);}
0
public void addChar(char value)
{    list.add(value);}
0
public void addShort(short value)
{    list.add(value);}
0
public void addInt(int value)
{    list.add(value);}
0
public void addLong(long value)
{    list.add(value);}
0
public void addFloat(float value)
{    list.add(value);}
0
public void addDouble(double value)
{    list.add(value);}
0
public void add(Object value)
{    list.add(value);}
0
public void add(Object value)
{    isSet = true;    setter.add(value);}
0
public void addByte(byte value)
{    isSet = true;    setter.addByte(value);}
0
public void addBoolean(boolean value)
{    isSet = true;    setter.addBoolean(value);}
0
public void addChar(char value)
{    isSet = true;    setter.addChar(value);}
0
public void addShort(short value)
{    isSet = true;    setter.addShort(value);}
0
public void addInt(int value)
{    isSet = true;    setter.addInt(value);}
0
public void addLong(long value)
{    isSet = true;    setter.addLong(value);}
0
public void addFloat(float value)
{    isSet = true;    setter.addFloat(value);}
0
public void addDouble(double value)
{    isSet = true;    setter.addDouble(value);}
0
public Converter getConverter(int fieldIndex)
{    Preconditions.checkArgument(fieldIndex == 0, "Illegal field index: " + fieldIndex);    return elementConverter;}
0
public void start()
{    isSet = false;}
0
public void end()
{    if (!isSet) {        container.add(null);    }}
0
 static boolean isElementType(Type repeatedType, Schema elementSchema)
{    if (repeatedType.isPrimitive() || repeatedType.asGroupType().getFieldCount() > 1 || repeatedType.asGroupType().getType(0).isRepetition(REPEATED)) {                return true;    } else if (elementSchema != null && elementSchema.getType() == Schema.Type.RECORD) {        Schema schemaFromRepeated = CONVERTER.convert(repeatedType.asGroupType());        if (checkReaderWriterCompatibility(elementSchema, schemaFromRepeated).getType() == COMPATIBLE) {            return true;        }    }    return false;}
0
public void add(Object value)
{    Preconditions.checkArgument(AvroUnionConverter.this.memberValue == null, "Union is resolving to more than one type");    memberValue = value;}
0
public Converter getConverter(int fieldIndex)
{    return memberConverters[fieldIndex];}
0
public void start()
{    memberValue = null;}
0
public void end()
{    parent.add(memberValue);}
0
public Converter getConverter(int fieldIndex)
{    return keyValueConverter;}
0
public void start()
{    this.map = newMap();}
0
public void end()
{    parent.add(map);}
0
private Map<K, V> newMap()
{    if (mapClass == null || mapClass.isAssignableFrom(HashMap.class)) {        return new HashMap<K, V>();    } else {        return (Map<K, V>) ReflectData.newInstance(mapClass, schema);    }}
0
public void add(Object value)
{    MapKeyValueConverter.this.key = (K) value;}
0
public void add(Object value)
{    MapKeyValueConverter.this.value = (V) value;}
0
public Converter getConverter(int fieldIndex)
{    if (fieldIndex == 0) {        return keyConverter;    } else if (fieldIndex == 1) {        return valueConverter;    }    throw new IllegalArgumentException("only the key (0) and value (1) fields expected: " + fieldIndex);}
0
public void start()
{    key = null;    value = null;}
0
public void end()
{    map.put(key, value);}
0
public T getCurrentRecord()
{    return root.getCurrentRecord();}
0
public GroupConverter getRootConverter()
{    return root;}
0
public static Schema getNonNull(Schema schema)
{    if (schema.getType().equals(Schema.Type.UNION)) {        List<Schema> schemas = schema.getTypes();        if (schemas.size() == 2) {            if (schemas.get(0).getType().equals(Schema.Type.NULL)) {                return schemas.get(1);            } else if (schemas.get(1).getType().equals(Schema.Type.NULL)) {                return schemas.get(0);            } else {                return schema;            }        } else {            return schema;        }    } else {        return schema;    }}
0
public MessageType convert(Schema avroSchema)
{    if (!avroSchema.getType().equals(Schema.Type.RECORD)) {        throw new IllegalArgumentException("Avro schema must be a record.");    }    return new MessageType(avroSchema.getFullName(), convertFields(avroSchema.getFields()));}
0
private List<Type> convertFields(List<Schema.Field> fields)
{    List<Type> types = new ArrayList<Type>();    for (Schema.Field field : fields) {        if (field.schema().getType().equals(Schema.Type.NULL)) {                        continue;        }        types.add(convertField(field));    }    return types;}
0
private Type convertField(String fieldName, Schema schema)
{    return convertField(fieldName, schema, Type.Repetition.REQUIRED);}
0
private Type convertField(String fieldName, Schema schema, Type.Repetition repetition)
{    Types.PrimitiveBuilder<PrimitiveType> builder;    Schema.Type type = schema.getType();    if (type.equals(Schema.Type.BOOLEAN)) {        builder = Types.primitive(BOOLEAN, repetition);    } else if (type.equals(Schema.Type.INT)) {        builder = Types.primitive(INT32, repetition);    } else if (type.equals(Schema.Type.LONG)) {        builder = Types.primitive(INT64, repetition);    } else if (type.equals(Schema.Type.FLOAT)) {        builder = Types.primitive(FLOAT, repetition);    } else if (type.equals(Schema.Type.DOUBLE)) {        builder = Types.primitive(DOUBLE, repetition);    } else if (type.equals(Schema.Type.BYTES)) {        builder = Types.primitive(BINARY, repetition);    } else if (type.equals(Schema.Type.STRING)) {        builder = Types.primitive(BINARY, repetition).as(stringType());    } else if (type.equals(Schema.Type.RECORD)) {        return new GroupType(repetition, fieldName, convertFields(schema.getFields()));    } else if (type.equals(Schema.Type.ENUM)) {        builder = Types.primitive(BINARY, repetition).as(enumType());    } else if (type.equals(Schema.Type.ARRAY)) {        if (writeOldListStructure) {            return ConversionPatterns.listType(repetition, fieldName, convertField("array", schema.getElementType(), REPEATED));        } else {            return ConversionPatterns.listOfElements(repetition, fieldName, convertField(AvroWriteSupport.LIST_ELEMENT_NAME, schema.getElementType()));        }    } else if (type.equals(Schema.Type.MAP)) {        Type valType = convertField("value", schema.getValueType());                return ConversionPatterns.stringKeyMapType(repetition, fieldName, valType);    } else if (type.equals(Schema.Type.FIXED)) {        builder = Types.primitive(FIXED_LEN_BYTE_ARRAY, repetition).length(schema.getFixedSize());    } else if (type.equals(Schema.Type.UNION)) {        return convertUnion(fieldName, schema, repetition);    } else {        throw new UnsupportedOperationException("Cannot convert Avro type " + type);    }            LogicalType logicalType = schema.getLogicalType();    if (logicalType != null) {        if (logicalType instanceof LogicalTypes.Decimal) {            LogicalTypes.Decimal decimal = (LogicalTypes.Decimal) logicalType;            builder = builder.as(decimalType(decimal.getScale(), decimal.getPrecision()));        } else {            LogicalTypeAnnotation annotation = convertLogicalType(logicalType);            if (annotation != null) {                builder.as(annotation);            }        }    }    return builder.named(fieldName);}
0
private Type convertUnion(String fieldName, Schema schema, Type.Repetition repetition)
{    List<Schema> nonNullSchemas = new ArrayList<Schema>(schema.getTypes().size());        boolean foundNullSchema = false;    for (Schema childSchema : schema.getTypes()) {        if (childSchema.getType().equals(Schema.Type.NULL)) {            foundNullSchema = true;            if (Type.Repetition.REQUIRED == repetition) {                repetition = Type.Repetition.OPTIONAL;            }        } else {            nonNullSchemas.add(childSchema);        }    }        switch(nonNullSchemas.size()) {        case 0:            throw new UnsupportedOperationException("Cannot convert Avro union of only nulls");        case 1:            return foundNullSchema ? convertField(fieldName, nonNullSchemas.get(0), repetition) : convertUnionToGroupType(fieldName, repetition, nonNullSchemas);        default:                        return convertUnionToGroupType(fieldName, repetition, nonNullSchemas);    }}
0
private Type convertUnionToGroupType(String fieldName, Type.Repetition repetition, List<Schema> nonNullSchemas)
{    List<Type> unionTypes = new ArrayList<Type>(nonNullSchemas.size());    int index = 0;    for (Schema childSchema : nonNullSchemas) {        unionTypes.add(convertField("member" + index++, childSchema, Type.Repetition.OPTIONAL));    }    return new GroupType(repetition, fieldName, unionTypes);}
0
private Type convertField(Schema.Field field)
{    return convertField(field.name(), field.schema());}
0
public Schema convert(MessageType parquetSchema)
{    return convertFields(parquetSchema.getName(), parquetSchema.getFields(), new HashMap<>());}
0
 Schema convert(GroupType parquetSchema)
{    return convertFields(parquetSchema.getName(), parquetSchema.getFields(), new HashMap<>());}
0
private Schema convertFields(String name, List<Type> parquetFields, Map<String, Integer> names)
{    List<Schema.Field> fields = new ArrayList<Schema.Field>();    Integer nameCount = names.merge(name, 1, (oldValue, value) -> oldValue + 1);    for (Type parquetType : parquetFields) {        Schema fieldSchema = convertField(parquetType, names);        if (parquetType.isRepetition(REPEATED)) {            throw new UnsupportedOperationException("REPEATED not supported outside LIST or MAP. Type: " + parquetType);        } else if (parquetType.isRepetition(Type.Repetition.OPTIONAL)) {            fields.add(new Schema.Field(parquetType.getName(), optional(fieldSchema), null, NULL_VALUE));        } else {                        fields.add(new Schema.Field(parquetType.getName(), fieldSchema, null, (Object) null));        }    }    Schema schema = Schema.createRecord(name, null, nameCount > 1 ? name + nameCount : null, false);    schema.setFields(fields);    return schema;}
0
private Schema convertField(final Type parquetType, Map<String, Integer> names)
{    if (parquetType.isPrimitive()) {        final PrimitiveType asPrimitive = parquetType.asPrimitiveType();        final PrimitiveTypeName parquetPrimitiveTypeName = asPrimitive.getPrimitiveTypeName();        final LogicalTypeAnnotation annotation = parquetType.getLogicalTypeAnnotation();        Schema schema = parquetPrimitiveTypeName.convert(new PrimitiveType.PrimitiveTypeNameConverter<Schema, RuntimeException>() {            @Override            public Schema convertBOOLEAN(PrimitiveTypeName primitiveTypeName) {                return Schema.create(Schema.Type.BOOLEAN);            }            @Override            public Schema convertINT32(PrimitiveTypeName primitiveTypeName) {                return Schema.create(Schema.Type.INT);            }            @Override            public Schema convertINT64(PrimitiveTypeName primitiveTypeName) {                return Schema.create(Schema.Type.LONG);            }            @Override            public Schema convertINT96(PrimitiveTypeName primitiveTypeName) {                throw new IllegalArgumentException("INT96 not implemented and is deprecated");            }            @Override            public Schema convertFLOAT(PrimitiveTypeName primitiveTypeName) {                return Schema.create(Schema.Type.FLOAT);            }            @Override            public Schema convertDOUBLE(PrimitiveTypeName primitiveTypeName) {                return Schema.create(Schema.Type.DOUBLE);            }            @Override            public Schema convertFIXED_LEN_BYTE_ARRAY(PrimitiveTypeName primitiveTypeName) {                int size = parquetType.asPrimitiveType().getTypeLength();                return Schema.createFixed(parquetType.getName(), null, null, size);            }            @Override            public Schema convertBINARY(PrimitiveTypeName primitiveTypeName) {                if (annotation instanceof LogicalTypeAnnotation.StringLogicalTypeAnnotation || annotation instanceof LogicalTypeAnnotation.EnumLogicalTypeAnnotation) {                    return Schema.create(Schema.Type.STRING);                } else {                    return Schema.create(Schema.Type.BYTES);                }            }        });        LogicalType logicalType = convertLogicalType(annotation);        if (logicalType != null && (!(annotation instanceof LogicalTypeAnnotation.DecimalLogicalTypeAnnotation) || parquetPrimitiveTypeName == BINARY || parquetPrimitiveTypeName == FIXED_LEN_BYTE_ARRAY)) {            schema = logicalType.addToSchema(schema);        }        return schema;    } else {        GroupType parquetGroupType = parquetType.asGroupType();        LogicalTypeAnnotation logicalTypeAnnotation = parquetGroupType.getLogicalTypeAnnotation();        if (logicalTypeAnnotation != null) {            return logicalTypeAnnotation.accept(new LogicalTypeAnnotation.LogicalTypeAnnotationVisitor<Schema>() {                @Override                public Optional<Schema> visit(LogicalTypeAnnotation.ListLogicalTypeAnnotation listLogicalType) {                    if (parquetGroupType.getFieldCount() != 1) {                        throw new UnsupportedOperationException("Invalid list type " + parquetGroupType);                    }                    Type repeatedType = parquetGroupType.getType(0);                    if (!repeatedType.isRepetition(REPEATED)) {                        throw new UnsupportedOperationException("Invalid list type " + parquetGroupType);                    }                    if (isElementType(repeatedType, parquetGroupType.getName())) {                                                return of(Schema.createArray(convertField(repeatedType, names)));                    } else {                        Type elementType = repeatedType.asGroupType().getType(0);                        if (elementType.isRepetition(Type.Repetition.OPTIONAL)) {                            return of(Schema.createArray(optional(convertField(elementType, names))));                        } else {                            return of(Schema.createArray(convertField(elementType, names)));                        }                    }                }                @Override                public                 Optional<Schema> visit(LogicalTypeAnnotation.MapKeyValueTypeAnnotation mapKeyValueLogicalType) {                    return visitMapOrMapKeyValue();                }                @Override                public Optional<Schema> visit(LogicalTypeAnnotation.MapLogicalTypeAnnotation mapLogicalType) {                    return visitMapOrMapKeyValue();                }                private Optional<Schema> visitMapOrMapKeyValue() {                    if (parquetGroupType.getFieldCount() != 1 || parquetGroupType.getType(0).isPrimitive()) {                        throw new UnsupportedOperationException("Invalid map type " + parquetGroupType);                    }                    GroupType mapKeyValType = parquetGroupType.getType(0).asGroupType();                    if (!mapKeyValType.isRepetition(REPEATED) || mapKeyValType.getFieldCount() != 2) {                        throw new UnsupportedOperationException("Invalid map type " + parquetGroupType);                    }                    Type keyType = mapKeyValType.getType(0);                    if (!keyType.isPrimitive() || !keyType.asPrimitiveType().getPrimitiveTypeName().equals(PrimitiveTypeName.BINARY) || !keyType.getLogicalTypeAnnotation().equals(stringType())) {                        throw new IllegalArgumentException("Map key type must be binary (UTF8): " + keyType);                    }                    Type valueType = mapKeyValType.getType(1);                    if (valueType.isRepetition(Type.Repetition.OPTIONAL)) {                        return of(Schema.createMap(optional(convertField(valueType, names))));                    } else {                        return of(Schema.createMap(convertField(valueType, names)));                    }                }                @Override                public Optional<Schema> visit(LogicalTypeAnnotation.EnumLogicalTypeAnnotation enumLogicalType) {                    return of(Schema.create(Schema.Type.STRING));                }            }).orElseThrow(() -> new UnsupportedOperationException("Cannot convert Parquet type " + parquetType));        } else {                        return convertFields(parquetGroupType.getName(), parquetGroupType.getFields(), names);        }    }}
0
public Schema convertBOOLEAN(PrimitiveTypeName primitiveTypeName)
{    return Schema.create(Schema.Type.BOOLEAN);}
0
public Schema convertINT32(PrimitiveTypeName primitiveTypeName)
{    return Schema.create(Schema.Type.INT);}
0
public Schema convertINT64(PrimitiveTypeName primitiveTypeName)
{    return Schema.create(Schema.Type.LONG);}
0
public Schema convertINT96(PrimitiveTypeName primitiveTypeName)
{    throw new IllegalArgumentException("INT96 not implemented and is deprecated");}
0
public Schema convertFLOAT(PrimitiveTypeName primitiveTypeName)
{    return Schema.create(Schema.Type.FLOAT);}
0
public Schema convertDOUBLE(PrimitiveTypeName primitiveTypeName)
{    return Schema.create(Schema.Type.DOUBLE);}
0
public Schema convertFIXED_LEN_BYTE_ARRAY(PrimitiveTypeName primitiveTypeName)
{    int size = parquetType.asPrimitiveType().getTypeLength();    return Schema.createFixed(parquetType.getName(), null, null, size);}
0
public Schema convertBINARY(PrimitiveTypeName primitiveTypeName)
{    if (annotation instanceof LogicalTypeAnnotation.StringLogicalTypeAnnotation || annotation instanceof LogicalTypeAnnotation.EnumLogicalTypeAnnotation) {        return Schema.create(Schema.Type.STRING);    } else {        return Schema.create(Schema.Type.BYTES);    }}
0
public Optional<Schema> visit(LogicalTypeAnnotation.ListLogicalTypeAnnotation listLogicalType)
{    if (parquetGroupType.getFieldCount() != 1) {        throw new UnsupportedOperationException("Invalid list type " + parquetGroupType);    }    Type repeatedType = parquetGroupType.getType(0);    if (!repeatedType.isRepetition(REPEATED)) {        throw new UnsupportedOperationException("Invalid list type " + parquetGroupType);    }    if (isElementType(repeatedType, parquetGroupType.getName())) {                return of(Schema.createArray(convertField(repeatedType, names)));    } else {        Type elementType = repeatedType.asGroupType().getType(0);        if (elementType.isRepetition(Type.Repetition.OPTIONAL)) {            return of(Schema.createArray(optional(convertField(elementType, names))));        } else {            return of(Schema.createArray(convertField(elementType, names)));        }    }}
0
public Optional<Schema> visit(LogicalTypeAnnotation.MapKeyValueTypeAnnotation mapKeyValueLogicalType)
{    return visitMapOrMapKeyValue();}
0
public Optional<Schema> visit(LogicalTypeAnnotation.MapLogicalTypeAnnotation mapLogicalType)
{    return visitMapOrMapKeyValue();}
0
private Optional<Schema> visitMapOrMapKeyValue()
{    if (parquetGroupType.getFieldCount() != 1 || parquetGroupType.getType(0).isPrimitive()) {        throw new UnsupportedOperationException("Invalid map type " + parquetGroupType);    }    GroupType mapKeyValType = parquetGroupType.getType(0).asGroupType();    if (!mapKeyValType.isRepetition(REPEATED) || mapKeyValType.getFieldCount() != 2) {        throw new UnsupportedOperationException("Invalid map type " + parquetGroupType);    }    Type keyType = mapKeyValType.getType(0);    if (!keyType.isPrimitive() || !keyType.asPrimitiveType().getPrimitiveTypeName().equals(PrimitiveTypeName.BINARY) || !keyType.getLogicalTypeAnnotation().equals(stringType())) {        throw new IllegalArgumentException("Map key type must be binary (UTF8): " + keyType);    }    Type valueType = mapKeyValType.getType(1);    if (valueType.isRepetition(Type.Repetition.OPTIONAL)) {        return of(Schema.createMap(optional(convertField(valueType, names))));    } else {        return of(Schema.createMap(convertField(valueType, names)));    }}
0
public Optional<Schema> visit(LogicalTypeAnnotation.EnumLogicalTypeAnnotation enumLogicalType)
{    return of(Schema.create(Schema.Type.STRING));}
0
private LogicalTypeAnnotation convertLogicalType(LogicalType logicalType)
{    if (logicalType == null) {        return null;    } else if (logicalType instanceof LogicalTypes.Decimal) {        LogicalTypes.Decimal decimal = (LogicalTypes.Decimal) logicalType;        return decimalType(decimal.getScale(), decimal.getPrecision());    } else if (logicalType instanceof LogicalTypes.Date) {        return dateType();    } else if (logicalType instanceof LogicalTypes.TimeMillis) {        return timeType(true, MILLIS);    } else if (logicalType instanceof LogicalTypes.TimeMicros) {        return timeType(true, MICROS);    } else if (logicalType instanceof LogicalTypes.TimestampMillis) {        return timestampType(true, MILLIS);    } else if (logicalType instanceof LogicalTypes.TimestampMicros) {        return timestampType(true, MICROS);    }    return null;}
0
private LogicalType convertLogicalType(LogicalTypeAnnotation annotation)
{    if (annotation == null) {        return null;    }    return annotation.accept(new LogicalTypeAnnotation.LogicalTypeAnnotationVisitor<LogicalType>() {        @Override        public Optional<LogicalType> visit(LogicalTypeAnnotation.DecimalLogicalTypeAnnotation decimalLogicalType) {            return of(LogicalTypes.decimal(decimalLogicalType.getPrecision(), decimalLogicalType.getScale()));        }        @Override        public Optional<LogicalType> visit(LogicalTypeAnnotation.DateLogicalTypeAnnotation dateLogicalType) {            return of(LogicalTypes.date());        }        @Override        public Optional<LogicalType> visit(LogicalTypeAnnotation.TimeLogicalTypeAnnotation timeLogicalType) {            LogicalTypeAnnotation.TimeUnit unit = timeLogicalType.getUnit();            switch(unit) {                case MILLIS:                    return of(LogicalTypes.timeMillis());                case MICROS:                    return of(LogicalTypes.timeMicros());            }            return empty();        }        @Override        public Optional<LogicalType> visit(LogicalTypeAnnotation.TimestampLogicalTypeAnnotation timestampLogicalType) {            LogicalTypeAnnotation.TimeUnit unit = timestampLogicalType.getUnit();            switch(unit) {                case MILLIS:                    return of(LogicalTypes.timestampMillis());                case MICROS:                    return of(LogicalTypes.timestampMicros());            }            return empty();        }    }).orElse(null);}
0
public Optional<LogicalType> visit(LogicalTypeAnnotation.DecimalLogicalTypeAnnotation decimalLogicalType)
{    return of(LogicalTypes.decimal(decimalLogicalType.getPrecision(), decimalLogicalType.getScale()));}
0
public Optional<LogicalType> visit(LogicalTypeAnnotation.DateLogicalTypeAnnotation dateLogicalType)
{    return of(LogicalTypes.date());}
0
public Optional<LogicalType> visit(LogicalTypeAnnotation.TimeLogicalTypeAnnotation timeLogicalType)
{    LogicalTypeAnnotation.TimeUnit unit = timeLogicalType.getUnit();    switch(unit) {        case MILLIS:            return of(LogicalTypes.timeMillis());        case MICROS:            return of(LogicalTypes.timeMicros());    }    return empty();}
0
public Optional<LogicalType> visit(LogicalTypeAnnotation.TimestampLogicalTypeAnnotation timestampLogicalType)
{    LogicalTypeAnnotation.TimeUnit unit = timestampLogicalType.getUnit();    switch(unit) {        case MILLIS:            return of(LogicalTypes.timestampMillis());        case MICROS:            return of(LogicalTypes.timestampMicros());    }    return empty();}
0
private boolean isElementType(Type repeatedType, String parentName)
{    return (    repeatedType.isPrimitive() || repeatedType.asGroupType().getFieldCount() > 1 || repeatedType.asGroupType().getType(0).isRepetition(REPEATED) ||     repeatedType.getName().equals("array") || repeatedType.getName().equals(parentName + "_tuple") ||     assumeRepeatedIsListElement);}
0
private static Schema optional(Schema original)
{        return Schema.createUnion(Arrays.asList(Schema.create(Schema.Type.NULL), original));}
0
public static void setAvroDataSupplier(Configuration configuration, Class<? extends AvroDataSupplier> suppClass)
{    configuration.set(AVRO_DATA_SUPPLIER, suppClass.getName());}
0
public String getName()
{    return "avro";}
0
public static void setSchema(Configuration configuration, Schema schema)
{    configuration.set(AVRO_SCHEMA, schema.toString());}
0
public WriteContext init(Configuration configuration)
{    if (rootAvroSchema == null) {        this.rootAvroSchema = new Schema.Parser().parse(configuration.get(AVRO_SCHEMA));        this.rootSchema = new AvroSchemaConverter().convert(rootAvroSchema);    }    if (model == null) {        this.model = getDataModel(configuration);    }    boolean writeOldListStructure = configuration.getBoolean(WRITE_OLD_LIST_STRUCTURE, WRITE_OLD_LIST_STRUCTURE_DEFAULT);    if (writeOldListStructure) {        this.listWriter = new TwoLevelListWriter();    } else {        this.listWriter = new ThreeLevelListWriter();    }    Map<String, String> extraMetaData = new HashMap<String, String>();    extraMetaData.put(AvroReadSupport.AVRO_SCHEMA_METADATA_KEY, rootAvroSchema.toString());    return new WriteContext(rootSchema, extraMetaData);}
0
public void prepareForWrite(RecordConsumer recordConsumer)
{    this.recordConsumer = recordConsumer;}
0
public void write(IndexedRecord record)
{    write((T) record);}
0
public void write(T record)
{    if (rootLogicalType != null) {        Conversion<?> conversion = model.getConversionByClass(record.getClass(), rootLogicalType);        recordConsumer.startMessage();        writeRecordFields(rootSchema, rootAvroSchema, convert(rootAvroSchema, rootLogicalType, conversion, record));        recordConsumer.endMessage();    } else {        recordConsumer.startMessage();        writeRecordFields(rootSchema, rootAvroSchema, record);        recordConsumer.endMessage();    }}
0
private void writeRecord(GroupType schema, Schema avroSchema, Object record)
{    recordConsumer.startGroup();    writeRecordFields(schema, avroSchema, record);    recordConsumer.endGroup();}
0
private void writeRecordFields(GroupType schema, Schema avroSchema, Object record)
{    List<Type> fields = schema.getFields();    List<Schema.Field> avroFields = avroSchema.getFields();        int index = 0;    for (int avroIndex = 0; avroIndex < avroFields.size(); avroIndex++) {        Schema.Field avroField = avroFields.get(avroIndex);        if (avroField.schema().getType().equals(Schema.Type.NULL)) {            continue;        }        Type fieldType = fields.get(index);        Object value = model.getField(record, avroField.name(), avroIndex);        if (value != null) {            recordConsumer.startField(fieldType.getName(), index);            writeValue(fieldType, avroField.schema(), value);            recordConsumer.endField(fieldType.getName(), index);        } else if (fieldType.isRepetition(Type.Repetition.REQUIRED)) {            throw new RuntimeException("Null-value for required field: " + avroField.name());        }        index++;    }}
0
private void writeMap(GroupType schema, Schema avroSchema, Map<CharSequence, V> map)
{    GroupType innerGroup = schema.getType(0).asGroupType();    Type keyType = innerGroup.getType(0);    Type valueType = innerGroup.getType(1);        recordConsumer.startGroup();    if (map.size() > 0) {        recordConsumer.startField(MAP_REPEATED_NAME, 0);        for (Map.Entry<CharSequence, V> entry : map.entrySet()) {                        recordConsumer.startGroup();            recordConsumer.startField(MAP_KEY_NAME, 0);            writeValue(keyType, MAP_KEY_SCHEMA, entry.getKey());            recordConsumer.endField(MAP_KEY_NAME, 0);            V value = entry.getValue();            if (value != null) {                recordConsumer.startField(MAP_VALUE_NAME, 1);                writeValue(valueType, avroSchema.getValueType(), value);                recordConsumer.endField(MAP_VALUE_NAME, 1);            } else if (!valueType.isRepetition(Type.Repetition.OPTIONAL)) {                throw new RuntimeException("Null map value for " + avroSchema.getName());            }            recordConsumer.endGroup();        }        recordConsumer.endField(MAP_REPEATED_NAME, 0);    }    recordConsumer.endGroup();}
0
private void writeUnion(GroupType parquetSchema, Schema avroSchema, Object value)
{    recordConsumer.startGroup();            int avroIndex = model.resolveUnion(avroSchema, value);        GroupType parquetGroup = parquetSchema.asGroupType();    int parquetIndex = avroIndex;    for (int i = 0; i < avroIndex; i++) {        if (avroSchema.getTypes().get(i).getType().equals(Schema.Type.NULL)) {            parquetIndex--;        }    }                String memberName = "member" + parquetIndex;    recordConsumer.startField(memberName, parquetIndex);    writeValue(parquetGroup.getType(parquetIndex), avroSchema.getTypes().get(avroIndex), value);    recordConsumer.endField(memberName, parquetIndex);    recordConsumer.endGroup();}
0
private void writeValue(Type type, Schema avroSchema, Object value)
{    Schema nonNullAvroSchema = AvroSchemaConverter.getNonNull(avroSchema);    LogicalType logicalType = nonNullAvroSchema.getLogicalType();    if (logicalType != null) {        Conversion<?> conversion = model.getConversionByClass(value.getClass(), logicalType);        writeValueWithoutConversion(type, nonNullAvroSchema, convert(nonNullAvroSchema, logicalType, conversion, value));    } else {        writeValueWithoutConversion(type, nonNullAvroSchema, value);    }}
0
private Object convert(Schema schema, LogicalType logicalType, Conversion<D> conversion, Object datum)
{    if (conversion == null) {        return datum;    }    Class<D> fromClass = conversion.getConvertedType();    switch(schema.getType()) {        case RECORD:            return conversion.toRecord(fromClass.cast(datum), schema, logicalType);        case ENUM:            return conversion.toEnumSymbol(fromClass.cast(datum), schema, logicalType);        case ARRAY:            return conversion.toArray(fromClass.cast(datum), schema, logicalType);        case MAP:            return conversion.toMap(fromClass.cast(datum), schema, logicalType);        case FIXED:            return conversion.toFixed(fromClass.cast(datum), schema, logicalType);        case STRING:            return conversion.toCharSequence(fromClass.cast(datum), schema, logicalType);        case BYTES:            return conversion.toBytes(fromClass.cast(datum), schema, logicalType);        case INT:            return conversion.toInt(fromClass.cast(datum), schema, logicalType);        case LONG:            return conversion.toLong(fromClass.cast(datum), schema, logicalType);        case FLOAT:            return conversion.toFloat(fromClass.cast(datum), schema, logicalType);        case DOUBLE:            return conversion.toDouble(fromClass.cast(datum), schema, logicalType);        case BOOLEAN:            return conversion.toBoolean(fromClass.cast(datum), schema, logicalType);    }    return datum;}
0
private void writeValueWithoutConversion(Type type, Schema avroSchema, Object value)
{    switch(avroSchema.getType()) {        case BOOLEAN:            recordConsumer.addBoolean((Boolean) value);            break;        case INT:            if (value instanceof Character) {                recordConsumer.addInteger((Character) value);            } else {                recordConsumer.addInteger(((Number) value).intValue());            }            break;        case LONG:            recordConsumer.addLong(((Number) value).longValue());            break;        case FLOAT:            recordConsumer.addFloat(((Number) value).floatValue());            break;        case DOUBLE:            recordConsumer.addDouble(((Number) value).doubleValue());            break;        case FIXED:            recordConsumer.addBinary(Binary.fromReusedByteArray(((GenericFixed) value).bytes()));            break;        case BYTES:            if (value instanceof byte[]) {                recordConsumer.addBinary(Binary.fromReusedByteArray((byte[]) value));            } else {                recordConsumer.addBinary(Binary.fromReusedByteBuffer((ByteBuffer) value));            }            break;        case STRING:            recordConsumer.addBinary(fromAvroString(value));            break;        case RECORD:            writeRecord(type.asGroupType(), avroSchema, value);            break;        case ENUM:            recordConsumer.addBinary(Binary.fromString(value.toString()));            break;        case ARRAY:            listWriter.writeList(type.asGroupType(), avroSchema, value);            break;        case MAP:            writeMap(type.asGroupType(), avroSchema, (Map<CharSequence, ?>) value);            break;        case UNION:            writeUnion(type.asGroupType(), avroSchema, value);            break;    }}
0
private Binary fromAvroString(Object value)
{    if (value instanceof Utf8) {        Utf8 utf8 = (Utf8) value;        return Binary.fromReusedByteArray(utf8.getBytes(), 0, utf8.getByteLength());    } else if (value instanceof CharSequence) {        return Binary.fromCharSequence((CharSequence) value);    }    return Binary.fromCharSequence(value.toString());}
0
private static GenericData getDataModel(Configuration conf)
{    Class<? extends AvroDataSupplier> suppClass = conf.getClass(AVRO_DATA_SUPPLIER, SpecificDataSupplier.class, AvroDataSupplier.class);    return ReflectionUtils.newInstance(suppClass, conf).get();}
0
public void writeList(GroupType schema, Schema avroSchema, Object value)
{        recordConsumer.startGroup();    if (value instanceof Collection) {        writeCollection(schema, avroSchema, (Collection) value);    } else {        Class<?> arrayClass = value.getClass();        Preconditions.checkArgument(arrayClass.isArray(), "Cannot write unless collection or array: " + arrayClass.getName());        writeJavaArray(schema, avroSchema, arrayClass, value);    }    recordConsumer.endGroup();}
0
public void writeJavaArray(GroupType schema, Schema avroSchema, Class<?> arrayClass, Object value)
{    Class<?> elementClass = arrayClass.getComponentType();    if (!elementClass.isPrimitive()) {        writeObjectArray(schema, avroSchema, (Object[]) value);        return;    }    switch(avroSchema.getElementType().getType()) {        case BOOLEAN:            Preconditions.checkArgument(elementClass == boolean.class, "Cannot write as boolean array: " + arrayClass.getName());            writeBooleanArray((boolean[]) value);            break;        case INT:            if (elementClass == byte.class) {                writeByteArray((byte[]) value);            } else if (elementClass == char.class) {                writeCharArray((char[]) value);            } else if (elementClass == short.class) {                writeShortArray((short[]) value);            } else if (elementClass == int.class) {                writeIntArray((int[]) value);            } else {                throw new IllegalArgumentException("Cannot write as an int array: " + arrayClass.getName());            }            break;        case LONG:            Preconditions.checkArgument(elementClass == long.class, "Cannot write as long array: " + arrayClass.getName());            writeLongArray((long[]) value);            break;        case FLOAT:            Preconditions.checkArgument(elementClass == float.class, "Cannot write as float array: " + arrayClass.getName());            writeFloatArray((float[]) value);            break;        case DOUBLE:            Preconditions.checkArgument(elementClass == double.class, "Cannot write as double array: " + arrayClass.getName());            writeDoubleArray((double[]) value);            break;        default:            throw new IllegalArgumentException("Cannot write " + avroSchema.getElementType() + " array: " + arrayClass.getName());    }}
0
protected void writeBooleanArray(boolean[] array)
{    if (array.length > 0) {        startArray();        for (boolean element : array) {            recordConsumer.addBoolean(element);        }        endArray();    }}
0
protected void writeByteArray(byte[] array)
{    if (array.length > 0) {        startArray();        for (byte element : array) {            recordConsumer.addInteger(element);        }        endArray();    }}
0
protected void writeShortArray(short[] array)
{    if (array.length > 0) {        startArray();        for (short element : array) {            recordConsumer.addInteger(element);        }        endArray();    }}
0
protected void writeCharArray(char[] array)
{    if (array.length > 0) {        startArray();        for (char element : array) {            recordConsumer.addInteger(element);        }        endArray();    }}
0
protected void writeIntArray(int[] array)
{    if (array.length > 0) {        startArray();        for (int element : array) {            recordConsumer.addInteger(element);        }        endArray();    }}
0
protected void writeLongArray(long[] array)
{    if (array.length > 0) {        startArray();        for (long element : array) {            recordConsumer.addLong(element);        }        endArray();    }}
0
protected void writeFloatArray(float[] array)
{    if (array.length > 0) {        startArray();        for (float element : array) {            recordConsumer.addFloat(element);        }        endArray();    }}
0
protected void writeDoubleArray(double[] array)
{    if (array.length > 0) {        startArray();        for (double element : array) {            recordConsumer.addDouble(element);        }        endArray();    }}
0
public void writeCollection(GroupType schema, Schema avroSchema, Collection<?> array)
{    if (array.size() > 0) {        recordConsumer.startField(OLD_LIST_REPEATED_NAME, 0);        try {            for (Object elt : array) {                writeValue(schema.getType(0), avroSchema.getElementType(), elt);            }        } catch (NullPointerException e) {                        int i = 0;            for (Object elt : array) {                if (elt == null) {                    throw new NullPointerException("Array contains a null element at " + i + "\n" + "Set parquet.avro.write-old-list-structure=false to turn " + "on support for arrays with null elements.");                }                i += 1;            }                        throw e;        }        recordConsumer.endField(OLD_LIST_REPEATED_NAME, 0);    }}
0
protected void writeObjectArray(GroupType type, Schema schema, Object[] array)
{    if (array.length > 0) {        recordConsumer.startField(OLD_LIST_REPEATED_NAME, 0);        try {            for (Object element : array) {                writeValue(type.getType(0), schema.getElementType(), element);            }        } catch (NullPointerException e) {                        for (int i = 0; i < array.length; i += 1) {                if (array[i] == null) {                    throw new NullPointerException("Array contains a null element at " + i + "\n" + "Set parquet.avro.write-old-list-structure=false to turn " + "on support for arrays with null elements.");                }            }                        throw e;        }        recordConsumer.endField(OLD_LIST_REPEATED_NAME, 0);    }}
0
protected void startArray()
{    recordConsumer.startField(OLD_LIST_REPEATED_NAME, 0);}
0
protected void endArray()
{    recordConsumer.endField(OLD_LIST_REPEATED_NAME, 0);}
0
protected void writeCollection(GroupType type, Schema schema, Collection<?> collection)
{    if (collection.size() > 0) {        recordConsumer.startField(LIST_REPEATED_NAME, 0);        GroupType repeatedType = type.getType(0).asGroupType();        Type elementType = repeatedType.getType(0);        for (Object element : collection) {                        recordConsumer.startGroup();            if (element != null) {                recordConsumer.startField(LIST_ELEMENT_NAME, 0);                writeValue(elementType, schema.getElementType(), element);                recordConsumer.endField(LIST_ELEMENT_NAME, 0);            } else if (!elementType.isRepetition(Type.Repetition.OPTIONAL)) {                throw new RuntimeException("Null list element for " + schema.getName());            }            recordConsumer.endGroup();        }        recordConsumer.endField(LIST_REPEATED_NAME, 0);    }}
0
protected void writeObjectArray(GroupType type, Schema schema, Object[] array)
{    if (array.length > 0) {        recordConsumer.startField(LIST_REPEATED_NAME, 0);        GroupType repeatedType = type.getType(0).asGroupType();        Type elementType = repeatedType.getType(0);        for (Object element : array) {                        recordConsumer.startGroup();            if (element != null) {                recordConsumer.startField(LIST_ELEMENT_NAME, 0);                writeValue(elementType, schema.getElementType(), element);                recordConsumer.endField(LIST_ELEMENT_NAME, 0);            } else if (!elementType.isRepetition(Type.Repetition.OPTIONAL)) {                throw new RuntimeException("Null list element for " + schema.getName());            }            recordConsumer.endGroup();        }        recordConsumer.endField(LIST_REPEATED_NAME, 0);    }}
0
protected void startArray()
{    recordConsumer.startField(LIST_REPEATED_NAME, 0);        recordConsumer.startGroup();    recordConsumer.startField(LIST_ELEMENT_NAME, 0);}
0
protected void endArray()
{    recordConsumer.endField(LIST_ELEMENT_NAME, 0);    recordConsumer.endGroup();    recordConsumer.endField(LIST_REPEATED_NAME, 0);}
0
public GenericData get()
{    return GenericData.get();}
0
public void add(Object value)
{    throw new RuntimeException("[BUG] ParentValueContainer#add was not overridden");}
0
public void addBoolean(boolean value)
{    add(value);}
0
public void addByte(byte value)
{    add(value);}
0
public void addChar(char value)
{    add(value);}
0
public void addShort(short value)
{    add(value);}
0
public void addInt(int value)
{    add(value);}
0
public void addLong(long value)
{    add(value);}
0
public void addFloat(float value)
{    add(value);}
0
public void addDouble(double value)
{    add(value);}
0
public void addDouble(double value)
{    wrapped.add(conversion.fromDouble(value, schema, logicalType));}
0
public void addFloat(float value)
{    wrapped.add(conversion.fromFloat(value, schema, logicalType));}
0
public void addLong(long value)
{    wrapped.add(conversion.fromLong(value, schema, logicalType));}
0
public void addInt(int value)
{    wrapped.add(conversion.fromInt(value, schema, logicalType));}
0
public void addShort(short value)
{    wrapped.add(conversion.fromInt((int) value, schema, logicalType));}
0
public void addChar(char value)
{    wrapped.add(conversion.fromInt((int) value, schema, logicalType));}
0
public void addByte(byte value)
{    wrapped.add(conversion.fromInt((int) value, schema, logicalType));}
0
public void addBoolean(boolean value)
{    wrapped.add(conversion.fromBoolean(value, schema, logicalType));}
0
 static ParentValueContainer getConversionContainer(final ParentValueContainer parent, final Conversion<?> conversion, final Schema schema)
{    if (conversion == null) {        return parent;    }    final LogicalType logicalType = schema.getLogicalType();    switch(schema.getType()) {        case STRING:            return new ParentValueContainer() {                @Override                public void add(Object value) {                    parent.add(conversion.fromCharSequence((CharSequence) value, schema, logicalType));                }            };        case BOOLEAN:            return new LogicalTypePrimitiveContainer(parent, schema, conversion) {                @Override                public void add(Object value) {                    parent.add(conversion.fromBoolean((Boolean) value, schema, logicalType));                }            };        case INT:            return new LogicalTypePrimitiveContainer(parent, schema, conversion) {                @Override                public void add(Object value) {                    parent.add(conversion.fromInt((Integer) value, schema, logicalType));                }            };        case LONG:            return new LogicalTypePrimitiveContainer(parent, schema, conversion) {                @Override                public void add(Object value) {                    parent.add(conversion.fromLong((Long) value, schema, logicalType));                }            };        case FLOAT:            return new LogicalTypePrimitiveContainer(parent, schema, conversion) {                @Override                public void add(Object value) {                    parent.add(conversion.fromFloat((Float) value, schema, logicalType));                }            };        case DOUBLE:            return new LogicalTypePrimitiveContainer(parent, schema, conversion) {                @Override                public void add(Object value) {                    parent.add(conversion.fromDouble((Double) value, schema, logicalType));                }            };        case BYTES:            return new ParentValueContainer() {                @Override                public void add(Object value) {                    parent.add(conversion.fromBytes((ByteBuffer) value, schema, logicalType));                }            };        case FIXED:            return new ParentValueContainer() {                @Override                public void add(Object value) {                    parent.add(conversion.fromFixed((GenericData.Fixed) value, schema, logicalType));                }            };        case RECORD:            return new ParentValueContainer() {                @Override                public void add(Object value) {                    parent.add(conversion.fromRecord((IndexedRecord) value, schema, logicalType));                }            };        case ARRAY:            return new ParentValueContainer() {                @Override                public void add(Object value) {                    parent.add(conversion.fromArray((Collection<?>) value, schema, logicalType));                }            };        case MAP:            return new ParentValueContainer() {                @Override                public void add(Object value) {                    parent.add(conversion.fromMap((Map<?, ?>) value, schema, logicalType));                }            };        case ENUM:            return new ParentValueContainer() {                @Override                public void add(Object value) {                    parent.add(conversion.fromEnumSymbol((GenericEnumSymbol) value, schema, logicalType));                }            };        default:            return new LogicalTypePrimitiveContainer(parent, schema, conversion);    }}
0
public void add(Object value)
{    parent.add(conversion.fromCharSequence((CharSequence) value, schema, logicalType));}
0
public void add(Object value)
{    parent.add(conversion.fromBoolean((Boolean) value, schema, logicalType));}
0
public void add(Object value)
{    parent.add(conversion.fromInt((Integer) value, schema, logicalType));}
0
public void add(Object value)
{    parent.add(conversion.fromLong((Long) value, schema, logicalType));}
0
public void add(Object value)
{    parent.add(conversion.fromFloat((Float) value, schema, logicalType));}
0
public void add(Object value)
{    parent.add(conversion.fromDouble((Double) value, schema, logicalType));}
0
public void add(Object value)
{    parent.add(conversion.fromBytes((ByteBuffer) value, schema, logicalType));}
0
public void add(Object value)
{    parent.add(conversion.fromFixed((GenericData.Fixed) value, schema, logicalType));}
0
public void add(Object value)
{    parent.add(conversion.fromRecord((IndexedRecord) value, schema, logicalType));}
0
public void add(Object value)
{    parent.add(conversion.fromArray((Collection<?>) value, schema, logicalType));}
0
public void add(Object value)
{    parent.add(conversion.fromMap((Map<?, ?>) value, schema, logicalType));}
0
public void add(Object value)
{    parent.add(conversion.fromEnumSymbol((GenericEnumSymbol) value, schema, logicalType));}
0
public GenericData get()
{    return ReflectData.get();}
0
public GenericData get()
{    return SpecificData.get();}
0
public static Schema record(String name, String namespace, Schema.Field... fields)
{    Schema record = Schema.createRecord(name, null, namespace, false);    record.setFields(Arrays.asList(fields));    return record;}
0
public static Schema record(String name, Schema.Field... fields)
{    return record(name, null, fields);}
0
public static Schema.Field field(String name, Schema schema)
{    return new Schema.Field(name, schema, null, null);}
0
public static Schema.Field optionalField(String name, Schema schema)
{    return new Schema.Field(name, optional(schema), null, JsonProperties.NULL_VALUE);}
0
public static Schema array(Schema element)
{    return Schema.createArray(element);}
0
public static Schema primitive(Schema.Type type)
{    return Schema.create(type);}
0
public static Schema optional(Schema original)
{    return Schema.createUnion(Lists.newArrayList(Schema.create(Schema.Type.NULL), original));}
0
public static GenericRecord instance(Schema schema, Object... pairs)
{    if ((pairs.length % 2) != 0) {        throw new RuntimeException("Not enough values");    }    GenericRecord record = new GenericData.Record(schema);    for (int i = 0; i < pairs.length; i += 2) {        record.put(pairs[i].toString(), pairs[i + 1]);    }    return record;}
0
public static List<D> read(GenericData model, Schema schema, File file) throws IOException
{    List<D> data = new ArrayList<D>();    Configuration conf = new Configuration(false);    AvroReadSupport.setRequestedProjection(conf, schema);    AvroReadSupport.setAvroReadSchema(conf, schema);    try (ParquetReader<D> fileReader = AvroParquetReader.<D>builder(new Path(file.toString())).withDataModel(    model).withConf(conf).build()) {        D datum;        while ((datum = fileReader.read()) != null) {            data.add(datum);        }    }    return data;}
0
public static File write(TemporaryFolder temp, GenericData model, Schema schema, D... data) throws IOException
{    File file = temp.newFile();    Assert.assertTrue(file.delete());    try (ParquetWriter<D> writer = AvroParquetWriter.<D>builder(new Path(file.toString())).withDataModel(model).withSchema(schema).build()) {        for (D datum : data) {            writer.write(datum);        }    }    return file;}
0
public static void setupNewBehaviorConfiguration()
{    OLD_BEHAVIOR_CONF.setBoolean(AvroSchemaConverter.ADD_LIST_ELEMENT_RECORDS, true);    NEW_BEHAVIOR_CONF.setBoolean(AvroSchemaConverter.ADD_LIST_ELEMENT_RECORDS, false);}
0
public void testUnannotatedListOfPrimitives() throws Exception
{    Path test = writeDirect("message UnannotatedListOfPrimitives {" + "  repeated int32 list_of_ints;" + "}", rc -> {        rc.startMessage();        rc.startField("list_of_ints", 0);        rc.addInteger(34);        rc.addInteger(35);        rc.addInteger(36);        rc.endField("list_of_ints", 0);        rc.endMessage();    });    Schema expectedSchema = record("OldPrimitiveInList", field("list_of_ints", array(primitive(Schema.Type.INT))));    GenericRecord expectedRecord = instance(expectedSchema, "list_of_ints", Arrays.asList(34, 35, 36));        assertReaderContains(oldBehaviorReader(test), expectedSchema, expectedRecord);    assertReaderContains(newBehaviorReader(test), expectedSchema, expectedRecord);}
0
public void testUnannotatedListOfGroups() throws Exception
{    Path test = writeDirect("message UnannotatedListOfGroups {" + "  repeated group list_of_points {" + "    required float x;" + "    required float y;" + "  }" + "}", rc -> {        rc.startMessage();        rc.startField("list_of_points", 0);        rc.startGroup();        rc.startField("x", 0);        rc.addFloat(1.0f);        rc.endField("x", 0);        rc.startField("y", 1);        rc.addFloat(1.0f);        rc.endField("y", 1);        rc.endGroup();        rc.startGroup();        rc.startField("x", 0);        rc.addFloat(2.0f);        rc.endField("x", 0);        rc.startField("y", 1);        rc.addFloat(2.0f);        rc.endField("y", 1);        rc.endGroup();        rc.endField("list_of_points", 0);        rc.endMessage();    });    Schema point = record("?", field("x", primitive(Schema.Type.FLOAT)), field("y", primitive(Schema.Type.FLOAT)));    Schema expectedSchema = record("OldPrimitiveInList", field("list_of_points", array(point)));    GenericRecord expectedRecord = instance(expectedSchema, "list_of_points", Arrays.asList(instance(point, "x", 1.0f, "y", 1.0f), instance(point, "x", 2.0f, "y", 2.0f)));        assertReaderContains(oldBehaviorReader(test), expectedSchema, expectedRecord);    assertReaderContains(newBehaviorReader(test), expectedSchema, expectedRecord);}
0
public void testRepeatedPrimitiveInList() throws Exception
{    Path test = writeDirect("message RepeatedPrimitiveInList {" + "  required group list_of_ints (LIST) {" + "    repeated int32 array;" + "  }" + "}", rc -> {        rc.startMessage();        rc.startField("list_of_ints", 0);        rc.startGroup();        rc.startField("array", 0);        rc.addInteger(34);        rc.addInteger(35);        rc.addInteger(36);        rc.endField("array", 0);        rc.endGroup();        rc.endField("list_of_ints", 0);        rc.endMessage();    });    Schema expectedSchema = record("RepeatedPrimitiveInList", field("list_of_ints", array(Schema.create(Schema.Type.INT))));    GenericRecord expectedRecord = instance(expectedSchema, "list_of_ints", Arrays.asList(34, 35, 36));        assertReaderContains(oldBehaviorReader(test), expectedSchema, expectedRecord);    assertReaderContains(newBehaviorReader(test), expectedSchema, expectedRecord);}
0
public void testMultiFieldGroupInList() throws Exception
{        Path test = writeDirect("message MultiFieldGroupInList {" + "  optional group locations (LIST) {" + "    repeated group element {" + "      required double latitude;" + "      required double longitude;" + "    }" + "  }" + "}", rc -> {        rc.startMessage();        rc.startField("locations", 0);        rc.startGroup();        rc.startField("element", 0);        rc.startGroup();        rc.startField("latitude", 0);        rc.addDouble(0.0);        rc.endField("latitude", 0);        rc.startField("longitude", 1);        rc.addDouble(0.0);        rc.endField("longitude", 1);        rc.endGroup();        rc.startGroup();        rc.startField("latitude", 0);        rc.addDouble(0.0);        rc.endField("latitude", 0);        rc.startField("longitude", 1);        rc.addDouble(180.0);        rc.endField("longitude", 1);        rc.endGroup();        rc.endField("element", 0);        rc.endGroup();        rc.endField("locations", 0);        rc.endMessage();    });    Schema location = record("element", field("latitude", primitive(Schema.Type.DOUBLE)), field("longitude", primitive(Schema.Type.DOUBLE)));    Schema expectedSchema = record("MultiFieldGroupInList", optionalField("locations", array(location)));    GenericRecord expectedRecord = instance(expectedSchema, "locations", Arrays.asList(instance(location, "latitude", 0.0, "longitude", 0.0), instance(location, "latitude", 0.0, "longitude", 180.0)));        assertReaderContains(oldBehaviorReader(test), expectedSchema, expectedRecord);    assertReaderContains(newBehaviorReader(test), expectedSchema, expectedRecord);}
0
public void testSingleFieldGroupInList() throws Exception
{        Path test = writeDirect("message SingleFieldGroupInList {" + "  optional group single_element_groups (LIST) {" + "    repeated group single_element_group {" + "      required int64 count;" + "    }" + "  }" + "}", rc -> {        rc.startMessage();        rc.startField("single_element_groups", 0);        rc.startGroup();                rc.startField("single_element_group", 0);        rc.startGroup();        rc.startField("count", 0);        rc.addLong(1234L);        rc.endField("count", 0);        rc.endGroup();        rc.startGroup();        rc.startField("count", 0);        rc.addLong(2345L);        rc.endField("count", 0);        rc.endGroup();                rc.endField("single_element_group", 0);        rc.endGroup();        rc.endField("single_element_groups", 0);        rc.endMessage();    });                Schema singleElementGroupSchema = record("single_element_group", field("count", primitive(Schema.Type.LONG)));    Schema oldSchema = record("SingleFieldGroupInList", optionalField("single_element_groups", array(singleElementGroupSchema)));    GenericRecord oldRecord = instance(oldSchema, "single_element_groups", Arrays.asList(instance(singleElementGroupSchema, "count", 1234L), instance(singleElementGroupSchema, "count", 2345L)));    assertReaderContains(oldBehaviorReader(test), oldSchema, oldRecord);        Schema newSchema = record("SingleFieldGroupInList", optionalField("single_element_groups", array(primitive(Schema.Type.LONG))));    GenericRecord newRecord = instance(newSchema, "single_element_groups", Arrays.asList(1234L, 2345L));    assertReaderContains(newBehaviorReader(test), newSchema, newRecord);}
0
public void testSingleFieldGroupInListWithSchema() throws Exception
{            Schema singleElementRecord = record("single_element_group", field("count", primitive(Schema.Type.LONG)));    Schema expectedSchema = record("SingleFieldGroupInList", optionalField("single_element_groups", array(singleElementRecord)));    Map<String, String> metadata = new HashMap<String, String>();    metadata.put(AvroWriteSupport.AVRO_SCHEMA, expectedSchema.toString());    Path test = writeDirect("message SingleFieldGroupInList {" + "  optional group single_element_groups (LIST) {" + "    repeated group single_element_group {" + "      required int64 count;" + "    }" + "  }" + "}", rc -> {        rc.startMessage();        rc.startField("single_element_groups", 0);        rc.startGroup();                rc.startField("single_element_group", 0);        rc.startGroup();        rc.startField("count", 0);        rc.addLong(1234L);        rc.endField("count", 0);        rc.endGroup();        rc.startGroup();        rc.startField("count", 0);        rc.addLong(2345L);        rc.endField("count", 0);        rc.endGroup();                rc.endField("single_element_group", 0);        rc.endGroup();        rc.endField("single_element_groups", 0);        rc.endMessage();    }, metadata);    GenericRecord expectedRecord = instance(expectedSchema, "single_element_groups", Arrays.asList(instance(singleElementRecord, "count", 1234L), instance(singleElementRecord, "count", 2345L)));        assertReaderContains(oldBehaviorReader(test), expectedSchema, expectedRecord);    assertReaderContains(newBehaviorReader(test), expectedSchema, expectedRecord);}
0
public void testNewOptionalGroupInList() throws Exception
{    Path test = writeDirect("message NewOptionalGroupInList {" + "  optional group locations (LIST) {" + "    repeated group list {" + "      optional group element {" + "        required double latitude;" + "        required double longitude;" + "      }" + "    }" + "  }" + "}", rc -> {        rc.startMessage();        rc.startField("locations", 0);        rc.startGroup();                rc.startField("list", 0);                        rc.startGroup();        rc.startField("element", 0);        rc.startGroup();        rc.startField("latitude", 0);        rc.addDouble(0.0);        rc.endField("latitude", 0);        rc.startField("longitude", 1);        rc.addDouble(0.0);        rc.endField("longitude", 1);        rc.endGroup();        rc.endField("element", 0);                rc.endGroup();                        rc.startGroup();                rc.endGroup();                        rc.startGroup();        rc.startField("element", 0);        rc.startGroup();        rc.startField("latitude", 0);        rc.addDouble(0.0);        rc.endField("latitude", 0);        rc.startField("longitude", 1);        rc.addDouble(180.0);        rc.endField("longitude", 1);        rc.endGroup();        rc.endField("element", 0);                rc.endGroup();                rc.endField("list", 0);        rc.endGroup();        rc.endField("locations", 0);        rc.endMessage();    });    Schema location = record("element", field("latitude", primitive(Schema.Type.DOUBLE)), field("longitude", primitive(Schema.Type.DOUBLE)));        Schema elementRecord = record("list", optionalField("element", location));    Schema oldSchema = record("NewOptionalGroupInList", optionalField("locations", array(elementRecord)));    GenericRecord oldRecord = instance(oldSchema, "locations", Arrays.asList(instance(elementRecord, "element", instance(location, "latitude", 0.0, "longitude", 0.0)), instance(elementRecord), instance(elementRecord, "element", instance(location, "latitude", 0.0, "longitude", 180.0))));    assertReaderContains(oldBehaviorReader(test), oldSchema, oldRecord);        Schema newSchema = record("NewOptionalGroupInList", optionalField("locations", array(optional(location))));    GenericRecord newRecord = instance(newSchema, "locations", Arrays.asList(instance(location, "latitude", 0.0, "longitude", 0.0), null, instance(location, "latitude", 0.0, "longitude", 180.0)));    assertReaderContains(newBehaviorReader(test), newSchema, newRecord);}
0
public void testNewRequiredGroupInList() throws Exception
{    Path test = writeDirect("message NewRequiredGroupInList {" + "  optional group locations (LIST) {" + "    repeated group list {" + "      required group element {" + "        required double latitude;" + "        required double longitude;" + "      }" + "    }" + "  }" + "}", rc -> {        rc.startMessage();        rc.startField("locations", 0);        rc.startGroup();                rc.startField("list", 0);                        rc.startGroup();        rc.startField("element", 0);        rc.startGroup();        rc.startField("latitude", 0);        rc.addDouble(0.0);        rc.endField("latitude", 0);        rc.startField("longitude", 1);        rc.addDouble(180.0);        rc.endField("longitude", 1);        rc.endGroup();        rc.endField("element", 0);                rc.endGroup();                        rc.startGroup();        rc.startField("element", 0);        rc.startGroup();        rc.startField("latitude", 0);        rc.addDouble(0.0);        rc.endField("latitude", 0);        rc.startField("longitude", 1);        rc.addDouble(0.0);        rc.endField("longitude", 1);        rc.endGroup();        rc.endField("element", 0);                rc.endGroup();                rc.endField("list", 0);        rc.endGroup();        rc.endField("locations", 0);        rc.endMessage();    });    Schema location = record("element", field("latitude", primitive(Schema.Type.DOUBLE)), field("longitude", primitive(Schema.Type.DOUBLE)));        Schema elementRecord = record("list", field("element", location));    Schema oldSchema = record("NewRequiredGroupInList", optionalField("locations", array(elementRecord)));    GenericRecord oldRecord = instance(oldSchema, "locations", Arrays.asList(instance(elementRecord, "element", instance(location, "latitude", 0.0, "longitude", 180.0)), instance(elementRecord, "element", instance(location, "latitude", 0.0, "longitude", 0.0))));    assertReaderContains(oldBehaviorReader(test), oldSchema, oldRecord);        Schema newSchema = record("NewRequiredGroupInList", optionalField("locations", array(location)));    GenericRecord newRecord = instance(newSchema, "locations", Arrays.asList(instance(location, "latitude", 0.0, "longitude", 180.0), instance(location, "latitude", 0.0, "longitude", 0.0)));    assertReaderContains(newBehaviorReader(test), newSchema, newRecord);}
0
public void testAvroCompatOptionalGroupInList() throws Exception
{    Path test = writeDirect("message AvroCompatOptionalGroupInList {" + "  optional group locations (LIST) {" + "    repeated group array {" + "      optional group element {" + "        required double latitude;" + "        required double longitude;" + "      }" + "    }" + "  }" + "}", rc -> {        rc.startMessage();        rc.startField("locations", 0);        rc.startGroup();                rc.startField("array", 0);                        rc.startGroup();        rc.startField("element", 0);        rc.startGroup();        rc.startField("latitude", 0);        rc.addDouble(0.0);        rc.endField("latitude", 0);        rc.startField("longitude", 1);        rc.addDouble(180.0);        rc.endField("longitude", 1);        rc.endGroup();        rc.endField("element", 0);                rc.endGroup();                        rc.startGroup();        rc.startField("element", 0);        rc.startGroup();        rc.startField("latitude", 0);        rc.addDouble(0.0);        rc.endField("latitude", 0);        rc.startField("longitude", 1);        rc.addDouble(0.0);        rc.endField("longitude", 1);        rc.endGroup();        rc.endField("element", 0);                rc.endGroup();                rc.endField("array", 0);        rc.endGroup();        rc.endField("locations", 0);        rc.endMessage();    });    Schema location = record("element", field("latitude", primitive(Schema.Type.DOUBLE)), field("longitude", primitive(Schema.Type.DOUBLE)));        Schema elementRecord = record("array", optionalField("element", location));    Schema oldSchema = record("AvroCompatOptionalGroupInList", optionalField("locations", array(elementRecord)));    GenericRecord oldRecord = instance(oldSchema, "locations", Arrays.asList(instance(elementRecord, "element", instance(location, "latitude", 0.0, "longitude", 180.0)), instance(elementRecord, "element", instance(location, "latitude", 0.0, "longitude", 0.0))));        assertReaderContains(oldBehaviorReader(test), oldSchema, oldRecord);    assertReaderContains(newBehaviorReader(test), oldSchema, oldRecord);}
0
public void testAvroCompatOptionalGroupInListWithSchema() throws Exception
{    Path test = writeDirect("message AvroCompatOptionalGroupInListWithSchema {" + "  optional group locations (LIST) {" + "    repeated group array {" + "      optional group element {" + "        required double latitude;" + "        required double longitude;" + "      }" + "    }" + "  }" + "}", rc -> {        rc.startMessage();        rc.startField("locations", 0);        rc.startGroup();                rc.startField("array", 0);                        rc.startGroup();        rc.startField("element", 0);        rc.startGroup();        rc.startField("latitude", 0);        rc.addDouble(0.0);        rc.endField("latitude", 0);        rc.startField("longitude", 1);        rc.addDouble(180.0);        rc.endField("longitude", 1);        rc.endGroup();        rc.endField("element", 0);                rc.endGroup();                        rc.startGroup();        rc.startField("element", 0);        rc.startGroup();        rc.startField("latitude", 0);        rc.addDouble(0.0);        rc.endField("latitude", 0);        rc.startField("longitude", 1);        rc.addDouble(0.0);        rc.endField("longitude", 1);        rc.endGroup();        rc.endField("element", 0);                rc.endGroup();                rc.endField("array", 0);        rc.endGroup();        rc.endField("locations", 0);        rc.endMessage();    });    Schema location = record("element", field("latitude", primitive(Schema.Type.DOUBLE)), field("longitude", primitive(Schema.Type.DOUBLE)));    Schema newSchema = record("AvroCompatOptionalGroupInListWithSchema", optionalField("locations", array(optional(location))));    GenericRecord newRecord = instance(newSchema, "locations", Arrays.asList(instance(location, "latitude", 0.0, "longitude", 180.0), instance(location, "latitude", 0.0, "longitude", 0.0)));    Configuration oldConfWithSchema = new Configuration();    AvroReadSupport.setAvroReadSchema(oldConfWithSchema, newSchema);        assertReaderContains(new AvroParquetReader<GenericRecord>(oldConfWithSchema, test), newSchema, newRecord);    Configuration newConfWithSchema = new Configuration(NEW_BEHAVIOR_CONF);    AvroReadSupport.setAvroReadSchema(newConfWithSchema, newSchema);    assertReaderContains(new AvroParquetReader<GenericRecord>(newConfWithSchema, test), newSchema, newRecord);}
0
public void testAvroCompatListInList() throws Exception
{    Path test = writeDirect("message AvroCompatListInList {" + "  optional group listOfLists (LIST) {" + "    repeated group array (LIST) {" + "      repeated int32 array;" + "    }" + "  }" + "}", rc -> {        rc.startMessage();        rc.startField("locations", 0);        rc.startGroup();                rc.startField("array", 0);        rc.startGroup();                rc.startField("array", 0);                rc.addInteger(34);        rc.addInteger(35);        rc.addInteger(36);                rc.endField("array", 0);        rc.endGroup();                rc.startGroup();        rc.endGroup();        rc.startGroup();                rc.startField("array", 0);                rc.addInteger(32);        rc.addInteger(33);        rc.addInteger(34);                rc.endField("array", 0);        rc.endGroup();                rc.endField("array", 0);        rc.endGroup();        rc.endField("locations", 0);        rc.endMessage();    });    Schema listOfLists = array(array(primitive(Schema.Type.INT)));    Schema oldSchema = record("AvroCompatListInList", optionalField("listOfLists", listOfLists));    GenericRecord oldRecord = instance(oldSchema, "listOfLists", Arrays.asList(Arrays.asList(34, 35, 36), Arrays.asList(), Arrays.asList(32, 33, 34)));        assertReaderContains(oldBehaviorReader(test), oldSchema, oldRecord);    assertReaderContains(newBehaviorReader(test), oldSchema, oldRecord);}
0
public void testThriftCompatListInList() throws Exception
{    Path test = writeDirect("message ThriftCompatListInList {" + "  optional group listOfLists (LIST) {" + "    repeated group listOfLists_tuple (LIST) {" + "      repeated int32 listOfLists_tuple_tuple;" + "    }" + "  }" + "}", rc -> {        rc.startMessage();        rc.startField("locations", 0);        rc.startGroup();                rc.startField("listOfLists_tuple", 0);        rc.startGroup();                rc.startField("listOfLists_tuple_tuple", 0);                rc.addInteger(34);        rc.addInteger(35);        rc.addInteger(36);                rc.endField("listOfLists_tuple_tuple", 0);        rc.endGroup();                rc.startGroup();        rc.endGroup();        rc.startGroup();                rc.startField("listOfLists_tuple_tuple", 0);                rc.addInteger(32);        rc.addInteger(33);        rc.addInteger(34);                rc.endField("listOfLists_tuple_tuple", 0);        rc.endGroup();                rc.endField("listOfLists_tuple", 0);        rc.endGroup();        rc.endField("locations", 0);        rc.endMessage();    });    Schema listOfLists = array(array(primitive(Schema.Type.INT)));    Schema oldSchema = record("ThriftCompatListInList", optionalField("listOfLists", listOfLists));    GenericRecord oldRecord = instance(oldSchema, "listOfLists", Arrays.asList(Arrays.asList(34, 35, 36), Arrays.asList(), Arrays.asList(32, 33, 34)));        assertReaderContains(oldBehaviorReader(test), oldSchema, oldRecord);    assertReaderContains(newBehaviorReader(test), oldSchema, oldRecord);}
0
public void testThriftCompatRequiredGroupInList() throws Exception
{    Path test = writeDirect("message ThriftCompatRequiredGroupInList {" + "  optional group locations (LIST) {" + "    repeated group locations_tuple {" + "      optional group element {" + "        required double latitude;" + "        required double longitude;" + "      }" + "    }" + "  }" + "}", rc -> {        rc.startMessage();        rc.startField("locations", 0);        rc.startGroup();                rc.startField("locations_tuple", 0);                        rc.startGroup();        rc.startField("element", 0);        rc.startGroup();        rc.startField("latitude", 0);        rc.addDouble(0.0);        rc.endField("latitude", 0);        rc.startField("longitude", 1);        rc.addDouble(180.0);        rc.endField("longitude", 1);        rc.endGroup();        rc.endField("element", 0);                rc.endGroup();                        rc.startGroup();        rc.startField("element", 0);        rc.startGroup();        rc.startField("latitude", 0);        rc.addDouble(0.0);        rc.endField("latitude", 0);        rc.startField("longitude", 1);        rc.addDouble(0.0);        rc.endField("longitude", 1);        rc.endGroup();        rc.endField("element", 0);                rc.endGroup();                rc.endField("locations_tuple", 0);        rc.endGroup();        rc.endField("locations", 0);        rc.endMessage();    });    Schema location = record("element", field("latitude", primitive(Schema.Type.DOUBLE)), field("longitude", primitive(Schema.Type.DOUBLE)));        Schema elementRecord = record("locations_tuple", optionalField("element", location));    Schema oldSchema = record("ThriftCompatRequiredGroupInList", optionalField("locations", array(elementRecord)));    GenericRecord oldRecord = instance(oldSchema, "locations", Arrays.asList(instance(elementRecord, "element", instance(location, "latitude", 0.0, "longitude", 180.0)), instance(elementRecord, "element", instance(location, "latitude", 0.0, "longitude", 0.0))));        assertReaderContains(oldBehaviorReader(test), oldSchema, oldRecord);    assertReaderContains(newBehaviorReader(test), oldSchema, oldRecord);}
0
public void testHiveCompatOptionalGroupInList() throws Exception
{    Path test = writeDirect("message HiveCompatOptionalGroupInList {" + "  optional group locations (LIST) {" + "    repeated group bag {" + "      optional group element {" + "        required double latitude;" + "        required double longitude;" + "      }" + "    }" + "  }" + "}", rc -> {        rc.startMessage();        rc.startField("locations", 0);        rc.startGroup();                rc.startField("bag", 0);                        rc.startGroup();        rc.startField("element", 0);        rc.startGroup();        rc.startField("latitude", 0);        rc.addDouble(0.0);        rc.endField("latitude", 0);        rc.startField("longitude", 1);        rc.addDouble(180.0);        rc.endField("longitude", 1);        rc.endGroup();        rc.endField("element", 0);                rc.endGroup();                        rc.startGroup();        rc.startField("element", 0);        rc.startGroup();        rc.startField("latitude", 0);        rc.addDouble(0.0);        rc.endField("latitude", 0);        rc.startField("longitude", 1);        rc.addDouble(0.0);        rc.endField("longitude", 1);        rc.endGroup();        rc.endField("element", 0);                rc.endGroup();                rc.endField("bag", 0);        rc.endGroup();        rc.endField("locations", 0);        rc.endMessage();    });    Schema location = record("element", field("latitude", primitive(Schema.Type.DOUBLE)), field("longitude", primitive(Schema.Type.DOUBLE)));        Schema elementRecord = record("bag", optionalField("element", location));    Schema oldSchema = record("HiveCompatOptionalGroupInList", optionalField("locations", array(elementRecord)));    GenericRecord oldRecord = instance(oldSchema, "locations", Arrays.asList(instance(elementRecord, "element", instance(location, "latitude", 0.0, "longitude", 180.0)), instance(elementRecord, "element", instance(location, "latitude", 0.0, "longitude", 0.0))));        assertReaderContains(oldBehaviorReader(test), oldSchema, oldRecord);    Schema newSchema = record("HiveCompatOptionalGroupInList", optionalField("locations", array(optional(location))));    GenericRecord newRecord = instance(newSchema, "locations", Arrays.asList(instance(location, "latitude", 0.0, "longitude", 180.0), instance(location, "latitude", 0.0, "longitude", 0.0)));    assertReaderContains(newBehaviorReader(test), newSchema, newRecord);}
0
public void testListOfSingleElementStructsWithElementField() throws Exception
{    Path test = writeDirect("message ListOfSingleElementStructsWithElementField {" + "  optional group list_of_structs (LIST) {" + "    repeated group list {" + "      required group element {" + "        required float element;" + "      }" + "    }" + "  }" + "}", rc -> {        rc.startMessage();        rc.startField("list_of_structs", 0);        rc.startGroup();                rc.startField("list", 0);                        rc.startGroup();        rc.startField("element", 0);                rc.startGroup();        rc.startField("element", 0);        rc.addFloat(33.0F);        rc.endField("element", 0);        rc.endGroup();        rc.endField("element", 0);                rc.endGroup();                        rc.startGroup();        rc.startField("element", 0);                rc.startGroup();        rc.startField("element", 0);        rc.addFloat(34.0F);        rc.endField("element", 0);        rc.endGroup();        rc.endField("element", 0);                rc.endGroup();                rc.endField("list", 0);        rc.endGroup();        rc.endField("list_of_structs", 0);        rc.endMessage();    });    Schema structWithElementField = record("element", field("element", primitive(Schema.Type.FLOAT)));        Schema elementRecord = record("list", field("element", structWithElementField));    Schema oldSchema = record("ListOfSingleElementStructsWithElementField", optionalField("list_of_structs", array(elementRecord)));    GenericRecord oldRecord = instance(oldSchema, "list_of_structs", Arrays.asList(instance(elementRecord, "element", instance(structWithElementField, "element", 33.0F)), instance(elementRecord, "element", instance(structWithElementField, "element", 34.0F))));        final MessageType fileSchema;    try (ParquetFileReader reader = ParquetFileReader.open(new Configuration(), test)) {        fileSchema = reader.getFileMetaData().getSchema();        Assert.assertEquals("Converted schema should assume 2-layer structure", oldSchema, new AvroSchemaConverter(OLD_BEHAVIOR_CONF).convert(fileSchema));    }        assertReaderContains(oldBehaviorReader(test), oldSchema, oldRecord);    Schema newSchema = record("ListOfSingleElementStructsWithElementField", optionalField("list_of_structs", array(structWithElementField)));    GenericRecord newRecord = instance(newSchema, "list_of_structs", Arrays.asList(instance(structWithElementField, "element", 33.0F), instance(structWithElementField, "element", 34.0F)));        Assert.assertEquals("Converted schema should assume 3-layer structure", newSchema, new AvroSchemaConverter(NEW_BEHAVIOR_CONF).convert(fileSchema));    assertReaderContains(newBehaviorReader(test), newSchema, newRecord);        Schema structWithDoubleElementField = record("element", field("element", primitive(Schema.Type.DOUBLE)));    Schema doubleElementRecord = record("list", field("element", structWithDoubleElementField));    Schema oldDoubleSchema = record("ListOfSingleElementStructsWithElementField", optionalField("list_of_structs", array(doubleElementRecord)));    GenericRecord oldDoubleRecord = instance(oldDoubleSchema, "list_of_structs", Arrays.asList(instance(doubleElementRecord, "element", instance(structWithDoubleElementField, "element", 33.0)), instance(doubleElementRecord, "element", instance(structWithDoubleElementField, "element", 34.0))));    assertReaderContains(oldBehaviorReader(test, oldDoubleSchema), oldDoubleSchema, oldDoubleRecord);    Schema newDoubleSchema = record("ListOfSingleElementStructsWithElementField", optionalField("list_of_structs", array(structWithDoubleElementField)));    GenericRecord newDoubleRecord = instance(newDoubleSchema, "list_of_structs", Arrays.asList(instance(structWithDoubleElementField, "element", 33.0), instance(structWithDoubleElementField, "element", 34.0)));    assertReaderContains(newBehaviorReader(test, newDoubleSchema), newDoubleSchema, newDoubleRecord);}
0
public AvroParquetReader<T> oldBehaviorReader(Path path) throws IOException
{    return new AvroParquetReader<T>(OLD_BEHAVIOR_CONF, path);}
0
public AvroParquetReader<T> oldBehaviorReader(Path path, Schema expectedSchema) throws IOException
{    Configuration conf = new Configuration(OLD_BEHAVIOR_CONF);    AvroReadSupport.setAvroReadSchema(conf, expectedSchema);    return new AvroParquetReader<T>(conf, path);}
0
public AvroParquetReader<T> newBehaviorReader(Path path) throws IOException
{    return new AvroParquetReader<T>(NEW_BEHAVIOR_CONF, path);}
0
public AvroParquetReader<T> newBehaviorReader(Path path, Schema expectedSchema) throws IOException
{    Configuration conf = new Configuration(NEW_BEHAVIOR_CONF);    AvroReadSupport.setAvroReadSchema(conf, expectedSchema);    return new AvroParquetReader<T>(conf, path);}
0
public void assertReaderContains(AvroParquetReader<T> reader, Schema expectedSchema, T... expectedRecords) throws IOException
{    for (T expectedRecord : expectedRecords) {        T actualRecord = reader.read();        Assert.assertEquals("Should match expected schema", expectedSchema, actualRecord.getSchema());        Assert.assertEquals("Should match the expected record", expectedRecord, actualRecord);    }    Assert.assertNull("Should only contain " + expectedRecords.length + " record" + (expectedRecords.length == 1 ? "" : "s"), reader.read());}
0
public GenericData get()
{    return GenericData.get();}
0
public void testSetSupplierMethod()
{    Configuration conf = new Configuration(false);    AvroReadSupport.setAvroDataSupplier(conf, GenericDataSupplier.class);    Assert.assertEquals("Should contain the class name", "org.apache.parquet.avro.TestAvroDataSupplier$GenericDataSupplier", conf.get(AvroReadSupport.AVRO_DATA_SUPPLIER));}
0
public static void setupConf()
{    NEW_BEHAVIOR.setBoolean("parquet.avro.add-list-element-records", false);    NEW_BEHAVIOR.setBoolean("parquet.avro.write-old-list-structure", false);}
0
private void testAvroToParquetConversion(Schema avroSchema, String schemaString) throws Exception
{    testAvroToParquetConversion(new Configuration(false), avroSchema, schemaString);}
0
private void testAvroToParquetConversion(Configuration conf, Schema avroSchema, String schemaString) throws Exception
{    AvroSchemaConverter avroSchemaConverter = new AvroSchemaConverter(conf);    MessageType schema = avroSchemaConverter.convert(avroSchema);    MessageType expectedMT = MessageTypeParser.parseMessageType(schemaString);    assertEquals("converting " + schema + " to " + schemaString, expectedMT.toString(), schema.toString());}
0
private void testParquetToAvroConversion(Schema avroSchema, String schemaString) throws Exception
{    testParquetToAvroConversion(new Configuration(false), avroSchema, schemaString);}
0
private void testParquetToAvroConversion(Configuration conf, Schema avroSchema, String schemaString) throws Exception
{    AvroSchemaConverter avroSchemaConverter = new AvroSchemaConverter(conf);    Schema schema = avroSchemaConverter.convert(MessageTypeParser.parseMessageType(schemaString));    assertEquals("converting " + schemaString + " to " + avroSchema, avroSchema.toString(), schema.toString());}
0
private void testRoundTripConversion(Schema avroSchema, String schemaString) throws Exception
{    testRoundTripConversion(new Configuration(), avroSchema, schemaString);}
0
private void testRoundTripConversion(Configuration conf, Schema avroSchema, String schemaString) throws Exception
{    AvroSchemaConverter avroSchemaConverter = new AvroSchemaConverter(conf);    MessageType schema = avroSchemaConverter.convert(avroSchema);    MessageType expectedMT = MessageTypeParser.parseMessageType(schemaString);    assertEquals("converting " + schema + " to " + schemaString, expectedMT.toString(), schema.toString());    Schema convertedAvroSchema = avroSchemaConverter.convert(expectedMT);    assertEquals("converting " + expectedMT + " to " + avroSchema.toString(true), avroSchema.toString(), convertedAvroSchema.toString());}
0
public void testTopLevelMustBeARecord()
{    new AvroSchemaConverter().convert(Schema.create(INT));}
0
public void testAllTypes() throws Exception
{    Schema schema = new Schema.Parser().parse(Resources.getResource("all.avsc").openStream());    testAvroToParquetConversion(NEW_BEHAVIOR, schema, "message org.apache.parquet.avro.myrecord {\n" +     "  required boolean myboolean;\n" + "  required int32 myint;\n" + "  required int64 mylong;\n" + "  required float myfloat;\n" + "  required double mydouble;\n" + "  required binary mybytes;\n" + "  required binary mystring (UTF8);\n" + "  required group mynestedrecord {\n" + "    required int32 mynestedint;\n" + "  }\n" + "  required binary myenum (ENUM);\n" + "  required group myarray (LIST) {\n" + "    repeated group list {\n" + "      required int32 element;\n" + "    }\n" + "  }\n" + "  required group myemptyarray (LIST) {\n" + "    repeated group list {\n" + "      required int32 element;\n" + "    }\n" + "  }\n" + "  optional group myoptionalarray (LIST) {\n" + "    repeated group list {\n" + "      required int32 element;\n" + "    }\n" + "  }\n" + "  required group myarrayofoptional (LIST) {\n" + "    repeated group list {\n" + "      optional int32 element;\n" + "    }\n" + "  }\n" + "  required group mymap (MAP) {\n" + "    repeated group map (MAP_KEY_VALUE) {\n" + "      required binary key (UTF8);\n" + "      required int32 value;\n" + "    }\n" + "  }\n" + "  required group myemptymap (MAP) {\n" + "    repeated group map (MAP_KEY_VALUE) {\n" + "      required binary key (UTF8);\n" + "      required int32 value;\n" + "    }\n" + "  }\n" + "  required fixed_len_byte_array(1) myfixed;\n" + "}\n");}
0
public void testAllTypesOldListBehavior() throws Exception
{    Schema schema = new Schema.Parser().parse(Resources.getResource("all.avsc").openStream());    testAvroToParquetConversion(schema, "message org.apache.parquet.avro.myrecord {\n" +     "  required boolean myboolean;\n" + "  required int32 myint;\n" + "  required int64 mylong;\n" + "  required float myfloat;\n" + "  required double mydouble;\n" + "  required binary mybytes;\n" + "  required binary mystring (UTF8);\n" + "  required group mynestedrecord {\n" + "    required int32 mynestedint;\n" + "  }\n" + "  required binary myenum (ENUM);\n" + "  required group myarray (LIST) {\n" + "    repeated int32 array;\n" + "  }\n" + "  required group myemptyarray (LIST) {\n" + "    repeated int32 array;\n" + "  }\n" + "  optional group myoptionalarray (LIST) {\n" + "    repeated int32 array;\n" + "  }\n" + "  required group myarrayofoptional (LIST) {\n" + "    repeated int32 array;\n" + "  }\n" + "  required group mymap (MAP) {\n" + "    repeated group map (MAP_KEY_VALUE) {\n" + "      required binary key (UTF8);\n" + "      required int32 value;\n" + "    }\n" + "  }\n" + "  required group myemptymap (MAP) {\n" + "    repeated group map (MAP_KEY_VALUE) {\n" + "      required binary key (UTF8);\n" + "      required int32 value;\n" + "    }\n" + "  }\n" + "  required fixed_len_byte_array(1) myfixed;\n" + "}\n");}
0
public void testAllTypesParquetToAvro() throws Exception
{    Schema schema = new Schema.Parser().parse(Resources.getResource("allFromParquetNewBehavior.avsc").openStream());        testParquetToAvroConversion(NEW_BEHAVIOR, schema, ALL_PARQUET_SCHEMA);}
0
public void testAllTypesParquetToAvroOldBehavior() throws Exception
{    Schema schema = new Schema.Parser().parse(Resources.getResource("allFromParquetOldBehavior.avsc").openStream());        testParquetToAvroConversion(schema, ALL_PARQUET_SCHEMA);}
0
public void testParquetMapWithNonStringKeyFails() throws Exception
{    MessageType parquetSchema = MessageTypeParser.parseMessageType("message myrecord {\n" + "  required group mymap (MAP) {\n" + "    repeated group map (MAP_KEY_VALUE) {\n" + "      required int32 key;\n" + "      required int32 value;\n" + "    }\n" + "  }\n" + "}\n");    new AvroSchemaConverter().convert(parquetSchema);}
0
public void testOptionalFields() throws Exception
{    Schema schema = Schema.createRecord("record1", null, null, false);    Schema optionalInt = optional(Schema.create(INT));    schema.setFields(Collections.singletonList(new Schema.Field("myint", optionalInt, null, JsonProperties.NULL_VALUE)));    testRoundTripConversion(schema, "message record1 {\n" + "  optional int32 myint;\n" + "}\n");}
0
public void testOptionalMapValue() throws Exception
{    Schema schema = Schema.createRecord("record1", null, null, false);    Schema optionalIntMap = Schema.createMap(optional(Schema.create(INT)));    schema.setFields(Arrays.asList(new Schema.Field("myintmap", optionalIntMap, null, null)));    testRoundTripConversion(schema, "message record1 {\n" + "  required group myintmap (MAP) {\n" + "    repeated group map (MAP_KEY_VALUE) {\n" + "      required binary key (UTF8);\n" + "      optional int32 value;\n" + "    }\n" + "  }\n" + "}\n");}
0
public void testOptionalArrayElement() throws Exception
{    Schema schema = Schema.createRecord("record1", null, null, false);    Schema optionalIntArray = Schema.createArray(optional(Schema.create(INT)));    schema.setFields(Arrays.asList(new Schema.Field("myintarray", optionalIntArray, null, null)));    testRoundTripConversion(NEW_BEHAVIOR, schema, "message record1 {\n" + "  required group myintarray (LIST) {\n" + "    repeated group list {\n" + "      optional int32 element;\n" + "    }\n" + "  }\n" + "}\n");}
0
public void testUnionOfTwoTypes() throws Exception
{    Schema schema = Schema.createRecord("record2", null, null, false);    Schema multipleTypes = Schema.createUnion(Arrays.asList(Schema.create(Schema.Type.NULL), Schema.create(INT), Schema.create(Schema.Type.FLOAT)));    schema.setFields(Arrays.asList(new Schema.Field("myunion", multipleTypes, null, JsonProperties.NULL_VALUE)));            testAvroToParquetConversion(schema, "message record2 {\n" + "  optional group myunion {\n" + "    optional int32 member0;\n" + "    optional float member1;\n" + "  }\n" + "}\n");}
0
public void testArrayOfOptionalRecords() throws Exception
{    Schema innerRecord = Schema.createRecord("element", null, null, false);    Schema optionalString = optional(Schema.create(Schema.Type.STRING));    innerRecord.setFields(Lists.newArrayList(new Schema.Field("s1", optionalString, null, JsonProperties.NULL_VALUE), new Schema.Field("s2", optionalString, null, JsonProperties.NULL_VALUE)));    Schema schema = Schema.createRecord("HasArray", null, null, false);    schema.setFields(Lists.newArrayList(new Schema.Field("myarray", Schema.createArray(optional(innerRecord)), null, null)));    System.err.println("Avro schema: " + schema.toString(true));    testRoundTripConversion(NEW_BEHAVIOR, schema, "message HasArray {\n" + "  required group myarray (LIST) {\n" + "    repeated group list {\n" + "      optional group element {\n" + "        optional binary s1 (UTF8);\n" + "        optional binary s2 (UTF8);\n" + "      }\n" + "    }\n" + "  }\n" + "}\n");}
0
public void testArrayOfOptionalRecordsOldBehavior() throws Exception
{    Schema innerRecord = Schema.createRecord("InnerRecord", null, null, false);    Schema optionalString = optional(Schema.create(Schema.Type.STRING));    innerRecord.setFields(Lists.newArrayList(new Schema.Field("s1", optionalString, null, JsonProperties.NULL_VALUE), new Schema.Field("s2", optionalString, null, JsonProperties.NULL_VALUE)));    Schema schema = Schema.createRecord("HasArray", null, null, false);    schema.setFields(Lists.newArrayList(new Schema.Field("myarray", Schema.createArray(optional(innerRecord)), null, null)));    System.err.println("Avro schema: " + schema.toString(true));        testAvroToParquetConversion(schema, "message HasArray {\n" + "  required group myarray (LIST) {\n" + "    repeated group array {\n" + "      optional binary s1 (UTF8);\n" + "      optional binary s2 (UTF8);\n" + "    }\n" + "  }\n" + "}\n");}
0
public void testOldAvroListOfLists() throws Exception
{    Schema listOfLists = optional(Schema.createArray(Schema.createArray(Schema.create(INT))));    Schema schema = Schema.createRecord("AvroCompatListInList", null, null, false);    schema.setFields(Lists.newArrayList(new Schema.Field("listOfLists", listOfLists, null, JsonProperties.NULL_VALUE)));    System.err.println("Avro schema: " + schema.toString(true));    testRoundTripConversion(schema, "message AvroCompatListInList {\n" + "  optional group listOfLists (LIST) {\n" + "    repeated group array (LIST) {\n" + "      repeated int32 array;\n" + "    }\n" + "  }\n" + "}");        testParquetToAvroConversion(NEW_BEHAVIOR, schema, "message AvroCompatListInList {\n" + "  optional group listOfLists (LIST) {\n" + "    repeated group array (LIST) {\n" + "      repeated int32 array;\n" + "    }\n" + "  }\n" + "}");}
0
public void testOldThriftListOfLists() throws Exception
{    Schema listOfLists = optional(Schema.createArray(Schema.createArray(Schema.create(INT))));    Schema schema = Schema.createRecord("ThriftCompatListInList", null, null, false);    schema.setFields(Lists.newArrayList(new Schema.Field("listOfLists", listOfLists, null, JsonProperties.NULL_VALUE)));    System.err.println("Avro schema: " + schema.toString(true));        testParquetToAvroConversion(schema, "message ThriftCompatListInList {\n" + "  optional group listOfLists (LIST) {\n" + "    repeated group listOfLists_tuple (LIST) {\n" + "      repeated int32 listOfLists_tuple_tuple;\n" + "    }\n" + "  }\n" + "}");        testParquetToAvroConversion(NEW_BEHAVIOR, schema, "message ThriftCompatListInList {\n" + "  optional group listOfLists (LIST) {\n" + "    repeated group listOfLists_tuple (LIST) {\n" + "      repeated int32 listOfLists_tuple_tuple;\n" + "    }\n" + "  }\n" + "}");}
0
public void testUnknownTwoLevelListOfLists() throws Exception
{                Schema listOfLists = optional(Schema.createArray(Schema.createArray(Schema.create(INT))));    Schema schema = Schema.createRecord("UnknownTwoLevelListInList", null, null, false);    schema.setFields(Lists.newArrayList(new Schema.Field("listOfLists", listOfLists, null, JsonProperties.NULL_VALUE)));    System.err.println("Avro schema: " + schema.toString(true));        testParquetToAvroConversion(schema, "message UnknownTwoLevelListInList {\n" + "  optional group listOfLists (LIST) {\n" + "    repeated group mylist (LIST) {\n" + "      repeated int32 innerlist;\n" + "    }\n" + "  }\n" + "}");        testParquetToAvroConversion(NEW_BEHAVIOR, schema, "message UnknownTwoLevelListInList {\n" + "  optional group listOfLists (LIST) {\n" + "    repeated group mylist (LIST) {\n" + "      repeated int32 innerlist;\n" + "    }\n" + "  }\n" + "}");}
0
public void testParquetMapWithoutMapKeyValueAnnotation() throws Exception
{    Schema schema = Schema.createRecord("myrecord", null, null, false);    Schema map = Schema.createMap(Schema.create(INT));    schema.setFields(Collections.singletonList(new Schema.Field("mymap", map, null, null)));    String parquetSchema = "message myrecord {\n" + "  required group mymap (MAP) {\n" + "    repeated group map {\n" + "      required binary key (UTF8);\n" + "      required int32 value;\n" + "    }\n" + "  }\n" + "}\n";    testParquetToAvroConversion(schema, parquetSchema);    testParquetToAvroConversion(NEW_BEHAVIOR, schema, parquetSchema);}
0
public void testDecimalBytesType() throws Exception
{    Schema schema = Schema.createRecord("myrecord", null, null, false);    Schema decimal = LogicalTypes.decimal(9, 2).addToSchema(Schema.create(Schema.Type.BYTES));    schema.setFields(Collections.singletonList(new Schema.Field("dec", decimal, null, null)));    testRoundTripConversion(schema, "message myrecord {\n" + "  required binary dec (DECIMAL(9,2));\n" + "}\n");}
0
public void testDecimalFixedType() throws Exception
{    Schema schema = Schema.createRecord("myrecord", null, null, false);    Schema decimal = LogicalTypes.decimal(9, 2).addToSchema(Schema.createFixed("dec", null, null, 8));    schema.setFields(Collections.singletonList(new Schema.Field("dec", decimal, null, null)));    testRoundTripConversion(schema, "message myrecord {\n" + "  required fixed_len_byte_array(8) dec (DECIMAL(9,2));\n" + "}\n");}
0
public void testDecimalIntegerType() throws Exception
{    Schema expected = Schema.createRecord("myrecord", null, null, false, Arrays.asList(new Schema.Field("dec", Schema.create(INT), null, null)));        testParquetToAvroConversion(expected, "message myrecord {\n" + "  required int32 dec (DECIMAL(9,2));\n" + "}\n");}
0
public void testDecimalLongType() throws Exception
{    Schema expected = Schema.createRecord("myrecord", null, null, false, Arrays.asList(new Schema.Field("dec", Schema.create(LONG), null, null)));        testParquetToAvroConversion(expected, "message myrecord {\n" + "  required int64 dec (DECIMAL(9,2));\n" + "}\n");}
0
public void testDateType() throws Exception
{    Schema date = LogicalTypes.date().addToSchema(Schema.create(INT));    Schema expected = Schema.createRecord("myrecord", null, null, false, Arrays.asList(new Schema.Field("date", date, null, null)));    testRoundTripConversion(expected, "message myrecord {\n" + "  required int32 date (DATE);\n" + "}\n");    for (PrimitiveTypeName primitive : new PrimitiveTypeName[] { INT64, INT96, FLOAT, DOUBLE, BOOLEAN, BINARY, FIXED_LEN_BYTE_ARRAY }) {        final PrimitiveType type;        if (primitive == FIXED_LEN_BYTE_ARRAY) {            type = new PrimitiveType(REQUIRED, primitive, 12, "test", DATE);        } else {            type = new PrimitiveType(REQUIRED, primitive, "test", DATE);        }        assertThrows("Should not allow TIME_MICROS with " + primitive, IllegalArgumentException.class, () -> new AvroSchemaConverter().convert(message(type)));    }}
0
public void testTimeMillisType() throws Exception
{    Schema date = LogicalTypes.timeMillis().addToSchema(Schema.create(INT));    Schema expected = Schema.createRecord("myrecord", null, null, false, Arrays.asList(new Schema.Field("time", date, null, null)));    testRoundTripConversion(expected, "message myrecord {\n" + "  required int32 time (TIME(MILLIS,true));\n" + "}\n");    for (PrimitiveTypeName primitive : new PrimitiveTypeName[] { INT64, INT96, FLOAT, DOUBLE, BOOLEAN, BINARY, FIXED_LEN_BYTE_ARRAY }) {        final PrimitiveType type;        if (primitive == FIXED_LEN_BYTE_ARRAY) {            type = new PrimitiveType(REQUIRED, primitive, 12, "test", TIME_MILLIS);        } else {            type = new PrimitiveType(REQUIRED, primitive, "test", TIME_MILLIS);        }        assertThrows("Should not allow TIME_MICROS with " + primitive, IllegalArgumentException.class, () -> new AvroSchemaConverter().convert(message(type)));    }}
0
public void testTimeMicrosType() throws Exception
{    Schema date = LogicalTypes.timeMicros().addToSchema(Schema.create(LONG));    Schema expected = Schema.createRecord("myrecord", null, null, false, Arrays.asList(new Schema.Field("time", date, null, null)));    testRoundTripConversion(expected, "message myrecord {\n" + "  required int64 time (TIME(MICROS,true));\n" + "}\n");    for (PrimitiveTypeName primitive : new PrimitiveTypeName[] { INT32, INT96, FLOAT, DOUBLE, BOOLEAN, BINARY, FIXED_LEN_BYTE_ARRAY }) {        final PrimitiveType type;        if (primitive == FIXED_LEN_BYTE_ARRAY) {            type = new PrimitiveType(REQUIRED, primitive, 12, "test", TIME_MICROS);        } else {            type = new PrimitiveType(REQUIRED, primitive, "test", TIME_MICROS);        }        assertThrows("Should not allow TIME_MICROS with " + primitive, IllegalArgumentException.class, () -> new AvroSchemaConverter().convert(message(type)));    }}
0
public void testTimestampMillisType() throws Exception
{    Schema date = LogicalTypes.timestampMillis().addToSchema(Schema.create(LONG));    Schema expected = Schema.createRecord("myrecord", null, null, false, Arrays.asList(new Schema.Field("timestamp", date, null, null)));    testRoundTripConversion(expected, "message myrecord {\n" + "  required int64 timestamp (TIMESTAMP(MILLIS,true));\n" + "}\n");    for (PrimitiveTypeName primitive : new PrimitiveTypeName[] { INT32, INT96, FLOAT, DOUBLE, BOOLEAN, BINARY, FIXED_LEN_BYTE_ARRAY }) {        final PrimitiveType type;        if (primitive == FIXED_LEN_BYTE_ARRAY) {            type = new PrimitiveType(REQUIRED, primitive, 12, "test", TIMESTAMP_MILLIS);        } else {            type = new PrimitiveType(REQUIRED, primitive, "test", TIMESTAMP_MILLIS);        }        assertThrows("Should not allow TIMESTAMP_MILLIS with " + primitive, IllegalArgumentException.class, () -> new AvroSchemaConverter().convert(message(type)));    }}
0
public void testTimestampMicrosType() throws Exception
{    Schema date = LogicalTypes.timestampMicros().addToSchema(Schema.create(LONG));    Schema expected = Schema.createRecord("myrecord", null, null, false, Arrays.asList(new Schema.Field("timestamp", date, null, null)));    testRoundTripConversion(expected, "message myrecord {\n" + "  required int64 timestamp (TIMESTAMP(MICROS,true));\n" + "}\n");    for (PrimitiveTypeName primitive : new PrimitiveTypeName[] { INT32, INT96, FLOAT, DOUBLE, BOOLEAN, BINARY, FIXED_LEN_BYTE_ARRAY }) {        final PrimitiveType type;        if (primitive == FIXED_LEN_BYTE_ARRAY) {            type = new PrimitiveType(REQUIRED, primitive, 12, "test", TIMESTAMP_MICROS);        } else {            type = new PrimitiveType(REQUIRED, primitive, "test", TIMESTAMP_MICROS);        }        assertThrows("Should not allow TIMESTAMP_MICROS with " + primitive, IllegalArgumentException.class, () -> new AvroSchemaConverter().convert(message(type)));    }}
0
public void testReuseNameInNestedStructure() throws Exception
{    Schema innerA1 = record("a1", "a12", field("a4", primitive(Schema.Type.FLOAT)));    Schema outerA1 = record("a1", field("a2", primitive(Schema.Type.FLOAT)), optionalField("a1", innerA1));    Schema schema = record("Message", optionalField("a1", outerA1));    String parquetSchema = "message Message {\n" + "      optional group a1 {\n" + "        required float a2;\n" + "        optional group a1 {\n" + "          required float a4;\n" + "         }\n" + "      }\n" + "}\n";    testParquetToAvroConversion(schema, parquetSchema);    testParquetToAvroConversion(NEW_BEHAVIOR, schema, parquetSchema);}
0
public void testReuseNameInNestedStructureAtSameLevel() throws Exception
{    Schema a2 = record("a2", field("a4", primitive(Schema.Type.FLOAT)));    Schema a22 = record("a2", "a22", field("a4", primitive(Schema.Type.FLOAT)), field("a5", primitive(Schema.Type.FLOAT)));    Schema a1 = record("a1", optionalField("a2", a2));    Schema a3 = record("a3", optionalField("a2", a22));    Schema schema = record("Message", optionalField("a1", a1), optionalField("a3", a3));    String parquetSchema = "message Message {\n" + "      optional group a1 {\n" + "        optional group a2 {\n" + "          required float a4;\n" + "         }\n" + "      }\n" + "      optional group a3 {\n" + "        optional group a2 {\n" + "          required float a4;\n" + "          required float a5;\n" + "         }\n" + "      }\n" + "}\n";    testParquetToAvroConversion(schema, parquetSchema);    testParquetToAvroConversion(NEW_BEHAVIOR, schema, parquetSchema);}
0
public static Schema optional(Schema original)
{    return Schema.createUnion(Lists.newArrayList(Schema.create(Schema.Type.NULL), original));}
0
public static MessageType message(PrimitiveType primitive)
{    return Types.buildMessage().addField(primitive).named("myrecord");}
0
public static void assertThrows(String message, Class<? extends Exception> expected, Runnable runnable)
{    try {        runnable.run();        Assert.fail("No exception was thrown (" + message + "), expected: " + expected.getName());    } catch (Exception actual) {        try {            Assert.assertEquals(message, expected, actual.getClass());        } catch (AssertionError e) {            e.addSuppressed(actual);            throw e;        }    }}
0
public void testCompatStringCompatibility() throws IOException
{                    Path testFile = new Path(Resources.getResource("strings-2.parquet").getFile());    Configuration conf = new Configuration();    ParquetReader<GenericRecord> reader = AvroParquetReader.builder(new AvroReadSupport<GenericRecord>(), testFile).withConf(conf).build();    GenericRecord r;    while ((r = reader.read()) != null) {        Assert.assertTrue("Should read value into a String", r.get("text") instanceof String);    }}
0
public void testStringCompatibility() throws IOException
{    Path testFile = new Path(Resources.getResource("strings-2.parquet").getFile());    Configuration conf = new Configuration();    conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, false);    ParquetReader<GenericRecord> reader = AvroParquetReader.builder(new AvroReadSupport<GenericRecord>(), testFile).withConf(conf).build();    GenericRecord r;    while ((r = reader.read()) != null) {        Assert.assertTrue("Should read value into a String", r.get("text") instanceof Utf8);    }}
0
public Schema addToSchema(Schema schema)
{    super.addToSchema(schema);    schema.addProp(REF_FIELD_NAME, refFieldName);    return schema;}
0
public String getName()
{    return REFERENCE;}
0
public String getRefFieldName()
{    return refFieldName;}
0
public void validate(Schema schema)
{    super.validate(schema);    if (schema.getField(refFieldName) == null) {        throw new IllegalArgumentException("Invalid field name for reference field: " + refFieldName);    }}
0
public Schema addToSchema(Schema schema)
{    super.addToSchema(schema);    schema.addProp(ID_FIELD_NAME, idFieldName);    return schema;}
0
public String getName()
{    return REFERENCEABLE;}
0
public String getIdFieldName()
{    return idFieldName;}
0
public void validate(Schema schema)
{    super.validate(schema);    Schema.Field idField = schema.getField(idFieldName);    if (idField == null || idField.schema().getType() != Schema.Type.LONG) {        throw new IllegalArgumentException("Invalid ID field: " + idFieldName + ": " + idField);    }}
0
public static void addReferenceTypes()
{    LogicalTypes.register(Referenceable.REFERENCEABLE, new LogicalTypes.LogicalTypeFactory() {        @Override        public LogicalType fromSchema(Schema schema) {            return new Referenceable(schema);        }    });    LogicalTypes.register(Reference.REFERENCE, new LogicalTypes.LogicalTypeFactory() {        @Override        public LogicalType fromSchema(Schema schema) {            return new Reference(schema);        }    });}
0
public LogicalType fromSchema(Schema schema)
{    return new Referenceable(schema);}
0
public LogicalType fromSchema(Schema schema)
{    return new Reference(schema);}
0
public ReferenceableTracker getTracker()
{    return tracker;}
0
public ReferenceHandler getHandler()
{    return handler;}
0
public Class<IndexedRecord> getConvertedType()
{    return (Class) Record.class;}
0
public String getLogicalTypeName()
{    return Referenceable.REFERENCEABLE;}
0
public IndexedRecord fromRecord(IndexedRecord value, Schema schema, LogicalType type)
{        long id = getId(value, schema);        references.put(id, value);        List<Callback> callbacks = callbacksById.get(id);    for (Callback callback : callbacks) {        callback.set(value);    }    return value;}
0
public IndexedRecord toRecord(IndexedRecord value, Schema schema, LogicalType type)
{        long id = getId(value, schema);            ids.put(value, id);    return value;}
0
private long getId(IndexedRecord referenceable, Schema schema)
{    Referenceable info = (Referenceable) schema.getLogicalType();    int idField = schema.getField(info.getIdFieldName()).pos();    return (Long) referenceable.get(idField);}
0
public Class<IndexedRecord> getConvertedType()
{    return (Class) Record.class;}
0
public String getLogicalTypeName()
{    return Reference.REFERENCE;}
0
public IndexedRecord fromRecord(final IndexedRecord record, Schema schema, LogicalType type)
{        final Schema.Field refField = schema.getField(((Reference) type).getRefFieldName());    Long id = (Long) record.get(refField.pos());    if (id != null) {        if (references.containsKey(id)) {            record.put(refField.pos(), references.get(id));        } else {            List<Callback> callbacks = callbacksById.get(id);            if (callbacks == null) {                callbacks = new ArrayList<Callback>();                callbacksById.put(id, callbacks);            }                        callbacks.add(new Callback() {                @Override                public void set(Object referenceable) {                    record.put(refField.pos(), referenceable);                }            });        }    }    return record;}
0
public void set(Object referenceable)
{    record.put(refField.pos(), referenceable);}
0
public IndexedRecord toRecord(IndexedRecord record, Schema schema, LogicalType type)
{        Schema.Field refField = schema.getField(((Reference) type).getRefFieldName());    IndexedRecord referenced = (IndexedRecord) record.get(refField.pos());    if (referenced == null) {        return record;    }        return new HijackingIndexedRecord(record, refField.pos(), ids.get(referenced));}
0
public void put(int i, Object v)
{    throw new RuntimeException("[BUG] This is a read-only class.");}
0
public Object get(int i)
{    if (i == index) {        return data;    }    return wrapped.get(i);}
0
public Schema getSchema()
{    return wrapped.getSchema();}
0
public void test() throws IOException
{    ReferenceManager manager = new ReferenceManager();    GenericData model = new GenericData();    model.addLogicalTypeConversion(manager.getTracker());    model.addLogicalTypeConversion(manager.getHandler());    Schema parentSchema = Schema.createRecord("Parent", null, null, false);    Schema placeholderSchema = Schema.createRecord("Placeholder", null, null, false);    List<Schema.Field> placeholderFields = new ArrayList<Schema.Field>();        placeholderFields.add(new Schema.Field("id", Schema.create(Schema.Type.LONG), null, null));    placeholderSchema.setFields(placeholderFields);    Referenceable idRef = new Referenceable("id");    Schema parentRefSchema = Schema.createUnion(Schema.create(Schema.Type.NULL), Schema.create(Schema.Type.LONG), idRef.addToSchema(placeholderSchema));    Reference parentRef = new Reference("parent");    List<Schema.Field> childFields = new ArrayList<Schema.Field>();    childFields.add(new Schema.Field("c", Schema.create(Schema.Type.STRING), null, null));    childFields.add(new Schema.Field("parent", parentRefSchema, null, null));    Schema childSchema = parentRef.addToSchema(Schema.createRecord("Child", null, null, false, childFields));    List<Schema.Field> parentFields = new ArrayList<Schema.Field>();    parentFields.add(new Schema.Field("id", Schema.create(Schema.Type.LONG), null, null));    parentFields.add(new Schema.Field("p", Schema.create(Schema.Type.STRING), null, null));    parentFields.add(new Schema.Field("child", childSchema, null, null));    parentSchema.setFields(parentFields);    Schema schema = idRef.addToSchema(parentSchema);    System.out.println("Schema: " + schema.toString(true));    Record parent = new Record(schema);    parent.put("id", 1L);    parent.put("p", "parent data!");    Record child = new Record(childSchema);    child.put("c", "child data!");    child.put("parent", parent);    parent.put("child", child);        File data = AvroTestUtil.write(temp, model, schema, parent);    List<Record> records = AvroTestUtil.read(model, schema, data);    Record actual = records.get(0);        Assert.assertEquals("Should correctly read back the parent id", 1L, actual.get("id"));    Assert.assertEquals("Should correctly read back the parent data", new Utf8("parent data!"), actual.get("p"));    Record actualChild = (Record) actual.get("child");    Assert.assertEquals("Should correctly read back the child data", new Utf8("child data!"), actualChild.get("c"));    Object childParent = actualChild.get("parent");    Assert.assertTrue("Should have a parent Record object", childParent instanceof Record);    Record childParentRecord = (Record) actualChild.get("parent");    Assert.assertEquals("Should have the right parent id", 1L, childParentRecord.get("id"));    Assert.assertEquals("Should have the right parent data", new Utf8("parent data!"), childParentRecord.get("p"));}
0
public static void addDecimalAndUUID()
{    GENERIC.addLogicalTypeConversion(new Conversions.DecimalConversion());    GENERIC.addLogicalTypeConversion(new Conversions.UUIDConversion());}
0
private List<T> getFieldValues(Collection<GenericRecord> records, String field, Class<T> expectedClass)
{    List<T> values = new ArrayList<T>();    for (GenericRecord record : records) {        values.add(expectedClass.cast(record.get(field)));    }    return values;}
0
public void testReadUUID() throws IOException
{    Schema uuidSchema = record("R", field("uuid", LogicalTypes.uuid().addToSchema(Schema.create(STRING))));    GenericRecord u1 = instance(uuidSchema, "uuid", UUID.randomUUID());    GenericRecord u2 = instance(uuidSchema, "uuid", UUID.randomUUID());    Schema stringSchema = record("R", field("uuid", Schema.create(STRING)));    GenericRecord s1 = instance(stringSchema, "uuid", u1.get("uuid").toString());    GenericRecord s2 = instance(stringSchema, "uuid", u2.get("uuid").toString());    File test = write(stringSchema, s1, s2);    Assert.assertEquals("Should convert Strings to UUIDs", Arrays.asList(u1, u2), read(GENERIC, uuidSchema, test));}
0
public void testWriteUUIDReadStringSchema() throws IOException
{    Schema uuidSchema = record("R", field("uuid", LogicalTypes.uuid().addToSchema(Schema.create(STRING))));    GenericRecord u1 = instance(uuidSchema, "uuid", UUID.randomUUID());    GenericRecord u2 = instance(uuidSchema, "uuid", UUID.randomUUID());    Schema stringUuidSchema = Schema.create(STRING);    stringUuidSchema.addProp(GenericData.STRING_PROP, "String");    Schema stringSchema = record("R", field("uuid", stringUuidSchema));    GenericRecord s1 = instance(stringSchema, "uuid", u1.get("uuid").toString());    GenericRecord s2 = instance(stringSchema, "uuid", u2.get("uuid").toString());    File test = write(GENERIC, uuidSchema, u1, u2);    Assert.assertEquals("Should read UUIDs as Strings", Arrays.asList(s1, s2), read(GENERIC, stringSchema, test));}
0
public void testWriteUUIDReadStringMissingLogicalType() throws IOException
{    Schema uuidSchema = record("R", field("uuid", LogicalTypes.uuid().addToSchema(Schema.create(STRING))));    GenericRecord u1 = instance(uuidSchema, "uuid", UUID.randomUUID());    GenericRecord u2 = instance(uuidSchema, "uuid", UUID.randomUUID());    GenericRecord s1 = instance(uuidSchema, "uuid", new Utf8(u1.get("uuid").toString()));    GenericRecord s2 = instance(uuidSchema, "uuid", new Utf8(u2.get("uuid").toString()));    File test = write(GENERIC, uuidSchema, u1, u2);    Assert.assertEquals("Should read UUIDs as Strings", Arrays.asList(s1, s2), read(GenericData.get(), uuidSchema, test));}
0
public void testWriteNullableUUID() throws IOException
{    Schema nullableUuidSchema = record("R", optionalField("uuid", LogicalTypes.uuid().addToSchema(Schema.create(STRING))));    GenericRecord u1 = instance(nullableUuidSchema, "uuid", UUID.randomUUID());    GenericRecord u2 = instance(nullableUuidSchema, "uuid", UUID.randomUUID());    Schema stringUuidSchema = Schema.create(STRING);    stringUuidSchema.addProp(GenericData.STRING_PROP, "String");    Schema nullableStringSchema = record("R", optionalField("uuid", stringUuidSchema));    GenericRecord s1 = instance(nullableStringSchema, "uuid", u1.get("uuid").toString());    GenericRecord s2 = instance(nullableStringSchema, "uuid", u2.get("uuid").toString());    File test = write(GENERIC, nullableUuidSchema, u1, u2);    Assert.assertEquals("Should read UUIDs as Strings", Arrays.asList(s1, s2), read(GENERIC, nullableStringSchema, test));}
0
public void testReadDecimalFixed() throws IOException
{    Schema fixedSchema = Schema.createFixed("aFixed", null, null, 4);    Schema fixedRecord = record("R", field("dec", fixedSchema));    Schema decimalSchema = DECIMAL_9_2.addToSchema(Schema.createFixed("aFixed", null, null, 4));    Schema decimalRecord = record("R", field("dec", decimalSchema));    GenericRecord r1 = instance(decimalRecord, "dec", D1);    GenericRecord r2 = instance(decimalRecord, "dec", D2);    List<GenericRecord> expected = Arrays.asList(r1, r2);    Conversion<BigDecimal> conversion = new Conversions.DecimalConversion();        GenericRecord r1fixed = instance(fixedRecord, "dec", conversion.toFixed(D1, fixedSchema, DECIMAL_9_2));    GenericRecord r2fixed = instance(fixedRecord, "dec", conversion.toFixed(D2, fixedSchema, DECIMAL_9_2));    File test = write(fixedRecord, r1fixed, r2fixed);    Assert.assertEquals("Should convert fixed to BigDecimals", expected, read(GENERIC, decimalRecord, test));}
0
public void testWriteDecimalFixed() throws IOException
{    Schema fixedSchema = Schema.createFixed("aFixed", null, null, 4);    Schema fixedRecord = record("R", field("dec", fixedSchema));    Schema decimalSchema = DECIMAL_9_2.addToSchema(Schema.createFixed("aFixed", null, null, 4));    Schema decimalRecord = record("R", field("dec", decimalSchema));    GenericRecord r1 = instance(decimalRecord, "dec", D1);    GenericRecord r2 = instance(decimalRecord, "dec", D2);    Conversion<BigDecimal> conversion = new Conversions.DecimalConversion();        GenericRecord r1fixed = instance(fixedRecord, "dec", conversion.toFixed(D1, fixedSchema, DECIMAL_9_2));    GenericRecord r2fixed = instance(fixedRecord, "dec", conversion.toFixed(D2, fixedSchema, DECIMAL_9_2));    List<GenericRecord> expected = Arrays.asList(r1fixed, r2fixed);    File test = write(GENERIC, decimalRecord, r1, r2);    Assert.assertEquals("Should read BigDecimals as fixed", expected, read(GENERIC, fixedRecord, test));}
0
public void testReadDecimalBytes() throws IOException
{    Schema bytesSchema = Schema.create(Schema.Type.BYTES);    Schema bytesRecord = record("R", field("dec", bytesSchema));    Schema decimalSchema = DECIMAL_9_2.addToSchema(Schema.create(Schema.Type.BYTES));    Schema decimalRecord = record("R", field("dec", decimalSchema));    GenericRecord r1 = instance(decimalRecord, "dec", D1);    GenericRecord r2 = instance(decimalRecord, "dec", D2);    List<GenericRecord> expected = Arrays.asList(r1, r2);    Conversion<BigDecimal> conversion = new Conversions.DecimalConversion();        GenericRecord r1bytes = instance(bytesRecord, "dec", conversion.toBytes(D1, bytesSchema, DECIMAL_9_2));    GenericRecord r2bytes = instance(bytesRecord, "dec", conversion.toBytes(D2, bytesSchema, DECIMAL_9_2));    File test = write(bytesRecord, r1bytes, r2bytes);    Assert.assertEquals("Should convert bytes to BigDecimals", expected, read(GENERIC, decimalRecord, test));}
0
public void testWriteDecimalBytes() throws IOException
{    Schema bytesSchema = Schema.create(Schema.Type.BYTES);    Schema bytesRecord = record("R", field("dec", bytesSchema));    Schema decimalSchema = DECIMAL_9_2.addToSchema(Schema.create(Schema.Type.BYTES));    Schema decimalRecord = record("R", field("dec", decimalSchema));    GenericRecord r1 = instance(decimalRecord, "dec", D1);    GenericRecord r2 = instance(decimalRecord, "dec", D2);    Conversion<BigDecimal> conversion = new Conversions.DecimalConversion();        GenericRecord r1bytes = instance(bytesRecord, "dec", conversion.toBytes(D1, bytesSchema, DECIMAL_9_2));    GenericRecord r2bytes = instance(bytesRecord, "dec", conversion.toBytes(D2, bytesSchema, DECIMAL_9_2));    List<GenericRecord> expected = Arrays.asList(r1bytes, r2bytes);    File test = write(GENERIC, decimalRecord, r1, r2);    Assert.assertEquals("Should read BigDecimals as bytes", expected, read(GENERIC, bytesRecord, test));}
0
private File write(Schema schema, D... data) throws IOException
{    return write(GenericData.get(), schema, data);}
0
private File write(GenericData model, Schema schema, D... data) throws IOException
{    return AvroTestUtil.write(temp, model, schema, data);}
0
public static GenericRecord nextRecord(Integer i)
{    return new GenericRecordBuilder(avroSchema).set("a", i).build();}
0
public void run(Context context) throws IOException, InterruptedException
{    for (int i = 0; i < 10; i++) {        GenericRecord a;        a = TestInputOutputFormat.nextRecord(i == 4 ? null : i);        context.write(null, a);    }}
0
protected void map(Void key, GenericRecord value, Context context) throws IOException, InterruptedException
{    context.write(null, new Text(value.toString()));}
0
public void testReadWrite() throws Exception
{    final Configuration conf = new Configuration();    final Path inputPath = new Path("src/test/java/org/apache/parquet/avro/TestInputOutputFormat.java");    final Path parquetPath = new Path("target/test/hadoop/TestInputOutputFormat/parquet");    final Path outputPath = new Path("target/test/hadoop/TestInputOutputFormat/out");    final FileSystem fileSystem = parquetPath.getFileSystem(conf);    fileSystem.delete(parquetPath, true);    fileSystem.delete(outputPath, true);    {        final Job job = new Job(conf, "write");                TextInputFormat.addInputPath(job, inputPath);        job.setInputFormatClass(TextInputFormat.class);        job.setMapperClass(TestInputOutputFormat.MyMapper.class);        job.setNumReduceTasks(0);        job.setOutputFormatClass(AvroParquetOutputFormat.class);        AvroParquetOutputFormat.setOutputPath(job, parquetPath);        AvroParquetOutputFormat.setSchema(job, avroSchema);        waitForJob(job);    }    {        final Job job = new Job(conf, "read");        job.setInputFormatClass(AvroParquetInputFormat.class);        AvroParquetInputFormat.setInputPaths(job, parquetPath);        job.setMapperClass(TestInputOutputFormat.MyMapper2.class);        job.setNumReduceTasks(0);        job.setOutputFormatClass(TextOutputFormat.class);        TextOutputFormat.setOutputPath(job, outputPath);        waitForJob(job);    }    try (final BufferedReader out = new BufferedReader(new FileReader(new File(outputPath.toString(), "part-m-00000")))) {        String lineOut;        int lineNumber = 0;        while ((lineOut = out.readLine()) != null) {            lineOut = lineOut.substring(lineOut.indexOf("\t") + 1);            GenericRecord a = nextRecord(lineNumber == 4 ? null : lineNumber);            assertEquals("line " + lineNumber, a.toString(), lineOut);            ++lineNumber;        }        assertNull("line " + lineNumber, out.readLine());    }}
0
private void waitForJob(Job job) throws Exception
{    job.submit();    while (!job.isComplete()) {                sleep(100);    }        if (!job.isSuccessful()) {        throw new RuntimeException("job failed " + job.getJobName());    }}
1
public static Collection<Object[]> data()
{    Object[][] data = new Object[][] {     { false },     { true } };    return Arrays.asList(data);}
0
public void testEmptyArray() throws Exception
{    Schema schema = new Schema.Parser().parse(Resources.getResource("array.avsc").openStream());        List<Integer> emptyArray = new ArrayList<>();    Path file = new Path(createTempFile().getPath());    try (ParquetWriter<GenericRecord> writer = AvroParquetWriter.<GenericRecord>builder(file).withSchema(schema).withConf(testConf).build()) {        GenericData.Record record = new GenericRecordBuilder(schema).set("myarray", emptyArray).build();        writer.write(record);    }    try (AvroParquetReader<GenericRecord> reader = new AvroParquetReader<>(testConf, file)) {        GenericRecord nextRecord = reader.read();        assertNotNull(nextRecord);        assertEquals(emptyArray, nextRecord.get("myarray"));    }}
0
public void testEmptyMap() throws Exception
{    Schema schema = new Schema.Parser().parse(Resources.getResource("map.avsc").openStream());    Path file = new Path(createTempFile().getPath());    ImmutableMap<String, Integer> emptyMap = new ImmutableMap.Builder<String, Integer>().build();    try (ParquetWriter<GenericRecord> writer = AvroParquetWriter.<GenericRecord>builder(file).withSchema(schema).withConf(testConf).build()) {                GenericData.Record record = new GenericRecordBuilder(schema).set("mymap", emptyMap).build();        writer.write(record);    }    try (AvroParquetReader<GenericRecord> reader = new AvroParquetReader<GenericRecord>(testConf, file)) {        GenericRecord nextRecord = reader.read();        assertNotNull(nextRecord);        assertEquals(emptyMap, nextRecord.get("mymap"));    }}
0
public void testMapWithNulls() throws Exception
{    Schema schema = new Schema.Parser().parse(Resources.getResource("map_with_nulls.avsc").openStream());    Path file = new Path(createTempFile().getPath());        Map<CharSequence, Integer> map = new HashMap<>();    map.put(str("thirty-four"), 34);    map.put(str("eleventy-one"), null);    map.put(str("one-hundred"), 100);    try (ParquetWriter<GenericRecord> writer = AvroParquetWriter.<GenericRecord>builder(file).withSchema(schema).withConf(testConf).build()) {        GenericData.Record record = new GenericRecordBuilder(schema).set("mymap", map).build();        writer.write(record);    }    try (AvroParquetReader<GenericRecord> reader = new AvroParquetReader<>(testConf, file)) {        GenericRecord nextRecord = reader.read();        assertNotNull(nextRecord);        assertEquals(map, nextRecord.get("mymap"));    }}
0
public void testMapRequiredValueWithNull() throws Exception
{    Schema schema = Schema.createRecord("record1", null, null, false);    schema.setFields(Lists.newArrayList(new Schema.Field("mymap", Schema.createMap(Schema.create(Schema.Type.INT)), null, null)));    Path file = new Path(createTempFile().getPath());    try (ParquetWriter<GenericRecord> writer = AvroParquetWriter.<GenericRecord>builder(file).withSchema(schema).withConf(testConf).build()) {                Map<String, Integer> map = new HashMap<String, Integer>();        map.put("thirty-four", 34);        map.put("eleventy-one", null);        map.put("one-hundred", 100);        GenericData.Record record = new GenericRecordBuilder(schema).set("mymap", map).build();        writer.write(record);    }}
0
public void testMapWithUtf8Key() throws Exception
{    Schema schema = new Schema.Parser().parse(Resources.getResource("map.avsc").openStream());    Path file = new Path(createTempFile().getPath());    try (ParquetWriter<GenericRecord> writer = AvroParquetWriter.<GenericRecord>builder(file).withSchema(schema).withConf(testConf).build()) {                GenericData.Record record = new GenericRecordBuilder(schema).set("mymap", ImmutableMap.of(new Utf8("a"), 1, new Utf8("b"), 2)).build();        writer.write(record);    }    try (AvroParquetReader<GenericRecord> reader = new AvroParquetReader<>(testConf, file)) {        GenericRecord nextRecord = reader.read();        assertNotNull(nextRecord);        assertEquals(ImmutableMap.of(str("a"), 1, str("b"), 2), nextRecord.get("mymap"));    }}
0
public void testDecimalValues() throws Exception
{    Schema decimalSchema = Schema.createRecord("myrecord", null, null, false);    Schema decimal = LogicalTypes.decimal(9, 2).addToSchema(Schema.create(Schema.Type.BYTES));    decimalSchema.setFields(Collections.singletonList(new Schema.Field("dec", decimal, null, null)));        GenericData decimalSupport = new GenericData();    decimalSupport.addLogicalTypeConversion(new Conversions.DecimalConversion());    File file = temp.newFile("decimal.parquet");    file.delete();    Path path = new Path(file.toString());    List<GenericRecord> expected = Lists.newArrayList();    try (ParquetWriter<GenericRecord> writer = AvroParquetWriter.<GenericRecord>builder(path).withDataModel(decimalSupport).withSchema(decimalSchema).build()) {        Random random = new Random(34L);        GenericRecordBuilder builder = new GenericRecordBuilder(decimalSchema);        for (int i = 0; i < 1000; i += 1) {            BigDecimal dec = new BigDecimal(new BigInteger(31, random), 2);            builder.set("dec", dec);            GenericRecord rec = builder.build();            expected.add(rec);            writer.write(builder.build());        }    }    List<GenericRecord> records = Lists.newArrayList();    try (ParquetReader<GenericRecord> reader = AvroParquetReader.<GenericRecord>builder(path).withDataModel(decimalSupport).disableCompatibility().build()) {        GenericRecord rec;        while ((rec = reader.read()) != null) {            records.add(rec);        }    }    Assert.assertTrue("dec field should be a BigDecimal instance", records.get(0).get("dec") instanceof BigDecimal);    Assert.assertEquals("Content should match", expected, records);}
0
public void testFixedDecimalValues() throws Exception
{    Schema decimalSchema = Schema.createRecord("myrecord", null, null, false);    Schema decimal = LogicalTypes.decimal(9, 2).addToSchema(Schema.createFixed("dec", null, null, 4));    decimalSchema.setFields(Collections.singletonList(new Schema.Field("dec", decimal, null, null)));        GenericData decimalSupport = new GenericData();    decimalSupport.addLogicalTypeConversion(new Conversions.DecimalConversion());    File file = temp.newFile("decimal.parquet");    file.delete();    Path path = new Path(file.toString());    List<GenericRecord> expected = Lists.newArrayList();    try (ParquetWriter<GenericRecord> writer = AvroParquetWriter.<GenericRecord>builder(path).withDataModel(decimalSupport).withSchema(decimalSchema).build()) {        Random random = new Random(34L);        GenericRecordBuilder builder = new GenericRecordBuilder(decimalSchema);        for (int i = 0; i < 1000; i += 1) {            BigDecimal dec = new BigDecimal(new BigInteger(31, random), 2);            builder.set("dec", dec);            GenericRecord rec = builder.build();            expected.add(rec);            writer.write(builder.build());        }    }    List<GenericRecord> records = Lists.newArrayList();    try (ParquetReader<GenericRecord> reader = AvroParquetReader.<GenericRecord>builder(path).withDataModel(decimalSupport).disableCompatibility().build()) {        GenericRecord rec;        while ((rec = reader.read()) != null) {            records.add(rec);        }    }    Assert.assertTrue("dec field should be a BigDecimal instance", records.get(0).get("dec") instanceof BigDecimal);    Assert.assertEquals("Content should match", expected, records);}
0
public void testAll() throws Exception
{    Schema schema = new Schema.Parser().parse(Resources.getResource("all.avsc").openStream());    Path file = new Path(createTempFile().getPath());    List<Integer> integerArray = Arrays.asList(1, 2, 3);    GenericData.Record nestedRecord = new GenericRecordBuilder(schema.getField("mynestedrecord").schema()).set("mynestedint", 1).build();    List<Integer> emptyArray = new ArrayList<Integer>();    Schema arrayOfOptionalIntegers = Schema.createArray(optional(Schema.create(Schema.Type.INT)));    GenericData.Array<Integer> genericIntegerArrayWithNulls = new GenericData.Array<Integer>(arrayOfOptionalIntegers, Arrays.asList(1, null, 2, null, 3));    GenericFixed genericFixed = new GenericData.Fixed(Schema.createFixed("fixed", null, null, 1), new byte[] { (byte) 65 });    ImmutableMap<String, Integer> emptyMap = new ImmutableMap.Builder<String, Integer>().build();    try (ParquetWriter<GenericRecord> writer = AvroParquetWriter.<GenericRecord>builder(file).withSchema(schema).withConf(testConf).build()) {        GenericData.Array<Integer> genericIntegerArray = new GenericData.Array<Integer>(Schema.createArray(Schema.create(Schema.Type.INT)), integerArray);        GenericData.Record record = new GenericRecordBuilder(schema).set("mynull", null).set("myboolean", true).set("myint", 1).set("mylong", 2L).set("myfloat", 3.1f).set("mydouble", 4.1).set("mybytes", ByteBuffer.wrap("hello".getBytes(StandardCharsets.UTF_8))).set("mystring", "hello").set("mynestedrecord", nestedRecord).set("myenum", "a").set("myarray", genericIntegerArray).set("myemptyarray", emptyArray).set("myoptionalarray", genericIntegerArray).set("myarrayofoptional", genericIntegerArrayWithNulls).set("mymap", ImmutableMap.of("a", 1, "b", 2)).set("myemptymap", emptyMap).set("myfixed", genericFixed).build();        writer.write(record);    }    final GenericRecord nextRecord;    try (AvroParquetReader<GenericRecord> reader = new AvroParquetReader<GenericRecord>(testConf, file)) {        nextRecord = reader.read();    }    Object expectedEnumSymbol = compat ? "a" : new GenericData.EnumSymbol(schema.getField("myenum").schema(), "a");    assertNotNull(nextRecord);    assertEquals(null, nextRecord.get("mynull"));    assertEquals(true, nextRecord.get("myboolean"));    assertEquals(1, nextRecord.get("myint"));    assertEquals(2L, nextRecord.get("mylong"));    assertEquals(3.1f, nextRecord.get("myfloat"));    assertEquals(4.1, nextRecord.get("mydouble"));    assertEquals(ByteBuffer.wrap("hello".getBytes(StandardCharsets.UTF_8)), nextRecord.get("mybytes"));    assertEquals(str("hello"), nextRecord.get("mystring"));    assertEquals(expectedEnumSymbol, nextRecord.get("myenum"));    assertEquals(nestedRecord, nextRecord.get("mynestedrecord"));    assertEquals(integerArray, nextRecord.get("myarray"));    assertEquals(emptyArray, nextRecord.get("myemptyarray"));    assertEquals(integerArray, nextRecord.get("myoptionalarray"));    assertEquals(genericIntegerArrayWithNulls, nextRecord.get("myarrayofoptional"));    assertEquals(ImmutableMap.of(str("a"), 1, str("b"), 2), nextRecord.get("mymap"));    assertEquals(emptyMap, nextRecord.get("myemptymap"));    assertEquals(genericFixed, nextRecord.get("myfixed"));}
0
public void testAllUsingDefaultAvroSchema() throws Exception
{    Path file = new Path(createTempFile().getPath());        try (ParquetWriter<Map<String, Object>> parquetWriter = new ParquetWriter<>(file, new WriteSupport<Map<String, Object>>() {        private RecordConsumer recordConsumer;        @Override        public WriteContext init(Configuration configuration) {            return new WriteContext(MessageTypeParser.parseMessageType(TestAvroSchemaConverter.ALL_PARQUET_SCHEMA), new HashMap<String, String>());        }        @Override        public void prepareForWrite(RecordConsumer recordConsumer) {            this.recordConsumer = recordConsumer;        }        @Override        public void write(Map<String, Object> record) {            recordConsumer.startMessage();            int index = 0;            recordConsumer.startField("myboolean", index);            recordConsumer.addBoolean((Boolean) record.get("myboolean"));            recordConsumer.endField("myboolean", index++);            recordConsumer.startField("myint", index);            recordConsumer.addInteger((Integer) record.get("myint"));            recordConsumer.endField("myint", index++);            recordConsumer.startField("mylong", index);            recordConsumer.addLong((Long) record.get("mylong"));            recordConsumer.endField("mylong", index++);            recordConsumer.startField("myfloat", index);            recordConsumer.addFloat((Float) record.get("myfloat"));            recordConsumer.endField("myfloat", index++);            recordConsumer.startField("mydouble", index);            recordConsumer.addDouble((Double) record.get("mydouble"));            recordConsumer.endField("mydouble", index++);            recordConsumer.startField("mybytes", index);            recordConsumer.addBinary(Binary.fromReusedByteBuffer((ByteBuffer) record.get("mybytes")));            recordConsumer.endField("mybytes", index++);            recordConsumer.startField("mystring", index);            recordConsumer.addBinary(Binary.fromString((String) record.get("mystring")));            recordConsumer.endField("mystring", index++);            recordConsumer.startField("mynestedrecord", index);            recordConsumer.startGroup();            recordConsumer.startField("mynestedint", 0);            recordConsumer.addInteger((Integer) record.get("mynestedint"));            recordConsumer.endField("mynestedint", 0);            recordConsumer.endGroup();            recordConsumer.endField("mynestedrecord", index++);            recordConsumer.startField("myenum", index);            recordConsumer.addBinary(Binary.fromString((String) record.get("myenum")));            recordConsumer.endField("myenum", index++);            recordConsumer.startField("myarray", index);            recordConsumer.startGroup();            recordConsumer.startField("array", 0);            for (int val : (int[]) record.get("myarray")) {                recordConsumer.addInteger(val);            }            recordConsumer.endField("array", 0);            recordConsumer.endGroup();            recordConsumer.endField("myarray", index++);            recordConsumer.startField("myoptionalarray", index);            recordConsumer.startGroup();            recordConsumer.startField("array", 0);            for (int val : (int[]) record.get("myoptionalarray")) {                recordConsumer.addInteger(val);            }            recordConsumer.endField("array", 0);            recordConsumer.endGroup();            recordConsumer.endField("myoptionalarray", index++);            recordConsumer.startField("myarrayofoptional", index);            recordConsumer.startGroup();            recordConsumer.startField("list", 0);            for (Integer val : (Integer[]) record.get("myarrayofoptional")) {                recordConsumer.startGroup();                if (val != null) {                    recordConsumer.startField("element", 0);                    recordConsumer.addInteger(val);                    recordConsumer.endField("element", 0);                }                recordConsumer.endGroup();            }            recordConsumer.endField("list", 0);            recordConsumer.endGroup();            recordConsumer.endField("myarrayofoptional", index++);            recordConsumer.startField("myrecordarray", index);            recordConsumer.startGroup();            recordConsumer.startField("array", 0);            recordConsumer.startGroup();            recordConsumer.startField("a", 0);            for (int val : (int[]) record.get("myrecordarraya")) {                recordConsumer.addInteger(val);            }            recordConsumer.endField("a", 0);            recordConsumer.startField("b", 1);            for (int val : (int[]) record.get("myrecordarrayb")) {                recordConsumer.addInteger(val);            }            recordConsumer.endField("b", 1);            recordConsumer.endGroup();            recordConsumer.endField("array", 0);            recordConsumer.endGroup();            recordConsumer.endField("myrecordarray", index++);            recordConsumer.startField("mymap", index);            recordConsumer.startGroup();            recordConsumer.startField("map", 0);            recordConsumer.startGroup();            Map<String, Integer> mymap = (Map<String, Integer>) record.get("mymap");            recordConsumer.startField("key", 0);            for (String key : mymap.keySet()) {                recordConsumer.addBinary(Binary.fromString(key));            }            recordConsumer.endField("key", 0);            recordConsumer.startField("value", 1);            for (int val : mymap.values()) {                recordConsumer.addInteger(val);            }            recordConsumer.endField("value", 1);            recordConsumer.endGroup();            recordConsumer.endField("map", 0);            recordConsumer.endGroup();            recordConsumer.endField("mymap", index++);            recordConsumer.startField("myfixed", index);            recordConsumer.addBinary(Binary.fromReusedByteArray((byte[]) record.get("myfixed")));            recordConsumer.endField("myfixed", index++);            recordConsumer.endMessage();        }    })) {        Map<String, Object> record = new HashMap<String, Object>();        record.put("myboolean", true);        record.put("myint", 1);        record.put("mylong", 2L);        record.put("myfloat", 3.1f);        record.put("mydouble", 4.1);        record.put("mybytes", ByteBuffer.wrap("hello".getBytes(StandardCharsets.UTF_8)));        record.put("mystring", "hello");        record.put("myenum", "a");        record.put("mynestedint", 1);        record.put("myarray", new int[] { 1, 2, 3 });        record.put("myoptionalarray", new int[] { 1, 2, 3 });        record.put("myarrayofoptional", new Integer[] { 1, null, 2, null, 3 });        record.put("myrecordarraya", new int[] { 1, 2, 3 });        record.put("myrecordarrayb", new int[] { 4, 5, 6 });        record.put("mymap", ImmutableMap.of("a", 1, "b", 2));        record.put("myfixed", new byte[] { (byte) 65 });        parquetWriter.write(record);    }    Schema nestedRecordSchema = Schema.createRecord("mynestedrecord", null, null, false);    nestedRecordSchema.setFields(Arrays.asList(new Schema.Field("mynestedint", Schema.create(Schema.Type.INT), null, null)));    GenericData.Record nestedRecord = new GenericRecordBuilder(nestedRecordSchema).set("mynestedint", 1).build();    List<Integer> integerArray = Arrays.asList(1, 2, 3);    List<Integer> ingeterArrayWithNulls = Arrays.asList(1, null, 2, null, 3);    Schema recordArraySchema = Schema.createRecord("array", null, null, false);    recordArraySchema.setFields(Arrays.asList(new Schema.Field("a", Schema.create(Schema.Type.INT), null, null), new Schema.Field("b", Schema.create(Schema.Type.INT), null, null)));    GenericRecordBuilder builder = new GenericRecordBuilder(recordArraySchema);    List<GenericData.Record> recordArray = new ArrayList<GenericData.Record>();    recordArray.add(builder.set("a", 1).set("b", 4).build());    recordArray.add(builder.set("a", 2).set("b", 5).build());    recordArray.add(builder.set("a", 3).set("b", 6).build());    GenericData.Array<GenericData.Record> genericRecordArray = new GenericData.Array<GenericData.Record>(Schema.createArray(recordArraySchema), recordArray);    GenericFixed genericFixed = new GenericData.Fixed(Schema.createFixed("fixed", null, null, 1), new byte[] { (byte) 65 });    try (AvroParquetReader<GenericRecord> reader = new AvroParquetReader<>(testConf, file)) {        GenericRecord nextRecord = reader.read();        assertNotNull(nextRecord);        assertEquals(true, nextRecord.get("myboolean"));        assertEquals(1, nextRecord.get("myint"));        assertEquals(2L, nextRecord.get("mylong"));        assertEquals(3.1f, nextRecord.get("myfloat"));        assertEquals(4.1, nextRecord.get("mydouble"));        assertEquals(ByteBuffer.wrap("hello".getBytes(StandardCharsets.UTF_8)), nextRecord.get("mybytes"));        assertEquals(str("hello"), nextRecord.get("mystring"));                assertEquals(str("a"), nextRecord.get("myenum"));        assertEquals(nestedRecord, nextRecord.get("mynestedrecord"));        assertEquals(integerArray, nextRecord.get("myarray"));        assertEquals(integerArray, nextRecord.get("myoptionalarray"));        assertEquals(ingeterArrayWithNulls, nextRecord.get("myarrayofoptional"));        assertEquals(genericRecordArray, nextRecord.get("myrecordarray"));        assertEquals(ImmutableMap.of(str("a"), 1, str("b"), 2), nextRecord.get("mymap"));        assertEquals(genericFixed, nextRecord.get("myfixed"));    }}
0
public WriteContext init(Configuration configuration)
{    return new WriteContext(MessageTypeParser.parseMessageType(TestAvroSchemaConverter.ALL_PARQUET_SCHEMA), new HashMap<String, String>());}
0
public void prepareForWrite(RecordConsumer recordConsumer)
{    this.recordConsumer = recordConsumer;}
0
public void write(Map<String, Object> record)
{    recordConsumer.startMessage();    int index = 0;    recordConsumer.startField("myboolean", index);    recordConsumer.addBoolean((Boolean) record.get("myboolean"));    recordConsumer.endField("myboolean", index++);    recordConsumer.startField("myint", index);    recordConsumer.addInteger((Integer) record.get("myint"));    recordConsumer.endField("myint", index++);    recordConsumer.startField("mylong", index);    recordConsumer.addLong((Long) record.get("mylong"));    recordConsumer.endField("mylong", index++);    recordConsumer.startField("myfloat", index);    recordConsumer.addFloat((Float) record.get("myfloat"));    recordConsumer.endField("myfloat", index++);    recordConsumer.startField("mydouble", index);    recordConsumer.addDouble((Double) record.get("mydouble"));    recordConsumer.endField("mydouble", index++);    recordConsumer.startField("mybytes", index);    recordConsumer.addBinary(Binary.fromReusedByteBuffer((ByteBuffer) record.get("mybytes")));    recordConsumer.endField("mybytes", index++);    recordConsumer.startField("mystring", index);    recordConsumer.addBinary(Binary.fromString((String) record.get("mystring")));    recordConsumer.endField("mystring", index++);    recordConsumer.startField("mynestedrecord", index);    recordConsumer.startGroup();    recordConsumer.startField("mynestedint", 0);    recordConsumer.addInteger((Integer) record.get("mynestedint"));    recordConsumer.endField("mynestedint", 0);    recordConsumer.endGroup();    recordConsumer.endField("mynestedrecord", index++);    recordConsumer.startField("myenum", index);    recordConsumer.addBinary(Binary.fromString((String) record.get("myenum")));    recordConsumer.endField("myenum", index++);    recordConsumer.startField("myarray", index);    recordConsumer.startGroup();    recordConsumer.startField("array", 0);    for (int val : (int[]) record.get("myarray")) {        recordConsumer.addInteger(val);    }    recordConsumer.endField("array", 0);    recordConsumer.endGroup();    recordConsumer.endField("myarray", index++);    recordConsumer.startField("myoptionalarray", index);    recordConsumer.startGroup();    recordConsumer.startField("array", 0);    for (int val : (int[]) record.get("myoptionalarray")) {        recordConsumer.addInteger(val);    }    recordConsumer.endField("array", 0);    recordConsumer.endGroup();    recordConsumer.endField("myoptionalarray", index++);    recordConsumer.startField("myarrayofoptional", index);    recordConsumer.startGroup();    recordConsumer.startField("list", 0);    for (Integer val : (Integer[]) record.get("myarrayofoptional")) {        recordConsumer.startGroup();        if (val != null) {            recordConsumer.startField("element", 0);            recordConsumer.addInteger(val);            recordConsumer.endField("element", 0);        }        recordConsumer.endGroup();    }    recordConsumer.endField("list", 0);    recordConsumer.endGroup();    recordConsumer.endField("myarrayofoptional", index++);    recordConsumer.startField("myrecordarray", index);    recordConsumer.startGroup();    recordConsumer.startField("array", 0);    recordConsumer.startGroup();    recordConsumer.startField("a", 0);    for (int val : (int[]) record.get("myrecordarraya")) {        recordConsumer.addInteger(val);    }    recordConsumer.endField("a", 0);    recordConsumer.startField("b", 1);    for (int val : (int[]) record.get("myrecordarrayb")) {        recordConsumer.addInteger(val);    }    recordConsumer.endField("b", 1);    recordConsumer.endGroup();    recordConsumer.endField("array", 0);    recordConsumer.endGroup();    recordConsumer.endField("myrecordarray", index++);    recordConsumer.startField("mymap", index);    recordConsumer.startGroup();    recordConsumer.startField("map", 0);    recordConsumer.startGroup();    Map<String, Integer> mymap = (Map<String, Integer>) record.get("mymap");    recordConsumer.startField("key", 0);    for (String key : mymap.keySet()) {        recordConsumer.addBinary(Binary.fromString(key));    }    recordConsumer.endField("key", 0);    recordConsumer.startField("value", 1);    for (int val : mymap.values()) {        recordConsumer.addInteger(val);    }    recordConsumer.endField("value", 1);    recordConsumer.endGroup();    recordConsumer.endField("map", 0);    recordConsumer.endGroup();    recordConsumer.endField("mymap", index++);    recordConsumer.startField("myfixed", index);    recordConsumer.addBinary(Binary.fromReusedByteArray((byte[]) record.get("myfixed")));    recordConsumer.endField("myfixed", index++);    recordConsumer.endMessage();}
0
public void testUnionWithSingleNonNullType() throws Exception
{    Schema avroSchema = Schema.createRecord("SingleStringUnionRecord", null, null, false);    avroSchema.setFields(Collections.singletonList(new Schema.Field("value", Schema.createUnion(Schema.create(Schema.Type.STRING)), null, null)));    Path file = new Path(createTempFile().getPath());        try (ParquetWriter parquetWriter = AvroParquetWriter.builder(file).withSchema(avroSchema).withConf(new Configuration()).build()) {        GenericRecord record = new GenericRecordBuilder(avroSchema).set("value", "theValue").build();        parquetWriter.write(record);    }    try (AvroParquetReader<GenericRecord> reader = new AvroParquetReader<>(testConf, file)) {        GenericRecord nextRecord = reader.read();        assertNotNull(nextRecord);        assertEquals(str("theValue"), nextRecord.get("value"));    }}
0
public void testDuplicatedValuesWithDictionary() throws Exception
{    Schema schema = SchemaBuilder.record("spark_schema").fields().optionalBytes("value").endRecord();    Path file = new Path(createTempFile().getPath());    String[] records = { "one", "two", "three", "three", "two", "one", "zero" };    try (ParquetWriter<GenericData.Record> writer = AvroParquetWriter.<GenericData.Record>builder(file).withSchema(schema).withConf(testConf).build()) {        for (String record : records) {            writer.write(new GenericRecordBuilder(schema).set("value", record.getBytes()).build());        }    }    try (ParquetReader<GenericRecord> reader = AvroParquetReader.<GenericRecord>builder(file).withConf(testConf).build()) {        GenericRecord rec;        int i = 0;        while ((rec = reader.read()) != null) {            ByteBuffer buf = (ByteBuffer) rec.get("value");            byte[] bytes = new byte[buf.remaining()];            buf.get(bytes);            assertEquals(records[i++], new String(bytes));        }    }}
0
public void testNestedLists() throws Exception
{    Schema schema = new Schema.Parser().parse(Resources.getResource("nested_array.avsc").openStream());    Path file = new Path(createTempFile().getPath());        ParquetWriter parquetWriter = AvroParquetWriter.builder(file).withSchema(schema).withConf(testConf).build();    Schema innerRecordSchema = schema.getField("l1").schema().getTypes().get(1).getElementType().getTypes().get(1);    GenericRecord record = new GenericRecordBuilder(schema).set("l1", Collections.singletonList(new GenericRecordBuilder(innerRecordSchema).set("l2", Collections.singletonList("hello")).build())).build();    parquetWriter.write(record);    parquetWriter.close();    AvroParquetReader<GenericRecord> reader = new AvroParquetReader(testConf, file);    GenericRecord nextRecord = reader.read();    assertNotNull(nextRecord);    assertNotNull(nextRecord.get("l1"));    List l1List = (List) nextRecord.get("l1");    assertNotNull(l1List.get(0));    List l2List = (List) ((GenericRecord) l1List.get(0)).get("l2");    assertEquals(str("hello"), l2List.get(0));}
0
private File createTempFile() throws IOException
{    File tmp = File.createTempFile(getClass().getSimpleName(), ".tmp");    tmp.deleteOnExit();    tmp.delete();    return tmp;}
0
public CharSequence str(String value)
{    return compat ? value : new Utf8(value);}
0
public static Collection<Object[]> data()
{    Object[][] data = new Object[][] {     { false },     { true } };    return Arrays.asList(data);}
0
public void testEmptyArray() throws Exception
{    Schema schema = new Schema.Parser().parse(Resources.getResource("array.avsc").openStream());    File tmp = File.createTempFile(getClass().getSimpleName(), ".tmp");    tmp.deleteOnExit();    tmp.delete();    Path file = new Path(tmp.getPath());    List<Integer> emptyArray = new ArrayList<Integer>();    try (AvroParquetWriter<GenericRecord> writer = new AvroParquetWriter<GenericRecord>(file, schema)) {                GenericData.Record record = new GenericRecordBuilder(schema).set("myarray", emptyArray).build();        writer.write(record);    }    try (AvroParquetReader<GenericRecord> reader = new AvroParquetReader<>(testConf, file)) {        GenericRecord nextRecord = reader.read();        assertNotNull(nextRecord);        assertEquals(emptyArray, nextRecord.get("myarray"));    }}
0
public void testEmptyMap() throws Exception
{    Schema schema = new Schema.Parser().parse(Resources.getResource("map.avsc").openStream());    File tmp = File.createTempFile(getClass().getSimpleName(), ".tmp");    tmp.deleteOnExit();    tmp.delete();    Path file = new Path(tmp.getPath());    ImmutableMap emptyMap = new ImmutableMap.Builder<String, Integer>().build();    try (AvroParquetWriter<GenericRecord> writer = new AvroParquetWriter<GenericRecord>(file, schema)) {                GenericData.Record record = new GenericRecordBuilder(schema).set("mymap", emptyMap).build();        writer.write(record);    }    try (AvroParquetReader<GenericRecord> reader = new AvroParquetReader<>(testConf, file)) {        GenericRecord nextRecord = reader.read();        assertNotNull(nextRecord);        assertEquals(emptyMap, nextRecord.get("mymap"));    }}
0
public void testMapWithNulls() throws Exception
{    Schema schema = new Schema.Parser().parse(Resources.getResource("map_with_nulls.avsc").openStream());    File tmp = File.createTempFile(getClass().getSimpleName(), ".tmp");    tmp.deleteOnExit();    tmp.delete();    Path file = new Path(tmp.getPath());    Map<CharSequence, Integer> map = new HashMap<>();    try (AvroParquetWriter<GenericRecord> writer = new AvroParquetWriter<GenericRecord>(file, schema)) {                map.put(str("thirty-four"), 34);        map.put(str("eleventy-one"), null);        map.put(str("one-hundred"), 100);        GenericData.Record record = new GenericRecordBuilder(schema).set("mymap", map).build();        writer.write(record);    }    try (AvroParquetReader<GenericRecord> reader = new AvroParquetReader<>(testConf, file)) {        GenericRecord nextRecord = reader.read();        assertNotNull(nextRecord);        assertEquals(map, nextRecord.get("mymap"));    }}
0
public void testMapRequiredValueWithNull() throws Exception
{    Schema schema = Schema.createRecord("record1", null, null, false);    schema.setFields(Lists.newArrayList(new Schema.Field("mymap", Schema.createMap(Schema.create(Schema.Type.INT)), null, null)));    File tmp = File.createTempFile(getClass().getSimpleName(), ".tmp");    tmp.deleteOnExit();    tmp.delete();    Path file = new Path(tmp.getPath());    try (AvroParquetWriter<GenericRecord> writer = new AvroParquetWriter<GenericRecord>(file, schema)) {                Map<String, Integer> map = new HashMap<String, Integer>();        map.put("thirty-four", 34);        map.put("eleventy-one", null);        map.put("one-hundred", 100);        GenericData.Record record = new GenericRecordBuilder(schema).set("mymap", map).build();        writer.write(record);    }}
0
public void testMapWithUtf8Key() throws Exception
{    Schema schema = new Schema.Parser().parse(Resources.getResource("map.avsc").openStream());    File tmp = File.createTempFile(getClass().getSimpleName(), ".tmp");    tmp.deleteOnExit();    tmp.delete();    Path file = new Path(tmp.getPath());    try (AvroParquetWriter<GenericRecord> writer = new AvroParquetWriter<GenericRecord>(file, schema)) {                GenericData.Record record = new GenericRecordBuilder(schema).set("mymap", ImmutableMap.of(new Utf8("a"), 1, new Utf8("b"), 2)).build();        writer.write(record);    }    try (AvroParquetReader<GenericRecord> reader = new AvroParquetReader<>(testConf, file)) {        GenericRecord nextRecord = reader.read();        assertNotNull(nextRecord);        assertEquals(ImmutableMap.of(str("a"), 1, str("b"), 2), nextRecord.get("mymap"));    }}
0
public void testAll() throws Exception
{    Schema schema = new Schema.Parser().parse(Resources.getResource("all.avsc").openStream());    File tmp = File.createTempFile(getClass().getSimpleName(), ".tmp");    tmp.deleteOnExit();    tmp.delete();    Path file = new Path(tmp.getPath());    GenericData.Record nestedRecord = new GenericRecordBuilder(schema.getField("mynestedrecord").schema()).set("mynestedint", 1).build();    List<Integer> integerArray = Arrays.asList(1, 2, 3);    GenericData.Array<Integer> genericIntegerArray = new GenericData.Array<Integer>(Schema.createArray(Schema.create(Schema.Type.INT)), integerArray);    GenericFixed genericFixed = new GenericData.Fixed(Schema.createFixed("fixed", null, null, 1), new byte[] { (byte) 65 });    List<Integer> emptyArray = new ArrayList<Integer>();    ImmutableMap emptyMap = new ImmutableMap.Builder<String, Integer>().build();    try (AvroParquetWriter<GenericRecord> writer = new AvroParquetWriter<>(file, schema)) {        GenericData.Record record = new GenericRecordBuilder(schema).set("mynull", null).set("myboolean", true).set("myint", 1).set("mylong", 2L).set("myfloat", 3.1f).set("mydouble", 4.1).set("mybytes", ByteBuffer.wrap("hello".getBytes(StandardCharsets.UTF_8))).set("mystring", "hello").set("mynestedrecord", nestedRecord).set("myenum", "a").set("myarray", genericIntegerArray).set("myemptyarray", emptyArray).set("myoptionalarray", genericIntegerArray).set("myarrayofoptional", genericIntegerArray).set("mymap", ImmutableMap.of("a", 1, "b", 2)).set("myemptymap", emptyMap).set("myfixed", genericFixed).build();        writer.write(record);    }    try (AvroParquetReader<GenericRecord> reader = new AvroParquetReader<>(testConf, file)) {        GenericRecord nextRecord = reader.read();        Object expectedEnumSymbol = compat ? "a" : new GenericData.EnumSymbol(schema.getField("myenum").schema(), "a");        assertNotNull(nextRecord);        assertEquals(null, nextRecord.get("mynull"));        assertEquals(true, nextRecord.get("myboolean"));        assertEquals(1, nextRecord.get("myint"));        assertEquals(2L, nextRecord.get("mylong"));        assertEquals(3.1f, nextRecord.get("myfloat"));        assertEquals(4.1, nextRecord.get("mydouble"));        assertEquals(ByteBuffer.wrap("hello".getBytes(StandardCharsets.UTF_8)), nextRecord.get("mybytes"));        assertEquals(str("hello"), nextRecord.get("mystring"));        assertEquals(expectedEnumSymbol, nextRecord.get("myenum"));        assertEquals(nestedRecord, nextRecord.get("mynestedrecord"));        assertEquals(integerArray, nextRecord.get("myarray"));        assertEquals(emptyArray, nextRecord.get("myemptyarray"));        assertEquals(integerArray, nextRecord.get("myoptionalarray"));        assertEquals(integerArray, nextRecord.get("myarrayofoptional"));        assertEquals(ImmutableMap.of(str("a"), 1, str("b"), 2), nextRecord.get("mymap"));        assertEquals(emptyMap, nextRecord.get("myemptymap"));        assertEquals(genericFixed, nextRecord.get("myfixed"));    }}
0
public void testArrayWithNullValues() throws Exception
{    Schema schema = new Schema.Parser().parse(Resources.getResource("all.avsc").openStream());    File tmp = File.createTempFile(getClass().getSimpleName(), ".tmp");    tmp.deleteOnExit();    tmp.delete();    Path file = new Path(tmp.getPath());    GenericData.Record nestedRecord = new GenericRecordBuilder(schema.getField("mynestedrecord").schema()).set("mynestedint", 1).build();    List<Integer> integerArray = Arrays.asList(1, 2, 3);    GenericData.Array<Integer> genericIntegerArray = new GenericData.Array<Integer>(Schema.createArray(Schema.create(Schema.Type.INT)), integerArray);    GenericFixed genericFixed = new GenericData.Fixed(Schema.createFixed("fixed", null, null, 1), new byte[] { (byte) 65 });    List<Integer> emptyArray = new ArrayList<Integer>();    ImmutableMap emptyMap = new ImmutableMap.Builder<String, Integer>().build();    Schema arrayOfOptionalIntegers = Schema.createArray(optional(Schema.create(Schema.Type.INT)));    GenericData.Array<Integer> genericIntegerArrayWithNulls = new GenericData.Array<>(arrayOfOptionalIntegers, Arrays.asList(1, null, 2, null, 3));    GenericData.Record record = new GenericRecordBuilder(schema).set("mynull", null).set("myboolean", true).set("myint", 1).set("mylong", 2L).set("myfloat", 3.1f).set("mydouble", 4.1).set("mybytes", ByteBuffer.wrap("hello".getBytes(StandardCharsets.UTF_8))).set("mystring", "hello").set("mynestedrecord", nestedRecord).set("myenum", "a").set("myarray", genericIntegerArray).set("myemptyarray", emptyArray).set("myoptionalarray", genericIntegerArray).set("myarrayofoptional", genericIntegerArrayWithNulls).set("mymap", ImmutableMap.of("a", 1, "b", 2)).set("myemptymap", emptyMap).set("myfixed", genericFixed).build();    try (AvroParquetWriter<GenericRecord> writer = new AvroParquetWriter<>(file, schema)) {        writer.write(record);        fail("Should not succeed writing an array with null values");    } catch (Exception e) {        Assert.assertTrue("Error message should provide context and help", e.getMessage().contains("parquet.avro.write-old-list-structure"));    }}
0
public void testAllUsingDefaultAvroSchema() throws Exception
{    File tmp = File.createTempFile(getClass().getSimpleName(), ".tmp");    tmp.deleteOnExit();    tmp.delete();    Path file = new Path(tmp.getPath());        try (ParquetWriter<Map<String, Object>> parquetWriter = new ParquetWriter<Map<String, Object>>(file, new WriteSupport<Map<String, Object>>() {        private RecordConsumer recordConsumer;        @Override        public WriteContext init(Configuration configuration) {            return new WriteContext(MessageTypeParser.parseMessageType(TestAvroSchemaConverter.ALL_PARQUET_SCHEMA), new HashMap<String, String>());        }        @Override        public void prepareForWrite(RecordConsumer recordConsumer) {            this.recordConsumer = recordConsumer;        }        @Override        public void write(Map<String, Object> record) {            recordConsumer.startMessage();            int index = 0;            recordConsumer.startField("myboolean", index);            recordConsumer.addBoolean((Boolean) record.get("myboolean"));            recordConsumer.endField("myboolean", index++);            recordConsumer.startField("myint", index);            recordConsumer.addInteger((Integer) record.get("myint"));            recordConsumer.endField("myint", index++);            recordConsumer.startField("mylong", index);            recordConsumer.addLong((Long) record.get("mylong"));            recordConsumer.endField("mylong", index++);            recordConsumer.startField("myfloat", index);            recordConsumer.addFloat((Float) record.get("myfloat"));            recordConsumer.endField("myfloat", index++);            recordConsumer.startField("mydouble", index);            recordConsumer.addDouble((Double) record.get("mydouble"));            recordConsumer.endField("mydouble", index++);            recordConsumer.startField("mybytes", index);            recordConsumer.addBinary(Binary.fromReusedByteBuffer((ByteBuffer) record.get("mybytes")));            recordConsumer.endField("mybytes", index++);            recordConsumer.startField("mystring", index);            recordConsumer.addBinary(Binary.fromString((String) record.get("mystring")));            recordConsumer.endField("mystring", index++);            recordConsumer.startField("mynestedrecord", index);            recordConsumer.startGroup();            recordConsumer.startField("mynestedint", 0);            recordConsumer.addInteger((Integer) record.get("mynestedint"));            recordConsumer.endField("mynestedint", 0);            recordConsumer.endGroup();            recordConsumer.endField("mynestedrecord", index++);            recordConsumer.startField("myenum", index);            recordConsumer.addBinary(Binary.fromString((String) record.get("myenum")));            recordConsumer.endField("myenum", index++);            recordConsumer.startField("myarray", index);            recordConsumer.startGroup();            recordConsumer.startField("array", 0);            for (int val : (int[]) record.get("myarray")) {                recordConsumer.addInteger(val);            }            recordConsumer.endField("array", 0);            recordConsumer.endGroup();            recordConsumer.endField("myarray", index++);            recordConsumer.startField("myoptionalarray", index);            recordConsumer.startGroup();            recordConsumer.startField("array", 0);            for (int val : (int[]) record.get("myoptionalarray")) {                recordConsumer.addInteger(val);            }            recordConsumer.endField("array", 0);            recordConsumer.endGroup();            recordConsumer.endField("myoptionalarray", index++);            recordConsumer.startField("myarrayofoptional", index);            recordConsumer.startGroup();            recordConsumer.startField("list", 0);            for (Integer val : (Integer[]) record.get("myarrayofoptional")) {                recordConsumer.startGroup();                if (val != null) {                    recordConsumer.startField("element", 0);                    recordConsumer.addInteger(val);                    recordConsumer.endField("element", 0);                }                recordConsumer.endGroup();            }            recordConsumer.endField("list", 0);            recordConsumer.endGroup();            recordConsumer.endField("myarrayofoptional", index++);            recordConsumer.startField("myrecordarray", index);            recordConsumer.startGroup();            recordConsumer.startField("array", 0);            recordConsumer.startGroup();            recordConsumer.startField("a", 0);            for (int val : (int[]) record.get("myrecordarraya")) {                recordConsumer.addInteger(val);            }            recordConsumer.endField("a", 0);            recordConsumer.startField("b", 1);            for (int val : (int[]) record.get("myrecordarrayb")) {                recordConsumer.addInteger(val);            }            recordConsumer.endField("b", 1);            recordConsumer.endGroup();            recordConsumer.endField("array", 0);            recordConsumer.endGroup();            recordConsumer.endField("myrecordarray", index++);            recordConsumer.startField("mymap", index);            recordConsumer.startGroup();            recordConsumer.startField("map", 0);            recordConsumer.startGroup();            Map<String, Integer> mymap = (Map<String, Integer>) record.get("mymap");            recordConsumer.startField("key", 0);            for (String key : mymap.keySet()) {                recordConsumer.addBinary(Binary.fromString(key));            }            recordConsumer.endField("key", 0);            recordConsumer.startField("value", 1);            for (int val : mymap.values()) {                recordConsumer.addInteger(val);            }            recordConsumer.endField("value", 1);            recordConsumer.endGroup();            recordConsumer.endField("map", 0);            recordConsumer.endGroup();            recordConsumer.endField("mymap", index++);            recordConsumer.startField("myfixed", index);            recordConsumer.addBinary(Binary.fromReusedByteArray((byte[]) record.get("myfixed")));            recordConsumer.endField("myfixed", index++);            recordConsumer.endMessage();        }    })) {        Map<String, Object> record = new HashMap<String, Object>();        record.put("myboolean", true);        record.put("myint", 1);        record.put("mylong", 2L);        record.put("myfloat", 3.1f);        record.put("mydouble", 4.1);        record.put("mybytes", ByteBuffer.wrap("hello".getBytes(StandardCharsets.UTF_8)));        record.put("mystring", "hello");        record.put("myenum", "a");        record.put("mynestedint", 1);        record.put("myarray", new int[] { 1, 2, 3 });        record.put("myoptionalarray", new int[] { 1, 2, 3 });        record.put("myarrayofoptional", new Integer[] { 1, null, 2, null, 3 });        record.put("myrecordarraya", new int[] { 1, 2, 3 });        record.put("myrecordarrayb", new int[] { 4, 5, 6 });        record.put("mymap", ImmutableMap.of("a", 1, "b", 2));        record.put("myfixed", new byte[] { (byte) 65 });        parquetWriter.write(record);    }    Schema nestedRecordSchema = Schema.createRecord("mynestedrecord", null, null, false);    nestedRecordSchema.setFields(Arrays.asList(new Schema.Field("mynestedint", Schema.create(Schema.Type.INT), null, null)));    GenericData.Record nestedRecord = new GenericRecordBuilder(nestedRecordSchema).set("mynestedint", 1).build();    List<Integer> integerArray = Arrays.asList(1, 2, 3);    Schema recordArraySchema = Schema.createRecord("array", null, null, false);    recordArraySchema.setFields(Arrays.asList(new Schema.Field("a", Schema.create(Schema.Type.INT), null, null), new Schema.Field("b", Schema.create(Schema.Type.INT), null, null)));    GenericRecordBuilder builder = new GenericRecordBuilder(recordArraySchema);    List<GenericData.Record> recordArray = new ArrayList<GenericData.Record>();    recordArray.add(builder.set("a", 1).set("b", 4).build());    recordArray.add(builder.set("a", 2).set("b", 5).build());    recordArray.add(builder.set("a", 3).set("b", 6).build());    GenericData.Array<GenericData.Record> genericRecordArray = new GenericData.Array<GenericData.Record>(Schema.createArray(recordArraySchema), recordArray);    GenericFixed genericFixed = new GenericData.Fixed(Schema.createFixed("fixed", null, null, 1), new byte[] { (byte) 65 });        Schema elementSchema = record("list", optionalField("element", primitive(Schema.Type.INT)));    GenericRecordBuilder elementBuilder = new GenericRecordBuilder(elementSchema);    GenericData.Array<GenericData.Record> genericRecordArrayWithNullIntegers = new GenericData.Array<GenericData.Record>(array(elementSchema), Arrays.asList(elementBuilder.set("element", 1).build(), elementBuilder.set("element", null).build(), elementBuilder.set("element", 2).build(), elementBuilder.set("element", null).build(), elementBuilder.set("element", 3).build()));    try (AvroParquetReader<GenericRecord> reader = new AvroParquetReader<>(testConf, file)) {        GenericRecord nextRecord = reader.read();        assertNotNull(nextRecord);        assertEquals(true, nextRecord.get("myboolean"));        assertEquals(1, nextRecord.get("myint"));        assertEquals(2L, nextRecord.get("mylong"));        assertEquals(3.1f, nextRecord.get("myfloat"));        assertEquals(4.1, nextRecord.get("mydouble"));        assertEquals(ByteBuffer.wrap("hello".getBytes(StandardCharsets.UTF_8)), nextRecord.get("mybytes"));        assertEquals(str("hello"), nextRecord.get("mystring"));        assertEquals(str("a"), nextRecord.get("myenum"));        assertEquals(nestedRecord, nextRecord.get("mynestedrecord"));        assertEquals(integerArray, nextRecord.get("myarray"));        assertEquals(integerArray, nextRecord.get("myoptionalarray"));        assertEquals(genericRecordArrayWithNullIntegers, nextRecord.get("myarrayofoptional"));        assertEquals(genericRecordArray, nextRecord.get("myrecordarray"));        assertEquals(ImmutableMap.of(str("a"), 1, str("b"), 2), nextRecord.get("mymap"));        assertEquals(genericFixed, nextRecord.get("myfixed"));    }}
0
public WriteContext init(Configuration configuration)
{    return new WriteContext(MessageTypeParser.parseMessageType(TestAvroSchemaConverter.ALL_PARQUET_SCHEMA), new HashMap<String, String>());}
0
public void prepareForWrite(RecordConsumer recordConsumer)
{    this.recordConsumer = recordConsumer;}
0
public void write(Map<String, Object> record)
{    recordConsumer.startMessage();    int index = 0;    recordConsumer.startField("myboolean", index);    recordConsumer.addBoolean((Boolean) record.get("myboolean"));    recordConsumer.endField("myboolean", index++);    recordConsumer.startField("myint", index);    recordConsumer.addInteger((Integer) record.get("myint"));    recordConsumer.endField("myint", index++);    recordConsumer.startField("mylong", index);    recordConsumer.addLong((Long) record.get("mylong"));    recordConsumer.endField("mylong", index++);    recordConsumer.startField("myfloat", index);    recordConsumer.addFloat((Float) record.get("myfloat"));    recordConsumer.endField("myfloat", index++);    recordConsumer.startField("mydouble", index);    recordConsumer.addDouble((Double) record.get("mydouble"));    recordConsumer.endField("mydouble", index++);    recordConsumer.startField("mybytes", index);    recordConsumer.addBinary(Binary.fromReusedByteBuffer((ByteBuffer) record.get("mybytes")));    recordConsumer.endField("mybytes", index++);    recordConsumer.startField("mystring", index);    recordConsumer.addBinary(Binary.fromString((String) record.get("mystring")));    recordConsumer.endField("mystring", index++);    recordConsumer.startField("mynestedrecord", index);    recordConsumer.startGroup();    recordConsumer.startField("mynestedint", 0);    recordConsumer.addInteger((Integer) record.get("mynestedint"));    recordConsumer.endField("mynestedint", 0);    recordConsumer.endGroup();    recordConsumer.endField("mynestedrecord", index++);    recordConsumer.startField("myenum", index);    recordConsumer.addBinary(Binary.fromString((String) record.get("myenum")));    recordConsumer.endField("myenum", index++);    recordConsumer.startField("myarray", index);    recordConsumer.startGroup();    recordConsumer.startField("array", 0);    for (int val : (int[]) record.get("myarray")) {        recordConsumer.addInteger(val);    }    recordConsumer.endField("array", 0);    recordConsumer.endGroup();    recordConsumer.endField("myarray", index++);    recordConsumer.startField("myoptionalarray", index);    recordConsumer.startGroup();    recordConsumer.startField("array", 0);    for (int val : (int[]) record.get("myoptionalarray")) {        recordConsumer.addInteger(val);    }    recordConsumer.endField("array", 0);    recordConsumer.endGroup();    recordConsumer.endField("myoptionalarray", index++);    recordConsumer.startField("myarrayofoptional", index);    recordConsumer.startGroup();    recordConsumer.startField("list", 0);    for (Integer val : (Integer[]) record.get("myarrayofoptional")) {        recordConsumer.startGroup();        if (val != null) {            recordConsumer.startField("element", 0);            recordConsumer.addInteger(val);            recordConsumer.endField("element", 0);        }        recordConsumer.endGroup();    }    recordConsumer.endField("list", 0);    recordConsumer.endGroup();    recordConsumer.endField("myarrayofoptional", index++);    recordConsumer.startField("myrecordarray", index);    recordConsumer.startGroup();    recordConsumer.startField("array", 0);    recordConsumer.startGroup();    recordConsumer.startField("a", 0);    for (int val : (int[]) record.get("myrecordarraya")) {        recordConsumer.addInteger(val);    }    recordConsumer.endField("a", 0);    recordConsumer.startField("b", 1);    for (int val : (int[]) record.get("myrecordarrayb")) {        recordConsumer.addInteger(val);    }    recordConsumer.endField("b", 1);    recordConsumer.endGroup();    recordConsumer.endField("array", 0);    recordConsumer.endGroup();    recordConsumer.endField("myrecordarray", index++);    recordConsumer.startField("mymap", index);    recordConsumer.startGroup();    recordConsumer.startField("map", 0);    recordConsumer.startGroup();    Map<String, Integer> mymap = (Map<String, Integer>) record.get("mymap");    recordConsumer.startField("key", 0);    for (String key : mymap.keySet()) {        recordConsumer.addBinary(Binary.fromString(key));    }    recordConsumer.endField("key", 0);    recordConsumer.startField("value", 1);    for (int val : mymap.values()) {        recordConsumer.addInteger(val);    }    recordConsumer.endField("value", 1);    recordConsumer.endGroup();    recordConsumer.endField("map", 0);    recordConsumer.endGroup();    recordConsumer.endField("mymap", index++);    recordConsumer.startField("myfixed", index);    recordConsumer.addBinary(Binary.fromReusedByteArray((byte[]) record.get("myfixed")));    recordConsumer.endField("myfixed", index++);    recordConsumer.endMessage();}
0
public CharSequence str(String value)
{    return compat ? value : new Utf8(value);}
0
public boolean equals(Object o)
{    if (this == o)        return true;    if (o == null || getClass() != o.getClass())        return false;    Service service = (Service) o;    if (date != service.date)        return false;    if (!mechanic.equals(service.mechanic))        return false;    return true;}
0
public int hashCode()
{    int result = (int) (date ^ (date >>> 32));    result = 31 * result + mechanic.hashCode();    return result;}
0
public boolean equals(Object o)
{    if (this == o)        return true;    if (o == null || getClass() != o.getClass())        return false;    Engine engine = (Engine) o;    if (Float.compare(engine.capacity, capacity) != 0)        return false;    if (hasTurboCharger != engine.hasTurboCharger)        return false;    if (type != engine.type)        return false;    return true;}
0
public int hashCode()
{    int result = type.hashCode();    result = 31 * result + (capacity != +0.0f ? Float.floatToIntBits(capacity) : 0);    result = 31 * result + (hasTurboCharger ? 1 : 0);    return result;}
0
public boolean equals(Object o)
{    if (this == o)        return true;    if (o == null || getClass() != o.getClass())        return false;    Stereo stereo = (Stereo) o;    if (speakers != stereo.speakers)        return false;    if (!make.equals(stereo.make))        return false;    return true;}
0
public int hashCode()
{    int result = make.hashCode();    result = 31 * result + speakers;    return result;}
0
public boolean equals(Object o)
{    if (this == o)        return true;    if (o == null || getClass() != o.getClass())        return false;    LeatherTrim that = (LeatherTrim) o;    if (!colour.equals(that.colour))        return false;    return true;}
0
public int hashCode()
{    return colour.hashCode();}
0
public boolean equals(Object o)
{    if (this == o)        return true;    if (o == null || getClass() != o.getClass())        return false;    Car car = (Car) o;    if (doors != car.doors)        return false;    if (year != car.year)        return false;    if (!engine.equals(car.engine))        return false;    if (!make.equals(car.make))        return false;    if (!model.equals(car.model))        return false;    if (optionalExtra != null ? !optionalExtra.equals(car.optionalExtra) : car.optionalExtra != null)        return false;    if (!registration.equals(car.registration))        return false;    if (serviceHistory != null ? !serviceHistory.equals(car.serviceHistory) : car.serviceHistory != null)        return false;    if (!Arrays.equals(vin, car.vin))        return false;    return true;}
0
public int hashCode()
{    int result = (int) (year ^ (year >>> 32));    result = 31 * result + registration.hashCode();    result = 31 * result + make.hashCode();    result = 31 * result + model.hashCode();    result = 31 * result + Arrays.hashCode(vin);    result = 31 * result + doors;    result = 31 * result + engine.hashCode();    result = 31 * result + (optionalExtra != null ? optionalExtra.hashCode() : 0);    result = 31 * result + (serviceHistory != null ? serviceHistory.hashCode() : 0);    return result;}
0
public boolean equals(Object o)
{    if (this == o)        return true;    if (o == null || getClass() != o.getClass())        return false;    ShortCar shortCar = (ShortCar) o;    if (year != shortCar.year)        return false;    if (!engine.equals(shortCar.engine))        return false;    if (make != null ? !make.equals(shortCar.make) : shortCar.make != null)        return false;    if (!Arrays.equals(vin, shortCar.vin))        return false;    return true;}
0
public int hashCode()
{    int result = make != null ? make.hashCode() : 0;    result = 31 * result + engine.hashCode();    result = 31 * result + (int) (year ^ (year >>> 32));    result = 31 * result + Arrays.hashCode(vin);    return result;}
0
public static Car nextRecord(int i)
{    Car car = new Car();    car.doors = 2;    car.make = "Tesla";    car.model = String.format("Model X v%d", i % 2);    car.vin = String.format("1VXBR12EXCP%06d", i).getBytes();    car.year = 2014 + i;    car.registration = "California";    LeatherTrim trim = new LeatherTrim();    trim.colour = "black";    car.optionalExtra = trim;    Engine engine = new Engine();    engine.capacity = 85.0f;    engine.type = (i % 2) == 0 ? EngineType.ELECTRIC : EngineType.PETROL;    engine.hasTurboCharger = false;    car.engine = engine;    if (i % 4 == 0) {        Service service = new Service();        service.date = 1374084640;        service.mechanic = "Elon Musk";        car.serviceHistory = Lists.newArrayList();        car.serviceHistory.add(service);    }    return car;}
0
public void run(Context context) throws IOException, InterruptedException
{    for (int i = 0; i < 10; i++) {        context.write(null, nextRecord(i));    }}
0
protected void map(Void key, Car car, Context context) throws IOException, InterruptedException
{        if (car != null) {        context.write(null, car);    }}
0
protected void map(Void key, ShortCar car, Context context) throws IOException, InterruptedException
{        if (car != null) {        context.write(null, car);    }}
0
public RecordFilter bind(Iterable<ColumnReader> readers)
{    return filter.bind(readers);}
0
public void createParquetFile() throws Exception
{        conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, false);    AvroReadSupport.setAvroDataSupplier(conf, ReflectDataSupplier.class);    AvroWriteSupport.setAvroDataSupplier(conf, ReflectDataSupplier.class);    final FileSystem fileSystem = parquetPath.getFileSystem(conf);    fileSystem.delete(parquetPath, true);    fileSystem.delete(outputPath, true);    {        final Job job = new Job(conf, "write");                TextInputFormat.addInputPath(job, inputPath);        job.setInputFormatClass(TextInputFormat.class);        job.setMapperClass(TestReflectInputOutputFormat.MyMapper.class);        job.setNumReduceTasks(0);        job.setOutputFormatClass(AvroParquetOutputFormat.class);        AvroParquetOutputFormat.setOutputPath(job, parquetPath);        AvroParquetOutputFormat.setSchema(job, CAR_SCHEMA);        AvroParquetOutputFormat.setAvroDataSupplier(job, ReflectDataSupplier.class);        waitForJob(job);    }}
0
public void testReadWrite() throws Exception
{    conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, false);    final Job job = new Job(conf, "read");    job.setInputFormatClass(AvroParquetInputFormat.class);    AvroParquetInputFormat.setInputPaths(job, parquetPath);        AvroParquetInputFormat.setUnboundRecordFilter(job, ElectricCarFilter.class);        Schema projection = Schema.createRecord(CAR_SCHEMA.getName(), CAR_SCHEMA.getDoc(), CAR_SCHEMA.getNamespace(), false);    List<Schema.Field> fields = Lists.newArrayList();    for (Schema.Field field : ReflectData.get().getSchema(Car.class).getFields()) {        if (!"optionalExtra".equals(field.name())) {            fields.add(new Schema.Field(field.name(), field.schema(), field.doc(), field.defaultVal(), field.order()));        }    }    projection.setFields(fields);    AvroParquetInputFormat.setRequestedProjection(job, projection);    job.setMapperClass(TestReflectInputOutputFormat.MyMapper2.class);    job.setNumReduceTasks(0);    job.setOutputFormatClass(AvroParquetOutputFormat.class);    AvroParquetOutputFormat.setOutputPath(job, outputPath);    AvroParquetOutputFormat.setSchema(job, CAR_SCHEMA);    waitForJob(job);    final Path mapperOutput = new Path(outputPath.toString(), "part-m-00000.parquet");    try (final AvroParquetReader<Car> out = new AvroParquetReader<Car>(conf, mapperOutput)) {        Car car;        Car previousCar = null;        int lineNumber = 0;        while ((car = out.read()) != null) {            if (previousCar != null) {                                assertTrue(car.model == previousCar.model);            }                        if (car.engine.type == EngineType.PETROL) {                fail("UnboundRecordFilter failed to remove cars with PETROL engines");            }                        Car expectedCar = nextRecord(lineNumber * 2);                                    expectedCar.optionalExtra = null;            assertEquals("line " + lineNumber, expectedCar, car);            ++lineNumber;            previousCar = car;        }    }}
0
public void testReadWriteChangedCar() throws Exception
{    conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, false);    final Job job = new Job(conf, "read changed/short");    job.setInputFormatClass(AvroParquetInputFormat.class);    AvroParquetInputFormat.setInputPaths(job, parquetPath);        AvroParquetInputFormat.setUnboundRecordFilter(job, ElectricCarFilter.class);            Schema projection = Schema.createRecord(CAR_SCHEMA.getName(), CAR_SCHEMA.getDoc(), CAR_SCHEMA.getNamespace(), false);    List<Schema.Field> fields = Lists.newArrayList();    for (Schema.Field field : CAR_SCHEMA.getFields()) {                if ("engine".equals(field.name()) || "year".equals(field.name()) || "vin".equals(field.name())) {            fields.add(new Schema.Field(field.name(), field.schema(), field.doc(), field.defaultVal(), field.order()));        }    }    projection.setFields(fields);    AvroParquetInputFormat.setRequestedProjection(job, projection);    AvroParquetInputFormat.setAvroReadSchema(job, SHORT_CAR_SCHEMA);    job.setMapperClass(TestReflectInputOutputFormat.MyMapperShort.class);    job.setNumReduceTasks(0);    job.setOutputFormatClass(AvroParquetOutputFormat.class);    AvroParquetOutputFormat.setOutputPath(job, outputPath);    AvroParquetOutputFormat.setSchema(job, SHORT_CAR_SCHEMA);    waitForJob(job);    final Path mapperOutput = new Path(outputPath.toString(), "part-m-00000.parquet");    try (final AvroParquetReader<ShortCar> out = new AvroParquetReader<ShortCar>(conf, mapperOutput)) {        ShortCar car;        int lineNumber = 0;        while ((car = out.read()) != null) {                                    Car expectedCar = nextRecord(lineNumber * 2);                        assertNull(car.make);            assertEquals(car.engine, expectedCar.engine);            assertEquals(car.year, expectedCar.year);            assertArrayEquals(car.vin, expectedCar.vin);            ++lineNumber;        }    }}
0
private void waitForJob(Job job) throws Exception
{    job.submit();    while (!job.isComplete()) {                sleep(100);    }        if (!job.isSuccessful()) {        throw new RuntimeException("job failed " + job.getJobName());    }}
1
public void deleteOutputFile() throws IOException
{    final FileSystem fileSystem = parquetPath.getFileSystem(conf);    fileSystem.delete(parquetPath, true);    fileSystem.delete(outputPath, true);}
0
public static void addUUID()
{    REFLECT.addLogicalTypeConversion(new Conversions.UUIDConversion());    REFLECT.addLogicalTypeConversion(new Conversions.DecimalConversion());}
0
public void testReflectedSchema()
{    Schema expected = SchemaBuilder.record(RecordWithUUIDList.class.getName()).fields().name("uuids").type().array().items().stringType().noDefault().endRecord();    expected.getField("uuids").schema().addProp(SpecificData.CLASS_PROP, List.class.getName());    LogicalTypes.uuid().addToSchema(expected.getField("uuids").schema().getElementType());    Schema actual = REFLECT.getSchema(RecordWithUUIDList.class);    Assert.assertEquals("Should use the UUID logical type", expected, actual);}
0
public boolean equals(Object other)
{    if (this == other) {        return true;    }    if (other == null || getClass() != other.getClass()) {        return false;    }    DecimalRecordBytes that = (DecimalRecordBytes) other;    if (decimal == null) {        return (that.decimal == null);    }    return decimal.equals(that.decimal);}
0
public int hashCode()
{    return decimal != null ? decimal.hashCode() : 0;}
0
public void testDecimalBytes() throws IOException
{    Schema schema = REFLECT.getSchema(DecimalRecordBytes.class);    Assert.assertEquals("Should have the correct record name", "org.apache.parquet.avro.TestReflectLogicalTypes", schema.getNamespace());    Assert.assertEquals("Should have the correct record name", "DecimalRecordBytes", schema.getName());    Assert.assertEquals("Should have the correct logical type", LogicalTypes.decimal(9, 2), LogicalTypes.fromSchema(schema.getField("decimal").schema()));    DecimalRecordBytes record = new DecimalRecordBytes();    record.decimal = new BigDecimal("3.14");    File test = write(REFLECT, schema, record);    Assert.assertEquals("Should match the decimal after round trip", Arrays.asList(record), read(REFLECT, schema, test));}
0
public boolean equals(Object other)
{    if (this == other) {        return true;    }    if (other == null || getClass() != other.getClass()) {        return false;    }    DecimalRecordFixed that = (DecimalRecordFixed) other;    if (decimal == null) {        return (that.decimal == null);    }    return decimal.equals(that.decimal);}
0
public int hashCode()
{    return decimal != null ? decimal.hashCode() : 0;}
0
public void testDecimalFixed() throws IOException
{    Schema schema = REFLECT.getSchema(DecimalRecordFixed.class);    Assert.assertEquals("Should have the correct record name", "org.apache.parquet.avro.TestReflectLogicalTypes", schema.getNamespace());    Assert.assertEquals("Should have the correct record name", "DecimalRecordFixed", schema.getName());    Assert.assertEquals("Should have the correct logical type", LogicalTypes.decimal(9, 2), LogicalTypes.fromSchema(schema.getField("decimal").schema()));    DecimalRecordFixed record = new DecimalRecordFixed();    record.decimal = new BigDecimal("3.14");    File test = write(REFLECT, schema, record);    Assert.assertEquals("Should match the decimal after round trip", Arrays.asList(record), read(REFLECT, schema, test));}
0
public boolean equals(Object other)
{    if (this == other) {        return true;    }    if (other == null || getClass() != other.getClass()) {        return false;    }    Pair<?, ?> that = (Pair<?, ?>) other;    if (first == null) {        if (that.first != null) {            return false;        }    } else if (first.equals(that.first)) {        return false;    }    if (second == null) {        if (that.second != null) {            return false;        }    } else if (second.equals(that.second)) {        return false;    }    return true;}
0
public int hashCode()
{    return Arrays.hashCode(new Object[] { first, second });}
0
public static Pair<X, Y> of(X first, Y second)
{    return new Pair<X, Y>(first, second);}
0
public void testPairRecord() throws IOException
{    ReflectData model = new ReflectData();    model.addLogicalTypeConversion(new Conversion<Pair>() {        @Override        public Class<Pair> getConvertedType() {            return Pair.class;        }        @Override        public String getLogicalTypeName() {            return "pair";        }        @Override        public Pair fromRecord(IndexedRecord value, Schema schema, LogicalType type) {            return Pair.of(value.get(0), value.get(1));        }        @Override        public IndexedRecord toRecord(Pair value, Schema schema, LogicalType type) {            GenericData.Record record = new GenericData.Record(schema);            record.put(0, value.first);            record.put(1, value.second);            return record;        }    });    LogicalTypes.register("pair", new LogicalTypes.LogicalTypeFactory() {        private final LogicalType PAIR = new LogicalType("pair");        @Override        public LogicalType fromSchema(Schema schema) {            return PAIR;        }    });    Schema schema = model.getSchema(PairRecord.class);    Assert.assertEquals("Should have the correct record name", "org.apache.parquet.avro.TestReflectLogicalTypes", schema.getNamespace());    Assert.assertEquals("Should have the correct record name", "PairRecord", schema.getName());    Assert.assertEquals("Should have the correct logical type", "pair", LogicalTypes.fromSchema(schema.getField("pair").schema()).getName());    PairRecord record = new PairRecord();    record.pair = Pair.of(34L, 35L);    List<PairRecord> expected = new ArrayList<PairRecord>();    expected.add(record);    File test = write(model, schema, record);    Pair<Long, Long> actual = AvroTestUtil.<PairRecord>read(model, schema, test).get(0).pair;    Assert.assertEquals("Data should match after serialization round-trip", 34L, (long) actual.first);    Assert.assertEquals("Data should match after serialization round-trip", 35L, (long) actual.second);}
0
public Class<Pair> getConvertedType()
{    return Pair.class;}
0
public String getLogicalTypeName()
{    return "pair";}
0
public Pair fromRecord(IndexedRecord value, Schema schema, LogicalType type)
{    return Pair.of(value.get(0), value.get(1));}
0
public IndexedRecord toRecord(Pair value, Schema schema, LogicalType type)
{    GenericData.Record record = new GenericData.Record(schema);    record.put(0, value.first);    record.put(1, value.second);    return record;}
0
public LogicalType fromSchema(Schema schema)
{    return PAIR;}
0
public void testReadUUID() throws IOException
{    Schema uuidSchema = SchemaBuilder.record(RecordWithUUID.class.getName()).fields().requiredString("uuid").endRecord();    LogicalTypes.uuid().addToSchema(uuidSchema.getField("uuid").schema());    UUID u1 = UUID.randomUUID();    UUID u2 = UUID.randomUUID();    RecordWithStringUUID r1 = new RecordWithStringUUID();    r1.uuid = u1.toString();    RecordWithStringUUID r2 = new RecordWithStringUUID();    r2.uuid = u2.toString();    List<RecordWithUUID> expected = Arrays.asList(new RecordWithUUID(), new RecordWithUUID());    expected.get(0).uuid = u1;    expected.get(1).uuid = u2;    File test = write(ReflectData.get().getSchema(RecordWithStringUUID.class), r1, r2);    Assert.assertEquals("Should convert Strings to UUIDs", expected, read(REFLECT, uuidSchema, test));        Schema uuidStringSchema = SchemaBuilder.record(RecordWithStringUUID.class.getName()).fields().requiredString("uuid").endRecord();    LogicalTypes.uuid().addToSchema(uuidStringSchema.getField("uuid").schema());    Assert.assertEquals("Should not convert to UUID if accessor is String", Arrays.asList(r1, r2), read(REFLECT, uuidStringSchema, test));}
0
public void testWriteUUID() throws IOException
{    Schema uuidSchema = SchemaBuilder.record(RecordWithUUID.class.getName()).fields().requiredString("uuid").endRecord();    LogicalTypes.uuid().addToSchema(uuidSchema.getField("uuid").schema());    UUID u1 = UUID.randomUUID();    UUID u2 = UUID.randomUUID();    RecordWithUUID r1 = new RecordWithUUID();    r1.uuid = u1;    RecordWithUUID r2 = new RecordWithUUID();    r2.uuid = u2;    List<RecordWithStringUUID> expected = Arrays.asList(new RecordWithStringUUID(), new RecordWithStringUUID());    expected.get(0).uuid = u1.toString();    expected.get(1).uuid = u2.toString();    File test = write(REFLECT, uuidSchema, r1, r2);        Schema uuidStringSchema = SchemaBuilder.record(RecordWithStringUUID.class.getName()).fields().requiredString("uuid").endRecord();    Assert.assertEquals("Should read uuid as String without UUID conversion", expected, read(REFLECT, uuidStringSchema, test));    LogicalTypes.uuid().addToSchema(uuidStringSchema.getField("uuid").schema());    Assert.assertEquals("Should read uuid as String without UUID logical type", expected, read(ReflectData.get(), uuidStringSchema, test));}
0
public void testWriteNullableUUID() throws IOException
{    Schema nullableUuidSchema = SchemaBuilder.record(RecordWithUUID.class.getName()).fields().optionalString("uuid").endRecord();    LogicalTypes.uuid().addToSchema(nullableUuidSchema.getField("uuid").schema().getTypes().get(1));    UUID u1 = UUID.randomUUID();    UUID u2 = UUID.randomUUID();    RecordWithUUID r1 = new RecordWithUUID();    r1.uuid = u1;    RecordWithUUID r2 = new RecordWithUUID();    r2.uuid = u2;    List<RecordWithStringUUID> expected = Arrays.asList(new RecordWithStringUUID(), new RecordWithStringUUID());    expected.get(0).uuid = u1.toString();    expected.get(1).uuid = u2.toString();    File test = write(REFLECT, nullableUuidSchema, r1, r2);        Schema nullableUuidStringSchema = SchemaBuilder.record(RecordWithStringUUID.class.getName()).fields().optionalString("uuid").endRecord();    Assert.assertEquals("Should read uuid as String without UUID conversion", expected, read(REFLECT, nullableUuidStringSchema, test));}
0
public void testWriteUUIDMissingLogicalType() throws IOException
{    Schema uuidSchema = SchemaBuilder.record(RecordWithUUID.class.getName()).fields().requiredString("uuid").endRecord();    LogicalTypes.uuid().addToSchema(uuidSchema.getField("uuid").schema());    UUID u1 = UUID.randomUUID();    UUID u2 = UUID.randomUUID();    RecordWithUUID r1 = new RecordWithUUID();    r1.uuid = u1;    RecordWithUUID r2 = new RecordWithUUID();    r2.uuid = u2;    List<RecordWithStringUUID> expected = Arrays.asList(new RecordWithStringUUID(), new RecordWithStringUUID());    expected.get(0).uuid = u1.toString();    expected.get(1).uuid = u2.toString();        File test = write(uuidSchema, r1, r2);        Schema uuidStringSchema = SchemaBuilder.record(RecordWithStringUUID.class.getName()).fields().requiredString("uuid").endRecord();    Assert.assertEquals("Should read uuid as String without UUID conversion", expected, read(REFLECT, uuidStringSchema, test));    Assert.assertEquals("Should read uuid as String without UUID logical type", expected, read(ReflectData.get(), uuidStringSchema, test));}
0
public void testReadUUIDGenericRecord() throws IOException
{    Schema uuidSchema = SchemaBuilder.record("RecordWithUUID").fields().requiredString("uuid").endRecord();    LogicalTypes.uuid().addToSchema(uuidSchema.getField("uuid").schema());    UUID u1 = UUID.randomUUID();    UUID u2 = UUID.randomUUID();    RecordWithStringUUID r1 = new RecordWithStringUUID();    r1.uuid = u1.toString();    RecordWithStringUUID r2 = new RecordWithStringUUID();    r2.uuid = u2.toString();    List<GenericData.Record> expected = Arrays.asList(new GenericData.Record(uuidSchema), new GenericData.Record(uuidSchema));    expected.get(0).put("uuid", u1);    expected.get(1).put("uuid", u2);    File test = write(ReflectData.get().getSchema(RecordWithStringUUID.class), r1, r2);    Assert.assertEquals("Should convert Strings to UUIDs", expected, read(REFLECT, uuidSchema, test));        Schema uuidStringSchema = SchemaBuilder.record(RecordWithStringUUID.class.getName()).fields().requiredString("uuid").endRecord();    LogicalTypes.uuid().addToSchema(uuidSchema.getField("uuid").schema());    Assert.assertEquals("Should not convert to UUID if accessor is String", Arrays.asList(r1, r2), read(REFLECT, uuidStringSchema, test));}
0
public void testReadUUIDArray() throws IOException
{    Schema uuidArraySchema = SchemaBuilder.record(RecordWithUUIDArray.class.getName()).fields().name("uuids").type().array().items().stringType().noDefault().endRecord();    LogicalTypes.uuid().addToSchema(uuidArraySchema.getField("uuids").schema().getElementType());    UUID u1 = UUID.randomUUID();    UUID u2 = UUID.randomUUID();    GenericRecord r = new GenericData.Record(uuidArraySchema);    r.put("uuids", Arrays.asList(u1.toString(), u2.toString()));    RecordWithUUIDArray expected = new RecordWithUUIDArray();    expected.uuids = new UUID[] { u1, u2 };    File test = write(uuidArraySchema, r);    Assert.assertEquals("Should convert Strings to UUIDs", expected, read(REFLECT, uuidArraySchema, test).get(0));}
0
public void testWriteUUIDArray() throws IOException
{    Schema uuidArraySchema = SchemaBuilder.record(RecordWithUUIDArray.class.getName()).fields().name("uuids").type().array().items().stringType().noDefault().endRecord();    LogicalTypes.uuid().addToSchema(uuidArraySchema.getField("uuids").schema().getElementType());    Schema stringArraySchema = SchemaBuilder.record("RecordWithUUIDArray").fields().name("uuids").type().array().items().stringType().noDefault().endRecord();    stringArraySchema.getField("uuids").schema().addProp(SpecificData.CLASS_PROP, List.class.getName());    UUID u1 = UUID.randomUUID();    UUID u2 = UUID.randomUUID();    GenericRecord expected = new GenericData.Record(stringArraySchema);    List<String> uuids = new ArrayList<String>();    uuids.add(u1.toString());    uuids.add(u2.toString());    expected.put("uuids", uuids);    RecordWithUUIDArray r = new RecordWithUUIDArray();    r.uuids = new UUID[] { u1, u2 };    File test = write(REFLECT, uuidArraySchema, r);    Assert.assertEquals("Should read UUIDs as Strings", expected, read(ReflectData.get(), stringArraySchema, test).get(0));}
0
public void testReadUUIDList() throws IOException
{    Schema uuidListSchema = SchemaBuilder.record(RecordWithUUIDList.class.getName()).fields().name("uuids").type().array().items().stringType().noDefault().endRecord();    uuidListSchema.getField("uuids").schema().addProp(SpecificData.CLASS_PROP, List.class.getName());    LogicalTypes.uuid().addToSchema(uuidListSchema.getField("uuids").schema().getElementType());    UUID u1 = UUID.randomUUID();    UUID u2 = UUID.randomUUID();    GenericRecord r = new GenericData.Record(uuidListSchema);    r.put("uuids", Arrays.asList(u1.toString(), u2.toString()));    RecordWithUUIDList expected = new RecordWithUUIDList();    expected.uuids = Arrays.asList(u1, u2);    File test = write(uuidListSchema, r);    Assert.assertEquals("Should convert Strings to UUIDs", expected, read(REFLECT, uuidListSchema, test).get(0));}
0
public void testWriteUUIDList() throws IOException
{    Schema uuidListSchema = SchemaBuilder.record(RecordWithUUIDList.class.getName()).fields().name("uuids").type().array().items().stringType().noDefault().endRecord();    uuidListSchema.getField("uuids").schema().addProp(SpecificData.CLASS_PROP, List.class.getName());    LogicalTypes.uuid().addToSchema(uuidListSchema.getField("uuids").schema().getElementType());    Schema stringArraySchema = SchemaBuilder.record("RecordWithUUIDArray").fields().name("uuids").type().array().items().stringType().noDefault().endRecord();    stringArraySchema.getField("uuids").schema().addProp(SpecificData.CLASS_PROP, List.class.getName());    UUID u1 = UUID.randomUUID();    UUID u2 = UUID.randomUUID();    GenericRecord expected = new GenericData.Record(stringArraySchema);    expected.put("uuids", Arrays.asList(u1.toString(), u2.toString()));    RecordWithUUIDList r = new RecordWithUUIDList();    r.uuids = Arrays.asList(u1, u2);    File test = write(REFLECT, uuidListSchema, r);    Assert.assertEquals("Should read UUIDs as Strings", expected, read(REFLECT, stringArraySchema, test).get(0));}
0
private File write(Schema schema, D... data) throws IOException
{    return write(ReflectData.get(), schema, data);}
0
private File write(GenericData model, Schema schema, D... data) throws IOException
{    return AvroTestUtil.write(temp, model, schema, data);}
0
public int hashCode()
{    return uuid.hashCode();}
0
public boolean equals(Object obj)
{    if (obj == null) {        return false;    }    if (!(obj instanceof RecordWithUUID)) {        return false;    }    RecordWithUUID that = (RecordWithUUID) obj;    return this.uuid.equals(that.uuid);}
0
public int hashCode()
{    return uuid.hashCode();}
0
public boolean equals(Object obj)
{    if (obj == null) {        return false;    }    if (!(obj instanceof RecordWithStringUUID)) {        return false;    }    RecordWithStringUUID that = (RecordWithStringUUID) obj;    return this.uuid.equals(that.uuid);}
0
public int hashCode()
{    return Arrays.hashCode(uuids);}
0
public boolean equals(Object obj)
{    if (obj == null) {        return false;    }    if (!(obj instanceof RecordWithUUIDArray)) {        return false;    }    RecordWithUUIDArray that = (RecordWithUUIDArray) obj;    return Arrays.equals(this.uuids, that.uuids);}
0
public int hashCode()
{    return uuids.hashCode();}
0
public boolean equals(Object obj)
{    if (obj == null) {        return false;    }    if (!(obj instanceof RecordWithUUIDList)) {        return false;    }    RecordWithUUIDList that = (RecordWithUUIDList) obj;    return this.uuids.equals(that.uuids);}
0
public void testReadWriteReflect() throws IOException
{    Configuration conf = new Configuration(false);    conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, false);    AvroReadSupport.setAvroDataSupplier(conf, ReflectDataSupplier.class);    Path path = writePojosToParquetFile(10, CompressionCodecName.UNCOMPRESSED, false);    try (ParquetReader<Pojo> reader = new AvroParquetReader<Pojo>(conf, path)) {        Pojo object = getPojo();        for (int i = 0; i < 10; i++) {            assertEquals(object, reader.read());        }        assertNull(reader.read());    }}
0
public void testWriteReflectReadGeneric() throws IOException
{    Configuration conf = new Configuration(false);    conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, false);    AvroReadSupport.setAvroDataSupplier(conf, GenericDataSupplier.class);    Path path = writePojosToParquetFile(2, CompressionCodecName.UNCOMPRESSED, false);    try (ParquetReader<GenericRecord> reader = new AvroParquetReader<GenericRecord>(conf, path)) {        GenericRecord object = getGenericPojoUtf8();        for (int i = 0; i < 2; i += 1) {            assertEquals(object, reader.read());        }        assertNull(reader.read());    }}
0
private GenericRecord getGenericPojoUtf8()
{    Schema schema = ReflectData.get().getSchema(Pojo.class);    GenericData.Record record = new GenericData.Record(schema);    record.put("myboolean", true);    record.put("mybyte", 1);    record.put("myshort", 1);    record.put("myint", 1);    record.put("mylong", 2L);    record.put("myfloat", 3.1f);    record.put("mydouble", 4.1);    record.put("mybytes", ByteBuffer.wrap(new byte[] { 1, 2, 3, 4 }));    record.put("mystring", new Utf8("Hello"));    record.put("myenum", new GenericData.EnumSymbol(schema.getField("myenum").schema(), "A"));    Map<CharSequence, CharSequence> map = new HashMap<CharSequence, CharSequence>();    map.put(new Utf8("a"), new Utf8("1"));    map.put(new Utf8("b"), new Utf8("2"));    record.put("mymap", map);    record.put("myshortarray", new GenericData.Array<Integer>(schema.getField("myshortarray").schema(), Lists.newArrayList(1, 2)));    record.put("myintarray", new GenericData.Array<Integer>(schema.getField("myintarray").schema(), Lists.newArrayList(1, 2)));    record.put("mystringarray", new GenericData.Array<Utf8>(schema.getField("mystringarray").schema(), Lists.newArrayList(new Utf8("a"), new Utf8("b"))));    record.put("mylist", new GenericData.Array<Utf8>(schema.getField("mylist").schema(), Lists.newArrayList(new Utf8("a"), new Utf8("b"), new Utf8("c"))));    record.put("mystringable", new StringableObj("blah blah"));    return record;}
0
private Pojo getPojo()
{    Pojo object = new Pojo();    object.myboolean = true;    object.mybyte = 1;    object.myshort = 1;    object.myint = 1;    object.mylong = 2L;    object.myfloat = 3.1f;    object.mydouble = 4.1;    object.mybytes = new byte[] { 1, 2, 3, 4 };    object.mystring = "Hello";    object.myenum = E.A;    Map<String, String> map = new HashMap<String, String>();    map.put("a", "1");    map.put("b", "2");    object.mymap = map;    object.myshortarray = new short[] { 1, 2 };    object.myintarray = new int[] { 1, 2 };    object.mystringarray = new String[] { "a", "b" };    object.mylist = Lists.newArrayList("a", "b", "c");    object.mystringable = new StringableObj("blah blah");    return object;}
0
private Path writePojosToParquetFile(int num, CompressionCodecName compression, boolean enableDictionary) throws IOException
{    File tmp = File.createTempFile(getClass().getSimpleName(), ".tmp");    tmp.deleteOnExit();    tmp.delete();    Path path = new Path(tmp.getPath());    Pojo object = getPojo();    Schema schema = ReflectData.get().getSchema(object.getClass());    try (ParquetWriter<Pojo> writer = AvroParquetWriter.<Pojo>builder(path).withSchema(schema).withCompressionCodec(compression).withDataModel(ReflectData.get()).withDictionaryEncoding(enableDictionary).build()) {        for (int i = 0; i < num; i++) {            writer.write(object);        }    }    return path;}
0
public String toString()
{    return this.value;}
0
public boolean equals(Object other)
{    return other instanceof StringableObj && this.value.equals(((StringableObj) other).value);}
0
public boolean equals(Object o)
{    if (!(o instanceof Pojo))        return false;    Pojo that = (Pojo) o;    return myboolean == that.myboolean && mybyte == that.mybyte && myshort == that.myshort && myint == that.myint && mylong == that.mylong && myfloat == that.myfloat && mydouble == that.mydouble && Arrays.equals(mybytes, that.mybytes) && mystring.equals(that.mystring) && myenum == that.myenum && mymap.equals(that.mymap) && Arrays.equals(myshortarray, that.myshortarray) && Arrays.equals(myintarray, that.myintarray) && Arrays.equals(mystringarray, that.mystringarray) && mylist.equals(that.mylist) && mystringable.equals(that.mystringable);}
0
public String toString()
{    return "Pojo{" + "myboolean=" + myboolean + ", mybyte=" + mybyte + ", myshort=" + myshort + ", myint=" + myint + ", mylong=" + mylong + ", myfloat=" + myfloat + ", mydouble=" + mydouble + ", mybytes=" + Arrays.toString(mybytes) + ", mystring='" + mystring + '\'' + ", myenum=" + myenum + ", mymap=" + mymap + ", myshortarray=" + Arrays.toString(myshortarray) + ", myintarray=" + Arrays.toString(myintarray) + ", mystringarray=" + Arrays.toString(mystringarray) + ", mylist=" + mylist + ", mystringable=" + mystringable.toString() + '}';}
0
public static Car nextRecord(int i)
{    String vin = "1VXBR12EXCP000000";    Car.Builder carBuilder = Car.newBuilder().setDoors(2).setMake("Tesla").setModel(String.format("Model X v%d", i % 2)).setVin(new Vin(vin.getBytes())).setYear(2014 + i).setOptionalExtra(LeatherTrim.newBuilder().setColour("black").build()).setRegistration("California");    Engine.Builder engineBuilder = Engine.newBuilder().setCapacity(85.0f).setHasTurboCharger(false);    if (i % 2 == 0) {        engineBuilder.setType(EngineType.ELECTRIC);    } else {        engineBuilder.setType(EngineType.PETROL);    }    carBuilder.setEngine(engineBuilder.build());    if (i % 4 == 0) {        List<Service> serviceList = Lists.newArrayList();        serviceList.add(Service.newBuilder().setDate(1374084640).setMechanic("Elon Musk").build());        carBuilder.setServiceHistory(serviceList);    }    return carBuilder.build();}
0
public void run(Context context) throws IOException, InterruptedException
{    for (int i = 0; i < 10; i++) {        context.write(null, nextRecord(i));    }}
0
protected void map(Void key, Car car, Context context) throws IOException, InterruptedException
{        if (car != null) {        context.write(null, car);    }}
0
protected void map(Void key, ShortCar car, Context context) throws IOException, InterruptedException
{        if (car != null) {        context.write(null, car);    }}
0
public RecordFilter bind(Iterable<ColumnReader> readers)
{    return filter.bind(readers);}
0
public void createParquetFile() throws Exception
{    final FileSystem fileSystem = parquetPath.getFileSystem(conf);    fileSystem.delete(parquetPath, true);    fileSystem.delete(outputPath, true);    {        final Job job = new Job(conf, "write");                TextInputFormat.addInputPath(job, inputPath);        job.setInputFormatClass(TextInputFormat.class);        job.setMapperClass(TestSpecificInputOutputFormat.MyMapper.class);        job.setNumReduceTasks(0);        job.setOutputFormatClass(AvroParquetOutputFormat.class);        AvroParquetOutputFormat.setOutputPath(job, parquetPath);        AvroParquetOutputFormat.setSchema(job, Car.SCHEMA$);        waitForJob(job);    }}
0
public void testReadWrite() throws Exception
{    final Job job = new Job(conf, "read");    job.setInputFormatClass(AvroParquetInputFormat.class);    AvroParquetInputFormat.setInputPaths(job, parquetPath);        AvroParquetInputFormat.setUnboundRecordFilter(job, ElectricCarFilter.class);        Schema projection = Schema.createRecord(Car.SCHEMA$.getName(), Car.SCHEMA$.getDoc(), Car.SCHEMA$.getNamespace(), false);    List<Schema.Field> fields = Lists.newArrayList();    for (Schema.Field field : Car.SCHEMA$.getFields()) {        if (!"optionalExtra".equals(field.name())) {            fields.add(new Schema.Field(field.name(), field.schema(), field.doc(), field.defaultVal(), field.order()));        }    }    projection.setFields(fields);    AvroParquetInputFormat.setRequestedProjection(job, projection);    job.setMapperClass(TestSpecificInputOutputFormat.MyMapper2.class);    job.setNumReduceTasks(0);    job.setOutputFormatClass(AvroParquetOutputFormat.class);    AvroParquetOutputFormat.setOutputPath(job, outputPath);    AvroParquetOutputFormat.setSchema(job, Car.SCHEMA$);    waitForJob(job);    final Path mapperOutput = new Path(outputPath.toString(), "part-m-00000.parquet");    try (final AvroParquetReader<Car> out = new AvroParquetReader<>(mapperOutput)) {        Car car;        Car previousCar = null;        int lineNumber = 0;        while ((car = out.read()) != null) {            if (previousCar != null) {                                assertTrue(car.getModel() == previousCar.getModel());            }                        if (car.getEngine().getType() == EngineType.PETROL) {                fail("UnboundRecordFilter failed to remove cars with PETROL engines");            }                        Car expectedCar = nextRecord(lineNumber * 2);                                    expectedCar.setOptionalExtra(null);            assertEquals("line " + lineNumber, expectedCar, car);            ++lineNumber;            previousCar = car;        }    }}
0
public void testReadWriteChangedCar() throws Exception
{    final Job job = new Job(conf, "read changed/short");    job.setInputFormatClass(AvroParquetInputFormat.class);    AvroParquetInputFormat.setInputPaths(job, parquetPath);        AvroParquetInputFormat.setUnboundRecordFilter(job, ElectricCarFilter.class);            Schema projection = Schema.createRecord(Car.SCHEMA$.getName(), Car.SCHEMA$.getDoc(), Car.SCHEMA$.getNamespace(), false);    List<Schema.Field> fields = Lists.newArrayList();    for (Schema.Field field : Car.SCHEMA$.getFields()) {                if ("engine".equals(field.name()) || "year".equals(field.name()) || "vin".equals(field.name())) {            fields.add(new Schema.Field(field.name(), field.schema(), field.doc(), field.defaultVal(), field.order()));        }    }    projection.setFields(fields);    AvroParquetInputFormat.setRequestedProjection(job, projection);    AvroParquetInputFormat.setAvroReadSchema(job, ShortCar.SCHEMA$);    job.setMapperClass(TestSpecificInputOutputFormat.MyMapperShort.class);    job.setNumReduceTasks(0);    job.setOutputFormatClass(AvroParquetOutputFormat.class);    AvroParquetOutputFormat.setOutputPath(job, outputPath);    AvroParquetOutputFormat.setSchema(job, ShortCar.SCHEMA$);    waitForJob(job);    final Path mapperOutput = new Path(outputPath.toString(), "part-m-00000.parquet");    try (final AvroParquetReader<ShortCar> out = new AvroParquetReader<>(mapperOutput)) {        ShortCar car;        int lineNumber = 0;        while ((car = out.read()) != null) {                                    Car expectedCar = nextRecord(lineNumber * 2);                        assertNull(car.getMake());            assertEquals(car.getEngine(), expectedCar.getEngine());            assertEquals(car.getYear(), expectedCar.getYear());            assertEquals(car.getVin(), expectedCar.getVin());            ++lineNumber;        }    }}
0
private void waitForJob(Job job) throws Exception
{    job.submit();    while (!job.isComplete()) {                sleep(100);    }        if (!job.isSuccessful()) {        throw new RuntimeException("job failed " + job.getJobName());    }}
1
public void deleteOutputFile() throws IOException
{    final FileSystem fileSystem = parquetPath.getFileSystem(conf);    fileSystem.delete(parquetPath, true);    fileSystem.delete(outputPath, true);}
0
public static Collection<Object[]> data()
{    Object[][] data = new Object[][] {     { false },     { true } };    return Arrays.asList(data);}
0
public void testCompatReadWriteSpecific() throws IOException
{    Path path = writeCarsToParquetFile(10, CompressionCodecName.UNCOMPRESSED, false);    try (ParquetReader<Car> reader = new AvroParquetReader<>(testConf, path)) {        for (int i = 0; i < 10; i++) {            assertEquals(getVwPolo().toString(), reader.read().toString());            assertEquals(getVwPassat().toString(), reader.read().toString());            assertEquals(getBmwMini().toString(), reader.read().toString());        }        assertNull(reader.read());    }}
0
public void testReadWriteSpecificWithDictionary() throws IOException
{    Path path = writeCarsToParquetFile(10, CompressionCodecName.UNCOMPRESSED, true);    try (ParquetReader<Car> reader = new AvroParquetReader<>(testConf, path)) {        for (int i = 0; i < 10; i++) {            assertEquals(getVwPolo().toString(), reader.read().toString());            assertEquals(getVwPassat().toString(), reader.read().toString());            assertEquals(getBmwMini().toString(), reader.read().toString());        }        assertNull(reader.read());    }}
0
public void testFilterMatchesMultiple() throws IOException
{    Path path = writeCarsToParquetFile(10, CompressionCodecName.UNCOMPRESSED, false);    try (ParquetReader<Car> reader = new AvroParquetReader<>(testConf, path, column("make", equalTo("Volkswagen")))) {        for (int i = 0; i < 10; i++) {            assertEquals(getVwPolo().toString(), reader.read().toString());            assertEquals(getVwPassat().toString(), reader.read().toString());        }        assertNull(reader.read());    }}
0
public void testFilterMatchesMultipleBlocks() throws IOException
{    Path path = writeCarsToParquetFile(10000, CompressionCodecName.UNCOMPRESSED, false, DEFAULT_BLOCK_SIZE / 64, DEFAULT_PAGE_SIZE / 64);    try (ParquetReader<Car> reader = new AvroParquetReader<>(testConf, path, column("make", equalTo("Volkswagen")))) {        for (int i = 0; i < 10000; i++) {            assertEquals(getVwPolo().toString(), reader.read().toString());            assertEquals(getVwPassat().toString(), reader.read().toString());        }        assertNull(reader.read());    }}
0
public void testFilterMatchesNoBlocks() throws IOException
{    Path path = writeCarsToParquetFile(10000, CompressionCodecName.UNCOMPRESSED, false, DEFAULT_BLOCK_SIZE / 64, DEFAULT_PAGE_SIZE / 64);    try (ParquetReader<Car> reader = new AvroParquetReader<>(testConf, path, column("make", equalTo("Bogus")))) {        assertNull(reader.read());    }}
0
public void testFilterMatchesFinalBlockOnly() throws IOException
{    File tmp = File.createTempFile(getClass().getSimpleName(), ".tmp");    tmp.deleteOnExit();    tmp.delete();    Path path = new Path(tmp.getPath());    Car vwPolo = getVwPolo();    Car vwPassat = getVwPassat();    Car bmwMini = getBmwMini();    try (ParquetWriter<Car> writer = new AvroParquetWriter<Car>(path, Car.SCHEMA$, CompressionCodecName.UNCOMPRESSED, DEFAULT_BLOCK_SIZE / 128, DEFAULT_PAGE_SIZE / 128, false)) {        for (int i = 0; i < 10000; i++) {            writer.write(vwPolo);            writer.write(vwPassat);            writer.write(vwPolo);        }                writer.write(bmwMini);    }    try (ParquetReader<Car> reader = new AvroParquetReader<Car>(testConf, path, column("make", equalTo("BMW")))) {        assertEquals(getBmwMini().toString(), reader.read().toString());        assertNull(reader.read());    }}
0
public void testFilterWithDictionary() throws IOException
{    Path path = writeCarsToParquetFile(1, CompressionCodecName.UNCOMPRESSED, true);    try (ParquetReader<Car> reader = new AvroParquetReader<>(testConf, path, column("make", equalTo("Volkswagen")))) {        assertEquals(getVwPolo().toString(), reader.read().toString());        assertEquals(getVwPassat().toString(), reader.read().toString());        assertNull(reader.read());    }}
0
public void testFilterOnSubAttribute() throws IOException
{    Path path = writeCarsToParquetFile(1, CompressionCodecName.UNCOMPRESSED, false);    ParquetReader<Car> reader = new AvroParquetReader<Car>(testConf, path, column("engine.type", equalTo(EngineType.DIESEL)));    assertEquals(reader.read().toString(), getVwPassat().toString());    assertNull(reader.read());    reader = new AvroParquetReader<Car>(testConf, path, column("engine.capacity", equalTo(1.4f)));    assertEquals(getVwPolo().toString(), reader.read().toString());    assertNull(reader.read());    reader = new AvroParquetReader<Car>(testConf, path, column("engine.hasTurboCharger", equalTo(true)));    assertEquals(getBmwMini().toString(), reader.read().toString());    assertNull(reader.read());}
0
public void testProjection() throws IOException
{    Path path = writeCarsToParquetFile(1, CompressionCodecName.UNCOMPRESSED, false);    Configuration conf = new Configuration(testConf);    Schema schema = Car.getClassSchema();    List<Schema.Field> fields = schema.getFields();        List<Schema.Field> projectedFields = new ArrayList<Schema.Field>();    for (Schema.Field field : fields) {        String name = field.name();        if ("optionalExtra".equals(name) || "serviceHistory".equals(name)) {            continue;        }                Schema.Field fieldClone = new Schema.Field(name, field.schema(), field.doc(), field.defaultVal());        projectedFields.add(fieldClone);    }    Schema projectedSchema = Schema.createRecord(schema.getName(), schema.getDoc(), schema.getNamespace(), schema.isError());    projectedSchema.setFields(projectedFields);    AvroReadSupport.setRequestedProjection(conf, projectedSchema);    try (ParquetReader<Car> reader = new AvroParquetReader<Car>(conf, path)) {        for (Car car = reader.read(); car != null; car = reader.read()) {            assertTrue(car.getDoors() == 4 || car.getDoors() == 5);            assertNotNull(car.getEngine());            assertNotNull(car.getMake());            assertNotNull(car.getModel());            assertEquals(2010, car.getYear());            assertNotNull(car.getVin());            assertNull(car.getOptionalExtra());            assertNull(car.getServiceHistory());        }    }}
0
public void testAvroReadSchema() throws IOException
{    Path path = writeCarsToParquetFile(1, CompressionCodecName.UNCOMPRESSED, false);    Configuration conf = new Configuration(testConf);    AvroReadSupport.setAvroReadSchema(conf, NewCar.SCHEMA$);    try (ParquetReader<NewCar> reader = new AvroParquetReader<>(conf, path)) {        for (NewCar car = reader.read(); car != null; car = reader.read()) {            assertNotNull(car.getEngine());            assertNotNull(car.getBrand());            assertEquals(2010, car.getYear());            assertNotNull(car.getVin());            assertNull(car.getDescription());            assertEquals(5, car.getOpt());        }    }}
0
private Path writeCarsToParquetFile(int num, CompressionCodecName compression, boolean enableDictionary) throws IOException
{    return writeCarsToParquetFile(num, compression, enableDictionary, DEFAULT_BLOCK_SIZE, DEFAULT_PAGE_SIZE);}
0
private Path writeCarsToParquetFile(int num, CompressionCodecName compression, boolean enableDictionary, int blockSize, int pageSize) throws IOException
{    File tmp = File.createTempFile(getClass().getSimpleName(), ".tmp");    tmp.deleteOnExit();    tmp.delete();    Path path = new Path(tmp.getPath());    Car vwPolo = getVwPolo();    Car vwPassat = getVwPassat();    Car bmwMini = getBmwMini();    try (ParquetWriter<Car> writer = new AvroParquetWriter<>(path, Car.SCHEMA$, compression, blockSize, pageSize, enableDictionary)) {        for (int i = 0; i < num; i++) {            writer.write(vwPolo);            writer.write(vwPassat);            writer.write(bmwMini);        }    }    return path;}
0
public static Car getVwPolo()
{    String vin = "WVWDB4505LK000001";    return Car.newBuilder().setYear(2010).setRegistration("A123 GTR").setMake("Volkswagen").setModel("Polo").setVin(new Vin(vin.getBytes())).setDoors(4).setEngine(Engine.newBuilder().setType(EngineType.PETROL).setCapacity(1.4f).setHasTurboCharger(false).build()).setOptionalExtra(Stereo.newBuilder().setMake("Blaupunkt").setSpeakers(4).build()).setServiceHistory(ImmutableList.of(Service.newBuilder().setDate(1325376000l).setMechanic("Jim").build(), Service.newBuilder().setDate(1356998400l).setMechanic("Mike").build())).build();}
0
public static Car getVwPassat()
{    String vin = "WVWDB4505LK000002";    return Car.newBuilder().setYear(2010).setRegistration("A123 GXR").setMake("Volkswagen").setModel("Passat").setVin(new Vin(vin.getBytes())).setDoors(5).setEngine(Engine.newBuilder().setType(EngineType.DIESEL).setCapacity(2.0f).setHasTurboCharger(false).build()).setOptionalExtra(LeatherTrim.newBuilder().setColour("Black").build()).setServiceHistory(ImmutableList.of(Service.newBuilder().setDate(1325376000l).setMechanic("Jim").build())).build();}
0
public static Car getBmwMini()
{    String vin = "WBABA91060AL00003";    return Car.newBuilder().setYear(2010).setRegistration("A124 GSR").setMake("BMW").setModel("Mini").setVin(new Vin(vin.getBytes())).setDoors(4).setEngine(Engine.newBuilder().setType(EngineType.PETROL).setCapacity(1.6f).setHasTurboCharger(true).build()).setOptionalExtra(null).setServiceHistory(ImmutableList.of(Service.newBuilder().setDate(1356998400l).setMechanic("Mike").build())).build();}
0
public static void readSchemaFile() throws IOException
{    TestStringBehavior.SCHEMA = new Schema.Parser().parse(Resources.getResource("stringBehavior.avsc").openStream());}
0
public void writeDataFiles() throws IOException
{        GenericRecord record = new GenericRecordBuilder(SCHEMA).set("default_class", "default").set("string_class", "string").set("stringable_class", BIG_DECIMAL.toString()).set("default_map", ImmutableMap.of("default_key", 34)).set("string_map", ImmutableMap.of("string_key", 35)).set("stringable_map", ImmutableMap.of(BIG_DECIMAL.toString(), 36)).build();    File file = temp.newFile("parquet");    file.delete();    file.deleteOnExit();    parquetFile = new Path(file.getPath());    try (ParquetWriter<GenericRecord> parquet = AvroParquetWriter.<GenericRecord>builder(parquetFile).withDataModel(GenericData.get()).withSchema(SCHEMA).build()) {        parquet.write(record);    }    avroFile = temp.newFile("avro");    avroFile.delete();    avroFile.deleteOnExit();    try (DataFileWriter<GenericRecord> avro = new DataFileWriter<GenericRecord>(new GenericDatumWriter<>(SCHEMA)).create(SCHEMA, avroFile)) {        avro.append(record);    }}
0
public void testGeneric() throws IOException
{    GenericRecord avroRecord;    try (DataFileReader<GenericRecord> avro = new DataFileReader<>(avroFile, new GenericDatumReader<>(SCHEMA))) {        avroRecord = avro.next();    }    GenericRecord parquetRecord;    Configuration conf = new Configuration();    conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, false);    AvroReadSupport.setAvroDataSupplier(conf, GenericDataSupplier.class);    AvroReadSupport.setAvroReadSchema(conf, SCHEMA);    try (ParquetReader<GenericRecord> parquet = AvroParquetReader.<GenericRecord>builder(parquetFile).withConf(conf).build()) {        parquetRecord = parquet.read();    }    Assert.assertEquals("Avro default string class should be Utf8", Utf8.class, avroRecord.get("default_class").getClass());    Assert.assertEquals("Parquet default string class should be Utf8", Utf8.class, parquetRecord.get("default_class").getClass());    Assert.assertEquals("Avro avro.java.string=String class should be String", String.class, avroRecord.get("string_class").getClass());    Assert.assertEquals("Parquet avro.java.string=String class should be String", String.class, parquetRecord.get("string_class").getClass());    Assert.assertEquals("Avro stringable class should be Utf8", Utf8.class, avroRecord.get("stringable_class").getClass());    Assert.assertEquals("Parquet stringable class should be Utf8", Utf8.class, parquetRecord.get("stringable_class").getClass());    Assert.assertEquals("Avro map default string class should be Utf8", Utf8.class, keyClass(avroRecord.get("default_map")));    Assert.assertEquals("Parquet map default string class should be Utf8", Utf8.class, keyClass(parquetRecord.get("default_map")));    Assert.assertEquals("Avro map avro.java.string=String class should be String", String.class, keyClass(avroRecord.get("string_map")));    Assert.assertEquals("Parquet map avro.java.string=String class should be String", String.class, keyClass(parquetRecord.get("string_map")));    Assert.assertEquals("Avro map stringable class should be Utf8", Utf8.class, keyClass(avroRecord.get("stringable_map")));    Assert.assertEquals("Parquet map stringable class should be Utf8", Utf8.class, keyClass(parquetRecord.get("stringable_map")));}
0
public void testSpecific() throws IOException
{    org.apache.parquet.avro.StringBehaviorTest avroRecord;    try (DataFileReader<org.apache.parquet.avro.StringBehaviorTest> avro = new DataFileReader<>(avroFile, new SpecificDatumReader<>(org.apache.parquet.avro.StringBehaviorTest.getClassSchema()))) {        avroRecord = avro.next();    }    org.apache.parquet.avro.StringBehaviorTest parquetRecord;    Configuration conf = new Configuration();    conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, false);    AvroReadSupport.setAvroDataSupplier(conf, SpecificDataSupplier.class);    AvroReadSupport.setAvroReadSchema(conf, org.apache.parquet.avro.StringBehaviorTest.getClassSchema());    try (ParquetReader<org.apache.parquet.avro.StringBehaviorTest> parquet = AvroParquetReader.<org.apache.parquet.avro.StringBehaviorTest>builder(parquetFile).withConf(conf).build()) {        parquetRecord = parquet.read();    }    Assert.assertEquals("Avro default string class should be String", Utf8.class, avroRecord.default_class.getClass());    Assert.assertEquals("Parquet default string class should be String", Utf8.class, parquetRecord.default_class.getClass());    Assert.assertEquals("Avro avro.java.string=String class should be String", String.class, avroRecord.string_class.getClass());    Assert.assertEquals("Parquet avro.java.string=String class should be String", String.class, parquetRecord.string_class.getClass());    Assert.assertEquals("Avro stringable class should be BigDecimal", BigDecimal.class, avroRecord.stringable_class.getClass());    Assert.assertEquals("Parquet stringable class should be BigDecimal", BigDecimal.class, parquetRecord.stringable_class.getClass());    Assert.assertEquals("Should have the correct BigDecimal value", BIG_DECIMAL, parquetRecord.stringable_class);    Assert.assertEquals("Avro map default string class should be String", Utf8.class, keyClass(avroRecord.default_map));    Assert.assertEquals("Parquet map default string class should be String", Utf8.class, keyClass(parquetRecord.default_map));    Assert.assertEquals("Avro map avro.java.string=String class should be String", String.class, keyClass(avroRecord.string_map));    Assert.assertEquals("Parquet map avro.java.string=String class should be String", String.class, keyClass(parquetRecord.string_map));    Assert.assertEquals("Avro map stringable class should be BigDecimal", BigDecimal.class, keyClass(avroRecord.stringable_map));    Assert.assertEquals("Parquet map stringable class should be BigDecimal", BigDecimal.class, keyClass(parquetRecord.stringable_map));}
0
public void testReflect() throws IOException
{    Schema reflectSchema = ReflectData.get().getSchema(ReflectRecord.class);    ReflectRecord avroRecord;    try (DataFileReader<ReflectRecord> avro = new DataFileReader<>(avroFile, new ReflectDatumReader<>(reflectSchema))) {        avroRecord = avro.next();    }    ReflectRecord parquetRecord;    Configuration conf = new Configuration();    conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, false);    AvroReadSupport.setAvroDataSupplier(conf, ReflectDataSupplier.class);    AvroReadSupport.setAvroReadSchema(conf, reflectSchema);    try (ParquetReader<ReflectRecord> parquet = AvroParquetReader.<ReflectRecord>builder(parquetFile).withConf(conf).build()) {        parquetRecord = parquet.read();    }    Assert.assertEquals("Avro default string class should be String", String.class, avroRecord.default_class.getClass());    Assert.assertEquals("Parquet default string class should be String", String.class, parquetRecord.default_class.getClass());    Assert.assertEquals("Avro avro.java.string=String class should be String", String.class, avroRecord.string_class.getClass());    Assert.assertEquals("Parquet avro.java.string=String class should be String", String.class, parquetRecord.string_class.getClass());    Assert.assertEquals("Avro stringable class should be BigDecimal", BigDecimal.class, avroRecord.stringable_class.getClass());    Assert.assertEquals("Parquet stringable class should be BigDecimal", BigDecimal.class, parquetRecord.stringable_class.getClass());    Assert.assertEquals("Should have the correct BigDecimal value", BIG_DECIMAL, parquetRecord.stringable_class);    Assert.assertEquals("Avro map default string class should be String", String.class, keyClass(avroRecord.default_map));    Assert.assertEquals("Parquet map default string class should be String", String.class, keyClass(parquetRecord.default_map));    Assert.assertEquals("Avro map avro.java.string=String class should be String", String.class, keyClass(avroRecord.string_map));    Assert.assertEquals("Parquet map avro.java.string=String class should be String", String.class, keyClass(parquetRecord.string_map));    Assert.assertEquals("Avro map stringable class should be BigDecimal", BigDecimal.class, keyClass(avroRecord.stringable_map));    Assert.assertEquals("Parquet map stringable class should be BigDecimal", BigDecimal.class, keyClass(parquetRecord.stringable_map));}
0
public void testReflectJavaClass() throws IOException
{    Schema reflectSchema = ReflectData.get().getSchema(ReflectRecordJavaClass.class);    System.err.println("Schema: " + reflectSchema.toString(true));    ReflectRecordJavaClass avroRecord;    try (DataFileReader<ReflectRecordJavaClass> avro = new DataFileReader<>(avroFile, new ReflectDatumReader<>(reflectSchema))) {        avroRecord = avro.next();    }    ReflectRecordJavaClass parquetRecord;    Configuration conf = new Configuration();    conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, false);    AvroReadSupport.setAvroDataSupplier(conf, ReflectDataSupplier.class);    AvroReadSupport.setAvroReadSchema(conf, reflectSchema);    AvroReadSupport.setRequestedProjection(conf, reflectSchema);    try (ParquetReader<ReflectRecordJavaClass> parquet = AvroParquetReader.<ReflectRecordJavaClass>builder(parquetFile).withConf(conf).build()) {        parquetRecord = parquet.read();    }        Assert.assertEquals("Avro default string class should be String", String.class, avroRecord.default_class.getClass());    Assert.assertEquals("Parquet default string class should be String", String.class, parquetRecord.default_class.getClass());    Assert.assertEquals("Avro stringable class should be BigDecimal", BigDecimal.class, avroRecord.stringable_class.getClass());    Assert.assertEquals("Parquet stringable class should be BigDecimal", BigDecimal.class, parquetRecord.stringable_class.getClass());    Assert.assertEquals("Should have the correct BigDecimal value", BIG_DECIMAL, parquetRecord.stringable_class);}
0
public static Class<?> keyClass(Object obj)
{    Assert.assertTrue("Should be a map", obj instanceof Map);    Map<?, ?> map = (Map<?, ?>) obj;    return Iterables.getFirst(map.keySet(), null).getClass();}
0
public static void deleteIfExists(Configuration conf, Path path)
{    try {        FileSystem fs = path.getFileSystem(conf);        if (fs.exists(path)) {            if (!fs.delete(path, true)) {                System.err.println("Couldn't delete " + path);            }        }    } catch (IOException e) {        System.err.println("Couldn't delete " + path);        e.printStackTrace();    }}
0
public static boolean exists(Configuration conf, Path path) throws IOException
{    FileSystem fs = path.getFileSystem(conf);    return fs.exists(path);}
0
public void generateAll()
{    try {        generateData(file_1M, configuration, PARQUET_2_0, BLOCK_SIZE_DEFAULT, PAGE_SIZE_DEFAULT, FIXED_LEN_BYTEARRAY_SIZE, UNCOMPRESSED, ONE_MILLION);                generateData(file_1M_BS256M_PS4M, configuration, PARQUET_2_0, BLOCK_SIZE_256M, PAGE_SIZE_4M, FIXED_LEN_BYTEARRAY_SIZE, UNCOMPRESSED, ONE_MILLION);        generateData(file_1M_BS256M_PS8M, configuration, PARQUET_2_0, BLOCK_SIZE_256M, PAGE_SIZE_8M, FIXED_LEN_BYTEARRAY_SIZE, UNCOMPRESSED, ONE_MILLION);        generateData(file_1M_BS512M_PS4M, configuration, PARQUET_2_0, BLOCK_SIZE_512M, PAGE_SIZE_4M, FIXED_LEN_BYTEARRAY_SIZE, UNCOMPRESSED, ONE_MILLION);        generateData(file_1M_BS512M_PS8M, configuration, PARQUET_2_0, BLOCK_SIZE_512M, PAGE_SIZE_8M, FIXED_LEN_BYTEARRAY_SIZE, UNCOMPRESSED, ONE_MILLION);                        generateData(file_1M_SNAPPY, configuration, PARQUET_2_0, BLOCK_SIZE_DEFAULT, PAGE_SIZE_DEFAULT, FIXED_LEN_BYTEARRAY_SIZE, SNAPPY, ONE_MILLION);        generateData(file_1M_GZIP, configuration, PARQUET_2_0, BLOCK_SIZE_DEFAULT, PAGE_SIZE_DEFAULT, FIXED_LEN_BYTEARRAY_SIZE, GZIP, ONE_MILLION);    } catch (IOException e) {        throw new RuntimeException(e);    }}
0
public void generateData(Path outFile, Configuration configuration, ParquetProperties.WriterVersion version, int blockSize, int pageSize, int fixedLenByteArraySize, CompressionCodecName codec, int nRows) throws IOException
{    if (exists(configuration, outFile)) {        System.out.println("File already exists " + outFile);        return;    }    System.out.println("Generating data @ " + outFile);    MessageType schema = parseMessageType("message test { " + "required binary binary_field; " + "required int32 int32_field; " + "required int64 int64_field; " + "required boolean boolean_field; " + "required float float_field; " + "required double double_field; " + "required fixed_len_byte_array(" + fixedLenByteArraySize + ") flba_field; " + "required int96 int96_field; " + "} ");    GroupWriteSupport.setSchema(schema, configuration);    SimpleGroupFactory f = new SimpleGroupFactory(schema);    ParquetWriter<Group> writer = new ParquetWriter<Group>(outFile, new GroupWriteSupport(), codec, blockSize, pageSize, DICT_PAGE_SIZE, true, false, version, configuration);        char[] chars = new char[fixedLenByteArraySize];    Arrays.fill(chars, '*');    for (int i = 0; i < nRows; i++) {        writer.write(f.newGroup().append("binary_field", randomUUID().toString()).append("int32_field", i).append("int64_field", 64l).append("boolean_field", true).append("float_field", 1.0f).append("double_field", 2.0d).append("flba_field", new String(chars)).append("int96_field", Binary.fromConstantByteArray(new byte[12])));    }    writer.close();}
0
public void cleanup()
{    deleteIfExists(configuration, targetDir);}
0
public static void main(String[] args)
{    DataGenerator generator = new DataGenerator();    if (args.length < 1) {        System.err.println("Please specify a command (generate|cleanup).");        System.exit(1);    }    String command = args[0];    if (command.equalsIgnoreCase("generate")) {        generator.generateAll();    } else if (command.equalsIgnoreCase("cleanup")) {        generator.cleanup();    } else {        throw new IllegalArgumentException("invalid command " + command);    }}
0
public String toString()
{    try {        FileSystem fs = file.getFileSystem(new Configuration());        long bytes = fs.getFileStatus(file).getLen();        int exp = (int) (Math.log(bytes) / Math.log(1024));        if (exp == 0) {            return Long.toString(bytes);        }        String suffix = SUFFIXES[exp - 1];        return String.format("%d [%.2f%s]", bytes, bytes / Math.pow(1024, exp), suffix);    } catch (IOException e) {        return "N/A";    }}
0
 String nextString()
{    char[] str = new char[MIN_LENGTH + random.nextInt(MAX_LENGTH - MIN_LENGTH)];    for (int i = 0, n = str.length; i < n; ++i) {        str[i] = ALPHABET.charAt(random.nextInt(ALPHABET.length()));    }    return new String(str);}
0
public Builder<T> configureBuilder(Builder<T> builder)
{    return builder;}
0
public String toString()
{    return "DEFAULT";}
0
public org.apache.parquet.hadoop.ParquetWriter.Builder<T, ?> configureBuilder(org.apache.parquet.hadoop.ParquetWriter.Builder<T, ?> builder)
{    return builder;}
0
public String toString()
{    return "DEFAULT";}
0
public Builder<T> configureBuilder(Builder<T> builder)
{    return builder.useColumnIndexFilter(false);}
0
public Builder<T> configureBuilder(Builder<T> builder)
{    return builder.useColumnIndexFilter(true);}
0
public static void arrangeToBuckets(long[] data, int bucketCnt)
{    long bucketSize = (long) (Long.MAX_VALUE / (bucketCnt / 2.0));    long[] bucketBorders = new long[bucketCnt - 1];    for (int i = 0, n = bucketBorders.length; i < n; ++i) {        bucketBorders[i] = Long.MIN_VALUE + (i + 1) * bucketSize;    }    LongList[] buckets = new LongList[bucketCnt];    for (int i = 0; i < bucketCnt; ++i) {        buckets[i] = new LongArrayList(data.length / bucketCnt);    }    for (int i = 0, n = data.length; i < n; ++i) {        long value = data[i];        int bucket = Arrays.binarySearch(bucketBorders, value);        if (bucket < 0) {            bucket = -(bucket + 1);        }        buckets[bucket].add(value);    }    int offset = 0;    int mid = bucketCnt / 2;    for (int i = 0; i < bucketCnt; ++i) {        int bucketIndex;        if (i % 2 == 0) {            bucketIndex = mid + i / 2;        } else {            bucketIndex = mid - i / 2 - 1;        }        LongList bucket = buckets[bucketIndex];        bucket.getElements(0, data, offset, bucket.size());        offset += bucket.size();    }}
0
 void arrangeData(long[] data)
{    Arrays.parallelSort(data);}
0
 void arrangeData(long[] data)
{    arrangeToBuckets(data, 9);}
0
 void arrangeData(long[] data)
{    arrangeToBuckets(data, 5);}
0
 void arrangeData(long[] data)
{    arrangeToBuckets(data, 3);}
0
 void arrangeData(long[] data)
{}
0
public ParquetWriter.Builder<T, ?> configureBuilder(ParquetWriter.Builder<T, ?> builder)
{    return builder.withPageSize(    Integer.MAX_VALUE).withPageRowCountLimit(1_000);}
0
public ParquetWriter.Builder<T, ?> configureBuilder(ParquetWriter.Builder<T, ?> builder)
{    return builder.withPageSize(    Integer.MAX_VALUE).withPageRowCountLimit(10_000);}
0
public ParquetWriter.Builder<T, ?> configureBuilder(ParquetWriter.Builder<T, ?> builder)
{    return builder.withPageSize(    Integer.MAX_VALUE).withPageRowCountLimit(50_000);}
0
public ParquetWriter.Builder<T, ?> configureBuilder(ParquetWriter.Builder<T, ?> builder)
{    return builder.withPageSize(    Integer.MAX_VALUE).withPageRowCountLimit(100_000);}
0
public void writeFile() throws IOException
{    WriteConfigurator writeConfigurator = getWriteConfigurator();    file = new Path(Files.createTempFile("benchmark-filtering_" + characteristic + '_' + writeConfigurator + '_', ".parquet").toAbsolutePath().toString());    long[] data = generateData();    characteristic.arrangeData(data);    try (ParquetWriter<Group> writer = writeConfigurator.configureBuilder(ExampleParquetWriter.builder(file).config(GroupWriteSupport.PARQUET_EXAMPLE_SCHEMA, SCHEMA.toString()).withRowGroupSize(    Integer.MAX_VALUE).withWriteMode(OVERWRITE)).build()) {        for (long value : data) {            Group group = new SimpleGroup(SCHEMA);            group.add(0, value);            group.add(1, Binary.fromString(dummyGenerator.nextString()));            group.add(2, Binary.fromString(dummyGenerator.nextString()));            group.add(3, Binary.fromString(dummyGenerator.nextString()));            group.add(4, Binary.fromString(dummyGenerator.nextString()));            group.add(5, Binary.fromString(dummyGenerator.nextString()));            writer.write(group);        }    }}
0
 WriteConfigurator getWriteConfigurator()
{    return WriteConfigurator.DEFAULT;}
0
 ReadConfigurator getReadConfigurator()
{    return ReadConfigurator.DEFAULT;}
0
private long[] generateData()
{    Random random = new Random(43);    long[] data = new long[RECORD_COUNT];    for (int i = 0, n = data.length; i < n; ++i) {        data[i] = random.nextLong();    }    return data;}
0
public void resetRandom()
{    random = new Random(42);    dummyGenerator = new StringGenerator();}
0
public void gc()
{    System.gc();}
0
public void deleteFile() throws IOException
{        file.getFileSystem(new Configuration()).delete(file, false);}
1
public ParquetReader.Builder<Group> createReaderBuilder() throws IOException
{    ReadConfigurator readConfigurator = getReadConfigurator();    return readConfigurator.configureBuilder(new ParquetReader.Builder<Group>(HadoopInputFile.fromPath(file, new Configuration())) {        @Override        protected ReadSupport<Group> getReadSupport() {            return new GroupReadSupport();        }    }.set(GroupWriteSupport.PARQUET_EXAMPLE_SCHEMA, SCHEMA.toString()));}
0
protected ReadSupport<Group> getReadSupport()
{    return new GroupReadSupport();}
0
public Random getRandom()
{    return random;}
0
 ReadConfigurator getReadConfigurator()
{    return columnIndexUsage;}
0
 WriteConfigurator getWriteConfigurator()
{    return pageRowLimit;}
0
 ReadConfigurator getReadConfigurator()
{    return ColumnIndexUsage.WITH_COLUMN_INDEX;}
0
public void benchmarkWithOrWithoutColumnIndex(Blackhole blackhole, WithOrWithoutColumnIndexContext context) throws Exception
{    benchmark(blackhole, context);}
0
public void benchmarkPageSize(Blackhole blackhole, PageSizeContext context) throws Exception
{    benchmark(blackhole, context);}
0
private void benchmark(Blackhole blackhole, BaseContext context) throws Exception
{    FilterPredicate filter = FilterApi.eq(BaseContext.COLUMN, context.getRandom().nextLong());    try (ParquetReader<Group> reader = context.createReaderBuilder().withFilter(FilterCompat.get(filter)).build()) {        blackhole.consume(reader.read());    }}
0
public boolean supportsBlockSize()
{    return false;}
0
public long defaultBlockSize()
{    return -1L;}
0
public PositionOutputStream createOrOverwrite(long blockSizeHint)
{    return create(blockSizeHint);}
0
public PositionOutputStream create(long blockSizeHint)
{    return new PositionOutputStream() {        private long pos;        @Override        public long getPos() throws IOException {            return pos;        }        @Override        public void write(int b) throws IOException {            ++pos;        }    };}
0
public long getPos() throws IOException
{    return pos;}
0
public void write(int b) throws IOException
{    ++pos;}
0
public Group nextValue()
{    if (random.nextDouble() > NULL_RATIO) {        Group group = FACTORY.newGroup();        group.addGroup("int_list").addGroup("list").append("element", random.nextInt());        return group;    } else {        return NULL;    }}
0
public void benchmarkWriting() throws IOException
{    ValueGenerator generator = new ValueGenerator();    try (ParquetWriter<Group> writer = ExampleParquetWriter.builder(BLACK_HOLE).withWriteMode(Mode.OVERWRITE).withType(SCHEMA).build()) {        for (int i = 0; i < RECORD_COUNT; ++i) {            writer.write(generator.nextValue());        }    }}
0
public void generateData(Path outFile, int nRows, boolean writeChecksums, CompressionCodecName compression) throws IOException
{    if (exists(configuration, outFile)) {        System.out.println("File already exists " + outFile);        return;    }    ParquetWriter<Group> writer = ExampleParquetWriter.builder(outFile).withConf(configuration).withWriteMode(ParquetFileWriter.Mode.OVERWRITE).withCompressionCodec(compression).withDictionaryEncoding(true).withType(SCHEMA).withPageWriteChecksumEnabled(writeChecksums).build();    GroupFactory groupFactory = new SimpleGroupFactory(SCHEMA);    Random rand = new Random(42);    for (int i = 0; i < nRows; i++) {        Group group = groupFactory.newGroup();        group.append("long_field", (long) i).append("binary_field", randomUUID().toString()).addGroup("group").append("int_field", rand.nextInt() % 100).append("int_field", rand.nextInt() % 100).append("int_field", rand.nextInt() % 100).append("int_field", rand.nextInt() % 100);        writer.write(group);    }    writer.close();}
0
public void generateAll()
{    try {                        generateData(file_100K_CHECKSUMS_UNCOMPRESSED, 100 * ONE_K, true, UNCOMPRESSED);        generateData(file_100K_CHECKSUMS_GZIP, 100 * ONE_K, true, GZIP);        generateData(file_100K_CHECKSUMS_SNAPPY, 100 * ONE_K, true, SNAPPY);        generateData(file_1M_CHECKSUMS_UNCOMPRESSED, ONE_MILLION, true, UNCOMPRESSED);        generateData(file_1M_CHECKSUMS_GZIP, ONE_MILLION, true, GZIP);        generateData(file_1M_CHECKSUMS_SNAPPY, ONE_MILLION, true, SNAPPY);        generateData(file_10M_CHECKSUMS_UNCOMPRESSED, 10 * ONE_MILLION, true, UNCOMPRESSED);        generateData(file_10M_CHECKSUMS_GZIP, 10 * ONE_MILLION, true, GZIP);        generateData(file_10M_CHECKSUMS_SNAPPY, 10 * ONE_MILLION, true, SNAPPY);    } catch (IOException e) {        throw new RuntimeException(e);    }}
0
public void setup()
{    pageChecksumDataGenerator.generateAll();}
0
private void readFile(Path file, int nRows, boolean verifyChecksums, Blackhole blackhole) throws IOException
{    try (ParquetReader<Group> reader = ParquetReader.builder(new GroupReadSupport(), file).withConf(configuration).usePageChecksumVerification(verifyChecksums).build()) {        for (int i = 0; i < nRows; i++) {            Group group = reader.read();            blackhole.consume(group.getLong("long_field", 0));            blackhole.consume(group.getBinary("binary_field", 0));            Group subgroup = group.getGroup("group", 0);            blackhole.consume(subgroup.getInteger("int_field", 0));            blackhole.consume(subgroup.getInteger("int_field", 1));            blackhole.consume(subgroup.getInteger("int_field", 2));            blackhole.consume(subgroup.getInteger("int_field", 3));        }    }}
0
public void read100KRowsUncompressedWithoutVerification(Blackhole blackhole) throws IOException
{    readFile(file_100K_CHECKSUMS_UNCOMPRESSED, 100 * ONE_K, false, blackhole);}
0
public void read100KRowsUncompressedWithVerification(Blackhole blackhole) throws IOException
{    readFile(file_100K_CHECKSUMS_UNCOMPRESSED, 100 * ONE_K, true, blackhole);}
0
public void read100KRowsGzipWithoutVerification(Blackhole blackhole) throws IOException
{    readFile(file_100K_CHECKSUMS_GZIP, 100 * ONE_K, false, blackhole);}
0
public void read100KRowsGzipWithVerification(Blackhole blackhole) throws IOException
{    readFile(file_100K_CHECKSUMS_GZIP, 100 * ONE_K, true, blackhole);}
0
public void read100KRowsSnappyWithoutVerification(Blackhole blackhole) throws IOException
{    readFile(file_100K_CHECKSUMS_SNAPPY, 100 * ONE_K, false, blackhole);}
0
public void read100KRowsSnappyWithVerification(Blackhole blackhole) throws IOException
{    readFile(file_100K_CHECKSUMS_SNAPPY, 100 * ONE_K, true, blackhole);}
0
public void read1MRowsUncompressedWithoutVerification(Blackhole blackhole) throws IOException
{    readFile(file_1M_CHECKSUMS_UNCOMPRESSED, ONE_MILLION, false, blackhole);}
0
public void read1MRowsUncompressedWithVerification(Blackhole blackhole) throws IOException
{    readFile(file_1M_CHECKSUMS_UNCOMPRESSED, ONE_MILLION, true, blackhole);}
0
public void read1MRowsGzipWithoutVerification(Blackhole blackhole) throws IOException
{    readFile(file_1M_CHECKSUMS_GZIP, ONE_MILLION, false, blackhole);}
0
public void read1MRowsGzipWithVerification(Blackhole blackhole) throws IOException
{    readFile(file_1M_CHECKSUMS_GZIP, ONE_MILLION, true, blackhole);}
0
public void read1MRowsSnappyWithoutVerification(Blackhole blackhole) throws IOException
{    readFile(file_1M_CHECKSUMS_SNAPPY, ONE_MILLION, false, blackhole);}
0
public void read1MRowsSnappyWithVerification(Blackhole blackhole) throws IOException
{    readFile(file_1M_CHECKSUMS_SNAPPY, ONE_MILLION, true, blackhole);}
0
public void read10MRowsUncompressedWithoutVerification(Blackhole blackhole) throws IOException
{    readFile(file_10M_CHECKSUMS_UNCOMPRESSED, 10 * ONE_MILLION, false, blackhole);}
0
public void read10MRowsUncompressedWithVerification(Blackhole blackhole) throws IOException
{    readFile(file_10M_CHECKSUMS_UNCOMPRESSED, 10 * ONE_MILLION, true, blackhole);}
0
public void read10MRowsGzipWithoutVerification(Blackhole blackhole) throws IOException
{    readFile(file_10M_CHECKSUMS_GZIP, 10 * ONE_MILLION, false, blackhole);}
0
public void read10MRowsGzipWithVerification(Blackhole blackhole) throws IOException
{    readFile(file_10M_CHECKSUMS_GZIP, 10 * ONE_MILLION, true, blackhole);}
0
public void read10MRowsSnappyWithoutVerification(Blackhole blackhole) throws IOException
{    readFile(file_10M_CHECKSUMS_SNAPPY, 10 * ONE_MILLION, false, blackhole);}
0
public void read10MRowsSnappyWithVerification(Blackhole blackhole) throws IOException
{    readFile(file_10M_CHECKSUMS_SNAPPY, 10 * ONE_MILLION, true, blackhole);}
0
public void setup()
{    pageChecksumDataGenerator.cleanup();}
0
public void write100KRowsUncompressedWithoutChecksums() throws IOException
{    pageChecksumDataGenerator.generateData(file_100K_NOCHECKSUMS_UNCOMPRESSED, 100 * ONE_K, false, UNCOMPRESSED);}
0
public void write100KRowsUncompressedWithChecksums() throws IOException
{    pageChecksumDataGenerator.generateData(file_100K_CHECKSUMS_UNCOMPRESSED, 100 * ONE_K, true, UNCOMPRESSED);}
0
public void write100KRowsGzipWithoutChecksums() throws IOException
{    pageChecksumDataGenerator.generateData(file_100K_NOCHECKSUMS_GZIP, 100 * ONE_K, false, GZIP);}
0
public void write100KRowsGzipWithChecksums() throws IOException
{    pageChecksumDataGenerator.generateData(file_100K_CHECKSUMS_GZIP, 100 * ONE_K, true, GZIP);}
0
public void write100KRowsSnappyWithoutChecksums() throws IOException
{    pageChecksumDataGenerator.generateData(file_100K_NOCHECKSUMS_SNAPPY, 100 * ONE_K, false, SNAPPY);}
0
public void write100KRowsSnappyWithChecksums() throws IOException
{    pageChecksumDataGenerator.generateData(file_100K_CHECKSUMS_SNAPPY, 100 * ONE_K, true, SNAPPY);}
0
public void write1MRowsUncompressedWithoutChecksums() throws IOException
{    pageChecksumDataGenerator.generateData(file_1M_NOCHECKSUMS_UNCOMPRESSED, ONE_MILLION, false, UNCOMPRESSED);}
0
public void write1MRowsUncompressedWithChecksums() throws IOException
{    pageChecksumDataGenerator.generateData(file_1M_CHECKSUMS_UNCOMPRESSED, ONE_MILLION, true, UNCOMPRESSED);}
0
public void write1MRowsGzipWithoutChecksums() throws IOException
{    pageChecksumDataGenerator.generateData(file_1M_NOCHECKSUMS_GZIP, ONE_MILLION, false, GZIP);}
0
public void write1MRowsGzipWithChecksums() throws IOException
{    pageChecksumDataGenerator.generateData(file_1M_CHECKSUMS_GZIP, ONE_MILLION, true, GZIP);}
0
public void write1MRowsSnappyWithoutChecksums() throws IOException
{    pageChecksumDataGenerator.generateData(file_1M_NOCHECKSUMS_SNAPPY, ONE_MILLION, false, SNAPPY);}
0
public void write1MRowsSnappyWithChecksums() throws IOException
{    pageChecksumDataGenerator.generateData(file_1M_CHECKSUMS_SNAPPY, ONE_MILLION, true, SNAPPY);}
0
public void write10MRowsUncompressedWithoutChecksums() throws IOException
{    pageChecksumDataGenerator.generateData(file_10M_NOCHECKSUMS_UNCOMPRESSED, 10 * ONE_MILLION, false, UNCOMPRESSED);}
0
public void write10MRowsUncompressedWithChecksums() throws IOException
{    pageChecksumDataGenerator.generateData(file_10M_CHECKSUMS_UNCOMPRESSED, 10 * ONE_MILLION, true, UNCOMPRESSED);}
0
public void write10MRowsGzipWithoutChecksums() throws IOException
{    pageChecksumDataGenerator.generateData(file_10M_NOCHECKSUMS_GZIP, 10 * ONE_MILLION, false, GZIP);}
0
public void write10MRowsGzipWithChecksums() throws IOException
{    pageChecksumDataGenerator.generateData(file_10M_CHECKSUMS_GZIP, 10 * ONE_MILLION, true, GZIP);}
0
public void write10MRowsSnappyWithoutChecksums() throws IOException
{    pageChecksumDataGenerator.generateData(file_10M_NOCHECKSUMS_SNAPPY, 10 * ONE_MILLION, false, SNAPPY);}
0
public void write10MRowsSnappyWithChecksums() throws IOException
{    pageChecksumDataGenerator.generateData(file_10M_CHECKSUMS_SNAPPY, 10 * ONE_MILLION, true, SNAPPY);}
0
private void read(Path parquetFile, int nRows, Blackhole blackhole) throws IOException
{    ParquetReader<Group> reader = ParquetReader.builder(new GroupReadSupport(), parquetFile).withConf(configuration).build();    for (int i = 0; i < nRows; i++) {        Group group = reader.read();        blackhole.consume(group.getBinary("binary_field", 0));        blackhole.consume(group.getInteger("int32_field", 0));        blackhole.consume(group.getLong("int64_field", 0));        blackhole.consume(group.getBoolean("boolean_field", 0));        blackhole.consume(group.getFloat("float_field", 0));        blackhole.consume(group.getDouble("double_field", 0));        blackhole.consume(group.getBinary("flba_field", 0));        blackhole.consume(group.getInt96("int96_field", 0));    }    reader.close();}
0
public void generateFilesForRead()
{    new DataGenerator().generateAll();}
0
public void read1MRowsDefaultBlockAndPageSizeUncompressed(Blackhole blackhole) throws IOException
{    read(file_1M, ONE_MILLION, blackhole);}
0
public void read1MRowsBS256MPS4MUncompressed(Blackhole blackhole) throws IOException
{    read(file_1M_BS256M_PS4M, ONE_MILLION, blackhole);}
0
public void read1MRowsBS256MPS8MUncompressed(Blackhole blackhole) throws IOException
{    read(file_1M_BS256M_PS8M, ONE_MILLION, blackhole);}
0
public void read1MRowsBS512MPS4MUncompressed(Blackhole blackhole) throws IOException
{    read(file_1M_BS512M_PS4M, ONE_MILLION, blackhole);}
0
public void read1MRowsBS512MPS8MUncompressed(Blackhole blackhole) throws IOException
{    read(file_1M_BS512M_PS8M, ONE_MILLION, blackhole);}
0
public void read1MRowsDefaultBlockAndPageSizeSNAPPY(Blackhole blackhole) throws IOException
{    read(file_1M_SNAPPY, ONE_MILLION, blackhole);}
0
public void read1MRowsDefaultBlockAndPageSizeGZIP(Blackhole blackhole) throws IOException
{    read(file_1M_GZIP, ONE_MILLION, blackhole);}
0
public void setup()
{        dataGenerator.cleanup();}
0
public void write1MRowsDefaultBlockAndPageSizeUncompressed() throws IOException
{    dataGenerator.generateData(file_1M, configuration, PARQUET_2_0, BLOCK_SIZE_DEFAULT, PAGE_SIZE_DEFAULT, FIXED_LEN_BYTEARRAY_SIZE, UNCOMPRESSED, ONE_MILLION);}
0
public void write1MRowsBS256MPS4MUncompressed() throws IOException
{    dataGenerator.generateData(file_1M_BS256M_PS4M, configuration, PARQUET_2_0, BLOCK_SIZE_256M, PAGE_SIZE_4M, FIXED_LEN_BYTEARRAY_SIZE, UNCOMPRESSED, ONE_MILLION);}
0
public void write1MRowsBS256MPS8MUncompressed() throws IOException
{    dataGenerator.generateData(file_1M_BS256M_PS8M, configuration, PARQUET_2_0, BLOCK_SIZE_256M, PAGE_SIZE_8M, FIXED_LEN_BYTEARRAY_SIZE, UNCOMPRESSED, ONE_MILLION);}
0
public void write1MRowsBS512MPS4MUncompressed() throws IOException
{    dataGenerator.generateData(file_1M_BS512M_PS4M, configuration, PARQUET_2_0, BLOCK_SIZE_512M, PAGE_SIZE_4M, FIXED_LEN_BYTEARRAY_SIZE, UNCOMPRESSED, ONE_MILLION);}
0
public void write1MRowsBS512MPS8MUncompressed() throws IOException
{    dataGenerator.generateData(file_1M_BS512M_PS8M, configuration, PARQUET_2_0, BLOCK_SIZE_512M, PAGE_SIZE_8M, FIXED_LEN_BYTEARRAY_SIZE, UNCOMPRESSED, ONE_MILLION);}
0
public void write1MRowsDefaultBlockAndPageSizeSNAPPY() throws IOException
{    dataGenerator.generateData(file_1M_SNAPPY, configuration, PARQUET_2_0, BLOCK_SIZE_DEFAULT, PAGE_SIZE_DEFAULT, FIXED_LEN_BYTEARRAY_SIZE, SNAPPY, ONE_MILLION);}
0
public void write1MRowsDefaultBlockAndPageSizeGZIP() throws IOException
{    dataGenerator.generateData(file_1M_GZIP, configuration, PARQUET_2_0, BLOCK_SIZE_DEFAULT, PAGE_SIZE_DEFAULT, FIXED_LEN_BYTEARRAY_SIZE, GZIP, ONE_MILLION);}
0
public void sourceConfInit(FlowProcess<JobConf> fp, Tap<JobConf, RecordReader, OutputCollector> tap, JobConf jobConf)
{    super.sourceConfInit(fp, tap, jobConf);    jobConf.setInputFormat(DeprecatedParquetInputFormat.class);    ParquetInputFormat.setReadSupportClass(jobConf, ThriftReadSupport.class);    ThriftReadSupport.setRecordConverterClass(jobConf, TBaseRecordConverter.class);}
0
public void sinkConfInit(FlowProcess<JobConf> fp, Tap<JobConf, RecordReader, OutputCollector> tap, JobConf jobConf)
{    if (this.config.getKlass() == null) {        throw new IllegalArgumentException("To use ParquetTBaseScheme as a sink, you must specify a thrift class in the constructor");    }    DeprecatedParquetOutputFormat.setAsOutputFormat(jobConf);    DeprecatedParquetOutputFormat.setWriteSupportClass(jobConf, TBaseWriteSupport.class);    TBaseWriteSupport.<T>setThriftClass(jobConf, this.config.getKlass());}
0
public void sourceConfInit(FlowProcess<JobConf> fp, Tap<JobConf, RecordReader, OutputCollector> tap, JobConf jobConf)
{    if (filterPredicate != null) {        ParquetInputFormat.setFilterPredicate(jobConf, filterPredicate);    }    jobConf.setInputFormat(DeprecatedParquetInputFormat.class);    ParquetInputFormat.setReadSupportClass(jobConf, TupleReadSupport.class);    TupleReadSupport.setRequestedFields(jobConf, getSourceFields());}
0
public Fields retrieveSourceFields(FlowProcess<JobConf> flowProcess, Tap tap)
{    MessageType schema = readSchema(flowProcess, tap);    SchemaIntersection intersection = new SchemaIntersection(schema, getSourceFields());    setSourceFields(intersection.getSourceFields());    return getSourceFields();}
0
private MessageType readSchema(FlowProcess<JobConf> flowProcess, Tap tap)
{    try {        Hfs hfs;        if (tap instanceof CompositeTap)            hfs = (Hfs) ((CompositeTap) tap).getChildTaps().next();        else            hfs = (Hfs) tap;        List<Footer> footers = getFooters(flowProcess, hfs);        if (footers.isEmpty()) {            throw new TapException("Could not read Parquet metadata at " + hfs.getPath());        } else {            return footers.get(0).getParquetMetadata().getFileMetaData().getSchema();        }    } catch (IOException e) {        throw new TapException(e);    }}
0
private List<Footer> getFooters(FlowProcess<JobConf> flowProcess, Hfs hfs) throws IOException
{    JobConf jobConf = flowProcess.getConfigCopy();    DeprecatedParquetInputFormat format = new DeprecatedParquetInputFormat();    format.addInputPath(jobConf, hfs.getPath());    return format.getFooters(jobConf);}
0
public boolean source(FlowProcess<JobConf> fp, SourceCall<Object[], RecordReader> sc) throws IOException
{    Container<Tuple> value = (Container<Tuple>) sc.getInput().createValue();    boolean hasNext = sc.getInput().next(null, value);    if (!hasNext) {        return false;    }        if (value == null) {        return true;    }    sc.getIncomingEntry().setTuple(value.get());    return true;}
0
public void sinkConfInit(FlowProcess<JobConf> fp, Tap<JobConf, RecordReader, OutputCollector> tap, JobConf jobConf)
{    DeprecatedParquetOutputFormat.setAsOutputFormat(jobConf);    jobConf.set(TupleWriteSupport.PARQUET_CASCADING_SCHEMA, parquetSchema);    ParquetOutputFormat.setWriteSupportClass(jobConf, TupleWriteSupport.class);}
0
public boolean isSink()
{    return parquetSchema != null;}
0
public void sink(FlowProcess<JobConf> fp, SinkCall<Object[], OutputCollector> sink) throws IOException
{    TupleEntry tuple = sink.getOutgoingEntry();    OutputCollector outputCollector = sink.getOutput();    outputCollector.collect(null, tuple);}
0
public FilterPredicate getFilterPredicate()
{    return filterPredicate;}
0
public String getProjectionString()
{    return deprecatedProjectionString;}
0
public String getStrictProjectionString()
{    return strictProjectionString;}
0
public Class<T> getKlass()
{    return klass;}
0
public Config<T> withFilterPredicate(FilterPredicate f)
{    return new Config<T>(this.klass, checkNotNull(f, "filterPredicate"), this.deprecatedProjectionString, this.strictProjectionString);}
0
public Config<T> withProjectionString(String p)
{    return new Config<T>(this.klass, this.filterPredicate, checkNotNull(p, "projectionString"), this.strictProjectionString);}
0
public Config<T> withStrictProjectionString(String p)
{    return new Config<T>(this.klass, this.filterPredicate, this.deprecatedProjectionString, checkNotNull(p, "projectionString"));}
0
public Config<T> withRecordClass(Class<T> klass)
{    return new Config<T>(checkNotNull(klass, "recordClass"), this.filterPredicate, this.deprecatedProjectionString, this.strictProjectionString);}
0
private void setProjectionPushdown(JobConf jobConf)
{    if (this.config.deprecatedProjectionString != null) {        ThriftReadSupport.setProjectionPushdown(jobConf, this.config.deprecatedProjectionString);    }}
0
private void setStrictProjectionPushdown(JobConf jobConf)
{    if (this.config.strictProjectionString != null) {        ThriftReadSupport.setStrictFieldProjectionFilter(jobConf, this.config.strictProjectionString);    }}
0
private void setPredicatePushdown(JobConf jobConf)
{    if (this.config.filterPredicate != null) {        ParquetInputFormat.setFilterPredicate(jobConf, this.config.filterPredicate);    }}
0
public void sourceConfInit(FlowProcess<JobConf> jobConfFlowProcess, Tap<JobConf, RecordReader, OutputCollector> jobConfRecordReaderOutputCollectorTap, final JobConf jobConf)
{    setPredicatePushdown(jobConf);    setProjectionPushdown(jobConf);    setStrictProjectionPushdown(jobConf);    setRecordClass(jobConf);}
0
private void setRecordClass(JobConf jobConf)
{    if (config.klass != null) {        ParquetThriftInputFormat.setThriftClass(jobConf, config.klass);    }}
0
public boolean source(FlowProcess<JobConf> fp, SourceCall<Object[], RecordReader> sc) throws IOException
{    Container<T> value = (Container<T>) sc.getInput().createValue();    boolean hasNext = sc.getInput().next(null, value);    if (!hasNext) {        return false;    }        if (value == null) {        return true;    }    sc.getIncomingEntry().setTuple(new Tuple(value.get()));    return true;}
0
public void sink(FlowProcess<JobConf> fp, SinkCall<Object[], OutputCollector> sc) throws IOException
{    TupleEntry tuple = sc.getOutgoingEntry();    if (tuple.size() != 1) {        throw new RuntimeException("ParquetValueScheme expects tuples with an arity of exactly 1, but found " + tuple.getFields());    }    T value = (T) tuple.getObject(0);    OutputCollector output = sc.getOutput();    output.collect(null, value);}
0
public void testWrite() throws Exception
{    Path path = new Path(parquetOutputPath);    JobConf jobConf = new JobConf();    final FileSystem fs = path.getFileSystem(jobConf);    if (fs.exists(path))        fs.delete(path, true);    Scheme sourceScheme = new TextLine(new Fields("first", "last"));    Tap source = new Hfs(sourceScheme, txtInputPath);    Scheme sinkScheme = new ParquetTBaseScheme(Name.class);    Tap sink = new Hfs(sinkScheme, parquetOutputPath);    Pipe assembly = new Pipe("namecp");    assembly = new Each(assembly, new PackThriftFunction());    HadoopFlowConnector hadoopFlowConnector = new HadoopFlowConnector();    Flow flow = hadoopFlowConnector.connect("namecp", source, sink, assembly);    flow.complete();    assertTrue(fs.exists(new Path(parquetOutputPath)));    assertTrue(fs.exists(new Path(parquetOutputPath + "/_metadata")));    assertTrue(fs.exists(new Path(parquetOutputPath + "/_common_metadata")));}
0
public void testRead() throws Exception
{    doRead(new ParquetTBaseScheme(Name.class));}
0
public void testReadWithoutClass() throws Exception
{    doRead(new ParquetTBaseScheme());}
0
private void doRead(Scheme sourceScheme) throws Exception
{    createFileForRead();    Path path = new Path(txtOutputPath);    final FileSystem fs = path.getFileSystem(new Configuration());    if (fs.exists(path))        fs.delete(path, true);    Tap source = new Hfs(sourceScheme, parquetInputPath);    Scheme sinkScheme = new TextLine(new Fields("first", "last"));    Tap sink = new Hfs(sinkScheme, txtOutputPath);    Pipe assembly = new Pipe("namecp");    assembly = new Each(assembly, new UnpackThriftFunction());    Flow flow = new HadoopFlowConnector().connect("namecp", source, sink, assembly);    flow.complete();    String result = FileUtils.readFileToString(new File(txtOutputPath + "/part-00000"));    assertEquals("Alice\tPractice\nBob\tHope\nCharlie\tHorse\n", result);}
0
private void createFileForRead() throws Exception
{    final Path fileToCreate = new Path(parquetInputPath + "/names.parquet");    final Configuration conf = new Configuration();    final FileSystem fs = fileToCreate.getFileSystem(conf);    if (fs.exists(fileToCreate))        fs.delete(fileToCreate, true);    TProtocolFactory protocolFactory = new TCompactProtocol.Factory();    TaskAttemptID taskId = new TaskAttemptID("local", 0, true, 0, 0);    ThriftToParquetFileWriter w = new ThriftToParquetFileWriter(fileToCreate, ContextUtil.newTaskAttemptContext(conf, taskId), protocolFactory, Name.class);    final ByteArrayOutputStream baos = new ByteArrayOutputStream();    final TProtocol protocol = protocolFactory.getProtocol(new TIOStreamTransport(baos));    Name n1 = new Name();    n1.setFirst_name("Alice");    n1.setLast_name("Practice");    Name n2 = new Name();    n2.setFirst_name("Bob");    n2.setLast_name("Hope");    Name n3 = new Name();    n3.setFirst_name("Charlie");    n3.setLast_name("Horse");    n1.write(protocol);    w.write(new BytesWritable(baos.toByteArray()));    baos.reset();    n2.write(protocol);    w.write(new BytesWritable(baos.toByteArray()));    baos.reset();    n3.write(protocol);    w.write(new BytesWritable(baos.toByteArray()));    w.close();}
0
public void operate(FlowProcess flowProcess, FunctionCall functionCall)
{    TupleEntry arguments = functionCall.getArguments();    Tuple result = new Tuple();    Name name = new Name();    name.setFirst_name(arguments.getString(0));    name.setLast_name(arguments.getString(1));    result.add(name);    functionCall.getOutputCollector().add(result);}
0
public void operate(FlowProcess flowProcess, FunctionCall functionCall)
{    TupleEntry arguments = functionCall.getArguments();    Tuple result = new Tuple();    Name name = (Name) arguments.get(0);    result.add(name.getFirst_name());    result.add(name.getLast_name());    functionCall.getOutputCollector().add(result);}
0
private Converter newConverter(Type type, int i)
{    if (!type.isPrimitive()) {        throw new IllegalArgumentException("cascading can only build tuples from primitive types");    } else {        return new TuplePrimitiveConverter(this, i);    }}
0
public Converter getConverter(int fieldIndex)
{    return converters[fieldIndex];}
0
public final void start()
{    currentTuple = Tuple.size(converters.length);}
0
public void end()
{}
0
public final Tuple getCurrentTuple()
{    return currentTuple;}
0
public void addBinary(Binary value)
{    parent.getCurrentTuple().setString(index, value.toStringUsingUTF8());}
0
public void addBoolean(boolean value)
{    parent.getCurrentTuple().setBoolean(index, value);}
0
public void addDouble(double value)
{    parent.getCurrentTuple().setDouble(index, value);}
0
public void addFloat(float value)
{    parent.getCurrentTuple().setFloat(index, value);}
0
public void addInt(int value)
{    parent.getCurrentTuple().setInteger(index, value);}
0
public void addLong(long value)
{    parent.getCurrentTuple().setLong(index, value);}
0
public Tuple getCurrentRecord()
{    return root.getCurrentTuple();}
0
public GroupConverter getRootConverter()
{    return root;}
0
public MessageType getRequestedSchema()
{    return requestedSchema;}
0
public Fields getSourceFields()
{    return sourceFields;}
0
protected static Fields getRequestedFields(Configuration configuration)
{    String fieldsString = configuration.get(PARQUET_CASCADING_REQUESTED_FIELDS);    if (fieldsString == null)        return Fields.ALL;    String[] parts = StringUtils.split(fieldsString, ":");    if (parts.length == 0)        return Fields.ALL;    else        return new Fields(parts);}
0
protected static void setRequestedFields(JobConf configuration, Fields fields)
{    String fieldsString = StringUtils.join(fields.iterator(), ":");    configuration.set(PARQUET_CASCADING_REQUESTED_FIELDS, fieldsString);}
0
public ReadContext init(Configuration configuration, Map<String, String> keyValueMetaData, MessageType fileSchema)
{    Fields requestedFields = getRequestedFields(configuration);    if (requestedFields == null) {        return new ReadContext(fileSchema);    } else {        SchemaIntersection intersection = new SchemaIntersection(fileSchema, requestedFields);        return new ReadContext(intersection.getRequestedSchema());    }}
0
public RecordMaterializer<Tuple> prepareForRead(Configuration configuration, Map<String, String> keyValueMetaData, MessageType fileSchema, ReadContext readContext)
{    MessageType requestedSchema = readContext.getRequestedSchema();    return new TupleRecordMaterializer(requestedSchema);}
0
public String getName()
{    return "cascading";}
0
public WriteContext init(Configuration configuration)
{    String schema = configuration.get(PARQUET_CASCADING_SCHEMA);    rootSchema = MessageTypeParser.parseMessageType(schema);    return new WriteContext(rootSchema, new HashMap<String, String>());}
0
public void prepareForWrite(RecordConsumer recordConsumer)
{    this.recordConsumer = recordConsumer;}
0
public void write(TupleEntry record)
{    recordConsumer.startMessage();    final List<Type> fields = rootSchema.getFields();    for (int i = 0; i < fields.size(); i++) {        Type field = fields.get(i);        if (record == null || record.getObject(field.getName()) == null) {            continue;        }        recordConsumer.startField(field.getName(), i);        if (field.isPrimitive()) {            writePrimitive(record, field.asPrimitiveType());        } else {            throw new UnsupportedOperationException("Complex type not implemented");        }        recordConsumer.endField(field.getName(), i);    }    recordConsumer.endMessage();}
0
private void writePrimitive(TupleEntry record, PrimitiveType field)
{    switch(field.getPrimitiveTypeName()) {        case BINARY:            recordConsumer.addBinary(Binary.fromString(record.getString(field.getName())));            break;        case BOOLEAN:            recordConsumer.addBoolean(record.getBoolean(field.getName()));            break;        case INT32:            recordConsumer.addInteger(record.getInteger(field.getName()));            break;        case INT64:            recordConsumer.addLong(record.getLong(field.getName()));            break;        case DOUBLE:            recordConsumer.addDouble(record.getDouble(field.getName()));            break;        case FLOAT:            recordConsumer.addFloat(record.getFloat(field.getName()));            break;        case FIXED_LEN_BYTE_ARRAY:            throw new UnsupportedOperationException("Fixed len byte array type not implemented");        case INT96:            throw new UnsupportedOperationException("Int96 type not implemented");        default:            throw new UnsupportedOperationException(field.getName() + " type not implemented");    }}
0
public void testReadPattern() throws Exception
{    String sourceFolder = parquetInputPath;    testReadWrite(sourceFolder);    String sourceGlobPattern = parquetInputPath + "/*";    testReadWrite(sourceGlobPattern);    String multiLevelGlobPattern = "target/test/ParquetTupleIn/**/*";    testReadWrite(multiLevelGlobPattern);}
0
public void testFieldProjection() throws Exception
{    createFileForRead();    Path path = new Path(txtOutputPath);    final FileSystem fs = path.getFileSystem(new Configuration());    if (fs.exists(path))        fs.delete(path, true);    Scheme sourceScheme = new ParquetTupleScheme(new Fields("last_name"));    Tap source = new Hfs(sourceScheme, parquetInputPath);    Scheme sinkScheme = new TextLine(new Fields("last_name"));    Tap sink = new Hfs(sinkScheme, txtOutputPath);    Pipe assembly = new Pipe("namecp");    assembly = new Each(assembly, new ProjectedTupleFunction());    Flow flow = new HadoopFlowConnector().connect("namecp", source, sink, assembly);    flow.complete();    String result = FileUtils.readFileToString(new File(txtOutputPath + "/part-00000"));    assertEquals("Practice\nHope\nHorse\n", result);}
0
public void testReadWrite(String inputPath) throws Exception
{    createFileForRead();    Path path = new Path(txtOutputPath);    final FileSystem fs = path.getFileSystem(new Configuration());    if (fs.exists(path))        fs.delete(path, true);    Scheme sourceScheme = new ParquetTupleScheme(new Fields("first_name", "last_name"));    Tap source = new Hfs(sourceScheme, inputPath);    Scheme sinkScheme = new TextLine(new Fields("first", "last"));    Tap sink = new Hfs(sinkScheme, txtOutputPath);    Pipe assembly = new Pipe("namecp");    assembly = new Each(assembly, new UnpackTupleFunction());    Flow flow = new HadoopFlowConnector().connect("namecp", source, sink, assembly);    flow.complete();    String result = FileUtils.readFileToString(new File(txtOutputPath + "/part-00000"));    assertEquals("Alice\tPractice\nBob\tHope\nCharlie\tHorse\n", result);}
0
private void createFileForRead() throws Exception
{    final Path fileToCreate = new Path(parquetInputPath + "/names.parquet");    final Configuration conf = new Configuration();    final FileSystem fs = fileToCreate.getFileSystem(conf);    if (fs.exists(fileToCreate))        fs.delete(fileToCreate, true);    TProtocolFactory protocolFactory = new TCompactProtocol.Factory();    TaskAttemptID taskId = new TaskAttemptID("local", 0, true, 0, 0);    ThriftToParquetFileWriter w = new ThriftToParquetFileWriter(fileToCreate, ContextUtil.newTaskAttemptContext(conf, taskId), protocolFactory, Name.class);    final ByteArrayOutputStream baos = new ByteArrayOutputStream();    final TProtocol protocol = protocolFactory.getProtocol(new TIOStreamTransport(baos));    Name n1 = new Name();    n1.setFirst_name("Alice");    n1.setLast_name("Practice");    Name n2 = new Name();    n2.setFirst_name("Bob");    n2.setLast_name("Hope");    Name n3 = new Name();    n3.setFirst_name("Charlie");    n3.setLast_name("Horse");    n1.write(protocol);    w.write(new BytesWritable(baos.toByteArray()));    baos.reset();    n2.write(protocol);    w.write(new BytesWritable(baos.toByteArray()));    baos.reset();    n3.write(protocol);    w.write(new BytesWritable(baos.toByteArray()));    w.close();}
0
public void operate(FlowProcess flowProcess, FunctionCall functionCall)
{    TupleEntry arguments = functionCall.getArguments();    Tuple result = new Tuple();    Tuple name = new Tuple();    name.addString(arguments.getString(0));    name.addString(arguments.getString(1));    result.add(name);    functionCall.getOutputCollector().add(result);}
0
public void operate(FlowProcess flowProcess, FunctionCall functionCall)
{    TupleEntry arguments = functionCall.getArguments();    Tuple result = new Tuple();    Tuple name = new Tuple();    name.addString(arguments.getString(0));        result.add(name);    functionCall.getOutputCollector().add(result);}
0
public void sourceConfInit(FlowProcess<? extends JobConf> fp, Tap<JobConf, RecordReader, OutputCollector> tap, JobConf jobConf)
{    super.sourceConfInit(fp, tap, jobConf);    jobConf.setInputFormat(DeprecatedParquetInputFormat.class);    ParquetInputFormat.setReadSupportClass(jobConf, ThriftReadSupport.class);    ThriftReadSupport.setRecordConverterClass(jobConf, TBaseRecordConverter.class);}
0
public void sinkConfInit(FlowProcess<? extends JobConf> fp, Tap<JobConf, RecordReader, OutputCollector> tap, JobConf jobConf)
{    if (this.config.getKlass() == null) {        throw new IllegalArgumentException("To use ParquetTBaseScheme as a sink, you must specify a thrift class in the constructor");    }    DeprecatedParquetOutputFormat.setAsOutputFormat(jobConf);    DeprecatedParquetOutputFormat.setWriteSupportClass(jobConf, TBaseWriteSupport.class);    TBaseWriteSupport.<T>setThriftClass(jobConf, this.config.getKlass());}
0
public void sourceConfInit(FlowProcess<? extends JobConf> fp, Tap<JobConf, RecordReader, OutputCollector> tap, JobConf jobConf)
{    if (filterPredicate != null) {        ParquetInputFormat.setFilterPredicate(jobConf, filterPredicate);    }    jobConf.setInputFormat(DeprecatedParquetInputFormat.class);    ParquetInputFormat.setReadSupportClass(jobConf, TupleReadSupport.class);    TupleReadSupport.setRequestedFields(jobConf, getSourceFields());}
0
public Fields retrieveSourceFields(FlowProcess<? extends JobConf> flowProcess, Tap tap)
{    MessageType schema = readSchema(flowProcess, tap);    SchemaIntersection intersection = new SchemaIntersection(schema, getSourceFields());    setSourceFields(intersection.getSourceFields());    return getSourceFields();}
0
private MessageType readSchema(FlowProcess<? extends JobConf> flowProcess, Tap tap)
{    try {        Hfs hfs;        if (tap instanceof CompositeTap)            hfs = (Hfs) ((CompositeTap) tap).getChildTaps().next();        else            hfs = (Hfs) tap;        List<Footer> footers = getFooters(flowProcess, hfs);        if (footers.isEmpty()) {            throw new TapException("Could not read Parquet metadata at " + hfs.getPath());        } else {            return footers.get(0).getParquetMetadata().getFileMetaData().getSchema();        }    } catch (IOException e) {        throw new TapException(e);    }}
0
private List<Footer> getFooters(FlowProcess<? extends JobConf> flowProcess, Hfs hfs) throws IOException
{    JobConf jobConf = flowProcess.getConfigCopy();    DeprecatedParquetInputFormat format = new DeprecatedParquetInputFormat();    format.addInputPath(jobConf, hfs.getPath());    return format.getFooters(jobConf);}
0
public boolean source(FlowProcess<? extends JobConf> fp, SourceCall<Object[], RecordReader> sc) throws IOException
{    Container<Tuple> value = (Container<Tuple>) sc.getInput().createValue();    boolean hasNext = sc.getInput().next(null, value);    if (!hasNext) {        return false;    }        if (value == null) {        return true;    }    sc.getIncomingEntry().setTuple(value.get());    return true;}
0
public void sinkConfInit(FlowProcess<? extends JobConf> fp, Tap<JobConf, RecordReader, OutputCollector> tap, JobConf jobConf)
{    DeprecatedParquetOutputFormat.setAsOutputFormat(jobConf);    jobConf.set(TupleWriteSupport.PARQUET_CASCADING_SCHEMA, parquetSchema);    ParquetOutputFormat.setWriteSupportClass(jobConf, TupleWriteSupport.class);}
0
public boolean isSink()
{    return parquetSchema != null;}
0
public void sink(FlowProcess<? extends JobConf> fp, SinkCall<Object[], OutputCollector> sink) throws IOException
{    TupleEntry tuple = sink.getOutgoingEntry();    OutputCollector outputCollector = sink.getOutput();    outputCollector.collect(null, tuple);}
0
public FilterPredicate getFilterPredicate()
{    return filterPredicate;}
0
public String getProjectionString()
{    return deprecatedProjectionString;}
0
public String getStrictProjectionString()
{    return strictProjectionString;}
0
public Class<T> getKlass()
{    return klass;}
0
public Config<T> withFilterPredicate(FilterPredicate f)
{    return new Config<T>(this.klass, checkNotNull(f, "filterPredicate"), this.deprecatedProjectionString, this.strictProjectionString);}
0
public Config<T> withProjectionString(String p)
{    return new Config<T>(this.klass, this.filterPredicate, checkNotNull(p, "projectionString"), this.strictProjectionString);}
0
public Config<T> withStrictProjectionString(String p)
{    return new Config<T>(this.klass, this.filterPredicate, this.deprecatedProjectionString, checkNotNull(p, "projectionString"));}
0
public Config<T> withRecordClass(Class<T> klass)
{    return new Config<T>(checkNotNull(klass, "recordClass"), this.filterPredicate, this.deprecatedProjectionString, this.strictProjectionString);}
0
private void setProjectionPushdown(JobConf jobConf)
{    if (this.config.deprecatedProjectionString != null) {        ThriftReadSupport.setProjectionPushdown(jobConf, this.config.deprecatedProjectionString);    }}
0
private void setStrictProjectionPushdown(JobConf jobConf)
{    if (this.config.strictProjectionString != null) {        ThriftReadSupport.setStrictFieldProjectionFilter(jobConf, this.config.strictProjectionString);    }}
0
private void setPredicatePushdown(JobConf jobConf)
{    if (this.config.filterPredicate != null) {        ParquetInputFormat.setFilterPredicate(jobConf, this.config.filterPredicate);    }}
0
public void sourceConfInit(FlowProcess<? extends JobConf> jobConfFlowProcess, Tap<JobConf, RecordReader, OutputCollector> jobConfRecordReaderOutputCollectorTap, JobConf jobConf)
{    setPredicatePushdown(jobConf);    setProjectionPushdown(jobConf);    setStrictProjectionPushdown(jobConf);    setRecordClass(jobConf);}
0
private void setRecordClass(JobConf jobConf)
{    if (config.klass != null) {        ParquetThriftInputFormat.setThriftClass(jobConf, config.klass);    }}
0
public boolean source(FlowProcess<? extends JobConf> fp, SourceCall<Object[], RecordReader> sc) throws IOException
{    Container<T> value = (Container<T>) sc.getInput().createValue();    boolean hasNext = sc.getInput().next(null, value);    if (!hasNext) {        return false;    }        if (value == null) {        return true;    }    sc.getIncomingEntry().setTuple(new Tuple(value.get()));    return true;}
0
public void sink(FlowProcess<? extends JobConf> fp, SinkCall<Object[], OutputCollector> sc) throws IOException
{    TupleEntry tuple = sc.getOutgoingEntry();    if (tuple.size() != 1) {        throw new RuntimeException("ParquetValueScheme expects tuples with an arity of exactly 1, but found " + tuple.getFields());    }    T value = (T) tuple.getObject(0);    OutputCollector output = sc.getOutput();    output.collect(null, value);}
0
public void testWrite() throws Exception
{    Path path = new Path(parquetOutputPath);    JobConf jobConf = new JobConf();    final FileSystem fs = path.getFileSystem(jobConf);    if (fs.exists(path))        fs.delete(path, true);    Scheme sourceScheme = new TextLine(new Fields("first", "last"));    Tap source = new Hfs(sourceScheme, txtInputPath);    Scheme sinkScheme = new ParquetTBaseScheme(Name.class);    Tap sink = new Hfs(sinkScheme, parquetOutputPath);    Pipe assembly = new Pipe("namecp");    assembly = new Each(assembly, new PackThriftFunction());    HadoopFlowConnector hadoopFlowConnector = new HadoopFlowConnector();    Flow flow = hadoopFlowConnector.connect("namecp", source, sink, assembly);    flow.complete();    assertTrue(fs.exists(new Path(parquetOutputPath)));    assertTrue(fs.exists(new Path(parquetOutputPath + "/_metadata")));    assertTrue(fs.exists(new Path(parquetOutputPath + "/_common_metadata")));}
0
public void testRead() throws Exception
{    doRead(new ParquetTBaseScheme(Name.class));}
0
public void testReadWithoutClass() throws Exception
{    doRead(new ParquetTBaseScheme());}
0
private void doRead(Scheme sourceScheme) throws Exception
{    createFileForRead();    Path path = new Path(txtOutputPath);    final FileSystem fs = path.getFileSystem(new Configuration());    if (fs.exists(path))        fs.delete(path, true);    Tap source = new Hfs(sourceScheme, parquetInputPath);    Scheme sinkScheme = new TextLine(new Fields("first", "last"));    Tap sink = new Hfs(sinkScheme, txtOutputPath);    Pipe assembly = new Pipe("namecp");    assembly = new Each(assembly, new UnpackThriftFunction());    Flow flow = new HadoopFlowConnector().connect("namecp", source, sink, assembly);    flow.complete();    String result = FileUtils.readFileToString(new File(txtOutputPath + "/part-00000"));    assertEquals("Alice\tPractice\nBob\tHope\nCharlie\tHorse\n", result);}
0
private void createFileForRead() throws Exception
{    final Path fileToCreate = new Path(parquetInputPath + "/names.parquet");    final Configuration conf = new Configuration();    final FileSystem fs = fileToCreate.getFileSystem(conf);    if (fs.exists(fileToCreate))        fs.delete(fileToCreate, true);    TProtocolFactory protocolFactory = new TCompactProtocol.Factory();    TaskAttemptID taskId = new TaskAttemptID("local", 0, true, 0, 0);    ThriftToParquetFileWriter w = new ThriftToParquetFileWriter(fileToCreate, ContextUtil.newTaskAttemptContext(conf, taskId), protocolFactory, Name.class);    final ByteArrayOutputStream baos = new ByteArrayOutputStream();    final TProtocol protocol = protocolFactory.getProtocol(new TIOStreamTransport(baos));    Name n1 = new Name();    n1.setFirst_name("Alice");    n1.setLast_name("Practice");    Name n2 = new Name();    n2.setFirst_name("Bob");    n2.setLast_name("Hope");    Name n3 = new Name();    n3.setFirst_name("Charlie");    n3.setLast_name("Horse");    n1.write(protocol);    w.write(new BytesWritable(baos.toByteArray()));    baos.reset();    n2.write(protocol);    w.write(new BytesWritable(baos.toByteArray()));    baos.reset();    n3.write(protocol);    w.write(new BytesWritable(baos.toByteArray()));    w.close();}
0
public void operate(FlowProcess flowProcess, FunctionCall functionCall)
{    TupleEntry arguments = functionCall.getArguments();    Tuple result = new Tuple();    Name name = new Name();    name.setFirst_name(arguments.getString(0));    name.setLast_name(arguments.getString(1));    result.add(name);    functionCall.getOutputCollector().add(result);}
0
public void operate(FlowProcess flowProcess, FunctionCall functionCall)
{    TupleEntry arguments = functionCall.getArguments();    Tuple result = new Tuple();    Name name = (Name) arguments.getObject(0);    result.add(name.getFirst_name());    result.add(name.getLast_name());    functionCall.getOutputCollector().add(result);}
0
public FileSystem defaultFS() throws IOException
{    if (localFS == null) {        this.localFS = FileSystem.getLocal(getConf());    }    return localFS;}
0
public void output(String content, Logger console, String filename) throws IOException
{    if (filename == null || "-".equals(filename)) {        console.info(content);    } else {        FSDataOutputStream outgoing = create(filename);        try {            outgoing.write(content.getBytes(StandardCharsets.UTF_8));        } finally {            outgoing.close();        }    }}
0
public FSDataOutputStream create(String filename) throws IOException
{    return create(filename, true);}
0
public FSDataOutputStream createWithChecksum(String filename) throws IOException
{    return create(filename, false);}
0
private FSDataOutputStream create(String filename, boolean noChecksum) throws IOException
{    Path filePath = qualifiedPath(filename);        FileSystem fs = filePath.getFileSystem(getConf());    if (noChecksum && fs instanceof ChecksumFileSystem) {        fs = ((ChecksumFileSystem) fs).getRawFileSystem();    }    return fs.create(filePath, true);}
0
public Path qualifiedPath(String filename) throws IOException
{    Path cwd = defaultFS().makeQualified(new Path("."));    return new Path(filename).makeQualified(defaultFS().getUri(), cwd);}
0
public URI qualifiedURI(String filename) throws IOException
{    try {        URI fileURI = new URI(filename);        if (RESOURCE_URI_SCHEME.equals(fileURI.getScheme())) {            return fileURI;        }    } catch (URISyntaxException ignore) {    }    return qualifiedPath(filename).toUri();}
0
public InputStream open(String filename) throws IOException
{    if (STDIN_AS_SOURCE.equals(filename)) {        return System.in;    }    URI uri = qualifiedURI(filename);    if (RESOURCE_URI_SCHEME.equals(uri.getScheme())) {        return Resources.getResource(uri.getRawSchemeSpecificPart()).openStream();    } else {        Path filePath = new Path(uri);                FileSystem fs = filePath.getFileSystem(getConf());        return fs.open(filePath);    }}
0
public SeekableInput openSeekable(String filename) throws IOException
{    Path path = qualifiedPath(filename);        FileSystem fs = path.getFileSystem(getConf());    return new SeekableFSDataInputStream(fs, path);}
0
public void setConf(Configuration conf)
{    this.conf = conf;    HadoopFileSystemURLStreamHandler.setDefaultConf(conf);}
0
public Configuration getConf()
{        return null != conf ? conf : new Configuration();}
0
protected static ClassLoader loaderFor(List<String> jars, List<String> paths) throws MalformedURLException
{    return AccessController.doPrivileged(new GetClassLoader(urls(jars, paths)));}
0
protected static ClassLoader loaderForJars(List<String> jars) throws MalformedURLException
{    return AccessController.doPrivileged(new GetClassLoader(urls(jars, null)));}
0
protected static ClassLoader loaderForPaths(List<String> paths) throws MalformedURLException
{    return AccessController.doPrivileged(new GetClassLoader(urls(null, paths)));}
0
private static List<URL> urls(List<String> jars, List<String> dirs) throws MalformedURLException
{        final List<URL> urls = Lists.newArrayList();    if (dirs != null) {        for (String lib : dirs) {                        File path = lib.endsWith("/") ? new File(lib) : new File(lib + "/");            Preconditions.checkArgument(path.exists(), "Lib directory does not exist: " + lib);            Preconditions.checkArgument(path.isDirectory(), "Not a directory: " + lib);            Preconditions.checkArgument(path.canRead() && path.canExecute(), "Insufficient permissions to access lib directory: " + lib);            urls.add(path.toURI().toURL());        }    }    if (jars != null) {        for (String jar : jars) {            File path = new File(jar);            Preconditions.checkArgument(path.exists(), "Jar files does not exist: " + jar);            Preconditions.checkArgument(path.isFile(), "Not a file: " + jar);            Preconditions.checkArgument(path.canRead(), "Cannot read jar file: " + jar);            urls.add(path.toURI().toURL());        }    }    return urls;}
0
protected Iterable<D> openDataFile(final String source, Schema projection) throws IOException
{    Formats.Format format = Formats.detectFormat(open(source));    switch(format) {        case PARQUET:            Configuration conf = new Configuration(getConf());                        AvroReadSupport.setRequestedProjection(conf, projection);            AvroReadSupport.setAvroReadSchema(conf, projection);            final ParquetReader<D> parquet = AvroParquetReader.<D>builder(qualifiedPath(source)).disableCompatibility().withDataModel(GenericData.get()).withConf(conf).build();            return new Iterable<D>() {                @Override                public Iterator<D> iterator() {                    return new Iterator<D>() {                        private boolean hasNext = false;                        private D next = advance();                        @Override                        public boolean hasNext() {                            return hasNext;                        }                        @Override                        public D next() {                            if (!hasNext) {                                throw new NoSuchElementException();                            }                            D toReturn = next;                            this.next = advance();                            return toReturn;                        }                        private D advance() {                            try {                                D next = parquet.read();                                this.hasNext = (next != null);                                return next;                            } catch (IOException e) {                                throw new RuntimeException("Failed while reading Parquet file: " + source, e);                            }                        }                        @Override                        public void remove() {                            throw new UnsupportedOperationException("Remove is not supported");                        }                    };                }            };        case AVRO:            Iterable<D> avroReader = (Iterable<D>) DataFileReader.openReader(openSeekable(source), new GenericDatumReader<>(projection));            return avroReader;        default:            if (source.endsWith("json")) {                return new AvroJsonReader<>(open(source), projection);            } else {                Preconditions.checkArgument(projection == null, "Cannot select columns from text files");                Iterable text = CharStreams.readLines(new InputStreamReader(open(source)));                return text;            }    }}
0
public Iterator<D> iterator()
{    return new Iterator<D>() {        private boolean hasNext = false;        private D next = advance();        @Override        public boolean hasNext() {            return hasNext;        }        @Override        public D next() {            if (!hasNext) {                throw new NoSuchElementException();            }            D toReturn = next;            this.next = advance();            return toReturn;        }        private D advance() {            try {                D next = parquet.read();                this.hasNext = (next != null);                return next;            } catch (IOException e) {                throw new RuntimeException("Failed while reading Parquet file: " + source, e);            }        }        @Override        public void remove() {            throw new UnsupportedOperationException("Remove is not supported");        }    };}
0
public boolean hasNext()
{    return hasNext;}
0
public D next()
{    if (!hasNext) {        throw new NoSuchElementException();    }    D toReturn = next;    this.next = advance();    return toReturn;}
0
private D advance()
{    try {        D next = parquet.read();        this.hasNext = (next != null);        return next;    } catch (IOException e) {        throw new RuntimeException("Failed while reading Parquet file: " + source, e);    }}
0
public void remove()
{    throw new UnsupportedOperationException("Remove is not supported");}
0
protected Schema getAvroSchema(String source) throws IOException
{    Formats.Format format;    try (SeekableInput in = openSeekable(source)) {        format = Formats.detectFormat((InputStream) in);        in.seek(0);        switch(format) {            case PARQUET:                return Schemas.fromParquet(getConf(), qualifiedURI(source));            case AVRO:                return Schemas.fromAvro(open(source));            case TEXT:                if (source.endsWith("avsc")) {                    return Schemas.fromAvsc(open(source));                } else if (source.endsWith("json")) {                    return Schemas.fromJSON("json", open(source));                }            default:        }        throw new IllegalArgumentException(String.format("Could not determine file format of %s.", source));    }}
0
public int run() throws IOException
{    Preconditions.checkArgument(sourceFiles != null && !sourceFiles.isEmpty(), "Missing file name");    Preconditions.checkArgument(sourceFiles.size() == 1, "Only one file can be given");    final String source = sourceFiles.get(0);    Schema schema = getAvroSchema(source);    Schema projection = Expressions.filterSchema(schema, columns);    Iterable<Object> reader = openDataFile(source, projection);    boolean threw = true;    long count = 0;    try {        for (Object record : reader) {            if (numRecords > 0 && count >= numRecords) {                break;            }            if (columns == null || columns.size() != 1) {                console.info(String.valueOf(record));            } else {                console.info(String.valueOf(select(projection, record, columns.get(0))));            }            count += 1;        }        threw = false;    } catch (RuntimeException e) {        throw new RuntimeException("Failed on record " + count, e);    } finally {        if (reader instanceof Closeable) {            Closeables.close((Closeable) reader, threw);        }    }    return 0;}
0
public List<String> getExamples()
{    return Lists.newArrayList("# Show the first 10 records in file \"data.avro\":", "data.avro", "# Show the first 50 records in file \"data.parquet\":", "data.parquet -n 50");}
0
public int run() throws IOException
{    boolean badFiles = false;    for (String file : files) {        String problem = check(file);        if (problem != null) {            badFiles = true;            console.info("{} has corrupt stats: {}", file, problem);        } else {            console.info("{} has no corrupt stats", file);        }    }    return badFiles ? 1 : 0;}
0
private String check(String file) throws IOException
{    Path path = qualifiedPath(file);    ParquetMetadata footer = ParquetFileReader.readFooter(getConf(), path, ParquetMetadataConverter.NO_FILTER);    FileMetaData meta = footer.getFileMetaData();    String createdBy = meta.getCreatedBy();    if (CorruptStatistics.shouldIgnoreStatistics(createdBy, BINARY)) {                FileMetaData fakeMeta = new FileMetaData(meta.getSchema(), meta.getKeyValueMetaData(), Version.FULL_VERSION);                List<ColumnDescriptor> columns = Lists.newArrayList();        Iterables.addAll(columns, Iterables.filter(meta.getSchema().getColumns(), new Predicate<ColumnDescriptor>() {            @Override            public boolean apply(@Nullable ColumnDescriptor input) {                return input != null && input.getType() == BINARY;            }        }));                ParquetFileReader reader = new ParquetFileReader(getConf(), fakeMeta, path, footer.getBlocks(), columns);        try {            PageStatsValidator validator = new PageStatsValidator();            for (PageReadStore pages = reader.readNextRowGroup(); pages != null; pages = reader.readNextRowGroup()) {                validator.validate(columns, pages);            }        } catch (BadStatsException e) {            return e.getMessage();        }    }    return null;}
0
public boolean apply(@Nullable ColumnDescriptor input)
{    return input != null && input.getType() == BINARY;}
0
public List<String> getExamples()
{    return Arrays.asList("# Check file1.parquet for corrupt page and column stats", "file1.parquet");}
0
public DictionaryPage readDictionaryPage()
{    return dict;}
0
public long getTotalValueCount()
{    return data.getValueCount();}
0
public DataPage readPage()
{    return data;}
0
private static Statistics<T> getStatisticsFromPageHeader(DataPage page)
{    return page.accept(new DataPage.Visitor<Statistics<T>>() {        @Override        @SuppressWarnings("unchecked")        public Statistics<T> visit(DataPageV1 dataPageV1) {            return (Statistics<T>) dataPageV1.getStatistics();        }        @Override        @SuppressWarnings("unchecked")        public Statistics<T> visit(DataPageV2 dataPageV2) {            return (Statistics<T>) dataPageV2.getStatistics();        }    });}
0
public Statistics<T> visit(DataPageV1 dataPageV1)
{    return (Statistics<T>) dataPageV1.getStatistics();}
0
public Statistics<T> visit(DataPageV2 dataPageV2)
{    return (Statistics<T>) dataPageV2.getStatistics();}
0
public void validate(T value)
{    if (hasNonNull) {        if (comparator.compare(min, value) > 0) {            throw new BadStatsException("Min should be <= all values.");        }        if (comparator.compare(max, value) < 0) {            throw new BadStatsException("Max should be >= all values.");        }    }}
0
private PrimitiveConverter getValidatingConverter(final DataPage page, PrimitiveTypeName type)
{    return type.convert(new PrimitiveTypeNameConverter<PrimitiveConverter, RuntimeException>() {        @Override        public PrimitiveConverter convertFLOAT(PrimitiveTypeName primitiveTypeName) {            final StatsValidator<Float> validator = new StatsValidator<Float>(page);            return new PrimitiveConverter() {                @Override                public void addFloat(float value) {                    validator.validate(value);                }            };        }        @Override        public PrimitiveConverter convertDOUBLE(PrimitiveTypeName primitiveTypeName) {            final StatsValidator<Double> validator = new StatsValidator<Double>(page);            return new PrimitiveConverter() {                @Override                public void addDouble(double value) {                    validator.validate(value);                }            };        }        @Override        public PrimitiveConverter convertINT32(PrimitiveTypeName primitiveTypeName) {            final StatsValidator<Integer> validator = new StatsValidator<Integer>(page);            return new PrimitiveConverter() {                @Override                public void addInt(int value) {                    validator.validate(value);                }            };        }        @Override        public PrimitiveConverter convertINT64(PrimitiveTypeName primitiveTypeName) {            final StatsValidator<Long> validator = new StatsValidator<Long>(page);            return new PrimitiveConverter() {                @Override                public void addLong(long value) {                    validator.validate(value);                }            };        }        @Override        public PrimitiveConverter convertBOOLEAN(PrimitiveTypeName primitiveTypeName) {            final StatsValidator<Boolean> validator = new StatsValidator<Boolean>(page);            return new PrimitiveConverter() {                @Override                public void addBoolean(boolean value) {                    validator.validate(value);                }            };        }        @Override        public PrimitiveConverter convertINT96(PrimitiveTypeName primitiveTypeName) {            return convertBINARY(primitiveTypeName);        }        @Override        public PrimitiveConverter convertFIXED_LEN_BYTE_ARRAY(PrimitiveTypeName primitiveTypeName) {            return convertBINARY(primitiveTypeName);        }        @Override        public PrimitiveConverter convertBINARY(PrimitiveTypeName primitiveTypeName) {            final StatsValidator<Binary> validator = new StatsValidator<Binary>(page);            return new PrimitiveConverter() {                @Override                public void addBinary(Binary value) {                    validator.validate(value);                }            };        }    });}
0
public PrimitiveConverter convertFLOAT(PrimitiveTypeName primitiveTypeName)
{    final StatsValidator<Float> validator = new StatsValidator<Float>(page);    return new PrimitiveConverter() {        @Override        public void addFloat(float value) {            validator.validate(value);        }    };}
0
public void addFloat(float value)
{    validator.validate(value);}
0
public PrimitiveConverter convertDOUBLE(PrimitiveTypeName primitiveTypeName)
{    final StatsValidator<Double> validator = new StatsValidator<Double>(page);    return new PrimitiveConverter() {        @Override        public void addDouble(double value) {            validator.validate(value);        }    };}
0
public void addDouble(double value)
{    validator.validate(value);}
0
public PrimitiveConverter convertINT32(PrimitiveTypeName primitiveTypeName)
{    final StatsValidator<Integer> validator = new StatsValidator<Integer>(page);    return new PrimitiveConverter() {        @Override        public void addInt(int value) {            validator.validate(value);        }    };}
0
public void addInt(int value)
{    validator.validate(value);}
0
public PrimitiveConverter convertINT64(PrimitiveTypeName primitiveTypeName)
{    final StatsValidator<Long> validator = new StatsValidator<Long>(page);    return new PrimitiveConverter() {        @Override        public void addLong(long value) {            validator.validate(value);        }    };}
0
public void addLong(long value)
{    validator.validate(value);}
0
public PrimitiveConverter convertBOOLEAN(PrimitiveTypeName primitiveTypeName)
{    final StatsValidator<Boolean> validator = new StatsValidator<Boolean>(page);    return new PrimitiveConverter() {        @Override        public void addBoolean(boolean value) {            validator.validate(value);        }    };}
0
public void addBoolean(boolean value)
{    validator.validate(value);}
0
public PrimitiveConverter convertINT96(PrimitiveTypeName primitiveTypeName)
{    return convertBINARY(primitiveTypeName);}
0
public PrimitiveConverter convertFIXED_LEN_BYTE_ARRAY(PrimitiveTypeName primitiveTypeName)
{    return convertBINARY(primitiveTypeName);}
0
public PrimitiveConverter convertBINARY(PrimitiveTypeName primitiveTypeName)
{    final StatsValidator<Binary> validator = new StatsValidator<Binary>(page);    return new PrimitiveConverter() {        @Override        public void addBinary(Binary value) {            validator.validate(value);        }    };}
0
public void addBinary(Binary value)
{    validator.validate(value);}
0
public void validate(List<ColumnDescriptor> columns, PageReadStore store)
{    for (ColumnDescriptor desc : columns) {        PageReader reader = store.getPageReader(desc);        DictionaryPage dict = reader.readDictionaryPage();        DictionaryPage reusableDict = null;        if (dict != null) {            try {                reusableDict = new DictionaryPage(BytesInput.from(dict.getBytes().toByteArray()), dict.getDictionarySize(), dict.getEncoding());            } catch (IOException e) {                throw new ParquetDecodingException("Cannot read dictionary", e);            }        }        DataPage page;        while ((page = reader.readPage()) != null) {            validateStatsForPage(page, reusableDict, desc);        }    }}
0
private void validateStatsForPage(DataPage page, DictionaryPage dict, ColumnDescriptor desc)
{    SingletonPageReader reader = new SingletonPageReader(dict, page);    PrimitiveConverter converter = getValidatingConverter(page, desc.getType());    Statistics stats = getStatisticsFromPageHeader(page);    long numNulls = 0;    ColumnReader column = COL_READER_CTOR.newInstance(desc, reader, converter, null);    for (int i = 0; i < reader.getTotalValueCount(); i += 1) {        if (column.getCurrentDefinitionLevel() >= desc.getMaxDefinitionLevel()) {            column.writeCurrentValueToConverter();        } else {            numNulls += 1;        }        column.consume();    }    if (numNulls != stats.getNumNulls()) {        throw new BadStatsException("Number of nulls doesn't match.");    }    console.debug(String.format("Validated stats min=%s max=%s nulls=%d for page=%s col=%s", stats.minAsString(), stats.maxAsString(), stats.getNumNulls(), page, Arrays.toString(desc.getPath())));}
0
public int run() throws IOException
{    Preconditions.checkArgument(targets != null && targets.size() == 1, "A data file is required.");    String source = targets.get(0);    CompressionCodecName codec = Codecs.parquetCodec(compressionCodecName);    Schema schema;    if (avroSchemaFile != null) {        schema = Schemas.fromAvsc(open(avroSchemaFile));    } else {        schema = getAvroSchema(source);    }    Schema projection = filterSchema(schema, columns);    Path outPath = qualifiedPath(outputPath);    FileSystem outFS = outPath.getFileSystem(getConf());    if (overwrite && outFS.exists(outPath)) {        console.debug("Deleting output file {} (already exists)", outPath);        outFS.delete(outPath);    }    Iterable<Record> reader = openDataFile(source, projection);    boolean threw = true;    long count = 0;    try {        try (ParquetWriter<Record> writer = AvroParquetWriter.<Record>builder(qualifiedPath(outputPath)).withWriterVersion(v2 ? PARQUET_2_0 : PARQUET_1_0).withConf(getConf()).withCompressionCodec(codec).withRowGroupSize(rowGroupSize).withDictionaryPageSize(dictionaryPageSize < 64 ? 64 : dictionaryPageSize).withDictionaryEncoding(dictionaryPageSize != 0).withPageSize(pageSize).withDataModel(GenericData.get()).withSchema(projection).build()) {            for (Record record : reader) {                writer.write(record);                count += 1;            }        }        threw = false;    } catch (RuntimeException e) {        throw new RuntimeException("Failed on record " + count, e);    } finally {        if (reader instanceof Closeable) {            Closeables.close((Closeable) reader, threw);        }    }    return 0;}
0
public List<String> getExamples()
{    return Lists.newArrayList("# Create a Parquet file from an Avro file", "sample.avro -o sample.parquet", "# Create a Parquet file in S3 from a local Avro file", "path/to/sample.avro -o s3:/user/me/sample.parquet", "# Create a Parquet file from Avro data in S3", "s3:/data/path/sample.avro -o sample.parquet");}
0
public int run() throws IOException
{    Preconditions.checkArgument(targets != null && targets.size() == 1, "CSV path is required.");    if (header != null) {                noHeader = true;    }    CSVProperties props = new CSVProperties.Builder().delimiter(delimiter).escape(escape).quote(quote).header(header).hasHeader(!noHeader).linesToSkip(linesToSkip).charset(charsetName).build();    String source = targets.get(0);    Schema csvSchema;    if (avroSchemaFile != null) {        csvSchema = Schemas.fromAvsc(open(avroSchemaFile));    } else {        Set<String> required = ImmutableSet.of();        if (requiredFields != null) {            required = ImmutableSet.copyOf(requiredFields);        }        String filename = new File(source).getName();        String recordName;        if (filename.contains(".")) {            recordName = filename.substring(0, filename.indexOf("."));        } else {            recordName = filename;        }        csvSchema = AvroCSV.inferNullableSchema(recordName, open(source), props, required);    }    long count = 0;    try (AvroCSVReader<Record> reader = new AvroCSVReader<>(open(source), props, csvSchema, Record.class, true)) {        CompressionCodecName codec = Codecs.parquetCodec(compressionCodecName);        try (ParquetWriter<Record> writer = AvroParquetWriter.<Record>builder(qualifiedPath(outputPath)).withWriterVersion(v2 ? PARQUET_2_0 : PARQUET_1_0).withWriteMode(overwrite ? ParquetFileWriter.Mode.OVERWRITE : ParquetFileWriter.Mode.CREATE).withCompressionCodec(codec).withDictionaryEncoding(true).withDictionaryPageSize(dictionaryPageSize).withPageSize(pageSize).withRowGroupSize(rowGroupSize).withDataModel(GenericData.get()).withConf(getConf()).withSchema(csvSchema).build()) {            for (Record record : reader) {                writer.write(record);            }        } catch (RuntimeException e) {            throw new RuntimeException("Failed on record " + count, e);        }    }    return 0;}
0
public List<String> getExamples()
{    return Lists.newArrayList("# Create a Parquet file from a CSV file", "sample.csv sample.parquet --schema schema.avsc", "# Create a Parquet file in HDFS from local CSV", "path/to/sample.csv hdfs:/user/me/sample.parquet --schema schema.avsc", "# Create an Avro file from CSV data in S3", "s3:/data/path/sample.csv sample.avro --format avro --schema s3:/schemas/schema.avsc");}
0
public int run() throws IOException
{    Preconditions.checkArgument(samplePaths != null && !samplePaths.isEmpty(), "Sample CSV path is required");    Preconditions.checkArgument(samplePaths.size() == 1, "Only one CSV sample can be given");    if (header != null) {                noHeader = true;    }    CSVProperties props = new CSVProperties.Builder().delimiter(delimiter).escape(escape).quote(quote).header(header).hasHeader(!noHeader).linesToSkip(linesToSkip).charset(charsetName).build();    Set<String> required = ImmutableSet.of();    if (requiredFields != null) {        required = ImmutableSet.copyOf(requiredFields);    }        String sampleSchema = AvroCSV.inferNullableSchema(recordName, open(samplePaths.get(0)), props, required).toString(!minimize);    output(sampleSchema, console, outputPath);    return 0;}
0
public List<String> getExamples()
{    return Lists.newArrayList("# Print the schema for samples.csv to standard out:", "samples.csv --record-name Sample", "# Write schema to sample.avsc:", "samples.csv -o sample.avsc --record-name Sample");}
0
public int run() throws IOException
{    Preconditions.checkArgument(targets != null && targets.size() >= 1, "A Parquet file is required.");    Preconditions.checkArgument(targets.size() == 1, "Cannot process multiple Parquet files.");    String source = targets.get(0);    ParquetMetadata footer = ParquetFileReader.readFooter(getConf(), qualifiedPath(source), ParquetMetadataConverter.NO_FILTER);    console.info("\nFile path:  {}", source);    console.info("Created by: {}", footer.getFileMetaData().getCreatedBy());    Map<String, String> kv = footer.getFileMetaData().getKeyValueMetaData();    if (kv != null && !kv.isEmpty()) {        console.info("Properties:");        String format = "  %" + maxSize(kv.keySet()) + "s: %s";        for (Map.Entry<String, String> entry : kv.entrySet()) {            console.info(String.format(format, entry.getKey(), entry.getValue()));        }    } else {        console.info("Properties: (none)");    }    MessageType schema = footer.getFileMetaData().getSchema();    console.info("Schema:\n{}", schema);    List<BlockMetaData> rowGroups = footer.getBlocks();    for (int index = 0, n = rowGroups.size(); index < n; index += 1) {        printRowGroup(console, index, rowGroups.get(index), schema);    }    console.info("");    return 0;}
0
public List<String> getExamples()
{    return Lists.newArrayList();}
0
private int maxSize(Iterable<String> strings)
{    int size = 0;    for (String s : strings) {        size = Math.max(size, s.length());    }    return size;}
0
private void printRowGroup(Logger console, int index, BlockMetaData rowGroup, MessageType schema)
{    long start = rowGroup.getStartingPos();    long rowCount = rowGroup.getRowCount();    long compressedSize = rowGroup.getCompressedSize();    long uncompressedSize = rowGroup.getTotalByteSize();    String filePath = rowGroup.getPath();    console.info(String.format("\nRow group %d:  count: %d  %s records  start: %d  total: %s%s\n%s", index, rowCount, humanReadable(((float) compressedSize) / rowCount), start, humanReadable(compressedSize), filePath != null ? " path: " + filePath : "", StringUtils.leftPad("", 80, '-')));    int size = maxSize(Iterables.transform(rowGroup.getColumns(), new Function<ColumnChunkMetaData, String>() {        @Override        public String apply(@Nullable ColumnChunkMetaData input) {            return input == null ? "" : input.getPath().toDotString();        }    }));    console.info(String.format("%-" + size + "s  %-9s %-9s %-9s %-10s %-7s %s", "", "type", "encodings", "count", "avg size", "nulls", "min / max"));    for (ColumnChunkMetaData column : rowGroup.getColumns()) {        printColumnChunk(console, size, column, schema);    }}
0
public String apply(@Nullable ColumnChunkMetaData input)
{    return input == null ? "" : input.getPath().toDotString();}
0
private void printColumnChunk(Logger console, int width, ColumnChunkMetaData column, MessageType schema)
{    String[] path = column.getPath().toArray();    PrimitiveType type = primitive(schema, path);    Preconditions.checkNotNull(type);    ColumnDescriptor desc = schema.getColumnDescription(path);    long size = column.getTotalSize();    long count = column.getValueCount();    float perValue = ((float) size) / count;    CompressionCodecName codec = column.getCodec();    Set<Encoding> encodings = column.getEncodings();    EncodingStats encodingStats = column.getEncodingStats();    String encodingSummary = encodingStats == null ? encodingsAsString(encodings, desc) : encodingStatsAsString(encodingStats);    Statistics stats = column.getStatistics();    String name = column.getPath().toDotString();    PrimitiveType.PrimitiveTypeName typeName = type.getPrimitiveTypeName();    if (typeName == PrimitiveType.PrimitiveTypeName.FIXED_LEN_BYTE_ARRAY) {        console.info(String.format("%-" + width + "s  FIXED[%d] %s %-7s %-9d %-8s %-7s %s", name, type.getTypeLength(), shortCodec(codec), encodingSummary, count, humanReadable(perValue), stats == null || !stats.isNumNullsSet() ? "" : String.valueOf(stats.getNumNulls()), minMaxAsString(stats)));    } else {        console.info(String.format("%-" + width + "s  %-9s %s %-7s %-9d %-10s %-7s %s", name, typeName, shortCodec(codec), encodingSummary, count, humanReadable(perValue), stats == null || !stats.isNumNullsSet() ? "" : String.valueOf(stats.getNumNulls()), minMaxAsString(stats)));    }}
0
public int run() throws IOException
{    Preconditions.checkArgument(targets != null && targets.size() == 1, "Parquet file is required.");    if (targets.size() > 1) {        Preconditions.checkArgument(outputPath == null, "Cannot output multiple schemas to file " + outputPath);        for (String source : targets) {            console.info("{}: {}", source, getSchema(source));        }    } else {        String source = targets.get(0);        if (outputPath != null) {            Path outPath = qualifiedPath(outputPath);            FileSystem outFS = outPath.getFileSystem(getConf());            if (overwrite && outFS.exists(outPath)) {                console.debug("Deleting output file {} (already exists)", outPath);                outFS.delete(outPath);            }            try (OutputStream out = create(outputPath)) {                out.write(getSchema(source).getBytes(StandardCharsets.UTF_8));            }        } else {            console.info(getSchema(source));        }    }    return 0;}
0
public List<String> getExamples()
{    return Lists.newArrayList("# Print the Avro schema for a Parquet file", "sample.parquet", "# Print the Avro schema for an Avro file", "sample.avro", "# Print the Avro schema for a JSON file", "sample.json");}
0
private String getSchema(String source) throws IOException
{    if (parquetSchema) {        return getParquetSchema(source);    } else {        return getAvroSchema(source).toString(true);    }}
0
private String getParquetSchema(String source) throws IOException
{    Formats.Format format;    try (SeekableInput in = openSeekable(source)) {        format = Formats.detectFormat((InputStream) in);        in.seek(0);        switch(format) {            case PARQUET:                return new ParquetFileReader(getConf(), qualifiedPath(source), ParquetMetadataConverter.NO_FILTER).getFileMetaData().getSchema().toString();            default:                throw new IllegalArgumentException(String.format("Could not get a Parquet schema for format %s: %s", format, source));        }    }}
0
public List<String> getExamples()
{    return Lists.newArrayList("# Show only column indexes for column 'col' from a Parquet file", "-c col -i sample.parquet");}
0
public int run() throws IOException
{    Preconditions.checkArgument(files != null && files.size() >= 1, "A Parquet file is required.");    Preconditions.checkArgument(files.size() == 1, "Cannot process multiple Parquet files.");    InputFile in = HadoopInputFile.fromPath(qualifiedPath(files.get(0)), getConf());    if (!showColumnIndex && !showOffsetIndex) {        showColumnIndex = true;        showOffsetIndex = true;    }    Set<String> rowGroupIndexSet = new HashSet<>();    if (rowGroupIndexes != null) {        rowGroupIndexSet.addAll(rowGroupIndexes);    }    try (ParquetFileReader reader = ParquetFileReader.open(in)) {        boolean firstBlock = true;        int rowGroupIndex = 0;        for (BlockMetaData block : reader.getFooter().getBlocks()) {            if (!rowGroupIndexSet.isEmpty() && !rowGroupIndexSet.contains(Integer.toString(rowGroupIndex))) {                ++rowGroupIndex;                continue;            }            if (!firstBlock) {                console.info("");            }            firstBlock = false;            console.info("row-group {}:", rowGroupIndex);            for (ColumnChunkMetaData column : getColumns(block)) {                String path = column.getPath().toDotString();                if (showColumnIndex) {                    console.info("column index for column {}:", path);                    ColumnIndex columnIndex = reader.readColumnIndex(column);                    if (columnIndex == null) {                        console.info("NONE");                    } else {                        console.info(columnIndex.toString());                    }                }                if (showOffsetIndex) {                    console.info("offset index for column {}:", path);                    OffsetIndex offsetIndex = reader.readOffsetIndex(column);                    if (offsetIndex == null) {                        console.info("NONE");                    } else {                        console.info(offsetIndex.toString());                    }                }            }            ++rowGroupIndex;        }    }    return 0;}
0
private List<ColumnChunkMetaData> getColumns(BlockMetaData block)
{    List<ColumnChunkMetaData> columns = block.getColumns();    if (ColumnPaths == null || ColumnPaths.isEmpty()) {        return columns;    }    Map<String, ColumnChunkMetaData> pathMap = new HashMap<>();    for (ColumnChunkMetaData column : columns) {        pathMap.put(column.getPath().toDotString(), column);    }    List<ColumnChunkMetaData> filtered = new ArrayList<>();    for (String path : ColumnPaths) {        ColumnChunkMetaData column = pathMap.get(path);        if (column != null) {            filtered.add(column);        }    }    return filtered;}
0
public int run() throws IOException
{    Preconditions.checkArgument(targets != null && targets.size() >= 1, "A Parquet file is required.");    Preconditions.checkArgument(targets.size() == 1, "Cannot process multiple Parquet files.");    String source = targets.get(0);    ParquetFileReader reader = ParquetFileReader.open(getConf(), qualifiedPath(source));    MessageType schema = reader.getFileMetaData().getSchema();    ColumnDescriptor descriptor = Util.descriptor(column, schema);    PrimitiveType type = Util.primitive(column, schema);    Preconditions.checkNotNull(type);    DictionaryPageReadStore dictionaryReader;    int rowGroup = 0;    while ((dictionaryReader = reader.getNextDictionaryReader()) != null) {        DictionaryPage page = dictionaryReader.readDictionaryPage(descriptor);        Dictionary dict = page.getEncoding().initDictionary(descriptor, page);        console.info("\nRow group {} dictionary for \"{}\":", rowGroup, column, page.getCompressedSize());        for (int i = 0; i <= dict.getMaxId(); i += 1) {            switch(type.getPrimitiveTypeName()) {                case BINARY:                    if (type.getLogicalTypeAnnotation() instanceof LogicalTypeAnnotation.StringLogicalTypeAnnotation) {                        console.info("{}: {}", String.format("%6d", i), Util.humanReadable(dict.decodeToBinary(i).toStringUsingUTF8(), 70));                    } else {                        console.info("{}: {}", String.format("%6d", i), Util.humanReadable(dict.decodeToBinary(i).getBytesUnsafe(), 70));                    }                    break;                case INT32:                    console.info("{}: {}", String.format("%6d", i), dict.decodeToInt(i));                    break;                case INT64:                    console.info("{}: {}", String.format("%6d", i), dict.decodeToLong(i));                    break;                case FLOAT:                    console.info("{}: {}", String.format("%6d", i), dict.decodeToFloat(i));                    break;                case DOUBLE:                    console.info("{}: {}", String.format("%6d", i), dict.decodeToDouble(i));                    break;                default:                    throw new IllegalArgumentException("Unknown dictionary type: " + type.getPrimitiveTypeName());            }        }        reader.skipNextRowGroup();        rowGroup += 1;    }    console.info("");    return 0;}
0
public List<String> getExamples()
{    return Lists.newArrayList("# Show the dictionary for column 'col' from a Parquet file", "-c col sample.parquet");}
0
public int run() throws IOException
{    Preconditions.checkArgument(targets != null && targets.size() >= 1, "A Parquet file is required.");    Preconditions.checkArgument(targets.size() == 1, "Cannot process multiple Parquet files.");    String source = targets.get(0);    ParquetFileReader reader = ParquetFileReader.open(getConf(), qualifiedPath(source));    MessageType schema = reader.getFileMetaData().getSchema();    Map<ColumnDescriptor, PrimitiveType> columns = Maps.newLinkedHashMap();    if (this.columns == null || this.columns.isEmpty()) {        for (ColumnDescriptor descriptor : schema.getColumns()) {            columns.put(descriptor, primitive(schema, descriptor.getPath()));        }    } else {        for (String column : this.columns) {            columns.put(descriptor(column, schema), primitive(column, schema));        }    }    CompressionCodecName codec = reader.getRowGroups().get(0).getColumns().get(0).getCodec();        Map<String, List<String>> formatted = Maps.newLinkedHashMap();    PageFormatter formatter = new PageFormatter();    PageReadStore pageStore;    int rowGroupNum = 0;    while ((pageStore = reader.readNextRowGroup()) != null) {        for (ColumnDescriptor descriptor : columns.keySet()) {            List<String> lines = formatted.get(columnName(descriptor));            if (lines == null) {                lines = Lists.newArrayList();                formatted.put(columnName(descriptor), lines);            }            formatter.setContext(rowGroupNum, columns.get(descriptor), codec);            PageReader pages = pageStore.getPageReader(descriptor);            DictionaryPage dict = pages.readDictionaryPage();            if (dict != null) {                lines.add(formatter.format(dict));            }            DataPage page;            while ((page = pages.readPage()) != null) {                lines.add(formatter.format(page));            }        }        rowGroupNum += 1;    }        for (String columnName : formatted.keySet()) {        console.info(String.format("\nColumn: %s\n%s", columnName, StringUtils.leftPad("", 80, '-')));        console.info(formatter.getHeader());        for (String line : formatted.get(columnName)) {            console.info(line);        }        console.info("");    }    return 0;}
0
public List<String> getExamples()
{    return Lists.newArrayList("# Show pages for column 'col' from a Parquet file", "-c col sample.parquet");}
0
 String getHeader()
{    return String.format("  %-6s %-5s %-4s %-7s %-10s %-10s %-8s %-7s %s", "page", "type", "enc", "count", "avg size", "size", "rows", "nulls", "min / max");}
0
 void setContext(int rowGroupNum, PrimitiveType type, CompressionCodecName codec)
{    this.rowGroupNum = rowGroupNum;    this.pageNum = 0;    this.type = type;    this.shortCodec = shortCodec(codec);}
0
 String format(Page page)
{    String formatted = "";    if (page instanceof DictionaryPage) {        formatted = printDictionaryPage((DictionaryPage) page);    } else if (page instanceof DataPage) {        formatted = ((DataPage) page).accept(this);    }    pageNum += 1;    return formatted;}
0
private String printDictionaryPage(DictionaryPage dict)
{        dict.getUncompressedSize();    long totalSize = dict.getCompressedSize();    int count = dict.getDictionarySize();    float perValue = ((float) totalSize) / count;    String enc = encodingAsString(dict.getEncoding(), true);    if (pageNum == 0) {        return String.format("%3d-D    %-5s %s %-2s %-7d %-10s %-10s", rowGroupNum, "dict", shortCodec, enc, count, humanReadable(perValue), humanReadable(totalSize));    } else {        return String.format("%3d-%-3d  %-5s %s %-2s %-7d %-10s %-10s", rowGroupNum, pageNum, "dict", shortCodec, enc, count, humanReadable(perValue), humanReadable(totalSize));    }}
0
public String visit(DataPageV1 page)
{    String enc = encodingAsString(page.getValueEncoding(), false);    long totalSize = page.getCompressedSize();    int count = page.getValueCount();    String numNulls = page.getStatistics().isNumNullsSet() ? Long.toString(page.getStatistics().getNumNulls()) : "";    float perValue = ((float) totalSize) / count;    String minMax = minMaxAsString(page.getStatistics());    return String.format("%3d-%-3d  %-5s %s %-2s %-7d %-10s %-10s %-8s %-7s %s", rowGroupNum, pageNum, "data", shortCodec, enc, count, humanReadable(perValue), humanReadable(totalSize), "", numNulls, minMax);}
0
public String visit(DataPageV2 page)
{    String enc = encodingAsString(page.getDataEncoding(), false);    long totalSize = page.getCompressedSize();    int count = page.getValueCount();    int numRows = page.getRowCount();    int numNulls = page.getNullCount();    float perValue = ((float) totalSize) / count;    String minMax = minMaxAsString(page.getStatistics());    String compression = (page.isCompressed() ? shortCodec : "_");    return String.format("%3d-%-3d  %-5s %s %-2s %-7d %-10s %-10s %-8d %-7s %s", rowGroupNum, pageNum, "data", compression, enc, count, humanReadable(perValue), humanReadable(totalSize), numRows, numNulls, minMax);}
0
public int run() throws IOException
{    Preconditions.checkArgument(targets != null && targets.size() == 1, "A data file is required.");    String source = targets.get(0);    CodecFactory codecFactory = Codecs.avroCodec(compressionCodecName);    final Schema schema;    if (avroSchemaFile != null) {        schema = Schemas.fromAvsc(open(avroSchemaFile));    } else {        schema = getAvroSchema(source);    }    final Schema projection = filterSchema(schema, columns);    Path outPath = qualifiedPath(outputPath);    try (FileSystem outFS = outPath.getFileSystem(getConf())) {        if (overwrite && outFS.exists(outPath)) {            console.debug("Deleting output file {} (already exists)", outPath);            outFS.delete(outPath);        }    }    Iterable<Record> reader = openDataFile(source, projection);    boolean threw = true;    long count = 0;    DatumWriter<Record> datumWriter = new GenericDatumWriter<>(schema);    try (DataFileWriter<Record> fileWriter = new DataFileWriter<>(datumWriter)) {        fileWriter.setCodec(codecFactory);        try (DataFileWriter<Record> writer = fileWriter.create(projection, create(outputPath))) {            for (Record record : reader) {                writer.append(record);                count += 1;            }        }        threw = false;    } catch (RuntimeException e) {        throw new RuntimeException("Failed on record " + count, e);    } finally {        if (reader instanceof Closeable) {            Closeables.close((Closeable) reader, threw);        }    }    return 0;}
0
public List<String> getExamples()
{    return Lists.newArrayList("# Create an Avro file from a Parquet file", "sample.parquet sample.avro", "# Create an Avro file in HDFS from a local JSON file", "path/to/sample.json hdfs:/user/me/sample.parquet", "# Create an Avro file from data in S3", "s3:/data/path/sample.parquet sample.avro");}
0
 static CSVReader newReader(InputStream incoming, CSVProperties props)
{    return new CSVReader(new InputStreamReader(incoming, Charset.forName(props.charset)), props.delimiter.charAt(0), props.quote.charAt(0), props.escape.charAt(0), props.linesToSkip, false, /* strict quotes off: don't ignore unquoted strings */    true);}
0
 static CSVParser newParser(CSVProperties props)
{    return new CSVParser(props.delimiter.charAt(0), props.quote.charAt(0), props.escape.charAt(0), false, /* strict quotes off: don't ignore unquoted strings */    true);}
0
public static Schema inferNullableSchema(String name, InputStream incoming, CSVProperties props) throws IOException
{    return inferSchemaInternal(name, incoming, props, NO_REQUIRED_FIELDS, true);}
0
public static Schema inferNullableSchema(String name, InputStream incoming, CSVProperties props, Set<String> requiredFields) throws IOException
{    return inferSchemaInternal(name, incoming, props, requiredFields, true);}
0
public static Schema inferSchema(String name, InputStream incoming, CSVProperties props) throws IOException
{    return inferSchemaInternal(name, incoming, props, NO_REQUIRED_FIELDS, false);}
0
public static Schema inferSchema(String name, InputStream incoming, CSVProperties props, Set<String> requiredFields) throws IOException
{    return inferSchemaInternal(name, incoming, props, requiredFields, false);}
0
private static Schema inferSchemaInternal(String name, InputStream incoming, CSVProperties props, Set<String> requiredFields, boolean makeNullable) throws IOException
{    CSVReader reader = newReader(incoming, props);    String[] header;    String[] line;    if (props.useHeader) {                header = reader.readNext();        line = reader.readNext();        Preconditions.checkNotNull(line, "No content to infer schema");    } else if (props.header != null) {        header = newParser(props).parseLine(props.header);        line = reader.readNext();        Preconditions.checkNotNull(line, "No content to infer schema");    } else {                line = reader.readNext();        Preconditions.checkNotNull(line, "No content to infer schema");        header = new String[line.length];        for (int i = 0; i < line.length; i += 1) {            header[i] = "field_" + String.valueOf(i);        }    }    Schema.Type[] types = new Schema.Type[header.length];    String[] values = new String[header.length];    boolean[] nullable = new boolean[header.length];    boolean[] empty = new boolean[header.length];    for (int processed = 0; processed < DEFAULT_INFER_LINES; processed += 1) {        if (line == null) {            break;        }        for (int i = 0; i < header.length; i += 1) {            if (i < line.length) {                if (types[i] == null) {                    types[i] = inferFieldType(line[i]);                    if (types[i] != null) {                                                values[i] = line[i];                    }                }                if (line[i] == null) {                    nullable[i] = true;                } else if (line[i].isEmpty()) {                    empty[i] = true;                }            } else {                                nullable[i] = true;            }        }        line = reader.readNext();    }    SchemaBuilder.FieldAssembler<Schema> fieldAssembler = SchemaBuilder.record(name).fields();        for (int i = 0; i < header.length; i += 1) {        if (header[i] == null) {            throw new RuntimeException("Bad header for field " + i + ": null");        }        String fieldName = header[i].trim();        if (fieldName.isEmpty()) {            throw new RuntimeException("Bad header for field " + i + ": \"" + fieldName + "\"");        } else if (!isAvroCompatibleName(fieldName)) {            throw new RuntimeException("Bad header for field, should start with a character " + "or _ and can contain only alphanumerics and _ " + i + ": \"" + fieldName + "\"");        }                boolean foundNull = (nullable[i] || (empty[i] && types[i] != Schema.Type.STRING));        if (requiredFields.contains(fieldName)) {            if (foundNull) {                throw new RuntimeException("Found null value for required field: " + fieldName + " (" + types[i] + ")");            }            fieldAssembler = fieldAssembler.name(fieldName).doc("Type inferred from '" + sample(values[i]) + "'").type(schema(types[i], false)).noDefault();        } else {            SchemaBuilder.GenericDefault<Schema> defaultBuilder = fieldAssembler.name(fieldName).doc("Type inferred from '" + sample(values[i]) + "'").type(schema(types[i], makeNullable || foundNull));            if (makeNullable || foundNull) {                fieldAssembler = defaultBuilder.withDefault(null);            } else {                fieldAssembler = defaultBuilder.noDefault();            }        }    }    return fieldAssembler.endRecord();}
0
private static String sample(String value)
{    if (value != null) {        return NON_PRINTABLE.replaceFrom(value.subSequence(0, min(50, value.length())), '.');    } else {        return "null";    }}
0
private static Schema schema(Schema.Type type, boolean makeNullable)
{    Schema schema = Schema.create(type == null ? Schema.Type.STRING : type);    if (makeNullable || type == null) {        schema = Schema.createUnion(Lists.newArrayList(Schema.create(Schema.Type.NULL), schema));    }    return schema;}
0
private static Schema.Type inferFieldType(String example)
{    if (example == null || example.isEmpty()) {                return null;    } else if (LONG.matcher(example).matches()) {        return Schema.Type.LONG;    } else if (DOUBLE.matcher(example).matches()) {        return Schema.Type.DOUBLE;    } else if (FLOAT.matcher(example).matches()) {        return Schema.Type.FLOAT;    }    return Schema.Type.STRING;}
0
private static boolean isAvroCompatibleName(String name)
{    return AVRO_COMPATIBLE.matcher(name).matches();}
0
public boolean hasNext()
{    return hasNext;}
0
public E next()
{    if (!hasNext) {        throw new NoSuchElementException();    }    try {        if (reuseRecords) {            this.record = builder.makeRecord(next, record);            return record;        } else {            return builder.makeRecord(next, null);        }    } finally {        this.hasNext = advance();    }}
0
private boolean advance()
{    try {        next = reader.readNext();    } catch (IOException ex) {        throw new RuntimeIOException("Could not read record", ex);    }    return (next != null);}
0
public void close()
{    try {        reader.close();    } catch (IOException e) {        throw new RuntimeIOException("Cannot close reader", e);    }}
0
public void remove()
{    throw new UnsupportedOperationException("Remove is not implemented.");}
0
public Iterator<E> iterator()
{    return this;}
0
public Builder charset(String charset)
{    this.charset = charset;    return this;}
0
public Builder delimiter(String delimiter)
{    this.delimiter = StringEscapeUtils.unescapeJava(delimiter);    return this;}
0
public Builder quote(String quote)
{    this.quote = StringEscapeUtils.unescapeJava(quote);    return this;}
0
public Builder escape(String escape)
{    this.escape = StringEscapeUtils.unescapeJava(escape);    return this;}
0
public Builder header(String header)
{    this.header = header;    return this;}
0
public Builder hasHeader()
{    this.useHeader = true;    return this;}
0
public Builder hasHeader(boolean hasHeader)
{    this.useHeader = hasHeader;    return this;}
0
public Builder linesToSkip(int linesToSkip)
{    this.linesToSkip = linesToSkip;    return this;}
0
public CSVProperties build()
{    return new CSVProperties(charset, delimiter, quote, escape, header, useHeader, linesToSkip);}
0
public E makeRecord(String[] fields, E reuse)
{    E record = reuse;    if (record == null) {        record = newRecordInstance();    }    if (record instanceof IndexedRecord) {        fillIndexed((IndexedRecord) record, fields);    } else {        fillReflect(record, fields);    }    return record;}
0
private E newRecordInstance()
{    if (recordClass != GenericData.Record.class && !recordClass.isInterface()) {        E record = (E) ReflectData.newInstance(recordClass, schema);        if (record != null) {            return record;        }    }    return (E) new GenericData.Record(schema);}
0
private void fillIndexed(IndexedRecord record, String[] data)
{    for (int i = 0; i < indexes.length; i += 1) {        int index = indexes[i];        record.put(i, makeValue(index < data.length ? data[index] : null, fields[i]));    }}
0
private void fillReflect(Object record, String[] data)
{    for (int i = 0; i < indexes.length; i += 1) {        Schema.Field field = fields[i];        int index = indexes[i];        Object value = makeValue(index < data.length ? data[index] : null, field);        ReflectData.get().setField(record, field.name(), i, value);    }}
0
private static Object makeValue(String string, Schema.Field field)
{    try {        Object value = makeValue(string, field.schema());        if (value != null || Schemas.nullOk(field.schema())) {            return value;        } else {                        return ReflectData.get().getDefaultValue(field);        }    } catch (RecordException e) {                throw new RecordException(String.format("Cannot convert field %s", field.name()), e);    } catch (NumberFormatException e) {        throw new RecordException(String.format("Field %s: value not a %s: '%s'", field.name(), field.schema(), string), e);    } catch (AvroRuntimeException e) {        throw new RecordException(String.format("Field %s: cannot make %s value: '%s'", field.name(), field.schema(), string), e);    }}
0
private static Object makeValue(String string, Schema schema)
{    if (string == null) {        return null;    }    try {        switch(schema.getType()) {            case BOOLEAN:                return Boolean.valueOf(string);            case STRING:                return string;            case FLOAT:                return Float.valueOf(string);            case DOUBLE:                return Double.valueOf(string);            case INT:                return Integer.valueOf(string);            case LONG:                return Long.valueOf(string);            case ENUM:                                if (schema.hasEnumSymbol(string)) {                    return string;                } else {                    try {                        return schema.getEnumSymbols().get(Integer.parseInt(string));                    } catch (IndexOutOfBoundsException ex) {                        return null;                    }                }            case UNION:                Object value = null;                for (Schema possible : schema.getTypes()) {                    value = makeValue(string, possible);                    if (value != null) {                        return value;                    }                }                return null;            case NULL:                return null;            default:                                throw new RecordException("Unsupported field type:" + schema.getType());        }    } catch (NumberFormatException e) {                if (string.isEmpty()) {            return null;        } else {            throw e;        }    }}
0
public static Configuration getDefaultConf()
{    return defaultConf;}
0
public static void setDefaultConf(Configuration defaultConf)
{    HadoopFileSystemURLStreamHandler.defaultConf = defaultConf;}
0
public void setConf(Configuration conf)
{    this.conf = conf;}
0
public Configuration getConf()
{    return conf;}
0
protected URLConnection openConnection(URL url) throws IOException
{    return new HadoopFileSystemURLConnection(url);}
0
public void connect() throws IOException
{}
0
public InputStream getInputStream() throws IOException
{    Path path = new Path(url.toExternalForm());    FileSystem fileSystem = path.getFileSystem(conf);    return fileSystem.open(path);}
0
public void setProgramName(String programName)
{    this.programName = programName;}
0
public int run()
{    if (helpCommands.isEmpty()) {        printGenericHelp();    } else {        for (String cmd : helpCommands) {            JCommander commander = jc.getCommands().get(cmd);            if (commander == null) {                console.error("\nUnknown command: {}\n", cmd);                printGenericHelp();                return 1;            }            boolean hasRequired = false;            console.info("\nUsage: {} [general options] {} {} [command options]", new Object[] { programName, cmd, commander.getMainParameterDescription() });            console.info("\n  Description:");            console.info("\n    {}", jc.getCommandDescription(cmd));            if (!commander.getParameters().isEmpty()) {                console.info("\n  Command options:\n");                for (ParameterDescription param : commander.getParameters()) {                    hasRequired = printOption(console, param) || hasRequired;                }                if (hasRequired) {                    console.info("\n  * = required");                }            }            List<String> examples = ((Command) commander.getObjects().get(0)).getExamples();            if (examples != null) {                console.info("\n  Examples:");                for (String example : examples) {                    if (example.startsWith("#")) {                                                console.info("\n    {}", example);                    } else {                        console.info("    {} {} {}", new Object[] { programName, cmd, example });                    }                }            }                        console.info("");        }    }    return 0;}
0
public void printGenericHelp()
{    boolean hasRequired = false;    console.info("\nUsage: {} [options] [command] [command options]", programName);    console.info("\n  Options:\n");    for (ParameterDescription param : jc.getParameters()) {        hasRequired = printOption(console, param) || hasRequired;    }    if (hasRequired) {        console.info("\n  * = required");    }    console.info("\n  Commands:\n");    for (String command : jc.getCommands().keySet()) {        console.info("    {}\n\t{}", command, jc.getCommandDescription(command));    }    console.info("\n  Examples:");    console.info("\n    # print information for create\n    {} help create", programName);    console.info("\n  See '{} help <command>' for more information on a " + "specific command.", programName);}
0
private boolean printOption(Logger console, ParameterDescription param)
{    boolean required = param.getParameter().required();    if (!param.getParameter().hidden()) {        console.info("  {} {}\n\t{}{}", new Object[] { required ? "*" : " ", param.getNames().trim(), param.getDescription(), formatDefault(param) });    }    return required;}
0
private String formatDefault(ParameterDescription param)
{    Object defaultValue = param.getDefault();    if (defaultValue == null || param.getParameter().arity() < 1) {        return "";    }    return " (default: " + ((defaultValue instanceof String) ? "\"" + defaultValue + "\"" : defaultValue.toString()) + ")";}
0
public List<String> getExamples()
{    return null;}
0
public static Iterator<JsonNode> parser(final InputStream stream)
{    try {                JsonParser parser = FACTORY.createParser(stream);        return parser.readValuesAs(JsonNode.class);    } catch (IOException e) {        throw new RuntimeIOException("Cannot read from stream", e);    }}
0
public static JsonNode parse(String json)
{    return parse(json, JsonNode.class);}
0
public static T parse(String json, Class<T> returnType)
{    try {        return MAPPER.readValue(json, returnType);    } catch (JsonParseException e) {        throw new IllegalArgumentException("Invalid JSON", e);    } catch (JsonMappingException e) {        throw new IllegalArgumentException("Invalid JSON", e);    } catch (IOException e) {        throw new RuntimeIOException("Cannot initialize JSON parser", e);    }}
0
public static JsonNode parse(InputStream json)
{    return parse(json, JsonNode.class);}
0
public static T parse(InputStream json, Class<T> returnType)
{    try {        return MAPPER.readValue(json, returnType);    } catch (JsonParseException e) {        throw new IllegalArgumentException("Invalid JSON stream", e);    } catch (JsonMappingException e) {        throw new IllegalArgumentException("Invalid JSON stream", e);    } catch (IOException e) {        throw new RuntimeIOException("Cannot initialize JSON parser", e);    }}
0
public static Object convertToAvro(GenericData model, JsonNode datum, Schema schema)
{    if (datum == null) {        return null;    }    switch(schema.getType()) {        case RECORD:            RecordException.check(datum.isObject(), "Cannot convert non-object to record: %s", datum);            Object record = model.newRecord(null, schema);            for (Schema.Field field : schema.getFields()) {                model.setField(record, field.name(), field.pos(), convertField(model, datum.get(field.name()), field));            }            return record;        case MAP:            RecordException.check(datum.isObject(), "Cannot convert non-object to map: %s", datum);            Map<String, Object> map = Maps.newLinkedHashMap();            Iterator<Map.Entry<String, JsonNode>> iter = datum.fields();            while (iter.hasNext()) {                Map.Entry<String, JsonNode> entry = iter.next();                map.put(entry.getKey(), convertToAvro(model, entry.getValue(), schema.getValueType()));            }            return map;        case ARRAY:            RecordException.check(datum.isArray(), "Cannot convert to array: %s", datum);            List<Object> list = Lists.newArrayListWithExpectedSize(datum.size());            for (JsonNode element : datum) {                list.add(convertToAvro(model, element, schema.getElementType()));            }            return list;        case UNION:            return convertToAvro(model, datum, resolveUnion(datum, schema.getTypes()));        case BOOLEAN:            RecordException.check(datum.isBoolean(), "Cannot convert to boolean: %s", datum);            return datum.booleanValue();        case FLOAT:            RecordException.check(datum.isFloat() || datum.isInt(), "Cannot convert to float: %s", datum);            return datum.floatValue();        case DOUBLE:            RecordException.check(datum.isDouble() || datum.isFloat() || datum.isLong() || datum.isInt(), "Cannot convert to double: %s", datum);            return datum.doubleValue();        case INT:            RecordException.check(datum.isInt(), "Cannot convert to int: %s", datum);            return datum.intValue();        case LONG:            RecordException.check(datum.isLong() || datum.isInt(), "Cannot convert to long: %s", datum);            return datum.longValue();        case STRING:            RecordException.check(datum.isTextual(), "Cannot convert to string: %s", datum);            return datum.textValue();        case ENUM:            RecordException.check(datum.isTextual(), "Cannot convert to string: %s", datum);            return model.createEnum(datum.textValue(), schema);        case BYTES:            RecordException.check(datum.isBinary(), "Cannot convert to binary: %s", datum);            try {                return ByteBuffer.wrap(datum.binaryValue());            } catch (IOException e) {                throw new RecordException("Failed to read JSON binary", e);            }        case FIXED:            RecordException.check(datum.isBinary(), "Cannot convert to fixed: %s", datum);            byte[] bytes;            try {                bytes = datum.binaryValue();            } catch (IOException e) {                throw new RecordException("Failed to read JSON binary", e);            }            RecordException.check(bytes.length < schema.getFixedSize(), "Binary data is too short: %s bytes for %s", bytes.length, schema);            return model.createFixed(null, bytes, schema);        case NULL:            return null;        default:                        throw new IllegalArgumentException("Unknown schema type: " + schema);    }}
0
private static Object convertField(GenericData model, JsonNode datum, Schema.Field field)
{    try {        Object value = convertToAvro(model, datum, field.schema());        if (value != null || Schemas.nullOk(field.schema())) {            return value;        } else {            return model.getDefaultValue(field);        }    } catch (RecordException e) {                throw new RecordException(String.format("Cannot convert field %s", field.name()), e);    } catch (AvroRuntimeException e) {        throw new RecordException(String.format("Field %s: cannot make %s value: '%s'", field.name(), field.schema(), String.valueOf(datum)), e);    }}
0
private static Schema resolveUnion(JsonNode datum, Collection<Schema> schemas)
{    Set<Schema.Type> primitives = Sets.newHashSet();    List<Schema> others = Lists.newArrayList();    for (Schema schema : schemas) {        if (PRIMITIVES.containsKey(schema.getType())) {            primitives.add(schema.getType());        } else {            others.add(schema);        }    }        Schema primitiveSchema = null;    if (datum == null || datum.isNull()) {        primitiveSchema = closestPrimitive(primitives, Schema.Type.NULL);    } else if (datum.isShort() || datum.isInt()) {        primitiveSchema = closestPrimitive(primitives, Schema.Type.INT, Schema.Type.LONG, Schema.Type.FLOAT, Schema.Type.DOUBLE);    } else if (datum.isLong()) {        primitiveSchema = closestPrimitive(primitives, Schema.Type.LONG, Schema.Type.DOUBLE);    } else if (datum.isFloat()) {        primitiveSchema = closestPrimitive(primitives, Schema.Type.FLOAT, Schema.Type.DOUBLE);    } else if (datum.isDouble()) {        primitiveSchema = closestPrimitive(primitives, Schema.Type.DOUBLE);    } else if (datum.isBoolean()) {        primitiveSchema = closestPrimitive(primitives, Schema.Type.BOOLEAN);    }    if (primitiveSchema != null) {        return primitiveSchema;    }        for (Schema schema : others) {        if (matches(datum, schema)) {            return schema;        }    }    throw new RecordException(String.format("Cannot resolve union: %s not in %s", datum, schemas));}
0
private static Schema closestPrimitive(Set<Schema.Type> possible, Schema.Type... types)
{    for (Schema.Type type : types) {        if (possible.contains(type) && PRIMITIVES.containsKey(type)) {            return PRIMITIVES.get(type);        }    }    return null;}
0
private static boolean matches(JsonNode datum, Schema schema)
{    switch(schema.getType()) {        case RECORD:            if (datum.isObject()) {                                boolean missingField = false;                for (Schema.Field field : schema.getFields()) {                    if (!datum.has(field.name()) && field.defaultVal() == null) {                        missingField = true;                        break;                    }                }                if (!missingField) {                    return true;                }            }            break;        case UNION:            if (resolveUnion(datum, schema.getTypes()) != null) {                return true;            }            break;        case MAP:            if (datum.isObject()) {                return true;            }            break;        case ARRAY:            if (datum.isArray()) {                return true;            }            break;        case BOOLEAN:            if (datum.isBoolean()) {                return true;            }            break;        case FLOAT:            if (datum.isFloat() || datum.isInt()) {                return true;            }            break;        case DOUBLE:            if (datum.isDouble() || datum.isFloat() || datum.isLong() || datum.isInt()) {                return true;            }            break;        case INT:            if (datum.isInt()) {                return true;            }            break;        case LONG:            if (datum.isLong() || datum.isInt()) {                return true;            }            break;        case STRING:            if (datum.isTextual()) {                return true;            }            break;        case ENUM:            if (datum.isTextual() && schema.hasEnumSymbol(datum.textValue())) {                return true;            }            break;        case BYTES:        case FIXED:            if (datum.isBinary()) {                return true;            }            break;        case NULL:            if (datum == null || datum.isNull()) {                return true;            }            break;        default:                        throw new IllegalArgumentException("Unsupported schema: " + schema);    }    return false;}
0
public static Schema inferSchema(InputStream incoming, final String name, int numRecords)
{    Iterator<Schema> schemas = Iterators.transform(parser(incoming), new Function<JsonNode, Schema>() {        @Override        public Schema apply(JsonNode node) {            return inferSchema(node, name);        }    });    if (!schemas.hasNext()) {        return null;    }    Schema result = schemas.next();    for (int i = 1; schemas.hasNext() && i < numRecords; i += 1) {        result = Schemas.merge(result, schemas.next());    }    return result;}
0
public Schema apply(JsonNode node)
{    return inferSchema(node, name);}
0
public static Schema inferSchema(JsonNode node, String name)
{    return visit(node, new JsonSchemaVisitor(name));}
0
public static Schema inferSchemaWithMaps(JsonNode node, String name)
{    return visit(node, new JsonSchemaVisitor(name).useMaps());}
0
public JsonSchemaVisitor useMaps()
{    this.objectsToRecords = false;    return this;}
0
public Schema object(ObjectNode object, Map<String, Schema> fields)
{    if (objectsToRecords || recordLevels.size() < 1) {        List<Schema.Field> recordFields = Lists.newArrayListWithExpectedSize(fields.size());        for (Map.Entry<String, Schema> entry : fields.entrySet()) {            recordFields.add(new Schema.Field(entry.getKey(), entry.getValue(), "Type inferred from '" + object.get(entry.getKey()) + "'", null));        }        Schema recordSchema;        if (recordLevels.size() < 1) {            recordSchema = Schema.createRecord(name, null, null, false);        } else {            recordSchema = Schema.createRecord(DOT.join(recordLevels), null, null, false);        }        recordSchema.setFields(recordFields);        return recordSchema;    } else {                switch(fields.size()) {            case 0:                return Schema.createMap(Schema.create(Schema.Type.NULL));            case 1:                return Schema.createMap(Iterables.getOnlyElement(fields.values()));            default:                return Schema.createMap(Schemas.mergeOrUnion(fields.values()));        }    }}
0
public Schema array(ArrayNode ignored, List<Schema> elementSchemas)
{        switch(elementSchemas.size()) {        case 0:            return Schema.createArray(Schema.create(Schema.Type.NULL));        case 1:            return Schema.createArray(Iterables.getOnlyElement(elementSchemas));        default:            return Schema.createArray(Schemas.mergeOrUnion(elementSchemas));    }}
0
public Schema binary(BinaryNode ignored)
{    return Schema.create(Schema.Type.BYTES);}
0
public Schema text(TextNode ignored)
{    return Schema.create(Schema.Type.STRING);}
0
public Schema number(NumericNode number)
{    if (number.isInt()) {        return Schema.create(Schema.Type.INT);    } else if (number.isLong()) {        return Schema.create(Schema.Type.LONG);    } else if (number.isFloat()) {        return Schema.create(Schema.Type.FLOAT);    } else if (number.isDouble()) {        return Schema.create(Schema.Type.DOUBLE);    } else {        throw new UnsupportedOperationException(number.getClass().getName() + " is not supported");    }}
0
public Schema bool(BooleanNode ignored)
{    return Schema.create(Schema.Type.BOOLEAN);}
0
public Schema nullNode(NullNode ignored)
{    return Schema.create(Schema.Type.NULL);}
0
public Schema missing(MissingNode ignored)
{    throw new UnsupportedOperationException("MissingNode is not supported.");}
0
private static T visit(JsonNode node, JsonTreeVisitor<T> visitor)
{    switch(node.getNodeType()) {        case OBJECT:            Preconditions.checkArgument(node instanceof ObjectNode, "Expected instance of ObjectNode: " + node);                        Map<String, T> fields = Maps.newLinkedHashMap();            Iterator<Map.Entry<String, JsonNode>> iter = node.fields();            while (iter.hasNext()) {                Map.Entry<String, JsonNode> entry = iter.next();                visitor.recordLevels.push(entry.getKey());                fields.put(entry.getKey(), visit(entry.getValue(), visitor));                visitor.recordLevels.pop();            }            return visitor.object((ObjectNode) node, fields);        case ARRAY:            Preconditions.checkArgument(node instanceof ArrayNode, "Expected instance of ArrayNode: " + node);            List<T> elements = Lists.newArrayListWithExpectedSize(node.size());            for (JsonNode element : node) {                elements.add(visit(element, visitor));            }            return visitor.array((ArrayNode) node, elements);        case BINARY:            Preconditions.checkArgument(node instanceof BinaryNode, "Expected instance of BinaryNode: " + node);            return visitor.binary((BinaryNode) node);        case STRING:            Preconditions.checkArgument(node instanceof TextNode, "Expected instance of TextNode: " + node);            return visitor.text((TextNode) node);        case NUMBER:            Preconditions.checkArgument(node instanceof NumericNode, "Expected instance of NumericNode: " + node);            return visitor.number((NumericNode) node);        case BOOLEAN:            Preconditions.checkArgument(node instanceof BooleanNode, "Expected instance of BooleanNode: " + node);            return visitor.bool((BooleanNode) node);        case MISSING:            Preconditions.checkArgument(node instanceof MissingNode, "Expected instance of MissingNode: " + node);            return visitor.missing((MissingNode) node);        case NULL:            Preconditions.checkArgument(node instanceof NullNode, "Expected instance of NullNode: " + node);            return visitor.nullNode((NullNode) node);        default:            throw new IllegalArgumentException("Unknown node type: " + node.getNodeType() + ": " + node);    }}
0
public T object(ObjectNode object, Map<String, T> fields)
{    return null;}
0
public T array(ArrayNode array, List<T> elements)
{    return null;}
0
public T binary(BinaryNode binary)
{    return null;}
0
public T text(TextNode text)
{    return null;}
0
public T number(NumericNode number)
{    return null;}
0
public T bool(BooleanNode bool)
{    return null;}
0
public T missing(MissingNode missing)
{    return null;}
0
public T nullNode(NullNode nullNode)
{    return null;}
0
public boolean hasNext()
{    return iterator.hasNext();}
0
public E next()
{    return iterator.next();}
0
public void close()
{    iterator = null;    try {        stream.close();    } catch (IOException e) {        throw new RuntimeIOException("Cannot close reader", e);    }}
0
public void remove()
{    throw new UnsupportedOperationException("Remove is not implemented.");}
0
public Iterator<E> iterator()
{    return this;}
0
public int run(String[] args) throws Exception
{    try {        jc.parse(args);    } catch (MissingCommandException e) {        console.error(e.getMessage());        return 1;    } catch (ParameterException e) {        help.setProgramName(programName);        String cmd = jc.getParsedCommand();        if (args.length == 1) {                        help.helpCommands.add(cmd);            help.run();            return 1;        } else {                        for (String arg : args) {                if (HELP_ARGS.contains(arg)) {                    help.helpCommands.add(cmd);                    help.run();                    return 0;                }            }        }        console.error(e.getMessage());        return 1;    }    help.setProgramName(programName);        if (debug) {        org.apache.log4j.Logger console = org.apache.log4j.Logger.getLogger(Main.class);        console.setLevel(Level.DEBUG);    }    String parsed = jc.getParsedCommand();    if (parsed == null) {        help.run();        return 1;    } else if ("help".equals(parsed)) {        return help.run();    }    Command command = (Command) jc.getCommands().get(parsed).getObjects().get(0);    if (command == null) {        help.run();        return 1;    }    try {        if (command instanceof Configurable) {            ((Configurable) command).setConf(getConf());        }        return command.run();    } catch (IllegalArgumentException e) {        if (debug) {            console.error("Argument error", e);        } else {            console.error("Argument error: {}", e.getMessage());        }        return 1;    } catch (IllegalStateException e) {        if (debug) {            console.error("State error", e);        } else {            console.error("State error: {}", e.getMessage());        }        return 1;    } catch (Exception e) {        console.error("Unknown error", e);        return 1;    }}
0
public static void main(String[] args) throws Exception
{        PropertyConfigurator.configure(Main.class.getResource("/cli-logging.properties"));    Logger console = LoggerFactory.getLogger(Main.class);        LogFactory.getFactory().setAttribute("org.apache.commons.logging.Log", "org.apache.commons.logging.impl.Log4JLogger");    int rc = ToolRunner.run(new Configuration(), new Main(console), args);    System.exit(rc);}
0
public static CompressionCodecName parquetCodec(String codec)
{    try {        return CompressionCodecName.valueOf(codec.toUpperCase(Locale.ENGLISH));    } catch (IllegalArgumentException e) {        throw new IllegalArgumentException("Unknown compression codec: " + codec);    }}
0
public static CodecFactory avroCodec(String codec)
{    CompressionCodecName parquetCodec = parquetCodec(codec);    switch(parquetCodec) {        case UNCOMPRESSED:            return CodecFactory.nullCodec();        case SNAPPY:            return CodecFactory.snappyCodec();        case GZIP:            return CodecFactory.deflateCodec(9);        case ZSTD:            return CodecFactory.zstandardCodec(CodecFactory.DEFAULT_ZSTANDARD_LEVEL);        default:            throw new IllegalArgumentException("Codec incompatible with Avro: " + codec);    }}
0
public static Object select(Schema schema, Object datum, String path)
{    return select(schema, datum, Lists.newArrayList(parse(path)));}
0
private static Object select(Schema schema, Object datum, List<PathExpr> tokens)
{    if (tokens.isEmpty()) {        return datum;    }    Preconditions.checkArgument(tokens.size() == 1, "Cannot return multiple values");    PathExpr token = tokens.get(0);    switch(schema.getType()) {        case RECORD:            if (!(datum instanceof GenericRecord) && "json".equals(schema.getName())) {                                return select(schema.getField("value").schema(), datum, tokens);            }            Preconditions.checkArgument(token.type == PathExpr.Type.FIELD, "Cannot dereference records");            Preconditions.checkArgument(datum instanceof GenericRecord, "Not a record: %s", datum);            GenericRecord record = (GenericRecord) datum;            Schema.Field field = schema.getField(token.value);            Preconditions.checkArgument(field != null, "No such field '%s' in schema: %s", token.value, schema);            return select(field.schema(), record.get(token.value), token.children);        case MAP:            Preconditions.checkArgument(datum instanceof Map, "Not a map: %s", datum);            Map<Object, Object> map = (Map<Object, Object>) datum;            Object value = map.get(token.value);            if (value == null) {                                value = map.get(new Utf8(token.value));            }            return select(schema.getValueType(), value, token.children);        case ARRAY:            Preconditions.checkArgument(token.type == PathExpr.Type.DEREF, "Cannot access fields of an array");            Preconditions.checkArgument(datum instanceof Collection, "Not an array: %s", datum);            Preconditions.checkArgument(NUMERIC_RE.matcher(token.value).matches(), "Not an array index: %s", token.value);            List<Object> list = (List<Object>) datum;            return select(schema.getElementType(), list.get(Integer.parseInt(token.value)), token.children);        case UNION:            int branch = GenericData.get().resolveUnion(schema, datum);            return select(schema.getTypes().get(branch), datum, tokens);        default:            throw new IllegalArgumentException("Cannot access child of primitive value: " + datum);    }}
0
public static Schema filterSchema(Schema schema, String... fieldPaths)
{    return filterSchema(schema, Lists.newArrayList(fieldPaths));}
0
public static Schema filterSchema(Schema schema, List<String> fieldPaths)
{    if (fieldPaths == null) {        return schema;    }    List<PathExpr> paths = merge(Lists.newArrayList(fieldPaths));    return filter(schema, paths);}
0
private static PathExpr parse(String path)
{    PathExpr expr = null;    PathExpr last = null;    boolean inDeref = false;    boolean afterDeref = false;    int valueStart = 0;    for (int i = 0; i < path.length(); i += 1) {        switch(path.charAt(i)) {            case '.':                Preconditions.checkState(valueStart != i || afterDeref, "Empty reference: ''");                if (!inDeref) {                    if (valueStart != i) {                        PathExpr current = PathExpr.field(path.substring(valueStart, i));                        if (last != null) {                            last.children.add(current);                        } else {                            expr = current;                        }                        last = current;                    }                    valueStart = i + 1;                    afterDeref = false;                }                break;            case '[':                Preconditions.checkState(!inDeref, "Cannot nest [ within []");                Preconditions.checkState(valueStart != i || afterDeref, "Empty reference: ''");                if (valueStart != i) {                    PathExpr current = PathExpr.field(path.substring(valueStart, i));                    if (last != null) {                        last.children.add(current);                    } else {                        expr = current;                    }                    last = current;                }                valueStart = i + 1;                inDeref = true;                afterDeref = false;                break;            case ']':                Preconditions.checkState(inDeref, "Cannot use ] without a starting [");                Preconditions.checkState(valueStart != i, "Empty reference: ''");                PathExpr current = PathExpr.deref(path.substring(valueStart, i));                if (last != null) {                    last.children.add(current);                } else {                    expr = current;                }                last = current;                valueStart = i + 1;                inDeref = false;                afterDeref = true;                break;            default:                Preconditions.checkState(!afterDeref, "Fields after [] must start with .");        }    }    Preconditions.checkState(!inDeref, "Fields after [ must end with ]");    if (valueStart < path.length()) {        PathExpr current = PathExpr.field(path.substring(valueStart, path.length()));        if (last != null) {            last.children.add(current);        } else {            expr = current;        }    }    return expr;}
0
private static List<PathExpr> merge(List<String> fields)
{    List<PathExpr> paths = Lists.newArrayList();    for (String field : fields) {        merge(paths, parse(field));    }    return paths;}
0
private static List<PathExpr> merge(List<PathExpr> tokens, PathExpr toAdd)
{    boolean merged = false;    for (PathExpr token : tokens) {        if ((token.type == toAdd.type) && (token.type == PathExpr.Type.DEREF || token.value.equals(toAdd.value))) {            for (PathExpr child : toAdd.children) {                merge(token.children, child);            }            merged = true;        }    }    if (!merged) {        tokens.add(toAdd);    }    return tokens;}
0
private static Schema filter(Schema schema, List<PathExpr> exprs)
{    if (exprs.isEmpty()) {        return schema;    }    switch(schema.getType()) {        case RECORD:            List<Schema.Field> fields = Lists.newArrayList();            for (PathExpr expr : exprs) {                Schema.Field field = schema.getField(expr.value);                Preconditions.checkArgument(field != null, "Cannot find field '%s' in schema: %s", expr.value, schema);                fields.add(new Schema.Field(expr.value, filter(field.schema(), expr.children), field.doc(), field.defaultVal(), field.order()));            }            return Schema.createRecord(schema.getName(), schema.getDoc(), schema.getNamespace(), schema.isError(), fields);        case UNION:                        if (schema.getTypes().size() == 2) {                if (schema.getTypes().get(0).getType() == Schema.Type.NULL) {                    return filter(schema.getTypes().get(1), exprs);                } else if (schema.getTypes().get(1).getType() == Schema.Type.NULL) {                    return filter(schema.getTypes().get(0), exprs);                }            }            List<Schema> schemas = Lists.newArrayList();            for (PathExpr expr : exprs) {                schemas.add(filter(schema, expr));            }            if (schemas.size() > 1) {                return Schema.createUnion(schemas);            } else {                return schemas.get(0);            }        case MAP:            Preconditions.checkArgument(exprs.size() == 1, "Cannot find multiple children of map schema: %s", schema);            return filter(schema, exprs.get(0));        case ARRAY:            Preconditions.checkArgument(exprs.size() == 1, "Cannot find multiple children of array schema: %s", schema);            return filter(schema, exprs.get(0));        default:            throw new IllegalArgumentException(String.format("Cannot find child of primitive schema: %s", schema));    }}
0
private static Schema filter(Schema schema, PathExpr expr)
{    if (expr == null) {        return schema;    }    switch(schema.getType()) {        case RECORD:            Preconditions.checkArgument(expr.type == PathExpr.Type.FIELD, "Cannot index a record: [%s]", expr.value);            Schema.Field field = schema.getField(expr.value);            if (field != null) {                return filter(field.schema(), expr.children);            } else {                throw new IllegalArgumentException(String.format("Cannot find field '%s' in schema: %s", expr.value, schema.toString(true)));            }        case MAP:            return Schema.createMap(filter(schema.getValueType(), expr.children));        case ARRAY:            Preconditions.checkArgument(expr.type == PathExpr.Type.DEREF, "Cannot find field '%s' in an array", expr.value);            Preconditions.checkArgument(NUMERIC_RE.matcher(expr.value).matches(), "Cannot index array by non-numeric value '%s'", expr.value);            return Schema.createArray(filter(schema.getElementType(), expr.children));        case UNION:                                    Preconditions.checkArgument(expr.type == PathExpr.Type.DEREF, "Cannot find field '%s' in a union", expr.value);            List<Schema> options = schema.getTypes();            if (NUMERIC_RE.matcher(expr.value).matches()) {                                int i = Integer.parseInt(expr.value);                if (i < options.size()) {                    return filter(options.get(i), expr.children);                }            } else {                                for (Schema option : options) {                    if (expr.value.equalsIgnoreCase(option.getName())) {                        return filter(option, expr.children);                    }                }            }            throw new IllegalArgumentException(String.format("Invalid union index '%s' for schema: %s", expr.value, schema));        default:            throw new IllegalArgumentException(String.format("Cannot find '%s' in primitive schema: %s", expr.value, schema));    }}
0
 static PathExpr deref(String value)
{    return new PathExpr(Type.DEREF, value);}
0
 static PathExpr deref(String value, PathExpr child)
{    return new PathExpr(Type.DEREF, value, Lists.newArrayList(child));}
0
 static PathExpr field(String value)
{    return new PathExpr(Type.FIELD, value);}
0
 static PathExpr field(String value, PathExpr child)
{    return new PathExpr(Type.FIELD, value, Lists.newArrayList(child));}
0
public boolean equals(Object o)
{    if (this == o)        return true;    if (o == null || getClass() != o.getClass())        return false;    PathExpr pathExpr = (PathExpr) o;    if (type != pathExpr.type)        return false;    if (value != null ? !value.equals(pathExpr.value) : pathExpr.value != null)        return false;    return children != null ? children.equals(pathExpr.children) : pathExpr.children == null;}
0
public int hashCode()
{    int result = type != null ? type.hashCode() : 0;    result = 31 * result + (value != null ? value.hashCode() : 0);    result = 31 * result + (children != null ? children.hashCode() : 0);    return result;}
0
public String toString()
{    return MoreObjects.toStringHelper(this).add("type", type).add("value", value).add("children", children).toString();}
0
public static Format detectFormat(InputStream stream) throws IOException
{    byte[] first3 = new byte[3];    stream.read(first3);    if (Arrays.equals(first3, new byte[] { 'P', 'A', 'R' })) {        return Format.PARQUET;    } else if (Arrays.equals(first3, new byte[] { 'O', 'b', 'j' })) {        return Format.AVRO;    } else if (Arrays.equals(first3, new byte[] { 'S', 'E', 'Q' })) {        return Format.SEQUENCE;    } else {        return Format.TEXT;    }}
0
public ClassLoader run()
{    return new URLClassLoader(urls, Thread.currentThread().getContextClassLoader());}
0
public static void check(boolean isValid, String message, Object... args)
{    if (!isValid) {        String[] argStrings = new String[args.length];        for (int i = 0; i < args.length; i += 1) {            argStrings[i] = String.valueOf(args[i]);        }        throw new RecordException(String.format(String.valueOf(message), (Object[]) argStrings));    }}
0
public static Schema fromAvsc(InputStream in) throws IOException
{        return new Schema.Parser().parse(in);}
0
public static Schema fromAvro(InputStream in) throws IOException
{    GenericDatumReader<GenericRecord> datumReader = new GenericDatumReader<GenericRecord>();    DataFileStream<GenericRecord> stream = null;    boolean threw = true;    try {        stream = new DataFileStream<>(in, datumReader);        Schema schema = stream.getSchema();        threw = false;        return schema;    } finally {        Closeables.close(stream, threw);    }}
0
public static Schema fromParquet(Configuration conf, URI location) throws IOException
{    Path path = new Path(location);    FileSystem fs = path.getFileSystem(conf);    ParquetMetadata footer = ParquetFileReader.readFooter(fs.getConf(), path);    String schemaString = footer.getFileMetaData().getKeyValueMetaData().get("parquet.avro.schema");    if (schemaString == null) {                schemaString = footer.getFileMetaData().getKeyValueMetaData().get("avro.schema");    }    if (schemaString != null) {        return new Schema.Parser().parse(schemaString);    } else {        return new AvroSchemaConverter().convert(footer.getFileMetaData().getSchema());    }}
0
public static Schema fromJSON(String name, InputStream in) throws IOException
{    return AvroJson.inferSchema(in, name, 20);}
0
public static boolean nullOk(Schema schema)
{    if (Schema.Type.NULL == schema.getType()) {        return true;    } else if (Schema.Type.UNION == schema.getType()) {        for (Schema possible : schema.getTypes()) {            if (nullOk(possible)) {                return true;            }        }    }    return false;}
0
public static Schema merge(Iterable<Schema> schemas)
{    Iterator<Schema> iter = schemas.iterator();    if (!iter.hasNext()) {        return null;    }    Schema result = iter.next();    while (iter.hasNext()) {        result = merge(result, iter.next());    }    return result;}
0
public static Schema mergeOrUnion(Iterable<Schema> schemas)
{    Iterator<Schema> iter = schemas.iterator();    if (!iter.hasNext()) {        return null;    }    Schema result = iter.next();    while (iter.hasNext()) {        result = mergeOrUnion(result, iter.next());    }    return result;}
0
public static Schema merge(Schema left, Schema right)
{    Schema merged = mergeOnly(left, right);    Preconditions.checkState(merged != null, "Cannot merge %s and %s", left, right);    return merged;}
0
private static Schema mergeOrUnion(Schema left, Schema right)
{    Schema merged = mergeOnly(left, right);    if (merged != null) {        return merged;    }    return union(left, right);}
0
private static Schema union(Schema left, Schema right)
{    if (left.getType() == Schema.Type.UNION) {        if (right.getType() == Schema.Type.UNION) {                        Schema combined = left;            for (Schema type : right.getTypes()) {                combined = union(combined, type);            }            return combined;        } else {            boolean notMerged = true;                        List<Schema> types = Lists.newArrayList();            Iterator<Schema> schemas = left.getTypes().iterator();                        while (schemas.hasNext()) {                Schema next = schemas.next();                Schema merged = mergeOnly(next, right);                if (merged != null) {                    types.add(merged);                    notMerged = false;                    break;                } else {                                        types.add(next);                }            }                        while (schemas.hasNext()) {                types.add(schemas.next());            }            if (notMerged) {                types.add(right);            }            return Schema.createUnion(types);        }    } else if (right.getType() == Schema.Type.UNION) {        return union(right, left);    }    return Schema.createUnion(ImmutableList.of(left, right));}
0
private static Schema mergeOnly(Schema left, Schema right)
{    if (Objects.equal(left, right)) {        return left;    }        switch(left.getType()) {        case INT:            if (right.getType() == Schema.Type.LONG) {                return right;            }            break;        case LONG:            if (right.getType() == Schema.Type.INT) {                return left;            }            break;        case FLOAT:            if (right.getType() == Schema.Type.DOUBLE) {                return right;            }            break;        case DOUBLE:            if (right.getType() == Schema.Type.FLOAT) {                return left;            }    }        if (left.getType() != right.getType()) {        return null;    }    switch(left.getType()) {        case UNION:            return union(left, right);        case RECORD:            if (left.getName() == null && right.getName() == null && fieldSimilarity(left, right) < SIMILARITY_THRESH) {                return null;            } else if (!Objects.equal(left.getName(), right.getName())) {                return null;            }            Schema combinedRecord = Schema.createRecord(coalesce(left.getName(), right.getName()), coalesce(left.getDoc(), right.getDoc()), coalesce(left.getNamespace(), right.getNamespace()), false);            combinedRecord.setFields(mergeFields(left, right));            return combinedRecord;        case MAP:            return Schema.createMap(mergeOrUnion(left.getValueType(), right.getValueType()));        case ARRAY:            return Schema.createArray(mergeOrUnion(left.getElementType(), right.getElementType()));        case ENUM:            if (!Objects.equal(left.getName(), right.getName())) {                return null;            }            Set<String> symbols = Sets.newLinkedHashSet();            symbols.addAll(left.getEnumSymbols());            symbols.addAll(right.getEnumSymbols());            return Schema.createEnum(left.getName(), coalesce(left.getDoc(), right.getDoc()), coalesce(left.getNamespace(), right.getNamespace()), ImmutableList.copyOf(symbols));        default:                        throw new UnsupportedOperationException("Unknown schema type: " + left.getType());    }}
0
private static Schema nullableForDefault(Schema schema)
{    if (schema.getType() == Schema.Type.NULL) {        return schema;    }    if (schema.getType() != Schema.Type.UNION) {        return Schema.createUnion(ImmutableList.of(NULL, schema));    }    if (schema.getTypes().get(0).getType() == Schema.Type.NULL) {        return schema;    }    List<Schema> types = Lists.newArrayList();    types.add(NULL);    for (Schema type : schema.getTypes()) {        if (type.getType() != Schema.Type.NULL) {            types.add(type);        }    }    return Schema.createUnion(types);}
0
private static List<Schema.Field> mergeFields(Schema left, Schema right)
{    List<Schema.Field> fields = Lists.newArrayList();    for (Schema.Field leftField : left.getFields()) {        Schema.Field rightField = right.getField(leftField.name());        if (rightField != null) {            fields.add(new Schema.Field(leftField.name(), mergeOrUnion(leftField.schema(), rightField.schema()), coalesce(leftField.doc(), rightField.doc()), coalesce(leftField.defaultVal(), rightField.defaultVal())));        } else {            if (leftField.defaultVal() != null) {                fields.add(copy(leftField));            } else {                fields.add(new Schema.Field(leftField.name(), nullableForDefault(leftField.schema()), leftField.doc(), NULL_DEFAULT));            }        }    }    for (Schema.Field rightField : right.getFields()) {        if (left.getField(rightField.name()) == null) {            if (rightField.defaultVal() != null) {                fields.add(copy(rightField));            } else {                fields.add(new Schema.Field(rightField.name(), nullableForDefault(rightField.schema()), rightField.doc(), NULL_DEFAULT));            }        }    }    return fields;}
0
public static Schema.Field copy(Schema.Field field)
{    return new Schema.Field(field.name(), field.schema(), field.doc(), field.defaultVal());}
0
private static float fieldSimilarity(Schema left, Schema right)
{        Set<String> leftNames = names(left.getFields());    Set<String> rightNames = names(right.getFields());    int common = Sets.intersection(leftNames, rightNames).size();    float leftRatio = ((float) common) / ((float) leftNames.size());    float rightRatio = ((float) common) / ((float) rightNames.size());    return hmean(leftRatio, rightRatio);}
0
private static Set<String> names(Collection<Schema.Field> fields)
{    Set<String> names = Sets.newHashSet();    for (Schema.Field field : fields) {        names.add(field.name());    }    return names;}
0
private static float hmean(float left, float right)
{    return (2.0f * left * right) / (left + right);}
0
private static E coalesce(E... objects)
{    for (E object : objects) {        if (object != null) {            return object;        }    }    return null;}
0
public void seek(long p) throws IOException
{    in.seek(p);}
0
public long tell() throws IOException
{    return in.getPos();}
0
public long length() throws IOException
{    return stat.getLen();}
0
public int read(byte[] b) throws IOException
{    return in.read(b);}
0
public int read() throws IOException
{    return in.read();}
0
public int read(byte[] b, int off, int len) throws IOException
{    return in.read(b, off, len);}
0
public void close() throws IOException
{    in.close();}
0
public static String humanReadable(float bytes)
{    if (bytes > TB) {        return String.format("%.03f TB", bytes / TB);    } else if (bytes > GB) {        return String.format("%.03f GB", bytes / GB);    } else if (bytes > MB) {        return String.format("%.03f MB", bytes / MB);    } else if (bytes > KB) {        return String.format("%.03f kB", bytes / KB);    } else {        return String.format("%.02f B", bytes);    }}
0
public static String humanReadable(long bytes)
{    if (bytes > TB) {        return String.format("%.03f TB", ((float) bytes) / TB);    } else if (bytes > GB) {        return String.format("%.03f GB", ((float) bytes) / GB);    } else if (bytes > MB) {        return String.format("%.03f MB", ((float) bytes) / MB);    } else if (bytes > KB) {        return String.format("%.03f kB", ((float) bytes) / KB);    } else {        return String.format("%d B", bytes);    }}
0
public static String minMaxAsString(Statistics stats, OriginalType annotation)
{    return minMaxAsString(stats);}
0
public static String minMaxAsString(Statistics stats)
{    if (stats == null) {        return "no stats";    }    if (!stats.hasNonNullValue()) {        return "";    }    return String.format("%s / %s", humanReadable(stats.minAsString(), 30), humanReadable(stats.maxAsString(), 30));}
0
public static String toString(Statistics stats, long count, OriginalType annotation)
{    return toString(stats, count);}
0
public static String toString(Statistics stats, long count)
{    if (stats == null) {        return "no stats";    }    return String.format("min: %s max: %s nulls: %d/%d", humanReadable(stats.minAsString(), 30), humanReadable(stats.maxAsString(), 30), stats.getNumNulls(), count);}
0
public static String humanReadable(String str, int len)
{    if (str == null) {        return "null";    }    StringBuilder sb = new StringBuilder();    sb.append("\"");    if (str.length() > len - 2) {        sb.append(str.substring(0, len - 5)).append("...");    } else {        sb.append(str);    }    sb.append("\"");    return sb.toString();}
0
public static String humanReadable(byte[] bytes, int len)
{    Preconditions.checkArgument(len >= 5, "Display length must be minimum 5");    if (bytes == null || bytes.length == 0) {        return "null";    }    final String asString = HashCode.fromBytes(bytes).toString();    return "0x" + Ascii.truncate(asString, len - 2, "...");}
0
public static String shortCodec(CompressionCodecName codec)
{    switch(codec) {        case UNCOMPRESSED:            return "_";        case SNAPPY:            return "S";        case GZIP:            return "G";        case LZO:            return "L";        case BROTLI:            return "B";        case LZ4:            return "4";        case ZSTD:            return "Z";        default:            return "?";    }}
0
public static String encodingAsString(Encoding encoding, boolean isDict)
{    switch(encoding) {        case PLAIN:            return "_";        case PLAIN_DICTIONARY:                        return isDict ? "_" : "R";        case RLE_DICTIONARY:            return "R";        case DELTA_BINARY_PACKED:        case DELTA_LENGTH_BYTE_ARRAY:        case DELTA_BYTE_ARRAY:            return "D";        default:            return "?";    }}
0
public static String encodingStatsAsString(EncodingStats encodingStats)
{    StringBuilder sb = new StringBuilder();    if (encodingStats.hasDictionaryPages()) {        for (Encoding encoding : encodingStats.getDictionaryEncodings()) {            sb.append(encodingAsString(encoding, true));        }        sb.append(" ");    } else {        sb.append("  ");    }    Set<Encoding> encodings = encodingStats.getDataEncodings();    if (encodings.contains(RLE_DICTIONARY) || encodings.contains(PLAIN_DICTIONARY)) {        sb.append("R");    }    if (encodings.contains(PLAIN)) {        sb.append("_");    }    if (encodings.contains(DELTA_BYTE_ARRAY) || encodings.contains(DELTA_BINARY_PACKED) || encodings.contains(DELTA_LENGTH_BYTE_ARRAY)) {        sb.append("D");    }        if (encodingStats.hasDictionaryEncodedPages() && encodingStats.hasNonDictionaryEncodedPages()) {        sb.append(" F");    }    return sb.toString();}
0
public static String encodingsAsString(Set<Encoding> encodings, ColumnDescriptor desc)
{    StringBuilder sb = new StringBuilder();    if (encodings.contains(RLE) || encodings.contains(BIT_PACKED)) {        sb.append(desc.getMaxDefinitionLevel() == 0 ? "B" : "R");        sb.append(desc.getMaxRepetitionLevel() == 0 ? "B" : "R");        if (encodings.contains(PLAIN_DICTIONARY)) {            sb.append("R");        }        if (encodings.contains(PLAIN)) {            sb.append("_");        }    } else {        sb.append("RR");        if (encodings.contains(RLE_DICTIONARY)) {            sb.append("R");        }        if (encodings.contains(PLAIN)) {            sb.append("_");        }        if (encodings.contains(DELTA_BYTE_ARRAY) || encodings.contains(DELTA_BINARY_PACKED) || encodings.contains(DELTA_LENGTH_BYTE_ARRAY)) {            sb.append("D");        }    }    return sb.toString();}
0
public static ColumnDescriptor descriptor(String column, MessageType schema)
{    String[] path = Iterables.toArray(DOT.split(column), String.class);    Preconditions.checkArgument(schema.containsPath(path), "Schema doesn't have column: " + column);    return schema.getColumnDescription(path);}
0
public static String columnName(ColumnDescriptor desc)
{    return Joiner.on('.').join(desc.getPath());}
0
public static PrimitiveType primitive(MessageType schema, String[] path)
{    Type current = schema;    for (String part : path) {        current = current.asGroupType().getType(part);        if (current.isPrimitive()) {            return current.asPrimitiveType();        }    }    return null;}
0
public static PrimitiveType primitive(String column, MessageType schema)
{    String[] path = Iterables.toArray(DOT.split(column), String.class);    Preconditions.checkArgument(schema.containsPath(path), "Schema doesn't have column: " + column);    return primitive(schema, path);}
0
public void setUp()
{    this.command = new TestCommand(this.console);}
0
public void qualifiedPathTest() throws IOException
{    Path path = this.command.qualifiedPath(FILE_PATH);    Assert.assertEquals("test.parquet", path.getName());}
0
public void qualifiedURITest() throws IOException
{    URI uri = this.command.qualifiedURI(FILE_PATH);    Assert.assertEquals("/var/tmp/test.parquet", uri.getPath());}
0
public void qualifiedURIResourceURITest() throws IOException
{    URI uri = this.command.qualifiedURI("resource:/a");    Assert.assertEquals("/a", uri.getPath());}
0
public void qualifiedPathTestForWindows() throws IOException
{    Assume.assumeTrue(System.getProperty("os.name").toLowerCase().startsWith("win"));    Path path = this.command.qualifiedPath(WIN_FILE_PATH);    Assert.assertEquals("test.parquet", path.getName());}
0
public void qualifiedURITestForWindows() throws IOException
{    Assume.assumeTrue(System.getProperty("os.name").toLowerCase().startsWith("win"));    URI uri = this.command.qualifiedURI(WIN_FILE_PATH);    Assert.assertEquals("/C:/Test/Downloads/test.parquet", uri.getPath());}
0
public int run() throws IOException
{    return 0;}
0
public List<String> getExamples()
{    return null;}
0
protected File toAvro(File parquetFile) throws IOException
{    return toAvro(parquetFile, "GZIP");}
0
protected File toAvro(File parquetFile, String compressionCodecName) throws IOException
{    ToAvroCommand command = new ToAvroCommand(createLogger());    command.targets = Arrays.asList(parquetFile.getAbsolutePath());    File output = new File(getTempFolder(), getClass().getSimpleName() + ".avro");    command.outputPath = output.getAbsolutePath();    command.compressionCodecName = compressionCodecName;    command.setConf(new Configuration());    int exitCode = command.run();    assert (exitCode == 0);    return output;}
0
public void testCatCommand() throws IOException
{    File file = parquetFile();    CatCommand command = new CatCommand(createLogger(), 0);    command.sourceFiles = Arrays.asList(file.getAbsolutePath());    command.setConf(new Configuration());    Assert.assertEquals(0, command.run());}
0
public void testCheckParquet251Command() throws IOException
{    File file = parquetFile();    CheckParquet251Command command = new CheckParquet251Command(createLogger());    command.files = Arrays.asList(file.getAbsolutePath());    command.setConf(new Configuration());    Assert.assertEquals(0, command.run());}
0
public void testConvertCommand() throws IOException
{    File file = toAvro(parquetFile());    ConvertCommand command = new ConvertCommand(createLogger());    command.targets = Arrays.asList(file.getAbsolutePath());    File output = new File(getTempFolder(), "converted.avro");    command.outputPath = output.getAbsolutePath();    command.setConf(new Configuration());    Assert.assertEquals(0, command.run());    Assert.assertTrue(output.exists());}
0
public void testConvertCSVCommand() throws IOException
{    File file = csvFile();    ConvertCSVCommand command = new ConvertCSVCommand(createLogger());    command.targets = Arrays.asList(file.getAbsolutePath());    File output = new File(getTempFolder(), getClass().getSimpleName() + ".parquet");    command.outputPath = output.getAbsolutePath();    command.setConf(new Configuration());    Assert.assertEquals(0, command.run());    Assert.assertTrue(output.exists());}
0
public void setUp() throws IOException
{    createTestCSVFile();}
0
protected File csvFile()
{    File tmpDir = getTempFolder();    return new File(tmpDir, getClass().getSimpleName() + ".csv");}
0
private void createTestCSVFile() throws IOException
{    File file = csvFile();    try (BufferedWriter writer = new BufferedWriter(new FileWriter(file))) {        writer.write(String.format("%s,%s,%s\n", INT32_FIELD, INT64_FIELD, BINARY_FIELD));        writer.write(String.format("%d,%d,\"%s\"\n", Integer.MIN_VALUE, Long.MIN_VALUE, COLORS[0]));        writer.write(String.format("%d,%d,\"%s\"\n", Integer.MAX_VALUE, Long.MAX_VALUE, COLORS[1]));    }}
0
public void testCSVSchemaCommand() throws IOException
{    File file = csvFile();    CSVSchemaCommand command = new CSVSchemaCommand(createLogger());    command.samplePaths = Arrays.asList(file.getAbsolutePath());    command.recordName = "Test";    command.setConf(new Configuration());    Assert.assertEquals(0, command.run());}
0
protected File getTempFolder()
{    return this.tempFolder.getRoot();}
0
protected static Logger createLogger()
{    PropertyConfigurator.configure(ParquetFileTest.class.getResource("/cli-logging.properties"));    Logger console = LoggerFactory.getLogger(ParquetFileTest.class);    LogFactory.getFactory().setAttribute("org.apache.commons.logging.Log", "org.apache.commons.logging.impl.Log4JLogger");    return console;}
0
public void setUp() throws IOException
{    createTestParquetFile();}
0
protected File parquetFile()
{    File tmpDir = getTempFolder();    return new File(tmpDir, getClass().getSimpleName() + ".parquet");}
0
private static MessageType createSchema()
{    return new MessageType("schema", new PrimitiveType(REQUIRED, PrimitiveTypeName.INT32, INT32_FIELD), new PrimitiveType(REQUIRED, PrimitiveTypeName.INT64, INT64_FIELD), new PrimitiveType(REQUIRED, PrimitiveTypeName.FLOAT, FLOAT_FIELD), new PrimitiveType(REQUIRED, PrimitiveTypeName.DOUBLE, DOUBLE_FIELD), new PrimitiveType(REQUIRED, PrimitiveTypeName.BINARY, BINARY_FIELD), new PrimitiveType(REQUIRED, PrimitiveTypeName.FIXED_LEN_BYTE_ARRAY, 12, FIXED_LEN_BYTE_ARRAY_FIELD));}
0
private void createTestParquetFile() throws IOException
{    File file = parquetFile();    Path fsPath = new Path(file.getPath());    Configuration conf = new Configuration();    MessageType schema = createSchema();    SimpleGroupFactory fact = new SimpleGroupFactory(schema);    GroupWriteSupport.setSchema(schema, conf);    try (ParquetWriter<Group> writer = new ParquetWriter<>(fsPath, new GroupWriteSupport(), CompressionCodecName.UNCOMPRESSED, 1024, 1024, 512, true, false, ParquetProperties.WriterVersion.PARQUET_2_0, conf)) {        for (int i = 0; i < 10; i++) {            final byte[] bytes = new byte[12];            ThreadLocalRandom.current().nextBytes(bytes);            writer.write(fact.newGroup().append(INT32_FIELD, 32 + i).append(INT64_FIELD, 64L + i).append(FLOAT_FIELD, 1.0f + i).append(DOUBLE_FIELD, 2.0d + i).append(BINARY_FIELD, Binary.fromString(COLORS[i % COLORS.length])).append(FIXED_LEN_BYTE_ARRAY_FIELD, Binary.fromConstantByteArray(bytes)));        }    }}
0
public void testParquetMetadataCommand() throws IOException
{    File file = parquetFile();    ParquetMetadataCommand command = new ParquetMetadataCommand(createLogger());    command.targets = Arrays.asList(file.getAbsolutePath());    command.setConf(new Configuration());    Assert.assertEquals(0, command.run());}
0
public void testSchemaCommand() throws IOException
{    File file = parquetFile();    SchemaCommand command = new SchemaCommand(createLogger());    command.targets = Arrays.asList(file.getAbsolutePath());    command.setConf(new Configuration());    Assert.assertEquals(0, command.run());}
0
public void testShowColumnIndexCommand() throws IOException
{    File file = parquetFile();    ShowColumnIndexCommand command = new ShowColumnIndexCommand(createLogger());    command.files = Arrays.asList(file.getAbsolutePath());    command.setConf(new Configuration());    Assert.assertEquals(0, command.run());}
0
public void testShowDirectoryCommand() throws IOException
{    File file = parquetFile();    ShowDictionaryCommand command = new ShowDictionaryCommand(createLogger());    command.targets = Arrays.asList(file.getAbsolutePath());    command.column = BINARY_FIELD;    command.setConf(new Configuration());    Assert.assertEquals(0, command.run());}
0
public void testShowPagesCommand() throws IOException
{    File file = parquetFile();    ShowPagesCommand command = new ShowPagesCommand(createLogger());    command.targets = Arrays.asList(file.getAbsolutePath());    command.setConf(new Configuration());    Assert.assertEquals(0, command.run());}
0
public void testToAvroCommandFromParquet() throws IOException
{    File avroFile = toAvro(parquetFile());    Assert.assertTrue(avroFile.exists());}
0
public void testToAvroCommandFromJson() throws IOException
{    final File jsonInputFile = folder.newFile("sample.json");    final File avroOutputFile = folder.newFile("sample.avro");        final String inputJson = "{\"id\": 1, \"name\": \"Alice\"}\n" + "{\"id\": 2, \"name\": \"Bob\"}\n" + "{\"id\": 3, \"name\": \"Carol\"}\n" + "{\"id\": 4, \"name\": \"Dave\"}";    try (BufferedWriter writer = new BufferedWriter(new FileWriter(jsonInputFile))) {        writer.write(inputJson);    }    ToAvroCommand cmd = new ToAvroCommand(null);    JCommander.newBuilder().addObject(cmd).build().parse(jsonInputFile.getAbsolutePath(), "--output", avroOutputFile.getAbsolutePath());    assert (cmd.run() == 0);}
0
public void testToAvroCommandWithGzipCompression() throws IOException
{    File avroFile = toAvro(parquetFile(), "GZIP");    Assert.assertTrue(avroFile.exists());}
0
public void testToAvroCommandWithSnappyCompression() throws IOException
{    File avroFile = toAvro(parquetFile(), "SNAPPY");    Assert.assertTrue(avroFile.exists());}
0
public void testToAvroCommandWithZstdCompression() throws IOException
{    File avroFile = toAvro(parquetFile(), "ZSTD");    Assert.assertTrue(avroFile.exists());}
0
public void testToAvroCommandWithInvalidCompression() throws IOException
{    toAvro(parquetFile(), "FOO");}
0
public String[] getPath()
{    return path;}
0
public int getMaxRepetitionLevel()
{    return maxRep;}
0
public int getMaxDefinitionLevel()
{    return maxDef;}
0
public PrimitiveTypeName getType()
{    return type.getPrimitiveTypeName();}
0
public int getTypeLength()
{    return type.getTypeLength();}
0
public PrimitiveType getPrimitiveType()
{    return type;}
0
public int hashCode()
{    return Arrays.hashCode(path);}
0
public boolean equals(Object other)
{    if (other == this)        return true;    if (!(other instanceof ColumnDescriptor))        return false;    ColumnDescriptor descriptor = (ColumnDescriptor) other;    return Arrays.equals(path, descriptor.path);}
0
public int compareTo(ColumnDescriptor o)
{    int length = path.length < o.path.length ? path.length : o.path.length;    for (int i = 0; i < length; i++) {        int compareTo = path[i].compareTo(o.path[i]);        if (compareTo != 0) {            return compareTo;        }    }    return path.length - o.path.length;}
0
public String toString()
{    return Arrays.toString(path) + " " + type;}
0
 boolean isColumnFlushNeeded()
{    return false;}
0
public Encoding getEncoding()
{    return encoding;}
0
public Binary decodeToBinary(int id)
{    throw new UnsupportedOperationException(this.getClass().getName());}
0
public int decodeToInt(int id)
{    throw new UnsupportedOperationException(this.getClass().getName());}
0
public long decodeToLong(int id)
{    throw new UnsupportedOperationException(this.getClass().getName());}
0
public float decodeToFloat(int id)
{    throw new UnsupportedOperationException(this.getClass().getName());}
0
public double decodeToDouble(int id)
{    throw new UnsupportedOperationException(this.getClass().getName());}
0
public boolean decodeToBoolean(int id)
{    throw new UnsupportedOperationException(this.getClass().getName());}
0
 int getMaxLevel(ColumnDescriptor descriptor, ValuesType valuesType)
{    int maxLevel;    switch(valuesType) {        case REPETITION_LEVEL:            maxLevel = descriptor.getMaxRepetitionLevel();            break;        case DEFINITION_LEVEL:            maxLevel = descriptor.getMaxDefinitionLevel();            break;        case VALUES:            if (descriptor.getType() == BOOLEAN) {                maxLevel = 1;                break;            }        default:            throw new ParquetDecodingException("Unsupported encoding for values: " + this);    }    return maxLevel;}
0
public boolean usesDictionary()
{    return false;}
0
public Dictionary initDictionary(ColumnDescriptor descriptor, DictionaryPage dictionaryPage) throws IOException
{    throw new UnsupportedOperationException(this.name() + " does not support dictionary");}
0
public ValuesReader getValuesReader(ColumnDescriptor descriptor, ValuesType valuesType)
{    throw new UnsupportedOperationException("Error decoding " + descriptor + ". " + this.name() + " is dictionary based");}
0
public ValuesReader getDictionaryBasedValuesReader(ColumnDescriptor descriptor, ValuesType valuesType, Dictionary dictionary)
{    throw new UnsupportedOperationException(this.name() + " is not dictionary based");}
0
public ValuesReader getValuesReader(ColumnDescriptor descriptor, ValuesType valuesType)
{    switch(descriptor.getType()) {        case BOOLEAN:            return new BooleanPlainValuesReader();        case BINARY:            return new BinaryPlainValuesReader();        case FLOAT:            return new FloatPlainValuesReader();        case DOUBLE:            return new DoublePlainValuesReader();        case INT32:            return new IntegerPlainValuesReader();        case INT64:            return new LongPlainValuesReader();        case INT96:            return new FixedLenByteArrayPlainValuesReader(12);        case FIXED_LEN_BYTE_ARRAY:            return new FixedLenByteArrayPlainValuesReader(descriptor.getTypeLength());        default:            throw new ParquetDecodingException("no plain reader for type " + descriptor.getType());    }}
0
public Dictionary initDictionary(ColumnDescriptor descriptor, DictionaryPage dictionaryPage) throws IOException
{    switch(descriptor.getType()) {        case BINARY:            return new PlainBinaryDictionary(dictionaryPage);        case FIXED_LEN_BYTE_ARRAY:            return new PlainBinaryDictionary(dictionaryPage, descriptor.getTypeLength());        case INT96:            return new PlainBinaryDictionary(dictionaryPage, 12);        case INT64:            return new PlainLongDictionary(dictionaryPage);        case DOUBLE:            return new PlainDoubleDictionary(dictionaryPage);        case INT32:            return new PlainIntegerDictionary(dictionaryPage);        case FLOAT:            return new PlainFloatDictionary(dictionaryPage);        default:            throw new ParquetDecodingException("Dictionary encoding not supported for type: " + descriptor.getType());    }}
0
public ValuesReader getValuesReader(ColumnDescriptor descriptor, ValuesType valuesType)
{    int bitWidth = BytesUtils.getWidthFromMaxInt(getMaxLevel(descriptor, valuesType));    if (bitWidth == 0) {        return new ZeroIntegerValuesReader();    }    return new RunLengthBitPackingHybridValuesReader(bitWidth);}
0
public ValuesReader getValuesReader(ColumnDescriptor descriptor, ValuesType valuesType)
{    return new ByteBitPackingValuesReader(getMaxLevel(descriptor, valuesType), BIG_ENDIAN);}
0
public ValuesReader getDictionaryBasedValuesReader(ColumnDescriptor descriptor, ValuesType valuesType, Dictionary dictionary)
{    return RLE_DICTIONARY.getDictionaryBasedValuesReader(descriptor, valuesType, dictionary);}
0
public Dictionary initDictionary(ColumnDescriptor descriptor, DictionaryPage dictionaryPage) throws IOException
{    return PLAIN.initDictionary(descriptor, dictionaryPage);}
0
public boolean usesDictionary()
{    return true;}
0
public ValuesReader getValuesReader(ColumnDescriptor descriptor, ValuesType valuesType)
{    if (descriptor.getType() != INT32 && descriptor.getType() != INT64) {        throw new ParquetDecodingException("Encoding DELTA_BINARY_PACKED is only supported for type INT32 and INT64");    }    return new DeltaBinaryPackingValuesReader();}
0
public ValuesReader getValuesReader(ColumnDescriptor descriptor, ValuesType valuesType)
{    if (descriptor.getType() != BINARY) {        throw new ParquetDecodingException("Encoding DELTA_LENGTH_BYTE_ARRAY is only supported for type BINARY");    }    return new DeltaLengthByteArrayValuesReader();}
0
public ValuesReader getValuesReader(ColumnDescriptor descriptor, ValuesType valuesType)
{    if (descriptor.getType() != BINARY && descriptor.getType() != FIXED_LEN_BYTE_ARRAY) {        throw new ParquetDecodingException("Encoding DELTA_BYTE_ARRAY is only supported for type BINARY and FIXED_LEN_BYTE_ARRAY");    }    return new DeltaByteArrayReader();}
0
public ValuesReader getDictionaryBasedValuesReader(ColumnDescriptor descriptor, ValuesType valuesType, Dictionary dictionary)
{    switch(descriptor.getType()) {        case BINARY:        case FIXED_LEN_BYTE_ARRAY:        case INT96:        case INT64:        case DOUBLE:        case INT32:        case FLOAT:            return new DictionaryValuesReader(dictionary);        default:            throw new ParquetDecodingException("Dictionary encoding not supported for type: " + descriptor.getType());    }}
0
public boolean usesDictionary()
{    return true;}
0
public Set<Encoding> getDictionaryEncodings()
{    return dictStats.keySet();}
0
public Set<Encoding> getDataEncodings()
{    return dataStats.keySet();}
0
public int getNumDictionaryPagesEncodedAs(Encoding enc)
{    if (dictStats.containsKey(enc)) {        return dictStats.get(enc);    } else {        return 0;    }}
0
public int getNumDataPagesEncodedAs(Encoding enc)
{    if (dataStats.containsKey(enc)) {        return dataStats.get(enc);    } else {        return 0;    }}
0
public boolean hasDictionaryPages()
{    return !dictStats.isEmpty();}
0
public boolean hasDictionaryEncodedPages()
{    Set<Encoding> encodings = dataStats.keySet();    return (encodings.contains(RLE_DICTIONARY) || encodings.contains(PLAIN_DICTIONARY));}
0
public boolean hasNonDictionaryEncodedPages()
{    if (dataStats.isEmpty()) {                return false;    }        Set<Encoding> encodings = new HashSet<Encoding>(dataStats.keySet());    if (!encodings.remove(RLE_DICTIONARY) && !encodings.remove(PLAIN_DICTIONARY)) {                return true;    }    if (encodings.isEmpty()) {        return false;    }        return true;}
0
public boolean usesV2Pages()
{    return usesV2Pages;}
0
public Builder clear()
{    this.usesV2Pages = false;    dictStats.clear();    dataStats.clear();    return this;}
0
public Builder withV2Pages()
{    this.usesV2Pages = true;    return this;}
0
public Builder addDictEncoding(Encoding encoding)
{    return addDictEncoding(encoding, 1);}
0
public Builder addDictEncoding(Encoding encoding, int numPages)
{    Integer pages = dictStats.get(encoding);    dictStats.put(encoding, numPages + (pages != null ? pages : 0));    return this;}
0
public Builder addDataEncodings(Collection<Encoding> encodings)
{    for (Encoding encoding : encodings) {        addDataEncoding(encoding);    }    return this;}
0
public Builder addDataEncoding(Encoding encoding)
{    return addDataEncoding(encoding, 1);}
0
public Builder addDataEncoding(Encoding encoding, int numPages)
{    Integer pages = dataStats.get(encoding);    dataStats.put(encoding, numPages + (pages != null ? pages : 0));    return this;}
0
public EncodingStats build()
{    return new EncodingStats(Collections.unmodifiableMap(new LinkedHashMap<Encoding, Integer>(dictStats)), Collections.unmodifiableMap(new LinkedHashMap<Encoding, Integer>(dataStats)), usesV2Pages);}
0
public int getDictionaryId()
{    throw new UnsupportedOperationException();}
0
public int getInteger()
{    throw new UnsupportedOperationException();}
0
public boolean getBoolean()
{    throw new UnsupportedOperationException();}
0
public long getLong()
{    throw new UnsupportedOperationException();}
0
public Binary getBinary()
{    throw new UnsupportedOperationException();}
0
public float getFloat()
{    throw new UnsupportedOperationException();}
0
public double getDouble()
{    throw new UnsupportedOperationException();}
0
private void bindToDictionary(final Dictionary dictionary)
{    binding = new Binding() {        void read() {            dictionaryId = dataColumn.readValueDictionaryId();        }        public void skip() {            dataColumn.skip();        }        @Override        void skip(int n) {            dataColumn.skip(n);        }        public int getDictionaryId() {            return dictionaryId;        }        void writeValue() {            converter.addValueFromDictionary(dictionaryId);        }        public int getInteger() {            return dictionary.decodeToInt(dictionaryId);        }        public boolean getBoolean() {            return dictionary.decodeToBoolean(dictionaryId);        }        public long getLong() {            return dictionary.decodeToLong(dictionaryId);        }        public Binary getBinary() {            return dictionary.decodeToBinary(dictionaryId);        }        public float getFloat() {            return dictionary.decodeToFloat(dictionaryId);        }        public double getDouble() {            return dictionary.decodeToDouble(dictionaryId);        }    };}
0
 void read()
{    dictionaryId = dataColumn.readValueDictionaryId();}
0
public void skip()
{    dataColumn.skip();}
0
 void skip(int n)
{    dataColumn.skip(n);}
0
public int getDictionaryId()
{    return dictionaryId;}
0
 void writeValue()
{    converter.addValueFromDictionary(dictionaryId);}
0
public int getInteger()
{    return dictionary.decodeToInt(dictionaryId);}
0
public boolean getBoolean()
{    return dictionary.decodeToBoolean(dictionaryId);}
0
public long getLong()
{    return dictionary.decodeToLong(dictionaryId);}
0
public Binary getBinary()
{    return dictionary.decodeToBinary(dictionaryId);}
0
public float getFloat()
{    return dictionary.decodeToFloat(dictionaryId);}
0
public double getDouble()
{    return dictionary.decodeToDouble(dictionaryId);}
0
private void bind(PrimitiveTypeName type)
{    binding = type.convert(new PrimitiveTypeNameConverter<Binding, RuntimeException>() {        @Override        public Binding convertFLOAT(PrimitiveTypeName primitiveTypeName) throws RuntimeException {            return new Binding() {                float current;                void read() {                    current = dataColumn.readFloat();                }                public void skip() {                    current = 0;                    dataColumn.skip();                }                @Override                void skip(int n) {                    current = 0;                    dataColumn.skip(n);                }                public float getFloat() {                    return current;                }                void writeValue() {                    converter.addFloat(current);                }            };        }        @Override        public Binding convertDOUBLE(PrimitiveTypeName primitiveTypeName) throws RuntimeException {            return new Binding() {                double current;                void read() {                    current = dataColumn.readDouble();                }                public void skip() {                    current = 0;                    dataColumn.skip();                }                @Override                void skip(int n) {                    current = 0;                    dataColumn.skip(n);                }                public double getDouble() {                    return current;                }                void writeValue() {                    converter.addDouble(current);                }            };        }        @Override        public Binding convertINT32(PrimitiveTypeName primitiveTypeName) throws RuntimeException {            return new Binding() {                int current;                void read() {                    current = dataColumn.readInteger();                }                public void skip() {                    current = 0;                    dataColumn.skip();                }                @Override                void skip(int n) {                    current = 0;                    dataColumn.skip(n);                }                @Override                public int getInteger() {                    return current;                }                void writeValue() {                    converter.addInt(current);                }            };        }        @Override        public Binding convertINT64(PrimitiveTypeName primitiveTypeName) throws RuntimeException {            return new Binding() {                long current;                void read() {                    current = dataColumn.readLong();                }                public void skip() {                    current = 0;                    dataColumn.skip();                }                @Override                void skip(int n) {                    current = 0;                    dataColumn.skip(n);                }                @Override                public long getLong() {                    return current;                }                void writeValue() {                    converter.addLong(current);                }            };        }        @Override        public Binding convertINT96(PrimitiveTypeName primitiveTypeName) throws RuntimeException {            return this.convertBINARY(primitiveTypeName);        }        @Override        public Binding convertFIXED_LEN_BYTE_ARRAY(PrimitiveTypeName primitiveTypeName) throws RuntimeException {            return this.convertBINARY(primitiveTypeName);        }        @Override        public Binding convertBOOLEAN(PrimitiveTypeName primitiveTypeName) throws RuntimeException {            return new Binding() {                boolean current;                void read() {                    current = dataColumn.readBoolean();                }                public void skip() {                    current = false;                    dataColumn.skip();                }                @Override                void skip(int n) {                    current = false;                    dataColumn.skip(n);                }                @Override                public boolean getBoolean() {                    return current;                }                void writeValue() {                    converter.addBoolean(current);                }            };        }        @Override        public Binding convertBINARY(PrimitiveTypeName primitiveTypeName) throws RuntimeException {            return new Binding() {                Binary current;                void read() {                    current = dataColumn.readBytes();                }                public void skip() {                    current = null;                    dataColumn.skip();                }                @Override                void skip(int n) {                    current = null;                    dataColumn.skip(n);                }                @Override                public Binary getBinary() {                    return current;                }                void writeValue() {                    converter.addBinary(current);                }            };        }    });}
0
public Binding convertFLOAT(PrimitiveTypeName primitiveTypeName) throws RuntimeException
{    return new Binding() {        float current;        void read() {            current = dataColumn.readFloat();        }        public void skip() {            current = 0;            dataColumn.skip();        }        @Override        void skip(int n) {            current = 0;            dataColumn.skip(n);        }        public float getFloat() {            return current;        }        void writeValue() {            converter.addFloat(current);        }    };}
0
 void read()
{    current = dataColumn.readFloat();}
0
public void skip()
{    current = 0;    dataColumn.skip();}
0
 void skip(int n)
{    current = 0;    dataColumn.skip(n);}
0
public float getFloat()
{    return current;}
0
 void writeValue()
{    converter.addFloat(current);}
0
public Binding convertDOUBLE(PrimitiveTypeName primitiveTypeName) throws RuntimeException
{    return new Binding() {        double current;        void read() {            current = dataColumn.readDouble();        }        public void skip() {            current = 0;            dataColumn.skip();        }        @Override        void skip(int n) {            current = 0;            dataColumn.skip(n);        }        public double getDouble() {            return current;        }        void writeValue() {            converter.addDouble(current);        }    };}
0
 void read()
{    current = dataColumn.readDouble();}
0
public void skip()
{    current = 0;    dataColumn.skip();}
0
 void skip(int n)
{    current = 0;    dataColumn.skip(n);}
0
public double getDouble()
{    return current;}
0
 void writeValue()
{    converter.addDouble(current);}
0
public Binding convertINT32(PrimitiveTypeName primitiveTypeName) throws RuntimeException
{    return new Binding() {        int current;        void read() {            current = dataColumn.readInteger();        }        public void skip() {            current = 0;            dataColumn.skip();        }        @Override        void skip(int n) {            current = 0;            dataColumn.skip(n);        }        @Override        public int getInteger() {            return current;        }        void writeValue() {            converter.addInt(current);        }    };}
0
 void read()
{    current = dataColumn.readInteger();}
0
public void skip()
{    current = 0;    dataColumn.skip();}
0
 void skip(int n)
{    current = 0;    dataColumn.skip(n);}
0
public int getInteger()
{    return current;}
0
 void writeValue()
{    converter.addInt(current);}
0
public Binding convertINT64(PrimitiveTypeName primitiveTypeName) throws RuntimeException
{    return new Binding() {        long current;        void read() {            current = dataColumn.readLong();        }        public void skip() {            current = 0;            dataColumn.skip();        }        @Override        void skip(int n) {            current = 0;            dataColumn.skip(n);        }        @Override        public long getLong() {            return current;        }        void writeValue() {            converter.addLong(current);        }    };}
0
 void read()
{    current = dataColumn.readLong();}
0
public void skip()
{    current = 0;    dataColumn.skip();}
0
 void skip(int n)
{    current = 0;    dataColumn.skip(n);}
0
public long getLong()
{    return current;}
0
 void writeValue()
{    converter.addLong(current);}
0
public Binding convertINT96(PrimitiveTypeName primitiveTypeName) throws RuntimeException
{    return this.convertBINARY(primitiveTypeName);}
0
public Binding convertFIXED_LEN_BYTE_ARRAY(PrimitiveTypeName primitiveTypeName) throws RuntimeException
{    return this.convertBINARY(primitiveTypeName);}
0
public Binding convertBOOLEAN(PrimitiveTypeName primitiveTypeName) throws RuntimeException
{    return new Binding() {        boolean current;        void read() {            current = dataColumn.readBoolean();        }        public void skip() {            current = false;            dataColumn.skip();        }        @Override        void skip(int n) {            current = false;            dataColumn.skip(n);        }        @Override        public boolean getBoolean() {            return current;        }        void writeValue() {            converter.addBoolean(current);        }    };}
0
 void read()
{    current = dataColumn.readBoolean();}
0
public void skip()
{    current = false;    dataColumn.skip();}
0
 void skip(int n)
{    current = false;    dataColumn.skip(n);}
0
public boolean getBoolean()
{    return current;}
0
 void writeValue()
{    converter.addBoolean(current);}
0
public Binding convertBINARY(PrimitiveTypeName primitiveTypeName) throws RuntimeException
{    return new Binding() {        Binary current;        void read() {            current = dataColumn.readBytes();        }        public void skip() {            current = null;            dataColumn.skip();        }        @Override        void skip(int n) {            current = null;            dataColumn.skip(n);        }        @Override        public Binary getBinary() {            return current;        }        void writeValue() {            converter.addBinary(current);        }    };}
0
 void read()
{    current = dataColumn.readBytes();}
0
public void skip()
{    current = null;    dataColumn.skip();}
0
 void skip(int n)
{    current = null;    dataColumn.skip(n);}
0
public Binary getBinary()
{    return current;}
0
 void writeValue()
{    converter.addBinary(current);}
0
 boolean isFullyConsumed()
{    return readValues >= totalValueCount;}
0
public void writeCurrentValueToConverter()
{    readValue();    this.binding.writeValue();}
0
public int getCurrentValueDictionaryID()
{    readValue();    return binding.getDictionaryId();}
0
public int getInteger()
{    readValue();    return this.binding.getInteger();}
0
public boolean getBoolean()
{    readValue();    return this.binding.getBoolean();}
0
public long getLong()
{    readValue();    return this.binding.getLong();}
0
public Binary getBinary()
{    readValue();    return this.binding.getBinary();}
0
public float getFloat()
{    readValue();    return this.binding.getFloat();}
0
public double getDouble()
{    readValue();    return this.binding.getDouble();}
0
public int getCurrentRepetitionLevel()
{    return repetitionLevel;}
0
public ColumnDescriptor getDescriptor()
{    return path;}
0
public void readValue()
{    try {        if (!valueRead) {            binding.read();            valueRead = true;        }    } catch (RuntimeException e) {        if (CorruptDeltaByteArrays.requiresSequentialReads(writerVersion, currentEncoding) && e instanceof ArrayIndexOutOfBoundsException) {                        throw new ParquetDecodingException("Read failure possibly due to " + "PARQUET-246: try setting parquet.split.files to false", new ParquetDecodingException(format("Can't read value in column %s at value %d out of %d, " + "%d out of %d in currentPage. repetition level: " + "%d, definition level: %d", path, readValues, totalValueCount, readValues - (endOfPageValueCount - pageValueCount), pageValueCount, repetitionLevel, definitionLevel), e));        }        throw new ParquetDecodingException(format("Can't read value in column %s at value %d out of %d, " + "%d out of %d in currentPage. repetition level: " + "%d, definition level: %d", path, readValues, totalValueCount, readValues - (endOfPageValueCount - pageValueCount), pageValueCount, repetitionLevel, definitionLevel), e);    }}
0
public void skip()
{    if (!valueRead) {        binding.skip();        valueRead = true;    }}
0
public int getCurrentDefinitionLevel()
{    return definitionLevel;}
0
private void checkRead()
{    int rl, dl;    int skipValues = 0;    for (; ; ) {        if (isPageFullyConsumed()) {            if (isFullyConsumed()) {                                                repetitionLevel = 0;                return;            }            readPage();            skipValues = 0;        }        rl = repetitionLevelColumn.nextInt();        dl = definitionLevelColumn.nextInt();        ++readValues;        if (!skipRL(rl)) {            break;        }        if (dl == maxDefinitionLevel) {            ++skipValues;        }    }    binding.skip(skipValues);    repetitionLevel = rl;    definitionLevel = dl;}
1
private void readPage()
{        DataPage page = pageReader.readPage();    page.accept(new DataPage.Visitor<Void>() {        @Override        public Void visit(DataPageV1 dataPageV1) {            readPageV1(dataPageV1);            return null;        }        @Override        public Void visit(DataPageV2 dataPageV2) {            readPageV2(dataPageV2);            return null;        }    });}
1
public Void visit(DataPageV1 dataPageV1)
{    readPageV1(dataPageV1);    return null;}
0
public Void visit(DataPageV2 dataPageV2)
{    readPageV2(dataPageV2);    return null;}
0
private void initDataReader(Encoding dataEncoding, ByteBufferInputStream in, int valueCount)
{    ValuesReader previousReader = this.dataColumn;    this.currentEncoding = dataEncoding;    this.pageValueCount = valueCount;    this.endOfPageValueCount = readValues + pageValueCount;    if (dataEncoding.usesDictionary()) {        if (dictionary == null) {            throw new ParquetDecodingException("could not read page in col " + path + " as the dictionary was missing for encoding " + dataEncoding);        }        this.dataColumn = dataEncoding.getDictionaryBasedValuesReader(path, VALUES, dictionary);    } else {        this.dataColumn = dataEncoding.getValuesReader(path, VALUES);    }    if (dataEncoding.usesDictionary() && converter.hasDictionarySupport()) {        bindToDictionary(dictionary);    } else {        bind(path.getType());    }    try {        dataColumn.initFromPage(pageValueCount, in);    } catch (IOException e) {        throw new ParquetDecodingException("could not read page in col " + path, e);    }    if (CorruptDeltaByteArrays.requiresSequentialReads(writerVersion, dataEncoding) && previousReader != null && previousReader instanceof RequiresPreviousReader) {                ((RequiresPreviousReader) dataColumn).setPreviousReader(previousReader);    }}
0
private void readPageV1(DataPageV1 page)
{    ValuesReader rlReader = page.getRlEncoding().getValuesReader(path, REPETITION_LEVEL);    ValuesReader dlReader = page.getDlEncoding().getValuesReader(path, DEFINITION_LEVEL);    this.repetitionLevelColumn = new ValuesReaderIntIterator(rlReader);    this.definitionLevelColumn = new ValuesReaderIntIterator(dlReader);    int valueCount = page.getValueCount();    try {        BytesInput bytes = page.getBytes();                        ByteBufferInputStream in = bytes.toInputStream();        rlReader.initFromPage(valueCount, in);                dlReader.initFromPage(valueCount, in);                initDataReader(page.getValueEncoding(), in, valueCount);    } catch (IOException e) {        throw new ParquetDecodingException("could not read page " + page + " in col " + path, e);    }    newPageInitialized(page);}
1
private void readPageV2(DataPageV2 page)
{    this.repetitionLevelColumn = newRLEIterator(path.getMaxRepetitionLevel(), page.getRepetitionLevels());    this.definitionLevelColumn = newRLEIterator(path.getMaxDefinitionLevel(), page.getDefinitionLevels());    int valueCount = page.getValueCount();        try {        initDataReader(page.getDataEncoding(), page.getData().toInputStream(), valueCount);    } catch (IOException e) {        throw new ParquetDecodingException("could not read page " + page + " in col " + path, e);    }    newPageInitialized(page);}
1
 final int getPageValueCount()
{    return pageValueCount;}
0
private IntIterator newRLEIterator(int maxLevel, BytesInput bytes)
{    try {        if (maxLevel == 0) {            return new NullIntIterator();        }        return new RLEIntIterator(new RunLengthBitPackingHybridDecoder(BytesUtils.getWidthFromMaxInt(maxLevel), bytes.toInputStream()));    } catch (IOException e) {        throw new ParquetDecodingException("could not read levels in page for col " + path, e);    }}
0
 boolean isPageFullyConsumed()
{    return readValues >= endOfPageValueCount;}
0
public void consume()
{    checkRead();    valueRead = false;}
0
public long getTotalValueCount()
{    return totalValueCount;}
0
 int nextInt()
{    return delegate.readInteger();}
0
 int nextInt()
{    try {        return delegate.readInt();    } catch (IOException e) {        throw new ParquetDecodingException(e);    }}
0
 int nextInt()
{    return 0;}
0
 boolean skipRL(int rl)
{    return false;}
0
 void newPageInitialized(DataPage page)
{}
0
public ColumnReader getColumnReader(ColumnDescriptor path)
{    PrimitiveConverter converter = getPrimitiveConverter(path);    PageReader pageReader = pageReadStore.getPageReader(path);    Optional<PrimitiveIterator.OfLong> rowIndexes = pageReadStore.getRowIndexes();    if (rowIndexes.isPresent()) {        return new SynchronizingColumnReader(path, pageReader, converter, writerVersion, rowIndexes.get());    } else {        return new ColumnReaderImpl(path, pageReader, converter, writerVersion);    }}
0
private ColumnReaderImpl newMemColumnReader(ColumnDescriptor path, PageReader pageReader)
{    PrimitiveConverter converter = getPrimitiveConverter(path);    return new ColumnReaderImpl(path, pageReader, converter, writerVersion);}
0
private PrimitiveConverter getPrimitiveConverter(ColumnDescriptor path)
{    Type currentType = schema;    Converter currentConverter = recordConverter;    for (String fieldName : path.getPath()) {        final GroupType groupType = currentType.asGroupType();        int fieldIndex = groupType.getFieldIndex(fieldName);        currentType = groupType.getType(fieldName);        currentConverter = currentConverter.asGroupConverter().getConverter(fieldIndex);    }    PrimitiveConverter converter = currentConverter.asPrimitiveConverter();    return converter;}
0
private void log(Object value, int r, int d)
{    }
1
private void resetStatistics()
{    this.statistics = Statistics.createStats(path.getPrimitiveType());}
0
private void definitionLevel(int definitionLevel)
{    definitionLevelColumn.writeInteger(definitionLevel);}
0
private void repetitionLevel(int repetitionLevel)
{    repetitionLevelColumn.writeInteger(repetitionLevel);    assert pageRowCount == 0 ? repetitionLevel == 0 : true : "Every page shall start on record boundaries";    if (repetitionLevel == 0) {        ++pageRowCount;    }}
0
public void writeNull(int repetitionLevel, int definitionLevel)
{    if (DEBUG)        log(null, repetitionLevel, definitionLevel);    repetitionLevel(repetitionLevel);    definitionLevel(definitionLevel);    statistics.incrementNumNulls();    ++valueCount;}
0
public void close()
{        repetitionLevelColumn.close();    definitionLevelColumn.close();    dataColumn.close();}
0
public long getBufferedSizeInMemory()
{    return repetitionLevelColumn.getBufferedSize() + definitionLevelColumn.getBufferedSize() + dataColumn.getBufferedSize() + pageWriter.getMemSize();}
0
public void write(double value, int repetitionLevel, int definitionLevel)
{    if (DEBUG)        log(value, repetitionLevel, definitionLevel);    repetitionLevel(repetitionLevel);    definitionLevel(definitionLevel);    dataColumn.writeDouble(value);    statistics.updateStats(value);    ++valueCount;}
0
public void write(float value, int repetitionLevel, int definitionLevel)
{    if (DEBUG)        log(value, repetitionLevel, definitionLevel);    repetitionLevel(repetitionLevel);    definitionLevel(definitionLevel);    dataColumn.writeFloat(value);    statistics.updateStats(value);    ++valueCount;}
0
public void write(Binary value, int repetitionLevel, int definitionLevel)
{    if (DEBUG)        log(value, repetitionLevel, definitionLevel);    repetitionLevel(repetitionLevel);    definitionLevel(definitionLevel);    dataColumn.writeBytes(value);    statistics.updateStats(value);    ++valueCount;}
0
public void write(boolean value, int repetitionLevel, int definitionLevel)
{    if (DEBUG)        log(value, repetitionLevel, definitionLevel);    repetitionLevel(repetitionLevel);    definitionLevel(definitionLevel);    dataColumn.writeBoolean(value);    statistics.updateStats(value);    ++valueCount;}
0
public void write(int value, int repetitionLevel, int definitionLevel)
{    if (DEBUG)        log(value, repetitionLevel, definitionLevel);    repetitionLevel(repetitionLevel);    definitionLevel(definitionLevel);    dataColumn.writeInteger(value);    statistics.updateStats(value);    ++valueCount;}
0
public void write(long value, int repetitionLevel, int definitionLevel)
{    if (DEBUG)        log(value, repetitionLevel, definitionLevel);    repetitionLevel(repetitionLevel);    definitionLevel(definitionLevel);    dataColumn.writeLong(value);    statistics.updateStats(value);    ++valueCount;}
0
 void finalizeColumnChunk()
{    final DictionaryPage dictionaryPage = dataColumn.toDictPageAndClose();    if (dictionaryPage != null) {        if (DEBUG)                    try {            pageWriter.writeDictionaryPage(dictionaryPage);        } catch (IOException e) {            throw new ParquetEncodingException("could not write dictionary page for " + path, e);        }        dataColumn.resetDictionary();    }}
1
 long getCurrentPageBufferedSize()
{    return repetitionLevelColumn.getBufferedSize() + definitionLevelColumn.getBufferedSize() + dataColumn.getBufferedSize();}
0
 long getTotalBufferedSize()
{    return repetitionLevelColumn.getBufferedSize() + definitionLevelColumn.getBufferedSize() + dataColumn.getBufferedSize() + pageWriter.getMemSize();}
0
 long allocatedSize()
{    return repetitionLevelColumn.getAllocatedSize() + definitionLevelColumn.getAllocatedSize() + dataColumn.getAllocatedSize() + pageWriter.allocatedSize();}
0
 String memUsageString(String indent)
{    StringBuilder b = new StringBuilder(indent).append(path).append(" {\n");    b.append(indent).append(" r:").append(repetitionLevelColumn.getAllocatedSize()).append(" bytes\n");    b.append(indent).append(" d:").append(definitionLevelColumn.getAllocatedSize()).append(" bytes\n");    b.append(dataColumn.memUsageString(indent + "  data:")).append("\n");    b.append(pageWriter.memUsageString(indent + "  pages:")).append("\n");    b.append(indent).append(String.format("  total: %,d/%,d", getTotalBufferedSize(), allocatedSize())).append("\n");    b.append(indent).append("}\n");    return b.toString();}
0
 long getRowsWrittenSoFar()
{    return this.rowsWrittenSoFar;}
0
 void writePage()
{    if (valueCount == 0) {        throw new ParquetEncodingException("writing empty page");    }    this.rowsWrittenSoFar += pageRowCount;    if (DEBUG)            try {        writePage(pageRowCount, valueCount, statistics, repetitionLevelColumn, definitionLevelColumn, dataColumn);    } catch (IOException e) {        throw new ParquetEncodingException("could not write page for " + path, e);    }    repetitionLevelColumn.reset();    definitionLevelColumn.reset();    dataColumn.reset();    valueCount = 0;    resetStatistics();    pageRowCount = 0;}
1
 ValuesWriter createRLWriter(ParquetProperties props, ColumnDescriptor path)
{    return props.newRepetitionLevelWriter(path);}
0
 ValuesWriter createDLWriter(ParquetProperties props, ColumnDescriptor path)
{    return props.newDefinitionLevelWriter(path);}
0
 void writePage(int rowCount, int valueCount, Statistics<?> statistics, ValuesWriter repetitionLevels, ValuesWriter definitionLevels, ValuesWriter values) throws IOException
{    pageWriter.writePage(concat(repetitionLevels.getBytes(), definitionLevels.getBytes(), values.getBytes()), valueCount, rowCount, statistics, repetitionLevels.getEncoding(), definitionLevels.getEncoding(), values.getEncoding());}
0
public BytesInput getBytes()
{    try {        return encoder.toBytes();    } catch (IOException e) {        throw new ParquetEncodingException(e);    }}
0
 ValuesWriter createRLWriter(ParquetProperties props, ColumnDescriptor path)
{    return path.getMaxRepetitionLevel() == 0 ? NULL_WRITER : new RLEWriterForV2(props.newRepetitionLevelEncoder(path));}
0
 ValuesWriter createDLWriter(ParquetProperties props, ColumnDescriptor path)
{    return path.getMaxDefinitionLevel() == 0 ? NULL_WRITER : new RLEWriterForV2(props.newDefinitionLevelEncoder(path));}
0
 void writePage(int rowCount, int valueCount, Statistics<?> statistics, ValuesWriter repetitionLevels, ValuesWriter definitionLevels, ValuesWriter values) throws IOException
{        BytesInput bytes = values.getBytes();    Encoding encoding = values.getEncoding();    pageWriter.writePageV2(rowCount, Math.toIntExact(statistics.getNumNulls()), valueCount, repetitionLevels.getBytes(), definitionLevels.getBytes(), encoding, bytes, statistics);}
0
public ColumnWriter getColumnWriter(ColumnDescriptor path)
{    ColumnWriterBase column = columns.get(path);    if (column == null) {        column = createColumnWriter(path, pageWriteStore.getPageWriter(path), props);        columns.put(path, column);    }    return column;}
0
public ColumnWriter getColumnWriter(ColumnDescriptor path)
{    return columns.get(path);}
0
public ColumnWriter getColumnWriter(ColumnDescriptor path)
{    return columnWriterProvider.getColumnWriter(path);}
0
public Set<ColumnDescriptor> getColumnDescriptors()
{    return columns.keySet();}
0
public String toString()
{    StringBuilder sb = new StringBuilder();    for (Entry<ColumnDescriptor, ColumnWriterBase> entry : columns.entrySet()) {        sb.append(Arrays.toString(entry.getKey().getPath())).append(": ");        sb.append(entry.getValue().getTotalBufferedSize()).append(" bytes");        sb.append("\n");    }    return sb.toString();}
0
public long getAllocatedSize()
{    long total = 0;    for (ColumnWriterBase memColumn : columns.values()) {        total += memColumn.allocatedSize();    }    return total;}
0
public long getBufferedSize()
{    long total = 0;    for (ColumnWriterBase memColumn : columns.values()) {        total += memColumn.getTotalBufferedSize();    }    return total;}
0
public void flush()
{    for (ColumnWriterBase memColumn : columns.values()) {        long rows = rowCount - memColumn.getRowsWrittenSoFar();        if (rows > 0) {            memColumn.writePage();        }        memColumn.finalizeColumnChunk();    }}
0
public String memUsageString()
{    StringBuilder b = new StringBuilder("Store {\n");    for (ColumnWriterBase memColumn : columns.values()) {        b.append(memColumn.memUsageString(" "));    }    b.append("}\n");    return b.toString();}
0
public long maxColMemSize()
{    long max = 0;    for (ColumnWriterBase memColumn : columns.values()) {        max = Math.max(max, memColumn.getBufferedSizeInMemory());    }    return max;}
0
public void close()
{        flush();    for (ColumnWriterBase memColumn : columns.values()) {        memColumn.close();    }}
0
public void endRecord()
{    ++rowCount;    if (rowCount >= rowCountForNextSizeCheck) {        sizeCheck();    }}
0
private void sizeCheck()
{    long minRecordToWait = Long.MAX_VALUE;    int pageRowCountLimit = props.getPageRowCountLimit();    long rowCountForNextRowCountCheck = rowCount + pageRowCountLimit;    for (ColumnWriterBase writer : columns.values()) {        long usedMem = writer.getCurrentPageBufferedSize();        long rows = rowCount - writer.getRowsWrittenSoFar();        long remainingMem = props.getPageSizeThreshold() - usedMem;        if (remainingMem <= thresholdTolerance || rows >= pageRowCountLimit) {            writer.writePage();            remainingMem = props.getPageSizeThreshold();        } else {            rowCountForNextRowCountCheck = min(rowCountForNextRowCountCheck, writer.getRowsWrittenSoFar() + pageRowCountLimit);        }        long rowsToFillPage = usedMem == 0 ? props.getMaxRowCountForPageSizeCheck() : (long) ((float) rows) / usedMem * remainingMem;        if (rowsToFillPage < minRecordToWait) {            minRecordToWait = rowsToFillPage;        }    }    if (minRecordToWait == Long.MAX_VALUE) {        minRecordToWait = props.getMinRowCountForPageSizeCheck();    }    if (props.estimateNextSizeCheck()) {                rowCountForNextSizeCheck = rowCount + min(max(minRecordToWait / 2, props.getMinRowCountForPageSizeCheck()), props.getMaxRowCountForPageSizeCheck());    } else {        rowCountForNextSizeCheck = rowCount + props.getMinRowCountForPageSizeCheck();    }        if (rowCountForNextRowCountCheck < rowCountForNextSizeCheck) {        rowCountForNextSizeCheck = rowCountForNextRowCountCheck;    }}
0
public boolean isColumnFlushNeeded()
{    return rowCount + 1 >= rowCountForNextSizeCheck;}
0
 ColumnWriterBase createColumnWriter(ColumnDescriptor path, PageWriter pageWriter, ParquetProperties props)
{    return new ColumnWriterV1(path, pageWriter, props);}
0
 ColumnWriterBase createColumnWriter(ColumnDescriptor path, PageWriter pageWriter, ParquetProperties props)
{    return new ColumnWriterV2(path, pageWriter, props);}
0
 boolean isPageFullyConsumed()
{    return getPageValueCount() <= valuesReadFromPage || lastRowInPage < targetRow;}
0
 boolean isFullyConsumed()
{    return !rowIndexes.hasNext();}
0
 boolean skipRL(int rl)
{    ++valuesReadFromPage;    if (rl == 0) {        ++currentRow;        if (currentRow > targetRow) {            targetRow = rowIndexes.hasNext() ? rowIndexes.nextLong() : Long.MAX_VALUE;        }    }    return currentRow < targetRow;}
0
protected void newPageInitialized(DataPage page)
{    long firstRowIndex = page.getFirstRowIndex().orElseThrow(() -> new IllegalArgumentException("Missing firstRowIndex for synchronizing values"));    int rowCount = page.getIndexRowCount().orElseThrow(() -> new IllegalArgumentException("Missing rowCount for synchronizing values"));    currentRow = firstRowIndex - 1;    lastRowInPage = firstRowIndex + rowCount - 1;    valuesReadFromPage = 0;}
0
public int getValueCount()
{    return valueCount;}
0
public Optional<Long> getFirstRowIndex()
{    return firstRowIndex < 0 ? Optional.empty() : Optional.of(firstRowIndex);}
0
public BytesInput getBytes()
{    return bytes;}
0
public Statistics<?> getStatistics()
{    return statistics;}
0
public Encoding getDlEncoding()
{    return dlEncoding;}
0
public Encoding getRlEncoding()
{    return rlEncoding;}
0
public Encoding getValueEncoding()
{    return valuesEncoding;}
0
public String toString()
{    return "Page [bytes.size=" + bytes.size() + ", valueCount=" + getValueCount() + ", uncompressedSize=" + getUncompressedSize() + "]";}
0
public T accept(Visitor<T> visitor)
{    return visitor.visit(this);}
0
public Optional<Integer> getIndexRowCount()
{    return indexRowCount < 0 ? Optional.empty() : Optional.of(indexRowCount);}
0
public static DataPageV2 uncompressed(int rowCount, int nullCount, int valueCount, BytesInput repetitionLevels, BytesInput definitionLevels, Encoding dataEncoding, BytesInput data, Statistics<?> statistics)
{    return new DataPageV2(rowCount, nullCount, valueCount, repetitionLevels, definitionLevels, dataEncoding, data, Math.toIntExact(repetitionLevels.size() + definitionLevels.size() + data.size()), statistics, false);}
0
public static DataPageV2 uncompressed(int rowCount, int nullCount, int valueCount, long firstRowIndex, BytesInput repetitionLevels, BytesInput definitionLevels, Encoding dataEncoding, BytesInput data, Statistics<?> statistics)
{    return new DataPageV2(rowCount, nullCount, valueCount, firstRowIndex, repetitionLevels, definitionLevels, dataEncoding, data, Math.toIntExact(repetitionLevels.size() + definitionLevels.size() + data.size()), statistics, false);}
0
public static DataPageV2 compressed(int rowCount, int nullCount, int valueCount, BytesInput repetitionLevels, BytesInput definitionLevels, Encoding dataEncoding, BytesInput data, int uncompressedSize, Statistics<?> statistics)
{    return new DataPageV2(rowCount, nullCount, valueCount, repetitionLevels, definitionLevels, dataEncoding, data, uncompressedSize, statistics, true);}
0
public int getRowCount()
{    return rowCount;}
0
public int getNullCount()
{    return nullCount;}
0
public BytesInput getRepetitionLevels()
{    return repetitionLevels;}
0
public BytesInput getDefinitionLevels()
{    return definitionLevels;}
0
public Encoding getDataEncoding()
{    return dataEncoding;}
0
public BytesInput getData()
{    return data;}
0
public Statistics<?> getStatistics()
{    return statistics;}
0
public boolean isCompressed()
{    return isCompressed;}
0
public Optional<Integer> getIndexRowCount()
{    return Optional.of(rowCount);}
0
public T accept(Visitor<T> visitor)
{    return visitor.visit(this);}
0
public String toString()
{    return "Page V2 [" + "dl size=" + definitionLevels.size() + ", " + "rl size=" + repetitionLevels.size() + ", " + "data size=" + data.size() + ", " + "data enc=" + dataEncoding + ", " + "valueCount=" + getValueCount() + ", " + "rowCount=" + getRowCount() + ", " + "is compressed=" + isCompressed + ", " + "uncompressedSize=" + getUncompressedSize() + "]";}
0
public BytesInput getBytes()
{    return bytes;}
0
public int getDictionarySize()
{    return dictionarySize;}
0
public Encoding getEncoding()
{    return encoding;}
0
public DictionaryPage copy() throws IOException
{    return new DictionaryPage(BytesInput.copy(bytes), getUncompressedSize(), dictionarySize, encoding);}
0
public String toString()
{    return "Page [bytes.size=" + bytes.size() + ", entryCount=" + dictionarySize + ", uncompressedSize=" + getUncompressedSize() + ", encoding=" + encoding + "]";}
0
public int getCompressedSize()
{    return compressedSize;}
0
public int getUncompressedSize()
{    return uncompressedSize;}
0
public void setCrc(int crc)
{    this.crc = OptionalInt.of(crc);}
0
public OptionalInt getCrc()
{    return crc;}
0
 Optional<PrimitiveIterator.OfLong> getRowIndexes()
{    return Optional.empty();}
0
public static WriterVersion fromString(String name)
{    for (WriterVersion v : WriterVersion.values()) {        if (v.shortName.equals(name)) {            return v;        }    }        return WriterVersion.valueOf(name);}
0
public ValuesWriter newRepetitionLevelWriter(ColumnDescriptor path)
{    return newColumnDescriptorValuesWriter(path.getMaxRepetitionLevel());}
0
public ValuesWriter newDefinitionLevelWriter(ColumnDescriptor path)
{    return newColumnDescriptorValuesWriter(path.getMaxDefinitionLevel());}
0
private ValuesWriter newColumnDescriptorValuesWriter(int maxLevel)
{    if (maxLevel == 0) {        return new DevNullValuesWriter();    } else {        return new RunLengthBitPackingHybridValuesWriter(getWidthFromMaxInt(maxLevel), MIN_SLAB_SIZE, pageSizeThreshold, allocator);    }}
0
public RunLengthBitPackingHybridEncoder newRepetitionLevelEncoder(ColumnDescriptor path)
{    return newLevelEncoder(path.getMaxRepetitionLevel());}
0
public RunLengthBitPackingHybridEncoder newDefinitionLevelEncoder(ColumnDescriptor path)
{    return newLevelEncoder(path.getMaxDefinitionLevel());}
0
private RunLengthBitPackingHybridEncoder newLevelEncoder(int maxLevel)
{    return new RunLengthBitPackingHybridEncoder(getWidthFromMaxInt(maxLevel), MIN_SLAB_SIZE, pageSizeThreshold, allocator);}
0
public ValuesWriter newValuesWriter(ColumnDescriptor path)
{    return valuesWriterFactory.newValuesWriter(path);}
0
public int getPageSizeThreshold()
{    return pageSizeThreshold;}
0
public int getInitialSlabSize()
{    return initialSlabSize;}
0
public int getDictionaryPageSizeThreshold()
{    return dictionaryPageSizeThreshold;}
0
public WriterVersion getWriterVersion()
{    return writerVersion;}
0
public boolean isEnableDictionary()
{    return enableDictionary;}
0
public ByteBufferAllocator getAllocator()
{    return allocator;}
0
public ColumnWriteStore newColumnWriteStore(MessageType schema, PageWriteStore pageStore)
{    switch(writerVersion) {        case PARQUET_1_0:            return new ColumnWriteStoreV1(schema, pageStore, this);        case PARQUET_2_0:            return new ColumnWriteStoreV2(schema, pageStore, this);        default:            throw new IllegalArgumentException("unknown version " + writerVersion);    }}
0
public int getMinRowCountForPageSizeCheck()
{    return minRowCountForPageSizeCheck;}
0
public int getMaxRowCountForPageSizeCheck()
{    return maxRowCountForPageSizeCheck;}
0
public ValuesWriterFactory getValuesWriterFactory()
{    return valuesWriterFactory;}
0
public int getColumnIndexTruncateLength()
{    return columnIndexTruncateLength;}
0
public boolean estimateNextSizeCheck()
{    return estimateNextSizeCheck;}
0
public int getPageRowCountLimit()
{    return pageRowCountLimit;}
0
public boolean getPageWriteChecksumEnabled()
{    return pageWriteChecksumEnabled;}
0
public static Builder builder()
{    return new Builder();}
0
public static Builder copy(ParquetProperties toCopy)
{    return new Builder(toCopy);}
0
public Builder withPageSize(int pageSize)
{    Preconditions.checkArgument(pageSize > 0, "Invalid page size (negative): %s", pageSize);    this.pageSize = pageSize;    return this;}
0
public Builder withDictionaryEncoding(boolean enableDictionary)
{    this.enableDict = enableDictionary;    return this;}
0
public Builder withDictionaryPageSize(int dictionaryPageSize)
{    Preconditions.checkArgument(dictionaryPageSize > 0, "Invalid dictionary page size (negative): %s", dictionaryPageSize);    this.dictPageSize = dictionaryPageSize;    return this;}
0
public Builder withWriterVersion(WriterVersion version)
{    this.writerVersion = version;    return this;}
0
public Builder withMinRowCountForPageSizeCheck(int min)
{    Preconditions.checkArgument(min > 0, "Invalid row count for page size check (negative): %s", min);    this.minRowCountForPageSizeCheck = min;    return this;}
0
public Builder withMaxRowCountForPageSizeCheck(int max)
{    Preconditions.checkArgument(max > 0, "Invalid row count for page size check (negative): %s", max);    this.maxRowCountForPageSizeCheck = max;    return this;}
0
public Builder estimateRowCountForPageSizeCheck(boolean estimateNextSizeCheck)
{    this.estimateNextSizeCheck = estimateNextSizeCheck;    return this;}
0
public Builder withAllocator(ByteBufferAllocator allocator)
{    Preconditions.checkNotNull(allocator, "ByteBufferAllocator");    this.allocator = allocator;    return this;}
0
public Builder withValuesWriterFactory(ValuesWriterFactory factory)
{    Preconditions.checkNotNull(factory, "ValuesWriterFactory");    this.valuesWriterFactory = factory;    return this;}
0
public Builder withColumnIndexTruncateLength(int length)
{    Preconditions.checkArgument(length > 0, "Invalid column index min/max truncate length (negative) : %s", length);    this.columnIndexTruncateLength = length;    return this;}
0
public Builder withPageRowCountLimit(int rowCount)
{    Preconditions.checkArgument(rowCount > 0, "Invalid row count limit for pages: " + rowCount);    pageRowCountLimit = rowCount;    return this;}
0
public Builder withPageWriteChecksumEnabled(boolean val)
{    this.pageWriteChecksumEnabled = val;    return this;}
0
public ParquetProperties build()
{    ParquetProperties properties = new ParquetProperties(writerVersion, pageSize, dictPageSize, enableDict, minRowCountForPageSizeCheck, maxRowCountForPageSizeCheck, estimateNextSizeCheck, allocator, valuesWriterFactory, columnIndexTruncateLength, pageRowCountLimit, pageWriteChecksumEnabled);                    valuesWriterFactory.initialize(properties);    return properties;}
0
public void updateStats(Binary value)
{    if (!this.hasNonNullValue()) {        initializeStats(value, value);    } else {        updateStats(value, value);    }}
0
public void mergeStatisticsMinMax(Statistics stats)
{    BinaryStatistics binaryStats = (BinaryStatistics) stats;    if (!this.hasNonNullValue()) {        initializeStats(binaryStats.getMin(), binaryStats.getMax());    } else {        updateStats(binaryStats.getMin(), binaryStats.getMax());    }}
0
public void setMinMaxFromBytes(byte[] minBytes, byte[] maxBytes)
{    max = Binary.fromReusedByteArray(maxBytes);    min = Binary.fromReusedByteArray(minBytes);    this.markAsNotEmpty();}
0
public byte[] getMaxBytes()
{    return max == null ? null : max.getBytes();}
0
public byte[] getMinBytes()
{    return min == null ? null : min.getBytes();}
0
 String stringify(Binary value)
{    return stringifier.stringify(value);}
0
public boolean isSmallerThan(long size)
{    return !hasNonNullValue() || ((min.length() + max.length()) < size);}
0
public void updateStats(Binary min_value, Binary max_value)
{    if (comparator().compare(min, min_value) > 0) {        min = min_value.copy();    }    if (comparator().compare(max, max_value) < 0) {        max = max_value.copy();    }}
0
public void initializeStats(Binary min_value, Binary max_value)
{    min = min_value.copy();    max = max_value.copy();    this.markAsNotEmpty();}
0
public Binary genericGetMin()
{    return min;}
0
public Binary genericGetMax()
{    return max;}
0
public Binary getMax()
{    return max;}
0
public Binary getMin()
{    return min;}
0
public void setMinMax(Binary min, Binary max)
{    this.max = max;    this.min = min;    this.markAsNotEmpty();}
0
public BinaryStatistics copy()
{    return new BinaryStatistics(this);}
0
public void updateStats(boolean value)
{    if (!this.hasNonNullValue()) {        initializeStats(value, value);    } else {        updateStats(value, value);    }}
0
public void mergeStatisticsMinMax(Statistics stats)
{    BooleanStatistics boolStats = (BooleanStatistics) stats;    if (!this.hasNonNullValue()) {        initializeStats(boolStats.getMin(), boolStats.getMax());    } else {        updateStats(boolStats.getMin(), boolStats.getMax());    }}
0
public void setMinMaxFromBytes(byte[] minBytes, byte[] maxBytes)
{    max = BytesUtils.bytesToBool(maxBytes);    min = BytesUtils.bytesToBool(minBytes);    this.markAsNotEmpty();}
0
public byte[] getMaxBytes()
{    return BytesUtils.booleanToBytes(max);}
0
public byte[] getMinBytes()
{    return BytesUtils.booleanToBytes(min);}
0
 String stringify(Boolean value)
{    return stringifier.stringify(value);}
0
public boolean isSmallerThan(long size)
{    return !hasNonNullValue() || (2 < size);}
0
public void updateStats(boolean min_value, boolean max_value)
{    if (comparator().compare(min, min_value) > 0) {        min = min_value;    }    if (comparator().compare(max, max_value) < 0) {        max = max_value;    }}
0
public void initializeStats(boolean min_value, boolean max_value)
{    min = min_value;    max = max_value;    this.markAsNotEmpty();}
0
public Boolean genericGetMin()
{    return min;}
0
public Boolean genericGetMax()
{    return max;}
0
public int compareMinToValue(boolean value)
{    return comparator().compare(min, value);}
0
public int compareMaxToValue(boolean value)
{    return comparator().compare(max, value);}
0
public boolean getMax()
{    return max;}
0
public boolean getMin()
{    return min;}
0
public void setMinMax(boolean min, boolean max)
{    this.max = max;    this.min = min;    this.markAsNotEmpty();}
0
public BooleanStatistics copy()
{    return new BooleanStatistics(this);}
0
public void updateStats(double value)
{    if (!this.hasNonNullValue()) {        initializeStats(value, value);    } else {        updateStats(value, value);    }}
0
public void mergeStatisticsMinMax(Statistics stats)
{    DoubleStatistics doubleStats = (DoubleStatistics) stats;    if (!this.hasNonNullValue()) {        initializeStats(doubleStats.getMin(), doubleStats.getMax());    } else {        updateStats(doubleStats.getMin(), doubleStats.getMax());    }}
0
public void setMinMaxFromBytes(byte[] minBytes, byte[] maxBytes)
{    max = Double.longBitsToDouble(BytesUtils.bytesToLong(maxBytes));    min = Double.longBitsToDouble(BytesUtils.bytesToLong(minBytes));    this.markAsNotEmpty();}
0
public byte[] getMaxBytes()
{    return BytesUtils.longToBytes(Double.doubleToLongBits(max));}
0
public byte[] getMinBytes()
{    return BytesUtils.longToBytes(Double.doubleToLongBits(min));}
0
 String stringify(Double value)
{    return stringifier.stringify(value);}
0
public boolean isSmallerThan(long size)
{    return !hasNonNullValue() || (16 < size);}
0
public void updateStats(double min_value, double max_value)
{    if (comparator().compare(min, min_value) > 0) {        min = min_value;    }    if (comparator().compare(max, max_value) < 0) {        max = max_value;    }}
0
public void initializeStats(double min_value, double max_value)
{    min = min_value;    max = max_value;    this.markAsNotEmpty();}
0
public Double genericGetMin()
{    return min;}
0
public Double genericGetMax()
{    return max;}
0
public int compareMinToValue(double value)
{    return comparator().compare(min, value);}
0
public int compareMaxToValue(double value)
{    return comparator().compare(max, value);}
0
public double getMax()
{    return max;}
0
public double getMin()
{    return min;}
0
public void setMinMax(double min, double max)
{    this.max = max;    this.min = min;    this.markAsNotEmpty();}
0
public DoubleStatistics copy()
{    return new DoubleStatistics(this);}
0
public void updateStats(float value)
{    if (!this.hasNonNullValue()) {        initializeStats(value, value);    } else {        updateStats(value, value);    }}
0
public void mergeStatisticsMinMax(Statistics stats)
{    FloatStatistics floatStats = (FloatStatistics) stats;    if (!this.hasNonNullValue()) {        initializeStats(floatStats.getMin(), floatStats.getMax());    } else {        updateStats(floatStats.getMin(), floatStats.getMax());    }}
0
public void setMinMaxFromBytes(byte[] minBytes, byte[] maxBytes)
{    max = Float.intBitsToFloat(BytesUtils.bytesToInt(maxBytes));    min = Float.intBitsToFloat(BytesUtils.bytesToInt(minBytes));    this.markAsNotEmpty();}
0
public byte[] getMaxBytes()
{    return BytesUtils.intToBytes(Float.floatToIntBits(max));}
0
public byte[] getMinBytes()
{    return BytesUtils.intToBytes(Float.floatToIntBits(min));}
0
 String stringify(Float value)
{    return stringifier.stringify(value);}
0
public boolean isSmallerThan(long size)
{    return !hasNonNullValue() || (8 < size);}
0
public void updateStats(float min_value, float max_value)
{    if (comparator().compare(min, min_value) > 0) {        min = min_value;    }    if (comparator().compare(max, max_value) < 0) {        max = max_value;    }}
0
public void initializeStats(float min_value, float max_value)
{    min = min_value;    max = max_value;    this.markAsNotEmpty();}
0
public Float genericGetMin()
{    return min;}
0
public Float genericGetMax()
{    return max;}
0
public int compareMinToValue(float value)
{    return comparator().compare(min, value);}
0
public int compareMaxToValue(float value)
{    return comparator().compare(max, value);}
0
public float getMax()
{    return max;}
0
public float getMin()
{    return min;}
0
public void setMinMax(float min, float max)
{    this.max = max;    this.min = min;    this.markAsNotEmpty();}
0
public FloatStatistics copy()
{    return new FloatStatistics(this);}
0
public void updateStats(int value)
{    if (!this.hasNonNullValue()) {        initializeStats(value, value);    } else {        updateStats(value, value);    }}
0
public void mergeStatisticsMinMax(Statistics stats)
{    IntStatistics intStats = (IntStatistics) stats;    if (!this.hasNonNullValue()) {        initializeStats(intStats.getMin(), intStats.getMax());    } else {        updateStats(intStats.getMin(), intStats.getMax());    }}
0
public void setMinMaxFromBytes(byte[] minBytes, byte[] maxBytes)
{    max = BytesUtils.bytesToInt(maxBytes);    min = BytesUtils.bytesToInt(minBytes);    this.markAsNotEmpty();}
0
public byte[] getMaxBytes()
{    return BytesUtils.intToBytes(max);}
0
public byte[] getMinBytes()
{    return BytesUtils.intToBytes(min);}
0
 String stringify(Integer value)
{    return stringifier.stringify(value);}
0
public boolean isSmallerThan(long size)
{    return !hasNonNullValue() || (8 < size);}
0
public void updateStats(int min_value, int max_value)
{    if (comparator().compare(min, min_value) > 0) {        min = min_value;    }    if (comparator().compare(max, max_value) < 0) {        max = max_value;    }}
0
public void initializeStats(int min_value, int max_value)
{    min = min_value;    max = max_value;    this.markAsNotEmpty();}
0
public Integer genericGetMin()
{    return min;}
0
public Integer genericGetMax()
{    return max;}
0
public int compareMinToValue(int value)
{    return comparator().compare(min, value);}
0
public int compareMaxToValue(int value)
{    return comparator().compare(max, value);}
0
public int getMax()
{    return max;}
0
public int getMin()
{    return min;}
0
public void setMinMax(int min, int max)
{    this.max = max;    this.min = min;    this.markAsNotEmpty();}
0
public IntStatistics copy()
{    return new IntStatistics(this);}
0
public void updateStats(long value)
{    if (!this.hasNonNullValue()) {        initializeStats(value, value);    } else {        updateStats(value, value);    }}
0
public void mergeStatisticsMinMax(Statistics stats)
{    LongStatistics longStats = (LongStatistics) stats;    if (!this.hasNonNullValue()) {        initializeStats(longStats.getMin(), longStats.getMax());    } else {        updateStats(longStats.getMin(), longStats.getMax());    }}
0
public void setMinMaxFromBytes(byte[] minBytes, byte[] maxBytes)
{    max = BytesUtils.bytesToLong(maxBytes);    min = BytesUtils.bytesToLong(minBytes);    this.markAsNotEmpty();}
0
public byte[] getMaxBytes()
{    return BytesUtils.longToBytes(max);}
0
public byte[] getMinBytes()
{    return BytesUtils.longToBytes(min);}
0
 String stringify(Long value)
{    return stringifier.stringify(value);}
0
public boolean isSmallerThan(long size)
{    return !hasNonNullValue() || (16 < size);}
0
public void updateStats(long min_value, long max_value)
{    if (comparator().compare(min, min_value) > 0) {        min = min_value;    }    if (comparator().compare(max, max_value) < 0) {        max = max_value;    }}
0
public void initializeStats(long min_value, long max_value)
{    min = min_value;    max = max_value;    this.markAsNotEmpty();}
0
public Long genericGetMin()
{    return min;}
0
public Long genericGetMax()
{    return max;}
0
public int compareMinToValue(long value)
{    return comparator().compare(min, value);}
0
public int compareMaxToValue(long value)
{    return comparator().compare(max, value);}
0
public long getMax()
{    return max;}
0
public long getMin()
{    return min;}
0
public void setMinMax(long min, long max)
{    this.max = max;    this.min = min;    this.markAsNotEmpty();}
0
public LongStatistics copy()
{    return new LongStatistics(this);}
0
public Builder withMin(byte[] min)
{    this.min = min;    return this;}
0
public Builder withMax(byte[] max)
{    this.max = max;    return this;}
0
public Builder withNumNulls(long numNulls)
{    this.numNulls = numNulls;    return this;}
0
public Statistics<?> build()
{    Statistics<?> stats = createStats(type);    if (min != null && max != null) {        stats.setMinMaxFromBytes(min, max);    }    stats.num_nulls = this.numNulls;    return stats;}
0
public Statistics<?> build()
{    FloatStatistics stats = (FloatStatistics) super.build();    if (stats.hasNonNullValue()) {        Float min = stats.genericGetMin();        Float max = stats.genericGetMax();                if (min.isNaN() || max.isNaN()) {            stats.setMinMax(0.0f, 0.0f);            ((Statistics<?>) stats).hasNonNullValue = false;        } else {                        if (Float.compare(min, 0.0f) == 0) {                min = -0.0f;                stats.setMinMax(min, max);            }            if (Float.compare(max, -0.0f) == 0) {                max = 0.0f;                stats.setMinMax(min, max);            }        }    }    return stats;}
0
public Statistics<?> build()
{    DoubleStatistics stats = (DoubleStatistics) super.build();    if (stats.hasNonNullValue()) {        Double min = stats.genericGetMin();        Double max = stats.genericGetMax();                if (min.isNaN() || max.isNaN()) {            stats.setMinMax(0.0, 0.0);            ((Statistics<?>) stats).hasNonNullValue = false;        } else {                        if (Double.compare(min, 0.0) == 0) {                min = -0.0;                stats.setMinMax(min, max);            }            if (Double.compare(max, -0.0) == 0) {                max = 0.0;                stats.setMinMax(min, max);            }        }    }    return stats;}
0
public static Statistics getStatsBasedOnType(PrimitiveTypeName type)
{    switch(type) {        case INT32:            return new IntStatistics();        case INT64:            return new LongStatistics();        case FLOAT:            return new FloatStatistics();        case DOUBLE:            return new DoubleStatistics();        case BOOLEAN:            return new BooleanStatistics();        case BINARY:            return new BinaryStatistics();        case INT96:            return new BinaryStatistics();        case FIXED_LEN_BYTE_ARRAY:            return new BinaryStatistics();        default:            throw new UnknownColumnTypeException(type);    }}
0
public static Statistics<?> createStats(Type type)
{    PrimitiveType primitive = type.asPrimitiveType();    switch(primitive.getPrimitiveTypeName()) {        case INT32:            return new IntStatistics(primitive);        case INT64:            return new LongStatistics(primitive);        case FLOAT:            return new FloatStatistics(primitive);        case DOUBLE:            return new DoubleStatistics(primitive);        case BOOLEAN:            return new BooleanStatistics(primitive);        case BINARY:        case INT96:        case FIXED_LEN_BYTE_ARRAY:            return new BinaryStatistics(primitive);        default:            throw new UnknownColumnTypeException(primitive.getPrimitiveTypeName());    }}
0
public static Builder getBuilderForReading(PrimitiveType type)
{    switch(type.getPrimitiveTypeName()) {        case FLOAT:            return new FloatBuilder(type);        case DOUBLE:            return new DoubleBuilder(type);        default:            return new Builder(type);    }}
0
public void updateStats(int value)
{    throw new UnsupportedOperationException();}
0
public void updateStats(long value)
{    throw new UnsupportedOperationException();}
0
public void updateStats(float value)
{    throw new UnsupportedOperationException();}
0
public void updateStats(double value)
{    throw new UnsupportedOperationException();}
0
public void updateStats(boolean value)
{    throw new UnsupportedOperationException();}
0
public void updateStats(Binary value)
{    throw new UnsupportedOperationException();}
0
public boolean equals(Object other)
{    if (other == this)        return true;    if (!(other instanceof Statistics))        return false;    Statistics stats = (Statistics) other;    return type.equals(stats.type) && Arrays.equals(stats.getMaxBytes(), this.getMaxBytes()) && Arrays.equals(stats.getMinBytes(), this.getMinBytes()) && stats.getNumNulls() == this.getNumNulls();}
0
public int hashCode()
{    return 31 * type.hashCode() + 31 * Arrays.hashCode(getMaxBytes()) + 17 * Arrays.hashCode(getMinBytes()) + Long.valueOf(this.getNumNulls()).hashCode();}
0
public void mergeStatistics(Statistics stats)
{    if (stats.isEmpty())        return;        if (type.equals(stats.type)) {        incrementNumNulls(stats.getNumNulls());        if (stats.hasNonNullValue()) {            mergeStatisticsMinMax(stats);            markAsNotEmpty();        }    } else {        throw StatisticsClassException.create(this, stats);    }}
0
public final PrimitiveComparator<T> comparator()
{    return comparator;}
0
public final int compareMinToValue(T value)
{    return comparator.compare(genericGetMin(), value);}
0
public final int compareMaxToValue(T value)
{    return comparator.compare(genericGetMax(), value);}
0
public String minAsString()
{    return stringify(genericGetMin());}
0
public String maxAsString()
{    return stringify(genericGetMax());}
0
public String toString()
{    if (this.hasNonNullValue()) {        if (isNumNullsSet()) {            return String.format("min: %s, max: %s, num_nulls: %d", minAsString(), maxAsString(), this.getNumNulls());        } else {            return String.format("min: %s, max: %s, num_nulls not defined", minAsString(), maxAsString());        }    } else if (!this.isEmpty())        return String.format("num_nulls: %d, min/max not defined", this.getNumNulls());    else        return "no stats for this column";}
0
public void incrementNumNulls()
{    num_nulls++;}
0
public void incrementNumNulls(long increment)
{    num_nulls += increment;}
0
public long getNumNulls()
{    return num_nulls;}
0
public void setNumNulls(long nulls)
{    num_nulls = nulls;}
0
public boolean isEmpty()
{    return !hasNonNullValue && !isNumNullsSet();}
0
public boolean hasNonNullValue()
{    return hasNonNullValue;}
0
public boolean isNumNullsSet()
{    return num_nulls >= 0;}
0
protected void markAsNotEmpty()
{    hasNonNullValue = true;}
0
public PrimitiveType type()
{    return type;}
0
 static StatisticsClassException create(Statistics<?> stats1, Statistics<?> stats2)
{    if (stats1.getClass() != stats2.getClass()) {        return new StatisticsClassException(stats1.getClass().toString(), stats2.getClass().toString());    }    return new StatisticsClassException("Statistics comparator mismatched: " + stats1.comparator() + " vs. " + stats2.comparator());}
0
public ColumnDescriptor getDescriptor()
{    return descriptor;}
0
public PrimitiveTypeName getType()
{    return this.type;}
0
public int readInteger()
{    try {        return bitPackingReader.read();    } catch (IOException e) {        throw new ParquetDecodingException(e);    }}
0
public void initFromPage(int valueCount, ByteBufferInputStream stream) throws IOException
{    int effectiveBitLength = valueCount * bitsPerValue;    int length = BytesUtils.paddedByteCountFromBits(effectiveBitLength);        this.in = stream.sliceStream(length);    this.bitPackingReader = createBitPackingReader(bitsPerValue, this.in, valueCount);    updateNextOffset(length);}
1
public void skip()
{    readInteger();}
0
private void init()
{    this.bitPackingWriter = getBitPackingWriter(bitsPerValue, out);}
0
public void writeInteger(int v)
{    try {        bitPackingWriter.write(v);    } catch (IOException e) {        throw new ParquetEncodingException(e);    }}
0
public long getBufferedSize()
{    return out.size();}
0
public BytesInput getBytes()
{    try {        this.bitPackingWriter.finish();        return BytesInput.from(out);    } catch (IOException e) {        throw new ParquetEncodingException(e);    }}
0
public void reset()
{    out.reset();    init();}
0
public void close()
{    out.close();}
0
public long getAllocatedSize()
{    return out.getCapacity();}
0
public String memUsageString(String prefix)
{    return out.memUsageString(prefix);}
0
public Encoding getEncoding()
{    return BIT_PACKED;}
0
public int readInteger()
{    ++decodedPosition;    if (decodedPosition == decoded.length) {        try {            if (in.available() < bitWidth) {                                                byte[] tempEncode = new byte[bitWidth];                in.read(tempEncode, 0, in.available());                packer.unpack8Values(tempEncode, 0, decoded, 0);            } else {                ByteBuffer encoded = in.slice(bitWidth);                packer.unpack8Values(encoded, encoded.position(), decoded, 0);            }        } catch (IOException e) {            throw new ParquetDecodingException("Failed to read packed values", e);        }        decodedPosition = 0;    }    return decoded[decodedPosition];}
0
public void initFromPage(int valueCount, ByteBufferInputStream stream) throws IOException
{    int effectiveBitLength = valueCount * bitWidth;        int length = BytesUtils.paddedByteCountFromBits(effectiveBitLength);                    length = Math.min(length, stream.available());    this.in = stream.sliceStream(length);    this.decodedPosition = VALUES_AT_A_TIME - 1;    updateNextOffset(length);}
1
public void skip()
{    readInteger();}
0
public void writeInteger(int v)
{    try {        this.encoder.writeInt(v);    } catch (IOException e) {        throw new ParquetEncodingException(e);    }}
0
public Encoding getEncoding()
{    return BIT_PACKED;}
0
public BytesInput getBytes()
{    try {        return encoder.toBytes();    } catch (IOException e) {        throw new ParquetEncodingException(e);    }}
0
public void reset()
{    encoder = new ByteBasedBitPackingEncoder(bitWidth, packer);}
0
public long getBufferedSize()
{    return encoder.getBufferSize();}
0
public long getAllocatedSize()
{    return encoder.getAllocatedSize();}
0
public String memUsageString(String prefix)
{    return encoder.memUsageString(prefix);}
0
public long getBufferedSize()
{    return 0;}
0
public void reset()
{}
0
public void writeInteger(int v)
{}
0
public void writeByte(int value)
{}
0
public void writeBoolean(boolean v)
{}
0
public void writeBytes(Binary v)
{}
0
public void writeLong(long v)
{}
0
public void writeDouble(double v)
{}
0
public void writeFloat(float v)
{}
0
public BytesInput getBytes()
{    return BytesInput.empty();}
0
public long getAllocatedSize()
{    return 0;}
0
public Encoding getEncoding()
{    return BIT_PACKED;}
0
public String memUsageString(String prefix)
{    return prefix + "0";}
0
public static DeltaBinaryPackingConfig readConfig(InputStream in) throws IOException
{    return new DeltaBinaryPackingConfig(BytesUtils.readUnsignedVarInt(in), BytesUtils.readUnsignedVarInt(in));}
0
public BytesInput toBytesInput()
{    return BytesInput.concat(BytesInput.fromUnsignedVarInt(blockSizeInValues), BytesInput.fromUnsignedVarInt(miniBlockNumInABlock));}
0
public void initFromPage(int valueCount, ByteBufferInputStream stream) throws IOException
{    this.in = stream;    long startPos = in.position();    this.config = DeltaBinaryPackingConfig.readConfig(in);    this.totalValueCount = BytesUtils.readUnsignedVarInt(in);    allocateValuesBuffer();    bitWidths = new int[config.miniBlockNumInABlock];        valuesBuffer[valuesBuffered++] = BytesUtils.readZigZagVarLong(in);    while (valuesBuffered < totalValueCount) {                loadNewBlockToBuffer();    }    updateNextOffset((int) (in.position() - startPos));}
0
private void allocateValuesBuffer()
{    int totalMiniBlockCount = (int) Math.ceil((double) totalValueCount / config.miniBlockSizeInValues);        valuesBuffer = new long[totalMiniBlockCount * config.miniBlockSizeInValues + 1];}
0
public void skip()
{    checkRead();    valuesRead++;}
0
public void skip(int n)
{        valuesRead += n - 1;    checkRead();    ++valuesRead;}
0
public int readInteger()
{        return (int) readLong();}
0
public long readLong()
{    checkRead();    return valuesBuffer[valuesRead++];}
0
private void checkRead()
{    if (valuesRead >= totalValueCount) {        throw new ParquetDecodingException("no more value to read, total value count is " + totalValueCount);    }}
0
private void loadNewBlockToBuffer() throws IOException
{    try {        minDeltaInCurrentBlock = BytesUtils.readZigZagVarLong(in);    } catch (IOException e) {        throw new ParquetDecodingException("can not read min delta in current block", e);    }    readBitWidthsForMiniBlocks();        int i;    for (i = 0; i < config.miniBlockNumInABlock && valuesBuffered < totalValueCount; i++) {        BytePackerForLong packer = Packer.LITTLE_ENDIAN.newBytePackerForLong(bitWidths[i]);        unpackMiniBlock(packer);    }        int valueUnpacked = i * config.miniBlockSizeInValues;    for (int j = valuesBuffered - valueUnpacked; j < valuesBuffered; j++) {        int index = j;        valuesBuffer[index] += minDeltaInCurrentBlock + valuesBuffer[index - 1];    }}
0
private void unpackMiniBlock(BytePackerForLong packer) throws IOException
{    for (int j = 0; j < config.miniBlockSizeInValues; j += 8) {        unpack8Values(packer);    }}
0
private void unpack8Values(BytePackerForLong packer) throws IOException
{            ByteBuffer buffer = in.slice(packer.getBitWidth());    packer.unpack8Values(buffer, buffer.position(), valuesBuffer, valuesBuffered);    this.valuesBuffered += 8;}
0
private void readBitWidthsForMiniBlocks()
{    for (int i = 0; i < config.miniBlockNumInABlock; i++) {        try {            bitWidths[i] = BytesUtils.readIntLittleEndianOnOneByte(in);        } catch (IOException e) {            throw new ParquetDecodingException("Can not decode bitwidth in block header", e);        }    }}
0
public long getBufferedSize()
{    return baos.size();}
0
protected void writeBitWidthForMiniBlock(int i)
{    try {        BytesUtils.writeIntLittleEndianOnOneByte(baos, bitWidths[i]);    } catch (IOException e) {        throw new ParquetEncodingException("can not write bitwith for miniblock", e);    }}
0
protected int getMiniBlockCountToFlush(double numberCount)
{    return (int) Math.ceil(numberCount / config.miniBlockSizeInValues);}
0
public Encoding getEncoding()
{    return Encoding.DELTA_BINARY_PACKED;}
0
public void reset()
{    this.totalValueCount = 0;    this.baos.reset();    this.deltaValuesToFlush = 0;}
0
public void close()
{    this.totalValueCount = 0;    this.baos.close();    this.deltaValuesToFlush = 0;}
0
public long getAllocatedSize()
{    return baos.getCapacity();}
0
public String memUsageString(String prefix)
{    return String.format("%s DeltaBinaryPacking %d bytes", prefix, getAllocatedSize());}
0
public void writeInteger(int v)
{    totalValueCount++;    if (totalValueCount == 1) {        firstValue = v;        previousValue = firstValue;        return;    }                int delta = v - previousValue;    previousValue = v;    deltaBlockBuffer[deltaValuesToFlush++] = delta;    if (delta < minDeltaInCurrentBlock) {        minDeltaInCurrentBlock = delta;    }    if (config.blockSizeInValues == deltaValuesToFlush) {        flushBlockBuffer();    }}
0
private void flushBlockBuffer()
{        for (int i = 0; i < deltaValuesToFlush; i++) {        deltaBlockBuffer[i] = deltaBlockBuffer[i] - minDeltaInCurrentBlock;    }    writeMinDelta();    int miniBlocksToFlush = getMiniBlockCountToFlush(deltaValuesToFlush);    calculateBitWidthsForDeltaBlockBuffer(miniBlocksToFlush);    for (int i = 0; i < config.miniBlockNumInABlock; i++) {        writeBitWidthForMiniBlock(i);    }    for (int i = 0; i < miniBlocksToFlush; i++) {                int currentBitWidth = bitWidths[i];        int blockOffset = 0;        BytePacker packer = Packer.LITTLE_ENDIAN.newBytePacker(currentBitWidth);        int miniBlockStart = i * config.miniBlockSizeInValues;        for (int j = miniBlockStart; j < (i + 1) * config.miniBlockSizeInValues; j += 8) {                                                                        packer.pack8Values(deltaBlockBuffer, j, miniBlockByteBuffer, blockOffset);            blockOffset += currentBitWidth;        }        baos.write(miniBlockByteBuffer, 0, blockOffset);    }    minDeltaInCurrentBlock = Integer.MAX_VALUE;    deltaValuesToFlush = 0;}
0
private void writeMinDelta()
{    try {        BytesUtils.writeZigZagVarInt(minDeltaInCurrentBlock, baos);    } catch (IOException e) {        throw new ParquetEncodingException("can not write min delta for block", e);    }}
0
private void calculateBitWidthsForDeltaBlockBuffer(int miniBlocksToFlush)
{    for (int miniBlockIndex = 0; miniBlockIndex < miniBlocksToFlush; miniBlockIndex++) {        int mask = 0;        int miniStart = miniBlockIndex * config.miniBlockSizeInValues;                int miniEnd = Math.min((miniBlockIndex + 1) * config.miniBlockSizeInValues, deltaValuesToFlush);        for (int i = miniStart; i < miniEnd; i++) {            mask |= deltaBlockBuffer[i];        }        bitWidths[miniBlockIndex] = 32 - Integer.numberOfLeadingZeros(mask);    }}
0
public BytesInput getBytes()
{        if (deltaValuesToFlush != 0) {        flushBlockBuffer();    }    return BytesInput.concat(config.toBytesInput(), BytesInput.fromUnsignedVarInt(totalValueCount), BytesInput.fromZigZagVarInt(firstValue), BytesInput.from(baos));}
0
public void reset()
{    super.reset();    this.minDeltaInCurrentBlock = Integer.MAX_VALUE;}
0
public void close()
{    super.close();    this.minDeltaInCurrentBlock = Integer.MAX_VALUE;}
0
public void writeLong(long v)
{    totalValueCount++;    if (totalValueCount == 1) {        firstValue = v;        previousValue = firstValue;        return;    }                long delta = v - previousValue;    previousValue = v;    deltaBlockBuffer[deltaValuesToFlush++] = delta;    if (delta < minDeltaInCurrentBlock) {        minDeltaInCurrentBlock = delta;    }    if (config.blockSizeInValues == deltaValuesToFlush) {        flushBlockBuffer();    }}
0
private void flushBlockBuffer()
{        for (int i = 0; i < deltaValuesToFlush; i++) {        deltaBlockBuffer[i] = deltaBlockBuffer[i] - minDeltaInCurrentBlock;    }    writeMinDelta();    int miniBlocksToFlush = getMiniBlockCountToFlush(deltaValuesToFlush);    calculateBitWidthsForDeltaBlockBuffer(miniBlocksToFlush);    for (int i = 0; i < config.miniBlockNumInABlock; i++) {        writeBitWidthForMiniBlock(i);    }    for (int i = 0; i < miniBlocksToFlush; i++) {                int currentBitWidth = bitWidths[i];        int blockOffset = 0;                BytePackerForLong packer = Packer.LITTLE_ENDIAN.newBytePackerForLong(currentBitWidth);        int miniBlockStart = i * config.miniBlockSizeInValues;                for (int j = miniBlockStart; j < (i + 1) * config.miniBlockSizeInValues; j += 8) {                                                            packer.pack8Values(deltaBlockBuffer, j, miniBlockByteBuffer, blockOffset);            blockOffset += currentBitWidth;        }        baos.write(miniBlockByteBuffer, 0, blockOffset);    }    minDeltaInCurrentBlock = Long.MAX_VALUE;    deltaValuesToFlush = 0;}
0
private void writeMinDelta()
{    try {        BytesUtils.writeZigZagVarLong(minDeltaInCurrentBlock, baos);    } catch (IOException e) {        throw new ParquetEncodingException("can not write min delta for block", e);    }}
0
private void calculateBitWidthsForDeltaBlockBuffer(int miniBlocksToFlush)
{    for (int miniBlockIndex = 0; miniBlockIndex < miniBlocksToFlush; miniBlockIndex++) {        long mask = 0;        int miniStart = miniBlockIndex * config.miniBlockSizeInValues;                int miniEnd = Math.min((miniBlockIndex + 1) * config.miniBlockSizeInValues, deltaValuesToFlush);        for (int i = miniStart; i < miniEnd; i++) {            mask |= deltaBlockBuffer[i];        }        bitWidths[miniBlockIndex] = 64 - Long.numberOfLeadingZeros(mask);    }}
0
public BytesInput getBytes()
{        if (deltaValuesToFlush != 0) {        flushBlockBuffer();    }    return BytesInput.concat(config.toBytesInput(), BytesInput.fromUnsignedVarInt(totalValueCount), BytesInput.fromZigZagVarLong(firstValue), BytesInput.from(baos));}
0
public void reset()
{    super.reset();    this.minDeltaInCurrentBlock = Long.MAX_VALUE;}
0
public void close()
{    super.close();    this.minDeltaInCurrentBlock = Long.MAX_VALUE;}
0
public void initFromPage(int valueCount, ByteBufferInputStream stream) throws IOException
{        lengthReader.initFromPage(valueCount, stream);    this.in = stream.remainingStream();}
1
public Binary readBytes()
{    int length = lengthReader.readInteger();    try {        return Binary.fromConstantByteBuffer(in.slice(length));    } catch (IOException e) {        throw new ParquetDecodingException("Failed to read " + length + " bytes");    }}
0
public void skip()
{    skip(1);}
0
public void skip(int n)
{    int length = 0;    for (int i = 0; i < n; ++i) {        length += lengthReader.readInteger();    }    try {        in.skipFully(length);    } catch (IOException e) {        throw new ParquetDecodingException("Failed to skip " + length + " bytes");    }}
0
public void writeBytes(Binary v)
{    try {        lengthWriter.writeInteger(v.length());        v.writeTo(out);    } catch (IOException e) {        throw new ParquetEncodingException("could not write bytes", e);    }}
0
public long getBufferedSize()
{    return lengthWriter.getBufferedSize() + arrayOut.size();}
0
public BytesInput getBytes()
{    try {        out.flush();    } catch (IOException e) {        throw new ParquetEncodingException("could not write page", e);    }        return BytesInput.concat(lengthWriter.getBytes(), BytesInput.from(arrayOut));}
1
public Encoding getEncoding()
{    return Encoding.DELTA_LENGTH_BYTE_ARRAY;}
0
public void reset()
{    lengthWriter.reset();    arrayOut.reset();}
0
public void close()
{    lengthWriter.close();    arrayOut.close();}
0
public long getAllocatedSize()
{    return lengthWriter.getAllocatedSize() + arrayOut.getCapacity();}
0
public String memUsageString(String prefix)
{    return arrayOut.memUsageString(lengthWriter.memUsageString(prefix) + " DELTA_LENGTH_BYTE_ARRAY");}
0
public void initFromPage(int valueCount, ByteBufferInputStream stream) throws IOException
{    prefixLengthReader.initFromPage(valueCount, stream);    suffixReader.initFromPage(valueCount, stream);}
0
public void skip()
{        readBytes();}
0
public Binary readBytes()
{    int prefixLength = prefixLengthReader.readInteger();        Binary suffix = suffixReader.readBytes();    int length = prefixLength + suffix.length();        if (prefixLength != 0) {        byte[] out = new byte[length];        System.arraycopy(previous.getBytesUnsafe(), 0, out, 0, prefixLength);        System.arraycopy(suffix.getBytesUnsafe(), 0, out, prefixLength, suffix.length());        previous = Binary.fromConstantByteArray(out);    } else {        previous = suffix;    }    return previous;}
0
public void setPreviousReader(ValuesReader reader)
{    if (reader != null) {        this.previous = ((DeltaByteArrayReader) reader).previous;    }}
0
public long getBufferedSize()
{    return prefixLengthWriter.getBufferedSize() + suffixWriter.getBufferedSize();}
0
public BytesInput getBytes()
{    return BytesInput.concat(prefixLengthWriter.getBytes(), suffixWriter.getBytes());}
0
public Encoding getEncoding()
{    return Encoding.DELTA_BYTE_ARRAY;}
0
public void reset()
{    prefixLengthWriter.reset();    suffixWriter.reset();    previous = new byte[0];}
0
public void close()
{    prefixLengthWriter.close();    suffixWriter.close();}
0
public long getAllocatedSize()
{    return prefixLengthWriter.getAllocatedSize() + suffixWriter.getAllocatedSize();}
0
public String memUsageString(String prefix)
{    prefix = prefixLengthWriter.memUsageString(prefix);    return suffixWriter.memUsageString(prefix + "  DELTA_STRINGS");}
0
public void writeBytes(Binary v)
{    int i = 0;    byte[] vb = v.getBytes();    int length = previous.length < vb.length ? previous.length : vb.length;        for (i = 0; (i < length) && (previous[i] == vb[i]); i++) ;    prefixLengthWriter.writeInteger(i);    suffixWriter.writeBytes(v.slice(i, vb.length - i));    previous = vb;}
0
public void initFromPage(int valueCount, ByteBufferInputStream stream) throws IOException
{    this.in = stream.remainingStream();    if (in.available() > 0) {                int bitWidth = BytesUtils.readIntLittleEndianOnOneByte(in);                decoder = new RunLengthBitPackingHybridDecoder(bitWidth, in);    } else {        decoder = new RunLengthBitPackingHybridDecoder(1, in) {            @Override            public int readInt() throws IOException {                throw new IOException("Attempt to read from empty page");            }        };    }}
1
public int readInt() throws IOException
{    throw new IOException("Attempt to read from empty page");}
0
public int readValueDictionaryId()
{    try {        return decoder.readInt();    } catch (IOException e) {        throw new ParquetDecodingException(e);    }}
0
public Binary readBytes()
{    try {        return dictionary.decodeToBinary(decoder.readInt());    } catch (IOException e) {        throw new ParquetDecodingException(e);    }}
0
public float readFloat()
{    try {        return dictionary.decodeToFloat(decoder.readInt());    } catch (IOException e) {        throw new ParquetDecodingException(e);    }}
0
public double readDouble()
{    try {        return dictionary.decodeToDouble(decoder.readInt());    } catch (IOException e) {        throw new ParquetDecodingException(e);    }}
0
public int readInteger()
{    try {        return dictionary.decodeToInt(decoder.readInt());    } catch (IOException e) {        throw new ParquetDecodingException(e);    }}
0
public long readLong()
{    try {        return dictionary.decodeToLong(decoder.readInt());    } catch (IOException e) {        throw new ParquetDecodingException(e);    }}
0
public void skip()
{    try {                decoder.readInt();    } catch (IOException e) {        throw new ParquetDecodingException(e);    }}
0
protected DictionaryPage dictPage(ValuesWriter dictPageWriter)
{    DictionaryPage ret = new DictionaryPage(dictPageWriter.getBytes(), lastUsedDictionarySize, encodingForDictionaryPage);    dictPageWriter.close();    return ret;}
0
public boolean shouldFallBack()
{        return dictionaryByteSize > maxDictionaryByteSize || getDictionarySize() > MAX_DICTIONARY_ENTRIES;}
0
public boolean isCompressionSatisfying(long rawSize, long encodedSize)
{    return (encodedSize + dictionaryByteSize) < rawSize;}
0
public void fallBackAllValuesTo(ValuesWriter writer)
{    fallBackDictionaryEncodedData(writer);    if (lastUsedDictionarySize == 0) {                        clearDictionaryContent();        dictionaryByteSize = 0;        encodedValues = new IntList();    }}
0
public long getBufferedSize()
{    return encodedValues.size() * 4;}
0
public long getAllocatedSize()
{        return encodedValues.size() * 4 + dictionaryByteSize;}
0
public BytesInput getBytes()
{    int maxDicId = getDictionarySize() - 1;        int bitWidth = BytesUtils.getWidthFromMaxInt(maxDicId);    int initialSlabSize = CapacityByteArrayOutputStream.initialSlabSizeHeuristic(MIN_INITIAL_SLAB_SIZE, maxDictionaryByteSize, 10);    RunLengthBitPackingHybridEncoder encoder = new RunLengthBitPackingHybridEncoder(bitWidth, initialSlabSize, maxDictionaryByteSize, this.allocator);    encoders.add(encoder);    IntIterator iterator = encodedValues.iterator();    try {        while (iterator.hasNext()) {            encoder.writeInt(iterator.next());        }                byte[] bytesHeader = new byte[] { (byte) bitWidth };        BytesInput rleEncodedBytes = encoder.toBytes();                BytesInput bytes = concat(BytesInput.from(bytesHeader), rleEncodedBytes);                lastUsedDictionarySize = getDictionarySize();        lastUsedDictionaryByteSize = dictionaryByteSize;        return bytes;    } catch (IOException e) {        throw new ParquetEncodingException("could not encode the values", e);    }}
1
public Encoding getEncoding()
{    return encodingForDataPage;}
0
public void reset()
{    close();    encodedValues = new IntList();}
0
public void close()
{    encodedValues = null;    for (RunLengthBitPackingHybridEncoder encoder : encoders) {        encoder.close();    }    encoders.clear();}
0
public void resetDictionary()
{    lastUsedDictionaryByteSize = 0;    lastUsedDictionarySize = 0;    dictionaryTooBig = false;    clearDictionaryContent();}
0
public String memUsageString(String prefix)
{    return String.format("%s DictionaryValuesWriter{\n" + "%s\n" + "%s\n" + "%s}\n", prefix, prefix + " dict:" + dictionaryByteSize, prefix + " values:" + String.valueOf(encodedValues.size() * 4), prefix);}
0
public void writeBytes(Binary v)
{    int id = binaryDictionaryContent.getInt(v);    if (id == -1) {        id = binaryDictionaryContent.size();        binaryDictionaryContent.put(v.copy(), id);                dictionaryByteSize += 4 + v.length();    }    encodedValues.add(id);}
0
public DictionaryPage toDictPageAndClose()
{    if (lastUsedDictionarySize > 0) {                PlainValuesWriter dictionaryEncoder = new PlainValuesWriter(lastUsedDictionaryByteSize, maxDictionaryByteSize, allocator);        Iterator<Binary> binaryIterator = binaryDictionaryContent.keySet().iterator();                for (int i = 0; i < lastUsedDictionarySize; i++) {            Binary entry = binaryIterator.next();            dictionaryEncoder.writeBytes(entry);        }        return dictPage(dictionaryEncoder);    }    return null;}
0
public int getDictionarySize()
{    return binaryDictionaryContent.size();}
0
protected void clearDictionaryContent()
{    binaryDictionaryContent.clear();}
0
public void fallBackDictionaryEncodedData(ValuesWriter writer)
{        Binary[] reverseDictionary = new Binary[getDictionarySize()];    for (Object2IntMap.Entry<Binary> entry : binaryDictionaryContent.object2IntEntrySet()) {        reverseDictionary[entry.getIntValue()] = entry.getKey();    }        IntIterator iterator = encodedValues.iterator();    while (iterator.hasNext()) {        int id = iterator.next();        writer.writeBytes(reverseDictionary[id]);    }}
0
public void writeBytes(Binary value)
{    int id = binaryDictionaryContent.getInt(value);    if (id == -1) {        id = binaryDictionaryContent.size();        binaryDictionaryContent.put(value.copy(), id);        dictionaryByteSize += length;    }    encodedValues.add(id);}
0
public DictionaryPage toDictPageAndClose()
{    if (lastUsedDictionarySize > 0) {                FixedLenByteArrayPlainValuesWriter dictionaryEncoder = new FixedLenByteArrayPlainValuesWriter(length, lastUsedDictionaryByteSize, maxDictionaryByteSize, allocator);        Iterator<Binary> binaryIterator = binaryDictionaryContent.keySet().iterator();                for (int i = 0; i < lastUsedDictionarySize; i++) {            Binary entry = binaryIterator.next();            dictionaryEncoder.writeBytes(entry);        }        return dictPage(dictionaryEncoder);    }    return null;}
0
public void writeLong(long v)
{    int id = longDictionaryContent.get(v);    if (id == -1) {        id = longDictionaryContent.size();        longDictionaryContent.put(v, id);        dictionaryByteSize += 8;    }    encodedValues.add(id);}
0
public DictionaryPage toDictPageAndClose()
{    if (lastUsedDictionarySize > 0) {                PlainValuesWriter dictionaryEncoder = new PlainValuesWriter(lastUsedDictionaryByteSize, maxDictionaryByteSize, allocator);        LongIterator longIterator = longDictionaryContent.keySet().iterator();                for (int i = 0; i < lastUsedDictionarySize; i++) {            dictionaryEncoder.writeLong(longIterator.nextLong());        }        return dictPage(dictionaryEncoder);    }    return null;}
0
public int getDictionarySize()
{    return longDictionaryContent.size();}
0
protected void clearDictionaryContent()
{    longDictionaryContent.clear();}
0
public void fallBackDictionaryEncodedData(ValuesWriter writer)
{        long[] reverseDictionary = new long[getDictionarySize()];    ObjectIterator<Long2IntMap.Entry> entryIterator = longDictionaryContent.long2IntEntrySet().iterator();    while (entryIterator.hasNext()) {        Long2IntMap.Entry entry = entryIterator.next();        reverseDictionary[entry.getIntValue()] = entry.getLongKey();    }        IntIterator iterator = encodedValues.iterator();    while (iterator.hasNext()) {        int id = iterator.next();        writer.writeLong(reverseDictionary[id]);    }}
0
public void writeDouble(double v)
{    int id = doubleDictionaryContent.get(v);    if (id == -1) {        id = doubleDictionaryContent.size();        doubleDictionaryContent.put(v, id);        dictionaryByteSize += 8;    }    encodedValues.add(id);}
0
public DictionaryPage toDictPageAndClose()
{    if (lastUsedDictionarySize > 0) {                PlainValuesWriter dictionaryEncoder = new PlainValuesWriter(lastUsedDictionaryByteSize, maxDictionaryByteSize, allocator);        DoubleIterator doubleIterator = doubleDictionaryContent.keySet().iterator();                for (int i = 0; i < lastUsedDictionarySize; i++) {            dictionaryEncoder.writeDouble(doubleIterator.nextDouble());        }        return dictPage(dictionaryEncoder);    }    return null;}
0
public int getDictionarySize()
{    return doubleDictionaryContent.size();}
0
protected void clearDictionaryContent()
{    doubleDictionaryContent.clear();}
0
public void fallBackDictionaryEncodedData(ValuesWriter writer)
{        double[] reverseDictionary = new double[getDictionarySize()];    ObjectIterator<Double2IntMap.Entry> entryIterator = doubleDictionaryContent.double2IntEntrySet().iterator();    while (entryIterator.hasNext()) {        Double2IntMap.Entry entry = entryIterator.next();        reverseDictionary[entry.getIntValue()] = entry.getDoubleKey();    }        IntIterator iterator = encodedValues.iterator();    while (iterator.hasNext()) {        int id = iterator.next();        writer.writeDouble(reverseDictionary[id]);    }}
0
public void writeInteger(int v)
{    int id = intDictionaryContent.get(v);    if (id == -1) {        id = intDictionaryContent.size();        intDictionaryContent.put(v, id);        dictionaryByteSize += 4;    }    encodedValues.add(id);}
0
public DictionaryPage toDictPageAndClose()
{    if (lastUsedDictionarySize > 0) {                PlainValuesWriter dictionaryEncoder = new PlainValuesWriter(lastUsedDictionaryByteSize, maxDictionaryByteSize, allocator);        it.unimi.dsi.fastutil.ints.IntIterator intIterator = intDictionaryContent.keySet().iterator();                for (int i = 0; i < lastUsedDictionarySize; i++) {            dictionaryEncoder.writeInteger(intIterator.nextInt());        }        return dictPage(dictionaryEncoder);    }    return null;}
0
public int getDictionarySize()
{    return intDictionaryContent.size();}
0
protected void clearDictionaryContent()
{    intDictionaryContent.clear();}
0
public void fallBackDictionaryEncodedData(ValuesWriter writer)
{        int[] reverseDictionary = new int[getDictionarySize()];    ObjectIterator<Int2IntMap.Entry> entryIterator = intDictionaryContent.int2IntEntrySet().iterator();    while (entryIterator.hasNext()) {        Int2IntMap.Entry entry = entryIterator.next();        reverseDictionary[entry.getIntValue()] = entry.getIntKey();    }        IntIterator iterator = encodedValues.iterator();    while (iterator.hasNext()) {        int id = iterator.next();        writer.writeInteger(reverseDictionary[id]);    }}
0
public void writeFloat(float v)
{    int id = floatDictionaryContent.get(v);    if (id == -1) {        id = floatDictionaryContent.size();        floatDictionaryContent.put(v, id);        dictionaryByteSize += 4;    }    encodedValues.add(id);}
0
public DictionaryPage toDictPageAndClose()
{    if (lastUsedDictionarySize > 0) {                PlainValuesWriter dictionaryEncoder = new PlainValuesWriter(lastUsedDictionaryByteSize, maxDictionaryByteSize, allocator);        FloatIterator floatIterator = floatDictionaryContent.keySet().iterator();                for (int i = 0; i < lastUsedDictionarySize; i++) {            dictionaryEncoder.writeFloat(floatIterator.nextFloat());        }        return dictPage(dictionaryEncoder);    }    return null;}
0
public int getDictionarySize()
{    return floatDictionaryContent.size();}
0
protected void clearDictionaryContent()
{    floatDictionaryContent.clear();}
0
public void fallBackDictionaryEncodedData(ValuesWriter writer)
{        float[] reverseDictionary = new float[getDictionarySize()];    ObjectIterator<Float2IntMap.Entry> entryIterator = floatDictionaryContent.float2IntEntrySet().iterator();    while (entryIterator.hasNext()) {        Float2IntMap.Entry entry = entryIterator.next();        reverseDictionary[entry.getIntValue()] = entry.getFloatKey();    }        IntIterator iterator = encodedValues.iterator();    while (iterator.hasNext()) {        int id = iterator.next();        writer.writeFloat(reverseDictionary[id]);    }}
0
 int getCurrentSlabSize()
{    return currentSlabSize;}
0
public boolean hasNext()
{    return current < count;}
0
public int next()
{    final int result = slabs[currentRow][currentCol];    incrementPosition();    return result;}
0
private void incrementPosition()
{    current++;    currentCol++;    if (currentCol >= slabs[currentRow].length) {        currentCol = 0;        currentRow++;    }}
0
private void allocateSlab()
{    currentSlab = new int[currentSlabSize];    currentSlabPos = 0;}
0
private void updateCurrentSlabSize()
{    if (currentSlabSize < MAX_SLAB_SIZE) {        currentSlabSize *= 2;        if (currentSlabSize > MAX_SLAB_SIZE) {            currentSlabSize = MAX_SLAB_SIZE;        }    }}
0
public void add(int i)
{    if (currentSlab == null) {        allocateSlab();    } else if (currentSlabPos == currentSlab.length) {        slabs.add(currentSlab);        updateCurrentSlabSize();        allocateSlab();    }    currentSlab[currentSlabPos] = i;    ++currentSlabPos;}
0
public IntIterator iterator()
{    if (currentSlab == null) {        allocateSlab();    }    int[][] itSlabs = slabs.toArray(new int[slabs.size() + 1][]);    itSlabs[slabs.size()] = currentSlab;    return new IntIterator(itSlabs, size());}
0
public int size()
{    int size = currentSlabPos;    for (int[] slab : slabs) {        size += slab.length;    }    return size;}
0
public Binary decodeToBinary(int id)
{    return binaryDictionaryContent[id];}
0
public String toString()
{    StringBuilder sb = new StringBuilder("PlainBinaryDictionary {\n");    for (int i = 0; i < binaryDictionaryContent.length; i++) {        sb.append(i).append(" => ").append(binaryDictionaryContent[i]).append("\n");    }    return sb.append("}").toString();}
0
public int getMaxId()
{    return binaryDictionaryContent.length - 1;}
0
public long decodeToLong(int id)
{    return longDictionaryContent[id];}
0
public String toString()
{    StringBuilder sb = new StringBuilder("PlainLongDictionary {\n");    for (int i = 0; i < longDictionaryContent.length; i++) {        sb.append(i).append(" => ").append(longDictionaryContent[i]).append("\n");    }    return sb.append("}").toString();}
0
public int getMaxId()
{    return longDictionaryContent.length - 1;}
0
public double decodeToDouble(int id)
{    return doubleDictionaryContent[id];}
0
public String toString()
{    StringBuilder sb = new StringBuilder("PlainDoubleDictionary {\n");    for (int i = 0; i < doubleDictionaryContent.length; i++) {        sb.append(i).append(" => ").append(doubleDictionaryContent[i]).append("\n");    }    return sb.append("}").toString();}
0
public int getMaxId()
{    return doubleDictionaryContent.length - 1;}
0
public int decodeToInt(int id)
{    return intDictionaryContent[id];}
0
public String toString()
{    StringBuilder sb = new StringBuilder("PlainIntegerDictionary {\n");    for (int i = 0; i < intDictionaryContent.length; i++) {        sb.append(i).append(" => ").append(intDictionaryContent[i]).append("\n");    }    return sb.append("}").toString();}
0
public int getMaxId()
{    return intDictionaryContent.length - 1;}
0
public float decodeToFloat(int id)
{    return floatDictionaryContent[id];}
0
public String toString()
{    StringBuilder sb = new StringBuilder("PlainFloatDictionary {\n");    for (int i = 0; i < floatDictionaryContent.length; i++) {        sb.append(i).append(" => ").append(floatDictionaryContent[i]).append("\n");    }    return sb.append("}").toString();}
0
public int getMaxId()
{    return floatDictionaryContent.length - 1;}
0
public void initialize(ParquetProperties properties)
{    this.parquetProperties = properties;}
0
private Encoding getEncodingForDataPage()
{    return PLAIN_DICTIONARY;}
0
private Encoding getEncodingForDictionaryPage()
{    return PLAIN_DICTIONARY;}
0
public ValuesWriter newValuesWriter(ColumnDescriptor descriptor)
{    switch(descriptor.getType()) {        case BOOLEAN:            return getBooleanValuesWriter();        case FIXED_LEN_BYTE_ARRAY:            return getFixedLenByteArrayValuesWriter(descriptor);        case BINARY:            return getBinaryValuesWriter(descriptor);        case INT32:            return getInt32ValuesWriter(descriptor);        case INT64:            return getInt64ValuesWriter(descriptor);        case INT96:            return getInt96ValuesWriter(descriptor);        case DOUBLE:            return getDoubleValuesWriter(descriptor);        case FLOAT:            return getFloatValuesWriter(descriptor);        default:            throw new IllegalArgumentException("Unknown type " + descriptor.getType());    }}
0
private ValuesWriter getBooleanValuesWriter()
{        return new BooleanPlainValuesWriter();}
0
private ValuesWriter getFixedLenByteArrayValuesWriter(ColumnDescriptor path)
{        return new FixedLenByteArrayPlainValuesWriter(path.getTypeLength(), parquetProperties.getInitialSlabSize(), parquetProperties.getPageSizeThreshold(), parquetProperties.getAllocator());}
0
private ValuesWriter getBinaryValuesWriter(ColumnDescriptor path)
{    ValuesWriter fallbackWriter = new PlainValuesWriter(parquetProperties.getInitialSlabSize(), parquetProperties.getPageSizeThreshold(), parquetProperties.getAllocator());    return DefaultValuesWriterFactory.dictWriterWithFallBack(path, parquetProperties, getEncodingForDictionaryPage(), getEncodingForDataPage(), fallbackWriter);}
0
private ValuesWriter getInt32ValuesWriter(ColumnDescriptor path)
{    ValuesWriter fallbackWriter = new PlainValuesWriter(parquetProperties.getInitialSlabSize(), parquetProperties.getPageSizeThreshold(), parquetProperties.getAllocator());    return DefaultValuesWriterFactory.dictWriterWithFallBack(path, parquetProperties, getEncodingForDictionaryPage(), getEncodingForDataPage(), fallbackWriter);}
0
private ValuesWriter getInt64ValuesWriter(ColumnDescriptor path)
{    ValuesWriter fallbackWriter = new PlainValuesWriter(parquetProperties.getInitialSlabSize(), parquetProperties.getPageSizeThreshold(), parquetProperties.getAllocator());    return DefaultValuesWriterFactory.dictWriterWithFallBack(path, parquetProperties, getEncodingForDictionaryPage(), getEncodingForDataPage(), fallbackWriter);}
0
private ValuesWriter getInt96ValuesWriter(ColumnDescriptor path)
{    ValuesWriter fallbackWriter = new FixedLenByteArrayPlainValuesWriter(12, parquetProperties.getInitialSlabSize(), parquetProperties.getPageSizeThreshold(), parquetProperties.getAllocator());    return DefaultValuesWriterFactory.dictWriterWithFallBack(path, parquetProperties, getEncodingForDictionaryPage(), getEncodingForDataPage(), fallbackWriter);}
0
private ValuesWriter getDoubleValuesWriter(ColumnDescriptor path)
{    ValuesWriter fallbackWriter = new PlainValuesWriter(parquetProperties.getInitialSlabSize(), parquetProperties.getPageSizeThreshold(), parquetProperties.getAllocator());    return DefaultValuesWriterFactory.dictWriterWithFallBack(path, parquetProperties, getEncodingForDictionaryPage(), getEncodingForDataPage(), fallbackWriter);}
0
private ValuesWriter getFloatValuesWriter(ColumnDescriptor path)
{    ValuesWriter fallbackWriter = new PlainValuesWriter(parquetProperties.getInitialSlabSize(), parquetProperties.getPageSizeThreshold(), parquetProperties.getAllocator());    return DefaultValuesWriterFactory.dictWriterWithFallBack(path, parquetProperties, getEncodingForDictionaryPage(), getEncodingForDataPage(), fallbackWriter);}
0
public void initialize(ParquetProperties properties)
{    this.parquetProperties = properties;}
0
private Encoding getEncodingForDataPage()
{    return RLE_DICTIONARY;}
0
private Encoding getEncodingForDictionaryPage()
{    return PLAIN;}
0
public ValuesWriter newValuesWriter(ColumnDescriptor descriptor)
{    switch(descriptor.getType()) {        case BOOLEAN:            return getBooleanValuesWriter();        case FIXED_LEN_BYTE_ARRAY:            return getFixedLenByteArrayValuesWriter(descriptor);        case BINARY:            return getBinaryValuesWriter(descriptor);        case INT32:            return getInt32ValuesWriter(descriptor);        case INT64:            return getInt64ValuesWriter(descriptor);        case INT96:            return getInt96ValuesWriter(descriptor);        case DOUBLE:            return getDoubleValuesWriter(descriptor);        case FLOAT:            return getFloatValuesWriter(descriptor);        default:            throw new IllegalArgumentException("Unknown type " + descriptor.getType());    }}
0
private ValuesWriter getBooleanValuesWriter()
{        return new RunLengthBitPackingHybridValuesWriter(1, parquetProperties.getInitialSlabSize(), parquetProperties.getPageSizeThreshold(), parquetProperties.getAllocator());}
0
private ValuesWriter getFixedLenByteArrayValuesWriter(ColumnDescriptor path)
{    ValuesWriter fallbackWriter = new DeltaByteArrayWriter(parquetProperties.getInitialSlabSize(), parquetProperties.getPageSizeThreshold(), parquetProperties.getAllocator());    return DefaultValuesWriterFactory.dictWriterWithFallBack(path, parquetProperties, getEncodingForDictionaryPage(), getEncodingForDataPage(), fallbackWriter);}
0
private ValuesWriter getBinaryValuesWriter(ColumnDescriptor path)
{    ValuesWriter fallbackWriter = new DeltaByteArrayWriter(parquetProperties.getInitialSlabSize(), parquetProperties.getPageSizeThreshold(), parquetProperties.getAllocator());    return DefaultValuesWriterFactory.dictWriterWithFallBack(path, parquetProperties, getEncodingForDictionaryPage(), getEncodingForDataPage(), fallbackWriter);}
0
private ValuesWriter getInt32ValuesWriter(ColumnDescriptor path)
{    ValuesWriter fallbackWriter = new DeltaBinaryPackingValuesWriterForInteger(parquetProperties.getInitialSlabSize(), parquetProperties.getPageSizeThreshold(), parquetProperties.getAllocator());    return DefaultValuesWriterFactory.dictWriterWithFallBack(path, parquetProperties, getEncodingForDictionaryPage(), getEncodingForDataPage(), fallbackWriter);}
0
private ValuesWriter getInt64ValuesWriter(ColumnDescriptor path)
{    ValuesWriter fallbackWriter = new DeltaBinaryPackingValuesWriterForLong(parquetProperties.getInitialSlabSize(), parquetProperties.getPageSizeThreshold(), parquetProperties.getAllocator());    return DefaultValuesWriterFactory.dictWriterWithFallBack(path, parquetProperties, getEncodingForDictionaryPage(), getEncodingForDataPage(), fallbackWriter);}
0
private ValuesWriter getInt96ValuesWriter(ColumnDescriptor path)
{    ValuesWriter fallbackWriter = new FixedLenByteArrayPlainValuesWriter(12, parquetProperties.getInitialSlabSize(), parquetProperties.getPageSizeThreshold(), parquetProperties.getAllocator());    return DefaultValuesWriterFactory.dictWriterWithFallBack(path, parquetProperties, getEncodingForDictionaryPage(), getEncodingForDataPage(), fallbackWriter);}
0
private ValuesWriter getDoubleValuesWriter(ColumnDescriptor path)
{    ValuesWriter fallbackWriter = new PlainValuesWriter(parquetProperties.getInitialSlabSize(), parquetProperties.getPageSizeThreshold(), parquetProperties.getAllocator());    return DefaultValuesWriterFactory.dictWriterWithFallBack(path, parquetProperties, getEncodingForDictionaryPage(), getEncodingForDataPage(), fallbackWriter);}
0
private ValuesWriter getFloatValuesWriter(ColumnDescriptor path)
{    ValuesWriter fallbackWriter = new PlainValuesWriter(parquetProperties.getInitialSlabSize(), parquetProperties.getPageSizeThreshold(), parquetProperties.getAllocator());    return DefaultValuesWriterFactory.dictWriterWithFallBack(path, parquetProperties, getEncodingForDictionaryPage(), getEncodingForDataPage(), fallbackWriter);}
0
public void initialize(ParquetProperties properties)
{    if (properties.getWriterVersion() == WriterVersion.PARQUET_1_0) {        delegateFactory = DEFAULT_V1_WRITER_FACTORY;    } else {        delegateFactory = DEFAULT_V2_WRITER_FACTORY;    }    delegateFactory.initialize(properties);}
0
public ValuesWriter newValuesWriter(ColumnDescriptor descriptor)
{    return delegateFactory.newValuesWriter(descriptor);}
0
 static DictionaryValuesWriter dictionaryWriter(ColumnDescriptor path, ParquetProperties properties, Encoding dictPageEncoding, Encoding dataPageEncoding)
{    switch(path.getType()) {        case BOOLEAN:            throw new IllegalArgumentException("no dictionary encoding for BOOLEAN");        case BINARY:            return new DictionaryValuesWriter.PlainBinaryDictionaryValuesWriter(properties.getDictionaryPageSizeThreshold(), dataPageEncoding, dictPageEncoding, properties.getAllocator());        case INT32:            return new DictionaryValuesWriter.PlainIntegerDictionaryValuesWriter(properties.getDictionaryPageSizeThreshold(), dataPageEncoding, dictPageEncoding, properties.getAllocator());        case INT64:            return new DictionaryValuesWriter.PlainLongDictionaryValuesWriter(properties.getDictionaryPageSizeThreshold(), dataPageEncoding, dictPageEncoding, properties.getAllocator());        case INT96:            return new DictionaryValuesWriter.PlainFixedLenArrayDictionaryValuesWriter(properties.getDictionaryPageSizeThreshold(), 12, dataPageEncoding, dictPageEncoding, properties.getAllocator());        case DOUBLE:            return new DictionaryValuesWriter.PlainDoubleDictionaryValuesWriter(properties.getDictionaryPageSizeThreshold(), dataPageEncoding, dictPageEncoding, properties.getAllocator());        case FLOAT:            return new DictionaryValuesWriter.PlainFloatDictionaryValuesWriter(properties.getDictionaryPageSizeThreshold(), dataPageEncoding, dictPageEncoding, properties.getAllocator());        case FIXED_LEN_BYTE_ARRAY:            return new DictionaryValuesWriter.PlainFixedLenArrayDictionaryValuesWriter(properties.getDictionaryPageSizeThreshold(), path.getTypeLength(), dataPageEncoding, dictPageEncoding, properties.getAllocator());        default:            throw new IllegalArgumentException("Unknown type " + path.getType());    }}
0
 static ValuesWriter dictWriterWithFallBack(ColumnDescriptor path, ParquetProperties parquetProperties, Encoding dictPageEncoding, Encoding dataPageEncoding, ValuesWriter writerToFallBackTo)
{    if (parquetProperties.isEnableDictionary()) {        return FallbackValuesWriter.of(dictionaryWriter(path, parquetProperties, dictPageEncoding, dataPageEncoding), writerToFallBackTo);    } else {        return writerToFallBackTo;    }}
0
public static FallbackValuesWriter<I, F> of(I initialWriter, F fallBackWriter)
{    return new FallbackValuesWriter<I, F>(initialWriter, fallBackWriter);}
0
public long getBufferedSize()
{        return rawDataByteSize;}
0
public BytesInput getBytes()
{    if (!fellBackAlready && firstPage) {                BytesInput bytes = initialWriter.getBytes();        if (!initialWriter.isCompressionSatisfying(rawDataByteSize, bytes.size())) {            fallBack();        } else {            return bytes;        }    }    return currentWriter.getBytes();}
0
public Encoding getEncoding()
{    Encoding encoding = currentWriter.getEncoding();    if (!fellBackAlready && !initialUsedAndHadDictionary) {        initialUsedAndHadDictionary = encoding.usesDictionary();    }    return encoding;}
0
public void reset()
{    rawDataByteSize = 0;    firstPage = false;    currentWriter.reset();}
0
public void close()
{    initialWriter.close();    fallBackWriter.close();}
0
public DictionaryPage toDictPageAndClose()
{    if (initialUsedAndHadDictionary) {        return initialWriter.toDictPageAndClose();    } else {        return currentWriter.toDictPageAndClose();    }}
0
public void resetDictionary()
{    if (initialUsedAndHadDictionary) {        initialWriter.resetDictionary();    } else {        currentWriter.resetDictionary();    }    currentWriter = initialWriter;    fellBackAlready = false;    initialUsedAndHadDictionary = false;    firstPage = true;}
0
public long getAllocatedSize()
{    return currentWriter.getAllocatedSize();}
0
public String memUsageString(String prefix)
{    return String.format("%s FallbackValuesWriter{\n" + "%s\n" + "%s\n" + "%s}\n", prefix, initialWriter.memUsageString(prefix + " initial:"), fallBackWriter.memUsageString(prefix + " fallback:"), prefix);}
0
private void checkFallback()
{    if (!fellBackAlready && initialWriter.shouldFallBack()) {        fallBack();    }}
0
private void fallBack()
{    fellBackAlready = true;    initialWriter.fallBackAllValuesTo(fallBackWriter);    currentWriter = fallBackWriter;}
0
public void writeByte(int value)
{    rawDataByteSize += 1;    currentWriter.writeByte(value);    checkFallback();}
0
public void writeBytes(Binary v)
{        rawDataByteSize += v.length() + 4;    currentWriter.writeBytes(v);    checkFallback();}
0
public void writeInteger(int v)
{    rawDataByteSize += 4;    currentWriter.writeInteger(v);    checkFallback();}
0
public void writeLong(long v)
{    rawDataByteSize += 8;    currentWriter.writeLong(v);    checkFallback();}
0
public void writeFloat(float v)
{    rawDataByteSize += 4;    currentWriter.writeFloat(v);    checkFallback();}
0
public void writeDouble(double v)
{    rawDataByteSize += 8;    currentWriter.writeDouble(v);    checkFallback();}
0
public Binary readBytes()
{    try {        int length = BytesUtils.readIntLittleEndian(in);        return Binary.fromConstantByteBuffer(in.slice(length));    } catch (IOException e) {        throw new ParquetDecodingException("could not read bytes at offset " + in.position(), e);    } catch (RuntimeException e) {        throw new ParquetDecodingException("could not read bytes at offset " + in.position(), e);    }}
0
public void skip()
{    try {        int length = BytesUtils.readIntLittleEndian(in);        in.skipFully(length);    } catch (IOException e) {        throw new ParquetDecodingException("could not skip bytes at offset " + in.position(), e);    } catch (RuntimeException e) {        throw new ParquetDecodingException("could not skip bytes at offset " + in.position(), e);    }}
0
public void initFromPage(int valueCount, ByteBufferInputStream stream) throws IOException
{        this.in = stream.remainingStream();}
1
public boolean readBoolean()
{    return in.readInteger() == 0 ? false : true;}
0
public void skip()
{    in.readInteger();}
0
public void initFromPage(int valueCount, ByteBufferInputStream stream) throws IOException
{        this.in.initFromPage(valueCount, stream);}
1
public int getNextOffset()
{    return in.getNextOffset();}
0
public final void writeBoolean(boolean v)
{    bitPackingWriter.writeInteger(v ? 1 : 0);}
0
public long getBufferedSize()
{    return bitPackingWriter.getBufferedSize();}
0
public BytesInput getBytes()
{    return bitPackingWriter.getBytes();}
0
public void reset()
{    bitPackingWriter.reset();}
0
public void close()
{    bitPackingWriter.close();}
0
public long getAllocatedSize()
{    return bitPackingWriter.getAllocatedSize();}
0
public Encoding getEncoding()
{    return PLAIN;}
0
public String memUsageString(String prefix)
{    return bitPackingWriter.memUsageString(prefix);}
0
public Binary readBytes()
{    try {        return Binary.fromConstantByteBuffer(in.slice(length));    } catch (IOException | RuntimeException e) {        throw new ParquetDecodingException("could not read bytes at offset " + in.position(), e);    }}
0
public void skip()
{    skip(1);}
0
public void skip(int n)
{    try {        in.skipFully(n * length);    } catch (IOException | RuntimeException e) {        throw new ParquetDecodingException("could not skip bytes at offset " + in.position(), e);    }}
0
public void initFromPage(int valueCount, ByteBufferInputStream stream) throws IOException
{        this.in = stream.remainingStream();}
1
public final void writeBytes(Binary v)
{    if (v.length() != length) {        throw new IllegalArgumentException("Fixed Binary size " + v.length() + " does not match field type length " + length);    }    try {        v.writeTo(out);    } catch (IOException e) {        throw new ParquetEncodingException("could not write fixed bytes", e);    }}
0
public long getBufferedSize()
{    return arrayOut.size();}
0
public BytesInput getBytes()
{    try {        out.flush();    } catch (IOException e) {        throw new ParquetEncodingException("could not write page", e);    }        return BytesInput.from(arrayOut);}
1
public void reset()
{    arrayOut.reset();}
0
public void close()
{    arrayOut.close();}
0
public long getAllocatedSize()
{    return arrayOut.getCapacity();}
0
public Encoding getEncoding()
{    return Encoding.PLAIN;}
0
public String memUsageString(String prefix)
{    return arrayOut.memUsageString(prefix + " PLAIN");}
0
public void initFromPage(int valueCount, ByteBufferInputStream stream) throws IOException
{        this.in = new LittleEndianDataInputStream(stream.remainingStream());}
1
public void skip()
{    skip(1);}
0
 void skipBytesFully(int n) throws IOException
{    int skipped = 0;    while (skipped < n) {        skipped += in.skipBytes(n - skipped);    }}
0
public void skip(int n)
{    try {        skipBytesFully(n * 8);    } catch (IOException e) {        throw new ParquetDecodingException("could not skip " + n + " double values", e);    }}
0
public double readDouble()
{    try {        return in.readDouble();    } catch (IOException e) {        throw new ParquetDecodingException("could not read double", e);    }}
0
public void skip(int n)
{    try {        skipBytesFully(n * 4);    } catch (IOException e) {        throw new ParquetDecodingException("could not skip " + n + " floats", e);    }}
0
public float readFloat()
{    try {        return in.readFloat();    } catch (IOException e) {        throw new ParquetDecodingException("could not read float", e);    }}
0
public void skip(int n)
{    try {        in.skipBytes(n * 4);    } catch (IOException e) {        throw new ParquetDecodingException("could not skip " + n + " ints", e);    }}
0
public int readInteger()
{    try {        return in.readInt();    } catch (IOException e) {        throw new ParquetDecodingException("could not read int", e);    }}
0
public void skip(int n)
{    try {        in.skipBytes(n * 8);    } catch (IOException e) {        throw new ParquetDecodingException("could not skip " + n + " longs", e);    }}
0
public long readLong()
{    try {        return in.readLong();    } catch (IOException e) {        throw new ParquetDecodingException("could not read long", e);    }}
0
public final void writeBytes(Binary v)
{    try {        out.writeInt(v.length());        v.writeTo(out);    } catch (IOException e) {        throw new ParquetEncodingException("could not write bytes", e);    }}
0
public final void writeInteger(int v)
{    try {        out.writeInt(v);    } catch (IOException e) {        throw new ParquetEncodingException("could not write int", e);    }}
0
public final void writeLong(long v)
{    try {        out.writeLong(v);    } catch (IOException e) {        throw new ParquetEncodingException("could not write long", e);    }}
0
public final void writeFloat(float v)
{    try {        out.writeFloat(v);    } catch (IOException e) {        throw new ParquetEncodingException("could not write float", e);    }}
0
public final void writeDouble(double v)
{    try {        out.writeDouble(v);    } catch (IOException e) {        throw new ParquetEncodingException("could not write double", e);    }}
0
public void writeByte(int value)
{    try {        out.write(value);    } catch (IOException e) {        throw new ParquetEncodingException("could not write byte", e);    }}
0
public long getBufferedSize()
{    return arrayOut.size();}
0
public BytesInput getBytes()
{    try {        out.flush();    } catch (IOException e) {        throw new ParquetEncodingException("could not write page", e);    }    if (LOG.isDebugEnabled())            return BytesInput.from(arrayOut);}
1
public void reset()
{    arrayOut.reset();}
0
public void close()
{    arrayOut.close();    out.close();}
0
public long getAllocatedSize()
{    return arrayOut.getCapacity();}
0
public Encoding getEncoding()
{    return Encoding.PLAIN;}
0
public String memUsageString(String prefix)
{    return arrayOut.memUsageString(prefix + " PLAIN");}
0
public int readInt() throws IOException
{    if (currentCount == 0) {        readNext();    }    --currentCount;    int result;    switch(mode) {        case RLE:            result = currentValue;            break;        case PACKED:            result = currentBuffer[currentBuffer.length - 1 - currentCount];            break;        default:            throw new ParquetDecodingException("not a valid mode " + mode);    }    return result;}
0
private void readNext() throws IOException
{    Preconditions.checkArgument(in.available() > 0, "Reading past RLE/BitPacking stream.");    final int header = BytesUtils.readUnsignedVarInt(in);    mode = (header & 1) == 0 ? MODE.RLE : MODE.PACKED;    switch(mode) {        case RLE:            currentCount = header >>> 1;                        currentValue = BytesUtils.readIntLittleEndianPaddedOnBitWidth(in, bitWidth);            break;        case PACKED:            int numGroups = header >>> 1;            currentCount = numGroups * 8;                                    currentBuffer = new int[currentCount];            byte[] bytes = new byte[numGroups * bitWidth];                        int bytesToRead = (int) Math.ceil(currentCount * bitWidth / 8.0);            bytesToRead = Math.min(bytesToRead, in.available());            new DataInputStream(in).readFully(bytes, 0, bytesToRead);            for (int valueIndex = 0, byteIndex = 0; valueIndex < currentCount; valueIndex += 8, byteIndex += bitWidth) {                packer.unpack8Values(bytes, byteIndex, currentBuffer, valueIndex);            }            break;        default:            throw new ParquetDecodingException("not a valid mode " + mode);    }}
1
private void reset(boolean resetBaos)
{    if (resetBaos) {        this.baos.reset();    }    this.previousValue = 0;    this.numBufferedValues = 0;    this.repeatCount = 0;    this.bitPackedGroupCount = 0;    this.bitPackedRunHeaderPointer = -1;    this.toBytesCalled = false;}
0
public void writeInt(int value) throws IOException
{    if (value == previousValue) {                        ++repeatCount;        if (repeatCount >= 8) {                        return;        }    } else {                if (repeatCount >= 8) {                        writeRleRun();        }                repeatCount = 1;                previousValue = value;    }            bufferedValues[numBufferedValues] = value;    ++numBufferedValues;    if (numBufferedValues == 8) {                                writeOrAppendBitPackedRun();    }}
0
private void writeOrAppendBitPackedRun() throws IOException
{    if (bitPackedGroupCount >= 63) {                        endPreviousBitPackedRun();    }    if (bitPackedRunHeaderPointer == -1) {                                baos.write(0);        bitPackedRunHeaderPointer = baos.getCurrentIndex();    }    packer.pack8Values(bufferedValues, 0, packBuffer, 0);    baos.write(packBuffer);        numBufferedValues = 0;            repeatCount = 0;    ++bitPackedGroupCount;}
0
private void endPreviousBitPackedRun()
{    if (bitPackedRunHeaderPointer == -1) {                return;    }        byte bitPackHeader = (byte) ((bitPackedGroupCount << 1) | 1);        baos.setByte(bitPackedRunHeaderPointer, bitPackHeader);        bitPackedRunHeaderPointer = -1;        bitPackedGroupCount = 0;}
0
private void writeRleRun() throws IOException
{                endPreviousBitPackedRun();        BytesUtils.writeUnsignedVarInt(repeatCount << 1, baos);        BytesUtils.writeIntLittleEndianPaddedOnBitWidth(baos, previousValue, bitWidth);        repeatCount = 0;        numBufferedValues = 0;}
0
public BytesInput toBytes() throws IOException
{    Preconditions.checkArgument(!toBytesCalled, "You cannot call toBytes() more than once without calling reset()");        if (repeatCount >= 8) {        writeRleRun();    } else if (numBufferedValues > 0) {        for (int i = numBufferedValues; i < 8; i++) {            bufferedValues[i] = 0;        }        writeOrAppendBitPackedRun();        endPreviousBitPackedRun();    } else {        endPreviousBitPackedRun();    }    toBytesCalled = true;    return BytesInput.from(baos);}
0
public void reset()
{    reset(true);}
0
public void close()
{    reset(false);    baos.close();}
0
public long getBufferedSize()
{    return baos.size();}
0
public long getAllocatedSize()
{    return baos.getCapacity();}
0
public void initFromPage(int valueCountL, ByteBufferInputStream stream) throws IOException
{    int length = BytesUtils.readIntLittleEndian(stream);    this.decoder = new RunLengthBitPackingHybridDecoder(bitWidth, stream.sliceStream(length));        updateNextOffset(length + 4);}
0
public int readInteger()
{    try {        return decoder.readInt();    } catch (IOException e) {        throw new ParquetDecodingException(e);    }}
0
public boolean readBoolean()
{    return readInteger() == 0 ? false : true;}
0
public void skip()
{    readInteger();}
0
public void writeInteger(int v)
{    try {        encoder.writeInt(v);    } catch (IOException e) {        throw new ParquetEncodingException(e);    }}
0
public void writeBoolean(boolean v)
{    writeInteger(v ? 1 : 0);}
0
public long getBufferedSize()
{    return encoder.getBufferedSize();}
0
public long getAllocatedSize()
{    return encoder.getAllocatedSize();}
0
public BytesInput getBytes()
{    try {                BytesInput rle = encoder.toBytes();        return BytesInput.concat(BytesInput.fromInt(Math.toIntExact(rle.size())), rle);    } catch (IOException e) {        throw new ParquetEncodingException(e);    }}
0
public Encoding getEncoding()
{    return Encoding.RLE;}
0
public void reset()
{    encoder.reset();}
0
public void close()
{    encoder.close();}
0
public String memUsageString(String prefix)
{    return String.format("%s RunLengthBitPackingHybrid %d bytes", prefix, getAllocatedSize());}
0
public int readInteger()
{    return 0;}
0
public void initFromPage(int valueCount, ByteBufferInputStream stream) throws IOException
{    updateNextOffset(0);}
0
public void skip()
{}
0
public void skip(int n)
{}
0
public void initFromPage(int valueCount, ByteBuffer page, int offset) throws IOException
{    if (offset < 0) {        throw new IllegalArgumentException("Illegal offset: " + offset);    }    actualOffset = offset;    ByteBuffer pageWithOffset = page.duplicate();    pageWithOffset.position(offset);    initFromPage(valueCount, ByteBufferInputStream.wrap(pageWithOffset));    actualOffset = -1;}
0
public void initFromPage(int valueCount, byte[] page, int offset) throws IOException
{    this.initFromPage(valueCount, ByteBuffer.wrap(page), offset);}
0
public void initFromPage(int valueCount, ByteBufferInputStream in) throws IOException
{    if (actualOffset != -1) {        throw new UnsupportedOperationException("Either initFromPage(int, ByteBuffer, int) or initFromPage(int, ByteBufferInputStream) must be implemented in " + getClass().getName());    }    initFromPage(valueCount, in.slice(valueCount), 0);}
0
public int getNextOffset()
{    if (nextOffset == -1) {        throw new ParquetDecodingException("Unsupported: cannot get offset of the next section.");    } else {        return nextOffset;    }}
0
protected void updateNextOffset(int bytesRead)
{    nextOffset = actualOffset == -1 ? -1 : actualOffset + bytesRead;}
0
public int readValueDictionaryId()
{    throw new UnsupportedOperationException();}
0
public boolean readBoolean()
{    throw new UnsupportedOperationException();}
0
public Binary readBytes()
{    throw new UnsupportedOperationException();}
0
public float readFloat()
{    throw new UnsupportedOperationException();}
0
public double readDouble()
{    throw new UnsupportedOperationException();}
0
public int readInteger()
{    throw new UnsupportedOperationException();}
0
public long readLong()
{    throw new UnsupportedOperationException();}
0
public void skip(int n)
{    for (int i = 0; i < n; ++i) {        skip();    }}
0
public void close()
{}
0
public DictionaryPage toDictPageAndClose()
{    return null;}
0
public void resetDictionary()
{}
0
public void writeByte(int value)
{    throw new UnsupportedOperationException(getClass().getName());}
0
public void writeBoolean(boolean v)
{    throw new UnsupportedOperationException(getClass().getName());}
0
public void writeBytes(Binary v)
{    throw new UnsupportedOperationException(getClass().getName());}
0
public void writeInteger(int v)
{    throw new UnsupportedOperationException(getClass().getName());}
0
public void writeLong(long v)
{    throw new UnsupportedOperationException(getClass().getName());}
0
public void writeDouble(double v)
{    throw new UnsupportedOperationException(getClass().getName());}
0
public void writeFloat(float v)
{    throw new UnsupportedOperationException(getClass().getName());}
0
public static boolean requiresSequentialReads(ParsedVersion version, Encoding encoding)
{    if (encoding != Encoding.DELTA_BYTE_ARRAY) {        return false;    }    if (version == null) {        return true;    }    if (!"parquet-mr".equals(version.application)) {                return false;    }    if (!version.hasSemanticVersion()) {                return true;    }    return requiresSequentialReads(version.getSemanticVersion(), encoding);}
1
public static boolean requiresSequentialReads(SemanticVersion semver, Encoding encoding)
{    if (encoding != Encoding.DELTA_BYTE_ARRAY) {        return false;    }    if (semver == null) {        return true;    }    if (semver.compareTo(PARQUET_246_FIXED_VERSION) < 0) {                return true;    }        return false;}
1
public static boolean requiresSequentialReads(String createdBy, Encoding encoding)
{    if (encoding != Encoding.DELTA_BYTE_ARRAY) {        return false;    }    if (Strings.isNullOrEmpty(createdBy)) {                return true;    }    try {        return requiresSequentialReads(VersionParser.parse(createdBy), encoding);    } catch (RuntimeException e) {        warnParseError(createdBy, e);        return true;    } catch (VersionParser.VersionParseException e) {        warnParseError(createdBy, e);        return true;    }}
1
private static void warnParseError(String createdBy, Throwable e)
{    }
1
public static boolean shouldIgnoreStatistics(String createdBy, PrimitiveTypeName columnType)
{    if (columnType != PrimitiveTypeName.BINARY && columnType != PrimitiveTypeName.FIXED_LEN_BYTE_ARRAY) {                return false;    }    if (Strings.isNullOrEmpty(createdBy)) {                        warnOnce("Ignoring statistics because created_by is null or empty! See PARQUET-251 and PARQUET-297");        return true;    }    try {        ParsedVersion version = VersionParser.parse(createdBy);        if (!"parquet-mr".equals(version.application)) {                        return false;        }        if (Strings.isNullOrEmpty(version.version)) {            warnOnce("Ignoring statistics because created_by did not contain a semver (see PARQUET-251): " + createdBy);            return true;        }        SemanticVersion semver = SemanticVersion.parse(version.version);        if (semver.compareTo(PARQUET_251_FIXED_VERSION) < 0 && !(semver.compareTo(CDH_5_PARQUET_251_FIXED_START) >= 0 && semver.compareTo(CDH_5_PARQUET_251_FIXED_END) < 0)) {            warnOnce("Ignoring statistics because this file was created prior to " + PARQUET_251_FIXED_VERSION + ", see PARQUET-251");            return true;        }                return false;    } catch (RuntimeException e) {                        warnParseErrorOnce(createdBy, e);        return true;    } catch (SemanticVersionParseException e) {                        warnParseErrorOnce(createdBy, e);        return true;    } catch (VersionParseException e) {                        warnParseErrorOnce(createdBy, e);        return true;    }}
0
private static void warnParseErrorOnce(String createdBy, Throwable e)
{    if (!alreadyLogged.getAndSet(true)) {            }}
1
private static void warnOnce(String message)
{    if (!alreadyLogged.getAndSet(true)) {            }}
1
public void add(String field, int value)
{    add(getType().getFieldIndex(field), value);}
0
public void add(String field, long value)
{    add(getType().getFieldIndex(field), value);}
0
public void add(String field, float value)
{    add(getType().getFieldIndex(field), value);}
0
public void add(String field, double value)
{    add(getType().getFieldIndex(field), value);}
0
public void add(String field, String value)
{    add(getType().getFieldIndex(field), value);}
0
public void add(String field, NanoTime value)
{    add(getType().getFieldIndex(field), value);}
0
public void add(String field, boolean value)
{    add(getType().getFieldIndex(field), value);}
0
public void add(String field, Binary value)
{    add(getType().getFieldIndex(field), value);}
0
public void add(String field, Group value)
{    add(getType().getFieldIndex(field), value);}
0
public Group addGroup(String field)
{    if (LOG.isDebugEnabled()) {            }    return addGroup(getType().getFieldIndex(field));}
1
public Group getGroup(String field, int index)
{    return getGroup(getType().getFieldIndex(field), index);}
0
public Group asGroup()
{    return this;}
0
public Group append(String fieldName, int value)
{    add(fieldName, value);    return this;}
0
public Group append(String fieldName, float value)
{    add(fieldName, value);    return this;}
0
public Group append(String fieldName, double value)
{    add(fieldName, value);    return this;}
0
public Group append(String fieldName, long value)
{    add(fieldName, value);    return this;}
0
public Group append(String fieldName, NanoTime value)
{    add(fieldName, value);    return this;}
0
public Group append(String fieldName, String value)
{    add(fieldName, Binary.fromString(value));    return this;}
0
public Group append(String fieldName, boolean value)
{    add(fieldName, value);    return this;}
0
public Group append(String fieldName, Binary value)
{    add(fieldName, value);    return this;}
0
public int getFieldRepetitionCount(String field)
{    return getFieldRepetitionCount(getType().getFieldIndex(field));}
0
public GroupValueSource getGroup(String field, int index)
{    return getGroup(getType().getFieldIndex(field), index);}
0
public String getString(String field, int index)
{    return getString(getType().getFieldIndex(field), index);}
0
public int getInteger(String field, int index)
{    return getInteger(getType().getFieldIndex(field), index);}
0
public long getLong(String field, int index)
{    return getLong(getType().getFieldIndex(field), index);}
0
public double getDouble(String field, int index)
{    return getDouble(getType().getFieldIndex(field), index);}
0
public float getFloat(String field, int index)
{    return getFloat(getType().getFieldIndex(field), index);}
0
public boolean getBoolean(String field, int index)
{    return getBoolean(getType().getFieldIndex(field), index);}
0
public Binary getBinary(String field, int index)
{    return getBinary(getType().getFieldIndex(field), index);}
0
public Binary getInt96(String field, int index)
{    return getInt96(getType().getFieldIndex(field), index);}
0
public void write(Group group)
{    recordConsumer.startMessage();    writeGroup(group, schema);    recordConsumer.endMessage();}
0
private void writeGroup(Group group, GroupType type)
{    int fieldCount = type.getFieldCount();    for (int field = 0; field < fieldCount; ++field) {        int valueCount = group.getFieldRepetitionCount(field);        if (valueCount > 0) {            Type fieldType = type.getType(field);            String fieldName = fieldType.getName();            recordConsumer.startField(fieldName, field);            for (int index = 0; index < valueCount; ++index) {                if (fieldType.isPrimitive()) {                    group.writeValue(field, index, recordConsumer);                } else {                    recordConsumer.startGroup();                    writeGroup(group.getGroup(field, index), fieldType.asGroupType());                    recordConsumer.endGroup();                }            }            recordConsumer.endField(fieldName, field);        }    }}
0
public Binary getBinary()
{    return binary;}
0
public String getString()
{    return binary.toStringUsingUTF8();}
0
public void writeValue(RecordConsumer recordConsumer)
{    recordConsumer.addBinary(binary);}
0
public String toString()
{    return getString();}
0
public String toString()
{    return String.valueOf(bool);}
0
public boolean getBoolean()
{    return bool;}
0
public void writeValue(RecordConsumer recordConsumer)
{    recordConsumer.addBoolean(bool);}
0
public void start()
{    this.current = simpleGroupFactory.newGroup();}
0
public void end()
{}
0
public Group getCurrentRecord()
{    return root.getCurrentRecord();}
0
public GroupConverter getRootConverter()
{    return root;}
0
public void start()
{    current = parent.getCurrentRecord().addGroup(index);}
0
public Converter getConverter(int fieldIndex)
{    return converters[fieldIndex];}
0
public void end()
{}
0
public Group getCurrentRecord()
{    return current;}
0
public void addBinary(Binary value)
{    parent.getCurrentRecord().add(index, value);}
0
public void addBoolean(boolean value)
{    parent.getCurrentRecord().add(index, value);}
0
public void addDouble(double value)
{    parent.getCurrentRecord().add(index, value);}
0
public void addFloat(float value)
{    parent.getCurrentRecord().add(index, value);}
0
public void addInt(int value)
{    parent.getCurrentRecord().add(index, value);}
0
public void addLong(long value)
{    parent.getCurrentRecord().add(index, value);}
0
public double getDouble()
{    return value;}
0
public void writeValue(RecordConsumer recordConsumer)
{    recordConsumer.addDouble(value);}
0
public String toString()
{    return String.valueOf(value);}
0
public float getFloat()
{    return value;}
0
public void writeValue(RecordConsumer recordConsumer)
{    recordConsumer.addFloat(value);}
0
public String toString()
{    return String.valueOf(value);}
0
public Binary getInt96()
{    return value;}
0
public void writeValue(RecordConsumer recordConsumer)
{    recordConsumer.addBinary(value);}
0
public String toString()
{    return "Int96Value{" + String.valueOf(value) + "}";}
0
public String toString()
{    return String.valueOf(value);}
0
public int getInteger()
{    return value;}
0
public void writeValue(RecordConsumer recordConsumer)
{    recordConsumer.addInteger(value);}
0
public String toString()
{    return String.valueOf(value);}
0
public long getLong()
{    return value;}
0
public void writeValue(RecordConsumer recordConsumer)
{    recordConsumer.addLong(value);}
0
public static NanoTime fromBinary(Binary bytes)
{    Preconditions.checkArgument(bytes.length() == 12, "Must be 12 bytes");    ByteBuffer buf = bytes.toByteBuffer();    buf.order(ByteOrder.LITTLE_ENDIAN);    long timeOfDayNanos = buf.getLong();    int julianDay = buf.getInt();    return new NanoTime(julianDay, timeOfDayNanos);}
0
public static NanoTime fromInt96(Int96Value int96)
{    ByteBuffer buf = int96.getInt96().toByteBuffer();    return new NanoTime(buf.getInt(), buf.getLong());}
0
public int getJulianDay()
{    return julianDay;}
0
public long getTimeOfDayNanos()
{    return timeOfDayNanos;}
0
public Binary toBinary()
{    ByteBuffer buf = ByteBuffer.allocate(12);    buf.order(ByteOrder.LITTLE_ENDIAN);    buf.putLong(timeOfDayNanos);    buf.putInt(julianDay);    buf.flip();    return Binary.fromConstantByteBuffer(buf);}
0
public Int96Value toInt96()
{    return new Int96Value(toBinary());}
0
public void writeValue(RecordConsumer recordConsumer)
{    recordConsumer.addBinary(toBinary());}
0
public String toString()
{    return "NanoTime{julianDay=" + julianDay + ", timeOfDayNanos=" + timeOfDayNanos + "}";}
0
public String getString()
{    throw new UnsupportedOperationException();}
0
public int getInteger()
{    throw new UnsupportedOperationException();}
0
public long getLong()
{    throw new UnsupportedOperationException();}
0
public boolean getBoolean()
{    throw new UnsupportedOperationException();}
0
public Binary getBinary()
{    throw new UnsupportedOperationException();}
0
public Binary getInt96()
{    throw new UnsupportedOperationException();}
0
public float getFloat()
{    throw new UnsupportedOperationException();}
0
public double getDouble()
{    throw new UnsupportedOperationException();}
0
public String toString()
{    return toString("");}
0
public String toString(String indent)
{    String result = "";    int i = 0;    for (Type field : schema.getFields()) {        String name = field.getName();        List<Object> values = data[i];        ++i;        if (values != null) {            if (values.size() > 0) {                for (Object value : values) {                    result += indent + name;                    if (value == null) {                        result += ": NULL\n";                    } else if (value instanceof Group) {                        result += "\n" + ((SimpleGroup) value).toString(indent + "  ");                    } else {                        result += ": " + value.toString() + "\n";                    }                }            }        }    }    return result;}
0
public Group addGroup(int fieldIndex)
{    SimpleGroup g = new SimpleGroup(schema.getType(fieldIndex).asGroupType());    add(fieldIndex, g);    return g;}
0
public Group getGroup(int fieldIndex, int index)
{    return (Group) getValue(fieldIndex, index);}
0
private Object getValue(int fieldIndex, int index)
{    List<Object> list;    try {        list = data[fieldIndex];    } catch (IndexOutOfBoundsException e) {        throw new RuntimeException("not found " + fieldIndex + "(" + schema.getFieldName(fieldIndex) + ") in group:\n" + this);    }    try {        return list.get(index);    } catch (IndexOutOfBoundsException e) {        throw new RuntimeException("not found " + fieldIndex + "(" + schema.getFieldName(fieldIndex) + ") element number " + index + " in group:\n" + this);    }}
0
private void add(int fieldIndex, Primitive value)
{    Type type = schema.getType(fieldIndex);    List<Object> list = data[fieldIndex];    if (!type.isRepetition(Type.Repetition.REPEATED) && !list.isEmpty()) {        throw new IllegalStateException("field " + fieldIndex + " (" + type.getName() + ") can not have more than one value: " + list);    }    list.add(value);}
0
public int getFieldRepetitionCount(int fieldIndex)
{    List<Object> list = data[fieldIndex];    return list == null ? 0 : list.size();}
0
public String getValueToString(int fieldIndex, int index)
{    return String.valueOf(getValue(fieldIndex, index));}
0
public String getString(int fieldIndex, int index)
{    return ((BinaryValue) getValue(fieldIndex, index)).getString();}
0
public int getInteger(int fieldIndex, int index)
{    return ((IntegerValue) getValue(fieldIndex, index)).getInteger();}
0
public long getLong(int fieldIndex, int index)
{    return ((LongValue) getValue(fieldIndex, index)).getLong();}
0
public double getDouble(int fieldIndex, int index)
{    return ((DoubleValue) getValue(fieldIndex, index)).getDouble();}
0
public float getFloat(int fieldIndex, int index)
{    return ((FloatValue) getValue(fieldIndex, index)).getFloat();}
0
public boolean getBoolean(int fieldIndex, int index)
{    return ((BooleanValue) getValue(fieldIndex, index)).getBoolean();}
0
public Binary getBinary(int fieldIndex, int index)
{    return ((BinaryValue) getValue(fieldIndex, index)).getBinary();}
0
public NanoTime getTimeNanos(int fieldIndex, int index)
{    return NanoTime.fromInt96((Int96Value) getValue(fieldIndex, index));}
0
public Binary getInt96(int fieldIndex, int index)
{    return ((Int96Value) getValue(fieldIndex, index)).getInt96();}
0
public void add(int fieldIndex, int value)
{    add(fieldIndex, new IntegerValue(value));}
0
public void add(int fieldIndex, long value)
{    add(fieldIndex, new LongValue(value));}
0
public void add(int fieldIndex, String value)
{    add(fieldIndex, new BinaryValue(Binary.fromString(value)));}
0
public void add(int fieldIndex, NanoTime value)
{    add(fieldIndex, value.toInt96());}
0
public void add(int fieldIndex, boolean value)
{    add(fieldIndex, new BooleanValue(value));}
0
public void add(int fieldIndex, Binary value)
{    switch(getType().getType(fieldIndex).asPrimitiveType().getPrimitiveTypeName()) {        case BINARY:        case FIXED_LEN_BYTE_ARRAY:            add(fieldIndex, new BinaryValue(value));            break;        case INT96:            add(fieldIndex, new Int96Value(value));            break;        default:            throw new UnsupportedOperationException(getType().asPrimitiveType().getName() + " not supported for Binary");    }}
0
public void add(int fieldIndex, float value)
{    add(fieldIndex, new FloatValue(value));}
0
public void add(int fieldIndex, double value)
{    add(fieldIndex, new DoubleValue(value));}
0
public void add(int fieldIndex, Group value)
{    data[fieldIndex].add(value);}
0
public GroupType getType()
{    return schema;}
0
public void writeValue(int field, int index, RecordConsumer recordConsumer)
{    ((Primitive) getValue(field, index)).writeValue(recordConsumer);}
0
public Group newGroup()
{    return new SimpleGroup(schema);}
0
public Converter convertPrimitiveType(List<GroupType> path, PrimitiveType primitiveType)
{    return new PrimitiveConverter() {        @Override        public void addBinary(Binary value) {            a = value;        }        @Override        public void addBoolean(boolean value) {            a = value;        }        @Override        public void addDouble(double value) {            a = value;        }        @Override        public void addFloat(float value) {            a = value;        }        @Override        public void addInt(int value) {            a = value;        }        @Override        public void addLong(long value) {            a = value;        }    };}
0
public void addBinary(Binary value)
{    a = value;}
0
public void addBoolean(boolean value)
{    a = value;}
0
public void addDouble(double value)
{    a = value;}
0
public void addFloat(float value)
{    a = value;}
0
public void addInt(int value)
{    a = value;}
0
public void addLong(long value)
{    a = value;}
0
public Converter convertGroupType(List<GroupType> path, GroupType groupType, final List<Converter> converters)
{    return new GroupConverter() {        public Converter getConverter(int fieldIndex) {            return converters.get(fieldIndex);        }        public void start() {            a = "start()";        }        public void end() {            a = "end()";        }    };}
0
public Converter getConverter(int fieldIndex)
{    return converters.get(fieldIndex);}
0
public void start()
{    a = "start()";}
0
public void end()
{    a = "end()";}
0
public Converter convertMessageType(MessageType messageType, List<Converter> children)
{    return convertGroupType(null, messageType, children);}
0
public Object getCurrentRecord()
{    return a;}
0
public GroupConverter getRootConverter()
{    return root;}
0
public static final UnboundRecordFilter and(final UnboundRecordFilter filter1, final UnboundRecordFilter filter2)
{    Preconditions.checkNotNull(filter1, "filter1");    Preconditions.checkNotNull(filter2, "filter2");    return new UnboundRecordFilter() {        @Override        public RecordFilter bind(Iterable<ColumnReader> readers) {            return new AndRecordFilter(filter1.bind(readers), filter2.bind(readers));        }    };}
0
public RecordFilter bind(Iterable<ColumnReader> readers)
{    return new AndRecordFilter(filter1.bind(readers), filter2.bind(readers));}
0
public boolean isMatch()
{    return boundFilter1.isMatch() && boundFilter2.isMatch();}
0
public static Predicate equalTo(final String target)
{    Preconditions.checkNotNull(target, "target");    return new Predicate() {        @Override        public boolean apply(ColumnReader input) {            return target.equals(input.getBinary().toStringUsingUTF8());        }    };}
0
public boolean apply(ColumnReader input)
{    return target.equals(input.getBinary().toStringUsingUTF8());}
0
public static Predicate applyFunctionToString(final PredicateFunction<String> fn)
{    return new Predicate() {        @Override        public boolean apply(ColumnReader input) {            return fn.functionToApply(input.getBinary().toStringUsingUTF8());        }    };}
0
public boolean apply(ColumnReader input)
{    return fn.functionToApply(input.getBinary().toStringUsingUTF8());}
0
public static Predicate equalTo(final int target)
{    return new Predicate() {        @Override        public boolean apply(ColumnReader input) {            return input.getInteger() == target;        }    };}
0
public boolean apply(ColumnReader input)
{    return input.getInteger() == target;}
0
public static Predicate applyFunctionToInteger(final IntegerPredicateFunction fn)
{    return new Predicate() {        @Override        public boolean apply(ColumnReader input) {            return fn.functionToApply(input.getInteger());        }    };}
0
public boolean apply(ColumnReader input)
{    return fn.functionToApply(input.getInteger());}
0
public static Predicate equalTo(final long target)
{    return new Predicate() {        @Override        public boolean apply(ColumnReader input) {            return input.getLong() == target;        }    };}
0
public boolean apply(ColumnReader input)
{    return input.getLong() == target;}
0
public static Predicate applyFunctionToLong(final LongPredicateFunction fn)
{    return new Predicate() {        @Override        public boolean apply(ColumnReader input) {            return fn.functionToApply(input.getLong());        }    };}
0
public boolean apply(ColumnReader input)
{    return fn.functionToApply(input.getLong());}
0
public static Predicate equalTo(final float target)
{    return new Predicate() {        @Override        public boolean apply(ColumnReader input) {            return input.getFloat() == target;        }    };}
0
public boolean apply(ColumnReader input)
{    return input.getFloat() == target;}
0
public static Predicate applyFunctionToFloat(final FloatPredicateFunction fn)
{    return new Predicate() {        @Override        public boolean apply(ColumnReader input) {            return fn.functionToApply(input.getFloat());        }    };}
0
public boolean apply(ColumnReader input)
{    return fn.functionToApply(input.getFloat());}
0
public static Predicate equalTo(final double target)
{    return new Predicate() {        @Override        public boolean apply(ColumnReader input) {            return input.getDouble() == target;        }    };}
0
public boolean apply(ColumnReader input)
{    return input.getDouble() == target;}
0
public static Predicate applyFunctionToDouble(final DoublePredicateFunction fn)
{    return new Predicate() {        @Override        public boolean apply(ColumnReader input) {            return fn.functionToApply(input.getDouble());        }    };}
0
public boolean apply(ColumnReader input)
{    return fn.functionToApply(input.getDouble());}
0
public static Predicate equalTo(final boolean target)
{    return new Predicate() {        @Override        public boolean apply(ColumnReader input) {            return input.getBoolean() == target;        }    };}
0
public boolean apply(ColumnReader input)
{    return input.getBoolean() == target;}
0
public static Predicate applyFunctionToBoolean(final BooleanPredicateFunction fn)
{    return new Predicate() {        @Override        public boolean apply(ColumnReader input) {            return fn.functionToApply(input.getBoolean());        }    };}
0
public boolean apply(ColumnReader input)
{    return fn.functionToApply(input.getBoolean());}
0
public static Predicate equalTo(final E target)
{    Preconditions.checkNotNull(target, "target");    final String targetAsString = target.name();    return new Predicate() {        @Override        public boolean apply(ColumnReader input) {            return targetAsString.equals(input.getBinary().toStringUsingUTF8());        }    };}
0
public boolean apply(ColumnReader input)
{    return targetAsString.equals(input.getBinary().toStringUsingUTF8());}
0
public static Predicate applyFunctionToBinary(final PredicateFunction<Binary> fn)
{    return new Predicate() {        @Override        public boolean apply(ColumnReader input) {            return fn.functionToApply(input.getBinary());        }    };}
0
public boolean apply(ColumnReader input)
{    return fn.functionToApply(input.getBinary());}
0
public static final UnboundRecordFilter column(final String columnPath, final ColumnPredicates.Predicate predicate)
{    checkNotNull(columnPath, "columnPath");    checkNotNull(predicate, "predicate");    return new UnboundRecordFilter() {        final String[] filterPath = columnPath.split("\\.");        @Override        public RecordFilter bind(Iterable<ColumnReader> readers) {            for (ColumnReader reader : readers) {                if (Arrays.equals(reader.getDescriptor().getPath(), filterPath)) {                    return new ColumnRecordFilter(reader, predicate);                }            }            throw new IllegalArgumentException("Column " + columnPath + " does not exist.");        }    };}
0
public RecordFilter bind(Iterable<ColumnReader> readers)
{    for (ColumnReader reader : readers) {        if (Arrays.equals(reader.getDescriptor().getPath(), filterPath)) {            return new ColumnRecordFilter(reader, predicate);        }    }    throw new IllegalArgumentException("Column " + columnPath + " does not exist.");}
0
public boolean isMatch()
{    return filterPredicate.apply(filterOnColumn);}
0
public static final UnboundRecordFilter not(final UnboundRecordFilter filter)
{    Preconditions.checkNotNull(filter, "filter");    return new UnboundRecordFilter() {        @Override        public RecordFilter bind(Iterable<ColumnReader> readers) {            return new NotRecordFilter(filter.bind(readers));        }    };}
0
public RecordFilter bind(Iterable<ColumnReader> readers)
{    return new NotRecordFilter(filter.bind(readers));}
0
public boolean isMatch()
{    return !(boundFilter.isMatch());}
0
public static final UnboundRecordFilter or(final UnboundRecordFilter filter1, final UnboundRecordFilter filter2)
{    Preconditions.checkNotNull(filter1, "filter1");    Preconditions.checkNotNull(filter2, "filter2");    return new UnboundRecordFilter() {        @Override        public RecordFilter bind(Iterable<ColumnReader> readers) {            return new OrRecordFilter(filter1.bind(readers), filter2.bind(readers));        }    };}
0
public RecordFilter bind(Iterable<ColumnReader> readers)
{    return new OrRecordFilter(filter1.bind(readers), filter2.bind(readers));}
0
public boolean isMatch()
{    return boundFilter1.isMatch() || boundFilter2.isMatch();}
0
public static final UnboundRecordFilter page(final long startPos, final long pageSize)
{    return new UnboundRecordFilter() {        @Override        public RecordFilter bind(Iterable<ColumnReader> readers) {            return new PagedRecordFilter(startPos, pageSize);        }    };}
0
public RecordFilter bind(Iterable<ColumnReader> readers)
{    return new PagedRecordFilter(startPos, pageSize);}
0
public boolean isMatch()
{    currentPos++;    return ((currentPos >= startPos) && (currentPos < endPos));}
0
public static Filter get(FilterPredicate filterPredicate)
{    checkNotNull(filterPredicate, "filterPredicate");            FilterPredicate collapsedPredicate = LogicalInverseRewriter.rewrite(filterPredicate);    if (!filterPredicate.equals(collapsedPredicate)) {            }    return new FilterPredicateCompat(collapsedPredicate);}
1
public static Filter get(UnboundRecordFilter unboundRecordFilter)
{    return new UnboundRecordFilterCompat(unboundRecordFilter);}
0
public static Filter get(FilterPredicate filterPredicate, UnboundRecordFilter unboundRecordFilter)
{    checkArgument(filterPredicate == null || unboundRecordFilter == null, "Cannot provide both a FilterPredicate and an UnboundRecordFilter");    if (filterPredicate != null) {        return get(filterPredicate);    }    if (unboundRecordFilter != null) {        return get(unboundRecordFilter);    }    return NOOP;}
0
public FilterPredicate getFilterPredicate()
{    return filterPredicate;}
0
public R accept(Visitor<R> visitor)
{    return visitor.visit(this);}
0
public UnboundRecordFilter getUnboundRecordFilter()
{    return unboundRecordFilter;}
0
public R accept(Visitor<R> visitor)
{    return visitor.visit(this);}
0
public R accept(Visitor<R> visitor)
{    return visitor.visit(this);}
0
public static IntColumn intColumn(String columnPath)
{    return new IntColumn(ColumnPath.fromDotString(columnPath));}
0
public static LongColumn longColumn(String columnPath)
{    return new LongColumn(ColumnPath.fromDotString(columnPath));}
0
public static FloatColumn floatColumn(String columnPath)
{    return new FloatColumn(ColumnPath.fromDotString(columnPath));}
0
public static DoubleColumn doubleColumn(String columnPath)
{    return new DoubleColumn(ColumnPath.fromDotString(columnPath));}
0
public static BooleanColumn booleanColumn(String columnPath)
{    return new BooleanColumn(ColumnPath.fromDotString(columnPath));}
0
public static BinaryColumn binaryColumn(String columnPath)
{    return new BinaryColumn(ColumnPath.fromDotString(columnPath));}
0
public static Eq<T> eq(C column, T value)
{    return new Eq<T>(column, value);}
0
public static NotEq<T> notEq(C column, T value)
{    return new NotEq<T>(column, value);}
0
public static Lt<T> lt(C column, T value)
{    return new Lt<T>(column, value);}
0
public static LtEq<T> ltEq(C column, T value)
{    return new LtEq<T>(column, value);}
0
public static Gt<T> gt(C column, T value)
{    return new Gt<T>(column, value);}
0
public static GtEq<T> gtEq(C column, T value)
{    return new GtEq<T>(column, value);}
0
public static UserDefined<T, U> userDefined(Column<T> column, Class<U> clazz)
{    return new UserDefinedByClass<T, U>(column, clazz);}
0
public static UserDefined<T, U> userDefined(Column<T> column, U udp)
{    return new UserDefinedByInstance<T, U>(column, udp);}
0
public static FilterPredicate and(FilterPredicate left, FilterPredicate right)
{    return new And(left, right);}
0
public static FilterPredicate or(FilterPredicate left, FilterPredicate right)
{    return new Or(left, right);}
0
public static FilterPredicate not(FilterPredicate predicate)
{    return new Not(predicate);}
0
public static FilterPredicate rewrite(FilterPredicate pred)
{    checkNotNull(pred, "pred");    return pred.accept(INSTANCE);}
0
public FilterPredicate visit(Eq<T> eq)
{    return eq;}
0
public FilterPredicate visit(NotEq<T> notEq)
{    return notEq;}
0
public FilterPredicate visit(Lt<T> lt)
{    return lt;}
0
public FilterPredicate visit(LtEq<T> ltEq)
{    return ltEq;}
0
public FilterPredicate visit(Gt<T> gt)
{    return gt;}
0
public FilterPredicate visit(GtEq<T> gtEq)
{    return gtEq;}
0
public FilterPredicate visit(And and)
{    return and(and.getLeft().accept(this), and.getRight().accept(this));}
0
public FilterPredicate visit(Or or)
{    return or(or.getLeft().accept(this), or.getRight().accept(this));}
0
public FilterPredicate visit(Not not)
{    return LogicalInverter.invert(not.getPredicate().accept(this));}
0
public FilterPredicate visit(UserDefined<T, U> udp)
{    return udp;}
0
public FilterPredicate visit(LogicalNotUserDefined<T, U> udp)
{    return udp;}
0
public static FilterPredicate invert(FilterPredicate pred)
{    checkNotNull(pred, "pred");    return pred.accept(INSTANCE);}
0
public FilterPredicate visit(Eq<T> eq)
{    return new NotEq<T>(eq.getColumn(), eq.getValue());}
0
public FilterPredicate visit(NotEq<T> notEq)
{    return new Eq<T>(notEq.getColumn(), notEq.getValue());}
0
public FilterPredicate visit(Lt<T> lt)
{    return new GtEq<T>(lt.getColumn(), lt.getValue());}
0
public FilterPredicate visit(LtEq<T> ltEq)
{    return new Gt<T>(ltEq.getColumn(), ltEq.getValue());}
0
public FilterPredicate visit(Gt<T> gt)
{    return new LtEq<T>(gt.getColumn(), gt.getValue());}
0
public FilterPredicate visit(GtEq<T> gtEq)
{    return new Lt<T>(gtEq.getColumn(), gtEq.getValue());}
0
public FilterPredicate visit(And and)
{    return new Or(and.getLeft().accept(this), and.getRight().accept(this));}
0
public FilterPredicate visit(Or or)
{    return new And(or.getLeft().accept(this), or.getRight().accept(this));}
0
public FilterPredicate visit(Not not)
{    return not.getPredicate();}
0
public FilterPredicate visit(UserDefined<T, U> udp)
{    return new LogicalNotUserDefined<T, U>(udp);}
0
public FilterPredicate visit(LogicalNotUserDefined<T, U> udp)
{    return udp.getUserDefined();}
0
public Class<T> getColumnType()
{    return columnType;}
0
public ColumnPath getColumnPath()
{    return columnPath;}
0
public String toString()
{    return "column(" + columnPath.toDotString() + ")";}
0
public boolean equals(Object o)
{    if (this == o)        return true;    if (o == null || getClass() != o.getClass())        return false;    Column column = (Column) o;    if (!columnType.equals(column.columnType))        return false;    if (!columnPath.equals(column.columnPath))        return false;    return true;}
0
public int hashCode()
{    int result = columnPath.hashCode();    result = 31 * result + columnType.hashCode();    return result;}
0
public Column<T> getColumn()
{    return column;}
0
public T getValue()
{    return value;}
0
public String toString()
{    return toString;}
0
public boolean equals(Object o)
{    if (this == o)        return true;    if (o == null || getClass() != o.getClass())        return false;    ColumnFilterPredicate that = (ColumnFilterPredicate) o;    if (!column.equals(that.column))        return false;    if (value != null ? !value.equals(that.value) : that.value != null)        return false;    return true;}
0
public int hashCode()
{    int result = column.hashCode();    result = 31 * result + (value != null ? value.hashCode() : 0);    result = 31 * result + getClass().hashCode();    return result;}
0
public R accept(Visitor<R> visitor)
{    return visitor.visit(this);}
0
public R accept(Visitor<R> visitor)
{    return visitor.visit(this);}
0
public R accept(Visitor<R> visitor)
{    return visitor.visit(this);}
0
public R accept(Visitor<R> visitor)
{    return visitor.visit(this);}
0
public R accept(Visitor<R> visitor)
{    return visitor.visit(this);}
0
public R accept(Visitor<R> visitor)
{    return visitor.visit(this);}
0
public FilterPredicate getLeft()
{    return left;}
0
public FilterPredicate getRight()
{    return right;}
0
public String toString()
{    return toString;}
0
public boolean equals(Object o)
{    if (this == o)        return true;    if (o == null || getClass() != o.getClass())        return false;    BinaryLogicalFilterPredicate that = (BinaryLogicalFilterPredicate) o;    if (!left.equals(that.left))        return false;    if (!right.equals(that.right))        return false;    return true;}
0
public int hashCode()
{    int result = left.hashCode();    result = 31 * result + right.hashCode();    result = 31 * result + getClass().hashCode();    return result;}
0
public R accept(Visitor<R> visitor)
{    return visitor.visit(this);}
0
public R accept(Visitor<R> visitor)
{    return visitor.visit(this);}
0
public FilterPredicate getPredicate()
{    return predicate;}
0
public String toString()
{    return toString;}
0
public R accept(Visitor<R> visitor)
{    return visitor.visit(this);}
0
public boolean equals(Object o)
{    if (this == o)        return true;    if (o == null || getClass() != o.getClass())        return false;    Not not = (Not) o;    return predicate.equals(not.predicate);}
0
public int hashCode()
{    return predicate.hashCode() * 31 + getClass().hashCode();}
0
public Column<T> getColumn()
{    return column;}
0
public R accept(Visitor<R> visitor)
{    return visitor.visit(this);}
0
public Class<U> getUserDefinedPredicateClass()
{    return udpClass;}
0
public U getUserDefinedPredicate()
{    try {        return udpClass.newInstance();    } catch (InstantiationException e) {        throw new RuntimeException(String.format(INSTANTIATION_ERROR_MESSAGE, udpClass), e);    } catch (IllegalAccessException e) {        throw new RuntimeException(String.format(INSTANTIATION_ERROR_MESSAGE, udpClass), e);    }}
0
public String toString()
{    return toString;}
0
public boolean equals(Object o)
{    if (this == o)        return true;    if (o == null || getClass() != o.getClass())        return false;    UserDefinedByClass that = (UserDefinedByClass) o;    if (!column.equals(that.column))        return false;    if (!udpClass.equals(that.udpClass))        return false;    return true;}
0
public int hashCode()
{    int result = column.hashCode();    result = 31 * result + udpClass.hashCode();    result = result * 31 + getClass().hashCode();    return result;}
0
public U getUserDefinedPredicate()
{    return udpInstance;}
0
public String toString()
{    return toString;}
0
public boolean equals(Object o)
{    if (this == o)        return true;    if (o == null || getClass() != o.getClass())        return false;    UserDefinedByInstance that = (UserDefinedByInstance) o;    if (!column.equals(that.column))        return false;    if (!udpInstance.equals(that.udpInstance))        return false;    return true;}
0
public int hashCode()
{    int result = column.hashCode();    result = 31 * result + udpInstance.hashCode();    result = result * 31 + getClass().hashCode();    return result;}
0
public UserDefined<T, U> getUserDefined()
{    return udp;}
0
public R accept(Visitor<R> visitor)
{    return visitor.visit(this);}
0
public String toString()
{    return toString;}
0
public boolean equals(Object o)
{    if (this == o)        return true;    if (o == null || getClass() != o.getClass())        return false;    LogicalNotUserDefined that = (LogicalNotUserDefined) o;    if (!udp.equals(that.udp))        return false;    return true;}
0
public int hashCode()
{    int result = udp.hashCode();    result = result * 31 + getClass().hashCode();    return result;}
0
public static Class<?> get(Class<?> c)
{    checkArgument(c.isPrimitive(), "Class " + c + " is not primitive!");    return primitiveToBoxed.get(c);}
0
public static void validate(FilterPredicate predicate, MessageType schema)
{    checkNotNull(predicate, "predicate");    checkNotNull(schema, "schema");    predicate.accept(new SchemaCompatibilityValidator(schema));}
0
public Void visit(Eq<T> pred)
{    validateColumnFilterPredicate(pred);    return null;}
0
public Void visit(NotEq<T> pred)
{    validateColumnFilterPredicate(pred);    return null;}
0
public Void visit(Lt<T> pred)
{    validateColumnFilterPredicate(pred);    return null;}
0
public Void visit(LtEq<T> pred)
{    validateColumnFilterPredicate(pred);    return null;}
0
public Void visit(Gt<T> pred)
{    validateColumnFilterPredicate(pred);    return null;}
0
public Void visit(GtEq<T> pred)
{    validateColumnFilterPredicate(pred);    return null;}
0
public Void visit(And and)
{    and.getLeft().accept(this);    and.getRight().accept(this);    return null;}
0
public Void visit(Or or)
{    or.getLeft().accept(this);    or.getRight().accept(this);    return null;}
0
public Void visit(Not not)
{    not.getPredicate().accept(this);    return null;}
0
public Void visit(UserDefined<T, U> udp)
{    validateColumn(udp.getColumn());    return null;}
0
public Void visit(LogicalNotUserDefined<T, U> udp)
{    return udp.getUserDefined().accept(this);}
0
private void validateColumnFilterPredicate(ColumnFilterPredicate<T> pred)
{    validateColumn(pred.getColumn());}
0
private void validateColumn(Column<T> column)
{    ColumnPath path = column.getColumnPath();    Class<?> alreadySeen = columnTypesEncountered.get(path);    if (alreadySeen != null && !alreadySeen.equals(column.getColumnType())) {        throw new IllegalArgumentException("Column: " + path.toDotString() + " was provided with different types in the same predicate." + " Found both: (" + alreadySeen + ", " + column.getColumnType() + ")");    }    if (alreadySeen == null) {        columnTypesEncountered.put(path, column.getColumnType());    }    ColumnDescriptor descriptor = getColumnDescriptor(path);    if (descriptor == null) {                return;    }    if (descriptor.getMaxRepetitionLevel() > 0) {        throw new IllegalArgumentException("FilterPredicates do not currently support repeated columns. " + "Column " + path.toDotString() + " is repeated.");    }    ValidTypeMap.assertTypeValid(column, descriptor.getType());}
0
private ColumnDescriptor getColumnDescriptor(ColumnPath columnPath)
{    return columnsAccordingToSchema.get(columnPath);}
0
public T getMin()
{    return min;}
0
public T getMax()
{    return max;}
0
public Comparator<T> getComparator()
{    return comparator;}
0
public boolean acceptsNullValue()
{    try {        return keep(null);    } catch (NullPointerException e) {                return false;    }}
0
private static void add(Class<?> c, PrimitiveTypeName p)
{    Set<PrimitiveTypeName> descriptors = classToParquetType.get(c);    if (descriptors == null) {        descriptors = new HashSet<PrimitiveTypeName>();        classToParquetType.put(c, descriptors);    }    descriptors.add(p);    Set<Class<?>> classes = parquetTypeToClass.get(p);    if (classes == null) {        classes = new HashSet<Class<?>>();        parquetTypeToClass.put(p, classes);    }    classes.add(c);}
0
public static void assertTypeValid(Column<T> foundColumn, PrimitiveTypeName primitiveType)
{    Class<T> foundColumnType = foundColumn.getColumnType();    ColumnPath columnPath = foundColumn.getColumnPath();    Set<PrimitiveTypeName> validTypeDescriptors = classToParquetType.get(foundColumnType);    if (validTypeDescriptors == null) {        StringBuilder message = new StringBuilder();        message.append("Column ").append(columnPath.toDotString()).append(" was declared as type: ").append(foundColumnType.getName()).append(" which is not supported in FilterPredicates.");        Set<Class<?>> supportedTypes = parquetTypeToClass.get(primitiveType);        if (supportedTypes != null) {            message.append(" Supported types for this column are: ").append(supportedTypes);        } else {            message.append(" There are no supported types for columns of " + primitiveType);        }        throw new IllegalArgumentException(message.toString());    }    if (!validTypeDescriptors.contains(primitiveType)) {        StringBuilder message = new StringBuilder();        message.append("FilterPredicate column: ").append(columnPath.toDotString()).append("'s declared type (").append(foundColumnType.getName()).append(") does not match the schema found in file metadata. Column ").append(columnPath.toDotString()).append(" is of type: ").append(primitiveType).append("\nValid types for this column are: ").append(parquetTypeToClass.get(primitiveType));        throw new IllegalArgumentException(message.toString());    }}
0
public Converter getConverter(int fieldIndex)
{        Converter delegateConverter = checkNotNull(delegate.getConverter(fieldIndex), "delegate converter");            List<Integer> newIndexFieldPath = new ArrayList<Integer>(indexFieldPath.size() + 1);    newIndexFieldPath.addAll(indexFieldPath);    newIndexFieldPath.add(fieldIndex);    if (delegateConverter.isPrimitive()) {        PrimitiveColumnIO columnIO = getColumnIO(newIndexFieldPath);        ColumnPath columnPath = ColumnPath.get(columnIO.getColumnDescriptor().getPath());        ValueInspector[] valueInspectors = getValueInspectors(columnPath);        return new FilteringPrimitiveConverter(delegateConverter.asPrimitiveConverter(), valueInspectors);    } else {        return new FilteringGroupConverter(delegateConverter.asGroupConverter(), newIndexFieldPath, valueInspectorsByColumn, columnIOsByIndexFieldPath);    }}
0
private PrimitiveColumnIO getColumnIO(List<Integer> indexFieldPath)
{    PrimitiveColumnIO found = columnIOsByIndexFieldPath.get(indexFieldPath);    checkArgument(found != null, "Did not find PrimitiveColumnIO for index field path" + indexFieldPath);    return found;}
0
private ValueInspector[] getValueInspectors(ColumnPath columnPath)
{    List<ValueInspector> inspectorsList = valueInspectorsByColumn.get(columnPath);    if (inspectorsList == null) {        return new ValueInspector[] {};    } else {        return inspectorsList.toArray(new ValueInspector[inspectorsList.size()]);    }}
0
public void start()
{    delegate.start();}
0
public void end()
{    delegate.end();}
0
public boolean hasDictionarySupport()
{    return false;}
0
public void setDictionary(Dictionary dictionary)
{    throw new UnsupportedOperationException("FilteringPrimitiveConverter doesn't have dictionary support");}
0
public void addValueFromDictionary(int dictionaryId)
{    throw new UnsupportedOperationException("FilteringPrimitiveConverter doesn't have dictionary support");}
0
public void addBinary(Binary value)
{    for (ValueInspector valueInspector : valueInspectors) {        valueInspector.update(value);    }    delegate.addBinary(value);}
0
public void addBoolean(boolean value)
{    for (ValueInspector valueInspector : valueInspectors) {        valueInspector.update(value);    }    delegate.addBoolean(value);}
0
public void addDouble(double value)
{    for (ValueInspector valueInspector : valueInspectors) {        valueInspector.update(value);    }    delegate.addDouble(value);}
0
public void addFloat(float value)
{    for (ValueInspector valueInspector : valueInspectors) {        valueInspector.update(value);    }    delegate.addFloat(value);}
0
public void addInt(int value)
{    for (ValueInspector valueInspector : valueInspectors) {        valueInspector.update(value);    }    delegate.addInt(value);}
0
public void addLong(long value)
{    for (ValueInspector valueInspector : valueInspectors) {        valueInspector.update(value);    }    delegate.addLong(value);}
0
public static List<Integer> getIndexFieldPathList(PrimitiveColumnIO c)
{    return intArrayToList(c.getIndexFieldPath());}
0
public static List<Integer> intArrayToList(int[] arr)
{    List<Integer> list = new ArrayList<Integer>(arr.length);    for (int i : arr) {        list.add(i);    }    return list;}
0
public T getCurrentRecord()
{        boolean keep = IncrementallyUpdatedFilterPredicateEvaluator.evaluate(filterPredicate);        IncrementallyUpdatedFilterPredicateResetter.reset(filterPredicate);    if (keep) {        return delegate.getCurrentRecord();    } else {                return null;    }}
0
public void skipCurrentRecord()
{    delegate.skipCurrentRecord();}
0
public GroupConverter getRootConverter()
{    return rootConverter;}
0
public void updateNull()
{    throw new UnsupportedOperationException();}
0
public void update(int value)
{    throw new UnsupportedOperationException();}
0
public void update(long value)
{    throw new UnsupportedOperationException();}
0
public void update(double value)
{    throw new UnsupportedOperationException();}
0
public void update(float value)
{    throw new UnsupportedOperationException();}
0
public void update(boolean value)
{    throw new UnsupportedOperationException();}
0
public void update(Binary value)
{    throw new UnsupportedOperationException();}
0
public final void reset()
{    isKnown = false;    result = false;}
0
protected final void setResult(boolean result)
{    if (isKnown) {        throw new IllegalStateException("setResult() called on a ValueInspector whose result is already known!" + " Did you forget to call reset()?");    }    this.result = result;    this.isKnown = true;}
0
public final boolean getResult()
{    if (!isKnown) {        throw new IllegalStateException("getResult() called on a ValueInspector whose result is not yet known!");    }    return result;}
0
public final boolean isKnown()
{    return isKnown;}
0
public boolean accept(Visitor visitor)
{    return visitor.visit(this);}
0
public final IncrementallyUpdatedFilterPredicate getLeft()
{    return left;}
0
public final IncrementallyUpdatedFilterPredicate getRight()
{    return right;}
0
public boolean accept(Visitor visitor)
{    return visitor.visit(this);}
0
public boolean accept(Visitor visitor)
{    return visitor.visit(this);}
0
public final IncrementallyUpdatedFilterPredicate build(FilterPredicate pred)
{    checkArgument(!built, "This builder has already been used");    IncrementallyUpdatedFilterPredicate incremental = pred.accept(this);    built = true;    return incremental;}
0
protected final void addValueInspector(ColumnPath columnPath, ValueInspector valueInspector)
{    List<ValueInspector> valueInspectors = valueInspectorsByColumn.get(columnPath);    if (valueInspectors == null) {        valueInspectors = new ArrayList<ValueInspector>();        valueInspectorsByColumn.put(columnPath, valueInspectors);    }    valueInspectors.add(valueInspector);}
0
public Map<ColumnPath, List<ValueInspector>> getValueInspectorsByColumn()
{    return valueInspectorsByColumn;}
0
protected final PrimitiveComparator<T> getComparator(ColumnPath path)
{    return (PrimitiveComparator<T>) comparatorsByColumn.get(path);}
0
public final IncrementallyUpdatedFilterPredicate visit(And and)
{    return new IncrementallyUpdatedFilterPredicate.And(and.getLeft().accept(this), and.getRight().accept(this));}
0
public final IncrementallyUpdatedFilterPredicate visit(Or or)
{    return new IncrementallyUpdatedFilterPredicate.Or(or.getLeft().accept(this), or.getRight().accept(this));}
0
public final IncrementallyUpdatedFilterPredicate visit(Not not)
{    throw new IllegalArgumentException("This predicate contains a not! Did you forget to run this predicate through LogicalInverseRewriter? " + not);}
0
public static boolean evaluate(IncrementallyUpdatedFilterPredicate pred)
{    checkNotNull(pred, "pred");    return pred.accept(INSTANCE);}
0
public boolean visit(ValueInspector p)
{    if (!p.isKnown()) {        p.updateNull();    }    return p.getResult();}
0
public boolean visit(And and)
{    return and.getLeft().accept(this) && and.getRight().accept(this);}
0
public boolean visit(Or or)
{    return or.getLeft().accept(this) || or.getRight().accept(this);}
0
public static void reset(IncrementallyUpdatedFilterPredicate pred)
{    checkNotNull(pred, "pred");    pred.accept(INSTANCE);}
0
public boolean visit(ValueInspector p)
{    p.reset();    return false;}
0
public boolean visit(And and)
{    and.getLeft().accept(this);    and.getRight().accept(this);    return false;}
0
public boolean visit(Or or)
{    or.getLeft().accept(this);    or.getRight().accept(this);    return false;}
0
 ByteBuffer getMinValueAsBytes(int pageIndex)
{    return convert(minValues[pageIndex]);}
0
 ByteBuffer getMaxValueAsBytes(int pageIndex)
{    return convert(maxValues[pageIndex]);}
0
 String getMinValueAsString(int pageIndex)
{    return stringifier.stringify(minValues[pageIndex]);}
0
 String getMaxValueAsString(int pageIndex)
{    return stringifier.stringify(maxValues[pageIndex]);}
0
 Statistics<T> createStats(int arrayIndex)
{    return (Statistics<T>) new Statistics<Binary>(minValues[arrayIndex], maxValues[arrayIndex], comparator);}
0
 ValueComparator createValueComparator(Object value)
{    final Binary v = (Binary) value;    return new ValueComparator() {        @Override        int compareValueToMin(int arrayIndex) {            return comparator.compare(v, minValues[arrayIndex]);        }        @Override        int compareValueToMax(int arrayIndex) {            return comparator.compare(v, maxValues[arrayIndex]);        }    };}
0
 int compareValueToMin(int arrayIndex)
{    return comparator.compare(v, minValues[arrayIndex]);}
0
 int compareValueToMax(int arrayIndex)
{    return comparator.compare(v, maxValues[arrayIndex]);}
0
private static Binary convert(ByteBuffer buffer)
{    return Binary.fromReusedByteBuffer(buffer);}
0
private static ByteBuffer convert(Binary value)
{    return value.toByteBuffer();}
0
 void addMinMaxFromBytes(ByteBuffer min, ByteBuffer max)
{    minValues.add(convert(min));    maxValues.add(convert(max));}
0
 void addMinMax(Object min, Object max)
{    minValues.add(min == null ? null : truncator.truncateMin((Binary) min, truncateLength));    maxValues.add(max == null ? null : truncator.truncateMax((Binary) max, truncateLength));}
0
 ColumnIndexBase<Binary> createColumnIndex(PrimitiveType type)
{    BinaryColumnIndex columnIndex = new BinaryColumnIndex(type);    columnIndex.minValues = minValues.toArray(new Binary[minValues.size()]);    columnIndex.maxValues = maxValues.toArray(new Binary[maxValues.size()]);    return columnIndex;}
0
 void clearMinMax()
{    minValues.clear();    maxValues.clear();}
0
 int compareMinValues(PrimitiveComparator<Binary> comparator, int index1, int index2)
{    return comparator.compare(minValues.get(index1), minValues.get(index2));}
0
 int compareMaxValues(PrimitiveComparator<Binary> comparator, int index1, int index2)
{    return comparator.compare(maxValues.get(index1), maxValues.get(index2));}
0
 int sizeOf(Object value)
{    return ((Binary) value).length();}
0
 Validity checkValidity(ByteBuffer buffer)
{    int pos = buffer.position();    CoderResult result = CoderResult.OVERFLOW;    while (result.isOverflow()) {        dummyBuffer.clear();        result = decoder.decode(buffer, dummyBuffer, true);    }    buffer.position(pos);    if (result.isUnderflow()) {        return Validity.VALID;    } else if (result.isMalformed()) {        return Validity.MALFORMED;    } else {        return Validity.UNMAPPABLE;    }}
0
 Binary truncateMin(Binary minValue, int length)
{    return minValue;}
0
 Binary truncateMax(Binary maxValue, int length)
{    return maxValue;}
0
 Binary truncateMin(Binary minValue, int length)
{    if (minValue.length() <= length) {        return minValue;    }    ByteBuffer buffer = minValue.toByteBuffer();    byte[] array;    if (validator.checkValidity(buffer) == Validity.VALID) {        array = truncateUtf8(buffer, length);    } else {        array = truncate(buffer, length);    }    return array == null ? minValue : Binary.fromConstantByteArray(array);}
0
 Binary truncateMax(Binary maxValue, int length)
{    if (maxValue.length() <= length) {        return maxValue;    }    byte[] array;    ByteBuffer buffer = maxValue.toByteBuffer();    if (validator.checkValidity(buffer) == Validity.VALID) {        array = incrementUtf8(truncateUtf8(buffer, length));    } else {        array = increment(truncate(buffer, length));    }    return array == null ? maxValue : Binary.fromConstantByteArray(array);}
0
private byte[] truncate(ByteBuffer buffer, int length)
{    assert length < buffer.remaining();    byte[] array = new byte[length];    buffer.get(array);    return array;}
0
private byte[] increment(byte[] array)
{    for (int i = array.length - 1; i >= 0; --i) {        byte elem = array[i];        ++elem;        array[i] = elem;        if (elem != 0) {                        return array;        }    }    return null;}
0
private byte[] truncateUtf8(ByteBuffer buffer, int length)
{    assert length < buffer.remaining();    ByteBuffer newBuffer = buffer.slice();    newBuffer.limit(newBuffer.position() + length);    while (validator.checkValidity(newBuffer) != Validity.VALID) {        newBuffer.limit(newBuffer.limit() - 1);        if (newBuffer.remaining() == 0) {            return null;        }    }    byte[] array = new byte[newBuffer.remaining()];    newBuffer.get(array);    return array;}
0
private byte[] incrementUtf8(byte[] array)
{    if (array == null) {        return null;    }    ByteBuffer buffer = ByteBuffer.wrap(array);    for (int i = array.length - 1; i >= 0; --i) {        byte prev = array[i];        byte inc = prev;        while (++inc != 0) {                        array[i] = inc;            switch(validator.checkValidity(buffer)) {                case VALID:                    return array;                case UNMAPPABLE:                                        continue;                case MALFORMED:                                        break;            }                        break;        }        array[i] = prev;    }        return null;}
0
 static BinaryTruncator getTruncator(PrimitiveType type)
{    if (type == null) {        return NO_OP_TRUNCATOR;    }    switch(type.getPrimitiveTypeName()) {        case INT96:            return NO_OP_TRUNCATOR;        case BINARY:        case FIXED_LEN_BYTE_ARRAY:            LogicalTypeAnnotation logicalTypeAnnotation = type.getLogicalTypeAnnotation();            if (logicalTypeAnnotation == null) {                return DEFAULT_UTF8_TRUNCATOR;            }            return logicalTypeAnnotation.accept(new LogicalTypeAnnotation.LogicalTypeAnnotationVisitor<BinaryTruncator>() {                @Override                public Optional<BinaryTruncator> visit(LogicalTypeAnnotation.StringLogicalTypeAnnotation stringLogicalType) {                    return Optional.of(DEFAULT_UTF8_TRUNCATOR);                }                @Override                public Optional<BinaryTruncator> visit(LogicalTypeAnnotation.EnumLogicalTypeAnnotation enumLogicalType) {                    return Optional.of(DEFAULT_UTF8_TRUNCATOR);                }                @Override                public Optional<BinaryTruncator> visit(LogicalTypeAnnotation.JsonLogicalTypeAnnotation jsonLogicalType) {                    return Optional.of(DEFAULT_UTF8_TRUNCATOR);                }                @Override                public Optional<BinaryTruncator> visit(LogicalTypeAnnotation.BsonLogicalTypeAnnotation bsonLogicalType) {                    return Optional.of(DEFAULT_UTF8_TRUNCATOR);                }            }).orElse(NO_OP_TRUNCATOR);        default:            throw new IllegalArgumentException("No truncator is available for the type: " + type);    }}
0
public Optional<BinaryTruncator> visit(LogicalTypeAnnotation.StringLogicalTypeAnnotation stringLogicalType)
{    return Optional.of(DEFAULT_UTF8_TRUNCATOR);}
0
public Optional<BinaryTruncator> visit(LogicalTypeAnnotation.EnumLogicalTypeAnnotation enumLogicalType)
{    return Optional.of(DEFAULT_UTF8_TRUNCATOR);}
0
public Optional<BinaryTruncator> visit(LogicalTypeAnnotation.JsonLogicalTypeAnnotation jsonLogicalType)
{    return Optional.of(DEFAULT_UTF8_TRUNCATOR);}
0
public Optional<BinaryTruncator> visit(LogicalTypeAnnotation.BsonLogicalTypeAnnotation bsonLogicalType)
{    return Optional.of(DEFAULT_UTF8_TRUNCATOR);}
0
 ByteBuffer getMinValueAsBytes(int pageIndex)
{    return convert(minValues[pageIndex]);}
0
 ByteBuffer getMaxValueAsBytes(int pageIndex)
{    return convert(maxValues[pageIndex]);}
0
 String getMinValueAsString(int pageIndex)
{    return stringifier.stringify(minValues[pageIndex]);}
0
 String getMaxValueAsString(int pageIndex)
{    return stringifier.stringify(maxValues[pageIndex]);}
0
 Statistics<T> createStats(int arrayIndex)
{    return (Statistics<T>) new Statistics<Boolean>(minValues[arrayIndex], maxValues[arrayIndex], comparator);}
0
 ValueComparator createValueComparator(Object value)
{    final boolean v = (boolean) value;    return new ValueComparator() {        @Override        int compareValueToMin(int arrayIndex) {            return comparator.compare(v, minValues[arrayIndex]);        }        @Override        int compareValueToMax(int arrayIndex) {            return comparator.compare(v, maxValues[arrayIndex]);        }    };}
0
 int compareValueToMin(int arrayIndex)
{    return comparator.compare(v, minValues[arrayIndex]);}
0
 int compareValueToMax(int arrayIndex)
{    return comparator.compare(v, maxValues[arrayIndex]);}
0
private static boolean convert(ByteBuffer buffer)
{    return buffer.get(0) != 0;}
0
private static ByteBuffer convert(boolean value)
{    return ByteBuffer.allocate(1).put(0, value ? (byte) 1 : 0);}
0
 void addMinMaxFromBytes(ByteBuffer min, ByteBuffer max)
{    minValues.add(convert(min));    maxValues.add(convert(max));}
0
 void addMinMax(Object min, Object max)
{    minValues.add((boolean) min);    maxValues.add((boolean) max);}
0
 ColumnIndexBase<Boolean> createColumnIndex(PrimitiveType type)
{    BooleanColumnIndex columnIndex = new BooleanColumnIndex(type);    columnIndex.minValues = minValues.toBooleanArray();    columnIndex.maxValues = maxValues.toBooleanArray();    return columnIndex;}
0
 void clearMinMax()
{    minValues.clear();    maxValues.clear();}
0
 int compareMinValues(PrimitiveComparator<Binary> comparator, int index1, int index2)
{    return comparator.compare(minValues.get(index1), minValues.get(index2));}
0
 int compareMaxValues(PrimitiveComparator<Binary> comparator, int index1, int index2)
{    return comparator.compare(maxValues.get(index1), maxValues.get(index2));}
0
 int sizeOf(Object value)
{    return 1;}
0
private static int floorMid(int left, int right)
{        return left + ((right - left) / 2);}
0
private static int ceilingMid(int left, int right)
{        return left + ((right - left + 1) / 2);}
0
 PrimitiveIterator.OfInt eq(ColumnIndexBase<?>.ValueComparator comparator)
{    return IndexIterator.filterTranslate(comparator.arrayLength(), arrayIndex -> comparator.compareValueToMin(arrayIndex) >= 0 && comparator.compareValueToMax(arrayIndex) <= 0, comparator::translate);}
0
 PrimitiveIterator.OfInt gt(ColumnIndexBase<?>.ValueComparator comparator)
{    return IndexIterator.filterTranslate(comparator.arrayLength(), arrayIndex -> comparator.compareValueToMax(arrayIndex) < 0, comparator::translate);}
0
 PrimitiveIterator.OfInt gtEq(ColumnIndexBase<?>.ValueComparator comparator)
{    return IndexIterator.filterTranslate(comparator.arrayLength(), arrayIndex -> comparator.compareValueToMax(arrayIndex) <= 0, comparator::translate);}
0
 PrimitiveIterator.OfInt lt(ColumnIndexBase<?>.ValueComparator comparator)
{    return IndexIterator.filterTranslate(comparator.arrayLength(), arrayIndex -> comparator.compareValueToMin(arrayIndex) > 0, comparator::translate);}
0
 PrimitiveIterator.OfInt ltEq(ColumnIndexBase<?>.ValueComparator comparator)
{    return IndexIterator.filterTranslate(comparator.arrayLength(), arrayIndex -> comparator.compareValueToMin(arrayIndex) >= 0, comparator::translate);}
0
 PrimitiveIterator.OfInt notEq(ColumnIndexBase<?>.ValueComparator comparator)
{    return IndexIterator.filterTranslate(comparator.arrayLength(), arrayIndex -> comparator.compareValueToMin(arrayIndex) != 0 || comparator.compareValueToMax(arrayIndex) != 0, comparator::translate);}
0
 OfInt eq(ColumnIndexBase<?>.ValueComparator comparator)
{    Bounds bounds = findBounds(comparator);    if (bounds == null) {        return IndexIterator.EMPTY;    }    return IndexIterator.rangeTranslate(bounds.lower, bounds.upper, comparator::translate);}
0
 OfInt gt(ColumnIndexBase<?>.ValueComparator comparator)
{    int length = comparator.arrayLength();    int left = 0;    int right = length;    do {        int i = floorMid(left, right);        if (comparator.compareValueToMax(i) >= 0) {            left = i + 1;        } else {            right = i;        }    } while (left < right);    return IndexIterator.rangeTranslate(right, length - 1, comparator::translate);}
0
 OfInt gtEq(ColumnIndexBase<?>.ValueComparator comparator)
{    int length = comparator.arrayLength();    int left = 0;    int right = length;    do {        int i = floorMid(left, right);        if (comparator.compareValueToMax(i) > 0) {            left = i + 1;        } else {            right = i;        }    } while (left < right);    return IndexIterator.rangeTranslate(right, length - 1, comparator::translate);}
0
 OfInt lt(ColumnIndexBase<?>.ValueComparator comparator)
{    int length = comparator.arrayLength();    int left = -1;    int right = length - 1;    do {        int i = ceilingMid(left, right);        if (comparator.compareValueToMin(i) <= 0) {            right = i - 1;        } else {            left = i;        }    } while (left < right);    return IndexIterator.rangeTranslate(0, left, comparator::translate);}
0
 OfInt ltEq(ColumnIndexBase<?>.ValueComparator comparator)
{    int length = comparator.arrayLength();    int left = -1;    int right = length - 1;    do {        int i = ceilingMid(left, right);        if (comparator.compareValueToMin(i) < 0) {            right = i - 1;        } else {            left = i;        }    } while (left < right);    return IndexIterator.rangeTranslate(0, left, comparator::translate);}
0
 OfInt notEq(ColumnIndexBase<?>.ValueComparator comparator)
{    Bounds bounds = findBounds(comparator);    int length = comparator.arrayLength();    if (bounds == null) {        return IndexIterator.all(comparator);    }    return IndexIterator.filterTranslate(length, i -> i < bounds.lower || i > bounds.upper || comparator.compareValueToMin(i) != 0 || comparator.compareValueToMax(i) != 0, comparator::translate);}
0
private Bounds findBounds(ColumnIndexBase<?>.ValueComparator comparator)
{    int length = comparator.arrayLength();    int lowerLeft = 0;    int upperLeft = 0;    int lowerRight = length - 1;    int upperRight = length - 1;    do {        if (lowerLeft > lowerRight) {            return null;        }        int i = floorMid(lowerLeft, lowerRight);        if (comparator.compareValueToMin(i) < 0) {            lowerRight = upperRight = i - 1;        } else if (comparator.compareValueToMax(i) > 0) {            lowerLeft = upperLeft = i + 1;        } else {            lowerRight = upperLeft = i;        }    } while (lowerLeft != lowerRight);    do {        if (upperLeft > upperRight) {            return null;        }        int i = ceilingMid(upperLeft, upperRight);        if (comparator.compareValueToMin(i) < 0) {            upperRight = i - 1;        } else if (comparator.compareValueToMax(i) > 0) {            upperLeft = i + 1;        } else {            upperLeft = i;        }    } while (upperLeft != upperRight);    return new Bounds(lowerLeft, upperRight);}
0
 OfInt eq(ColumnIndexBase<?>.ValueComparator comparator)
{    Bounds bounds = findBounds(comparator);    if (bounds == null) {        return IndexIterator.EMPTY;    }    return IndexIterator.rangeTranslate(bounds.lower, bounds.upper, comparator::translate);}
0
 OfInt gt(ColumnIndexBase<?>.ValueComparator comparator)
{    int length = comparator.arrayLength();    int left = -1;    int right = length - 1;    do {        int i = ceilingMid(left, right);        if (comparator.compareValueToMax(i) >= 0) {            right = i - 1;        } else {            left = i;        }    } while (left < right);    return IndexIterator.rangeTranslate(0, left, comparator::translate);}
0
 OfInt gtEq(ColumnIndexBase<?>.ValueComparator comparator)
{    int length = comparator.arrayLength();    int left = -1;    int right = length - 1;    do {        int i = ceilingMid(left, right);        if (comparator.compareValueToMax(i) > 0) {            right = i - 1;        } else {            left = i;        }    } while (left < right);    return IndexIterator.rangeTranslate(0, left, comparator::translate);}
0
 OfInt lt(ColumnIndexBase<?>.ValueComparator comparator)
{    int length = comparator.arrayLength();    int left = 0;    int right = length;    do {        int i = floorMid(left, right);        if (comparator.compareValueToMin(i) <= 0) {            left = i + 1;        } else {            right = i;        }    } while (left < right);    return IndexIterator.rangeTranslate(right, length - 1, comparator::translate);}
0
 OfInt ltEq(ColumnIndexBase<?>.ValueComparator comparator)
{    int length = comparator.arrayLength();    int left = 0;    int right = length;    do {        int i = floorMid(left, right);        if (comparator.compareValueToMin(i) < 0) {            left = i + 1;        } else {            right = i;        }    } while (left < right);    return IndexIterator.rangeTranslate(right, length - 1, comparator::translate);}
0
 OfInt notEq(ColumnIndexBase<?>.ValueComparator comparator)
{    Bounds bounds = findBounds(comparator);    int length = comparator.arrayLength();    if (bounds == null) {        return IndexIterator.all(comparator);    }    return IndexIterator.filterTranslate(length, i -> i < bounds.lower || i > bounds.upper || comparator.compareValueToMin(i) != 0 || comparator.compareValueToMax(i) != 0, comparator::translate);}
0
private Bounds findBounds(ColumnIndexBase<?>.ValueComparator comparator)
{    int length = comparator.arrayLength();    int lowerLeft = 0;    int upperLeft = 0;    int lowerRight = length - 1;    int upperRight = length - 1;    do {        if (lowerLeft > lowerRight) {            return null;        }        int i = floorMid(lowerLeft, lowerRight);        if (comparator.compareValueToMax(i) > 0) {            lowerRight = upperRight = i - 1;        } else if (comparator.compareValueToMin(i) < 0) {            lowerLeft = upperLeft = i + 1;        } else {            lowerRight = upperLeft = i;        }    } while (lowerLeft != lowerRight);    do {        if (upperLeft > upperRight) {            return null;        }        int i = ceilingMid(upperLeft, upperRight);        if (comparator.compareValueToMax(i) > 0) {            upperRight = i - 1;        } else if (comparator.compareValueToMin(i) < 0) {            upperLeft = i + 1;        } else {            upperLeft = i;        }    } while (upperLeft != upperRight);    return new Bounds(lowerLeft, upperRight);}
0
 int arrayLength()
{    return pageIndexes.length;}
0
 int translate(int arrayIndex)
{    return pageIndexes[arrayIndex];}
0
 static String truncate(String str)
{    if (str.length() <= MAX_VALUE_LENGTH_FOR_TOSTRING) {        return str;    }    return str.substring(0, TOSTRING_TRUNCATION_START_POS) + TOSTRING_TRUNCATION_MARKER + str.substring(str.length() - TOSTRING_TRUNCATION_END_POS);}
0
public BoundaryOrder getBoundaryOrder()
{    return boundaryOrder;}
0
public List<Long> getNullCounts()
{    if (nullCounts == null) {        return null;    }    return LongLists.unmodifiable(LongArrayList.wrap(nullCounts));}
0
public List<Boolean> getNullPages()
{    return BooleanLists.unmodifiable(BooleanArrayList.wrap(nullPages));}
0
public List<ByteBuffer> getMinValues()
{    List<ByteBuffer> list = new ArrayList<>(getPageCount());    int arrayIndex = 0;    for (int i = 0, n = getPageCount(); i < n; ++i) {        if (isNullPage(i)) {            list.add(EMPTY_BYTE_BUFFER);        } else {            list.add(getMinValueAsBytes(arrayIndex++));        }    }    return list;}
0
public List<ByteBuffer> getMaxValues()
{    List<ByteBuffer> list = new ArrayList<>(getPageCount());    int arrayIndex = 0;    for (int i = 0, n = getPageCount(); i < n; ++i) {        if (isNullPage(i)) {            list.add(EMPTY_BYTE_BUFFER);        } else {            list.add(getMaxValueAsBytes(arrayIndex++));        }    }    return list;}
0
public String toString()
{    try (Formatter formatter = new Formatter()) {        formatter.format("Boudary order: %s\n", boundaryOrder);        String minMaxPart = "  %-" + MAX_VALUE_LENGTH_FOR_TOSTRING + "s  %-" + MAX_VALUE_LENGTH_FOR_TOSTRING + "s\n";        formatter.format("%-10s  %20s" + minMaxPart, "", "null count", "min", "max");        String format = "page-%-5d  %20s" + minMaxPart;        int arrayIndex = 0;        for (int i = 0, n = nullPages.length; i < n; ++i) {            String nullCount = nullCounts == null ? TOSTRING_MISSING_VALUE_MARKER : Long.toString(nullCounts[i]);            String min, max;            if (nullPages[i]) {                min = max = TOSTRING_MISSING_VALUE_MARKER;            } else {                min = truncate(getMinValueAsString(arrayIndex));                max = truncate(getMaxValueAsString(arrayIndex++));            }            formatter.format(format, i, nullCount, min, max);        }        return formatter.toString();    }}
0
 int getPageCount()
{    return nullPages.length;}
0
 boolean isNullPage(int pageIndex)
{    return nullPages[pageIndex];}
0
public PrimitiveIterator.OfInt visit(And and)
{    throw new UnsupportedOperationException("AND shall not be used on column index directly");}
0
public PrimitiveIterator.OfInt visit(Not not)
{    throw new UnsupportedOperationException("NOT shall not be used on column index directly");}
0
public PrimitiveIterator.OfInt visit(Or or)
{    throw new UnsupportedOperationException("OR shall not be used on column index directly");}
0
public PrimitiveIterator.OfInt visit(Eq<T> eq)
{    T value = eq.getValue();    if (value == null) {        if (nullCounts == null) {                        return IndexIterator.all(getPageCount());        } else {            return IndexIterator.filter(getPageCount(), pageIndex -> nullCounts[pageIndex] > 0);        }    }    return getBoundaryOrder().eq(createValueComparator(value));}
0
public PrimitiveIterator.OfInt visit(Gt<T> gt)
{    return getBoundaryOrder().gt(createValueComparator(gt.getValue()));}
0
public PrimitiveIterator.OfInt visit(GtEq<T> gtEq)
{    return getBoundaryOrder().gtEq(createValueComparator(gtEq.getValue()));}
0
public PrimitiveIterator.OfInt visit(Lt<T> lt)
{    return getBoundaryOrder().lt(createValueComparator(lt.getValue()));}
0
public PrimitiveIterator.OfInt visit(LtEq<T> ltEq)
{    return getBoundaryOrder().ltEq(createValueComparator(ltEq.getValue()));}
0
public PrimitiveIterator.OfInt visit(NotEq<T> notEq)
{    T value = notEq.getValue();    if (value == null) {        return IndexIterator.filter(getPageCount(), pageIndex -> !nullPages[pageIndex]);    }    if (nullCounts == null) {                return IndexIterator.all(getPageCount());    }        IntSet matchingIndexes = new IntOpenHashSet();    getBoundaryOrder().notEq(createValueComparator(value)).forEachRemaining((int index) -> matchingIndexes.add(index));    return IndexIterator.filter(getPageCount(), pageIndex -> nullCounts[pageIndex] > 0 || matchingIndexes.contains(pageIndex));}
0
public PrimitiveIterator.OfInt visit(UserDefined<T, U> udp)
{    final UserDefinedPredicate<T> predicate = udp.getUserDefinedPredicate();    final boolean acceptNulls = predicate.acceptsNullValue();    if (acceptNulls && nullCounts == null) {                return IndexIterator.all(getPageCount());    }    return IndexIterator.filter(getPageCount(), new IntPredicate() {        private int arrayIndex = -1;        @Override        public boolean test(int pageIndex) {            if (isNullPage(pageIndex)) {                return acceptNulls;            } else {                ++arrayIndex;                if (acceptNulls && nullCounts[pageIndex] > 0) {                    return true;                }                org.apache.parquet.filter2.predicate.Statistics<T> stats = createStats(arrayIndex);                return !predicate.canDrop(stats);            }        }    });}
0
public boolean test(int pageIndex)
{    if (isNullPage(pageIndex)) {        return acceptNulls;    } else {        ++arrayIndex;        if (acceptNulls && nullCounts[pageIndex] > 0) {            return true;        }        org.apache.parquet.filter2.predicate.Statistics<T> stats = createStats(arrayIndex);        return !predicate.canDrop(stats);    }}
0
public PrimitiveIterator.OfInt visit(LogicalNotUserDefined<T, U> udp)
{    final UserDefinedPredicate<T> inversePredicate = udp.getUserDefined().getUserDefinedPredicate();    final boolean acceptNulls = !inversePredicate.acceptsNullValue();    if (acceptNulls && nullCounts == null) {                return IndexIterator.all(getPageCount());    }    return IndexIterator.filter(getPageCount(), new IntPredicate() {        private int arrayIndex = -1;        @Override        public boolean test(int pageIndex) {            if (isNullPage(pageIndex)) {                return acceptNulls;            } else {                ++arrayIndex;                if (acceptNulls && nullCounts[pageIndex] > 0) {                    return true;                }                org.apache.parquet.filter2.predicate.Statistics<T> stats = createStats(arrayIndex);                return !inversePredicate.inverseCanDrop(stats);            }        }    });}
0
public boolean test(int pageIndex)
{    if (isNullPage(pageIndex)) {        return acceptNulls;    } else {        ++arrayIndex;        if (acceptNulls && nullCounts[pageIndex] > 0) {            return true;        }        org.apache.parquet.filter2.predicate.Statistics<T> stats = createStats(arrayIndex);        return !inversePredicate.inverseCanDrop(stats);    }}
0
public ColumnIndex build()
{    return null;}
0
public void add(Statistics<?> stats)
{}
0
 void addMinMax(Object min, Object max)
{}
0
 ColumnIndexBase<?> createColumnIndex(PrimitiveType type)
{    return null;}
0
 void clearMinMax()
{}
0
 void addMinMaxFromBytes(ByteBuffer min, ByteBuffer max)
{}
0
 int compareMinValues(PrimitiveComparator<Binary> comparator, int index1, int index2)
{    return 0;}
0
 int compareMaxValues(PrimitiveComparator<Binary> comparator, int index1, int index2)
{    return 0;}
0
 int sizeOf(Object value)
{    return 0;}
0
public static ColumnIndexBuilder getNoOpBuilder()
{    return NO_OP_BUILDER;}
0
public static ColumnIndexBuilder getBuilder(PrimitiveType type, int truncateLength)
{    ColumnIndexBuilder builder = createNewBuilder(type, truncateLength);    builder.type = type;    return builder;}
0
private static ColumnIndexBuilder createNewBuilder(PrimitiveType type, int truncateLength)
{    switch(type.getPrimitiveTypeName()) {        case BINARY:        case FIXED_LEN_BYTE_ARRAY:        case INT96:            return new BinaryColumnIndexBuilder(type, truncateLength);        case BOOLEAN:            return new BooleanColumnIndexBuilder();        case DOUBLE:            return new DoubleColumnIndexBuilder();        case FLOAT:            return new FloatColumnIndexBuilder();        case INT32:            return new IntColumnIndexBuilder();        case INT64:            return new LongColumnIndexBuilder();        default:            throw new IllegalArgumentException("Unsupported type for column index: " + type);    }}
0
public static ColumnIndex build(PrimitiveType type, BoundaryOrder boundaryOrder, List<Boolean> nullPages, List<Long> nullCounts, List<ByteBuffer> minValues, List<ByteBuffer> maxValues)
{    ColumnIndexBuilder builder = createNewBuilder(type, Integer.MAX_VALUE);    builder.fill(nullPages, nullCounts, minValues, maxValues);    ColumnIndexBase<?> columnIndex = builder.build(type);    columnIndex.boundaryOrder = requireNonNull(boundaryOrder);    return columnIndex;}
0
public void add(Statistics<?> stats)
{    if (stats.hasNonNullValue()) {        nullPages.add(false);        Object min = stats.genericGetMin();        Object max = stats.genericGetMax();        addMinMax(min, max);        pageIndexes.add(nextPageIndex);        minMaxSize += sizeOf(min);        minMaxSize += sizeOf(max);    } else {        nullPages.add(true);    }    nullCounts.add(stats.getNumNulls());    ++nextPageIndex;}
0
private void fill(List<Boolean> nullPages, List<Long> nullCounts, List<ByteBuffer> minValues, List<ByteBuffer> maxValues)
{    clear();    int pageCount = nullPages.size();    if ((nullCounts != null && nullCounts.size() != pageCount) || minValues.size() != pageCount || maxValues.size() != pageCount) {        throw new IllegalArgumentException(String.format("Not all sizes are equal (nullPages:%d, nullCounts:%s, minValues:%d, maxValues:%d", nullPages.size(), nullCounts == null ? "null" : nullCounts.size(), minValues.size(), maxValues.size()));    }    this.nullPages.addAll(nullPages);        if (nullCounts != null) {        this.nullCounts.addAll(nullCounts);    }    for (int i = 0; i < pageCount; ++i) {        if (!nullPages.get(i)) {            ByteBuffer min = minValues.get(i);            ByteBuffer max = maxValues.get(i);            addMinMaxFromBytes(min, max);            pageIndexes.add(i);            minMaxSize += min.remaining();            minMaxSize += max.remaining();        }    }}
0
public ColumnIndex build()
{    ColumnIndexBase<?> columnIndex = build(type);    if (columnIndex == null) {        return null;    }    columnIndex.boundaryOrder = calculateBoundaryOrder(type.comparator());    return columnIndex;}
0
private ColumnIndexBase<?> build(PrimitiveType type)
{    if (nullPages.isEmpty()) {        return null;    }    ColumnIndexBase<?> columnIndex = createColumnIndex(type);    if (columnIndex == null) {                return null;    }    columnIndex.nullPages = nullPages.toBooleanArray();        if (!nullCounts.isEmpty()) {        columnIndex.nullCounts = nullCounts.toLongArray();    }    columnIndex.pageIndexes = pageIndexes.toIntArray();    return columnIndex;}
0
private BoundaryOrder calculateBoundaryOrder(PrimitiveComparator<Binary> comparator)
{    if (isAscending(comparator)) {        return BoundaryOrder.ASCENDING;    } else if (isDescending(comparator)) {        return BoundaryOrder.DESCENDING;    } else {        return BoundaryOrder.UNORDERED;    }}
0
private boolean isAscending(PrimitiveComparator<Binary> comparator)
{    for (int i = 1, n = pageIndexes.size(); i < n; ++i) {        if (compareMinValues(comparator, i - 1, i) > 0 || compareMaxValues(comparator, i - 1, i) > 0) {            return false;        }    }    return true;}
0
private boolean isDescending(PrimitiveComparator<Binary> comparator)
{    for (int i = 1, n = pageIndexes.size(); i < n; ++i) {        if (compareMinValues(comparator, i - 1, i) < 0 || compareMaxValues(comparator, i - 1, i) < 0) {            return false;        }    }    return true;}
0
private void clear()
{    nullPages.clear();    nullCounts.clear();    clearMinMax();    minMaxSize = 0;    nextPageIndex = 0;    pageIndexes.clear();}
0
public int getPageCount()
{    return nullPages.size();}
0
public long getMinMaxSize()
{    return minMaxSize;}
0
 ByteBuffer getMinValueAsBytes(int pageIndex)
{    return convert(minValues[pageIndex]);}
0
 ByteBuffer getMaxValueAsBytes(int pageIndex)
{    return convert(maxValues[pageIndex]);}
0
 String getMinValueAsString(int pageIndex)
{    return stringifier.stringify(minValues[pageIndex]);}
0
 String getMaxValueAsString(int pageIndex)
{    return stringifier.stringify(maxValues[pageIndex]);}
0
 Statistics<T> createStats(int arrayIndex)
{    return (Statistics<T>) new Statistics<Double>(minValues[arrayIndex], maxValues[arrayIndex], comparator);}
0
 ValueComparator createValueComparator(Object value)
{    final double v = (double) value;    return new ValueComparator() {        @Override        int compareValueToMin(int arrayIndex) {            return comparator.compare(v, minValues[arrayIndex]);        }        @Override        int compareValueToMax(int arrayIndex) {            return comparator.compare(v, maxValues[arrayIndex]);        }    };}
0
 int compareValueToMin(int arrayIndex)
{    return comparator.compare(v, minValues[arrayIndex]);}
0
 int compareValueToMax(int arrayIndex)
{    return comparator.compare(v, maxValues[arrayIndex]);}
0
private static double convert(ByteBuffer buffer)
{    return buffer.order(LITTLE_ENDIAN).getDouble(0);}
0
private static ByteBuffer convert(double value)
{    return ByteBuffer.allocate(Double.BYTES).order(LITTLE_ENDIAN).putDouble(0, value);}
0
 void addMinMaxFromBytes(ByteBuffer min, ByteBuffer max)
{    minValues.add(convert(min));    maxValues.add(convert(max));}
0
 void addMinMax(Object min, Object max)
{    double dMin = (double) min;    double dMax = (double) max;    if (Double.isNaN(dMin) || Double.isNaN(dMax)) {                invalid = true;    }        if (Double.compare(dMin, +0.0) == 0) {        dMin = -0.0;    }    if (Double.compare(dMax, -0.0) == 0) {        dMax = +0.0;    }    minValues.add(dMin);    maxValues.add(dMax);}
0
 ColumnIndexBase<Double> createColumnIndex(PrimitiveType type)
{    if (invalid) {        return null;    }    DoubleColumnIndex columnIndex = new DoubleColumnIndex(type);    columnIndex.minValues = minValues.toDoubleArray();    columnIndex.maxValues = maxValues.toDoubleArray();    return columnIndex;}
0
 void clearMinMax()
{    minValues.clear();    maxValues.clear();}
0
 int compareMinValues(PrimitiveComparator<Binary> comparator, int index1, int index2)
{    return comparator.compare(minValues.get(index1), minValues.get(index2));}
0
 int compareMaxValues(PrimitiveComparator<Binary> comparator, int index1, int index2)
{    return comparator.compare(maxValues.get(index1), maxValues.get(index2));}
0
 int sizeOf(Object value)
{    return Double.BYTES;}
0
 ByteBuffer getMinValueAsBytes(int pageIndex)
{    return convert(minValues[pageIndex]);}
0
 ByteBuffer getMaxValueAsBytes(int pageIndex)
{    return convert(maxValues[pageIndex]);}
0
 String getMinValueAsString(int pageIndex)
{    return stringifier.stringify(minValues[pageIndex]);}
0
 String getMaxValueAsString(int pageIndex)
{    return stringifier.stringify(maxValues[pageIndex]);}
0
 Statistics<T> createStats(int arrayIndex)
{    return (Statistics<T>) new Statistics<Float>(minValues[arrayIndex], maxValues[arrayIndex], comparator);}
0
 ValueComparator createValueComparator(Object value)
{    final float v = (float) value;    return new ValueComparator() {        @Override        int compareValueToMin(int arrayIndex) {            return comparator.compare(v, minValues[arrayIndex]);        }        @Override        int compareValueToMax(int arrayIndex) {            return comparator.compare(v, maxValues[arrayIndex]);        }    };}
0
 int compareValueToMin(int arrayIndex)
{    return comparator.compare(v, minValues[arrayIndex]);}
0
 int compareValueToMax(int arrayIndex)
{    return comparator.compare(v, maxValues[arrayIndex]);}
0
private static float convert(ByteBuffer buffer)
{    return buffer.order(LITTLE_ENDIAN).getFloat(0);}
0
private static ByteBuffer convert(float value)
{    return ByteBuffer.allocate(Float.BYTES).order(LITTLE_ENDIAN).putFloat(0, value);}
0
 void addMinMaxFromBytes(ByteBuffer min, ByteBuffer max)
{    minValues.add(convert(min));    maxValues.add(convert(max));}
0
 void addMinMax(Object min, Object max)
{    float fMin = (float) min;    float fMax = (float) max;    if (Float.isNaN(fMin) || Float.isNaN(fMax)) {                invalid = true;    }        if (Float.compare(fMin, +0.0f) == 0) {        fMin = -0.0f;    }    if (Float.compare(fMax, -0.0f) == 0) {        fMax = +0.0f;    }    minValues.add(fMin);    maxValues.add(fMax);}
0
 ColumnIndexBase<Float> createColumnIndex(PrimitiveType type)
{    if (invalid) {        return null;    }    FloatColumnIndex columnIndex = new FloatColumnIndex(type);    columnIndex.minValues = minValues.toFloatArray();    columnIndex.maxValues = maxValues.toFloatArray();    return columnIndex;}
0
 void clearMinMax()
{    minValues.clear();    maxValues.clear();}
0
 int compareMinValues(PrimitiveComparator<Binary> comparator, int index1, int index2)
{    return comparator.compare(minValues.get(index1), minValues.get(index2));}
0
 int compareMaxValues(PrimitiveComparator<Binary> comparator, int index1, int index2)
{    return comparator.compare(maxValues.get(index1), maxValues.get(index2));}
0
 int sizeOf(Object value)
{    return Float.BYTES;}
0
public boolean hasNext()
{    return false;}
0
public int nextInt()
{    throw new NoSuchElementException();}
0
 static PrimitiveIterator.OfInt all(int pageCount)
{    return new IndexIterator(0, pageCount, i -> true, i -> i);}
0
 static PrimitiveIterator.OfInt all(ColumnIndexBase<?>.ValueComparator comparator)
{    return new IndexIterator(0, comparator.arrayLength(), i -> true, comparator::translate);}
0
 static PrimitiveIterator.OfInt filter(int pageCount, IntPredicate filter)
{    return new IndexIterator(0, pageCount, filter, i -> i);}
0
 static PrimitiveIterator.OfInt filterTranslate(int arrayLength, IntPredicate filter, IntUnaryOperator translator)
{    return new IndexIterator(0, arrayLength, filter, translator);}
0
 static PrimitiveIterator.OfInt rangeTranslate(int from, int to, IntUnaryOperator translator)
{    return new IndexIterator(from, to + 1, i -> true, translator);}
0
private int nextPageIndex(int startIndex)
{    for (int i = startIndex; i < endIndex; ++i) {        if (filter.test(i)) {            return i;        }    }    return -1;}
0
public boolean hasNext()
{    return index >= 0;}
0
public int nextInt()
{    if (hasNext()) {        int ret = index;        index = nextPageIndex(index + 1);        return translator.applyAsInt(ret);    }    throw new NoSuchElementException();}
0
 ByteBuffer getMinValueAsBytes(int pageIndex)
{    return convert(minValues[pageIndex]);}
0
 ByteBuffer getMaxValueAsBytes(int pageIndex)
{    return convert(maxValues[pageIndex]);}
0
 String getMinValueAsString(int pageIndex)
{    return stringifier.stringify(minValues[pageIndex]);}
0
 String getMaxValueAsString(int pageIndex)
{    return stringifier.stringify(maxValues[pageIndex]);}
0
 Statistics<T> createStats(int arrayIndex)
{    return (Statistics<T>) new Statistics<Integer>(minValues[arrayIndex], maxValues[arrayIndex], comparator);}
0
 ValueComparator createValueComparator(Object value)
{    final int v = (int) value;    return new ValueComparator() {        @Override        int compareValueToMin(int arrayIndex) {            return comparator.compare(v, minValues[arrayIndex]);        }        @Override        int compareValueToMax(int arrayIndex) {            return comparator.compare(v, maxValues[arrayIndex]);        }    };}
0
 int compareValueToMin(int arrayIndex)
{    return comparator.compare(v, minValues[arrayIndex]);}
0
 int compareValueToMax(int arrayIndex)
{    return comparator.compare(v, maxValues[arrayIndex]);}
0
private static int convert(ByteBuffer buffer)
{    return buffer.order(LITTLE_ENDIAN).getInt(0);}
0
private static ByteBuffer convert(int value)
{    return ByteBuffer.allocate(Integer.BYTES).order(LITTLE_ENDIAN).putInt(0, value);}
0
 void addMinMaxFromBytes(ByteBuffer min, ByteBuffer max)
{    minValues.add(convert(min));    maxValues.add(convert(max));}
0
 void addMinMax(Object min, Object max)
{    minValues.add((int) min);    maxValues.add((int) max);}
0
 ColumnIndexBase<Integer> createColumnIndex(PrimitiveType type)
{    IntColumnIndex columnIndex = new IntColumnIndex(type);    columnIndex.minValues = minValues.toIntArray();    columnIndex.maxValues = maxValues.toIntArray();    return columnIndex;}
0
 void clearMinMax()
{    minValues.clear();    maxValues.clear();}
0
 int compareMinValues(PrimitiveComparator<Binary> comparator, int index1, int index2)
{    return comparator.compare(minValues.get(index1), minValues.get(index2));}
0
 int compareMaxValues(PrimitiveComparator<Binary> comparator, int index1, int index2)
{    return comparator.compare(maxValues.get(index1), maxValues.get(index2));}
0
 int sizeOf(Object value)
{    return Integer.BYTES;}
0
 ByteBuffer getMinValueAsBytes(int pageIndex)
{    return convert(minValues[pageIndex]);}
0
 ByteBuffer getMaxValueAsBytes(int pageIndex)
{    return convert(maxValues[pageIndex]);}
0
 String getMinValueAsString(int pageIndex)
{    return stringifier.stringify(minValues[pageIndex]);}
0
 String getMaxValueAsString(int pageIndex)
{    return stringifier.stringify(maxValues[pageIndex]);}
0
 Statistics<T> createStats(int arrayIndex)
{    return (Statistics<T>) new Statistics<Long>(minValues[arrayIndex], maxValues[arrayIndex], comparator);}
0
 ValueComparator createValueComparator(Object value)
{    final long v = (long) value;    return new ValueComparator() {        @Override        int compareValueToMin(int arrayIndex) {            return comparator.compare(v, minValues[arrayIndex]);        }        @Override        int compareValueToMax(int arrayIndex) {            return comparator.compare(v, maxValues[arrayIndex]);        }    };}
0
 int compareValueToMin(int arrayIndex)
{    return comparator.compare(v, minValues[arrayIndex]);}
0
 int compareValueToMax(int arrayIndex)
{    return comparator.compare(v, maxValues[arrayIndex]);}
0
private static long convert(ByteBuffer buffer)
{    return buffer.order(LITTLE_ENDIAN).getLong(0);}
0
private static ByteBuffer convert(long value)
{    return ByteBuffer.allocate(Long.BYTES).order(LITTLE_ENDIAN).putLong(0, value);}
0
 void addMinMaxFromBytes(ByteBuffer min, ByteBuffer max)
{    minValues.add(convert(min));    maxValues.add(convert(max));}
0
 void addMinMax(Object min, Object max)
{    minValues.add((long) min);    maxValues.add((long) max);}
0
 ColumnIndexBase<Long> createColumnIndex(PrimitiveType type)
{    LongColumnIndex columnIndex = new LongColumnIndex(type);    columnIndex.minValues = minValues.toLongArray();    columnIndex.maxValues = maxValues.toLongArray();    return columnIndex;}
0
 void clearMinMax()
{    minValues.clear();    maxValues.clear();}
0
 int compareMinValues(PrimitiveComparator<Binary> comparator, int index1, int index2)
{    return comparator.compare(minValues.get(index1), minValues.get(index2));}
0
 int compareMaxValues(PrimitiveComparator<Binary> comparator, int index1, int index2)
{    return comparator.compare(maxValues.get(index1), maxValues.get(index2));}
0
 int sizeOf(Object value)
{    return Long.BYTES;}
0
public long getLastRowIndex(int pageIndex, long rowGroupRowCount)
{    int nextPageIndex = pageIndex + 1;    return (nextPageIndex >= getPageCount() ? rowGroupRowCount : getFirstRowIndex(nextPageIndex)) - 1;}
0
public String toString()
{    try (Formatter formatter = new Formatter()) {        formatter.format("%-10s  %20s  %16s  %20s\n", "", "offset", "compressed size", "first row index");        for (int i = 0, n = offsets.length; i < n; ++i) {            formatter.format("page-%-5d  %20d  %16d  %20d\n", i, offsets[i], compressedPageSizes[i], firstRowIndexes[i]);        }        return formatter.toString();    }}
0
public int getPageCount()
{    return offsets.length;}
0
public long getOffset(int pageIndex)
{    return offsets[pageIndex];}
0
public int getCompressedPageSize(int pageIndex)
{    return compressedPageSizes[pageIndex];}
0
public long getFirstRowIndex(int pageIndex)
{    return firstRowIndexes[pageIndex];}
0
public void add(int compressedPageSize, long rowCount)
{}
0
public void add(long offset, int compressedPageSize, long rowCount)
{}
0
public static OffsetIndexBuilder getNoOpBuilder()
{    return NO_OP_BUILDER;}
0
public static OffsetIndexBuilder getBuilder()
{    return new OffsetIndexBuilder();}
0
public void add(int compressedPageSize, long rowCount)
{    add(previousOffset + previousPageSize, compressedPageSize, previousRowIndex + previousRowCount);    previousRowCount = rowCount;}
0
public void add(long offset, int compressedPageSize, long firstRowIndex)
{    previousOffset = offset;    offsets.add(offset);    previousPageSize = compressedPageSize;    compressedPageSizes.add(compressedPageSize);    previousRowIndex = firstRowIndex;    firstRowIndexes.add(firstRowIndex);}
0
public OffsetIndex build()
{    return build(0);}
0
public OffsetIndex build(long firstPageOffset)
{    if (compressedPageSizes.isEmpty()) {        return null;    }    long[] offsets = this.offsets.toLongArray();    if (firstPageOffset != 0) {        for (int i = 0, n = offsets.length; i < n; ++i) {            offsets[i] += firstPageOffset;        }    }    OffsetIndexImpl offsetIndex = new OffsetIndexImpl();    offsetIndex.offsets = offsets;    offsetIndex.compressedPageSizes = compressedPageSizes.toIntArray();    offsetIndex.firstRowIndexes = firstRowIndexes.toLongArray();    return offsetIndex;}
0
public static RowRanges calculateRowRanges(FilterCompat.Filter filter, ColumnIndexStore columnIndexStore, Set<ColumnPath> paths, long rowCount)
{    return filter.accept(new FilterCompat.Visitor<RowRanges>() {        @Override        public RowRanges visit(FilterPredicateCompat filterPredicateCompat) {            try {                return filterPredicateCompat.getFilterPredicate().accept(new ColumnIndexFilter(columnIndexStore, paths, rowCount));            } catch (MissingOffsetIndexException e) {                                return RowRanges.createSingle(rowCount);            }        }        @Override        public RowRanges visit(UnboundRecordFilterCompat unboundRecordFilterCompat) {            return RowRanges.createSingle(rowCount);        }        @Override        public RowRanges visit(NoOpFilter noOpFilter) {            return RowRanges.createSingle(rowCount);        }    });}
1
public RowRanges visit(FilterPredicateCompat filterPredicateCompat)
{    try {        return filterPredicateCompat.getFilterPredicate().accept(new ColumnIndexFilter(columnIndexStore, paths, rowCount));    } catch (MissingOffsetIndexException e) {                return RowRanges.createSingle(rowCount);    }}
1
public RowRanges visit(UnboundRecordFilterCompat unboundRecordFilterCompat)
{    return RowRanges.createSingle(rowCount);}
0
public RowRanges visit(NoOpFilter noOpFilter)
{    return RowRanges.createSingle(rowCount);}
0
private RowRanges allRows()
{    if (allRows == null) {        allRows = RowRanges.createSingle(rowCount);    }    return allRows;}
0
public RowRanges visit(Eq<T> eq)
{    return applyPredicate(eq.getColumn(), ci -> ci.visit(eq), eq.getValue() == null ? allRows() : RowRanges.EMPTY);}
0
public RowRanges visit(NotEq<T> notEq)
{    return applyPredicate(notEq.getColumn(), ci -> ci.visit(notEq), notEq.getValue() == null ? RowRanges.EMPTY : allRows());}
0
public RowRanges visit(Lt<T> lt)
{    return applyPredicate(lt.getColumn(), ci -> ci.visit(lt), RowRanges.EMPTY);}
0
public RowRanges visit(LtEq<T> ltEq)
{    return applyPredicate(ltEq.getColumn(), ci -> ci.visit(ltEq), RowRanges.EMPTY);}
0
public RowRanges visit(Gt<T> gt)
{    return applyPredicate(gt.getColumn(), ci -> ci.visit(gt), RowRanges.EMPTY);}
0
public RowRanges visit(GtEq<T> gtEq)
{    return applyPredicate(gtEq.getColumn(), ci -> ci.visit(gtEq), RowRanges.EMPTY);}
0
public RowRanges visit(UserDefined<T, U> udp)
{    return applyPredicate(udp.getColumn(), ci -> ci.visit(udp), udp.getUserDefinedPredicate().acceptsNullValue() ? allRows() : RowRanges.EMPTY);}
0
public RowRanges visit(LogicalNotUserDefined<T, U> udp)
{    return applyPredicate(udp.getUserDefined().getColumn(), ci -> ci.visit(udp), udp.getUserDefined().getUserDefinedPredicate().acceptsNullValue() ? RowRanges.EMPTY : allRows());}
0
public RowRanges visit(And and)
{    return RowRanges.intersection(and.getLeft().accept(this), and.getRight().accept(this));}
0
public RowRanges visit(Or or)
{    return RowRanges.union(or.getLeft().accept(this), or.getRight().accept(this));}
0
public RowRanges visit(Not not)
{    throw new IllegalArgumentException("Predicates containing a NOT must be run through LogicalInverseRewriter. " + not);}
0
private static Range union(Range left, Range right)
{    if (left.from <= right.from) {        if (left.to + 1 >= right.from) {            return new Range(left.from, Math.max(left.to, right.to));        }    } else if (right.to + 1 >= left.from) {        return new Range(right.from, Math.max(left.to, right.to));    }    return null;}
0
private static Range intersection(Range left, Range right)
{    if (left.from <= right.from) {        if (left.to >= right.from) {            return new Range(right.from, Math.min(left.to, right.to));        }    } else if (right.to >= left.from) {        return new Range(left.from, Math.min(left.to, right.to));    }    return null;}
0
 long count()
{    return to - from + 1;}
0
 boolean isBefore(Range other)
{    return to < other.from;}
0
 boolean isAfter(Range other)
{    return from > other.to;}
0
public String toString()
{    return "[" + from + ", " + to + ']';}
0
 static RowRanges createSingle(long rowCount)
{    RowRanges ranges = new RowRanges();    ranges.add(new Range(0, rowCount - 1));    return ranges;}
0
 static RowRanges create(long rowCount, PrimitiveIterator.OfInt pageIndexes, OffsetIndex offsetIndex)
{    RowRanges ranges = new RowRanges();    while (pageIndexes.hasNext()) {        int pageIndex = pageIndexes.nextInt();        ranges.add(new Range(offsetIndex.getFirstRowIndex(pageIndex), offsetIndex.getLastRowIndex(pageIndex, rowCount)));    }    return ranges;}
0
 static RowRanges union(RowRanges left, RowRanges right)
{    RowRanges result = new RowRanges();    Iterator<Range> it1 = left.ranges.iterator();    Iterator<Range> it2 = right.ranges.iterator();    if (it2.hasNext()) {        Range range2 = it2.next();        while (it1.hasNext()) {            Range range1 = it1.next();            if (range1.isAfter(range2)) {                result.add(range2);                range2 = range1;                Iterator<Range> tmp = it1;                it1 = it2;                it2 = tmp;            } else {                result.add(range1);            }        }        result.add(range2);    } else {        it2 = it1;    }    while (it2.hasNext()) {        result.add(it2.next());    }    return result;}
0
 static RowRanges intersection(RowRanges left, RowRanges right)
{    RowRanges result = new RowRanges();    int rightIndex = 0;    for (Range l : left.ranges) {        for (int i = rightIndex, n = right.ranges.size(); i < n; ++i) {            Range r = right.ranges.get(i);            if (l.isBefore(r)) {                break;            } else if (l.isAfter(r)) {                rightIndex = i + 1;                continue;            }            result.add(Range.intersection(l, r));        }    }    return result;}
0
private void add(Range range)
{    Range rangeToAdd = range;    for (int i = ranges.size() - 1; i >= 0; --i) {        Range last = ranges.get(i);        assert !last.isAfter(range);        Range u = Range.union(last, rangeToAdd);        if (u == null) {            break;        }        rangeToAdd = u;        ranges.remove(i);    }    ranges.add(rangeToAdd);}
0
public long rowCount()
{    long cnt = 0;    for (Range range : ranges) {        cnt += range.count();    }    return cnt;}
0
public PrimitiveIterator.OfLong iterator()
{    return new PrimitiveIterator.OfLong() {        private int currentRangeIndex = -1;        private Range currentRange;        private long next = findNext();        private long findNext() {            if (currentRange == null || next + 1 > currentRange.to) {                if (currentRangeIndex + 1 < ranges.size()) {                    currentRange = ranges.get(++currentRangeIndex);                    next = currentRange.from;                } else {                    return -1;                }            } else {                ++next;            }            return next;        }        @Override        public boolean hasNext() {            return next >= 0;        }        @Override        public long nextLong() {            long ret = next;            if (ret < 0) {                throw new NoSuchElementException();            }            next = findNext();            return ret;        }    };}
0
private long findNext()
{    if (currentRange == null || next + 1 > currentRange.to) {        if (currentRangeIndex + 1 < ranges.size()) {            currentRange = ranges.get(++currentRangeIndex);            next = currentRange.from;        } else {            return -1;        }    } else {        ++next;    }    return next;}
0
public boolean hasNext()
{    return next >= 0;}
0
public long nextLong()
{    long ret = next;    if (ret < 0) {        throw new NoSuchElementException();    }    next = findNext();    return ret;}
0
public boolean isOverlapping(long from, long to)
{    return Collections.binarySearch(ranges, new Range(from, to), (r1, r2) -> r1.isBefore(r2) ? -1 : r1.isAfter(r2) ? 1 : 0) >= 0;}
0
public String toString()
{    return ranges.toString();}
0
public boolean equals(Object obj)
{    if (obj == null) {        return false;    }    if (obj instanceof Binary) {        return equals((Binary) obj);    }    return false;}
0
public String toString()
{    return "Binary{" + length() + (isBackingBytesReused ? " reused" : " constant") + " bytes, " + Arrays.toString(getBytesUnsafe()) + "}";}
0
public Binary copy()
{    if (isBackingBytesReused) {        return Binary.fromConstantByteArray(getBytes());    } else {        return this;    }}
0
public boolean isBackingBytesReused()
{    return isBackingBytesReused;}
0
public String toStringUsingUTF8()
{        return StandardCharsets.UTF_8.decode(ByteBuffer.wrap(value, offset, length)).toString();}
0
public int length()
{    return length;}
0
public void writeTo(OutputStream out) throws IOException
{    out.write(value, offset, length);}
0
public byte[] getBytes()
{    return Arrays.copyOfRange(value, offset, offset + length);}
0
public byte[] getBytesUnsafe()
{        return getBytes();}
0
public Binary slice(int start, int length)
{    if (isBackingBytesReused) {        return Binary.fromReusedByteArray(value, offset + start, length);    } else {        return Binary.fromConstantByteArray(value, offset + start, length);    }}
0
public int hashCode()
{    return Binary.hashCode(value, offset, length);}
0
 boolean equals(Binary other)
{    return other.equals(value, offset, length);}
0
 boolean equals(byte[] other, int otherOffset, int otherLength)
{    return Binary.equals(value, offset, length, other, otherOffset, otherLength);}
0
 boolean equals(ByteBuffer bytes, int otherOffset, int otherLength)
{    return Binary.equals(value, offset, length, bytes, otherOffset, otherLength);}
0
public int compareTo(Binary other)
{    return PrimitiveComparator.UNSIGNED_LEXICOGRAPHICAL_BINARY_COMPARATOR.compare(this, other);}
0
public ByteBuffer toByteBuffer()
{    return ByteBuffer.wrap(value, offset, length);}
0
public void writeTo(DataOutput out) throws IOException
{    out.write(value, offset, length);}
0
public String toString()
{    return "Binary{\"" + toStringUsingUTF8() + "\"}";}
0
private static ByteBuffer encodeUTF8(String value)
{    return ByteBuffer.wrap(value.getBytes(StandardCharsets.UTF_8));}
0
public String toString()
{    return "Binary{\"" + toStringUsingUTF8() + "\"}";}
0
private static ByteBuffer encodeUTF8(CharSequence value)
{    try {        return ENCODER.get().encode(CharBuffer.wrap(value));    } catch (CharacterCodingException e) {        throw new ParquetEncodingException("UTF-8 not supported.", e);    }}
0
public static Binary fromReusedByteArray(final byte[] value, final int offset, final int length)
{    return new ByteArraySliceBackedBinary(value, offset, length, true);}
0
public static Binary fromConstantByteArray(final byte[] value, final int offset, final int length)
{    return new ByteArraySliceBackedBinary(value, offset, length, false);}
0
public static Binary fromByteArray(final byte[] value, final int offset, final int length)
{        return fromReusedByteArray(value, offset, length);}
0
public String toStringUsingUTF8()
{    return StandardCharsets.UTF_8.decode(ByteBuffer.wrap(value)).toString();}
0
public int length()
{    return value.length;}
0
public void writeTo(OutputStream out) throws IOException
{    out.write(value);}
0
public byte[] getBytes()
{    return Arrays.copyOfRange(value, 0, value.length);}
0
public byte[] getBytesUnsafe()
{    return value;}
0
public Binary slice(int start, int length)
{    if (isBackingBytesReused) {        return Binary.fromReusedByteArray(value, start, length);    } else {        return Binary.fromConstantByteArray(value, start, length);    }}
0
public int hashCode()
{    return Binary.hashCode(value, 0, value.length);}
0
 boolean equals(Binary other)
{    return other.equals(value, 0, value.length);}
0
 boolean equals(byte[] other, int otherOffset, int otherLength)
{    return Binary.equals(value, 0, value.length, other, otherOffset, otherLength);}
0
 boolean equals(ByteBuffer bytes, int otherOffset, int otherLength)
{    return Binary.equals(value, 0, value.length, bytes, otherOffset, otherLength);}
0
public int compareTo(Binary other)
{    return PrimitiveComparator.UNSIGNED_LEXICOGRAPHICAL_BINARY_COMPARATOR.compare(this, other);}
0
public ByteBuffer toByteBuffer()
{    return ByteBuffer.wrap(value);}
0
public void writeTo(DataOutput out) throws IOException
{    out.write(value);}
0
public static Binary fromReusedByteArray(final byte[] value)
{    return new ByteArrayBackedBinary(value, true);}
0
public static Binary fromConstantByteArray(final byte[] value)
{    return new ByteArrayBackedBinary(value, false);}
0
public static Binary fromByteArray(final byte[] value)
{        return fromReusedByteArray(value);}
0
public String toStringUsingUTF8()
{    String ret;    if (value.hasArray()) {        ret = new String(value.array(), value.arrayOffset() + offset, length, StandardCharsets.UTF_8);    } else {        int limit = value.limit();        value.limit(offset + length);        int position = value.position();        value.position(offset);                                ret = StandardCharsets.UTF_8.decode(value).toString();        value.limit(limit);        value.position(position);    }    return ret;}
0
public int length()
{    return length;}
0
public void writeTo(OutputStream out) throws IOException
{    if (value.hasArray()) {        out.write(value.array(), value.arrayOffset() + offset, length);    } else {        out.write(getBytesUnsafe(), 0, length);    }}
0
public byte[] getBytes()
{    byte[] bytes = new byte[length];    int limit = value.limit();    value.limit(offset + length);    int position = value.position();    value.position(offset);    value.get(bytes);    value.limit(limit);    value.position(position);    if (!isBackingBytesReused) {                cachedBytes = bytes;    }    return bytes;}
0
public byte[] getBytesUnsafe()
{    return cachedBytes != null ? cachedBytes : getBytes();}
0
public Binary slice(int start, int length)
{    return Binary.fromConstantByteArray(getBytesUnsafe(), start, length);}
0
public int hashCode()
{    if (value.hasArray()) {        return Binary.hashCode(value.array(), value.arrayOffset() + offset, length);    } else {        return Binary.hashCode(value, offset, length);    }}
0
 boolean equals(Binary other)
{    if (value.hasArray()) {        return other.equals(value.array(), value.arrayOffset() + offset, length);    } else {        return other.equals(value, offset, length);    }}
0
 boolean equals(byte[] other, int otherOffset, int otherLength)
{    if (value.hasArray()) {        return Binary.equals(value.array(), value.arrayOffset() + offset, length, other, otherOffset, otherLength);    } else {        return Binary.equals(other, otherOffset, otherLength, value, offset, length);    }}
0
 boolean equals(ByteBuffer otherBytes, int otherOffset, int otherLength)
{    return Binary.equals(value, 0, length, otherBytes, otherOffset, otherLength);}
0
public int compareTo(Binary other)
{    return PrimitiveComparator.UNSIGNED_LEXICOGRAPHICAL_BINARY_COMPARATOR.compare(this, other);}
0
public ByteBuffer toByteBuffer()
{    ByteBuffer ret = value.duplicate();    ret.position(offset);    ret.limit(offset + length);    return ret;}
0
public void writeTo(DataOutput out) throws IOException
{        out.write(getBytesUnsafe());}
0
private void writeObject(java.io.ObjectOutputStream out) throws IOException
{    byte[] bytes = getBytesUnsafe();    out.writeInt(bytes.length);    out.write(bytes);}
0
private void readObject(java.io.ObjectInputStream in) throws IOException, ClassNotFoundException
{    int length = in.readInt();    byte[] bytes = new byte[length];    in.readFully(bytes, 0, length);    this.value = ByteBuffer.wrap(bytes);    this.offset = 0;    this.length = length;}
0
private void readObjectNoData() throws ObjectStreamException
{    this.value = ByteBuffer.wrap(new byte[0]);}
0
public static Binary fromReusedByteBuffer(final ByteBuffer value, int offset, int length)
{    return new ByteBufferBackedBinary(value, offset, length, true);}
0
public static Binary fromConstantByteBuffer(final ByteBuffer value, int offset, int length)
{    return new ByteBufferBackedBinary(value, offset, length, false);}
0
public static Binary fromReusedByteBuffer(final ByteBuffer value)
{    return new ByteBufferBackedBinary(value, true);}
0
public static Binary fromConstantByteBuffer(final ByteBuffer value)
{    return new ByteBufferBackedBinary(value, false);}
0
public static Binary fromByteBuffer(final ByteBuffer value)
{        return fromReusedByteBuffer(value);}
0
public static Binary fromString(String value)
{    return new FromStringBinary(value);}
0
public static Binary fromCharSequence(CharSequence value)
{    return new FromCharSequenceBinary(value);}
0
private static final int hashCode(byte[] array, int offset, int length)
{    int result = 1;    for (int i = offset; i < offset + length; i++) {        byte b = array[i];        result = 31 * result + b;    }    return result;}
0
private static final int hashCode(ByteBuffer buf, int offset, int length)
{    int result = 1;    for (int i = offset; i < offset + length; i++) {        byte b = buf.get(i);        result = 31 * result + b;    }    return result;}
0
private static final boolean equals(ByteBuffer buf1, int offset1, int length1, ByteBuffer buf2, int offset2, int length2)
{    if (buf1 == null && buf2 == null)        return true;    if (buf1 == null || buf2 == null)        return false;    if (length1 != length2)        return false;    for (int i = 0; i < length1; i++) {        if (buf1.get(i + offset1) != buf2.get(i + offset2)) {            return false;        }    }    return true;}
0
private static final boolean equals(byte[] array1, int offset1, int length1, ByteBuffer buf, int offset2, int length2)
{    if (array1 == null && buf == null)        return true;    if (array1 == null || buf == null)        return false;    if (length1 != length2)        return false;    for (int i = 0; i < length1; i++) {        if (array1[i + offset1] != buf.get(i + offset2)) {            return false;        }    }    return true;}
0
private static final boolean equals(byte[] array1, int offset1, int length1, byte[] array2, int offset2, int length2)
{    if (array1 == null && array2 == null)        return true;    if (array1 == null || array2 == null)        return false;    if (length1 != length2)        return false;    if (array1 == array2 && offset1 == offset2)        return true;    for (int i = 0; i < length1; i++) {        if (array1[i + offset1] != array2[i + offset2]) {            return false;        }    }    return true;}
0
public PrimitiveConverter asPrimitiveConverter()
{    throw new ClassCastException("Expected instance of primitive converter but got \"" + getClass().getName() + "\"");}
0
public GroupConverter asGroupConverter()
{    throw new ClassCastException("Expected instance of group converter but got \"" + getClass().getName() + "\"");}
0
public boolean isPrimitive()
{    return false;}
0
public GroupConverter asGroupConverter()
{    return this;}
0
public boolean isPrimitive()
{    return true;}
0
public PrimitiveConverter asPrimitiveConverter()
{    return this;}
0
public boolean hasDictionarySupport()
{    return false;}
0
public void setDictionary(Dictionary dictionary)
{    throw new UnsupportedOperationException(getClass().getName());}
0
public void addValueFromDictionary(int dictionaryId)
{    throw new UnsupportedOperationException(getClass().getName());}
0
public void addBinary(Binary value)
{    throw new UnsupportedOperationException(getClass().getName());}
0
public void addBoolean(boolean value)
{    throw new UnsupportedOperationException(getClass().getName());}
0
public void addDouble(double value)
{    throw new UnsupportedOperationException(getClass().getName());}
0
public void addFloat(float value)
{    throw new UnsupportedOperationException(getClass().getName());}
0
public void addInt(int value)
{    throw new UnsupportedOperationException(getClass().getName());}
0
public void addLong(long value)
{    throw new UnsupportedOperationException(getClass().getName());}
0
public void flush()
{}
0
public void skipCurrentRecord()
{}
0
public T read()
{    readOneRecord();    return recordMaterializer.getCurrentRecord();}
0
protected void currentLevel(int currentLevel)
{    }
1
protected void log(String message)
{    }
1
protected final int getCaseId(int state, int currentLevel, int d, int nextR)
{    return caseLookup[state].getCase(currentLevel, d, nextR).getID();}
0
protected final void startMessage()
{        endField = null;        recordConsumer.startMessage();}
1
protected final void startGroup(String field, int index)
{    startField(field, index);        recordConsumer.startGroup();}
1
private void startField(String field, int index)
{        if (endField != null && index == endIndex) {                endField = null;    } else {        if (endField != null) {                        recordConsumer.endField(endField, endIndex);            endField = null;        }        recordConsumer.startField(field, index);    }}
1
protected final void addPrimitiveINT64(String field, int index, long value)
{    startField(field, index);        recordConsumer.addLong(value);    endField(field, index);}
1
private void endField(String field, int index)
{        if (endField != null) {        recordConsumer.endField(endField, endIndex);    }    endField = field;    endIndex = index;}
1
protected final void addPrimitiveBINARY(String field, int index, Binary value)
{    startField(field, index);        recordConsumer.addBinary(value);    endField(field, index);}
1
protected final void addPrimitiveINT32(String field, int index, int value)
{    startField(field, index);        recordConsumer.addInteger(value);    endField(field, index);}
1
protected final void endGroup(String field, int index)
{    if (endField != null) {                recordConsumer.endField(endField, endIndex);        endField = null;    }        recordConsumer.endGroup();    endField(field, index);}
1
protected final void endMessage()
{    if (endField != null) {                recordConsumer.endField(endField, endIndex);        endField = null;    }        recordConsumer.endMessage();}
1
protected void error(String message)
{    throw new ParquetDecodingException(message);}
0
 String[] getFieldPath()
{    return fieldPath;}
0
public String getFieldPath(int level)
{    return fieldPath[level];}
0
public int[] getIndexFieldPath()
{    return indexFieldPath;}
0
public int getIndexFieldPath(int level)
{    return indexFieldPath[level];}
0
public int getIndex()
{    return this.index;}
0
public String getName()
{    return name;}
0
 int getRepetitionLevel()
{    return repetitionLevel;}
0
 int getDefinitionLevel()
{    return definitionLevel;}
0
 void setRepetitionLevel(int repetitionLevel)
{    this.repetitionLevel = repetitionLevel;}
0
 void setDefinitionLevel(int definitionLevel)
{    this.definitionLevel = definitionLevel;}
0
 void setFieldPath(String[] fieldPath, int[] indexFieldPath)
{    this.fieldPath = fieldPath;    this.indexFieldPath = indexFieldPath;}
0
public Type getType()
{    return type;}
0
 void setLevels(int r, int d, String[] fieldPath, int[] indexFieldPath, List<ColumnIO> repetition, List<ColumnIO> path)
{    setRepetitionLevel(r);    setDefinitionLevel(d);    setFieldPath(fieldPath, indexFieldPath);}
0
public GroupColumnIO getParent()
{    return parent;}
0
 ColumnIO getParent(int r)
{    if (getRepetitionLevel() == r && getType().isRepetition(Repetition.REPEATED)) {        return this;    } else if (getParent() != null && getParent().getDefinitionLevel() >= r) {        return getParent().getParent(r);    } else {        throw new InvalidRecordException("no parent(" + r + ") for " + Arrays.toString(this.getFieldPath()));    }}
0
public String toString()
{    return this.getClass().getSimpleName() + " " + type.getName() + " r:" + repetitionLevel + " d:" + definitionLevel + " " + Arrays.toString(fieldPath);}
0
public void visit(MessageType messageType)
{    columnIO = new MessageColumnIO(requestedSchema, validating, createdBy);    visitChildren(columnIO, messageType, requestedSchema);    columnIO.setLevels();    columnIO.setLeaves(leaves);}
0
public void visit(GroupType groupType)
{    if (currentRequestedType.isPrimitive()) {        incompatibleSchema(groupType, currentRequestedType);    }    GroupColumnIO newIO = new GroupColumnIO(groupType, current, currentRequestedIndex);    current.add(newIO);    visitChildren(newIO, groupType, currentRequestedType.asGroupType());}
0
private void visitChildren(GroupColumnIO newIO, GroupType groupType, GroupType requestedGroupType)
{    GroupColumnIO oldIO = current;    current = newIO;    for (Type type : groupType.getFields()) {                if (requestedGroupType.containsField(type.getName())) {            currentRequestedIndex = requestedGroupType.getFieldIndex(type.getName());            currentRequestedType = requestedGroupType.getType(currentRequestedIndex);            if (currentRequestedType.getRepetition().isMoreRestrictiveThan(type.getRepetition())) {                incompatibleSchema(type, currentRequestedType);            }            type.accept(this);        }    }    current = oldIO;}
0
public void visit(PrimitiveType primitiveType)
{    if (!currentRequestedType.isPrimitive() || (this.strictTypeChecking && currentRequestedType.asPrimitiveType().getPrimitiveTypeName() != primitiveType.getPrimitiveTypeName())) {        incompatibleSchema(primitiveType, currentRequestedType);    }    PrimitiveColumnIO newIO = new PrimitiveColumnIO(primitiveType, current, currentRequestedIndex, leaves.size());    current.add(newIO);    leaves.add(newIO);}
0
private void incompatibleSchema(Type fileType, Type requestedType)
{    throw new ParquetDecodingException("The requested schema is not compatible with the file schema. incompatible types: " + requestedType + " != " + fileType);}
0
public MessageColumnIO getColumnIO()
{    return columnIO;}
0
public MessageColumnIO getColumnIO(MessageType requestedSchema, MessageType fileSchema)
{    return getColumnIO(requestedSchema, fileSchema, true);}
0
public MessageColumnIO getColumnIO(MessageType requestedSchema, MessageType fileSchema, boolean strict)
{    ColumnIOCreatorVisitor visitor = new ColumnIOCreatorVisitor(validating, requestedSchema, createdBy, strict);    fileSchema.accept(visitor);    return visitor.getColumnIO();}
0
public MessageColumnIO getColumnIO(MessageType schema)
{    return this.getColumnIO(schema, schema);}
0
public T read()
{    recordConsumer.start();    recordConsumer.end();    return recordMaterializer.getCurrentRecord();}
0
public T read()
{    skipToMatch();    if (recordsRead == recordCount) {        return null;    }    ++recordsRead;    return super.read();}
0
public boolean shouldSkipCurrentRecord()
{    return false;}
0
private void skipToMatch()
{    while (recordsRead < recordCount && !recordFilter.isMatch()) {        State currentState = getState(0);        do {            ColumnReader columnReader = currentState.column;                        if (columnReader.getCurrentDefinitionLevel() >= currentState.maxDefinitionLevel) {                columnReader.skip();            }            columnReader.consume();                        int nextR = currentState.maxRepetitionLevel == 0 ? 0 : columnReader.getCurrentRepetitionLevel();            currentState = currentState.getNextState(nextR);        } while (currentState != null);        ++recordsRead;    }}
0
 void add(ColumnIO child)
{    children.add(child);    childrenByName.put(child.getType().getName(), child);    ++childrenSize;}
0
 void setLevels(int r, int d, String[] fieldPath, int[] indexFieldPath, List<ColumnIO> repetition, List<ColumnIO> path)
{    super.setLevels(r, d, fieldPath, indexFieldPath, repetition, path);    for (ColumnIO child : this.children) {        String[] newFieldPath = Arrays.copyOf(fieldPath, fieldPath.length + 1);        int[] newIndexFieldPath = Arrays.copyOf(indexFieldPath, indexFieldPath.length + 1);        newFieldPath[fieldPath.length] = child.getType().getName();        newIndexFieldPath[indexFieldPath.length] = child.getIndex();        List<ColumnIO> newRepetition;        if (child.getType().isRepetition(REPEATED)) {            newRepetition = new ArrayList<ColumnIO>(repetition);            newRepetition.add(child);        } else {            newRepetition = repetition;        }        List<ColumnIO> newPath = new ArrayList<ColumnIO>(path);        newPath.add(child);        child.setLevels(        child.getType().isRepetition(REPEATED) ? r + 1 : r,         !child.getType().isRepetition(REQUIRED) ? d + 1 : d, newFieldPath, newIndexFieldPath, newRepetition, newPath);    }}
0
 List<String[]> getColumnNames()
{    ArrayList<String[]> result = new ArrayList<String[]>();    for (ColumnIO c : children) {        result.addAll(c.getColumnNames());    }    return result;}
0
 PrimitiveColumnIO getLast()
{    return children.get(children.size() - 1).getLast();}
0
 PrimitiveColumnIO getFirst()
{    return children.get(0).getFirst();}
0
public ColumnIO getChild(String name)
{    return childrenByName.get(name);}
0
public ColumnIO getChild(int fieldIndex)
{    try {        return children.get(fieldIndex);    } catch (IndexOutOfBoundsException e) {        throw new InvalidRecordException("could not get child " + fieldIndex + " from " + children, e);    }}
0
public int getChildrenCount()
{    return childrenSize;}
0
public List<String[]> getColumnNames()
{    return super.getColumnNames();}
0
public RecordReader<T> getRecordReader(PageReadStore columns, RecordMaterializer<T> recordMaterializer)
{    return getRecordReader(columns, recordMaterializer, FilterCompat.NOOP);}
0
public RecordReader<T> getRecordReader(PageReadStore columns, RecordMaterializer<T> recordMaterializer, UnboundRecordFilter filter)
{    return getRecordReader(columns, recordMaterializer, FilterCompat.get(filter));}
0
public RecordReader<T> getRecordReader(final PageReadStore columns, final RecordMaterializer<T> recordMaterializer, final Filter filter)
{    checkNotNull(columns, "columns");    checkNotNull(recordMaterializer, "recordMaterializer");    checkNotNull(filter, "filter");    if (leaves.isEmpty()) {        return new EmptyRecordReader<T>(recordMaterializer);    }    return filter.accept(new Visitor<RecordReader<T>>() {        @Override        public RecordReader<T> visit(FilterPredicateCompat filterPredicateCompat) {            FilterPredicate predicate = filterPredicateCompat.getFilterPredicate();            IncrementallyUpdatedFilterPredicateBuilder builder = new IncrementallyUpdatedFilterPredicateBuilder(leaves);            IncrementallyUpdatedFilterPredicate streamingPredicate = builder.build(predicate);            RecordMaterializer<T> filteringRecordMaterializer = new FilteringRecordMaterializer<T>(recordMaterializer, leaves, builder.getValueInspectorsByColumn(), streamingPredicate);            return new RecordReaderImplementation<T>(MessageColumnIO.this, filteringRecordMaterializer, validating, new ColumnReadStoreImpl(columns, filteringRecordMaterializer.getRootConverter(), getType(), createdBy));        }        @Override        public RecordReader<T> visit(UnboundRecordFilterCompat unboundRecordFilterCompat) {            return new FilteredRecordReader<T>(MessageColumnIO.this, recordMaterializer, validating, new ColumnReadStoreImpl(columns, recordMaterializer.getRootConverter(), getType(), createdBy), unboundRecordFilterCompat.getUnboundRecordFilter(), columns.getRowCount());        }        @Override        public RecordReader<T> visit(NoOpFilter noOpFilter) {            return new RecordReaderImplementation<T>(MessageColumnIO.this, recordMaterializer, validating, new ColumnReadStoreImpl(columns, recordMaterializer.getRootConverter(), getType(), createdBy));        }    });}
0
public RecordReader<T> visit(FilterPredicateCompat filterPredicateCompat)
{    FilterPredicate predicate = filterPredicateCompat.getFilterPredicate();    IncrementallyUpdatedFilterPredicateBuilder builder = new IncrementallyUpdatedFilterPredicateBuilder(leaves);    IncrementallyUpdatedFilterPredicate streamingPredicate = builder.build(predicate);    RecordMaterializer<T> filteringRecordMaterializer = new FilteringRecordMaterializer<T>(recordMaterializer, leaves, builder.getValueInspectorsByColumn(), streamingPredicate);    return new RecordReaderImplementation<T>(MessageColumnIO.this, filteringRecordMaterializer, validating, new ColumnReadStoreImpl(columns, filteringRecordMaterializer.getRootConverter(), getType(), createdBy));}
0
public RecordReader<T> visit(UnboundRecordFilterCompat unboundRecordFilterCompat)
{    return new FilteredRecordReader<T>(MessageColumnIO.this, recordMaterializer, validating, new ColumnReadStoreImpl(columns, recordMaterializer.getRootConverter(), getType(), createdBy), unboundRecordFilterCompat.getUnboundRecordFilter(), columns.getRowCount());}
0
public RecordReader<T> visit(NoOpFilter noOpFilter)
{    return new RecordReaderImplementation<T>(MessageColumnIO.this, recordMaterializer, validating, new ColumnReadStoreImpl(columns, recordMaterializer.getRootConverter(), getType(), createdBy));}
0
public String toString()
{    return "VistedIndex{" + "vistedIndexes=" + vistedIndexes + '}';}
0
public void reset(int fieldsCount)
{    this.vistedIndexes.clear(0, fieldsCount);}
0
public void markWritten(int i)
{    vistedIndexes.set(i);}
0
public boolean isWritten(int i)
{    return vistedIndexes.get(i);}
0
private void buildGroupToLeafWriterMap(PrimitiveColumnIO primitive, ColumnWriter writer)
{    GroupColumnIO parent = primitive.getParent();    do {        getLeafWriters(parent).add(writer);        parent = parent.getParent();    } while (parent != null);}
0
private List<ColumnWriter> getLeafWriters(GroupColumnIO group)
{    List<ColumnWriter> writers = groupToLeafWriter.get(group);    if (writers == null) {        writers = new ArrayList<ColumnWriter>();        groupToLeafWriter.put(group, writers);    }    return writers;}
0
private void printState()
{    if (DEBUG) {        log(currentLevel + ", " + fieldsWritten[currentLevel] + ": " + Arrays.toString(currentColumnIO.getFieldPath()) + " r:" + r[currentLevel]);        if (r[currentLevel] > currentColumnIO.getRepetitionLevel()) {                        throw new InvalidRecordException(r[currentLevel] + "(r) > " + currentColumnIO.getRepetitionLevel() + " ( schema r)");        }    }}
0
private void log(Object message, Object... parameters)
{    if (DEBUG) {        String indent = "";        for (int i = 0; i < currentLevel; ++i) {            indent += "  ";        }            }}
1
public void startMessage()
{    if (DEBUG)        log("< MESSAGE START >");    currentColumnIO = MessageColumnIO.this;    r[0] = 0;    int numberOfFieldsToVisit = ((GroupColumnIO) currentColumnIO).getChildrenCount();    fieldsWritten[0].reset(numberOfFieldsToVisit);    if (DEBUG)        printState();}
0
public void endMessage()
{    writeNullForMissingFieldsAtCurrentLevel();        if (columns.isColumnFlushNeeded()) {        flush();    }    columns.endRecord();    if (DEBUG)        log("< MESSAGE END >");    if (DEBUG)        printState();}
0
public void startField(String field, int index)
{    try {        if (DEBUG)            log("startField({}, {})", field, index);        currentColumnIO = ((GroupColumnIO) currentColumnIO).getChild(index);        emptyField = true;        if (DEBUG)            printState();    } catch (RuntimeException e) {        throw new ParquetEncodingException("error starting field " + field + " at " + index, e);    }}
0
public void endField(String field, int index)
{    if (DEBUG)        log("endField({}, {})", field, index);    currentColumnIO = currentColumnIO.getParent();    if (emptyField) {        throw new ParquetEncodingException("empty fields are illegal, the field should be ommited completely instead");    }    fieldsWritten[currentLevel].markWritten(index);    r[currentLevel] = currentLevel == 0 ? 0 : r[currentLevel - 1];    if (DEBUG)        printState();}
0
private void writeNullForMissingFieldsAtCurrentLevel()
{    int currentFieldsCount = ((GroupColumnIO) currentColumnIO).getChildrenCount();    for (int i = 0; i < currentFieldsCount; i++) {        if (!fieldsWritten[currentLevel].isWritten(i)) {            try {                ColumnIO undefinedField = ((GroupColumnIO) currentColumnIO).getChild(i);                int d = currentColumnIO.getDefinitionLevel();                if (DEBUG)                    log(Arrays.toString(undefinedField.getFieldPath()) + ".writeNull(" + r[currentLevel] + "," + d + ")");                writeNull(undefinedField, r[currentLevel], d);            } catch (RuntimeException e) {                throw new ParquetEncodingException("error while writing nulls for fields of indexes " + i + " . current index: " + fieldsWritten[currentLevel], e);            }        }    }}
0
private void writeNull(ColumnIO undefinedField, int r, int d)
{    if (undefinedField.getType().isPrimitive()) {        columnWriter[((PrimitiveColumnIO) undefinedField).getId()].writeNull(r, d);    } else {        GroupColumnIO groupColumnIO = (GroupColumnIO) undefinedField;                cacheNullForGroup(groupColumnIO, r);    }}
0
private void cacheNullForGroup(GroupColumnIO group, int r)
{    IntArrayList nulls = groupNullCache.get(group);    if (nulls == null) {        nulls = new IntArrayList();        groupNullCache.put(group, nulls);    }    nulls.add(r);}
0
private void writeNullToLeaves(GroupColumnIO group)
{    IntArrayList nullCache = groupNullCache.get(group);    if (nullCache == null || nullCache.isEmpty())        return;    int parentDefinitionLevel = group.getParent().getDefinitionLevel();    for (ColumnWriter leafWriter : groupToLeafWriter.get(group)) {        for (IntIterator iter = nullCache.iterator(); iter.hasNext(); ) {            int repetitionLevel = iter.nextInt();            leafWriter.writeNull(repetitionLevel, parentDefinitionLevel);        }    }    nullCache.clear();}
0
private void setRepetitionLevel()
{    r[currentLevel] = currentColumnIO.getRepetitionLevel();    if (DEBUG)        log("r: {}", r[currentLevel]);}
0
public void startGroup()
{    if (DEBUG)        log("startGroup()");    GroupColumnIO group = (GroupColumnIO) currentColumnIO;        if (hasNullCache(group)) {        flushCachedNulls(group);    }    ++currentLevel;    r[currentLevel] = r[currentLevel - 1];    int fieldsCount = ((GroupColumnIO) currentColumnIO).getChildrenCount();    fieldsWritten[currentLevel].reset(fieldsCount);    if (DEBUG)        printState();}
0
private boolean hasNullCache(GroupColumnIO group)
{    IntArrayList nulls = groupNullCache.get(group);    return nulls != null && !nulls.isEmpty();}
0
private void flushCachedNulls(GroupColumnIO group)
{        for (int i = 0; i < group.getChildrenCount(); i++) {        ColumnIO child = group.getChild(i);        if (child instanceof GroupColumnIO) {            flushCachedNulls((GroupColumnIO) child);        }    }        writeNullToLeaves(group);}
0
public void endGroup()
{    if (DEBUG)        log("endGroup()");    emptyField = false;    writeNullForMissingFieldsAtCurrentLevel();    --currentLevel;    setRepetitionLevel();    if (DEBUG)        printState();}
0
private ColumnWriter getColumnWriter()
{    return columnWriter[((PrimitiveColumnIO) currentColumnIO).getId()];}
0
public void addInteger(int value)
{    if (DEBUG)        log("addInt({})", value);    emptyField = false;    getColumnWriter().write(value, r[currentLevel], currentColumnIO.getDefinitionLevel());    setRepetitionLevel();    if (DEBUG)        printState();}
0
public void addLong(long value)
{    if (DEBUG)        log("addLong({})", value);    emptyField = false;    getColumnWriter().write(value, r[currentLevel], currentColumnIO.getDefinitionLevel());    setRepetitionLevel();    if (DEBUG)        printState();}
0
public void addBoolean(boolean value)
{    if (DEBUG)        log("addBoolean({})", value);    emptyField = false;    getColumnWriter().write(value, r[currentLevel], currentColumnIO.getDefinitionLevel());    setRepetitionLevel();    if (DEBUG)        printState();}
0
public void addBinary(Binary value)
{    if (DEBUG)        log("addBinary({} bytes)", value.length());    emptyField = false;    getColumnWriter().write(value, r[currentLevel], currentColumnIO.getDefinitionLevel());    setRepetitionLevel();    if (DEBUG)        printState();}
0
public void addFloat(float value)
{    if (DEBUG)        log("addFloat({})", value);    emptyField = false;    getColumnWriter().write(value, r[currentLevel], currentColumnIO.getDefinitionLevel());    setRepetitionLevel();    if (DEBUG)        printState();}
0
public void addDouble(double value)
{    if (DEBUG)        log("addDouble({})", value);    emptyField = false;    getColumnWriter().write(value, r[currentLevel], currentColumnIO.getDefinitionLevel());    setRepetitionLevel();    if (DEBUG)        printState();}
0
public void flush()
{    flushCachedNulls(MessageColumnIO.this);}
0
public RecordConsumer getRecordWriter(ColumnWriteStore columns)
{    RecordConsumer recordWriter = new MessageColumnIORecordConsumer(columns);    if (DEBUG)        recordWriter = new RecordConsumerLoggingWrapper(recordWriter);    return validating ? new ValidatingRecordConsumer(recordWriter, getType()) : recordWriter;}
0
 void setLevels()
{    setLevels(0, 0, new String[0], new int[0], Arrays.<ColumnIO>asList(this), Arrays.<ColumnIO>asList(this));}
0
 void setLeaves(List<PrimitiveColumnIO> leaves)
{    this.leaves = leaves;}
0
public List<PrimitiveColumnIO> getLeaves()
{    return this.leaves;}
0
public MessageType getType()
{    return (MessageType) super.getType();}
0
 void setLevels(int r, int d, String[] fieldPath, int[] fieldIndexPath, List<ColumnIO> repetition, List<ColumnIO> path)
{    super.setLevels(r, d, fieldPath, fieldIndexPath, repetition, path);    PrimitiveType type = getType().asPrimitiveType();    this.columnDescriptor = new ColumnDescriptor(fieldPath, type, getRepetitionLevel(), getDefinitionLevel());    this.path = path.toArray(new ColumnIO[path.size()]);}
0
 List<String[]> getColumnNames()
{    return Arrays.asList(new String[][] { getFieldPath() });}
0
public ColumnDescriptor getColumnDescriptor()
{    return columnDescriptor;}
0
public ColumnIO[] getPath()
{    return path;}
0
public boolean isLast(int r)
{    return getLast(r) == this;}
0
private PrimitiveColumnIO getLast(int r)
{    ColumnIO parent = getParent(r);    PrimitiveColumnIO last = parent.getLast();    return last;}
0
 PrimitiveColumnIO getLast()
{    return this;}
0
 PrimitiveColumnIO getFirst()
{    return this;}
0
public boolean isFirst(int r)
{    return getFirst(r) == this;}
0
private PrimitiveColumnIO getFirst(int r)
{    ColumnIO parent = getParent(r);    return parent.getFirst();}
0
public PrimitiveTypeName getPrimitive()
{    return getType().asPrimitiveType().getPrimitiveTypeName();}
0
public int getId()
{    return id;}
0
public void startField(String field, int index)
{    logOpen(field);    delegate.startField(field, index);}
0
private void logOpen(String field)
{    log("<{}>", field);}
0
private String indent()
{    StringBuilder result = new StringBuilder();    for (int i = 0; i < indent; i++) {        result.append("  ");    }    return result.toString();}
0
private void log(Object value, Object... parameters)
{    if (LOG.isDebugEnabled()) {            }}
1
public void startGroup()
{    ++indent;    log("<!-- start group -->");    delegate.startGroup();}
0
public void addInteger(int value)
{    log(value);    delegate.addInteger(value);}
0
public void addLong(long value)
{    log(value);    delegate.addLong(value);}
0
public void addBoolean(boolean value)
{    log(value);    delegate.addBoolean(value);}
0
public void addBinary(Binary value)
{    if (LOG.isDebugEnabled())        log(Arrays.toString(value.getBytesUnsafe()));    delegate.addBinary(value);}
0
public void addFloat(float value)
{    log(value);    delegate.addFloat(value);}
0
public void addDouble(double value)
{    log(value);    delegate.addDouble(value);}
0
public void flush()
{    log("<!-- flush -->");    delegate.flush();}
0
public void endGroup()
{    log("<!-- end group -->");    --indent;    delegate.endGroup();}
0
public void endField(String field, int index)
{    logClose(field);    delegate.endField(field, index);}
0
private void logClose(String field)
{    log("</{}>", field);}
0
public void startMessage()
{    log("<!-- start message -->");    delegate.startMessage();}
0
public void endMessage()
{    delegate.endMessage();    log("<!-- end message -->");}
0
public boolean shouldSkipCurrentRecord()
{    return false;}
0
public void setID(int id)
{    this.id = id;}
0
public int hashCode()
{    int hashCode = 17;    hashCode += 31 * startLevel;    hashCode += 31 * depth;    hashCode += 31 * nextLevel;    hashCode += 31 * nextState;    hashCode += 31 * (defined ? 0 : 1);    return hashCode;}
0
public boolean equals(Object obj)
{    if (obj instanceof Case) {        return equals((Case) obj);    }    return false;}
0
public boolean equals(Case other)
{    return other != null && startLevel == other.startLevel && depth == other.depth && nextLevel == other.nextLevel && nextState == other.nextState && ((defined && other.defined) || (!defined && !other.defined));}
0
public int getID()
{    return id;}
0
public int getStartLevel()
{    return startLevel;}
0
public int getDepth()
{    return depth;}
0
public int getNextLevel()
{    return nextLevel;}
0
public int getNextState()
{    return nextState;}
0
public boolean isGoingUp()
{    return goingUp;}
0
public boolean isGoingDown()
{    return goingDown;}
0
public boolean isDefined()
{    return defined;}
0
public String toString()
{    return "Case " + startLevel + " -> " + depth + " -> " + nextLevel + "; goto sate_" + getNextState();}
0
public int getDepth(int definitionLevel)
{    return definitionLevelToDepth[definitionLevel];}
0
public List<Case> getDefinedCases()
{    return definedCases;}
0
public List<Case> getUndefinedCases()
{    return undefinedCases;}
0
public Case getCase(int currentLevel, int d, int nextR)
{    return caseLookup[currentLevel][d][nextR];}
0
public State getNextState(int nextR)
{    return nextState[nextR];}
0
public int compare(Case o1, Case o2)
{    return o1.id - o2.id;}
0
private RecordConsumer validator(RecordConsumer recordConsumer, boolean validating, MessageType schema)
{    return validating ? new ValidatingRecordConsumer(recordConsumer, schema) : recordConsumer;}
0
private RecordConsumer wrap(RecordConsumer recordConsumer)
{    if (LOG.isDebugEnabled()) {        return new RecordConsumerLoggingWrapper(recordConsumer);    }    return recordConsumer;}
0
public T read()
{    int currentLevel = 0;    recordRootConverter.start();    State currentState = states[0];    do {        ColumnReader columnReader = currentState.column;        int d = columnReader.getCurrentDefinitionLevel();                int depth = currentState.definitionLevelToDepth[d];        for (; currentLevel <= depth; ++currentLevel) {            currentState.groupConverterPath[currentLevel].start();        }                if (d >= currentState.maxDefinitionLevel) {                        columnReader.writeCurrentValueToConverter();        }        columnReader.consume();        int nextR = currentState.maxRepetitionLevel == 0 ? 0 : columnReader.getCurrentRepetitionLevel();                int next = currentState.nextLevel[nextR];        for (; currentLevel > next; currentLevel--) {            currentState.groupConverterPath[currentLevel - 1].end();        }        currentState = currentState.nextState[nextR];    } while (currentState != null);    recordRootConverter.end();    T record = recordMaterializer.getCurrentRecord();    shouldSkipCurrentRecord = record == null;    if (shouldSkipCurrentRecord) {        recordMaterializer.skipCurrentRecord();    }    return record;}
0
public boolean shouldSkipCurrentRecord()
{    return shouldSkipCurrentRecord;}
0
private static void log(String string)
{    }
1
 int getNextReader(int current, int nextRepetitionLevel)
{    State nextState = states[current].nextState[nextRepetitionLevel];    return nextState == null ? states.length : nextState.id;}
0
 int getNextLevel(int current, int nextRepetitionLevel)
{    return states[current].nextLevel[nextRepetitionLevel];}
0
private int getCommonParentLevel(String[] previous, String[] next)
{    int i = 0;    while (i < Math.min(previous.length, next.length) && previous[i].equals(next[i])) {        ++i;    }    return i;}
0
protected int getStateCount()
{    return states.length;}
0
protected State getState(int i)
{    return states[i];}
0
protected RecordMaterializer<T> getMaterializer()
{    return recordMaterializer;}
0
protected Converter getRecordConsumer()
{    return recordRootConverter;}
0
protected Iterable<ColumnReader> getColumnReaders()
{        return Arrays.asList(columnReaders);}
0
public void startMessage()
{    previousField.push(-1);    delegate.startMessage();}
0
public void endMessage()
{    delegate.endMessage();    validateMissingFields(types.peek().asGroupType().getFieldCount());    previousField.pop();}
0
public void startField(String field, int index)
{    if (index <= previousField.peek()) {        throw new InvalidRecordException("fields must be added in order " + field + " index " + index + " is before previous field " + previousField.peek());    }    validateMissingFields(index);    fields.push(index);    fieldValueCount.push(0);    delegate.startField(field, index);}
0
private void validateMissingFields(int index)
{    for (int i = previousField.peek() + 1; i < index; i++) {        Type type = types.peek().asGroupType().getType(i);        if (type.isRepetition(Repetition.REQUIRED)) {            throw new InvalidRecordException("required field is missing " + type);        }    }}
0
public void endField(String field, int index)
{    delegate.endField(field, index);    fieldValueCount.pop();    previousField.push(fields.pop());}
0
public void startGroup()
{    previousField.push(-1);    types.push(types.peek().asGroupType().getType(fields.peek()));    delegate.startGroup();}
0
public void endGroup()
{    delegate.endGroup();    validateMissingFields(types.peek().asGroupType().getFieldCount());    types.pop();    previousField.pop();}
0
public void flush()
{    delegate.flush();}
0
private void validate(PrimitiveTypeName p)
{    Type currentType = types.peek().asGroupType().getType(fields.peek());    int c = fieldValueCount.pop() + 1;    fieldValueCount.push(c);        switch(currentType.getRepetition()) {        case OPTIONAL:        case REQUIRED:            if (c > 1) {                throw new InvalidRecordException("repeated value when the type is not repeated in " + currentType);            }            break;        case REPEATED:            break;        default:            throw new InvalidRecordException("unknown repetition " + currentType.getRepetition() + " in " + currentType);    }    if (!currentType.isPrimitive() || currentType.asPrimitiveType().getPrimitiveTypeName() != p) {        throw new InvalidRecordException("expected type " + p + " but got " + currentType);    }}
1
private void validate(PrimitiveTypeName... ptypes)
{    Type currentType = types.peek().asGroupType().getType(fields.peek());    int c = fieldValueCount.pop() + 1;    fieldValueCount.push(c);    if (LOG.isDebugEnabled())            switch(currentType.getRepetition()) {        case OPTIONAL:        case REQUIRED:            if (c > 1) {                throw new InvalidRecordException("repeated value when the type is not repeated in " + currentType);            }            break;        case REPEATED:            break;        default:            throw new InvalidRecordException("unknown repetition " + currentType.getRepetition() + " in " + currentType);    }    if (!currentType.isPrimitive()) {        throw new InvalidRecordException("expected type in " + Arrays.toString(ptypes) + " but got " + currentType);    }    for (PrimitiveTypeName p : ptypes) {        if (currentType.asPrimitiveType().getPrimitiveTypeName() == p) {                        return;        }    }    throw new InvalidRecordException("expected type in " + Arrays.toString(ptypes) + " but got " + currentType);}
1
public void addInteger(int value)
{    validate(INT32);    delegate.addInteger(value);}
0
public void addLong(long value)
{    validate(INT64);    delegate.addLong(value);}
0
public void addBoolean(boolean value)
{    validate(BOOLEAN);    delegate.addBoolean(value);}
0
public void addBinary(Binary value)
{    validate(BINARY, INT96, FIXED_LEN_BYTE_ARRAY);    delegate.addBinary(value);}
0
public void addFloat(float value)
{    validate(FLOAT);    delegate.addFloat(value);}
0
public void addDouble(double value)
{    validate(DOUBLE);    delegate.addDouble(value);}
0
public static ColumnOrder undefined()
{    return UNDEFINED_COLUMN_ORDER;}
0
public static ColumnOrder typeDefined()
{    return TYPE_DEFINED_COLUMN_ORDER;}
0
public ColumnOrderName getColumnOrderName()
{    return columnOrderName;}
0
public boolean equals(Object obj)
{    if (obj instanceof ColumnOrder) {        return columnOrderName == ((ColumnOrder) obj).columnOrderName;    }    return false;}
0
public int hashCode()
{    return columnOrderName.hashCode();}
0
public String toString()
{    return columnOrderName.toString();}
0
private static GroupType listWrapper(Repetition repetition, String alias, LogicalTypeAnnotation logicalTypeAnnotation, Type nested)
{    if (!nested.isRepetition(Repetition.REPEATED)) {        throw new IllegalArgumentException("Nested type should be repeated: " + nested);    }    return new GroupType(repetition, alias, logicalTypeAnnotation, nested);}
0
public static GroupType mapType(Repetition repetition, String alias, Type keyType, Type valueType)
{    return mapType(repetition, alias, "map", keyType, valueType);}
0
public static GroupType stringKeyMapType(Repetition repetition, String alias, String mapAlias, Type valueType)
{    return mapType(repetition, alias, mapAlias, new PrimitiveType(Repetition.REQUIRED, PrimitiveTypeName.BINARY, "key", stringType()), valueType);}
0
public static GroupType stringKeyMapType(Repetition repetition, String alias, Type valueType)
{    return stringKeyMapType(repetition, alias, "map", valueType);}
0
public static GroupType mapType(Repetition repetition, String alias, String mapAlias, Type keyType, Type valueType)
{        if (valueType == null) {        return listWrapper(repetition, alias, LogicalTypeAnnotation.mapType(), new GroupType(Repetition.REPEATED, mapAlias, LogicalTypeAnnotation.MapKeyValueTypeAnnotation.getInstance(), keyType));    } else {        if (!valueType.getName().equals("value")) {            throw new RuntimeException(valueType.getName() + " should be value");        }        return listWrapper(repetition, alias, LogicalTypeAnnotation.mapType(), new GroupType(Repetition.REPEATED, mapAlias, LogicalTypeAnnotation.MapKeyValueTypeAnnotation.getInstance(), keyType, valueType));    }}
0
public static GroupType listType(Repetition repetition, String alias, Type nestedType)
{    return listWrapper(repetition, alias, LogicalTypeAnnotation.listType(), nestedType);}
0
public static GroupType listOfElements(Repetition listRepetition, String name, Type elementType)
{    Preconditions.checkArgument(elementType.getName().equals(ELEMENT_NAME), "List element type must be named 'element'");    return listWrapper(listRepetition, name, LogicalTypeAnnotation.listType(), new GroupType(Repetition.REPEATED, "list", elementType));}
0
public int getPrecision()
{    return precision;}
0
public int getScale()
{    return scale;}
0
public boolean equals(Object o)
{    if (this == o)        return true;    if (o == null || getClass() != o.getClass())        return false;    DecimalMetadata that = (DecimalMetadata) o;    if (precision != that.precision)        return false;    if (scale != that.scale)        return false;    return true;}
0
public int hashCode()
{    int result = precision;    result = 31 * result + scale;    return result;}
0
public GroupType withId(int id)
{    return new GroupType(getRepetition(), getName(), getLogicalTypeAnnotation(), fields, new ID(id));}
0
public GroupType withNewFields(List<Type> newFields)
{    return new GroupType(getRepetition(), getName(), getLogicalTypeAnnotation(), newFields, getId());}
0
public GroupType withNewFields(Type... newFields)
{    return withNewFields(asList(newFields));}
0
public String getFieldName(int index)
{    return fields.get(index).getName();}
0
public boolean containsField(String name)
{    return indexByName.containsKey(name);}
0
public int getFieldIndex(String name)
{    if (!indexByName.containsKey(name)) {        throw new InvalidRecordException(name + " not found in " + this);    }    return indexByName.get(name);}
0
public List<Type> getFields()
{    return fields;}
0
public int getFieldCount()
{    return fields.size();}
0
public boolean isPrimitive()
{    return false;}
0
public Type getType(String fieldName)
{    return getType(getFieldIndex(fieldName));}
0
public Type getType(int index)
{    return fields.get(index);}
0
 void membersDisplayString(StringBuilder sb, String indent)
{    for (Type field : fields) {        field.writeToStringBuilder(sb, indent);        if (field.isPrimitive()) {            sb.append(";");        }        sb.append("\n");    }}
0
public void writeToStringBuilder(StringBuilder sb, String indent)
{    sb.append(indent).append(getRepetition().name().toLowerCase(Locale.ENGLISH)).append(" group ").append(getName()).append(getLogicalTypeAnnotation() == null ? "" : " (" + getLogicalTypeAnnotation().toString() + ")").append(getId() == null ? "" : " = " + getId()).append(" {\n");    membersDisplayString(sb, indent + "  ");    sb.append(indent).append("}");}
0
public void accept(TypeVisitor visitor)
{    visitor.visit(this);}
0
protected int typeHashCode()
{    return hashCode();}
0
protected boolean typeEquals(Type other)
{    return equals(other);}
0
public int hashCode()
{    return Objects.hash(getLogicalTypeAnnotation(), getFields());}
0
protected boolean equals(Type otherType)
{    return !otherType.isPrimitive() && super.equals(otherType) && Objects.equals(getLogicalTypeAnnotation(), otherType.getLogicalTypeAnnotation()) && getFields().equals(otherType.asGroupType().getFields());}
0
protected int getMaxRepetitionLevel(String[] path, int depth)
{    int myVal = isRepetition(Repetition.REPEATED) ? 1 : 0;    if (depth == path.length) {        return myVal;    }    return myVal + getType(path[depth]).getMaxRepetitionLevel(path, depth + 1);}
0
protected int getMaxDefinitionLevel(String[] path, int depth)
{    int myVal = !isRepetition(Repetition.REQUIRED) ? 1 : 0;    if (depth == path.length) {        return myVal;    }    return myVal + getType(path[depth]).getMaxDefinitionLevel(path, depth + 1);}
0
protected Type getType(String[] path, int depth)
{    if (depth == path.length) {        return this;    }    return getType(path[depth]).getType(path, depth + 1);}
0
protected boolean containsPath(String[] path, int depth)
{    if (depth == path.length) {        return false;    }    return containsField(path[depth]) && getType(path[depth]).containsPath(path, depth + 1);}
0
protected List<String[]> getPaths(int depth)
{    List<String[]> result = new ArrayList<String[]>();    for (Type field : fields) {        List<String[]> paths = field.getPaths(depth + 1);        for (String[] path : paths) {            path[depth] = field.getName();            result.add(path);        }    }    return result;}
0
 void checkContains(Type subType)
{    super.checkContains(subType);    checkGroupContains(subType);}
0
 void checkGroupContains(Type subType)
{    if (subType.isPrimitive()) {        throw new InvalidRecordException(subType + " found: expected " + this);    }    List<Type> fields = subType.asGroupType().getFields();    for (Type otherType : fields) {        Type thisType = this.getType(otherType.getName());        thisType.checkContains(otherType);    }}
0
 T convert(List<GroupType> path, TypeConverter<T> converter)
{    List<GroupType> childrenPath = new ArrayList<GroupType>(path);    childrenPath.add(this);    final List<T> children = convertChildren(childrenPath, converter);    return converter.convertGroupType(path, this, children);}
0
protected List<T> convertChildren(List<GroupType> path, TypeConverter<T> converter)
{    List<T> children = new ArrayList<T>(fields.size());    for (Type field : fields) {        children.add(field.convert(path, converter));    }    return children;}
0
protected Type union(Type toMerge)
{    return union(toMerge, true);}
0
protected Type union(Type toMerge, boolean strict)
{    if (toMerge.isPrimitive()) {        throw new IncompatibleSchemaModificationException("can not merge primitive type " + toMerge + " into group type " + this);    }    return new GroupType(toMerge.getRepetition(), getName(), toMerge.getLogicalTypeAnnotation(), mergeFields(toMerge.asGroupType()), getId());}
0
 List<Type> mergeFields(GroupType toMerge)
{    return mergeFields(toMerge, true);}
0
 List<Type> mergeFields(GroupType toMerge, boolean strict)
{    List<Type> newFields = new ArrayList<Type>();        for (Type type : this.getFields()) {        Type merged;        if (toMerge.containsField(type.getName())) {            Type fieldToMerge = toMerge.getType(type.getName());            if (type.getLogicalTypeAnnotation() != null && !type.getLogicalTypeAnnotation().equals(fieldToMerge.getLogicalTypeAnnotation())) {                throw new IncompatibleSchemaModificationException("cannot merge logical type " + fieldToMerge.getLogicalTypeAnnotation() + " into " + type.getLogicalTypeAnnotation());            }            merged = type.union(fieldToMerge, strict);        } else {            merged = type;        }        newFields.add(merged);    }        for (Type type : toMerge.getFields()) {        if (!this.containsField(type.getName())) {            newFields.add(type);        }    }    return newFields;}
0
protected LogicalTypeAnnotation fromString(List<String> params)
{    return mapType();}
0
protected LogicalTypeAnnotation fromString(List<String> params)
{    return listType();}
0
protected LogicalTypeAnnotation fromString(List<String> params)
{    return stringType();}
0
protected LogicalTypeAnnotation fromString(List<String> params)
{    return MapKeyValueTypeAnnotation.getInstance();}
0
protected LogicalTypeAnnotation fromString(List<String> params)
{    return enumType();}
0
protected LogicalTypeAnnotation fromString(List<String> params)
{    if (params.size() != 2) {        throw new RuntimeException("Expecting 2 parameters for decimal logical type, got " + params.size());    }    return decimalType(Integer.valueOf(params.get(1)), Integer.valueOf(params.get(0)));}
0
protected LogicalTypeAnnotation fromString(List<String> params)
{    return dateType();}
0
protected LogicalTypeAnnotation fromString(List<String> params)
{    if (params.size() != 2) {        throw new RuntimeException("Expecting 2 parameters for time logical type, got " + params.size());    }    return timeType(Boolean.parseBoolean(params.get(1)), TimeUnit.valueOf(params.get(0)));}
0
protected LogicalTypeAnnotation fromString(List<String> params)
{    if (params.size() != 2) {        throw new RuntimeException("Expecting 2 parameters for timestamp logical type, got " + params.size());    }    return timestampType(Boolean.parseBoolean(params.get(1)), TimeUnit.valueOf(params.get(0)));}
0
protected LogicalTypeAnnotation fromString(List<String> params)
{    if (params.size() != 2) {        throw new RuntimeException("Expecting 2 parameters for integer logical type, got " + params.size());    }    return intType(Integer.valueOf(params.get(0)), Boolean.parseBoolean(params.get(1)));}
0
protected LogicalTypeAnnotation fromString(List<String> params)
{    return jsonType();}
0
protected LogicalTypeAnnotation fromString(List<String> params)
{    return bsonType();}
0
protected LogicalTypeAnnotation fromString(List<String> params)
{    return IntervalLogicalTypeAnnotation.getInstance();}
0
 String typeParametersAsString()
{    return "";}
0
 boolean isValidColumnOrder(ColumnOrder columnOrder)
{    return columnOrder.getColumnOrderName() == UNDEFINED || columnOrder.getColumnOrderName() == TYPE_DEFINED_ORDER;}
0
public String toString()
{    StringBuilder sb = new StringBuilder();    sb.append(getType());    sb.append(typeParametersAsString());    return sb.toString();}
0
 PrimitiveStringifier valueStringifier(PrimitiveType primitiveType)
{    throw new UnsupportedOperationException("Stringifier is not supported for the logical type: " + this);}
0
public static LogicalTypeAnnotation fromOriginalType(OriginalType originalType, DecimalMetadata decimalMetadata)
{    if (originalType == null) {        return null;    }    switch(originalType) {        case UTF8:            return stringType();        case MAP:            return mapType();        case DECIMAL:            int scale = (decimalMetadata == null ? 0 : decimalMetadata.getScale());            int precision = (decimalMetadata == null ? 0 : decimalMetadata.getPrecision());            return decimalType(scale, precision);        case LIST:            return listType();        case DATE:            return dateType();        case INTERVAL:            return IntervalLogicalTypeAnnotation.getInstance();        case TIMESTAMP_MILLIS:            return timestampType(true, LogicalTypeAnnotation.TimeUnit.MILLIS);        case TIMESTAMP_MICROS:            return timestampType(true, LogicalTypeAnnotation.TimeUnit.MICROS);        case TIME_MILLIS:            return timeType(true, LogicalTypeAnnotation.TimeUnit.MILLIS);        case TIME_MICROS:            return timeType(true, LogicalTypeAnnotation.TimeUnit.MICROS);        case UINT_8:            return intType(8, false);        case UINT_16:            return intType(16, false);        case UINT_32:            return intType(32, false);        case UINT_64:            return intType(64, false);        case INT_8:            return intType(8, true);        case INT_16:            return intType(16, true);        case INT_32:            return intType(32, true);        case INT_64:            return intType(64, true);        case ENUM:            return enumType();        case JSON:            return jsonType();        case BSON:            return bsonType();        case MAP_KEY_VALUE:            return MapKeyValueTypeAnnotation.getInstance();        default:            throw new RuntimeException("Can't convert original type to logical type, unknown original type " + originalType);    }}
0
public static StringLogicalTypeAnnotation stringType()
{    return StringLogicalTypeAnnotation.INSTANCE;}
0
public static MapLogicalTypeAnnotation mapType()
{    return MapLogicalTypeAnnotation.INSTANCE;}
0
public static ListLogicalTypeAnnotation listType()
{    return ListLogicalTypeAnnotation.INSTANCE;}
0
public static EnumLogicalTypeAnnotation enumType()
{    return EnumLogicalTypeAnnotation.INSTANCE;}
0
public static DecimalLogicalTypeAnnotation decimalType(final int scale, final int precision)
{    return new DecimalLogicalTypeAnnotation(scale, precision);}
0
public static DateLogicalTypeAnnotation dateType()
{    return DateLogicalTypeAnnotation.INSTANCE;}
0
public static TimeLogicalTypeAnnotation timeType(final boolean isAdjustedToUTC, final TimeUnit unit)
{    return new TimeLogicalTypeAnnotation(isAdjustedToUTC, unit);}
0
public static TimestampLogicalTypeAnnotation timestampType(final boolean isAdjustedToUTC, final TimeUnit unit)
{    return new TimestampLogicalTypeAnnotation(isAdjustedToUTC, unit);}
0
public static IntLogicalTypeAnnotation intType(final int bitWidth, final boolean isSigned)
{    Preconditions.checkArgument(bitWidth == 8 || bitWidth == 16 || bitWidth == 32 || bitWidth == 64, "Invalid bit width for integer logical type, " + bitWidth + " is not allowed, " + "valid bit width values: 8, 16, 32, 64");    return new IntLogicalTypeAnnotation(bitWidth, isSigned);}
0
public static JsonLogicalTypeAnnotation jsonType()
{    return JsonLogicalTypeAnnotation.INSTANCE;}
0
public static BsonLogicalTypeAnnotation bsonType()
{    return BsonLogicalTypeAnnotation.INSTANCE;}
0
public OriginalType toOriginalType()
{    return OriginalType.UTF8;}
0
public Optional<T> accept(LogicalTypeAnnotationVisitor<T> logicalTypeAnnotationVisitor)
{    return logicalTypeAnnotationVisitor.visit(this);}
0
 LogicalTypeToken getType()
{    return LogicalTypeToken.STRING;}
0
public boolean equals(Object obj)
{    return obj instanceof StringLogicalTypeAnnotation;}
0
public int hashCode()
{        return getClass().hashCode();}
0
 PrimitiveStringifier valueStringifier(PrimitiveType primitiveType)
{    return PrimitiveStringifier.UTF8_STRINGIFIER;}
0
public OriginalType toOriginalType()
{    return OriginalType.MAP;}
0
public Optional<T> accept(LogicalTypeAnnotationVisitor<T> logicalTypeAnnotationVisitor)
{    return logicalTypeAnnotationVisitor.visit(this);}
0
 LogicalTypeToken getType()
{    return LogicalTypeToken.MAP;}
0
public boolean equals(Object obj)
{    return obj instanceof MapLogicalTypeAnnotation;}
0
public int hashCode()
{        return getClass().hashCode();}
0
public OriginalType toOriginalType()
{    return OriginalType.LIST;}
0
public Optional<T> accept(LogicalTypeAnnotationVisitor<T> logicalTypeAnnotationVisitor)
{    return logicalTypeAnnotationVisitor.visit(this);}
0
 LogicalTypeToken getType()
{    return LogicalTypeToken.LIST;}
0
public boolean equals(Object obj)
{    return obj instanceof ListLogicalTypeAnnotation;}
0
public int hashCode()
{        return getClass().hashCode();}
0
public OriginalType toOriginalType()
{    return OriginalType.ENUM;}
0
public Optional<T> accept(LogicalTypeAnnotationVisitor<T> logicalTypeAnnotationVisitor)
{    return logicalTypeAnnotationVisitor.visit(this);}
0
 LogicalTypeToken getType()
{    return LogicalTypeToken.ENUM;}
0
public boolean equals(Object obj)
{    return obj instanceof EnumLogicalTypeAnnotation;}
0
public int hashCode()
{        return getClass().hashCode();}
0
 PrimitiveStringifier valueStringifier(PrimitiveType primitiveType)
{    return PrimitiveStringifier.UTF8_STRINGIFIER;}
0
public int getPrecision()
{    return precision;}
0
public int getScale()
{    return scale;}
0
public OriginalType toOriginalType()
{    return OriginalType.DECIMAL;}
0
public Optional<T> accept(LogicalTypeAnnotationVisitor<T> logicalTypeAnnotationVisitor)
{    return logicalTypeAnnotationVisitor.visit(this);}
0
 LogicalTypeToken getType()
{    return LogicalTypeToken.DECIMAL;}
0
protected String typeParametersAsString()
{    StringBuilder sb = new StringBuilder();    sb.append("(");    sb.append(precision);    sb.append(",");    sb.append(scale);    sb.append(")");    return sb.toString();}
0
public boolean equals(Object obj)
{    if (!(obj instanceof DecimalLogicalTypeAnnotation)) {        return false;    }    DecimalLogicalTypeAnnotation other = (DecimalLogicalTypeAnnotation) obj;    return scale == other.scale && precision == other.precision;}
0
public int hashCode()
{    return Objects.hash(scale, precision);}
0
 PrimitiveStringifier valueStringifier(PrimitiveType primitiveType)
{    return stringifier;}
0
public OriginalType toOriginalType()
{    return OriginalType.DATE;}
0
public Optional<T> accept(LogicalTypeAnnotationVisitor<T> logicalTypeAnnotationVisitor)
{    return logicalTypeAnnotationVisitor.visit(this);}
0
 LogicalTypeToken getType()
{    return LogicalTypeToken.DATE;}
0
public boolean equals(Object obj)
{    return obj instanceof DateLogicalTypeAnnotation;}
0
public int hashCode()
{        return getClass().hashCode();}
0
 PrimitiveStringifier valueStringifier(PrimitiveType primitiveType)
{    return PrimitiveStringifier.DATE_STRINGIFIER;}
0
public OriginalType toOriginalType()
{    if (!isAdjustedToUTC) {        return null;    }    switch(unit) {        case MILLIS:            return OriginalType.TIME_MILLIS;        case MICROS:            return OriginalType.TIME_MICROS;        default:            return null;    }}
0
public Optional<T> accept(LogicalTypeAnnotationVisitor<T> logicalTypeAnnotationVisitor)
{    return logicalTypeAnnotationVisitor.visit(this);}
0
 LogicalTypeToken getType()
{    return LogicalTypeToken.TIME;}
0
protected String typeParametersAsString()
{    StringBuilder sb = new StringBuilder();    sb.append("(");    sb.append(unit);    sb.append(",");    sb.append(isAdjustedToUTC);    sb.append(")");    return sb.toString();}
0
public TimeUnit getUnit()
{    return unit;}
0
public boolean isAdjustedToUTC()
{    return isAdjustedToUTC;}
0
public boolean equals(Object obj)
{    if (!(obj instanceof TimeLogicalTypeAnnotation)) {        return false;    }    TimeLogicalTypeAnnotation other = (TimeLogicalTypeAnnotation) obj;    return isAdjustedToUTC == other.isAdjustedToUTC && unit == other.unit;}
0
public int hashCode()
{    return Objects.hash(isAdjustedToUTC, unit);}
0
 PrimitiveStringifier valueStringifier(PrimitiveType primitiveType)
{    switch(unit) {        case MICROS:        case MILLIS:            return isAdjustedToUTC ? TIME_UTC_STRINGIFIER : TIME_STRINGIFIER;        case NANOS:            return isAdjustedToUTC ? TIME_NANOS_UTC_STRINGIFIER : TIME_NANOS_STRINGIFIER;        default:            return super.valueStringifier(primitiveType);    }}
0
public OriginalType toOriginalType()
{    if (!isAdjustedToUTC) {        return null;    }    switch(unit) {        case MILLIS:            return OriginalType.TIMESTAMP_MILLIS;        case MICROS:            return OriginalType.TIMESTAMP_MICROS;        default:            return null;    }}
0
public Optional<T> accept(LogicalTypeAnnotationVisitor<T> logicalTypeAnnotationVisitor)
{    return logicalTypeAnnotationVisitor.visit(this);}
0
 LogicalTypeToken getType()
{    return LogicalTypeToken.TIMESTAMP;}
0
protected String typeParametersAsString()
{    StringBuilder sb = new StringBuilder();    sb.append("(");    sb.append(unit);    sb.append(",");    sb.append(isAdjustedToUTC);    sb.append(")");    return sb.toString();}
0
public TimeUnit getUnit()
{    return unit;}
0
public boolean isAdjustedToUTC()
{    return isAdjustedToUTC;}
0
public boolean equals(Object obj)
{    if (!(obj instanceof TimestampLogicalTypeAnnotation)) {        return false;    }    TimestampLogicalTypeAnnotation other = (TimestampLogicalTypeAnnotation) obj;    return (isAdjustedToUTC == other.isAdjustedToUTC) && (unit == other.unit);}
0
public int hashCode()
{    return Objects.hash(isAdjustedToUTC, unit);}
0
 PrimitiveStringifier valueStringifier(PrimitiveType primitiveType)
{    switch(unit) {        case MICROS:            return isAdjustedToUTC ? TIMESTAMP_MICROS_UTC_STRINGIFIER : TIMESTAMP_MICROS_STRINGIFIER;        case MILLIS:            return isAdjustedToUTC ? TIMESTAMP_MILLIS_UTC_STRINGIFIER : TIMESTAMP_MILLIS_STRINGIFIER;        case NANOS:            return isAdjustedToUTC ? TIMESTAMP_NANOS_UTC_STRINGIFIER : TIMESTAMP_NANOS_STRINGIFIER;        default:            return super.valueStringifier(primitiveType);    }}
0
public OriginalType toOriginalType()
{    switch(bitWidth) {        case 8:            return isSigned ? OriginalType.INT_8 : OriginalType.UINT_8;        case 16:            return isSigned ? OriginalType.INT_16 : OriginalType.UINT_16;        case 32:            return isSigned ? OriginalType.INT_32 : OriginalType.UINT_32;        case 64:            return isSigned ? OriginalType.INT_64 : OriginalType.UINT_64;        default:            return null;    }}
0
public Optional<T> accept(LogicalTypeAnnotationVisitor<T> logicalTypeAnnotationVisitor)
{    return logicalTypeAnnotationVisitor.visit(this);}
0
 LogicalTypeToken getType()
{    return LogicalTypeToken.INTEGER;}
0
protected String typeParametersAsString()
{    StringBuilder sb = new StringBuilder();    sb.append("(");    sb.append(bitWidth);    sb.append(",");    sb.append(isSigned);    sb.append(")");    return sb.toString();}
0
public int getBitWidth()
{    return bitWidth;}
0
public boolean isSigned()
{    return isSigned;}
0
public boolean equals(Object obj)
{    if (!(obj instanceof IntLogicalTypeAnnotation)) {        return false;    }    IntLogicalTypeAnnotation other = (IntLogicalTypeAnnotation) obj;    return (bitWidth == other.bitWidth) && (isSigned == other.isSigned);}
0
public int hashCode()
{    return Objects.hash(bitWidth, isSigned);}
0
 PrimitiveStringifier valueStringifier(PrimitiveType primitiveType)
{    return isSigned ? PrimitiveStringifier.DEFAULT_STRINGIFIER : PrimitiveStringifier.UNSIGNED_STRINGIFIER;}
0
public OriginalType toOriginalType()
{    return OriginalType.JSON;}
0
public Optional<T> accept(LogicalTypeAnnotationVisitor<T> logicalTypeAnnotationVisitor)
{    return logicalTypeAnnotationVisitor.visit(this);}
0
 LogicalTypeToken getType()
{    return LogicalTypeToken.JSON;}
0
public boolean equals(Object obj)
{    return obj instanceof JsonLogicalTypeAnnotation;}
0
public int hashCode()
{        return getClass().hashCode();}
0
 PrimitiveStringifier valueStringifier(PrimitiveType primitiveType)
{    return PrimitiveStringifier.UTF8_STRINGIFIER;}
0
public OriginalType toOriginalType()
{    return OriginalType.BSON;}
0
public Optional<T> accept(LogicalTypeAnnotationVisitor<T> logicalTypeAnnotationVisitor)
{    return logicalTypeAnnotationVisitor.visit(this);}
0
 LogicalTypeToken getType()
{    return LogicalTypeToken.BSON;}
0
public boolean equals(Object obj)
{    return obj instanceof BsonLogicalTypeAnnotation;}
0
public int hashCode()
{        return getClass().hashCode();}
0
 PrimitiveStringifier valueStringifier(PrimitiveType primitiveType)
{    return PrimitiveStringifier.DEFAULT_STRINGIFIER;}
0
public static LogicalTypeAnnotation getInstance()
{    return INSTANCE;}
0
public OriginalType toOriginalType()
{    return OriginalType.INTERVAL;}
0
public Optional<T> accept(LogicalTypeAnnotationVisitor<T> logicalTypeAnnotationVisitor)
{    return logicalTypeAnnotationVisitor.visit(this);}
0
 LogicalTypeToken getType()
{    return LogicalTypeToken.INTERVAL;}
0
public boolean equals(Object obj)
{    return obj instanceof IntervalLogicalTypeAnnotation;}
0
public int hashCode()
{        return getClass().hashCode();}
0
 PrimitiveStringifier valueStringifier(PrimitiveType primitiveType)
{    return PrimitiveStringifier.INTERVAL_STRINGIFIER;}
0
 boolean isValidColumnOrder(ColumnOrder columnOrder)
{    return columnOrder.getColumnOrderName() == UNDEFINED;}
0
public static MapKeyValueTypeAnnotation getInstance()
{    return INSTANCE;}
0
public OriginalType toOriginalType()
{    return OriginalType.MAP_KEY_VALUE;}
0
public Optional<T> accept(LogicalTypeAnnotationVisitor<T> logicalTypeAnnotationVisitor)
{    return logicalTypeAnnotationVisitor.visit(this);}
0
 LogicalTypeToken getType()
{    return LogicalTypeToken.MAP_KEY_VALUE;}
0
public boolean equals(Object obj)
{    return obj instanceof MapKeyValueTypeAnnotation;}
0
public int hashCode()
{        return getClass().hashCode();}
0
 Optional<T> visit(StringLogicalTypeAnnotation stringLogicalType)
{    return empty();}
0
 Optional<T> visit(MapLogicalTypeAnnotation mapLogicalType)
{    return empty();}
0
 Optional<T> visit(ListLogicalTypeAnnotation listLogicalType)
{    return empty();}
0
 Optional<T> visit(EnumLogicalTypeAnnotation enumLogicalType)
{    return empty();}
0
 Optional<T> visit(DecimalLogicalTypeAnnotation decimalLogicalType)
{    return empty();}
0
 Optional<T> visit(DateLogicalTypeAnnotation dateLogicalType)
{    return empty();}
0
 Optional<T> visit(TimeLogicalTypeAnnotation timeLogicalType)
{    return empty();}
0
 Optional<T> visit(TimestampLogicalTypeAnnotation timestampLogicalType)
{    return empty();}
0
 Optional<T> visit(IntLogicalTypeAnnotation intLogicalType)
{    return empty();}
0
 Optional<T> visit(JsonLogicalTypeAnnotation jsonLogicalType)
{    return empty();}
0
 Optional<T> visit(BsonLogicalTypeAnnotation bsonLogicalType)
{    return empty();}
0
 Optional<T> visit(IntervalLogicalTypeAnnotation intervalLogicalType)
{    return empty();}
0
 Optional<T> visit(MapKeyValueTypeAnnotation mapKeyValueLogicalType)
{    return empty();}
0
public void accept(TypeVisitor visitor)
{    visitor.visit(this);}
0
public void writeToStringBuilder(StringBuilder sb, String indent)
{    sb.append("message ").append(getName()).append(getLogicalTypeAnnotation() == null ? "" : " (" + getLogicalTypeAnnotation().toString() + ")").append(" {\n");    membersDisplayString(sb, "  ");    sb.append("}\n");}
0
public int getMaxRepetitionLevel(String... path)
{    return getMaxRepetitionLevel(path, 0) - 1;}
0
public int getMaxDefinitionLevel(String... path)
{    return getMaxDefinitionLevel(path, 0) - 1;}
0
public Type getType(String... path)
{    return getType(path, 0);}
0
public ColumnDescriptor getColumnDescription(String[] path)
{    int maxRep = getMaxRepetitionLevel(path);    int maxDef = getMaxDefinitionLevel(path);    PrimitiveType type = getType(path).asPrimitiveType();    return new ColumnDescriptor(path, type, maxRep, maxDef);}
0
public List<String[]> getPaths()
{    return this.getPaths(0);}
0
public List<ColumnDescriptor> getColumns()
{    List<String[]> paths = this.getPaths(0);    List<ColumnDescriptor> columns = new ArrayList<ColumnDescriptor>(paths.size());    for (String[] path : paths) {                PrimitiveType primitiveType = getType(path).asPrimitiveType();        columns.add(new ColumnDescriptor(path, primitiveType, getMaxRepetitionLevel(path), getMaxDefinitionLevel(path)));    }    return columns;}
0
public void checkContains(Type subType)
{    if (!(subType instanceof MessageType)) {        throw new InvalidRecordException(subType + " found: expected " + this);    }    checkGroupContains(subType);}
0
public T convertWith(TypeConverter<T> converter)
{    final ArrayList<GroupType> path = new ArrayList<GroupType>();    path.add(this);    return converter.convertMessageType(this, convertChildren(path, converter));}
0
public boolean containsPath(String[] path)
{    return containsPath(path, 0);}
0
public MessageType union(MessageType toMerge)
{    return union(toMerge, true);}
0
public MessageType union(MessageType toMerge, boolean strict)
{    return new MessageType(this.getName(), mergeFields(toMerge, strict));}
0
public String nextToken()
{    while (st.hasMoreTokens()) {        String t = st.nextToken();        if (t.equals("\n")) {            ++line;            currentLine.setLength(0);        } else {            currentLine.append(t);        }        if (!isWhitespace(t)) {            return t;        }    }    throw new IllegalArgumentException("unexpected end of schema");}
0
private boolean isWhitespace(String t)
{    return t.equals(" ") || t.equals("\t") || t.equals("\n");}
0
public String getLocationString()
{    return "line " + line + ": " + currentLine.toString();}
0
public static MessageType parseMessageType(String input)
{    return parse(input);}
0
private static MessageType parse(String schemaString)
{    Tokenizer st = new Tokenizer(schemaString, " ;{}()\n\t");    Types.MessageTypeBuilder builder = Types.buildMessage();    String t = st.nextToken();    check(t, "message", "start with 'message'", st);    String name = st.nextToken();    addGroupTypeFields(st.nextToken(), st, builder);    return builder.named(name);}
0
private static void addGroupTypeFields(String t, Tokenizer st, Types.GroupBuilder builder)
{    check(t, "{", "start of message", st);    while (!(t = st.nextToken()).equals("}")) {        addType(t, st, builder);    }}
0
private static void addType(String t, Tokenizer st, Types.GroupBuilder builder)
{    Repetition repetition = asRepetition(t, st);        String type = st.nextToken();    if ("group".equalsIgnoreCase(type)) {        addGroupType(st, repetition, builder);    } else {        addPrimitiveType(st, asPrimitive(type, st), repetition, builder);    }}
0
private static void addGroupType(Tokenizer st, Repetition r, GroupBuilder<?> builder)
{    GroupBuilder<?> childBuilder = builder.group(r);    String t;    String name = st.nextToken();        t = st.nextToken();    OriginalType originalType = null;    if (t.equalsIgnoreCase("(")) {        originalType = OriginalType.valueOf(st.nextToken());        childBuilder.as(originalType);        check(st.nextToken(), ")", "original type ended by )", st);        t = st.nextToken();    }    if (t.equals("=")) {        childBuilder.id(Integer.parseInt(st.nextToken()));        t = st.nextToken();    }    try {        addGroupTypeFields(t, st, childBuilder);    } catch (IllegalArgumentException e) {        throw new IllegalArgumentException("problem reading type: type = group, name = " + name + ", original type = " + originalType, e);    }    childBuilder.named(name);}
0
private static void addPrimitiveType(Tokenizer st, PrimitiveTypeName type, Repetition r, Types.GroupBuilder<?> builder)
{    PrimitiveBuilder<?> childBuilder = builder.primitive(type, r);    String t;    if (type == PrimitiveTypeName.FIXED_LEN_BYTE_ARRAY) {        t = st.nextToken();                if (!t.equalsIgnoreCase("(")) {            throw new IllegalArgumentException("expecting (length) for field of type fixed_len_byte_array");        }        childBuilder.length(Integer.parseInt(st.nextToken()));        check(st.nextToken(), ")", "type length ended by )", st);    }    String name = st.nextToken();        t = st.nextToken();    OriginalType originalType = null;    if (t.equalsIgnoreCase("(")) {        t = st.nextToken();        if (isLogicalType(t)) {            LogicalTypeAnnotation.LogicalTypeToken logicalType = LogicalTypeAnnotation.LogicalTypeToken.valueOf(t);            t = st.nextToken();            List<String> tokens = new ArrayList<>();            if ("(".equals(t)) {                while (!")".equals(t)) {                    if (!(",".equals(t) || "(".equals(t) || ")".equals(t))) {                        tokens.add(t);                    }                    t = st.nextToken();                }                t = st.nextToken();            }            LogicalTypeAnnotation logicalTypeAnnotation = logicalType.fromString(tokens);            childBuilder.as(logicalTypeAnnotation);        } else {                        originalType = OriginalType.valueOf(t);            childBuilder.as(originalType);            if (OriginalType.DECIMAL == originalType) {                t = st.nextToken();                                if (t.equalsIgnoreCase("(")) {                    childBuilder.precision(Integer.parseInt(st.nextToken()));                    t = st.nextToken();                    if (t.equalsIgnoreCase(",")) {                        childBuilder.scale(Integer.parseInt(st.nextToken()));                        t = st.nextToken();                    }                    check(t, ")", "decimal type ended by )", st);                    t = st.nextToken();                }            } else {                t = st.nextToken();            }        }        check(t, ")", "logical type ended by )", st);        t = st.nextToken();    }    if (t.equals("=")) {        childBuilder.id(Integer.parseInt(st.nextToken()));        t = st.nextToken();    }    check(t, ";", "field ended by ';'", st);    try {        childBuilder.named(name);    } catch (IllegalArgumentException e) {        throw new IllegalArgumentException("problem reading type: type = " + type + ", name = " + name + ", original type = " + originalType, e);    }}
0
private static boolean isLogicalType(String t)
{    return Arrays.stream(LogicalTypeAnnotation.LogicalTypeToken.values()).anyMatch((type) -> type.name().equals(t));}
0
private static PrimitiveTypeName asPrimitive(String t, Tokenizer st)
{    try {        return PrimitiveTypeName.valueOf(t.toUpperCase(Locale.ENGLISH));    } catch (IllegalArgumentException e) {        throw new IllegalArgumentException("expected one of " + Arrays.toString(PrimitiveTypeName.values()) + " got " + t + " at " + st.getLocationString(), e);    }}
0
private static Repetition asRepetition(String t, Tokenizer st)
{    try {        return Repetition.valueOf(t.toUpperCase(Locale.ENGLISH));    } catch (IllegalArgumentException e) {        throw new IllegalArgumentException("expected one of " + Arrays.toString(Repetition.values()) + " got " + t + " at " + st.getLocationString(), e);    }}
0
private static void check(String t, String expected, String message, Tokenizer tokenizer)
{    if (!t.equalsIgnoreCase(expected)) {        throw new IllegalArgumentException(message + ": expected '" + expected + "' but got '" + t + "' at " + tokenizer.getLocationString());    }}
0
public int compare(boolean b1, boolean b2)
{    throw new UnsupportedOperationException("compare(boolean, boolean) was called on a non-boolean comparator: " + toString());}
0
public int compare(int i1, int i2)
{    throw new UnsupportedOperationException("compare(int, int) was called on a non-int comparator: " + toString());}
0
public int compare(long l1, long l2)
{    throw new UnsupportedOperationException("compare(long, long) was called on a non-long comparator: " + toString());}
0
public int compare(float f1, float f2)
{    throw new UnsupportedOperationException("compare(float, float) was called on a non-float comparator: " + toString());}
0
public int compare(double d1, double d2)
{    throw new UnsupportedOperationException("compare(double, double) was called on a non-double comparator: " + toString());}
0
public final int compare(T o1, T o2)
{    if (o1 == null) {        return o2 == null ? 0 : -1;    }    return o2 == null ? 1 : compareNotNulls(o1, o2);}
0
 int compareNotNulls(Boolean o1, Boolean o2)
{    return compare(o1.booleanValue(), o2.booleanValue());}
0
public int compare(boolean b1, boolean b2)
{    return Boolean.compare(b1, b2);}
0
public String toString()
{    return "BOOLEAN_COMPARATOR";}
0
 int compareNotNulls(Integer o1, Integer o2)
{    return compare(o1.intValue(), o2.intValue());}
0
public int compare(int i1, int i2)
{    return Integer.compare(i1, i2);}
0
public String toString()
{    return "SIGNED_INT32_COMPARATOR";}
0
public int compare(int i1, int i2)
{        return Integer.compare(i1 ^ Integer.MIN_VALUE, i2 ^ Integer.MIN_VALUE);}
0
public String toString()
{    return "UNSIGNED_INT32_COMPARATOR";}
0
 int compareNotNulls(Long o1, Long o2)
{    return compare(o1.longValue(), o2.longValue());}
0
public int compare(long l1, long l2)
{    return Long.compare(l1, l2);}
0
public String toString()
{    return "SIGNED_INT64_COMPARATOR";}
0
public int compare(long l1, long l2)
{        return Long.compare(l1 ^ Long.MIN_VALUE, l2 ^ Long.MIN_VALUE);}
0
public String toString()
{    return "UNSIGNED_INT64_COMPARATOR";}
0
 int compareNotNulls(Float o1, Float o2)
{    return compare(o1.floatValue(), o2.floatValue());}
0
public int compare(float f1, float f2)
{    return Float.compare(f1, f2);}
0
public String toString()
{    return "FLOAT_COMPARATOR";}
0
 int compareNotNulls(Double o1, Double o2)
{    return compare(o1.doubleValue(), o2.doubleValue());}
0
public int compare(double d1, double d2)
{    return Double.compare(d1, d2);}
0
public String toString()
{    return "DOUBLE_COMPARATOR";}
0
 int compareNotNulls(Binary o1, Binary o2)
{    return compare(o1.toByteBuffer(), o2.toByteBuffer());}
0
 final int toUnsigned(byte b)
{    return b & 0xFF;}
0
 int compare(ByteBuffer b1, ByteBuffer b2)
{    int l1 = b1.remaining();    int l2 = b2.remaining();    int p1 = b1.position();    int p2 = b2.position();    int minL = Math.min(l1, l2);    for (int i = 0; i < minL; ++i) {        int result = unsignedCompare(b1.get(p1 + i), b2.get(p2 + i));        if (result != 0) {            return result;        }    }    return l1 - l2;}
0
private int unsignedCompare(byte b1, byte b2)
{    return toUnsigned(b1) - toUnsigned(b2);}
0
public String toString()
{    return "UNSIGNED_LEXICOGRAPHICAL_BINARY_COMPARATOR";}
0
 int compare(ByteBuffer b1, ByteBuffer b2)
{    int l1 = b1.remaining();    int l2 = b2.remaining();    int p1 = b1.position();    int p2 = b2.position();    boolean isNegative1 = l1 > 0 && b1.get(p1) < 0;    boolean isNegative2 = l2 > 0 && b2.get(p2) < 0;    if (isNegative1 != isNegative2) {        return isNegative1 ? -1 : 1;    }    int result = 0;        if (l1 < l2) {        int lengthDiff = l2 - l1;        result = -compareWithPadding(lengthDiff, b2, p2, isNegative1 ? NEGATIVE_PADDING : POSITIVE_PADDING);        p2 += lengthDiff;    } else if (l1 > l2) {        int lengthDiff = l1 - l2;        result = compareWithPadding(lengthDiff, b1, p1, isNegative2 ? NEGATIVE_PADDING : POSITIVE_PADDING);        p1 += lengthDiff;    }        if (result == 0) {        result = compare(Math.min(l1, l2), b1, p1, b2, p2);    }    return result;}
0
private int compareWithPadding(int length, ByteBuffer b, int p, int paddingByte)
{    for (int i = p, n = p + length; i < n; ++i) {        int result = toUnsigned(b.get(i)) - paddingByte;        if (result != 0) {            return result;        }    }    return 0;}
0
private int compare(int length, ByteBuffer b1, int p1, ByteBuffer b2, int p2)
{    for (int i = 0; i < length; ++i) {        int result = toUnsigned(b1.get(p1 + i)) - toUnsigned(b2.get(p2 + i));        if (result != 0) {            return result;        }    }    return 0;}
0
public String toString()
{    return "BINARY_AS_SIGNED_INTEGER_COMPARATOR";}
0
public final String toString()
{    return name;}
0
public String stringify(boolean value)
{    throw new UnsupportedOperationException("stringify(boolean) was called on a non-boolean stringifier: " + toString());}
0
public String stringify(int value)
{    throw new UnsupportedOperationException("stringify(int) was called on a non-int stringifier: " + toString());}
0
public String stringify(long value)
{    throw new UnsupportedOperationException("stringify(long) was called on a non-long stringifier: " + toString());}
0
public String stringify(float value)
{    throw new UnsupportedOperationException("stringify(float) was called on a non-float stringifier: " + toString());}
0
public String stringify(double value)
{    throw new UnsupportedOperationException("stringify(double) was called on a non-double stringifier: " + toString());}
0
public String stringify(Binary value)
{    throw new UnsupportedOperationException("stringify(Binary) was called on a non-Binary stringifier: " + toString());}
0
public final String stringify(Binary value)
{    return value == null ? BINARY_NULL : stringifyNotNull(value);}
0
public String stringify(boolean value)
{    return Boolean.toString(value);}
0
public String stringify(int value)
{    return Integer.toString(value);}
0
public String stringify(long value)
{    return Long.toString(value);}
0
public String stringify(float value)
{    return Float.toString(value);}
0
public String stringify(double value)
{    return Double.toString(value);}
0
 String stringifyNotNull(Binary value)
{    ByteBuffer buffer = value.toByteBuffer();    StringBuilder builder = new StringBuilder(2 + buffer.remaining() * 2);    builder.append(BINARY_HEXA_PREFIX);    for (int i = buffer.position(), n = buffer.limit(); i < n; ++i) {        byte b = buffer.get(i);        builder.append(digits[(b >>> 4) & 0x0F]);        builder.append(digits[b & 0x0F]);    }    return builder.toString();}
0
public String stringify(int value)
{    return Long.toString(value & INT_MASK);}
0
public String stringify(long value)
{    if (value == 0) {                return "0";    } else if (value > 0) {        return Long.toString(value);    } else {        char[] buf = new char[64];        int i = buf.length;                                long top = value >>> 32;        long bot = (value & INT_MASK) + ((top % 10) << 32);        top /= 10;        while ((bot > 0) || (top > 0)) {            buf[--i] = Character.forDigit((int) (bot % 10), 10);            bot = (bot / 10) + ((top % 10) << 32);            top /= 10;        }                return new String(buf, i, buf.length - i);    }}
0
 String stringifyNotNull(Binary value)
{    return value.toStringUsingUTF8();}
0
 String stringifyNotNull(Binary value)
{    if (value.length() != 12) {        return BINARY_INVALID;    }    ByteBuffer buffer = value.toByteBuffer().order(ByteOrder.LITTLE_ENDIAN);    int pos = buffer.position();    String months = UNSIGNED_STRINGIFIER.stringify(buffer.getInt(pos));    String days = UNSIGNED_STRINGIFIER.stringify(buffer.getInt(pos + 4));    String millis = UNSIGNED_STRINGIFIER.stringify(buffer.getInt(pos + 8));    return "interval(" + months + " months, " + days + " days, " + millis + " millis)";}
0
public String stringify(int value)
{    return toFormattedString(getInstant(value));}
0
public String stringify(long value)
{    return toFormattedString(getInstant(value));}
0
private String toFormattedString(Instant instant)
{    return formatter.format(instant);}
0
 Instant getInstant(int value)
{        super.stringify(value);    return null;}
0
 Instant getInstant(long value)
{        super.stringify(value);    return null;}
0
 Instant getInstant(int value)
{    return Instant.ofEpochMilli(TimeUnit.DAYS.toMillis(value));}
0
 Instant getInstant(long value)
{    return Instant.ofEpochMilli(value);}
0
 Instant getInstant(long value)
{    return Instant.ofEpochSecond(MICROSECONDS.toSeconds(value), MICROSECONDS.toNanos(value % SECONDS.toMicros(1)));}
0
 Instant getInstant(long value)
{    return Instant.ofEpochSecond(NANOSECONDS.toSeconds(value), NANOSECONDS.toNanos(value % SECONDS.toNanos(1)));}
0
 Instant getInstant(long value)
{    return Instant.ofEpochMilli(value);}
0
 Instant getInstant(long value)
{    return Instant.ofEpochSecond(MICROSECONDS.toSeconds(value), MICROSECONDS.toNanos(value % SECONDS.toMicros(1)));}
0
 Instant getInstant(long value)
{    return Instant.ofEpochSecond(NANOSECONDS.toSeconds(value), NANOSECONDS.toNanos(value % SECONDS.toNanos(1)));}
0
protected String toTimeString(long duration, TimeUnit unit)
{    String additionalFormat = (unit == MILLISECONDS ? "3d" : unit == MICROSECONDS ? "6d" : "9d");    String timeZone = withZone ? "+0000" : "";    String format = "%02d:%02d:%02d.%0" + additionalFormat + timeZone;    return String.format(format, unit.toHours(duration), convert(duration, unit, MINUTES, HOURS), convert(duration, unit, SECONDS, MINUTES), convert(duration, unit, unit, SECONDS));}
0
protected long convert(long duration, TimeUnit from, TimeUnit to, TimeUnit higher)
{    return Math.abs(to.convert(duration, from) % to.convert(1, higher));}
0
public String stringify(int millis)
{    return toTimeString(millis, MILLISECONDS);}
0
public String stringify(long micros)
{    return toTimeString(micros, MICROSECONDS);}
0
public String stringify(long nanos)
{    return toTimeString(nanos, NANOSECONDS);}
0
public String stringify(int millis)
{    return toTimeString(millis, MILLISECONDS);}
0
public String stringify(long micros)
{    return toTimeString(micros, MICROSECONDS);}
0
public String stringify(long nanos)
{    return toTimeString(nanos, NANOSECONDS);}
0
 static PrimitiveStringifier createDecimalStringifier(final int scale)
{    return new BinaryStringifierBase("DECIMAL_STRINGIFIER(scale: " + scale + ")") {        @Override        public String stringify(int value) {            return stringifyWithScale(BigInteger.valueOf(value));        }        @Override        public String stringify(long value) {            return stringifyWithScale(BigInteger.valueOf(value));        }        @Override        String stringifyNotNull(Binary value) {            try {                return stringifyWithScale(new BigInteger(value.getBytesUnsafe()));            } catch (NumberFormatException e) {                return BINARY_INVALID;            }        }        private String stringifyWithScale(BigInteger i) {            return new BigDecimal(i, scale).toString();        }    };}
0
public String stringify(int value)
{    return stringifyWithScale(BigInteger.valueOf(value));}
0
public String stringify(long value)
{    return stringifyWithScale(BigInteger.valueOf(value));}
0
 String stringifyNotNull(Binary value)
{    try {        return stringifyWithScale(new BigInteger(value.getBytesUnsafe()));    } catch (NumberFormatException e) {        return BINARY_INVALID;    }}
0
private String stringifyWithScale(BigInteger i)
{    return new BigDecimal(i, scale).toString();}
0
public String toString(ColumnReader columnReader)
{    return String.valueOf(columnReader.getLong());}
0
public void addValueToRecordConsumer(RecordConsumer recordConsumer, ColumnReader columnReader)
{    recordConsumer.addLong(columnReader.getLong());}
0
public void addValueToPrimitiveConverter(PrimitiveConverter primitiveConverter, ColumnReader columnReader)
{    primitiveConverter.addLong(columnReader.getLong());}
0
public T convert(PrimitiveTypeNameConverter<T, E> converter) throws E
{    return converter.convertINT64(this);}
0
 PrimitiveComparator<?> comparator(LogicalTypeAnnotation logicalType)
{    if (logicalType == null) {        return PrimitiveComparator.SIGNED_INT64_COMPARATOR;    }    return logicalType.accept(new LogicalTypeAnnotation.LogicalTypeAnnotationVisitor<PrimitiveComparator>() {        @Override        public Optional<PrimitiveComparator> visit(LogicalTypeAnnotation.IntLogicalTypeAnnotation intLogicalType) {            return intLogicalType.isSigned() ? of(PrimitiveComparator.SIGNED_INT64_COMPARATOR) : of(PrimitiveComparator.UNSIGNED_INT64_COMPARATOR);        }        @Override        public Optional<PrimitiveComparator> visit(LogicalTypeAnnotation.DecimalLogicalTypeAnnotation decimalLogicalType) {            return of(PrimitiveComparator.SIGNED_INT64_COMPARATOR);        }        @Override        public Optional<PrimitiveComparator> visit(LogicalTypeAnnotation.TimeLogicalTypeAnnotation timeLogicalType) {            return of(PrimitiveComparator.SIGNED_INT64_COMPARATOR);        }        @Override        public Optional<PrimitiveComparator> visit(LogicalTypeAnnotation.TimestampLogicalTypeAnnotation timestampLogicalType) {            return of(PrimitiveComparator.SIGNED_INT64_COMPARATOR);        }    }).orElseThrow(() -> new ShouldNeverHappenException("No comparator logic implemented for INT64 logical type: " + logicalType));}
0
public Optional<PrimitiveComparator> visit(LogicalTypeAnnotation.IntLogicalTypeAnnotation intLogicalType)
{    return intLogicalType.isSigned() ? of(PrimitiveComparator.SIGNED_INT64_COMPARATOR) : of(PrimitiveComparator.UNSIGNED_INT64_COMPARATOR);}
0
public Optional<PrimitiveComparator> visit(LogicalTypeAnnotation.DecimalLogicalTypeAnnotation decimalLogicalType)
{    return of(PrimitiveComparator.SIGNED_INT64_COMPARATOR);}
0
public Optional<PrimitiveComparator> visit(LogicalTypeAnnotation.TimeLogicalTypeAnnotation timeLogicalType)
{    return of(PrimitiveComparator.SIGNED_INT64_COMPARATOR);}
0
public Optional<PrimitiveComparator> visit(LogicalTypeAnnotation.TimestampLogicalTypeAnnotation timestampLogicalType)
{    return of(PrimitiveComparator.SIGNED_INT64_COMPARATOR);}
0
public String toString(ColumnReader columnReader)
{    return String.valueOf(columnReader.getInteger());}
0
public void addValueToRecordConsumer(RecordConsumer recordConsumer, ColumnReader columnReader)
{    recordConsumer.addInteger(columnReader.getInteger());}
0
public void addValueToPrimitiveConverter(PrimitiveConverter primitiveConverter, ColumnReader columnReader)
{    primitiveConverter.addInt(columnReader.getInteger());}
0
public T convert(PrimitiveTypeNameConverter<T, E> converter) throws E
{    return converter.convertINT32(this);}
0
 PrimitiveComparator<?> comparator(LogicalTypeAnnotation logicalType)
{    if (logicalType == null) {        return PrimitiveComparator.SIGNED_INT32_COMPARATOR;    }    return logicalType.accept(new LogicalTypeAnnotation.LogicalTypeAnnotationVisitor<PrimitiveComparator>() {        @Override        public Optional<PrimitiveComparator> visit(LogicalTypeAnnotation.IntLogicalTypeAnnotation intLogicalType) {            if (intLogicalType.getBitWidth() == 64) {                return empty();            }            return intLogicalType.isSigned() ? of(PrimitiveComparator.SIGNED_INT32_COMPARATOR) : of(PrimitiveComparator.UNSIGNED_INT32_COMPARATOR);        }        @Override        public Optional<PrimitiveComparator> visit(LogicalTypeAnnotation.DecimalLogicalTypeAnnotation decimalLogicalType) {            return of(PrimitiveComparator.SIGNED_INT32_COMPARATOR);        }        @Override        public Optional<PrimitiveComparator> visit(LogicalTypeAnnotation.DateLogicalTypeAnnotation dateLogicalType) {            return of(PrimitiveComparator.SIGNED_INT32_COMPARATOR);        }        @Override        public Optional<PrimitiveComparator> visit(LogicalTypeAnnotation.TimeLogicalTypeAnnotation timeLogicalType) {            if (timeLogicalType.getUnit() == MILLIS) {                return of(PrimitiveComparator.SIGNED_INT32_COMPARATOR);            }            return empty();        }    }).orElseThrow(() -> new ShouldNeverHappenException("No comparator logic implemented for INT32 logical type: " + logicalType));}
0
public Optional<PrimitiveComparator> visit(LogicalTypeAnnotation.IntLogicalTypeAnnotation intLogicalType)
{    if (intLogicalType.getBitWidth() == 64) {        return empty();    }    return intLogicalType.isSigned() ? of(PrimitiveComparator.SIGNED_INT32_COMPARATOR) : of(PrimitiveComparator.UNSIGNED_INT32_COMPARATOR);}
0
public Optional<PrimitiveComparator> visit(LogicalTypeAnnotation.DecimalLogicalTypeAnnotation decimalLogicalType)
{    return of(PrimitiveComparator.SIGNED_INT32_COMPARATOR);}
0
public Optional<PrimitiveComparator> visit(LogicalTypeAnnotation.DateLogicalTypeAnnotation dateLogicalType)
{    return of(PrimitiveComparator.SIGNED_INT32_COMPARATOR);}
0
public Optional<PrimitiveComparator> visit(LogicalTypeAnnotation.TimeLogicalTypeAnnotation timeLogicalType)
{    if (timeLogicalType.getUnit() == MILLIS) {        return of(PrimitiveComparator.SIGNED_INT32_COMPARATOR);    }    return empty();}
0
public String toString(ColumnReader columnReader)
{    return String.valueOf(columnReader.getBoolean());}
0
public void addValueToRecordConsumer(RecordConsumer recordConsumer, ColumnReader columnReader)
{    recordConsumer.addBoolean(columnReader.getBoolean());}
0
public void addValueToPrimitiveConverter(PrimitiveConverter primitiveConverter, ColumnReader columnReader)
{    primitiveConverter.addBoolean(columnReader.getBoolean());}
0
public T convert(PrimitiveTypeNameConverter<T, E> converter) throws E
{    return converter.convertBOOLEAN(this);}
0
 PrimitiveComparator<?> comparator(LogicalTypeAnnotation logicalType)
{    return PrimitiveComparator.BOOLEAN_COMPARATOR;}
0
public String toString(ColumnReader columnReader)
{    return String.valueOf(columnReader.getBinary());}
0
public void addValueToRecordConsumer(RecordConsumer recordConsumer, ColumnReader columnReader)
{    recordConsumer.addBinary(columnReader.getBinary());}
0
public void addValueToPrimitiveConverter(PrimitiveConverter primitiveConverter, ColumnReader columnReader)
{    primitiveConverter.addBinary(columnReader.getBinary());}
0
public T convert(PrimitiveTypeNameConverter<T, E> converter) throws E
{    return converter.convertBINARY(this);}
0
 PrimitiveComparator<?> comparator(LogicalTypeAnnotation logicalType)
{    if (logicalType == null) {        return PrimitiveComparator.UNSIGNED_LEXICOGRAPHICAL_BINARY_COMPARATOR;    }    return logicalType.accept(new LogicalTypeAnnotation.LogicalTypeAnnotationVisitor<PrimitiveComparator>() {        @Override        public Optional<PrimitiveComparator> visit(LogicalTypeAnnotation.DecimalLogicalTypeAnnotation decimalLogicalType) {            return of(PrimitiveComparator.BINARY_AS_SIGNED_INTEGER_COMPARATOR);        }        @Override        public Optional<PrimitiveComparator> visit(LogicalTypeAnnotation.StringLogicalTypeAnnotation stringLogicalType) {            return of(PrimitiveComparator.UNSIGNED_LEXICOGRAPHICAL_BINARY_COMPARATOR);        }        @Override        public Optional<PrimitiveComparator> visit(LogicalTypeAnnotation.EnumLogicalTypeAnnotation enumLogicalType) {            return of(PrimitiveComparator.UNSIGNED_LEXICOGRAPHICAL_BINARY_COMPARATOR);        }        @Override        public Optional<PrimitiveComparator> visit(LogicalTypeAnnotation.JsonLogicalTypeAnnotation jsonLogicalType) {            return of(PrimitiveComparator.UNSIGNED_LEXICOGRAPHICAL_BINARY_COMPARATOR);        }        @Override        public Optional<PrimitiveComparator> visit(LogicalTypeAnnotation.BsonLogicalTypeAnnotation bsonLogicalType) {            return of(PrimitiveComparator.UNSIGNED_LEXICOGRAPHICAL_BINARY_COMPARATOR);        }    }).orElseThrow(() -> new ShouldNeverHappenException("No comparator logic implemented for BINARY logical type: " + logicalType));}
0
public Optional<PrimitiveComparator> visit(LogicalTypeAnnotation.DecimalLogicalTypeAnnotation decimalLogicalType)
{    return of(PrimitiveComparator.BINARY_AS_SIGNED_INTEGER_COMPARATOR);}
0
public Optional<PrimitiveComparator> visit(LogicalTypeAnnotation.StringLogicalTypeAnnotation stringLogicalType)
{    return of(PrimitiveComparator.UNSIGNED_LEXICOGRAPHICAL_BINARY_COMPARATOR);}
0
public Optional<PrimitiveComparator> visit(LogicalTypeAnnotation.EnumLogicalTypeAnnotation enumLogicalType)
{    return of(PrimitiveComparator.UNSIGNED_LEXICOGRAPHICAL_BINARY_COMPARATOR);}
0
public Optional<PrimitiveComparator> visit(LogicalTypeAnnotation.JsonLogicalTypeAnnotation jsonLogicalType)
{    return of(PrimitiveComparator.UNSIGNED_LEXICOGRAPHICAL_BINARY_COMPARATOR);}
0
public Optional<PrimitiveComparator> visit(LogicalTypeAnnotation.BsonLogicalTypeAnnotation bsonLogicalType)
{    return of(PrimitiveComparator.UNSIGNED_LEXICOGRAPHICAL_BINARY_COMPARATOR);}
0
public String toString(ColumnReader columnReader)
{    return String.valueOf(columnReader.getFloat());}
0
public void addValueToRecordConsumer(RecordConsumer recordConsumer, ColumnReader columnReader)
{    recordConsumer.addFloat(columnReader.getFloat());}
0
public void addValueToPrimitiveConverter(PrimitiveConverter primitiveConverter, ColumnReader columnReader)
{    primitiveConverter.addFloat(columnReader.getFloat());}
0
public T convert(PrimitiveTypeNameConverter<T, E> converter) throws E
{    return converter.convertFLOAT(this);}
0
 PrimitiveComparator<?> comparator(LogicalTypeAnnotation logicalType)
{    return PrimitiveComparator.FLOAT_COMPARATOR;}
0
public String toString(ColumnReader columnReader)
{    return String.valueOf(columnReader.getDouble());}
0
public void addValueToRecordConsumer(RecordConsumer recordConsumer, ColumnReader columnReader)
{    recordConsumer.addDouble(columnReader.getDouble());}
0
public void addValueToPrimitiveConverter(PrimitiveConverter primitiveConverter, ColumnReader columnReader)
{    primitiveConverter.addDouble(columnReader.getDouble());}
0
public T convert(PrimitiveTypeNameConverter<T, E> converter) throws E
{    return converter.convertDOUBLE(this);}
0
 PrimitiveComparator<?> comparator(LogicalTypeAnnotation logicalType)
{    return PrimitiveComparator.DOUBLE_COMPARATOR;}
0
public String toString(ColumnReader columnReader)
{    return Arrays.toString(columnReader.getBinary().getBytesUnsafe());}
0
public void addValueToRecordConsumer(RecordConsumer recordConsumer, ColumnReader columnReader)
{    recordConsumer.addBinary(columnReader.getBinary());}
0
public void addValueToPrimitiveConverter(PrimitiveConverter primitiveConverter, ColumnReader columnReader)
{    primitiveConverter.addBinary(columnReader.getBinary());}
0
public T convert(PrimitiveTypeNameConverter<T, E> converter) throws E
{    return converter.convertINT96(this);}
0
 PrimitiveComparator<?> comparator(LogicalTypeAnnotation logicalType)
{    return PrimitiveComparator.BINARY_AS_SIGNED_INTEGER_COMPARATOR;}
0
public String toString(ColumnReader columnReader)
{    return String.valueOf(columnReader.getBinary());}
0
public void addValueToRecordConsumer(RecordConsumer recordConsumer, ColumnReader columnReader)
{    recordConsumer.addBinary(columnReader.getBinary());}
0
public void addValueToPrimitiveConverter(PrimitiveConverter primitiveConverter, ColumnReader columnReader)
{    primitiveConverter.addBinary(columnReader.getBinary());}
0
public T convert(PrimitiveTypeNameConverter<T, E> converter) throws E
{    return converter.convertFIXED_LEN_BYTE_ARRAY(this);}
0
 PrimitiveComparator<?> comparator(LogicalTypeAnnotation logicalType)
{    if (logicalType == null) {        return PrimitiveComparator.UNSIGNED_LEXICOGRAPHICAL_BINARY_COMPARATOR;    }    return logicalType.accept(new LogicalTypeAnnotation.LogicalTypeAnnotationVisitor<PrimitiveComparator>() {        @Override        public Optional<PrimitiveComparator> visit(LogicalTypeAnnotation.DecimalLogicalTypeAnnotation decimalLogicalType) {            return of(PrimitiveComparator.BINARY_AS_SIGNED_INTEGER_COMPARATOR);        }        @Override        public Optional<PrimitiveComparator> visit(LogicalTypeAnnotation.IntervalLogicalTypeAnnotation intervalLogicalType) {            return of(PrimitiveComparator.UNSIGNED_LEXICOGRAPHICAL_BINARY_COMPARATOR);        }    }).orElseThrow(() -> new ShouldNeverHappenException("No comparator logic implemented for FIXED_LEN_BYTE_ARRAY logical type: " + logicalType));}
0
public Optional<PrimitiveComparator> visit(LogicalTypeAnnotation.DecimalLogicalTypeAnnotation decimalLogicalType)
{    return of(PrimitiveComparator.BINARY_AS_SIGNED_INTEGER_COMPARATOR);}
0
public Optional<PrimitiveComparator> visit(LogicalTypeAnnotation.IntervalLogicalTypeAnnotation intervalLogicalType)
{    return of(PrimitiveComparator.UNSIGNED_LEXICOGRAPHICAL_BINARY_COMPARATOR);}
0
private ColumnOrder requireValidColumnOrder(ColumnOrder columnOrder)
{    if (primitive == PrimitiveTypeName.INT96) {        Preconditions.checkArgument(columnOrder.getColumnOrderName() == ColumnOrderName.UNDEFINED, "The column order {} is not supported by INT96", columnOrder);    }    if (getLogicalTypeAnnotation() != null) {        Preconditions.checkArgument(getLogicalTypeAnnotation().isValidColumnOrder(columnOrder), "The column order {} is not supported by {} ({})", columnOrder, primitive, getLogicalTypeAnnotation());    }    return columnOrder;}
0
public PrimitiveType withId(int id)
{    return new PrimitiveType(getRepetition(), primitive, length, getName(), getLogicalTypeAnnotation(), new ID(id), columnOrder);}
0
public PrimitiveTypeName getPrimitiveTypeName()
{    return primitive;}
0
public int getTypeLength()
{    return length;}
0
public DecimalMetadata getDecimalMetadata()
{    return decimalMeta;}
0
public boolean isPrimitive()
{    return true;}
0
public void accept(TypeVisitor visitor)
{    visitor.visit(this);}
0
public void writeToStringBuilder(StringBuilder sb, String indent)
{    sb.append(indent).append(getRepetition().name().toLowerCase(Locale.ENGLISH)).append(" ").append(primitive.name().toLowerCase());    if (primitive == PrimitiveTypeName.FIXED_LEN_BYTE_ARRAY) {        sb.append("(" + length + ")");    }    sb.append(" ").append(getName());    if (getLogicalTypeAnnotation() != null) {                sb.append(" (").append(getLogicalTypeAnnotation().toString()).append(")");    }    if (getId() != null) {        sb.append(" = ").append(getId());    }}
0
protected int typeHashCode()
{    return hashCode();}
0
protected boolean typeEquals(Type other)
{    return equals(other);}
0
protected boolean equals(Type other)
{    if (!other.isPrimitive()) {        return false;    }    PrimitiveType otherPrimitive = other.asPrimitiveType();    return super.equals(other) && primitive == otherPrimitive.getPrimitiveTypeName() && length == otherPrimitive.length && columnOrder.equals(otherPrimitive.columnOrder) && eqOrBothNull(decimalMeta, otherPrimitive.decimalMeta);}
0
public int hashCode()
{    int hash = super.hashCode();    hash = hash * 31 + primitive.hashCode();    hash = hash * 31 + length;    hash = hash * 31 + columnOrder.hashCode();    if (decimalMeta != null) {        hash = hash * 31 + decimalMeta.hashCode();    }    return hash;}
0
public int getMaxRepetitionLevel(String[] path, int i)
{    if (path.length != i) {        throw new InvalidRecordException("Arrived at primitive node, path invalid");    }    return isRepetition(Repetition.REPEATED) ? 1 : 0;}
0
public int getMaxDefinitionLevel(String[] path, int i)
{    if (path.length != i) {        throw new InvalidRecordException("Arrived at primitive node, path invalid");    }    return isRepetition(Repetition.REQUIRED) ? 0 : 1;}
0
public Type getType(String[] path, int i)
{    if (path.length != i) {        throw new InvalidRecordException("Arrived at primitive node at index " + i + " , path invalid: " + Arrays.toString(path));    }    return this;}
0
protected List<String[]> getPaths(int depth)
{    return Arrays.<String[]>asList(new String[depth]);}
0
 void checkContains(Type subType)
{    super.checkContains(subType);    if (!subType.isPrimitive()) {        throw new InvalidRecordException(subType + " found: expected " + this);    }    PrimitiveType primitiveType = subType.asPrimitiveType();    if (this.primitive != primitiveType.primitive) {        throw new InvalidRecordException(subType + " found: expected " + this);    }}
0
public T convert(List<GroupType> path, TypeConverter<T> converter)
{    return converter.convertPrimitiveType(path, this);}
0
protected boolean containsPath(String[] path, int depth)
{    return path.length == depth;}
0
protected Type union(Type toMerge)
{    return union(toMerge, true);}
0
private void reportSchemaMergeError(Type toMerge)
{    throw new IncompatibleSchemaModificationException("can not merge type " + toMerge + " into " + this);}
0
private void reportSchemaMergeErrorWithColumnOrder(Type toMerge)
{    throw new IncompatibleSchemaModificationException("can not merge type " + toMerge + " with column order " + toMerge.asPrimitiveType().columnOrder() + " into " + this + " with column order " + columnOrder());}
0
protected Type union(Type toMerge, boolean strict)
{    if (!toMerge.isPrimitive()) {        reportSchemaMergeError(toMerge);    }    if (strict) {                if (!primitive.equals(toMerge.asPrimitiveType().getPrimitiveTypeName()) || !Objects.equals(getLogicalTypeAnnotation(), toMerge.getLogicalTypeAnnotation())) {            reportSchemaMergeError(toMerge);        }                int toMergeLength = toMerge.asPrimitiveType().getTypeLength();        if (primitive == PrimitiveTypeName.FIXED_LEN_BYTE_ARRAY && length != toMergeLength) {            reportSchemaMergeError(toMerge);        }                if (!columnOrder().equals(toMerge.asPrimitiveType().columnOrder())) {            reportSchemaMergeErrorWithColumnOrder(toMerge);        }    }    Repetition repetition = Repetition.leastRestrictive(this.getRepetition(), toMerge.getRepetition());    Types.PrimitiveBuilder<PrimitiveType> builder = Types.primitive(primitive, repetition);    if (PrimitiveTypeName.FIXED_LEN_BYTE_ARRAY == primitive) {        builder.length(length);    }    return builder.as(getLogicalTypeAnnotation()).named(getName());}
0
public PrimitiveComparator<T> comparator()
{    return (PrimitiveComparator<T>) getPrimitiveTypeName().comparator(getLogicalTypeAnnotation());}
0
public ColumnOrder columnOrder()
{    return columnOrder;}
0
public PrimitiveStringifier stringifier()
{    LogicalTypeAnnotation logicalTypeAnnotation = getLogicalTypeAnnotation();    return logicalTypeAnnotation == null ? PrimitiveStringifier.DEFAULT_STRINGIFIER : logicalTypeAnnotation.valueStringifier(this);}
0
public int getId()
{    return id;}
0
public int intValue()
{    return id;}
0
public boolean equals(Object obj)
{    return (obj instanceof ID) && ((ID) obj).id == id;}
0
public int hashCode()
{    return id;}
0
public String toString()
{    return String.valueOf(id);}
0
public static Repetition leastRestrictive(Repetition... repetitions)
{    boolean hasOptional = false;    for (Repetition repetition : repetitions) {        if (repetition == REPEATED) {            return REPEATED;        } else if (repetition == OPTIONAL) {            hasOptional = true;        }    }    if (hasOptional) {        return OPTIONAL;    }    return REQUIRED;}
0
public boolean isMoreRestrictiveThan(Repetition other)
{    return other != REQUIRED;}
0
public boolean isMoreRestrictiveThan(Repetition other)
{    return other == REPEATED;}
0
public boolean isMoreRestrictiveThan(Repetition other)
{    return false;}
0
public String getName()
{    return name;}
0
public boolean isRepetition(Repetition rep)
{    return repetition == rep;}
0
public Repetition getRepetition()
{    return repetition;}
0
public ID getId()
{    return id;}
0
public LogicalTypeAnnotation getLogicalTypeAnnotation()
{    return logicalTypeAnnotation;}
0
public OriginalType getOriginalType()
{    return logicalTypeAnnotation == null ? null : logicalTypeAnnotation.toOriginalType();}
0
public GroupType asGroupType()
{    if (isPrimitive()) {        throw new ClassCastException(this + " is not a group");    }    return (GroupType) this;}
0
public PrimitiveType asPrimitiveType()
{    if (!isPrimitive()) {        throw new ClassCastException(this + " is not primitive");    }    return (PrimitiveType) this;}
0
public int hashCode()
{    int c = repetition.hashCode();    c = 31 * c + name.hashCode();    if (logicalTypeAnnotation != null) {        c = 31 * c + logicalTypeAnnotation.hashCode();    }    if (id != null) {        c = 31 * c + id.hashCode();    }    return c;}
0
protected boolean equals(Type other)
{    return name.equals(other.name) && repetition == other.repetition && eqOrBothNull(repetition, other.repetition) && eqOrBothNull(id, other.id) && eqOrBothNull(logicalTypeAnnotation, other.logicalTypeAnnotation);}
0
public boolean equals(Object other)
{    if (!(other instanceof Type) || other == null) {        return false;    }    return equals((Type) other);}
0
protected boolean eqOrBothNull(Object o1, Object o2)
{    return (o1 == null && o2 == null) || (o1 != null && o1.equals(o2));}
0
public String toString()
{    StringBuilder sb = new StringBuilder();    writeToStringBuilder(sb, "");    return sb.toString();}
0
 void checkContains(Type subType)
{    if (!this.name.equals(subType.name) || this.repetition != subType.repetition) {        throw new InvalidRecordException(subType + " found: expected " + this);    }}
0
protected final THIS repetition(Type.Repetition repetition)
{    Preconditions.checkArgument(!repetitionAlreadySet, "Repetition has already been set");    Preconditions.checkNotNull(repetition, "Repetition cannot be null");    this.repetition = repetition;    this.repetitionAlreadySet = true;    return self();}
0
public THIS as(OriginalType type)
{    this.logicalTypeAnnotation = LogicalTypeAnnotation.fromOriginalType(type, null);    return self();}
0
public THIS as(LogicalTypeAnnotation type)
{    this.logicalTypeAnnotation = type;    this.newLogicalTypeSet = true;    return self();}
0
public THIS id(int id)
{    this.id = new ID(id);    return self();}
0
public P named(String name)
{    Preconditions.checkNotNull(name, "Name is required");    Preconditions.checkNotNull(repetition, "Repetition is required");    Type type = build(name);    if (parent != null) {                if (BaseGroupBuilder.class.isAssignableFrom(parent.getClass())) {            BaseGroupBuilder.class.cast(parent).addField(type);        }        return parent;    } else if (returnClass != null) {                return returnClass.cast(type);    } else {        throw new IllegalStateException("[BUG] Parent and return type are null: must override named");    }}
0
protected OriginalType getOriginalType()
{    return logicalTypeAnnotation == null ? null : logicalTypeAnnotation.toOriginalType();}
0
public THIS length(int length)
{    this.length = length;    return self();}
0
public THIS precision(int precision)
{    this.precision = precision;    precisionAlreadySet = true;    return self();}
0
public THIS scale(int scale)
{    this.scale = scale;    scaleAlreadySet = true;    return self();}
0
public THIS columnOrder(ColumnOrder columnOrder)
{    this.columnOrder = columnOrder;    return self();}
0
protected PrimitiveType build(String name)
{    if (PrimitiveTypeName.FIXED_LEN_BYTE_ARRAY == primitiveType) {        Preconditions.checkArgument(length > 0, "Invalid FIXED_LEN_BYTE_ARRAY length: " + length);    }    DecimalMetadata meta = decimalMetadata();        if (logicalTypeAnnotation != null) {        logicalTypeAnnotation.accept(new LogicalTypeAnnotation.LogicalTypeAnnotationVisitor<Boolean>() {            @Override            public Optional<Boolean> visit(LogicalTypeAnnotation.StringLogicalTypeAnnotation stringLogicalType) {                checkBinaryPrimitiveType(stringLogicalType);                return Optional.of(true);            }            @Override            public Optional<Boolean> visit(LogicalTypeAnnotation.JsonLogicalTypeAnnotation jsonLogicalType) {                checkBinaryPrimitiveType(jsonLogicalType);                return Optional.of(true);            }            @Override            public Optional<Boolean> visit(LogicalTypeAnnotation.BsonLogicalTypeAnnotation bsonLogicalType) {                checkBinaryPrimitiveType(bsonLogicalType);                return Optional.of(true);            }            @Override            public Optional<Boolean> visit(LogicalTypeAnnotation.DecimalLogicalTypeAnnotation decimalLogicalType) {                Preconditions.checkState((primitiveType == PrimitiveTypeName.INT32) || (primitiveType == PrimitiveTypeName.INT64) || (primitiveType == PrimitiveTypeName.BINARY) || (primitiveType == PrimitiveTypeName.FIXED_LEN_BYTE_ARRAY), "DECIMAL can only annotate INT32, INT64, BINARY, and FIXED");                if (primitiveType == PrimitiveTypeName.INT32) {                    Preconditions.checkState(meta.getPrecision() <= MAX_PRECISION_INT32, "INT32 cannot store " + meta.getPrecision() + " digits " + "(max " + MAX_PRECISION_INT32 + ")");                } else if (primitiveType == PrimitiveTypeName.INT64) {                    Preconditions.checkState(meta.getPrecision() <= MAX_PRECISION_INT64, "INT64 cannot store " + meta.getPrecision() + " digits " + "(max " + MAX_PRECISION_INT64 + ")");                    if (meta.getPrecision() <= MAX_PRECISION_INT32) {                                            }                } else if (primitiveType == PrimitiveTypeName.FIXED_LEN_BYTE_ARRAY) {                    Preconditions.checkState(meta.getPrecision() <= maxPrecision(length), "FIXED(" + length + ") cannot store " + meta.getPrecision() + " digits (max " + maxPrecision(length) + ")");                }                return Optional.of(true);            }            @Override            public Optional<Boolean> visit(LogicalTypeAnnotation.DateLogicalTypeAnnotation dateLogicalType) {                checkInt32PrimitiveType(dateLogicalType);                return Optional.of(true);            }            @Override            public Optional<Boolean> visit(LogicalTypeAnnotation.TimeLogicalTypeAnnotation timeLogicalType) {                LogicalTypeAnnotation.TimeUnit unit = timeLogicalType.getUnit();                switch(unit) {                    case MILLIS:                        checkInt32PrimitiveType(timeLogicalType);                        break;                    case MICROS:                    case NANOS:                        checkInt64PrimitiveType(timeLogicalType);                        break;                    default:                        throw new RuntimeException("Invalid time unit: " + unit);                }                return Optional.of(true);            }            @Override            public Optional<Boolean> visit(LogicalTypeAnnotation.IntLogicalTypeAnnotation intLogicalType) {                int bitWidth = intLogicalType.getBitWidth();                switch(bitWidth) {                    case 8:                    case 16:                    case 32:                        checkInt32PrimitiveType(intLogicalType);                        break;                    case 64:                        checkInt64PrimitiveType(intLogicalType);                        break;                    default:                        throw new RuntimeException("Invalid bit width: " + bitWidth);                }                return Optional.of(true);            }            @Override            public Optional<Boolean> visit(LogicalTypeAnnotation.TimestampLogicalTypeAnnotation timestampLogicalType) {                checkInt64PrimitiveType(timestampLogicalType);                return Optional.of(true);            }            @Override            public Optional<Boolean> visit(LogicalTypeAnnotation.IntervalLogicalTypeAnnotation intervalLogicalType) {                Preconditions.checkState((primitiveType == PrimitiveTypeName.FIXED_LEN_BYTE_ARRAY) && (length == 12), "INTERVAL can only annotate FIXED_LEN_BYTE_ARRAY(12)");                return Optional.of(true);            }            @Override            public Optional<Boolean> visit(LogicalTypeAnnotation.EnumLogicalTypeAnnotation enumLogicalType) {                Preconditions.checkState(primitiveType == PrimitiveTypeName.BINARY, "ENUM can only annotate binary fields");                return Optional.of(true);            }            private void checkBinaryPrimitiveType(LogicalTypeAnnotation logicalTypeAnnotation) {                Preconditions.checkState(primitiveType == PrimitiveTypeName.BINARY, logicalTypeAnnotation.toString() + " can only annotate binary fields");            }            private void checkInt32PrimitiveType(LogicalTypeAnnotation logicalTypeAnnotation) {                Preconditions.checkState(primitiveType == PrimitiveTypeName.INT32, logicalTypeAnnotation.toString() + " can only annotate INT32");            }            private void checkInt64PrimitiveType(LogicalTypeAnnotation logicalTypeAnnotation) {                Preconditions.checkState(primitiveType == PrimitiveTypeName.INT64, logicalTypeAnnotation.toString() + " can only annotate INT64");            }        }).orElseThrow(() -> new IllegalStateException(logicalTypeAnnotation + " can not be applied to a primitive type"));    }    if (newLogicalTypeSet) {        return new PrimitiveType(repetition, primitiveType, length, name, logicalTypeAnnotation, id, columnOrder);    } else {        return new PrimitiveType(repetition, primitiveType, length, name, getOriginalType(), meta, id, columnOrder);    }}
1
public Optional<Boolean> visit(LogicalTypeAnnotation.StringLogicalTypeAnnotation stringLogicalType)
{    checkBinaryPrimitiveType(stringLogicalType);    return Optional.of(true);}
0
public Optional<Boolean> visit(LogicalTypeAnnotation.JsonLogicalTypeAnnotation jsonLogicalType)
{    checkBinaryPrimitiveType(jsonLogicalType);    return Optional.of(true);}
0
public Optional<Boolean> visit(LogicalTypeAnnotation.BsonLogicalTypeAnnotation bsonLogicalType)
{    checkBinaryPrimitiveType(bsonLogicalType);    return Optional.of(true);}
0
public Optional<Boolean> visit(LogicalTypeAnnotation.DecimalLogicalTypeAnnotation decimalLogicalType)
{    Preconditions.checkState((primitiveType == PrimitiveTypeName.INT32) || (primitiveType == PrimitiveTypeName.INT64) || (primitiveType == PrimitiveTypeName.BINARY) || (primitiveType == PrimitiveTypeName.FIXED_LEN_BYTE_ARRAY), "DECIMAL can only annotate INT32, INT64, BINARY, and FIXED");    if (primitiveType == PrimitiveTypeName.INT32) {        Preconditions.checkState(meta.getPrecision() <= MAX_PRECISION_INT32, "INT32 cannot store " + meta.getPrecision() + " digits " + "(max " + MAX_PRECISION_INT32 + ")");    } else if (primitiveType == PrimitiveTypeName.INT64) {        Preconditions.checkState(meta.getPrecision() <= MAX_PRECISION_INT64, "INT64 cannot store " + meta.getPrecision() + " digits " + "(max " + MAX_PRECISION_INT64 + ")");        if (meta.getPrecision() <= MAX_PRECISION_INT32) {                    }    } else if (primitiveType == PrimitiveTypeName.FIXED_LEN_BYTE_ARRAY) {        Preconditions.checkState(meta.getPrecision() <= maxPrecision(length), "FIXED(" + length + ") cannot store " + meta.getPrecision() + " digits (max " + maxPrecision(length) + ")");    }    return Optional.of(true);}
1
public Optional<Boolean> visit(LogicalTypeAnnotation.DateLogicalTypeAnnotation dateLogicalType)
{    checkInt32PrimitiveType(dateLogicalType);    return Optional.of(true);}
0
public Optional<Boolean> visit(LogicalTypeAnnotation.TimeLogicalTypeAnnotation timeLogicalType)
{    LogicalTypeAnnotation.TimeUnit unit = timeLogicalType.getUnit();    switch(unit) {        case MILLIS:            checkInt32PrimitiveType(timeLogicalType);            break;        case MICROS:        case NANOS:            checkInt64PrimitiveType(timeLogicalType);            break;        default:            throw new RuntimeException("Invalid time unit: " + unit);    }    return Optional.of(true);}
0
public Optional<Boolean> visit(LogicalTypeAnnotation.IntLogicalTypeAnnotation intLogicalType)
{    int bitWidth = intLogicalType.getBitWidth();    switch(bitWidth) {        case 8:        case 16:        case 32:            checkInt32PrimitiveType(intLogicalType);            break;        case 64:            checkInt64PrimitiveType(intLogicalType);            break;        default:            throw new RuntimeException("Invalid bit width: " + bitWidth);    }    return Optional.of(true);}
0
public Optional<Boolean> visit(LogicalTypeAnnotation.TimestampLogicalTypeAnnotation timestampLogicalType)
{    checkInt64PrimitiveType(timestampLogicalType);    return Optional.of(true);}
0
public Optional<Boolean> visit(LogicalTypeAnnotation.IntervalLogicalTypeAnnotation intervalLogicalType)
{    Preconditions.checkState((primitiveType == PrimitiveTypeName.FIXED_LEN_BYTE_ARRAY) && (length == 12), "INTERVAL can only annotate FIXED_LEN_BYTE_ARRAY(12)");    return Optional.of(true);}
0
public Optional<Boolean> visit(LogicalTypeAnnotation.EnumLogicalTypeAnnotation enumLogicalType)
{    Preconditions.checkState(primitiveType == PrimitiveTypeName.BINARY, "ENUM can only annotate binary fields");    return Optional.of(true);}
0
private void checkBinaryPrimitiveType(LogicalTypeAnnotation logicalTypeAnnotation)
{    Preconditions.checkState(primitiveType == PrimitiveTypeName.BINARY, logicalTypeAnnotation.toString() + " can only annotate binary fields");}
0
private void checkInt32PrimitiveType(LogicalTypeAnnotation logicalTypeAnnotation)
{    Preconditions.checkState(primitiveType == PrimitiveTypeName.INT32, logicalTypeAnnotation.toString() + " can only annotate INT32");}
0
private void checkInt64PrimitiveType(LogicalTypeAnnotation logicalTypeAnnotation)
{    Preconditions.checkState(primitiveType == PrimitiveTypeName.INT64, logicalTypeAnnotation.toString() + " can only annotate INT64");}
0
private static long maxPrecision(int numBytes)
{    return     Math.round(Math.floor(    Math.log10(    Math.pow(2, 8 * numBytes - 1) - 1)));}
0
protected DecimalMetadata decimalMetadata()
{    DecimalMetadata meta = null;    if (logicalTypeAnnotation instanceof LogicalTypeAnnotation.DecimalLogicalTypeAnnotation) {        LogicalTypeAnnotation.DecimalLogicalTypeAnnotation decimalType = (LogicalTypeAnnotation.DecimalLogicalTypeAnnotation) logicalTypeAnnotation;        if (newLogicalTypeSet) {            if (scaleAlreadySet) {                Preconditions.checkArgument(this.scale == decimalType.getScale(), "Decimal scale should match with the scale of the logical type");            }            if (precisionAlreadySet) {                Preconditions.checkArgument(this.precision == decimalType.getPrecision(), "Decimal precision should match with the precision of the logical type");            }            scale = decimalType.getScale();            precision = decimalType.getPrecision();        }        Preconditions.checkArgument(precision > 0, "Invalid DECIMAL precision: " + precision);        Preconditions.checkArgument(this.scale >= 0, "Invalid DECIMAL scale: " + this.scale);        Preconditions.checkArgument(this.scale <= precision, "Invalid DECIMAL scale: cannot be greater than precision");        meta = new DecimalMetadata(precision, scale);    }    return meta;}
0
protected PrimitiveBuilder<P> self()
{    return this;}
0
public PrimitiveBuilder<THIS> primitive(PrimitiveTypeName type, Type.Repetition repetition)
{    return new PrimitiveBuilder<THIS>(self(), type).repetition(repetition);}
0
public PrimitiveBuilder<THIS> required(PrimitiveTypeName type)
{    return new PrimitiveBuilder<THIS>(self(), type).repetition(Type.Repetition.REQUIRED);}
0
public PrimitiveBuilder<THIS> optional(PrimitiveTypeName type)
{    return new PrimitiveBuilder<THIS>(self(), type).repetition(Type.Repetition.OPTIONAL);}
0
public PrimitiveBuilder<THIS> repeated(PrimitiveTypeName type)
{    return new PrimitiveBuilder<THIS>(self(), type).repetition(Type.Repetition.REPEATED);}
0
public GroupBuilder<THIS> group(Type.Repetition repetition)
{    return new GroupBuilder<THIS>(self()).repetition(repetition);}
0
public GroupBuilder<THIS> requiredGroup()
{    return new GroupBuilder<THIS>(self()).repetition(Type.Repetition.REQUIRED);}
0
public GroupBuilder<THIS> optionalGroup()
{    return new GroupBuilder<THIS>(self()).repetition(Type.Repetition.OPTIONAL);}
0
public GroupBuilder<THIS> repeatedGroup()
{    return new GroupBuilder<THIS>(self()).repetition(Type.Repetition.REPEATED);}
0
public THIS addField(Type type)
{    fields.add(type);    return self();}
0
public THIS addFields(Type... types)
{    Collections.addAll(fields, types);    return self();}
0
protected GroupType build(String name)
{    if (newLogicalTypeSet) {        return new GroupType(repetition, name, logicalTypeAnnotation, fields, id);    } else {        return new GroupType(repetition, name, getOriginalType(), fields, id);    }}
0
public MapBuilder<THIS> map(Type.Repetition repetition)
{    return new MapBuilder<THIS>(self()).repetition(repetition);}
0
public MapBuilder<THIS> requiredMap()
{    return new MapBuilder<THIS>(self()).repetition(Type.Repetition.REQUIRED);}
0
public MapBuilder<THIS> optionalMap()
{    return new MapBuilder<THIS>(self()).repetition(Type.Repetition.OPTIONAL);}
0
public ListBuilder<THIS> list(Type.Repetition repetition)
{    return new ListBuilder<THIS>(self()).repetition(repetition);}
0
public ListBuilder<THIS> requiredList()
{    return list(Type.Repetition.REQUIRED);}
0
public ListBuilder<THIS> optionalList()
{    return list(Type.Repetition.OPTIONAL);}
0
protected GroupBuilder<P> self()
{    return this;}
0
public ValueBuilder<MP, M> value(PrimitiveTypeName type, Type.Repetition repetition)
{    mapBuilder.setKeyType(build("key"));    return new ValueBuilder<MP, M>(mapBuilder, type).repetition(repetition);}
0
public ValueBuilder<MP, M> requiredValue(PrimitiveTypeName type)
{    return value(type, Type.Repetition.REQUIRED);}
0
public ValueBuilder<MP, M> optionalValue(PrimitiveTypeName type)
{    return value(type, Type.Repetition.OPTIONAL);}
0
public GroupValueBuilder<MP, M> groupValue(Type.Repetition repetition)
{    mapBuilder.setKeyType(build("key"));    return new GroupValueBuilder<MP, M>(mapBuilder).repetition(repetition);}
0
public GroupValueBuilder<MP, M> requiredGroupValue()
{    return groupValue(Type.Repetition.REQUIRED);}
0
public GroupValueBuilder<MP, M> optionalGroupValue()
{    return groupValue(Type.Repetition.OPTIONAL);}
0
public MapValueBuilder<MP, M> mapValue(Type.Repetition repetition)
{    mapBuilder.setKeyType(build("key"));    return new MapValueBuilder<MP, M>(mapBuilder).repetition(repetition);}
0
public MapValueBuilder<MP, M> requiredMapValue()
{    return mapValue(Type.Repetition.REQUIRED);}
0
public MapValueBuilder<MP, M> optionalMapValue()
{    return mapValue(Type.Repetition.OPTIONAL);}
0
public ListValueBuilder<MP, M> listValue(Type.Repetition repetition)
{    mapBuilder.setKeyType(build("key"));    return new ListValueBuilder<MP, M>(mapBuilder).repetition(repetition);}
0
public ListValueBuilder<MP, M> requiredListValue()
{    return listValue(Type.Repetition.REQUIRED);}
0
public ListValueBuilder<MP, M> optionalListValue()
{    return listValue(Type.Repetition.OPTIONAL);}
0
public M value(Type type)
{    mapBuilder.setKeyType(build("key"));    mapBuilder.setValueType(type);    return this.mapBuilder;}
0
public MP named(String name)
{    mapBuilder.setKeyType(build("key"));    return mapBuilder.named(name);}
0
protected KeyBuilder<MP, M> self()
{    return this;}
0
public MP named(String name)
{    mapBuilder.setValueType(build("value"));    return mapBuilder.named(name);}
0
protected ValueBuilder<MP, M> self()
{    return this;}
0
protected GroupKeyBuilder<MP, M> self()
{    return this;}
0
public ValueBuilder<MP, M> value(PrimitiveTypeName type, Type.Repetition repetition)
{    mapBuilder.setKeyType(build("key"));    return new ValueBuilder<MP, M>(mapBuilder, type).repetition(repetition);}
0
public ValueBuilder<MP, M> requiredValue(PrimitiveTypeName type)
{    return value(type, Type.Repetition.REQUIRED);}
0
public ValueBuilder<MP, M> optionalValue(PrimitiveTypeName type)
{    return value(type, Type.Repetition.OPTIONAL);}
0
public GroupValueBuilder<MP, M> groupValue(Type.Repetition repetition)
{    mapBuilder.setKeyType(build("key"));    return new GroupValueBuilder<MP, M>(mapBuilder).repetition(repetition);}
0
public GroupValueBuilder<MP, M> requiredGroupValue()
{    return groupValue(Type.Repetition.REQUIRED);}
0
public GroupValueBuilder<MP, M> optionalGroupValue()
{    return groupValue(Type.Repetition.OPTIONAL);}
0
public MapValueBuilder<MP, M> mapValue(Type.Repetition repetition)
{    mapBuilder.setKeyType(build("key"));    return new MapValueBuilder<MP, M>(mapBuilder).repetition(repetition);}
0
public MapValueBuilder<MP, M> requiredMapValue()
{    return mapValue(Type.Repetition.REQUIRED);}
0
public MapValueBuilder<MP, M> optionalMapValue()
{    return mapValue(Type.Repetition.OPTIONAL);}
0
public ListValueBuilder<MP, M> listValue(Type.Repetition repetition)
{    mapBuilder.setKeyType(build("key"));    return new ListValueBuilder<MP, M>(mapBuilder).repetition(repetition);}
0
public ListValueBuilder<MP, M> requiredListValue()
{    return listValue(Type.Repetition.REQUIRED);}
0
public ListValueBuilder<MP, M> optionalListValue()
{    return listValue(Type.Repetition.OPTIONAL);}
0
public M value(Type type)
{    mapBuilder.setKeyType(build("key"));    mapBuilder.setValueType(type);    return this.mapBuilder;}
0
public MP named(String name)
{    mapBuilder.setKeyType(build("key"));    return mapBuilder.named(name);}
0
public MP named(String name)
{    mapBuilder.setValueType(build("value"));    return mapBuilder.named(name);}
0
protected GroupValueBuilder<MP, M> self()
{    return this;}
0
public MP named(String name)
{    mapBuilder.setValueType(build("value"));    return mapBuilder.named(name);}
0
protected MapValueBuilder<MP, M> self()
{    return this;}
0
public MP named(String name)
{    mapBuilder.setValueType(build("value"));    return mapBuilder.named(name);}
0
protected ListValueBuilder<MP, M> self()
{    return this;}
0
protected void setKeyType(Type keyType)
{    Preconditions.checkState(this.keyType == null, "Only one key type can be built with a MapBuilder");    this.keyType = keyType;}
0
protected void setValueType(Type valueType)
{    Preconditions.checkState(this.valueType == null, "Only one key type can be built with a ValueBuilder");    this.valueType = valueType;}
0
public KeyBuilder<P, THIS> key(PrimitiveTypeName type)
{    return new KeyBuilder<P, THIS>(self(), type);}
0
public THIS key(Type type)
{    setKeyType(type);    return self();}
0
public GroupKeyBuilder<P, THIS> groupKey()
{    return new GroupKeyBuilder<P, THIS>(self());}
0
public ValueBuilder<P, THIS> value(PrimitiveTypeName type, Type.Repetition repetition)
{    return new ValueBuilder<P, THIS>(self(), type).repetition(repetition);}
0
public ValueBuilder<P, THIS> requiredValue(PrimitiveTypeName type)
{    return value(type, Type.Repetition.REQUIRED);}
0
public ValueBuilder<P, THIS> optionalValue(PrimitiveTypeName type)
{    return value(type, Type.Repetition.OPTIONAL);}
0
public GroupValueBuilder<P, THIS> groupValue(Type.Repetition repetition)
{    return new GroupValueBuilder<P, THIS>(self()).repetition(repetition);}
0
public GroupValueBuilder<P, THIS> requiredGroupValue()
{    return groupValue(Type.Repetition.REQUIRED);}
0
public GroupValueBuilder<P, THIS> optionalGroupValue()
{    return groupValue(Type.Repetition.OPTIONAL);}
0
public MapValueBuilder<P, THIS> mapValue(Type.Repetition repetition)
{    return new MapValueBuilder<P, THIS>(self()).repetition(repetition);}
0
public MapValueBuilder<P, THIS> requiredMapValue()
{    return mapValue(Type.Repetition.REQUIRED);}
0
public MapValueBuilder<P, THIS> optionalMapValue()
{    return mapValue(Type.Repetition.OPTIONAL);}
0
public ListValueBuilder<P, THIS> listValue(Type.Repetition repetition)
{    return new ListValueBuilder<P, THIS>(self()).repetition(repetition);}
0
public ListValueBuilder<P, THIS> requiredListValue()
{    return listValue(Type.Repetition.REQUIRED);}
0
public ListValueBuilder<P, THIS> optionalListValue()
{    return listValue(Type.Repetition.OPTIONAL);}
0
public THIS value(Type type)
{    setValueType(type);    return self();}
0
protected Type build(String name)
{    Preconditions.checkState(logicalTypeAnnotation == null, "MAP is already a logical type and can't be changed.");    if (keyType == null) {        keyType = STRING_KEY;    }    GroupBuilder<GroupType> builder = buildGroup(repetition).as(OriginalType.MAP);    if (id != null) {        builder.id(id.intValue());    }    if (valueType != null) {        return builder.repeatedGroup().addFields(keyType, valueType).named("map").named(name);    } else {        return builder.repeatedGroup().addFields(keyType).named("map").named(name);    }}
0
protected MapBuilder<P> self()
{    return this;}
0
public THIS setElementType(Type elementType)
{    Preconditions.checkState(this.elementType == null, "Only one element can be built with a ListBuilder");    this.elementType = elementType;    return self();}
0
public LP named(String name)
{    listBuilder.setElementType(build("element"));    return listBuilder.named(name);}
0
protected ElementBuilder<LP, L> self()
{    return this;}
0
public LP named(String name)
{    listBuilder.setElementType(build("element"));    return listBuilder.named(name);}
0
protected GroupElementBuilder<LP, L> self()
{    return this;}
0
protected MapElementBuilder<LP, L> self()
{    return this;}
0
public LP named(String name)
{    listBuilder.setElementType(build("element"));    return listBuilder.named(name);}
0
protected ListElementBuilder<LP, L> self()
{    return this;}
0
public LP named(String name)
{    listBuilder.setElementType(build("element"));    return listBuilder.named(name);}
0
protected Type build(String name)
{    Preconditions.checkState(logicalTypeAnnotation == null, "LIST is already the logical type and can't be changed");    Preconditions.checkNotNull(elementType, "List element type");    GroupBuilder<GroupType> builder = buildGroup(repetition).as(OriginalType.LIST);    if (id != null) {        builder.id(id.intValue());    }    return builder.repeatedGroup().addFields(elementType).named("list").named(name);}
0
public ElementBuilder<P, THIS> element(PrimitiveTypeName type, Type.Repetition repetition)
{    return new ElementBuilder<P, THIS>(self(), type).repetition(repetition);}
0
public ElementBuilder<P, THIS> requiredElement(PrimitiveTypeName type)
{    return element(type, Type.Repetition.REQUIRED);}
0
public ElementBuilder<P, THIS> optionalElement(PrimitiveTypeName type)
{    return element(type, Type.Repetition.OPTIONAL);}
0
public GroupElementBuilder<P, THIS> groupElement(Type.Repetition repetition)
{    return new GroupElementBuilder<P, THIS>(self()).repetition(repetition);}
0
public GroupElementBuilder<P, THIS> requiredGroupElement()
{    return groupElement(Type.Repetition.REQUIRED);}
0
public GroupElementBuilder<P, THIS> optionalGroupElement()
{    return groupElement(Type.Repetition.OPTIONAL);}
0
public MapElementBuilder<P, THIS> mapElement(Type.Repetition repetition)
{    return new MapElementBuilder<P, THIS>(self()).repetition(repetition);}
0
public MapElementBuilder<P, THIS> requiredMapElement()
{    return mapElement(Type.Repetition.REQUIRED);}
0
public MapElementBuilder<P, THIS> optionalMapElement()
{    return mapElement(Type.Repetition.OPTIONAL);}
0
public ListElementBuilder<P, THIS> listElement(Type.Repetition repetition)
{    return new ListElementBuilder<P, THIS>(self()).repetition(repetition);}
0
public ListElementBuilder<P, THIS> requiredListElement()
{    return listElement(Type.Repetition.REQUIRED);}
0
public ListElementBuilder<P, THIS> optionalListElement()
{    return listElement(Type.Repetition.OPTIONAL);}
0
public BaseListBuilder<P, THIS> element(Type type)
{    setElementType(type);    return self();}
0
protected ListBuilder<P> self()
{    return this;}
0
public MessageType named(String name)
{    Preconditions.checkNotNull(name, "Name is required");    return new MessageType(name, fields);}
0
public static MessageTypeBuilder buildMessage()
{    return new MessageTypeBuilder();}
0
public static PrimitiveBuilder<PrimitiveType> primitive(PrimitiveTypeName type, Type.Repetition repetition)
{    return new PrimitiveBuilder<PrimitiveType>(PrimitiveType.class, type).repetition(repetition);}
0
public static PrimitiveBuilder<PrimitiveType> required(PrimitiveTypeName type)
{    return new PrimitiveBuilder<PrimitiveType>(PrimitiveType.class, type).repetition(Type.Repetition.REQUIRED);}
0
public static PrimitiveBuilder<PrimitiveType> optional(PrimitiveTypeName type)
{    return new PrimitiveBuilder<PrimitiveType>(PrimitiveType.class, type).repetition(Type.Repetition.OPTIONAL);}
0
public static PrimitiveBuilder<PrimitiveType> repeated(PrimitiveTypeName type)
{    return new PrimitiveBuilder<PrimitiveType>(PrimitiveType.class, type).repetition(Type.Repetition.REPEATED);}
0
public static GroupBuilder<GroupType> buildGroup(Type.Repetition repetition)
{    return new GroupBuilder<GroupType>(GroupType.class).repetition(repetition);}
0
public static GroupBuilder<GroupType> requiredGroup()
{    return new GroupBuilder<GroupType>(GroupType.class).repetition(Type.Repetition.REQUIRED);}
0
public static GroupBuilder<GroupType> optionalGroup()
{    return new GroupBuilder<GroupType>(GroupType.class).repetition(Type.Repetition.OPTIONAL);}
0
public static GroupBuilder<GroupType> repeatedGroup()
{    return new GroupBuilder<GroupType>(GroupType.class).repetition(Type.Repetition.REPEATED);}
0
public static MapBuilder<GroupType> map(Type.Repetition repetition)
{    return new MapBuilder<GroupType>(GroupType.class).repetition(repetition);}
0
public static MapBuilder<GroupType> requiredMap()
{    return map(Type.Repetition.REQUIRED);}
0
public static MapBuilder<GroupType> optionalMap()
{    return map(Type.Repetition.OPTIONAL);}
0
public static ListBuilder<GroupType> list(Type.Repetition repetition)
{    return new ListBuilder<GroupType>(GroupType.class).repetition(repetition);}
0
public static ListBuilder<GroupType> requiredList()
{    return list(Type.Repetition.REQUIRED);}
0
public static ListBuilder<GroupType> optionalList()
{    return list(Type.Repetition.OPTIONAL);}
0
public static void checkValidWriteSchema(GroupType schema)
{    schema.accept(new TypeVisitor() {        @Override        public void visit(GroupType groupType) {            if (groupType.getFieldCount() <= 0) {                throw new InvalidSchemaException("Cannot write a schema with an empty group: " + groupType);            }            for (Type type : groupType.getFields()) {                type.accept(this);            }        }        @Override        public void visit(MessageType messageType) {            visit((GroupType) messageType);        }        @Override        public void visit(PrimitiveType primitiveType) {        }    });}
0
public void visit(GroupType groupType)
{    if (groupType.getFieldCount() <= 0) {        throw new InvalidSchemaException("Cannot write a schema with an empty group: " + groupType);    }    for (Type type : groupType.getFields()) {        type.accept(this);    }}
0
public void visit(MessageType messageType)
{    visit((GroupType) messageType);}
0
public void visit(PrimitiveType primitiveType)
{}
0
public void addBinary(Binary value)
{    assertEquals("bar" + count % 10, value.toStringUsingUTF8());    ++count;}
0
public void test() throws Exception
{    MessageType schema = MessageTypeParser.parseMessageType("message test { required binary foo; }");    ColumnDescriptor col = schema.getColumns().get(0);    MemPageWriter pageWriter = new MemPageWriter();    ColumnWriterV2 columnWriterV2 = new ColumnWriterV2(col, pageWriter, ParquetProperties.builder().withDictionaryPageSize(1024).withWriterVersion(PARQUET_2_0).withPageSize(2048).build());    for (int i = 0; i < rows; i++) {        columnWriterV2.write(Binary.fromString("bar" + i % 10), 0, 0);        if ((i + 1) % 1000 == 0) {            columnWriterV2.writePage();        }    }    columnWriterV2.writePage();    columnWriterV2.finalizeColumnChunk();    List<DataPage> pages = pageWriter.getPages();    int valueCount = 0;    int rowCount = 0;    for (DataPage dataPage : pages) {        valueCount += dataPage.getValueCount();        rowCount += ((DataPageV2) dataPage).getRowCount();    }    assertEquals(rows, rowCount);    assertEquals(rows, valueCount);    MemPageReader pageReader = new MemPageReader((long) rows, pages.iterator(), pageWriter.getDictionaryPage());    ValidatingConverter converter = new ValidatingConverter();    ColumnReader columnReader = new ColumnReaderImpl(col, pageReader, converter, VersionParser.parse(Version.FULL_VERSION));    for (int i = 0; i < rows; i++) {        assertEquals(0, columnReader.getCurrentRepetitionLevel());        assertEquals(0, columnReader.getCurrentDefinitionLevel());        columnReader.writeCurrentValueToConverter();        columnReader.consume();    }    assertEquals(rows, converter.count);}
0
public void testOptional() throws Exception
{    MessageType schema = MessageTypeParser.parseMessageType("message test { optional binary foo; }");    ColumnDescriptor col = schema.getColumns().get(0);    MemPageWriter pageWriter = new MemPageWriter();    ColumnWriterV2 columnWriterV2 = new ColumnWriterV2(col, pageWriter, ParquetProperties.builder().withDictionaryPageSize(1024).withWriterVersion(PARQUET_2_0).withPageSize(2048).build());    for (int i = 0; i < rows; i++) {        columnWriterV2.writeNull(0, 0);        if ((i + 1) % 1000 == 0) {            columnWriterV2.writePage();        }    }    columnWriterV2.writePage();    columnWriterV2.finalizeColumnChunk();    List<DataPage> pages = pageWriter.getPages();    int valueCount = 0;    int rowCount = 0;    for (DataPage dataPage : pages) {        valueCount += dataPage.getValueCount();        rowCount += ((DataPageV2) dataPage).getRowCount();    }    assertEquals(rows, rowCount);    assertEquals(rows, valueCount);    MemPageReader pageReader = new MemPageReader((long) rows, pages.iterator(), pageWriter.getDictionaryPage());    ValidatingConverter converter = new ValidatingConverter();    ColumnReader columnReader = new ColumnReaderImpl(col, pageReader, converter, VersionParser.parse(Version.FULL_VERSION));    for (int i = 0; i < rows; i++) {        assertEquals(0, columnReader.getCurrentRepetitionLevel());        assertEquals(0, columnReader.getCurrentDefinitionLevel());        columnReader.consume();    }    assertEquals(0, converter.count);}
0
public void testCorruptDeltaByteArrayVerisons()
{    assertTrue(CorruptDeltaByteArrays.requiresSequentialReads("parquet-mr version 1.6.0 (build abcd)", Encoding.DELTA_BYTE_ARRAY));    assertTrue(CorruptDeltaByteArrays.requiresSequentialReads((String) null, Encoding.DELTA_BYTE_ARRAY));    assertTrue(CorruptDeltaByteArrays.requiresSequentialReads((ParsedVersion) null, Encoding.DELTA_BYTE_ARRAY));    assertTrue(CorruptDeltaByteArrays.requiresSequentialReads((SemanticVersion) null, Encoding.DELTA_BYTE_ARRAY));    assertTrue(CorruptDeltaByteArrays.requiresSequentialReads("parquet-mr version 1.8.0-SNAPSHOT (build abcd)", Encoding.DELTA_BYTE_ARRAY));    assertFalse(CorruptDeltaByteArrays.requiresSequentialReads("parquet-mr version 1.6.0 (build abcd)", Encoding.DELTA_BINARY_PACKED));    assertFalse(CorruptDeltaByteArrays.requiresSequentialReads((String) null, Encoding.DELTA_LENGTH_BYTE_ARRAY));    assertFalse(CorruptDeltaByteArrays.requiresSequentialReads((ParsedVersion) null, Encoding.PLAIN));    assertFalse(CorruptDeltaByteArrays.requiresSequentialReads((SemanticVersion) null, Encoding.RLE));    assertFalse(CorruptDeltaByteArrays.requiresSequentialReads("parquet-mr version 1.8.0-SNAPSHOT (build abcd)", Encoding.RLE_DICTIONARY));    assertFalse(CorruptDeltaByteArrays.requiresSequentialReads("parquet-mr version 1.8.0-SNAPSHOT (build abcd)", Encoding.PLAIN_DICTIONARY));    assertFalse(CorruptDeltaByteArrays.requiresSequentialReads("parquet-mr version 1.8.0-SNAPSHOT (build abcd)", Encoding.BIT_PACKED));    assertFalse(CorruptDeltaByteArrays.requiresSequentialReads("parquet-mr version 1.8.0 (build abcd)", Encoding.DELTA_BYTE_ARRAY));}
0
public void testEncodingRequiresSequentailRead()
{    ParsedVersion impala = new ParsedVersion("impala", "1.2.0", "abcd");    assertFalse(CorruptDeltaByteArrays.requiresSequentialReads(impala, Encoding.DELTA_BYTE_ARRAY));    ParsedVersion broken = new ParsedVersion("parquet-mr", "1.8.0-SNAPSHOT", "abcd");    assertTrue(CorruptDeltaByteArrays.requiresSequentialReads(broken, Encoding.DELTA_BYTE_ARRAY));    ParsedVersion fixed = new ParsedVersion("parquet-mr", "1.8.0", "abcd");    assertFalse(CorruptDeltaByteArrays.requiresSequentialReads(fixed, Encoding.DELTA_BYTE_ARRAY));}
0
private DeltaByteArrayWriter getDeltaByteArrayWriter()
{    return new DeltaByteArrayWriter(10, 100, new HeapByteBufferAllocator());}
0
public void testReassemblyWithCorruptPage() throws Exception
{    DeltaByteArrayWriter writer = getDeltaByteArrayWriter();    String lastValue = null;    for (int i = 0; i < 10; i += 1) {        lastValue = str(i);        writer.writeBytes(Binary.fromString(lastValue));    }    ByteBuffer firstPageBytes = writer.getBytes().toByteBuffer();        writer.reset();    corruptWriter(writer, lastValue);    for (int i = 10; i < 20; i += 1) {        writer.writeBytes(Binary.fromString(str(i)));    }    ByteBuffer corruptPageBytes = writer.getBytes().toByteBuffer();    DeltaByteArrayReader firstPageReader = new DeltaByteArrayReader();    firstPageReader.initFromPage(10, ByteBufferInputStream.wrap(firstPageBytes));    for (int i = 0; i < 10; i += 1) {        assertEquals(str(i), firstPageReader.readBytes().toStringUsingUTF8());    }    DeltaByteArrayReader corruptPageReader = new DeltaByteArrayReader();    corruptPageReader.initFromPage(10, ByteBufferInputStream.wrap(corruptPageBytes));    try {        corruptPageReader.readBytes();        fail("Corrupt page did not throw an exception when read");    } catch (ArrayIndexOutOfBoundsException e) {        }    DeltaByteArrayReader secondPageReader = new DeltaByteArrayReader();    secondPageReader.initFromPage(10, ByteBufferInputStream.wrap(corruptPageBytes));    secondPageReader.setPreviousReader(firstPageReader);    for (int i = 10; i < 20; i += 1) {        assertEquals(secondPageReader.readBytes().toStringUsingUTF8(), str(i));    }}
0
public void testReassemblyWithoutCorruption() throws Exception
{    DeltaByteArrayWriter writer = getDeltaByteArrayWriter();    for (int i = 0; i < 10; i += 1) {        writer.writeBytes(Binary.fromString(str(i)));    }    ByteBuffer firstPageBytes = writer.getBytes().toByteBuffer();        writer.reset();    for (int i = 10; i < 20; i += 1) {        writer.writeBytes(Binary.fromString(str(i)));    }    ByteBuffer secondPageBytes = writer.getBytes().toByteBuffer();    DeltaByteArrayReader firstPageReader = new DeltaByteArrayReader();    firstPageReader.initFromPage(10, ByteBufferInputStream.wrap(firstPageBytes));    for (int i = 0; i < 10; i += 1) {        assertEquals(firstPageReader.readBytes().toStringUsingUTF8(), str(i));    }    DeltaByteArrayReader secondPageReader = new DeltaByteArrayReader();    secondPageReader.initFromPage(10, ByteBufferInputStream.wrap(secondPageBytes));    secondPageReader.setPreviousReader(firstPageReader);    for (int i = 10; i < 20; i += 1) {        assertEquals(secondPageReader.readBytes().toStringUsingUTF8(), str(i));    }}
0
public void testOldReassemblyWithoutCorruption() throws Exception
{    DeltaByteArrayWriter writer = getDeltaByteArrayWriter();    for (int i = 0; i < 10; i += 1) {        writer.writeBytes(Binary.fromString(str(i)));    }    ByteBuffer firstPageBytes = writer.getBytes().toByteBuffer();        writer.reset();    for (int i = 10; i < 20; i += 1) {        writer.writeBytes(Binary.fromString(str(i)));    }    ByteBuffer secondPageBytes = writer.getBytes().toByteBuffer();    DeltaByteArrayReader firstPageReader = new DeltaByteArrayReader();    firstPageReader.initFromPage(10, ByteBufferInputStream.wrap(firstPageBytes));    for (int i = 0; i < 10; i += 1) {        assertEquals(firstPageReader.readBytes().toStringUsingUTF8(), str(i));    }    DeltaByteArrayReader secondPageReader = new DeltaByteArrayReader();    secondPageReader.initFromPage(10, ByteBufferInputStream.wrap(secondPageBytes));    for (int i = 10; i < 20; i += 1) {        assertEquals(secondPageReader.readBytes().toStringUsingUTF8(), str(i));    }}
0
public void testColumnReaderImplWithCorruptPage() throws Exception
{    ColumnDescriptor column = new ColumnDescriptor(new String[] { "s" }, PrimitiveType.PrimitiveTypeName.BINARY, 0, 0);    MemPageStore pages = new MemPageStore(0);    PageWriter memWriter = pages.getPageWriter(column);    ParquetProperties parquetProps = ParquetProperties.builder().withDictionaryEncoding(false).build();        ValuesWriter rdValues = parquetProps.newDefinitionLevelWriter(column);    for (int i = 0; i < 10; i += 1) {        rdValues.writeInteger(0);    }        BytesInput rd = BytesInput.from(rdValues.getBytes().toByteArray());    DeltaByteArrayWriter writer = getDeltaByteArrayWriter();    String lastValue = null;    List<String> values = new ArrayList<String>();    for (int i = 0; i < 10; i += 1) {        lastValue = str(i);        writer.writeBytes(Binary.fromString(lastValue));        values.add(lastValue);    }    memWriter.writePage(BytesInput.concat(rd, rd, writer.getBytes()), 10, /* number of values in the page */    new BinaryStatistics(), rdValues.getEncoding(), rdValues.getEncoding(), writer.getEncoding());    pages.addRowCount(10);        writer.reset();    corruptWriter(writer, lastValue);    for (int i = 10; i < 20; i += 1) {        String value = str(i);        writer.writeBytes(Binary.fromString(value));        values.add(value);    }    memWriter.writePage(BytesInput.concat(rd, rd, writer.getBytes()), 10, /* number of values in the page */    new BinaryStatistics(), rdValues.getEncoding(), rdValues.getEncoding(), writer.getEncoding());    pages.addRowCount(10);    final List<String> actualValues = new ArrayList<String>();    PrimitiveConverter converter = new PrimitiveConverter() {        @Override        public void addBinary(Binary value) {            actualValues.add(value.toStringUsingUTF8());        }    };    ColumnReaderImpl columnReader = new ColumnReaderImpl(column, pages.getPageReader(column), converter, new ParsedVersion("parquet-mr", "1.6.0", "abcd"));    while (actualValues.size() < columnReader.getTotalValueCount()) {        columnReader.writeCurrentValueToConverter();        columnReader.consume();    }    Assert.assertEquals(values, actualValues);}
0
public void addBinary(Binary value)
{    actualValues.add(value.toStringUsingUTF8());}
0
public void corruptWriter(DeltaByteArrayWriter writer, String data) throws Exception
{    Field previous = writer.getClass().getDeclaredField("previous");    previous.setAccessible(true);    previous.set(writer, Binary.fromString(data).getBytesUnsafe());}
0
public String str(int i)
{    char c = 'a';    return "aaaaaaaaaaa" + (char) (c + i);}
0
public void testMemColumn() throws Exception
{    MessageType schema = MessageTypeParser.parseMessageType("message msg { required group foo { required int64 bar; } }");    ColumnDescriptor path = schema.getColumnDescription(new String[] { "foo", "bar" });    MemPageStore memPageStore = new MemPageStore(10);    ColumnWriteStoreV1 memColumnsStore = newColumnWriteStoreImpl(memPageStore);    ColumnWriter columnWriter = memColumnsStore.getColumnWriter(path);    columnWriter.write(42l, 0, 0);    memColumnsStore.endRecord();    memColumnsStore.flush();    ColumnReader columnReader = getColumnReader(memPageStore, path, schema);    for (int i = 0; i < columnReader.getTotalValueCount(); i++) {        assertEquals(columnReader.getCurrentRepetitionLevel(), 0);        assertEquals(columnReader.getCurrentDefinitionLevel(), 0);        assertEquals(columnReader.getLong(), 42);        columnReader.consume();    }}
0
private ColumnWriter getColumnWriter(ColumnDescriptor path, MemPageStore memPageStore)
{    ColumnWriteStoreV1 memColumnsStore = newColumnWriteStoreImpl(memPageStore);    ColumnWriter columnWriter = memColumnsStore.getColumnWriter(path);    return columnWriter;}
0
private ColumnReader getColumnReader(MemPageStore memPageStore, ColumnDescriptor path, MessageType schema)
{    return new ColumnReadStoreImpl(memPageStore, new DummyRecordConverter(schema).getRootConverter(), schema, null).getColumnReader(path);}
0
public void testMemColumnBinary() throws Exception
{    MessageType mt = MessageTypeParser.parseMessageType("message msg { required group foo { required binary bar; } }");    String[] col = new String[] { "foo", "bar" };    MemPageStore memPageStore = new MemPageStore(10);    ColumnWriteStoreV1 memColumnsStore = newColumnWriteStoreImpl(memPageStore);    ColumnDescriptor path1 = mt.getColumnDescription(col);    ColumnDescriptor path = path1;    ColumnWriter columnWriter = memColumnsStore.getColumnWriter(path);    columnWriter.write(Binary.fromString("42"), 0, 0);    memColumnsStore.endRecord();    memColumnsStore.flush();    ColumnReader columnReader = getColumnReader(memPageStore, path, mt);    for (int i = 0; i < columnReader.getTotalValueCount(); i++) {        assertEquals(columnReader.getCurrentRepetitionLevel(), 0);        assertEquals(columnReader.getCurrentDefinitionLevel(), 0);        assertEquals(columnReader.getBinary().toStringUsingUTF8(), "42");        columnReader.consume();    }}
0
public void testMemColumnSeveralPages() throws Exception
{    MessageType mt = MessageTypeParser.parseMessageType("message msg { required group foo { required int64 bar; } }");    String[] col = new String[] { "foo", "bar" };    MemPageStore memPageStore = new MemPageStore(10);    ColumnWriteStoreV1 memColumnsStore = newColumnWriteStoreImpl(memPageStore);    ColumnDescriptor path1 = mt.getColumnDescription(col);    ColumnDescriptor path = path1;    ColumnWriter columnWriter = memColumnsStore.getColumnWriter(path);    for (int i = 0; i < 2000; i++) {        columnWriter.write(42l, 0, 0);        memColumnsStore.endRecord();    }    memColumnsStore.flush();    ColumnReader columnReader = getColumnReader(memPageStore, path, mt);    for (int i = 0; i < columnReader.getTotalValueCount(); i++) {        assertEquals(columnReader.getCurrentRepetitionLevel(), 0);        assertEquals(columnReader.getCurrentDefinitionLevel(), 0);        assertEquals(columnReader.getLong(), 42);        columnReader.consume();    }}
0
public void testMemColumnSeveralPagesRepeated() throws Exception
{    MessageType mt = MessageTypeParser.parseMessageType("message msg { repeated group foo { repeated int64 bar; } }");    String[] col = new String[] { "foo", "bar" };    MemPageStore memPageStore = new MemPageStore(10);    ColumnWriteStoreV1 memColumnsStore = newColumnWriteStoreImpl(memPageStore);    ColumnDescriptor path1 = mt.getColumnDescription(col);    ColumnDescriptor path = path1;    ColumnWriter columnWriter = memColumnsStore.getColumnWriter(path);    int[] rs = { 0, 0, 0, 1, 1, 1, 2, 2, 2 };    int[] ds = { 0, 1, 2, 0, 1, 2, 0, 1, 2 };    for (int i = 0; i < 837; i++) {        int r = rs[i % rs.length];        int d = ds[i % ds.length];                if (i != 0 && r == 0) {            memColumnsStore.endRecord();        }        if (d == 2) {            columnWriter.write((long) i, r, d);        } else {            columnWriter.writeNull(r, d);        }    }    memColumnsStore.endRecord();    memColumnsStore.flush();    ColumnReader columnReader = getColumnReader(memPageStore, path, mt);    int i = 0;    for (int j = 0; j < columnReader.getTotalValueCount(); j++) {        int r = rs[i % rs.length];        int d = ds[i % ds.length];                assertEquals("r row " + i, r, columnReader.getCurrentRepetitionLevel());        assertEquals("d row " + i, d, columnReader.getCurrentDefinitionLevel());        if (d == 2) {            assertEquals("data row " + i, (long) i, columnReader.getLong());        }        columnReader.consume();        ++i;    }}
1
public void testPageSize()
{    MessageType schema = Types.buildMessage().requiredList().requiredElement(BINARY).named("binary_col").requiredList().requiredElement(INT32).named("int32_col").named("msg");    System.out.println(schema);    MemPageStore memPageStore = new MemPageStore(123);        ColumnWriteStore writeStore = new ColumnWriteStoreV2(schema, memPageStore, ParquetProperties.builder().withPageSize(    1024).withMinRowCountForPageSizeCheck(    1).withPageRowCountLimit(10).withDictionaryEncoding(    false).build());    ColumnDescriptor binaryCol = schema.getColumnDescription(new String[] { "binary_col", "list", "element" });    ColumnWriter binaryColWriter = writeStore.getColumnWriter(binaryCol);    ColumnDescriptor int32Col = schema.getColumnDescription(new String[] { "int32_col", "list", "element" });    ColumnWriter int32ColWriter = writeStore.getColumnWriter(int32Col);        for (int i = 0; i < 123; ++i) {                for (int j = 0; j < 10; ++j) {            binaryColWriter.write(Binary.fromString("aaaaaaaaaaaa"), j == 0 ? 0 : 2, 2);            int32ColWriter.write(42, j == 0 ? 0 : 2, 2);        }        writeStore.endRecord();    }    writeStore.flush();        {        PageReader binaryColPageReader = memPageStore.getPageReader(binaryCol);        assertEquals(1230, binaryColPageReader.getTotalValueCount());        int pageCnt = 0;        int valueCnt = 0;        while (valueCnt < binaryColPageReader.getTotalValueCount()) {            DataPage page = binaryColPageReader.readPage();            ++pageCnt;            valueCnt += page.getValueCount();                        assertTrue("Compressed size should be less than 1024", page.getCompressedSize() <= 1024);        }    }        {        PageReader int32ColPageReader = memPageStore.getPageReader(int32Col);        assertEquals(1230, int32ColPageReader.getTotalValueCount());        int pageCnt = 0;        int valueCnt = 0;        while (valueCnt < int32ColPageReader.getTotalValueCount()) {            DataPage page = int32ColPageReader.readPage();            ++pageCnt;            valueCnt += page.getValueCount();                        assertTrue("Row count should be less than 10", page.getIndexRowCount().get() <= 10);        }    }}
1
private ColumnWriteStoreV1 newColumnWriteStoreImpl(MemPageStore memPageStore)
{    return new ColumnWriteStoreV1(memPageStore, ParquetProperties.builder().withPageSize(2048).withDictionaryEncoding(false).build());}
0
public void test() throws IOException
{    MemPageStore memPageStore = new MemPageStore(10);    ColumnDescriptor col = new ColumnDescriptor(path, PrimitiveTypeName.INT64, 2, 2);    LongStatistics stats = new LongStatistics();    PageWriter pageWriter = memPageStore.getPageWriter(col);    pageWriter.writePage(BytesInput.from(new byte[735]), 209, stats, BIT_PACKED, BIT_PACKED, PLAIN);    pageWriter.writePage(BytesInput.from(new byte[743]), 209, stats, BIT_PACKED, BIT_PACKED, PLAIN);    pageWriter.writePage(BytesInput.from(new byte[743]), 209, stats, BIT_PACKED, BIT_PACKED, PLAIN);    pageWriter.writePage(BytesInput.from(new byte[735]), 209, stats, BIT_PACKED, BIT_PACKED, PLAIN);    PageReader pageReader = memPageStore.getPageReader(col);    long totalValueCount = pageReader.getTotalValueCount();    System.out.println(totalValueCount);    int total = 0;    do {        DataPage readPage = pageReader.readPage();        total += readPage.getValueCount();        System.out.println(readPage);        } while (total < totalValueCount);}
0
public long getTotalValueCount()
{    return totalValueCount;}
0
public DataPage readPage()
{    if (pages.hasNext()) {        DataPage next = pages.next();                return next;    } else {        throw new ParquetDecodingException("after last page");    }}
1
public DictionaryPage readDictionaryPage()
{    return dictionaryPage;}
0
public PageWriter getPageWriter(ColumnDescriptor path)
{    MemPageWriter pageWriter = pageWriters.get(path);    if (pageWriter == null) {        pageWriter = new MemPageWriter();        pageWriters.put(path, pageWriter);    }    return pageWriter;}
0
public PageReader getPageReader(ColumnDescriptor descriptor)
{    MemPageWriter pageWriter = pageWriters.get(descriptor);    if (pageWriter == null) {        throw new UnknownColumnException(descriptor);    }    List<DataPage> pages = new ArrayList<DataPage>(pageWriter.getPages());        return new MemPageReader(pageWriter.getTotalValueCount(), pages.iterator(), pageWriter.getDictionaryPage());}
1
public long getRowCount()
{    return rowCount;}
0
public void addRowCount(long count)
{    rowCount += count;}
0
public void writePage(BytesInput bytesInput, int valueCount, Statistics statistics, Encoding rlEncoding, Encoding dlEncoding, Encoding valuesEncoding) throws IOException
{    if (valueCount == 0) {        throw new ParquetEncodingException("illegal page of 0 values");    }    memSize += bytesInput.size();    pages.add(new DataPageV1(BytesInput.copy(bytesInput), valueCount, (int) bytesInput.size(), statistics, rlEncoding, dlEncoding, valuesEncoding));    totalValueCount += valueCount;    }
1
public void writePage(BytesInput bytesInput, int valueCount, int rowCount, Statistics<?> statistics, Encoding rlEncoding, Encoding dlEncoding, Encoding valuesEncoding) throws IOException
{    writePage(bytesInput, valueCount, statistics, rlEncoding, dlEncoding, valuesEncoding);}
0
public void writePageV2(int rowCount, int nullCount, int valueCount, BytesInput repetitionLevels, BytesInput definitionLevels, Encoding dataEncoding, BytesInput data, Statistics<?> statistics) throws IOException
{    if (valueCount == 0) {        throw new ParquetEncodingException("illegal page of 0 values");    }    long size = repetitionLevels.size() + definitionLevels.size() + data.size();    memSize += size;    pages.add(DataPageV2.uncompressed(rowCount, nullCount, valueCount, copy(repetitionLevels), copy(definitionLevels), dataEncoding, copy(data), statistics));    totalValueCount += valueCount;    }
1
public long getMemSize()
{    return memSize;}
0
public List<DataPage> getPages()
{    return pages;}
0
public DictionaryPage getDictionaryPage()
{    return dictionaryPage;}
0
public long getTotalValueCount()
{    return totalValueCount;}
0
public long allocatedSize()
{        return memSize;}
0
public void writeDictionaryPage(DictionaryPage dictionaryPage) throws IOException
{    if (this.dictionaryPage != null) {        throw new ParquetEncodingException("Only one dictionary page per block");    }    this.memSize += dictionaryPage.getBytes().size();    this.dictionaryPage = dictionaryPage.copy();    }
1
public String memUsageString(String prefix)
{    return String.format("%s %,d bytes", prefix, memSize);}
0
public void testNumNulls()
{    IntStatistics stats = new IntStatistics();    assertTrue(stats.isNumNullsSet());    assertEquals(stats.getNumNulls(), 0);    stats.incrementNumNulls();    stats.incrementNumNulls();    stats.incrementNumNulls();    stats.incrementNumNulls();    assertEquals(stats.getNumNulls(), 4);    stats.incrementNumNulls(5);    assertEquals(stats.getNumNulls(), 9);    stats.setNumNulls(22);    assertEquals(stats.getNumNulls(), 22);}
0
public void testIntMinMax()
{        integerArray = new int[] { 1, 3, 14, 54, 66, 8, 0, 23, 54 };    IntStatistics stats = new IntStatistics();    for (int i : integerArray) {        stats.updateStats(i);    }    assertEquals(stats.getMax(), 66);    assertEquals(stats.getMin(), 0);        integerArray = new int[] { -11, 3, -14, 54, -66, 8, 0, -23, 54 };    IntStatistics statsNeg = new IntStatistics();    for (int i : integerArray) {        statsNeg.updateStats(i);    }    assertEquals(statsNeg.getMax(), 54);    assertEquals(statsNeg.getMin(), -66);    assertTrue(statsNeg.compareMaxToValue(55) < 0);    assertTrue(statsNeg.compareMaxToValue(54) == 0);    assertTrue(statsNeg.compareMaxToValue(5) > 0);    assertTrue(statsNeg.compareMinToValue(0) < 0);    assertTrue(statsNeg.compareMinToValue(-66) == 0);    assertTrue(statsNeg.compareMinToValue(-67) > 0);        byte[] intMaxBytes = statsNeg.getMaxBytes();    byte[] intMinBytes = statsNeg.getMinBytes();    assertEquals(ByteBuffer.wrap(intMaxBytes).order(java.nio.ByteOrder.LITTLE_ENDIAN).getInt(), 54);    assertEquals(ByteBuffer.wrap(intMinBytes).order(java.nio.ByteOrder.LITTLE_ENDIAN).getInt(), -66);    IntStatistics statsFromBytes = new IntStatistics();    statsFromBytes.setMinMaxFromBytes(intMinBytes, intMaxBytes);    assertEquals(statsFromBytes.getMax(), 54);    assertEquals(statsFromBytes.getMin(), -66);    integerArray = new int[] { Integer.MAX_VALUE, Integer.MIN_VALUE };    IntStatistics minMaxValues = new IntStatistics();    for (int i : integerArray) {        minMaxValues.updateStats(i);    }    assertEquals(minMaxValues.getMax(), Integer.MAX_VALUE);    assertEquals(minMaxValues.getMin(), Integer.MIN_VALUE);        byte[] intMaxBytesMinMax = minMaxValues.getMaxBytes();    byte[] intMinBytesMinMax = minMaxValues.getMinBytes();    assertEquals(ByteBuffer.wrap(intMaxBytesMinMax).order(java.nio.ByteOrder.LITTLE_ENDIAN).getInt(), Integer.MAX_VALUE);    assertEquals(ByteBuffer.wrap(intMinBytesMinMax).order(java.nio.ByteOrder.LITTLE_ENDIAN).getInt(), Integer.MIN_VALUE);    IntStatistics statsFromBytesMinMax = new IntStatistics();    statsFromBytesMinMax.setMinMaxFromBytes(intMinBytesMinMax, intMaxBytesMinMax);    assertEquals(statsFromBytesMinMax.getMax(), Integer.MAX_VALUE);    assertEquals(statsFromBytesMinMax.getMin(), Integer.MIN_VALUE);        assertEquals(stats.toString(), "min: 0, max: 66, num_nulls: 0");}
0
public void testLongMinMax()
{        longArray = new long[] { 9, 39, 99, 3, 0, 12, 1000, 65, 542 };    LongStatistics stats = new LongStatistics();    for (long l : longArray) {        stats.updateStats(l);    }    assertEquals(stats.getMax(), 1000);    assertEquals(stats.getMin(), 0);        longArray = new long[] { -101, 993, -9914, 54, -9, 89, 0, -23, 90 };    LongStatistics statsNeg = new LongStatistics();    for (long l : longArray) {        statsNeg.updateStats(l);    }    assertEquals(statsNeg.getMax(), 993);    assertEquals(statsNeg.getMin(), -9914);    assertTrue(statsNeg.compareMaxToValue(994) < 0);    assertTrue(statsNeg.compareMaxToValue(993) == 0);    assertTrue(statsNeg.compareMaxToValue(-1000) > 0);    assertTrue(statsNeg.compareMinToValue(10000) < 0);    assertTrue(statsNeg.compareMinToValue(-9914) == 0);    assertTrue(statsNeg.compareMinToValue(-9915) > 0);        byte[] longMaxBytes = statsNeg.getMaxBytes();    byte[] longMinBytes = statsNeg.getMinBytes();    assertEquals(ByteBuffer.wrap(longMaxBytes).order(java.nio.ByteOrder.LITTLE_ENDIAN).getLong(), 993);    assertEquals(ByteBuffer.wrap(longMinBytes).order(java.nio.ByteOrder.LITTLE_ENDIAN).getLong(), -9914);    LongStatistics statsFromBytes = new LongStatistics();    statsFromBytes.setMinMaxFromBytes(longMinBytes, longMaxBytes);    assertEquals(statsFromBytes.getMax(), 993);    assertEquals(statsFromBytes.getMin(), -9914);    longArray = new long[] { Long.MAX_VALUE, Long.MIN_VALUE };    LongStatistics minMaxValues = new LongStatistics();    for (long l : longArray) {        minMaxValues.updateStats(l);    }    assertEquals(minMaxValues.getMax(), Long.MAX_VALUE);    assertEquals(minMaxValues.getMin(), Long.MIN_VALUE);        byte[] longMaxBytesMinMax = minMaxValues.getMaxBytes();    byte[] longMinBytesMinMax = minMaxValues.getMinBytes();    assertEquals(ByteBuffer.wrap(longMaxBytesMinMax).order(java.nio.ByteOrder.LITTLE_ENDIAN).getLong(), Long.MAX_VALUE);    assertEquals(ByteBuffer.wrap(longMinBytesMinMax).order(java.nio.ByteOrder.LITTLE_ENDIAN).getLong(), Long.MIN_VALUE);    LongStatistics statsFromBytesMinMax = new LongStatistics();    statsFromBytesMinMax.setMinMaxFromBytes(longMinBytesMinMax, longMaxBytesMinMax);    assertEquals(statsFromBytesMinMax.getMax(), Long.MAX_VALUE);    assertEquals(statsFromBytesMinMax.getMin(), Long.MIN_VALUE);        assertEquals(stats.toString(), "min: 0, max: 1000, num_nulls: 0");}
0
public void testFloatMinMax()
{        floatArray = new float[] { 1.5f, 44.5f, 412.99f, 0.65f, 5.6f, 100.6f, 0.0001f, 23.0f, 553.6f };    FloatStatistics stats = new FloatStatistics();    for (float f : floatArray) {        stats.updateStats(f);    }    assertEquals(stats.getMax(), 553.6f, 1e-10);    assertEquals(stats.getMin(), 0.0001f, 1e-10);        floatArray = new float[] { -1.5f, -44.5f, -412.99f, 0.65f, -5.6f, -100.6f, 0.0001f, -23.0f, -3.6f };    FloatStatistics statsNeg = new FloatStatistics();    for (float f : floatArray) {        statsNeg.updateStats(f);    }    assertEquals(statsNeg.getMax(), 0.65f, 1e-10);    assertEquals(statsNeg.getMin(), -412.99f, 1e-10);    assertTrue(statsNeg.compareMaxToValue(1) < 0);    assertTrue(statsNeg.compareMaxToValue(0.65F) == 0);    assertTrue(statsNeg.compareMaxToValue(0.649F) > 0);    assertTrue(statsNeg.compareMinToValue(-412.98F) < 0);    assertTrue(statsNeg.compareMinToValue(-412.99F) == 0);    assertTrue(statsNeg.compareMinToValue(-450) > 0);        byte[] floatMaxBytes = statsNeg.getMaxBytes();    byte[] floatMinBytes = statsNeg.getMinBytes();    assertEquals(ByteBuffer.wrap(floatMaxBytes).order(java.nio.ByteOrder.LITTLE_ENDIAN).getFloat(), 0.65f, 1e-10);    assertEquals(ByteBuffer.wrap(floatMinBytes).order(java.nio.ByteOrder.LITTLE_ENDIAN).getFloat(), -412.99f, 1e-10);    FloatStatistics statsFromBytes = new FloatStatistics();    statsFromBytes.setMinMaxFromBytes(floatMinBytes, floatMaxBytes);    assertEquals(statsFromBytes.getMax(), 0.65f, 1e-10);    assertEquals(statsFromBytes.getMin(), -412.99f, 1e-10);    floatArray = new float[] { Float.MAX_VALUE, Float.MIN_VALUE };    FloatStatistics minMaxValues = new FloatStatistics();    for (float f : floatArray) {        minMaxValues.updateStats(f);    }    assertEquals(minMaxValues.getMax(), Float.MAX_VALUE, 1e-10);    assertEquals(minMaxValues.getMin(), Float.MIN_VALUE, 1e-10);        byte[] floatMaxBytesMinMax = minMaxValues.getMaxBytes();    byte[] floatMinBytesMinMax = minMaxValues.getMinBytes();    assertEquals(ByteBuffer.wrap(floatMaxBytesMinMax).order(java.nio.ByteOrder.LITTLE_ENDIAN).getFloat(), Float.MAX_VALUE, 1e-10);    assertEquals(ByteBuffer.wrap(floatMinBytesMinMax).order(java.nio.ByteOrder.LITTLE_ENDIAN).getFloat(), Float.MIN_VALUE, 1e-10);    FloatStatistics statsFromBytesMinMax = new FloatStatistics();    statsFromBytesMinMax.setMinMaxFromBytes(floatMinBytesMinMax, floatMaxBytesMinMax);    assertEquals(statsFromBytesMinMax.getMax(), Float.MAX_VALUE, 1e-10);    assertEquals(statsFromBytesMinMax.getMin(), Float.MIN_VALUE, 1e-10);        assertEquals("min: 1.0E-4, max: 553.6, num_nulls: 0", stats.toString());}
0
public void testDoubleMinMax()
{        doubleArray = new double[] { 81.5d, 944.5f, 2.002d, 334.5d, 5.6d, 0.001d, 0.00001d, 23.0d, 553.6d };    DoubleStatistics stats = new DoubleStatistics();    for (double d : doubleArray) {        stats.updateStats(d);    }    assertEquals(stats.getMax(), 944.5d, 1e-10);    assertEquals(stats.getMin(), 0.00001d, 1e-10);        doubleArray = new double[] { -81.5d, -944.5d, 2.002d, -334.5d, -5.6d, -0.001d, -0.00001d, 23.0d, -3.6d };    DoubleStatistics statsNeg = new DoubleStatistics();    for (double d : doubleArray) {        statsNeg.updateStats(d);    }    assertEquals(statsNeg.getMax(), 23.0d, 1e-10);    assertEquals(statsNeg.getMin(), -944.5d, 1e-10);    assertTrue(statsNeg.compareMaxToValue(23.0001D) < 0);    assertTrue(statsNeg.compareMaxToValue(23D) == 0);    assertTrue(statsNeg.compareMaxToValue(0D) > 0);    assertTrue(statsNeg.compareMinToValue(-400D) < 0);    assertTrue(statsNeg.compareMinToValue(-944.5D) == 0);    assertTrue(statsNeg.compareMinToValue(-944.500001D) > 0);        byte[] doubleMaxBytes = statsNeg.getMaxBytes();    byte[] doubleMinBytes = statsNeg.getMinBytes();    assertEquals(ByteBuffer.wrap(doubleMaxBytes).order(java.nio.ByteOrder.LITTLE_ENDIAN).getDouble(), 23.0d, 1e-10);    assertEquals(ByteBuffer.wrap(doubleMinBytes).order(java.nio.ByteOrder.LITTLE_ENDIAN).getDouble(), -944.5d, 1e-10);    DoubleStatistics statsFromBytes = new DoubleStatistics();    statsFromBytes.setMinMaxFromBytes(doubleMinBytes, doubleMaxBytes);    assertEquals(statsFromBytes.getMax(), 23.0d, 1e-10);    assertEquals(statsFromBytes.getMin(), -944.5d, 1e-10);    doubleArray = new double[] { Double.MAX_VALUE, Double.MIN_VALUE };    DoubleStatistics minMaxValues = new DoubleStatistics();    for (double d : doubleArray) {        minMaxValues.updateStats(d);    }    assertEquals(minMaxValues.getMax(), Double.MAX_VALUE, 1e-10);    assertEquals(minMaxValues.getMin(), Double.MIN_VALUE, 1e-10);        byte[] doubleMaxBytesMinMax = minMaxValues.getMaxBytes();    byte[] doubleMinBytesMinMax = minMaxValues.getMinBytes();    assertEquals(ByteBuffer.wrap(doubleMaxBytesMinMax).order(java.nio.ByteOrder.LITTLE_ENDIAN).getDouble(), Double.MAX_VALUE, 1e-10);    assertEquals(ByteBuffer.wrap(doubleMinBytesMinMax).order(java.nio.ByteOrder.LITTLE_ENDIAN).getDouble(), Double.MIN_VALUE, 1e-10);    DoubleStatistics statsFromBytesMinMax = new DoubleStatistics();    statsFromBytesMinMax.setMinMaxFromBytes(doubleMinBytesMinMax, doubleMaxBytesMinMax);    assertEquals(statsFromBytesMinMax.getMax(), Double.MAX_VALUE, 1e-10);    assertEquals(statsFromBytesMinMax.getMin(), Double.MIN_VALUE, 1e-10);        assertEquals("min: 1.0E-5, max: 944.5, num_nulls: 0", stats.toString());}
0
public void testFloatingPointStringIndependentFromLocale()
{    Statistics<?> floatStats = Statistics.createStats(Types.optional(PrimitiveTypeName.FLOAT).named("test-float"));    floatStats.updateStats(123.456f);    Statistics<?> doubleStats = Statistics.createStats(Types.optional(PrimitiveTypeName.DOUBLE).named("test-double"));    doubleStats.updateStats(12345.6789);    Locale defaultLocale = Locale.getDefault();    try {                Locale.setDefault(Locale.FRENCH);        assertEquals("min: 123.456, max: 123.456, num_nulls: 0", floatStats.toString());        assertEquals("min: 12345.6789, max: 12345.6789, num_nulls: 0", doubleStats.toString());    } finally {        Locale.setDefault(defaultLocale);    }}
0
public void testBooleanMinMax()
{        booleanArray = new boolean[] { true, true, true };    BooleanStatistics statsTrue = new BooleanStatistics();    for (boolean i : booleanArray) {        statsTrue.updateStats(i);    }    assertTrue(statsTrue.getMax());    assertTrue(statsTrue.getMin());        booleanArray = new boolean[] { false, false, false };    BooleanStatistics statsFalse = new BooleanStatistics();    for (boolean i : booleanArray) {        statsFalse.updateStats(i);    }    assertFalse(statsFalse.getMax());    assertFalse(statsFalse.getMin());    booleanArray = new boolean[] { false, true, false };    BooleanStatistics statsBoth = new BooleanStatistics();    for (boolean i : booleanArray) {        statsBoth.updateStats(i);    }    assertTrue(statsBoth.getMax());    assertFalse(statsBoth.getMin());        byte[] boolMaxBytes = statsBoth.getMaxBytes();    byte[] boolMinBytes = statsBoth.getMinBytes();    assertEquals((int) (boolMaxBytes[0] & 255), 1);    assertEquals((int) (boolMinBytes[0] & 255), 0);    BooleanStatistics statsFromBytes = new BooleanStatistics();    statsFromBytes.setMinMaxFromBytes(boolMinBytes, boolMaxBytes);    assertTrue(statsFromBytes.getMax());    assertFalse(statsFromBytes.getMin());        assertEquals(statsBoth.toString(), "min: false, max: true, num_nulls: 0");}
0
public void testBinaryMinMax()
{        stringArray = new String[] { "hello", "world", "this", "is", "a", "test", "of", "the", "stats", "class" };    PrimitiveType type = Types.optional(PrimitiveTypeName.BINARY).as(OriginalType.UTF8).named("test_binary_utf8");    BinaryStatistics stats = (BinaryStatistics) Statistics.createStats(type);    for (String s : stringArray) {        stats.updateStats(Binary.fromString(s));    }    assertEquals(stats.genericGetMax(), Binary.fromString("world"));    assertEquals(stats.genericGetMin(), Binary.fromString("a"));        stringArray = new String[] { "", "", "", "", "" };    BinaryStatistics statsEmpty = (BinaryStatistics) Statistics.createStats(type);    for (String s : stringArray) {        statsEmpty.updateStats(Binary.fromString(s));    }    assertEquals(statsEmpty.genericGetMax(), Binary.fromString(""));    assertEquals(statsEmpty.genericGetMin(), Binary.fromString(""));        byte[] stringMaxBytes = stats.getMaxBytes();    byte[] stringMinBytes = stats.getMinBytes();    assertEquals(new String(stringMaxBytes), "world");    assertEquals(new String(stringMinBytes), "a");    BinaryStatistics statsFromBytes = (BinaryStatistics) Statistics.createStats(type);    statsFromBytes.setMinMaxFromBytes(stringMinBytes, stringMaxBytes);    assertEquals(statsFromBytes.genericGetMax(), Binary.fromString("world"));    assertEquals(statsFromBytes.genericGetMin(), Binary.fromString("a"));        assertEquals(stats.toString(), "min: a, max: world, num_nulls: 0");}
0
public void testBinaryMinMaxForReusedBackingByteArray()
{    BinaryStatistics stats = new BinaryStatistics();    byte[] bytes = new byte[] { 10 };    final Binary value = Binary.fromReusedByteArray(bytes);    stats.updateStats(value);    bytes[0] = 20;    stats.updateStats(value);    bytes[0] = 15;    stats.updateStats(value);    assertArrayEquals(new byte[] { 20 }, stats.getMaxBytes());    assertArrayEquals(new byte[] { 10 }, stats.getMinBytes());}
0
public void testMergingStatistics()
{    testMergingIntStats();    testMergingLongStats();    testMergingFloatStats();    testMergingDoubleStats();    testMergingBooleanStats();    testMergingStringStats();}
0
private void testMergingIntStats()
{    integerArray = new int[] { 1, 2, 3, 4, 5 };    IntStatistics intStats = new IntStatistics();    for (int s : integerArray) {        intStats.updateStats(s);    }    integerArray = new int[] { 0, 3, 3 };    IntStatistics intStats2 = new IntStatistics();    for (int s : integerArray) {        intStats2.updateStats(s);    }    intStats.mergeStatistics(intStats2);    assertEquals(intStats.getMax(), 5);    assertEquals(intStats.getMin(), 0);    integerArray = new int[] { -1, -100, 100 };    IntStatistics intStats3 = new IntStatistics();    for (int s : integerArray) {        intStats3.updateStats(s);    }    intStats.mergeStatistics(intStats3);    assertEquals(intStats.getMax(), 100);    assertEquals(intStats.getMin(), -100);}
0
private void testMergingLongStats()
{    longArray = new long[] { 1l, 2l, 3l, 4l, 5l };    LongStatistics longStats = new LongStatistics();    for (long s : longArray) {        longStats.updateStats(s);    }    longArray = new long[] { 0l, 3l, 3l };    LongStatistics longStats2 = new LongStatistics();    for (long s : longArray) {        longStats2.updateStats(s);    }    longStats.mergeStatistics(longStats2);    assertEquals(longStats.getMax(), 5l);    assertEquals(longStats.getMin(), 0l);    longArray = new long[] { -1l, -100l, 100l };    LongStatistics longStats3 = new LongStatistics();    for (long s : longArray) {        longStats3.updateStats(s);    }    longStats.mergeStatistics(longStats3);    assertEquals(longStats.getMax(), 100l);    assertEquals(longStats.getMin(), -100l);}
0
private void testMergingFloatStats()
{    floatArray = new float[] { 1.44f, 12.2f, 98.3f, 1.4f, 0.05f };    FloatStatistics floatStats = new FloatStatistics();    for (float s : floatArray) {        floatStats.updateStats(s);    }    floatArray = new float[] { 0.0001f, 9.9f, 3.1f };    FloatStatistics floatStats2 = new FloatStatistics();    for (float s : floatArray) {        floatStats2.updateStats(s);    }    floatStats.mergeStatistics(floatStats2);    assertEquals(floatStats.getMax(), 98.3f, 1e-10);    assertEquals(floatStats.getMin(), 0.0001f, 1e-10);    floatArray = new float[] { -1.91f, -100.9f, 100.54f };    FloatStatistics floatStats3 = new FloatStatistics();    for (float s : floatArray) {        floatStats3.updateStats(s);    }    floatStats.mergeStatistics(floatStats3);    assertEquals(floatStats.getMax(), 100.54f, 1e-10);    assertEquals(floatStats.getMin(), -100.9f, 1e-10);}
0
private void testMergingDoubleStats()
{    doubleArray = new double[] { 1.44d, 12.2d, 98.3d, 1.4d, 0.05d };    DoubleStatistics doubleStats = new DoubleStatistics();    for (double s : doubleArray) {        doubleStats.updateStats(s);    }    doubleArray = new double[] { 0.0001d, 9.9d, 3.1d };    DoubleStatistics doubleStats2 = new DoubleStatistics();    for (double s : doubleArray) {        doubleStats2.updateStats(s);    }    doubleStats.mergeStatistics(doubleStats2);    assertEquals(doubleStats.getMax(), 98.3d, 1e-10);    assertEquals(doubleStats.getMin(), 0.0001d, 1e-10);    doubleArray = new double[] { -1.91d, -100.9d, 100.54d };    DoubleStatistics doubleStats3 = new DoubleStatistics();    for (double s : doubleArray) {        doubleStats3.updateStats(s);    }    doubleStats.mergeStatistics(doubleStats3);    assertEquals(doubleStats.getMax(), 100.54d, 1e-10);    assertEquals(doubleStats.getMin(), -100.9d, 1e-10);}
0
private void testMergingBooleanStats()
{    booleanArray = new boolean[] { true, true, true };    BooleanStatistics booleanStats = new BooleanStatistics();    for (boolean s : booleanArray) {        booleanStats.updateStats(s);    }    booleanArray = new boolean[] { true, false };    BooleanStatistics booleanStats2 = new BooleanStatistics();    for (boolean s : booleanArray) {        booleanStats2.updateStats(s);    }    booleanStats.mergeStatistics(booleanStats2);    assertEquals(booleanStats.getMax(), true);    assertEquals(booleanStats.getMin(), false);    booleanArray = new boolean[] { false, false, false, false };    BooleanStatistics booleanStats3 = new BooleanStatistics();    for (boolean s : booleanArray) {        booleanStats3.updateStats(s);    }    booleanStats.mergeStatistics(booleanStats3);    assertEquals(booleanStats.getMax(), true);    assertEquals(booleanStats.getMin(), false);}
0
private void testMergingStringStats()
{    stringArray = new String[] { "hello", "world", "this", "is", "a", "test", "of", "the", "stats", "class" };    BinaryStatistics stats = new BinaryStatistics();    for (String s : stringArray) {        stats.updateStats(Binary.fromString(s));    }    stringArray = new String[] { "zzzz", "asdf", "testing" };    BinaryStatistics stats2 = new BinaryStatistics();    for (String s : stringArray) {        stats2.updateStats(Binary.fromString(s));    }    stats.mergeStatistics(stats2);    assertEquals(stats.getMax(), Binary.fromString("zzzz"));    assertEquals(stats.getMin(), Binary.fromString("a"));    stringArray = new String[] { "", "good", "testing" };    BinaryStatistics stats3 = new BinaryStatistics();    for (String s : stringArray) {        stats3.updateStats(Binary.fromString(s));    }    stats.mergeStatistics(stats3);    assertEquals(stats.getMax(), Binary.fromString("zzzz"));    assertEquals(stats.getMin(), Binary.fromString(""));}
0
public void testBuilder()
{    testBuilder(Types.required(BOOLEAN).named("test_boolean"), false, new byte[] { 0 }, true, new byte[] { 1 });    testBuilder(Types.required(INT32).named("test_int32"), -42, intToBytes(-42), 42, intToBytes(42));    testBuilder(Types.required(INT64).named("test_int64"), -42l, longToBytes(-42), 42l, longToBytes(42));    testBuilder(Types.required(FLOAT).named("test_float"), -42.0f, intToBytes(floatToIntBits(-42.0f)), 42.0f, intToBytes(floatToIntBits(42.0f)));    testBuilder(Types.required(DOUBLE).named("test_double"), -42.0, longToBytes(doubleToLongBits(-42.0)), 42.0, longToBytes(Double.doubleToLongBits(42.0f)));    byte[] min = { 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12 };    byte[] max = { 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24 };    testBuilder(Types.required(INT96).named("test_int96"), Binary.fromConstantByteArray(min), min, Binary.fromConstantByteArray(max), max);    testBuilder(Types.required(FIXED_LEN_BYTE_ARRAY).length(12).named("test_fixed"), Binary.fromConstantByteArray(min), min, Binary.fromConstantByteArray(max), max);    testBuilder(Types.required(BINARY).named("test_binary"), Binary.fromConstantByteArray(min), min, Binary.fromConstantByteArray(max), max);}
0
private void testBuilder(PrimitiveType type, Object min, byte[] minBytes, Object max, byte[] maxBytes)
{    Statistics.Builder builder = Statistics.getBuilderForReading(type);    Statistics<?> stats = builder.build();    assertTrue(stats.isEmpty());    assertFalse(stats.isNumNullsSet());    assertFalse(stats.hasNonNullValue());    builder = Statistics.getBuilderForReading(type);    stats = builder.withNumNulls(0).withMin(minBytes).build();    assertFalse(stats.isEmpty());    assertTrue(stats.isNumNullsSet());    assertFalse(stats.hasNonNullValue());    assertEquals(0, stats.getNumNulls());    builder = Statistics.getBuilderForReading(type);    stats = builder.withNumNulls(11).withMax(maxBytes).build();    assertFalse(stats.isEmpty());    assertTrue(stats.isNumNullsSet());    assertFalse(stats.hasNonNullValue());    assertEquals(11, stats.getNumNulls());    builder = Statistics.getBuilderForReading(type);    stats = builder.withNumNulls(42).withMin(minBytes).withMax(maxBytes).build();    assertFalse(stats.isEmpty());    assertTrue(stats.isNumNullsSet());    assertTrue(stats.hasNonNullValue());    assertEquals(42, stats.getNumNulls());    assertEquals(min, stats.genericGetMin());    assertEquals(max, stats.genericGetMax());}
0
public void testSpecBuilderForFloat()
{    PrimitiveType type = Types.required(FLOAT).named("test_float");    Statistics.Builder builder = Statistics.getBuilderForReading(type);    Statistics<?> stats = builder.withMin(intToBytes(floatToIntBits(Float.NaN))).withMax(intToBytes(floatToIntBits(42.0f))).withNumNulls(0).build();    assertTrue(stats.isNumNullsSet());    assertEquals(0, stats.getNumNulls());    assertFalse(stats.hasNonNullValue());    builder = Statistics.getBuilderForReading(type);    stats = builder.withMin(intToBytes(floatToIntBits(-42.0f))).withMax(intToBytes(floatToIntBits(Float.NaN))).withNumNulls(11).build();    assertTrue(stats.isNumNullsSet());    assertEquals(11, stats.getNumNulls());    assertFalse(stats.hasNonNullValue());    builder = Statistics.getBuilderForReading(type);    stats = builder.withMin(intToBytes(floatToIntBits(Float.NaN))).withMax(intToBytes(floatToIntBits(Float.NaN))).withNumNulls(42).build();    assertTrue(stats.isNumNullsSet());    assertEquals(42, stats.getNumNulls());    assertFalse(stats.hasNonNullValue());    builder = Statistics.getBuilderForReading(type);    stats = builder.withMin(intToBytes(floatToIntBits(0.0f))).withMax(intToBytes(floatToIntBits(42.0f))).build();    assertEquals(0, Float.compare(-0.0f, (Float) stats.genericGetMin()));    assertEquals(0, Float.compare(42.0f, (Float) stats.genericGetMax()));    builder = Statistics.getBuilderForReading(type);    stats = builder.withMin(intToBytes(floatToIntBits(-42.0f))).withMax(intToBytes(floatToIntBits(-0.0f))).build();    assertEquals(0, Float.compare(-42.0f, (Float) stats.genericGetMin()));    assertEquals(0, Float.compare(0.0f, (Float) stats.genericGetMax()));    builder = Statistics.getBuilderForReading(type);    stats = builder.withMin(intToBytes(floatToIntBits(0.0f))).withMax(intToBytes(floatToIntBits(-0.0f))).build();    assertEquals(0, Float.compare(-0.0f, (Float) stats.genericGetMin()));    assertEquals(0, Float.compare(0.0f, (Float) stats.genericGetMax()));}
0
public void testSpecBuilderForDouble()
{    PrimitiveType type = Types.required(DOUBLE).named("test_double");    Statistics.Builder builder = Statistics.getBuilderForReading(type);    Statistics<?> stats = builder.withMin(longToBytes(doubleToLongBits(Double.NaN))).withMax(longToBytes(doubleToLongBits(42.0))).withNumNulls(0).build();    assertTrue(stats.isNumNullsSet());    assertEquals(0, stats.getNumNulls());    assertFalse(stats.hasNonNullValue());    builder = Statistics.getBuilderForReading(type);    stats = builder.withMin(longToBytes(doubleToLongBits(-42.0))).withMax(longToBytes(doubleToLongBits(Double.NaN))).withNumNulls(11).build();    assertTrue(stats.isNumNullsSet());    assertEquals(11, stats.getNumNulls());    assertFalse(stats.hasNonNullValue());    builder = Statistics.getBuilderForReading(type);    stats = builder.withMin(longToBytes(doubleToLongBits(Double.NaN))).withMax(longToBytes(doubleToLongBits(Double.NaN))).withNumNulls(42).build();    assertTrue(stats.isNumNullsSet());    assertEquals(42, stats.getNumNulls());    assertFalse(stats.hasNonNullValue());    builder = Statistics.getBuilderForReading(type);    stats = builder.withMin(longToBytes(doubleToLongBits(0.0))).withMax(longToBytes(doubleToLongBits(42.0))).build();    assertEquals(0, Double.compare(-0.0, (Double) stats.genericGetMin()));    assertEquals(0, Double.compare(42.0, (Double) stats.genericGetMax()));    builder = Statistics.getBuilderForReading(type);    stats = builder.withMin(longToBytes(doubleToLongBits(-42.0))).withMax(longToBytes(doubleToLongBits(-0.0))).build();    assertEquals(0, Double.compare(-42.0, (Double) stats.genericGetMin()));    assertEquals(0, Double.compare(0.0, (Double) stats.genericGetMax()));    builder = Statistics.getBuilderForReading(type);    stats = builder.withMin(longToBytes(doubleToLongBits(0.0))).withMax(longToBytes(doubleToLongBits(-0.0))).build();    assertEquals(0, Double.compare(-0.0, (Double) stats.genericGetMin()));    assertEquals(0, Double.compare(0.0, (Double) stats.genericGetMax()));}
0
private ColumnDescriptor column(String... path)
{    return new ColumnDescriptor(path, PrimitiveType.PrimitiveTypeName.INT32, 0, 0);}
0
public void testComparesTo() throws Exception
{    assertEquals(column("a").compareTo(column("a")), 0);    assertEquals(column("a", "b").compareTo(column("a", "b")), 0);    assertEquals(column("a").compareTo(column("b")), -1);    assertEquals(column("b").compareTo(column("a")), 1);    assertEquals(column("a", "a").compareTo(column("a", "b")), -1);    assertEquals(column("b", "a").compareTo(column("a", "a")), 1);    assertEquals(column("a").compareTo(column("a", "b")), -1);    assertEquals(column("b").compareTo(column("a", "b")), 1);    assertEquals(column("a", "b").compareTo(column("a")), 1);    assertEquals(column("a", "b").compareTo(column("b")), -1);    assertEquals(column("").compareTo(column("")), 0);    assertEquals(column("").compareTo(column("a")), -1);    assertEquals(column("a").compareTo(column("")), 1);}
0
public void testReusedBuilder()
{    EncodingStats.Builder builder = new EncodingStats.Builder();    builder.withV2Pages();    builder.addDictEncoding(Encoding.PLAIN);    builder.addDataEncoding(Encoding.RLE_DICTIONARY, 3);    builder.addDataEncoding(Encoding.DELTA_BYTE_ARRAY);    builder.addDataEncoding(Encoding.DELTA_BYTE_ARRAY);    EncodingStats stats1 = builder.build();    Map<Encoding, Integer> expectedDictStats1 = new HashMap<Encoding, Integer>();    expectedDictStats1.put(Encoding.PLAIN, 1);    Map<Encoding, Integer> expectedDataStats1 = new HashMap<Encoding, Integer>();    expectedDataStats1.put(Encoding.RLE_DICTIONARY, 3);    expectedDataStats1.put(Encoding.DELTA_BYTE_ARRAY, 2);    builder.clear();    builder.addDataEncoding(Encoding.PLAIN);    builder.addDataEncoding(Encoding.PLAIN);    builder.addDataEncoding(Encoding.PLAIN);    builder.addDataEncoding(Encoding.PLAIN);    EncodingStats stats2 = builder.build();    Map<Encoding, Integer> expectedDictStats2 = new HashMap<Encoding, Integer>();    Map<Encoding, Integer> expectedDataStats2 = new HashMap<Encoding, Integer>();    expectedDataStats2.put(Encoding.PLAIN, 4);    assertEquals("Dictionary stats should be correct", expectedDictStats2, stats2.dictStats);    assertEquals("Data stats should be correct", expectedDataStats2, stats2.dataStats);    assertEquals("Dictionary stats should be correct after reuse", expectedDictStats1, stats1.dictStats);    assertEquals("Data stats should be correct after reuse", expectedDataStats1, stats1.dataStats);}
0
public void testNoPages()
{    EncodingStats.Builder builder = new EncodingStats.Builder();    EncodingStats stats = builder.build();    assertFalse(stats.usesV2Pages());    assertFalse("Should not have dictionary-encoded pages", stats.hasDictionaryEncodedPages());    assertFalse("Should not have non-dictionary pages", stats.hasNonDictionaryEncodedPages());    assertFalse("Should not have dictionary pages", stats.hasDictionaryPages());}
0
public void testNoDataPages()
{    EncodingStats.Builder builder = new EncodingStats.Builder();    builder.addDictEncoding(Encoding.PLAIN_DICTIONARY);    EncodingStats stats = builder.build();    assertFalse(stats.usesV2Pages());    assertFalse("Should not have dictionary-encoded pages", stats.hasDictionaryEncodedPages());    assertFalse("Should not have non-dictionary pages", stats.hasNonDictionaryEncodedPages());    assertTrue("Should have dictionary pages", stats.hasDictionaryPages());}
0
public void testV1AllDictionary()
{    EncodingStats.Builder builder = new EncodingStats.Builder();    builder.addDictEncoding(Encoding.PLAIN_DICTIONARY);    builder.addDataEncoding(Encoding.PLAIN_DICTIONARY);    builder.addDataEncoding(Encoding.PLAIN_DICTIONARY);    EncodingStats stats = builder.build();    assertFalse(stats.usesV2Pages());    assertTrue("Should have dictionary-encoded pages", stats.hasDictionaryEncodedPages());    assertFalse("Should not have non-dictionary pages", stats.hasNonDictionaryEncodedPages());    assertTrue("Should have dictionary pages", stats.hasDictionaryPages());}
0
public void testV1NoDictionary()
{    EncodingStats.Builder builder = new EncodingStats.Builder();    builder.addDataEncoding(Encoding.PLAIN);    EncodingStats stats = builder.build();    assertFalse(stats.usesV2Pages());    assertFalse("Should not have dictionary-encoded pages", stats.hasDictionaryEncodedPages());    assertTrue("Should have non-dictionary pages", stats.hasNonDictionaryEncodedPages());    assertFalse("Should not have dictionary pages", stats.hasDictionaryPages());}
0
public void testV1Fallback()
{    EncodingStats.Builder builder = new EncodingStats.Builder();    builder.addDictEncoding(Encoding.PLAIN_DICTIONARY);    builder.addDataEncoding(Encoding.PLAIN_DICTIONARY);    builder.addDataEncoding(Encoding.PLAIN_DICTIONARY);    builder.addDataEncoding(Encoding.PLAIN);    EncodingStats stats = builder.build();    assertFalse(stats.usesV2Pages());    assertTrue("Should have dictionary-encoded pages", stats.hasDictionaryEncodedPages());    assertTrue("Should have non-dictionary pages", stats.hasNonDictionaryEncodedPages());    assertTrue("Should have dictionary pages", stats.hasDictionaryPages());}
0
public void testV2AllDictionary()
{    EncodingStats.Builder builder = new EncodingStats.Builder();    builder.withV2Pages();    builder.addDictEncoding(Encoding.PLAIN);    builder.addDataEncoding(Encoding.RLE_DICTIONARY);    EncodingStats stats = builder.build();    assertTrue(stats.usesV2Pages());    assertTrue("Should have dictionary-encoded pages", stats.hasDictionaryEncodedPages());    assertFalse("Should not have non-dictionary pages", stats.hasNonDictionaryEncodedPages());    assertTrue("Should have dictionary pages", stats.hasDictionaryPages());}
0
public void testV2NoDictionary()
{    EncodingStats.Builder builder = new EncodingStats.Builder();    builder.withV2Pages();    builder.addDataEncoding(Encoding.DELTA_BINARY_PACKED);    builder.addDataEncoding(Encoding.DELTA_BINARY_PACKED);    EncodingStats stats = builder.build();    assertTrue(stats.usesV2Pages());    assertFalse("Should not have dictionary-encoded pages", stats.hasDictionaryEncodedPages());    assertTrue("Should have non-dictionary pages", stats.hasNonDictionaryEncodedPages());    assertFalse("Should not have dictionary pages", stats.hasDictionaryPages());}
0
public void testV2Fallback()
{    EncodingStats.Builder builder = new EncodingStats.Builder();    builder.withV2Pages();    builder.addDictEncoding(Encoding.PLAIN);    builder.addDataEncoding(Encoding.RLE_DICTIONARY);    builder.addDataEncoding(Encoding.DELTA_BYTE_ARRAY);    builder.addDataEncoding(Encoding.DELTA_BYTE_ARRAY);    EncodingStats stats = builder.build();    assertTrue(stats.usesV2Pages());    assertTrue("Should have dictionary-encoded pages", stats.hasDictionaryEncodedPages());    assertTrue("Should have non-dictionary pages", stats.hasNonDictionaryEncodedPages());    assertTrue("Should have dictionary pages", stats.hasDictionaryPages());}
0
public void testCounts()
{    EncodingStats.Builder builder = new EncodingStats.Builder();    builder.withV2Pages();    builder.addDictEncoding(Encoding.PLAIN);    builder.addDataEncoding(Encoding.RLE_DICTIONARY, 4);    builder.addDataEncoding(Encoding.RLE_DICTIONARY);    builder.addDataEncoding(Encoding.DELTA_BYTE_ARRAY);    builder.addDataEncoding(Encoding.DELTA_BYTE_ARRAY);    EncodingStats stats = builder.build();    assertEquals("Count should match", 1, stats.getNumDictionaryPagesEncodedAs(Encoding.PLAIN));    assertEquals("Count should match", 0, stats.getNumDictionaryPagesEncodedAs(Encoding.PLAIN_DICTIONARY));    assertEquals("Count should match", 0, stats.getNumDictionaryPagesEncodedAs(Encoding.RLE));    assertEquals("Count should match", 0, stats.getNumDictionaryPagesEncodedAs(Encoding.BIT_PACKED));    assertEquals("Count should match", 0, stats.getNumDictionaryPagesEncodedAs(Encoding.DELTA_BYTE_ARRAY));    assertEquals("Count should match", 0, stats.getNumDictionaryPagesEncodedAs(Encoding.DELTA_BINARY_PACKED));    assertEquals("Count should match", 0, stats.getNumDictionaryPagesEncodedAs(Encoding.DELTA_LENGTH_BYTE_ARRAY));    assertEquals("Count should match", 5, stats.getNumDataPagesEncodedAs(Encoding.RLE_DICTIONARY));    assertEquals("Count should match", 2, stats.getNumDataPagesEncodedAs(Encoding.DELTA_BYTE_ARRAY));    assertEquals("Count should match", 0, stats.getNumDataPagesEncodedAs(Encoding.RLE));    assertEquals("Count should match", 0, stats.getNumDataPagesEncodedAs(Encoding.BIT_PACKED));    assertEquals("Count should match", 0, stats.getNumDataPagesEncodedAs(Encoding.PLAIN));    assertEquals("Count should match", 0, stats.getNumDataPagesEncodedAs(Encoding.PLAIN_DICTIONARY));    assertEquals("Count should match", 0, stats.getNumDataPagesEncodedAs(Encoding.DELTA_BINARY_PACKED));    assertEquals("Count should match", 0, stats.getNumDataPagesEncodedAs(Encoding.DELTA_LENGTH_BYTE_ARRAY));}
0
public static void main(String[] args) throws IOException
{    int COUNT = 800000;    ByteArrayOutputStream baos = new ByteArrayOutputStream();    BitPackingWriter w = BitPacking.getBitPackingWriter(1, baos);    long t0 = System.currentTimeMillis();    for (int i = 0; i < COUNT; ++i) {        w.write(i % 2);    }    w.finish();    long t1 = System.currentTimeMillis();    System.out.println("written in " + (t1 - t0) + "ms");    System.out.println();    byte[] bytes = baos.toByteArray();    System.out.println(bytes.length);    int[] result = new int[COUNT];    for (int l = 0; l < 5; l++) {        long s = manual(bytes, result);        long b = generated(bytes, result);        float ratio = (float) b / s;        System.out.println("                                             " + ratio + (ratio < 1 ? " < 1 => GOOD" : " >= 1 => BAD"));    }}
0
private static void verify(int[] result)
{    int error = 0;    for (int i = 0; i < result.length; ++i) {        if (result[i] != i % 2) {            error++;        }    }    if (error != 0) {        throw new RuntimeException("errors: " + error + " / " + result.length);    }}
0
private static long manual(byte[] bytes, int[] result) throws IOException
{    return readNTimes(bytes, result, new BitPackingValuesReader(1));}
0
private static long generated(byte[] bytes, int[] result) throws IOException
{    return readNTimes(bytes, result, new ByteBitPackingValuesReader(1, Packer.BIG_ENDIAN));}
0
private static long readNTimes(byte[] bytes, int[] result, ValuesReader r) throws IOException
{    System.out.println();    long t = 0;    int N = 10;    System.gc();    System.out.print("                                             " + r.getClass().getSimpleName());    System.out.print(" no gc <");    for (int k = 0; k < N; k++) {        long t2 = System.nanoTime();        r.initFromPage(result.length, ByteBufferInputStream.wrap(ByteBuffer.wrap(bytes)));        for (int i = 0; i < result.length; i++) {            result[i] = r.readInteger();        }        long t3 = System.nanoTime();        t += t3 - t2;    }    System.out.println("> read in " + t / 1000 + "µs " + (N * result.length / (t / 1000)) + " values per µs");    verify(result);    return t;}
0
public void testZero() throws IOException
{    int bitLength = 0;    int[] vals = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 };    String expected = "";    validateEncodeDecode(bitLength, vals, expected);}
0
public void testOne_0() throws IOException
{    int[] vals = { 0 };    String expected = "00000000";    validateEncodeDecode(1, vals, expected);}
0
public void testOne_1() throws IOException
{    int[] vals = { 1 };    String expected = "10000000";    validateEncodeDecode(1, vals, expected);}
0
public void testOne_0_0() throws IOException
{    int[] vals = { 0, 0 };    String expected = "00000000";    validateEncodeDecode(1, vals, expected);}
0
public void testOne_1_1() throws IOException
{    int[] vals = { 1, 1 };    String expected = "11000000";    validateEncodeDecode(1, vals, expected);}
0
public void testOne_9_1s() throws IOException
{    int[] vals = { 1, 1, 1, 1, 1, 1, 1, 1, 1 };    String expected = "11111111 10000000";    validateEncodeDecode(1, vals, expected);}
0
public void testOne_9_0s() throws IOException
{    int[] vals = { 0, 0, 0, 0, 0, 0, 0, 0, 0 };    String expected = "00000000 00000000";    validateEncodeDecode(1, vals, expected);}
0
public void testOne_7_0s_1_1() throws IOException
{    int[] vals = { 0, 0, 0, 0, 0, 0, 0, 1 };    String expected = "00000001";    validateEncodeDecode(1, vals, expected);}
0
public void testOne_9_0s_1_1() throws IOException
{    int[] vals = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 1 };    String expected = "00000000 01000000";    validateEncodeDecode(1, vals, expected);}
0
public void testOne() throws IOException
{    int[] vals = { 0, 1, 0, 0, 1, 1, 1, 0, 0, 1 };    String expected = "01001110 01000000";    validateEncodeDecode(1, vals, expected);}
0
public void testTwo() throws IOException
{    int[] vals = { 0, 1, 2, 3, 3, 3, 2, 1, 1, 0, 0, 0, 1 };    String expected = "00011011 11111001 01000000 01000000";    validateEncodeDecode(2, vals, expected);}
0
public void testThree() throws IOException
{    int[] vals = { 0, 1, 2, 3, 4, 5, 6, 7, 1 };    String expected = "00000101 00111001 01110111 " + "00100000";    validateEncodeDecode(3, vals, expected);}
0
public void testFour() throws IOException
{    int[] vals = { 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1 };    String expected = "00000001 00100011 01000101 01100111 10001001 10101011 11001101 11101111 00010000";    validateEncodeDecode(4, vals, expected);}
0
public void testFive() throws IOException
{    int[] vals = { 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 1 };    String expected = "00000000 01000100 00110010 00010100 11000111 " + "01000010 01010100 10110110 00110101 11001111 " + "10000100 01100101 00111010 01010110 11010111 " + "11000110 01110101 10111110 01110111 11011111 " + "00001000";    validateEncodeDecode(5, vals, expected);}
0
public void testSix() throws IOException
{    int[] vals = { 0, 28, 34, 35, 63, 1 };        String expected = "00000001 11001000 10100011 " + "11111100 00010000";    validateEncodeDecode(6, vals, expected);}
0
public void testSeven() throws IOException
{    int[] vals = { 0, 28, 34, 35, 63, 1, 125, 1, 1 };        String expected = "00000000 01110001 00010010 00110111 11100000 01111110 10000001 " + "00000010";    validateEncodeDecode(7, vals, expected);}
0
private void validateEncodeDecode(int bitLength, int[] vals, String expected) throws IOException
{    for (PACKING_TYPE type : PACKING_TYPE.values()) {                final int bound = (int) Math.pow(2, bitLength) - 1;        ValuesWriter w = type.getWriter(bound);        for (int i : vals) {            w.writeInteger(i);        }        byte[] bytes = w.getBytes().toByteArray();                        assertEquals(type.toString(), expected, TestBitPacking.toString(bytes));        ValuesReader r = type.getReader(bound);        r.initFromPage(vals.length, ByteBufferInputStream.wrap(ByteBuffer.wrap(bytes)));        int[] result = new int[vals.length];        for (int i = 0; i < result.length; i++) {            result[i] = r.readInteger();        }                assertArrayEquals(type + " result: " + TestBitPacking.toString(result), vals, result);                r.initFromPage(vals.length, ByteBufferInputStream.wrap(ByteBuffer.wrap(bytes)));        for (int i = 0; i < vals.length; i += 2) {            assertEquals(vals[i], r.readInteger());            r.skip();        }                r.initFromPage(vals.length, ByteBufferInputStream.wrap(ByteBuffer.wrap(bytes)));        int skipCount;        for (int i = 0; i < vals.length; i += skipCount + 1) {            skipCount = (vals.length - i) / 2;            assertEquals(vals[i], r.readInteger());            r.skip(skipCount);        }    }}
1
public ValuesReader getReader(final int bound)
{    return new BitPackingValuesReader(bound);}
0
public ValuesWriter getWriter(final int bound)
{    return new BitPackingValuesWriter(bound, 32 * 1024, 64 * 1024, new DirectByteBufferAllocator());}
0
public ValuesReader getReader(final int bound)
{    return new ByteBitPackingValuesReader(bound, BIG_ENDIAN);}
0
public ValuesWriter getWriter(final int bound)
{    return new ByteBitPackingValuesWriter(bound, BIG_ENDIAN);}
0
public void testBigNumbers()
{    final Random r = new Random();    testRandomIntegers(new IntFunc() {        @Override        public int getIntValue() {            return r.nextInt();        }    }, 32);}
0
public int getIntValue()
{    return r.nextInt();}
0
public void testRangedNumbersWithSmallVariations()
{    final Random r = new Random();    testRandomIntegers(new IntFunc() {        @Override        public int getIntValue() {            return 1000 + r.nextInt(20);        }    }, 10);}
0
public int getIntValue()
{    return 1000 + r.nextInt(20);}
0
public void testSmallNumbersWithSmallVariations()
{    final Random r = new Random();    testRandomIntegers(new IntFunc() {        @Override        public int getIntValue() {            return 40 + r.nextInt(20);        }    }, 6);}
0
public int getIntValue()
{    return 40 + r.nextInt(20);}
0
public void testSmallNumberVariation()
{    final Random r = new Random();    testRandomIntegers(new IntFunc() {        @Override        public int getIntValue() {            return r.nextInt(20) - 10;        }    }, 4);}
0
public int getIntValue()
{    return r.nextInt(20) - 10;}
0
public void testRandomIntegers(IntFunc func, int bitWidth)
{    DeltaBinaryPackingValuesWriter delta = new DeltaBinaryPackingValuesWriterForInteger(blockSize, miniBlockNum, 100, 20000, new DirectByteBufferAllocator());    RunLengthBitPackingHybridValuesWriter rle = new RunLengthBitPackingHybridValuesWriter(bitWidth, 100, 20000, new DirectByteBufferAllocator());    for (int i = 0; i < dataSize; i++) {        int v = func.getIntValue();        delta.writeInteger(v);        rle.writeInteger(v);    }    System.out.println("delta size: " + delta.getBytes().size());    System.out.println("estimated size" + estimatedSize());    System.out.println("rle size: " + rle.getBytes().size());}
0
private double estimatedSize()
{    int miniBlockSize = blockSize / miniBlockNum;    double miniBlockFlushed = Math.ceil(((double) dataSize - 1) / miniBlockSize);    double blockFlushed = Math.ceil(((double) dataSize - 1) / blockSize);    double estimatedSize =     4 * 5 +     4 * miniBlockFlushed * miniBlockSize +     blockFlushed * miniBlockNum +     (5.0 * blockFlushed);    return estimatedSize;}
0
public static void prepare() throws IOException
{    Random random = new Random();    data = new int[100000 * blockSize];    for (int i = 0; i < data.length; i++) {        data[i] = random.nextInt(100) - 200;    }    ValuesWriter delta = new DeltaBinaryPackingValuesWriterForInteger(blockSize, miniBlockNum, 100, 20000, new DirectByteBufferAllocator());    ValuesWriter rle = new RunLengthBitPackingHybridValuesWriter(32, 100, 20000, new DirectByteBufferAllocator());    for (int i = 0; i < data.length; i++) {        delta.writeInteger(data[i]);        rle.writeInteger(data[i]);    }    deltaBytes = delta.getBytes().toByteArray();    rleBytes = rle.getBytes().toByteArray();}
0
public void readingDelta() throws IOException
{    for (int j = 0; j < 10; j++) {        DeltaBinaryPackingValuesReader reader = new DeltaBinaryPackingValuesReader();        readData(reader, deltaBytes);    }}
0
public void readingRLE() throws IOException
{    for (int j = 0; j < 10; j++) {        ValuesReader reader = new RunLengthBitPackingHybridValuesReader(32);        readData(reader, rleBytes);    }}
0
private void readData(ValuesReader reader, byte[] deltaBytes) throws IOException
{    reader.initFromPage(data.length, ByteBufferInputStream.wrap(ByteBuffer.wrap(deltaBytes)));    for (int i = 0; i < data.length; i++) {        reader.readInteger();    }}
0
protected void runWriteTest(ValuesWriter writer)
{    int pageCount = 10;    double avg = 0.0;    for (int i = 0; i < pageCount; i++) {        writer.reset();        long startTime = System.nanoTime();        for (int item : data) {            writer.writeInteger(item);        }        long endTime = System.nanoTime();        long duration = endTime - startTime;        avg += (double) duration / pageCount;    }    System.out.println("size is " + writer.getBytes().size());}
0
public static void prepare()
{    Random random = new Random();    data = new int[10000 * blockSize];    for (int i = 0; i < data.length; i++) {        data[i] = random.nextInt(100) - 200;    }}
0
public void writeDeltaPackingTest()
{    DeltaBinaryPackingValuesWriter writer = new DeltaBinaryPackingValuesWriterForInteger(blockSize, miniBlockNum, 100, 20000, new DirectByteBufferAllocator());    runWriteTest(writer);}
0
public void writeRLETest()
{    ValuesWriter writer = new RunLengthBitPackingHybridValuesWriter(32, 100, 20000, new DirectByteBufferAllocator());    runWriteTest(writer);}
0
public void writeDeltaPackingTest2()
{    DeltaBinaryPackingValuesWriter writer = new DeltaBinaryPackingValuesWriterForInteger(blockSize, miniBlockNum, 100, 20000, new DirectByteBufferAllocator());    runWriteTest(writer);}
0
public static void prepare()
{    Random random = new Random();    data = new int[100000 * blockSize];    for (int i = 0; i < data.length; i++) {        data[i] = random.nextInt(2) - 1;    }}
0
public void writeRLEWithSmallBitWidthTest()
{    ValuesWriter writer = new RunLengthBitPackingHybridValuesWriter(2, 100, 20000, new DirectByteBufferAllocator());    runWriteTest(writer);}
0
public void setUp()
{    blockSize = 128;    miniBlockNum = 4;    writer = new DeltaBinaryPackingValuesWriterForInteger(blockSize, miniBlockNum, 100, 200, new DirectByteBufferAllocator());    random = new Random(0);}
0
public void miniBlockSizeShouldBeMultipleOf8()
{    new DeltaBinaryPackingValuesWriterForInteger(1281, 4, 100, 100, new DirectByteBufferAllocator());}
0
public void shouldWriteWhenDataIsAlignedWithBlock() throws IOException
{    int[] data = new int[5 * blockSize];    for (int i = 0; i < blockSize * 5; i++) {        data[i] = random.nextInt();    }    shouldWriteAndRead(data);}
0
public void shouldWriteAndReadWhenBlockIsNotFullyWritten() throws IOException
{    int[] data = new int[blockSize - 3];    for (int i = 0; i < data.length; i++) {        data[i] = random.nextInt();    }    shouldWriteAndRead(data);}
0
public void shouldWriteAndReadWhenAMiniBlockIsNotFullyWritten() throws IOException
{    int miniBlockSize = blockSize / miniBlockNum;    int[] data = new int[miniBlockSize - 3];    for (int i = 0; i < data.length; i++) {        data[i] = random.nextInt();    }    shouldWriteAndRead(data);}
0
public void shouldWriteNegativeDeltas() throws IOException
{    int[] data = new int[blockSize];    for (int i = 0; i < data.length; i++) {        data[i] = 10 - (i * 32 - random.nextInt(6));    }    shouldWriteAndRead(data);}
0
public void shouldWriteAndReadWhenDeltasAreSame() throws IOException
{    int[] data = new int[2 * blockSize];    for (int i = 0; i < blockSize; i++) {        data[i] = i * 32;    }    shouldWriteAndRead(data);}
0
public void shouldWriteAndReadWhenValuesAreSame() throws IOException
{    int[] data = new int[2 * blockSize];    for (int i = 0; i < blockSize; i++) {        data[i] = 3;    }    shouldWriteAndRead(data);}
0
public void shouldWriteWhenDeltaIs0ForEachBlock() throws IOException
{    int[] data = new int[5 * blockSize + 1];    for (int i = 0; i < data.length; i++) {        data[i] = (i - 1) / blockSize;    }    shouldWriteAndRead(data);}
0
public void shouldReadWriteWhenDataIsNotAlignedWithBlock() throws IOException
{    int[] data = new int[5 * blockSize + 3];    for (int i = 0; i < data.length; i++) {        data[i] = random.nextInt(20) - 10;    }    shouldWriteAndRead(data);}
0
public void shouldReadMaxMinValue() throws IOException
{    int[] data = new int[10];    for (int i = 0; i < data.length; i++) {        if (i % 2 == 0) {            data[i] = Integer.MIN_VALUE;        } else {            data[i] = Integer.MAX_VALUE;        }    }    shouldWriteAndRead(data);}
0
public void shouldConsumePageDataInInitialization() throws IOException
{    int[] data = new int[2 * blockSize + 3];    for (int i = 0; i < data.length; i++) {        data[i] = i * 32;    }    writeData(data);    reader = new DeltaBinaryPackingValuesReader();    BytesInput bytes = writer.getBytes();    byte[] valueContent = bytes.toByteArray();    byte[] pageContent = new byte[valueContent.length * 10];    int contentOffsetInPage = 33;    System.arraycopy(valueContent, 0, pageContent, contentOffsetInPage, valueContent.length);        ByteBufferInputStream stream = ByteBufferInputStream.wrap(ByteBuffer.wrap(pageContent));    stream.skipFully(contentOffsetInPage);    reader.initFromPage(100, stream);    long offset = stream.position();    assertEquals(valueContent.length + contentOffsetInPage, offset);        for (int i : data) {        assertEquals(i, reader.readInteger());    }        reader = new DeltaBinaryPackingValuesReader();    reader.initFromPage(100, pageContent, contentOffsetInPage);    assertEquals(valueContent.length + contentOffsetInPage, reader.getNextOffset());    for (int i : data) {        assertEquals(i, reader.readInteger());    }}
0
public void shouldThrowExceptionWhenReadMoreThanWritten() throws IOException
{    int[] data = new int[5 * blockSize + 1];    for (int i = 0; i < data.length; i++) {        data[i] = i * 32;    }    shouldWriteAndRead(data);    try {        reader.readInteger();    } catch (ParquetDecodingException e) {        assertEquals("no more value to read, total value count is " + data.length, e.getMessage());    }}
0
public void shouldSkip() throws IOException
{    int[] data = new int[5 * blockSize + 1];    for (int i = 0; i < data.length; i++) {        data[i] = i * 32;    }    writeData(data);    reader = new DeltaBinaryPackingValuesReader();    reader.initFromPage(100, writer.getBytes().toInputStream());    for (int i = 0; i < data.length; i++) {        if (i % 3 == 0) {            reader.skip();        } else {            assertEquals(i * 32, reader.readInteger());        }    }}
0
public void shouldSkipN() throws IOException
{    int[] data = new int[5 * blockSize + 1];    for (int i = 0; i < data.length; i++) {        data[i] = i * 32;    }    writeData(data);    reader = new DeltaBinaryPackingValuesReader();    reader.initFromPage(100, writer.getBytes().toInputStream());    int skipCount;    for (int i = 0; i < data.length; i += skipCount + 1) {        skipCount = (data.length - i) / 2;        assertEquals(i * 32, reader.readInteger());        reader.skip(skipCount);    }}
0
public void shouldReset() throws IOException
{    shouldReadWriteWhenDataIsNotAlignedWithBlock();    int[] data = new int[5 * blockSize];    for (int i = 0; i < blockSize * 5; i++) {        data[i] = i * 2;    }    writer.reset();    shouldWriteAndRead(data);}
0
public void randomDataTest() throws IOException
{    int maxSize = 1000;    int[] data = new int[maxSize];    for (int round = 0; round < 100000; round++) {        int size = random.nextInt(maxSize);        for (int i = 0; i < size; i++) {            data[i] = random.nextInt();        }        shouldReadAndWrite(data, size);        writer.reset();    }}
0
private void shouldWriteAndRead(int[] data) throws IOException
{    shouldReadAndWrite(data, data.length);}
0
private void shouldReadAndWrite(int[] data, int length) throws IOException
{    writeData(data, length);    reader = new DeltaBinaryPackingValuesReader();    byte[] page = writer.getBytes().toByteArray();    int miniBlockSize = blockSize / miniBlockNum;    double miniBlockFlushed = Math.ceil(((double) length - 1) / miniBlockSize);    double blockFlushed = Math.ceil(((double) length - 1) / blockSize);    double estimatedSize =     4 * 5 +     4 * miniBlockFlushed * miniBlockSize +     blockFlushed * miniBlockNum +     (5.0 * blockFlushed);    assertTrue(estimatedSize >= page.length);    reader.initFromPage(100, ByteBufferInputStream.wrap(ByteBuffer.wrap(page)));    for (int i = 0; i < length; i++) {        assertEquals(data[i], reader.readInteger());    }}
0
private void writeData(int[] data)
{    writeData(data, data.length);}
0
private void writeData(int[] data, int length)
{    for (int i = 0; i < length; i++) {        writer.writeInteger(data[i]);    }}
0
public void setUp()
{    blockSize = 128;    miniBlockNum = 4;    writer = new DeltaBinaryPackingValuesWriterForLong(blockSize, miniBlockNum, 100, 200, new DirectByteBufferAllocator());    random = new Random(0);}
0
public void miniBlockSizeShouldBeMultipleOf8()
{    new DeltaBinaryPackingValuesWriterForLong(1281, 4, 100, 100, new DirectByteBufferAllocator());}
0
public void shouldWriteWhenDataIsAlignedWithBlock() throws IOException
{    long[] data = new long[5 * blockSize];    for (int i = 0; i < blockSize * 5; i++) {        data[i] = random.nextLong();    }    shouldWriteAndRead(data);}
0
public void shouldWriteAndReadWhenBlockIsNotFullyWritten() throws IOException
{    long[] data = new long[blockSize - 3];    for (int i = 0; i < data.length; i++) {        data[i] = random.nextLong();    }    shouldWriteAndRead(data);}
0
public void shouldWriteAndReadWhenAMiniBlockIsNotFullyWritten() throws IOException
{    int miniBlockSize = blockSize / miniBlockNum;    long[] data = new long[miniBlockSize - 3];    for (int i = 0; i < data.length; i++) {        data[i] = random.nextLong();    }    shouldWriteAndRead(data);}
0
public void shouldWriteNegativeDeltas() throws IOException
{    long[] data = new long[blockSize];    for (int i = 0; i < data.length; i++) {        data[i] = 10 - (i * 32 - random.nextInt(6));    }    shouldWriteAndRead(data);}
0
public void shouldWriteAndReadWhenDeltasAreSame() throws IOException
{    long[] data = new long[2 * blockSize];    for (int i = 0; i < blockSize; i++) {        data[i] = i * 32;    }    shouldWriteAndRead(data);}
0
public void shouldWriteAndReadWhenValuesAreSame() throws IOException
{    long[] data = new long[2 * blockSize];    for (int i = 0; i < blockSize; i++) {        data[i] = 3;    }    shouldWriteAndRead(data);}
0
public void shouldWriteWhenDeltaIs0ForEachBlock() throws IOException
{    long[] data = new long[5 * blockSize + 1];    for (int i = 0; i < data.length; i++) {        data[i] = (i - 1) / blockSize;    }    shouldWriteAndRead(data);}
0
public void shouldReadWriteWhenDataIsNotAlignedWithBlock() throws IOException
{    long[] data = new long[5 * blockSize + 3];    for (int i = 0; i < data.length; i++) {        data[i] = random.nextInt(20) - 10;    }    shouldWriteAndRead(data);}
0
public void shouldReadMaxMinValue() throws IOException
{    long[] data = new long[10];    for (int i = 0; i < data.length; i++) {        if (i % 2 == 0) {            data[i] = Long.MIN_VALUE;        } else {            data[i] = Long.MAX_VALUE;        }    }    shouldWriteAndRead(data);}
0
public void shouldReturnCorrectOffsetAfterInitialization() throws IOException
{    long[] data = new long[2 * blockSize + 3];    for (int i = 0; i < data.length; i++) {        data[i] = i * 32;    }    writeData(data);    reader = new DeltaBinaryPackingValuesReader();    BytesInput bytes = writer.getBytes();    byte[] valueContent = bytes.toByteArray();    byte[] pageContent = new byte[valueContent.length * 10];    int contentOffsetInPage = 33;    System.arraycopy(valueContent, 0, pageContent, contentOffsetInPage, valueContent.length);        ByteBufferInputStream stream = ByteBufferInputStream.wrap(ByteBuffer.wrap(pageContent));    stream.skipFully(contentOffsetInPage);    reader.initFromPage(100, stream);    long offset = stream.position();    assertEquals(valueContent.length + contentOffsetInPage, offset);        for (long i : data) {        assertEquals(i, reader.readLong());    }        reader = new DeltaBinaryPackingValuesReader();    reader.initFromPage(100, pageContent, contentOffsetInPage);    assertEquals(valueContent.length + contentOffsetInPage, reader.getNextOffset());    for (long i : data) {        assertEquals(i, reader.readLong());    }}
0
public void shouldThrowExceptionWhenReadMoreThanWritten() throws IOException
{    long[] data = new long[5 * blockSize + 1];    for (int i = 0; i < data.length; i++) {        data[i] = i * 32;    }    shouldWriteAndRead(data);    try {        reader.readLong();    } catch (ParquetDecodingException e) {        assertEquals("no more value to read, total value count is " + data.length, e.getMessage());    }}
0
public void shouldSkip() throws IOException
{    long[] data = new long[5 * blockSize + 1];    for (int i = 0; i < data.length; i++) {        data[i] = i * 32;    }    writeData(data);    reader = new DeltaBinaryPackingValuesReader();    reader.initFromPage(100, writer.getBytes().toInputStream());    for (int i = 0; i < data.length; i++) {        if (i % 3 == 0) {            reader.skip();        } else {            assertEquals(i * 32, reader.readLong());        }    }}
0
public void shouldSkipN() throws IOException
{    long[] data = new long[5 * blockSize + 1];    for (int i = 0; i < data.length; i++) {        data[i] = i * 32;    }    writeData(data);    reader = new DeltaBinaryPackingValuesReader();    reader.initFromPage(100, writer.getBytes().toInputStream());    int skipCount;    for (int i = 0; i < data.length; i += skipCount + 1) {        skipCount = (data.length - i) / 2;        assertEquals(i * 32, reader.readLong());        reader.skip(skipCount);    }}
0
public void shouldReset() throws IOException
{    shouldReadWriteWhenDataIsNotAlignedWithBlock();    long[] data = new long[5 * blockSize];    for (int i = 0; i < blockSize * 5; i++) {        data[i] = i * 2;    }    writer.reset();    shouldWriteAndRead(data);}
0
public void randomDataTest() throws IOException
{    int maxSize = 1000;    long[] data = new long[maxSize];    for (int round = 0; round < 100000; round++) {        int size = random.nextInt(maxSize);        for (int i = 0; i < size; i++) {            data[i] = random.nextLong();        }        shouldReadAndWrite(data, size);        writer.reset();    }}
0
private void shouldWriteAndRead(long[] data) throws IOException
{    shouldReadAndWrite(data, data.length);}
0
private void shouldReadAndWrite(long[] data, int length) throws IOException
{    writeData(data, length);    reader = new DeltaBinaryPackingValuesReader();    byte[] page = writer.getBytes().toByteArray();    int miniBlockSize = blockSize / miniBlockNum;    double miniBlockFlushed = Math.ceil(((double) length - 1) / miniBlockSize);    double blockFlushed = Math.ceil(((double) length - 1) / blockSize);    double estimatedSize =     3 * 5 + 1 * 10 +     8 * miniBlockFlushed * miniBlockSize +     blockFlushed * miniBlockNum +     (10.0 * blockFlushed);    assertTrue(estimatedSize >= page.length);    reader.initFromPage(100, ByteBufferInputStream.wrap(ByteBuffer.wrap(page)));    for (int i = 0; i < length; i++) {        assertEquals(data[i], reader.readLong());    }}
0
private void writeData(long[] data)
{    writeData(data, data.length);}
0
private void writeData(long[] data, int length)
{    for (int i = 0; i < length; i++) {        writer.writeLong(data[i]);    }}
0
public void benchmarkRandomStringsWithPlainValuesWriter() throws IOException
{    PlainValuesWriter writer = new PlainValuesWriter(64 * 1024, 64 * 1024, new DirectByteBufferAllocator());    BinaryPlainValuesReader reader = new BinaryPlainValuesReader();    Utils.writeData(writer, values);    ByteBufferInputStream data = writer.getBytes().toInputStream();    Binary[] bin = Utils.readData(reader, data, values.length);    System.out.println("size " + data.position());}
0
public void benchmarkRandomStringsWithDeltaLengthByteArrayValuesWriter() throws IOException
{    DeltaLengthByteArrayValuesWriter writer = new DeltaLengthByteArrayValuesWriter(64 * 1024, 64 * 1024, new DirectByteBufferAllocator());    DeltaLengthByteArrayValuesReader reader = new DeltaLengthByteArrayValuesReader();    Utils.writeData(writer, values);    ByteBufferInputStream data = writer.getBytes().toInputStream();    Binary[] bin = Utils.readData(reader, data, values.length);    System.out.println("size " + data.position());}
0
private DeltaLengthByteArrayValuesWriter getDeltaLengthByteArrayValuesWriter()
{    return new DeltaLengthByteArrayValuesWriter(64 * 1024, 64 * 1024, new DirectByteBufferAllocator());}
0
public void testSerialization() throws IOException
{    DeltaLengthByteArrayValuesWriter writer = getDeltaLengthByteArrayValuesWriter();    DeltaLengthByteArrayValuesReader reader = new DeltaLengthByteArrayValuesReader();    Utils.writeData(writer, values);    Binary[] bin = Utils.readData(reader, writer.getBytes().toInputStream(), values.length);    for (int i = 0; i < bin.length; i++) {        Assert.assertEquals(Binary.fromString(values[i]), bin[i]);    }}
0
public void testRandomStrings() throws IOException
{    DeltaLengthByteArrayValuesWriter writer = getDeltaLengthByteArrayValuesWriter();    DeltaLengthByteArrayValuesReader reader = new DeltaLengthByteArrayValuesReader();    String[] values = Utils.getRandomStringSamples(1000, 32);    Utils.writeData(writer, values);    Binary[] bin = Utils.readData(reader, writer.getBytes().toInputStream(), values.length);    for (int i = 0; i < bin.length; i++) {        Assert.assertEquals(Binary.fromString(values[i]), bin[i]);    }}
0
public void testSkipWithRandomStrings() throws IOException
{    DeltaLengthByteArrayValuesWriter writer = getDeltaLengthByteArrayValuesWriter();    DeltaLengthByteArrayValuesReader reader = new DeltaLengthByteArrayValuesReader();    String[] values = Utils.getRandomStringSamples(1000, 32);    Utils.writeData(writer, values);    reader.initFromPage(values.length, writer.getBytes().toInputStream());    for (int i = 0; i < values.length; i += 2) {        Assert.assertEquals(Binary.fromString(values[i]), reader.readBytes());        reader.skip();    }    reader = new DeltaLengthByteArrayValuesReader();    reader.initFromPage(values.length, writer.getBytes().toInputStream());    int skipCount;    for (int i = 0; i < values.length; i += skipCount + 1) {        skipCount = (values.length - i) / 2;        Assert.assertEquals(Binary.fromString(values[i]), reader.readBytes());        reader.skip(skipCount);    }}
0
public void testLengths() throws IOException
{    DeltaLengthByteArrayValuesWriter writer = getDeltaLengthByteArrayValuesWriter();    ValuesReader reader = new DeltaBinaryPackingValuesReader();    Utils.writeData(writer, values);    int[] bin = Utils.readInts(reader, writer.getBytes().toInputStream(), values.length);    for (int i = 0; i < bin.length; i++) {        Assert.assertEquals(values[i].length(), bin[i]);    }}
0
public void benchmarkRandomStringsWithPlainValuesWriter() throws IOException
{    PlainValuesWriter writer = new PlainValuesWriter(64 * 1024, 64 * 1024, new DirectByteBufferAllocator());    BinaryPlainValuesReader reader = new BinaryPlainValuesReader();    Utils.writeData(writer, values);    ByteBufferInputStream data = writer.getBytes().toInputStream();    Binary[] bin = Utils.readData(reader, data, values.length);    System.out.println("size " + data.position());}
0
public void benchmarkRandomStringsWithDeltaLengthByteArrayValuesWriter() throws IOException
{    DeltaByteArrayWriter writer = new DeltaByteArrayWriter(64 * 1024, 64 * 1024, new DirectByteBufferAllocator());    DeltaByteArrayReader reader = new DeltaByteArrayReader();    Utils.writeData(writer, values);    ByteBufferInputStream data = writer.getBytes().toInputStream();    Binary[] bin = Utils.readData(reader, data, values.length);    System.out.println("size " + data.position());}
0
public void benchmarkSortedStringsWithPlainValuesWriter() throws IOException
{    PlainValuesWriter writer = new PlainValuesWriter(64 * 1024, 64 * 1024, new DirectByteBufferAllocator());    BinaryPlainValuesReader reader = new BinaryPlainValuesReader();    Utils.writeData(writer, sortedVals);    ByteBufferInputStream data = writer.getBytes().toInputStream();    Binary[] bin = Utils.readData(reader, data, values.length);    System.out.println("size " + data.position());}
0
public void benchmarkSortedStringsWithDeltaLengthByteArrayValuesWriter() throws IOException
{    DeltaByteArrayWriter writer = new DeltaByteArrayWriter(64 * 1024, 64 * 1024, new DirectByteBufferAllocator());    DeltaByteArrayReader reader = new DeltaByteArrayReader();    Utils.writeData(writer, sortedVals);    ByteBufferInputStream data = writer.getBytes().toInputStream();    Binary[] bin = Utils.readData(reader, data, values.length);    System.out.println("size " + data.position());}
0
public void testSerialization() throws Exception
{    DeltaByteArrayWriter writer = new DeltaByteArrayWriter(64 * 1024, 64 * 1024, new DirectByteBufferAllocator());    DeltaByteArrayReader reader = new DeltaByteArrayReader();    assertReadWrite(writer, reader, values);}
0
public void testRandomStrings() throws Exception
{    DeltaByteArrayWriter writer = new DeltaByteArrayWriter(64 * 1024, 64 * 1024, new DirectByteBufferAllocator());    DeltaByteArrayReader reader = new DeltaByteArrayReader();    assertReadWrite(writer, reader, randvalues);}
0
public void testRandomStringsWithSkip() throws Exception
{    DeltaByteArrayWriter writer = new DeltaByteArrayWriter(64 * 1024, 64 * 1024, new DirectByteBufferAllocator());    DeltaByteArrayReader reader = new DeltaByteArrayReader();    assertReadWriteWithSkip(writer, reader, randvalues);}
0
public void testRandomStringsWithSkipN() throws Exception
{    DeltaByteArrayWriter writer = new DeltaByteArrayWriter(64 * 1024, 64 * 1024, new DirectByteBufferAllocator());    DeltaByteArrayReader reader = new DeltaByteArrayReader();    assertReadWriteWithSkipN(writer, reader, randvalues);}
0
public void testLengths() throws IOException
{    DeltaByteArrayWriter writer = new DeltaByteArrayWriter(64 * 1024, 64 * 1024, new DirectByteBufferAllocator());    ValuesReader reader = new DeltaBinaryPackingValuesReader();    Utils.writeData(writer, values);    ByteBufferInputStream data = writer.getBytes().toInputStream();    int[] bin = Utils.readInts(reader, data, values.length);        Assert.assertEquals(0, bin[0]);    Assert.assertEquals(7, bin[1]);    Assert.assertEquals(7, bin[2]);    reader = new DeltaBinaryPackingValuesReader();    bin = Utils.readInts(reader, data, values.length);        Assert.assertEquals(10, bin[0]);    Assert.assertEquals(0, bin[1]);    Assert.assertEquals(7, bin[2]);}
0
private void assertReadWrite(DeltaByteArrayWriter writer, DeltaByteArrayReader reader, String[] vals) throws Exception
{    Utils.writeData(writer, vals);    Binary[] bin = Utils.readData(reader, writer.getBytes().toInputStream(), vals.length);    for (int i = 0; i < bin.length; i++) {        Assert.assertEquals(Binary.fromString(vals[i]), bin[i]);    }}
0
private void assertReadWriteWithSkip(DeltaByteArrayWriter writer, DeltaByteArrayReader reader, String[] vals) throws Exception
{    Utils.writeData(writer, vals);    reader.initFromPage(vals.length, writer.getBytes().toInputStream());    for (int i = 0; i < vals.length; i += 2) {        Assert.assertEquals(Binary.fromString(vals[i]), reader.readBytes());        reader.skip();    }}
0
private void assertReadWriteWithSkipN(DeltaByteArrayWriter writer, DeltaByteArrayReader reader, String[] vals) throws Exception
{    Utils.writeData(writer, vals);    reader.initFromPage(vals.length, writer.getBytes().toInputStream());    int skipCount;    for (int i = 0; i < vals.length; i += skipCount + 1) {        skipCount = (vals.length - i) / 2;        Assert.assertEquals(Binary.fromString(vals[i]), reader.readBytes());        reader.skip(skipCount);    }}
0
public void testWriterReset() throws Exception
{    DeltaByteArrayWriter writer = new DeltaByteArrayWriter(64 * 1024, 64 * 1024, new DirectByteBufferAllocator());    assertReadWrite(writer, new DeltaByteArrayReader(), values);    writer.reset();    assertReadWrite(writer, new DeltaByteArrayReader(), values);}
0
public void testSmallList()
{    int testSize = IntList.INITIAL_SLAB_SIZE - 100;    doTestIntList(testSize, IntList.INITIAL_SLAB_SIZE);}
0
public void testListGreaterThanInitialSlabSize()
{    int testSize = IntList.INITIAL_SLAB_SIZE + 100;    doTestIntList(testSize, IntList.INITIAL_SLAB_SIZE * 2);}
0
public void testListGreaterThanMaxSlabSize()
{    int testSize = IntList.MAX_SLAB_SIZE * 4 + 100;    doTestIntList(testSize, IntList.MAX_SLAB_SIZE);}
0
private void doTestIntList(int testSize, int expectedSlabSize)
{    IntList testList = new IntList();    populateList(testList, testSize);    verifyIteratorResults(testSize, testList);        Assert.assertEquals(expectedSlabSize, testList.getCurrentSlabSize());}
0
private void populateList(IntList testList, int size)
{    for (int i = 0; i < size; i++) {        testList.add(i);    }}
0
private void verifyIteratorResults(int testSize, IntList testList)
{    IntList.IntIterator iterator = testList.iterator();    int expected = 0;    while (iterator.hasNext()) {        int val = iterator.next();        Assert.assertEquals(expected, val);        expected++;    }        Assert.assertEquals(testSize, expected);}
0
private FallbackValuesWriter<I, PlainValuesWriter> plainFallBack(I dvw, int initialSize)
{    return FallbackValuesWriter.of(dvw, new PlainValuesWriter(initialSize, initialSize * 5, new DirectByteBufferAllocator()));}
0
private FallbackValuesWriter<PlainBinaryDictionaryValuesWriter, PlainValuesWriter> newPlainBinaryDictionaryValuesWriter(int maxDictionaryByteSize, int initialSize)
{    return plainFallBack(new PlainBinaryDictionaryValuesWriter(maxDictionaryByteSize, PLAIN_DICTIONARY, PLAIN_DICTIONARY, new DirectByteBufferAllocator()), initialSize);}
0
private FallbackValuesWriter<PlainLongDictionaryValuesWriter, PlainValuesWriter> newPlainLongDictionaryValuesWriter(int maxDictionaryByteSize, int initialSize)
{    return plainFallBack(new PlainLongDictionaryValuesWriter(maxDictionaryByteSize, PLAIN_DICTIONARY, PLAIN_DICTIONARY, new DirectByteBufferAllocator()), initialSize);}
0
private FallbackValuesWriter<PlainIntegerDictionaryValuesWriter, PlainValuesWriter> newPlainIntegerDictionaryValuesWriter(int maxDictionaryByteSize, int initialSize)
{    return plainFallBack(new PlainIntegerDictionaryValuesWriter(maxDictionaryByteSize, PLAIN_DICTIONARY, PLAIN_DICTIONARY, new DirectByteBufferAllocator()), initialSize);}
0
private FallbackValuesWriter<PlainDoubleDictionaryValuesWriter, PlainValuesWriter> newPlainDoubleDictionaryValuesWriter(int maxDictionaryByteSize, int initialSize)
{    return plainFallBack(new PlainDoubleDictionaryValuesWriter(maxDictionaryByteSize, PLAIN_DICTIONARY, PLAIN_DICTIONARY, new DirectByteBufferAllocator()), initialSize);}
0
private FallbackValuesWriter<PlainFloatDictionaryValuesWriter, PlainValuesWriter> newPlainFloatDictionaryValuesWriter(int maxDictionaryByteSize, int initialSize)
{    return plainFallBack(new PlainFloatDictionaryValuesWriter(maxDictionaryByteSize, PLAIN_DICTIONARY, PLAIN_DICTIONARY, new DirectByteBufferAllocator()), initialSize);}
0
public void testBinaryDictionary() throws IOException
{    int COUNT = 100;    ValuesWriter cw = newPlainBinaryDictionaryValuesWriter(200, 10000);    writeRepeated(COUNT, cw, "a");    BytesInput bytes1 = getBytesAndCheckEncoding(cw, PLAIN_DICTIONARY);    writeRepeated(COUNT, cw, "b");    BytesInput bytes2 = getBytesAndCheckEncoding(cw, PLAIN_DICTIONARY);        writeDistinct(COUNT, cw, "c");    BytesInput bytes3 = getBytesAndCheckEncoding(cw, PLAIN);    DictionaryValuesReader cr = initDicReader(cw, BINARY);    checkRepeated(COUNT, bytes1, cr, "a");    checkRepeated(COUNT, bytes2, cr, "b");    BinaryPlainValuesReader cr2 = new BinaryPlainValuesReader();    checkDistinct(COUNT, bytes3, cr2, "c");}
0
public void testSkipInBinaryDictionary() throws Exception
{    ValuesWriter cw = newPlainBinaryDictionaryValuesWriter(1000, 10000);    writeRepeated(100, cw, "a");    writeDistinct(100, cw, "b");    assertEquals(PLAIN_DICTIONARY, cw.getEncoding());        ByteBufferInputStream stream = cw.getBytes().toInputStream();    DictionaryValuesReader cr = initDicReader(cw, BINARY);    cr.initFromPage(200, stream);    for (int i = 0; i < 100; i += 2) {        assertEquals(Binary.fromString("a" + i % 10), cr.readBytes());        cr.skip();    }    int skipCount;    for (int i = 0; i < 100; i += skipCount + 1) {        skipCount = (100 - i) / 2;        assertEquals(Binary.fromString("b" + i), cr.readBytes());        cr.skip(skipCount);    }        writeDistinct(1000, cw, "c");    assertEquals(PLAIN, cw.getEncoding());        ValuesReader plainReader = new BinaryPlainValuesReader();    plainReader.initFromPage(1200, cw.getBytes().toInputStream());    plainReader.skip(200);    for (int i = 0; i < 100; i += 2) {        assertEquals("c" + i, plainReader.readBytes().toStringUsingUTF8());        plainReader.skip();    }    for (int i = 100; i < 1000; i += skipCount + 1) {        skipCount = (1000 - i) / 2;        assertEquals(Binary.fromString("c" + i), plainReader.readBytes());        plainReader.skip(skipCount);    }}
0
public void testBinaryDictionaryFallBack() throws IOException
{    int slabSize = 100;    int maxDictionaryByteSize = 50;    final ValuesWriter cw = newPlainBinaryDictionaryValuesWriter(maxDictionaryByteSize, slabSize);    int fallBackThreshold = maxDictionaryByteSize;    int dataSize = 0;    for (long i = 0; i < 100; i++) {        Binary binary = Binary.fromString("str" + i);        cw.writeBytes(binary);        dataSize += (binary.length() + 4);        if (dataSize < fallBackThreshold) {            assertEquals(PLAIN_DICTIONARY, cw.getEncoding());        } else {            assertEquals(PLAIN, cw.getEncoding());        }    }        ValuesReader reader = new BinaryPlainValuesReader();    reader.initFromPage(100, cw.getBytes().toInputStream());    for (long i = 0; i < 100; i++) {        assertEquals(Binary.fromString("str" + i), reader.readBytes());    }        cw.reset();    assertEquals(0, cw.getBufferedSize());}
0
public void testBinaryDictionaryChangedValues() throws IOException
{    int COUNT = 100;    ValuesWriter cw = newPlainBinaryDictionaryValuesWriter(200, 10000);    writeRepeatedWithReuse(COUNT, cw, "a");    BytesInput bytes1 = getBytesAndCheckEncoding(cw, PLAIN_DICTIONARY);    writeRepeatedWithReuse(COUNT, cw, "b");    BytesInput bytes2 = getBytesAndCheckEncoding(cw, PLAIN_DICTIONARY);        writeDistinct(COUNT, cw, "c");    BytesInput bytes3 = getBytesAndCheckEncoding(cw, PLAIN);    DictionaryValuesReader cr = initDicReader(cw, BINARY);    checkRepeated(COUNT, bytes1, cr, "a");    checkRepeated(COUNT, bytes2, cr, "b");    BinaryPlainValuesReader cr2 = new BinaryPlainValuesReader();    checkDistinct(COUNT, bytes3, cr2, "c");}
0
public void testFirstPageFallBack() throws IOException
{    int COUNT = 1000;    ValuesWriter cw = newPlainBinaryDictionaryValuesWriter(10000, 10000);    writeDistinct(COUNT, cw, "a");        BytesInput bytes1 = getBytesAndCheckEncoding(cw, PLAIN);    writeRepeated(COUNT, cw, "b");        BytesInput bytes2 = getBytesAndCheckEncoding(cw, PLAIN);    ValuesReader cr = new BinaryPlainValuesReader();    checkDistinct(COUNT, bytes1, cr, "a");    checkRepeated(COUNT, bytes2, cr, "b");}
0
public void testSecondPageFallBack() throws IOException
{    int COUNT = 1000;    ValuesWriter cw = newPlainBinaryDictionaryValuesWriter(1000, 10000);    writeRepeated(COUNT, cw, "a");    BytesInput bytes1 = getBytesAndCheckEncoding(cw, PLAIN_DICTIONARY);    writeDistinct(COUNT, cw, "b");        BytesInput bytes2 = getBytesAndCheckEncoding(cw, PLAIN);    writeRepeated(COUNT, cw, "a");        BytesInput bytes3 = getBytesAndCheckEncoding(cw, PLAIN);    ValuesReader cr = initDicReader(cw, BINARY);    checkRepeated(COUNT, bytes1, cr, "a");    cr = new BinaryPlainValuesReader();    checkDistinct(COUNT, bytes2, cr, "b");    checkRepeated(COUNT, bytes3, cr, "a");}
0
public void testLongDictionary() throws IOException
{    int COUNT = 1000;    int COUNT2 = 2000;    final FallbackValuesWriter<PlainLongDictionaryValuesWriter, PlainValuesWriter> cw = newPlainLongDictionaryValuesWriter(10000, 10000);    for (long i = 0; i < COUNT; i++) {        cw.writeLong(i % 50);    }    BytesInput bytes1 = getBytesAndCheckEncoding(cw, PLAIN_DICTIONARY);    assertEquals(50, cw.initialWriter.getDictionarySize());    for (long i = COUNT2; i > 0; i--) {        cw.writeLong(i % 50);    }    BytesInput bytes2 = getBytesAndCheckEncoding(cw, PLAIN_DICTIONARY);    assertEquals(50, cw.initialWriter.getDictionarySize());    DictionaryValuesReader cr = initDicReader(cw, PrimitiveTypeName.INT64);    cr.initFromPage(COUNT, bytes1.toInputStream());    for (long i = 0; i < COUNT; i++) {        long back = cr.readLong();        assertEquals(i % 50, back);    }    cr.initFromPage(COUNT2, bytes2.toInputStream());    for (long i = COUNT2; i > 0; i--) {        long back = cr.readLong();        assertEquals(i % 50, back);    }}
0
private void roundTripLong(FallbackValuesWriter<PlainLongDictionaryValuesWriter, PlainValuesWriter> cw, ValuesReader reader, int maxDictionaryByteSize) throws IOException
{    int fallBackThreshold = maxDictionaryByteSize / 8;    for (long i = 0; i < 100; i++) {        cw.writeLong(i);        if (i < fallBackThreshold) {            assertEquals(cw.getEncoding(), PLAIN_DICTIONARY);        } else {            assertEquals(cw.getEncoding(), PLAIN);        }    }    reader.initFromPage(100, cw.getBytes().toInputStream());    for (long i = 0; i < 100; i++) {        assertEquals(i, reader.readLong());    }        reader.initFromPage(100, cw.getBytes().toInputStream());    for (int i = 0; i < 100; i += 2) {        assertEquals(i, reader.readLong());        reader.skip();    }        reader.initFromPage(100, cw.getBytes().toInputStream());    int skipCount;    for (int i = 0; i < 100; i += skipCount + 1) {        skipCount = (100 - i) / 2;        assertEquals(i, reader.readLong());        reader.skip(skipCount);    }}
0
public void testLongDictionaryFallBack() throws IOException
{    int slabSize = 100;    int maxDictionaryByteSize = 50;    final FallbackValuesWriter<PlainLongDictionaryValuesWriter, PlainValuesWriter> cw = newPlainLongDictionaryValuesWriter(maxDictionaryByteSize, slabSize);        ValuesReader reader = new PlainValuesReader.LongPlainValuesReader();    roundTripLong(cw, reader, maxDictionaryByteSize);        cw.reset();    assertEquals(0, cw.getBufferedSize());    cw.resetDictionary();    roundTripLong(cw, reader, maxDictionaryByteSize);}
0
public void testDoubleDictionary() throws IOException
{    int COUNT = 1000;    int COUNT2 = 2000;    final FallbackValuesWriter<PlainDoubleDictionaryValuesWriter, PlainValuesWriter> cw = newPlainDoubleDictionaryValuesWriter(10000, 10000);    for (double i = 0; i < COUNT; i++) {        cw.writeDouble(i % 50);    }    BytesInput bytes1 = getBytesAndCheckEncoding(cw, PLAIN_DICTIONARY);    assertEquals(50, cw.initialWriter.getDictionarySize());    for (double i = COUNT2; i > 0; i--) {        cw.writeDouble(i % 50);    }    BytesInput bytes2 = getBytesAndCheckEncoding(cw, PLAIN_DICTIONARY);    assertEquals(50, cw.initialWriter.getDictionarySize());    final DictionaryValuesReader cr = initDicReader(cw, DOUBLE);    cr.initFromPage(COUNT, bytes1.toInputStream());    for (double i = 0; i < COUNT; i++) {        double back = cr.readDouble();        assertEquals(i % 50, back, 0.0);    }    cr.initFromPage(COUNT2, bytes2.toInputStream());    for (double i = COUNT2; i > 0; i--) {        double back = cr.readDouble();        assertEquals(i % 50, back, 0.0);    }}
0
private void roundTripDouble(FallbackValuesWriter<PlainDoubleDictionaryValuesWriter, PlainValuesWriter> cw, ValuesReader reader, int maxDictionaryByteSize) throws IOException
{    int fallBackThreshold = maxDictionaryByteSize / 8;    for (double i = 0; i < 100; i++) {        cw.writeDouble(i);        if (i < fallBackThreshold) {            assertEquals(cw.getEncoding(), PLAIN_DICTIONARY);        } else {            assertEquals(cw.getEncoding(), PLAIN);        }    }    reader.initFromPage(100, cw.getBytes().toInputStream());    for (double i = 0; i < 100; i++) {        assertEquals(i, reader.readDouble(), 0.00001);    }        reader.initFromPage(100, cw.getBytes().toInputStream());    for (int i = 0; i < 100; i += 2) {        assertEquals(i, reader.readDouble(), 0.0);        reader.skip();    }        reader.initFromPage(100, cw.getBytes().toInputStream());    int skipCount;    for (int i = 0; i < 100; i += skipCount + 1) {        skipCount = (100 - i) / 2;        assertEquals(i, reader.readDouble(), 0.0);        reader.skip(skipCount);    }}
0
public void testDoubleDictionaryFallBack() throws IOException
{    int slabSize = 100;    int maxDictionaryByteSize = 50;    final FallbackValuesWriter<PlainDoubleDictionaryValuesWriter, PlainValuesWriter> cw = newPlainDoubleDictionaryValuesWriter(maxDictionaryByteSize, slabSize);        ValuesReader reader = new PlainValuesReader.DoublePlainValuesReader();    roundTripDouble(cw, reader, maxDictionaryByteSize);        cw.reset();    assertEquals(0, cw.getBufferedSize());    cw.resetDictionary();    roundTripDouble(cw, reader, maxDictionaryByteSize);}
0
public void testIntDictionary() throws IOException
{    int COUNT = 2000;    int COUNT2 = 4000;    final FallbackValuesWriter<PlainIntegerDictionaryValuesWriter, PlainValuesWriter> cw = newPlainIntegerDictionaryValuesWriter(10000, 10000);    for (int i = 0; i < COUNT; i++) {        cw.writeInteger(i % 50);    }    BytesInput bytes1 = getBytesAndCheckEncoding(cw, PLAIN_DICTIONARY);    assertEquals(50, cw.initialWriter.getDictionarySize());    for (int i = COUNT2; i > 0; i--) {        cw.writeInteger(i % 50);    }    BytesInput bytes2 = getBytesAndCheckEncoding(cw, PLAIN_DICTIONARY);    assertEquals(50, cw.initialWriter.getDictionarySize());    DictionaryValuesReader cr = initDicReader(cw, INT32);    cr.initFromPage(COUNT, bytes1.toInputStream());    for (int i = 0; i < COUNT; i++) {        int back = cr.readInteger();        assertEquals(i % 50, back);    }    cr.initFromPage(COUNT2, bytes2.toInputStream());    for (int i = COUNT2; i > 0; i--) {        int back = cr.readInteger();        assertEquals(i % 50, back);    }}
0
private void roundTripInt(FallbackValuesWriter<PlainIntegerDictionaryValuesWriter, PlainValuesWriter> cw, ValuesReader reader, int maxDictionaryByteSize) throws IOException
{    int fallBackThreshold = maxDictionaryByteSize / 4;    for (int i = 0; i < 100; i++) {        cw.writeInteger(i);        if (i < fallBackThreshold) {            assertEquals(cw.getEncoding(), PLAIN_DICTIONARY);        } else {            assertEquals(cw.getEncoding(), PLAIN);        }    }    reader.initFromPage(100, cw.getBytes().toInputStream());    for (int i = 0; i < 100; i++) {        assertEquals(i, reader.readInteger());    }        reader.initFromPage(100, cw.getBytes().toInputStream());    for (int i = 0; i < 100; i += 2) {        assertEquals(i, reader.readInteger());        reader.skip();    }        reader.initFromPage(100, cw.getBytes().toInputStream());    int skipCount;    for (int i = 0; i < 100; i += skipCount + 1) {        skipCount = (100 - i) / 2;        assertEquals(i, reader.readInteger());        reader.skip(skipCount);    }}
0
public void testIntDictionaryFallBack() throws IOException
{    int slabSize = 100;    int maxDictionaryByteSize = 50;    final FallbackValuesWriter<PlainIntegerDictionaryValuesWriter, PlainValuesWriter> cw = newPlainIntegerDictionaryValuesWriter(maxDictionaryByteSize, slabSize);        ValuesReader reader = new PlainValuesReader.IntegerPlainValuesReader();    roundTripInt(cw, reader, maxDictionaryByteSize);        cw.reset();    assertEquals(0, cw.getBufferedSize());    cw.resetDictionary();    roundTripInt(cw, reader, maxDictionaryByteSize);}
0
public void testFloatDictionary() throws IOException
{    int COUNT = 2000;    int COUNT2 = 4000;    final FallbackValuesWriter<PlainFloatDictionaryValuesWriter, PlainValuesWriter> cw = newPlainFloatDictionaryValuesWriter(10000, 10000);    for (float i = 0; i < COUNT; i++) {        cw.writeFloat(i % 50);    }    BytesInput bytes1 = getBytesAndCheckEncoding(cw, PLAIN_DICTIONARY);    assertEquals(50, cw.initialWriter.getDictionarySize());    for (float i = COUNT2; i > 0; i--) {        cw.writeFloat(i % 50);    }    BytesInput bytes2 = getBytesAndCheckEncoding(cw, PLAIN_DICTIONARY);    assertEquals(50, cw.initialWriter.getDictionarySize());    DictionaryValuesReader cr = initDicReader(cw, FLOAT);    cr.initFromPage(COUNT, bytes1.toInputStream());    for (float i = 0; i < COUNT; i++) {        float back = cr.readFloat();        assertEquals(i % 50, back, 0.0f);    }    cr.initFromPage(COUNT2, bytes2.toInputStream());    for (float i = COUNT2; i > 0; i--) {        float back = cr.readFloat();        assertEquals(i % 50, back, 0.0f);    }}
0
private void roundTripFloat(FallbackValuesWriter<PlainFloatDictionaryValuesWriter, PlainValuesWriter> cw, ValuesReader reader, int maxDictionaryByteSize) throws IOException
{    int fallBackThreshold = maxDictionaryByteSize / 4;    for (float i = 0; i < 100; i++) {        cw.writeFloat(i);        if (i < fallBackThreshold) {            assertEquals(cw.getEncoding(), PLAIN_DICTIONARY);        } else {            assertEquals(cw.getEncoding(), PLAIN);        }    }    reader.initFromPage(100, cw.getBytes().toInputStream());    for (float i = 0; i < 100; i++) {        assertEquals(i, reader.readFloat(), 0.00001);    }        reader.initFromPage(100, cw.getBytes().toInputStream());    for (int i = 0; i < 100; i += 2) {        assertEquals(i, reader.readFloat(), 0.0f);        reader.skip();    }        reader.initFromPage(100, cw.getBytes().toInputStream());    int skipCount;    for (int i = 0; i < 100; i += skipCount + 1) {        skipCount = (100 - i) / 2;        assertEquals(i, reader.readFloat(), 0.0f);        reader.skip(skipCount);    }}
0
public void testFloatDictionaryFallBack() throws IOException
{    int slabSize = 100;    int maxDictionaryByteSize = 50;    final FallbackValuesWriter<PlainFloatDictionaryValuesWriter, PlainValuesWriter> cw = newPlainFloatDictionaryValuesWriter(maxDictionaryByteSize, slabSize);        ValuesReader reader = new PlainValuesReader.FloatPlainValuesReader();    roundTripFloat(cw, reader, maxDictionaryByteSize);        cw.reset();    assertEquals(0, cw.getBufferedSize());    cw.resetDictionary();    roundTripFloat(cw, reader, maxDictionaryByteSize);}
0
public void testZeroValues() throws IOException
{    FallbackValuesWriter<PlainIntegerDictionaryValuesWriter, PlainValuesWriter> cw = newPlainIntegerDictionaryValuesWriter(100, 100);    cw.writeInteger(34);    cw.writeInteger(34);    getBytesAndCheckEncoding(cw, PLAIN_DICTIONARY);    DictionaryValuesReader reader = initDicReader(cw, INT32);            ByteBuffer bytes = ByteBuffer.wrap(new byte[] { 0x00, 0x01, 0x02, 0x03 });    ByteBufferInputStream stream = ByteBufferInputStream.wrap(bytes);    stream.skipFully(stream.available());    reader.initFromPage(100, stream);        reader = initDicReader(cw, INT32);    int offset = bytes.remaining();    reader.initFromPage(100, bytes, offset);}
0
private DictionaryValuesReader initDicReader(ValuesWriter cw, PrimitiveTypeName type) throws IOException
{    final DictionaryPage dictionaryPage = cw.toDictPageAndClose().copy();    final ColumnDescriptor descriptor = new ColumnDescriptor(new String[] { "foo" }, type, 0, 0);    final Dictionary dictionary = PLAIN.initDictionary(descriptor, dictionaryPage);    final DictionaryValuesReader cr = new DictionaryValuesReader(dictionary);    return cr;}
0
private void checkDistinct(int COUNT, BytesInput bytes, ValuesReader cr, String prefix) throws IOException
{    cr.initFromPage(COUNT, bytes.toInputStream());    for (int i = 0; i < COUNT; i++) {        Assert.assertEquals(prefix + i, cr.readBytes().toStringUsingUTF8());    }}
0
private void checkRepeated(int COUNT, BytesInput bytes, ValuesReader cr, String prefix) throws IOException
{    cr.initFromPage(COUNT, bytes.toInputStream());    for (int i = 0; i < COUNT; i++) {        Assert.assertEquals(prefix + i % 10, cr.readBytes().toStringUsingUTF8());    }}
0
private void writeDistinct(int COUNT, ValuesWriter cw, String prefix)
{    for (int i = 0; i < COUNT; i++) {        cw.writeBytes(Binary.fromString(prefix + i));    }}
0
private void writeRepeated(int COUNT, ValuesWriter cw, String prefix)
{    for (int i = 0; i < COUNT; i++) {        cw.writeBytes(Binary.fromString(prefix + i % 10));    }}
0
private void writeRepeatedWithReuse(int COUNT, ValuesWriter cw, String prefix)
{    Binary reused = Binary.fromReusedByteArray((prefix + "0").getBytes(StandardCharsets.UTF_8));    for (int i = 0; i < COUNT; i++) {        Binary content = Binary.fromString(prefix + i % 10);        System.arraycopy(content.getBytesUnsafe(), 0, reused.getBytesUnsafe(), 0, reused.length());        cw.writeBytes(reused);    }}
0
private BytesInput getBytesAndCheckEncoding(ValuesWriter cw, Encoding encoding) throws IOException
{    BytesInput bytes = BytesInput.copy(cw.getBytes());    assertEquals(encoding, cw.getEncoding());    cw.reset();    return bytes;}
0
public void testBoolean()
{    doTestValueWriter(PrimitiveTypeName.BOOLEAN, WriterVersion.PARQUET_1_0, true, BooleanPlainValuesWriter.class);}
0
public void testBoolean_V2()
{    doTestValueWriter(PrimitiveTypeName.BOOLEAN, WriterVersion.PARQUET_2_0, true, RunLengthBitPackingHybridValuesWriter.class);}
0
public void testFixedLenByteArray()
{    doTestValueWriter(PrimitiveTypeName.FIXED_LEN_BYTE_ARRAY, WriterVersion.PARQUET_1_0, true, FixedLenByteArrayPlainValuesWriter.class);}
0
public void testFixedLenByteArray_V2()
{    doTestValueWriter(PrimitiveTypeName.FIXED_LEN_BYTE_ARRAY, WriterVersion.PARQUET_2_0, true, DictionaryValuesWriter.class, DeltaByteArrayWriter.class);}
0
public void testFixedLenByteArray_V2_NoDict()
{    doTestValueWriter(PrimitiveTypeName.FIXED_LEN_BYTE_ARRAY, WriterVersion.PARQUET_2_0, false, DeltaByteArrayWriter.class);}
0
public void testBinary()
{    doTestValueWriter(PrimitiveTypeName.BINARY, WriterVersion.PARQUET_1_0, true, PlainBinaryDictionaryValuesWriter.class, PlainValuesWriter.class);}
0
public void testBinary_NoDict()
{    doTestValueWriter(PrimitiveTypeName.BINARY, WriterVersion.PARQUET_1_0, false, PlainValuesWriter.class);}
0
public void testBinary_V2()
{    doTestValueWriter(PrimitiveTypeName.BINARY, WriterVersion.PARQUET_2_0, true, PlainBinaryDictionaryValuesWriter.class, DeltaByteArrayWriter.class);}
0
public void testBinary_V2_NoDict()
{    doTestValueWriter(PrimitiveTypeName.BINARY, WriterVersion.PARQUET_2_0, false, DeltaByteArrayWriter.class);}
0
public void testInt32()
{    doTestValueWriter(PrimitiveTypeName.INT32, WriterVersion.PARQUET_1_0, true, PlainIntegerDictionaryValuesWriter.class, PlainValuesWriter.class);}
0
public void testInt32_NoDict()
{    doTestValueWriter(PrimitiveTypeName.INT32, WriterVersion.PARQUET_1_0, false, PlainValuesWriter.class);}
0
public void testInt32_V2()
{    doTestValueWriter(PrimitiveTypeName.INT32, WriterVersion.PARQUET_2_0, true, PlainIntegerDictionaryValuesWriter.class, DeltaBinaryPackingValuesWriter.class);}
0
public void testInt32_V2_NoDict()
{    doTestValueWriter(PrimitiveTypeName.INT32, WriterVersion.PARQUET_2_0, false, DeltaBinaryPackingValuesWriter.class);}
0
public void testInt64()
{    doTestValueWriter(PrimitiveTypeName.INT64, WriterVersion.PARQUET_1_0, true, PlainLongDictionaryValuesWriter.class, PlainValuesWriter.class);}
0
public void testInt64_NoDict()
{    doTestValueWriter(PrimitiveTypeName.INT64, WriterVersion.PARQUET_1_0, false, PlainValuesWriter.class);}
0
public void testInt64_V2()
{    doTestValueWriter(PrimitiveTypeName.INT64, WriterVersion.PARQUET_2_0, true, PlainLongDictionaryValuesWriter.class, DeltaBinaryPackingValuesWriterForLong.class);}
0
public void testInt64_V2_NoDict()
{    doTestValueWriter(PrimitiveTypeName.INT64, WriterVersion.PARQUET_2_0, false, DeltaBinaryPackingValuesWriterForLong.class);}
0
public void testInt96()
{    doTestValueWriter(PrimitiveTypeName.INT96, WriterVersion.PARQUET_1_0, true, PlainFixedLenArrayDictionaryValuesWriter.class, FixedLenByteArrayPlainValuesWriter.class);}
0
public void testInt96_NoDict()
{    doTestValueWriter(PrimitiveTypeName.INT96, WriterVersion.PARQUET_1_0, false, FixedLenByteArrayPlainValuesWriter.class);}
0
public void testInt96_V2()
{    doTestValueWriter(PrimitiveTypeName.INT96, WriterVersion.PARQUET_2_0, true, PlainFixedLenArrayDictionaryValuesWriter.class, FixedLenByteArrayPlainValuesWriter.class);}
0
public void testInt96_V2_NoDict()
{    doTestValueWriter(PrimitiveTypeName.INT96, WriterVersion.PARQUET_2_0, false, FixedLenByteArrayPlainValuesWriter.class);}
0
public void testDouble()
{    doTestValueWriter(PrimitiveTypeName.DOUBLE, WriterVersion.PARQUET_1_0, true, PlainDoubleDictionaryValuesWriter.class, PlainValuesWriter.class);}
0
public void testDouble_NoDict()
{    doTestValueWriter(PrimitiveTypeName.DOUBLE, WriterVersion.PARQUET_1_0, false, PlainValuesWriter.class);}
0
public void testDouble_V2()
{    doTestValueWriter(PrimitiveTypeName.DOUBLE, WriterVersion.PARQUET_2_0, true, PlainDoubleDictionaryValuesWriter.class, PlainValuesWriter.class);}
0
public void testDouble_V2_NoDict()
{    doTestValueWriter(PrimitiveTypeName.DOUBLE, WriterVersion.PARQUET_2_0, false, PlainValuesWriter.class);}
0
public void testFloat()
{    doTestValueWriter(PrimitiveTypeName.FLOAT, WriterVersion.PARQUET_1_0, true, PlainFloatDictionaryValuesWriter.class, PlainValuesWriter.class);}
0
public void testFloat_NoDict()
{    doTestValueWriter(PrimitiveTypeName.FLOAT, WriterVersion.PARQUET_1_0, false, PlainValuesWriter.class);}
0
public void testFloat_V2()
{    doTestValueWriter(PrimitiveTypeName.FLOAT, WriterVersion.PARQUET_2_0, true, PlainFloatDictionaryValuesWriter.class, PlainValuesWriter.class);}
0
public void testFloat_V2_NoDict()
{    doTestValueWriter(PrimitiveTypeName.FLOAT, WriterVersion.PARQUET_2_0, false, PlainValuesWriter.class);}
0
private void doTestValueWriter(PrimitiveTypeName typeName, WriterVersion version, boolean enableDictionary, Class<? extends ValuesWriter> expectedValueWriterClass)
{    ColumnDescriptor mockPath = getMockColumn(typeName);    ValuesWriterFactory factory = getDefaultFactory(version, enableDictionary);    ValuesWriter writer = factory.newValuesWriter(mockPath);    validateWriterType(writer, expectedValueWriterClass);}
0
private void doTestValueWriter(PrimitiveTypeName typeName, WriterVersion version, boolean enableDictionary, Class<? extends ValuesWriter> initialValueWriterClass, Class<? extends ValuesWriter> fallbackValueWriterClass)
{    ColumnDescriptor mockPath = getMockColumn(typeName);    ValuesWriterFactory factory = getDefaultFactory(version, enableDictionary);    ValuesWriter writer = factory.newValuesWriter(mockPath);    validateFallbackWriter(writer, initialValueWriterClass, fallbackValueWriterClass);}
0
private ColumnDescriptor getMockColumn(PrimitiveTypeName typeName)
{    ColumnDescriptor mockPath = mock(ColumnDescriptor.class);    when(mockPath.getType()).thenReturn(typeName);    return mockPath;}
0
private ValuesWriterFactory getDefaultFactory(WriterVersion writerVersion, boolean enableDictionary)
{    ValuesWriterFactory factory = new DefaultValuesWriterFactory();    ParquetProperties.builder().withDictionaryEncoding(enableDictionary).withWriterVersion(writerVersion).withValuesWriterFactory(factory).build();    return factory;}
0
private void validateWriterType(ValuesWriter writer, Class<? extends ValuesWriter> valuesWriterClass)
{    assertTrue("Not instance of: " + valuesWriterClass.getName(), valuesWriterClass.isInstance(writer));}
0
private void validateFallbackWriter(ValuesWriter writer, Class<? extends ValuesWriter> initialWriterClass, Class<? extends ValuesWriter> fallbackWriterClass)
{    validateWriterType(writer, FallbackValuesWriter.class);    FallbackValuesWriter wr = (FallbackValuesWriter) writer;    validateWriterType(wr.initialWriter, initialWriterClass);    validateWriterType(wr.fallBackWriter, fallbackWriterClass);}
0
public String get(int len)
{    StringBuffer out = new StringBuffer();    while (out.length() < len) {        int idx = Math.abs((rand.nextInt() % alphanumeric.length));        out.append(alphanumeric[idx]);    }    return out.toString();}
0
private char[] alphanumeric()
{    StringBuffer buf = new StringBuffer(128);        for (int i = 48; i <= 57; i++) buf.append((char) i);        for (int i = 65; i <= 90; i++) buf.append((char) i);        for (int i = 97; i <= 122; i++) buf.append((char) i);    return buf.toString().toCharArray();}
0
public void integrationTest() throws Exception
{    for (int i = 0; i <= 32; i++) {        doIntegrationTest(i);    }}
0
private void doIntegrationTest(int bitWidth) throws Exception
{    long modValue = 1L << bitWidth;    RunLengthBitPackingHybridEncoder encoder = new RunLengthBitPackingHybridEncoder(bitWidth, 1000, 64000, new DirectByteBufferAllocator());    int numValues = 0;    for (int i = 0; i < 100; i++) {        encoder.writeInt((int) (i % modValue));    }    numValues += 100;    for (int i = 0; i < 100; i++) {        encoder.writeInt((int) (77 % modValue));    }    numValues += 100;    for (int i = 0; i < 100; i++) {        encoder.writeInt((int) (88 % modValue));    }    numValues += 100;    for (int i = 0; i < 1000; i++) {        encoder.writeInt((int) (i % modValue));        encoder.writeInt((int) (i % modValue));        encoder.writeInt((int) (i % modValue));    }    numValues += 3000;    for (int i = 0; i < 1000; i++) {        encoder.writeInt((int) (17 % modValue));    }    numValues += 1000;    ByteBuffer encodedBytes = encoder.toBytes().toByteBuffer();    ByteBufferInputStream in = ByteBufferInputStream.wrap(encodedBytes);    RunLengthBitPackingHybridDecoder decoder = new RunLengthBitPackingHybridDecoder(bitWidth, in);    for (int i = 0; i < 100; i++) {        assertEquals(i % modValue, decoder.readInt());    }    for (int i = 0; i < 100; i++) {        assertEquals(77 % modValue, decoder.readInt());    }    for (int i = 0; i < 100; i++) {        assertEquals(88 % modValue, decoder.readInt());    }    for (int i = 0; i < 1000; i++) {        assertEquals(i % modValue, decoder.readInt());        assertEquals(i % modValue, decoder.readInt());        assertEquals(i % modValue, decoder.readInt());    }    for (int i = 0; i < 1000; i++) {        assertEquals(17 % modValue, decoder.readInt());    }}
0
private RunLengthBitPackingHybridEncoder getRunLengthBitPackingHybridEncoder()
{    return getRunLengthBitPackingHybridEncoder(3, 5, 10);}
0
private RunLengthBitPackingHybridEncoder getRunLengthBitPackingHybridEncoder(int bitWidth, int initialCapacity, int pageSize)
{    return new RunLengthBitPackingHybridEncoder(bitWidth, initialCapacity, pageSize, new DirectByteBufferAllocator());}
0
public void testRLEOnly() throws Exception
{    RunLengthBitPackingHybridEncoder encoder = getRunLengthBitPackingHybridEncoder();    for (int i = 0; i < 100; i++) {        encoder.writeInt(4);    }    for (int i = 0; i < 100; i++) {        encoder.writeInt(5);    }    ByteArrayInputStream is = new ByteArrayInputStream(encoder.toBytes().toByteArray());        assertEquals(200, BytesUtils.readUnsignedVarInt(is));        assertEquals(4, BytesUtils.readIntLittleEndianOnOneByte(is));        assertEquals(200, BytesUtils.readUnsignedVarInt(is));        assertEquals(5, BytesUtils.readIntLittleEndianOnOneByte(is));        assertEquals(-1, is.read());}
0
public void testRepeatedZeros() throws Exception
{                RunLengthBitPackingHybridEncoder encoder = getRunLengthBitPackingHybridEncoder();    for (int i = 0; i < 10; i++) {        encoder.writeInt(0);    }    ByteArrayInputStream is = new ByteArrayInputStream(encoder.toBytes().toByteArray());        assertEquals(20, BytesUtils.readUnsignedVarInt(is));        assertEquals(0, BytesUtils.readIntLittleEndianOnOneByte(is));        assertEquals(-1, is.read());}
0
public void testBitWidthZero() throws Exception
{    RunLengthBitPackingHybridEncoder encoder = getRunLengthBitPackingHybridEncoder(0, 5, 10);    for (int i = 0; i < 10; i++) {        encoder.writeInt(0);    }    ByteArrayInputStream is = new ByteArrayInputStream(encoder.toBytes().toByteArray());        assertEquals(20, BytesUtils.readUnsignedVarInt(is));        assertEquals(-1, is.read());}
0
public void testBitPackingOnly() throws Exception
{    RunLengthBitPackingHybridEncoder encoder = getRunLengthBitPackingHybridEncoder();    for (int i = 0; i < 100; i++) {        encoder.writeInt(i % 3);    }    ByteArrayInputStream is = new ByteArrayInputStream(encoder.toBytes().toByteArray());        assertEquals(27, BytesUtils.readUnsignedVarInt(is));    List<Integer> values = unpack(3, 104, is);    for (int i = 0; i < 100; i++) {        assertEquals(i % 3, (int) values.get(i));    }        assertEquals(-1, is.read());}
0
public void testBitPackingOverflow() throws Exception
{    RunLengthBitPackingHybridEncoder encoder = getRunLengthBitPackingHybridEncoder();    for (int i = 0; i < 1000; i++) {        encoder.writeInt(i % 3);    }    ByteArrayInputStream is = new ByteArrayInputStream(encoder.toBytes().toByteArray());                assertEquals(127, BytesUtils.readUnsignedVarInt(is));    List<Integer> values = unpack(3, 504, is);    for (int i = 0; i < 504; i++) {        assertEquals(i % 3, (int) values.get(i));    }            assertEquals(125, BytesUtils.readUnsignedVarInt(is));    values = unpack(3, 496, is);    for (int i = 0; i < 496; i++) {        assertEquals((i + 504) % 3, (int) values.get(i));    }        assertEquals(-1, is.read());}
0
public void testTransitionFromBitPackingToRle() throws Exception
{    RunLengthBitPackingHybridEncoder encoder = getRunLengthBitPackingHybridEncoder();        encoder.writeInt(0);    encoder.writeInt(1);    encoder.writeInt(0);    encoder.writeInt(1);    encoder.writeInt(0);        encoder.writeInt(2);    encoder.writeInt(2);    encoder.writeInt(2);        for (int i = 0; i < 100; i++) {        encoder.writeInt(2);    }    ByteArrayInputStream is = new ByteArrayInputStream(encoder.toBytes().toByteArray());        assertEquals(3, BytesUtils.readUnsignedVarInt(is));    List<Integer> values = unpack(3, 8, is);    assertEquals(Arrays.asList(0, 1, 0, 1, 0, 2, 2, 2), values);        assertEquals(200, BytesUtils.readUnsignedVarInt(is));        assertEquals(2, BytesUtils.readIntLittleEndianOnOneByte(is));        assertEquals(-1, is.read());}
0
public void testPaddingZerosOnUnfinishedBitPackedRuns() throws Exception
{    RunLengthBitPackingHybridEncoder encoder = getRunLengthBitPackingHybridEncoder(5, 5, 10);    for (int i = 0; i < 9; i++) {        encoder.writeInt(i + 1);    }    ByteArrayInputStream is = new ByteArrayInputStream(encoder.toBytes().toByteArray());        assertEquals(5, BytesUtils.readUnsignedVarInt(is));    List<Integer> values = unpack(5, 16, is);    assertEquals(Arrays.asList(1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 0, 0, 0, 0, 0, 0), values);    assertEquals(-1, is.read());}
0
public void testSwitchingModes() throws Exception
{    RunLengthBitPackingHybridEncoder encoder = getRunLengthBitPackingHybridEncoder(9, 100, 1000);        for (int i = 0; i < 25; i++) {        encoder.writeInt(17);    }        for (int i = 0; i < 7; i++) {        encoder.writeInt(7);    }    encoder.writeInt(8);    encoder.writeInt(9);    encoder.writeInt(10);        for (int i = 0; i < 25; i++) {        encoder.writeInt(6);    }        for (int i = 0; i < 8; i++) {        encoder.writeInt(5);    }    ByteArrayInputStream is = new ByteArrayInputStream(encoder.toBytes().toByteArray());        assertEquals(50, BytesUtils.readUnsignedVarInt(is));        assertEquals(17, BytesUtils.readIntLittleEndianOnTwoBytes(is));        assertEquals(5, BytesUtils.readUnsignedVarInt(is));    List<Integer> values = unpack(9, 16, is);    int v = 0;    for (int i = 0; i < 7; i++) {        assertEquals(7, (int) values.get(v));        v++;    }    assertEquals(8, (int) values.get(v++));    assertEquals(9, (int) values.get(v++));    assertEquals(10, (int) values.get(v++));    for (int i = 0; i < 6; i++) {        assertEquals(6, (int) values.get(v));        v++;    }        assertEquals(38, BytesUtils.readUnsignedVarInt(is));        assertEquals(6, BytesUtils.readIntLittleEndianOnTwoBytes(is));        assertEquals(16, BytesUtils.readUnsignedVarInt(is));        assertEquals(5, BytesUtils.readIntLittleEndianOnTwoBytes(is));        assertEquals(-1, is.read());}
0
public void testGroupBoundary() throws Exception
{    byte[] bytes = new byte[2];            bytes[0] = (1 << 1) | 1;    bytes[1] = (1 << 0) | (2 << 2) | (3 << 4);    ByteArrayInputStream stream = new ByteArrayInputStream(bytes);    RunLengthBitPackingHybridDecoder decoder = new RunLengthBitPackingHybridDecoder(2, stream);    assertEquals(decoder.readInt(), 1);    assertEquals(decoder.readInt(), 2);    assertEquals(decoder.readInt(), 3);    assertEquals(stream.available(), 0);}
0
private static List<Integer> unpack(int bitWidth, int numValues, ByteArrayInputStream is) throws Exception
{    BytePacker packer = Packer.LITTLE_ENDIAN.newBytePacker(bitWidth);    int[] unpacked = new int[8];    byte[] next8Values = new byte[bitWidth];    List<Integer> values = new ArrayList<Integer>(numValues);    while (values.size() < numValues) {        for (int i = 0; i < bitWidth; i++) {            next8Values[i] = (byte) is.read();        }        packer.unpack8Values(next8Values, 0, unpacked, 0);        for (int v = 0; v < 8; v++) {            values.add(unpacked[v]);        }    }    return values;}
0
public void skip()
{}
0
public void initFromPage(int valueCount, ByteBuffer page, int offset) throws IOException
{    data = new byte[valueCount];    ByteBuffer buffer = page.duplicate();    buffer.position(offset);    buffer.get(data);}
0
public void assertPageEquals(String expected)
{    Assert.assertEquals(expected, new String(data));}
0
public void skip()
{}
0
public Binary readBytes()
{    return Binary.fromConstantByteArray(data);}
0
public void initFromPage(int valueCount, ByteBufferInputStream in) throws IOException
{    data = new byte[valueCount];    int off = 0;    int len = valueCount;    int read;    while ((read = in.read(data, off, len)) != -1 && len > 0) {        off += read;        len -= read;    }}
0
public void skip()
{}
0
public Binary readBytes()
{    return Binary.fromConstantByteArray(data);}
0
public void testInvalidValuesReaderImpl() throws IOException
{    ValuesReader reader = new InvalidValuesReaderImpl();    try {        validateWithByteArray(reader);        fail("An UnsupportedOperationException should have been thrown");    } catch (UnsupportedOperationException e) {    }    try {        validateWithByteBuffer(reader);        fail("An UnsupportedOperationException should have been thrown");    } catch (UnsupportedOperationException e) {    }    try {        validateWithByteBufferInputStream(reader);        fail("An UnsupportedOperationException should have been thrown");    } catch (UnsupportedOperationException e) {    }}
0
public void testByteBufferValuesReaderImpl() throws IOException
{    ValuesReader reader = new ByteBufferValuesReaderImpl();    validateWithByteArray(reader);    validateWithByteBuffer(reader);    validateWithByteBufferInputStream(reader);}
0
public void testByteBufferInputStreamValuesReaderImpl() throws IOException
{    ValuesReader reader = new ByteBufferInputStreamValuesReaderImpl();    validateWithByteArray(reader);    validateWithByteBuffer(reader);    validateWithByteBufferInputStream(reader);}
0
private void validateWithByteArray(ValuesReader reader) throws IOException
{    reader.initFromPage(25, "==padding==The expected page content".getBytes(), 11);    assertEquals("The expected page content", reader.readBytes().toStringUsingUTF8());}
0
private void validateWithByteBuffer(ValuesReader reader) throws IOException
{    reader.initFromPage(25, ByteBuffer.wrap("==padding==The expected page content".getBytes()), 11);    assertEquals("The expected page content", reader.readBytes().toStringUsingUTF8());}
0
private void validateWithByteBufferInputStream(ValuesReader reader) throws IOException
{    ByteBufferInputStream bbis = ByteBufferInputStream.wrap(ByteBuffer.wrap("==padding==".getBytes()), ByteBuffer.wrap("The expected ".getBytes()), ByteBuffer.wrap("page content".getBytes()));    bbis.skipFully(11);    reader.initFromPage(25, bbis);    assertEquals("The expected page content", reader.readBytes().toStringUsingUTF8());}
0
public static String[] getRandomStringSamples(int numSamples, int maxLength)
{    String[] samples = new String[numSamples];    for (int i = 0; i < numSamples; i++) {        int len = randomLen.nextInt(maxLength);        samples[i] = randomStr.get(len);    }    return samples;}
0
public static void writeInts(ValuesWriter writer, int[] ints) throws IOException
{    for (int i = 0; i < ints.length; i++) {        writer.writeInteger(ints[i]);    }}
0
public static void writeData(ValuesWriter writer, String[] strings) throws IOException
{    for (int i = 0; i < strings.length; i++) {        writer.writeBytes(Binary.fromString(strings[i]));    }}
0
public static Binary[] readData(ValuesReader reader, ByteBufferInputStream stream, int length) throws IOException
{    Binary[] bins = new Binary[length];    reader.initFromPage(length, stream);    for (int i = 0; i < length; i++) {        bins[i] = reader.readBytes();    }    return bins;}
0
public static int[] readInts(ValuesReader reader, ByteBufferInputStream stream, int length) throws IOException
{    int[] ints = new int[length];    reader.initFromPage(length, stream);    for (int i = 0; i < length; i++) {        ints[i] = reader.readInteger();    }    return ints;}
0
public void testOnlyAppliesToBinary()
{    assertTrue(CorruptStatistics.shouldIgnoreStatistics("parquet-mr version 1.6.0 (build abcd)", PrimitiveTypeName.BINARY));    assertTrue(CorruptStatistics.shouldIgnoreStatistics("parquet-mr version 1.6.0 (build abcd)", PrimitiveTypeName.FIXED_LEN_BYTE_ARRAY));    assertFalse(CorruptStatistics.shouldIgnoreStatistics("parquet-mr version 1.6.0 (build abcd)", PrimitiveTypeName.DOUBLE));}
0
public void testCorruptStatistics()
{    assertTrue(CorruptStatistics.shouldIgnoreStatistics("parquet-mr version 1.6.0 (build abcd)", PrimitiveTypeName.BINARY));    assertTrue(CorruptStatistics.shouldIgnoreStatistics("parquet-mr version 1.4.2 (build abcd)", PrimitiveTypeName.BINARY));    assertTrue(CorruptStatistics.shouldIgnoreStatistics("parquet-mr version 1.6.100 (build abcd)", PrimitiveTypeName.BINARY));    assertTrue(CorruptStatistics.shouldIgnoreStatistics("parquet-mr version 1.7.999 (build abcd)", PrimitiveTypeName.BINARY));    assertTrue(CorruptStatistics.shouldIgnoreStatistics("parquet-mr version 1.6.22rc99 (build abcd)", PrimitiveTypeName.BINARY));    assertTrue(CorruptStatistics.shouldIgnoreStatistics("parquet-mr version 1.6.22rc99-SNAPSHOT (build abcd)", PrimitiveTypeName.BINARY));    assertTrue(CorruptStatistics.shouldIgnoreStatistics("parquet-mr version 1.6.1-SNAPSHOT (build abcd)", PrimitiveTypeName.BINARY));    assertTrue(CorruptStatistics.shouldIgnoreStatistics("parquet-mr version 1.6.0t-01-abcdefg (build abcd)", PrimitiveTypeName.BINARY));    assertTrue(CorruptStatistics.shouldIgnoreStatistics("unparseable string", PrimitiveTypeName.BINARY));        assertTrue(CorruptStatistics.shouldIgnoreStatistics("parquet-mr version (build abcd)", PrimitiveTypeName.BINARY));    assertTrue(CorruptStatistics.shouldIgnoreStatistics("parquet-mr version  (build abcd)", PrimitiveTypeName.BINARY));        assertTrue(CorruptStatistics.shouldIgnoreStatistics("parquet-mr version 1.6.0 (build )", PrimitiveTypeName.BINARY));    assertTrue(CorruptStatistics.shouldIgnoreStatistics("parquet-mr version 1.6.0 (build)", PrimitiveTypeName.BINARY));    assertTrue(CorruptStatistics.shouldIgnoreStatistics("parquet-mr version (build)", PrimitiveTypeName.BINARY));    assertFalse(CorruptStatistics.shouldIgnoreStatistics("imapla version 1.6.0 (build abcd)", PrimitiveTypeName.BINARY));    assertFalse(CorruptStatistics.shouldIgnoreStatistics("imapla version 1.10.0 (build abcd)", PrimitiveTypeName.BINARY));    assertFalse(CorruptStatistics.shouldIgnoreStatistics("parquet-mr version 1.8.0 (build abcd)", PrimitiveTypeName.BINARY));    assertFalse(CorruptStatistics.shouldIgnoreStatistics("parquet-mr version 1.8.1 (build abcd)", PrimitiveTypeName.BINARY));    assertFalse(CorruptStatistics.shouldIgnoreStatistics("parquet-mr version 1.8.1rc3 (build abcd)", PrimitiveTypeName.BINARY));    assertFalse(CorruptStatistics.shouldIgnoreStatistics("parquet-mr version 1.8.1rc3-SNAPSHOT (build abcd)", PrimitiveTypeName.BINARY));    assertFalse(CorruptStatistics.shouldIgnoreStatistics("parquet-mr version 1.9.0 (build abcd)", PrimitiveTypeName.BINARY));    assertFalse(CorruptStatistics.shouldIgnoreStatistics("parquet-mr version 2.0.0 (build abcd)", PrimitiveTypeName.BINARY));    assertFalse(CorruptStatistics.shouldIgnoreStatistics("parquet-mr version 1.9.0t-01-abcdefg (build abcd)", PrimitiveTypeName.BINARY));        assertFalse(CorruptStatistics.shouldIgnoreStatistics("impala version (build abcd)", PrimitiveTypeName.BINARY));    assertFalse(CorruptStatistics.shouldIgnoreStatistics("impala version  (build abcd)", PrimitiveTypeName.BINARY));        assertFalse(CorruptStatistics.shouldIgnoreStatistics("impala version 1.6.0 (build )", PrimitiveTypeName.BINARY));    assertFalse(CorruptStatistics.shouldIgnoreStatistics("impala version 1.6.0 (build)", PrimitiveTypeName.BINARY));    assertFalse(CorruptStatistics.shouldIgnoreStatistics("impala version (build)", PrimitiveTypeName.BINARY));}
0
public void testDistributionCorruptStatistics()
{    assertTrue(CorruptStatistics.shouldIgnoreStatistics("parquet-mr version 1.5.0-cdh5.4.999 (build abcd)", PrimitiveTypeName.BINARY));    assertFalse(CorruptStatistics.shouldIgnoreStatistics("parquet-mr version 1.5.0-cdh5.5.0-SNAPSHOT (build 956ed6c14c611b4c4eaaa1d6e5b9a9c6d4dfa336)", PrimitiveTypeName.BINARY));    assertFalse(CorruptStatistics.shouldIgnoreStatistics("parquet-mr version 1.5.0-cdh5.5.0 (build abcd)", PrimitiveTypeName.BINARY));    assertFalse(CorruptStatistics.shouldIgnoreStatistics("parquet-mr version 1.5.0-cdh5.5.1 (build abcd)", PrimitiveTypeName.BINARY));    assertFalse(CorruptStatistics.shouldIgnoreStatistics("parquet-mr version 1.5.0-cdh5.6.0 (build abcd)", PrimitiveTypeName.BINARY));    assertTrue(CorruptStatistics.shouldIgnoreStatistics("parquet-mr version 1.4.10 (build abcd)", PrimitiveTypeName.BINARY));    assertTrue(CorruptStatistics.shouldIgnoreStatistics("parquet-mr version 1.5.0 (build abcd)", PrimitiveTypeName.BINARY));    assertTrue(CorruptStatistics.shouldIgnoreStatistics("parquet-mr version 1.5.1 (build abcd)", PrimitiveTypeName.BINARY));    assertTrue(CorruptStatistics.shouldIgnoreStatistics("parquet-mr version 1.6.0 (build abcd)", PrimitiveTypeName.BINARY));    assertTrue(CorruptStatistics.shouldIgnoreStatistics("parquet-mr version 1.7.0 (build abcd)", PrimitiveTypeName.BINARY));}
0
public boolean keep(Integer value)
{    return false;}
0
public boolean canDrop(Statistics<Integer> statistics)
{    return false;}
0
public boolean inverseCanDrop(Statistics<Integer> statistics)
{    return false;}
0
public void testFilterPredicateCreation()
{    FilterPredicate outerAnd = predicate;    assertTrue(outerAnd instanceof And);    FilterPredicate not = ((And) outerAnd).getLeft();    FilterPredicate gt = ((And) outerAnd).getRight();    assertTrue(not instanceof Not);    FilterPredicate or = ((Not) not).getPredicate();    assertTrue(or instanceof Or);    FilterPredicate leftEq = ((Or) or).getLeft();    FilterPredicate rightNotEq = ((Or) or).getRight();    assertTrue(leftEq instanceof Eq);    assertTrue(rightNotEq instanceof NotEq);    assertEquals(7, ((Eq) leftEq).getValue());    assertEquals(17, ((NotEq) rightNotEq).getValue());    assertEquals(ColumnPath.get("a", "b", "c"), ((Eq) leftEq).getColumn().getColumnPath());    assertEquals(ColumnPath.get("a", "b", "c"), ((NotEq) rightNotEq).getColumn().getColumnPath());    assertTrue(gt instanceof Gt);    assertEquals(100.0, ((Gt) gt).getValue());    assertEquals(ColumnPath.get("x", "y", "z"), ((Gt) gt).getColumn().getColumnPath());}
0
public void testToString()
{    FilterPredicate pred = or(predicate, notEq(binColumn, Binary.fromString("foobarbaz")));    assertEquals("or(and(not(or(eq(a.b.c, 7), noteq(a.b.c, 17))), gt(x.y.z, 100.0)), " + "noteq(a.string.column, Binary{\"foobarbaz\"}))", pred.toString());}
0
public void testUdp()
{    FilterPredicate predicate = or(eq(doubleColumn, 12.0), userDefined(intColumn, DummyUdp.class));    assertTrue(predicate instanceof Or);    FilterPredicate ud = ((Or) predicate).getRight();    assertTrue(ud instanceof UserDefinedByClass);    assertEquals(DummyUdp.class, ((UserDefinedByClass) ud).getUserDefinedPredicateClass());    assertTrue(((UserDefined) ud).getUserDefinedPredicate() instanceof DummyUdp);}
0
public void testSerializable() throws Exception
{    BinaryColumn binary = binaryColumn("foo");    FilterPredicate p = and(or(and(userDefined(intColumn, DummyUdp.class), predicate), eq(binary, Binary.fromString("hi"))), userDefined(longColumn, new IsMultipleOf(7)));    ByteArrayOutputStream baos = new ByteArrayOutputStream();    ObjectOutputStream oos = new ObjectOutputStream(baos);    oos.writeObject(p);    oos.close();    ObjectInputStream is = new ObjectInputStream(new ByteArrayInputStream(baos.toByteArray()));    FilterPredicate read = (FilterPredicate) is.readObject();    assertEquals(p, read);}
0
public boolean keep(Long value)
{    if (value == null) {        return false;    }    return value % of == 0;}
0
public boolean canDrop(Statistics<Long> statistics)
{    return false;}
0
public boolean inverseCanDrop(Statistics<Long> statistics)
{    return false;}
0
public boolean equals(Object o)
{    if (this == o)        return true;    if (o == null || getClass() != o.getClass())        return false;    IsMultipleOf that = (IsMultipleOf) o;    return this.of == that.of;}
0
public int hashCode()
{    return new Long(of).hashCode();}
0
public String toString()
{    return "IsMultipleOf(" + of + ")";}
0
private static void assertNoOp(FilterPredicate p)
{    assertEquals(p, rewrite(p));}
0
public void testBaseCases()
{    UserDefined<Integer, DummyUdp> ud = userDefined(intColumn, DummyUdp.class);    assertNoOp(eq(intColumn, 17));    assertNoOp(notEq(intColumn, 17));    assertNoOp(lt(intColumn, 17));    assertNoOp(ltEq(intColumn, 17));    assertNoOp(gt(intColumn, 17));    assertNoOp(gtEq(intColumn, 17));    assertNoOp(and(eq(intColumn, 17), eq(doubleColumn, 12.0)));    assertNoOp(or(eq(intColumn, 17), eq(doubleColumn, 12.0)));    assertNoOp(ud);    assertEquals(notEq(intColumn, 17), rewrite(not(eq(intColumn, 17))));    assertEquals(eq(intColumn, 17), rewrite(not(notEq(intColumn, 17))));    assertEquals(gtEq(intColumn, 17), rewrite(not(lt(intColumn, 17))));    assertEquals(gt(intColumn, 17), rewrite(not(ltEq(intColumn, 17))));    assertEquals(ltEq(intColumn, 17), rewrite(not(gt(intColumn, 17))));    assertEquals(lt(intColumn, 17), rewrite(not(gtEq(intColumn, 17))));    assertEquals(new LogicalNotUserDefined<Integer, DummyUdp>(ud), rewrite(not(ud)));    FilterPredicate notedAnd = not(and(eq(intColumn, 17), eq(doubleColumn, 12.0)));    FilterPredicate distributedAnd = or(notEq(intColumn, 17), notEq(doubleColumn, 12.0));    assertEquals(distributedAnd, rewrite(notedAnd));    FilterPredicate andWithNots = and(not(gtEq(intColumn, 17)), lt(intColumn, 7));    FilterPredicate andWithoutNots = and(lt(intColumn, 17), lt(intColumn, 7));    assertEquals(andWithoutNots, rewrite(andWithNots));}
0
public void testComplex()
{    assertEquals(complexCollapsed, rewrite(complex));}
0
public void testBaseCases()
{    assertEquals(notEq(intColumn, 17), invert(eq(intColumn, 17)));    assertEquals(eq(intColumn, 17), invert(notEq(intColumn, 17)));    assertEquals(gtEq(intColumn, 17), invert(lt(intColumn, 17)));    assertEquals(gt(intColumn, 17), invert(ltEq(intColumn, 17)));    assertEquals(ltEq(intColumn, 17), invert(gt(intColumn, 17)));    assertEquals(lt(intColumn, 17), invert(gtEq(intColumn, 17)));    FilterPredicate andPos = and(eq(intColumn, 17), eq(doubleColumn, 12.0));    FilterPredicate andInv = or(notEq(intColumn, 17), notEq(doubleColumn, 12.0));    assertEquals(andInv, invert(andPos));    FilterPredicate orPos = or(eq(intColumn, 17), eq(doubleColumn, 12.0));    FilterPredicate orInv = and(notEq(intColumn, 17), notEq(doubleColumn, 12.0));    assertEquals(orPos, invert(orInv));    assertEquals(eq(intColumn, 17), invert(not(eq(intColumn, 17))));    UserDefined<Integer, DummyUdp> ud = userDefined(intColumn, DummyUdp.class);    assertEquals(new LogicalNotUserDefined<Integer, DummyUdp>(ud), invert(ud));    assertEquals(ud, invert(not(ud)));    assertEquals(ud, invert(new LogicalNotUserDefined<Integer, DummyUdp>(ud)));}
0
public void testComplex()
{    assertEquals(complexInverse, invert(complex));}
0
public boolean keep(Long value)
{    return false;}
0
public boolean canDrop(Statistics<Long> statistics)
{    return false;}
0
public boolean inverseCanDrop(Statistics<Long> statistics)
{    return false;}
0
public void testValidType()
{    validate(complexValid, schema);}
0
public void testFindsInvalidTypes()
{    try {        validate(complexWrongType, schema);        fail("this should throw");    } catch (IllegalArgumentException e) {        assertEquals("FilterPredicate column: x.bar's declared type (java.lang.Long) does not match the schema found in file metadata. " + "Column x.bar is of type: INT32\n" + "Valid types for this column are: [class java.lang.Integer]", e.getMessage());    }}
0
public void testTwiceDeclaredColumn()
{    validate(eq(stringC, Binary.fromString("larry")), schema);    try {        validate(complexMixedType, schema);        fail("this should throw");    } catch (IllegalArgumentException e) {        assertEquals("Column: x.bar was provided with different types in the same predicate. Found both: (class java.lang.Integer, class java.lang.Long)", e.getMessage());    }}
0
public void testRepeatedNotSupported()
{    try {        validate(eq(lotsOfLongs, 10l), schema);        fail("this should throw");    } catch (IllegalArgumentException e) {        assertEquals("FilterPredicates do not currently support repeated columns. Column lotsOfLongs is repeated.", e.getMessage());    }}
0
public int compareTo(InvalidColumnType o)
{    return 0;}
0
public void testValidTypes()
{    assertTypeValid(intColumn, PrimitiveTypeName.INT32);    assertTypeValid(longColumn, PrimitiveTypeName.INT64);    assertTypeValid(floatColumn, PrimitiveTypeName.FLOAT);    assertTypeValid(doubleColumn, PrimitiveTypeName.DOUBLE);    assertTypeValid(booleanColumn, PrimitiveTypeName.BOOLEAN);    assertTypeValid(binaryColumn, PrimitiveTypeName.BINARY);    assertTypeValid(binaryColumn, PrimitiveTypeName.FIXED_LEN_BYTE_ARRAY);    assertTypeValid(binaryColumn, PrimitiveTypeName.INT96);}
0
public void testMismatchedTypes()
{    try {        assertTypeValid(intColumn, PrimitiveTypeName.DOUBLE);        fail("This should throw!");    } catch (IllegalArgumentException e) {        assertEquals("FilterPredicate column: int.column's declared type (java.lang.Integer) does not match the " + "schema found in file metadata. Column int.column is of type: " + "DOUBLE\n" + "Valid types for this column are: [class java.lang.Double]", e.getMessage());    }}
0
public void testUnsupportedType()
{    try {        assertTypeValid(invalidColumn, PrimitiveTypeName.INT32);        fail("This should throw!");    } catch (IllegalArgumentException e) {        assertEquals("Column invalid.column was declared as type: " + "org.apache.parquet.filter2.predicate.TestValidTypeMap$InvalidColumnType which is not supported " + "in FilterPredicates. Supported types for this column are: [class java.lang.Integer]", e.getMessage());    }}
0
public static ValueInspector intIsNull()
{    return new ValueInspector() {        @Override        public void updateNull() {            setResult(true);        }        @Override        public void update(int value) {            setResult(false);        }    };}
0
public void updateNull()
{    setResult(true);}
0
public void update(int value)
{    setResult(false);}
0
public static ValueInspector intIsEven()
{    return new ValueInspector() {        @Override        public void updateNull() {            setResult(false);        }        @Override        public void update(int value) {            setResult(value % 2 == 0);        }    };}
0
public void updateNull()
{    setResult(false);}
0
public void update(int value)
{    setResult(value % 2 == 0);}
0
public static ValueInspector doubleMoreThan10()
{    return new ValueInspector() {        @Override        public void updateNull() {            setResult(false);        }        @Override        public void update(double value) {            setResult(value > 10.0);        }    };}
0
public void updateNull()
{    setResult(false);}
0
public void update(double value)
{    setResult(value > 10.0);}
0
public void testValueInspector()
{        ValueInspector v = intIsEven();    v.update(11);    assertFalse(evaluate(v));    v.reset();        v.update(12);    assertTrue(evaluate(v));    v.reset();        v.updateNull();    assertFalse(evaluate(v));    v.reset();        ValueInspector intIsNull = intIsNull();    intIsNull.update(10);    assertFalse(evaluate(intIsNull));    intIsNull.reset();        intIsNull.updateNull();    assertTrue(evaluate(intIsNull));    intIsNull.reset();        v.reset();    assertFalse(evaluate(v));        intIsNull.reset();    assertTrue(evaluate(intIsNull));}
0
private void doOrTest(ValueInspector v1, ValueInspector v2, int v1Value, int v2Value, boolean expected)
{    v1.update(v1Value);    v2.update(v2Value);    IncrementallyUpdatedFilterPredicate or = new Or(v1, v2);    assertEquals(expected, evaluate(or));    v1.reset();    v2.reset();}
0
private void doAndTest(ValueInspector v1, ValueInspector v2, int v1Value, int v2Value, boolean expected)
{    v1.update(v1Value);    v2.update(v2Value);    IncrementallyUpdatedFilterPredicate and = new And(v1, v2);    assertEquals(expected, evaluate(and));    v1.reset();    v2.reset();}
0
public void testOr()
{    ValueInspector v1 = intIsEven();    ValueInspector v2 = intIsEven();    int F = 11;    int T = 12;        doOrTest(v1, v2, F, F, false);        doOrTest(v1, v2, F, T, true);        doOrTest(v1, v2, T, F, true);        doOrTest(v1, v2, T, T, true);}
0
public void testAnd()
{    ValueInspector v1 = intIsEven();    ValueInspector v2 = intIsEven();    int F = 11;    int T = 12;        doAndTest(v1, v2, F, F, false);        doAndTest(v1, v2, F, T, false);        doAndTest(v1, v2, T, F, false);        doAndTest(v1, v2, T, T, true);}
0
public void testShortCircuit()
{    ValueInspector neverCalled = new ValueInspector() {        @Override        public boolean accept(Visitor visitor) {            throw new ShortCircuitException();        }    };    try {        evaluate(neverCalled);        fail("this should throw");    } catch (ShortCircuitException e) {        }        ValueInspector v = intIsEven();    v.update(10);    IncrementallyUpdatedFilterPredicate or = new Or(v, neverCalled);    assertTrue(evaluate(or));    v.reset();        v.update(11);    IncrementallyUpdatedFilterPredicate and = new And(v, neverCalled);    assertFalse(evaluate(and));    v.reset();}
0
public boolean accept(Visitor visitor)
{    throw new ShortCircuitException();}
0
public void testReset()
{    ValueInspector intIsNull = intIsNull();    ValueInspector intIsEven = intIsEven();    ValueInspector doubleMoreThan10 = doubleMoreThan10();    IncrementallyUpdatedFilterPredicate pred = new Or(intIsNull, new And(intIsEven, doubleMoreThan10));    intIsNull.updateNull();    intIsEven.update(11);    doubleMoreThan10.update(20.0D);    assertTrue(intIsNull.isKnown());    assertTrue(intIsEven.isKnown());    assertTrue(doubleMoreThan10.isKnown());    IncrementallyUpdatedFilterPredicateResetter.reset(pred);    assertFalse(intIsNull.isKnown());    assertFalse(intIsEven.isKnown());    assertFalse(doubleMoreThan10.isKnown());    intIsNull.updateNull();    assertTrue(intIsNull.isKnown());    assertFalse(intIsEven.isKnown());    assertFalse(doubleMoreThan10.isKnown());    IncrementallyUpdatedFilterPredicateResetter.reset(pred);    assertFalse(intIsNull.isKnown());    assertFalse(intIsEven.isKnown());    assertFalse(doubleMoreThan10.isKnown());}
0
public void testLifeCycle()
{    ValueInspector v = intIsEven();        assertFalse(v.isKnown());        try {        v.getResult();        fail("this should throw");    } catch (IllegalStateException e) {        assertEquals("getResult() called on a ValueInspector whose result is not yet known!", e.getMessage());    }        v.update(10);        assertTrue(v.isKnown());    assertTrue(v.getResult());        try {        v.update(11);        fail("this should throw");    } catch (IllegalStateException e) {        assertEquals("setResult() called on a ValueInspector whose result is already known!" + " Did you forget to call reset()?", e.getMessage());    }        v.reset();    assertFalse(v.isKnown());        try {        v.getResult();        fail("this should throw");    } catch (IllegalStateException e) {        assertEquals("getResult() called on a ValueInspector whose result is not yet known!", e.getMessage());    }        v.update(11);    assertTrue(v.isKnown());    assertFalse(v.getResult());}
0
public void testReusable()
{    List<Integer> values = Arrays.asList(2, 4, 7, 3, 8, 8, 11, 200);    ValueInspector v = intIsEven();    for (Integer x : values) {        v.update(x);        assertEquals(x % 2 == 0, v.getResult());        v.reset();    }}
0
public void testNonStringTruncate()
{    BinaryTruncator truncator = BinaryTruncator.getTruncator(Types.required(BINARY).as(DECIMAL).precision(10).scale(2).named("test_binary_decimal"));    assertEquals(binary(0xFF, 0xFE, 0xFD, 0xFC, 0xFB, 0xFA), truncator.truncateMin(binary(0xFF, 0xFE, 0xFD, 0xFC, 0xFB, 0xFA), 2));    assertEquals(binary(0x01, 0x02, 0x03, 0x04, 0x05, 0x06), truncator.truncateMax(binary(0x01, 0x02, 0x03, 0x04, 0x05, 0x06), 2));}
0
public void testContractNonStringTypes()
{    testTruncator(Types.required(FIXED_LEN_BYTE_ARRAY).length(8).as(DECIMAL).precision(18).scale(4).named("test_fixed_decimal"), false);    testTruncator(Types.required(FIXED_LEN_BYTE_ARRAY).length(12).as(INTERVAL).named("test_fixed_interval"), false);    testTruncator(Types.required(BINARY).as(DECIMAL).precision(10).scale(2).named("test_binary_decimal"), false);    testTruncator(Types.required(INT96).named("test_int96"), false);}
0
public void testStringTruncate()
{    BinaryTruncator truncator = BinaryTruncator.getTruncator(Types.required(BINARY).as(UTF8).named("test_utf8"));        assertEquals(Binary.fromString("abc"), truncator.truncateMin(Binary.fromString("abcdef"), 3));    assertEquals(Binary.fromString("abd"), truncator.truncateMax(Binary.fromString("abcdef"), 3));        assertEquals(Binary.fromString("árvízt"), truncator.truncateMin(Binary.fromString("árvíztűrő"), 9));    assertEquals(Binary.fromString("árvízu"), truncator.truncateMax(Binary.fromString("árvíztűrő"), 9));        assertEquals(Binary.fromString(UTF8_1BYTE_MAX_CHAR + UTF8_2BYTES_MAX_CHAR), truncator.truncateMin(Binary.fromString(UTF8_1BYTE_MAX_CHAR + UTF8_2BYTES_MAX_CHAR + UTF8_3BYTES_MAX_CHAR + UTF8_4BYTES_MAX_CHAR), 5));    assertEquals(Binary.fromString(UTF8_1BYTE_MAX_CHAR + UTF8_2BYTES_MAX_CHAR + UTF8_3BYTES_MAX_CHAR + UTF8_4BYTES_MAX_CHAR), truncator.truncateMax(Binary.fromString(UTF8_1BYTE_MAX_CHAR + UTF8_2BYTES_MAX_CHAR + UTF8_3BYTES_MAX_CHAR + UTF8_4BYTES_MAX_CHAR), 5));        assertEquals(Binary.fromString(UTF8_1BYTE_MAX_CHAR + UTF8_2BYTES_MAX_CHAR + "b" + UTF8_3BYTES_MAX_CHAR), truncator.truncateMax(Binary.fromString(UTF8_1BYTE_MAX_CHAR + UTF8_2BYTES_MAX_CHAR + "a" + UTF8_3BYTES_MAX_CHAR + UTF8_4BYTES_MAX_CHAR), 10));        assertEquals(binary(0xFF, 0xFE, 0xFD), truncator.truncateMin(binary(0xFF, 0xFE, 0xFD, 0xFC, 0xFB, 0xFA), 3));    assertEquals(binary(0xFF, 0xFE, 0xFE), truncator.truncateMax(binary(0xFF, 0xFE, 0xFD, 0xFC, 0xFB, 0xFA), 3));    assertEquals(binary(0xFF, 0xFE, 0xFE, 0x00, 0x00), truncator.truncateMax(binary(0xFF, 0xFE, 0xFD, 0xFF, 0xFF, 0xFF), 5));}
0
public void testContractStringTypes()
{    testTruncator(Types.required(BINARY).named("test_binary"), true);    testTruncator(Types.required(BINARY).as(UTF8).named("test_utf8"), true);    testTruncator(Types.required(BINARY).as(ENUM).named("test_enum"), true);    testTruncator(Types.required(BINARY).as(JSON).named("test_json"), true);    testTruncator(Types.required(BINARY).as(BSON).named("test_bson"), true);    testTruncator(Types.required(FIXED_LEN_BYTE_ARRAY).length(5).named("test_fixed"), true);}
0
private void testTruncator(PrimitiveType type, boolean strict)
{    BinaryTruncator truncator = BinaryTruncator.getTruncator(type);    Comparator<Binary> comparator = type.comparator();    checkContract(truncator, comparator, Binary.fromString("aaaaaaaaaa"), strict, strict);    checkContract(truncator, comparator, Binary.fromString("árvíztűrő tükörfúrógép"), strict, strict);    checkContract(truncator, comparator, Binary.fromString("aaaaaaaaaa" + UTF8_3BYTES_MAX_CHAR), strict, strict);    checkContract(truncator, comparator, Binary.fromString("a" + UTF8_3BYTES_MAX_CHAR + UTF8_1BYTE_MAX_CHAR), strict, strict);    checkContract(truncator, comparator, Binary.fromConstantByteArray(new byte[] { (byte) 0xFE, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, (byte) 0xFF }), strict, strict);        checkContract(truncator, comparator, Binary.fromString(""), false, false);        checkContract(truncator, comparator, Binary.fromString(UTF8_1BYTE_MAX_CHAR + UTF8_4BYTES_MAX_CHAR + UTF8_3BYTES_MAX_CHAR + UTF8_4BYTES_MAX_CHAR + UTF8_2BYTES_MAX_CHAR + UTF8_3BYTES_MAX_CHAR + UTF8_3BYTES_MAX_CHAR + UTF8_1BYTE_MAX_CHAR + UTF8_2BYTES_MAX_CHAR + UTF8_3BYTES_MAX_CHAR + UTF8_4BYTES_MAX_CHAR), strict, false);        checkContract(truncator, comparator, binary(0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF), strict, false);}
0
private void checkContract(BinaryTruncator truncator, Comparator<Binary> comparator, Binary value, boolean strictMin, boolean strictMax)
{    int length = value.length();        assertSame(value, truncator.truncateMin(value, length));    assertSame(value, truncator.truncateMax(value, length));    assertSame(value, truncator.truncateMin(value, random(length + 1, length * 2 + 1)));    assertSame(value, truncator.truncateMax(value, random(length + 1, length * 2 + 1)));    if (length > 1) {        checkMinContract(truncator, comparator, value, length - 1, strictMin);        checkMaxContract(truncator, comparator, value, length - 1, strictMax);        checkMinContract(truncator, comparator, value, random(1, length - 1), strictMin);        checkMaxContract(truncator, comparator, value, random(1, length - 1), strictMax);    }        checkMinContract(truncator, comparator, value, 0, strictMin);        assertSame(value, truncator.truncateMax(value, 0));}
0
private void checkMinContract(BinaryTruncator truncator, Comparator<Binary> comparator, Binary value, int length, boolean strict)
{    Binary truncated = truncator.truncateMin(value, length);        assertTrue("truncatedMin(value) should be <= than value", comparator.compare(truncated, value) <= 0);    assertFalse("length of truncateMin(value) should not be > than the length of value", truncated.length() > value.length());    if (isValidUtf8(value)) {        checkValidUtf8(truncated);    }    if (strict) {        assertTrue("length of truncateMin(value) ahould be < than the length of value", truncated.length() < value.length());    }}
1
private void checkMaxContract(BinaryTruncator truncator, Comparator<Binary> comparator, Binary value, int length, boolean strict)
{    Binary truncated = truncator.truncateMax(value, length);        assertTrue("truncatedMax(value) should be >= than value", comparator.compare(truncated, value) >= 0);    assertFalse("length of truncateMax(value) should not be > than the length of value", truncated.length() > value.length());    if (isValidUtf8(value)) {        checkValidUtf8(truncated);    }    if (strict) {        assertTrue("length of truncateMax(value) ahould be < than the length of value", truncated.length() < value.length());    }}
1
private static boolean isValidUtf8(Binary binary)
{    try {        UTF8_DECODER.decode(binary.toByteBuffer());        return true;    } catch (CharacterCodingException e) {        return false;    }}
0
private static void checkValidUtf8(Binary binary)
{    try {        UTF8_DECODER.decode(binary.toByteBuffer());    } catch (CharacterCodingException e) {        throw new AssertionError("Truncated value should be a valid UTF-8 string", e);    }}
0
private static int random(int min, int max)
{    return RANDOM.nextInt(max - min + 1) + min;}
0
private static Binary binary(int... unsignedBytes)
{    byte[] byteArray = new byte[unsignedBytes.length];    for (int i = 0, n = byteArray.length; i < n; ++i) {        int b = unsignedBytes[i];        assert (0xFFFFFF00 & b) == 0;        byteArray[i] = (byte) b;    }    return Binary.fromConstantByteArray(byteArray);}
0
 int getCompareCount()
{    return compareCount;}
0
 int arrayLength()
{    return delegate.arrayLength();}
0
 int translate(int arrayIndex)
{    return delegate.translate(arrayIndex);}
0
 int compareValueToMin(int arrayIndex)
{    ++compareCount;    return delegate.compareValueToMin(arrayIndex);}
0
 int compareValueToMax(int arrayIndex)
{    ++compareCount;    return delegate.compareValueToMax(arrayIndex);}
0
 SpyValueComparator build(ColumnIndexBase<?>.ValueComparator comparator)
{    return new SpyValueComparator(comparator);}
0
 ByteBuffer getMinValueAsBytes(int arrayIndex)
{    throw new Error("Shall never be invoked");}
0
 ByteBuffer getMaxValueAsBytes(int arrayIndex)
{    throw new Error("Shall never be invoked");}
0
 String getMinValueAsString(int arrayIndex)
{    throw new Error("Shall never be invoked");}
0
 String getMaxValueAsString(int arrayIndex)
{    throw new Error("Shall never be invoked");}
0
 org.apache.parquet.filter2.predicate.Statistics<T> createStats(int arrayIndex)
{    throw new Error("Shall never be invoked");}
0
 ColumnIndexBase<Integer>.ValueComparator createValueComparator(Object value)
{    throw new Error("Shall never be invoked");}
0
 IntList measureLinear(Function<ColumnIndexBase<?>.ValueComparator, PrimitiveIterator.OfInt> op, ColumnIndexBase<?>.ValueComparator comparator)
{    IntList list = new IntArrayList(comparator.arrayLength());    SpyValueComparatorBuilder.SpyValueComparator spyComparator = SPY_COMPARATOR_BUILDER.build(comparator);    long start = System.nanoTime();    op.apply(spyComparator).forEachRemaining((int value) -> list.add(value));    linearTime = System.nanoTime() - start;    linearCompareCount += spyComparator.getCompareCount();    return list;}
0
 IntList measureBinary(Function<ColumnIndexBase<?>.ValueComparator, PrimitiveIterator.OfInt> op, ColumnIndexBase<?>.ValueComparator comparator)
{    IntList list = new IntArrayList(comparator.arrayLength());    SpyValueComparatorBuilder.SpyValueComparator spyComparator = SPY_COMPARATOR_BUILDER.build(comparator);    long start = System.nanoTime();    op.apply(spyComparator).forEachRemaining((int value) -> list.add(value));    binaryTime = System.nanoTime() - start;    binaryCompareCount += spyComparator.getCompareCount();    return list;}
0
 void add(ExecStats stats)
{    linearTime += stats.linearTime;    linearCompareCount += stats.linearCompareCount;    binaryTime += stats.binaryTime;    binaryCompareCount += stats.binaryCompareCount;    ++execCount;}
0
public String toString()
{    double linearMs = linearTime / 1_000_000.0;    double binaryMs = binaryTime / 1_000_000.0;    return String.format("Linear search: %.2fms (avg: %.6fms); number of compares: %d (avg: %d) [100.00%%]%n" + "Binary search: %.2fms (avg: %.6fms); number of compares: %d (avg: %d) [%.2f%%]", linearMs, linearMs / execCount, linearCompareCount, linearCompareCount / execCount, binaryMs, binaryMs / execCount, binaryCompareCount, binaryCompareCount / execCount, 100.0 * binaryCompareCount / linearCompareCount);}
0
private static Statistics<?> stats(int min, int max)
{    Statistics<?> stats = Statistics.createStats(TYPE);    stats.updateStats(min);    stats.updateStats(max);    return stats;}
0
private static ExecStats validateOperator(String msg, Function<ColumnIndexBase<?>.ValueComparator, PrimitiveIterator.OfInt> validatorOp, Function<ColumnIndexBase<?>.ValueComparator, PrimitiveIterator.OfInt> actualOp, ColumnIndexBase<?>.ValueComparator comparator)
{    ExecStats stats = new ExecStats();    IntList expected = stats.measureLinear(validatorOp, comparator);    IntList actual = stats.measureBinary(actualOp, comparator);    Assert.assertEquals(msg, expected, actual);    return stats;}
0
public void testEq()
{    for (int i = FROM - 1; i <= TO + 1; ++i) {        validateOperator("Mismatching page indexes for value " + i + " with ASCENDING order", BoundaryOrder.UNORDERED::eq, BoundaryOrder.ASCENDING::eq, ASCENDING.createValueComparator(i));        validateOperator("Mismatching page indexes for value " + i + " with DESCENDING order", BoundaryOrder.UNORDERED::eq, BoundaryOrder.DESCENDING::eq, DESCENDING.createValueComparator(i));    }    for (int i = SINGLE_FROM - 1; i <= SINGLE_TO + 1; ++i) {        ColumnIndexBase<?>.ValueComparator singleComparator = SINGLE.createValueComparator(i);        validateOperator("Mismatching page indexes for value " + i + " with ASCENDING order", BoundaryOrder.UNORDERED::eq, BoundaryOrder.ASCENDING::eq, singleComparator);        validateOperator("Mismatching page indexes for value " + i + " with DESCENDING order", BoundaryOrder.UNORDERED::eq, BoundaryOrder.DESCENDING::eq, singleComparator);    }    ExecStats stats = new ExecStats();    for (int i = RAND_FROM - 1; i <= RAND_TO + 1; ++i) {        stats.add(validateOperator("Mismatching page indexes for value " + i + " with ASCENDING order", BoundaryOrder.UNORDERED::eq, BoundaryOrder.ASCENDING::eq, RAND_ASCENDING.createValueComparator(i)));        stats.add(validateOperator("Mismatching page indexes for value " + i + " with DESCENDING order", BoundaryOrder.UNORDERED::eq, BoundaryOrder.DESCENDING::eq, RAND_DESCENDING.createValueComparator(i)));    }    }
1
public void testGt()
{    for (int i = FROM - 1; i <= TO + 1; ++i) {        validateOperator("Mismatching page indexes for value " + i + " with ASCENDING order", BoundaryOrder.UNORDERED::gt, BoundaryOrder.ASCENDING::gt, ASCENDING.createValueComparator(i));        validateOperator("Mismatching page indexes for value " + i + " with DESCENDING order", BoundaryOrder.UNORDERED::gt, BoundaryOrder.DESCENDING::gt, DESCENDING.createValueComparator(i));    }    for (int i = SINGLE_FROM - 1; i <= SINGLE_TO + 1; ++i) {        ColumnIndexBase<?>.ValueComparator singleComparator = SINGLE.createValueComparator(i);        validateOperator("Mismatching page indexes for value " + i + " with ASCENDING order", BoundaryOrder.UNORDERED::gt, BoundaryOrder.ASCENDING::gt, singleComparator);        validateOperator("Mismatching page indexes for value " + i + " with DESCENDING order", BoundaryOrder.UNORDERED::gt, BoundaryOrder.DESCENDING::gt, singleComparator);    }    ExecStats stats = new ExecStats();    for (int i = RAND_FROM - 1; i <= RAND_TO + 1; ++i) {        stats.add(validateOperator("Mismatching page indexes for value " + i + " with ASCENDING order", BoundaryOrder.UNORDERED::gt, BoundaryOrder.ASCENDING::gt, RAND_ASCENDING.createValueComparator(i)));        stats.add(validateOperator("Mismatching page indexes for value " + i + " with DESCENDING order", BoundaryOrder.UNORDERED::gt, BoundaryOrder.DESCENDING::gt, RAND_DESCENDING.createValueComparator(i)));    }    }
1
public void testGtEq()
{    for (int i = FROM - 1; i <= TO + 1; ++i) {        validateOperator("Mismatching page indexes for value " + i + " with ASCENDING order", BoundaryOrder.UNORDERED::gtEq, BoundaryOrder.ASCENDING::gtEq, ASCENDING.createValueComparator(i));        validateOperator("Mismatching page indexes for value " + i + " with DESCENDING order", BoundaryOrder.UNORDERED::gtEq, BoundaryOrder.DESCENDING::gtEq, DESCENDING.createValueComparator(i));    }    for (int i = SINGLE_FROM - 1; i <= SINGLE_TO + 1; ++i) {        ColumnIndexBase<?>.ValueComparator singleComparator = SINGLE.createValueComparator(i);        validateOperator("Mismatching page indexes for value " + i + " with ASCENDING order", BoundaryOrder.UNORDERED::gtEq, BoundaryOrder.ASCENDING::gtEq, singleComparator);        validateOperator("Mismatching page indexes for value " + i + " with DESCENDING order", BoundaryOrder.UNORDERED::gtEq, BoundaryOrder.DESCENDING::gtEq, singleComparator);    }    ExecStats stats = new ExecStats();    for (int i = RAND_FROM - 1; i <= RAND_TO + 1; ++i) {        stats.add(validateOperator("Mismatching page indexes for value " + i + " with ASCENDING order", BoundaryOrder.UNORDERED::gtEq, BoundaryOrder.ASCENDING::gtEq, RAND_ASCENDING.createValueComparator(i)));        stats.add(validateOperator("Mismatching page indexes for value " + i + " with DESCENDING order", BoundaryOrder.UNORDERED::gtEq, BoundaryOrder.DESCENDING::gtEq, RAND_DESCENDING.createValueComparator(i)));    }    }
1
public void testLt()
{    for (int i = FROM - 1; i <= TO + 1; ++i) {        validateOperator("Mismatching page indexes for value " + i + " with ASCENDING order", BoundaryOrder.UNORDERED::lt, BoundaryOrder.ASCENDING::lt, ASCENDING.createValueComparator(i));        validateOperator("Mismatching page indexes for value " + i + " with DESCENDING order", BoundaryOrder.UNORDERED::lt, BoundaryOrder.DESCENDING::lt, DESCENDING.createValueComparator(i));    }    for (int i = SINGLE_FROM - 1; i <= SINGLE_TO + 1; ++i) {        ColumnIndexBase<?>.ValueComparator singleComparator = SINGLE.createValueComparator(i);        validateOperator("Mismatching page indexes for value " + i + " with ASCENDING order", BoundaryOrder.UNORDERED::lt, BoundaryOrder.ASCENDING::lt, singleComparator);        validateOperator("Mismatching page indexes for value " + i + " with DESCENDING order", BoundaryOrder.UNORDERED::lt, BoundaryOrder.DESCENDING::lt, singleComparator);    }    ExecStats stats = new ExecStats();    for (int i = RAND_FROM - 1; i <= RAND_TO + 1; ++i) {        stats.add(validateOperator("Mismatching page indexes for value " + i + " with ASCENDING order", BoundaryOrder.UNORDERED::lt, BoundaryOrder.ASCENDING::lt, RAND_ASCENDING.createValueComparator(i)));        stats.add(validateOperator("Mismatching page indexes for value " + i + " with DESCENDING order", BoundaryOrder.UNORDERED::lt, BoundaryOrder.DESCENDING::lt, RAND_DESCENDING.createValueComparator(i)));    }    }
1
public void testLtEq()
{    for (int i = FROM - 1; i <= TO + 1; ++i) {        validateOperator("Mismatching page indexes for value " + i + " with ASCENDING order", BoundaryOrder.UNORDERED::ltEq, BoundaryOrder.ASCENDING::ltEq, ASCENDING.createValueComparator(i));        validateOperator("Mismatching page indexes for value " + i + " with DESCENDING order", BoundaryOrder.UNORDERED::ltEq, BoundaryOrder.DESCENDING::ltEq, DESCENDING.createValueComparator(i));    }    for (int i = SINGLE_FROM - 1; i <= SINGLE_TO + 1; ++i) {        ColumnIndexBase<?>.ValueComparator singleComparator = SINGLE.createValueComparator(i);        validateOperator("Mismatching page indexes for value " + i + " with ASCENDING order", BoundaryOrder.UNORDERED::ltEq, BoundaryOrder.ASCENDING::ltEq, singleComparator);        validateOperator("Mismatching page indexes for value " + i + " with DESCENDING order", BoundaryOrder.UNORDERED::ltEq, BoundaryOrder.DESCENDING::ltEq, singleComparator);    }    ExecStats stats = new ExecStats();    for (int i = RAND_FROM - 1; i <= RAND_TO + 1; ++i) {        stats.add(validateOperator("Mismatching page indexes for value " + i + " with ASCENDING order", BoundaryOrder.UNORDERED::ltEq, BoundaryOrder.ASCENDING::ltEq, RAND_ASCENDING.createValueComparator(i)));        stats.add(validateOperator("Mismatching page indexes for value " + i + " with DESCENDING order", BoundaryOrder.UNORDERED::ltEq, BoundaryOrder.DESCENDING::ltEq, RAND_DESCENDING.createValueComparator(i)));    }    }
1
public void testNotEq()
{    for (int i = -16; i <= 16; ++i) {        validateOperator("Mismatching page indexes for value " + i + " with ASCENDING order", BoundaryOrder.UNORDERED::notEq, BoundaryOrder.ASCENDING::notEq, ASCENDING.createValueComparator(i));        validateOperator("Mismatching page indexes for value " + i + " with DESCENDING order", BoundaryOrder.UNORDERED::notEq, BoundaryOrder.DESCENDING::notEq, DESCENDING.createValueComparator(i));    }    for (int i = FROM - 1; i <= TO + 1; ++i) {        ColumnIndexBase<?>.ValueComparator singleComparator = SINGLE.createValueComparator(i);        validateOperator("Mismatching page indexes for value " + i + " with ASCENDING order", BoundaryOrder.UNORDERED::notEq, BoundaryOrder.ASCENDING::notEq, singleComparator);        validateOperator("Mismatching page indexes for value " + i + " with DESCENDING order", BoundaryOrder.UNORDERED::notEq, BoundaryOrder.DESCENDING::notEq, singleComparator);    }    ExecStats stats = new ExecStats();    for (int i = RAND_FROM - 1; i <= RAND_TO + 1; ++i) {        stats.add(validateOperator("Mismatching page indexes for value " + i + " with ASCENDING order", BoundaryOrder.UNORDERED::notEq, BoundaryOrder.ASCENDING::notEq, RAND_ASCENDING.createValueComparator(i)));        stats.add(validateOperator("Mismatching page indexes for value " + i + " with DESCENDING order", BoundaryOrder.UNORDERED::notEq, BoundaryOrder.DESCENDING::notEq, RAND_DESCENDING.createValueComparator(i)));    }    }
1
public boolean keep(Binary value)
{    return value == null || value.equals(ZERO);}
0
public boolean canDrop(org.apache.parquet.filter2.predicate.Statistics<Binary> statistics)
{    Comparator<Binary> cmp = statistics.getComparator();    return cmp.compare(statistics.getMin(), ZERO) > 0 || cmp.compare(statistics.getMax(), ZERO) < 0;}
0
public boolean inverseCanDrop(org.apache.parquet.filter2.predicate.Statistics<Binary> statistics)
{    Comparator<Binary> cmp = statistics.getComparator();    return cmp.compare(statistics.getMin(), ZERO) == 0 && cmp.compare(statistics.getMax(), ZERO) == 0;}
0
public boolean keep(Binary value)
{    return value != null && value.length() > 0 && value.getBytesUnsafe()[0] == 'B';}
0
public boolean canDrop(org.apache.parquet.filter2.predicate.Statistics<Binary> statistics)
{    Comparator<Binary> cmp = statistics.getComparator();    return cmp.compare(statistics.getMin(), C) >= 0 || cmp.compare(statistics.getMax(), B) < 0;}
0
public boolean inverseCanDrop(org.apache.parquet.filter2.predicate.Statistics<Binary> statistics)
{    Comparator<Binary> cmp = statistics.getComparator();    return cmp.compare(statistics.getMin(), B) >= 0 && cmp.compare(statistics.getMax(), C) < 0;}
0
public boolean keep(Boolean value)
{    return value == null || value;}
0
public boolean canDrop(org.apache.parquet.filter2.predicate.Statistics<Boolean> statistics)
{    return statistics.getComparator().compare(statistics.getMax(), true) != 0;}
0
public boolean inverseCanDrop(org.apache.parquet.filter2.predicate.Statistics<Boolean> statistics)
{    return statistics.getComparator().compare(statistics.getMin(), true) == 0;}
0
public boolean keep(Double value)
{    return value != null && Math.floor(value) == value;}
0
public boolean canDrop(org.apache.parquet.filter2.predicate.Statistics<Double> statistics)
{    double min = statistics.getMin();    double max = statistics.getMax();    Comparator<Double> cmp = statistics.getComparator();    return cmp.compare(Math.floor(min), Math.floor(max)) == 0 && cmp.compare(Math.floor(min), min) != 0 && cmp.compare(Math.floor(max), max) != 0;}
0
public boolean inverseCanDrop(org.apache.parquet.filter2.predicate.Statistics<Double> statistics)
{    double min = statistics.getMin();    double max = statistics.getMax();    Comparator<Double> cmp = statistics.getComparator();    return cmp.compare(min, max) == 0 && cmp.compare(Math.floor(min), min) == 0;}
0
private static float floor(float value)
{    return (float) Math.floor(value);}
0
public boolean keep(Float value)
{    return value != null && Math.floor(value) == value;}
0
public boolean canDrop(org.apache.parquet.filter2.predicate.Statistics<Float> statistics)
{    float min = statistics.getMin();    float max = statistics.getMax();    Comparator<Float> cmp = statistics.getComparator();    return cmp.compare(floor(min), floor(max)) == 0 && cmp.compare(floor(min), min) != 0 && cmp.compare(floor(max), max) != 0;}
0
public boolean inverseCanDrop(org.apache.parquet.filter2.predicate.Statistics<Float> statistics)
{    float min = statistics.getMin();    float max = statistics.getMax();    Comparator<Float> cmp = statistics.getComparator();    return cmp.compare(min, max) == 0 && cmp.compare(floor(min), min) == 0;}
0
public boolean keep(Integer value)
{    return value != null && value % 3 == 0;}
0
public boolean canDrop(org.apache.parquet.filter2.predicate.Statistics<Integer> statistics)
{    int min = statistics.getMin();    int max = statistics.getMax();    return min % 3 != 0 && max % 3 != 0 && max - min < 3;}
0
public boolean inverseCanDrop(org.apache.parquet.filter2.predicate.Statistics<Integer> statistics)
{    int min = statistics.getMin();    int max = statistics.getMax();    return min == max && min % 3 == 0;}
0
public boolean keep(Long value)
{    return value != null && value % 3 == 0;}
0
public boolean canDrop(org.apache.parquet.filter2.predicate.Statistics<Long> statistics)
{    long min = statistics.getMin();    long max = statistics.getMax();    return min % 3 != 0 && max % 3 != 0 && max - min < 3;}
0
public boolean inverseCanDrop(org.apache.parquet.filter2.predicate.Statistics<Long> statistics)
{    long min = statistics.getMin();    long max = statistics.getMax();    return min == max && min % 3 == 0;}
0
public void testBuildBinaryDecimal()
{    PrimitiveType type = Types.required(BINARY).as(DECIMAL).precision(12).scale(2).named("test_binary_decimal");    ColumnIndexBuilder builder = ColumnIndexBuilder.getBuilder(type, Integer.MAX_VALUE);    assertThat(builder, instanceOf(BinaryColumnIndexBuilder.class));    assertNull(builder.build());    BinaryColumn col = binaryColumn("test_col");    StatsBuilder sb = new StatsBuilder();    builder.add(sb.stats(type, null, null));    builder.add(sb.stats(type, decimalBinary("-0.17"), decimalBinary("1234567890.12")));    builder.add(sb.stats(type, decimalBinary("-234.23"), null, null, null));    builder.add(sb.stats(type, null, null, null));    builder.add(sb.stats(type, decimalBinary("-9999293.23"), decimalBinary("2348978.45")));    builder.add(sb.stats(type, null, null, null, null));    builder.add(sb.stats(type, null, null));    builder.add(sb.stats(type, decimalBinary("87656273")));    assertEquals(8, builder.getPageCount());    assertEquals(sb.getMinMaxSize(), builder.getMinMaxSize());    ColumnIndex columnIndex = builder.build();    assertEquals(BoundaryOrder.UNORDERED, columnIndex.getBoundaryOrder());    assertCorrectNullCounts(columnIndex, 2, 0, 3, 3, 0, 4, 2, 0);    assertCorrectNullPages(columnIndex, true, false, false, true, false, true, true, false);    assertCorrectValues(columnIndex.getMaxValues(), null, decimalBinary("1234567890.12"), decimalBinary("-234.23"), null, decimalBinary("2348978.45"), null, null, decimalBinary("87656273"));    assertCorrectValues(columnIndex.getMinValues(), null, decimalBinary("-0.17"), decimalBinary("-234.23"), null, decimalBinary("-9999293.23"), null, null, decimalBinary("87656273"));    assertCorrectFiltering(columnIndex, eq(col, decimalBinary("0.0")), 1, 4);    assertCorrectFiltering(columnIndex, eq(col, null), 0, 2, 3, 5, 6);    assertCorrectFiltering(columnIndex, notEq(col, decimalBinary("87656273")), 0, 1, 2, 3, 4, 5, 6);    assertCorrectFiltering(columnIndex, notEq(col, null), 1, 2, 4, 7);    assertCorrectFiltering(columnIndex, gt(col, decimalBinary("2348978.45")), 1);    assertCorrectFiltering(columnIndex, gtEq(col, decimalBinary("2348978.45")), 1, 4);    assertCorrectFiltering(columnIndex, lt(col, decimalBinary("-234.23")), 4);    assertCorrectFiltering(columnIndex, ltEq(col, decimalBinary("-234.23")), 2, 4);    assertCorrectFiltering(columnIndex, userDefined(col, BinaryDecimalIsNullOrZeroUdp.class), 0, 1, 2, 3, 4, 5, 6);    assertCorrectFiltering(columnIndex, invert(userDefined(col, BinaryDecimalIsNullOrZeroUdp.class)), 1, 2, 4, 7);    builder = ColumnIndexBuilder.getBuilder(type, Integer.MAX_VALUE);    sb = new StatsBuilder();    builder.add(sb.stats(type, null, null, null, null));    builder.add(sb.stats(type, decimalBinary("-9999293.23"), decimalBinary("-234.23")));    builder.add(sb.stats(type, decimalBinary("-0.17"), decimalBinary("87656273")));    builder.add(sb.stats(type, null, null));    builder.add(sb.stats(type, decimalBinary("87656273")));    builder.add(sb.stats(type, null, null));    builder.add(sb.stats(type, decimalBinary("1234567890.12"), null, null, null));    builder.add(sb.stats(type, null, null, null));    assertEquals(8, builder.getPageCount());    assertEquals(sb.getMinMaxSize(), builder.getMinMaxSize());    columnIndex = builder.build();    assertEquals(BoundaryOrder.ASCENDING, columnIndex.getBoundaryOrder());    assertCorrectNullCounts(columnIndex, 4, 0, 0, 2, 0, 2, 3, 3);    assertCorrectNullPages(columnIndex, true, false, false, true, false, true, false, true);    assertCorrectValues(columnIndex.getMaxValues(), null, decimalBinary("-234.23"), decimalBinary("87656273"), null, decimalBinary("87656273"), null, decimalBinary("1234567890.12"), null);    assertCorrectValues(columnIndex.getMinValues(), null, decimalBinary("-9999293.23"), decimalBinary("-0.17"), null, decimalBinary("87656273"), null, decimalBinary("1234567890.12"), null);    assertCorrectFiltering(columnIndex, eq(col, decimalBinary("87656273")), 2, 4);    assertCorrectFiltering(columnIndex, eq(col, null), 0, 3, 5, 6, 7);    assertCorrectFiltering(columnIndex, notEq(col, decimalBinary("87656273")), 0, 1, 2, 3, 5, 6, 7);    assertCorrectFiltering(columnIndex, notEq(col, null), 1, 2, 4, 6);    assertCorrectFiltering(columnIndex, gt(col, decimalBinary("87656273")), 6);    assertCorrectFiltering(columnIndex, gtEq(col, decimalBinary("87656273")), 2, 4, 6);    assertCorrectFiltering(columnIndex, lt(col, decimalBinary("-0.17")), 1);    assertCorrectFiltering(columnIndex, ltEq(col, decimalBinary("-0.17")), 1, 2);    assertCorrectFiltering(columnIndex, userDefined(col, BinaryDecimalIsNullOrZeroUdp.class), 0, 2, 3, 5, 6, 7);    assertCorrectFiltering(columnIndex, invert(userDefined(col, BinaryDecimalIsNullOrZeroUdp.class)), 1, 2, 4, 6);    builder = ColumnIndexBuilder.getBuilder(type, Integer.MAX_VALUE);    sb = new StatsBuilder();    builder.add(sb.stats(type, null, null, null));    builder.add(sb.stats(type, null, null));    builder.add(sb.stats(type, decimalBinary("1234567890.12"), null, null, null));    builder.add(sb.stats(type, null, null, null, null));    builder.add(sb.stats(type, decimalBinary("1234567890.12"), decimalBinary("87656273")));    builder.add(sb.stats(type, decimalBinary("987656273"), decimalBinary("-0.17")));    builder.add(sb.stats(type, null, null));    builder.add(sb.stats(type, decimalBinary("-234.23"), decimalBinary("-9999293.23")));    assertEquals(8, builder.getPageCount());    assertEquals(sb.getMinMaxSize(), builder.getMinMaxSize());    columnIndex = builder.build();    assertEquals(BoundaryOrder.DESCENDING, columnIndex.getBoundaryOrder());    assertCorrectNullCounts(columnIndex, 3, 2, 3, 4, 0, 0, 2, 0);    assertCorrectNullPages(columnIndex, true, true, false, true, false, false, true, false);    assertCorrectValues(columnIndex.getMaxValues(), null, null, decimalBinary("1234567890.12"), null, decimalBinary("1234567890.12"), decimalBinary("987656273"), null, decimalBinary("-234.23"));    assertCorrectValues(columnIndex.getMinValues(), null, null, decimalBinary("1234567890.12"), null, decimalBinary("87656273"), decimalBinary("-0.17"), null, decimalBinary("-9999293.23"));    assertCorrectFiltering(columnIndex, eq(col, decimalBinary("1234567890.12")), 2, 4);    assertCorrectFiltering(columnIndex, eq(col, null), 0, 1, 2, 3, 6);    assertCorrectFiltering(columnIndex, notEq(col, decimalBinary("0.0")), 0, 1, 2, 3, 4, 5, 6, 7);    assertCorrectFiltering(columnIndex, notEq(col, null), 2, 4, 5, 7);    assertCorrectFiltering(columnIndex, gt(col, decimalBinary("1234567890.12")));    assertCorrectFiltering(columnIndex, gtEq(col, decimalBinary("1234567890.12")), 2, 4);    assertCorrectFiltering(columnIndex, lt(col, decimalBinary("-0.17")), 7);    assertCorrectFiltering(columnIndex, ltEq(col, decimalBinary("-0.17")), 5, 7);    assertCorrectFiltering(columnIndex, userDefined(col, BinaryDecimalIsNullOrZeroUdp.class), 0, 1, 2, 3, 5, 6);    assertCorrectFiltering(columnIndex, invert(userDefined(col, BinaryDecimalIsNullOrZeroUdp.class)), 2, 4, 5, 7);}
0
public void testBuildBinaryUtf8()
{    PrimitiveType type = Types.required(BINARY).as(UTF8).named("test_binary_utf8");    ColumnIndexBuilder builder = ColumnIndexBuilder.getBuilder(type, Integer.MAX_VALUE);    assertThat(builder, instanceOf(BinaryColumnIndexBuilder.class));    assertNull(builder.build());    BinaryColumn col = binaryColumn("test_col");    StatsBuilder sb = new StatsBuilder();    builder.add(sb.stats(type, null, null));    builder.add(sb.stats(type, stringBinary("Jeltz"), stringBinary("Slartibartfast"), null, null));    builder.add(sb.stats(type, null, null, null, null, null));    builder.add(sb.stats(type, null, null));    builder.add(sb.stats(type, stringBinary("Beeblebrox"), stringBinary("Prefect")));    builder.add(sb.stats(type, stringBinary("Dent"), stringBinary("Trilian"), null));    builder.add(sb.stats(type, stringBinary("Beeblebrox")));    builder.add(sb.stats(type, null, null));    assertEquals(8, builder.getPageCount());    assertEquals(sb.getMinMaxSize(), builder.getMinMaxSize());    ColumnIndex columnIndex = builder.build();    assertEquals(BoundaryOrder.UNORDERED, columnIndex.getBoundaryOrder());    assertCorrectNullCounts(columnIndex, 2, 2, 5, 2, 0, 1, 0, 2);    assertCorrectNullPages(columnIndex, true, false, true, true, false, false, false, true);    assertCorrectValues(columnIndex.getMaxValues(), null, stringBinary("Slartibartfast"), null, null, stringBinary("Prefect"), stringBinary("Trilian"), stringBinary("Beeblebrox"), null);    assertCorrectValues(columnIndex.getMinValues(), null, stringBinary("Jeltz"), null, null, stringBinary("Beeblebrox"), stringBinary("Dent"), stringBinary("Beeblebrox"), null);    assertCorrectFiltering(columnIndex, eq(col, stringBinary("Marvin")), 1, 4, 5);    assertCorrectFiltering(columnIndex, eq(col, null), 0, 1, 2, 3, 5, 7);    assertCorrectFiltering(columnIndex, notEq(col, stringBinary("Beeblebrox")), 0, 1, 2, 3, 4, 5, 7);    assertCorrectFiltering(columnIndex, notEq(col, null), 1, 4, 5, 6);    assertCorrectFiltering(columnIndex, gt(col, stringBinary("Prefect")), 1, 5);    assertCorrectFiltering(columnIndex, gtEq(col, stringBinary("Prefect")), 1, 4, 5);    assertCorrectFiltering(columnIndex, lt(col, stringBinary("Dent")), 4, 6);    assertCorrectFiltering(columnIndex, ltEq(col, stringBinary("Dent")), 4, 5, 6);    assertCorrectFiltering(columnIndex, userDefined(col, BinaryUtf8StartsWithB.class), 4, 6);    assertCorrectFiltering(columnIndex, invert(userDefined(col, BinaryUtf8StartsWithB.class)), 0, 1, 2, 3, 4, 5, 7);    builder = ColumnIndexBuilder.getBuilder(type, Integer.MAX_VALUE);    sb = new StatsBuilder();    builder.add(sb.stats(type, stringBinary("Beeblebrox"), stringBinary("Dent"), null, null));    builder.add(sb.stats(type, null, null));    builder.add(sb.stats(type, null, null, null, null, null));    builder.add(sb.stats(type, stringBinary("Dent"), stringBinary("Jeltz")));    builder.add(sb.stats(type, stringBinary("Dent"), stringBinary("Prefect"), null));    builder.add(sb.stats(type, null, null));    builder.add(sb.stats(type, stringBinary("Slartibartfast")));    builder.add(sb.stats(type, null, null));    assertEquals(8, builder.getPageCount());    assertEquals(sb.getMinMaxSize(), builder.getMinMaxSize());    columnIndex = builder.build();    assertEquals(BoundaryOrder.ASCENDING, columnIndex.getBoundaryOrder());    assertCorrectNullCounts(columnIndex, 2, 2, 5, 0, 1, 2, 0, 2);    assertCorrectNullPages(columnIndex, false, true, true, false, false, true, false, true);    assertCorrectValues(columnIndex.getMaxValues(), stringBinary("Dent"), null, null, stringBinary("Jeltz"), stringBinary("Prefect"), null, stringBinary("Slartibartfast"), null);    assertCorrectValues(columnIndex.getMinValues(), stringBinary("Beeblebrox"), null, null, stringBinary("Dent"), stringBinary("Dent"), null, stringBinary("Slartibartfast"), null);    assertCorrectFiltering(columnIndex, eq(col, stringBinary("Jeltz")), 3, 4);    assertCorrectFiltering(columnIndex, eq(col, null), 0, 1, 2, 4, 5, 7);    assertCorrectFiltering(columnIndex, notEq(col, stringBinary("Slartibartfast")), 0, 1, 2, 3, 4, 5, 7);    assertCorrectFiltering(columnIndex, notEq(col, null), 0, 3, 4, 6);    assertCorrectFiltering(columnIndex, gt(col, stringBinary("Marvin")), 4, 6);    assertCorrectFiltering(columnIndex, gtEq(col, stringBinary("Marvin")), 4, 6);    assertCorrectFiltering(columnIndex, lt(col, stringBinary("Dent")), 0);    assertCorrectFiltering(columnIndex, ltEq(col, stringBinary("Dent")), 0, 3, 4);    assertCorrectFiltering(columnIndex, userDefined(col, BinaryUtf8StartsWithB.class), 0);    assertCorrectFiltering(columnIndex, invert(userDefined(col, BinaryUtf8StartsWithB.class)), 0, 1, 2, 3, 4, 5, 6, 7);    builder = ColumnIndexBuilder.getBuilder(type, Integer.MAX_VALUE);    sb = new StatsBuilder();    builder.add(sb.stats(type, null, null));    builder.add(sb.stats(type, stringBinary("Slartibartfast")));    builder.add(sb.stats(type, null, null, null, null, null));    builder.add(sb.stats(type, stringBinary("Prefect"), stringBinary("Jeltz"), null));    builder.add(sb.stats(type, stringBinary("Dent"), stringBinary("Dent")));    builder.add(sb.stats(type, null, null));    builder.add(sb.stats(type, null, null));    builder.add(sb.stats(type, stringBinary("Dent"), stringBinary("Beeblebrox"), null, null));    assertEquals(8, builder.getPageCount());    assertEquals(sb.getMinMaxSize(), builder.getMinMaxSize());    columnIndex = builder.build();    assertEquals(BoundaryOrder.DESCENDING, columnIndex.getBoundaryOrder());    assertCorrectNullCounts(columnIndex, 2, 0, 5, 1, 0, 2, 2, 2);    assertCorrectNullPages(columnIndex, true, false, true, false, false, true, true, false);    assertCorrectValues(columnIndex.getMaxValues(), null, stringBinary("Slartibartfast"), null, stringBinary("Prefect"), stringBinary("Dent"), null, null, stringBinary("Dent"));    assertCorrectValues(columnIndex.getMinValues(), null, stringBinary("Slartibartfast"), null, stringBinary("Jeltz"), stringBinary("Dent"), null, null, stringBinary("Beeblebrox"));    assertCorrectFiltering(columnIndex, eq(col, stringBinary("Marvin")), 3);    assertCorrectFiltering(columnIndex, eq(col, null), 0, 2, 3, 5, 6, 7);    assertCorrectFiltering(columnIndex, notEq(col, stringBinary("Dent")), 0, 1, 2, 3, 5, 6, 7);    assertCorrectFiltering(columnIndex, notEq(col, null), 1, 3, 4, 7);    assertCorrectFiltering(columnIndex, gt(col, stringBinary("Prefect")), 1);    assertCorrectFiltering(columnIndex, gtEq(col, stringBinary("Prefect")), 1, 3);    assertCorrectFiltering(columnIndex, lt(col, stringBinary("Marvin")), 3, 4, 7);    assertCorrectFiltering(columnIndex, ltEq(col, stringBinary("Marvin")), 3, 4, 7);    assertCorrectFiltering(columnIndex, userDefined(col, BinaryUtf8StartsWithB.class), 7);    assertCorrectFiltering(columnIndex, invert(userDefined(col, BinaryUtf8StartsWithB.class)), 0, 1, 2, 3, 4, 5, 6, 7);}
0
public void testStaticBuildBinary()
{    ColumnIndex columnIndex = ColumnIndexBuilder.build(Types.required(BINARY).as(UTF8).named("test_binary_utf8"), BoundaryOrder.ASCENDING, asList(true, true, false, false, true, false, true, false), asList(1l, 2l, 3l, 4l, 5l, 6l, 7l, 8l), toBBList(null, null, stringBinary("Beeblebrox"), stringBinary("Dent"), null, stringBinary("Jeltz"), null, stringBinary("Slartibartfast")), toBBList(null, null, stringBinary("Dent"), stringBinary("Dent"), null, stringBinary("Prefect"), null, stringBinary("Slartibartfast")));    assertEquals(BoundaryOrder.ASCENDING, columnIndex.getBoundaryOrder());    assertCorrectNullCounts(columnIndex, 1, 2, 3, 4, 5, 6, 7, 8);    assertCorrectNullPages(columnIndex, true, true, false, false, true, false, true, false);    assertCorrectValues(columnIndex.getMaxValues(), null, null, stringBinary("Dent"), stringBinary("Dent"), null, stringBinary("Prefect"), null, stringBinary("Slartibartfast"));    assertCorrectValues(columnIndex.getMinValues(), null, null, stringBinary("Beeblebrox"), stringBinary("Dent"), null, stringBinary("Jeltz"), null, stringBinary("Slartibartfast"));}
0
public void testFilterWithoutNullCounts()
{    ColumnIndex columnIndex = ColumnIndexBuilder.build(Types.required(BINARY).as(UTF8).named("test_binary_utf8"), BoundaryOrder.ASCENDING, asList(true, true, false, false, true, false, true, false), null, toBBList(null, null, stringBinary("Beeblebrox"), stringBinary("Dent"), null, stringBinary("Jeltz"), null, stringBinary("Slartibartfast")), toBBList(null, null, stringBinary("Dent"), stringBinary("Dent"), null, stringBinary("Prefect"), null, stringBinary("Slartibartfast")));    assertEquals(BoundaryOrder.ASCENDING, columnIndex.getBoundaryOrder());    assertNull(columnIndex.getNullCounts());    assertCorrectNullPages(columnIndex, true, true, false, false, true, false, true, false);    assertCorrectValues(columnIndex.getMaxValues(), null, null, stringBinary("Dent"), stringBinary("Dent"), null, stringBinary("Prefect"), null, stringBinary("Slartibartfast"));    assertCorrectValues(columnIndex.getMinValues(), null, null, stringBinary("Beeblebrox"), stringBinary("Dent"), null, stringBinary("Jeltz"), null, stringBinary("Slartibartfast"));    BinaryColumn col = binaryColumn("test_col");    assertCorrectFiltering(columnIndex, eq(col, stringBinary("Dent")), 2, 3);    assertCorrectFiltering(columnIndex, eq(col, null), 0, 1, 2, 3, 4, 5, 6, 7);    assertCorrectFiltering(columnIndex, notEq(col, stringBinary("Dent")), 0, 1, 2, 3, 4, 5, 6, 7);    assertCorrectFiltering(columnIndex, notEq(col, null), 2, 3, 5, 7);    assertCorrectFiltering(columnIndex, userDefined(col, BinaryDecimalIsNullOrZeroUdp.class), 0, 1, 2, 3, 4, 5, 6, 7);    assertCorrectFiltering(columnIndex, invert(userDefined(col, BinaryDecimalIsNullOrZeroUdp.class)), 2, 3, 5, 7);}
0
public void testBuildBoolean()
{    PrimitiveType type = Types.required(BOOLEAN).named("test_boolean");    ColumnIndexBuilder builder = ColumnIndexBuilder.getBuilder(type, Integer.MAX_VALUE);    assertThat(builder, instanceOf(BooleanColumnIndexBuilder.class));    assertNull(builder.build());    BooleanColumn col = booleanColumn("test_col");    builder = ColumnIndexBuilder.getBuilder(type, Integer.MAX_VALUE);    StatsBuilder sb = new StatsBuilder();    builder.add(sb.stats(type, false, true));    builder.add(sb.stats(type, true, false, null));    builder.add(sb.stats(type, true, true, null, null));    builder.add(sb.stats(type, null, null, null));    builder.add(sb.stats(type, false, false));    assertEquals(5, builder.getPageCount());    assertEquals(sb.getMinMaxSize(), builder.getMinMaxSize());    ColumnIndex columnIndex = builder.build();    assertEquals(BoundaryOrder.UNORDERED, columnIndex.getBoundaryOrder());    assertCorrectNullCounts(columnIndex, 0, 1, 2, 3, 0);    assertCorrectNullPages(columnIndex, false, false, false, true, false);    assertCorrectValues(columnIndex.getMaxValues(), true, true, true, null, false);    assertCorrectValues(columnIndex.getMinValues(), false, false, true, null, false);    assertCorrectFiltering(columnIndex, eq(col, true), 0, 1, 2);    assertCorrectFiltering(columnIndex, eq(col, null), 1, 2, 3);    assertCorrectFiltering(columnIndex, notEq(col, true), 0, 1, 2, 3, 4);    assertCorrectFiltering(columnIndex, notEq(col, null), 0, 1, 2, 4);    assertCorrectFiltering(columnIndex, userDefined(col, BooleanIsTrueOrNull.class), 0, 1, 2, 3);    assertCorrectFiltering(columnIndex, invert(userDefined(col, BooleanIsTrueOrNull.class)), 0, 1, 4);    builder = ColumnIndexBuilder.getBuilder(type, Integer.MAX_VALUE);    sb = new StatsBuilder();    builder.add(sb.stats(type, null, null));    builder.add(sb.stats(type, false, false));    builder.add(sb.stats(type, null, null, null));    builder.add(sb.stats(type, null, null, null, null));    builder.add(sb.stats(type, false, true, null));    builder.add(sb.stats(type, false, true, null, null));    builder.add(sb.stats(type, null, null, null));    assertEquals(7, builder.getPageCount());    assertEquals(sb.getMinMaxSize(), builder.getMinMaxSize());    columnIndex = builder.build();    assertEquals(BoundaryOrder.ASCENDING, columnIndex.getBoundaryOrder());    assertCorrectNullCounts(columnIndex, 2, 0, 3, 4, 1, 2, 3);    assertCorrectNullPages(columnIndex, true, false, true, true, false, false, true);    assertCorrectValues(columnIndex.getMaxValues(), null, false, null, null, true, true, null);    assertCorrectValues(columnIndex.getMinValues(), null, false, null, null, false, false, null);    assertCorrectFiltering(columnIndex, eq(col, true), 4, 5);    assertCorrectFiltering(columnIndex, eq(col, null), 0, 2, 3, 4, 5, 6);    assertCorrectFiltering(columnIndex, notEq(col, true), 0, 1, 2, 3, 4, 5, 6);    assertCorrectFiltering(columnIndex, notEq(col, null), 1, 4, 5);    assertCorrectFiltering(columnIndex, userDefined(col, BooleanIsTrueOrNull.class), 0, 2, 3, 4, 5, 6);    assertCorrectFiltering(columnIndex, invert(userDefined(col, BooleanIsTrueOrNull.class)), 1, 4, 5);    builder = ColumnIndexBuilder.getBuilder(type, Integer.MAX_VALUE);    sb = new StatsBuilder();    builder.add(sb.stats(type, null, null));    builder.add(sb.stats(type, true, true));    builder.add(sb.stats(type, null, null, null));    builder.add(sb.stats(type, null, null, null, null));    builder.add(sb.stats(type, true, false, null));    builder.add(sb.stats(type, false, false, null, null));    builder.add(sb.stats(type, null, null, null));    assertEquals(7, builder.getPageCount());    assertEquals(sb.getMinMaxSize(), builder.getMinMaxSize());    columnIndex = builder.build();    assertEquals(BoundaryOrder.DESCENDING, columnIndex.getBoundaryOrder());    assertCorrectNullCounts(columnIndex, 2, 0, 3, 4, 1, 2, 3);    assertCorrectNullPages(columnIndex, true, false, true, true, false, false, true);    assertCorrectValues(columnIndex.getMaxValues(), null, true, null, null, true, false, null);    assertCorrectValues(columnIndex.getMinValues(), null, true, null, null, false, false, null);    assertCorrectFiltering(columnIndex, eq(col, true), 1, 4);    assertCorrectFiltering(columnIndex, eq(col, null), 0, 2, 3, 4, 5, 6);    assertCorrectFiltering(columnIndex, notEq(col, true), 0, 2, 3, 4, 5, 6);    assertCorrectFiltering(columnIndex, notEq(col, null), 1, 4, 5);    assertCorrectFiltering(columnIndex, userDefined(col, BooleanIsTrueOrNull.class), 0, 1, 2, 3, 4, 5, 6);    assertCorrectFiltering(columnIndex, invert(userDefined(col, BooleanIsTrueOrNull.class)), 4, 5);}
0
public void testStaticBuildBoolean()
{    ColumnIndex columnIndex = ColumnIndexBuilder.build(Types.required(BOOLEAN).named("test_boolean"), BoundaryOrder.DESCENDING, asList(false, true, false, true, false, true), asList(9l, 8l, 7l, 6l, 5l, 0l), toBBList(false, null, false, null, true, null), toBBList(true, null, false, null, true, null));    assertEquals(BoundaryOrder.DESCENDING, columnIndex.getBoundaryOrder());    assertCorrectNullCounts(columnIndex, 9, 8, 7, 6, 5, 0);    assertCorrectNullPages(columnIndex, false, true, false, true, false, true);    assertCorrectValues(columnIndex.getMaxValues(), true, null, false, null, true, null);    assertCorrectValues(columnIndex.getMinValues(), false, null, false, null, true, null);}
0
public void testBuildDouble()
{    PrimitiveType type = Types.required(DOUBLE).named("test_double");    ColumnIndexBuilder builder = ColumnIndexBuilder.getBuilder(type, Integer.MAX_VALUE);    assertThat(builder, instanceOf(DoubleColumnIndexBuilder.class));    assertNull(builder.build());    DoubleColumn col = doubleColumn("test_col");    StatsBuilder sb = new StatsBuilder();    builder.add(sb.stats(type, -4.2, -4.1));    builder.add(sb.stats(type, -11.7, 7.0, null));    builder.add(sb.stats(type, 2.2, 2.2, null, null));    builder.add(sb.stats(type, null, null, null));    builder.add(sb.stats(type, 1.9, 2.32));    builder.add(sb.stats(type, -21.0, 8.1));    assertEquals(6, builder.getPageCount());    assertEquals(sb.getMinMaxSize(), builder.getMinMaxSize());    ColumnIndex columnIndex = builder.build();    assertEquals(BoundaryOrder.UNORDERED, columnIndex.getBoundaryOrder());    assertCorrectNullCounts(columnIndex, 0, 1, 2, 3, 0, 0);    assertCorrectNullPages(columnIndex, false, false, false, true, false, false);    assertCorrectValues(columnIndex.getMaxValues(), -4.1, 7.0, 2.2, null, 2.32, 8.1);    assertCorrectValues(columnIndex.getMinValues(), -4.2, -11.7, 2.2, null, 1.9, -21.0);    assertCorrectFiltering(columnIndex, eq(col, 0.0), 1, 5);    assertCorrectFiltering(columnIndex, eq(col, null), 1, 2, 3);    assertCorrectFiltering(columnIndex, notEq(col, 2.2), 0, 1, 2, 3, 4, 5);    assertCorrectFiltering(columnIndex, notEq(col, null), 0, 1, 2, 4, 5);    assertCorrectFiltering(columnIndex, gt(col, 2.2), 1, 4, 5);    assertCorrectFiltering(columnIndex, gtEq(col, 2.2), 1, 2, 4, 5);    assertCorrectFiltering(columnIndex, lt(col, -4.2), 1, 5);    assertCorrectFiltering(columnIndex, ltEq(col, -4.2), 0, 1, 5);    assertCorrectFiltering(columnIndex, userDefined(col, DoubleIsInteger.class), 1, 4, 5);    assertCorrectFiltering(columnIndex, invert(userDefined(col, DoubleIsInteger.class)), 0, 1, 2, 3, 4, 5);    builder = ColumnIndexBuilder.getBuilder(type, Integer.MAX_VALUE);    sb = new StatsBuilder();    builder.add(sb.stats(type, null, null));    builder.add(sb.stats(type, -532.3, -345.2, null, null));    builder.add(sb.stats(type, -234.7, -234.6, null));    builder.add(sb.stats(type, null, null));    builder.add(sb.stats(type, null, null, null));    builder.add(sb.stats(type, -234.6, 2.99999));    builder.add(sb.stats(type, null, null));    builder.add(sb.stats(type, 3.0, 42.83));    builder.add(sb.stats(type, null, null));    assertEquals(9, builder.getPageCount());    assertEquals(sb.getMinMaxSize(), builder.getMinMaxSize());    columnIndex = builder.build();    assertEquals(BoundaryOrder.ASCENDING, columnIndex.getBoundaryOrder());    assertCorrectNullCounts(columnIndex, 2, 2, 1, 2, 3, 0, 2, 0, 2);    assertCorrectNullPages(columnIndex, true, false, false, true, true, false, true, false, true);    assertCorrectValues(columnIndex.getMaxValues(), null, -345.2, -234.6, null, null, 2.99999, null, 42.83, null);    assertCorrectValues(columnIndex.getMinValues(), null, -532.3, -234.7, null, null, -234.6, null, 3.0, null);    assertCorrectFiltering(columnIndex, eq(col, 0.0), 5);    assertCorrectFiltering(columnIndex, eq(col, null), 0, 1, 2, 3, 4, 6, 8);    assertCorrectFiltering(columnIndex, notEq(col, 0.0), 0, 1, 2, 3, 4, 5, 6, 7, 8);    assertCorrectFiltering(columnIndex, notEq(col, null), 1, 2, 5, 7);    assertCorrectFiltering(columnIndex, gt(col, 2.99999), 7);    assertCorrectFiltering(columnIndex, gtEq(col, 2.99999), 5, 7);    assertCorrectFiltering(columnIndex, lt(col, -234.6), 1, 2);    assertCorrectFiltering(columnIndex, ltEq(col, -234.6), 1, 2, 5);    assertCorrectFiltering(columnIndex, userDefined(col, DoubleIsInteger.class), 1, 5, 7);    assertCorrectFiltering(columnIndex, invert(userDefined(col, DoubleIsInteger.class)), 0, 1, 2, 3, 4, 5, 6, 7, 8);    builder = ColumnIndexBuilder.getBuilder(type, Integer.MAX_VALUE);    sb = new StatsBuilder();    builder.add(sb.stats(type, null, null, null, null, null));    builder.add(sb.stats(type, 532.3, 345.2));    builder.add(sb.stats(type, null, null, null));    builder.add(sb.stats(type, 234.7, 234.6, null));    builder.add(sb.stats(type, null, null));    builder.add(sb.stats(type, 234.69, -2.99999));    builder.add(sb.stats(type, null, null));    builder.add(sb.stats(type, null, null));    builder.add(sb.stats(type, -3.0, -42.83));    assertEquals(9, builder.getPageCount());    assertEquals(sb.getMinMaxSize(), builder.getMinMaxSize());    columnIndex = builder.build();    assertEquals(BoundaryOrder.DESCENDING, columnIndex.getBoundaryOrder());    assertCorrectNullCounts(columnIndex, 5, 0, 3, 1, 2, 0, 2, 2, 0);    assertCorrectNullPages(columnIndex, true, false, true, false, true, false, true, true, false);    assertCorrectValues(columnIndex.getMaxValues(), null, 532.3, null, 234.7, null, 234.69, null, null, -3.0);    assertCorrectValues(columnIndex.getMinValues(), null, 345.2, null, 234.6, null, -2.99999, null, null, -42.83);    assertCorrectFiltering(columnIndex, eq(col, 234.6), 3, 5);    assertCorrectFiltering(columnIndex, eq(col, null), 0, 2, 3, 4, 6, 7);    assertCorrectFiltering(columnIndex, notEq(col, 2.2), 0, 1, 2, 3, 4, 5, 6, 7, 8);    assertCorrectFiltering(columnIndex, notEq(col, null), 1, 3, 5, 8);    assertCorrectFiltering(columnIndex, gt(col, 2.2), 1, 3, 5);    assertCorrectFiltering(columnIndex, gtEq(col, 234.69), 1, 3, 5);    assertCorrectFiltering(columnIndex, lt(col, -2.99999), 8);    assertCorrectFiltering(columnIndex, ltEq(col, -2.99999), 5, 8);    assertCorrectFiltering(columnIndex, userDefined(col, DoubleIsInteger.class), 1, 5, 8);    assertCorrectFiltering(columnIndex, invert(userDefined(col, DoubleIsInteger.class)), 0, 1, 2, 3, 4, 5, 6, 7, 8);}
0
public void testBuildDoubleZeroNaN()
{    PrimitiveType type = Types.required(DOUBLE).named("test_double");    ColumnIndexBuilder builder = ColumnIndexBuilder.getBuilder(type, Integer.MAX_VALUE);    StatsBuilder sb = new StatsBuilder();    builder.add(sb.stats(type, -1.0, -0.0));    builder.add(sb.stats(type, 0.0, 1.0));    builder.add(sb.stats(type, 1.0, 100.0));    ColumnIndex columnIndex = builder.build();    assertCorrectValues(columnIndex.getMinValues(), -1.0, -0.0, 1.0);    assertCorrectValues(columnIndex.getMaxValues(), 0.0, 1.0, 100.0);    builder = ColumnIndexBuilder.getBuilder(type, Integer.MAX_VALUE);    builder.add(sb.stats(type, -1.0, -0.0));    builder.add(sb.stats(type, 0.0, Double.NaN));    builder.add(sb.stats(type, 1.0, 100.0));    assertNull(builder.build());}
0
public void testStaticBuildDouble()
{    ColumnIndex columnIndex = ColumnIndexBuilder.build(Types.required(DOUBLE).named("test_double"), BoundaryOrder.UNORDERED, asList(false, false, false, false, false, false), asList(0l, 1l, 2l, 3l, 4l, 5l), toBBList(-1.0, -2.0, -3.0, -4.0, -5.0, -6.0), toBBList(1.0, 2.0, 3.0, 4.0, 5.0, 6.0));    assertEquals(BoundaryOrder.UNORDERED, columnIndex.getBoundaryOrder());    assertCorrectNullCounts(columnIndex, 0, 1, 2, 3, 4, 5);    assertCorrectNullPages(columnIndex, false, false, false, false, false, false);    assertCorrectValues(columnIndex.getMaxValues(), 1.0, 2.0, 3.0, 4.0, 5.0, 6.0);    assertCorrectValues(columnIndex.getMinValues(), -1.0, -2.0, -3.0, -4.0, -5.0, -6.0);}
0
public void testBuildFloat()
{    PrimitiveType type = Types.required(FLOAT).named("test_float");    ColumnIndexBuilder builder = ColumnIndexBuilder.getBuilder(type, Integer.MAX_VALUE);    assertThat(builder, instanceOf(FloatColumnIndexBuilder.class));    assertNull(builder.build());    FloatColumn col = floatColumn("test_col");    StatsBuilder sb = new StatsBuilder();    builder.add(sb.stats(type, -4.2f, -4.1f));    builder.add(sb.stats(type, -11.7f, 7.0f, null));    builder.add(sb.stats(type, 2.2f, 2.2f, null, null));    builder.add(sb.stats(type, null, null, null));    builder.add(sb.stats(type, 1.9f, 2.32f));    builder.add(sb.stats(type, -21.0f, 8.1f));    assertEquals(6, builder.getPageCount());    assertEquals(sb.getMinMaxSize(), builder.getMinMaxSize());    ColumnIndex columnIndex = builder.build();    assertEquals(BoundaryOrder.UNORDERED, columnIndex.getBoundaryOrder());    assertCorrectNullCounts(columnIndex, 0, 1, 2, 3, 0, 0);    assertCorrectNullPages(columnIndex, false, false, false, true, false, false);    assertCorrectValues(columnIndex.getMaxValues(), -4.1f, 7.0f, 2.2f, null, 2.32f, 8.1f);    assertCorrectValues(columnIndex.getMinValues(), -4.2f, -11.7f, 2.2f, null, 1.9f, -21.0f);    assertCorrectFiltering(columnIndex, eq(col, 0.0f), 1, 5);    assertCorrectFiltering(columnIndex, eq(col, null), 1, 2, 3);    assertCorrectFiltering(columnIndex, notEq(col, 2.2f), 0, 1, 2, 3, 4, 5);    assertCorrectFiltering(columnIndex, notEq(col, null), 0, 1, 2, 4, 5);    assertCorrectFiltering(columnIndex, gt(col, 2.2f), 1, 4, 5);    assertCorrectFiltering(columnIndex, gtEq(col, 2.2f), 1, 2, 4, 5);    assertCorrectFiltering(columnIndex, lt(col, 0.0f), 0, 1, 5);    assertCorrectFiltering(columnIndex, ltEq(col, 1.9f), 0, 1, 4, 5);    assertCorrectFiltering(columnIndex, userDefined(col, FloatIsInteger.class), 1, 4, 5);    assertCorrectFiltering(columnIndex, invert(userDefined(col, FloatIsInteger.class)), 0, 1, 2, 3, 4, 5);    builder = ColumnIndexBuilder.getBuilder(type, Integer.MAX_VALUE);    sb = new StatsBuilder();    builder.add(sb.stats(type, null, null));    builder.add(sb.stats(type, -532.3f, -345.2f, null, null));    builder.add(sb.stats(type, -300.6f, -234.7f, null));    builder.add(sb.stats(type, null, null));    builder.add(sb.stats(type, null, null, null));    builder.add(sb.stats(type, -234.6f, 2.99999f));    builder.add(sb.stats(type, null, null));    builder.add(sb.stats(type, 3.0f, 42.83f));    builder.add(sb.stats(type, null, null));    assertEquals(9, builder.getPageCount());    assertEquals(sb.getMinMaxSize(), builder.getMinMaxSize());    columnIndex = builder.build();    assertEquals(BoundaryOrder.ASCENDING, columnIndex.getBoundaryOrder());    assertCorrectNullCounts(columnIndex, 2, 2, 1, 2, 3, 0, 2, 0, 2);    assertCorrectNullPages(columnIndex, true, false, false, true, true, false, true, false, true);    assertCorrectValues(columnIndex.getMaxValues(), null, -345.2f, -234.7f, null, null, 2.99999f, null, 42.83f, null);    assertCorrectValues(columnIndex.getMinValues(), null, -532.3f, -300.6f, null, null, -234.6f, null, 3.0f, null);    assertCorrectFiltering(columnIndex, eq(col, 0.0f), 5);    assertCorrectFiltering(columnIndex, eq(col, null), 0, 1, 2, 3, 4, 6, 8);    assertCorrectFiltering(columnIndex, notEq(col, 2.2f), 0, 1, 2, 3, 4, 5, 6, 7, 8);    assertCorrectFiltering(columnIndex, notEq(col, null), 1, 2, 5, 7);    assertCorrectFiltering(columnIndex, gt(col, 2.2f), 5, 7);    assertCorrectFiltering(columnIndex, gtEq(col, -234.7f), 2, 5, 7);    assertCorrectFiltering(columnIndex, lt(col, -234.6f), 1, 2);    assertCorrectFiltering(columnIndex, ltEq(col, -234.6f), 1, 2, 5);    assertCorrectFiltering(columnIndex, userDefined(col, FloatIsInteger.class), 1, 2, 5, 7);    assertCorrectFiltering(columnIndex, invert(userDefined(col, FloatIsInteger.class)), 0, 1, 2, 3, 4, 5, 6, 7, 8);    builder = ColumnIndexBuilder.getBuilder(type, Integer.MAX_VALUE);    sb = new StatsBuilder();    builder.add(sb.stats(type, null, null, null, null, null));    builder.add(sb.stats(type, 532.3f, 345.2f));    builder.add(sb.stats(type, null, null, null));    builder.add(sb.stats(type, 234.7f, 234.6f, null));    builder.add(sb.stats(type, null, null));    builder.add(sb.stats(type, 234.6f, -2.99999f));    builder.add(sb.stats(type, null, null));    builder.add(sb.stats(type, null, null));    builder.add(sb.stats(type, -3.0f, -42.83f));    assertEquals(9, builder.getPageCount());    assertEquals(sb.getMinMaxSize(), builder.getMinMaxSize());    columnIndex = builder.build();    assertEquals(BoundaryOrder.DESCENDING, columnIndex.getBoundaryOrder());    assertCorrectNullCounts(columnIndex, 5, 0, 3, 1, 2, 0, 2, 2, 0);    assertCorrectNullPages(columnIndex, true, false, true, false, true, false, true, true, false);    assertCorrectValues(columnIndex.getMaxValues(), null, 532.3f, null, 234.7f, null, 234.6f, null, null, -3.0f);    assertCorrectValues(columnIndex.getMinValues(), null, 345.2f, null, 234.6f, null, -2.99999f, null, null, -42.83f);    assertCorrectFiltering(columnIndex, eq(col, 234.65f), 3);    assertCorrectFiltering(columnIndex, eq(col, null), 0, 2, 3, 4, 6, 7);    assertCorrectFiltering(columnIndex, notEq(col, 2.2f), 0, 1, 2, 3, 4, 5, 6, 7, 8);    assertCorrectFiltering(columnIndex, notEq(col, null), 1, 3, 5, 8);    assertCorrectFiltering(columnIndex, gt(col, 2.2f), 1, 3, 5);    assertCorrectFiltering(columnIndex, gtEq(col, 2.2f), 1, 3, 5);    assertCorrectFiltering(columnIndex, lt(col, 0.0f), 5, 8);    assertCorrectFiltering(columnIndex, ltEq(col, 0.0f), 5, 8);    assertCorrectFiltering(columnIndex, userDefined(col, FloatIsInteger.class), 1, 5, 8);    assertCorrectFiltering(columnIndex, invert(userDefined(col, FloatIsInteger.class)), 0, 1, 2, 3, 4, 5, 6, 7, 8);}
0
public void testBuildFloatZeroNaN()
{    PrimitiveType type = Types.required(FLOAT).named("test_float");    ColumnIndexBuilder builder = ColumnIndexBuilder.getBuilder(type, Integer.MAX_VALUE);    StatsBuilder sb = new StatsBuilder();    builder.add(sb.stats(type, -1.0f, -0.0f));    builder.add(sb.stats(type, 0.0f, 1.0f));    builder.add(sb.stats(type, 1.0f, 100.0f));    ColumnIndex columnIndex = builder.build();    assertCorrectValues(columnIndex.getMinValues(), -1.0f, -0.0f, 1.0f);    assertCorrectValues(columnIndex.getMaxValues(), 0.0f, 1.0f, 100.0f);    builder = ColumnIndexBuilder.getBuilder(type, Integer.MAX_VALUE);    builder.add(sb.stats(type, -1.0f, -0.0f));    builder.add(sb.stats(type, 0.0f, Float.NaN));    builder.add(sb.stats(type, 1.0f, 100.0f));    assertNull(builder.build());}
0
public void testStaticBuildFloat()
{    ColumnIndex columnIndex = ColumnIndexBuilder.build(Types.required(FLOAT).named("test_float"), BoundaryOrder.ASCENDING, asList(true, true, true, false, false, false), asList(9l, 8l, 7l, 6l, 0l, 0l), toBBList(null, null, null, -3.0f, -2.0f, 0.1f), toBBList(null, null, null, -2.0f, 0.0f, 6.0f));    assertEquals(BoundaryOrder.ASCENDING, columnIndex.getBoundaryOrder());    assertCorrectNullCounts(columnIndex, 9, 8, 7, 6, 0, 0);    assertCorrectNullPages(columnIndex, true, true, true, false, false, false);    assertCorrectValues(columnIndex.getMaxValues(), null, null, null, -2.0f, 0.0f, 6.0f);    assertCorrectValues(columnIndex.getMinValues(), null, null, null, -3.0f, -2.0f, 0.1f);}
0
public void testBuildInt32()
{    PrimitiveType type = Types.required(INT32).named("test_int32");    ColumnIndexBuilder builder = ColumnIndexBuilder.getBuilder(type, Integer.MAX_VALUE);    assertThat(builder, instanceOf(IntColumnIndexBuilder.class));    assertNull(builder.build());    IntColumn col = intColumn("test_col");    StatsBuilder sb = new StatsBuilder();    builder.add(sb.stats(type, -4, 10));    builder.add(sb.stats(type, -11, 7, null));    builder.add(sb.stats(type, 2, 2, null, null));    builder.add(sb.stats(type, null, null, null));    builder.add(sb.stats(type, 1, 2));    builder.add(sb.stats(type, -21, 8));    assertEquals(6, builder.getPageCount());    assertEquals(sb.getMinMaxSize(), builder.getMinMaxSize());    ColumnIndex columnIndex = builder.build();    assertEquals(BoundaryOrder.UNORDERED, columnIndex.getBoundaryOrder());    assertCorrectNullCounts(columnIndex, 0, 1, 2, 3, 0, 0);    assertCorrectNullPages(columnIndex, false, false, false, true, false, false);    assertCorrectValues(columnIndex.getMaxValues(), 10, 7, 2, null, 2, 8);    assertCorrectValues(columnIndex.getMinValues(), -4, -11, 2, null, 1, -21);    assertCorrectFiltering(columnIndex, eq(col, 2), 0, 1, 2, 4, 5);    assertCorrectFiltering(columnIndex, eq(col, null), 1, 2, 3);    assertCorrectFiltering(columnIndex, notEq(col, 2), 0, 1, 2, 3, 4, 5);    assertCorrectFiltering(columnIndex, notEq(col, null), 0, 1, 2, 4, 5);    assertCorrectFiltering(columnIndex, gt(col, 2), 0, 1, 5);    assertCorrectFiltering(columnIndex, gtEq(col, 2), 0, 1, 2, 4, 5);    assertCorrectFiltering(columnIndex, lt(col, 2), 0, 1, 4, 5);    assertCorrectFiltering(columnIndex, ltEq(col, 2), 0, 1, 2, 4, 5);    assertCorrectFiltering(columnIndex, userDefined(col, IntegerIsDivisableWith3.class), 0, 1, 5);    assertCorrectFiltering(columnIndex, invert(userDefined(col, IntegerIsDivisableWith3.class)), 0, 1, 2, 3, 4, 5);    builder = ColumnIndexBuilder.getBuilder(type, Integer.MAX_VALUE);    sb = new StatsBuilder();    builder.add(sb.stats(type, null, null));    builder.add(sb.stats(type, -532, -345, null, null));    builder.add(sb.stats(type, -500, -42, null));    builder.add(sb.stats(type, null, null));    builder.add(sb.stats(type, null, null, null));    builder.add(sb.stats(type, -42, 2));    builder.add(sb.stats(type, null, null));    builder.add(sb.stats(type, 3, 42));    builder.add(sb.stats(type, null, null));    assertEquals(9, builder.getPageCount());    assertEquals(sb.getMinMaxSize(), builder.getMinMaxSize());    columnIndex = builder.build();    assertEquals(BoundaryOrder.ASCENDING, columnIndex.getBoundaryOrder());    assertCorrectNullCounts(columnIndex, 2, 2, 1, 2, 3, 0, 2, 0, 2);    assertCorrectNullPages(columnIndex, true, false, false, true, true, false, true, false, true);    assertCorrectValues(columnIndex.getMaxValues(), null, -345, -42, null, null, 2, null, 42, null);    assertCorrectValues(columnIndex.getMinValues(), null, -532, -500, null, null, -42, null, 3, null);    assertCorrectFiltering(columnIndex, eq(col, 2), 5);    assertCorrectFiltering(columnIndex, eq(col, null), 0, 1, 2, 3, 4, 6, 8);    assertCorrectFiltering(columnIndex, notEq(col, 2), 0, 1, 2, 3, 4, 5, 6, 7, 8);    assertCorrectFiltering(columnIndex, notEq(col, null), 1, 2, 5, 7);    assertCorrectFiltering(columnIndex, gt(col, 2), 7);    assertCorrectFiltering(columnIndex, gtEq(col, 2), 5, 7);    assertCorrectFiltering(columnIndex, lt(col, 2), 1, 2, 5);    assertCorrectFiltering(columnIndex, ltEq(col, 2), 1, 2, 5);    assertCorrectFiltering(columnIndex, userDefined(col, IntegerIsDivisableWith3.class), 1, 2, 5, 7);    assertCorrectFiltering(columnIndex, invert(userDefined(col, IntegerIsDivisableWith3.class)), 0, 1, 2, 3, 4, 5, 6, 7, 8);    builder = ColumnIndexBuilder.getBuilder(type, Integer.MAX_VALUE);    sb = new StatsBuilder();    builder.add(sb.stats(type, null, null, null, null, null));    builder.add(sb.stats(type, 532, 345));    builder.add(sb.stats(type, null, null, null));    builder.add(sb.stats(type, 234, 42, null));    builder.add(sb.stats(type, null, null));    builder.add(sb.stats(type, 42, -2));    builder.add(sb.stats(type, null, null));    builder.add(sb.stats(type, null, null));    builder.add(sb.stats(type, -3, -42));    assertEquals(9, builder.getPageCount());    assertEquals(sb.getMinMaxSize(), builder.getMinMaxSize());    columnIndex = builder.build();    assertEquals(BoundaryOrder.DESCENDING, columnIndex.getBoundaryOrder());    assertCorrectNullCounts(columnIndex, 5, 0, 3, 1, 2, 0, 2, 2, 0);    assertCorrectNullPages(columnIndex, true, false, true, false, true, false, true, true, false);    assertCorrectValues(columnIndex.getMaxValues(), null, 532, null, 234, null, 42, null, null, -3);    assertCorrectValues(columnIndex.getMinValues(), null, 345, null, 42, null, -2, null, null, -42);    assertCorrectFiltering(columnIndex, eq(col, 2), 5);    assertCorrectFiltering(columnIndex, eq(col, null), 0, 2, 3, 4, 6, 7);    assertCorrectFiltering(columnIndex, notEq(col, 2), 0, 1, 2, 3, 4, 5, 6, 7, 8);    assertCorrectFiltering(columnIndex, notEq(col, null), 1, 3, 5, 8);    assertCorrectFiltering(columnIndex, gt(col, 2), 1, 3, 5);    assertCorrectFiltering(columnIndex, gtEq(col, 2), 1, 3, 5);    assertCorrectFiltering(columnIndex, lt(col, 2), 5, 8);    assertCorrectFiltering(columnIndex, ltEq(col, 2), 5, 8);    assertCorrectFiltering(columnIndex, userDefined(col, IntegerIsDivisableWith3.class), 1, 3, 5, 8);    assertCorrectFiltering(columnIndex, invert(userDefined(col, IntegerIsDivisableWith3.class)), 0, 1, 2, 3, 4, 5, 6, 7, 8);}
0
public void testStaticBuildInt32()
{    ColumnIndex columnIndex = ColumnIndexBuilder.build(Types.required(INT32).named("test_int32"), BoundaryOrder.DESCENDING, asList(false, false, false, true, true, true), asList(0l, 10l, 0l, 3l, 5l, 7l), toBBList(10, 8, 6, null, null, null), toBBList(9, 7, 5, null, null, null));    assertEquals(BoundaryOrder.DESCENDING, columnIndex.getBoundaryOrder());    assertCorrectNullCounts(columnIndex, 0, 10, 0, 3, 5, 7);    assertCorrectNullPages(columnIndex, false, false, false, true, true, true);    assertCorrectValues(columnIndex.getMaxValues(), 9, 7, 5, null, null, null);    assertCorrectValues(columnIndex.getMinValues(), 10, 8, 6, null, null, null);}
0
public void testBuildUInt8()
{    PrimitiveType type = Types.required(INT32).as(UINT_8).named("test_uint8");    ColumnIndexBuilder builder = ColumnIndexBuilder.getBuilder(type, Integer.MAX_VALUE);    assertThat(builder, instanceOf(IntColumnIndexBuilder.class));    assertNull(builder.build());    IntColumn col = intColumn("test_col");    StatsBuilder sb = new StatsBuilder();    builder.add(sb.stats(type, 4, 10));    builder.add(sb.stats(type, 11, 17, null));    builder.add(sb.stats(type, 2, 2, null, null));    builder.add(sb.stats(type, null, null, null));    builder.add(sb.stats(type, 1, 0xFF));    builder.add(sb.stats(type, 0xEF, 0xFA));    assertEquals(6, builder.getPageCount());    assertEquals(sb.getMinMaxSize(), builder.getMinMaxSize());    ColumnIndex columnIndex = builder.build();    assertEquals(BoundaryOrder.UNORDERED, columnIndex.getBoundaryOrder());    assertCorrectNullCounts(columnIndex, 0, 1, 2, 3, 0, 0);    assertCorrectNullPages(columnIndex, false, false, false, true, false, false);    assertCorrectValues(columnIndex.getMaxValues(), 10, 17, 2, null, 0xFF, 0xFA);    assertCorrectValues(columnIndex.getMinValues(), 4, 11, 2, null, 1, 0xEF);    assertCorrectFiltering(columnIndex, eq(col, 2), 2, 4);    assertCorrectFiltering(columnIndex, eq(col, null), 1, 2, 3);    assertCorrectFiltering(columnIndex, notEq(col, 2), 0, 1, 2, 3, 4, 5);    assertCorrectFiltering(columnIndex, notEq(col, null), 0, 1, 2, 4, 5);    assertCorrectFiltering(columnIndex, gt(col, 2), 0, 1, 4, 5);    assertCorrectFiltering(columnIndex, gtEq(col, 2), 0, 1, 2, 4, 5);    assertCorrectFiltering(columnIndex, lt(col, 0xEF), 0, 1, 2, 4);    assertCorrectFiltering(columnIndex, ltEq(col, 0xEF), 0, 1, 2, 4, 5);    assertCorrectFiltering(columnIndex, userDefined(col, IntegerIsDivisableWith3.class), 0, 1, 4, 5);    assertCorrectFiltering(columnIndex, invert(userDefined(col, IntegerIsDivisableWith3.class)), 0, 1, 2, 3, 4, 5);    builder = ColumnIndexBuilder.getBuilder(type, Integer.MAX_VALUE);    sb = new StatsBuilder();    builder.add(sb.stats(type, null, null));    builder.add(sb.stats(type, 0, 0, null, null));    builder.add(sb.stats(type, 0, 42, null));    builder.add(sb.stats(type, null, null));    builder.add(sb.stats(type, null, null, null));    builder.add(sb.stats(type, 42, 0xEE));    builder.add(sb.stats(type, null, null));    builder.add(sb.stats(type, 0xEF, 0xFF));    builder.add(sb.stats(type, null, null));    assertEquals(9, builder.getPageCount());    assertEquals(sb.getMinMaxSize(), builder.getMinMaxSize());    columnIndex = builder.build();    assertEquals(BoundaryOrder.ASCENDING, columnIndex.getBoundaryOrder());    assertCorrectNullCounts(columnIndex, 2, 2, 1, 2, 3, 0, 2, 0, 2);    assertCorrectNullPages(columnIndex, true, false, false, true, true, false, true, false, true);    assertCorrectValues(columnIndex.getMaxValues(), null, 0, 42, null, null, 0xEE, null, 0xFF, null);    assertCorrectValues(columnIndex.getMinValues(), null, 0, 0, null, null, 42, null, 0xEF, null);    assertCorrectFiltering(columnIndex, eq(col, 2), 2);    assertCorrectFiltering(columnIndex, eq(col, null), 0, 1, 2, 3, 4, 6, 8);    assertCorrectFiltering(columnIndex, notEq(col, 2), 0, 1, 2, 3, 4, 5, 6, 7, 8);    assertCorrectFiltering(columnIndex, notEq(col, null), 1, 2, 5, 7);    assertCorrectFiltering(columnIndex, gt(col, 0xEE), 7);    assertCorrectFiltering(columnIndex, gtEq(col, 0xEE), 5, 7);    assertCorrectFiltering(columnIndex, lt(col, 42), 1, 2);    assertCorrectFiltering(columnIndex, ltEq(col, 42), 1, 2, 5);    assertCorrectFiltering(columnIndex, userDefined(col, IntegerIsDivisableWith3.class), 1, 2, 5, 7);    assertCorrectFiltering(columnIndex, invert(userDefined(col, IntegerIsDivisableWith3.class)), 0, 1, 2, 3, 4, 5, 6, 7, 8);    builder = ColumnIndexBuilder.getBuilder(type, Integer.MAX_VALUE);    sb = new StatsBuilder();    builder.add(sb.stats(type, null, null, null, null, null));    builder.add(sb.stats(type, 0xFF, 0xFF));    builder.add(sb.stats(type, null, null, null));    builder.add(sb.stats(type, 0xEF, 0xEA, null));    builder.add(sb.stats(type, null, null));    builder.add(sb.stats(type, 0xEE, 42));    builder.add(sb.stats(type, null, null));    builder.add(sb.stats(type, null, null));    builder.add(sb.stats(type, 41, 0));    assertEquals(9, builder.getPageCount());    assertEquals(sb.getMinMaxSize(), builder.getMinMaxSize());    columnIndex = builder.build();    assertEquals(BoundaryOrder.DESCENDING, columnIndex.getBoundaryOrder());    assertCorrectNullCounts(columnIndex, 5, 0, 3, 1, 2, 0, 2, 2, 0);    assertCorrectNullPages(columnIndex, true, false, true, false, true, false, true, true, false);    assertCorrectValues(columnIndex.getMaxValues(), null, 0xFF, null, 0xEF, null, 0xEE, null, null, 41);    assertCorrectValues(columnIndex.getMinValues(), null, 0xFF, null, 0xEA, null, 42, null, null, 0);    assertCorrectFiltering(columnIndex, eq(col, 0xAB), 5);    assertCorrectFiltering(columnIndex, eq(col, null), 0, 2, 3, 4, 6, 7);    assertCorrectFiltering(columnIndex, notEq(col, 0xFF), 0, 2, 3, 4, 5, 6, 7, 8);    assertCorrectFiltering(columnIndex, notEq(col, null), 1, 3, 5, 8);    assertCorrectFiltering(columnIndex, gt(col, 0xFF));    assertCorrectFiltering(columnIndex, gtEq(col, 0xFF), 1);    assertCorrectFiltering(columnIndex, lt(col, 42), 8);    assertCorrectFiltering(columnIndex, ltEq(col, 42), 5, 8);    assertCorrectFiltering(columnIndex, userDefined(col, IntegerIsDivisableWith3.class), 1, 3, 5, 8);    assertCorrectFiltering(columnIndex, invert(userDefined(col, IntegerIsDivisableWith3.class)), 0, 2, 3, 4, 5, 6, 7, 8);}
0
public void testBuildInt64()
{    PrimitiveType type = Types.required(INT64).named("test_int64");    ColumnIndexBuilder builder = ColumnIndexBuilder.getBuilder(type, Integer.MAX_VALUE);    assertThat(builder, instanceOf(LongColumnIndexBuilder.class));    assertNull(builder.build());    LongColumn col = longColumn("test_col");    StatsBuilder sb = new StatsBuilder();    builder.add(sb.stats(type, -4l, 10l));    builder.add(sb.stats(type, -11l, 7l, null));    builder.add(sb.stats(type, 2l, 2l, null, null));    builder.add(sb.stats(type, null, null, null));    builder.add(sb.stats(type, 1l, 2l));    builder.add(sb.stats(type, -21l, 8l));    assertEquals(6, builder.getPageCount());    assertEquals(sb.getMinMaxSize(), builder.getMinMaxSize());    ColumnIndex columnIndex = builder.build();    assertEquals(BoundaryOrder.UNORDERED, columnIndex.getBoundaryOrder());    assertCorrectNullCounts(columnIndex, 0l, 1l, 2l, 3l, 0l, 0l);    assertCorrectNullPages(columnIndex, false, false, false, true, false, false);    assertCorrectValues(columnIndex.getMaxValues(), 10l, 7l, 2l, null, 2l, 8l);    assertCorrectValues(columnIndex.getMinValues(), -4l, -11l, 2l, null, 1l, -21l);    assertCorrectFiltering(columnIndex, eq(col, 0l), 0, 1, 5);    assertCorrectFiltering(columnIndex, eq(col, null), 1, 2, 3);    assertCorrectFiltering(columnIndex, notEq(col, 0l), 0, 1, 2, 3, 4, 5);    assertCorrectFiltering(columnIndex, notEq(col, null), 0, 1, 2, 4, 5);    assertCorrectFiltering(columnIndex, gt(col, 2l), 0, 1, 5);    assertCorrectFiltering(columnIndex, gtEq(col, 2l), 0, 1, 2, 4, 5);    assertCorrectFiltering(columnIndex, lt(col, -21l));    assertCorrectFiltering(columnIndex, ltEq(col, -21l), 5);    assertCorrectFiltering(columnIndex, userDefined(col, LongIsDivisableWith3.class), 0, 1, 5);    assertCorrectFiltering(columnIndex, invert(userDefined(col, LongIsDivisableWith3.class)), 0, 1, 2, 3, 4, 5);    builder = ColumnIndexBuilder.getBuilder(type, Integer.MAX_VALUE);    sb = new StatsBuilder();    builder.add(sb.stats(type, null, null));    builder.add(sb.stats(type, -532l, -345l, null, null));    builder.add(sb.stats(type, -234l, -42l, null));    builder.add(sb.stats(type, null, null));    builder.add(sb.stats(type, null, null, null));    builder.add(sb.stats(type, -42l, 2l));    builder.add(sb.stats(type, null, null));    builder.add(sb.stats(type, -3l, 42l));    builder.add(sb.stats(type, null, null));    assertEquals(9, builder.getPageCount());    assertEquals(sb.getMinMaxSize(), builder.getMinMaxSize());    columnIndex = builder.build();    assertEquals(BoundaryOrder.ASCENDING, columnIndex.getBoundaryOrder());    assertCorrectNullCounts(columnIndex, 2, 2, 1, 2, 3, 0, 2, 0, 2);    assertCorrectNullPages(columnIndex, true, false, false, true, true, false, true, false, true);    assertCorrectValues(columnIndex.getMaxValues(), null, -345l, -42l, null, null, 2l, null, 42l, null);    assertCorrectValues(columnIndex.getMinValues(), null, -532l, -234l, null, null, -42l, null, -3l, null);    assertCorrectFiltering(columnIndex, eq(col, -42l), 2, 5);    assertCorrectFiltering(columnIndex, eq(col, null), 0, 1, 2, 3, 4, 6, 8);    assertCorrectFiltering(columnIndex, notEq(col, -42l), 0, 1, 2, 3, 4, 5, 6, 7, 8);    assertCorrectFiltering(columnIndex, notEq(col, null), 1, 2, 5, 7);    assertCorrectFiltering(columnIndex, gt(col, 2l), 7);    assertCorrectFiltering(columnIndex, gtEq(col, 2l), 5, 7);    assertCorrectFiltering(columnIndex, lt(col, -42l), 1, 2);    assertCorrectFiltering(columnIndex, ltEq(col, -42l), 1, 2, 5);    assertCorrectFiltering(columnIndex, userDefined(col, LongIsDivisableWith3.class), 1, 2, 5, 7);    assertCorrectFiltering(columnIndex, invert(userDefined(col, LongIsDivisableWith3.class)), 0, 1, 2, 3, 4, 5, 6, 7, 8);    builder = ColumnIndexBuilder.getBuilder(type, Integer.MAX_VALUE);    sb = new StatsBuilder();    builder.add(sb.stats(type, null, null, null, null, null));    builder.add(sb.stats(type, 532l, 345l));    builder.add(sb.stats(type, null, null, null));    builder.add(sb.stats(type, 234l, 42l, null));    builder.add(sb.stats(type, null, null));    builder.add(sb.stats(type, 42l, -2l));    builder.add(sb.stats(type, null, null));    builder.add(sb.stats(type, null, null));    builder.add(sb.stats(type, -3l, -42l));    assertEquals(9, builder.getPageCount());    assertEquals(sb.getMinMaxSize(), builder.getMinMaxSize());    columnIndex = builder.build();    assertEquals(BoundaryOrder.DESCENDING, columnIndex.getBoundaryOrder());    assertCorrectNullCounts(columnIndex, 5, 0, 3, 1, 2, 0, 2, 2, 0);    assertCorrectNullPages(columnIndex, true, false, true, false, true, false, true, true, false);    assertCorrectValues(columnIndex.getMaxValues(), null, 532l, null, 234l, null, 42l, null, null, -3l);    assertCorrectValues(columnIndex.getMinValues(), null, 345l, null, 42l, null, -2l, null, null, -42l);    assertCorrectFiltering(columnIndex, eq(col, 0l), 5);    assertCorrectFiltering(columnIndex, eq(col, null), 0, 2, 3, 4, 6, 7);    assertCorrectFiltering(columnIndex, notEq(col, 0l), 0, 1, 2, 3, 4, 5, 6, 7, 8);    assertCorrectFiltering(columnIndex, notEq(col, null), 1, 3, 5, 8);    assertCorrectFiltering(columnIndex, gt(col, 2l), 1, 3, 5);    assertCorrectFiltering(columnIndex, gtEq(col, 2l), 1, 3, 5);    assertCorrectFiltering(columnIndex, lt(col, -42l));    assertCorrectFiltering(columnIndex, ltEq(col, -42l), 8);    assertCorrectFiltering(columnIndex, userDefined(col, LongIsDivisableWith3.class), 1, 3, 5, 8);    assertCorrectFiltering(columnIndex, invert(userDefined(col, LongIsDivisableWith3.class)), 0, 1, 2, 3, 4, 5, 6, 7, 8);}
0
public void testStaticBuildInt64()
{    ColumnIndex columnIndex = ColumnIndexBuilder.build(Types.required(INT64).named("test_int64"), BoundaryOrder.UNORDERED, asList(true, false, true, false, true, false), asList(1l, 2l, 3l, 4l, 5l, 6l), toBBList(null, 2l, null, 4l, null, 9l), toBBList(null, 3l, null, 15l, null, 10l));    assertEquals(BoundaryOrder.UNORDERED, columnIndex.getBoundaryOrder());    assertCorrectNullCounts(columnIndex, 1, 2, 3, 4, 5, 6);    assertCorrectNullPages(columnIndex, true, false, true, false, true, false);    assertCorrectValues(columnIndex.getMaxValues(), null, 3l, null, 15l, null, 10l);    assertCorrectValues(columnIndex.getMinValues(), null, 2l, null, 4l, null, 9l);}
0
public void testNoOpBuilder()
{    ColumnIndexBuilder builder = ColumnIndexBuilder.getNoOpBuilder();    StatsBuilder sb = new StatsBuilder();    builder.add(sb.stats(Types.required(BINARY).as(UTF8).named("test_binary_utf8"), stringBinary("Jeltz"), stringBinary("Slartibartfast"), null, null));    builder.add(sb.stats(Types.required(BOOLEAN).named("test_boolean"), true, true, null, null));    builder.add(sb.stats(Types.required(DOUBLE).named("test_double"), null, null, null));    builder.add(sb.stats(Types.required(INT32).named("test_int32"), null, null));    builder.add(sb.stats(Types.required(INT64).named("test_int64"), -234l, -42l, null));    assertEquals(0, builder.getPageCount());    assertEquals(0, builder.getMinMaxSize());    assertNull(builder.build());}
0
private static List<ByteBuffer> toBBList(Binary... values)
{    List<ByteBuffer> buffers = new ArrayList<>(values.length);    for (Binary value : values) {        if (value == null) {            buffers.add(ByteBuffer.allocate(0));        } else {            buffers.add(value.toByteBuffer());        }    }    return buffers;}
0
private static List<ByteBuffer> toBBList(Boolean... values)
{    List<ByteBuffer> buffers = new ArrayList<>(values.length);    for (Boolean value : values) {        if (value == null) {            buffers.add(ByteBuffer.allocate(0));        } else {            buffers.add(ByteBuffer.wrap(BytesUtils.booleanToBytes(value)));        }    }    return buffers;}
0
private static List<ByteBuffer> toBBList(Double... values)
{    List<ByteBuffer> buffers = new ArrayList<>(values.length);    for (Double value : values) {        if (value == null) {            buffers.add(ByteBuffer.allocate(0));        } else {            buffers.add(ByteBuffer.wrap(BytesUtils.longToBytes(Double.doubleToLongBits(value))));        }    }    return buffers;}
0
private static List<ByteBuffer> toBBList(Float... values)
{    List<ByteBuffer> buffers = new ArrayList<>(values.length);    for (Float value : values) {        if (value == null) {            buffers.add(ByteBuffer.allocate(0));        } else {            buffers.add(ByteBuffer.wrap(BytesUtils.intToBytes(Float.floatToIntBits(value))));        }    }    return buffers;}
0
private static List<ByteBuffer> toBBList(Integer... values)
{    List<ByteBuffer> buffers = new ArrayList<>(values.length);    for (Integer value : values) {        if (value == null) {            buffers.add(ByteBuffer.allocate(0));        } else {            buffers.add(ByteBuffer.wrap(BytesUtils.intToBytes(value)));        }    }    return buffers;}
0
private static List<ByteBuffer> toBBList(Long... values)
{    List<ByteBuffer> buffers = new ArrayList<>(values.length);    for (Long value : values) {        if (value == null) {            buffers.add(ByteBuffer.allocate(0));        } else {            buffers.add(ByteBuffer.wrap(BytesUtils.longToBytes(value)));        }    }    return buffers;}
0
private static Binary decimalBinary(String num)
{    return Binary.fromConstantByteArray(new BigDecimal(num).unscaledValue().toByteArray());}
0
private static Binary stringBinary(String str)
{    return Binary.fromString(str);}
0
private static void assertCorrectValues(List<ByteBuffer> values, Binary... expectedValues)
{    assertEquals(expectedValues.length, values.size());    for (int i = 0; i < expectedValues.length; ++i) {        Binary expectedValue = expectedValues[i];        ByteBuffer value = values.get(i);        if (expectedValue == null) {            assertFalse("The byte buffer should be empty for null pages", value.hasRemaining());        } else {            assertArrayEquals("Invalid value for page " + i, expectedValue.getBytesUnsafe(), value.array());        }    }}
0
private static void assertCorrectValues(List<ByteBuffer> values, Boolean... expectedValues)
{    assertEquals(expectedValues.length, values.size());    for (int i = 0; i < expectedValues.length; ++i) {        Boolean expectedValue = expectedValues[i];        ByteBuffer value = values.get(i);        if (expectedValue == null) {            assertFalse("The byte buffer should be empty for null pages", value.hasRemaining());        } else {            assertEquals("The byte buffer should be 1 byte long for boolean", 1, value.remaining());            assertEquals("Invalid value for page " + i, expectedValue.booleanValue(), value.get(0) != 0);        }    }}
0
private static void assertCorrectValues(List<ByteBuffer> values, Double... expectedValues)
{    assertEquals(expectedValues.length, values.size());    for (int i = 0; i < expectedValues.length; ++i) {        Double expectedValue = expectedValues[i];        ByteBuffer value = values.get(i);        if (expectedValue == null) {            assertFalse("The byte buffer should be empty for null pages", value.hasRemaining());        } else {            assertEquals("The byte buffer should be 8 bytes long for double", 8, value.remaining());            assertTrue("Invalid value for page " + i, Double.compare(expectedValue.doubleValue(), value.getDouble(0)) == 0);        }    }}
0
private static void assertCorrectValues(List<ByteBuffer> values, Float... expectedValues)
{    assertEquals(expectedValues.length, values.size());    for (int i = 0; i < expectedValues.length; ++i) {        Float expectedValue = expectedValues[i];        ByteBuffer value = values.get(i);        if (expectedValue == null) {            assertFalse("The byte buffer should be empty for null pages", value.hasRemaining());        } else {            assertEquals("The byte buffer should be 4 bytes long for double", 4, value.remaining());            assertTrue("Invalid value for page " + i, Float.compare(expectedValue.floatValue(), value.getFloat(0)) == 0);        }    }}
0
private static void assertCorrectValues(List<ByteBuffer> values, Integer... expectedValues)
{    assertEquals(expectedValues.length, values.size());    for (int i = 0; i < expectedValues.length; ++i) {        Integer expectedValue = expectedValues[i];        ByteBuffer value = values.get(i);        if (expectedValue == null) {            assertFalse("The byte buffer should be empty for null pages", value.hasRemaining());        } else {            assertEquals("The byte buffer should be 4 bytes long for int32", 4, value.remaining());            assertEquals("Invalid value for page " + i, expectedValue.intValue(), value.getInt(0));        }    }}
0
private static void assertCorrectValues(List<ByteBuffer> values, Long... expectedValues)
{    assertEquals(expectedValues.length, values.size());    for (int i = 0; i < expectedValues.length; ++i) {        Long expectedValue = expectedValues[i];        ByteBuffer value = values.get(i);        if (expectedValue == null) {            assertFalse("The byte buffer should be empty for null pages", value.hasRemaining());        } else {            assertEquals("The byte buffer should be 8 bytes long for int64", 8, value.remaining());            assertEquals("Invalid value for page " + i, expectedValue.intValue(), value.getLong(0));        }    }}
0
private static void assertCorrectNullCounts(ColumnIndex columnIndex, long... expectedNullCounts)
{    List<Long> nullCounts = columnIndex.getNullCounts();    assertEquals(expectedNullCounts.length, nullCounts.size());    for (int i = 0; i < expectedNullCounts.length; ++i) {        assertEquals("Invalid null count at page " + i, expectedNullCounts[i], nullCounts.get(i).longValue());    }}
0
private static void assertCorrectNullPages(ColumnIndex columnIndex, boolean... expectedNullPages)
{    List<Boolean> nullPages = columnIndex.getNullPages();    assertEquals(expectedNullPages.length, nullPages.size());    for (int i = 0; i < expectedNullPages.length; ++i) {        assertEquals("Invalid null pages at page " + i, expectedNullPages[i], nullPages.get(i).booleanValue());    }}
0
 Statistics<?> stats(PrimitiveType type, Object... values)
{    Statistics<?> stats = Statistics.createStats(type);    for (Object value : values) {        if (value == null) {            stats.incrementNumNulls();            continue;        }        switch(type.getPrimitiveTypeName()) {            case BINARY:            case FIXED_LEN_BYTE_ARRAY:            case INT96:                stats.updateStats((Binary) value);                break;            case BOOLEAN:                stats.updateStats((boolean) value);                break;            case DOUBLE:                stats.updateStats((double) value);                break;            case FLOAT:                stats.updateStats((float) value);                break;            case INT32:                stats.updateStats((int) value);                break;            case INT64:                stats.updateStats((long) value);                break;            default:                fail("Unsupported value type for stats: " + value.getClass());        }    }    if (stats.hasNonNullValue()) {        minMaxSize += stats.getMinBytes().length;        minMaxSize += stats.getMaxBytes().length;    }    return stats;}
0
 long getMinMaxSize()
{    return minMaxSize;}
0
private static void assertCorrectFiltering(ColumnIndex ci, FilterPredicate predicate, int... expectedIndexes)
{    TestIndexIterator.assertEquals(predicate.accept(ci), expectedIndexes);}
0
public void testAll()
{    assertEquals(IndexIterator.all(10), 0, 1, 2, 3, 4, 5, 6, 7, 8, 9);}
0
public void testFilter()
{    assertEquals(IndexIterator.filter(30, value -> value % 3 == 0), 0, 3, 6, 9, 12, 15, 18, 21, 24, 27);}
0
public void testFilterTranslate()
{    assertEquals(IndexIterator.filterTranslate(20, value -> value < 5, Math::negateExact), 0, -1, -2, -3, -4);}
0
public void testRangeTranslate()
{    assertEquals(IndexIterator.rangeTranslate(11, 18, i -> i - 10), 1, 2, 3, 4, 5, 6, 7, 8);}
0
 static void assertEquals(PrimitiveIterator.OfInt actualIt, int... expectedValues)
{    IntList actualList = new IntArrayList();    actualIt.forEachRemaining((int value) -> actualList.add(value));    int[] actualValues = actualList.toIntArray();    assertArrayEquals("ExpectedValues: " + Arrays.toString(expectedValues) + " ActualValues: " + Arrays.toString(actualValues), expectedValues, actualValues);}
0
public void testBuilderWithSizeAndRowCount()
{    OffsetIndexBuilder builder = OffsetIndexBuilder.getBuilder();    assertNull(builder.build());    assertNull(builder.build(1234));    builder.add(1000, 10);    builder.add(2000, 19);    builder.add(3000, 27);    builder.add(1200, 9);    assertCorrectValues(builder.build(), 0, 1000, 0, 1000, 2000, 10, 3000, 3000, 29, 6000, 1200, 56);    assertCorrectValues(builder.build(10000), 10000, 1000, 0, 11000, 2000, 10, 13000, 3000, 29, 16000, 1200, 56);}
0
public void testNoOpBuilderWithSizeAndRowCount()
{    OffsetIndexBuilder builder = OffsetIndexBuilder.getNoOpBuilder();    builder.add(1, 2);    builder.add(3, 4);    builder.add(5, 6);    builder.add(7, 8);    assertNull(builder.build());    assertNull(builder.build(1000));}
0
public void testBuilderWithOffsetSizeIndex()
{    OffsetIndexBuilder builder = OffsetIndexBuilder.getBuilder();    assertNull(builder.build());    assertNull(builder.build(1234));    builder.add(1000, 10000, 0);    builder.add(22000, 12000, 100);    builder.add(48000, 22000, 211);    builder.add(90000, 30000, 361);    assertCorrectValues(builder.build(), 1000, 10000, 0, 22000, 12000, 100, 48000, 22000, 211, 90000, 30000, 361);    assertCorrectValues(builder.build(100000), 101000, 10000, 0, 122000, 12000, 100, 148000, 22000, 211, 190000, 30000, 361);}
0
public void testNoOpBuilderWithOffsetSizeIndex()
{    OffsetIndexBuilder builder = OffsetIndexBuilder.getNoOpBuilder();    builder.add(1, 2, 3);    builder.add(4, 5, 6);    builder.add(7, 8, 9);    builder.add(10, 11, 12);    assertNull(builder.build());    assertNull(builder.build(1000));}
0
private void assertCorrectValues(OffsetIndex offsetIndex, long... offset_size_rowIndex_triplets)
{    assertEquals(offset_size_rowIndex_triplets.length % 3, 0);    int pageCount = offset_size_rowIndex_triplets.length / 3;    assertEquals("Invalid pageCount", pageCount, offsetIndex.getPageCount());    for (int i = 0; i < pageCount; ++i) {        assertEquals("Invalid offsetIndex at page " + i, offset_size_rowIndex_triplets[3 * i], offsetIndex.getOffset(i));        assertEquals("Invalid compressedPageSize at page " + i, offset_size_rowIndex_triplets[3 * i + 1], offsetIndex.getCompressedPageSize(i));        assertEquals("Invalid firstRowIndex at page " + i, offset_size_rowIndex_triplets[3 * i + 2], offsetIndex.getFirstRowIndex(i));        long expectedLastPageIndex = (i < pageCount - 1) ? (offset_size_rowIndex_triplets[3 * i + 5] - 1) : 999;        assertEquals("Invalid lastRowIndex at page " + i, expectedLastPageIndex, offsetIndex.getLastRowIndex(i, 1000));    }}
0
 CIBuilder addNullPage(long nullCount)
{    nullPages.add(true);    nullCounts.add(nullCount);    minValues.add(EMPTY);    maxValues.add(EMPTY);    return this;}
0
 CIBuilder addPage(long nullCount, int min, int max)
{    nullPages.add(false);    nullCounts.add(nullCount);    minValues.add(ByteBuffer.wrap(BytesUtils.intToBytes(min)));    maxValues.add(ByteBuffer.wrap(BytesUtils.intToBytes(max)));    return this;}
0
 CIBuilder addPage(long nullCount, String min, String max)
{    nullPages.add(false);    nullCounts.add(nullCount);    minValues.add(ByteBuffer.wrap(min.getBytes(UTF_8)));    maxValues.add(ByteBuffer.wrap(max.getBytes(UTF_8)));    return this;}
0
 CIBuilder addPage(long nullCount, double min, double max)
{    nullPages.add(false);    nullCounts.add(nullCount);    minValues.add(ByteBuffer.wrap(BytesUtils.longToBytes(Double.doubleToLongBits(min))));    maxValues.add(ByteBuffer.wrap(BytesUtils.longToBytes(Double.doubleToLongBits(max))));    return this;}
0
 ColumnIndex build()
{    return ColumnIndexBuilder.build(type, order, nullPages, nullCounts, minValues, maxValues);}
0
 OIBuilder addPage(long rowCount)
{    builder.add(1234, rowCount);    return this;}
0
 OffsetIndex build()
{    return builder.build();}
0
public boolean keep(Integer value)
{    return true;}
0
public boolean canDrop(Statistics<Integer> statistics)
{    return false;}
0
public boolean inverseCanDrop(Statistics<Integer> statistics)
{    return true;}
0
public ColumnIndex getColumnIndex(ColumnPath column)
{    switch(column.toDotString()) {        case "column1":            return COLUMN1_CI;        case "column2":            return COLUMN2_CI;        case "column3":            return COLUMN3_CI;        case "column4":            return COLUMN4_CI;        default:            return null;    }}
0
public OffsetIndex getOffsetIndex(ColumnPath column)
{    switch(column.toDotString()) {        case "column1":            return COLUMN1_OI;        case "column2":            return COLUMN2_OI;        case "column3":            return COLUMN3_OI;        case "column4":            return COLUMN4_OI;        default:            throw new MissingOffsetIndexException(column);    }}
0
private static Set<ColumnPath> paths(String... columns)
{    Set<ColumnPath> paths = new HashSet<>();    for (String column : columns) {        paths.add(ColumnPath.fromDotString(column));    }    return paths;}
0
private static void assertAllRows(RowRanges ranges, long rowCount)
{    LongList actualList = new LongArrayList();    ranges.iterator().forEachRemaining((long value) -> actualList.add(value));    LongList expectedList = new LongArrayList();    LongStream.range(0, rowCount).forEach(expectedList::add);    assertArrayEquals(expectedList + " != " + actualList, expectedList.toLongArray(), actualList.toLongArray());}
0
private static void assertRows(RowRanges ranges, long... expectedRows)
{    LongList actualList = new LongArrayList();    ranges.iterator().forEachRemaining((long value) -> actualList.add(value));    assertArrayEquals(Arrays.toString(expectedRows) + " != " + actualList, expectedRows, actualList.toLongArray());}
0
public void testFiltering()
{    Set<ColumnPath> paths = paths("column1", "column2", "column3", "column4");    assertAllRows(calculateRowRanges(FilterCompat.get(userDefined(intColumn("column1"), AnyInt.class)), STORE, paths, TOTAL_ROW_COUNT), TOTAL_ROW_COUNT);    assertRows(calculateRowRanges(FilterCompat.get(and(and(eq(intColumn("column1"), null), eq(binaryColumn("column2"), null)), and(eq(doubleColumn("column3"), null), eq(booleanColumn("column4"), null)))), STORE, paths, TOTAL_ROW_COUNT), 6, 9);    assertRows(calculateRowRanges(FilterCompat.get(and(and(notEq(intColumn("column1"), null), notEq(binaryColumn("column2"), null)), and(notEq(doubleColumn("column3"), null), notEq(booleanColumn("column4"), null)))), STORE, paths, TOTAL_ROW_COUNT), 0, 1, 2, 3, 4, 5, 6, 7, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25);    assertRows(calculateRowRanges(FilterCompat.get(or(and(lt(intColumn("column1"), 20), gtEq(binaryColumn("column2"), fromString("Quebec"))), and(gt(doubleColumn("column3"), 5.32), ltEq(binaryColumn("column4"), fromString("XYZ"))))), STORE, paths, TOTAL_ROW_COUNT), 0, 1, 2, 3, 4, 5, 6, 7, 10, 11, 12, 13, 14, 15, 23, 24, 25);    assertRows(calculateRowRanges(FilterCompat.get(and(and(gtEq(intColumn("column1"), 7), gt(binaryColumn("column2"), fromString("India"))), and(eq(doubleColumn("column3"), null), notEq(binaryColumn("column4"), null)))), STORE, paths, TOTAL_ROW_COUNT), 7, 16, 17, 18, 19, 20);    assertRows(calculateRowRanges(FilterCompat.get(and(or(invert(userDefined(intColumn("column1"), AnyInt.class)), eq(binaryColumn("column2"), fromString("Echo"))), eq(doubleColumn("column3"), 6.0))), STORE, paths, TOTAL_ROW_COUNT), 23, 24, 25);    assertRows(calculateRowRanges(FilterCompat.get(and(userDefined(intColumn("column1"), IntegerIsDivisableWith3.class), and(userDefined(binaryColumn("column2"), BinaryUtf8StartsWithB.class), userDefined(doubleColumn("column3"), DoubleIsInteger.class)))), STORE, paths, TOTAL_ROW_COUNT), 21, 22, 23, 24, 25);    assertRows(calculateRowRanges(FilterCompat.get(and(and(gtEq(intColumn("column1"), 7), lt(intColumn("column1"), 11)), and(gt(binaryColumn("column2"), fromString("Romeo")), ltEq(binaryColumn("column2"), fromString("Tango"))))), STORE, paths, TOTAL_ROW_COUNT), 7, 11, 12, 13);}
0
public void testFilteringOnMissingColumns()
{    Set<ColumnPath> paths = paths("column1", "column2", "column3", "column4");        assertAllRows(calculateRowRanges(FilterCompat.get(notEq(intColumn("missing_column"), 0)), STORE, paths, TOTAL_ROW_COUNT), TOTAL_ROW_COUNT);    assertRows(calculateRowRanges(FilterCompat.get(and(and(gtEq(intColumn("column1"), 7), lt(intColumn("column1"), 11)), eq(binaryColumn("missing_column"), null))), STORE, paths, TOTAL_ROW_COUNT), 7, 8, 9, 10, 11, 12, 13);        assertRows(calculateRowRanges(FilterCompat.get(or(and(gtEq(intColumn("column1"), 7), lt(intColumn("column1"), 11)), notEq(binaryColumn("missing_column"), null))), STORE, paths, TOTAL_ROW_COUNT), 7, 8, 9, 10, 11, 12, 13);    assertRows(calculateRowRanges(FilterCompat.get(gt(intColumn("missing_column"), 0)), STORE, paths, TOTAL_ROW_COUNT));}
0
public void testFilteringWithMissingOffsetIndex()
{    Set<ColumnPath> paths = paths("column1", "column2", "column3", "column4", "column_wo_oi");    assertAllRows(calculateRowRanges(FilterCompat.get(and(and(gtEq(intColumn("column1"), 7), lt(intColumn("column1"), 11)), and(gt(binaryColumn("column2"), fromString("Romeo")), ltEq(binaryColumn("column_wo_oi"), fromString("Tango"))))), STORE, paths, TOTAL_ROW_COUNT), TOTAL_ROW_COUNT);}
0
private static RowRanges buildRanges(long... rowIndexes)
{    if (rowIndexes.length == 0) {        return RowRanges.EMPTY;    }    OffsetIndexBuilder builder = OffsetIndexBuilder.getBuilder();    for (int i = 0, n = rowIndexes.length; i < n; i += 2) {        long from = rowIndexes[i];        long to = rowIndexes[i + 1];        builder.add(0, 0, from);        builder.add(0, 0, to + 1);    }    PrimitiveIterator.OfInt pageIndexes = new PrimitiveIterator.OfInt() {        private int index = 0;        @Override        public boolean hasNext() {            return index < rowIndexes.length;        }        @Override        public int nextInt() {            int ret = index;            index += 2;            return ret;        }    };    return RowRanges.create(rowIndexes[rowIndexes.length - 1], pageIndexes, builder.build());}
0
public boolean hasNext()
{    return index < rowIndexes.length;}
0
public int nextInt()
{    int ret = index;    index += 2;    return ret;}
0
private static void assertAllRowsEqual(PrimitiveIterator.OfLong actualIt, long... expectedValues)
{    LongList actualList = new LongArrayList();    actualIt.forEachRemaining((long value) -> actualList.add(value));    assertArrayEquals(Arrays.toString(expectedValues) + "!= " + actualList, expectedValues, actualList.toLongArray());}
0
public void testCreate()
{    RowRanges ranges = buildRanges(1, 2, 3, 4, 6, 7, 7, 10, 15, 17);    assertAllRowsEqual(ranges.iterator(), 1, 2, 3, 4, 6, 7, 8, 9, 10, 15, 16, 17);    assertEquals(12, ranges.rowCount());    assertTrue(ranges.isOverlapping(4, 5));    assertFalse(ranges.isOverlapping(5, 5));    assertTrue(ranges.isOverlapping(10, 14));    assertFalse(ranges.isOverlapping(11, 14));    assertFalse(ranges.isOverlapping(18, Long.MAX_VALUE));    ranges = RowRanges.createSingle(5);    assertAllRowsEqual(ranges.iterator(), 0, 1, 2, 3, 4);    assertEquals(5, ranges.rowCount());    assertTrue(ranges.isOverlapping(0, 100));    assertFalse(ranges.isOverlapping(5, Long.MAX_VALUE));    ranges = RowRanges.EMPTY;    assertAllRowsEqual(ranges.iterator());    assertEquals(0, ranges.rowCount());    assertFalse(ranges.isOverlapping(0, Long.MAX_VALUE));}
0
public void testUnion()
{    RowRanges ranges1 = buildRanges(2, 5, 7, 9, 14, 14, 20, 24);    RowRanges ranges2 = buildRanges(1, 2, 4, 5, 11, 12, 14, 15, 21, 22);    RowRanges empty = buildRanges();    assertAllRowsEqual(union(ranges1, ranges2).iterator(), 1, 2, 3, 4, 5, 7, 8, 9, 11, 12, 14, 15, 20, 21, 22, 23, 24);    assertAllRowsEqual(union(ranges2, ranges1).iterator(), 1, 2, 3, 4, 5, 7, 8, 9, 11, 12, 14, 15, 20, 21, 22, 23, 24);    assertAllRowsEqual(union(ranges1, ranges1).iterator(), 2, 3, 4, 5, 7, 8, 9, 14, 20, 21, 22, 23, 24);    assertAllRowsEqual(union(ranges1, empty).iterator(), 2, 3, 4, 5, 7, 8, 9, 14, 20, 21, 22, 23, 24);    assertAllRowsEqual(union(empty, ranges1).iterator(), 2, 3, 4, 5, 7, 8, 9, 14, 20, 21, 22, 23, 24);    assertAllRowsEqual(union(ranges2, ranges2).iterator(), 1, 2, 4, 5, 11, 12, 14, 15, 21, 22);    assertAllRowsEqual(union(ranges2, empty).iterator(), 1, 2, 4, 5, 11, 12, 14, 15, 21, 22);    assertAllRowsEqual(union(empty, ranges2).iterator(), 1, 2, 4, 5, 11, 12, 14, 15, 21, 22);    assertAllRowsEqual(union(empty, empty).iterator());}
0
public void testIntersection()
{    RowRanges ranges1 = buildRanges(2, 5, 7, 9, 14, 14, 20, 24);    RowRanges ranges2 = buildRanges(1, 2, 6, 7, 9, 9, 11, 12, 14, 15, 21, 22);    RowRanges empty = buildRanges();    assertAllRowsEqual(intersection(ranges1, ranges2).iterator(), 2, 7, 9, 14, 21, 22);    assertAllRowsEqual(intersection(ranges2, ranges1).iterator(), 2, 7, 9, 14, 21, 22);    assertAllRowsEqual(intersection(ranges1, ranges1).iterator(), 2, 3, 4, 5, 7, 8, 9, 14, 20, 21, 22, 23, 24);    assertAllRowsEqual(intersection(ranges1, empty).iterator());    assertAllRowsEqual(intersection(empty, ranges1).iterator());    assertAllRowsEqual(intersection(ranges2, ranges2).iterator(), 1, 2, 6, 7, 9, 11, 12, 14, 15, 21, 22);    assertAllRowsEqual(intersection(ranges2, empty).iterator());    assertAllRowsEqual(intersection(empty, ranges2).iterator());    assertAllRowsEqual(intersection(empty, empty).iterator());}
0
private static void mutate(byte[] bytes)
{    for (int i = 0; i < bytes.length; i++) {        bytes[i] = (byte) (bytes[i] + 1);    }}
0
public BinaryAndOriginal get(byte[] bytes, boolean reused) throws Exception
{    byte[] orig = Arrays.copyOf(bytes, bytes.length);    if (reused) {        return new BinaryAndOriginal(Binary.fromReusedByteArray(orig), orig);    } else {        return new BinaryAndOriginal(Binary.fromConstantByteArray(orig), orig);    }}
0
public BinaryAndOriginal get(byte[] bytes, boolean reused) throws Exception
{    byte[] orig = padded(bytes);    Binary b;    if (reused) {        b = Binary.fromReusedByteArray(orig, 5, bytes.length);    } else {        b = Binary.fromConstantByteArray(orig, 5, bytes.length);    }    assertArrayEquals(bytes, b.getBytes());    return new BinaryAndOriginal(b, orig);}
0
public BinaryAndOriginal get(byte[] bytes, boolean reused) throws Exception
{    byte[] orig = padded(bytes);    ByteBuffer buff = ByteBuffer.wrap(orig, 5, bytes.length);    Binary b;    if (reused) {        b = Binary.fromReusedByteBuffer(buff);    } else {        b = Binary.fromConstantByteBuffer(buff);    }    buff.mark();    assertArrayEquals(bytes, b.getBytes());    buff.reset();    return new BinaryAndOriginal(b, orig);}
0
public BinaryAndOriginal get(byte[] bytes, boolean reused) throws Exception
{    Binary b = Binary.fromString(new String(bytes, UTF8));        return new BinaryAndOriginal(b, b.getBytesUnsafe());}
0
private static byte[] padded(byte[] bytes)
{    byte[] padded = new byte[bytes.length + 10];    for (int i = 0; i < 5; i++) {        padded[i] = (byte) i;    }    System.arraycopy(bytes, 0, padded, 5, bytes.length);    for (int i = 0; i < 5; i++) {        padded[i + 5 + bytes.length] = (byte) i;    }    return padded;}
0
public void testByteArrayBackedBinary() throws Exception
{    testBinary(BYTE_ARRAY_BACKED_BF, true);    testBinary(BYTE_ARRAY_BACKED_BF, false);}
0
public void testByteArraySliceBackedBinary() throws Exception
{    testBinary(BYTE_ARRAY_SLICE_BACKED_BF, true);    testBinary(BYTE_ARRAY_SLICE_BACKED_BF, false);}
0
public void testByteBufferBackedBinary() throws Exception
{    testBinary(BUFFER_BF, true);    testBinary(BUFFER_BF, false);}
0
public void testEqualityMethods() throws Exception
{    Binary bin1 = Binary.fromConstantByteArray("alice".getBytes(), 1, 3);    Binary bin2 = Binary.fromConstantByteBuffer(ByteBuffer.wrap("alice".getBytes(), 1, 3));    assertEquals(bin1, bin2);}
0
public void testWriteAllTo() throws Exception
{    byte[] orig = { 10, 9, 8, 7, 6, 5, 4, 3, 2, 1 };    testWriteAllToHelper(Binary.fromConstantByteBuffer(ByteBuffer.wrap(orig)), orig);    ByteBuffer buf = ByteBuffer.allocateDirect(orig.length);    buf.put(orig);    buf.flip();    testWriteAllToHelper(Binary.fromConstantByteBuffer(buf), orig);}
0
private void testWriteAllToHelper(Binary binary, byte[] orig) throws IOException
{    ByteArrayOutputStream out = new ByteArrayOutputStream(orig.length);    binary.writeTo(out);    assertArrayEquals(orig, out.toByteArray());}
0
public void testFromStringBinary() throws Exception
{    testBinary(STRING_BF, false);}
0
private void testSlice(BinaryFactory bf, boolean reused) throws Exception
{    BinaryAndOriginal bao = bf.get(testString.getBytes(UTF8), reused);    assertArrayEquals(testString.getBytes(UTF8), bao.binary.slice(0, testString.length()).getBytesUnsafe());    assertArrayEquals("123".getBytes(UTF8), bao.binary.slice(5, 3).getBytesUnsafe());}
0
private void testConstantCopy(BinaryFactory bf) throws Exception
{    BinaryAndOriginal bao = bf.get(testString.getBytes(UTF8), false);    assertEquals(false, bao.binary.isBackingBytesReused());    assertArrayEquals(testString.getBytes(UTF8), bao.binary.getBytes());    assertArrayEquals(testString.getBytes(UTF8), bao.binary.getBytesUnsafe());    assertArrayEquals(testString.getBytes(UTF8), bao.binary.copy().getBytesUnsafe());    assertArrayEquals(testString.getBytes(UTF8), bao.binary.copy().getBytes());    bao = bf.get(testString.getBytes(UTF8), false);    assertEquals(false, bao.binary.isBackingBytesReused());    Binary copy = bao.binary.copy();    assertSame(copy, bao.binary);}
0
private void testReusedCopy(BinaryFactory bf) throws Exception
{    BinaryAndOriginal bao = bf.get(testString.getBytes(UTF8), true);    assertEquals(true, bao.binary.isBackingBytesReused());    assertArrayEquals(testString.getBytes(UTF8), bao.binary.getBytes());    assertArrayEquals(testString.getBytes(UTF8), bao.binary.getBytesUnsafe());    assertArrayEquals(testString.getBytes(UTF8), bao.binary.copy().getBytesUnsafe());    assertArrayEquals(testString.getBytes(UTF8), bao.binary.copy().getBytes());    bao = bf.get(testString.getBytes(UTF8), true);    assertEquals(true, bao.binary.isBackingBytesReused());    Binary copy = bao.binary.copy();    mutate(bao.original);    assertArrayEquals(testString.getBytes(UTF8), copy.getBytes());    assertArrayEquals(testString.getBytes(UTF8), copy.getBytesUnsafe());    assertArrayEquals(testString.getBytes(UTF8), copy.copy().getBytesUnsafe());    assertArrayEquals(testString.getBytes(UTF8), copy.copy().getBytes());}
0
private void testSerializable(BinaryFactory bf, boolean reused) throws Exception
{    BinaryAndOriginal bao = bf.get("polygon".getBytes(UTF8), reused);    ByteArrayOutputStream baos = new ByteArrayOutputStream();    ObjectOutputStream out = new ObjectOutputStream(baos);    out.writeObject(bao.binary);    out.close();    baos.close();    ObjectInputStream in = new ObjectInputStream(new ByteArrayInputStream(baos.toByteArray()));    Object object = in.readObject();    assertTrue(object instanceof Binary);    assertEquals(bao.binary, object);}
0
private void testBinary(BinaryFactory bf, boolean reused) throws Exception
{    testSlice(bf, reused);    if (reused) {        testReusedCopy(bf);    } else {        testConstantCopy(bf);    }    testSerializable(bf, reused);}
0
public void testCompare()
{    Binary b1 = Binary.fromCharSequence("aaaaaaaa");    Binary b2 = Binary.fromString("aaaaaaab");    Binary b3 = Binary.fromReusedByteArray("aaaaaaaaaaa".getBytes(), 1, 8);    Binary b4 = Binary.fromConstantByteBuffer(ByteBuffer.wrap("aaaaaaac".getBytes()));    assertTrue(b1.compareTo(b2) < 0);    assertTrue(b2.compareTo(b1) > 0);    assertTrue(b3.compareTo(b4) < 0);    assertTrue(b4.compareTo(b3) > 0);    assertTrue(b1.compareTo(b4) < 0);    assertTrue(b4.compareTo(b1) > 0);    assertTrue(b2.compareTo(b4) < 0);    assertTrue(b4.compareTo(b2) > 0);    assertTrue(b1.compareTo(b3) == 0);    assertTrue(b3.compareTo(b1) == 0);}
0
public void startMessage()
{    root.start();    this.currentType = schema;    this.current = root;}
0
public void endMessage()
{    root.end();}
0
public void startField(String field, int index)
{    path.push(current);    typePath.push(currentType);    currentType = currentType.asGroupType().getType(index);    if (currentType.isPrimitive()) {        currentPrimitive = current.getConverter(index).asPrimitiveConverter();    } else {        current = current.getConverter(index).asGroupConverter();    }}
0
public void endField(String field, int index)
{    currentType = typePath.pop();    current = path.pop();}
0
public void startGroup()
{    current.start();}
0
public void endGroup()
{    current.end();}
0
public void addInteger(int value)
{    currentPrimitive.addInt(value);}
0
public void addLong(long value)
{    currentPrimitive.addLong(value);}
0
public void addBoolean(boolean value)
{    currentPrimitive.addBoolean(value);}
0
public void addBinary(Binary value)
{    currentPrimitive.addBinary(value);}
0
public void addFloat(float value)
{    currentPrimitive.addFloat(value);}
0
public void addDouble(double value)
{    currentPrimitive.addDouble(value);}
0
public void flush()
{}
0
public void validate(String got)
{    assertEquals("event #" + count, expectations.pop(), got);    ++count;}
0
public Converter convertPrimitiveType(final List<GroupType> path, final PrimitiveType primitiveType)
{    return new PrimitiveConverter() {        private void validate(String message) {            ExpectationValidatingConverter.this.validate(path(path, primitiveType) + message);        }        @Override        public void addBinary(Binary value) {            validate("addBinary(" + value.toStringUsingUTF8() + ")");        }        @Override        public void addBoolean(boolean value) {            validate("addBoolean(" + value + ")");        }        @Override        public void addDouble(double value) {            validate("addDouble(" + value + ")");        }        @Override        public void addFloat(float value) {            validate("addFloat(" + value + ")");        }        @Override        public void addInt(int value) {            validate("addInt(" + value + ")");        }        @Override        public void addLong(long value) {            validate("addLong(" + value + ")");        }    };}
0
private void validate(String message)
{    ExpectationValidatingConverter.this.validate(path(path, primitiveType) + message);}
0
public void addBinary(Binary value)
{    validate("addBinary(" + value.toStringUsingUTF8() + ")");}
0
public void addBoolean(boolean value)
{    validate("addBoolean(" + value + ")");}
0
public void addDouble(double value)
{    validate("addDouble(" + value + ")");}
0
public void addFloat(float value)
{    validate("addFloat(" + value + ")");}
0
public void addInt(int value)
{    validate("addInt(" + value + ")");}
0
public void addLong(long value)
{    validate("addLong(" + value + ")");}
0
public Converter convertGroupType(final List<GroupType> path, final GroupType groupType, final List<Converter> children)
{    return new GroupConverter() {        private void validate(String message) {            ExpectationValidatingConverter.this.validate(path(path, groupType) + message);        }        @Override        public void start() {            validate("start()");        }        @Override        public void end() {            validate("end()");        }        @Override        public Converter getConverter(int fieldIndex) {            return children.get(fieldIndex);        }    };}
0
private void validate(String message)
{    ExpectationValidatingConverter.this.validate(path(path, groupType) + message);}
0
public void start()
{    validate("start()");}
0
public void end()
{    validate("end()");}
0
public Converter getConverter(int fieldIndex)
{    return children.get(fieldIndex);}
0
public Converter convertMessageType(MessageType messageType, final List<Converter> children)
{    return new GroupConverter() {        @Override        public Converter getConverter(int fieldIndex) {            return children.get(fieldIndex);        }        @Override        public void start() {            validate("startMessage()");        }        @Override        public void end() {            validate("endMessage()");        }    };}
0
public Converter getConverter(int fieldIndex)
{    return children.get(fieldIndex);}
0
public void start()
{    validate("startMessage()");}
0
public void end()
{    validate("endMessage()");}
0
public Void getCurrentRecord()
{    return null;}
0
private String path(List<GroupType> path, Type type)
{    String pathString = "";    if (path.size() > 0) {        for (int i = 1; i < path.size(); i++) {            pathString += path.get(i).getName() + ".";        }    }    pathString += type.getName() + ".";    return pathString;}
0
public GroupConverter getRootConverter()
{    return root;}
0
private void validate(String got)
{        assertEquals("event #" + count, expectations.pop(), got);    ++count;}
0
public void startMessage()
{    validate("startMessage()");}
0
public void startGroup()
{    validate("startGroup()");}
0
public void startField(String field, int index)
{    validate("startField(" + field + ", " + index + ")");}
0
public void endMessage()
{    validate("endMessage()");}
0
public void endGroup()
{    validate("endGroup()");}
0
public void endField(String field, int index)
{    validate("endField(" + field + ", " + index + ")");}
0
public void addInteger(int value)
{    validate("addInt(" + value + ")");}
0
public void addLong(long value)
{    validate("addLong(" + value + ")");}
0
public void addBoolean(boolean value)
{    validate("addBoolean(" + value + ")");}
0
public void addBinary(Binary value)
{    validate("addBinary(" + value.toStringUsingUTF8() + ")");}
0
public void addFloat(float value)
{    validate("addFloat(" + value + ")");}
0
public void addDouble(double value)
{    validate("addDouble(" + value + ")");}
0
public void flush()
{    validate("flush()");}
0
public static void main(String[] args)
{    MemPageStore memPageStore = new MemPageStore(0);    write(memPageStore);    read(memPageStore);}
0
private static void read(MemPageStore memPageStore)
{    read(memPageStore, schema, "read all");    read(memPageStore, schema, "read all");    read(memPageStore, schema2, "read projected");    read(memPageStore, schema3, "read projected no Strings");}
0
private static void read(MemPageStore memPageStore, MessageType myschema, String message)
{    MessageColumnIO columnIO = newColumnFactory(myschema);    System.out.println(message);    RecordMaterializer<Object> recordConsumer = new DummyRecordConverter(myschema);    RecordReader<Object> recordReader = columnIO.getRecordReader(memPageStore, recordConsumer);    read(recordReader, 2, myschema);    read(recordReader, 10000, myschema);    read(recordReader, 10000, myschema);    read(recordReader, 10000, myschema);    read(recordReader, 10000, myschema);    read(recordReader, 10000, myschema);    read(recordReader, 100000, myschema);    read(recordReader, 1000000, myschema);    System.out.println();}
0
private static void write(MemPageStore memPageStore)
{    ColumnWriteStoreV1 columns = new ColumnWriteStoreV1(memPageStore, ParquetProperties.builder().withPageSize(50 * 1024 * 1024).withDictionaryEncoding(false).build());    MessageColumnIO columnIO = newColumnFactory(schema);    GroupWriter groupWriter = new GroupWriter(columnIO.getRecordWriter(columns), schema);    groupWriter.write(r1);    groupWriter.write(r2);    write(memPageStore, groupWriter, 10000);    write(memPageStore, groupWriter, 10000);    write(memPageStore, groupWriter, 10000);    write(memPageStore, groupWriter, 10000);    write(memPageStore, groupWriter, 10000);    write(memPageStore, groupWriter, 100000);    write(memPageStore, groupWriter, 1000000);    columns.flush();    System.out.println();    System.out.println(columns.getBufferedSize() + " bytes used total");    System.out.println("max col size: " + columns.maxColMemSize() + " bytes");}
0
private static MessageColumnIO newColumnFactory(MessageType schema)
{    return new ColumnIOFactory().getColumnIO(schema);}
0
private static void read(RecordReader<Object> recordReader, int count, MessageType schema)
{    Object[] records = new Object[count];    System.gc();    System.out.print("no gc <");    long t0 = System.currentTimeMillis();    for (int i = 0; i < records.length; i++) {        records[i] = recordReader.read();    }    long t1 = System.currentTimeMillis();    System.out.print("> ");    long t = t1 - t0;        float err = (float) 100 * 2 / t;    System.out.printf("                                            read %,9d recs in %,5d ms at %,9d rec/s err: %3.2f%%\n", count, t, t == 0 ? 0 : count * 1000 / t, err);    if (!records[0].equals("end()")) {        throw new RuntimeException("" + records[0]);    }}
0
private static void write(MemPageStore memPageStore, GroupWriter groupWriter, int count)
{    long t0 = System.currentTimeMillis();    for (int i = 0; i < count; i++) {        groupWriter.write(r1);    }    long t1 = System.currentTimeMillis();    long t = t1 - t0;    memPageStore.addRowCount(count);    System.out.printf("written %,9d recs in %,5d ms at %,9d rec/s\n", count, t, t == 0 ? 0 : count * 1000 / t);}
0
public static Collection<Object[]> data() throws IOException
{    Object[][] data = { { true }, { false } };    return Arrays.asList(data);}
0
public void testSchema()
{    assertEquals(schemaString, schema.toString());}
0
public void testReadUsingRequestedSchemaWithExtraFields()
{    MessageType orginalSchema = new MessageType("schema", new PrimitiveType(REQUIRED, INT32, "a"), new PrimitiveType(OPTIONAL, INT32, "b"));    MessageType schemaWithExtraField = new MessageType("schema", new PrimitiveType(OPTIONAL, INT32, "b"), new PrimitiveType(OPTIONAL, INT32, "a"), new PrimitiveType(OPTIONAL, INT32, "c"));    MemPageStore memPageStoreForOriginalSchema = new MemPageStore(1);    MemPageStore memPageStoreForSchemaWithExtraField = new MemPageStore(1);    SimpleGroupFactory groupFactory = new SimpleGroupFactory(orginalSchema);    writeGroups(orginalSchema, memPageStoreForOriginalSchema, groupFactory.newGroup().append("a", 1).append("b", 2));    SimpleGroupFactory groupFactory2 = new SimpleGroupFactory(schemaWithExtraField);    writeGroups(schemaWithExtraField, memPageStoreForSchemaWithExtraField, groupFactory2.newGroup().append("a", 1).append("b", 2).append("c", 3));    {        List<Group> groups = new ArrayList<Group>();        groups.addAll(readGroups(memPageStoreForOriginalSchema, orginalSchema, schemaWithExtraField, 1));        groups.addAll(readGroups(memPageStoreForSchemaWithExtraField, schemaWithExtraField, schemaWithExtraField, 1));                        Object[][] expected = { { 2, 1, null }, { 2, 1, 3 }         };        validateGroups(groups, expected);    }}
0
public void testReadUsingRequestedSchemaWithIncompatibleField()
{    MessageType originalSchema = new MessageType("schema", new PrimitiveType(OPTIONAL, INT32, "e"));    MemPageStore store = new MemPageStore(1);    SimpleGroupFactory groupFactory = new SimpleGroupFactory(originalSchema);    writeGroups(originalSchema, store, groupFactory.newGroup().append("e", 4));    try {        MessageType schemaWithIncompatibleField = new MessageType("schema",         new PrimitiveType(OPTIONAL, BINARY, "e"));        readGroups(store, originalSchema, schemaWithIncompatibleField, 1);        fail("should have thrown an incompatible schema exception");    } catch (ParquetDecodingException e) {        assertEquals("The requested schema is not compatible with the file schema. incompatible types: optional binary e != optional int32 e", e.getMessage());    }}
0
public void testReadUsingSchemaWithRequiredFieldThatWasOptional()
{    MessageType originalSchema = new MessageType("schema", new PrimitiveType(OPTIONAL, INT32, "e"));    MemPageStore store = new MemPageStore(1);    SimpleGroupFactory groupFactory = new SimpleGroupFactory(originalSchema);    writeGroups(originalSchema, store, groupFactory.newGroup().append("e", 4));    try {        MessageType schemaWithRequiredFieldThatWasOptional = new MessageType("schema",         new PrimitiveType(REQUIRED, INT32, "e"));        readGroups(store, originalSchema, schemaWithRequiredFieldThatWasOptional, 1);        fail("should have thrown an incompatible schema exception");    } catch (ParquetDecodingException e) {        assertEquals("The requested schema is not compatible with the file schema. incompatible types: required int32 e != optional int32 e", e.getMessage());    }}
0
public void testReadUsingProjectedSchema()
{    MessageType orginalSchema = new MessageType("schema", new PrimitiveType(REQUIRED, INT32, "a"), new PrimitiveType(REQUIRED, INT32, "b"));    MessageType projectedSchema = new MessageType("schema", new PrimitiveType(OPTIONAL, INT32, "b"));    MemPageStore store = new MemPageStore(1);    SimpleGroupFactory groupFactory = new SimpleGroupFactory(orginalSchema);    writeGroups(orginalSchema, store, groupFactory.newGroup().append("a", 1).append("b", 2));    {        List<Group> groups = new ArrayList<Group>();        groups.addAll(readGroups(store, orginalSchema, projectedSchema, 1));        Object[][] expected = { { 2 } };        validateGroups(groups, expected);    }}
0
private void validateGroups(List<Group> groups1, Object[][] e1)
{    Iterator<Group> i1 = groups1.iterator();    for (int i = 0; i < e1.length; i++) {        Object[] objects = e1[i];        Group next = i1.next();        for (int j = 0; j < objects.length; j++) {            Object object = objects[j];            if (object == null) {                assertEquals(0, next.getFieldRepetitionCount(j));            } else {                assertEquals("looking for r[" + i + "][" + j + "][0]=" + object, 1, next.getFieldRepetitionCount(j));                assertEquals(object, next.getInteger(j, 0));            }        }    }}
0
private List<Group> readGroups(MemPageStore memPageStore, MessageType fileSchema, MessageType requestedSchema, int n)
{    ColumnIOFactory columnIOFactory = new ColumnIOFactory(true);    MessageColumnIO columnIO = columnIOFactory.getColumnIO(requestedSchema, fileSchema);    RecordReaderImplementation<Group> recordReader = getRecordReader(columnIO, requestedSchema, memPageStore);    List<Group> groups = new ArrayList<Group>();    for (int i = 0; i < n; i++) {        groups.add(recordReader.read());    }    return groups;}
0
private void writeGroups(MessageType writtenSchema, MemPageStore memPageStore, Group... groups)
{    ColumnIOFactory columnIOFactory = new ColumnIOFactory(true);    ColumnWriteStoreV1 columns = newColumnWriteStore(memPageStore);    MessageColumnIO columnIO = columnIOFactory.getColumnIO(writtenSchema);    RecordConsumer recordWriter = columnIO.getRecordWriter(columns);    GroupWriter groupWriter = new GroupWriter(recordWriter, writtenSchema);    for (Group group : groups) {        groupWriter.write(group);    }    recordWriter.flush();    columns.flush();}
0
public void testColumnIO()
{    log(schema);    log("r1");    log(r1);    log("r2");    log(r2);    MemPageStore memPageStore = new MemPageStore(2);    ColumnWriteStoreV1 columns = newColumnWriteStore(memPageStore);    ColumnIOFactory columnIOFactory = new ColumnIOFactory(true);    {        MessageColumnIO columnIO = columnIOFactory.getColumnIO(schema);        log(columnIO);        RecordConsumer recordWriter = columnIO.getRecordWriter(columns);        GroupWriter groupWriter = new GroupWriter(recordWriter, schema);        groupWriter.write(r1);        groupWriter.write(r2);        recordWriter.flush();        columns.flush();        log(columns);        log("=========");        RecordReaderImplementation<Group> recordReader = getRecordReader(columnIO, schema, memPageStore);        validateFSA(expectedFSA, columnIO, recordReader);        List<Group> records = new ArrayList<Group>();        records.add(recordReader.read());        records.add(recordReader.read());        int i = 0;        for (Group record : records) {            log("r" + (++i));            log(record);        }        assertEquals("deserialization does not display the same result", r1.toString(), records.get(0).toString());        assertEquals("deserialization does not display the same result", r2.toString(), records.get(1).toString());    }    {        MessageColumnIO columnIO2 = columnIOFactory.getColumnIO(schema2);        List<Group> records = new ArrayList<Group>();        RecordReaderImplementation<Group> recordReader = getRecordReader(columnIO2, schema2, memPageStore);        validateFSA(expectedFSA2, columnIO2, recordReader);        records.add(recordReader.read());        records.add(recordReader.read());        int i = 0;        for (Group record : records) {            log("r" + (++i));            log(record);        }        assertEquals("deserialization does not display the expected result", pr1.toString(), records.get(0).toString());        assertEquals("deserialization does not display the expected result", pr2.toString(), records.get(1).toString());    }}
0
public void testOneOfEach()
{    MessageType oneOfEachSchema = MessageTypeParser.parseMessageType(oneOfEach);    GroupFactory gf = new SimpleGroupFactory(oneOfEachSchema);    Group g1 = gf.newGroup().append("a", 1l).append("b", 2).append("c", 3.0f).append("d", 4.0d).append("e", true).append("f", Binary.fromString("6")).append("g", new NanoTime(1234, System.currentTimeMillis() * 1000)).append("h", Binary.fromString("abc"));    testSchema(oneOfEachSchema, Arrays.asList(g1));}
0
public void testRequiredOfRequired()
{    MessageType reqreqSchema = MessageTypeParser.parseMessageType("message Document {\n" + "  required group foo {\n" + "    required int64 bar;\n" + "  }\n" + "}\n");    GroupFactory gf = new SimpleGroupFactory(reqreqSchema);    Group g1 = gf.newGroup();    g1.addGroup("foo").append("bar", 2l);    testSchema(reqreqSchema, Arrays.asList(g1));}
0
public void testOptionalRequiredInteraction()
{    for (int i = 0; i < 6; i++) {        Type current = new PrimitiveType(Repetition.REQUIRED, PrimitiveTypeName.BINARY, "primitive");        for (int j = 0; j < i; j++) {            current = new GroupType(Repetition.REQUIRED, "req" + j, current);        }        MessageType groupSchema = new MessageType("schema" + i, current);        GroupFactory gf = new SimpleGroupFactory(groupSchema);        List<Group> groups = new ArrayList<Group>();        Group root = gf.newGroup();        Group currentGroup = root;        for (int j = 0; j < i; j++) {            currentGroup = currentGroup.addGroup(0);        }        currentGroup.add(0, Binary.fromString("foo"));        groups.add(root);        testSchema(groupSchema, groups);    }    for (int i = 0; i < 6; i++) {        Type current = new PrimitiveType(Repetition.OPTIONAL, PrimitiveTypeName.BINARY, "primitive");        for (int j = 0; j < i; j++) {            current = new GroupType(Repetition.REQUIRED, "req" + j, current);        }        MessageType groupSchema = new MessageType("schema" + (i + 6), current);        GroupFactory gf = new SimpleGroupFactory(groupSchema);        List<Group> groups = new ArrayList<Group>();        Group rootDefined = gf.newGroup();        Group rootUndefined = gf.newGroup();        Group currentDefinedGroup = rootDefined;        Group currentUndefinedGroup = rootUndefined;        for (int j = 0; j < i; j++) {            currentDefinedGroup = currentDefinedGroup.addGroup(0);            currentUndefinedGroup = currentUndefinedGroup.addGroup(0);        }        currentDefinedGroup.add(0, Binary.fromString("foo"));        groups.add(rootDefined);        groups.add(rootUndefined);        testSchema(groupSchema, groups);    }    for (int i = 0; i < 6; i++) {        Type current = new PrimitiveType(Repetition.OPTIONAL, PrimitiveTypeName.BINARY, "primitive");        for (int j = 0; j < 6; j++) {            current = new GroupType(i == j ? Repetition.OPTIONAL : Repetition.REQUIRED, "req" + j, current);        }        MessageType groupSchema = new MessageType("schema" + (i + 12), current);        GroupFactory gf = new SimpleGroupFactory(groupSchema);        List<Group> groups = new ArrayList<Group>();        Group rootDefined = gf.newGroup();        Group rootUndefined = gf.newGroup();        Group currentDefinedGroup = rootDefined;        Group currentUndefinedGroup = rootUndefined;        for (int j = 0; j < 6; j++) {            currentDefinedGroup = currentDefinedGroup.addGroup(0);            if (i < j) {                currentUndefinedGroup = currentUndefinedGroup.addGroup(0);            }        }        currentDefinedGroup.add(0, Binary.fromString("foo"));        groups.add(rootDefined);        groups.add(rootUndefined);        testSchema(groupSchema, groups);    }}
0
private void testSchema(MessageType messageSchema, List<Group> groups)
{    MemPageStore memPageStore = new MemPageStore(groups.size());    ColumnWriteStoreV1 columns = newColumnWriteStore(memPageStore);    ColumnIOFactory columnIOFactory = new ColumnIOFactory(true);    MessageColumnIO columnIO = columnIOFactory.getColumnIO(messageSchema);    log(columnIO);        RecordConsumer recordWriter = columnIO.getRecordWriter(columns);    GroupWriter groupWriter = new GroupWriter(recordWriter, messageSchema);    for (Group group : groups) {        groupWriter.write(group);    }    recordWriter.flush();    columns.flush();        RecordReaderImplementation<Group> recordReader = getRecordReader(columnIO, messageSchema, memPageStore);    for (Group group : groups) {        final Group got = recordReader.read();        assertEquals("deserialization does not display the same result", group.toString(), got.toString());    }}
0
private RecordReaderImplementation<Group> getRecordReader(MessageColumnIO columnIO, MessageType schema, PageReadStore pageReadStore)
{    RecordMaterializer<Group> recordConverter = new GroupRecordConverter(schema);    return (RecordReaderImplementation<Group>) columnIO.getRecordReader(pageReadStore, recordConverter);}
0
private void log(Object o)
{    }
1
private void validateFSA(int[][] expectedFSA, MessageColumnIO columnIO, RecordReaderImplementation<?> recordReader)
{    log("FSA: ----");    List<PrimitiveColumnIO> leaves = columnIO.getLeaves();    for (int i = 0; i < leaves.size(); ++i) {        PrimitiveColumnIO primitiveColumnIO = leaves.get(i);        log(Arrays.toString(primitiveColumnIO.getFieldPath()));        for (int r = 0; r < expectedFSA[i].length; r++) {            int next = expectedFSA[i][r];            log(" " + r + " -> " + (next == leaves.size() ? "end" : Arrays.toString(leaves.get(next).getFieldPath())) + ": " + recordReader.getNextLevel(i, r));            assertEquals(Arrays.toString(primitiveColumnIO.getFieldPath()) + ": " + r + " -> ", next, recordReader.getNextReader(i, r));        }    }    log("----");}
0
public void testPushParser()
{    MemPageStore memPageStore = new MemPageStore(1);    ColumnWriteStoreV1 columns = newColumnWriteStore(memPageStore);    MessageColumnIO columnIO = new ColumnIOFactory().getColumnIO(schema);    RecordConsumer recordWriter = columnIO.getRecordWriter(columns);    new GroupWriter(recordWriter, schema).write(r1);    recordWriter.flush();    columns.flush();    RecordReader<Void> recordReader = columnIO.getRecordReader(memPageStore, new ExpectationValidatingConverter(expectedEventsForR1, schema));    recordReader.read();}
0
private ColumnWriteStoreV1 newColumnWriteStore(MemPageStore memPageStore)
{    return new ColumnWriteStoreV1(memPageStore, ParquetProperties.builder().withPageSize(800).withDictionaryPageSize(800).withDictionaryEncoding(useDictionary).build());}
0
public void testEmptyField()
{    MemPageStore memPageStore = new MemPageStore(1);    ColumnWriteStoreV1 columns = newColumnWriteStore(memPageStore);    MessageColumnIO columnIO = new ColumnIOFactory(true).getColumnIO(schema);    final RecordConsumer recordWriter = columnIO.getRecordWriter(columns);    recordWriter.startMessage();    recordWriter.startField("DocId", 0);    recordWriter.addLong(0);    recordWriter.endField("DocId", 0);    recordWriter.startField("Links", 1);    try {        recordWriter.endField("Links", 1);        Assert.fail("expected exception because of empty field");    } catch (ParquetEncodingException e) {        Assert.assertEquals("empty fields are illegal, the field should be ommited completely instead", e.getMessage());    }}
0
public void testGroupWriter()
{    List<Group> result = new ArrayList<Group>();    final GroupRecordConverter groupRecordConverter = new GroupRecordConverter(schema);    RecordConsumer groupConsumer = new ConverterConsumer(groupRecordConverter.getRootConverter(), schema);    GroupWriter groupWriter = new GroupWriter(new RecordConsumerLoggingWrapper(groupConsumer), schema);    groupWriter.write(r1);    result.add(groupRecordConverter.getCurrentRecord());    groupWriter.write(r2);    result.add(groupRecordConverter.getCurrentRecord());    assertEquals("deserialization does not display the expected result", result.get(0).toString(), r1.toString());    assertEquals("deserialization does not display the expected result", result.get(1).toString(), r2.toString());}
0
public void testWriteWithGroupWriter()
{    final String[] expected = { "[DocId]: 10, r:0, d:0", "[Links, Forward]: 20, r:0, d:2", "[Links, Forward]: 40, r:1, d:2", "[Links, Forward]: 60, r:1, d:2", "[Links, Backward]: null, r:0, d:1", "[Name, Language, Code]: en-us, r:0, d:2", "[Name, Language, Country]: us, r:0, d:3", "[Name, Language, Code]: en, r:2, d:2", "[Name, Language, Country]: null, r:2, d:2", "[Name, Url]: http://A, r:0, d:2", "[Name, Url]: http://B, r:1, d:2", "[Name, Language, Code]: null, r:1, d:1", "[Name, Language, Country]: null, r:1, d:1", "[Name, Language, Code]: en-gb, r:1, d:2", "[Name, Language, Country]: gb, r:1, d:3", "[Name, Url]: null, r:1, d:1", "[DocId]: 20, r:0, d:0", "[Links, Backward]: 10, r:0, d:2", "[Links, Backward]: 30, r:1, d:2", "[Links, Forward]: 80, r:0, d:2", "[Name, Url]: http://C, r:0, d:2", "[Name, Language, Code]: null, r:0, d:1", "[Name, Language, Country]: null, r:0, d:1" };    ValidatingColumnWriteStore columns = new ValidatingColumnWriteStore(expected);    MessageColumnIO columnIO = new ColumnIOFactory().getColumnIO(schema);    RecordConsumer recordWriter = columnIO.getRecordWriter(columns);    GroupWriter groupWriter = new GroupWriter(recordWriter, schema);    groupWriter.write(r1);    groupWriter.write(r2);    recordWriter.flush();    columns.validate();    columns.flush();    columns.close();}
0
public void close()
{}
0
public ColumnWriter getColumnWriter(final ColumnDescriptor path)
{    return new ColumnWriter() {        private void validate(Object value, int repetitionLevel, int definitionLevel) {            String actual = Arrays.toString(path.getPath()) + ": " + value + ", r:" + repetitionLevel + ", d:" + definitionLevel;            assertEquals("event #" + counter, expected[counter], actual);            ++counter;        }        @Override        public void writeNull(int repetitionLevel, int definitionLevel) {            validate(null, repetitionLevel, definitionLevel);        }        @Override        public void write(Binary value, int repetitionLevel, int definitionLevel) {            validate(value.toStringUsingUTF8(), repetitionLevel, definitionLevel);        }        @Override        public void write(float value, int repetitionLevel, int definitionLevel) {            validate(value, repetitionLevel, definitionLevel);        }        @Override        public void write(boolean value, int repetitionLevel, int definitionLevel) {            validate(value, repetitionLevel, definitionLevel);        }        @Override        public void write(int value, int repetitionLevel, int definitionLevel) {            validate(value, repetitionLevel, definitionLevel);        }        @Override        public void write(long value, int repetitionLevel, int definitionLevel) {            validate(value, repetitionLevel, definitionLevel);        }        @Override        public void close() {        }        @Override        public long getBufferedSizeInMemory() {            throw new UnsupportedOperationException();        }        @Override        public void write(double value, int repetitionLevel, int definitionLevel) {            validate(value, repetitionLevel, definitionLevel);        }    };}
0
private void validate(Object value, int repetitionLevel, int definitionLevel)
{    String actual = Arrays.toString(path.getPath()) + ": " + value + ", r:" + repetitionLevel + ", d:" + definitionLevel;    assertEquals("event #" + counter, expected[counter], actual);    ++counter;}
0
public void writeNull(int repetitionLevel, int definitionLevel)
{    validate(null, repetitionLevel, definitionLevel);}
0
public void write(Binary value, int repetitionLevel, int definitionLevel)
{    validate(value.toStringUsingUTF8(), repetitionLevel, definitionLevel);}
0
public void write(float value, int repetitionLevel, int definitionLevel)
{    validate(value, repetitionLevel, definitionLevel);}
0
public void write(boolean value, int repetitionLevel, int definitionLevel)
{    validate(value, repetitionLevel, definitionLevel);}
0
public void write(int value, int repetitionLevel, int definitionLevel)
{    validate(value, repetitionLevel, definitionLevel);}
0
public void write(long value, int repetitionLevel, int definitionLevel)
{    validate(value, repetitionLevel, definitionLevel);}
0
public void close()
{}
0
public long getBufferedSizeInMemory()
{    throw new UnsupportedOperationException();}
0
public void write(double value, int repetitionLevel, int definitionLevel)
{    validate(value, repetitionLevel, definitionLevel);}
0
public void validate()
{    assertEquals("read all events", expected.length, counter);}
0
public void endRecord()
{}
0
public void flush()
{}
0
public long getAllocatedSize()
{    return 0;}
0
public long getBufferedSize()
{    return 0;}
0
public String memUsageString()
{    return null;}
0
public boolean functionToApply(long input)
{    return input > 15;}
0
public boolean functionToApply(String input)
{    return input.endsWith("A");}
0
private List<Group> readAll(RecordReader<Group> reader)
{    List<Group> result = new ArrayList<Group>();    Group g;    while ((g = reader.read()) != null) {        result.add(g);    }    return result;}
0
private void readOne(RecordReader<Group> reader, String message, Group expected)
{    List<Group> result = readAll(reader);    assertEquals(message + ": " + result, 1, result.size());    assertEquals("filtering did not return the correct record", expected.toString(), result.get(0).toString());}
0
public void testFilterOnInteger()
{    MessageColumnIO columnIO = new ColumnIOFactory(true).getColumnIO(schema);    MemPageStore memPageStore = writeTestRecords(columnIO, 1);        RecordMaterializer<Group> recordConverter = new GroupRecordConverter(schema);    RecordReaderImplementation<Group> recordReader = (RecordReaderImplementation<Group>) columnIO.getRecordReader(memPageStore, recordConverter, FilterCompat.get(column("DocId", equalTo(10l))));    readOne(recordReader, "r2 filtered out", r1);        recordReader = (RecordReaderImplementation<Group>) columnIO.getRecordReader(memPageStore, recordConverter, FilterCompat.get(column("DocId", equalTo(20l))));    readOne(recordReader, "r1 filtered out", r2);}
0
public void testApplyFunctionFilterOnLong()
{    MessageColumnIO columnIO = new ColumnIOFactory(true).getColumnIO(schema);    MemPageStore memPageStore = writeTestRecords(columnIO, 1);        RecordMaterializer<Group> recordConverter = new GroupRecordConverter(schema);    RecordReaderImplementation<Group> recordReader = (RecordReaderImplementation<Group>) columnIO.getRecordReader(memPageStore, recordConverter, FilterCompat.get(column("DocId", equalTo(10l))));    readOne(recordReader, "r2 filtered out", r1);        recordReader = (RecordReaderImplementation<Group>) columnIO.getRecordReader(memPageStore, recordConverter, FilterCompat.get(column("DocId", applyFunctionToLong(new LongGreaterThan15Predicate()))));    readOne(recordReader, "r1 filtered out", r2);}
0
public void testFilterOnString()
{    MessageColumnIO columnIO = new ColumnIOFactory(true).getColumnIO(schema);    MemPageStore memPageStore = writeTestRecords(columnIO, 1);        RecordMaterializer<Group> recordConverter = new GroupRecordConverter(schema);    RecordReaderImplementation<Group> recordReader = (RecordReaderImplementation<Group>) columnIO.getRecordReader(memPageStore, recordConverter, FilterCompat.get(column("Name.Url", equalTo("http://A"))));    readOne(recordReader, "r2 filtered out", r1);            recordReader = (RecordReaderImplementation<Group>) columnIO.getRecordReader(memPageStore, recordConverter, FilterCompat.get(column("Name.Url", equalTo("http://B"))));    List<Group> all = readAll(recordReader);    assertEquals("There should be no matching records: " + all, 0, all.size());        recordReader = (RecordReaderImplementation<Group>) columnIO.getRecordReader(memPageStore, recordConverter, FilterCompat.get(column("Name.Url", equalTo("http://C"))));    readOne(recordReader, "r1 filtered out", r2);}
0
public void testApplyFunctionFilterOnString()
{    MessageColumnIO columnIO = new ColumnIOFactory(true).getColumnIO(schema);    MemPageStore memPageStore = writeTestRecords(columnIO, 1);        RecordMaterializer<Group> recordConverter = new GroupRecordConverter(schema);    RecordReaderImplementation<Group> recordReader = (RecordReaderImplementation<Group>) columnIO.getRecordReader(memPageStore, recordConverter, FilterCompat.get(column("Name.Url", applyFunctionToString(new StringEndsWithAPredicate()))));    readOne(recordReader, "r2 filtered out", r1);            recordReader = (RecordReaderImplementation<Group>) columnIO.getRecordReader(memPageStore, recordConverter, FilterCompat.get(column("Name.Url", equalTo("http://B"))));    List<Group> all = readAll(recordReader);    assertEquals("There should be no matching records: " + all, 0, all.size());        recordReader = (RecordReaderImplementation<Group>) columnIO.getRecordReader(memPageStore, recordConverter, FilterCompat.get(column("Name.Url", equalTo("http://C"))));    readOne(recordReader, "r1 filtered out", r2);}
0
public void testPaged()
{    MessageColumnIO columnIO = new ColumnIOFactory(true).getColumnIO(schema);    MemPageStore memPageStore = writeTestRecords(columnIO, 6);    RecordMaterializer<Group> recordConverter = new GroupRecordConverter(schema);    RecordReaderImplementation<Group> recordReader = (RecordReaderImplementation<Group>) columnIO.getRecordReader(memPageStore, recordConverter, FilterCompat.get(page(4, 4)));    List<Group> all = readAll(recordReader);    assertEquals("expecting records " + all, 4, all.size());    for (int i = 0; i < all.size(); i++) {        assertEquals("expecting record", (i % 2 == 0 ? r2 : r1).toString(), all.get(i).toString());    }}
0
public void testFilteredAndPaged()
{    MessageColumnIO columnIO = new ColumnIOFactory(true).getColumnIO(schema);    MemPageStore memPageStore = writeTestRecords(columnIO, 8);    RecordMaterializer<Group> recordConverter = new GroupRecordConverter(schema);    RecordReaderImplementation<Group> recordReader = (RecordReaderImplementation<Group>) columnIO.getRecordReader(memPageStore, recordConverter, FilterCompat.get(and(column("DocId", equalTo(10l)), page(2, 4))));    List<Group> all = readAll(recordReader);    assertEquals("expecting 4 records " + all, 4, all.size());    for (int i = 0; i < all.size(); i++) {        assertEquals("expecting record1", r1.toString(), all.get(i).toString());    }}
0
public void testFilteredOrPaged()
{    MessageColumnIO columnIO = new ColumnIOFactory(true).getColumnIO(schema);    MemPageStore memPageStore = writeTestRecords(columnIO, 8);    RecordMaterializer<Group> recordConverter = new GroupRecordConverter(schema);    RecordReaderImplementation<Group> recordReader = (RecordReaderImplementation<Group>) columnIO.getRecordReader(memPageStore, recordConverter, FilterCompat.get(or(column("DocId", equalTo(10l)), column("DocId", equalTo(20l)))));    List<Group> all = readAll(recordReader);    assertEquals("expecting 8 records " + all, 16, all.size());    for (int i = 0; i < all.size() / 2; i++) {        assertEquals("expecting record1", r1.toString(), all.get(2 * i).toString());        assertEquals("expecting record2", r2.toString(), all.get(2 * i + 1).toString());    }}
0
public void testFilteredNotPaged()
{    MessageColumnIO columnIO = new ColumnIOFactory(true).getColumnIO(schema);    MemPageStore memPageStore = writeTestRecords(columnIO, 8);    RecordMaterializer<Group> recordConverter = new GroupRecordConverter(schema);    RecordReaderImplementation<Group> recordReader = (RecordReaderImplementation<Group>) columnIO.getRecordReader(memPageStore, recordConverter, FilterCompat.get(not(column("DocId", equalTo(10l)))));    List<Group> all = readAll(recordReader);    assertEquals("expecting 8 records " + all, 8, all.size());    for (int i = 0; i < all.size(); i++) {        assertEquals("expecting record2", r2.toString(), all.get(i).toString());    }}
0
private MemPageStore writeTestRecords(MessageColumnIO columnIO, int number)
{    MemPageStore memPageStore = new MemPageStore(number * 2);    ColumnWriteStoreV1 columns = new ColumnWriteStoreV1(memPageStore, ParquetProperties.builder().withPageSize(800).withDictionaryEncoding(false).build());    RecordConsumer recordWriter = columnIO.getRecordWriter(columns);    GroupWriter groupWriter = new GroupWriter(recordWriter, schema);    for (int i = 0; i < number; i++) {        groupWriter.write(r1);        groupWriter.write(r2);    }    recordWriter.flush();    columns.flush();    return memPageStore;}
0
public void testPaperExample()
{    String example = "message Document {\n" + "  required int64 DocId;\n" + "  optional group Links {\n" + "    repeated int64 Backward;\n" + "    repeated int64 Forward; }\n" + "  repeated group Name {\n" + "    repeated group Language {\n" + "      required binary Code;\n" + "      required binary Country; }\n" + "    optional binary Url; }}";    MessageType parsed = parseMessageType(example);    MessageType manuallyMade = new MessageType("Document", new PrimitiveType(REQUIRED, INT64, "DocId"), new GroupType(OPTIONAL, "Links", new PrimitiveType(REPEATED, INT64, "Backward"), new PrimitiveType(REPEATED, INT64, "Forward")), new GroupType(REPEATED, "Name", new GroupType(REPEATED, "Language", new PrimitiveType(REQUIRED, BINARY, "Code"), new PrimitiveType(REQUIRED, BINARY, "Country")), new PrimitiveType(OPTIONAL, BINARY, "Url")));    assertEquals(manuallyMade, parsed);    MessageType parsedThenReparsed = parseMessageType(parsed.toString());    assertEquals(manuallyMade, parsedThenReparsed);    parsed = parseMessageType("message m { required group a {required binary b;} required group c { required int64 d; }}");    manuallyMade = new MessageType("m", new GroupType(REQUIRED, "a", new PrimitiveType(REQUIRED, BINARY, "b")), new GroupType(REQUIRED, "c", new PrimitiveType(REQUIRED, INT64, "d")));    assertEquals(manuallyMade, parsed);    parsedThenReparsed = parseMessageType(parsed.toString());    assertEquals(manuallyMade, parsedThenReparsed);}
0
public void testEachPrimitiveType()
{    MessageTypeBuilder builder = buildMessage();    StringBuilder schema = new StringBuilder();    schema.append("message EachType {\n");    for (PrimitiveTypeName type : PrimitiveTypeName.values()) {                if (type == FIXED_LEN_BYTE_ARRAY) {            schema.append("  required fixed_len_byte_array(3) fixed_;");            builder.required(FIXED_LEN_BYTE_ARRAY).length(3).named("fixed_");        } else {            schema.append("  required ").append(type).append(" ").append(type).append("_;\n");            builder.required(type).named(type.toString() + "_");        }    }    schema.append("}\n");    MessageType expected = builder.named("EachType");    MessageType parsed = parseMessageType(schema.toString());    assertEquals(expected, parsed);    MessageType reparsed = parseMessageType(parsed.toString());    assertEquals(expected, reparsed);}
0
public void testSTRINGAnnotation()
{    String message = "message StringMessage {\n" + "  required binary string (STRING);\n" + "}\n";    MessageType parsed = parseMessageType(message);    MessageType expected = buildMessage().required(BINARY).as(stringType()).named("string").named("StringMessage");    assertEquals(expected, parsed);    MessageType reparsed = parseMessageType(parsed.toString());    assertEquals(expected, reparsed);}
0
public void testUTF8Annotation()
{    String message = "message StringMessage {\n" + "  required binary string (UTF8);\n" + "}\n";    MessageType parsed = parseMessageType(message);    MessageType expected = buildMessage().required(BINARY).as(UTF8).named("string").named("StringMessage");    assertEquals(expected, parsed);    MessageType reparsed = parseMessageType(parsed.toString());    assertEquals(expected, reparsed);}
0
public void testIDs()
{    String message = "message Message {\n" + "  required binary string (UTF8) = 6;\n" + "  required int32 i=1;\n" + "  required binary s2= 3;\n" + "  required binary s3 =4;\n" + "}\n";    MessageType parsed = parseMessageType(message);    MessageType expected = buildMessage().required(BINARY).as(OriginalType.UTF8).id(6).named("string").required(INT32).id(1).named("i").required(BINARY).id(3).named("s2").required(BINARY).id(4).named("s3").named("Message");    assertEquals(expected, parsed);    MessageType reparsed = parseMessageType(parsed.toString());    assertEquals(expected, reparsed);}
0
public void testMAPAnnotations()
{        String message = "message Message {\n" + "  optional group aMap (MAP) {\n" + "    repeated group map (MAP_KEY_VALUE) {\n" + "      required binary key (UTF8);\n" + "      required int32 value;\n" + "    }\n" + "  }\n" + "}\n";    MessageType parsed = parseMessageType(message);    MessageType expected = buildMessage().optionalGroup().as(MAP).repeatedGroup().as(MAP_KEY_VALUE).required(BINARY).as(UTF8).named("key").required(INT32).named("value").named("map").named("aMap").named("Message");    assertEquals(expected, parsed);    MessageType reparsed = parseMessageType(parsed.toString());    assertEquals(expected, reparsed);}
0
public void testLISTAnnotation()
{        String message = "message Message {\n" + "  required group aList (LIST) {\n" + "    repeated binary string (UTF8);\n" + "  }\n" + "}\n";    MessageType parsed = parseMessageType(message);    MessageType expected = buildMessage().requiredGroup().as(LIST).repeated(BINARY).as(UTF8).named("string").named("aList").named("Message");    assertEquals(expected, parsed);    MessageType reparsed = parseMessageType(parsed.toString());    assertEquals(expected, reparsed);}
0
public void testDecimalFixedAnnotation()
{    String message = "message DecimalMessage {\n" + "  required FIXED_LEN_BYTE_ARRAY(4) aDecimal (DECIMAL(9,2));\n" + "}\n";    MessageType parsed = parseMessageType(message);    MessageType expected = buildMessage().required(FIXED_LEN_BYTE_ARRAY).length(4).as(DECIMAL).precision(9).scale(2).named("aDecimal").named("DecimalMessage");    assertEquals(expected, parsed);    MessageType reparsed = parseMessageType(parsed.toString());    assertEquals(expected, reparsed);}
0
public void testDecimalBinaryAnnotation()
{    String message = "message DecimalMessage {\n" + "  required binary aDecimal (DECIMAL(9,2));\n" + "}\n";    MessageType parsed = parseMessageType(message);    MessageType expected = buildMessage().required(BINARY).as(DECIMAL).precision(9).scale(2).named("aDecimal").named("DecimalMessage");    assertEquals(expected, parsed);    MessageType reparsed = parseMessageType(parsed.toString());    assertEquals(expected, reparsed);}
0
public void testTimeAnnotations()
{    String message = "message TimeMessage {" + "  required int32 date (DATE);" + "  required int32 time (TIME_MILLIS);" + "  required int64 timestamp (TIMESTAMP_MILLIS);" + "  required FIXED_LEN_BYTE_ARRAY(12) interval (INTERVAL);" + "  required int32 newTime (TIME(MILLIS,true));" + "  required int64 nanoTime (TIME(NANOS,true));" + "  required int64 newTimestamp (TIMESTAMP(MILLIS,false));" + "  required int64 nanoTimestamp (TIMESTAMP(NANOS,false));" + "}\n";    MessageType parsed = MessageTypeParser.parseMessageType(message);    MessageType expected = Types.buildMessage().required(INT32).as(DATE).named("date").required(INT32).as(TIME_MILLIS).named("time").required(INT64).as(TIMESTAMP_MILLIS).named("timestamp").required(FIXED_LEN_BYTE_ARRAY).length(12).as(INTERVAL).named("interval").required(INT32).as(timeType(true, MILLIS)).named("newTime").required(INT64).as(timeType(true, NANOS)).named("nanoTime").required(INT64).as(timestampType(false, MILLIS)).named("newTimestamp").required(INT64).as(timestampType(false, NANOS)).named("nanoTimestamp").named("TimeMessage");    assertEquals(expected, parsed);    MessageType reparsed = MessageTypeParser.parseMessageType(parsed.toString());    assertEquals(expected, reparsed);}
0
public void testIntAnnotations()
{    String message = "message IntMessage {" + "  required int32 i8 (INT_8);" + "  required int32 i16 (INT_16);" + "  required int32 i32 (INT_32);" + "  required int64 i64 (INT_64);" + "  required int32 u8 (UINT_8);" + "  required int32 u16 (UINT_16);" + "  required int32 u32 (UINT_32);" + "  required int64 u64 (UINT_64);" + "}\n";    MessageType parsed = MessageTypeParser.parseMessageType(message);    MessageType expected = Types.buildMessage().required(INT32).as(INT_8).named("i8").required(INT32).as(INT_16).named("i16").required(INT32).as(INT_32).named("i32").required(INT64).as(INT_64).named("i64").required(INT32).as(UINT_8).named("u8").required(INT32).as(UINT_16).named("u16").required(INT32).as(UINT_32).named("u32").required(INT64).as(UINT_64).named("u64").named("IntMessage");    assertEquals(expected, parsed);    MessageType reparsed = MessageTypeParser.parseMessageType(parsed.toString());    assertEquals(expected, reparsed);}
0
public void testIntegerAnnotations()
{    String message = "message IntMessage {" + "  required int32 i8 (INTEGER(8,true));" + "  required int32 i16 (INTEGER(16,true));" + "  required int32 i32 (INTEGER(32,true));" + "  required int64 i64 (INTEGER(64,true));" + "  required int32 u8 (INTEGER(8,false));" + "  required int32 u16 (INTEGER(16,false));" + "  required int32 u32 (INTEGER(32,false));" + "  required int64 u64 (INTEGER(64,false));" + "}\n";    MessageType parsed = MessageTypeParser.parseMessageType(message);    MessageType expected = Types.buildMessage().required(INT32).as(intType(8, true)).named("i8").required(INT32).as(intType(16, true)).named("i16").required(INT32).as(intType(32, true)).named("i32").required(INT64).as(intType(64, true)).named("i64").required(INT32).as(intType(8, false)).named("u8").required(INT32).as(intType(16, false)).named("u16").required(INT32).as(intType(32, false)).named("u32").required(INT64).as(intType(64, false)).named("u64").named("IntMessage");    assertEquals(expected, parsed);    MessageType reparsed = MessageTypeParser.parseMessageType(parsed.toString());    assertEquals(expected, reparsed);}
0
public void testEmbeddedAnnotations()
{    String message = "message EmbeddedMessage {" + "  required binary json (JSON);" + "  required binary bson (BSON);" + "}\n";    MessageType parsed = MessageTypeParser.parseMessageType(message);    MessageType expected = Types.buildMessage().required(BINARY).as(JSON).named("json").required(BINARY).as(BSON).named("bson").named("EmbeddedMessage");    assertEquals(expected, parsed);    MessageType reparsed = MessageTypeParser.parseMessageType(parsed.toString());    assertEquals(expected, reparsed);}
0
public void test() throws Exception
{    MessageType schema = MessageTypeParser.parseMessageType(Paper.schema.toString());    assertEquals(Paper.schema, schema);    assertEquals(schema.toString(), Paper.schema.toString());}
0
public void testNestedTypes()
{    MessageType schema = MessageTypeParser.parseMessageType(Paper.schema.toString());    Type type = schema.getType("Links", "Backward");    assertEquals(PrimitiveTypeName.INT64, type.asPrimitiveType().getPrimitiveTypeName());    assertEquals(0, schema.getMaxRepetitionLevel("DocId"));    assertEquals(1, schema.getMaxRepetitionLevel("Name"));    assertEquals(2, schema.getMaxRepetitionLevel("Name", "Language"));    assertEquals(0, schema.getMaxDefinitionLevel("DocId"));    assertEquals(1, schema.getMaxDefinitionLevel("Links"));    assertEquals(2, schema.getMaxDefinitionLevel("Links", "Backward"));}
0
public void testMergeSchema()
{    MessageType t1 = new MessageType("root1", new PrimitiveType(REPEATED, BINARY, "a"), new PrimitiveType(OPTIONAL, BINARY, "b"));    MessageType t2 = new MessageType("root2", new PrimitiveType(REQUIRED, BINARY, "c"));    assertEquals(t1.union(t2), new MessageType("root1", new PrimitiveType(REPEATED, BINARY, "a"), new PrimitiveType(OPTIONAL, BINARY, "b"), new PrimitiveType(REQUIRED, BINARY, "c")));    assertEquals(t2.union(t1), new MessageType("root2", new PrimitiveType(REQUIRED, BINARY, "c"), new PrimitiveType(REPEATED, BINARY, "a"), new PrimitiveType(OPTIONAL, BINARY, "b")));    MessageType t3 = new MessageType("root1", new PrimitiveType(OPTIONAL, BINARY, "a"));    MessageType t4 = new MessageType("root2", new PrimitiveType(REQUIRED, BINARY, "a"));    assertEquals(t3.union(t4), new MessageType("root1", new PrimitiveType(OPTIONAL, BINARY, "a")));    assertEquals(t4.union(t3), new MessageType("root2", new PrimitiveType(OPTIONAL, BINARY, "a")));    MessageType t5 = new MessageType("root1", new GroupType(REQUIRED, "g1", new PrimitiveType(OPTIONAL, BINARY, "a")), new GroupType(REQUIRED, "g2", new PrimitiveType(OPTIONAL, BINARY, "b")));    MessageType t6 = new MessageType("root1", new GroupType(REQUIRED, "g1", new PrimitiveType(OPTIONAL, BINARY, "a")), new GroupType(REQUIRED, "g2", new GroupType(REQUIRED, "g3", new PrimitiveType(OPTIONAL, BINARY, "c")), new PrimitiveType(OPTIONAL, BINARY, "b")));    assertEquals(t5.union(t6), new MessageType("root1", new GroupType(REQUIRED, "g1", new PrimitiveType(OPTIONAL, BINARY, "a")), new GroupType(REQUIRED, "g2", new PrimitiveType(OPTIONAL, BINARY, "b"), new GroupType(REQUIRED, "g3", new PrimitiveType(OPTIONAL, BINARY, "c")))));    MessageType t7 = new MessageType("root1", new PrimitiveType(OPTIONAL, BINARY, "a"));    MessageType t8 = new MessageType("root2", new PrimitiveType(OPTIONAL, INT32, "a"));    try {        t7.union(t8);        fail("moving from BINARY to INT32");    } catch (IncompatibleSchemaModificationException e) {        assertEquals("can not merge type optional int32 a into optional binary a", e.getMessage());    }    MessageType t9 = Types.buildMessage().addField(Types.optional(BINARY).as(OriginalType.UTF8).named("a")).named("root1");    MessageType t10 = Types.buildMessage().addField(Types.optional(BINARY).named("a")).named("root1");    assertEquals(t9.union(t9), t9);    try {        t9.union(t10);        fail("moving from BINARY (UTF8) to BINARY");    } catch (IncompatibleSchemaModificationException e) {        assertEquals("cannot merge logical type null into STRING", e.getMessage());    }    MessageType t11 = Types.buildMessage().addField(Types.optional(FIXED_LEN_BYTE_ARRAY).length(10).named("a")).named("root1");    MessageType t12 = Types.buildMessage().addField(Types.optional(FIXED_LEN_BYTE_ARRAY).length(20).named("a")).named("root2");    try {        t11.union(t12);        fail("moving from FIXED_LEN_BYTE_ARRAY(10) to FIXED_LEN_BYTE_ARRAY(20)");    } catch (IncompatibleSchemaModificationException e) {        assertEquals("can not merge type optional fixed_len_byte_array(20) a into optional fixed_len_byte_array(10) a", e.getMessage());    }}
0
public void testMergeSchemaWithOriginalType() throws Exception
{    MessageType t5 = new MessageType("root1", new GroupType(REQUIRED, "g1", LIST, new PrimitiveType(OPTIONAL, BINARY, "a")), new GroupType(REQUIRED, "g2", new PrimitiveType(OPTIONAL, BINARY, "b")));    MessageType t6 = new MessageType("root1", new GroupType(REQUIRED, "g1", LIST, new PrimitiveType(OPTIONAL, BINARY, "a")), new GroupType(REQUIRED, "g2", LIST, new GroupType(REQUIRED, "g3", new PrimitiveType(OPTIONAL, BINARY, "c")), new PrimitiveType(OPTIONAL, BINARY, "b")));    assertEquals(new MessageType("root1", new GroupType(REQUIRED, "g1", LIST, new PrimitiveType(OPTIONAL, BINARY, "a")), new GroupType(REQUIRED, "g2", LIST, new PrimitiveType(OPTIONAL, BINARY, "b"), new GroupType(REQUIRED, "g3", new PrimitiveType(OPTIONAL, BINARY, "c")))), t5.union(t6));}
0
public void testMergeSchemaWithColumnOrder()
{    MessageType m1 = Types.buildMessage().addFields(Types.requiredList().element(Types.optional(BINARY).columnOrder(ColumnOrder.undefined()).named("a")).named("g"), Types.optional(INT96).named("b")).named("root");    MessageType m2 = Types.buildMessage().addFields(Types.requiredList().element(Types.optional(BINARY).columnOrder(ColumnOrder.undefined()).named("a")).named("g"), Types.optional(BINARY).named("c")).named("root");    MessageType m3 = Types.buildMessage().addFields(Types.requiredList().element(Types.optional(BINARY).named("a")).named("g")).named("root");    assertEquals(Types.buildMessage().addFields(Types.requiredList().element(Types.optional(BINARY).named("a")).named("g"), Types.optional(INT96).named("b"), Types.optional(BINARY).named("c")).named("root"), m1.union(m2));    try {        m1.union(m3);        fail("An IncompatibleSchemaModificationException should have been thrown");    } catch (Exception e) {        assertTrue("The thrown exception should have been IncompatibleSchemaModificationException but was " + e.getClass(), e instanceof IncompatibleSchemaModificationException);        assertEquals("can not merge type optional binary a with column order TYPE_DEFINED_ORDER into optional binary a with column order UNDEFINED", e.getMessage());    }}
0
public void testIDs() throws Exception
{    MessageType schema = new MessageType("test", new PrimitiveType(REQUIRED, BINARY, "foo").withId(4), new GroupType(REQUIRED, "bar", new PrimitiveType(REQUIRED, BINARY, "baz").withId(3)).withId(8));    MessageType schema2 = MessageTypeParser.parseMessageType(schema.toString());    assertEquals(schema, schema2);    assertEquals(schema.toString(), schema2.toString());}
0
public void testBooleanComparator()
{    Boolean[] valuesInAscendingOrder = { null, false, true };    for (int i = 0; i < valuesInAscendingOrder.length; ++i) {        for (int j = 0; j < valuesInAscendingOrder.length; ++j) {            Boolean vi = valuesInAscendingOrder[i];            Boolean vj = valuesInAscendingOrder[j];            int exp = i - j;            assertSignumEquals(vi, vj, exp, BOOLEAN_COMPARATOR.compare(vi, vj));            if (vi != null && vj != null) {                assertSignumEquals(vi, vj, exp, BOOLEAN_COMPARATOR.compare(vi.booleanValue(), vj.booleanValue()));            }        }    }    checkThrowingUnsupportedException(BOOLEAN_COMPARATOR, Boolean.TYPE);}
0
public void testSignedInt32Comparator()
{    testInt32Comparator(SIGNED_INT32_COMPARATOR, null, Integer.MIN_VALUE, -12345, -1, 0, 1, 12345, Integer.MAX_VALUE);}
0
public void testUnsignedInt32Comparator()
{    testInt32Comparator(UNSIGNED_INT32_COMPARATOR, null,     0,     1,     12345,     Integer.MAX_VALUE,     Integer.MIN_VALUE,     -12345,     -1);}
0
private void testInt32Comparator(PrimitiveComparator<Integer> comparator, Integer... valuesInAscendingOrder)
{    for (int i = 0; i < valuesInAscendingOrder.length; ++i) {        for (int j = 0; j < valuesInAscendingOrder.length; ++j) {            Integer vi = valuesInAscendingOrder[i];            Integer vj = valuesInAscendingOrder[j];            int exp = i - j;            assertSignumEquals(vi, vj, exp, comparator.compare(vi, vj));            if (vi != null && vj != null) {                assertSignumEquals(vi, vj, exp, comparator.compare(vi.intValue(), vj.intValue()));            }        }    }    checkThrowingUnsupportedException(comparator, Integer.TYPE);}
0
public void testSignedInt64Comparator()
{    testInt64Comparator(SIGNED_INT64_COMPARATOR, null, Long.MIN_VALUE, -12345678901L, -1L, 0L, 1L, 12345678901L, Long.MAX_VALUE);}
0
public void testUnsignedInt64Comparator()
{    testInt64Comparator(UNSIGNED_INT64_COMPARATOR, null,     0L,     1L,     12345678901L,     Long.MAX_VALUE,     Long.MIN_VALUE,     -12345678901L,     -1L);}
0
private void testInt64Comparator(PrimitiveComparator<Long> comparator, Long... valuesInAscendingOrder)
{    for (int i = 0; i < valuesInAscendingOrder.length; ++i) {        for (int j = 0; j < valuesInAscendingOrder.length; ++j) {            Long vi = valuesInAscendingOrder[i];            Long vj = valuesInAscendingOrder[j];            int exp = i - j;            assertSignumEquals(vi, vj, exp, comparator.compare(vi, vj));            if (vi != null && vj != null) {                assertSignumEquals(vi, vj, exp, comparator.compare(vi.longValue(), vj.longValue()));            }        }    }    checkThrowingUnsupportedException(comparator, Long.TYPE);}
0
public void testFloatComparator()
{    Float[] valuesInAscendingOrder = { null, Float.NEGATIVE_INFINITY, -Float.MAX_VALUE, -1234.5678F, -Float.MIN_VALUE, 0.0F, Float.MIN_VALUE, 1234.5678F, Float.MAX_VALUE, Float.POSITIVE_INFINITY };    for (int i = 0; i < valuesInAscendingOrder.length; ++i) {        for (int j = 0; j < valuesInAscendingOrder.length; ++j) {            Float vi = valuesInAscendingOrder[i];            Float vj = valuesInAscendingOrder[j];            int exp = i - j;            assertSignumEquals(vi, vj, exp, FLOAT_COMPARATOR.compare(vi, vj));            if (vi != null && vj != null) {                assertSignumEquals(vi, vj, exp, FLOAT_COMPARATOR.compare(vi.floatValue(), vj.floatValue()));            }        }    }    checkThrowingUnsupportedException(FLOAT_COMPARATOR, Float.TYPE);}
0
public void testDoubleComparator()
{    Double[] valuesInAscendingOrder = { null, Double.NEGATIVE_INFINITY, -Double.MAX_VALUE, -123456.7890123456789, -Double.MIN_VALUE, 0.0, Double.MIN_VALUE, 123456.7890123456789, Double.MAX_VALUE, Double.POSITIVE_INFINITY };    for (int i = 0; i < valuesInAscendingOrder.length; ++i) {        for (int j = 0; j < valuesInAscendingOrder.length; ++j) {            Double vi = valuesInAscendingOrder[i];            Double vj = valuesInAscendingOrder[j];            int exp = i - j;            assertSignumEquals(vi, vj, exp, DOUBLE_COMPARATOR.compare(vi, vj));            if (vi != null && vj != null) {                assertSignumEquals(vi, vj, exp, DOUBLE_COMPARATOR.compare(vi.doubleValue(), vj.doubleValue()));            }        }    }    checkThrowingUnsupportedException(DOUBLE_COMPARATOR, Double.TYPE);}
0
public void testLexicographicalBinaryComparator()
{    testObjectComparator(UNSIGNED_LEXICOGRAPHICAL_BINARY_COMPARATOR, null,     Binary.fromConstantByteArray(new byte[0]),     Binary.fromConstantByteArray(new byte[] { 127, 127, 0, 127 }, 2, 1),     Binary.fromCharSequence("aaa"),     Binary.fromString("aaaa"),     Binary.fromReusedByteArray("aaab".getBytes()),     Binary.fromReusedByteArray("azzza".getBytes(), 1, 3),     Binary.fromReusedByteBuffer(ByteBuffer.wrap("zzzzzz".getBytes())),     Binary.fromReusedByteBuffer(ByteBuffer.wrap("aazzzzzzaa".getBytes(), 2, 7)),     Binary.fromConstantByteBuffer(ByteBuffer.wrap(new byte[] { -128, -128, -128 })),     Binary.fromConstantByteBuffer(ByteBuffer.wrap(new byte[] { -128, -128, -1 }, 1, 2)));}
0
public void testBinaryAsSignedIntegerComparator()
{    testObjectComparator(BINARY_AS_SIGNED_INTEGER_COMPARATOR, null, Binary.fromConstantByteArray(new BigInteger("-9999999999999999999999999999999999999999").toByteArray()), Binary.fromReusedByteArray(new BigInteger("-9999999999999999999999999999999999999998").toByteArray()), Binary.fromConstantByteArray(BigInteger.valueOf(Long.MIN_VALUE).subtract(BigInteger.ONE).toByteArray()), Binary.fromConstantByteArray(BigInteger.valueOf(Long.MIN_VALUE).toByteArray()), Binary.fromConstantByteArray(BigInteger.valueOf(Long.MIN_VALUE).add(BigInteger.ONE).toByteArray()), Binary.fromReusedByteArray(new byte[] { (byte) 0xFF, (byte) 0xFF, (byte) 0xFF, -2 }, 1, 3), Binary.fromReusedByteArray(new BigInteger("-1").toByteArray()), Binary.fromConstantByteBuffer(ByteBuffer.wrap(new BigInteger("0").toByteArray())), Binary.fromReusedByteBuffer(ByteBuffer.wrap(new byte[] { 0, 0, 0, 1 })), Binary.fromConstantByteBuffer(ByteBuffer.wrap(new byte[] { 0, 0, 0, 2 }), 2, 2), Binary.fromConstantByteBuffer(ByteBuffer.wrap(BigInteger.valueOf(Long.MAX_VALUE).subtract(BigInteger.ONE).toByteArray())), Binary.fromConstantByteBuffer(ByteBuffer.wrap(BigInteger.valueOf(Long.MAX_VALUE).toByteArray())), Binary.fromConstantByteBuffer(ByteBuffer.wrap(BigInteger.valueOf(Long.MAX_VALUE).add(BigInteger.ONE).toByteArray())), Binary.fromConstantByteBuffer(ByteBuffer.wrap(new BigInteger("999999999999999999999999999999999999999").toByteArray())), Binary.fromReusedByteBuffer(ByteBuffer.wrap(new BigInteger("9999999999999999999999999999999999999998").toByteArray())), Binary.fromConstantByteBuffer(ByteBuffer.wrap(new BigInteger("9999999999999999999999999999999999999999").toByteArray())));}
0
public void testBinaryAsSignedIntegerComparatorWithEquals()
{    List<Binary> valuesToCompare = new ArrayList<>();    valuesToCompare.add(Binary.fromConstantByteBuffer(ByteBuffer.wrap(new byte[] { 0, 0, -108 })));    valuesToCompare.add(Binary.fromConstantByteBuffer(ByteBuffer.wrap(new byte[] { 0, 0, 0, 0, 0, -108 })));    valuesToCompare.add(Binary.fromConstantByteBuffer(ByteBuffer.wrap(new byte[] { 0, 0, 0, -108 })));    valuesToCompare.add(Binary.fromConstantByteBuffer(ByteBuffer.wrap(new byte[] { 0, 0, 0, 0, -108 })));    valuesToCompare.add(Binary.fromConstantByteBuffer(ByteBuffer.wrap(new byte[] { 0, -108 })));    for (Binary v1 : valuesToCompare) {        for (Binary v2 : valuesToCompare) {            assertEquals(String.format("Wrong result of comparison %s and %s", v1, v2), 0, BINARY_AS_SIGNED_INTEGER_COMPARATOR.compare(v1, v2));        }    }}
0
private void testObjectComparator(PrimitiveComparator<T> comparator, T... valuesInAscendingOrder)
{    for (int i = 0; i < valuesInAscendingOrder.length; ++i) {        for (int j = 0; j < valuesInAscendingOrder.length; ++j) {            T vi = valuesInAscendingOrder[i];            T vj = valuesInAscendingOrder[j];            int exp = i - j;            assertSignumEquals(vi, vj, exp, comparator.compare(vi, vj));        }    }    checkThrowingUnsupportedException(comparator, null);}
0
private void assertSignumEquals(T v1, T v2, int expected, int actual)
{    String sign = expected < 0 ? " < " : expected > 0 ? " > " : " = ";    assertEquals("expected: " + v1 + sign + v2, signum(expected), signum(actual));}
0
private int signum(int i)
{    return i < 0 ? -1 : i > 0 ? 1 : 0;}
0
private void checkThrowingUnsupportedException(PrimitiveComparator<?> comparator, Class<?> exclude)
{    if (Integer.TYPE != exclude) {        try {            comparator.compare(0, 0);            fail("An UnsupportedOperationException should have been thrown");        } catch (UnsupportedOperationException e) {        }    }    if (Long.TYPE != exclude) {        try {            comparator.compare(0L, 0L);            fail("An UnsupportedOperationException should have been thrown");        } catch (UnsupportedOperationException e) {        }    }    if (Float.TYPE != exclude) {        try {            comparator.compare(0.0F, 0.0F);            fail("An UnsupportedOperationException should have been thrown");        } catch (UnsupportedOperationException e) {        }    }    if (Double.TYPE != exclude) {        try {            comparator.compare(0.0, 0.0);            fail("An UnsupportedOperationException should have been thrown");        } catch (UnsupportedOperationException e) {        }    }    if (Boolean.TYPE != exclude) {        try {            comparator.compare(false, false);            fail("An UnsupportedOperationException should have been thrown");        } catch (UnsupportedOperationException e) {        }    }}
0
public void testDefaultStringifier()
{    PrimitiveStringifier stringifier = DEFAULT_STRINGIFIER;    assertEquals("true", stringifier.stringify(true));    assertEquals("false", stringifier.stringify(false));    assertEquals("0.0", stringifier.stringify(0.0));    assertEquals("123456.7891234567", stringifier.stringify(123456.7891234567));    assertEquals("-98765.43219876543", stringifier.stringify(-98765.43219876543));    assertEquals("0.0", stringifier.stringify(0.0f));    assertEquals("987.6543", stringifier.stringify(987.6543f));    assertEquals("-123.4567", stringifier.stringify(-123.4567f));    assertEquals("0", stringifier.stringify(0));    assertEquals("1234567890", stringifier.stringify(1234567890));    assertEquals("-987654321", stringifier.stringify(-987654321));    assertEquals("0", stringifier.stringify(0l));    assertEquals("1234567890123456789", stringifier.stringify(1234567890123456789l));    assertEquals("-987654321987654321", stringifier.stringify(-987654321987654321l));    assertEquals("null", stringifier.stringify(null));    assertEquals("0x", stringifier.stringify(Binary.EMPTY));    assertEquals("0x0123456789ABCDEF", stringifier.stringify(Binary.fromConstantByteArray(new byte[] { 0x01, 0x23, 0x45, 0x67, (byte) 0x89, (byte) 0xAB, (byte) 0xCD, (byte) 0xEF })));}
0
public void testUnsignedStringifier()
{    PrimitiveStringifier stringifier = UNSIGNED_STRINGIFIER;    assertEquals("0", stringifier.stringify(0));    assertEquals("2147483647", stringifier.stringify(2147483647));    assertEquals("4294967295", stringifier.stringify(0xFFFFFFFF));    assertEquals("0", stringifier.stringify(0l));    assertEquals("9223372036854775807", stringifier.stringify(9223372036854775807l));    assertEquals("18446744073709551615", stringifier.stringify(0xFFFFFFFFFFFFFFFFl));    checkThrowingUnsupportedException(stringifier, Integer.TYPE, Long.TYPE);}
0
public void testUTF8Stringifier()
{    PrimitiveStringifier stringifier = UTF8_STRINGIFIER;    assertEquals("null", stringifier.stringify(null));    assertEquals("", stringifier.stringify(Binary.EMPTY));    assertEquals("This is a UTF-8 test", stringifier.stringify(Binary.fromString("This is a UTF-8 test")));    assertEquals("これはUTF-8のテストです", stringifier.stringify(Binary.fromConstantByteArray("これはUTF-8のテストです".getBytes(UTF_8))));    checkThrowingUnsupportedException(stringifier, Binary.class);}
0
public void testIntervalStringifier()
{    PrimitiveStringifier stringifier = INTERVAL_STRINGIFIER;    assertEquals("null", stringifier.stringify(null));    assertEquals("<INVALID>", stringifier.stringify(Binary.EMPTY));    assertEquals("<INVALID>", stringifier.stringify(Binary.fromConstantByteArray(new byte[] { 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11 })));    assertEquals("<INVALID>", stringifier.stringify(Binary.fromReusedByteArray(new byte[] { 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13 })));    ByteBuffer buffer = ByteBuffer.allocate(12);    assertEquals("interval(0 months, 0 days, 0 millis)", stringifier.stringify(Binary.fromConstantByteBuffer(buffer)));    buffer.putInt(0x03000000);    buffer.putInt(0x06000000);    buffer.putInt(0x09000000);    buffer.flip();    assertEquals("interval(3 months, 6 days, 9 millis)", stringifier.stringify(Binary.fromConstantByteBuffer(buffer)));    buffer.clear();    buffer.putInt(0xFFFFFFFF);    buffer.putInt(0xFEFFFFFF);    buffer.putInt(0xFDFFFFFF);    buffer.flip();    assertEquals("interval(4294967295 months, 4294967294 days, 4294967293 millis)", stringifier.stringify(Binary.fromReusedByteBuffer(buffer)));    checkThrowingUnsupportedException(stringifier, Binary.class);}
0
public void testDateStringifier()
{    PrimitiveStringifier stringifier = DATE_STRINGIFIER;    assertEquals("1970-01-01", stringifier.stringify(0));    Calendar cal = Calendar.getInstance(UTC);    cal.clear();    cal.set(2017, Calendar.DECEMBER, 14);    assertEquals("2017-12-14", stringifier.stringify((int) MILLISECONDS.toDays(cal.getTimeInMillis())));    cal.clear();    cal.set(1583, Calendar.AUGUST, 3);    assertEquals("1583-08-03", stringifier.stringify((int) MILLISECONDS.toDays(cal.getTimeInMillis())));    checkThrowingUnsupportedException(stringifier, Integer.TYPE);}
0
public void testTimestampMillisStringifier()
{    for (PrimitiveStringifier stringifier : asList(TIMESTAMP_MILLIS_STRINGIFIER, TIMESTAMP_MILLIS_UTC_STRINGIFIER)) {        String timezoneAmendment = (stringifier == TIMESTAMP_MILLIS_STRINGIFIER ? "" : "+0000");        assertEquals(withZoneString("1970-01-01T00:00:00.000", timezoneAmendment), stringifier.stringify(0l));        Calendar cal = Calendar.getInstance(UTC);        cal.clear();        cal.set(2017, Calendar.DECEMBER, 15, 10, 9, 54);        cal.set(Calendar.MILLISECOND, 120);        assertEquals(withZoneString("2017-12-15T10:09:54.120", timezoneAmendment), stringifier.stringify(cal.getTimeInMillis()));        cal.clear();        cal.set(1948, Calendar.NOVEMBER, 23, 20, 19, 1);        cal.set(Calendar.MILLISECOND, 9);        assertEquals(withZoneString("1948-11-23T20:19:01.009", timezoneAmendment), stringifier.stringify(cal.getTimeInMillis()));        checkThrowingUnsupportedException(stringifier, Long.TYPE);    }}
0
public void testTimestampMicrosStringifier()
{    for (PrimitiveStringifier stringifier : asList(TIMESTAMP_MICROS_STRINGIFIER, TIMESTAMP_MICROS_UTC_STRINGIFIER)) {        String timezoneAmendment = (stringifier == TIMESTAMP_MICROS_STRINGIFIER ? "" : "+0000");        assertEquals(withZoneString("1970-01-01T00:00:00.000000", timezoneAmendment), stringifier.stringify(0l));        Calendar cal = Calendar.getInstance(UTC);        cal.clear();        cal.set(2053, Calendar.JULY, 10, 22, 13, 24);        cal.set(Calendar.MILLISECOND, 84);        long micros = cal.getTimeInMillis() * 1000 + 900;        assertEquals(withZoneString("2053-07-10T22:13:24.084900", timezoneAmendment), stringifier.stringify(micros));        cal.clear();        cal.set(1848, Calendar.MARCH, 15, 9, 23, 59);        cal.set(Calendar.MILLISECOND, 765);        micros = cal.getTimeInMillis() * 1000 - 1;        assertEquals(withZoneString("1848-03-15T09:23:59.764999", timezoneAmendment), stringifier.stringify(micros));        checkThrowingUnsupportedException(stringifier, Long.TYPE);    }}
0
public void testTimestampNanosStringifier()
{    for (PrimitiveStringifier stringifier : asList(TIMESTAMP_NANOS_STRINGIFIER, TIMESTAMP_NANOS_UTC_STRINGIFIER)) {        String timezoneAmendment = (stringifier == TIMESTAMP_NANOS_STRINGIFIER ? "" : "+0000");        assertEquals(withZoneString("1970-01-01T00:00:00.000000000", timezoneAmendment), stringifier.stringify(0l));        Calendar cal = Calendar.getInstance(UTC);        cal.clear();        cal.set(2053, Calendar.JULY, 10, 22, 13, 24);        cal.set(Calendar.MILLISECOND, 84);        long nanos = cal.getTimeInMillis() * 1_000_000 + 536;        assertEquals(withZoneString("2053-07-10T22:13:24.084000536", timezoneAmendment), stringifier.stringify(nanos));        cal.clear();        cal.set(1848, Calendar.MARCH, 15, 9, 23, 59);        cal.set(Calendar.MILLISECOND, 765);        nanos = cal.getTimeInMillis() * 1_000_000 - 1;        assertEquals(withZoneString("1848-03-15T09:23:59.764999999", timezoneAmendment), stringifier.stringify(nanos));        checkThrowingUnsupportedException(stringifier, Long.TYPE);    }}
0
public void testTimeStringifier()
{    for (PrimitiveStringifier stringifier : asList(TIME_STRINGIFIER, TIME_UTC_STRINGIFIER)) {        String timezoneAmendment = (stringifier == TIME_STRINGIFIER ? "" : "+0000");        assertEquals(withZoneString("00:00:00.000", timezoneAmendment), stringifier.stringify(0));        assertEquals(withZoneString("00:00:00.000000", timezoneAmendment), stringifier.stringify(0l));        assertEquals(withZoneString("12:34:56.789", timezoneAmendment), stringifier.stringify((int) convert(MILLISECONDS, 12, 34, 56, 789)));        assertEquals(withZoneString("12:34:56.789012", timezoneAmendment), stringifier.stringify(convert(MICROSECONDS, 12, 34, 56, 789012)));        assertEquals(withZoneString("-12:34:56.789", timezoneAmendment), stringifier.stringify((int) convert(MILLISECONDS, -12, -34, -56, -789)));        assertEquals(withZoneString("-12:34:56.789012", timezoneAmendment), stringifier.stringify(convert(MICROSECONDS, -12, -34, -56, -789012)));        assertEquals(withZoneString("123:12:34.567", timezoneAmendment), stringifier.stringify((int) convert(MILLISECONDS, 123, 12, 34, 567)));        assertEquals(withZoneString("12345:12:34.056789", timezoneAmendment), stringifier.stringify(convert(MICROSECONDS, 12345, 12, 34, 56789)));        assertEquals(withZoneString("-123:12:34.567", timezoneAmendment), stringifier.stringify((int) convert(MILLISECONDS, -123, -12, -34, -567)));        assertEquals(withZoneString("-12345:12:34.056789", timezoneAmendment), stringifier.stringify(convert(MICROSECONDS, -12345, -12, -34, -56789)));        checkThrowingUnsupportedException(stringifier, Integer.TYPE, Long.TYPE);    }}
0
public void testTimeNanoStringifier()
{    for (PrimitiveStringifier stringifier : asList(TIME_NANOS_STRINGIFIER, TIME_NANOS_UTC_STRINGIFIER)) {        String timezoneAmendment = (stringifier == TIME_NANOS_STRINGIFIER ? "" : "+0000");        assertEquals(withZoneString("00:00:00.000000000", timezoneAmendment), stringifier.stringify(0l));        assertEquals(withZoneString("12:34:56.789012987", timezoneAmendment), stringifier.stringify(convert(NANOSECONDS, 12, 34, 56, 789012987)));        assertEquals(withZoneString("-12:34:56.000789012", timezoneAmendment), stringifier.stringify(convert(NANOSECONDS, -12, -34, -56, -789012)));        assertEquals(withZoneString("12345:12:34.000056789", timezoneAmendment), stringifier.stringify(convert(NANOSECONDS, 12345, 12, 34, 56789)));        assertEquals(withZoneString("-12345:12:34.000056789", timezoneAmendment), stringifier.stringify(convert(NANOSECONDS, -12345, -12, -34, -56789)));        checkThrowingUnsupportedException(stringifier, Integer.TYPE, Long.TYPE);    }}
0
private String withZoneString(String expected, String zoneString)
{    return expected + zoneString;}
0
private long convert(TimeUnit unit, long hours, long minutes, long seconds, long rest)
{    return unit.convert(hours, HOURS) + unit.convert(minutes, MINUTES) + unit.convert(seconds, SECONDS) + rest;}
0
public void testDecimalStringifier()
{    PrimitiveStringifier stringifier = PrimitiveStringifier.createDecimalStringifier(4);    assertEquals("0.0000", stringifier.stringify(0));    assertEquals("123456.7890", stringifier.stringify(1234567890));    assertEquals("-98765.4321", stringifier.stringify(-987654321));    assertEquals("0.0000", stringifier.stringify(0l));    assertEquals("123456789012345.6789", stringifier.stringify(1234567890123456789l));    assertEquals("-98765432109876.5432", stringifier.stringify(-987654321098765432l));    assertEquals("null", stringifier.stringify(null));    assertEquals("<INVALID>", stringifier.stringify(Binary.EMPTY));    assertEquals("0.0000", stringifier.stringify(Binary.fromReusedByteArray(new byte[] { 0 })));    assertEquals("9876543210987654321098765432109876543210987654.3210", stringifier.stringify(Binary.fromConstantByteArray(new BigInteger("98765432109876543210987654321098765432109876543210").toByteArray())));    assertEquals("-1234567890123456789012345678901234567890123456.7890", stringifier.stringify(Binary.fromConstantByteArray(new BigInteger("-12345678901234567890123456789012345678901234567890").toByteArray())));    checkThrowingUnsupportedException(stringifier, Integer.TYPE, Long.TYPE, Binary.class);}
0
private void checkThrowingUnsupportedException(PrimitiveStringifier stringifier, Class<?>... excludes)
{    Set<Class<?>> set = new HashSet<>(asList(excludes));    if (!set.contains(Integer.TYPE)) {        try {            stringifier.stringify(0);            fail("An UnsupportedOperationException should have been thrown");        } catch (UnsupportedOperationException e) {        }    }    if (!set.contains(Long.TYPE)) {        try {            stringifier.stringify(0l);            fail("An UnsupportedOperationException should have been thrown");        } catch (UnsupportedOperationException e) {        }    }    if (!set.contains(Float.TYPE)) {        try {            stringifier.stringify(0.0f);            fail("An UnsupportedOperationException should have been thrown");        } catch (UnsupportedOperationException e) {        }    }    if (!set.contains(Double.TYPE)) {        try {            stringifier.stringify(0.0);            fail("An UnsupportedOperationException should have been thrown");        } catch (UnsupportedOperationException e) {        }    }    if (!set.contains(Boolean.TYPE)) {        try {            stringifier.stringify(false);            fail("An UnsupportedOperationException should have been thrown");        } catch (UnsupportedOperationException e) {        }    }    if (!set.contains(Binary.class)) {        try {            stringifier.stringify(Binary.EMPTY);            fail("An UnsupportedOperationException should have been thrown");        } catch (UnsupportedOperationException e) {        }    }}
0
public void testLeastRestrictiveRepetition()
{    Type.Repetition REQUIRED = Type.Repetition.REQUIRED;    Type.Repetition OPTIONAL = Type.Repetition.OPTIONAL;    Type.Repetition REPEATED = Type.Repetition.REPEATED;    assertEquals(REPEATED, Type.Repetition.leastRestrictive(REQUIRED, OPTIONAL, REPEATED, REQUIRED, OPTIONAL, REPEATED));    assertEquals(OPTIONAL, Type.Repetition.leastRestrictive(REQUIRED, OPTIONAL, REQUIRED, OPTIONAL));    assertEquals(REQUIRED, Type.Repetition.leastRestrictive(REQUIRED, REQUIRED));}
0
public void testPaperExample()
{    MessageType expected = new MessageType("Document", new PrimitiveType(REQUIRED, INT64, "DocId"), new GroupType(OPTIONAL, "Links", new PrimitiveType(REPEATED, INT64, "Backward"), new PrimitiveType(REPEATED, INT64, "Forward")), new GroupType(REPEATED, "Name", new GroupType(REPEATED, "Language", new PrimitiveType(REQUIRED, BINARY, "Code"), new PrimitiveType(REQUIRED, BINARY, "Country")), new PrimitiveType(OPTIONAL, BINARY, "Url")));    MessageType builderType = Types.buildMessage().required(INT64).named("DocId").optionalGroup().repeated(INT64).named("Backward").repeated(INT64).named("Forward").named("Links").repeatedGroup().repeatedGroup().required(BINARY).named("Code").required(BINARY).named("Country").named("Language").optional(BINARY).named("Url").named("Name").named("Document");    Assert.assertEquals(expected, builderType);}
0
public void testGroupTypeConstruction()
{    PrimitiveType f1 = Types.required(BINARY).as(UTF8).named("f1");    PrimitiveType f2 = Types.required(INT32).named("f2");    PrimitiveType f3 = Types.optional(INT32).named("f3");    String name = "group";    for (Type.Repetition repetition : Type.Repetition.values()) {        GroupType expected = new GroupType(repetition, name, f1, new GroupType(repetition, "g1", f2, f3));        GroupType built = Types.buildGroup(repetition).addField(f1).group(repetition).addFields(f2, f3).named("g1").named(name);        Assert.assertEquals(expected, built);        switch(repetition) {            case REQUIRED:                built = Types.requiredGroup().addField(f1).requiredGroup().addFields(f2, f3).named("g1").named(name);                break;            case OPTIONAL:                built = Types.optionalGroup().addField(f1).optionalGroup().addFields(f2, f3).named("g1").named(name);                break;            case REPEATED:                built = Types.repeatedGroup().addField(f1).repeatedGroup().addFields(f2, f3).named("g1").named(name);                break;        }        Assert.assertEquals(expected, built);    }}
0
public void testPrimitiveTypeConstruction()
{    PrimitiveTypeName[] types = new PrimitiveTypeName[] { BOOLEAN, INT32, INT64, INT96, FLOAT, DOUBLE, BINARY };    for (PrimitiveTypeName type : types) {        String name = type.toString() + "_";        for (Type.Repetition repetition : Type.Repetition.values()) {            PrimitiveType expected = new PrimitiveType(repetition, type, name);            PrimitiveType built = Types.primitive(type, repetition).named(name);            Assert.assertEquals(expected, built);            switch(repetition) {                case REQUIRED:                    built = Types.required(type).named(name);                    break;                case OPTIONAL:                    built = Types.optional(type).named(name);                    break;                case REPEATED:                    built = Types.repeated(type).named(name);                    break;            }            Assert.assertEquals(expected, built);        }    }}
0
public void testFixedTypeConstruction()
{    String name = "fixed_";    int len = 5;    for (Type.Repetition repetition : Type.Repetition.values()) {        PrimitiveType expected = new PrimitiveType(repetition, FIXED_LEN_BYTE_ARRAY, len, name);        PrimitiveType built = Types.primitive(FIXED_LEN_BYTE_ARRAY, repetition).length(len).named(name);        Assert.assertEquals(expected, built);        switch(repetition) {            case REQUIRED:                built = Types.required(FIXED_LEN_BYTE_ARRAY).length(len).named(name);                break;            case OPTIONAL:                built = Types.optional(FIXED_LEN_BYTE_ARRAY).length(len).named(name);                break;            case REPEATED:                built = Types.repeated(FIXED_LEN_BYTE_ARRAY).length(len).named(name);                break;        }        Assert.assertEquals(expected, built);    }}
0
public void testEmptyGroup()
{        Assert.assertEquals("Should not complain about an empty required group", Types.requiredGroup().named("g"), new GroupType(REQUIRED, "g"));    Assert.assertEquals("Should not complain about an empty required group", Types.optionalGroup().named("g"), new GroupType(OPTIONAL, "g"));    Assert.assertEquals("Should not complain about an empty required group", Types.repeatedGroup().named("g"), new GroupType(REPEATED, "g"));}
0
public void testEmptyMessage()
{        Assert.assertEquals("Should not complain about an empty required group", Types.buildMessage().named("m"), new MessageType("m"));}
0
public void testFixedWithoutLength()
{    Types.required(FIXED_LEN_BYTE_ARRAY).named("fixed");}
0
public void testFixedWithLength()
{    PrimitiveType expected = new PrimitiveType(REQUIRED, FIXED_LEN_BYTE_ARRAY, 7, "fixed");    PrimitiveType fixed = Types.required(FIXED_LEN_BYTE_ARRAY).length(7).named("fixed");    Assert.assertEquals(expected, fixed);}
0
public void testFixedLengthEquals()
{    Type f4 = Types.required(FIXED_LEN_BYTE_ARRAY).length(4).named("f4");    Type f8 = Types.required(FIXED_LEN_BYTE_ARRAY).length(8).named("f8");    Assert.assertFalse("Types with different lengths should not be equal", f4.equals(f8));}
0
public void testDecimalAnnotation()
{        MessageType expected = new MessageType("DecimalMessage", new PrimitiveType(REQUIRED, INT32, 0, "aDecimal", DECIMAL, new DecimalMetadata(9, 2), null));    MessageType builderType = Types.buildMessage().required(INT32).as(DECIMAL).precision(9).scale(2).named("aDecimal").named("DecimalMessage");    Assert.assertEquals(expected, builderType);        expected = new MessageType("DecimalMessage", new PrimitiveType(REQUIRED, INT64, 0, "aDecimal", DECIMAL, new DecimalMetadata(18, 2), null));    builderType = Types.buildMessage().required(INT64).as(DECIMAL).precision(18).scale(2).named("aDecimal").named("DecimalMessage");    Assert.assertEquals(expected, builderType);        expected = new MessageType("DecimalMessage", new PrimitiveType(REQUIRED, BINARY, 0, "aDecimal", DECIMAL, new DecimalMetadata(9, 2), null));    builderType = Types.buildMessage().required(BINARY).as(DECIMAL).precision(9).scale(2).named("aDecimal").named("DecimalMessage");    Assert.assertEquals(expected, builderType);        expected = new MessageType("DecimalMessage", new PrimitiveType(REQUIRED, FIXED_LEN_BYTE_ARRAY, 4, "aDecimal", DECIMAL, new DecimalMetadata(9, 2), null));    builderType = Types.buildMessage().required(FIXED_LEN_BYTE_ARRAY).length(4).as(DECIMAL).precision(9).scale(2).named("aDecimal").named("DecimalMessage");    Assert.assertEquals(expected, builderType);}
0
public void testDecimalAnnotationMissingScale()
{    MessageType expected = new MessageType("DecimalMessage", new PrimitiveType(REQUIRED, INT32, 0, "aDecimal", DECIMAL, new DecimalMetadata(9, 0), null));    MessageType builderType = Types.buildMessage().required(INT32).as(DECIMAL).precision(9).named("aDecimal").named("DecimalMessage");    Assert.assertEquals(expected, builderType);    expected = new MessageType("DecimalMessage", new PrimitiveType(REQUIRED, INT64, 0, "aDecimal", DECIMAL, new DecimalMetadata(9, 0), null));    builderType = Types.buildMessage().required(INT64).as(DECIMAL).precision(9).named("aDecimal").named("DecimalMessage");    Assert.assertEquals(expected, builderType);    expected = new MessageType("DecimalMessage", new PrimitiveType(REQUIRED, BINARY, 0, "aDecimal", DECIMAL, new DecimalMetadata(9, 0), null));    builderType = Types.buildMessage().required(BINARY).as(DECIMAL).precision(9).named("aDecimal").named("DecimalMessage");    Assert.assertEquals(expected, builderType);    expected = new MessageType("DecimalMessage", new PrimitiveType(REQUIRED, FIXED_LEN_BYTE_ARRAY, 7, "aDecimal", DECIMAL, new DecimalMetadata(9, 0), null));    builderType = Types.buildMessage().required(FIXED_LEN_BYTE_ARRAY).length(7).as(DECIMAL).precision(9).named("aDecimal").named("DecimalMessage");    Assert.assertEquals(expected, builderType);}
0
public void testDecimalAnnotationMissingPrecision()
{    assertThrows("Should reject decimal annotation without precision", IllegalArgumentException.class, (Callable<Type>) () -> Types.buildMessage().required(INT32).as(DECIMAL).scale(2).named("aDecimal").named("DecimalMessage"));    assertThrows("Should reject decimal annotation without precision", IllegalArgumentException.class, (Callable<Type>) () -> Types.buildMessage().required(INT64).as(DECIMAL).scale(2).named("aDecimal").named("DecimalMessage"));    assertThrows("Should reject decimal annotation without precision", IllegalArgumentException.class, (Callable<Type>) () -> Types.buildMessage().required(BINARY).as(DECIMAL).scale(2).named("aDecimal").named("DecimalMessage"));    assertThrows("Should reject decimal annotation without precision", IllegalArgumentException.class, (Callable<Type>) () -> Types.buildMessage().required(FIXED_LEN_BYTE_ARRAY).length(7).as(DECIMAL).scale(2).named("aDecimal").named("DecimalMessage"));}
0
public void testDecimalAnnotationPrecisionScaleBound()
{    assertThrows("Should reject scale greater than precision", IllegalArgumentException.class, (Callable<Type>) () -> Types.buildMessage().required(INT32).as(DECIMAL).precision(3).scale(4).named("aDecimal").named("DecimalMessage"));    assertThrows("Should reject scale greater than precision", IllegalArgumentException.class, (Callable<Type>) () -> Types.buildMessage().required(INT64).as(DECIMAL).precision(3).scale(4).named("aDecimal").named("DecimalMessage"));    assertThrows("Should reject scale greater than precision", IllegalArgumentException.class, (Callable<Type>) () -> Types.buildMessage().required(BINARY).as(DECIMAL).precision(3).scale(4).named("aDecimal").named("DecimalMessage"));    assertThrows("Should reject scale greater than precision", IllegalArgumentException.class, (Callable<Type>) () -> Types.buildMessage().required(FIXED_LEN_BYTE_ARRAY).length(7).as(DECIMAL).precision(3).scale(4).named("aDecimal").named("DecimalMessage"));}
0
public void testDecimalAnnotationLengthCheck()
{        assertThrows("should reject precision 10 with length 4", IllegalStateException.class, (Callable<Type>) () -> Types.required(FIXED_LEN_BYTE_ARRAY).length(4).as(DECIMAL).precision(10).scale(2).named("aDecimal"));    assertThrows("should reject precision 10 with length 4", IllegalStateException.class, (Callable<Type>) () -> Types.required(INT32).as(DECIMAL).precision(10).scale(2).named("aDecimal"));        assertThrows("should reject precision 19 with length 8", IllegalStateException.class, (Callable<Type>) () -> Types.required(FIXED_LEN_BYTE_ARRAY).length(8).as(DECIMAL).precision(19).scale(4).named("aDecimal"));    assertThrows("should reject precision 19 with length 8", IllegalStateException.class, (Callable<Type>) () -> Types.required(INT64).length(8).as(DECIMAL).precision(19).scale(4).named("aDecimal"));}
0
public void testDECIMALAnnotationRejectsUnsupportedTypes()
{    PrimitiveTypeName[] unsupported = new PrimitiveTypeName[] { BOOLEAN, INT96, DOUBLE, FLOAT };    for (final PrimitiveTypeName type : unsupported) {        assertThrows("Should reject non-binary type: " + type, IllegalStateException.class, (Callable<Type>) () -> Types.required(type).as(DECIMAL).precision(9).scale(2).named("d"));    }}
0
public void testBinaryAnnotations()
{    OriginalType[] types = new OriginalType[] { UTF8, JSON, BSON };    for (final OriginalType logicalType : types) {        PrimitiveType expected = new PrimitiveType(REQUIRED, BINARY, "col", logicalType);        PrimitiveType string = Types.required(BINARY).as(logicalType).named("col");        Assert.assertEquals(expected, string);    }}
0
public void testBinaryAnnotationsRejectsNonBinary()
{    OriginalType[] types = new OriginalType[] { UTF8, JSON, BSON };    for (final OriginalType logicalType : types) {        PrimitiveTypeName[] nonBinary = new PrimitiveTypeName[] { BOOLEAN, INT32, INT64, INT96, DOUBLE, FLOAT };        for (final PrimitiveTypeName type : nonBinary) {            assertThrows("Should reject non-binary type: " + type, IllegalStateException.class, (Callable<Type>) () -> Types.required(type).as(logicalType).named("col"));        }        assertThrows("Should reject non-binary type: FIXED_LEN_BYTE_ARRAY", IllegalStateException.class, (Callable<Type>) () -> Types.required(FIXED_LEN_BYTE_ARRAY).length(1).as(logicalType).named("col"));    }}
0
public void testInt32Annotations()
{    OriginalType[] types = new OriginalType[] { DATE, TIME_MILLIS, UINT_8, UINT_16, UINT_32, INT_8, INT_16, INT_32 };    for (OriginalType logicalType : types) {        PrimitiveType expected = new PrimitiveType(REQUIRED, INT32, "col", logicalType);        PrimitiveType date = Types.required(INT32).as(logicalType).named("col");        Assert.assertEquals(expected, date);    }}
0
public void testInt32AnnotationsRejectNonInt32()
{    OriginalType[] types = new OriginalType[] { DATE, TIME_MILLIS, UINT_8, UINT_16, UINT_32, INT_8, INT_16, INT_32 };    for (final OriginalType logicalType : types) {        PrimitiveTypeName[] nonInt32 = new PrimitiveTypeName[] { BOOLEAN, INT64, INT96, DOUBLE, FLOAT, BINARY };        for (final PrimitiveTypeName type : nonInt32) {            assertThrows("Should reject non-int32 type: " + type, IllegalStateException.class, (Callable<Type>) () -> Types.required(type).as(logicalType).named("col"));        }        assertThrows("Should reject non-int32 type: FIXED_LEN_BYTE_ARRAY", IllegalStateException.class, (Callable<Type>) () -> Types.required(FIXED_LEN_BYTE_ARRAY).length(1).as(logicalType).named("col"));    }}
0
public void testInt64Annotations()
{    OriginalType[] types = new OriginalType[] { TIME_MICROS, TIMESTAMP_MILLIS, TIMESTAMP_MICROS, UINT_64, INT_64 };    for (OriginalType logicalType : types) {        PrimitiveType expected = new PrimitiveType(REQUIRED, INT64, "col", logicalType);        PrimitiveType date = Types.required(INT64).as(logicalType).named("col");        Assert.assertEquals(expected, date);    }}
0
public void testInt64AnnotationsRejectNonInt64()
{    OriginalType[] types = new OriginalType[] { TIME_MICROS, TIMESTAMP_MILLIS, TIMESTAMP_MICROS, UINT_64, INT_64 };    for (final OriginalType logicalType : types) {        PrimitiveTypeName[] nonInt64 = new PrimitiveTypeName[] { BOOLEAN, INT32, INT96, DOUBLE, FLOAT, BINARY };        for (final PrimitiveTypeName type : nonInt64) {            assertThrows("Should reject non-int64 type: " + type, IllegalStateException.class, (Callable<Type>) () -> Types.required(type).as(logicalType).named("col"));        }        assertThrows("Should reject non-int64 type: FIXED_LEN_BYTE_ARRAY", IllegalStateException.class, (Callable<Type>) () -> Types.required(FIXED_LEN_BYTE_ARRAY).length(1).as(logicalType).named("col"));    }}
0
public void testIntervalAnnotation()
{    PrimitiveType expected = new PrimitiveType(REQUIRED, FIXED_LEN_BYTE_ARRAY, 12, "interval", INTERVAL);    PrimitiveType string = Types.required(FIXED_LEN_BYTE_ARRAY).length(12).as(INTERVAL).named("interval");    Assert.assertEquals(expected, string);}
0
public void testIntervalAnnotationRejectsNonFixed()
{    PrimitiveTypeName[] nonFixed = new PrimitiveTypeName[] { BOOLEAN, INT32, INT64, INT96, DOUBLE, FLOAT, BINARY };    for (final PrimitiveTypeName type : nonFixed) {        assertThrows("Should reject non-fixed type: " + type, IllegalStateException.class, (Callable<Type>) () -> Types.required(type).as(INTERVAL).named("interval"));    }}
0
public void testIntervalAnnotationRejectsNonFixed12()
{    assertThrows("Should reject fixed with length != 12: " + 11, IllegalStateException.class, (Callable<Type>) () -> Types.required(FIXED_LEN_BYTE_ARRAY).length(11).as(INTERVAL).named("interval"));}
0
public void testRequiredMap()
{    List<Type> typeList = new ArrayList<Type>();    typeList.add(new PrimitiveType(REQUIRED, INT64, "key"));    typeList.add(new PrimitiveType(REQUIRED, INT64, "value"));    GroupType expected = new GroupType(REQUIRED, "myMap", OriginalType.MAP, new GroupType(REPEATED, "map", typeList));    GroupType actual = Types.requiredMap().key(INT64).requiredValue(INT64).named("myMap");    Assert.assertEquals(expected, actual);}
0
public void testOptionalMap()
{    List<Type> typeList = new ArrayList<Type>();    typeList.add(new PrimitiveType(REQUIRED, INT64, "key"));    typeList.add(new PrimitiveType(REQUIRED, INT64, "value"));    GroupType expected = new GroupType(OPTIONAL, "myMap", OriginalType.MAP, new GroupType(REPEATED, "map", typeList));    GroupType actual = Types.optionalMap().key(INT64).requiredValue(INT64).named("myMap");    Assert.assertEquals(expected, actual);}
0
public void testMapWithRequiredValue()
{    List<Type> typeList = new ArrayList<Type>();    typeList.add(new PrimitiveType(REQUIRED, INT64, "key"));    typeList.add(new PrimitiveType(REQUIRED, INT64, "value"));    GroupType map = new GroupType(REQUIRED, "myMap", OriginalType.MAP, new GroupType(REPEATED, "map", typeList));    MessageType expected = new MessageType("mapParent", map);    GroupType actual = Types.buildMessage().requiredMap().key(INT64).requiredValue(INT64).named("myMap").named("mapParent");    Assert.assertEquals(expected, actual);}
0
public void testMapWithOptionalValue()
{    List<Type> typeList = new ArrayList<Type>();    typeList.add(new PrimitiveType(REQUIRED, INT64, "key"));    typeList.add(new PrimitiveType(OPTIONAL, INT64, "value"));    GroupType map = new GroupType(REQUIRED, "myMap", OriginalType.MAP, new GroupType(REPEATED, "map", typeList));    MessageType expected = new MessageType("mapParent", map);    GroupType actual = Types.buildMessage().requiredMap().key(INT64).optionalValue(INT64).named("myMap").named("mapParent");    Assert.assertEquals(expected, actual);}
0
public void testMapWithGroupKeyAndOptionalGroupValue()
{    List<Type> typeList = new ArrayList<Type>();    List<Type> keyFields = new ArrayList<Type>();    keyFields.add(new PrimitiveType(OPTIONAL, INT64, "first"));    keyFields.add(new PrimitiveType(OPTIONAL, DOUBLE, "second"));    typeList.add(new GroupType(REQUIRED, "key", keyFields));    List<Type> valueFields = new ArrayList<Type>();    valueFields.add(new PrimitiveType(OPTIONAL, DOUBLE, "one"));    valueFields.add(new PrimitiveType(OPTIONAL, INT32, "two"));    typeList.add(new GroupType(OPTIONAL, "value", valueFields));    GroupType map = new GroupType(OPTIONAL, "myMap", OriginalType.MAP, new GroupType(REPEATED, "map", typeList));    GroupType actual = Types.optionalMap().groupKey().optional(INT64).named("first").optional(DOUBLE).named("second").optionalGroupValue().optional(DOUBLE).named("one").optional(INT32).named("two").named("myMap");    Assert.assertEquals(map, actual);}
0
public void testMapWithGroupKeyAndRequiredGroupValue()
{    List<Type> typeList = new ArrayList<Type>();    List<Type> keyFields = new ArrayList<Type>();    keyFields.add(new PrimitiveType(OPTIONAL, INT64, "first"));    keyFields.add(new PrimitiveType(OPTIONAL, DOUBLE, "second"));    typeList.add(new GroupType(REQUIRED, "key", keyFields));    List<Type> valueFields = new ArrayList<Type>();    valueFields.add(new PrimitiveType(OPTIONAL, DOUBLE, "one"));    valueFields.add(new PrimitiveType(OPTIONAL, INT32, "two"));    typeList.add(new GroupType(REQUIRED, "value", valueFields));    GroupType map = new GroupType(OPTIONAL, "myMap", OriginalType.MAP, new GroupType(REPEATED, "map", typeList));    MessageType expected = new MessageType("mapParent", map);    GroupType actual = Types.buildMessage().optionalMap().groupKey().optional(INT64).named("first").optional(DOUBLE).named("second").requiredGroupValue().optional(DOUBLE).named("one").optional(INT32).named("two").named("myMap").named("mapParent");    Assert.assertEquals(expected, actual);}
0
public void testMapWithGroupKeyAndOptionalValue()
{    List<Type> typeList = new ArrayList<Type>();    List<Type> keyFields = new ArrayList<Type>();    keyFields.add(new PrimitiveType(OPTIONAL, INT64, "first"));    keyFields.add(new PrimitiveType(OPTIONAL, DOUBLE, "second"));    typeList.add(new GroupType(REQUIRED, "key", keyFields));    typeList.add(new PrimitiveType(OPTIONAL, DOUBLE, "value"));    GroupType map = new GroupType(OPTIONAL, "myMap", OriginalType.MAP, new GroupType(REPEATED, "map", typeList));    MessageType expected = new MessageType("mapParent", map);    GroupType actual = Types.buildMessage().optionalMap().groupKey().optional(INT64).named("first").optional(DOUBLE).named("second").optionalValue(DOUBLE).named("myMap").named("mapParent");    Assert.assertEquals(expected, actual);}
0
public void testMapWithGroupKeyAndRequiredValue()
{    List<Type> typeList = new ArrayList<Type>();    List<Type> keyFields = new ArrayList<Type>();    keyFields.add(new PrimitiveType(OPTIONAL, INT64, "first"));    keyFields.add(new PrimitiveType(OPTIONAL, DOUBLE, "second"));    typeList.add(new GroupType(REQUIRED, "key", keyFields));    typeList.add(new PrimitiveType(REQUIRED, DOUBLE, "value"));    GroupType map = new GroupType(OPTIONAL, "myMap", OriginalType.MAP, new GroupType(REPEATED, "map", typeList));    MessageType expected = new MessageType("mapParent", map);    GroupType actual = Types.buildMessage().optionalMap().groupKey().optional(INT64).named("first").optional(DOUBLE).named("second").requiredValue(DOUBLE).named("myMap").named("mapParent");    Assert.assertEquals(expected, actual);}
0
public void testMapWithOptionalGroupValue()
{    List<Type> typeList = new ArrayList<Type>();    List<Type> keyFields = new ArrayList<Type>();    keyFields.add(new PrimitiveType(OPTIONAL, INT64, "first"));    keyFields.add(new PrimitiveType(OPTIONAL, DOUBLE, "second"));    typeList.add(new PrimitiveType(REQUIRED, INT64, "key"));    List<Type> valueFields = new ArrayList<Type>();    valueFields.add(new PrimitiveType(OPTIONAL, DOUBLE, "one"));    valueFields.add(new PrimitiveType(OPTIONAL, INT32, "two"));    typeList.add(new GroupType(OPTIONAL, "value", valueFields));    GroupType map = new GroupType(OPTIONAL, "myMap", OriginalType.MAP, new GroupType(REPEATED, "map", typeList));    MessageType expected = new MessageType("mapParent", map);    GroupType actual = Types.buildMessage().optionalMap().key(INT64).optionalGroupValue().optional(DOUBLE).named("one").optional(INT32).named("two").named("myMap").named("mapParent");    Assert.assertEquals(expected, actual);}
0
public void testMapWithRequiredGroupValue()
{    List<Type> typeList = new ArrayList<Type>();    typeList.add(new PrimitiveType(REQUIRED, INT64, "key"));    List<Type> valueFields = new ArrayList<Type>();    valueFields.add(new PrimitiveType(OPTIONAL, DOUBLE, "one"));    valueFields.add(new PrimitiveType(OPTIONAL, INT32, "two"));    typeList.add(new GroupType(REQUIRED, "value", valueFields));    GroupType map = new GroupType(OPTIONAL, "myMap", OriginalType.MAP, new GroupType(REPEATED, "map", typeList));    MessageType expected = new MessageType("mapParent", map);    GroupType actual = Types.buildMessage().optionalMap().key(INT64).requiredGroupValue().optional(DOUBLE).named("one").optional(INT32).named("two").named("myMap").named("mapParent");    Assert.assertEquals(expected, actual);}
0
public void testMapWithNestedGroupKeyAndNestedGroupValue()
{    List<Type> typeList = new ArrayList<Type>();    List<Type> innerFields = new ArrayList<Type>();    innerFields.add(new PrimitiveType(REQUIRED, FLOAT, "inner_key_1"));    innerFields.add(new PrimitiveType(OPTIONAL, INT32, "inner_key_2"));    List<Type> keyFields = new ArrayList<Type>();    keyFields.add(new PrimitiveType(OPTIONAL, INT64, "first"));    keyFields.add(new GroupType(REQUIRED, "second", innerFields));    typeList.add(new GroupType(REQUIRED, "key", keyFields));    List<Type> valueFields = new ArrayList<Type>();    valueFields.add(new GroupType(OPTIONAL, "one", innerFields));    valueFields.add(new PrimitiveType(OPTIONAL, INT32, "two"));    typeList.add(new GroupType(OPTIONAL, "value", valueFields));    GroupType map = new GroupType(REQUIRED, "myMap", OriginalType.MAP, new GroupType(REPEATED, "map", typeList));    MessageType expected = new MessageType("mapParent", map);    GroupType actual = Types.buildMessage().requiredMap().groupKey().optional(INT64).named("first").requiredGroup().required(FLOAT).named("inner_key_1").optional(INT32).named("inner_key_2").named("second").optionalGroupValue().optionalGroup().required(FLOAT).named("inner_key_1").optional(INT32).named("inner_key_2").named("one").optional(INT32).named("two").named("myMap").named("mapParent");    Assert.assertEquals(expected, actual);}
0
public void testMapWithRequiredListValue()
{    List<Type> typeList = new ArrayList<Type>();    typeList.add(new PrimitiveType(REQUIRED, INT64, "key"));    typeList.add(new GroupType(REQUIRED, "value", OriginalType.LIST, new GroupType(REPEATED, "list", new PrimitiveType(OPTIONAL, INT64, "element"))));    GroupType map = new GroupType(OPTIONAL, "myMap", OriginalType.MAP, new GroupType(REPEATED, "map", typeList));    MessageType expected = new MessageType("mapParent", map);    GroupType actual = Types.buildMessage().optionalMap().key(INT64).requiredListValue().optionalElement(INT64).named("myMap").named("mapParent");    Assert.assertEquals(expected, actual);}
0
public void testMapWithOptionalListValue()
{    List<Type> typeList = new ArrayList<Type>();    typeList.add(new PrimitiveType(REQUIRED, INT64, "key"));    typeList.add(new GroupType(OPTIONAL, "value", OriginalType.LIST, new GroupType(REPEATED, "list", new PrimitiveType(OPTIONAL, INT64, "element"))));    GroupType map = new GroupType(OPTIONAL, "myMap", OriginalType.MAP, new GroupType(REPEATED, "map", typeList));    MessageType expected = new MessageType("mapParent", map);    GroupType actual = Types.buildMessage().optionalMap().key(INT64).optionalListValue().optionalElement(INT64).named("myMap").named("mapParent");    Assert.assertEquals(expected, actual);}
0
public void testMapWithRequiredMapValue()
{    List<Type> typeList = new ArrayList<Type>();    List<Type> innerMapTypeList = new ArrayList<Type>();    innerMapTypeList.add(new PrimitiveType(REQUIRED, INT64, "key"));    innerMapTypeList.add(new PrimitiveType(REQUIRED, INT64, "value"));    typeList.add(new PrimitiveType(REQUIRED, INT64, "key"));    typeList.add(new GroupType(REQUIRED, "value", OriginalType.MAP, new GroupType(REPEATED, "map", innerMapTypeList)));    GroupType map = new GroupType(OPTIONAL, "myMap", OriginalType.MAP, new GroupType(REPEATED, "map", typeList));    MessageType expected = new MessageType("mapParent", map);    GroupType actual = Types.buildMessage().optionalMap().key(INT64).requiredMapValue().key(INT64).requiredValue(INT64).named("myMap").named("mapParent");    Assert.assertEquals(expected, actual);}
0
public void testMapWithOptionalMapValue()
{    List<Type> typeList = new ArrayList<Type>();    List<Type> innerMapTypeList = new ArrayList<Type>();    innerMapTypeList.add(new PrimitiveType(REQUIRED, INT64, "key"));    innerMapTypeList.add(new PrimitiveType(REQUIRED, INT64, "value"));    typeList.add(new PrimitiveType(REQUIRED, INT64, "key"));    typeList.add(new GroupType(OPTIONAL, "value", OriginalType.MAP, new GroupType(REPEATED, "map", innerMapTypeList)));    GroupType map = new GroupType(OPTIONAL, "myMap", OriginalType.MAP, new GroupType(REPEATED, "map", typeList));    MessageType expected = new MessageType("mapParent", map);    GroupType actual = Types.buildMessage().optionalMap().key(INT64).optionalMapValue().key(INT64).requiredValue(INT64).named("myMap").named("mapParent");    Assert.assertEquals(expected, actual);}
0
public void testMapWithGroupKeyAndRequiredListValue()
{    List<Type> typeList = new ArrayList<Type>();    typeList.add(new GroupType(REQUIRED, "key", new PrimitiveType(REQUIRED, INT64, "first")));    typeList.add(new GroupType(REQUIRED, "value", OriginalType.LIST, new GroupType(REPEATED, "list", new PrimitiveType(OPTIONAL, INT64, "element"))));    GroupType map = new GroupType(OPTIONAL, "myMap", OriginalType.MAP, new GroupType(REPEATED, "map", typeList));    MessageType expected = new MessageType("mapParent", map);    GroupType actual = Types.buildMessage().optionalMap().groupKey().required(INT64).named("first").requiredListValue().optionalElement(INT64).named("myMap").named("mapParent");    Assert.assertEquals(expected, actual);}
0
public void testMapWithGroupKeyAndOptionalListValue()
{    List<Type> typeList = new ArrayList<Type>();    typeList.add(new GroupType(REQUIRED, "key", new PrimitiveType(REQUIRED, INT64, "first")));    typeList.add(new GroupType(OPTIONAL, "value", OriginalType.LIST, new GroupType(REPEATED, "list", new PrimitiveType(OPTIONAL, INT64, "element"))));    GroupType map = new GroupType(OPTIONAL, "myMap", OriginalType.MAP, new GroupType(REPEATED, "map", typeList));    MessageType expected = new MessageType("mapParent", map);    GroupType actual = Types.buildMessage().optionalMap().groupKey().required(INT64).named("first").optionalListValue().optionalElement(INT64).named("myMap").named("mapParent");    Assert.assertEquals(expected, actual);}
0
public void testMapWithGroupKeyAndRequiredMapValue()
{    List<Type> typeList = new ArrayList<Type>();    List<Type> innerMapTypeList = new ArrayList<Type>();    innerMapTypeList.add(new PrimitiveType(REQUIRED, INT64, "key"));    innerMapTypeList.add(new PrimitiveType(REQUIRED, INT64, "value"));    typeList.add(new GroupType(REQUIRED, "key", new PrimitiveType(REQUIRED, INT64, "first")));    typeList.add(new GroupType(REQUIRED, "value", OriginalType.MAP, new GroupType(REPEATED, "map", innerMapTypeList)));    GroupType map = new GroupType(OPTIONAL, "myMap", OriginalType.MAP, new GroupType(REPEATED, "map", typeList));    MessageType expected = new MessageType("mapParent", map);    GroupType actual = Types.buildMessage().optionalMap().groupKey().required(INT64).named("first").requiredMapValue().key(INT64).requiredValue(INT64).named("myMap").named("mapParent");    Assert.assertEquals(expected, actual);}
0
public void testMapWithGroupKeyAndOptionalMapValue()
{    List<Type> typeList = new ArrayList<Type>();    List<Type> innerMapTypeList = new ArrayList<Type>();    innerMapTypeList.add(new PrimitiveType(REQUIRED, INT64, "key"));    innerMapTypeList.add(new PrimitiveType(REQUIRED, INT64, "value"));    typeList.add(new GroupType(REQUIRED, "key", new PrimitiveType(REQUIRED, INT64, "first")));    typeList.add(new GroupType(OPTIONAL, "value", OriginalType.MAP, new GroupType(REPEATED, "map", innerMapTypeList)));    GroupType map = new GroupType(OPTIONAL, "myMap", OriginalType.MAP, new GroupType(REPEATED, "map", typeList));    MessageType expected = new MessageType("mapParent", map);    GroupType actual = Types.buildMessage().optionalMap().groupKey().required(INT64).named("first").optionalMapValue().key(INT64).requiredValue(INT64).named("myMap").named("mapParent");    Assert.assertEquals(expected, actual);}
0
public void testMapWithNullValue()
{    List<Type> typeList = new ArrayList<Type>();    typeList.add(new PrimitiveType(REQUIRED, INT64, "key"));    GroupType map = new GroupType(OPTIONAL, "myMap", OriginalType.MAP, new GroupType(REPEATED, "map", typeList));    MessageType expected = new MessageType("mapParent", map);    GroupType actual = Types.buildMessage().optionalMap().key(INT64).named("myMap").named("mapParent");    Assert.assertEquals(expected, actual);}
0
public void testMapWithDefaultKeyAndNullValue()
{    List<Type> typeList = new ArrayList<Type>();    typeList.add(new PrimitiveType(REQUIRED, BINARY, "key", OriginalType.UTF8));    GroupType map = new GroupType(OPTIONAL, "myMap", OriginalType.MAP, new GroupType(REPEATED, "map", typeList));    MessageType expected = new MessageType("mapParent", map);    GroupType actual = Types.buildMessage().optionalMap().named("myMap").named("mapParent");    Assert.assertEquals(expected, actual);}
0
public void testMapWithPreBuiltKeyAndValueTypes()
{    Type keyType = Types.required(INT64).named("key");    Type valueType = Types.required(BOOLEAN).named("value");    GroupType map = new GroupType(REQUIRED, "myMap", OriginalType.MAP, new GroupType(REPEATED, "map", new Type[] { keyType, valueType }));    MessageType expected = new MessageType("mapParent", map);    GroupType actual = Types.buildMessage().requiredMap().key(keyType).value(valueType).named("myMap").named("mapParent");    Assert.assertEquals(expected, actual);}
0
public void testListWithRequiredPreBuiltElement()
{    GroupType expected = new GroupType(REQUIRED, "myList", OriginalType.LIST, new GroupType(REPEATED, "list", new PrimitiveType(REQUIRED, INT64, "element")));    Type element = Types.primitive(INT64, REQUIRED).named("element");    Type actual = Types.requiredList().element(element).named("myList");    Assert.assertEquals(expected, actual);}
0
public void testRequiredList()
{    GroupType expected = new GroupType(REQUIRED, "myList", OriginalType.LIST, new GroupType(REPEATED, "list", new PrimitiveType(OPTIONAL, INT64, "element")));    Type actual = Types.requiredList().optionalElement(INT64).named("myList");    Assert.assertEquals(expected, actual);}
0
public void testOptionalList()
{    GroupType expected = new GroupType(OPTIONAL, "myList", OriginalType.LIST, new GroupType(REPEATED, "list", new PrimitiveType(OPTIONAL, INT64, "element")));    Type actual = Types.optionalList().optionalElement(INT64).named("myList");    Assert.assertEquals(expected, actual);}
0
public void testListOfReqGroup()
{    List<Type> fields = new ArrayList<Type>();    fields.add(new PrimitiveType(OPTIONAL, BOOLEAN, "field"));    GroupType expected = new GroupType(REQUIRED, "myList", OriginalType.LIST, new GroupType(REPEATED, "list", new GroupType(REQUIRED, "element", fields)));    Type actual = Types.requiredList().requiredGroupElement().optional(BOOLEAN).named("field").named("myList");    Assert.assertEquals(expected, actual);}
0
public void testListOfOptionalGroup()
{    List<Type> fields = new ArrayList<Type>();    fields.add(new PrimitiveType(OPTIONAL, BOOLEAN, "field"));    GroupType expected = new GroupType(REQUIRED, "myList", OriginalType.LIST, new GroupType(REPEATED, "list", new GroupType(OPTIONAL, "element", fields)));    Type actual = Types.requiredList().optionalGroupElement().optional(BOOLEAN).named("field").named("myList");    Assert.assertEquals(expected, actual);}
0
public void testRequiredNestedList()
{    List<Type> fields = new ArrayList<Type>();    fields.add(new GroupType(REQUIRED, "element", OriginalType.LIST, new GroupType(REPEATED, "list", new PrimitiveType(OPTIONAL, DOUBLE, "element"))));    GroupType expected = new GroupType(OPTIONAL, "myList", OriginalType.LIST, new GroupType(REPEATED, "list", fields));    Type actual = Types.optionalList().requiredListElement().optionalElement(DOUBLE).named("myList");    Assert.assertEquals(expected, actual);}
0
public void testOptionalNestedList()
{    List<Type> fields = new ArrayList<Type>();    fields.add(new GroupType(OPTIONAL, "element", OriginalType.LIST, new GroupType(REPEATED, "list", new PrimitiveType(OPTIONAL, DOUBLE, "element"))));    GroupType expected = new GroupType(OPTIONAL, "myList", OriginalType.LIST, new GroupType(REPEATED, "list", fields));    Type actual = Types.optionalList().optionalListElement().optionalElement(DOUBLE).named("myList");    Assert.assertEquals(expected, actual);}
0
public void testRequiredListWithinGroup()
{    List<Type> fields = new ArrayList<Type>();    fields.add(new GroupType(REQUIRED, "element", OriginalType.LIST, new GroupType(REPEATED, "list", new PrimitiveType(OPTIONAL, INT64, "element"))));    GroupType expected = new GroupType(REQUIRED, "topGroup", fields);    Type actual = Types.requiredGroup().requiredList().optionalElement(INT64).named("element").named("topGroup");    Assert.assertEquals(expected, actual);}
0
public void testOptionalListWithinGroup()
{    List<Type> fields = new ArrayList<Type>();    fields.add(new GroupType(OPTIONAL, "element", OriginalType.LIST, new GroupType(REPEATED, "list", new PrimitiveType(OPTIONAL, INT64, "element"))));    GroupType expected = new GroupType(REQUIRED, "topGroup", fields);    Type actual = Types.requiredGroup().optionalList().optionalElement(INT64).named("element").named("topGroup");    Assert.assertEquals(expected, actual);}
0
public void testOptionalListWithinGroupWithReqElement()
{    List<Type> fields = new ArrayList<Type>();    fields.add(new GroupType(OPTIONAL, "element", OriginalType.LIST, new GroupType(REPEATED, "list", new PrimitiveType(REQUIRED, INT64, "element"))));    GroupType expected = new GroupType(REQUIRED, "topGroup", fields);    Type actual = Types.requiredGroup().optionalList().requiredElement(INT64).named("element").named("topGroup");    Assert.assertEquals(expected, actual);}
0
public void testRequiredMapWithinList()
{    List<Type> innerFields = new ArrayList<Type>();    innerFields.add(new PrimitiveType(REQUIRED, DOUBLE, "key"));    innerFields.add(new PrimitiveType(REQUIRED, INT32, "value"));    List<Type> fields = new ArrayList<Type>();    fields.add(new GroupType(REQUIRED, "element", OriginalType.MAP, new GroupType(REPEATED, "map", innerFields)));    GroupType expected = new GroupType(OPTIONAL, "myList", OriginalType.LIST, new GroupType(REPEATED, "list", fields));    Type actual = Types.optionalList().requiredMapElement().key(DOUBLE).requiredValue(INT32).named("myList");    Assert.assertEquals(expected, actual);}
0
public void testOptionalMapWithinList()
{    List<Type> innerFields = new ArrayList<Type>();    innerFields.add(new PrimitiveType(REQUIRED, DOUBLE, "key"));    innerFields.add(new PrimitiveType(REQUIRED, INT32, "value"));    List<Type> fields = new ArrayList<Type>();    fields.add(new GroupType(OPTIONAL, "element", OriginalType.MAP, new GroupType(REPEATED, "map", innerFields)));    GroupType expected = new GroupType(OPTIONAL, "myList", OriginalType.LIST, new GroupType(REPEATED, "list", fields));    Type actual = Types.optionalList().optionalMapElement().key(DOUBLE).requiredValue(INT32).named("myList");    Assert.assertEquals(expected, actual);}
0
public void testTypeConstructionWithUndefinedColumnOrder()
{    PrimitiveTypeName[] types = new PrimitiveTypeName[] { BOOLEAN, INT32, INT64, INT96, FLOAT, DOUBLE, BINARY, FIXED_LEN_BYTE_ARRAY };    for (PrimitiveTypeName type : types) {        String name = type.toString() + "_";        int len = type == FIXED_LEN_BYTE_ARRAY ? 42 : 0;        PrimitiveType expected = new PrimitiveType(Repetition.OPTIONAL, type, len, name, null, null, null, ColumnOrder.undefined());        PrimitiveType built = Types.optional(type).length(len).columnOrder(ColumnOrder.undefined()).named(name);        Assert.assertEquals(expected, built);    }}
0
public void testTypeConstructionWithTypeDefinedColumnOrder()
{    PrimitiveTypeName[] types = new PrimitiveTypeName[] { BOOLEAN, INT32, INT64, FLOAT, DOUBLE, BINARY, FIXED_LEN_BYTE_ARRAY };    for (PrimitiveTypeName type : types) {        String name = type.toString() + "_";        int len = type == FIXED_LEN_BYTE_ARRAY ? 42 : 0;        PrimitiveType expected = new PrimitiveType(Repetition.OPTIONAL, type, len, name, null, null, null, ColumnOrder.typeDefined());        PrimitiveType built = Types.optional(type).length(len).columnOrder(ColumnOrder.typeDefined()).named(name);        Assert.assertEquals(expected, built);    }}
0
public void testTypeConstructionWithUnsupportedColumnOrder()
{    assertThrows(null, IllegalArgumentException.class, (Callable<PrimitiveType>) () -> Types.optional(INT96).columnOrder(ColumnOrder.typeDefined()).named("int96_unsupported"));    assertThrows(null, IllegalArgumentException.class, (Callable<PrimitiveType>) () -> Types.optional(FIXED_LEN_BYTE_ARRAY).length(12).as(INTERVAL).columnOrder(ColumnOrder.typeDefined()).named("interval_unsupported"));}
0
public void testDecimalLogicalType()
{    PrimitiveType expected = new PrimitiveType(REQUIRED, BINARY, "aDecimal", LogicalTypeAnnotation.decimalType(3, 4));    PrimitiveType actual = Types.required(BINARY).as(LogicalTypeAnnotation.decimalType(3, 4)).named("aDecimal");    Assert.assertEquals(expected, actual);}
0
public void testDecimalLogicalTypeWithDeprecatedScale()
{    PrimitiveType expected = new PrimitiveType(REQUIRED, BINARY, "aDecimal", LogicalTypeAnnotation.decimalType(3, 4));    PrimitiveType actual = Types.required(BINARY).as(LogicalTypeAnnotation.decimalType(3, 4)).scale(3).named("aDecimal");    Assert.assertEquals(expected, actual);}
0
public void testDecimalLogicalTypeWithDeprecatedPrecision()
{    PrimitiveType expected = new PrimitiveType(REQUIRED, BINARY, "aDecimal", LogicalTypeAnnotation.decimalType(3, 4));    PrimitiveType actual = Types.required(BINARY).as(LogicalTypeAnnotation.decimalType(3, 4)).precision(4).named("aDecimal");    Assert.assertEquals(expected, actual);}
0
public void testTimestampLogicalTypeWithUTCParameter()
{    PrimitiveType utcMillisExpected = new PrimitiveType(REQUIRED, INT64, "aTimestamp", timestampType(true, MILLIS));    PrimitiveType nonUtcMillisExpected = new PrimitiveType(REQUIRED, INT64, "aTimestamp", timestampType(false, MILLIS));    PrimitiveType utcMicrosExpected = new PrimitiveType(REQUIRED, INT64, "aTimestamp", timestampType(true, MICROS));    PrimitiveType nonUtcMicrosExpected = new PrimitiveType(REQUIRED, INT64, "aTimestamp", timestampType(false, MICROS));    PrimitiveType utcMillisActual = Types.required(INT64).as(timestampType(true, MILLIS)).named("aTimestamp");    PrimitiveType nonUtcMillisActual = Types.required(INT64).as(timestampType(false, MILLIS)).named("aTimestamp");    PrimitiveType utcMicrosActual = Types.required(INT64).as(timestampType(true, MICROS)).named("aTimestamp");    PrimitiveType nonUtcMicrosActual = Types.required(INT64).as(timestampType(false, MICROS)).named("aTimestamp");    Assert.assertEquals(utcMillisExpected, utcMillisActual);    Assert.assertEquals(nonUtcMillisExpected, nonUtcMillisActual);    Assert.assertEquals(utcMicrosExpected, utcMicrosActual);    Assert.assertEquals(nonUtcMicrosExpected, nonUtcMicrosActual);}
0
public void testDecimalLogicalTypeWithDeprecatedScaleMismatch()
{    Types.required(BINARY).as(LogicalTypeAnnotation.decimalType(3, 4)).scale(4).named("aDecimal");}
0
public void testDecimalLogicalTypeWithDeprecatedPrecisionMismatch()
{    Types.required(BINARY).as(LogicalTypeAnnotation.decimalType(3, 4)).precision(5).named("aDecimal");}
0
public static void assertThrows(String message, Class<? extends Exception> expected, Callable callable)
{    try {        callable.call();        Assert.fail("No exception was thrown (" + message + "), expected: " + expected.getName());    } catch (Exception actual) {        Assert.assertEquals(message, expected, actual.getClass());    }}
0
public void testGroupTypeConstruction()
{    PrimitiveType f1 = Types.required(BINARY).as(stringType()).named("f1");    PrimitiveType f2 = Types.required(INT32).named("f2");    PrimitiveType f3 = Types.optional(INT32).named("f3");    String name = "group";    for (Repetition repetition : Repetition.values()) {        GroupType expected = new GroupType(repetition, name, f1, new GroupType(repetition, "g1", f2, f3));        GroupType built = Types.buildGroup(repetition).addField(f1).group(repetition).addFields(f2, f3).named("g1").named(name);        Assert.assertEquals(expected, built);        switch(repetition) {            case REQUIRED:                built = Types.requiredGroup().addField(f1).requiredGroup().addFields(f2, f3).named("g1").named(name);                break;            case OPTIONAL:                built = Types.optionalGroup().addField(f1).optionalGroup().addFields(f2, f3).named("g1").named(name);                break;            case REPEATED:                built = Types.repeatedGroup().addField(f1).repeatedGroup().addFields(f2, f3).named("g1").named(name);                break;        }        Assert.assertEquals(expected, built);    }}
0
public void testDecimalAnnotation()
{        MessageType expected = new MessageType("DecimalMessage", new PrimitiveType(REQUIRED, INT32, 0, "aDecimal", decimalType(2, 9), null));    MessageType builderType = Types.buildMessage().required(INT32).as(decimalType(2, 9)).named("aDecimal").named("DecimalMessage");    Assert.assertEquals(expected, builderType);        expected = new MessageType("DecimalMessage", new PrimitiveType(REQUIRED, INT64, 0, "aDecimal", decimalType(2, 18), null));    builderType = Types.buildMessage().required(INT64).as(decimalType(2, 18)).precision(18).scale(2).named("aDecimal").named("DecimalMessage");    Assert.assertEquals(expected, builderType);        expected = new MessageType("DecimalMessage", new PrimitiveType(REQUIRED, BINARY, 0, "aDecimal", decimalType(2, 9), null));    builderType = Types.buildMessage().required(BINARY).as(decimalType(2, 9)).named("aDecimal").named("DecimalMessage");    Assert.assertEquals(expected, builderType);        expected = new MessageType("DecimalMessage", new PrimitiveType(REQUIRED, FIXED_LEN_BYTE_ARRAY, 4, "aDecimal", decimalType(2, 9), null));    builderType = Types.buildMessage().required(FIXED_LEN_BYTE_ARRAY).length(4).as(decimalType(2, 9)).named("aDecimal").named("DecimalMessage");    Assert.assertEquals(expected, builderType);}
0
public void testDecimalAnnotationPrecisionScaleBound()
{    assertThrows("Should reject scale greater than precision", IllegalArgumentException.class, () -> Types.buildMessage().required(INT32).as(decimalType(4, 3)).named("aDecimal").named("DecimalMessage"));    assertThrows("Should reject scale greater than precision", IllegalArgumentException.class, () -> Types.buildMessage().required(INT64).as(decimalType(4, 3)).named("aDecimal").named("DecimalMessage"));    assertThrows("Should reject scale greater than precision", IllegalArgumentException.class, () -> Types.buildMessage().required(BINARY).as(decimalType(4, 3)).named("aDecimal").named("DecimalMessage"));    assertThrows("Should reject scale greater than precision", IllegalArgumentException.class, () -> Types.buildMessage().required(FIXED_LEN_BYTE_ARRAY).length(7).as(decimalType(4, 3)).named("aDecimal").named("DecimalMessage"));}
0
public void testDecimalAnnotationLengthCheck()
{        assertThrows("should reject precision 10 with length 4", IllegalStateException.class, () -> Types.required(FIXED_LEN_BYTE_ARRAY).length(4).as(decimalType(2, 10)).named("aDecimal"));    assertThrows("should reject precision 10 with length 4", IllegalStateException.class, () -> Types.required(INT32).as(decimalType(2, 10)).named("aDecimal"));        assertThrows("should reject precision 19 with length 8", IllegalStateException.class, () -> Types.required(FIXED_LEN_BYTE_ARRAY).length(8).as(decimalType(4, 19)).named("aDecimal"));    assertThrows("should reject precision 19 with length 8", IllegalStateException.class, () -> Types.required(INT64).length(8).as(decimalType(4, 19)).named("aDecimal"));}
0
public void testDECIMALAnnotationRejectsUnsupportedTypes()
{    PrimitiveTypeName[] unsupported = new PrimitiveTypeName[] { BOOLEAN, INT96, DOUBLE, FLOAT };    for (final PrimitiveTypeName type : unsupported) {        assertThrows("Should reject non-binary type: " + type, IllegalStateException.class, () -> Types.required(type).as(decimalType(2, 9)).named("d"));    }}
0
public void testBinaryAnnotations()
{    LogicalTypeAnnotation[] types = new LogicalTypeAnnotation[] { stringType(), jsonType(), bsonType() };    for (final LogicalTypeAnnotation logicalType : types) {        PrimitiveType expected = new PrimitiveType(REQUIRED, BINARY, "col", logicalType);        PrimitiveType string = Types.required(BINARY).as(logicalType).named("col");        Assert.assertEquals(expected, string);    }}
0
public void testBinaryAnnotationsRejectsNonBinary()
{    LogicalTypeAnnotation[] types = new LogicalTypeAnnotation[] { stringType(), jsonType(), bsonType() };    for (final LogicalTypeAnnotation logicalType : types) {        PrimitiveTypeName[] nonBinary = new PrimitiveTypeName[] { BOOLEAN, INT32, INT64, INT96, DOUBLE, FLOAT };        for (final PrimitiveTypeName type : nonBinary) {            assertThrows("Should reject non-binary type: " + type, IllegalStateException.class, () -> Types.required(type).as(logicalType).named("col"));        }        assertThrows("Should reject non-binary type: FIXED_LEN_BYTE_ARRAY", IllegalStateException.class, () -> Types.required(FIXED_LEN_BYTE_ARRAY).length(1).as(logicalType).named("col"));    }}
0
public void testInt32Annotations()
{    LogicalTypeAnnotation[] types = new LogicalTypeAnnotation[] { dateType(), timeType(true, MILLIS), timeType(false, MILLIS), intType(8, false), intType(16, false), intType(32, false), intType(8, true), intType(16, true), intType(32, true) };    for (LogicalTypeAnnotation logicalType : types) {        PrimitiveType expected = new PrimitiveType(REQUIRED, INT32, "col", logicalType);        PrimitiveType date = Types.required(INT32).as(logicalType).named("col");        Assert.assertEquals(expected, date);    }}
0
public void testInt32AnnotationsRejectNonInt32()
{    LogicalTypeAnnotation[] types = new LogicalTypeAnnotation[] { dateType(), timeType(true, MILLIS), timeType(false, MILLIS), intType(8, false), intType(16, false), intType(32, false), intType(8, true), intType(16, true), intType(32, true) };    for (final LogicalTypeAnnotation logicalType : types) {        PrimitiveTypeName[] nonInt32 = new PrimitiveTypeName[] { BOOLEAN, INT64, INT96, DOUBLE, FLOAT, BINARY };        for (final PrimitiveTypeName type : nonInt32) {            assertThrows("Should reject non-int32 type: " + type, IllegalStateException.class, () -> Types.required(type).as(logicalType).named("col"));        }        assertThrows("Should reject non-int32 type: FIXED_LEN_BYTE_ARRAY", IllegalStateException.class, () -> Types.required(FIXED_LEN_BYTE_ARRAY).length(1).as(logicalType).named("col"));    }}
0
public void testInt64Annotations()
{    LogicalTypeAnnotation[] types = new LogicalTypeAnnotation[] { timeType(true, MICROS), timeType(false, MICROS), timeType(true, NANOS), timeType(false, NANOS), timestampType(true, MILLIS), timestampType(false, MILLIS), timestampType(true, MICROS), timestampType(false, MICROS), timestampType(true, NANOS), timestampType(false, NANOS), intType(64, true), intType(64, false) };    for (LogicalTypeAnnotation logicalType : types) {        PrimitiveType expected = new PrimitiveType(REQUIRED, INT64, "col", logicalType);        PrimitiveType date = Types.required(INT64).as(logicalType).named("col");        Assert.assertEquals(expected, date);    }}
0
public void testInt64AnnotationsRejectNonInt64()
{    LogicalTypeAnnotation[] types = new LogicalTypeAnnotation[] { timeType(true, MICROS), timeType(false, MICROS), timeType(true, NANOS), timeType(false, NANOS), timestampType(true, MILLIS), timestampType(false, MILLIS), timestampType(true, MICROS), timestampType(false, MICROS), timestampType(true, NANOS), timestampType(false, NANOS), intType(64, true), intType(64, false) };    for (final LogicalTypeAnnotation logicalType : types) {        PrimitiveTypeName[] nonInt64 = new PrimitiveTypeName[] { BOOLEAN, INT32, INT96, DOUBLE, FLOAT, BINARY };        for (final PrimitiveTypeName type : nonInt64) {            assertThrows("Should reject non-int64 type: " + type, IllegalStateException.class, (Callable<Type>) () -> Types.required(type).as(logicalType).named("col"));        }        assertThrows("Should reject non-int64 type: FIXED_LEN_BYTE_ARRAY", IllegalStateException.class, (Callable<Type>) () -> Types.required(FIXED_LEN_BYTE_ARRAY).length(1).as(logicalType).named("col"));    }}
0
public void testIntervalAnnotationRejectsNonFixed()
{    PrimitiveTypeName[] nonFixed = new PrimitiveTypeName[] { BOOLEAN, INT32, INT64, INT96, DOUBLE, FLOAT, BINARY };    for (final PrimitiveTypeName type : nonFixed) {        assertThrows("Should reject non-fixed type: " + type, IllegalStateException.class, () -> Types.required(type).as(LogicalTypeAnnotation.IntervalLogicalTypeAnnotation.getInstance()).named("interval"));    }}
0
public void testIntervalAnnotationRejectsNonFixed12()
{    assertThrows("Should reject fixed with length != 12: " + 11, IllegalStateException.class, () -> Types.required(FIXED_LEN_BYTE_ARRAY).length(11).as(LogicalTypeAnnotation.IntervalLogicalTypeAnnotation.getInstance()).named("interval"));}
0
public void testTypeConstructionWithUnsupportedColumnOrder()
{    assertThrows(null, IllegalArgumentException.class, () -> Types.optional(INT96).columnOrder(ColumnOrder.typeDefined()).named("int96_unsupported"));    assertThrows(null, IllegalArgumentException.class, () -> Types.optional(PrimitiveTypeName.FIXED_LEN_BYTE_ARRAY).length(12).as(LogicalTypeAnnotation.IntervalLogicalTypeAnnotation.getInstance()).columnOrder(ColumnOrder.typeDefined()).named("interval_unsupported"));}
0
public void testDecimalLogicalType()
{    PrimitiveType expected = new PrimitiveType(REQUIRED, BINARY, "aDecimal", LogicalTypeAnnotation.decimalType(3, 4));    PrimitiveType actual = Types.required(BINARY).as(LogicalTypeAnnotation.decimalType(3, 4)).named("aDecimal");    Assert.assertEquals(expected, actual);}
0
public void testDecimalLogicalTypeWithDeprecatedScale()
{    PrimitiveType expected = new PrimitiveType(REQUIRED, BINARY, "aDecimal", LogicalTypeAnnotation.decimalType(3, 4));    PrimitiveType actual = Types.required(BINARY).as(LogicalTypeAnnotation.decimalType(3, 4)).scale(3).named("aDecimal");    Assert.assertEquals(expected, actual);}
0
public void testDecimalLogicalTypeWithDeprecatedPrecision()
{    PrimitiveType expected = new PrimitiveType(REQUIRED, BINARY, "aDecimal", LogicalTypeAnnotation.decimalType(3, 4));    PrimitiveType actual = Types.required(BINARY).as(LogicalTypeAnnotation.decimalType(3, 4)).precision(4).named("aDecimal");    Assert.assertEquals(expected, actual);}
0
public void testTimestampLogicalTypeWithUTCParameter()
{    PrimitiveType utcMillisExpected = new PrimitiveType(REQUIRED, INT64, "aTimestamp", timestampType(true, MILLIS));    PrimitiveType nonUtcMillisExpected = new PrimitiveType(REQUIRED, INT64, "aTimestamp", timestampType(false, MILLIS));    PrimitiveType utcMicrosExpected = new PrimitiveType(REQUIRED, INT64, "aTimestamp", timestampType(true, MICROS));    PrimitiveType nonUtcMicrosExpected = new PrimitiveType(REQUIRED, INT64, "aTimestamp", timestampType(false, MICROS));    PrimitiveType utcMillisActual = Types.required(INT64).as(timestampType(true, MILLIS)).named("aTimestamp");    PrimitiveType nonUtcMillisActual = Types.required(INT64).as(timestampType(false, MILLIS)).named("aTimestamp");    PrimitiveType utcMicrosActual = Types.required(INT64).as(timestampType(true, MICROS)).named("aTimestamp");    PrimitiveType nonUtcMicrosActual = Types.required(INT64).as(timestampType(false, MICROS)).named("aTimestamp");    Assert.assertEquals(utcMillisExpected, utcMillisActual);    Assert.assertEquals(nonUtcMillisExpected, nonUtcMillisActual);    Assert.assertEquals(utcMicrosExpected, utcMicrosActual);    Assert.assertEquals(nonUtcMicrosExpected, nonUtcMicrosActual);}
0
public void testDecimalLogicalTypeWithDeprecatedScaleMismatch()
{    Types.required(BINARY).as(LogicalTypeAnnotation.decimalType(3, 4)).scale(4).named("aDecimal");}
0
public void testDecimalLogicalTypeWithDeprecatedPrecisionMismatch()
{    Types.required(BINARY).as(LogicalTypeAnnotation.decimalType(3, 4)).precision(5).named("aDecimal");}
0
public static void assertThrows(String message, Class<? extends Exception> expected, Callable callable)
{    try {        callable.call();        Assert.fail("No exception was thrown (" + message + "), expected: " + expected.getName());    } catch (Exception actual) {        Assert.assertEquals(message, expected, actual.getClass());    }}
0
public void testWriteCheckMessageType()
{    TypeUtil.checkValidWriteSchema(Types.buildMessage().required(INT32).named("a").optional(BINARY).as(UTF8).named("b").named("valid_schema"));    TestTypeBuilders.assertThrows("Should complain about empty MessageType", InvalidSchemaException.class, (Callable<Void>) () -> {        TypeUtil.checkValidWriteSchema(new MessageType("invalid_schema"));        return null;    });}
0
public void testWriteCheckGroupType()
{    TypeUtil.checkValidWriteSchema(Types.repeatedGroup().required(INT32).named("a").optional(BINARY).as(UTF8).named("b").named("valid_group"));    TestTypeBuilders.assertThrows("Should complain about empty GroupType", InvalidSchemaException.class, (Callable<Void>) () -> {        TypeUtil.checkValidWriteSchema(new GroupType(REPEATED, "invalid_group"));        return null;    });}
0
public void testWriteCheckNestedGroupType()
{    TypeUtil.checkValidWriteSchema(Types.buildMessage().repeatedGroup().required(INT32).named("a").optional(BINARY).as(UTF8).named("b").named("valid_group").named("valid_message"));    TestTypeBuilders.assertThrows("Should complain about empty GroupType", InvalidSchemaException.class, (Callable<Void>) () -> {        TypeUtil.checkValidWriteSchema(Types.buildMessage().addField(new GroupType(REPEATED, "invalid_group")).named("invalid_message"));        return null;    });}
0
public static ByteBufferInputStream wrap(ByteBuffer... buffers)
{    if (buffers.length == 1) {        return new SingleBufferInputStream(buffers[0]);    } else {        return new MultiBufferInputStream(Arrays.asList(buffers));    }}
0
public static ByteBufferInputStream wrap(List<ByteBuffer> buffers)
{    if (buffers.size() == 1) {        return new SingleBufferInputStream(buffers.get(0));    } else {        return new MultiBufferInputStream(buffers);    }}
0
public ByteBuffer toByteBuffer()
{    try {        return slice(available());    } catch (EOFException e) {        throw new ShouldNeverHappenException(e);    }}
0
public long position()
{    return delegate.position();}
0
public void skipFully(long n) throws IOException
{    long skipped = skip(n);    if (skipped < n) {        throw new EOFException("Not enough bytes to skip: " + skipped + " < " + n);    }}
0
public int read(ByteBuffer out)
{    return delegate.read(out);}
0
public ByteBuffer slice(int length) throws EOFException
{    return delegate.slice(length);}
0
public List<ByteBuffer> sliceBuffers(long length) throws EOFException
{    return delegate.sliceBuffers(length);}
0
public ByteBufferInputStream sliceStream(long length) throws EOFException
{    return ByteBufferInputStream.wrap(sliceBuffers(length));}
0
public List<ByteBuffer> remainingBuffers()
{    return delegate.remainingBuffers();}
0
public ByteBufferInputStream remainingStream()
{    return ByteBufferInputStream.wrap(remainingBuffers());}
0
public int read() throws IOException
{    return delegate.read();}
0
public int read(byte[] b, int off, int len) throws IOException
{    return delegate.read(b, off, len);}
0
public long skip(long n)
{    return delegate.skip(n);}
0
public int available()
{    return delegate.available();}
0
public void mark(int readlimit)
{    delegate.mark(readlimit);}
0
public void reset() throws IOException
{    delegate.reset();}
0
public boolean markSupported()
{    return delegate.markSupported();}
0
public static BytesInput concat(BytesInput... inputs)
{    return new SequenceBytesIn(Arrays.asList(inputs));}
0
public static BytesInput concat(List<BytesInput> inputs)
{    return new SequenceBytesIn(inputs);}
0
public static BytesInput from(InputStream in, int bytes)
{    return new StreamBytesInput(in, bytes);}
0
public static BytesInput from(ByteBuffer buffer, int offset, int length)
{    ByteBuffer tmp = buffer.duplicate();    tmp.position(offset);    ByteBuffer slice = tmp.slice();    slice.limit(length);    return new ByteBufferBytesInput(slice);}
0
public static BytesInput from(ByteBuffer... buffers)
{    if (buffers.length == 1) {        return new ByteBufferBytesInput(buffers[0]);    }    return new BufferListBytesInput(Arrays.asList(buffers));}
0
public static BytesInput from(List<ByteBuffer> buffers)
{    if (buffers.size() == 1) {        return new ByteBufferBytesInput(buffers.get(0));    }    return new BufferListBytesInput(buffers);}
0
public static BytesInput from(byte[] in)
{        return new ByteArrayBytesInput(in, 0, in.length);}
1
public static BytesInput from(byte[] in, int offset, int length)
{        return new ByteArrayBytesInput(in, offset, length);}
1
public static BytesInput fromInt(int intValue)
{    return new IntBytesInput(intValue);}
0
public static BytesInput fromUnsignedVarInt(int intValue)
{    return new UnsignedVarIntBytesInput(intValue);}
0
public static BytesInput fromZigZagVarInt(int intValue)
{    int zigZag = (intValue << 1) ^ (intValue >> 31);    return new UnsignedVarIntBytesInput(zigZag);}
0
public static BytesInput fromUnsignedVarLong(long longValue)
{    return new UnsignedVarLongBytesInput(longValue);}
0
public static BytesInput fromZigZagVarLong(long longValue)
{    long zigZag = (longValue << 1) ^ (longValue >> 63);    return new UnsignedVarLongBytesInput(zigZag);}
0
public static BytesInput from(CapacityByteArrayOutputStream arrayOut)
{    return new CapacityBAOSBytesInput(arrayOut);}
0
public static BytesInput from(ByteArrayOutputStream baos)
{    return new BAOSBytesInput(baos);}
0
public static BytesInput empty()
{    return EMPTY_BYTES_INPUT;}
0
public static BytesInput copy(BytesInput bytesInput) throws IOException
{    return from(bytesInput.toByteArray());}
0
public byte[] toByteArray() throws IOException
{    BAOS baos = new BAOS((int) size());    this.writeAllTo(baos);        return baos.getBuf();}
1
public ByteBuffer toByteBuffer() throws IOException
{    return ByteBuffer.wrap(toByteArray());}
0
public ByteBufferInputStream toInputStream() throws IOException
{    return ByteBufferInputStream.wrap(toByteBuffer());}
0
public byte[] getBuf()
{    return this.buf;}
0
public void writeAllTo(OutputStream out) throws IOException
{            out.write(this.toByteArray());}
1
public byte[] toByteArray() throws IOException
{        byte[] buf = new byte[byteCount];    new DataInputStream(in).readFully(buf);    return buf;}
1
public long size()
{    return byteCount;}
0
public long size()
{    return size;}
0
public void writeAllTo(OutputStream out) throws IOException
{    BytesUtils.writeIntLittleEndian(out, intValue);}
0
public ByteBuffer toByteBuffer() throws IOException
{    return ByteBuffer.allocate(4).putInt(0, intValue);}
0
public long size()
{    return 4;}
0
public void writeAllTo(OutputStream out) throws IOException
{    BytesUtils.writeUnsignedVarInt(intValue, out);}
0
public ByteBuffer toByteBuffer() throws IOException
{    ByteBuffer ret = ByteBuffer.allocate((int) size());    BytesUtils.writeUnsignedVarInt(intValue, ret);    return ret;}
0
public long size()
{    int s = (38 - Integer.numberOfLeadingZeros(intValue)) / 7;    return s == 0 ? 1 : s;}
0
public void writeAllTo(OutputStream out) throws IOException
{    BytesUtils.writeUnsignedVarLong(longValue, out);}
0
public long size()
{    int s = (70 - Long.numberOfLeadingZeros(longValue)) / 7;    return s == 0 ? 1 : s;}
0
public void writeAllTo(OutputStream out) throws IOException
{}
0
public long size()
{    return 0;}
0
public ByteBuffer toByteBuffer() throws IOException
{    return ByteBuffer.allocate(0);}
0
public void writeAllTo(OutputStream out) throws IOException
{    arrayOut.writeTo(out);}
0
public long size()
{    return arrayOut.size();}
0
public void writeAllTo(OutputStream out) throws IOException
{    arrayOut.writeTo(out);}
0
public long size()
{    return arrayOut.size();}
0
public void writeAllTo(OutputStream out) throws IOException
{    out.write(in, offset, length);}
0
public ByteBuffer toByteBuffer() throws IOException
{    return java.nio.ByteBuffer.wrap(in, offset, length);}
0
public long size()
{    return length;}
0
public void writeAllTo(OutputStream out) throws IOException
{    WritableByteChannel channel = Channels.newChannel(out);    for (ByteBuffer buffer : buffers) {        channel.write(buffer.duplicate());    }}
0
public ByteBufferInputStream toInputStream()
{    return ByteBufferInputStream.wrap(buffers);}
0
public long size()
{    return length;}
0
public void writeAllTo(OutputStream out) throws IOException
{    Channels.newChannel(out).write(buffer.duplicate());}
0
public ByteBufferInputStream toInputStream()
{    return ByteBufferInputStream.wrap(buffer);}
0
public long size()
{    return buffer.remaining();}
0
public static int getWidthFromMaxInt(int bound)
{    return 32 - Integer.numberOfLeadingZeros(bound);}
0
public static int readIntLittleEndian(ByteBuffer in, int offset) throws IOException
{    int ch4 = in.get(offset) & 0xff;    int ch3 = in.get(offset + 1) & 0xff;    int ch2 = in.get(offset + 2) & 0xff;    int ch1 = in.get(offset + 3) & 0xff;    return ((ch1 << 24) + (ch2 << 16) + (ch3 << 8) + (ch4 << 0));}
0
public static int readIntLittleEndian(byte[] in, int offset) throws IOException
{    int ch4 = in[offset] & 0xff;    int ch3 = in[offset + 1] & 0xff;    int ch2 = in[offset + 2] & 0xff;    int ch1 = in[offset + 3] & 0xff;    return ((ch1 << 24) + (ch2 << 16) + (ch3 << 8) + (ch4 << 0));}
0
public static int readIntLittleEndian(InputStream in) throws IOException
{        int ch1 = in.read();    int ch2 = in.read();    int ch3 = in.read();    int ch4 = in.read();    if ((ch1 | ch2 | ch3 | ch4) < 0) {        throw new EOFException();    }    return ((ch4 << 24) + (ch3 << 16) + (ch2 << 8) + (ch1 << 0));}
0
public static int readIntLittleEndianOnOneByte(InputStream in) throws IOException
{    int ch1 = in.read();    if (ch1 < 0) {        throw new EOFException();    }    return ch1;}
0
public static int readIntLittleEndianOnTwoBytes(InputStream in) throws IOException
{    int ch1 = in.read();    int ch2 = in.read();    if ((ch1 | ch2) < 0) {        throw new EOFException();    }    return ((ch2 << 8) + (ch1 << 0));}
0
public static int readIntLittleEndianOnThreeBytes(InputStream in) throws IOException
{    int ch1 = in.read();    int ch2 = in.read();    int ch3 = in.read();    if ((ch1 | ch2 | ch3) < 0) {        throw new EOFException();    }    return ((ch3 << 16) + (ch2 << 8) + (ch1 << 0));}
0
public static int readIntLittleEndianPaddedOnBitWidth(InputStream in, int bitWidth) throws IOException
{    int bytesWidth = paddedByteCountFromBits(bitWidth);    switch(bytesWidth) {        case 0:            return 0;        case 1:            return BytesUtils.readIntLittleEndianOnOneByte(in);        case 2:            return BytesUtils.readIntLittleEndianOnTwoBytes(in);        case 3:            return BytesUtils.readIntLittleEndianOnThreeBytes(in);        case 4:            return BytesUtils.readIntLittleEndian(in);        default:            throw new IOException(String.format("Encountered bitWidth (%d) that requires more than 4 bytes", bitWidth));    }}
0
public static void writeIntLittleEndianOnOneByte(OutputStream out, int v) throws IOException
{    out.write((v >>> 0) & 0xFF);}
0
public static void writeIntLittleEndianOnTwoBytes(OutputStream out, int v) throws IOException
{    out.write((v >>> 0) & 0xFF);    out.write((v >>> 8) & 0xFF);}
0
public static void writeIntLittleEndianOnThreeBytes(OutputStream out, int v) throws IOException
{    out.write((v >>> 0) & 0xFF);    out.write((v >>> 8) & 0xFF);    out.write((v >>> 16) & 0xFF);}
0
public static void writeIntLittleEndianPaddedOnBitWidth(OutputStream out, int v, int bitWidth) throws IOException
{    int bytesWidth = paddedByteCountFromBits(bitWidth);    switch(bytesWidth) {        case 0:            break;        case 1:            writeIntLittleEndianOnOneByte(out, v);            break;        case 2:            writeIntLittleEndianOnTwoBytes(out, v);            break;        case 3:            writeIntLittleEndianOnThreeBytes(out, v);            break;        case 4:            writeIntLittleEndian(out, v);            break;        default:            throw new IOException(String.format("Encountered value (%d) that requires more than 4 bytes", v));    }}
0
public static int readUnsignedVarInt(InputStream in) throws IOException
{    int value = 0;    int i = 0;    int b;    while (((b = in.read()) & 0x80) != 0) {        value |= (b & 0x7F) << i;        i += 7;    }    return value | (b << i);}
0
public static int readZigZagVarInt(InputStream in) throws IOException
{    int raw = readUnsignedVarInt(in);    int temp = (((raw << 31) >> 31) ^ raw) >> 1;    return temp ^ (raw & (1 << 31));}
0
public static void writeUnsignedVarInt(int value, OutputStream out) throws IOException
{    while ((value & 0xFFFFFF80) != 0L) {        out.write((value & 0x7F) | 0x80);        value >>>= 7;    }    out.write(value & 0x7F);}
0
public static void writeUnsignedVarInt(int value, ByteBuffer dest) throws IOException
{    while ((value & 0xFFFFFF80) != 0L) {        dest.putInt((value & 0x7F) | 0x80);        value >>>= 7;    }    dest.putInt(value & 0x7F);}
0
public static void writeZigZagVarInt(int intValue, OutputStream out) throws IOException
{    writeUnsignedVarInt((intValue << 1) ^ (intValue >> 31), out);}
0
public static long readZigZagVarLong(InputStream in) throws IOException
{    long raw = readUnsignedVarLong(in);    long temp = (((raw << 63) >> 63) ^ raw) >> 1;    return temp ^ (raw & (1L << 63));}
0
public static long readUnsignedVarLong(InputStream in) throws IOException
{    long value = 0;    int i = 0;    long b;    while (((b = in.read()) & 0x80) != 0) {        value |= (b & 0x7F) << i;        i += 7;    }    return value | (b << i);}
0
public static void writeUnsignedVarLong(long value, OutputStream out) throws IOException
{    while ((value & 0xFFFFFFFFFFFFFF80L) != 0L) {        out.write((int) ((value & 0x7F) | 0x80));        value >>>= 7;    }    out.write((int) (value & 0x7F));}
0
public static void writeZigZagVarLong(long longValue, OutputStream out) throws IOException
{    writeUnsignedVarLong((longValue << 1) ^ (longValue >> 63), out);}
0
public static int paddedByteCountFromBits(int bitLength)
{    return (bitLength + 7) / 8;}
0
public static byte[] intToBytes(int value)
{    byte[] outBuffer = new byte[4];    outBuffer[3] = (byte) (value >>> 24);    outBuffer[2] = (byte) (value >>> 16);    outBuffer[1] = (byte) (value >>> 8);    outBuffer[0] = (byte) (value >>> 0);    return outBuffer;}
0
public static int bytesToInt(byte[] bytes)
{    return ((int) (bytes[3] & 255) << 24) + ((int) (bytes[2] & 255) << 16) + ((int) (bytes[1] & 255) << 8) + ((int) (bytes[0] & 255) << 0);}
0
public static byte[] longToBytes(long value)
{    byte[] outBuffer = new byte[8];    outBuffer[7] = (byte) (value >>> 56);    outBuffer[6] = (byte) (value >>> 48);    outBuffer[5] = (byte) (value >>> 40);    outBuffer[4] = (byte) (value >>> 32);    outBuffer[3] = (byte) (value >>> 24);    outBuffer[2] = (byte) (value >>> 16);    outBuffer[1] = (byte) (value >>> 8);    outBuffer[0] = (byte) (value >>> 0);    return outBuffer;}
0
public static long bytesToLong(byte[] bytes)
{    return (((long) bytes[7] << 56) + ((long) (bytes[6] & 255) << 48) + ((long) (bytes[5] & 255) << 40) + ((long) (bytes[4] & 255) << 32) + ((long) (bytes[3] & 255) << 24) + ((long) (bytes[2] & 255) << 16) + ((long) (bytes[1] & 255) << 8) + ((long) (bytes[0] & 255) << 0));}
0
public static byte[] booleanToBytes(boolean value)
{    byte[] outBuffer = new byte[1];    outBuffer[0] = (byte) (value ? 1 : 0);    return outBuffer;}
0
public static boolean bytesToBool(byte[] bytes)
{    return ((int) (bytes[0] & 255) != 0);}
0
public static int initialSlabSizeHeuristic(int minSlabSize, int targetCapacity, int targetNumSlabs)
{        return max(minSlabSize, ((int) (targetCapacity / pow(2, targetNumSlabs))));}
0
public static CapacityByteArrayOutputStream withTargetNumSlabs(int minSlabSize, int maxCapacityHint, int targetNumSlabs)
{    return withTargetNumSlabs(minSlabSize, maxCapacityHint, targetNumSlabs, new HeapByteBufferAllocator());}
0
public static CapacityByteArrayOutputStream withTargetNumSlabs(int minSlabSize, int maxCapacityHint, int targetNumSlabs, ByteBufferAllocator allocator)
{    return new CapacityByteArrayOutputStream(initialSlabSizeHeuristic(minSlabSize, maxCapacityHint, targetNumSlabs), maxCapacityHint, allocator);}
0
private void addSlab(int minimumSize)
{    int nextSlabSize;    if (bytesUsed == 0) {        nextSlabSize = initialSlabSize;    } else if (bytesUsed > maxCapacityHint / 5) {                nextSlabSize = maxCapacityHint / 5;    } else {                nextSlabSize = bytesUsed;    }    if (nextSlabSize < minimumSize) {                nextSlabSize = minimumSize;    }        this.currentSlab = allocator.allocate(nextSlabSize);    this.slabs.add(currentSlab);    this.bytesAllocated += nextSlabSize;    this.currentSlabIndex = 0;}
1
public void write(int b)
{    if (!currentSlab.hasRemaining()) {        addSlab(1);    }    currentSlab.put(currentSlabIndex, (byte) b);    currentSlabIndex += 1;    currentSlab.position(currentSlabIndex);    bytesUsed += 1;}
0
public void write(byte[] b, int off, int len)
{    if ((off < 0) || (off > b.length) || (len < 0) || ((off + len) - b.length > 0)) {        throw new IndexOutOfBoundsException(String.format("Given byte array of size %d, with requested length(%d) and offset(%d)", b.length, len, off));    }    if (len >= currentSlab.remaining()) {        final int length1 = currentSlab.remaining();        currentSlab.put(b, off, length1);        bytesUsed += length1;        currentSlabIndex += length1;        final int length2 = len - length1;        addSlab(length2);        currentSlab.put(b, off + length1, length2);        currentSlabIndex = length2;        bytesUsed += length2;    } else {        currentSlab.put(b, off, len);        currentSlabIndex += len;        bytesUsed += len;    }}
0
private void writeToOutput(OutputStream out, ByteBuffer buf, int len) throws IOException
{    if (buf.hasArray()) {        out.write(buf.array(), buf.arrayOffset(), len);    } else {                        byte[] copy = new byte[len];        buf.flip();        buf.get(copy);        out.write(copy);    }}
0
public void writeTo(OutputStream out) throws IOException
{    for (int i = 0; i < slabs.size() - 1; i++) {        writeToOutput(out, slabs.get(i), slabs.get(i).position());    }    writeToOutput(out, currentSlab, currentSlabIndex);}
0
public long size()
{    return bytesUsed;}
0
public int getCapacity()
{    return bytesAllocated;}
0
public void reset()
{            this.initialSlabSize = max(bytesUsed / 7, initialSlabSize);        for (ByteBuffer slab : slabs) {        allocator.release(slab);    }    this.slabs.clear();    this.bytesAllocated = 0;    this.bytesUsed = 0;    this.currentSlab = EMPTY_SLAB;    this.currentSlabIndex = 0;}
1
public long getCurrentIndex()
{    checkArgument(bytesUsed > 0, "This is an empty stream");    return bytesUsed - 1;}
0
public void setByte(long index, byte value)
{    checkArgument(index < bytesUsed, "Index: " + index + " is >= the current size of: " + bytesUsed);    long seen = 0;    for (int i = 0; i < slabs.size(); i++) {        ByteBuffer slab = slabs.get(i);        if (index < seen + slab.limit()) {                        slab.put((int) (index - seen), value);            break;        }        seen += slab.limit();    }}
0
public String memUsageString(String prefix)
{    return format("%s %s %d slabs, %,d bytes", prefix, getClass().getSimpleName(), slabs.size(), getCapacity());}
0
 int getSlabCount()
{    return slabs.size();}
0
public void close()
{    for (ByteBuffer slab : slabs) {        allocator.release(slab);    }    try {        super.close();    } catch (IOException e) {        throw new OutputStreamCloseException(e);    }}
0
public void collect(BytesInput bytesInput) throws IOException
{    byte[] bytes = bytesInput.toByteArray();    slabs.add(bytes);    size += bytes.length;}
0
public void reset()
{    size = 0;    slabs.clear();}
0
public void writeAllTo(OutputStream out) throws IOException
{    for (byte[] slab : slabs) {        out.write(slab);    }}
0
public long size()
{    return size;}
0
public String memUsageString(String prefix)
{    return format("%s %s %d slabs, %,d bytes", prefix, getClass().getSimpleName(), slabs.size(), size);}
0
public static final DirectByteBufferAllocator getInstance()
{    return new DirectByteBufferAllocator();}
0
public ByteBuffer allocate(final int size)
{    return ByteBuffer.allocateDirect(size);}
0
public void release(ByteBuffer b)
{        return;}
0
public boolean isDirect()
{    return true;}
0
public static final HeapByteBufferAllocator getInstance()
{    return new HeapByteBufferAllocator();}
0
public ByteBuffer allocate(final int size)
{    return ByteBuffer.allocate(size);}
0
public void release(ByteBuffer b)
{    return;}
0
public boolean isDirect()
{    return false;}
0
public final void readFully(byte[] b) throws IOException
{    readFully(b, 0, b.length);}
0
public final void readFully(byte[] b, int off, int len) throws IOException
{    if (len < 0)        throw new IndexOutOfBoundsException();    int n = 0;    while (n < len) {        int count = in.read(b, off + n, len - n);        if (count < 0)            throw new EOFException();        n += count;    }}
0
public final int skipBytes(int n) throws IOException
{    int total = 0;    int cur = 0;    while ((total < n) && ((cur = (int) in.skip(n - total)) > 0)) {        total += cur;    }    return total;}
0
public int read() throws IOException
{    return in.read();}
0
public int hashCode()
{    return in.hashCode();}
0
public int read(byte[] b) throws IOException
{    return in.read(b);}
0
public boolean equals(Object obj)
{    return in.equals(obj);}
0
public int read(byte[] b, int off, int len) throws IOException
{    return in.read(b, off, len);}
0
public long skip(long n) throws IOException
{    return in.skip(n);}
0
public int available() throws IOException
{    return in.available();}
0
public void close() throws IOException
{    in.close();}
0
public void mark(int readlimit)
{    in.mark(readlimit);}
0
public void reset() throws IOException
{    in.reset();}
0
public boolean markSupported()
{    return in.markSupported();}
0
public final boolean readBoolean() throws IOException
{    int ch = in.read();    if (ch < 0)        throw new EOFException();    return (ch != 0);}
0
public final byte readByte() throws IOException
{    int ch = in.read();    if (ch < 0)        throw new EOFException();    return (byte) (ch);}
0
public final int readUnsignedByte() throws IOException
{    int ch = in.read();    if (ch < 0)        throw new EOFException();    return ch;}
0
public final short readShort() throws IOException
{    int ch2 = in.read();    int ch1 = in.read();    if ((ch1 | ch2) < 0)        throw new EOFException();    return (short) ((ch1 << 8) + (ch2 << 0));}
0
public final int readUnsignedShort() throws IOException
{    int ch2 = in.read();    int ch1 = in.read();    if ((ch1 | ch2) < 0)        throw new EOFException();    return (ch1 << 8) + (ch2 << 0);}
0
public final int readInt() throws IOException
{                        int ch4 = in.read();    int ch3 = in.read();    int ch2 = in.read();    int ch1 = in.read();    if ((ch1 | ch2 | ch3 | ch4) < 0)        throw new EOFException();    return ((ch1 << 24) + (ch2 << 16) + (ch3 << 8) + (ch4 << 0));}
0
public final long readLong() throws IOException
{        readFully(readBuffer, 0, 8);    return (((long) readBuffer[7] << 56) + ((long) (readBuffer[6] & 255) << 48) + ((long) (readBuffer[5] & 255) << 40) + ((long) (readBuffer[4] & 255) << 32) + ((long) (readBuffer[3] & 255) << 24) + ((readBuffer[2] & 255) << 16) + ((readBuffer[1] & 255) << 8) + ((readBuffer[0] & 255) << 0));}
0
public final float readFloat() throws IOException
{    return Float.intBitsToFloat(readInt());}
0
public final double readDouble() throws IOException
{    return Double.longBitsToDouble(readLong());}
0
public void write(int b) throws IOException
{    out.write(b);}
0
public void write(byte[] b, int off, int len) throws IOException
{    out.write(b, off, len);}
0
public void flush() throws IOException
{    out.flush();}
0
public final void writeBoolean(boolean v) throws IOException
{    out.write(v ? 1 : 0);}
0
public final void writeByte(int v) throws IOException
{    out.write(v);}
0
public final void writeShort(int v) throws IOException
{    out.write((v >>> 0) & 0xFF);    out.write((v >>> 8) & 0xFF);}
0
public final void writeInt(int v) throws IOException
{                out.write((v >>> 0) & 0xFF);    out.write((v >>> 8) & 0xFF);    out.write((v >>> 16) & 0xFF);    out.write((v >>> 24) & 0xFF);}
0
public final void writeLong(long v) throws IOException
{    writeBuffer[7] = (byte) (v >>> 56);    writeBuffer[6] = (byte) (v >>> 48);    writeBuffer[5] = (byte) (v >>> 40);    writeBuffer[4] = (byte) (v >>> 32);    writeBuffer[3] = (byte) (v >>> 24);    writeBuffer[2] = (byte) (v >>> 16);    writeBuffer[1] = (byte) (v >>> 8);    writeBuffer[0] = (byte) (v >>> 0);    out.write(writeBuffer, 0, 8);}
0
public final void writeFloat(float v) throws IOException
{    writeInt(Float.floatToIntBits(v));}
0
public final void writeDouble(double v) throws IOException
{    writeLong(Double.doubleToLongBits(v));}
0
public void close()
{    IOExceptionUtils.closeQuietly(out);}
0
public long position()
{    return position;}
0
public long skip(long n)
{    if (n <= 0) {        return 0;    }    if (current == null) {        return -1;    }    long bytesSkipped = 0;    while (bytesSkipped < n) {        if (current.remaining() > 0) {            long bytesToSkip = Math.min(n - bytesSkipped, current.remaining());            current.position(current.position() + (int) bytesToSkip);            bytesSkipped += bytesToSkip;            this.position += bytesToSkip;        } else if (!nextBuffer()) {                        return bytesSkipped > 0 ? bytesSkipped : -1;        }    }    return bytesSkipped;}
0
public int read(ByteBuffer out)
{    int len = out.remaining();    if (len <= 0) {        return 0;    }    if (current == null) {        return -1;    }    int bytesCopied = 0;    while (bytesCopied < len) {        if (current.remaining() > 0) {            int bytesToCopy;            ByteBuffer copyBuffer;            if (current.remaining() <= out.remaining()) {                                bytesToCopy = current.remaining();                copyBuffer = current;            } else {                                bytesToCopy = out.remaining();                copyBuffer = current.duplicate();                copyBuffer.limit(copyBuffer.position() + bytesToCopy);                current.position(copyBuffer.position() + bytesToCopy);            }            out.put(copyBuffer);            bytesCopied += bytesToCopy;            this.position += bytesToCopy;        } else if (!nextBuffer()) {                        return bytesCopied > 0 ? bytesCopied : -1;        }    }    return bytesCopied;}
0
public ByteBuffer slice(int length) throws EOFException
{    if (length <= 0) {        return EMPTY;    }    if (current == null) {        throw new EOFException();    }    ByteBuffer slice;    if (length > current.remaining()) {                        slice = ByteBuffer.allocate(length);        int bytesCopied = read(slice);        slice.flip();        if (bytesCopied < length) {            throw new EOFException();        }    } else {        slice = current.duplicate();        slice.limit(slice.position() + length);        current.position(slice.position() + length);        this.position += length;    }    return slice;}
0
public List<ByteBuffer> sliceBuffers(long len) throws EOFException
{    if (len <= 0) {        return Collections.emptyList();    }    if (current == null) {        throw new EOFException();    }    List<ByteBuffer> buffers = new ArrayList<>();    long bytesAccumulated = 0;    while (bytesAccumulated < len) {        if (current.remaining() > 0) {                                    int bufLen = (int) Math.min(len - bytesAccumulated, current.remaining());            ByteBuffer slice = current.duplicate();            slice.limit(slice.position() + bufLen);            buffers.add(slice);            bytesAccumulated += bufLen;                        current.position(current.position() + bufLen);            this.position += bufLen;        } else if (!nextBuffer()) {                        throw new EOFException();        }    }    return buffers;}
0
public List<ByteBuffer> remainingBuffers()
{    if (position >= length) {        return Collections.emptyList();    }    try {        return sliceBuffers(length - position);    } catch (EOFException e) {        throw new RuntimeException("[Parquet bug] Stream is bad: incorrect bytes remaining " + (length - position));    }}
0
public int read(byte[] bytes, int off, int len)
{    if (len <= 0) {        if (len < 0) {            throw new IndexOutOfBoundsException("Read length must be greater than 0: " + len);        }        return 0;    }    if (current == null) {        return -1;    }    int bytesRead = 0;    while (bytesRead < len) {        if (current.remaining() > 0) {            int bytesToRead = Math.min(len - bytesRead, current.remaining());            current.get(bytes, off + bytesRead, bytesToRead);            bytesRead += bytesToRead;            this.position += bytesToRead;        } else if (!nextBuffer()) {                        return bytesRead > 0 ? bytesRead : -1;        }    }    return bytesRead;}
0
public int read(byte[] bytes)
{    return read(bytes, 0, bytes.length);}
0
public int read() throws IOException
{    if (current == null) {        throw new EOFException();    }    while (true) {        if (current.remaining() > 0) {            this.position += 1;                        return current.get() & 0xFF;        } else if (!nextBuffer()) {                        throw new EOFException();        }    }}
0
public int available()
{    long remaining = length - position;    if (remaining > Integer.MAX_VALUE) {        return Integer.MAX_VALUE;    } else {        return (int) remaining;    }}
0
public void mark(int readlimit)
{    if (mark >= 0) {        discardMark();    }    this.mark = position;    this.markLimit = mark + readlimit + 1;    if (current != null) {        markBuffers.add(current.duplicate());    }}
0
public void reset() throws IOException
{    if (mark >= 0 && position < markLimit) {        this.position = mark;                        this.iterator = concat(markBuffers.iterator(), iterator);        discardMark();                nextBuffer();    } else {        throw new IOException("No mark defined or has read past the previous mark limit");    }}
0
private void discardMark()
{    this.mark = -1;    this.markLimit = 0;    markBuffers = new ArrayList<>();}
0
public boolean markSupported()
{    return true;}
0
private boolean nextBuffer()
{    if (!iterator.hasNext()) {        this.current = null;        return false;    }    this.current = iterator.next().duplicate();    if (mark >= 0) {        if (position < markLimit) {                        markBuffers.add(current.duplicate());        } else {                        discardMark();        }    }    return true;}
0
private static Iterator<E> concat(Iterator<E> first, Iterator<E> second)
{    return new ConcatIterator<>(first, second);}
0
public boolean hasNext()
{    if (useFirst) {        if (first.hasNext()) {            return true;        } else {            useFirst = false;            return second.hasNext();        }    }    return second.hasNext();}
0
public E next()
{    if (useFirst && !first.hasNext()) {        useFirst = false;    }    if (!useFirst && !second.hasNext()) {        throw new NoSuchElementException();    }    if (useFirst) {        return first.next();    }    return second.next();}
0
public void remove()
{    if (useFirst) {        first.remove();    }    second.remove();}
0
public long position()
{        return buffer.position() - startPosition;}
0
public int read() throws IOException
{    if (!buffer.hasRemaining()) {        throw new EOFException();    }        return buffer.get() & 0xFF;}
0
public int read(byte[] bytes, int offset, int length) throws IOException
{    if (length == 0) {        return 0;    }    int remaining = buffer.remaining();    if (remaining <= 0) {        return -1;    }    int bytesToRead = Math.min(buffer.remaining(), length);    buffer.get(bytes, offset, bytesToRead);    return bytesToRead;}
0
public long skip(long n)
{    if (n == 0) {        return 0;    }    if (buffer.remaining() <= 0) {        return -1;    }        int bytesToSkip = (int) Math.min(buffer.remaining(), n);    buffer.position(buffer.position() + bytesToSkip);    return bytesToSkip;}
0
public int read(ByteBuffer out)
{    int bytesToCopy;    ByteBuffer copyBuffer;    if (buffer.remaining() <= out.remaining()) {                bytesToCopy = buffer.remaining();        copyBuffer = buffer;    } else {                bytesToCopy = out.remaining();        copyBuffer = buffer.duplicate();        copyBuffer.limit(buffer.position() + bytesToCopy);        buffer.position(buffer.position() + bytesToCopy);    }    out.put(copyBuffer);    out.flip();    return bytesToCopy;}
0
public ByteBuffer slice(int length) throws EOFException
{    if (buffer.remaining() < length) {        throw new EOFException();    }        ByteBuffer copy = buffer.duplicate();    copy.limit(copy.position() + length);    buffer.position(buffer.position() + length);    return copy;}
0
public List<ByteBuffer> sliceBuffers(long length) throws EOFException
{    if (length == 0) {        return Collections.emptyList();    }    if (length > buffer.remaining()) {        throw new EOFException();    }        return Collections.singletonList(slice((int) length));}
0
public List<ByteBuffer> remainingBuffers()
{    if (buffer.remaining() <= 0) {        return Collections.emptyList();    }    ByteBuffer remaining = buffer.duplicate();    buffer.position(buffer.limit());    return Collections.singletonList(remaining);}
0
public void mark(int readlimit)
{    this.mark = buffer.position();}
0
public void reset() throws IOException
{    if (mark >= 0) {        buffer.position(mark);        this.mark = -1;    } else {        throw new IOException("No mark defined");    }}
0
public boolean markSupported()
{    return true;}
0
public int available()
{    return buffer.remaining();}
0
public static void close(Closeable c) throws IOException
{    if (c == null) {        return;    }    c.close();}
0
public static void closeAndSwallowIOExceptions(Closeable c)
{    if (c == null) {        return;    }    try {        c.close();    } catch (IOException e) {            }}
1
public static void throwIfInstance(Throwable t, Class<E> excClass) throws E
{    if (excClass.isAssignableFrom(t.getClass())) {                throw excClass.cast(t);    }}
0
public static List<String> readAllLines(File file, Charset charset) throws IOException
{    BufferedReader reader = new BufferedReader(new InputStreamReader(new FileInputStream(file), charset));    try {        List<String> result = new ArrayList<String>();        for (; ; ) {            String line = reader.readLine();            if (line == null)                break;            result.add(line);        }        return result;    } finally {        reader.close();    }}
0
public static List<String> expand(String globPattern)
{    return GlobExpanderImpl.expand(GlobParser.parse(globPattern));}
0
public static List<String> expand(GlobNode node)
{    return node.accept(INSTANCE);}
0
public List<String> visit(Atom atom)
{        return Arrays.asList(atom.get());}
0
public List<String> visit(OneOf oneOf)
{                List<String> results = new ArrayList<String>();    for (GlobNode n : oneOf.getChildren()) {        results.addAll(n.accept(this));    }    return results;}
0
public List<String> visit(GlobNodeSequence seq)
{                    List<String> results = new ArrayList<String>();    for (GlobNode n : seq.getChildren()) {        results = crossOrTakeNonEmpty(results, n.accept(this));    }    return results;}
0
public static List<String> crossOrTakeNonEmpty(List<String> list1, List<String> list2)
{    if (list1.isEmpty()) {        ArrayList<String> result = new ArrayList<String>(list2.size());        result.addAll(list2);        return result;    }    if (list2.isEmpty()) {        ArrayList<String> result = new ArrayList<String>(list1.size());        result.addAll(list1);        return result;    }    List<String> result = new ArrayList<String>(list1.size() * list2.size());    for (String s1 : list1) {        for (String s2 : list2) {            result.add(s1 + s2);        }    }    return result;}
0
public String get()
{    return s;}
0
public boolean equals(Object o)
{    if (this == o)        return true;    return o != null && getClass() == o.getClass() && s.equals(((Atom) o).s);}
0
public int hashCode()
{    return s.hashCode();}
0
public String toString()
{    return "Atom(" + s + ")";}
0
public R accept(Visitor<R> visitor)
{    return visitor.visit(this);}
0
public List<GlobNode> getChildren()
{    return children;}
0
public boolean equals(Object o)
{    if (this == o)        return true;    return o != null && getClass() == o.getClass() && children.equals(((OneOf) o).children);}
0
public int hashCode()
{    return children.hashCode();}
0
public String toString()
{    return "OneOf" + children;}
0
public R accept(Visitor<R> visitor)
{    return visitor.visit(this);}
0
public List<GlobNode> getChildren()
{    return children;}
0
public boolean equals(Object o)
{    if (this == o)        return true;    return o != null && getClass() == o.getClass() && children.equals(((OneOf) o).children);}
0
public int hashCode()
{    return children.hashCode();}
0
public String toString()
{    return "GlobNodeSequence" + children;}
0
public R accept(Visitor<R> visitor)
{    return visitor.visit(this);}
0
public static GlobNodeSequence parse(String pattern)
{    if (pattern.isEmpty() || pattern.equals("{}")) {        return new GlobNodeSequence(Arrays.<GlobNode>asList(new Atom("")));    }            List<GlobNode> children = new ArrayList<GlobNode>();        int unmatchedBraces = 0;        int firstBrace = 0;        int anchor = 0;    for (int i = 0; i < pattern.length(); i++) {        char c = pattern.charAt(i);        switch(c) {            case ',':                if (unmatchedBraces == 0) {                                        throw new GlobParseException("Unexpected comma outside of a {} group:\n" + annotateMessage(pattern, i));                }                break;            case '{':                if (unmatchedBraces == 0) {                                        firstBrace = i;                }                unmatchedBraces++;                break;            case '}':                unmatchedBraces--;                if (unmatchedBraces < 0) {                    throw new GlobParseException("Unexpected closing }:\n" + annotateMessage(pattern, i));                }                if (unmatchedBraces == 0) {                                        if (anchor != firstBrace) {                                                                        children.add(new Atom(pattern.substring(anchor, firstBrace)));                    }                                                            children.add(parseOneOf(pattern.substring(firstBrace + 1, i)));                                        anchor = i + 1;                }                break;        }    }    if (unmatchedBraces > 0) {        throw new GlobParseException("Not enough close braces in: " + pattern);    }    if (anchor != pattern.length()) {                                children.add(new Atom(pattern.substring(anchor, pattern.length())));    }    return new GlobNodeSequence(children);}
0
private static OneOf parseOneOf(String pattern)
{    /*     * This method is only called when parsing the inside of a {} expression.     * So in the example above, of calling parse("apache{one,pre{x,y}post,two}parquet{a,b}")     * this method will get called on first "one,pre{x,y}post,two", then on "x,y" and then on "a,b"     *     * The inside of a {} expression essentially means "one of these comma separated expressions".     * So this gets parsed slightly differently than the top level string passed to parse().     *     * The algorithm works as follows:     * 1) Split the string on ',' -- but only commas that are not inside of {} expressions     * 2) Each of the splits can be parsed via the parse() method above     * 3) Add all parsed splits to a single parent OneOf.     */            List<GlobNode> children = new ArrayList<GlobNode>();        int unmatchedBraces = 0;        int anchor = 0;    for (int i = 0; i < pattern.length(); i++) {        char c = pattern.charAt(i);        switch(c) {            case ',':                                if (unmatchedBraces == 0) {                                                                                children.add(parse(pattern.substring(anchor, i)));                                        anchor = i + 1;                }                break;            case '{':                unmatchedBraces++;                break;            case '}':                unmatchedBraces--;                if (unmatchedBraces < 0) {                    throw new GlobParseException("Unexpected closing }:\n" + annotateMessage(pattern, i));                }                break;        }    }    if (unmatchedBraces > 0) {        throw new GlobParseException("Not enough close braces in: " + pattern);    }    if (anchor != pattern.length()) {                                children.add(parse(pattern.substring(anchor, pattern.length())));    }    if (pattern.length() > 0 && pattern.charAt(pattern.length() - 1) == ',') {                children.add(parse(""));    }    return new OneOf(children);}
0
private static String annotateMessage(String message, int pos)
{    StringBuilder sb = new StringBuilder(message);    sb.append('\n');    for (int i = 0; i < pos; i++) {        sb.append('-');    }    sb.append('^');    return sb.toString();}
0
public static String buildRegex(String wildcardPath, char delim)
{    if (wildcardPath.isEmpty()) {        return wildcardPath;    }    String delimStr = Pattern.quote(Character.toString(delim));        String[] splits = wildcardPath.split("\\*", -1);    StringBuilder regex = new StringBuilder();    for (int i = 0; i < splits.length; i++) {        if ((i == 0 || i == splits.length - 1) && splits[i].isEmpty()) {                        regex.append(STAR_REGEX);            continue;        }        if (splits[i].isEmpty()) {                        continue;        }                        regex.append(Pattern.quote(splits[i]));        if (i < splits.length - 1) {                        regex.append(STAR_REGEX);        }    }        regex.append(String.format(MORE_NESTED_FIELDS_TEMPLATE, delimStr));    return regex.toString();}
0
public boolean matches(String path)
{    return pattern.matcher(path).matches();}
0
public String getParentGlobPath()
{    return parentGlobPath;}
0
public String getOriginalPattern()
{    return originalPattern;}
0
public String toString()
{    return String.format("WildcardPath(parentGlobPath: '%s', pattern: '%s')", parentGlobPath, originalPattern);}
0
public boolean equals(Object o)
{    if (this == o)        return true;    if (o == null || getClass() != o.getClass())        return false;    WildcardPath wildcardPath = (WildcardPath) o;    return originalPattern.equals(wildcardPath.originalPattern);}
0
public int hashCode()
{    return originalPattern.hashCode();}
0
public Class getCodecClass()
{    return codecClass;}
0
public final T canonicalize(T value)
{    T canonical = canonicals.get(value);    if (canonical == null) {        value = toCanonical(value);        T existing = canonicals.putIfAbsent(value, value);                if (existing == null) {            canonical = value;        } else {            canonical = existing;        }    }    return canonical;}
0
protected T toCanonical(T value)
{    return value;}
0
protected ColumnPath toCanonical(ColumnPath value)
{    String[] path = new String[value.p.length];    for (int i = 0; i < value.p.length; i++) {        path[i] = value.p[i].intern();    }    return new ColumnPath(path);}
0
public static ColumnPath fromDotString(String path)
{    checkNotNull(path, "path");    return get(path.split("\\."));}
0
public static ColumnPath get(String... path)
{    return paths.canonicalize(new ColumnPath(path));}
0
public boolean equals(Object obj)
{    if (obj instanceof ColumnPath) {        return Arrays.equals(p, ((ColumnPath) obj).p);    }    return false;}
0
public int hashCode()
{    return Arrays.hashCode(p);}
0
public String toDotString()
{    return Strings.join(p, ".");}
0
public String toString()
{    return Arrays.toString(p);}
0
public Iterator<String> iterator()
{    return Arrays.asList(p).iterator();}
0
public int size()
{    return p.length;}
0
public String[] toArray()
{    return p;}
0
public static CompressionCodecName fromConf(String name)
{    if (name == null) {        return UNCOMPRESSED;    }    return valueOf(name.toUpperCase(Locale.ENGLISH));}
0
public static CompressionCodecName fromCompressionCodec(Class<?> clazz)
{    if (clazz == null) {        return UNCOMPRESSED;    }    String name = clazz.getName();    for (CompressionCodecName codec : CompressionCodecName.values()) {        if (name.equals(codec.getHadoopCompressionCodecClassName())) {            return codec;        }    }    throw new CompressionCodecNotSupportedException(clazz);}
0
public static CompressionCodecName fromParquet(CompressionCodec codec)
{    for (CompressionCodecName codecName : CompressionCodecName.values()) {        if (codec.equals(codecName.parquetCompressionCodec)) {            return codecName;        }    }    throw new IllegalArgumentException("Unknown compression codec " + codec);}
0
public String getHadoopCompressionCodecClassName()
{    return hadoopCompressionCodecClass;}
0
public Class getHadoopCompressionCodecClass()
{    String codecClassName = getHadoopCompressionCodecClassName();    if (codecClassName == null) {        return null;    }    try {        return Class.forName(codecClassName);    } catch (ClassNotFoundException e) {        return null;    }}
0
public CompressionCodec getParquetCompressionCodec()
{    return parquetCompressionCodec;}
0
public String getExtension()
{    return extension;}
0
public static int checkedCast(long value)
{    int valueI = (int) value;    if (valueI != value) {        throw new IllegalArgumentException(String.format("Overflow casting %d to an int", value));    }    return valueI;}
0
public OutputStream getStream()
{    return stream;}
0
public void close() throws IOException
{    stream.close();}
0
public void flush() throws IOException
{    stream.flush();}
0
public void write(int b) throws IOException
{    stream.write(b);}
0
public void write(byte[] b) throws IOException
{    stream.write(b);}
0
public void write(byte[] b, int off, int len) throws IOException
{    stream.write(b, off, len);}
0
public InputStream getStream()
{    return stream;}
0
public void close() throws IOException
{    stream.close();}
0
public int read() throws IOException
{    return stream.read();}
0
public int read(byte[] b, int off, int len) throws IOException
{    return stream.read(b, off, len);}
0
public void readFully(byte[] bytes) throws IOException
{    readFully(stream, bytes, 0, bytes.length);}
0
public void readFully(byte[] bytes, int start, int len) throws IOException
{    readFully(stream, bytes, start, len);}
0
public int read(ByteBuffer buf) throws IOException
{    if (buf.hasArray()) {        return readHeapBuffer(stream, buf);    } else {        return readDirectBuffer(stream, buf, temp);    }}
0
public void readFully(ByteBuffer buf) throws IOException
{    if (buf.hasArray()) {        readFullyHeapBuffer(stream, buf);    } else {        readFullyDirectBuffer(stream, buf, temp);    }}
0
 static void readFully(InputStream f, byte[] bytes, int start, int len) throws IOException
{    int offset = start;    int remaining = len;    while (remaining > 0) {        int bytesRead = f.read(bytes, offset, remaining);        if (bytesRead < 0) {            throw new EOFException("Reached the end of stream with " + remaining + " bytes left to read");        }        remaining -= bytesRead;        offset += bytesRead;    }}
0
 static int readHeapBuffer(InputStream f, ByteBuffer buf) throws IOException
{    int bytesRead = f.read(buf.array(), buf.arrayOffset() + buf.position(), buf.remaining());    if (bytesRead < 0) {                return bytesRead;    } else {        buf.position(buf.position() + bytesRead);        return bytesRead;    }}
0
 static void readFullyHeapBuffer(InputStream f, ByteBuffer buf) throws IOException
{    readFully(f, buf.array(), buf.arrayOffset() + buf.position(), buf.remaining());    buf.position(buf.limit());}
0
 static int readDirectBuffer(InputStream f, ByteBuffer buf, byte[] temp) throws IOException
{            int nextReadLength = Math.min(buf.remaining(), temp.length);    int totalBytesRead = 0;    int bytesRead;    while ((bytesRead = f.read(temp, 0, nextReadLength)) == temp.length) {        buf.put(temp);        totalBytesRead += bytesRead;        nextReadLength = Math.min(buf.remaining(), temp.length);    }    if (bytesRead < 0) {                return totalBytesRead == 0 ? -1 : totalBytesRead;    } else {                buf.put(temp, 0, bytesRead);        totalBytesRead += bytesRead;        return totalBytesRead;    }}
0
 static void readFullyDirectBuffer(InputStream f, ByteBuffer buf, byte[] temp) throws IOException
{    int nextReadLength = Math.min(buf.remaining(), temp.length);    int bytesRead = 0;    while (nextReadLength > 0 && (bytesRead = f.read(temp, 0, nextReadLength)) >= 0) {        buf.put(temp, 0, bytesRead);        nextReadLength = Math.min(buf.remaining(), temp.length);    }    if (bytesRead < 0 && buf.remaining() > 0) {        throw new EOFException("Reached the end of stream with " + buf.remaining() + " bytes left to read");    }}
0
public static void closeQuietly(Closeable closeable)
{    try {        closeable.close();    } catch (IOException e) {        throw new ParquetRuntimeException("Error closing I/O related resources.", e) {        };    }}
0
public static Log getLog(Class<?> c)
{    return new Log(c);}
0
public void debug(Object m)
{    if (m instanceof Throwable) {            } else {            }}
1
public void debug(Object m, Throwable t)
{    }
1
public void info(Object m)
{    if (m instanceof Throwable) {            } else {            }}
1
public void info(Object m, Throwable t)
{    }
1
public void warn(Object m)
{    if (m instanceof Throwable) {            } else {            }}
1
public void warn(Object m, Throwable t)
{    }
1
public void error(Object m)
{    if (m instanceof Throwable) {            } else {            }}
1
public void error(Object m, Throwable t)
{    }
1
public static T checkNotNull(T o, String name) throws NullPointerException
{    if (o == null) {        throw new NullPointerException(name + " should not be null");    }    return o;}
0
public static void checkArgument(boolean isValid, String message) throws IllegalArgumentException
{    if (!isValid) {        throw new IllegalArgumentException(message);    }}
0
public static void checkArgument(boolean isValid, String message, Object... args) throws IllegalArgumentException
{    if (!isValid) {        throw new IllegalArgumentException(String.format(String.valueOf(message), strings(args)));    }}
0
public static void checkState(boolean isValid, String message) throws IllegalStateException
{    if (!isValid) {        throw new IllegalStateException(message);    }}
0
public static void checkState(boolean isValid, String message, Object... args) throws IllegalStateException
{    if (!isValid) {        throw new IllegalStateException(String.format(String.valueOf(message), strings(args)));    }}
0
private static String[] strings(Object[] objects)
{    String[] strings = new String[objects.length];    for (int i = 0; i < objects.length; i += 1) {        strings[i] = String.valueOf(objects[i]);    }    return strings;}
0
public static SemanticVersion parse(String version) throws SemanticVersionParseException
{    Matcher matcher = PATTERN.matcher(version);    if (!matcher.matches()) {        throw new SemanticVersionParseException("" + version + " does not match format " + FORMAT);    }    final int major;    final int minor;    final int patch;    try {        major = Integer.valueOf(matcher.group(1));        minor = Integer.valueOf(matcher.group(2));        patch = Integer.valueOf(matcher.group(3));    } catch (NumberFormatException e) {        throw new SemanticVersionParseException(e);    }    final String unknown = matcher.group(4);    final String prerelease = matcher.group(5);    final String buildInfo = matcher.group(6);    if (major < 0 || minor < 0 || patch < 0) {        throw new SemanticVersionParseException(String.format("major(%d), minor(%d), and patch(%d) must all be >= 0", major, minor, patch));    }    return new SemanticVersion(major, minor, patch, unknown, prerelease, buildInfo);}
0
public int compareTo(SemanticVersion o)
{    int cmp;    cmp = compareIntegers(major, o.major);    if (cmp != 0) {        return cmp;    }    cmp = compareIntegers(minor, o.minor);    if (cmp != 0) {        return cmp;    }    cmp = compareIntegers(patch, o.patch);    if (cmp != 0) {        return cmp;    }    cmp = compareBooleans(o.prerelease, prerelease);    if (cmp != 0) {        return cmp;    }    if (pre != null) {        if (o.pre != null) {            return pre.compareTo(o.pre);        } else {            return -1;        }    } else if (o.pre != null) {        return 1;    }    return 0;}
0
private static int compareIntegers(int x, int y)
{    return (x < y) ? -1 : ((x == y) ? 0 : 1);}
0
private static int compareBooleans(boolean x, boolean y)
{    return (x == y) ? 0 : (x ? 1 : -1);}
0
public boolean equals(Object o)
{    if (this == o)        return true;    if (o == null || getClass() != o.getClass())        return false;    SemanticVersion that = (SemanticVersion) o;    return compareTo(that) == 0;}
0
public int hashCode()
{    int result = major;    result = 31 * result + minor;    result = 31 * result + patch;    return result;}
0
public String toString()
{    StringBuilder sb = new StringBuilder();    sb.append(major).append(".").append(minor).append(".").append(patch);    if (prerelease) {        sb.append(unknown);    }    if (pre != null) {        sb.append(pre.original);    }    if (buildInfo != null) {        sb.append(buildInfo);    }    return sb.toString();}
0
public int compareTo(NumberOrString that)
{        int cmp = compareBooleans(that.isNumeric, this.isNumeric);    if (cmp != 0) {        return cmp;    }    if (isNumeric) {                return compareIntegers(this.number, that.number);    }        return this.original.compareTo(that.original);}
0
public String toString()
{    return original;}
0
public int compareTo(Prerelease that)
{            int size = Math.min(this.identifiers.size(), that.identifiers.size());    for (int i = 0; i < size; i += 1) {        int cmp = identifiers.get(i).compareTo(that.identifiers.get(i));        if (cmp != 0) {            return cmp;        }    }    return compareIntegers(this.identifiers.size(), that.identifiers.size());}
0
public String toString()
{    return original;}
0
public static String join(Iterable<String> s, String on)
{    return join(s.iterator(), on);}
0
public static String join(Iterator<String> iter, String on)
{    StringBuilder sb = new StringBuilder();    while (iter.hasNext()) {        sb.append(iter.next());        if (iter.hasNext()) {            sb.append(on);        }    }    return sb.toString();}
0
public static String join(String[] s, String on)
{    return join(Arrays.asList(s), on);}
0
public static boolean isNullOrEmpty(String s)
{    return s == null || s.isEmpty();}
0
public static List<String> expandGlob(String globPattern)
{    return GlobExpander.expand(globPattern);}
0
public static List<WildcardPath> expandGlobToWildCardPaths(String globPattern, char delim)
{    List<WildcardPath> ret = new ArrayList<WildcardPath>();    for (String expandedGlob : Strings.expandGlob(globPattern)) {        ret.add(new WildcardPath(globPattern, expandedGlob, delim));    }    return ret;}
0
public Class<? extends C> getConstructedClass()
{    return constructed;}
0
public C newInstanceChecked(Object... args) throws Exception
{    try {        return ctor.newInstance(args);    } catch (InstantiationException e) {        throw e;    } catch (IllegalAccessException e) {        throw e;    } catch (InvocationTargetException e) {        throwIfInstance(e.getCause(), Exception.class);        throwIfInstance(e.getCause(), RuntimeException.class);        throw new RuntimeException(e.getCause());    }}
0
public C newInstance(Object... args)
{    try {        return newInstanceChecked(args);    } catch (Exception e) {        throwIfInstance(e, RuntimeException.class);        throw new RuntimeException(e);    }}
0
public R invoke(Object target, Object... args)
{    Preconditions.checkArgument(target == null, "Invalid call to constructor: target must be null");    return (R) newInstance(args);}
0
public R invokeChecked(Object target, Object... args) throws Exception
{    Preconditions.checkArgument(target == null, "Invalid call to constructor: target must be null");    return (R) newInstanceChecked(args);}
0
public DynMethods.BoundMethod bind(Object receiver)
{    throw new IllegalStateException("Cannot bind constructors");}
0
public boolean isStatic()
{    return true;}
0
public String toString()
{    return getClass().getSimpleName() + "(constructor=" + ctor + ", class=" + constructed + ")";}
0
public Builder loader(ClassLoader loader)
{    this.loader = loader;    return this;}
0
public Builder impl(String className, Class<?>... types)
{        if (ctor != null) {        return this;    }    try {        Class<?> targetClass = Class.forName(className, true, loader);        impl(targetClass, types);    } catch (NoClassDefFoundError e) {                problems.put(className, e);    } catch (ClassNotFoundException e) {                problems.put(className, e);    }    return this;}
0
public Builder impl(Class<T> targetClass, Class<?>... types)
{        if (ctor != null) {        return this;    }    try {        ctor = new Ctor<T>(targetClass.getConstructor(types), targetClass);    } catch (NoSuchMethodException e) {                problems.put(methodName(targetClass, types), e);    }    return this;}
0
public Builder hiddenImpl(Class<?>... types)
{    hiddenImpl(baseClass, types);    return this;}
0
public Builder hiddenImpl(String className, Class<?>... types)
{        if (ctor != null) {        return this;    }    try {        Class targetClass = Class.forName(className, true, loader);        hiddenImpl(targetClass, types);    } catch (NoClassDefFoundError e) {                problems.put(className, e);    } catch (ClassNotFoundException e) {                problems.put(className, e);    }    return this;}
0
public Builder hiddenImpl(Class<T> targetClass, Class<?>... types)
{        if (ctor != null) {        return this;    }    try {        Constructor<T> hidden = targetClass.getDeclaredConstructor(types);        AccessController.doPrivileged(new MakeAccessible(hidden));        ctor = new Ctor<T>(hidden, targetClass);    } catch (SecurityException e) {                problems.put(methodName(targetClass, types), e);    } catch (NoSuchMethodException e) {                problems.put(methodName(targetClass, types), e);    }    return this;}
0
public Ctor<C> buildChecked() throws NoSuchMethodException
{    if (ctor != null) {        return ctor;    }    throw new NoSuchMethodException("Cannot find constructor for " + baseClass + "\n" + formatProblems(problems));}
0
public Ctor<C> build()
{    if (ctor != null) {        return ctor;    }    throw new RuntimeException("Cannot find constructor for " + baseClass + "\n" + formatProblems(problems));}
0
public Void run()
{    hidden.setAccessible(true);    return null;}
0
private static String formatProblems(Map<String, Throwable> problems)
{    StringBuilder sb = new StringBuilder();    boolean first = true;    for (Map.Entry<String, Throwable> problem : problems.entrySet()) {        if (first) {            first = false;        } else {            sb.append("\n");        }        sb.append("\tMissing ").append(problem.getKey()).append(" [").append(problem.getValue().getClass().getName()).append(": ").append(problem.getValue().getMessage()).append("]");    }    return sb.toString();}
0
private static String methodName(Class<?> targetClass, Class<?>... types)
{    StringBuilder sb = new StringBuilder();    sb.append(targetClass.getName()).append("(");    boolean first = true;    for (Class<?> type : types) {        if (first) {            first = false;        } else {            sb.append(",");        }        sb.append(type.getName());    }    sb.append(")");    return sb.toString();}
0
public R invokeChecked(Object target, Object... args) throws Exception
{    try {        if (argLength < 0) {            return (R) method.invoke(target, args);        } else {            return (R) method.invoke(target, Arrays.copyOfRange(args, 0, argLength));        }    } catch (InvocationTargetException e) {        throwIfInstance(e.getCause(), Exception.class);        throwIfInstance(e.getCause(), RuntimeException.class);        throw new RuntimeException(e.getCause());    }}
0
public R invoke(Object target, Object... args)
{    try {        return this.<R>invokeChecked(target, args);    } catch (Exception e) {        throwIfInstance(e, RuntimeException.class);        throw new RuntimeException(e);    }}
0
public BoundMethod bind(Object receiver)
{    Preconditions.checkState(!isStatic(), "Cannot bind static method " + method.toGenericString());    Preconditions.checkArgument(method.getDeclaringClass().isAssignableFrom(receiver.getClass()), "Cannot bind " + method.toGenericString() + " to instance of " + receiver.getClass());    return new BoundMethod(this, receiver);}
0
public boolean isStatic()
{    return Modifier.isStatic(method.getModifiers());}
0
public boolean isNoop()
{    return this == NOOP;}
0
public StaticMethod asStatic()
{    Preconditions.checkState(isStatic(), "Method is not static");    return new StaticMethod(this);}
0
public String toString()
{    return "DynMethods.UnboundMethod(name=" + name + " method=" + method.toGenericString() + ")";}
0
public R invokeChecked(Object target, Object... args) throws Exception
{    return null;}
0
public BoundMethod bind(Object receiver)
{    return new BoundMethod(this, receiver);}
0
public StaticMethod asStatic()
{    return new StaticMethod(this);}
0
public boolean isStatic()
{    return true;}
0
public String toString()
{    return "DynMethods.UnboundMethod(NOOP)";}
0
public R invokeChecked(Object... args) throws Exception
{    return method.invokeChecked(receiver, args);}
0
public R invoke(Object... args)
{    return method.invoke(receiver, args);}
0
public R invokeChecked(Object... args) throws Exception
{    return method.invokeChecked(null, args);}
0
public R invoke(Object... args)
{    return method.invoke(null, args);}
0
public Builder loader(ClassLoader loader)
{    this.loader = loader;    return this;}
0
public Builder orNoop()
{    if (method == null) {        this.method = UnboundMethod.NOOP;    }    return this;}
0
public Builder impl(String className, String methodName, Class<?>... argClasses)
{        if (method != null) {        return this;    }    try {        Class<?> targetClass = Class.forName(className, true, loader);        impl(targetClass, methodName, argClasses);    } catch (ClassNotFoundException e) {        }    return this;}
0
public Builder impl(String className, Class<?>... argClasses)
{    impl(className, name, argClasses);    return this;}
0
public Builder impl(Class<?> targetClass, String methodName, Class<?>... argClasses)
{        if (method != null) {        return this;    }    try {        this.method = new UnboundMethod(targetClass.getMethod(methodName, argClasses), name);    } catch (NoSuchMethodException e) {        }    return this;}
0
public Builder impl(Class<?> targetClass, Class<?>... argClasses)
{    impl(targetClass, name, argClasses);    return this;}
0
public Builder ctorImpl(Class<?> targetClass, Class<?>... argClasses)
{        if (method != null) {        return this;    }    try {        this.method = new DynConstructors.Builder().impl(targetClass, argClasses).buildChecked();    } catch (NoSuchMethodException e) {        }    return this;}
0
public Builder ctorImpl(String className, Class<?>... argClasses)
{        if (method != null) {        return this;    }    try {        this.method = new DynConstructors.Builder().impl(className, argClasses).buildChecked();    } catch (NoSuchMethodException e) {        }    return this;}
0
public Builder hiddenImpl(String className, String methodName, Class<?>... argClasses)
{        if (method != null) {        return this;    }    try {        Class<?> targetClass = Class.forName(className, true, loader);        hiddenImpl(targetClass, methodName, argClasses);    } catch (ClassNotFoundException e) {        }    return this;}
0
public Builder hiddenImpl(String className, Class<?>... argClasses)
{    hiddenImpl(className, name, argClasses);    return this;}
0
public Builder hiddenImpl(Class<?> targetClass, String methodName, Class<?>... argClasses)
{        if (method != null) {        return this;    }    try {        Method hidden = targetClass.getDeclaredMethod(methodName, argClasses);        AccessController.doPrivileged(new MakeAccessible(hidden));        this.method = new UnboundMethod(hidden, name);    } catch (SecurityException e) {        } catch (NoSuchMethodException e) {        }    return this;}
0
public Builder hiddenImpl(Class<?> targetClass, Class<?>... argClasses)
{    hiddenImpl(targetClass, name, argClasses);    return this;}
0
public UnboundMethod buildChecked() throws NoSuchMethodException
{    if (method != null) {        return method;    } else {        throw new NoSuchMethodException("Cannot find method: " + name);    }}
0
public UnboundMethod build()
{    if (method != null) {        return method;    } else {        throw new RuntimeException("Cannot find method: " + name);    }}
0
public BoundMethod buildChecked(Object receiver) throws NoSuchMethodException
{    return buildChecked().bind(receiver);}
0
public BoundMethod build(Object receiver)
{    return build().bind(receiver);}
0
public StaticMethod buildStaticChecked() throws NoSuchMethodException
{    return buildChecked().asStatic();}
0
public StaticMethod buildStatic()
{    return build().asStatic();}
0
public Void run()
{    hidden.setAccessible(true);    return null;}
0
public boolean hasSemanticVersion()
{    return hasSemver;}
0
public SemanticVersion getSemanticVersion()
{    return semver;}
0
public boolean equals(Object o)
{    if (this == o)        return true;    if (o == null || getClass() != o.getClass())        return false;    ParsedVersion version = (ParsedVersion) o;    if (appBuildHash != null ? !appBuildHash.equals(version.appBuildHash) : version.appBuildHash != null)        return false;    if (application != null ? !application.equals(version.application) : version.application != null)        return false;    if (this.version != null ? !this.version.equals(version.version) : version.version != null)        return false;    return true;}
0
public int hashCode()
{    int result = application != null ? application.hashCode() : 0;    result = 31 * result + (version != null ? version.hashCode() : 0);    result = 31 * result + (appBuildHash != null ? appBuildHash.hashCode() : 0);    return result;}
0
public String toString()
{    return "ParsedVersion(" + "application=" + application + ", semver=" + version + ", appBuildHash=" + appBuildHash + ')';}
0
public static ParsedVersion parse(String createdBy) throws VersionParseException
{    Matcher matcher = PATTERN.matcher(createdBy);    if (!matcher.matches()) {        throw new VersionParseException("Could not parse created_by: " + createdBy + " using format: " + FORMAT);    }    String application = matcher.group(1);    String semver = matcher.group(2);    String appBuildHash = matcher.group(3);    if (Strings.isNullOrEmpty(application)) {        throw new VersionParseException("application cannot be null or empty");    }    return new ParsedVersion(application, semver, appBuildHash);}
0
public void testRead0() throws Exception
{    byte[] bytes = new byte[0];    ByteBufferInputStream stream = newStream();    Assert.assertEquals("Should read 0 bytes", 0, stream.read(bytes));    int bytesRead = stream.read(new byte[100]);    Assert.assertTrue("Should read to end of stream", bytesRead < 100);    Assert.assertEquals("Should read 0 bytes at end of stream", 0, stream.read(bytes));}
0
public void testReadAll() throws Exception
{    byte[] bytes = new byte[35];    ByteBufferInputStream stream = newStream();    int bytesRead = stream.read(bytes);    Assert.assertEquals("Should read the entire buffer", bytes.length, bytesRead);    for (int i = 0; i < bytes.length; i += 1) {        Assert.assertEquals("Byte i should be i", i, bytes[i]);        Assert.assertEquals("Should advance position", 35, stream.position());    }    Assert.assertEquals("Should have no more remaining content", 0, stream.available());    Assert.assertEquals("Should return -1 at end of stream", -1, stream.read(bytes));    Assert.assertEquals("Should have no more remaining content", 0, stream.available());    checkOriginalData();}
0
public void testSmallReads() throws Exception
{    for (int size = 1; size < 36; size += 1) {        byte[] bytes = new byte[size];        ByteBufferInputStream stream = newStream();        long length = stream.available();        int lastBytesRead = bytes.length;        for (int offset = 0; offset < length; offset += bytes.length) {            Assert.assertEquals("Should read requested len", bytes.length, lastBytesRead);            lastBytesRead = stream.read(bytes, 0, bytes.length);            Assert.assertEquals("Should advance position", offset + lastBytesRead, stream.position());                        for (int i = 0; i < lastBytesRead; i += 1) {                Assert.assertEquals("Byte i should be i", offset + i, bytes[i]);            }        }        Assert.assertEquals("Should read fewer bytes at end of buffer", length % bytes.length, lastBytesRead % bytes.length);        Assert.assertEquals("Should have no more remaining content", 0, stream.available());        Assert.assertEquals("Should return -1 at end of stream", -1, stream.read(bytes));        Assert.assertEquals("Should have no more remaining content", 0, stream.available());    }    checkOriginalData();}
0
public void testPartialBufferReads() throws Exception
{    for (int size = 1; size < 35; size += 1) {        byte[] bytes = new byte[33];        ByteBufferInputStream stream = newStream();        int lastBytesRead = size;        for (int offset = 0; offset < bytes.length; offset += size) {            Assert.assertEquals("Should read requested len", size, lastBytesRead);            lastBytesRead = stream.read(bytes, offset, Math.min(size, bytes.length - offset));            Assert.assertEquals("Should advance position", lastBytesRead > 0 ? offset + lastBytesRead : offset, stream.position());        }        Assert.assertEquals("Should read fewer bytes at end of buffer", bytes.length % size, lastBytesRead % size);        for (int i = 0; i < bytes.length; i += 1) {            Assert.assertEquals("Byte i should be i", i, bytes[i]);        }        Assert.assertEquals("Should have no more remaining content", 2, stream.available());        Assert.assertEquals("Should return 2 more bytes", 2, stream.read(bytes));        Assert.assertEquals("Should have no more remaining content", 0, stream.available());        Assert.assertEquals("Should return -1 at end of stream", -1, stream.read(bytes));        Assert.assertEquals("Should have no more remaining content", 0, stream.available());    }    checkOriginalData();}
0
public void testReadByte() throws Exception
{    final ByteBufferInputStream stream = newStream();    int length = stream.available();    for (int i = 0; i < length; i += 1) {        Assert.assertEquals("Position should increment", i, stream.position());        Assert.assertEquals(i, stream.read());    }    assertThrows("Should throw EOFException at end of stream", EOFException.class, (Callable<Integer>) stream::read);    checkOriginalData();}
0
public void testSlice() throws Exception
{    ByteBufferInputStream stream = newStream();    int length = stream.available();    ByteBuffer empty = stream.slice(0);    Assert.assertNotNull("slice(0) should produce a non-null buffer", empty);    Assert.assertEquals("slice(0) should produce an empty buffer", 0, empty.remaining());    Assert.assertEquals("Position should be at start", 0, stream.position());    int i = 0;    while (stream.available() > 0) {        int bytesToSlice = Math.min(stream.available(), 10);        ByteBuffer buffer = stream.slice(bytesToSlice);        for (int j = 0; j < bytesToSlice; j += 1) {            Assert.assertEquals("Data should be correct", i + j, buffer.get());        }        i += bytesToSlice;    }    Assert.assertEquals("Position should be at end", length, stream.position());    checkOriginalData();}
0
public void testSliceBuffers0() throws Exception
{    ByteBufferInputStream stream = newStream();    Assert.assertEquals("Should return an empty list", Collections.emptyList(), stream.sliceBuffers(0));}
0
public void testWholeSliceBuffers() throws Exception
{    final ByteBufferInputStream stream = newStream();    final int length = stream.available();    List<ByteBuffer> buffers = stream.sliceBuffers(stream.available());    Assert.assertEquals("Should consume all buffers", length, stream.position());    assertThrows("Should throw EOFException when empty", EOFException.class, (Callable<List<ByteBuffer>>) () -> stream.sliceBuffers(length));    ByteBufferInputStream copy = ByteBufferInputStream.wrap(buffers);    for (int i = 0; i < length; i += 1) {        Assert.assertEquals("Slice should have identical data", i, copy.read());    }    checkOriginalData();}
0
public void testSliceBuffersCoverage() throws Exception
{    for (int size = 1; size < 36; size += 1) {        ByteBufferInputStream stream = newStream();        int length = stream.available();        List<ByteBuffer> buffers = new ArrayList<>();        while (stream.available() > 0) {            buffers.addAll(stream.sliceBuffers(Math.min(size, stream.available())));        }        Assert.assertEquals("Should consume all content", length, stream.position());        ByteBufferInputStream newStream = new MultiBufferInputStream(buffers);        for (int i = 0; i < length; i += 1) {            Assert.assertEquals("Data should be correct", i, newStream.read());        }    }    checkOriginalData();}
0
public void testSliceBuffersModification() throws Exception
{    ByteBufferInputStream stream = newStream();    int length = stream.available();    int sliceLength = 5;    List<ByteBuffer> buffers = stream.sliceBuffers(sliceLength);    Assert.assertEquals("Should advance the original stream", length - sliceLength, stream.available());    Assert.assertEquals("Should advance the original stream position", sliceLength, stream.position());    Assert.assertEquals("Should return a slice of the first buffer", 1, buffers.size());    ByteBuffer buffer = buffers.get(0);    Assert.assertEquals("Should have requested bytes", sliceLength, buffer.remaining());            buffer.limit(sliceLength + 1);    for (int i = 0; i < sliceLength + 1; i += 1) {        Assert.assertEquals("Should have correct data", i, buffer.get());    }    Assert.assertEquals("Reading a slice shouldn't advance the original stream", sliceLength, stream.position());    Assert.assertEquals("Reading a slice shouldn't change the underlying data", sliceLength, stream.read());        buffer.limit(sliceLength + 2);    int originalValue = buffer.duplicate().get();    ByteBuffer undoBuffer = buffer.duplicate();    try {        buffer.put((byte) 255);        Assert.assertEquals("Writing to a slice shouldn't advance the original stream", sliceLength + 1, stream.position());        Assert.assertEquals("Writing to a slice should change the underlying data", 255, stream.read());    } finally {        undoBuffer.put((byte) originalValue);    }}
0
public void testSkip() throws Exception
{    ByteBufferInputStream stream = newStream();    while (stream.available() > 0) {        int bytesToSkip = Math.min(stream.available(), 10);        Assert.assertEquals("Should skip all, regardless of backing buffers", bytesToSkip, stream.skip(bytesToSkip));    }    stream = newStream();    Assert.assertEquals(0, stream.skip(0));    int length = stream.available();    Assert.assertEquals("Should stop at end when out of bytes", length, stream.skip(length + 10));    Assert.assertEquals("Should return -1 when at end", -1, stream.skip(10));}
0
public void testSkipFully() throws Exception
{    ByteBufferInputStream stream = newStream();    long lastPosition = 0;    while (stream.available() > 0) {        int bytesToSkip = Math.min(stream.available(), 10);        stream.skipFully(bytesToSkip);        Assert.assertEquals("Should skip all, regardless of backing buffers", bytesToSkip, stream.position() - lastPosition);        lastPosition = stream.position();    }    final ByteBufferInputStream stream2 = newStream();    stream2.skipFully(0);    Assert.assertEquals(0, stream2.position());    final int length = stream2.available();    assertThrows("Should throw when out of bytes", EOFException.class, () -> {        stream2.skipFully(length + 10);        return null;    });}
0
public void testMark() throws Exception
{    ByteBufferInputStream stream = newStream();    stream.read(new byte[7]);    stream.mark(100);    long mark = stream.position();    byte[] expected = new byte[100];    int expectedBytesRead = stream.read(expected);    long end = stream.position();    stream.reset();    Assert.assertEquals("Position should return to the mark", mark, stream.position());    byte[] afterReset = new byte[100];    int bytesReadAfterReset = stream.read(afterReset);    Assert.assertEquals("Should read the same number of bytes", expectedBytesRead, bytesReadAfterReset);    Assert.assertEquals("Read should end at the same position", end, stream.position());    Assert.assertArrayEquals("Content should be equal", expected, afterReset);}
0
public void testMarkTwice() throws Exception
{    ByteBufferInputStream stream = newStream();    stream.read(new byte[7]);    stream.mark(1);    stream.mark(100);    long mark = stream.position();    byte[] expected = new byte[100];    int expectedBytesRead = stream.read(expected);    long end = stream.position();    stream.reset();    Assert.assertEquals("Position should return to the mark", mark, stream.position());    byte[] afterReset = new byte[100];    int bytesReadAfterReset = stream.read(afterReset);    Assert.assertEquals("Should read the same number of bytes", expectedBytesRead, bytesReadAfterReset);    Assert.assertEquals("Read should end at the same position", end, stream.position());    Assert.assertArrayEquals("Content should be equal", expected, afterReset);}
0
public void testMarkAtStart() throws Exception
{    ByteBufferInputStream stream = newStream();    stream.mark(100);    long mark = stream.position();    byte[] expected = new byte[10];    Assert.assertEquals("Should read 10 bytes", 10, stream.read(expected));    long end = stream.position();    stream.reset();    Assert.assertEquals("Position should return to the mark", mark, stream.position());    byte[] afterReset = new byte[10];    Assert.assertEquals("Should read 10 bytes", 10, stream.read(afterReset));    Assert.assertEquals("Read should end at the same position", end, stream.position());    Assert.assertArrayEquals("Content should be equal", expected, afterReset);}
0
public void testMarkAtEnd() throws Exception
{    ByteBufferInputStream stream = newStream();    int bytesRead = stream.read(new byte[100]);    Assert.assertTrue("Should read to end of stream", bytesRead < 100);    stream.mark(100);    long mark = stream.position();    byte[] expected = new byte[10];    Assert.assertEquals("Should read 0 bytes", -1, stream.read(expected));    long end = stream.position();    stream.reset();    Assert.assertEquals("Position should return to the mark", mark, stream.position());    byte[] afterReset = new byte[10];    Assert.assertEquals("Should read 0 bytes", -1, stream.read(afterReset));    Assert.assertEquals("Read should end at the same position", end, stream.position());    Assert.assertArrayEquals("Content should be equal", expected, afterReset);}
0
public void testMarkUnset()
{    final ByteBufferInputStream stream = newStream();    assertThrows("Should throw an error for reset() without mark()", IOException.class, () -> {        stream.reset();        return null;    });}
0
public void testMarkAndResetTwiceOverSameRange() throws Exception
{    final ByteBufferInputStream stream = newStream();    byte[] expected = new byte[6];    stream.mark(10);    Assert.assertEquals("Should read expected bytes", expected.length, stream.read(expected));    stream.reset();    stream.mark(10);    byte[] firstRead = new byte[6];    Assert.assertEquals("Should read firstRead bytes", firstRead.length, stream.read(firstRead));    stream.reset();    byte[] secondRead = new byte[6];    Assert.assertEquals("Should read secondRead bytes", secondRead.length, stream.read(secondRead));    Assert.assertArrayEquals("First read should be correct", expected, firstRead);    Assert.assertArrayEquals("Second read should be correct", expected, secondRead);}
0
public void testMarkLimit() throws Exception
{    final ByteBufferInputStream stream = newStream();    stream.mark(5);    Assert.assertEquals("Should read 5 bytes", 5, stream.read(new byte[5]));    stream.reset();    Assert.assertEquals("Should read 6 bytes", 6, stream.read(new byte[6]));    assertThrows("Should throw an error for reset() after limit", IOException.class, () -> {        stream.reset();        return null;    });}
0
public void testMarkDoubleReset() throws Exception
{    final ByteBufferInputStream stream = newStream();    stream.mark(5);    Assert.assertEquals("Should read 5 bytes", 5, stream.read(new byte[5]));    stream.reset();    assertThrows("Should throw an error for double reset()", IOException.class, () -> {        stream.reset();        return null;    });}
0
public void testToByteBuffer()
{    final ByteBufferInputStream stream = newStream();    ByteBuffer buffer = stream.toByteBuffer();    for (int i = 0; i < DATA_LENGTH; ++i) {        assertEquals(i, buffer.get());    }}
0
public static void assertThrows(String message, Class<? extends Exception> expected, Callable callable)
{    try {        callable.call();        Assert.fail("No exception was thrown (" + message + "), expected: " + expected.getName());    } catch (Exception actual) {        try {            Assert.assertEquals(message, expected, actual.getClass());        } catch (AssertionError e) {            e.addSuppressed(actual);            throw e;        }    }}
0
public void testWidth()
{    assertEquals(0, getWidthFromMaxInt(0));    assertEquals(1, getWidthFromMaxInt(1));    assertEquals(2, getWidthFromMaxInt(2));    assertEquals(2, getWidthFromMaxInt(3));    assertEquals(3, getWidthFromMaxInt(4));    assertEquals(3, getWidthFromMaxInt(5));    assertEquals(3, getWidthFromMaxInt(6));    assertEquals(3, getWidthFromMaxInt(7));    assertEquals(4, getWidthFromMaxInt(8));    assertEquals(4, getWidthFromMaxInt(15));    assertEquals(5, getWidthFromMaxInt(16));    assertEquals(5, getWidthFromMaxInt(31));    assertEquals(6, getWidthFromMaxInt(32));    assertEquals(6, getWidthFromMaxInt(63));    assertEquals(7, getWidthFromMaxInt(64));    assertEquals(7, getWidthFromMaxInt(127));    assertEquals(8, getWidthFromMaxInt(128));    assertEquals(8, getWidthFromMaxInt(255));}
0
public static List<Object[]> parameters()
{    return Arrays.asList(new Object[] { TestSingleBufferInputStream.DATA, null }, new Object[] { TestSingleBufferInputStream.DATA, 0 }, new Object[] { ByteBuffer.wrap(new byte[] { -1, -1, -1, -1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34 }), 4 }, new Object[] { ByteBuffer.wrap(new byte[] { 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, -1, -1, -1 }), 0 }, new Object[] { ByteBuffer.wrap(new byte[] { -1, -1, -1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, -1, -1 }), 3 });}
0
protected ByteBufferInputStream newStream()
{    if (offset == null) {        return new ByteBufferInputStream(data);    } else {        return new ByteBufferInputStream(data, offset, DATA_LENGTH);    }}
0
protected void checkOriginalData()
{    Assert.assertEquals("Position should not change", 0, data.position());    Assert.assertEquals("Limit should not change", data.array().length, data.limit());}
0
public void testSliceData() throws Exception
{    ByteBufferInputStream stream = newStream();    int length = stream.available();    List<ByteBuffer> buffers = new ArrayList<>();        while (stream.available() > 0) {        int bytesToSlice = Math.min(stream.available(), 8);        buffers.add(stream.slice(bytesToSlice));    }    Assert.assertEquals("Position should be at end", length, stream.position());    Assert.assertEquals("Should produce 5 buffers", 5, buffers.size());    int i = 0;    ByteBuffer one = buffers.get(0);    Assert.assertSame("Should use the same backing array", one.array(), data.array());    Assert.assertEquals(8, one.remaining());    Assert.assertEquals(0, one.position());    Assert.assertEquals(8, one.limit());    for (; i < 8; i += 1) {        Assert.assertEquals("Should produce correct values", i, one.get());    }    ByteBuffer two = buffers.get(1);    Assert.assertSame("Should use the same backing array", two.array(), data.array());    Assert.assertEquals(8, two.remaining());    Assert.assertEquals(8, two.position());    Assert.assertEquals(16, two.limit());    for (; i < 16; i += 1) {        Assert.assertEquals("Should produce correct values", i, two.get());    }        ByteBuffer three = buffers.get(2);    Assert.assertSame("Should use the same backing array", three.array(), data.array());    Assert.assertEquals(8, three.remaining());    Assert.assertEquals(16, three.position());    Assert.assertEquals(24, three.limit());    for (; i < 24; i += 1) {        Assert.assertEquals("Should produce correct values", i, three.get());    }        ByteBuffer four = buffers.get(3);    Assert.assertSame("Should use the same backing array", four.array(), data.array());    Assert.assertEquals(8, four.remaining());    Assert.assertEquals(24, four.position());    Assert.assertEquals(32, four.limit());    for (; i < 32; i += 1) {        Assert.assertEquals("Should produce correct values", i, four.get());    }        ByteBuffer five = buffers.get(4);    Assert.assertSame("Should use the same backing array", five.array(), data.array());    Assert.assertEquals(3, five.remaining());    Assert.assertEquals(32, five.position());    Assert.assertEquals(35, five.limit());    for (; i < 35; i += 1) {        Assert.assertEquals("Should produce correct values", i, five.get());    }}
0
public void testWholeSliceBuffersData() throws Exception
{    ByteBufferInputStream stream = newStream();    List<ByteBuffer> buffers = stream.sliceBuffers(stream.available());    Assert.assertEquals("Should return duplicates of all non-empty buffers", Collections.singletonList(TestSingleBufferInputStream.DATA), buffers);}
0
protected ByteBufferInputStream newStream()
{    return new MultiBufferInputStream(DATA);}
0
protected void checkOriginalData()
{    for (ByteBuffer buffer : DATA) {        Assert.assertEquals("Position should not change", 0, buffer.position());        Assert.assertEquals("Limit should not change", buffer.array().length, buffer.limit());    }}
0
public void testSliceData() throws Exception
{    ByteBufferInputStream stream = newStream();    int length = stream.available();    List<ByteBuffer> buffers = new ArrayList<>();        while (stream.available() > 0) {        int bytesToSlice = Math.min(stream.available(), 8);        buffers.add(stream.slice(bytesToSlice));    }    Assert.assertEquals("Position should be at end", length, stream.position());    Assert.assertEquals("Should produce 5 buffers", 5, buffers.size());    int i = 0;        ByteBuffer one = buffers.get(0);    Assert.assertSame("Should be a duplicate of the first array", one.array(), DATA.get(0).array());    Assert.assertEquals(8, one.remaining());    Assert.assertEquals(0, one.position());    Assert.assertEquals(8, one.limit());    Assert.assertEquals(9, one.capacity());    for (; i < 8; i += 1) {        Assert.assertEquals("Should produce correct values", i, one.get());    }        ByteBuffer two = buffers.get(1);    Assert.assertEquals(8, two.remaining());    Assert.assertEquals(0, two.position());    Assert.assertEquals(8, two.limit());    Assert.assertEquals(8, two.capacity());    for (; i < 16; i += 1) {        Assert.assertEquals("Should produce correct values", i, two.get());    }        ByteBuffer three = buffers.get(2);    Assert.assertSame("Should be a duplicate of the fourth array", three.array(), DATA.get(3).array());    Assert.assertEquals(8, three.remaining());    Assert.assertEquals(3, three.position());    Assert.assertEquals(11, three.limit());    Assert.assertEquals(12, three.capacity());    for (; i < 24; i += 1) {        Assert.assertEquals("Should produce correct values", i, three.get());    }        ByteBuffer four = buffers.get(3);    Assert.assertEquals(8, four.remaining());    Assert.assertEquals(0, four.position());    Assert.assertEquals(8, four.limit());    Assert.assertEquals(8, four.capacity());    for (; i < 32; i += 1) {        Assert.assertEquals("Should produce correct values", i, four.get());    }        ByteBuffer five = buffers.get(4);    Assert.assertEquals(3, five.remaining());    Assert.assertEquals(0, five.position());    Assert.assertEquals(3, five.limit());    Assert.assertEquals(3, five.capacity());    for (; i < 35; i += 1) {        Assert.assertEquals("Should produce correct values", i, five.get());    }}
0
public void testSliceBuffersData() throws Exception
{    ByteBufferInputStream stream = newStream();    List<ByteBuffer> buffers = stream.sliceBuffers(stream.available());    List<ByteBuffer> nonEmptyBuffers = new ArrayList<>();    for (ByteBuffer buffer : DATA) {        if (buffer.remaining() > 0) {            nonEmptyBuffers.add(buffer);        }    }    Assert.assertEquals("Should return duplicates of all non-empty buffers", nonEmptyBuffers, buffers);}
0
protected ByteBufferInputStream newStream()
{    return new SingleBufferInputStream(DATA);}
0
protected void checkOriginalData()
{    Assert.assertEquals("Position should not change", 0, DATA.position());    Assert.assertEquals("Limit should not change", DATA.array().length, DATA.limit());}
0
public void testSliceData() throws Exception
{    ByteBufferInputStream stream = newStream();    int length = stream.available();    List<ByteBuffer> buffers = new ArrayList<>();        while (stream.available() > 0) {        int bytesToSlice = Math.min(stream.available(), 8);        buffers.add(stream.slice(bytesToSlice));    }    Assert.assertEquals("Position should be at end", length, stream.position());    Assert.assertEquals("Should produce 5 buffers", 5, buffers.size());    int i = 0;    ByteBuffer one = buffers.get(0);    Assert.assertSame("Should use the same backing array", one.array(), DATA.array());    Assert.assertEquals(8, one.remaining());    Assert.assertEquals(0, one.position());    Assert.assertEquals(8, one.limit());    Assert.assertEquals(35, one.capacity());    for (; i < 8; i += 1) {        Assert.assertEquals("Should produce correct values", i, one.get());    }    ByteBuffer two = buffers.get(1);    Assert.assertSame("Should use the same backing array", two.array(), DATA.array());    Assert.assertEquals(8, two.remaining());    Assert.assertEquals(8, two.position());    Assert.assertEquals(16, two.limit());    Assert.assertEquals(35, two.capacity());    for (; i < 16; i += 1) {        Assert.assertEquals("Should produce correct values", i, two.get());    }        ByteBuffer three = buffers.get(2);    Assert.assertSame("Should use the same backing array", three.array(), DATA.array());    Assert.assertEquals(8, three.remaining());    Assert.assertEquals(16, three.position());    Assert.assertEquals(24, three.limit());    Assert.assertEquals(35, three.capacity());    for (; i < 24; i += 1) {        Assert.assertEquals("Should produce correct values", i, three.get());    }        ByteBuffer four = buffers.get(3);    Assert.assertSame("Should use the same backing array", four.array(), DATA.array());    Assert.assertEquals(8, four.remaining());    Assert.assertEquals(24, four.position());    Assert.assertEquals(32, four.limit());    Assert.assertEquals(35, four.capacity());    for (; i < 32; i += 1) {        Assert.assertEquals("Should produce correct values", i, four.get());    }        ByteBuffer five = buffers.get(4);    Assert.assertSame("Should use the same backing array", five.array(), DATA.array());    Assert.assertEquals(3, five.remaining());    Assert.assertEquals(32, five.position());    Assert.assertEquals(35, five.limit());    Assert.assertEquals(35, five.capacity());    for (; i < 35; i += 1) {        Assert.assertEquals("Should produce correct values", i, five.get());    }}
0
public void testWholeSliceBuffersData() throws Exception
{    ByteBufferInputStream stream = newStream();    List<ByteBuffer> buffers = stream.sliceBuffers(stream.available());    Assert.assertEquals("Should return duplicates of all non-empty buffers", Collections.singletonList(DATA), buffers);}
0
public void testNoGlobs()
{    assertEquals(Arrays.asList("foo"), Strings.expandGlob("foo"));}
0
public void testEmptyGroup()
{    assertEquals(Arrays.asList(""), Strings.expandGlob(""));    assertEquals(Arrays.asList(""), Strings.expandGlob("{}"));    assertEquals(Arrays.asList("a"), Strings.expandGlob("a{}"));    assertEquals(Arrays.asList("ab"), Strings.expandGlob("a{}b"));    assertEquals(Arrays.asList("a"), Strings.expandGlob("{}a"));    assertEquals(Arrays.asList("a"), Strings.expandGlob("a{}"));    assertEquals(Arrays.asList("", ""), Strings.expandGlob("{,}"));    assertEquals(Arrays.asList("ab", "a", "ac"), Strings.expandGlob("a{b,{},c}"));}
0
public void testSingleLevel()
{    assertEquals(Arrays.asList("foobar", "foobaz"), Strings.expandGlob("foo{bar,baz}"));    assertEquals(Arrays.asList("startfooend", "startbarend"), Strings.expandGlob("start{foo,bar}end"));    assertEquals(Arrays.asList("fooend", "barend"), Strings.expandGlob("{foo,bar}end"));    assertEquals(Arrays.asList("startfooenda", "startfooendb", "startfooendc", "startfooendd", "startbarenda", "startbarendb", "startbarendc", "startbarendd"), Strings.expandGlob("start{foo,bar}end{a,b,c,d}"));    assertEquals(Arrays.asList("xa", "xb", "xc", "ya", "yb", "yc"), Strings.expandGlob("{x,y}{a,b,c}"));    assertEquals(Arrays.asList("x", "y", "z"), Strings.expandGlob("{x,y,z}"));}
0
public void testNested()
{    assertEquals(Arrays.asList("startoneend", "startpretwopostend", "startprethreepostend", "startfourend", "startfiveend", "a", "b", "foox", "fooy"), Strings.expandGlob("{start{one,pre{two,three}post,{four,five}}end,a,b,foo{x,y}}"));}
0
public void testExtraBraces()
{    assertEquals(Arrays.asList("x", "y", "z"), Strings.expandGlob("{{x,y,z}}"));    assertEquals(Arrays.asList("x", "y", "z"), Strings.expandGlob("{{{x,y,z}}}"));    assertEquals(Arrays.asList("startx", "starta", "startb", "starty"), Strings.expandGlob("start{x,{a,b},y}"));}
0
public void testCommaInTopLevel()
{    try {        Strings.expandGlob("foo,bar");        fail("This should throw");    } catch (GlobParseException e) {        Assert.assertEquals("Unexpected comma outside of a {} group:\n" + "foo,bar\n" + "---^", e.getMessage());    }}
0
public void testCommaCornerCases()
{        assertEquals(Arrays.asList("foobar", "foo", "foobaz"), Strings.expandGlob("foo{bar,,baz}"));    assertEquals(Arrays.asList("foo", "foobar", "foobaz"), Strings.expandGlob("foo{,bar,baz}"));    assertEquals(Arrays.asList("foobar", "foobaz", "foo"), Strings.expandGlob("foo{bar,baz,}"));        assertEquals(Arrays.asList("foobar", "foo", "foo", "foobaz"), Strings.expandGlob("foo{bar,,,baz}"));    assertEquals(Arrays.asList("foo", "foo", "foobar", "foobaz"), Strings.expandGlob("foo{,,bar,baz}"));    assertEquals(Arrays.asList("foobar", "foobaz", "foo", "foo"), Strings.expandGlob("foo{bar,baz,,}"));        assertEquals(Arrays.asList("x", "y", "", "a", "b"), Strings.expandGlob("{{x,y},,{a,b}}"));}
0
private void assertNotEnoughCloseBraces(String s)
{    String expected = "Not enough close braces in: ";    try {        Strings.expandGlob(s);        fail("this should throw");    } catch (GlobParseException e) {        Assert.assertEquals(expected, e.getMessage().substring(0, expected.length()));    }}
0
private void assertTooManyCloseBraces(String s)
{    String expected = "Unexpected closing }:";    try {        Strings.expandGlob(s);        fail("this should throw");    } catch (GlobParseException e) {        Assert.assertEquals(expected, e.getMessage().substring(0, expected.length()));    }}
0
public void testMismatchedBraces()
{    assertNotEnoughCloseBraces("{");    assertNotEnoughCloseBraces("{}{}{}{{}{}{");    assertNotEnoughCloseBraces("foo{bar");    assertNotEnoughCloseBraces("foo{{bar}");    assertNotEnoughCloseBraces("foo{}{{bar}");    assertTooManyCloseBraces("{}}{");    assertTooManyCloseBraces("}");    assertTooManyCloseBraces("{}{}{}}{}{}{");    assertTooManyCloseBraces("foo}bar");    assertTooManyCloseBraces("foo}}bar}");    assertTooManyCloseBraces("foo{}{{bar}}}");}
0
private static void assertMatches(WildcardPath wp, String... strings)
{    for (String s : strings) {        if (!wp.matches(s)) {            fail(String.format("String '%s' was expected to match '%s'", s, wp));        }    }}
0
private static void assertDoesNotMatch(WildcardPath wp, String... strings)
{    for (String s : strings) {        if (wp.matches(s)) {            fail(String.format("String '%s' was not expected to match '%s'", s, wp));        }    }}
0
public void testNoWildcards()
{    WildcardPath wp = new WildcardPath("", "foo", '.');    assertMatches(wp, "foo", "foo.x", "foo.x.y");    assertDoesNotMatch(wp, "xfoo", "xfoox", "fooa.x.y");}
0
public void testStarMatchesEverything()
{    WildcardPath wp = new WildcardPath("", "*", '.');    assertMatches(wp, "", ".", "hi", "foo.bar", "*", "foo.");}
0
public void testChildrenPathsMatch()
{    WildcardPath wp = new WildcardPath("", "x.y.z", '.');    assertMatches(wp, "x.y.z", "x.y.z.bar", "x.y.z.bar.baz.bop");    assertDoesNotMatch(wp, "x.y.zzzz", "x.y.b", "x.y.a.z", "x.y.zhi.z");}
0
public void testEmptyString()
{    WildcardPath wp = new WildcardPath("", "", '.');    assertMatches(wp, "");    assertDoesNotMatch(wp, "x");}
0
public void testDoubleStarsIgnored()
{    WildcardPath wp = new WildcardPath("", "foo**bar", '.');    assertMatches(wp, "foobar", "fooxyzbar", "foo.x.y.z.bar");    assertDoesNotMatch(wp, "fobar", "hi", "foobazr");    wp = new WildcardPath("", "foo********bar", '.');    assertMatches(wp, "foobar", "fooxyzbar", "foo.x.y.z.bar");    assertDoesNotMatch(wp, "fobar", "hi", "foobazr");}
0
public void testStarsAtBeginAndEnd()
{    WildcardPath wp = new WildcardPath("", "*x.y.z", '.');    assertMatches(wp, "a.b.c.x.y.z", "x.y.z", "zoopx.y.z", "zoopx.y.z.child");    assertDoesNotMatch(wp, "a.b.c.x.y", "xy.z", "hi");    wp = new WildcardPath("", "*.x.y.z", '.');    assertMatches(wp, "a.b.c.x.y.z", "foo.x.y.z", "foo.x.y.z.child");    assertDoesNotMatch(wp, "x.y.z", "a.b.c.x.y", "xy.z", "hi", "zoopx.y.z", "zoopx.y.z.child");    wp = new WildcardPath("", "x.y.z*", '.');    assertMatches(wp, "x.y.z", "x.y.z.foo", "x.y.zoo", "x.y.zoo.bar");    assertDoesNotMatch(wp, "a.b.c.x.y.z", "foo.x.y.z", "hi");    wp = new WildcardPath("", "x.y.z.*", '.');    assertMatches(wp, "x.y.z.foo", "x.y.z.bar.baz");    assertDoesNotMatch(wp, "x.y.z", "a.b.c.x.y.z", "x.y.zoo", "foo.x.y.z", "hi", "x.y.zoo.bar");}
0
public void testComplex()
{    WildcardPath wp = new WildcardPath("", "*.street", '.');    assertMatches(wp, "home.address.street", "home.address.street.number", "work.address.street", "work.address.street.foo", "street.street", "street.street.street.street", "thing.street.thing");    assertDoesNotMatch(wp, "home.address.street_2", "home.address.street_2.number", "work.addressstreet", "work.addressstreet.foo", "", "x.y.z.street2", "x.y.z.street2.z");    wp = new WildcardPath("", "x.y.*_stat.average", '.');    assertMatches(wp, "x.y.z_stat.average", "x.y.foo_stat.average", "x.y.z.a.b_stat.average", "x.y.z.a.b_stat.average.child", "x.y.z._stat.average");    assertDoesNotMatch(wp, "x.y.z_stats.average", "x.y.z_stat.averages", "x.y_stat.average", "x.yyy.foo_stat.average");    wp = new WildcardPath("", "x.y.pre*.bar", '.');    assertMatches(wp, "x.y.pre.bar", "x.y.preabc.bar", "x.y.prebar.bar");    assertDoesNotMatch(wp, "x.y.pre.baraaaa", "x.y.preabc.baraaaa");}
0
public synchronized int read(byte[] b, int off, int len)
{    if (current < lengths.length) {        if (len <= lengths[current]) {                        int bytesRead = super.read(b, off, len);            lengths[current] -= bytesRead;            return bytesRead;        } else {            int bytesRead = super.read(b, off, lengths[current]);            current += 1;            return bytesRead;        }    } else {        return super.read(b, off, len);    }}
0
public long getPos()
{    return this.pos;}
0
public void testReadFully() throws Exception
{    byte[] buffer = new byte[5];    MockInputStream stream = new MockInputStream();    DelegatingSeekableInputStream.readFully(stream, buffer, 0, buffer.length);    Assert.assertArrayEquals("Byte array contents should match", Arrays.copyOfRange(TEST_ARRAY, 0, 5), buffer);    Assert.assertEquals("Stream position should reflect bytes read", 5, stream.getPos());}
0
public void testReadFullySmallReads() throws Exception
{    byte[] buffer = new byte[5];    MockInputStream stream = new MockInputStream(2, 3, 3);    DelegatingSeekableInputStream.readFully(stream, buffer, 0, buffer.length);    Assert.assertArrayEquals("Byte array contents should match", Arrays.copyOfRange(TEST_ARRAY, 0, 5), buffer);    Assert.assertEquals("Stream position should reflect bytes read", 5, stream.getPos());}
0
public void testReadFullyJustRight() throws Exception
{    final byte[] buffer = new byte[10];    final MockInputStream stream = new MockInputStream(2, 3, 3);    DelegatingSeekableInputStream.readFully(stream, buffer, 0, buffer.length);    Assert.assertArrayEquals("Byte array contents should match", TEST_ARRAY, buffer);    Assert.assertEquals("Stream position should reflect bytes read", 10, stream.getPos());    TestUtils.assertThrows("Should throw EOFException if no more bytes left", EOFException.class, (Callable<Void>) () -> {        DelegatingSeekableInputStream.readFully(stream, buffer, 0, 1);        return null;    });}
0
public void testReadFullyUnderflow() throws Exception
{    final byte[] buffer = new byte[11];    final MockInputStream stream = new MockInputStream(2, 3, 3);    TestUtils.assertThrows("Should throw EOFException if no more bytes left", EOFException.class, (Callable<Void>) () -> {        DelegatingSeekableInputStream.readFully(stream, buffer, 0, buffer.length);        return null;    });    Assert.assertArrayEquals("Should have consumed bytes", TEST_ARRAY, Arrays.copyOfRange(buffer, 0, 10));    Assert.assertEquals("Stream position should reflect bytes read", 10, stream.getPos());}
0
public void testReadFullyStartAndLength() throws IOException
{    byte[] buffer = new byte[10];    MockInputStream stream = new MockInputStream();    DelegatingSeekableInputStream.readFully(stream, buffer, 2, 5);    Assert.assertArrayEquals("Byte array contents should match", Arrays.copyOfRange(TEST_ARRAY, 0, 5), Arrays.copyOfRange(buffer, 2, 7));    Assert.assertEquals("Stream position should reflect bytes read", 5, stream.getPos());}
0
public void testReadFullyZeroByteRead() throws IOException
{    byte[] buffer = new byte[0];    MockInputStream stream = new MockInputStream();    DelegatingSeekableInputStream.readFully(stream, buffer, 0, buffer.length);    Assert.assertEquals("Stream position should reflect bytes read", 0, stream.getPos());}
0
public void testReadFullySmallReadsWithStartAndLength() throws IOException
{    byte[] buffer = new byte[10];    MockInputStream stream = new MockInputStream(2, 2, 3);    DelegatingSeekableInputStream.readFully(stream, buffer, 2, 5);    Assert.assertArrayEquals("Byte array contents should match", Arrays.copyOfRange(TEST_ARRAY, 0, 5), Arrays.copyOfRange(buffer, 2, 7));    Assert.assertEquals("Stream position should reflect bytes read", 5, stream.getPos());}
0
public void testHeapRead() throws Exception
{    ByteBuffer readBuffer = ByteBuffer.allocate(20);    MockInputStream stream = new MockInputStream();    int len = DelegatingSeekableInputStream.readHeapBuffer(stream, readBuffer);    Assert.assertEquals(10, len);    Assert.assertEquals(10, readBuffer.position());    Assert.assertEquals(20, readBuffer.limit());    len = DelegatingSeekableInputStream.readHeapBuffer(stream, readBuffer);    Assert.assertEquals(-1, len);    readBuffer.flip();    Assert.assertEquals("Buffer contents should match", ByteBuffer.wrap(TEST_ARRAY), readBuffer);}
0
public void testHeapSmallBuffer() throws Exception
{    ByteBuffer readBuffer = ByteBuffer.allocate(5);    MockInputStream stream = new MockInputStream();    int len = DelegatingSeekableInputStream.readHeapBuffer(stream, readBuffer);    Assert.assertEquals(5, len);    Assert.assertEquals(5, readBuffer.position());    Assert.assertEquals(5, readBuffer.limit());    len = DelegatingSeekableInputStream.readHeapBuffer(stream, readBuffer);    Assert.assertEquals(0, len);    readBuffer.flip();    Assert.assertEquals("Buffer contents should match", ByteBuffer.wrap(TEST_ARRAY, 0, 5), readBuffer);}
0
public void testHeapSmallReads() throws Exception
{    ByteBuffer readBuffer = ByteBuffer.allocate(10);    MockInputStream stream = new MockInputStream(2, 3, 3);    int len = DelegatingSeekableInputStream.readHeapBuffer(stream, readBuffer);    Assert.assertEquals(2, len);    Assert.assertEquals(2, readBuffer.position());    Assert.assertEquals(10, readBuffer.limit());    len = DelegatingSeekableInputStream.readHeapBuffer(stream, readBuffer);    Assert.assertEquals(3, len);    Assert.assertEquals(5, readBuffer.position());    Assert.assertEquals(10, readBuffer.limit());    len = DelegatingSeekableInputStream.readHeapBuffer(stream, readBuffer);    Assert.assertEquals(3, len);    Assert.assertEquals(8, readBuffer.position());    Assert.assertEquals(10, readBuffer.limit());    len = DelegatingSeekableInputStream.readHeapBuffer(stream, readBuffer);    Assert.assertEquals(2, len);    Assert.assertEquals(10, readBuffer.position());    Assert.assertEquals(10, readBuffer.limit());    readBuffer.flip();    Assert.assertEquals("Buffer contents should match", ByteBuffer.wrap(TEST_ARRAY), readBuffer);}
0
public void testHeapPosition() throws Exception
{    ByteBuffer readBuffer = ByteBuffer.allocate(20);    readBuffer.position(10);    readBuffer.mark();    MockInputStream stream = new MockInputStream(8);    int len = DelegatingSeekableInputStream.readHeapBuffer(stream, readBuffer);    Assert.assertEquals(8, len);    Assert.assertEquals(18, readBuffer.position());    Assert.assertEquals(20, readBuffer.limit());    len = DelegatingSeekableInputStream.readHeapBuffer(stream, readBuffer);    Assert.assertEquals(2, len);    Assert.assertEquals(20, readBuffer.position());    Assert.assertEquals(20, readBuffer.limit());    len = DelegatingSeekableInputStream.readHeapBuffer(stream, readBuffer);    Assert.assertEquals(-1, len);    readBuffer.reset();    Assert.assertEquals("Buffer contents should match", ByteBuffer.wrap(TEST_ARRAY), readBuffer);}
0
public void testHeapLimit() throws Exception
{    ByteBuffer readBuffer = ByteBuffer.allocate(20);    readBuffer.limit(8);    MockInputStream stream = new MockInputStream(7);    int len = DelegatingSeekableInputStream.readHeapBuffer(stream, readBuffer);    Assert.assertEquals(7, len);    Assert.assertEquals(7, readBuffer.position());    Assert.assertEquals(8, readBuffer.limit());    len = DelegatingSeekableInputStream.readHeapBuffer(stream, readBuffer);    Assert.assertEquals(1, len);    Assert.assertEquals(8, readBuffer.position());    Assert.assertEquals(8, readBuffer.limit());    len = DelegatingSeekableInputStream.readHeapBuffer(stream, readBuffer);    Assert.assertEquals(0, len);    readBuffer.flip();    Assert.assertEquals("Buffer contents should match", ByteBuffer.wrap(TEST_ARRAY, 0, 8), readBuffer);}
0
public void testHeapPositionAndLimit() throws Exception
{    ByteBuffer readBuffer = ByteBuffer.allocate(20);    readBuffer.position(5);    readBuffer.limit(13);    readBuffer.mark();    MockInputStream stream = new MockInputStream(7);    int len = DelegatingSeekableInputStream.readHeapBuffer(stream, readBuffer);    Assert.assertEquals(7, len);    Assert.assertEquals(12, readBuffer.position());    Assert.assertEquals(13, readBuffer.limit());    len = DelegatingSeekableInputStream.readHeapBuffer(stream, readBuffer);    Assert.assertEquals(1, len);    Assert.assertEquals(13, readBuffer.position());    Assert.assertEquals(13, readBuffer.limit());    len = DelegatingSeekableInputStream.readHeapBuffer(stream, readBuffer);    Assert.assertEquals(0, len);    readBuffer.reset();    Assert.assertEquals("Buffer contents should match", ByteBuffer.wrap(TEST_ARRAY, 0, 8), readBuffer);}
0
public void testDirectRead() throws Exception
{    ByteBuffer readBuffer = ByteBuffer.allocateDirect(20);    MockInputStream stream = new MockInputStream();    int len = DelegatingSeekableInputStream.readDirectBuffer(stream, readBuffer, TEMP.get());    Assert.assertEquals(10, len);    Assert.assertEquals(10, readBuffer.position());    Assert.assertEquals(20, readBuffer.limit());    len = DelegatingSeekableInputStream.readDirectBuffer(stream, readBuffer, TEMP.get());    Assert.assertEquals(-1, len);    readBuffer.flip();    Assert.assertEquals("Buffer contents should match", ByteBuffer.wrap(TEST_ARRAY), readBuffer);}
0
public void testDirectSmallBuffer() throws Exception
{    ByteBuffer readBuffer = ByteBuffer.allocateDirect(5);    MockInputStream stream = new MockInputStream();    int len = DelegatingSeekableInputStream.readDirectBuffer(stream, readBuffer, TEMP.get());    Assert.assertEquals(5, len);    Assert.assertEquals(5, readBuffer.position());    Assert.assertEquals(5, readBuffer.limit());    len = DelegatingSeekableInputStream.readDirectBuffer(stream, readBuffer, TEMP.get());    Assert.assertEquals(0, len);    readBuffer.flip();    Assert.assertEquals("Buffer contents should match", ByteBuffer.wrap(TEST_ARRAY, 0, 5), readBuffer);}
0
public void testDirectSmallReads() throws Exception
{    ByteBuffer readBuffer = ByteBuffer.allocateDirect(10);    MockInputStream stream = new MockInputStream(2, 3, 3);    int len = DelegatingSeekableInputStream.readDirectBuffer(stream, readBuffer, TEMP.get());    Assert.assertEquals(2, len);    Assert.assertEquals(2, readBuffer.position());    Assert.assertEquals(10, readBuffer.limit());    len = DelegatingSeekableInputStream.readDirectBuffer(stream, readBuffer, TEMP.get());    Assert.assertEquals(3, len);    Assert.assertEquals(5, readBuffer.position());    Assert.assertEquals(10, readBuffer.limit());    len = DelegatingSeekableInputStream.readDirectBuffer(stream, readBuffer, TEMP.get());    Assert.assertEquals(3, len);    Assert.assertEquals(8, readBuffer.position());    Assert.assertEquals(10, readBuffer.limit());    len = DelegatingSeekableInputStream.readDirectBuffer(stream, readBuffer, TEMP.get());    Assert.assertEquals(2, len);    Assert.assertEquals(10, readBuffer.position());    Assert.assertEquals(10, readBuffer.limit());    readBuffer.flip();    Assert.assertEquals("Buffer contents should match", ByteBuffer.wrap(TEST_ARRAY), readBuffer);}
0
public void testDirectPosition() throws Exception
{    ByteBuffer readBuffer = ByteBuffer.allocateDirect(20);    readBuffer.position(10);    readBuffer.mark();    MockInputStream stream = new MockInputStream(8);    int len = DelegatingSeekableInputStream.readDirectBuffer(stream, readBuffer, TEMP.get());    Assert.assertEquals(8, len);    Assert.assertEquals(18, readBuffer.position());    Assert.assertEquals(20, readBuffer.limit());    len = DelegatingSeekableInputStream.readDirectBuffer(stream, readBuffer, TEMP.get());    Assert.assertEquals(2, len);    Assert.assertEquals(20, readBuffer.position());    Assert.assertEquals(20, readBuffer.limit());    len = DelegatingSeekableInputStream.readDirectBuffer(stream, readBuffer, TEMP.get());    Assert.assertEquals(-1, len);    readBuffer.reset();    Assert.assertEquals("Buffer contents should match", ByteBuffer.wrap(TEST_ARRAY), readBuffer);}
0
public void testDirectLimit() throws Exception
{    ByteBuffer readBuffer = ByteBuffer.allocate(20);    readBuffer.limit(8);    MockInputStream stream = new MockInputStream(7);    int len = DelegatingSeekableInputStream.readDirectBuffer(stream, readBuffer, TEMP.get());    Assert.assertEquals(7, len);    Assert.assertEquals(7, readBuffer.position());    Assert.assertEquals(8, readBuffer.limit());    len = DelegatingSeekableInputStream.readDirectBuffer(stream, readBuffer, TEMP.get());    Assert.assertEquals(1, len);    Assert.assertEquals(8, readBuffer.position());    Assert.assertEquals(8, readBuffer.limit());    len = DelegatingSeekableInputStream.readDirectBuffer(stream, readBuffer, TEMP.get());    Assert.assertEquals(0, len);    readBuffer.flip();    Assert.assertEquals("Buffer contents should match", ByteBuffer.wrap(TEST_ARRAY, 0, 8), readBuffer);}
0
public void testDirectPositionAndLimit() throws Exception
{    ByteBuffer readBuffer = ByteBuffer.allocateDirect(20);    readBuffer.position(5);    readBuffer.limit(13);    readBuffer.mark();    MockInputStream stream = new MockInputStream(7);    int len = DelegatingSeekableInputStream.readDirectBuffer(stream, readBuffer, TEMP.get());    Assert.assertEquals(7, len);    Assert.assertEquals(12, readBuffer.position());    Assert.assertEquals(13, readBuffer.limit());    len = DelegatingSeekableInputStream.readDirectBuffer(stream, readBuffer, TEMP.get());    Assert.assertEquals(1, len);    Assert.assertEquals(13, readBuffer.position());    Assert.assertEquals(13, readBuffer.limit());    len = DelegatingSeekableInputStream.readDirectBuffer(stream, readBuffer, TEMP.get());    Assert.assertEquals(0, len);    readBuffer.reset();    Assert.assertEquals("Buffer contents should match", ByteBuffer.wrap(TEST_ARRAY, 0, 8), readBuffer);}
0
public void testDirectSmallTempBufferSmallReads() throws Exception
{        byte[] temp = new byte[2];    ByteBuffer readBuffer = ByteBuffer.allocateDirect(10);    MockInputStream stream = new MockInputStream(2, 3, 3);    int len = DelegatingSeekableInputStream.readDirectBuffer(stream, readBuffer, temp);    Assert.assertEquals(2, len);    Assert.assertEquals(2, readBuffer.position());    Assert.assertEquals(10, readBuffer.limit());    len = DelegatingSeekableInputStream.readDirectBuffer(stream, readBuffer, temp);    Assert.assertEquals(3, len);    Assert.assertEquals(5, readBuffer.position());    Assert.assertEquals(10, readBuffer.limit());    len = DelegatingSeekableInputStream.readDirectBuffer(stream, readBuffer, temp);    Assert.assertEquals(3, len);    Assert.assertEquals(8, readBuffer.position());    Assert.assertEquals(10, readBuffer.limit());    len = DelegatingSeekableInputStream.readDirectBuffer(stream, readBuffer, temp);    Assert.assertEquals(2, len);    Assert.assertEquals(10, readBuffer.position());    Assert.assertEquals(10, readBuffer.limit());    len = DelegatingSeekableInputStream.readDirectBuffer(stream, readBuffer, temp);    Assert.assertEquals(-1, len);    readBuffer.flip();    Assert.assertEquals("Buffer contents should match", ByteBuffer.wrap(TEST_ARRAY), readBuffer);}
0
public void testDirectSmallTempBufferWithPositionAndLimit() throws Exception
{        byte[] temp = new byte[2];    ByteBuffer readBuffer = ByteBuffer.allocateDirect(20);    readBuffer.position(5);    readBuffer.limit(13);    readBuffer.mark();    MockInputStream stream = new MockInputStream(7);    int len = DelegatingSeekableInputStream.readDirectBuffer(stream, readBuffer, temp);    Assert.assertEquals(7, len);    Assert.assertEquals(12, readBuffer.position());    Assert.assertEquals(13, readBuffer.limit());    len = DelegatingSeekableInputStream.readDirectBuffer(stream, readBuffer, temp);    Assert.assertEquals(1, len);    Assert.assertEquals(13, readBuffer.position());    Assert.assertEquals(13, readBuffer.limit());    len = DelegatingSeekableInputStream.readDirectBuffer(stream, readBuffer, temp);    Assert.assertEquals(0, len);    readBuffer.reset();    Assert.assertEquals("Buffer contents should match", ByteBuffer.wrap(TEST_ARRAY, 0, 8), readBuffer);}
0
public void testHeapReadFullySmallBuffer() throws Exception
{    ByteBuffer readBuffer = ByteBuffer.allocate(8);    MockInputStream stream = new MockInputStream();    DelegatingSeekableInputStream.readFullyHeapBuffer(stream, readBuffer);    Assert.assertEquals(8, readBuffer.position());    Assert.assertEquals(8, readBuffer.limit());    DelegatingSeekableInputStream.readFullyHeapBuffer(stream, readBuffer);    Assert.assertEquals(8, readBuffer.position());    Assert.assertEquals(8, readBuffer.limit());    readBuffer.flip();    Assert.assertEquals("Buffer contents should match", ByteBuffer.wrap(TEST_ARRAY, 0, 8), readBuffer);}
0
public void testHeapReadFullyLargeBuffer() throws Exception
{    final ByteBuffer readBuffer = ByteBuffer.allocate(20);    final MockInputStream stream = new MockInputStream();    TestUtils.assertThrows("Should throw EOFException", EOFException.class, () -> {        DelegatingSeekableInputStream.readFullyHeapBuffer(stream, readBuffer);        return null;    });    Assert.assertEquals(0, readBuffer.position());    Assert.assertEquals(20, readBuffer.limit());}
0
public void testHeapReadFullyJustRight() throws Exception
{    final ByteBuffer readBuffer = ByteBuffer.allocate(10);    MockInputStream stream = new MockInputStream();        DelegatingSeekableInputStream.readFullyHeapBuffer(stream, readBuffer);    Assert.assertEquals(10, readBuffer.position());    Assert.assertEquals(10, readBuffer.limit());        DelegatingSeekableInputStream.readFullyHeapBuffer(stream, readBuffer);    Assert.assertEquals(10, readBuffer.position());    Assert.assertEquals(10, readBuffer.limit());    readBuffer.flip();    Assert.assertEquals("Buffer contents should match", ByteBuffer.wrap(TEST_ARRAY), readBuffer);}
0
public void testHeapReadFullySmallReads() throws Exception
{    final ByteBuffer readBuffer = ByteBuffer.allocate(10);    MockInputStream stream = new MockInputStream(2, 3, 3);    DelegatingSeekableInputStream.readFullyHeapBuffer(stream, readBuffer);    Assert.assertEquals(10, readBuffer.position());    Assert.assertEquals(10, readBuffer.limit());    DelegatingSeekableInputStream.readFullyHeapBuffer(stream, readBuffer);    Assert.assertEquals(10, readBuffer.position());    Assert.assertEquals(10, readBuffer.limit());    readBuffer.flip();    Assert.assertEquals("Buffer contents should match", ByteBuffer.wrap(TEST_ARRAY), readBuffer);}
0
public void testHeapReadFullyPosition() throws Exception
{    final ByteBuffer readBuffer = ByteBuffer.allocate(10);    readBuffer.position(3);    readBuffer.mark();    MockInputStream stream = new MockInputStream(2, 3, 3);    DelegatingSeekableInputStream.readFullyHeapBuffer(stream, readBuffer);    Assert.assertEquals(10, readBuffer.position());    Assert.assertEquals(10, readBuffer.limit());    DelegatingSeekableInputStream.readFullyHeapBuffer(stream, readBuffer);    Assert.assertEquals(10, readBuffer.position());    Assert.assertEquals(10, readBuffer.limit());    readBuffer.reset();    Assert.assertEquals("Buffer contents should match", ByteBuffer.wrap(TEST_ARRAY, 0, 7), readBuffer);}
0
public void testHeapReadFullyLimit() throws Exception
{    final ByteBuffer readBuffer = ByteBuffer.allocate(10);    readBuffer.limit(7);    MockInputStream stream = new MockInputStream(2, 3, 3);    DelegatingSeekableInputStream.readFullyHeapBuffer(stream, readBuffer);    Assert.assertEquals(7, readBuffer.position());    Assert.assertEquals(7, readBuffer.limit());    DelegatingSeekableInputStream.readFullyHeapBuffer(stream, readBuffer);    Assert.assertEquals(7, readBuffer.position());    Assert.assertEquals(7, readBuffer.limit());    readBuffer.flip();    Assert.assertEquals("Buffer contents should match", ByteBuffer.wrap(TEST_ARRAY, 0, 7), readBuffer);    readBuffer.position(7);    readBuffer.limit(10);    DelegatingSeekableInputStream.readFullyHeapBuffer(stream, readBuffer);    Assert.assertEquals(10, readBuffer.position());    Assert.assertEquals(10, readBuffer.limit());    readBuffer.flip();    Assert.assertEquals("Buffer contents should match", ByteBuffer.wrap(TEST_ARRAY), readBuffer);}
0
public void testHeapReadFullyPositionAndLimit() throws Exception
{    final ByteBuffer readBuffer = ByteBuffer.allocate(10);    readBuffer.position(3);    readBuffer.limit(7);    readBuffer.mark();    MockInputStream stream = new MockInputStream(2, 3, 3);    DelegatingSeekableInputStream.readFullyHeapBuffer(stream, readBuffer);    Assert.assertEquals(7, readBuffer.position());    Assert.assertEquals(7, readBuffer.limit());    DelegatingSeekableInputStream.readFullyHeapBuffer(stream, readBuffer);    Assert.assertEquals(7, readBuffer.position());    Assert.assertEquals(7, readBuffer.limit());    readBuffer.reset();    Assert.assertEquals("Buffer contents should match", ByteBuffer.wrap(TEST_ARRAY, 0, 4), readBuffer);    readBuffer.position(7);    readBuffer.limit(10);    DelegatingSeekableInputStream.readFullyHeapBuffer(stream, readBuffer);    Assert.assertEquals(10, readBuffer.position());    Assert.assertEquals(10, readBuffer.limit());    readBuffer.reset();    Assert.assertEquals("Buffer contents should match", ByteBuffer.wrap(TEST_ARRAY, 0, 7), readBuffer);}
0
public void testDirectReadFullySmallBuffer() throws Exception
{    ByteBuffer readBuffer = ByteBuffer.allocateDirect(8);    MockInputStream stream = new MockInputStream();    DelegatingSeekableInputStream.readFullyDirectBuffer(stream, readBuffer, TEMP.get());    Assert.assertEquals(8, readBuffer.position());    Assert.assertEquals(8, readBuffer.limit());    DelegatingSeekableInputStream.readFullyDirectBuffer(stream, readBuffer, TEMP.get());    Assert.assertEquals(8, readBuffer.position());    Assert.assertEquals(8, readBuffer.limit());    readBuffer.flip();    Assert.assertEquals("Buffer contents should match", ByteBuffer.wrap(TEST_ARRAY, 0, 8), readBuffer);}
0
public void testDirectReadFullyLargeBuffer() throws Exception
{    final ByteBuffer readBuffer = ByteBuffer.allocateDirect(20);    final MockInputStream stream = new MockInputStream();    TestUtils.assertThrows("Should throw EOFException", EOFException.class, () -> {        DelegatingSeekableInputStream.readFullyDirectBuffer(stream, readBuffer, TEMP.get());        return null;    });                            Assert.assertEquals(10, readBuffer.position());    Assert.assertEquals(20, readBuffer.limit());}
0
public void testDirectReadFullyJustRight() throws Exception
{    final ByteBuffer readBuffer = ByteBuffer.allocateDirect(10);    MockInputStream stream = new MockInputStream();        DelegatingSeekableInputStream.readFullyDirectBuffer(stream, readBuffer, TEMP.get());    Assert.assertEquals(10, readBuffer.position());    Assert.assertEquals(10, readBuffer.limit());        DelegatingSeekableInputStream.readFullyDirectBuffer(stream, readBuffer, TEMP.get());    Assert.assertEquals(10, readBuffer.position());    Assert.assertEquals(10, readBuffer.limit());    readBuffer.flip();    Assert.assertEquals("Buffer contents should match", ByteBuffer.wrap(TEST_ARRAY), readBuffer);}
0
public void testDirectReadFullySmallReads() throws Exception
{    final ByteBuffer readBuffer = ByteBuffer.allocateDirect(10);    MockInputStream stream = new MockInputStream(2, 3, 3);    DelegatingSeekableInputStream.readFullyDirectBuffer(stream, readBuffer, TEMP.get());    Assert.assertEquals(10, readBuffer.position());    Assert.assertEquals(10, readBuffer.limit());    DelegatingSeekableInputStream.readFullyDirectBuffer(stream, readBuffer, TEMP.get());    Assert.assertEquals(10, readBuffer.position());    Assert.assertEquals(10, readBuffer.limit());    readBuffer.flip();    Assert.assertEquals("Buffer contents should match", ByteBuffer.wrap(TEST_ARRAY), readBuffer);}
0
public void testDirectReadFullyPosition() throws Exception
{    final ByteBuffer readBuffer = ByteBuffer.allocateDirect(10);    readBuffer.position(3);    readBuffer.mark();    MockInputStream stream = new MockInputStream(2, 3, 3);    DelegatingSeekableInputStream.readFullyDirectBuffer(stream, readBuffer, TEMP.get());    Assert.assertEquals(10, readBuffer.position());    Assert.assertEquals(10, readBuffer.limit());    DelegatingSeekableInputStream.readFullyDirectBuffer(stream, readBuffer, TEMP.get());    Assert.assertEquals(10, readBuffer.position());    Assert.assertEquals(10, readBuffer.limit());    readBuffer.reset();    Assert.assertEquals("Buffer contents should match", ByteBuffer.wrap(TEST_ARRAY, 0, 7), readBuffer);}
0
public void testDirectReadFullyLimit() throws Exception
{    final ByteBuffer readBuffer = ByteBuffer.allocateDirect(10);    readBuffer.limit(7);    MockInputStream stream = new MockInputStream(2, 3, 3);    DelegatingSeekableInputStream.readFullyDirectBuffer(stream, readBuffer, TEMP.get());    Assert.assertEquals(7, readBuffer.position());    Assert.assertEquals(7, readBuffer.limit());    DelegatingSeekableInputStream.readFullyDirectBuffer(stream, readBuffer, TEMP.get());    Assert.assertEquals(7, readBuffer.position());    Assert.assertEquals(7, readBuffer.limit());    readBuffer.flip();    Assert.assertEquals("Buffer contents should match", ByteBuffer.wrap(TEST_ARRAY, 0, 7), readBuffer);    readBuffer.position(7);    readBuffer.limit(10);    DelegatingSeekableInputStream.readFullyDirectBuffer(stream, readBuffer, TEMP.get());    Assert.assertEquals(10, readBuffer.position());    Assert.assertEquals(10, readBuffer.limit());    readBuffer.flip();    Assert.assertEquals("Buffer contents should match", ByteBuffer.wrap(TEST_ARRAY), readBuffer);}
0
public void testDirectReadFullyPositionAndLimit() throws Exception
{    final ByteBuffer readBuffer = ByteBuffer.allocateDirect(10);    readBuffer.position(3);    readBuffer.limit(7);    readBuffer.mark();    MockInputStream stream = new MockInputStream(2, 3, 3);    DelegatingSeekableInputStream.readFullyDirectBuffer(stream, readBuffer, TEMP.get());    Assert.assertEquals(7, readBuffer.position());    Assert.assertEquals(7, readBuffer.limit());    DelegatingSeekableInputStream.readFullyDirectBuffer(stream, readBuffer, TEMP.get());    Assert.assertEquals(7, readBuffer.position());    Assert.assertEquals(7, readBuffer.limit());    readBuffer.reset();    Assert.assertEquals("Buffer contents should match", ByteBuffer.wrap(TEST_ARRAY, 0, 4), readBuffer);    readBuffer.position(7);    readBuffer.limit(10);    DelegatingSeekableInputStream.readFullyDirectBuffer(stream, readBuffer, TEMP.get());    Assert.assertEquals(10, readBuffer.position());    Assert.assertEquals(10, readBuffer.limit());    readBuffer.reset();    Assert.assertEquals("Buffer contents should match", ByteBuffer.wrap(TEST_ARRAY, 0, 7), readBuffer);}
0
public void testDirectReadFullySmallTempBufferWithPositionAndLimit() throws Exception
{        byte[] temp = new byte[2];    final ByteBuffer readBuffer = ByteBuffer.allocateDirect(10);    readBuffer.position(3);    readBuffer.limit(7);    readBuffer.mark();    MockInputStream stream = new MockInputStream(2, 3, 3);    DelegatingSeekableInputStream.readFullyDirectBuffer(stream, readBuffer, temp);    Assert.assertEquals(7, readBuffer.position());    Assert.assertEquals(7, readBuffer.limit());    DelegatingSeekableInputStream.readFullyDirectBuffer(stream, readBuffer, temp);    Assert.assertEquals(7, readBuffer.position());    Assert.assertEquals(7, readBuffer.limit());    readBuffer.reset();    Assert.assertEquals("Buffer contents should match", ByteBuffer.wrap(TEST_ARRAY, 0, 4), readBuffer);    readBuffer.position(7);    readBuffer.limit(10);    DelegatingSeekableInputStream.readFullyDirectBuffer(stream, readBuffer, temp);    Assert.assertEquals(10, readBuffer.position());    Assert.assertEquals(10, readBuffer.limit());    readBuffer.reset();    Assert.assertEquals("Buffer contents should match", ByteBuffer.wrap(TEST_ARRAY, 0, 7), readBuffer);}
0
public void testCompare()
{    assertTrue(new SemanticVersion(1, 8, 1).compareTo(new SemanticVersion(1, 8, 1)) == 0);    assertTrue(new SemanticVersion(1, 8, 0).compareTo(new SemanticVersion(1, 8, 1)) < 0);    assertTrue(new SemanticVersion(1, 8, 2).compareTo(new SemanticVersion(1, 8, 1)) > 0);    assertTrue(new SemanticVersion(1, 8, 1).compareTo(new SemanticVersion(1, 8, 1)) == 0);    assertTrue(new SemanticVersion(1, 8, 0).compareTo(new SemanticVersion(1, 8, 1)) < 0);    assertTrue(new SemanticVersion(1, 8, 2).compareTo(new SemanticVersion(1, 8, 1)) > 0);    assertTrue(new SemanticVersion(1, 7, 0).compareTo(new SemanticVersion(1, 8, 0)) < 0);    assertTrue(new SemanticVersion(1, 9, 0).compareTo(new SemanticVersion(1, 8, 0)) > 0);    assertTrue(new SemanticVersion(0, 0, 0).compareTo(new SemanticVersion(1, 0, 0)) < 0);    assertTrue(new SemanticVersion(2, 0, 0).compareTo(new SemanticVersion(1, 0, 0)) > 0);    assertTrue(new SemanticVersion(1, 8, 100).compareTo(new SemanticVersion(1, 9, 0)) < 0);    assertTrue(new SemanticVersion(1, 8, 0).compareTo(new SemanticVersion(1, 8, 0, true)) > 0);    assertTrue(new SemanticVersion(1, 8, 0, true).compareTo(new SemanticVersion(1, 8, 0, true)) == 0);    assertTrue(new SemanticVersion(1, 8, 0, true).compareTo(new SemanticVersion(1, 8, 0)) < 0);}
0
public void testSemverPrereleaseExamples() throws Exception
{    List<String> examples = Arrays.asList("1.0.0-alpha", "1.0.0-alpha.1", "1.0.0-alpha.beta", "1.0.0-beta", "1.0.0-beta.2", "1.0.0-beta.11", "1.0.0-rc.1", "1.0.0");    for (int i = 0; i < examples.size() - 1; i += 1) {        assertLessThan(examples.get(i), examples.get(i + 1));        assertEqualTo(examples.get(i), examples.get(i));    }        assertEqualTo(examples.get(examples.size() - 1), examples.get(examples.size() - 1));}
0
public void testSemverBuildInfoExamples() throws Exception
{    assertEqualTo("1.0.0-alpha+001", "1.0.0-alpha+001");    assertEqualTo("1.0.0-alpha", "1.0.0-alpha+001");    assertEqualTo("1.0.0+20130313144700", "1.0.0+20130313144700");    assertEqualTo("1.0.0", "1.0.0+20130313144700");    assertEqualTo("1.0.0-beta+exp.sha.5114f85", "1.0.0-beta+exp.sha.5114f85");    assertEqualTo("1.0.0-beta", "1.0.0-beta+exp.sha.5114f85");}
0
public void testUnknownComparisons() throws Exception
{        assertLessThan("1.0.0rc0-alpha+001", "1.0.0-alpha");}
0
public void testDistributionVersions() throws Exception
{    assertEqualTo("1.5.0-cdh5.5.0", "1.5.0-cdh5.5.0");    assertLessThan("1.5.0-cdh5.5.0", "1.5.0-cdh5.5.1");    assertLessThan("1.5.0-cdh5.5.0", "1.5.0-cdh5.5.1-SNAPSHOT");    assertLessThan("1.5.0-cdh5.5.0", "1.5.0-cdh5.6.0");    assertLessThan("1.5.0-cdh5.5.0", "1.5.0-cdh6.0.0");    assertLessThan("1.5.0-cdh5.5.0", "1.5.0");        assertLessThan("1.5.0-cdh5.5.0", "1.5.0-cdh5.5.0-SNAPSHOT");}
0
public void testParse() throws Exception
{    assertEquals(new SemanticVersion(1, 8, 0), SemanticVersion.parse("1.8.0"));    assertEquals(new SemanticVersion(1, 8, 0, true), SemanticVersion.parse("1.8.0rc3"));    assertEquals(new SemanticVersion(1, 8, 0, "rc3", "SNAPSHOT", null), SemanticVersion.parse("1.8.0rc3-SNAPSHOT"));    assertEquals(new SemanticVersion(1, 8, 0, null, "SNAPSHOT", null), SemanticVersion.parse("1.8.0-SNAPSHOT"));    assertEquals(new SemanticVersion(1, 5, 0, null, "cdh5.5.0", null), SemanticVersion.parse("1.5.0-cdh5.5.0"));}
0
private static void assertLessThan(String a, String b) throws SemanticVersion.SemanticVersionParseException
{    assertTrue(a + " should be < " + b, SemanticVersion.parse(a).compareTo(SemanticVersion.parse(b)) < 0);    assertTrue(b + " should be > " + a, SemanticVersion.parse(b).compareTo(SemanticVersion.parse(a)) > 0);}
0
private static void assertEqualTo(String a, String b) throws SemanticVersion.SemanticVersionParseException
{    assertTrue(a + " should equal " + b, SemanticVersion.parse(a).compareTo(SemanticVersion.parse(b)) == 0);}
0
public void testCheckArgument()
{    try {        Preconditions.checkArgument(true, "Test message: %s %s", 12, null);    } catch (IllegalArgumentException e) {        Assert.fail("Should not throw exception when isValid is true");    }    try {        Preconditions.checkArgument(false, "Test message: %s %s", 12, null);        Assert.fail("Should throw exception when isValid is false");    } catch (IllegalArgumentException e) {        Assert.assertEquals("Should format message", "Test message: 12 null", e.getMessage());    }}
0
public void testCheckState()
{    try {        Preconditions.checkState(true, "Test message: %s %s", 12, null);    } catch (IllegalStateException e) {        Assert.fail("Should not throw exception when isValid is true");    }    try {        Preconditions.checkState(false, "Test message: %s %s", 12, null);        Assert.fail("Should throw exception when isValid is false");    } catch (IllegalStateException e) {        Assert.assertEquals("Should format message", "Test message: 12 null", e.getMessage());    }}
0
public static void assertThrows(String message, Class<? extends Exception> expected, Callable callable)
{    try {        callable.call();        Assert.fail("No exception was thrown (" + message + "), expected: " + expected.getName());    } catch (Exception actual) {        try {            Assert.assertEquals(message, expected, actual.getClass());        } catch (AssertionError e) {            e.addSuppressed(actual);            throw e;        }    }}
0
public static void assertThrows(String message, Class<? extends Exception> expected, Runnable runnable)
{    try {        runnable.run();        Assert.fail("No exception was thrown (" + message + "), expected: " + expected.getName());    } catch (Exception actual) {        try {            Assert.assertEquals(message, expected, actual.getClass());        } catch (AssertionError e) {            e.addSuppressed(actual);            throw e;        }    }}
0
public static Concatenator newConcatenator(String sep)
{    return new Concatenator(sep);}
0
private void setSeparator(String sep)
{    this.sep = sep;}
0
public String concat(String left, String right)
{    return left + sep + right;}
0
public String concat(String left, String middle, String right)
{    return left + sep + middle + sep + right;}
0
public String concat(Exception e) throws Exception
{    throw e;}
0
public String concat(String... strings)
{    if (strings.length >= 1) {        StringBuilder sb = new StringBuilder();        sb.append(strings[0]);        for (int i = 1; i < strings.length; i += 1) {            sb.append(sep);            sb.append(strings[i]);        }        return sb.toString();    }    return null;}
0
public static String cat(String... strings)
{    return new Concatenator().concat(strings);}
0
public void testNoImplCall()
{    final DynConstructors.Builder builder = new DynConstructors.Builder();    TestUtils.assertThrows("Checked build should throw NoSuchMethodException", NoSuchMethodException.class, (Callable) builder::buildChecked);    TestUtils.assertThrows("Normal build should throw RuntimeException", RuntimeException.class, (Runnable) builder::build);}
0
public void testMissingClass()
{    final DynConstructors.Builder builder = new DynConstructors.Builder().impl("not.a.RealClass");    TestUtils.assertThrows("Checked build should throw NoSuchMethodException", NoSuchMethodException.class, (Callable) builder::buildChecked);    TestUtils.assertThrows("Normal build should throw RuntimeException", RuntimeException.class, (Callable) builder::build);}
0
public void testMissingConstructor()
{    final DynConstructors.Builder builder = new DynConstructors.Builder().impl(Concatenator.class, String.class, String.class);    TestUtils.assertThrows("Checked build should throw NoSuchMethodException", NoSuchMethodException.class, (Callable) builder::buildChecked);    TestUtils.assertThrows("Normal build should throw RuntimeException", RuntimeException.class, (Callable) builder::build);}
0
public void testFirstImplReturned() throws Exception
{    final DynConstructors.Ctor<Concatenator> sepCtor = new DynConstructors.Builder().impl("not.a.RealClass", String.class).impl(Concatenator.class, String.class).impl(Concatenator.class).buildChecked();    Concatenator dashCat = sepCtor.newInstanceChecked("-");    Assert.assertEquals("Should construct with the 1-arg version", "a-b", dashCat.concat("a", "b"));    TestUtils.assertThrows("Should complain about extra arguments", IllegalArgumentException.class, () -> sepCtor.newInstanceChecked("/", "-"));    TestUtils.assertThrows("Should complain about extra arguments", IllegalArgumentException.class, () -> sepCtor.newInstance("/", "-"));    DynConstructors.Ctor<Concatenator> defaultCtor = new DynConstructors.Builder().impl("not.a.RealClass", String.class).impl(Concatenator.class).impl(Concatenator.class, String.class).buildChecked();    Concatenator cat = defaultCtor.newInstanceChecked();    Assert.assertEquals("Should construct with the no-arg version", "ab", cat.concat("a", "b"));}
0
public void testExceptionThrown() throws Exception
{    final SomeCheckedException exc = new SomeCheckedException();    final DynConstructors.Ctor<Concatenator> sepCtor = new DynConstructors.Builder().impl("not.a.RealClass", String.class).impl(Concatenator.class, Exception.class).buildChecked();    TestUtils.assertThrows("Should re-throw the exception", SomeCheckedException.class, () -> sepCtor.newInstanceChecked(exc));    TestUtils.assertThrows("Should wrap the exception in RuntimeException", RuntimeException.class, () -> sepCtor.newInstance(exc));}
0
public void testStringClassname() throws Exception
{    final DynConstructors.Ctor<Concatenator> sepCtor = new DynConstructors.Builder().impl(Concatenator.class.getName(), String.class).buildChecked();    Assert.assertNotNull("Should find 1-arg constructor", sepCtor.newInstance("-"));}
0
public void testHiddenMethod() throws Exception
{    TestUtils.assertThrows("Should fail to find hidden method", NoSuchMethodException.class, () -> new DynMethods.Builder("setSeparator").impl(Concatenator.class, char.class).buildChecked());    final DynConstructors.Ctor<Concatenator> sepCtor = new DynConstructors.Builder().hiddenImpl(Concatenator.class.getName(), char.class).buildChecked();    Assert.assertNotNull("Should find hidden ctor with hiddenImpl", sepCtor);    Concatenator slashCat = sepCtor.newInstanceChecked('/');    Assert.assertEquals("Should use separator /", "a/b", slashCat.concat("a", "b"));}
0
public void testBind() throws Exception
{    final DynConstructors.Ctor<Concatenator> ctor = new DynConstructors.Builder().impl(Concatenator.class.getName()).buildChecked();    Assert.assertTrue("Should always be static", ctor.isStatic());    TestUtils.assertThrows("Should complain that method is static", IllegalStateException.class, () -> ctor.bind(null));}
0
public void testInvoke() throws Exception
{    final DynMethods.UnboundMethod ctor = new DynConstructors.Builder().impl(Concatenator.class.getName()).buildChecked();    TestUtils.assertThrows("Should complain that target must be null", IllegalArgumentException.class, () -> ctor.invokeChecked("a"));    TestUtils.assertThrows("Should complain that target must be null", IllegalArgumentException.class, () -> ctor.invoke("a"));    Assert.assertNotNull("Should allow invokeChecked(null, ...)", ctor.invokeChecked(null));    Assert.assertNotNull("Should allow invoke(null, ...)", ctor.invoke(null));}
0
public void testNoImplCall()
{    final DynMethods.Builder builder = new DynMethods.Builder("concat");    TestUtils.assertThrows("Checked build should throw NoSuchMethodException", NoSuchMethodException.class, (Callable) builder::buildChecked);    TestUtils.assertThrows("Normal build should throw RuntimeException", RuntimeException.class, (Callable) builder::build);}
0
public void testMissingClass()
{    final DynMethods.Builder builder = new DynMethods.Builder("concat").impl("not.a.RealClass", String.class, String.class);    TestUtils.assertThrows("Checked build should throw NoSuchMethodException", NoSuchMethodException.class, (Callable) builder::buildChecked);    TestUtils.assertThrows("Normal build should throw RuntimeException", RuntimeException.class, (Runnable) builder::build);}
0
public void testMissingMethod()
{    final DynMethods.Builder builder = new DynMethods.Builder("concat").impl(Concatenator.class, "cat2strings", String.class, String.class);    TestUtils.assertThrows("Checked build should throw NoSuchMethodException", NoSuchMethodException.class, (Callable) builder::buildChecked);    TestUtils.assertThrows("Normal build should throw RuntimeException", RuntimeException.class, (Runnable) builder::build);}
0
public void testFirstImplReturned() throws Exception
{    Concatenator obj = new Concatenator("-");    DynMethods.UnboundMethod cat2 = new DynMethods.Builder("concat").impl("not.a.RealClass", String.class, String.class).impl(Concatenator.class, String.class, String.class).impl(Concatenator.class, String.class, String.class, String.class).buildChecked();    Assert.assertEquals("Should call the 2-arg version successfully", "a-b", cat2.invoke(obj, "a", "b"));    Assert.assertEquals("Should ignore extra arguments", "a-b", cat2.invoke(obj, "a", "b", "c"));    DynMethods.UnboundMethod cat3 = new DynMethods.Builder("concat").impl("not.a.RealClass", String.class, String.class).impl(Concatenator.class, String.class, String.class, String.class).impl(Concatenator.class, String.class, String.class).build();    Assert.assertEquals("Should call the 3-arg version successfully", "a-b-c", cat3.invoke(obj, "a", "b", "c"));    Assert.assertEquals("Should call the 3-arg version null padding", "a-b-null", cat3.invoke(obj, "a", "b"));}
0
public void testVarArgs() throws Exception
{    DynMethods.UnboundMethod cat = new DynMethods.Builder("concat").impl(Concatenator.class, String[].class).buildChecked();    Assert.assertEquals("Should use the varargs version", "abcde", cat.invokeChecked(new Concatenator(), (Object) new String[] { "a", "b", "c", "d", "e" }));    Assert.assertEquals("Should use the varargs version", "abcde", cat.bind(new Concatenator()).invokeChecked((Object) new String[] { "a", "b", "c", "d", "e" }));}
0
public void testIncorrectArguments() throws Exception
{    final Concatenator obj = new Concatenator("-");    final DynMethods.UnboundMethod cat = new DynMethods.Builder("concat").impl("not.a.RealClass", String.class, String.class).impl(Concatenator.class, String.class, String.class).buildChecked();    TestUtils.assertThrows("Should fail if non-string arguments are passed", IllegalArgumentException.class, () -> cat.invoke(obj, 3, 4));    TestUtils.assertThrows("Should fail if non-string arguments are passed", IllegalArgumentException.class, () -> cat.invokeChecked(obj, 3, 4));}
0
public void testExceptionThrown() throws Exception
{    final SomeCheckedException exc = new SomeCheckedException();    final Concatenator obj = new Concatenator("-");    final DynMethods.UnboundMethod cat = new DynMethods.Builder("concat").impl("not.a.RealClass", String.class, String.class).impl(Concatenator.class, Exception.class).buildChecked();    TestUtils.assertThrows("Should re-throw the exception", SomeCheckedException.class, () -> cat.invokeChecked(obj, exc));    TestUtils.assertThrows("Should wrap the exception in RuntimeException", RuntimeException.class, () -> cat.invoke(obj, exc));}
0
public void testNameChange() throws Exception
{    Concatenator obj = new Concatenator("-");    DynMethods.UnboundMethod cat = new DynMethods.Builder("cat").impl(Concatenator.class, "concat", String.class, String.class).buildChecked();    Assert.assertEquals("Should find 2-arg concat method", "a-b", cat.invoke(obj, "a", "b"));}
0
public void testStringClassname() throws Exception
{    Concatenator obj = new Concatenator("-");    DynMethods.UnboundMethod cat = new DynMethods.Builder("concat").impl(Concatenator.class.getName(), String.class, String.class).buildChecked();    Assert.assertEquals("Should find 2-arg concat method", "a-b", cat.invoke(obj, "a", "b"));}
0
public void testHiddenMethod() throws Exception
{    Concatenator obj = new Concatenator("-");    TestUtils.assertThrows("Should fail to find hidden method", NoSuchMethodException.class, () -> new DynMethods.Builder("setSeparator").impl(Concatenator.class, String.class).buildChecked());    DynMethods.UnboundMethod changeSep = new DynMethods.Builder("setSeparator").hiddenImpl(Concatenator.class, String.class).buildChecked();    Assert.assertNotNull("Should find hidden method with hiddenImpl", changeSep);    changeSep.invokeChecked(obj, "/");    Assert.assertEquals("Should use separator / instead of -", "a/b", obj.concat("a", "b"));}
0
public void testBoundMethod() throws Exception
{    DynMethods.UnboundMethod cat = new DynMethods.Builder("concat").impl(Concatenator.class, String.class, String.class).buildChecked();        DynMethods.BoundMethod dashCat = cat.bind(new Concatenator("-"));    DynMethods.BoundMethod underCat = cat.bind(new Concatenator("_"));    Assert.assertEquals("Should use '-' object without passing", "a-b", dashCat.invoke("a", "b"));    Assert.assertEquals("Should use '_' object without passing", "a_b", underCat.invoke("a", "b"));    DynMethods.BoundMethod slashCat = new DynMethods.Builder("concat").impl(Concatenator.class, String.class, String.class).buildChecked(new Concatenator("/"));    Assert.assertEquals("Should use bound object from builder without passing", "a/b", slashCat.invoke("a", "b"));}
0
public void testBindStaticMethod() throws Exception
{    final DynMethods.Builder builder = new DynMethods.Builder("cat").impl(Concatenator.class, String[].class);    TestUtils.assertThrows("Should complain that method is static", IllegalStateException.class, () -> builder.buildChecked(new Concatenator()));    TestUtils.assertThrows("Should complain that method is static", IllegalStateException.class, () -> builder.build(new Concatenator()));    final DynMethods.UnboundMethod staticCat = builder.buildChecked();    Assert.assertTrue("Should be static", staticCat.isStatic());    TestUtils.assertThrows("Should complain that method is static", IllegalStateException.class, () -> staticCat.bind(new Concatenator()));}
0
public void testStaticMethod() throws Exception
{    DynMethods.StaticMethod staticCat = new DynMethods.Builder("cat").impl(Concatenator.class, String[].class).buildStaticChecked();    Assert.assertEquals("Should call varargs static method cat(String...)", "abcde", staticCat.invokeChecked((Object) new String[] { "a", "b", "c", "d", "e" }));}
0
public void testNonStaticMethod() throws Exception
{    final DynMethods.Builder builder = new DynMethods.Builder("concat").impl(Concatenator.class, String.class, String.class);    TestUtils.assertThrows("Should complain that method is not static", IllegalStateException.class, builder::buildStatic);    TestUtils.assertThrows("Should complain that method is not static", IllegalStateException.class, builder::buildStaticChecked);    final DynMethods.UnboundMethod cat2 = builder.buildChecked();    Assert.assertFalse("concat(String,String) should not be static", cat2.isStatic());    TestUtils.assertThrows("Should complain that method is not static", IllegalStateException.class, cat2::asStatic);}
0
public void testConstructorImpl() throws Exception
{    final DynMethods.Builder builder = new DynMethods.Builder("newConcatenator").ctorImpl(Concatenator.class, String.class).impl(Concatenator.class, String.class);    DynMethods.UnboundMethod newConcatenator = builder.buildChecked();    Assert.assertTrue("Should find constructor implementation", newConcatenator instanceof DynConstructors.Ctor);    Assert.assertTrue("Constructor should be a static method", newConcatenator.isStatic());    Assert.assertFalse("Constructor should not be NOOP", newConcatenator.isNoop());        TestUtils.assertThrows("Should complain that ctor method is static", IllegalStateException.class, () -> builder.buildChecked(new Concatenator()));    TestUtils.assertThrows("Should complain that ctor method is static", IllegalStateException.class, () -> builder.build(new Concatenator()));    Concatenator concatenator = newConcatenator.asStatic().invoke("*");    Assert.assertEquals("Should function as a concatenator", "a*b", concatenator.concat("a", "b"));    concatenator = newConcatenator.asStatic().invokeChecked("@");    Assert.assertEquals("Should function as a concatenator", "a@b", concatenator.concat("a", "b"));}
0
public void testConstructorImplAfterFactoryMethod() throws Exception
{    DynMethods.UnboundMethod newConcatenator = new DynMethods.Builder("newConcatenator").impl(Concatenator.class, String.class).ctorImpl(Concatenator.class, String.class).buildChecked();    Assert.assertFalse("Should find factory method before constructor method", newConcatenator instanceof DynConstructors.Ctor);}
0
public void testNoop() throws Exception
{        DynMethods.UnboundMethod noop = new DynMethods.Builder("concat").impl("not.a.RealClass", String.class, String.class).orNoop().buildChecked();    Assert.assertTrue("No implementation found, should return NOOP", noop.isNoop());    Assert.assertNull("NOOP should always return null", noop.invoke(new Concatenator(), "a"));    Assert.assertNull("NOOP can be called with null", noop.invoke(null, "a"));    Assert.assertNull("NOOP can be bound", noop.bind(new Concatenator()).invoke("a"));    Assert.assertNull("NOOP can be bound to null", noop.bind(null).invoke("a"));    Assert.assertNull("NOOP can be static", noop.asStatic().invoke("a"));}
0
private void assertVersionValid(String v)
{    try {        org.semver.Version.parse(v);    } catch (RuntimeException e) {        throw new RuntimeException(v + " is not a valid semver!", e);    }}
0
public void testVersion()
{    assertVersionValid(Version.VERSION_NUMBER);}
0
public void testFullVersion() throws Exception
{    ParsedVersion version = VersionParser.parse(Version.FULL_VERSION);    assertVersionValid(version.version);    assertEquals(Version.VERSION_NUMBER, version.version);    assertEquals("parquet-mr", version.application);}
0
public void testVersionParser() throws Exception
{    assertEquals(new ParsedVersion("parquet-mr", "1.6.0", "abcd"), VersionParser.parse("parquet-mr version 1.6.0 (build abcd)"));    assertEquals(new ParsedVersion("parquet-mr", "1.6.22rc99-SNAPSHOT", "abcd"), VersionParser.parse("parquet-mr version 1.6.22rc99-SNAPSHOT (build abcd)"));    try {        VersionParser.parse("unparseable string");        fail("this should throw");    } catch (VersionParseException e) {        }        assertEquals(new ParsedVersion("parquet-mr", null, "abcd"), VersionParser.parse("parquet-mr version (build abcd)"));    assertEquals(new ParsedVersion("parquet-mr", null, "abcd"), VersionParser.parse("parquet-mr version  (build abcd)"));        assertEquals(new ParsedVersion("parquet-mr", "1.6.0", null), VersionParser.parse("parquet-mr version 1.6.0 (build )"));    assertEquals(new ParsedVersion("parquet-mr", "1.6.0", null), VersionParser.parse("parquet-mr version 1.6.0 (build)"));    assertEquals(new ParsedVersion("parquet-mr", null, null), VersionParser.parse("parquet-mr version (build)"));    assertEquals(new ParsedVersion("parquet-mr", null, null), VersionParser.parse("parquet-mr version (build )"));        assertEquals(new ParsedVersion("parquet-mr", "1.6.0", null), VersionParser.parse("parquet-mr version 1.6.0"));    assertEquals(new ParsedVersion("parquet-mr", "1.8.0rc4", null), VersionParser.parse("parquet-mr version 1.8.0rc4"));    assertEquals(new ParsedVersion("parquet-mr", "1.8.0rc4-SNAPSHOT", null), VersionParser.parse("parquet-mr version 1.8.0rc4-SNAPSHOT"));    assertEquals(new ParsedVersion("parquet-mr", null, null), VersionParser.parse("parquet-mr version"));        assertEquals(new ParsedVersion("parquet-mr", "1.6.0", null), VersionParser.parse("parquet-mr     version    1.6.0"));    assertEquals(new ParsedVersion("parquet-mr", "1.8.0rc4", null), VersionParser.parse("parquet-mr     version    1.8.0rc4"));    assertEquals(new ParsedVersion("parquet-mr", "1.8.0rc4-SNAPSHOT", null), VersionParser.parse("parquet-mr      version    1.8.0rc4-SNAPSHOT  "));    assertEquals(new ParsedVersion("parquet-mr", null, null), VersionParser.parse("parquet-mr      version"));    assertEquals(new ParsedVersion("parquet-mr", "1.6.0", null), VersionParser.parse("parquet-mr version 1.6.0 (  build )"));    assertEquals(new ParsedVersion("parquet-mr", "1.6.0", null), VersionParser.parse("parquet-mr     version 1.6.0 (    build)"));    assertEquals(new ParsedVersion("parquet-mr", null, null), VersionParser.parse("parquet-mr     version (    build)"));    assertEquals(new ParsedVersion("parquet-mr", null, null), VersionParser.parse("parquet-mr    version    (build    )"));}
0
public static BitPackingWriter getBitPackingWriter(int bitLength, OutputStream out)
{    switch(bitLength) {        case 0:            return new ZeroBitPackingWriter();        case 1:            return new OneBitPackingWriter(out);        case 2:            return new TwoBitPackingWriter(out);        case 3:            return new ThreeBitPackingWriter(out);        case 4:            return new FourBitPackingWriter(out);        case 5:            return new FiveBitPackingWriter(out);        case 6:            return new SixBitPackingWriter(out);        case 7:            return new SevenBitPackingWriter(out);        case 8:            return new EightBitPackingWriter(out);        default:            throw new UnsupportedOperationException("only support up to 8 for now");    }}
0
public static BitPackingReader createBitPackingReader(int bitLength, InputStream in, long valueCount)
{    switch(bitLength) {        case 0:            return new ZeroBitPackingReader();        case 1:            return new OneBitPackingReader(in);        case 2:            return new TwoBitPackingReader(in);        case 3:            return new ThreeBitPackingReader(in, valueCount);        case 4:            return new FourBitPackingReader(in);        case 5:            return new FiveBitPackingReader(in, valueCount);        case 6:            return new SixBitPackingReader(in, valueCount);        case 7:            return new SevenBitPackingReader(in, valueCount);        case 8:            return new EightBitPackingReader(in);        default:            throw new UnsupportedOperationException("only support up to 8 for now");    }}
0
 void finish(int numberOfBits, int buffer, OutputStream out) throws IOException
{    int padding = numberOfBits % 8 == 0 ? 0 : 8 - (numberOfBits % 8);    buffer = buffer << padding;    int numberOfBytes = (numberOfBits + padding) / 8;    for (int i = (numberOfBytes - 1) * 8; i >= 0; i -= 8) {        out.write((buffer >>> i) & 0xFF);    }}
0
 void finish(int numberOfBits, long buffer, OutputStream out) throws IOException
{    int padding = numberOfBits % 8 == 0 ? 0 : 8 - (numberOfBits % 8);    buffer = buffer << padding;    int numberOfBytes = (numberOfBits + padding) / 8;    for (int i = (numberOfBytes - 1) * 8; i >= 0; i -= 8) {        out.write((int) (buffer >>> i) & 0xFF);    }}
0
 int alignToBytes(int bitsCount)
{    return BytesUtils.paddedByteCountFromBits(bitsCount);}
0
public void write(int val) throws IOException
{}
0
public void finish()
{}
0
public int read() throws IOException
{    return 0;}
0
public void write(int val) throws IOException
{    buffer = buffer << 1;    buffer |= val;    ++count;    if (count == 8) {        out.write(buffer);        buffer = 0;        count = 0;    }}
0
public void finish() throws IOException
{    while (count != 0) {        write(0);    }        out = null;}
0
public int read() throws IOException
{    if (count == 0) {        buffer = in.read();        count = 8;    }    int result = (buffer >> (count - 1)) & 1;    --count;    return result;}
0
public void write(int val) throws IOException
{    buffer = buffer << 2;    buffer |= val;    ++count;    if (count == 4) {        out.write(buffer);        buffer = 0;        count = 0;    }}
0
public void finish() throws IOException
{    while (count != 0) {        write(0);    }        out = null;}
0
public int read() throws IOException
{    if (count == 0) {        buffer = in.read();        count = 4;    }    int result = (buffer >> ((count - 1) * 2)) & 3;    --count;    return result;}
0
public void write(int val) throws IOException
{    buffer = buffer << 3;    buffer |= val;    ++count;    if (count == 8) {        out.write((buffer >>> 16) & 0xFF);        out.write((buffer >>> 8) & 0xFF);        out.write((buffer >>> 0) & 0xFF);        buffer = 0;        count = 0;    }}
0
public void finish() throws IOException
{    if (count != 0) {        int numberOfBits = count * 3;        finish(numberOfBits, buffer, out);        buffer = 0;        count = 0;    }        out = null;}
0
public int read() throws IOException
{    if (count == 0) {        if (valueCount - totalRead < 8) {            buffer = 0;            int bitsToRead = 3 * (int) (valueCount - totalRead);            int bytesToRead = alignToBytes(bitsToRead);            for (int i = 3 - 1; i >= 3 - bytesToRead; i--) {                buffer |= in.read() << (i * 8);            }            count = 8;            totalRead = valueCount;        } else {            buffer = (in.read() << 16) + (in.read() << 8) + in.read();            count = 8;            totalRead += 8;        }    }    int result = (buffer >> ((count - 1) * 3)) & 7;    --count;    return result;}
0
public void write(int val) throws IOException
{    buffer = buffer << 4;    buffer |= val;    ++count;    if (count == 2) {        out.write(buffer);        buffer = 0;        count = 0;    }}
0
public void finish() throws IOException
{    while (count != 0) {                write(0);    }        out = null;}
0
public int read() throws IOException
{    if (count == 0) {        buffer = in.read();        count = 2;    }    int result = (buffer >> ((count - 1) * 4)) & 15;    --count;    return result;}
0
public void write(int val) throws IOException
{    buffer = buffer << 5;    buffer |= val;    ++count;    if (count == 8) {        out.write((int) (buffer >>> 32) & 0xFF);        out.write((int) (buffer >>> 24) & 0xFF);        out.write((int) (buffer >>> 16) & 0xFF);        out.write((int) (buffer >>> 8) & 0xFF);        out.write((int) (buffer >>> 0) & 0xFF);        buffer = 0;        count = 0;    }}
0
public void finish() throws IOException
{    if (count != 0) {        int numberOfBits = count * 5;        finish(numberOfBits, buffer, out);        buffer = 0;        count = 0;    }        out = null;}
0
public int read() throws IOException
{    if (count == 0) {        if (valueCount - totalRead < 8) {            buffer = 0;            int bitsToRead = 5 * (int) (valueCount - totalRead);            int bytesToRead = alignToBytes(bitsToRead);            for (int i = 5 - 1; i >= 5 - bytesToRead; i--) {                buffer |= (((long) in.read()) & 255) << (i * 8);            }            count = 8;            totalRead = valueCount;        } else {            buffer = ((((long) in.read()) & 255) << 32) + ((((long) in.read()) & 255) << 24) + (in.read() << 16) + (in.read() << 8) + in.read();            count = 8;            totalRead += 8;        }    }    int result = (((int) (buffer >> ((count - 1) * 5))) & 31);    --count;    return result;}
0
public void write(int val) throws IOException
{    buffer = buffer << 6;    buffer |= val;    ++count;    if (count == 4) {        out.write((buffer >>> 16) & 0xFF);        out.write((buffer >>> 8) & 0xFF);        out.write((buffer >>> 0) & 0xFF);        buffer = 0;        count = 0;    }}
0
public void finish() throws IOException
{    if (count != 0) {        int numberOfBits = count * 6;        finish(numberOfBits, buffer, out);        buffer = 0;        count = 0;    }        out = null;}
0
public int read() throws IOException
{    if (count == 0) {        if (valueCount - totalRead < 4) {            buffer = 0;            int bitsToRead = 6 * (int) (valueCount - totalRead);            int bytesToRead = alignToBytes(bitsToRead);            for (int i = 3 - 1; i >= 3 - bytesToRead; i--) {                buffer |= in.read() << (i * 8);            }            count = 4;            totalRead = valueCount;        } else {            buffer = (in.read() << 16) + (in.read() << 8) + in.read();            count = 4;            totalRead += 4;        }    }    int result = (buffer >> ((count - 1) * 6)) & 63;    --count;    return result;}
0
public void write(int val) throws IOException
{    buffer = buffer << 7;    buffer |= val;    ++count;    if (count == 8) {        out.write((int) (buffer >>> 48) & 0xFF);        out.write((int) (buffer >>> 40) & 0xFF);        out.write((int) (buffer >>> 32) & 0xFF);        out.write((int) (buffer >>> 24) & 0xFF);        out.write((int) (buffer >>> 16) & 0xFF);        out.write((int) (buffer >>> 8) & 0xFF);        out.write((int) (buffer >>> 0) & 0xFF);        buffer = 0;        count = 0;    }}
0
public void finish() throws IOException
{    if (count != 0) {        int numberOfBits = count * 7;        finish(numberOfBits, buffer, out);        buffer = 0;        count = 0;    }        out = null;}
0
public int read() throws IOException
{    if (count == 0) {        if (valueCount - totalRead < 8) {            buffer = 0;            int bitsToRead = 7 * (int) (valueCount - totalRead);            int bytesToRead = alignToBytes(bitsToRead);            for (int i = 7 - 1; i >= 7 - bytesToRead; i--) {                buffer |= (((long) in.read()) & 255) << (i * 8);            }            count = 8;            totalRead = valueCount;        } else {            buffer = ((((long) in.read()) & 255) << 48) + ((((long) in.read()) & 255) << 40) + ((((long) in.read()) & 255) << 32) + ((((long) in.read()) & 255) << 24) + (in.read() << 16) + (in.read() << 8) + in.read();            count = 8;            totalRead += 8;        }    }    int result = (((int) (buffer >> ((count - 1) * 7))) & 127);    --count;    return result;}
0
public void write(int val) throws IOException
{    out.write(val);}
0
public void finish() throws IOException
{        out = null;}
0
public int read() throws IOException
{    return in.read();}
0
public void writeInt(int value) throws IOException
{    input[inputSize] = value;    ++inputSize;    if (inputSize == VALUES_WRITTEN_AT_A_TIME) {        pack();        if (packedPosition == slabSize) {            slabs.add(BytesInput.from(packed));            totalFullSlabSize += slabSize;            if (slabSize < bitWidth * MAX_SLAB_SIZE_MULT) {                slabSize *= 2;            }            initPackedSlab();        }    }}
0
private void pack()
{    packer.pack8Values(input, 0, packed, packedPosition);    packedPosition += bitWidth;    totalValues += inputSize;    inputSize = 0;}
0
private void initPackedSlab()
{    packed = new byte[slabSize];    packedPosition = 0;}
0
public BytesInput toBytes() throws IOException
{    int packedByteLength = packedPosition + BytesUtils.paddedByteCountFromBits(inputSize * bitWidth);        if (inputSize > 0) {        for (int i = inputSize; i < input.length; i++) {            input[i] = 0;        }        pack();    }    return concat(concat(slabs), BytesInput.from(packed, 0, packedByteLength));}
1
public long getBufferSize()
{    return BytesUtils.paddedByteCountFromBits((totalValues + inputSize) * bitWidth);}
0
public long getAllocatedSize()
{    return totalFullSlabSize + packed.length + input.length * 4;}
0
public String memUsageString(String prefix)
{    return String.format("%s ByteBitPacking %d slabs, %d bytes", prefix, slabs.size(), getAllocatedSize());}
0
 int getNumSlabs()
{    return slabs.size() + 1;}
0
public final int getBitWidth()
{    return bitWidth;}
0
public void unpack8Values(final byte[] input, final int inPos, final int[] output, final int outPos)
{    unpack8Values(ByteBuffer.wrap(input), inPos, output, outPos);}
0
public void unpack32Values(byte[] input, int inPos, int[] output, int outPos)
{    unpack32Values(ByteBuffer.wrap(input), inPos, output, outPos);}
0
public final int getBitWidth()
{    return bitWidth;}
0
public final int getBitWidth()
{    return bitWidth;}
0
private static IntPackerFactory getIntPackerFactory(String name)
{    return (IntPackerFactory) getStaticField("org.apache.parquet.column.values.bitpacking." + name, "factory");}
0
private static BytePackerFactory getBytePackerFactory(String name)
{    return (BytePackerFactory) getStaticField("org.apache.parquet.column.values.bitpacking." + name, "factory");}
0
private static BytePackerForLongFactory getBytePackerForLongFactory(String name)
{    return (BytePackerForLongFactory) getStaticField("org.apache.parquet.column.values.bitpacking." + name, "factory");}
0
private static Object getStaticField(String className, String fieldName)
{    try {        return Class.forName(className).getField(fieldName).get(null);    } catch (IllegalArgumentException e) {        throw new RuntimeException(e);    } catch (IllegalAccessException e) {        throw new RuntimeException(e);    } catch (NoSuchFieldException e) {        throw new RuntimeException(e);    } catch (SecurityException e) {        throw new RuntimeException(e);    } catch (ClassNotFoundException e) {        throw new RuntimeException(e);    }}
0
public IntPacker newIntPacker(int width)
{    return beIntPackerFactory.newIntPacker(width);}
0
public BytePacker newBytePacker(int width)
{    return beBytePackerFactory.newBytePacker(width);}
0
public BytePackerForLong newBytePackerForLong(int width)
{    return beBytePackerForLongFactory.newBytePackerForLong(width);}
0
public IntPacker newIntPacker(int width)
{    return leIntPackerFactory.newIntPacker(width);}
0
public BytePacker newBytePacker(int width)
{    return leBytePackerFactory.newBytePacker(width);}
0
public BytePackerForLong newBytePackerForLong(int width)
{    return leBytePackerForLongFactory.newBytePackerForLong(width);}
0
public void testWriteInt() throws Throwable
{    int[] testVals = { Integer.MIN_VALUE, Integer.MAX_VALUE, 0, 100, 1000, 0xdaedbeef };    for (Integer testVal : testVals) {        BytesInput varInt = BytesInput.fromUnsignedVarInt(testVal);        byte[] rno = varInt.toByteArray();        int i = BytesUtils.readUnsignedVarInt(new ByteArrayInputStream(rno));        assertEquals((int) testVal, i);    }}
0
public void testWrite() throws Throwable
{    CapacityByteArrayOutputStream capacityByteArrayOutputStream = newCapacityBAOS(10);    final int expectedSize = 54;    for (int i = 0; i < expectedSize; i++) {        capacityByteArrayOutputStream.write(i);        assertEquals(i + 1, capacityByteArrayOutputStream.size());    }    validate(capacityByteArrayOutputStream, expectedSize);}
0
public void testWriteArray() throws Throwable
{    CapacityByteArrayOutputStream capacityByteArrayOutputStream = newCapacityBAOS(10);    int v = 23;    writeArraysOf3(capacityByteArrayOutputStream, v);    validate(capacityByteArrayOutputStream, v * 3);}
0
public void testWriteArrayAndInt() throws Throwable
{    CapacityByteArrayOutputStream capacityByteArrayOutputStream = newCapacityBAOS(10);    for (int i = 0; i < 23; i++) {        byte[] toWrite = { (byte) (i * 3), (byte) (i * 3 + 1) };        capacityByteArrayOutputStream.write(toWrite);        capacityByteArrayOutputStream.write((byte) (i * 3 + 2));        assertEquals((i + 1) * 3, capacityByteArrayOutputStream.size());    }    validate(capacityByteArrayOutputStream, 23 * 3);}
0
protected CapacityByteArrayOutputStream newCapacityBAOS(int initialSize)
{    return new CapacityByteArrayOutputStream(initialSize, 1000000, new HeapByteBufferAllocator());}
0
public void testReset() throws Throwable
{    CapacityByteArrayOutputStream capacityByteArrayOutputStream = newCapacityBAOS(10);    for (int i = 0; i < 54; i++) {        capacityByteArrayOutputStream.write(i);        assertEquals(i + 1, capacityByteArrayOutputStream.size());    }    capacityByteArrayOutputStream.reset();    for (int i = 0; i < 54; i++) {        capacityByteArrayOutputStream.write(54 + i);        assertEquals(i + 1, capacityByteArrayOutputStream.size());    }    final byte[] byteArray = BytesInput.from(capacityByteArrayOutputStream).toByteArray();    assertEquals(54, byteArray.length);    for (int i = 0; i < 54; i++) {        assertEquals(i + " in " + Arrays.toString(byteArray), 54 + i, byteArray[i]);    }}
0
public void testWriteArrayBiggerThanSlab() throws Throwable
{    CapacityByteArrayOutputStream capacityByteArrayOutputStream = newCapacityBAOS(10);    int v = 23;    writeArraysOf3(capacityByteArrayOutputStream, v);    int n = v * 3;    byte[] toWrite = {     (byte) n, (byte) (n + 1), (byte) (n + 2), (byte) (n + 3), (byte) (n + 4), (byte) (n + 5), (byte) (n + 6), (byte) (n + 7), (byte) (n + 8), (byte) (n + 9), (byte) (n + 10), (byte) (n + 11), (byte) (n + 12), (byte) (n + 13), (byte) (n + 14), (byte) (n + 15), (byte) (n + 16), (byte) (n + 17), (byte) (n + 18), (byte) (n + 19), (byte) (n + 20) };    capacityByteArrayOutputStream.write(toWrite);    n = n + toWrite.length;    assertEquals(n, capacityByteArrayOutputStream.size());    validate(capacityByteArrayOutputStream, n);    capacityByteArrayOutputStream.reset();        capacityByteArrayOutputStream.write(toWrite);    assertEquals(toWrite.length, capacityByteArrayOutputStream.size());    byte[] byteArray = BytesInput.from(capacityByteArrayOutputStream).toByteArray();    assertEquals(toWrite.length, byteArray.length);    for (int i = 0; i < toWrite.length; i++) {        assertEquals(toWrite[i], byteArray[i]);    }}
0
public void testWriteArrayManySlabs() throws Throwable
{    CapacityByteArrayOutputStream capacityByteArrayOutputStream = newCapacityBAOS(10);    int it = 500;    int v = 23;    for (int j = 0; j < it; j++) {        for (int i = 0; i < v; i++) {            byte[] toWrite = { (byte) (i * 3), (byte) (i * 3 + 1), (byte) (i * 3 + 2) };            capacityByteArrayOutputStream.write(toWrite);            assertEquals((i + 1) * 3 + v * 3 * j, capacityByteArrayOutputStream.size());        }    }    byte[] byteArray = BytesInput.from(capacityByteArrayOutputStream).toByteArray();    assertEquals(v * 3 * it, byteArray.length);    for (int i = 0; i < v * 3 * it; i++) {        assertEquals(i % (v * 3), byteArray[i]);    }        assertTrue("slab count: " + capacityByteArrayOutputStream.getSlabCount(), capacityByteArrayOutputStream.getSlabCount() <= 20);    capacityByteArrayOutputStream.reset();    writeArraysOf3(capacityByteArrayOutputStream, v);    validate(capacityByteArrayOutputStream, v * 3);        assertTrue("slab count: " + capacityByteArrayOutputStream.getSlabCount(), capacityByteArrayOutputStream.getSlabCount() <= 2);}
0
public void testReplaceByte() throws Throwable
{        {        CapacityByteArrayOutputStream cbaos = newCapacityBAOS(5);        cbaos.write(10);        assertEquals(0, cbaos.getCurrentIndex());        cbaos.setByte(0, (byte) 7);        ByteArrayOutputStream baos = new ByteArrayOutputStream();        cbaos.writeTo(baos);        assertEquals(7, baos.toByteArray()[0]);    }        {        CapacityByteArrayOutputStream cbaos = newCapacityBAOS(5);        cbaos.write(10);        cbaos.write(13);        cbaos.write(15);        cbaos.write(17);        assertEquals(3, cbaos.getCurrentIndex());        cbaos.write(19);        cbaos.setByte(3, (byte) 7);        ByteArrayOutputStream baos = new ByteArrayOutputStream();        cbaos.writeTo(baos);        assertArrayEquals(new byte[] { 10, 13, 15, 7, 19 }, baos.toByteArray());    }        {        CapacityByteArrayOutputStream cbaos = newCapacityBAOS(5);                for (int i = 0; i < 12; i++) {            cbaos.write(100 + i);        }        assertEquals(11, cbaos.getCurrentIndex());        cbaos.setByte(6, (byte) 7);        ByteArrayOutputStream baos = new ByteArrayOutputStream();        cbaos.writeTo(baos);        assertArrayEquals(new byte[] { 100, 101, 102, 103, 104, 105, 7, 107, 108, 109, 110, 111 }, baos.toByteArray());    }        {        CapacityByteArrayOutputStream cbaos = newCapacityBAOS(5);                for (int i = 0; i < 12; i++) {            cbaos.write(100 + i);        }        assertEquals(11, cbaos.getCurrentIndex());        cbaos.setByte(9, (byte) 7);        ByteArrayOutputStream baos = new ByteArrayOutputStream();        cbaos.writeTo(baos);        assertArrayEquals(new byte[] { 100, 101, 102, 103, 104, 105, 106, 107, 108, 7, 110, 111 }, baos.toByteArray());    }        {        CapacityByteArrayOutputStream cbaos = newCapacityBAOS(5);                for (int i = 0; i < 12; i++) {            cbaos.write(100 + i);        }        assertEquals(11, cbaos.getCurrentIndex());        cbaos.setByte(11, (byte) 7);        ByteArrayOutputStream baos = new ByteArrayOutputStream();        cbaos.writeTo(baos);        assertArrayEquals(new byte[] { 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 7 }, baos.toByteArray());    }}
0
private void writeArraysOf3(CapacityByteArrayOutputStream capacityByteArrayOutputStream, int n) throws IOException
{    for (int i = 0; i < n; i++) {        byte[] toWrite = { (byte) (i * 3), (byte) (i * 3 + 1), (byte) (i * 3 + 2) };        capacityByteArrayOutputStream.write(toWrite);        assertEquals((i + 1) * 3, capacityByteArrayOutputStream.size());    }}
0
private void validate(CapacityByteArrayOutputStream capacityByteArrayOutputStream, final int expectedSize) throws IOException
{    final byte[] byteArray = BytesInput.from(capacityByteArrayOutputStream).toByteArray();    assertEquals(expectedSize, byteArray.length);    for (int i = 0; i < expectedSize; i++) {        assertEquals(i, byteArray[i]);    }}
0
public void testZero() throws IOException
{    int bitLength = 0;    int[] vals = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 };    String expected = "";    validateEncodeDecode(bitLength, vals, expected);}
0
public void testOne_0() throws IOException
{    int[] vals = { 0 };    String expected = "00000000";    validateEncodeDecode(1, vals, expected);}
0
public void testOne_1() throws IOException
{    int[] vals = { 1 };    String expected = "10000000";    validateEncodeDecode(1, vals, expected);}
0
public void testOne_0_0() throws IOException
{    int[] vals = { 0, 0 };    String expected = "00000000";    validateEncodeDecode(1, vals, expected);}
0
public void testOne_1_1() throws IOException
{    int[] vals = { 1, 1 };    String expected = "11000000";    validateEncodeDecode(1, vals, expected);}
0
public void testOne_9_1s() throws IOException
{    int[] vals = { 1, 1, 1, 1, 1, 1, 1, 1, 1 };    String expected = "11111111 10000000";    validateEncodeDecode(1, vals, expected);}
0
public void testOne_9_0s() throws IOException
{    int[] vals = { 0, 0, 0, 0, 0, 0, 0, 0, 0 };    String expected = "00000000 00000000";    validateEncodeDecode(1, vals, expected);}
0
public void testOne_7_0s_1_1() throws IOException
{    int[] vals = { 0, 0, 0, 0, 0, 0, 0, 1 };    String expected = "00000001";    validateEncodeDecode(1, vals, expected);}
0
public void testOne_9_0s_1_1() throws IOException
{    int[] vals = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 1 };    String expected = "00000000 01000000";    validateEncodeDecode(1, vals, expected);}
0
public void testOne() throws IOException
{    int[] vals = { 0, 1, 0, 0, 1, 1, 1, 0, 0, 1 };    String expected = "01001110 01000000";    validateEncodeDecode(1, vals, expected);}
0
public void testTwo() throws IOException
{    int[] vals = { 0, 1, 2, 3, 3, 3, 2, 1, 1, 0, 0, 0, 1 };    String expected = "00011011 11111001 01000000 01000000";    validateEncodeDecode(2, vals, expected);}
0
public void testThree() throws IOException
{    int[] vals = { 0, 1, 2, 3, 4, 5, 6, 7, 1 };    String expected = "00000101 00111001 01110111 " + "00100000";    validateEncodeDecode(3, vals, expected);}
0
public void testFour() throws IOException
{    int[] vals = { 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1 };    String expected = "00000001 00100011 01000101 01100111 10001001 10101011 11001101 11101111 00010000";    validateEncodeDecode(4, vals, expected);}
0
public void testFive() throws IOException
{    int[] vals = { 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 1 };    String expected = "00000000 01000100 00110010 00010100 11000111 " + "01000010 01010100 10110110 00110101 11001111 " + "10000100 01100101 00111010 01010110 11010111 " + "11000110 01110101 10111110 01110111 11011111 " + "00001000";    validateEncodeDecode(5, vals, expected);}
0
public void testSix() throws IOException
{    int[] vals = { 0, 28, 34, 35, 63, 1 };        String expected = "00000001 11001000 10100011 " + "11111100 00010000";    validateEncodeDecode(6, vals, expected);}
0
public void testSeven() throws IOException
{    int[] vals = { 0, 28, 34, 35, 63, 1, 125, 1, 1 };        String expected = "00000000 01110001 00010010 00110111 11100000 01111110 10000001 " + "00000010";    validateEncodeDecode(7, vals, expected);}
0
private void validateEncodeDecode(int bitLength, int[] vals, String expected) throws IOException
{    ByteArrayOutputStream baos = new ByteArrayOutputStream();    BitPackingWriter w = BitPacking.getBitPackingWriter(bitLength, baos);    for (int i : vals) {        w.write(i);    }    w.finish();    byte[] bytes = baos.toByteArray();            Assert.assertEquals(expected, toString(bytes));    ByteArrayInputStream bais = new ByteArrayInputStream(bytes);    BitPackingReader r = BitPacking.createBitPackingReader(bitLength, bais, vals.length);    int[] result = new int[vals.length];    for (int i = 0; i < result.length; i++) {        result[i] = r.read();    }        assertArrayEquals(vals, result);}
1
public static String toString(int[] vals)
{    StringBuilder sb = new StringBuilder();    boolean first = true;    for (int i : vals) {        if (first) {            first = false;        } else {            sb.append(" ");        }        sb.append(i);    }    return sb.toString();}
0
public static String toString(long[] vals)
{    StringBuilder sb = new StringBuilder();    boolean first = true;    for (long i : vals) {        if (first) {            first = false;        } else {            sb.append(" ");        }        sb.append(i);    }    return sb.toString();}
0
public static String toString(byte[] bytes)
{    StringBuilder sb = new StringBuilder();    boolean first = true;    for (byte b : bytes) {        if (first) {            first = false;        } else {            sb.append(" ");        }        int i = b < 0 ? 256 + b : b;        String binaryString = Integer.toBinaryString(i);        for (int j = binaryString.length(); j < 8; ++j) {            sb.append("0");        }        sb.append(binaryString);    }    return sb.toString();}
0
public void testSlabBoundary()
{    for (int i = 0; i <= 32; i++) {        final ByteBasedBitPackingEncoder encoder = new ByteBasedBitPackingEncoder(i, Packer.BIG_ENDIAN);                final int totalValues = 191 * 1024 * 8 + 10;        for (int j = 0; j < totalValues; j++) {            try {                encoder.writeInt(j);            } catch (Exception e) {                throw new RuntimeException(i + ": error writing " + j, e);            }        }        assertEquals(BytesUtils.paddedByteCountFromBits(totalValues * i), encoder.getBufferSize());        assertEquals(i == 0 ? 1 : 9, encoder.getNumSlabs());    }}
0
public void testPackUnPack()
{            for (int i = 1; i < 32; i++) {                int[] unpacked = new int[32];        int[] values = generateValues(i);        packUnpack(Packer.BIG_ENDIAN.newBytePacker(i), values, unpacked);                Assert.assertArrayEquals("width " + i, values, unpacked);    }}
1
public void testPackUnPackLong()
{            for (int i = 1; i < 64; i++) {                long[] unpacked32 = new long[32];        long[] unpacked8 = new long[32];        long[] values = generateValuesLong(i);        packUnpack32(Packer.BIG_ENDIAN.newBytePackerForLong(i), values, unpacked32);                Assert.assertArrayEquals("width " + i, values, unpacked32);        packUnpack8(Packer.BIG_ENDIAN.newBytePackerForLong(i), values, unpacked8);                Assert.assertArrayEquals("width " + i, values, unpacked8);    }}
1
private void packUnpack(BytePacker packer, int[] values, int[] unpacked)
{    byte[] packed = new byte[packer.getBitWidth() * 4];    packer.pack32Values(values, 0, packed, 0);        packer.unpack32Values(ByteBuffer.wrap(packed), 0, unpacked, 0);}
1
private void packUnpack32(BytePackerForLong packer, long[] values, long[] unpacked)
{    byte[] packed = new byte[packer.getBitWidth() * 4];    packer.pack32Values(values, 0, packed, 0);        packer.unpack32Values(packed, 0, unpacked, 0);}
1
private void packUnpack8(BytePackerForLong packer, long[] values, long[] unpacked)
{    byte[] packed = new byte[packer.getBitWidth() * 4];    for (int i = 0; i < 4; i++) {        packer.pack8Values(values, 8 * i, packed, packer.getBitWidth() * i);    }        for (int i = 0; i < 4; i++) {        packer.unpack8Values(packed, packer.getBitWidth() * i, unpacked, 8 * i);    }}
1
private int[] generateValues(int bitWidth)
{    int[] values = new int[32];    for (int j = 0; j < values.length; j++) {        values[j] = (int) (Math.random() * 100000) % (int) Math.pow(2, bitWidth);    }        return values;}
1
private long[] generateValuesLong(int bitWidth)
{    long[] values = new long[32];    Random random = new Random(0);    for (int j = 0; j < values.length; j++) {        values[j] = random.nextLong() & ((1l << bitWidth) - 1l);    }        return values;}
1
public void testPackUnPackAgainstHandWritten() throws IOException
{            for (int i = 1; i < 8; i++) {                byte[] packed = new byte[i * 4];        int[] unpacked = new int[32];        int[] values = generateValues(i);                final BytePacker packer = Packer.BIG_ENDIAN.newBytePacker(i);        packer.pack32Values(values, 0, packed, 0);                        final ByteArrayOutputStream manualOut = new ByteArrayOutputStream();        final BitPackingWriter writer = BitPacking.getBitPackingWriter(i, manualOut);        for (int j = 0; j < values.length; j++) {            writer.write(values[j]);        }        final byte[] packedManualAsBytes = manualOut.toByteArray();                        final BitPackingReader reader = BitPacking.createBitPackingReader(i, new ByteArrayInputStream(packed), 32);        for (int j = 0; j < unpacked.length; j++) {            unpacked[j] = reader.read();        }                Assert.assertArrayEquals("width " + i, values, unpacked);    }}
1
public void testPackUnPackAgainstLemire() throws IOException
{    for (Packer pack : Packer.values()) {                        for (int i = 1; i < 32; i++) {                        int[] packed = new int[i];            int[] unpacked = new int[32];            int[] values = generateValues(i);                        final IntPacker packer = pack.newIntPacker(i);            packer.pack32Values(values, 0, packed, 0);                        final ByteArrayOutputStream lemireOut = new ByteArrayOutputStream();            for (int v : packed) {                switch(pack) {                    case LITTLE_ENDIAN:                        lemireOut.write((v >>> 0) & 0xFF);                        lemireOut.write((v >>> 8) & 0xFF);                        lemireOut.write((v >>> 16) & 0xFF);                        lemireOut.write((v >>> 24) & 0xFF);                        break;                    case BIG_ENDIAN:                        lemireOut.write((v >>> 24) & 0xFF);                        lemireOut.write((v >>> 16) & 0xFF);                        lemireOut.write((v >>> 8) & 0xFF);                        lemireOut.write((v >>> 0) & 0xFF);                        break;                }            }            final byte[] packedByLemireAsBytes = lemireOut.toByteArray();                                    final BytePacker bytePacker = pack.newBytePacker(i);            byte[] packedGenerated = new byte[i * 4];            bytePacker.pack32Values(values, 0, packedGenerated, 0);                        Assert.assertEquals(pack.name() + " width " + i, TestBitPacking.toString(packedByLemireAsBytes), TestBitPacking.toString(packedGenerated));            bytePacker.unpack32Values(ByteBuffer.wrap(packedByLemireAsBytes), 0, unpacked, 0);                        Assert.assertArrayEquals("width " + i, values, unpacked);        }    }}
1
public void testPackUnPack()
{    for (Packer packer : Packer.values()) {                        for (int i = 1; i < 32; i++) {                        int[] values = generateValues(i);            int[] unpacked = new int[32];            {                packUnpack(packer.newIntPacker(i), values, unpacked);                                Assert.assertArrayEquals(packer.name() + " width " + i, values, unpacked);            }            {                packUnpack(packer.newBytePacker(i), values, unpacked);                                Assert.assertArrayEquals(packer.name() + " width " + i, values, unpacked);            }        }    }}
1
private void packUnpack(IntPacker packer, int[] values, int[] unpacked)
{    int[] packed = new int[packer.getBitWidth()];    packer.pack32Values(values, 0, packed, 0);    packer.unpack32Values(packed, 0, unpacked, 0);}
0
private void packUnpack(BytePacker packer, int[] values, int[] unpacked)
{    byte[] packed = new byte[packer.getBitWidth() * 4];    packer.pack32Values(values, 0, packed, 0);    packer.unpack32Values(ByteBuffer.wrap(packed), 0, unpacked, 0);}
0
private int[] generateValues(int bitWidth)
{    int[] values = new int[32];    for (int j = 0; j < values.length; j++) {        values[j] = (int) (Math.random() * 100000) % (int) Math.pow(2, bitWidth);    }        return values;}
1
public void testPackUnPackAgainstHandWritten() throws IOException
{            for (int i = 1; i < 8; i++) {                int[] packed = new int[i];        int[] unpacked = new int[32];        int[] values = generateValues(i);                final IntPacker packer = Packer.BIG_ENDIAN.newIntPacker(i);        packer.pack32Values(values, 0, packed, 0);                final ByteArrayOutputStream lemireOut = new ByteArrayOutputStream();        for (int v : packed) {            lemireOut.write((v >>> 24) & 0xFF);            lemireOut.write((v >>> 16) & 0xFF);            lemireOut.write((v >>> 8) & 0xFF);            lemireOut.write((v >>> 0) & 0xFF);        }        final byte[] packedByLemireAsBytes = lemireOut.toByteArray();                        final ByteArrayOutputStream manualOut = new ByteArrayOutputStream();        final BitPackingWriter writer = BitPacking.getBitPackingWriter(i, manualOut);        for (int j = 0; j < values.length; j++) {            writer.write(values[j]);        }        final byte[] packedManualAsBytes = manualOut.toByteArray();                        final BitPackingReader reader = BitPacking.createBitPackingReader(i, new ByteArrayInputStream(packedByLemireAsBytes), 32);        for (int j = 0; j < unpacked.length; j++) {            unpacked[j] = reader.read();        }                Assert.assertArrayEquals("width " + i, values, unpacked);    }}
1
public DelegatingFieldConsumer onField(TFieldIdEnum e, TypedConsumer typedConsumer)
{    Map<Short, TypedConsumer> newContexts = new HashMap<Short, TypedConsumer>(contexts);    newContexts.put(e.getThriftFieldId(), typedConsumer);    return new DelegatingFieldConsumer(defaultFieldEventConsumer, newContexts);}
0
public void consumeField(TProtocol protocol, EventBasedThriftReader reader, short id, byte type) throws TException
{    TypedConsumer delegate = contexts.get(id);    if (delegate != null) {        delegate.read(protocol, reader, type);    } else {        defaultFieldEventConsumer.consumeField(protocol, reader, id, type);    }}
0
public static DelegatingFieldConsumer fieldConsumer()
{    return new DelegatingFieldConsumer();}
0
public static ListConsumer listOf(Class<T> c, final Consumer<List<T>> consumer)
{    class ListConsumer implements Consumer<T> {        List<T> list;        @Override        public void consume(T t) {            list.add(t);        }    }    final ListConsumer co = new ListConsumer();    return new DelegatingListElementsConsumer(struct(c, co)) {        @Override        public void consumeList(TProtocol protocol, EventBasedThriftReader reader, TList tList) throws TException {            co.list = new ArrayList<T>();            super.consumeList(protocol, reader, tList);            consumer.consume(co.list);        }    };}
0
public void consume(T t)
{    list.add(t);}
0
public void consumeList(TProtocol protocol, EventBasedThriftReader reader, TList tList) throws TException
{    co.list = new ArrayList<T>();    super.consumeList(protocol, reader, tList);    consumer.consume(co.list);}
0
public static ListConsumer listElementsOf(TypedConsumer consumer)
{    return new DelegatingListElementsConsumer(consumer);}
0
public static StructConsumer struct(final Class<T> c, final Consumer<T> consumer)
{    return new TBaseStructConsumer<T>(c, consumer);}
0
public void consumeField(TProtocol protocol, EventBasedThriftReader reader, short id, byte type) throws TException
{    TProtocolUtil.skip(protocol, type);}
0
public void consumeElement(TProtocol protocol, EventBasedThriftReader reader, byte elemType) throws TException
{    elementConsumer.read(protocol, reader, elemType);}
0
public void consumeStruct(TProtocol protocol, EventBasedThriftReader reader) throws TException
{    T o = newObject();    o.read(protocol);    consumer.consume(o);}
0
protected T newObject()
{    try {        return c.newInstance();    } catch (InstantiationException e) {        throw new RuntimeException(c.getName(), e);    } catch (IllegalAccessException e) {        throw new RuntimeException(c.getName(), e);    }}
0
public void readStruct(FieldConsumer c) throws TException
{    protocol.readStructBegin();    readStructContent(c);    protocol.readStructEnd();}
0
public void readStructContent(FieldConsumer c) throws TException
{    TField field;    while (true) {        field = protocol.readFieldBegin();        if (field.type == TType.STOP) {            break;        }        c.consumeField(protocol, this, field.id, field.type);    }}
0
public void readSetContent(SetConsumer eventConsumer, TSet tSet) throws TException
{    for (int i = 0; i < tSet.size; i++) {        eventConsumer.consumeElement(protocol, this, tSet.elemType);    }}
0
public void readMapContent(MapConsumer eventConsumer, TMap tMap) throws TException
{    for (int i = 0; i < tMap.size; i++) {        eventConsumer.consumeEntry(protocol, this, tMap.keyType, tMap.valueType);    }}
0
public void readMapEntry(byte keyType, TypedConsumer keyConsumer, byte valueType, TypedConsumer valueConsumer) throws TException
{    keyConsumer.read(protocol, this, keyType);    valueConsumer.read(protocol, this, valueType);}
0
public void readListContent(ListConsumer eventConsumer, TList tList) throws TException
{    for (int i = 0; i < tList.size; i++) {        eventConsumer.consumeElement(protocol, this, tList.elemType);    }}
0
 final void read(TProtocol protocol, EventBasedThriftReader reader) throws TException
{    this.consume(protocol.readDouble());}
0
 final void read(TProtocol protocol, EventBasedThriftReader reader) throws TException
{    this.consume(protocol.readByte());}
0
 final void read(TProtocol protocol, EventBasedThriftReader reader) throws TException
{    this.consume(protocol.readBool());}
0
 final void read(TProtocol protocol, EventBasedThriftReader reader) throws TException
{    this.consume(protocol.readI32());}
0
 final void read(TProtocol protocol, EventBasedThriftReader reader) throws TException
{    this.consume(protocol.readI64());}
0
 final void read(TProtocol protocol, EventBasedThriftReader reader) throws TException
{    this.consume(protocol.readI16());}
0
 final void read(TProtocol protocol, EventBasedThriftReader reader) throws TException
{    this.consume(protocol.readString());}
0
 final void read(TProtocol protocol, EventBasedThriftReader reader) throws TException
{    this.consumeStruct(protocol, reader);}
0
 final void read(TProtocol protocol, EventBasedThriftReader reader) throws TException
{    this.consumeList(protocol, reader, protocol.readListBegin());    protocol.readListEnd();}
0
public void consumeList(TProtocol protocol, EventBasedThriftReader reader, TList tList) throws TException
{    reader.readListContent(this, tList);}
0
 final void read(TProtocol protocol, EventBasedThriftReader reader) throws TException
{    this.consumeSet(protocol, reader, protocol.readSetBegin());    protocol.readSetEnd();}
0
public void consumeSet(TProtocol protocol, EventBasedThriftReader reader, TSet tSet) throws TException
{    reader.readSetContent(this, tSet);}
0
 final void read(TProtocol protocol, EventBasedThriftReader reader) throws TException
{    this.consumeMap(protocol, reader, protocol.readMapBegin());    protocol.readMapEnd();}
0
public void consumeMap(TProtocol protocol, EventBasedThriftReader reader, TMap tMap) throws TException
{    reader.readMapContent(this, tMap);}
0
public final void read(TProtocol protocol, EventBasedThriftReader reader, byte type) throws TException
{    if (this.type != type) {        throw new TException("Incorrect type in stream. " + "Expected " + this.type + " but got " + type);    }    this.read(protocol, reader);}
0
public TTransport getTransport()
{    return delegate.getTransport();}
0
public void writeMessageBegin(TMessage message) throws TException
{    delegate.writeMessageBegin(message);}
0
public void writeMessageEnd() throws TException
{    delegate.writeMessageEnd();}
0
public int hashCode()
{    return delegate.hashCode();}
0
public void writeStructBegin(TStruct struct) throws TException
{    delegate.writeStructBegin(struct);}
0
public void writeStructEnd() throws TException
{    delegate.writeStructEnd();}
0
public void writeFieldBegin(TField field) throws TException
{    delegate.writeFieldBegin(field);}
0
public void writeFieldEnd() throws TException
{    delegate.writeFieldEnd();}
0
public void writeFieldStop() throws TException
{    delegate.writeFieldStop();}
0
public void writeMapBegin(TMap map) throws TException
{    delegate.writeMapBegin(map);}
0
public void writeMapEnd() throws TException
{    delegate.writeMapEnd();}
0
public void writeListBegin(TList list) throws TException
{    delegate.writeListBegin(list);}
0
public void writeListEnd() throws TException
{    delegate.writeListEnd();}
0
public void writeSetBegin(TSet set) throws TException
{    delegate.writeSetBegin(set);}
0
public void writeSetEnd() throws TException
{    delegate.writeSetEnd();}
0
public void writeBool(boolean b) throws TException
{    delegate.writeBool(b);}
0
public void writeByte(byte b) throws TException
{    delegate.writeByte(b);}
0
public void writeI16(short i16) throws TException
{    delegate.writeI16(i16);}
0
public void writeI32(int i32) throws TException
{    delegate.writeI32(i32);}
0
public void writeI64(long i64) throws TException
{    delegate.writeI64(i64);}
0
public void writeDouble(double dub) throws TException
{    delegate.writeDouble(dub);}
0
public void writeString(String str) throws TException
{    delegate.writeString(str);}
0
public void writeBinary(ByteBuffer buf) throws TException
{    delegate.writeBinary(buf);}
0
public TMessage readMessageBegin() throws TException
{    return delegate.readMessageBegin();}
0
public void readMessageEnd() throws TException
{    delegate.readMessageEnd();}
0
public TStruct readStructBegin() throws TException
{    return delegate.readStructBegin();}
0
public void readStructEnd() throws TException
{    delegate.readStructEnd();}
0
public TField readFieldBegin() throws TException
{    return delegate.readFieldBegin();}
0
public void readFieldEnd() throws TException
{    delegate.readFieldEnd();}
0
public TMap readMapBegin() throws TException
{    return delegate.readMapBegin();}
0
public void readMapEnd() throws TException
{    delegate.readMapEnd();}
0
public TList readListBegin() throws TException
{    return delegate.readListBegin();}
0
public void readListEnd() throws TException
{    delegate.readListEnd();}
0
public TSet readSetBegin() throws TException
{    return delegate.readSetBegin();}
0
public void readSetEnd() throws TException
{    delegate.readSetEnd();}
0
public boolean equals(Object obj)
{    return delegate.equals(obj);}
0
public boolean readBool() throws TException
{    return delegate.readBool();}
0
public byte readByte() throws TException
{    return delegate.readByte();}
0
public short readI16() throws TException
{    return delegate.readI16();}
0
public int readI32() throws TException
{    return delegate.readI32();}
0
public long readI64() throws TException
{    return delegate.readI64();}
0
public double readDouble() throws TException
{    return delegate.readDouble();}
0
public String readString() throws TException
{        return delegate.readString().intern();}
0
public ByteBuffer readBinary() throws TException
{    return delegate.readBinary();}
0
public void reset()
{    delegate.reset();}
0
public String toString()
{    return delegate.toString();}
0
public static LogicalType DECIMAL(int scale, int precision)
{    return LogicalType.DECIMAL(new DecimalType(scale, precision));}
0
public static void writeColumnIndex(ColumnIndex columnIndex, OutputStream to) throws IOException
{    write(columnIndex, to);}
0
public static ColumnIndex readColumnIndex(InputStream from) throws IOException
{    return read(from, new ColumnIndex());}
0
public static void writeOffsetIndex(OffsetIndex offsetIndex, OutputStream to) throws IOException
{    write(offsetIndex, to);}
0
public static OffsetIndex readOffsetIndex(InputStream from) throws IOException
{    return read(from, new OffsetIndex());}
0
public static void writePageHeader(PageHeader pageHeader, OutputStream to) throws IOException
{    write(pageHeader, to);}
0
public static PageHeader readPageHeader(InputStream from) throws IOException
{    return read(from, new PageHeader());}
0
public static void writeFileMetaData(org.apache.parquet.format.FileMetaData fileMetadata, OutputStream to) throws IOException
{    write(fileMetadata, to);}
0
public static FileMetaData readFileMetaData(InputStream from) throws IOException
{    return read(from, new FileMetaData());}
0
public static FileMetaData readFileMetaData(InputStream from, boolean skipRowGroups) throws IOException
{    FileMetaData md = new FileMetaData();    if (skipRowGroups) {        readFileMetaData(from, new DefaultFileMetaDataConsumer(md), skipRowGroups);    } else {        read(from, md);    }    return md;}
0
public void setVersion(int version)
{    md.setVersion(version);}
0
public void setSchema(List<SchemaElement> schema)
{    md.setSchema(schema);}
0
public void setNumRows(long numRows)
{    md.setNum_rows(numRows);}
0
public void setCreatedBy(String createdBy)
{    md.setCreated_by(createdBy);}
0
public void addRowGroup(RowGroup rowGroup)
{    md.addToRow_groups(rowGroup);}
0
public void addKeyValueMetaData(KeyValue kv)
{    md.addToKey_value_metadata(kv);}
0
public static void readFileMetaData(InputStream from, FileMetaDataConsumer consumer) throws IOException
{    readFileMetaData(from, consumer, false);}
0
public static void readFileMetaData(InputStream from, final FileMetaDataConsumer consumer, boolean skipRowGroups) throws IOException
{    try {        DelegatingFieldConsumer eventConsumer = fieldConsumer().onField(VERSION, new I32Consumer() {            @Override            public void consume(int value) {                consumer.setVersion(value);            }        }).onField(SCHEMA, listOf(SchemaElement.class, new Consumer<List<SchemaElement>>() {            @Override            public void consume(List<SchemaElement> schema) {                consumer.setSchema(schema);            }        })).onField(NUM_ROWS, new I64Consumer() {            @Override            public void consume(long value) {                consumer.setNumRows(value);            }        }).onField(KEY_VALUE_METADATA, listElementsOf(struct(KeyValue.class, new Consumer<KeyValue>() {            @Override            public void consume(KeyValue kv) {                consumer.addKeyValueMetaData(kv);            }        }))).onField(CREATED_BY, new StringConsumer() {            @Override            public void consume(String value) {                consumer.setCreatedBy(value);            }        });        if (!skipRowGroups) {            eventConsumer = eventConsumer.onField(ROW_GROUPS, listElementsOf(struct(RowGroup.class, new Consumer<RowGroup>() {                @Override                public void consume(RowGroup rowGroup) {                    consumer.addRowGroup(rowGroup);                }            })));        }        new EventBasedThriftReader(protocol(from)).readStruct(eventConsumer);    } catch (TException e) {        throw new IOException("can not read FileMetaData: " + e.getMessage(), e);    }}
0
public void consume(int value)
{    consumer.setVersion(value);}
0
public void consume(List<SchemaElement> schema)
{    consumer.setSchema(schema);}
0
public void consume(long value)
{    consumer.setNumRows(value);}
0
public void consume(KeyValue kv)
{    consumer.addKeyValueMetaData(kv);}
0
public void consume(String value)
{    consumer.setCreatedBy(value);}
0
public void consume(RowGroup rowGroup)
{    consumer.addRowGroup(rowGroup);}
0
private static TProtocol protocol(OutputStream to)
{    return protocol(new TIOStreamTransport(to));}
0
private static TProtocol protocol(InputStream from)
{    return protocol(new TIOStreamTransport(from));}
0
private static InterningProtocol protocol(TIOStreamTransport t)
{    return new InterningProtocol(new TCompactProtocol(t));}
0
private static T read(InputStream from, T tbase) throws IOException
{    try {        tbase.read(protocol(from));        return tbase;    } catch (TException e) {        throw new IOException("can not read " + tbase.getClass() + ": " + e.getMessage(), e);    }}
0
private static void write(TBase<?, ?> tbase, OutputStream to) throws IOException
{    try {        tbase.write(protocol(to));    } catch (TException e) {        throw new IOException("can not write " + tbase, e);    }}
0
public void testReadFileMetadata() throws Exception
{    ByteArrayOutputStream baos = new ByteArrayOutputStream();    FileMetaData md = new FileMetaData(1, asList(new SchemaElement("foo")), 10, asList(new RowGroup(asList(new ColumnChunk(0), new ColumnChunk(1)), 10, 5), new RowGroup(asList(new ColumnChunk(2), new ColumnChunk(3)), 11, 5)));    writeFileMetaData(md, baos);    FileMetaData md2 = readFileMetaData(in(baos));    FileMetaData md3 = new FileMetaData();    readFileMetaData(in(baos), new DefaultFileMetaDataConsumer(md3));    FileMetaData md4 = new FileMetaData();    readFileMetaData(in(baos), new DefaultFileMetaDataConsumer(md4), true);    FileMetaData md5 = readFileMetaData(in(baos), true);    FileMetaData md6 = readFileMetaData(in(baos), false);    assertEquals(md, md2);    assertEquals(md, md3);    assertNull(md4.getRow_groups());    assertNull(md5.getRow_groups());    assertEquals(md4, md5);    md4.setRow_groups(md.getRow_groups());    md5.setRow_groups(md.getRow_groups());    assertEquals(md, md4);    assertEquals(md, md5);    assertEquals(md4, md5);    assertEquals(md, md6);}
0
private ByteArrayInputStream in(ByteArrayOutputStream baos)
{    return new ByteArrayInputStream(baos.toByteArray());}
0
public static void main(String[] args) throws Exception
{    String basePath = args[0];        generateScheme(false, true, basePath);        generateScheme(false, false, basePath);        generateScheme(true, true, basePath);        generateScheme(true, false, basePath);}
0
private static void generateScheme(boolean isLong, boolean msbFirst, String basePath) throws IOException
{    String baseClassName = isLong ? CLASS_NAME_PREFIX_FOR_LONG : CLASS_NAME_PREFIX_FOR_INT;    String className = msbFirst ? (baseClassName + "BE") : (baseClassName + "LE");    int maxBits = isLong ? MAX_BITS_FOR_LONG : MAX_BITS_FOR_INT;    String nameSuffix = isLong ? "ForLong" : "";    final File file = new File(basePath + "/org/apache/parquet/column/values/bitpacking/" + className + ".java").getAbsoluteFile();    if (!file.getParentFile().exists()) {        file.getParentFile().mkdirs();    }    try (FileWriter fw = new FileWriter(file)) {        fw.append("package org.apache.parquet.column.values.bitpacking;\n");        fw.append("import java.nio.ByteBuffer;\n");        fw.append("\n");        fw.append("/**\n");        if (msbFirst) {            fw.append(" * Packs from the Most Significant Bit first\n");        } else {            fw.append(" * Packs from the Least Significant Bit first\n");        }        fw.append(" * \n");        fw.append(" * See ByteBasedBitPackingGenerator to make changes to this file\n");        fw.append(" * Automatically generated\n");        fw.append(" *\n");        fw.append(" */\n");        fw.append("public abstract class " + className + " {\n");        fw.append("\n");        fw.append("  private static final BytePacker" + nameSuffix + "[] packers = new BytePacker" + nameSuffix + "[" + (maxBits + 1) + "];\n");        fw.append("  static {\n");        for (int i = 0; i <= maxBits; i++) {            fw.append("    packers[" + i + "] = new Packer" + i + "();\n");        }        fw.append("  }\n");        fw.append("\n");        fw.append("  public static final BytePacker" + nameSuffix + "Factory factory = new BytePacker" + nameSuffix + "Factory() {\n");        fw.append("    public BytePacker" + nameSuffix + " newBytePacker" + nameSuffix + "(int bitWidth) {\n");        fw.append("      return packers[bitWidth];\n");        fw.append("    }\n");        fw.append("  };\n");        fw.append("\n");        for (int i = 0; i <= maxBits; i++) {            generateClass(fw, i, isLong, msbFirst);            fw.append("\n");        }        fw.append("}\n");    }}
0
private static void generateClass(FileWriter fw, int bitWidth, boolean isLong, boolean msbFirst) throws IOException
{    String nameSuffix = isLong ? "ForLong" : "";    fw.append("  private static final class Packer" + bitWidth + " extends BytePacker" + nameSuffix + " {\n");    fw.append("\n");    fw.append("    private Packer" + bitWidth + "() {\n");    fw.append("      super(" + bitWidth + ");\n");    fw.append("    }\n");    fw.append("\n");        generatePack(fw, bitWidth, 1, isLong, msbFirst);    generatePack(fw, bitWidth, 4, isLong, msbFirst);        generateUnpack(fw, bitWidth, 1, isLong, msbFirst, true);    generateUnpack(fw, bitWidth, 1, isLong, msbFirst, false);    generateUnpack(fw, bitWidth, 4, isLong, msbFirst, true);    generateUnpack(fw, bitWidth, 4, isLong, msbFirst, false);    fw.append("  }\n");}
0
private static ShiftMask getShift(FileWriter fw, int bitWidth, boolean isLong, boolean msbFirst, int byteIndex, int valueIndex) throws IOException
{        int valueStartBitIndex = (valueIndex * bitWidth) - (8 * (byteIndex));    int valueEndBitIndex = ((valueIndex + 1) * bitWidth) - (8 * (byteIndex + 1));        int valueStartBitWanted;    int valueEndBitWanted;        int byteStartBitWanted;    int byteEndBitWanted;    int shift;    int widthWanted;    if (msbFirst) {        valueStartBitWanted = valueStartBitIndex < 0 ? bitWidth - 1 + valueStartBitIndex : bitWidth - 1;        valueEndBitWanted = valueEndBitIndex > 0 ? valueEndBitIndex : 0;        byteStartBitWanted = valueStartBitIndex < 0 ? 8 : 7 - valueStartBitIndex;        byteEndBitWanted = valueEndBitIndex > 0 ? 0 : -valueEndBitIndex;        shift = valueEndBitWanted - byteEndBitWanted;        widthWanted = Math.min(7, byteStartBitWanted) - Math.min(7, byteEndBitWanted) + 1;    } else {        valueStartBitWanted = bitWidth - 1 - (valueEndBitIndex > 0 ? valueEndBitIndex : 0);        valueEndBitWanted = bitWidth - 1 - (valueStartBitIndex < 0 ? bitWidth - 1 + valueStartBitIndex : bitWidth - 1);        byteStartBitWanted = 7 - (valueEndBitIndex > 0 ? 0 : -valueEndBitIndex);        byteEndBitWanted = 7 - (valueStartBitIndex < 0 ? 8 : 7 - valueStartBitIndex);        shift = valueStartBitWanted - byteStartBitWanted;        widthWanted = Math.max(0, byteStartBitWanted) - Math.max(0, byteEndBitWanted) + 1;    }    int maskWidth = widthWanted + Math.max(0, shift);    visualizeAlignment(fw, bitWidth, valueEndBitIndex, valueStartBitWanted, valueEndBitWanted, byteStartBitWanted, byteEndBitWanted, shift);    return new ShiftMask(shift, genMask(maskWidth, isLong));}
0
private static void visualizeAlignment(FileWriter fw, int bitWidth, int valueEndBitIndex, int valueStartBitWanted, int valueEndBitWanted, int byteStartBitWanted, int byteEndBitWanted, int shift) throws IOException
{        fw.append("//");    int buf = 2 + Math.max(0, bitWidth + 8);    for (int i = 0; i < buf; i++) {        fw.append(" ");    }    fw.append("[");    for (int i = 7; i >= 0; i--) {        if (i <= byteStartBitWanted && i >= byteEndBitWanted) {            fw.append(String.valueOf(i));        } else {            fw.append("_");        }    }    fw.append("]\n          //");    for (int i = 0; i < buf + (8 - bitWidth + shift); i++) {        fw.append(" ");    }    fw.append("[");    for (int i = bitWidth - 1; i >= 0; i--) {        if (i <= valueStartBitWanted && i >= valueEndBitWanted) {            fw.append(String.valueOf(i % 10));        } else {            fw.append("_");        }    }    fw.append("]\n");    fw.append("           ");}
0
private static void generatePack(FileWriter fw, int bitWidth, int batch, boolean isLong, boolean msbFirst) throws IOException
{    long mask = genMask(bitWidth, isLong);    String maskSuffix = isLong ? "L" : "";    String variableType = isLong ? VARIABLE_TYPE_FOR_LONG : VARIABLE_TYPE_FOR_INT;    fw.append("    public final void pack" + (batch * 8) + "Values(final " + variableType + "[] in, final int inPos, final byte[] out, final int outPos) {\n");    for (int byteIndex = 0; byteIndex < bitWidth * batch; ++byteIndex) {        fw.append("      out[" + align(byteIndex, 2) + " + outPos] = (byte)((\n");        int startIndex = (byteIndex * 8) / bitWidth;        int endIndex = ((byteIndex + 1) * 8 + bitWidth - 1) / bitWidth;        for (int valueIndex = startIndex; valueIndex < endIndex; valueIndex++) {            if (valueIndex == startIndex) {                fw.append("          ");            } else {                fw.append("\n        | ");            }            ShiftMask shiftMask = getShift(fw, bitWidth, isLong, msbFirst, byteIndex, valueIndex);                        String shiftString = "";            if (shiftMask.shift > 0) {                shiftString = " >>> " + shiftMask.shift;            } else if (shiftMask.shift < 0) {                shiftString = " <<  " + (-shiftMask.shift);            }            fw.append("((in[" + align(valueIndex, 2) + " + inPos] & " + mask + maskSuffix + ")" + shiftString + ")");        }        fw.append(") & 255);\n");    }    fw.append("    }\n");}
0
private static void generateUnpack(FileWriter fw, int bitWidth, int batch, boolean isLong, boolean msbFirst, boolean useByteArray) throws IOException
{    final String variableType = isLong ? VARIABLE_TYPE_FOR_LONG : VARIABLE_TYPE_FOR_INT;    final String bufferDataType = useByteArray ? "byte[]" : "ByteBuffer";    fw.append("    public final void unpack" + (batch * 8) + "Values(final " + bufferDataType + " in, " + "final int inPos, final " + variableType + "[] out, final int outPos) {\n");    if (bitWidth > 0) {        String maskSuffix = isLong ? "L" : "";        for (int valueIndex = 0; valueIndex < (batch * 8); ++valueIndex) {            fw.append("      out[" + align(valueIndex, 2) + " + outPos] =\n");            int startIndex = valueIndex * bitWidth / 8;            int endIndex = paddedByteCountFromBits((valueIndex + 1) * bitWidth);            for (int byteIndex = startIndex; byteIndex < endIndex; byteIndex++) {                if (byteIndex == startIndex) {                    fw.append("          ");                } else {                    fw.append("\n        | ");                }                ShiftMask shiftMask = getShift(fw, bitWidth, isLong, msbFirst, byteIndex, valueIndex);                                String shiftString = "";                if (shiftMask.shift < 0) {                    shiftString = ">>  " + (-shiftMask.shift);                } else if (shiftMask.shift > 0) {                    shiftString = "<<  " + shiftMask.shift;                }                final String byteAccess;                if (useByteArray) {                    byteAccess = "in[" + align(byteIndex, 2) + " + inPos]";                } else {                                        byteAccess = "in.get(" + align(byteIndex, 2) + " + inPos)";                }                                fw.append(" ((((" + variableType + ")" + byteAccess + ") " + shiftString + ") & " + shiftMask.mask + maskSuffix + ")");            }            fw.append(";\n");        }    }    fw.append("    }\n");}
0
private static long genMask(int bitWidth, boolean isLong)
{    int maxBitWidth = isLong ? MAX_BITS_FOR_LONG : MAX_BITS_FOR_INT;    if (bitWidth >= maxBitWidth) {                return -1;    }    long mask = 0;    for (int i = 0; i < bitWidth; i++) {        mask <<= 1;        mask |= 1;    }    return mask;}
0
private static String align(int value, int digits)
{    final String valueString = String.valueOf(value);    StringBuilder result = new StringBuilder();    for (int i = valueString.length(); i < digits; i++) {        result.append(" ");    }    result.append(valueString);    return result.toString();}
0
private static int paddedByteCountFromBits(int bitLength)
{    return (bitLength + 7) / 8;}
0
public static void main(String[] args) throws Exception
{    String basePath = args[0];    generateScheme(CLASS_NAME_PREFIX + "BE", true, basePath);    generateScheme(CLASS_NAME_PREFIX + "LE", false, basePath);}
0
private static void generateScheme(String className, boolean msbFirst, String basePath) throws IOException
{    final File file = new File(basePath + "/org/apache/parquet/column/values/bitpacking/" + className + ".java").getAbsoluteFile();    if (!file.getParentFile().exists()) {        file.getParentFile().mkdirs();    }    try (FileWriter fw = new FileWriter(file)) {        fw.append("package org.apache.parquet.column.values.bitpacking;\n");        fw.append("\n");        fw.append("/**\n");        fw.append(" * Based on the original implementation at at https://github.com/lemire/JavaFastPFOR/blob/master/src/integercompression/BitPacking.java\n");        fw.append(" * Which is released under the\n");        fw.append(" * Apache License Version 2.0 http://www.apache.org/licenses/.\n");        fw.append(" * By Daniel Lemire, http://lemire.me/en/\n");        fw.append(" * \n");        fw.append(" * Scheme designed by D. Lemire\n");        if (msbFirst) {            fw.append(" * Adapted to pack from the Most Significant Bit first\n");        }        fw.append(" * \n");        fw.append(" * Automatically generated\n");        fw.append(" * @see IntBasedBitPackingGenerator\n");        fw.append(" *\n");        fw.append(" */\n");        fw.append("abstract class " + className + " {\n");        fw.append("\n");        fw.append("  private static final IntPacker[] packers = new IntPacker[32];\n");        fw.append("  static {\n");        for (int i = 0; i < 32; i++) {            fw.append("    packers[" + i + "] = new Packer" + i + "();\n");        }        fw.append("  }\n");        fw.append("\n");        fw.append("  public static final IntPackerFactory factory = new IntPackerFactory() {\n");        fw.append("    public IntPacker newIntPacker(int bitWidth) {\n");        fw.append("      return packers[bitWidth];\n");        fw.append("    }\n");        fw.append("  };\n");        fw.append("\n");        for (int i = 0; i < 32; i++) {            generateClass(fw, i, msbFirst);            fw.append("\n");        }        fw.append("}\n");    }}
0
private static void generateClass(FileWriter fw, int bitWidth, boolean msbFirst) throws IOException
{    int mask = 0;    for (int i = 0; i < bitWidth; i++) {        mask <<= 1;        mask |= 1;    }    fw.append("  private static final class Packer" + bitWidth + " extends IntPacker {\n");    fw.append("\n");    fw.append("    private Packer" + bitWidth + "() {\n");    fw.append("      super(" + bitWidth + ");\n");    fw.append("    }\n");    fw.append("\n");        fw.append("    public final void pack32Values(final int[] in, final int inPos, final int[] out, final int outPos) {\n");    for (int i = 0; i < bitWidth; ++i) {        fw.append("      out[" + align(i, 2) + " + outPos] =\n");        int startIndex = (i * 32) / bitWidth;        int endIndex = ((i + 1) * 32 + bitWidth - 1) / bitWidth;        for (int j = startIndex; j < endIndex; j++) {            if (j == startIndex) {                fw.append("          ");            } else {                fw.append("\n        | ");            }            String shiftString = getPackShiftString(bitWidth, i, startIndex, j, msbFirst);            fw.append("((in[" + align(j, 2) + " + inPos] & " + mask + ")" + shiftString + ")");        }        fw.append(";\n");    }    fw.append("    }\n");        fw.append("    public final void unpack32Values(final int[] in, final int inPos, final int[] out, final int outPos) {\n");    if (bitWidth > 0) {        for (int i = 0; i < 32; ++i) {            fw.append("      out[" + align(i, 2) + " + outPos] =");            int byteIndex = i * bitWidth / 32;            String shiftString = getUnpackShiftString(bitWidth, i, msbFirst);            fw.append(" ((in[" + align(byteIndex, 2) + " + inPos] " + shiftString + ") & " + mask + ")");            if (((i + 1) * bitWidth - 1) / 32 != byteIndex) {                                int bitsRead = ((i + 1) * bitWidth - 1) % 32 + 1;                fw.append(" | ((in[" + align(byteIndex + 1, 2) + " + inPos]");                if (msbFirst) {                    fw.append(") >>> " + align(32 - bitsRead, 2) + ")");                } else {                    int lowerMask = 0;                    for (int j = 0; j < bitsRead; j++) {                        lowerMask <<= 1;                        lowerMask |= 1;                    }                    fw.append(" & " + lowerMask + ") << " + align(bitWidth - bitsRead, 2) + ")");                }            }            fw.append(";\n");        }    }    fw.append("    }\n");    fw.append("  }\n");}
0
private static String getUnpackShiftString(int bitWidth, int i, boolean msbFirst)
{    final int regularShift = i * bitWidth % 32;    String shiftString;    if (msbFirst) {        int shift = 32 - (regularShift + bitWidth);        if (shift < 0) {            shiftString = "<<  " + align(-shift, 2);        } else {            shiftString = ">>> " + align(shift, 2);        }    } else {        shiftString = ">>> " + align(regularShift, 2);    }    return shiftString;}
0
private static String getPackShiftString(int bitWidth, int integerIndex, int startIndex, int valueIndex, boolean msbFirst)
{    String shiftString;    int regularShift = (valueIndex * bitWidth) % 32;    if (msbFirst) {                int shift = 32 - (regularShift + bitWidth);        if (valueIndex == startIndex && (integerIndex * 32) % bitWidth != 0) {                        shiftString = " <<  " + align(32 - (((valueIndex + 1) * bitWidth) % 32), 2);        } else if (shift < 0) {                        shiftString = " >>> " + align(-shift, 2);        } else {            shiftString = " <<  " + align(shift, 2);        }    } else {                if (valueIndex == startIndex && (integerIndex * 32) % bitWidth != 0) {                        shiftString = " >>> " + align(32 - regularShift, 2);        } else {            shiftString = " <<  " + align(regularShift, 2);        }    }    return shiftString;}
0
private static String align(int value, int digits)
{    final String valueString = String.valueOf(value);    StringBuilder result = new StringBuilder();    for (int i = valueString.length(); i < digits; i++) {        result.append(" ");    }    result.append(valueString);    return result.toString();}
0
public static void main(String[] args) throws Exception
{    IntBasedBitPackingGenerator.main(args);    ByteBasedBitPackingGenerator.main(args);}
0
public static void main(String[] args) throws Exception
{    IncrementallyUpdatedFilterPredicateGenerator.main(args);}
0
public static void main(String[] args) throws IOException
{    File srcFile = new File(args[0] + "/org/apache/parquet/filter2/recordlevel/IncrementallyUpdatedFilterPredicateBuilder.java");    srcFile = srcFile.getAbsoluteFile();    File parent = srcFile.getParentFile();    if (!parent.exists()) {        if (!parent.mkdirs()) {            throw new IOException("Couldn't mkdirs for " + parent);        }    }    new IncrementallyUpdatedFilterPredicateGenerator(srcFile).run();}
0
public void run() throws IOException
{    add("package org.apache.parquet.filter2.recordlevel;\n" + "\n" + "import java.util.List;\n" + "\n" + "import org.apache.parquet.hadoop.metadata.ColumnPath;\n" + "import org.apache.parquet.filter2.predicate.Operators.Eq;\n" + "import org.apache.parquet.filter2.predicate.Operators.Gt;\n" + "import org.apache.parquet.filter2.predicate.Operators.GtEq;\n" + "import org.apache.parquet.filter2.predicate.Operators.LogicalNotUserDefined;\n" + "import org.apache.parquet.filter2.predicate.Operators.Lt;\n" + "import org.apache.parquet.filter2.predicate.Operators.LtEq;\n" + "import org.apache.parquet.filter2.predicate.Operators.NotEq;\n" + "import org.apache.parquet.filter2.predicate.Operators.UserDefined;\n" + "import org.apache.parquet.filter2.predicate.UserDefinedPredicate;\n" + "import org.apache.parquet.filter2.recordlevel.IncrementallyUpdatedFilterPredicate.ValueInspector;\n" + "import org.apache.parquet.io.api.Binary;\n" + "import org.apache.parquet.io.PrimitiveColumnIO;\n" + "import org.apache.parquet.schema.PrimitiveComparator;\n\n" + "/**\n" + " * This class is auto-generated by org.apache.parquet.filter2.IncrementallyUpdatedFilterPredicateGenerator\n" + " * Do not manually edit!\n" + " * See {@link IncrementallyUpdatedFilterPredicateBuilderBase}\n" + " */\n");    add("public class IncrementallyUpdatedFilterPredicateBuilder extends IncrementallyUpdatedFilterPredicateBuilderBase {\n\n");    add("  public IncrementallyUpdatedFilterPredicateBuilder(List<PrimitiveColumnIO> leaves) {\n" + "    super(leaves);\n" + "  }\n\n");    addVisitBegin("Eq");    for (TypeInfo info : TYPES) {        addEqNotEqCase(info, true);    }    addVisitEnd();    addVisitBegin("NotEq");    for (TypeInfo info : TYPES) {        addEqNotEqCase(info, false);    }    addVisitEnd();    addVisitBegin("Lt");    for (TypeInfo info : TYPES) {        addInequalityCase(info, "<");    }    addVisitEnd();    addVisitBegin("LtEq");    for (TypeInfo info : TYPES) {        addInequalityCase(info, "<=");    }    addVisitEnd();    addVisitBegin("Gt");    for (TypeInfo info : TYPES) {        addInequalityCase(info, ">");    }    addVisitEnd();    addVisitBegin("GtEq");    for (TypeInfo info : TYPES) {        addInequalityCase(info, ">=");    }    addVisitEnd();    add("  @Override\n" + "  public <T extends Comparable<T>, U extends UserDefinedPredicate<T>> IncrementallyUpdatedFilterPredicate visit(UserDefined<T, U> pred) {\n");    addUdpBegin();    for (TypeInfo info : TYPES) {        addUdpCase(info, false);    }    addVisitEnd();    add("  @Override\n" + "  public <T extends Comparable<T>, U extends UserDefinedPredicate<T>> IncrementallyUpdatedFilterPredicate visit(LogicalNotUserDefined<T, U> notPred) {\n" + "    UserDefined<T, U> pred = notPred.getUserDefined();\n");    addUdpBegin();    for (TypeInfo info : TYPES) {        addUdpCase(info, true);    }    addVisitEnd();    add("}\n");    writer.close();}
0
private void addVisitBegin(String inVar) throws IOException
{    add("  @Override\n" + "  public <T extends Comparable<T>> IncrementallyUpdatedFilterPredicate visit(" + inVar + "<T> pred) {\n" + "    ColumnPath columnPath = pred.getColumn().getColumnPath();\n" + "    Class<T> clazz = pred.getColumn().getColumnType();\n" + "\n" + "    ValueInspector valueInspector = null;\n\n");}
0
private void addVisitEnd() throws IOException
{    add("    if (valueInspector == null) {\n" + "      throw new IllegalArgumentException(\"Encountered unknown type \" + clazz);\n" + "    }\n" + "\n" + "    addValueInspector(columnPath, valueInspector);\n" + "    return valueInspector;\n" + "  }\n\n");}
0
private void addEqNotEqCase(TypeInfo info, boolean isEq) throws IOException
{    add("    if (clazz.equals(" + info.className + ".class)) {\n" + "      if (pred.getValue() == null) {\n" + "        valueInspector = new ValueInspector() {\n" + "          @Override\n" + "          public void updateNull() {\n" + "            setResult(" + isEq + ");\n" + "          }\n" + "\n" + "          @Override\n" + "          public void update(" + info.primitiveName + " value) {\n" + "            setResult(" + !isEq + ");\n" + "          }\n" + "        };\n" + "      } else {\n" + "        final " + info.primitiveName + " target = (" + info.className + ") (Object) pred.getValue();\n" + "        final PrimitiveComparator<" + info.className + "> comparator = getComparator(columnPath);\n" + "\n" + "        valueInspector = new ValueInspector() {\n" + "          @Override\n" + "          public void updateNull() {\n" + "            setResult(" + !isEq + ");\n" + "          }\n" + "\n" + "          @Override\n" + "          public void update(" + info.primitiveName + " value) {\n");    add("            setResult(" + compareEquality("value", "target", isEq) + ");\n");    add("          }\n" + "        };\n" + "      }\n" + "    }\n\n");}
0
private void addInequalityCase(TypeInfo info, String op) throws IOException
{    if (!info.supportsInequality) {        add("    if (clazz.equals(" + info.className + ".class)) {\n");        add("      throw new IllegalArgumentException(\"Operator " + op + " not supported for " + info.className + "\");\n");        add("    }\n\n");        return;    }    add("    if (clazz.equals(" + info.className + ".class)) {\n" + "      final " + info.primitiveName + " target = (" + info.className + ") (Object) pred.getValue();\n" + "      final PrimitiveComparator<" + info.className + "> comparator = getComparator(columnPath);\n" + "\n" + "      valueInspector = new ValueInspector() {\n" + "        @Override\n" + "        public void updateNull() {\n" + "          setResult(false);\n" + "        }\n" + "\n" + "        @Override\n" + "        public void update(" + info.primitiveName + " value) {\n");    add("          setResult(comparator.compare(value, target) " + op + " 0);\n");    add("        }\n" + "      };\n" + "    }\n\n");}
0
private void addUdpBegin() throws IOException
{    add("    ColumnPath columnPath = pred.getColumn().getColumnPath();\n" + "    Class<T> clazz = pred.getColumn().getColumnType();\n" + "\n" + "    ValueInspector valueInspector = null;\n" + "\n" + "    final U udp = pred.getUserDefinedPredicate();\n" + "\n");}
0
private void addUdpCase(TypeInfo info, boolean invert) throws IOException
{    add("    if (clazz.equals(" + info.className + ".class)) {\n" + "      valueInspector = new ValueInspector() {\n" + "        @Override\n" + "        public void updateNull() {\n" + "          setResult(" + (invert ? "!" : "") + "udp.acceptsNullValue());\n" + "        }\n" + "\n" + "        @SuppressWarnings(\"unchecked\")\n" + "        @Override\n" + "        public void update(" + info.primitiveName + " value) {\n" + "          setResult(" + (invert ? "!" : "") + "udp.keep((T) (Object) value));\n" + "        }\n" + "      };\n" + "    }\n\n");}
0
private String compareEquality(String var, String target, boolean eq)
{    return "comparator.compare(" + var + ", " + target + ")" + (eq ? " == 0 " : " != 0");}
0
private void add(String s) throws IOException
{    writer.write(s);}
0
public static void main(String[] args) throws Exception
{    VersionGenerator.main(args);}
0
public static void main(String[] args) throws IOException
{    File srcFile = new File(args[0] + "/org/apache/parquet/Version.java");    srcFile = srcFile.getAbsoluteFile();    File parent = srcFile.getParentFile();    if (!parent.exists()) {        if (!parent.mkdirs()) {            throw new IOException("Couldn't mkdirs for " + parent);        }    }    new VersionGenerator(srcFile).run();}
0
public void run() throws IOException
{    InputStream in = VersionGenerator.class.getResourceAsStream("/parquet-version.properties");    if (in == null) {        throw new IOException("/parquet-version.properties not found");    }    Properties props = new Properties();    try {        props.load(in);    } finally {        in.close();    }    add("package org.apache.parquet;\n" + "\n" + "/**\n" + " * This class is auto-generated by org.apache.parquet.version.VersionGenerator\n" + " * Do not manually edit!\n" + " */\n");    add("public class Version {\n");    add("  public static final String VERSION_NUMBER = \"");    add(props.getProperty("versionNumber"));    add("\";\n");    add("  public static final String FULL_VERSION = \"");    add(props.getProperty("fullVersion"));    add("\";\n\n");    add("  public static void main(String[] args) {\n");    add("    System.out.println(FULL_VERSION);\n");    add("  }\n");    add("}\n");    writer.close();}
0
private void add(String s) throws IOException
{    writer.write(s);}
0
public static List<BlockMetaData> filterRowGroups(Filter filter, List<BlockMetaData> blocks, MessageType schema)
{    checkNotNull(filter, "filter");    return filter.accept(new RowGroupFilter(blocks, schema));}
0
public static List<BlockMetaData> filterRowGroups(List<FilterLevel> levels, Filter filter, List<BlockMetaData> blocks, ParquetFileReader reader)
{    checkNotNull(filter, "filter");    return filter.accept(new RowGroupFilter(levels, blocks, reader));}
0
public List<BlockMetaData> visit(FilterCompat.FilterPredicateCompat filterPredicateCompat)
{    FilterPredicate filterPredicate = filterPredicateCompat.getFilterPredicate();        SchemaCompatibilityValidator.validate(filterPredicate, schema);    List<BlockMetaData> filteredBlocks = new ArrayList<BlockMetaData>();    for (BlockMetaData block : blocks) {        boolean drop = false;        if (levels.contains(FilterLevel.STATISTICS)) {            drop = StatisticsFilter.canDrop(filterPredicate, block.getColumns());        }        if (!drop && levels.contains(FilterLevel.DICTIONARY)) {            drop = DictionaryFilter.canDrop(filterPredicate, block.getColumns(), reader.getDictionaryReader(block));        }        if (!drop) {            filteredBlocks.add(block);        }    }    return filteredBlocks;}
0
public List<BlockMetaData> visit(FilterCompat.UnboundRecordFilterCompat unboundRecordFilterCompat)
{    return blocks;}
0
public List<BlockMetaData> visit(NoOpFilter noOpFilter)
{    return blocks;}
0
public static boolean canDrop(FilterPredicate pred, List<ColumnChunkMetaData> columns, DictionaryPageReadStore dictionaries)
{    checkNotNull(pred, "pred");    checkNotNull(columns, "columns");    return pred.accept(new DictionaryFilter(columns, dictionaries));}
0
private ColumnChunkMetaData getColumnChunk(ColumnPath columnPath)
{    return columns.get(columnPath);}
0
private Set<T> expandDictionary(ColumnChunkMetaData meta) throws IOException
{    ColumnDescriptor col = new ColumnDescriptor(meta.getPath().toArray(), meta.getPrimitiveType(), -1, -1);    DictionaryPage page = dictionaries.readDictionaryPage(col);        if (page == null) {        return null;    }    Dictionary dict = page.getEncoding().initDictionary(col, page);    IntFunction<Object> dictValueProvider;    PrimitiveTypeName type = meta.getPrimitiveType().getPrimitiveTypeName();    switch(type) {                case FIXED_LEN_BYTE_ARRAY:        case BINARY:            dictValueProvider = dict::decodeToBinary;            break;        case INT32:            dictValueProvider = dict::decodeToInt;            break;        case INT64:            dictValueProvider = dict::decodeToLong;            break;        case FLOAT:            dictValueProvider = dict::decodeToFloat;            break;        case DOUBLE:            dictValueProvider = dict::decodeToDouble;            break;        default:                        return null;    }    Set<T> dictSet = new HashSet<>();    for (int i = 0; i <= dict.getMaxId(); i++) {        dictSet.add((T) dictValueProvider.apply(i));    }    return dictSet;}
1
public Boolean visit(Eq<T> eq)
{    T value = eq.getValue();    if (value == null) {                return BLOCK_MIGHT_MATCH;    }    Column<T> filterColumn = eq.getColumn();    ColumnChunkMetaData meta = getColumnChunk(filterColumn.getColumnPath());    if (meta == null) {                return BLOCK_CANNOT_MATCH;    }        if (hasNonDictionaryPages(meta)) {        return BLOCK_MIGHT_MATCH;    }    try {        Set<T> dictSet = expandDictionary(meta);        if (dictSet != null && !dictSet.contains(value)) {            return BLOCK_CANNOT_MATCH;        }    } catch (IOException e) {            }        return BLOCK_MIGHT_MATCH;}
1
public Boolean visit(NotEq<T> notEq)
{    Column<T> filterColumn = notEq.getColumn();    ColumnChunkMetaData meta = getColumnChunk(filterColumn.getColumnPath());    T value = notEq.getValue();    if (value == null && meta == null) {                return BLOCK_CANNOT_MATCH;    }    if (value == null) {                return BLOCK_MIGHT_MATCH;    }    if (meta == null) {                return BLOCK_MIGHT_MATCH;    }        if (hasNonDictionaryPages(meta)) {        return BLOCK_MIGHT_MATCH;    }    try {        Set<T> dictSet = expandDictionary(meta);        boolean mayContainNull = (meta.getStatistics() == null || !meta.getStatistics().isNumNullsSet() || meta.getStatistics().getNumNulls() > 0);        if (dictSet != null && dictSet.size() == 1 && dictSet.contains(value) && !mayContainNull) {            return BLOCK_CANNOT_MATCH;        }    } catch (IOException e) {            }    return BLOCK_MIGHT_MATCH;}
1
public Boolean visit(Lt<T> lt)
{    Column<T> filterColumn = lt.getColumn();    ColumnChunkMetaData meta = getColumnChunk(filterColumn.getColumnPath());    if (meta == null) {                return BLOCK_CANNOT_MATCH;    }        if (hasNonDictionaryPages(meta)) {        return BLOCK_MIGHT_MATCH;    }    T value = lt.getValue();    try {        Set<T> dictSet = expandDictionary(meta);        if (dictSet == null) {            return BLOCK_MIGHT_MATCH;        }        Comparator<T> comparator = meta.getPrimitiveType().comparator();        for (T entry : dictSet) {            if (comparator.compare(value, entry) > 0) {                return BLOCK_MIGHT_MATCH;            }        }        return BLOCK_CANNOT_MATCH;    } catch (IOException e) {            }    return BLOCK_MIGHT_MATCH;}
1
public Boolean visit(LtEq<T> ltEq)
{    Column<T> filterColumn = ltEq.getColumn();    ColumnChunkMetaData meta = getColumnChunk(filterColumn.getColumnPath());    if (meta == null) {                return BLOCK_CANNOT_MATCH;    }        if (hasNonDictionaryPages(meta)) {        return BLOCK_MIGHT_MATCH;    }    T value = ltEq.getValue();    filterColumn.getColumnPath();    try {        Set<T> dictSet = expandDictionary(meta);        if (dictSet == null) {            return BLOCK_MIGHT_MATCH;        }        Comparator<T> comparator = meta.getPrimitiveType().comparator();        for (T entry : dictSet) {            if (comparator.compare(value, entry) >= 0) {                return BLOCK_MIGHT_MATCH;            }        }        return BLOCK_CANNOT_MATCH;    } catch (IOException e) {            }    return BLOCK_MIGHT_MATCH;}
1
public Boolean visit(Gt<T> gt)
{    Column<T> filterColumn = gt.getColumn();    ColumnChunkMetaData meta = getColumnChunk(filterColumn.getColumnPath());    if (meta == null) {                return BLOCK_CANNOT_MATCH;    }        if (hasNonDictionaryPages(meta)) {        return BLOCK_MIGHT_MATCH;    }    T value = gt.getValue();    try {        Set<T> dictSet = expandDictionary(meta);        if (dictSet == null) {            return BLOCK_MIGHT_MATCH;        }        Comparator<T> comparator = meta.getPrimitiveType().comparator();        for (T entry : dictSet) {            if (comparator.compare(value, entry) < 0) {                return BLOCK_MIGHT_MATCH;            }        }        return BLOCK_CANNOT_MATCH;    } catch (IOException e) {            }    return BLOCK_MIGHT_MATCH;}
1
public Boolean visit(GtEq<T> gtEq)
{    Column<T> filterColumn = gtEq.getColumn();    ColumnChunkMetaData meta = getColumnChunk(filterColumn.getColumnPath());    if (meta == null) {                return BLOCK_CANNOT_MATCH;    }        if (hasNonDictionaryPages(meta)) {        return BLOCK_MIGHT_MATCH;    }    T value = gtEq.getValue();    filterColumn.getColumnPath();    try {        Set<T> dictSet = expandDictionary(meta);        if (dictSet == null) {            return BLOCK_MIGHT_MATCH;        }        Comparator<T> comparator = meta.getPrimitiveType().comparator();        for (T entry : dictSet) {            if (comparator.compare(value, entry) <= 0) {                return BLOCK_MIGHT_MATCH;            }        }        return BLOCK_CANNOT_MATCH;    } catch (IOException e) {            }    return BLOCK_MIGHT_MATCH;}
1
public Boolean visit(And and)
{    return and.getLeft().accept(this) || and.getRight().accept(this);}
0
public Boolean visit(Or or)
{    return or.getLeft().accept(this) && or.getRight().accept(this);}
0
public Boolean visit(Not not)
{    throw new IllegalArgumentException("This predicate contains a not! Did you forget to run this predicate through LogicalInverseRewriter? " + not);}
0
private Boolean visit(UserDefined<T, U> ud, boolean inverted)
{    Column<T> filterColumn = ud.getColumn();    ColumnChunkMetaData meta = getColumnChunk(filterColumn.getColumnPath());    U udp = ud.getUserDefinedPredicate();        if (meta == null) {        if (inverted) {            return udp.acceptsNullValue();        } else {            return !udp.acceptsNullValue();        }    }    if (hasNonDictionaryPages(meta)) {        return BLOCK_MIGHT_MATCH;    }    try {        Set<T> dictSet = expandDictionary(meta);        if (dictSet == null) {            return BLOCK_MIGHT_MATCH;        }        for (T entry : dictSet) {            boolean keep = udp.keep(entry);            if ((keep && !inverted) || (!keep && inverted))                return BLOCK_MIGHT_MATCH;        }        return BLOCK_CANNOT_MATCH;    } catch (IOException e) {            }    return BLOCK_MIGHT_MATCH;}
1
public Boolean visit(UserDefined<T, U> udp)
{    return visit(udp, false);}
0
public Boolean visit(LogicalNotUserDefined<T, U> udp)
{    return visit(udp.getUserDefined(), true);}
0
private static boolean hasNonDictionaryPages(ColumnChunkMetaData meta)
{    EncodingStats stats = meta.getEncodingStats();    if (stats != null) {        return stats.hasNonDictionaryEncodedPages();    }        Set<Encoding> encodings = new HashSet<Encoding>(meta.getEncodings());    if (encodings.remove(Encoding.PLAIN_DICTIONARY)) {                                encodings.remove(Encoding.RLE);        encodings.remove(Encoding.BIT_PACKED);        if (encodings.isEmpty()) {                        return false;        }        return true;    } else {                return true;    }}
0
public static boolean canDrop(FilterPredicate pred, List<ColumnChunkMetaData> columns)
{    checkNotNull(pred, "pred");    checkNotNull(columns, "columns");    return pred.accept(new StatisticsFilter(columns));}
0
private ColumnChunkMetaData getColumnChunk(ColumnPath columnPath)
{    return columns.get(columnPath);}
0
private boolean isAllNulls(ColumnChunkMetaData column)
{    return column.getStatistics().getNumNulls() == column.getValueCount();}
0
private boolean hasNulls(ColumnChunkMetaData column)
{    return column.getStatistics().getNumNulls() > 0;}
0
public Boolean visit(Eq<T> eq)
{    Column<T> filterColumn = eq.getColumn();    ColumnChunkMetaData meta = getColumnChunk(filterColumn.getColumnPath());    T value = eq.getValue();    if (meta == null) {                if (value != null) {                        return BLOCK_CANNOT_MATCH;        }        return BLOCK_MIGHT_MATCH;    }    Statistics<T> stats = meta.getStatistics();    if (stats.isEmpty()) {                return BLOCK_MIGHT_MATCH;    }    if (value == null) {                if (!stats.isNumNullsSet()) {            return BLOCK_MIGHT_MATCH;        }                return !hasNulls(meta);    }    if (isAllNulls(meta)) {                return BLOCK_CANNOT_MATCH;    }    if (!stats.hasNonNullValue()) {                return BLOCK_MIGHT_MATCH;    }        return stats.compareMinToValue(value) > 0 || stats.compareMaxToValue(value) < 0;}
0
public Boolean visit(NotEq<T> notEq)
{    Column<T> filterColumn = notEq.getColumn();    ColumnChunkMetaData meta = getColumnChunk(filterColumn.getColumnPath());    T value = notEq.getValue();    if (meta == null) {        if (value == null) {                        return BLOCK_CANNOT_MATCH;        }        return BLOCK_MIGHT_MATCH;    }    Statistics<T> stats = meta.getStatistics();    if (stats.isEmpty()) {                return BLOCK_MIGHT_MATCH;    }    if (value == null) {                return isAllNulls(meta);    }    if (stats.isNumNullsSet() && hasNulls(meta)) {                return BLOCK_MIGHT_MATCH;    }    if (!stats.hasNonNullValue()) {                return BLOCK_MIGHT_MATCH;    }        return stats.compareMinToValue(value) == 0 && stats.compareMaxToValue(value) == 0;}
0
public Boolean visit(Lt<T> lt)
{    Column<T> filterColumn = lt.getColumn();    ColumnChunkMetaData meta = getColumnChunk(filterColumn.getColumnPath());    if (meta == null) {                return BLOCK_CANNOT_MATCH;    }    Statistics<T> stats = meta.getStatistics();    if (stats.isEmpty()) {                return BLOCK_MIGHT_MATCH;    }    if (isAllNulls(meta)) {                return BLOCK_CANNOT_MATCH;    }    if (!stats.hasNonNullValue()) {                return BLOCK_MIGHT_MATCH;    }    T value = lt.getValue();        return stats.compareMinToValue(value) >= 0;}
0
public Boolean visit(LtEq<T> ltEq)
{    Column<T> filterColumn = ltEq.getColumn();    ColumnChunkMetaData meta = getColumnChunk(filterColumn.getColumnPath());    if (meta == null) {                return BLOCK_CANNOT_MATCH;    }    Statistics<T> stats = meta.getStatistics();    if (stats.isEmpty()) {                return BLOCK_MIGHT_MATCH;    }    if (isAllNulls(meta)) {                return BLOCK_CANNOT_MATCH;    }    if (!stats.hasNonNullValue()) {                return BLOCK_MIGHT_MATCH;    }    T value = ltEq.getValue();        return stats.compareMinToValue(value) > 0;}
0
public Boolean visit(Gt<T> gt)
{    Column<T> filterColumn = gt.getColumn();    ColumnChunkMetaData meta = getColumnChunk(filterColumn.getColumnPath());    if (meta == null) {                return BLOCK_CANNOT_MATCH;    }    Statistics<T> stats = meta.getStatistics();    if (stats.isEmpty()) {                return BLOCK_MIGHT_MATCH;    }    if (isAllNulls(meta)) {                return BLOCK_CANNOT_MATCH;    }    if (!stats.hasNonNullValue()) {                return BLOCK_MIGHT_MATCH;    }    T value = gt.getValue();        return stats.compareMaxToValue(value) <= 0;}
0
public Boolean visit(GtEq<T> gtEq)
{    Column<T> filterColumn = gtEq.getColumn();    ColumnChunkMetaData meta = getColumnChunk(filterColumn.getColumnPath());    if (meta == null) {                return BLOCK_CANNOT_MATCH;    }    Statistics<T> stats = meta.getStatistics();    if (stats.isEmpty()) {                return BLOCK_MIGHT_MATCH;    }    if (isAllNulls(meta)) {                return BLOCK_CANNOT_MATCH;    }    if (!stats.hasNonNullValue()) {                return BLOCK_MIGHT_MATCH;    }    T value = gtEq.getValue();        return stats.compareMaxToValue(value) < 0;}
0
public Boolean visit(And and)
{        return and.getLeft().accept(this) || and.getRight().accept(this);}
0
public Boolean visit(Or or)
{        return or.getLeft().accept(this) && or.getRight().accept(this);}
0
public Boolean visit(Not not)
{    throw new IllegalArgumentException("This predicate contains a not! Did you forget to run this predicate through LogicalInverseRewriter? " + not);}
0
private Boolean visit(UserDefined<T, U> ud, boolean inverted)
{    Column<T> filterColumn = ud.getColumn();    ColumnChunkMetaData columnChunk = getColumnChunk(filterColumn.getColumnPath());    U udp = ud.getUserDefinedPredicate();    if (columnChunk == null) {                if (inverted) {            return udp.acceptsNullValue();        } else {            return !udp.acceptsNullValue();        }    }    Statistics<T> stats = columnChunk.getStatistics();    if (stats.isEmpty()) {                return BLOCK_MIGHT_MATCH;    }    if (isAllNulls(columnChunk)) {                if (inverted) {            return udp.acceptsNullValue();        } else {            return !udp.acceptsNullValue();        }    }    if (!stats.hasNonNullValue()) {                return BLOCK_MIGHT_MATCH;    }    org.apache.parquet.filter2.predicate.Statistics<T> udpStats = new org.apache.parquet.filter2.predicate.Statistics<T>(stats.genericGetMin(), stats.genericGetMax(), stats.comparator());    if (inverted) {        return udp.inverseCanDrop(udpStats);    } else {        return udp.canDrop(udpStats);    }}
0
public Boolean visit(UserDefined<T, U> ud)
{    return visit(ud, false);}
0
public Boolean visit(LogicalNotUserDefined<T, U> lnud)
{    return visit(lnud.getUserDefined(), true);}
0
public FileMetaData toParquetMetadata(int currentVersion, ParquetMetadata parquetMetadata)
{    List<BlockMetaData> blocks = parquetMetadata.getBlocks();    List<RowGroup> rowGroups = new ArrayList<RowGroup>();    long numRows = 0;    for (BlockMetaData block : blocks) {        numRows += block.getRowCount();        addRowGroup(parquetMetadata, rowGroups, block);    }    FileMetaData fileMetaData = new FileMetaData(currentVersion, toParquetSchema(parquetMetadata.getFileMetaData().getSchema()), numRows, rowGroups);    Set<Entry<String, String>> keyValues = parquetMetadata.getFileMetaData().getKeyValueMetaData().entrySet();    for (Entry<String, String> keyValue : keyValues) {        addKeyValue(fileMetaData, keyValue.getKey(), keyValue.getValue());    }    fileMetaData.setCreated_by(parquetMetadata.getFileMetaData().getCreatedBy());    fileMetaData.setColumn_orders(getColumnOrders(parquetMetadata.getFileMetaData().getSchema()));    return fileMetaData;}
0
private List<ColumnOrder> getColumnOrders(MessageType schema)
{    List<ColumnOrder> columnOrders = new ArrayList<>();        for (int i = 0, n = schema.getPaths().size(); i < n; ++i) {        ColumnOrder columnOrder = new ColumnOrder();        columnOrder.setTYPE_ORDER(TYPE_DEFINED_ORDER);        columnOrders.add(columnOrder);    }    return columnOrders;}
0
 List<SchemaElement> toParquetSchema(MessageType schema)
{    List<SchemaElement> result = new ArrayList<SchemaElement>();    addToList(result, schema);    return result;}
0
private void addToList(final List<SchemaElement> result, org.apache.parquet.schema.Type field)
{    field.accept(new TypeVisitor() {        @Override        public void visit(PrimitiveType primitiveType) {            SchemaElement element = new SchemaElement(primitiveType.getName());            element.setRepetition_type(toParquetRepetition(primitiveType.getRepetition()));            element.setType(getType(primitiveType.getPrimitiveTypeName()));            if (primitiveType.getLogicalTypeAnnotation() != null) {                element.setConverted_type(convertToConvertedType(primitiveType.getLogicalTypeAnnotation()));                element.setLogicalType(convertToLogicalType(primitiveType.getLogicalTypeAnnotation()));            }            if (primitiveType.getDecimalMetadata() != null) {                element.setPrecision(primitiveType.getDecimalMetadata().getPrecision());                element.setScale(primitiveType.getDecimalMetadata().getScale());            }            if (primitiveType.getTypeLength() > 0) {                element.setType_length(primitiveType.getTypeLength());            }            if (primitiveType.getId() != null) {                element.setField_id(primitiveType.getId().intValue());            }            result.add(element);        }        @Override        public void visit(MessageType messageType) {            SchemaElement element = new SchemaElement(messageType.getName());            if (messageType.getId() != null) {                element.setField_id(messageType.getId().intValue());            }            visitChildren(result, messageType.asGroupType(), element);        }        @Override        public void visit(GroupType groupType) {            SchemaElement element = new SchemaElement(groupType.getName());            element.setRepetition_type(toParquetRepetition(groupType.getRepetition()));            if (groupType.getLogicalTypeAnnotation() != null) {                element.setConverted_type(convertToConvertedType(groupType.getLogicalTypeAnnotation()));                element.setLogicalType(convertToLogicalType(groupType.getLogicalTypeAnnotation()));            }            if (groupType.getId() != null) {                element.setField_id(groupType.getId().intValue());            }            visitChildren(result, groupType, element);        }        private void visitChildren(final List<SchemaElement> result, GroupType groupType, SchemaElement element) {            element.setNum_children(groupType.getFieldCount());            result.add(element);            for (org.apache.parquet.schema.Type field : groupType.getFields()) {                addToList(result, field);            }        }    });}
0
public void visit(PrimitiveType primitiveType)
{    SchemaElement element = new SchemaElement(primitiveType.getName());    element.setRepetition_type(toParquetRepetition(primitiveType.getRepetition()));    element.setType(getType(primitiveType.getPrimitiveTypeName()));    if (primitiveType.getLogicalTypeAnnotation() != null) {        element.setConverted_type(convertToConvertedType(primitiveType.getLogicalTypeAnnotation()));        element.setLogicalType(convertToLogicalType(primitiveType.getLogicalTypeAnnotation()));    }    if (primitiveType.getDecimalMetadata() != null) {        element.setPrecision(primitiveType.getDecimalMetadata().getPrecision());        element.setScale(primitiveType.getDecimalMetadata().getScale());    }    if (primitiveType.getTypeLength() > 0) {        element.setType_length(primitiveType.getTypeLength());    }    if (primitiveType.getId() != null) {        element.setField_id(primitiveType.getId().intValue());    }    result.add(element);}
0
public void visit(MessageType messageType)
{    SchemaElement element = new SchemaElement(messageType.getName());    if (messageType.getId() != null) {        element.setField_id(messageType.getId().intValue());    }    visitChildren(result, messageType.asGroupType(), element);}
0
public void visit(GroupType groupType)
{    SchemaElement element = new SchemaElement(groupType.getName());    element.setRepetition_type(toParquetRepetition(groupType.getRepetition()));    if (groupType.getLogicalTypeAnnotation() != null) {        element.setConverted_type(convertToConvertedType(groupType.getLogicalTypeAnnotation()));        element.setLogicalType(convertToLogicalType(groupType.getLogicalTypeAnnotation()));    }    if (groupType.getId() != null) {        element.setField_id(groupType.getId().intValue());    }    visitChildren(result, groupType, element);}
0
private void visitChildren(final List<SchemaElement> result, GroupType groupType, SchemaElement element)
{    element.setNum_children(groupType.getFieldCount());    result.add(element);    for (org.apache.parquet.schema.Type field : groupType.getFields()) {        addToList(result, field);    }}
0
 LogicalType convertToLogicalType(LogicalTypeAnnotation logicalTypeAnnotation)
{    return logicalTypeAnnotation.accept(LOGICAL_TYPE_ANNOTATION_VISITOR).get();}
0
 ConvertedType convertToConvertedType(LogicalTypeAnnotation logicalTypeAnnotation)
{    return logicalTypeAnnotation.accept(CONVERTED_TYPE_CONVERTER_VISITOR).orElse(null);}
0
 static org.apache.parquet.format.TimeUnit convertUnit(LogicalTypeAnnotation.TimeUnit unit)
{    switch(unit) {        case MICROS:            return org.apache.parquet.format.TimeUnit.MICROS(new MicroSeconds());        case MILLIS:            return org.apache.parquet.format.TimeUnit.MILLIS(new MilliSeconds());        case NANOS:            return TimeUnit.NANOS(new NanoSeconds());        default:            throw new RuntimeException("Unknown time unit " + unit);    }}
0
public Optional<ConvertedType> visit(LogicalTypeAnnotation.StringLogicalTypeAnnotation stringLogicalType)
{    return of(ConvertedType.UTF8);}
0
public Optional<ConvertedType> visit(LogicalTypeAnnotation.MapLogicalTypeAnnotation mapLogicalType)
{    return of(ConvertedType.MAP);}
0
public Optional<ConvertedType> visit(LogicalTypeAnnotation.ListLogicalTypeAnnotation listLogicalType)
{    return of(ConvertedType.LIST);}
0
public Optional<ConvertedType> visit(LogicalTypeAnnotation.EnumLogicalTypeAnnotation enumLogicalType)
{    return of(ConvertedType.ENUM);}
0
public Optional<ConvertedType> visit(LogicalTypeAnnotation.DecimalLogicalTypeAnnotation decimalLogicalType)
{    return of(ConvertedType.DECIMAL);}
0
public Optional<ConvertedType> visit(LogicalTypeAnnotation.DateLogicalTypeAnnotation dateLogicalType)
{    return of(ConvertedType.DATE);}
0
public Optional<ConvertedType> visit(LogicalTypeAnnotation.TimeLogicalTypeAnnotation timeLogicalType)
{    if (!timeLogicalType.isAdjustedToUTC()) {        return empty();    }    switch(timeLogicalType.getUnit()) {        case MILLIS:            return of(ConvertedType.TIME_MILLIS);        case MICROS:            return of(ConvertedType.TIME_MICROS);        case NANOS:            return empty();        default:            throw new RuntimeException("Unknown converted type for " + timeLogicalType.toOriginalType());    }}
0
public Optional<ConvertedType> visit(LogicalTypeAnnotation.TimestampLogicalTypeAnnotation timestampLogicalType)
{    if (!timestampLogicalType.isAdjustedToUTC()) {        return empty();    }    switch(timestampLogicalType.getUnit()) {        case MICROS:            return of(ConvertedType.TIMESTAMP_MICROS);        case MILLIS:            return of(ConvertedType.TIMESTAMP_MILLIS);        case NANOS:            return empty();        default:            throw new RuntimeException("Unknown converted type for " + timestampLogicalType.toOriginalType());    }}
0
public Optional<ConvertedType> visit(LogicalTypeAnnotation.IntLogicalTypeAnnotation intLogicalType)
{    boolean signed = intLogicalType.isSigned();    switch(intLogicalType.getBitWidth()) {        case 8:            return of(signed ? ConvertedType.INT_8 : ConvertedType.UINT_8);        case 16:            return of(signed ? ConvertedType.INT_16 : ConvertedType.UINT_16);        case 32:            return of(signed ? ConvertedType.INT_32 : ConvertedType.UINT_32);        case 64:            return of(signed ? ConvertedType.INT_64 : ConvertedType.UINT_64);        default:            throw new RuntimeException("Unknown original type " + intLogicalType.toOriginalType());    }}
0
public Optional<ConvertedType> visit(LogicalTypeAnnotation.JsonLogicalTypeAnnotation jsonLogicalType)
{    return of(ConvertedType.JSON);}
0
public Optional<ConvertedType> visit(LogicalTypeAnnotation.BsonLogicalTypeAnnotation bsonLogicalType)
{    return of(ConvertedType.BSON);}
0
public Optional<ConvertedType> visit(LogicalTypeAnnotation.IntervalLogicalTypeAnnotation intervalLogicalType)
{    return of(ConvertedType.INTERVAL);}
0
public Optional<ConvertedType> visit(LogicalTypeAnnotation.MapKeyValueTypeAnnotation mapKeyValueLogicalType)
{    return of(ConvertedType.MAP_KEY_VALUE);}
0
public Optional<LogicalType> visit(LogicalTypeAnnotation.StringLogicalTypeAnnotation stringLogicalType)
{    return of(LogicalType.STRING(new StringType()));}
0
public Optional<LogicalType> visit(LogicalTypeAnnotation.MapLogicalTypeAnnotation mapLogicalType)
{    return of(LogicalType.MAP(new MapType()));}
0
public Optional<LogicalType> visit(LogicalTypeAnnotation.ListLogicalTypeAnnotation listLogicalType)
{    return of(LogicalType.LIST(new ListType()));}
0
public Optional<LogicalType> visit(LogicalTypeAnnotation.EnumLogicalTypeAnnotation enumLogicalType)
{    return of(LogicalType.ENUM(new EnumType()));}
0
public Optional<LogicalType> visit(LogicalTypeAnnotation.DecimalLogicalTypeAnnotation decimalLogicalType)
{    return of(LogicalType.DECIMAL(new DecimalType(decimalLogicalType.getScale(), decimalLogicalType.getPrecision())));}
0
public Optional<LogicalType> visit(LogicalTypeAnnotation.DateLogicalTypeAnnotation dateLogicalType)
{    return of(LogicalType.DATE(new DateType()));}
0
public Optional<LogicalType> visit(LogicalTypeAnnotation.TimeLogicalTypeAnnotation timeLogicalType)
{    return of(LogicalType.TIME(new TimeType(timeLogicalType.isAdjustedToUTC(), convertUnit(timeLogicalType.getUnit()))));}
0
public Optional<LogicalType> visit(LogicalTypeAnnotation.TimestampLogicalTypeAnnotation timestampLogicalType)
{    return of(LogicalType.TIMESTAMP(new TimestampType(timestampLogicalType.isAdjustedToUTC(), convertUnit(timestampLogicalType.getUnit()))));}
0
public Optional<LogicalType> visit(LogicalTypeAnnotation.IntLogicalTypeAnnotation intLogicalType)
{    return of(LogicalType.INTEGER(new IntType((byte) intLogicalType.getBitWidth(), intLogicalType.isSigned())));}
0
public Optional<LogicalType> visit(LogicalTypeAnnotation.JsonLogicalTypeAnnotation jsonLogicalType)
{    return of(LogicalType.JSON(new JsonType()));}
0
public Optional<LogicalType> visit(LogicalTypeAnnotation.BsonLogicalTypeAnnotation bsonLogicalType)
{    return of(LogicalType.BSON(new BsonType()));}
0
public Optional<LogicalType> visit(LogicalTypeAnnotation.IntervalLogicalTypeAnnotation intervalLogicalType)
{    return of(LogicalType.UNKNOWN(new NullType()));}
0
public Optional<LogicalType> visit(LogicalTypeAnnotation.MapKeyValueTypeAnnotation mapKeyValueLogicalType)
{    return of(LogicalType.UNKNOWN(new NullType()));}
0
private void addRowGroup(ParquetMetadata parquetMetadata, List<RowGroup> rowGroups, BlockMetaData block)
{        List<ColumnChunkMetaData> columns = block.getColumns();    List<ColumnChunk> parquetColumns = new ArrayList<ColumnChunk>();    for (ColumnChunkMetaData columnMetaData : columns) {                ColumnChunk columnChunk = new ColumnChunk(columnMetaData.getFirstDataPageOffset());                columnChunk.file_path = block.getPath();        columnChunk.meta_data = new ColumnMetaData(getType(columnMetaData.getType()), toFormatEncodings(columnMetaData.getEncodings()), Arrays.asList(columnMetaData.getPath().toArray()), toFormatCodec(columnMetaData.getCodec()), columnMetaData.getValueCount(), columnMetaData.getTotalUncompressedSize(), columnMetaData.getTotalSize(), columnMetaData.getFirstDataPageOffset());        columnChunk.meta_data.dictionary_page_offset = columnMetaData.getDictionaryPageOffset();        if (!columnMetaData.getStatistics().isEmpty()) {            columnChunk.meta_data.setStatistics(toParquetStatistics(columnMetaData.getStatistics()));        }        if (columnMetaData.getEncodingStats() != null) {            columnChunk.meta_data.setEncoding_stats(convertEncodingStats(columnMetaData.getEncodingStats()));        }                        IndexReference columnIndexRef = columnMetaData.getColumnIndexReference();        if (columnIndexRef != null) {            columnChunk.setColumn_index_offset(columnIndexRef.getOffset());            columnChunk.setColumn_index_length(columnIndexRef.getLength());        }        IndexReference offsetIndexRef = columnMetaData.getOffsetIndexReference();        if (offsetIndexRef != null) {            columnChunk.setOffset_index_offset(offsetIndexRef.getOffset());            columnChunk.setOffset_index_length(offsetIndexRef.getLength());        }        parquetColumns.add(columnChunk);    }    RowGroup rowGroup = new RowGroup(parquetColumns, block.getTotalByteSize(), block.getRowCount());    rowGroups.add(rowGroup);}
0
private List<Encoding> toFormatEncodings(Set<org.apache.parquet.column.Encoding> encodings)
{    List<Encoding> converted = new ArrayList<Encoding>(encodings.size());    for (org.apache.parquet.column.Encoding encoding : encodings) {        converted.add(getEncoding(encoding));    }    return converted;}
0
 Set<org.apache.parquet.column.Encoding> fromFormatEncodings(List<Encoding> encodings)
{    Set<org.apache.parquet.column.Encoding> converted = new HashSet<org.apache.parquet.column.Encoding>();    for (Encoding encoding : encodings) {        converted.add(getEncoding(encoding));    }        converted = Collections.unmodifiableSet(converted);        Set<org.apache.parquet.column.Encoding> cached = cachedEncodingSets.putIfAbsent(converted, converted);    if (cached == null) {                                cached = converted;    }    return cached;}
0
private CompressionCodecName fromFormatCodec(CompressionCodec codec)
{    return CompressionCodecName.valueOf(codec.toString());}
0
private CompressionCodec toFormatCodec(CompressionCodecName codec)
{    return CompressionCodec.valueOf(codec.toString());}
0
public org.apache.parquet.column.Encoding getEncoding(Encoding encoding)
{    return org.apache.parquet.column.Encoding.valueOf(encoding.name());}
0
public Encoding getEncoding(org.apache.parquet.column.Encoding encoding)
{    return Encoding.valueOf(encoding.name());}
0
public EncodingStats convertEncodingStats(List<PageEncodingStats> stats)
{    if (stats == null) {        return null;    }    EncodingStats.Builder builder = new EncodingStats.Builder();    for (PageEncodingStats stat : stats) {        switch(stat.getPage_type()) {            case DATA_PAGE_V2:                builder.withV2Pages();                        case DATA_PAGE:                builder.addDataEncoding(getEncoding(stat.getEncoding()), stat.getCount());                break;            case DICTIONARY_PAGE:                builder.addDictEncoding(getEncoding(stat.getEncoding()), stat.getCount());                break;        }    }    return builder.build();}
0
public List<PageEncodingStats> convertEncodingStats(EncodingStats stats)
{    if (stats == null) {        return null;    }    List<PageEncodingStats> formatStats = new ArrayList<PageEncodingStats>();    for (org.apache.parquet.column.Encoding encoding : stats.getDictionaryEncodings()) {        formatStats.add(new PageEncodingStats(PageType.DICTIONARY_PAGE, getEncoding(encoding), stats.getNumDictionaryPagesEncodedAs(encoding)));    }    PageType dataPageType = (stats.usesV2Pages() ? PageType.DATA_PAGE_V2 : PageType.DATA_PAGE);    for (org.apache.parquet.column.Encoding encoding : stats.getDataEncodings()) {        formatStats.add(new PageEncodingStats(dataPageType, getEncoding(encoding), stats.getNumDataPagesEncodedAs(encoding)));    }    return formatStats;}
0
public static Statistics toParquetStatistics(org.apache.parquet.column.statistics.Statistics stats)
{    Statistics formatStats = new Statistics();        if (!stats.isEmpty() && stats.isSmallerThan(MAX_STATS_SIZE)) {        formatStats.setNull_count(stats.getNumNulls());        if (stats.hasNonNullValue()) {            byte[] min = stats.getMinBytes();            byte[] max = stats.getMaxBytes();                        if (sortOrder(stats.type()) == SortOrder.SIGNED || Arrays.equals(min, max)) {                formatStats.setMin(min);                formatStats.setMax(max);            }            if (isMinMaxStatsSupported(stats.type()) || Arrays.equals(min, max)) {                formatStats.setMin_value(min);                formatStats.setMax_value(max);            }        }    }    return formatStats;}
0
private static boolean isMinMaxStatsSupported(PrimitiveType type)
{    return type.columnOrder().getColumnOrderName() == ColumnOrderName.TYPE_DEFINED_ORDER;}
0
public static org.apache.parquet.column.statistics.Statistics fromParquetStatistics(Statistics statistics, PrimitiveTypeName type)
{    return fromParquetStatistics(null, statistics, type);}
0
public static org.apache.parquet.column.statistics.Statistics fromParquetStatistics(String createdBy, Statistics statistics, PrimitiveTypeName type)
{    return fromParquetStatisticsInternal(createdBy, statistics, new PrimitiveType(Repetition.OPTIONAL, type, "fake_type"), defaultSortOrder(type));}
0
 static org.apache.parquet.column.statistics.Statistics fromParquetStatisticsInternal(String createdBy, Statistics formatStats, PrimitiveType type, SortOrder typeSortOrder)
{        org.apache.parquet.column.statistics.Statistics.Builder statsBuilder = org.apache.parquet.column.statistics.Statistics.getBuilderForReading(type);    if (formatStats != null) {                if (formatStats.isSetMin_value() && formatStats.isSetMax_value()) {            byte[] min = formatStats.min_value.array();            byte[] max = formatStats.max_value.array();            if (isMinMaxStatsSupported(type) || Arrays.equals(min, max)) {                statsBuilder.withMin(min);                statsBuilder.withMax(max);            }        } else {            boolean isSet = formatStats.isSetMax() && formatStats.isSetMin();            boolean maxEqualsMin = isSet ? Arrays.equals(formatStats.getMin(), formatStats.getMax()) : false;            boolean sortOrdersMatch = SortOrder.SIGNED == typeSortOrder;                        if (!CorruptStatistics.shouldIgnoreStatistics(createdBy, type.getPrimitiveTypeName()) && (sortOrdersMatch || maxEqualsMin)) {                if (isSet) {                    statsBuilder.withMin(formatStats.min.array());                    statsBuilder.withMax(formatStats.max.array());                }            }        }        if (formatStats.isSetNull_count()) {            statsBuilder.withNumNulls(formatStats.null_count);        }    }    return statsBuilder.build();}
0
public org.apache.parquet.column.statistics.Statistics fromParquetStatistics(String createdBy, Statistics statistics, PrimitiveType type)
{    SortOrder expectedOrder = overrideSortOrderToSigned(type) ? SortOrder.SIGNED : sortOrder(type);    return fromParquetStatisticsInternal(createdBy, statistics, type, expectedOrder);}
0
private boolean overrideSortOrderToSigned(PrimitiveType type)
{                LogicalTypeAnnotation annotation = type.getLogicalTypeAnnotation();    return useSignedStringMinMax && PrimitiveTypeName.BINARY == type.getPrimitiveTypeName() && (annotation == null || STRING_TYPES.contains(annotation.getClass()));}
0
private static SortOrder defaultSortOrder(PrimitiveTypeName primitive)
{    switch(primitive) {        case BOOLEAN:        case INT32:        case INT64:        case FLOAT:        case DOUBLE:            return SortOrder.SIGNED;        case BINARY:        case FIXED_LEN_BYTE_ARRAY:            return SortOrder.UNSIGNED;    }    return SortOrder.UNKNOWN;}
0
private static SortOrder sortOrder(PrimitiveType primitive)
{    LogicalTypeAnnotation annotation = primitive.getLogicalTypeAnnotation();    if (annotation != null) {        return annotation.accept(new LogicalTypeAnnotation.LogicalTypeAnnotationVisitor<SortOrder>() {            @Override            public Optional<SortOrder> visit(LogicalTypeAnnotation.IntLogicalTypeAnnotation intLogicalType) {                return intLogicalType.isSigned() ? of(SortOrder.SIGNED) : of(SortOrder.UNSIGNED);            }            @Override            public Optional<SortOrder> visit(LogicalTypeAnnotation.IntervalLogicalTypeAnnotation intervalLogicalType) {                return of(SortOrder.UNKNOWN);            }            @Override            public Optional<SortOrder> visit(LogicalTypeAnnotation.DateLogicalTypeAnnotation dateLogicalType) {                return of(SortOrder.SIGNED);            }            @Override            public Optional<SortOrder> visit(LogicalTypeAnnotation.EnumLogicalTypeAnnotation enumLogicalType) {                return of(SortOrder.UNSIGNED);            }            @Override            public Optional<SortOrder> visit(LogicalTypeAnnotation.BsonLogicalTypeAnnotation bsonLogicalType) {                return of(SortOrder.UNSIGNED);            }            @Override            public Optional<SortOrder> visit(LogicalTypeAnnotation.JsonLogicalTypeAnnotation jsonLogicalType) {                return of(SortOrder.UNSIGNED);            }            @Override            public Optional<SortOrder> visit(LogicalTypeAnnotation.StringLogicalTypeAnnotation stringLogicalType) {                return of(SortOrder.UNSIGNED);            }            @Override            public Optional<SortOrder> visit(LogicalTypeAnnotation.DecimalLogicalTypeAnnotation decimalLogicalType) {                return of(SortOrder.UNKNOWN);            }            @Override            public Optional<SortOrder> visit(LogicalTypeAnnotation.MapKeyValueTypeAnnotation mapKeyValueLogicalType) {                return of(SortOrder.UNKNOWN);            }            @Override            public Optional<SortOrder> visit(LogicalTypeAnnotation.MapLogicalTypeAnnotation mapLogicalType) {                return of(SortOrder.UNKNOWN);            }            @Override            public Optional<SortOrder> visit(LogicalTypeAnnotation.ListLogicalTypeAnnotation listLogicalType) {                return of(SortOrder.UNKNOWN);            }            @Override            public Optional<SortOrder> visit(LogicalTypeAnnotation.TimeLogicalTypeAnnotation timeLogicalType) {                return of(SortOrder.SIGNED);            }            @Override            public Optional<SortOrder> visit(LogicalTypeAnnotation.TimestampLogicalTypeAnnotation timestampLogicalType) {                return of(SortOrder.SIGNED);            }        }).orElse(defaultSortOrder(primitive.getPrimitiveTypeName()));    }    return defaultSortOrder(primitive.getPrimitiveTypeName());}
0
public Optional<SortOrder> visit(LogicalTypeAnnotation.IntLogicalTypeAnnotation intLogicalType)
{    return intLogicalType.isSigned() ? of(SortOrder.SIGNED) : of(SortOrder.UNSIGNED);}
0
public Optional<SortOrder> visit(LogicalTypeAnnotation.IntervalLogicalTypeAnnotation intervalLogicalType)
{    return of(SortOrder.UNKNOWN);}
0
public Optional<SortOrder> visit(LogicalTypeAnnotation.DateLogicalTypeAnnotation dateLogicalType)
{    return of(SortOrder.SIGNED);}
0
public Optional<SortOrder> visit(LogicalTypeAnnotation.EnumLogicalTypeAnnotation enumLogicalType)
{    return of(SortOrder.UNSIGNED);}
0
public Optional<SortOrder> visit(LogicalTypeAnnotation.BsonLogicalTypeAnnotation bsonLogicalType)
{    return of(SortOrder.UNSIGNED);}
0
public Optional<SortOrder> visit(LogicalTypeAnnotation.JsonLogicalTypeAnnotation jsonLogicalType)
{    return of(SortOrder.UNSIGNED);}
0
public Optional<SortOrder> visit(LogicalTypeAnnotation.StringLogicalTypeAnnotation stringLogicalType)
{    return of(SortOrder.UNSIGNED);}
0
public Optional<SortOrder> visit(LogicalTypeAnnotation.DecimalLogicalTypeAnnotation decimalLogicalType)
{    return of(SortOrder.UNKNOWN);}
0
public Optional<SortOrder> visit(LogicalTypeAnnotation.MapKeyValueTypeAnnotation mapKeyValueLogicalType)
{    return of(SortOrder.UNKNOWN);}
0
public Optional<SortOrder> visit(LogicalTypeAnnotation.MapLogicalTypeAnnotation mapLogicalType)
{    return of(SortOrder.UNKNOWN);}
0
public Optional<SortOrder> visit(LogicalTypeAnnotation.ListLogicalTypeAnnotation listLogicalType)
{    return of(SortOrder.UNKNOWN);}
0
public Optional<SortOrder> visit(LogicalTypeAnnotation.TimeLogicalTypeAnnotation timeLogicalType)
{    return of(SortOrder.SIGNED);}
0
public Optional<SortOrder> visit(LogicalTypeAnnotation.TimestampLogicalTypeAnnotation timestampLogicalType)
{    return of(SortOrder.SIGNED);}
0
public PrimitiveTypeName getPrimitive(Type type)
{    switch(type) {        case         BYTE_ARRAY:            return PrimitiveTypeName.BINARY;        case INT64:            return PrimitiveTypeName.INT64;        case INT32:            return PrimitiveTypeName.INT32;        case BOOLEAN:            return PrimitiveTypeName.BOOLEAN;        case FLOAT:            return PrimitiveTypeName.FLOAT;        case DOUBLE:            return PrimitiveTypeName.DOUBLE;        case INT96:            return PrimitiveTypeName.INT96;        case FIXED_LEN_BYTE_ARRAY:            return PrimitiveTypeName.FIXED_LEN_BYTE_ARRAY;        default:            throw new RuntimeException("Unknown type " + type);    }}
0
 Type getType(PrimitiveTypeName type)
{    switch(type) {        case INT64:            return Type.INT64;        case INT32:            return Type.INT32;        case BOOLEAN:            return Type.BOOLEAN;        case BINARY:            return Type.BYTE_ARRAY;        case FLOAT:            return Type.FLOAT;        case DOUBLE:            return Type.DOUBLE;        case INT96:            return Type.INT96;        case FIXED_LEN_BYTE_ARRAY:            return Type.FIXED_LEN_BYTE_ARRAY;        default:            throw new RuntimeException("Unknown primitive type " + type);    }}
0
 LogicalTypeAnnotation getLogicalTypeAnnotation(ConvertedType type, SchemaElement schemaElement)
{    switch(type) {        case UTF8:            return LogicalTypeAnnotation.stringType();        case MAP:            return LogicalTypeAnnotation.mapType();        case MAP_KEY_VALUE:            return LogicalTypeAnnotation.MapKeyValueTypeAnnotation.getInstance();        case LIST:            return LogicalTypeAnnotation.listType();        case ENUM:            return LogicalTypeAnnotation.enumType();        case DECIMAL:            int scale = (schemaElement == null ? 0 : schemaElement.scale);            int precision = (schemaElement == null ? 0 : schemaElement.precision);            return LogicalTypeAnnotation.decimalType(scale, precision);        case DATE:            return LogicalTypeAnnotation.dateType();        case TIME_MILLIS:            return LogicalTypeAnnotation.timeType(true, LogicalTypeAnnotation.TimeUnit.MILLIS);        case TIME_MICROS:            return LogicalTypeAnnotation.timeType(true, LogicalTypeAnnotation.TimeUnit.MICROS);        case TIMESTAMP_MILLIS:            return LogicalTypeAnnotation.timestampType(true, LogicalTypeAnnotation.TimeUnit.MILLIS);        case TIMESTAMP_MICROS:            return LogicalTypeAnnotation.timestampType(true, LogicalTypeAnnotation.TimeUnit.MICROS);        case INTERVAL:            return LogicalTypeAnnotation.IntervalLogicalTypeAnnotation.getInstance();        case INT_8:            return LogicalTypeAnnotation.intType(8, true);        case INT_16:            return LogicalTypeAnnotation.intType(16, true);        case INT_32:            return LogicalTypeAnnotation.intType(32, true);        case INT_64:            return LogicalTypeAnnotation.intType(64, true);        case UINT_8:            return LogicalTypeAnnotation.intType(8, false);        case UINT_16:            return LogicalTypeAnnotation.intType(16, false);        case UINT_32:            return LogicalTypeAnnotation.intType(32, false);        case UINT_64:            return LogicalTypeAnnotation.intType(64, false);        case JSON:            return LogicalTypeAnnotation.jsonType();        case BSON:            return LogicalTypeAnnotation.bsonType();        default:            throw new RuntimeException("Can't convert converted type to logical type, unknown converted type " + type);    }}
0
 LogicalTypeAnnotation getLogicalTypeAnnotation(LogicalType type)
{    switch(type.getSetField()) {        case MAP:            return LogicalTypeAnnotation.mapType();        case BSON:            return LogicalTypeAnnotation.bsonType();        case DATE:            return LogicalTypeAnnotation.dateType();        case ENUM:            return LogicalTypeAnnotation.enumType();        case JSON:            return LogicalTypeAnnotation.jsonType();        case LIST:            return LogicalTypeAnnotation.listType();        case TIME:            TimeType time = type.getTIME();            return LogicalTypeAnnotation.timeType(time.isAdjustedToUTC, convertTimeUnit(time.unit));        case STRING:            return LogicalTypeAnnotation.stringType();        case DECIMAL:            DecimalType decimal = type.getDECIMAL();            return LogicalTypeAnnotation.decimalType(decimal.scale, decimal.precision);        case INTEGER:            IntType integer = type.getINTEGER();            return LogicalTypeAnnotation.intType(integer.bitWidth, integer.isSigned);        case UNKNOWN:            return null;        case TIMESTAMP:            TimestampType timestamp = type.getTIMESTAMP();            return LogicalTypeAnnotation.timestampType(timestamp.isAdjustedToUTC, convertTimeUnit(timestamp.unit));        default:            throw new RuntimeException("Unknown logical type " + type);    }}
0
private LogicalTypeAnnotation.TimeUnit convertTimeUnit(TimeUnit unit)
{    switch(unit.getSetField()) {        case MICROS:            return LogicalTypeAnnotation.TimeUnit.MICROS;        case MILLIS:            return LogicalTypeAnnotation.TimeUnit.MILLIS;        case NANOS:            return LogicalTypeAnnotation.TimeUnit.NANOS;        default:            throw new RuntimeException("Unknown time unit " + unit);    }}
0
private static void addKeyValue(FileMetaData fileMetaData, String key, String value)
{    KeyValue keyValue = new KeyValue(key);    keyValue.value = value;    fileMetaData.addToKey_value_metadata(keyValue);}
0
public static MetadataFilter range(long startOffset, long endOffset)
{    return new RangeMetadataFilter(startOffset, endOffset);}
0
public static MetadataFilter offsets(long... offsets)
{    Set<Long> set = new HashSet<Long>();    for (long offset : offsets) {        set.add(offset);    }    return new OffsetMetadataFilter(set);}
0
 T accept(MetadataFilterVisitor<T, E> visitor) throws E
{    return visitor.visit(this);}
0
public String toString()
{    return "NO_FILTER";}
0
 T accept(MetadataFilterVisitor<T, E> visitor) throws E
{    return visitor.visit(this);}
0
public String toString()
{    return "SKIP_ROW_GROUPS";}
0
 T accept(MetadataFilterVisitor<T, E> visitor) throws E
{    return visitor.visit(this);}
0
public boolean contains(long offset)
{    return offset >= this.startOffset && offset < this.endOffset;}
0
public String toString()
{    return "range(s:" + startOffset + ", e:" + endOffset + ")";}
0
public boolean contains(long offset)
{    return offsets.contains(offset);}
0
 T accept(MetadataFilterVisitor<T, E> visitor) throws E
{    return visitor.visit(this);}
0
public ParquetMetadata readParquetMetadata(InputStream from) throws IOException
{    return readParquetMetadata(from, NO_FILTER);}
0
 static FileMetaData filterFileMetaDataByMidpoint(FileMetaData metaData, RangeMetadataFilter filter)
{    List<RowGroup> rowGroups = metaData.getRow_groups();    List<RowGroup> newRowGroups = new ArrayList<RowGroup>();    for (RowGroup rowGroup : rowGroups) {        long totalSize = 0;        long startIndex = getOffset(rowGroup.getColumns().get(0));        for (ColumnChunk col : rowGroup.getColumns()) {            totalSize += col.getMeta_data().getTotal_compressed_size();        }        long midPoint = startIndex + totalSize / 2;        if (filter.contains(midPoint)) {            newRowGroups.add(rowGroup);        }    }    metaData.setRow_groups(newRowGroups);    return metaData;}
0
 static FileMetaData filterFileMetaDataByStart(FileMetaData metaData, OffsetMetadataFilter filter)
{    List<RowGroup> rowGroups = metaData.getRow_groups();    List<RowGroup> newRowGroups = new ArrayList<RowGroup>();    for (RowGroup rowGroup : rowGroups) {        long startIndex = getOffset(rowGroup.getColumns().get(0));        if (filter.contains(startIndex)) {            newRowGroups.add(rowGroup);        }    }    metaData.setRow_groups(newRowGroups);    return metaData;}
0
 static long getOffset(RowGroup rowGroup)
{    return getOffset(rowGroup.getColumns().get(0));}
0
 static long getOffset(ColumnChunk columnChunk)
{    ColumnMetaData md = columnChunk.getMeta_data();    long offset = md.getData_page_offset();    if (md.isSetDictionary_page_offset() && offset > md.getDictionary_page_offset()) {        offset = md.getDictionary_page_offset();    }    return offset;}
0
public ParquetMetadata readParquetMetadata(final InputStream from, MetadataFilter filter) throws IOException
{    FileMetaData fileMetaData = filter.accept(new MetadataFilterVisitor<FileMetaData, IOException>() {        @Override        public FileMetaData visit(NoFilter filter) throws IOException {            return readFileMetaData(from);        }        @Override        public FileMetaData visit(SkipMetadataFilter filter) throws IOException {            return readFileMetaData(from, true);        }        @Override        public FileMetaData visit(OffsetMetadataFilter filter) throws IOException {            return filterFileMetaDataByStart(readFileMetaData(from), filter);        }        @Override        public FileMetaData visit(RangeMetadataFilter filter) throws IOException {            return filterFileMetaDataByMidpoint(readFileMetaData(from), filter);        }    });        ParquetMetadata parquetMetadata = fromParquetMetadata(fileMetaData);    if (LOG.isDebugEnabled())            return parquetMetadata;}
1
public FileMetaData visit(NoFilter filter) throws IOException
{    return readFileMetaData(from);}
0
public FileMetaData visit(SkipMetadataFilter filter) throws IOException
{    return readFileMetaData(from, true);}
0
public FileMetaData visit(OffsetMetadataFilter filter) throws IOException
{    return filterFileMetaDataByStart(readFileMetaData(from), filter);}
0
public FileMetaData visit(RangeMetadataFilter filter) throws IOException
{    return filterFileMetaDataByMidpoint(readFileMetaData(from), filter);}
0
public ParquetMetadata fromParquetMetadata(FileMetaData parquetMetadata) throws IOException
{    MessageType messageType = fromParquetSchema(parquetMetadata.getSchema(), parquetMetadata.getColumn_orders());    List<BlockMetaData> blocks = new ArrayList<BlockMetaData>();    List<RowGroup> row_groups = parquetMetadata.getRow_groups();    if (row_groups != null) {        for (RowGroup rowGroup : row_groups) {            BlockMetaData blockMetaData = new BlockMetaData();            blockMetaData.setRowCount(rowGroup.getNum_rows());            blockMetaData.setTotalByteSize(rowGroup.getTotal_byte_size());            List<ColumnChunk> columns = rowGroup.getColumns();            String filePath = columns.get(0).getFile_path();            for (ColumnChunk columnChunk : columns) {                if ((filePath == null && columnChunk.getFile_path() != null) || (filePath != null && !filePath.equals(columnChunk.getFile_path()))) {                    throw new ParquetDecodingException("all column chunks of the same row group must be in the same file for now");                }                ColumnMetaData metaData = columnChunk.meta_data;                ColumnPath path = getPath(metaData);                ColumnChunkMetaData column = ColumnChunkMetaData.get(path, messageType.getType(path.toArray()).asPrimitiveType(), fromFormatCodec(metaData.codec), convertEncodingStats(metaData.getEncoding_stats()), fromFormatEncodings(metaData.encodings), fromParquetStatistics(parquetMetadata.getCreated_by(), metaData.statistics, messageType.getType(path.toArray()).asPrimitiveType()), metaData.data_page_offset, metaData.dictionary_page_offset, metaData.num_values, metaData.total_compressed_size, metaData.total_uncompressed_size);                column.setColumnIndexReference(toColumnIndexReference(columnChunk));                column.setOffsetIndexReference(toOffsetIndexReference(columnChunk));                                                                blockMetaData.addColumn(column);            }            blockMetaData.setPath(filePath);            blocks.add(blockMetaData);        }    }    Map<String, String> keyValueMetaData = new HashMap<String, String>();    List<KeyValue> key_value_metadata = parquetMetadata.getKey_value_metadata();    if (key_value_metadata != null) {        for (KeyValue keyValue : key_value_metadata) {            keyValueMetaData.put(keyValue.key, keyValue.value);        }    }    return new ParquetMetadata(new org.apache.parquet.hadoop.metadata.FileMetaData(messageType, keyValueMetaData, parquetMetadata.getCreated_by()), blocks);}
0
private static IndexReference toColumnIndexReference(ColumnChunk columnChunk)
{    if (columnChunk.isSetColumn_index_offset() && columnChunk.isSetColumn_index_length()) {        return new IndexReference(columnChunk.getColumn_index_offset(), columnChunk.getColumn_index_length());    }    return null;}
0
private static IndexReference toOffsetIndexReference(ColumnChunk columnChunk)
{    if (columnChunk.isSetOffset_index_offset() && columnChunk.isSetOffset_index_length()) {        return new IndexReference(columnChunk.getOffset_index_offset(), columnChunk.getOffset_index_length());    }    return null;}
0
private static ColumnPath getPath(ColumnMetaData metaData)
{    String[] path = metaData.path_in_schema.toArray(new String[metaData.path_in_schema.size()]);    return ColumnPath.get(path);}
0
 MessageType fromParquetSchema(List<SchemaElement> schema, List<ColumnOrder> columnOrders)
{    Iterator<SchemaElement> iterator = schema.iterator();    SchemaElement root = iterator.next();    Types.MessageTypeBuilder builder = Types.buildMessage();    if (root.isSetField_id()) {        builder.id(root.field_id);    }    buildChildren(builder, iterator, root.getNum_children(), columnOrders, 0);    return builder.named(root.name);}
0
private void buildChildren(Types.GroupBuilder builder, Iterator<SchemaElement> schema, int childrenCount, List<ColumnOrder> columnOrders, int columnCount)
{    for (int i = 0; i < childrenCount; i++) {        SchemaElement schemaElement = schema.next();                Types.Builder childBuilder;        if (schemaElement.type != null) {            Types.PrimitiveBuilder primitiveBuilder = builder.primitive(getPrimitive(schemaElement.type), fromParquetRepetition(schemaElement.repetition_type));            if (schemaElement.isSetType_length()) {                primitiveBuilder.length(schemaElement.type_length);            }            if (schemaElement.isSetPrecision()) {                primitiveBuilder.precision(schemaElement.precision);            }            if (schemaElement.isSetScale()) {                primitiveBuilder.scale(schemaElement.scale);            }            if (columnOrders != null) {                org.apache.parquet.schema.ColumnOrder columnOrder = fromParquetColumnOrder(columnOrders.get(columnCount));                                if (columnOrder.getColumnOrderName() == ColumnOrderName.TYPE_DEFINED_ORDER && (schemaElement.type == Type.INT96 || schemaElement.converted_type == ConvertedType.INTERVAL)) {                    columnOrder = org.apache.parquet.schema.ColumnOrder.undefined();                }                primitiveBuilder.columnOrder(columnOrder);            }            childBuilder = primitiveBuilder;        } else {            childBuilder = builder.group(fromParquetRepetition(schemaElement.repetition_type));            buildChildren((Types.GroupBuilder) childBuilder, schema, schemaElement.num_children, columnOrders, columnCount);        }        if (schemaElement.isSetLogicalType()) {            childBuilder.as(getLogicalTypeAnnotation(schemaElement.logicalType));        }        if (schemaElement.isSetConverted_type()) {            OriginalType originalType = getLogicalTypeAnnotation(schemaElement.converted_type, schemaElement).toOriginalType();            OriginalType newOriginalType = (schemaElement.isSetLogicalType() && getLogicalTypeAnnotation(schemaElement.logicalType) != null) ? getLogicalTypeAnnotation(schemaElement.logicalType).toOriginalType() : null;            if (!originalType.equals(newOriginalType)) {                if (newOriginalType != null) {                                    }                childBuilder.as(originalType);            }        }        if (schemaElement.isSetField_id()) {            childBuilder.id(schemaElement.field_id);        }        childBuilder.named(schemaElement.name);        ++columnCount;    }}
1
 FieldRepetitionType toParquetRepetition(Repetition repetition)
{    return FieldRepetitionType.valueOf(repetition.name());}
0
 Repetition fromParquetRepetition(FieldRepetitionType repetition)
{    return Repetition.valueOf(repetition.name());}
0
private static org.apache.parquet.schema.ColumnOrder fromParquetColumnOrder(ColumnOrder columnOrder)
{    if (columnOrder.isSetTYPE_ORDER()) {        return org.apache.parquet.schema.ColumnOrder.typeDefined();    }        return org.apache.parquet.schema.ColumnOrder.undefined();}
0
public void writeDataPageHeader(int uncompressedSize, int compressedSize, int valueCount, org.apache.parquet.column.Encoding rlEncoding, org.apache.parquet.column.Encoding dlEncoding, org.apache.parquet.column.Encoding valuesEncoding, OutputStream to) throws IOException
{    writePageHeader(newDataPageHeader(uncompressedSize, compressedSize, valueCount, rlEncoding, dlEncoding, valuesEncoding), to);}
0
public void writeDataPageHeader(int uncompressedSize, int compressedSize, int valueCount, org.apache.parquet.column.statistics.Statistics statistics, org.apache.parquet.column.Encoding rlEncoding, org.apache.parquet.column.Encoding dlEncoding, org.apache.parquet.column.Encoding valuesEncoding, OutputStream to) throws IOException
{    writePageHeader(newDataPageHeader(uncompressedSize, compressedSize, valueCount, rlEncoding, dlEncoding, valuesEncoding), to);}
0
private PageHeader newDataPageHeader(int uncompressedSize, int compressedSize, int valueCount, org.apache.parquet.column.Encoding rlEncoding, org.apache.parquet.column.Encoding dlEncoding, org.apache.parquet.column.Encoding valuesEncoding)
{    PageHeader pageHeader = new PageHeader(PageType.DATA_PAGE, uncompressedSize, compressedSize);    pageHeader.setData_page_header(new DataPageHeader(valueCount, getEncoding(valuesEncoding), getEncoding(dlEncoding), getEncoding(rlEncoding)));    return pageHeader;}
0
private PageHeader newDataPageHeader(int uncompressedSize, int compressedSize, int valueCount, org.apache.parquet.column.Encoding rlEncoding, org.apache.parquet.column.Encoding dlEncoding, org.apache.parquet.column.Encoding valuesEncoding, int crc)
{    PageHeader pageHeader = new PageHeader(PageType.DATA_PAGE, uncompressedSize, compressedSize);    pageHeader.setCrc(crc);    pageHeader.setData_page_header(new DataPageHeader(valueCount, getEncoding(valuesEncoding), getEncoding(dlEncoding), getEncoding(rlEncoding)));    return pageHeader;}
0
public void writeDataPageV2Header(int uncompressedSize, int compressedSize, int valueCount, int nullCount, int rowCount, org.apache.parquet.column.statistics.Statistics statistics, org.apache.parquet.column.Encoding dataEncoding, int rlByteLength, int dlByteLength, OutputStream to) throws IOException
{    writePageHeader(newDataPageV2Header(uncompressedSize, compressedSize, valueCount, nullCount, rowCount, dataEncoding, rlByteLength, dlByteLength), to);}
0
public void writeDataPageV1Header(int uncompressedSize, int compressedSize, int valueCount, org.apache.parquet.column.Encoding rlEncoding, org.apache.parquet.column.Encoding dlEncoding, org.apache.parquet.column.Encoding valuesEncoding, OutputStream to) throws IOException
{    writePageHeader(newDataPageHeader(uncompressedSize, compressedSize, valueCount, rlEncoding, dlEncoding, valuesEncoding), to);}
0
public void writeDataPageV1Header(int uncompressedSize, int compressedSize, int valueCount, org.apache.parquet.column.Encoding rlEncoding, org.apache.parquet.column.Encoding dlEncoding, org.apache.parquet.column.Encoding valuesEncoding, int crc, OutputStream to) throws IOException
{    writePageHeader(newDataPageHeader(uncompressedSize, compressedSize, valueCount, rlEncoding, dlEncoding, valuesEncoding, crc), to);}
0
public void writeDataPageV2Header(int uncompressedSize, int compressedSize, int valueCount, int nullCount, int rowCount, org.apache.parquet.column.Encoding dataEncoding, int rlByteLength, int dlByteLength, OutputStream to) throws IOException
{    writePageHeader(newDataPageV2Header(uncompressedSize, compressedSize, valueCount, nullCount, rowCount, dataEncoding, rlByteLength, dlByteLength), to);}
0
private PageHeader newDataPageV2Header(int uncompressedSize, int compressedSize, int valueCount, int nullCount, int rowCount, org.apache.parquet.column.Encoding dataEncoding, int rlByteLength, int dlByteLength)
{        DataPageHeaderV2 dataPageHeaderV2 = new DataPageHeaderV2(valueCount, nullCount, rowCount, getEncoding(dataEncoding), dlByteLength, rlByteLength);    PageHeader pageHeader = new PageHeader(PageType.DATA_PAGE_V2, uncompressedSize, compressedSize);    pageHeader.setData_page_header_v2(dataPageHeaderV2);    return pageHeader;}
0
public void writeDictionaryPageHeader(int uncompressedSize, int compressedSize, int valueCount, org.apache.parquet.column.Encoding valuesEncoding, OutputStream to) throws IOException
{    PageHeader pageHeader = new PageHeader(PageType.DICTIONARY_PAGE, uncompressedSize, compressedSize);    pageHeader.setDictionary_page_header(new DictionaryPageHeader(valueCount, getEncoding(valuesEncoding)));    writePageHeader(pageHeader, to);}
0
public void writeDictionaryPageHeader(int uncompressedSize, int compressedSize, int valueCount, org.apache.parquet.column.Encoding valuesEncoding, int crc, OutputStream to) throws IOException
{    PageHeader pageHeader = new PageHeader(PageType.DICTIONARY_PAGE, uncompressedSize, compressedSize);    pageHeader.setCrc(crc);    pageHeader.setDictionary_page_header(new DictionaryPageHeader(valueCount, getEncoding(valuesEncoding)));    writePageHeader(pageHeader, to);}
0
private static BoundaryOrder toParquetBoundaryOrder(org.apache.parquet.internal.column.columnindex.BoundaryOrder boundaryOrder)
{    switch(boundaryOrder) {        case ASCENDING:            return BoundaryOrder.ASCENDING;        case DESCENDING:            return BoundaryOrder.DESCENDING;        case UNORDERED:            return BoundaryOrder.UNORDERED;        default:            throw new IllegalArgumentException("Unsupported boundary order: " + boundaryOrder);    }}
0
private static org.apache.parquet.internal.column.columnindex.BoundaryOrder fromParquetBoundaryOrder(BoundaryOrder boundaryOrder)
{    switch(boundaryOrder) {        case ASCENDING:            return org.apache.parquet.internal.column.columnindex.BoundaryOrder.ASCENDING;        case DESCENDING:            return org.apache.parquet.internal.column.columnindex.BoundaryOrder.DESCENDING;        case UNORDERED:            return org.apache.parquet.internal.column.columnindex.BoundaryOrder.UNORDERED;        default:            throw new IllegalArgumentException("Unsupported boundary order: " + boundaryOrder);    }}
0
public static ColumnIndex toParquetColumnIndex(PrimitiveType type, org.apache.parquet.internal.column.columnindex.ColumnIndex columnIndex)
{    if (!isMinMaxStatsSupported(type) || columnIndex == null) {        return null;    }    ColumnIndex parquetColumnIndex = new ColumnIndex(columnIndex.getNullPages(), columnIndex.getMinValues(), columnIndex.getMaxValues(), toParquetBoundaryOrder(columnIndex.getBoundaryOrder()));    parquetColumnIndex.setNull_counts(columnIndex.getNullCounts());    return parquetColumnIndex;}
0
public static org.apache.parquet.internal.column.columnindex.ColumnIndex fromParquetColumnIndex(PrimitiveType type, ColumnIndex parquetColumnIndex)
{    if (!isMinMaxStatsSupported(type)) {        return null;    }    return ColumnIndexBuilder.build(type, fromParquetBoundaryOrder(parquetColumnIndex.getBoundary_order()), parquetColumnIndex.getNull_pages(), parquetColumnIndex.getNull_counts(), parquetColumnIndex.getMin_values(), parquetColumnIndex.getMax_values());}
0
public static OffsetIndex toParquetOffsetIndex(org.apache.parquet.internal.column.columnindex.OffsetIndex offsetIndex)
{    List<PageLocation> pageLocations = new ArrayList<>(offsetIndex.getPageCount());    for (int i = 0, n = offsetIndex.getPageCount(); i < n; ++i) {        pageLocations.add(new PageLocation(offsetIndex.getOffset(i), offsetIndex.getCompressedPageSize(i), offsetIndex.getFirstRowIndex(i)));    }    return new OffsetIndex(pageLocations);}
0
public static org.apache.parquet.internal.column.columnindex.OffsetIndex fromParquetOffsetIndex(OffsetIndex parquetOffsetIndex)
{    OffsetIndexBuilder builder = OffsetIndexBuilder.getBuilder();    for (PageLocation pageLocation : parquetOffsetIndex.getPage_locations()) {        builder.add(pageLocation.getOffset(), pageLocation.getCompressed_page_size(), pageLocation.getFirst_row_index());    }    return builder.build();}
0
public ReadSupport.ReadContext init(InitContext context)
{    return delegate.init(context);}
0
public RecordMaterializer<T> prepareForRead(Configuration configuration, Map<String, String> keyValueMetaData, MessageType fileSchema, ReadSupport.ReadContext readContext)
{    return delegate.prepareForRead(configuration, keyValueMetaData, fileSchema, readContext);}
0
public String toString()
{    return this.getClass().getName() + "(" + delegate.toString() + ")";}
0
public WriteSupport.WriteContext init(Configuration configuration)
{    return delegate.init(configuration);}
0
public void prepareForWrite(RecordConsumer recordConsumer)
{    delegate.prepareForWrite(recordConsumer);}
0
public void write(T record)
{    delegate.write(record);}
0
public String getName()
{    return delegate.getName();}
0
public WriteSupport.FinalizedWriteContext finalizeWrite()
{    return delegate.finalizeWrite();}
0
public String toString()
{    return getClass().getName() + "(" + delegate.toString() + ")";}
0
public Map<String, String> getMergedKeyValueMetaData()
{    if (mergedKeyValueMetadata == null) {        Map<String, String> mergedKeyValues = new HashMap<String, String>();        for (Entry<String, Set<String>> entry : keyValueMetadata.entrySet()) {            if (entry.getValue().size() > 1) {                throw new RuntimeException("could not merge metadata: key " + entry.getKey() + " has conflicting values: " + entry.getValue());            }            mergedKeyValues.put(entry.getKey(), entry.getValue().iterator().next());        }        mergedKeyValueMetadata = mergedKeyValues;    }    return mergedKeyValueMetadata;}
0
public Configuration getConfiguration()
{    return configuration;}
0
public MessageType getFileSchema()
{    return fileSchema;}
0
public Map<String, Set<String>> getKeyValueMetadata()
{    return keyValueMetadata;}
0
public static MessageType getSchemaForRead(MessageType fileMessageType, String partialReadSchemaString)
{    if (partialReadSchemaString == null)        return fileMessageType;    MessageType requestedMessageType = MessageTypeParser.parseMessageType(partialReadSchemaString);    return getSchemaForRead(fileMessageType, requestedMessageType);}
0
public static MessageType getSchemaForRead(MessageType fileMessageType, MessageType projectedMessageType)
{    fileMessageType.checkContains(projectedMessageType);    return projectedMessageType;}
0
public ReadContext init(Configuration configuration, Map<String, String> keyValueMetaData, MessageType fileSchema)
{    throw new UnsupportedOperationException("Override init(InitContext)");}
0
public ReadContext init(InitContext context)
{    return init(context.getConfiguration(), context.getMergedKeyValueMetaData(), context.getFileSchema());}
0
public MessageType getRequestedSchema()
{    return requestedSchema;}
0
public Map<String, String> getReadSupportMetadata()
{    return readSupportMetadata;}
0
public MessageType getSchema()
{    return schema;}
0
public Map<String, String> getExtraMetaData()
{    return extraMetaData;}
0
public Map<String, String> getExtraMetaData()
{    return extraMetaData;}
0
public String getName()
{    return null;}
0
public FinalizedWriteContext finalizeWrite()
{    return new FinalizedWriteContext(new HashMap<String, String>());}
0
public static void cleanDirectBuffer(ByteBuffer buf)
{    if (cleanMethod != null) {        try {            cleanMethod.invoke(cleanerMethod.invoke(buf));        } catch (IllegalAccessException | IllegalArgumentException | InvocationTargetException | SecurityException e) {                    }    } else if (invokeCleanerMethod != null) {        try {            invokeCleanerMethod.invoke(unsafe, buf);        } catch (IllegalAccessException | IllegalArgumentException | InvocationTargetException | SecurityException e) {                    }    }}
1
public static CodecConfig from(JobConf jobConf)
{    return new MapredCodecConfig(jobConf);}
0
public static CodecConfig from(TaskAttemptContext context)
{    return new MapreduceCodecConfig(context);}
0
public static boolean isParquetCompressionSet(Configuration conf)
{    return conf.get(ParquetOutputFormat.COMPRESSION) != null;}
0
public static CompressionCodecName getParquetCompressionCodec(Configuration configuration)
{    return CompressionCodecName.fromConf(configuration.get(ParquetOutputFormat.COMPRESSION, UNCOMPRESSED.name()));}
0
public CompressionCodecName getCodec()
{    CompressionCodecName codec;    Configuration configuration = getConfiguration();    if (isParquetCompressionSet(configuration)) {                codec = getParquetCompressionCodec(configuration);    } else if (isHadoopCompressionSet()) {                codec = getHadoopCompressionCodec();    } else {                codec = CompressionCodecName.UNCOMPRESSED;    }        return codec;}
1
private CompressionCodecName getHadoopCompressionCodec()
{    CompressionCodecName codec;    try {                Class<?> codecClass = getHadoopOutputCompressorClass(CompressionCodecName.UNCOMPRESSED.getHadoopCompressionCodecClass());                codec = CompressionCodecName.fromCompressionCodec(codecClass);    } catch (CompressionCodecNotSupportedException e) {                codec = CompressionCodecName.UNCOMPRESSED;    } catch (IllegalArgumentException e) {                codec = CompressionCodecName.UNCOMPRESSED;    }    return codec;}
1
public boolean isHadoopCompressionSet()
{    return FileOutputFormat.getCompressOutput(context);}
0
public Class getHadoopOutputCompressorClass(Class defaultCodec)
{    return FileOutputFormat.getOutputCompressorClass(context, defaultCodec);}
0
public Configuration getConfiguration()
{    return ContextUtil.getConfiguration(context);}
0
public boolean isHadoopCompressionSet()
{    return org.apache.hadoop.mapred.FileOutputFormat.getCompressOutput(conf);}
0
public Class getHadoopOutputCompressorClass(Class defaultCodec)
{    return org.apache.hadoop.mapred.FileOutputFormat.getOutputCompressorClass(conf, defaultCodec);}
0
public Configuration getConfiguration()
{    return conf;}
0
public void write(byte[] b, int off, int len) throws IOException
{        if (compressor.finished()) {        throw new IOException("write beyond end of stream");    }    if ((off | len | (off + len) | (b.length - (off + len))) < 0) {        throw new IndexOutOfBoundsException();    } else if (len == 0) {        return;    }    compressor.setInput(b, off, len);}
0
public int read(byte[] b, int off, int len) throws IOException
{    if (!inputHandled) {                while (true) {            int compressedBytes = getCompressedData();            if (compressedBytes == -1)                break;            decompressor.setInput(buffer, 0, compressedBytes);        }        inputHandled = true;    }    int decompressedBytes = decompressor.decompress(b, off, len);    if (decompressor.finished()) {        decompressor.reset();    }    return decompressedBytes;}
0
public void setConf(Configuration conf)
{    this.conf = conf;}
0
public Configuration getConf()
{    return conf;}
0
public Compressor createCompressor()
{    return new SnappyCompressor();}
0
public Decompressor createDecompressor()
{    return new SnappyDecompressor();}
0
public CompressionInputStream createInputStream(InputStream stream) throws IOException
{    return createInputStream(stream, createDecompressor());}
0
public CompressionInputStream createInputStream(InputStream stream, Decompressor decompressor) throws IOException
{    return new NonBlockedDecompressorStream(stream, decompressor, conf.getInt(BUFFER_SIZE_CONFIG, 4 * 1024));}
0
public CompressionOutputStream createOutputStream(OutputStream stream) throws IOException
{    return createOutputStream(stream, createCompressor());}
0
public CompressionOutputStream createOutputStream(OutputStream stream, Compressor compressor) throws IOException
{    return new NonBlockedCompressorStream(stream, compressor, conf.getInt(BUFFER_SIZE_CONFIG, 4 * 1024));}
0
public Class<? extends Compressor> getCompressorType()
{    return SnappyCompressor.class;}
0
public Class<? extends Decompressor> getDecompressorType()
{    return SnappyDecompressor.class;}
0
public String getDefaultExtension()
{    return ".snappy";}
0
public synchronized int compress(byte[] buffer, int off, int len) throws IOException
{    SnappyUtil.validateBuffer(buffer, off, len);    if (needsInput()) {                return 0;    }    if (!outputBuffer.hasRemaining()) {                int maxOutputSize = Snappy.maxCompressedLength(inputBuffer.position());        if (maxOutputSize > outputBuffer.capacity()) {            ByteBuffer oldBuffer = outputBuffer;            outputBuffer = ByteBuffer.allocateDirect(maxOutputSize);            CleanUtil.cleanDirectBuffer(oldBuffer);        }                outputBuffer.clear();        inputBuffer.limit(inputBuffer.position());        inputBuffer.position(0);        int size = Snappy.compress(inputBuffer, outputBuffer);        outputBuffer.limit(size);        inputBuffer.limit(0);        inputBuffer.rewind();    }        int numBytes = Math.min(len, outputBuffer.remaining());    outputBuffer.get(buffer, off, numBytes);    bytesWritten += numBytes;    return numBytes;}
0
public synchronized void setInput(byte[] buffer, int off, int len)
{    SnappyUtil.validateBuffer(buffer, off, len);    Preconditions.checkArgument(!outputBuffer.hasRemaining(), "Output buffer should be empty. Caller must call compress()");    if (inputBuffer.capacity() - inputBuffer.position() < len) {        ByteBuffer tmp = ByteBuffer.allocateDirect(inputBuffer.position() + len);        inputBuffer.rewind();        tmp.put(inputBuffer);        ByteBuffer oldBuffer = inputBuffer;        inputBuffer = tmp;        CleanUtil.cleanDirectBuffer(oldBuffer);    } else {        inputBuffer.limit(inputBuffer.position() + len);    }        inputBuffer.put(buffer, off, len);    bytesRead += len;}
0
public void end()
{    CleanUtil.cleanDirectBuffer(inputBuffer);    CleanUtil.cleanDirectBuffer(outputBuffer);}
0
public synchronized void finish()
{    finishCalled = true;}
0
public synchronized boolean finished()
{    return finishCalled && inputBuffer.position() == 0 && !outputBuffer.hasRemaining();}
0
public long getBytesRead()
{    return bytesRead;}
0
public long getBytesWritten()
{    return bytesWritten;}
0
public synchronized boolean needsInput()
{    return !finishCalled;}
0
public void reinit(Configuration c)
{    reset();}
0
public synchronized void reset()
{    finishCalled = false;    bytesRead = bytesWritten = 0;    inputBuffer.rewind();    outputBuffer.rewind();    inputBuffer.limit(0);    outputBuffer.limit(0);}
0
public void setDictionary(byte[] dictionary, int off, int len)
{}
0
public synchronized int decompress(byte[] buffer, int off, int len) throws IOException
{    SnappyUtil.validateBuffer(buffer, off, len);    if (inputBuffer.position() == 0 && !outputBuffer.hasRemaining()) {        return 0;    }    if (!outputBuffer.hasRemaining()) {        inputBuffer.rewind();        Preconditions.checkArgument(inputBuffer.position() == 0, "Invalid position of 0.");        Preconditions.checkArgument(outputBuffer.position() == 0, "Invalid position of 0.");                int decompressedSize = Snappy.uncompressedLength(inputBuffer);        if (decompressedSize > outputBuffer.capacity()) {            ByteBuffer oldBuffer = outputBuffer;            outputBuffer = ByteBuffer.allocateDirect(decompressedSize);            CleanUtil.cleanDirectBuffer(oldBuffer);        }                outputBuffer.clear();        int size = Snappy.uncompress(inputBuffer, outputBuffer);        outputBuffer.limit(size);                inputBuffer.clear();        inputBuffer.limit(0);        finished = true;    }        int numBytes = Math.min(len, outputBuffer.remaining());    outputBuffer.get(buffer, off, numBytes);    return numBytes;}
0
public synchronized void setInput(byte[] buffer, int off, int len)
{    SnappyUtil.validateBuffer(buffer, off, len);    if (inputBuffer.capacity() - inputBuffer.position() < len) {        final ByteBuffer newBuffer = ByteBuffer.allocateDirect(inputBuffer.position() + len);        inputBuffer.rewind();        newBuffer.put(inputBuffer);        final ByteBuffer oldBuffer = inputBuffer;        inputBuffer = newBuffer;        CleanUtil.cleanDirectBuffer(oldBuffer);    } else {        inputBuffer.limit(inputBuffer.position() + len);    }    inputBuffer.put(buffer, off, len);}
0
public void end()
{    CleanUtil.cleanDirectBuffer(inputBuffer);    CleanUtil.cleanDirectBuffer(outputBuffer);}
0
public synchronized boolean finished()
{    return finished && !outputBuffer.hasRemaining();}
0
public int getRemaining()
{    return 0;}
0
public synchronized boolean needsInput()
{    return !inputBuffer.hasRemaining() && !outputBuffer.hasRemaining();}
0
public synchronized void reset()
{    finished = false;    inputBuffer.rewind();    outputBuffer.rewind();    inputBuffer.limit(0);    outputBuffer.limit(0);}
0
public boolean needsDictionary()
{    return false;}
0
public void setDictionary(byte[] b, int off, int len)
{}
0
public static void validateBuffer(byte[] buffer, int off, int len)
{    Preconditions.checkNotNull(buffer, "buffer");    Preconditions.checkArgument(off >= 0 && len >= 0 && off <= buffer.length - len, "Invalid buffer offset or length: buffer.length=%s off=%s len=%s", buffer.length, off, len);}
0
public static CodecFactory createDirectCodecFactory(Configuration config, ByteBufferAllocator allocator, int pageSize)
{    return new DirectCodecFactory(config, allocator, pageSize);}
0
public BytesInput decompress(BytesInput bytes, int uncompressedSize) throws IOException
{    final BytesInput decompressed;    if (codec != null) {        decompressor.reset();        InputStream is = codec.createInputStream(bytes.toInputStream(), decompressor);        decompressed = BytesInput.from(is, uncompressedSize);    } else {        decompressed = bytes;    }    return decompressed;}
0
public void decompress(ByteBuffer input, int compressedSize, ByteBuffer output, int uncompressedSize) throws IOException
{    ByteBuffer decompressed = decompress(BytesInput.from(input), uncompressedSize).toByteBuffer();    output.put(decompressed);}
0
public void release()
{    if (decompressor != null) {        CodecPool.returnDecompressor(decompressor);    }}
0
public BytesInput compress(BytesInput bytes) throws IOException
{    final BytesInput compressedBytes;    if (codec == null) {        compressedBytes = bytes;    } else {        compressedOutBuffer.reset();        if (compressor != null) {                        compressor.reset();        }        CompressionOutputStream cos = codec.createOutputStream(compressedOutBuffer, compressor);        bytes.writeAllTo(cos);        cos.finish();        cos.close();        compressedBytes = BytesInput.from(compressedOutBuffer);    }    return compressedBytes;}
0
public void release()
{    if (compressor != null) {        CodecPool.returnCompressor(compressor);    }}
0
public CompressionCodecName getCodecName()
{    return codecName;}
0
public BytesCompressor getCompressor(CompressionCodecName codecName)
{    BytesCompressor comp = compressors.get(codecName);    if (comp == null) {        comp = createCompressor(codecName);        compressors.put(codecName, comp);    }    return comp;}
0
public BytesDecompressor getDecompressor(CompressionCodecName codecName)
{    BytesDecompressor decomp = decompressors.get(codecName);    if (decomp == null) {        decomp = createDecompressor(codecName);        decompressors.put(codecName, decomp);    }    return decomp;}
0
protected BytesCompressor createCompressor(CompressionCodecName codecName)
{    return new HeapBytesCompressor(codecName);}
0
protected BytesDecompressor createDecompressor(CompressionCodecName codecName)
{    return new HeapBytesDecompressor(codecName);}
0
protected CompressionCodec getCodec(CompressionCodecName codecName)
{    String codecClassName = codecName.getHadoopCompressionCodecClassName();    if (codecClassName == null) {        return null;    }    CompressionCodec codec = CODEC_BY_NAME.get(codecClassName);    if (codec != null) {        return codec;    }    try {        Class<?> codecClass = Class.forName(codecClassName);        codec = (CompressionCodec) ReflectionUtils.newInstance(codecClass, configuration);        CODEC_BY_NAME.put(codecClassName, codec);        return codec;    } catch (ClassNotFoundException e) {        throw new BadConfigurationException("Class " + codecClassName + " was not found", e);    }}
0
public void release()
{    for (BytesCompressor compressor : compressors.values()) {        compressor.release();    }    compressors.clear();    for (BytesDecompressor decompressor : decompressors.values()) {        decompressor.release();    }    decompressors.clear();}
0
public long getTotalValueCount()
{    return valueCount;}
0
public DataPage readPage()
{    if (compressedPages.isEmpty()) {        return null;    }    DataPage compressedPage = compressedPages.remove(0);    final int currentPageIndex = pageIndex++;    return compressedPage.accept(new DataPage.Visitor<DataPage>() {        @Override        public DataPage visit(DataPageV1 dataPageV1) {            try {                BytesInput decompressed = decompressor.decompress(dataPageV1.getBytes(), dataPageV1.getUncompressedSize());                final DataPageV1 decompressedPage;                if (offsetIndex == null) {                    decompressedPage = new DataPageV1(decompressed, dataPageV1.getValueCount(), dataPageV1.getUncompressedSize(), dataPageV1.getStatistics(), dataPageV1.getRlEncoding(), dataPageV1.getDlEncoding(), dataPageV1.getValueEncoding());                } else {                    long firstRowIndex = offsetIndex.getFirstRowIndex(currentPageIndex);                    decompressedPage = new DataPageV1(decompressed, dataPageV1.getValueCount(), dataPageV1.getUncompressedSize(), firstRowIndex, Math.toIntExact(offsetIndex.getLastRowIndex(currentPageIndex, rowCount) - firstRowIndex + 1), dataPageV1.getStatistics(), dataPageV1.getRlEncoding(), dataPageV1.getDlEncoding(), dataPageV1.getValueEncoding());                }                if (dataPageV1.getCrc().isPresent()) {                    decompressedPage.setCrc(dataPageV1.getCrc().getAsInt());                }                return decompressedPage;            } catch (IOException e) {                throw new ParquetDecodingException("could not decompress page", e);            }        }        @Override        public DataPage visit(DataPageV2 dataPageV2) {            if (!dataPageV2.isCompressed()) {                if (offsetIndex == null) {                    return dataPageV2;                } else {                    return DataPageV2.uncompressed(dataPageV2.getRowCount(), dataPageV2.getNullCount(), dataPageV2.getValueCount(), offsetIndex.getFirstRowIndex(currentPageIndex), dataPageV2.getRepetitionLevels(), dataPageV2.getDefinitionLevels(), dataPageV2.getDataEncoding(), dataPageV2.getData(), dataPageV2.getStatistics());                }            }            try {                int uncompressedSize = Math.toIntExact(dataPageV2.getUncompressedSize() - dataPageV2.getDefinitionLevels().size() - dataPageV2.getRepetitionLevels().size());                BytesInput decompressed = decompressor.decompress(dataPageV2.getData(), uncompressedSize);                if (offsetIndex == null) {                    return DataPageV2.uncompressed(dataPageV2.getRowCount(), dataPageV2.getNullCount(), dataPageV2.getValueCount(), dataPageV2.getRepetitionLevels(), dataPageV2.getDefinitionLevels(), dataPageV2.getDataEncoding(), decompressed, dataPageV2.getStatistics());                } else {                    return DataPageV2.uncompressed(dataPageV2.getRowCount(), dataPageV2.getNullCount(), dataPageV2.getValueCount(), offsetIndex.getFirstRowIndex(currentPageIndex), dataPageV2.getRepetitionLevels(), dataPageV2.getDefinitionLevels(), dataPageV2.getDataEncoding(), decompressed, dataPageV2.getStatistics());                }            } catch (IOException e) {                throw new ParquetDecodingException("could not decompress page", e);            }        }    });}
0
public DataPage visit(DataPageV1 dataPageV1)
{    try {        BytesInput decompressed = decompressor.decompress(dataPageV1.getBytes(), dataPageV1.getUncompressedSize());        final DataPageV1 decompressedPage;        if (offsetIndex == null) {            decompressedPage = new DataPageV1(decompressed, dataPageV1.getValueCount(), dataPageV1.getUncompressedSize(), dataPageV1.getStatistics(), dataPageV1.getRlEncoding(), dataPageV1.getDlEncoding(), dataPageV1.getValueEncoding());        } else {            long firstRowIndex = offsetIndex.getFirstRowIndex(currentPageIndex);            decompressedPage = new DataPageV1(decompressed, dataPageV1.getValueCount(), dataPageV1.getUncompressedSize(), firstRowIndex, Math.toIntExact(offsetIndex.getLastRowIndex(currentPageIndex, rowCount) - firstRowIndex + 1), dataPageV1.getStatistics(), dataPageV1.getRlEncoding(), dataPageV1.getDlEncoding(), dataPageV1.getValueEncoding());        }        if (dataPageV1.getCrc().isPresent()) {            decompressedPage.setCrc(dataPageV1.getCrc().getAsInt());        }        return decompressedPage;    } catch (IOException e) {        throw new ParquetDecodingException("could not decompress page", e);    }}
0
public DataPage visit(DataPageV2 dataPageV2)
{    if (!dataPageV2.isCompressed()) {        if (offsetIndex == null) {            return dataPageV2;        } else {            return DataPageV2.uncompressed(dataPageV2.getRowCount(), dataPageV2.getNullCount(), dataPageV2.getValueCount(), offsetIndex.getFirstRowIndex(currentPageIndex), dataPageV2.getRepetitionLevels(), dataPageV2.getDefinitionLevels(), dataPageV2.getDataEncoding(), dataPageV2.getData(), dataPageV2.getStatistics());        }    }    try {        int uncompressedSize = Math.toIntExact(dataPageV2.getUncompressedSize() - dataPageV2.getDefinitionLevels().size() - dataPageV2.getRepetitionLevels().size());        BytesInput decompressed = decompressor.decompress(dataPageV2.getData(), uncompressedSize);        if (offsetIndex == null) {            return DataPageV2.uncompressed(dataPageV2.getRowCount(), dataPageV2.getNullCount(), dataPageV2.getValueCount(), dataPageV2.getRepetitionLevels(), dataPageV2.getDefinitionLevels(), dataPageV2.getDataEncoding(), decompressed, dataPageV2.getStatistics());        } else {            return DataPageV2.uncompressed(dataPageV2.getRowCount(), dataPageV2.getNullCount(), dataPageV2.getValueCount(), offsetIndex.getFirstRowIndex(currentPageIndex), dataPageV2.getRepetitionLevels(), dataPageV2.getDefinitionLevels(), dataPageV2.getDataEncoding(), decompressed, dataPageV2.getStatistics());        }    } catch (IOException e) {        throw new ParquetDecodingException("could not decompress page", e);    }}
0
public DictionaryPage readDictionaryPage()
{    if (compressedDictionaryPage == null) {        return null;    }    try {        DictionaryPage decompressedPage = new DictionaryPage(decompressor.decompress(compressedDictionaryPage.getBytes(), compressedDictionaryPage.getUncompressedSize()), compressedDictionaryPage.getDictionarySize(), compressedDictionaryPage.getEncoding());        if (compressedDictionaryPage.getCrc().isPresent()) {            decompressedPage.setCrc(compressedDictionaryPage.getCrc().getAsInt());        }        return decompressedPage;    } catch (IOException e) {        throw new ParquetDecodingException("Could not decompress dictionary page", e);    }}
0
public long getRowCount()
{    return rowCount;}
0
public PageReader getPageReader(ColumnDescriptor path)
{    if (!readers.containsKey(path)) {        throw new IllegalArgumentException(path + " is not in the store: " + readers.keySet() + " " + rowCount);    }    return readers.get(path);}
0
public DictionaryPage readDictionaryPage(ColumnDescriptor descriptor)
{    return readers.get(descriptor).readDictionaryPage();}
0
public Optional<PrimitiveIterator.OfLong> getRowIndexes()
{    return rowRanges == null ? Optional.empty() : Optional.of(rowRanges.iterator());}
0
 void addColumn(ColumnDescriptor path, ColumnChunkPageReader reader)
{    if (readers.put(path, reader) != null) {        throw new RuntimeException(path + " was added twice");    }}
0
public void writePage(BytesInput bytesInput, int valueCount, Statistics<?> statistics, Encoding rlEncoding, Encoding dlEncoding, Encoding valuesEncoding) throws IOException
{        columnIndexBuilder = ColumnIndexBuilder.getNoOpBuilder();    offsetIndexBuilder = OffsetIndexBuilder.getNoOpBuilder();    writePage(bytesInput, valueCount, -1, statistics, rlEncoding, dlEncoding, valuesEncoding);}
0
public void writePage(BytesInput bytes, int valueCount, int rowCount, Statistics statistics, Encoding rlEncoding, Encoding dlEncoding, Encoding valuesEncoding) throws IOException
{    long uncompressedSize = bytes.size();    if (uncompressedSize > Integer.MAX_VALUE) {        throw new ParquetEncodingException("Cannot write page larger than Integer.MAX_VALUE bytes: " + uncompressedSize);    }    BytesInput compressedBytes = compressor.compress(bytes);    long compressedSize = compressedBytes.size();    if (compressedSize > Integer.MAX_VALUE) {        throw new ParquetEncodingException("Cannot write compressed page larger than Integer.MAX_VALUE bytes: " + compressedSize);    }    tempOutputStream.reset();    if (pageWriteChecksumEnabled) {        crc.reset();        crc.update(compressedBytes.toByteArray());        parquetMetadataConverter.writeDataPageV1Header((int) uncompressedSize, (int) compressedSize, valueCount, rlEncoding, dlEncoding, valuesEncoding, (int) crc.getValue(), tempOutputStream);    } else {        parquetMetadataConverter.writeDataPageV1Header((int) uncompressedSize, (int) compressedSize, valueCount, rlEncoding, dlEncoding, valuesEncoding, tempOutputStream);    }    this.uncompressedLength += uncompressedSize;    this.compressedLength += compressedSize;    this.totalValueCount += valueCount;    this.pageCount += 1;        if (totalStatistics == null) {        totalStatistics = statistics.copy();    } else {        totalStatistics.mergeStatistics(statistics);    }    columnIndexBuilder.add(statistics);    offsetIndexBuilder.add(toIntWithCheck(tempOutputStream.size() + compressedSize), rowCount);            buf.collect(BytesInput.concat(BytesInput.from(tempOutputStream), compressedBytes));    rlEncodings.add(rlEncoding);    dlEncodings.add(dlEncoding);    dataEncodings.add(valuesEncoding);}
0
public void writePageV2(int rowCount, int nullCount, int valueCount, BytesInput repetitionLevels, BytesInput definitionLevels, Encoding dataEncoding, BytesInput data, Statistics<?> statistics) throws IOException
{    int rlByteLength = toIntWithCheck(repetitionLevels.size());    int dlByteLength = toIntWithCheck(definitionLevels.size());    int uncompressedSize = toIntWithCheck(data.size() + repetitionLevels.size() + definitionLevels.size());        BytesInput compressedData = compressor.compress(data);    int compressedSize = toIntWithCheck(compressedData.size() + repetitionLevels.size() + definitionLevels.size());    tempOutputStream.reset();    parquetMetadataConverter.writeDataPageV2Header(uncompressedSize, compressedSize, valueCount, nullCount, rowCount, dataEncoding, rlByteLength, dlByteLength, tempOutputStream);    this.uncompressedLength += uncompressedSize;    this.compressedLength += compressedSize;    this.totalValueCount += valueCount;    this.pageCount += 1;        if (totalStatistics == null) {        totalStatistics = statistics.copy();    } else {        totalStatistics.mergeStatistics(statistics);    }    columnIndexBuilder.add(statistics);    offsetIndexBuilder.add(toIntWithCheck((long) tempOutputStream.size() + compressedSize), rowCount);            buf.collect(BytesInput.concat(BytesInput.from(tempOutputStream), repetitionLevels, definitionLevels, compressedData));    dataEncodings.add(dataEncoding);}
0
private int toIntWithCheck(long size)
{    if (size > Integer.MAX_VALUE) {        throw new ParquetEncodingException("Cannot write page larger than " + Integer.MAX_VALUE + " bytes: " + size);    }    return (int) size;}
0
public long getMemSize()
{    return buf.size();}
0
public void writeToFileWriter(ParquetFileWriter writer) throws IOException
{    writer.writeColumnChunk(path, totalValueCount, compressor.getCodecName(), dictionaryPage, buf, uncompressedLength, compressedLength, totalStatistics, columnIndexBuilder, offsetIndexBuilder, rlEncodings, dlEncodings, dataEncodings);    if (LOG.isDebugEnabled()) {            }    rlEncodings.clear();    dlEncodings.clear();    dataEncodings.clear();    pageCount = 0;}
1
public long allocatedSize()
{    return buf.size();}
0
public void writeDictionaryPage(DictionaryPage dictionaryPage) throws IOException
{    if (this.dictionaryPage != null) {        throw new ParquetEncodingException("Only one dictionary page is allowed");    }    BytesInput dictionaryBytes = dictionaryPage.getBytes();    int uncompressedSize = (int) dictionaryBytes.size();    BytesInput compressedBytes = compressor.compress(dictionaryBytes);    this.dictionaryPage = new DictionaryPage(BytesInput.copy(compressedBytes), uncompressedSize, dictionaryPage.getDictionarySize(), dictionaryPage.getEncoding());}
0
public String memUsageString(String prefix)
{    return buf.memUsageString(prefix + " ColumnChunkPageWriter");}
0
public PageWriter getPageWriter(ColumnDescriptor path)
{    return writers.get(path);}
0
public void flushToFileWriter(ParquetFileWriter writer) throws IOException
{    for (ColumnDescriptor path : schema.getColumns()) {        ColumnChunkPageWriter pageWriter = writers.get(path);        pageWriter.writeToFileWriter(writer);    }}
0
 long getOffset()
{    return offset;}
0
 long getLength()
{    return length;}
0
private boolean extend(long offset, int length)
{    if (this.offset + this.length == offset) {        this.length += length;        return true;    } else {        return false;    }}
0
public int getPageCount()
{    return indexMap.length;}
0
public long getOffset(int pageIndex)
{    return offsetIndex.getOffset(indexMap[pageIndex]);}
0
public int getCompressedPageSize(int pageIndex)
{    return offsetIndex.getCompressedPageSize(indexMap[pageIndex]);}
0
public long getFirstRowIndex(int pageIndex)
{    return offsetIndex.getFirstRowIndex(indexMap[pageIndex]);}
0
public long getLastRowIndex(int pageIndex, long totalRowCount)
{    int nextIndex = indexMap[pageIndex] + 1;    return (nextIndex >= offsetIndex.getPageCount() ? totalRowCount : offsetIndex.getFirstRowIndex(nextIndex)) - 1;}
0
public String toString()
{    try (Formatter formatter = new Formatter()) {        formatter.format("%-12s  %20s  %16s  %20s\n", "", "offset", "compressed size", "first row index");        for (int i = 0, n = offsetIndex.getPageCount(); i < n; ++i) {            int index = Arrays.binarySearch(indexMap, i);            boolean isHidden = index < 0;            formatter.format("%spage-%-5d  %20d  %16d  %20d\n", isHidden ? "- " : "  ", isHidden ? i : index, offsetIndex.getOffset(i), offsetIndex.getCompressedPageSize(i), offsetIndex.getFirstRowIndex(i));        }        return formatter.toString();    }}
0
 static OffsetIndex filterOffsetIndex(OffsetIndex offsetIndex, RowRanges rowRanges, long totalRowCount)
{    IntList indexMap = new IntArrayList();    for (int i = 0, n = offsetIndex.getPageCount(); i < n; ++i) {        long from = offsetIndex.getFirstRowIndex(i);        if (rowRanges.isOverlapping(from, offsetIndex.getLastRowIndex(i, totalRowCount))) {            indexMap.add(i);        }    }    return new FilteredOffsetIndex(offsetIndex, indexMap.toIntArray());}
0
 static List<OffsetRange> calculateOffsetRanges(OffsetIndex offsetIndex, ColumnChunkMetaData cm, long firstPageOffset)
{    List<OffsetRange> ranges = new ArrayList<>();    int n = offsetIndex.getPageCount();    if (n > 0) {        OffsetRange currentRange = null;                long rowGroupOffset = cm.getStartingPos();        if (rowGroupOffset < firstPageOffset) {            currentRange = new OffsetRange(rowGroupOffset, (int) (firstPageOffset - rowGroupOffset));            ranges.add(currentRange);        }        for (int i = 0; i < n; ++i) {            long offset = offsetIndex.getOffset(i);            int length = offsetIndex.getCompressedPageSize(i);            if (currentRange == null || !currentRange.extend(offset, length)) {                currentRange = new OffsetRange(offset, length);                ranges.add(currentRange);            }        }    }    return ranges;}
0
public ColumnIndex getColumnIndex()
{    if (!columnIndexRead) {        try {            columnIndex = reader.readColumnIndex(meta);        } catch (IOException e) {                                            }        columnIndexRead = true;    }    return columnIndex;}
1
public OffsetIndex getOffsetIndex()
{    return offsetIndex;}
0
public ColumnIndex getColumnIndex()
{    return null;}
0
public OffsetIndex getOffsetIndex()
{    return null;}
0
public ColumnIndex getColumnIndex(ColumnPath column)
{    return null;}
0
public OffsetIndex getOffsetIndex(ColumnPath column)
{    throw new MissingOffsetIndexException(column);}
0
 static ColumnIndexStore create(ParquetFileReader reader, BlockMetaData block, Set<ColumnPath> paths)
{    try {        return new ColumnIndexStoreImpl(reader, block, paths);    } catch (MissingOffsetIndexException e) {        return EMPTY;    }}
0
public ColumnIndex getColumnIndex(ColumnPath column)
{    return store.getOrDefault(column, MISSING_INDEX_STORE).getColumnIndex();}
0
public OffsetIndex getOffsetIndex(ColumnPath column)
{    return store.getOrDefault(column, MISSING_INDEX_STORE).getOffsetIndex();}
0
 void setRowGroup(ColumnChunkPageReadStore rowGroup)
{    this.rowGroup = rowGroup;}
0
public DictionaryPage readDictionaryPage(ColumnDescriptor descriptor)
{    if (rowGroup != null) {                return rowGroup.readDictionaryPage(descriptor);    }    String dotPath = Strings.join(descriptor.getPath(), ".");    ColumnChunkMetaData column = columns.get(dotPath);    if (column == null) {        throw new ParquetDecodingException("Cannot load dictionary, unknown column: " + dotPath);    }    if (cache.containsKey(dotPath)) {        return cache.get(dotPath);    }    try {        synchronized (cache) {                        if (!cache.containsKey(dotPath)) {                DictionaryPage dict = hasDictionaryPage(column) ? reader.readDictionary(column) : null;                                                                cache.put(dotPath, reusableCopy(dict));            }        }        return cache.get(dotPath);    } catch (IOException e) {        throw new ParquetDecodingException("Failed to read dictionary", e);    }}
0
private static DictionaryPage reusableCopy(DictionaryPage dict)
{    if (dict == null) {        return null;    }    try {        return new DictionaryPage(BytesInput.from(dict.getBytes().toByteArray()), dict.getDictionarySize(), dict.getEncoding());    } catch (IOException e) {        throw new ParquetDecodingException("Cannot read dictionary", e);    }}
0
private boolean hasDictionaryPage(ColumnChunkMetaData column)
{    EncodingStats stats = column.getEncodingStats();    if (stats != null) {                return stats.hasDictionaryPages() && stats.hasDictionaryEncodedPages();    }    Set<Encoding> encodings = column.getEncodings();    return (encodings.contains(PLAIN_DICTIONARY) || encodings.contains(RLE_DICTIONARY));}
0
private ByteBuffer ensure(ByteBuffer buffer, int size)
{    if (buffer == null) {        buffer = allocator.allocate(size);    } else if (buffer.capacity() >= size) {        buffer.clear();    } else {        release(buffer);        buffer = allocator.allocate(size);    }    return buffer;}
0
 ByteBuffer release(ByteBuffer buffer)
{    if (buffer != null) {        allocator.release(buffer);    }    return null;}
0
protected BytesCompressor createCompressor(final CompressionCodecName codecName)
{    CompressionCodec codec = getCodec(codecName);    if (codec == null) {        return new NoopCompressor();    } else if (codecName == CompressionCodecName.SNAPPY) {                return new SnappyCompressor();    } else {                return new HeapBytesCompressor(codecName);    }}
0
protected BytesDecompressor createDecompressor(final CompressionCodecName codecName)
{    CompressionCodec codec = getCodec(codecName);    if (codec == null) {        return new NoopDecompressor();    } else if (codecName == CompressionCodecName.SNAPPY) {        return new SnappyDecompressor();    } else if (DirectCodecPool.INSTANCE.codec(codec).supportsDirectDecompression()) {        return new FullDirectDecompressor(codecName);    } else {        return new IndirectDecompressor(codec);    }}
0
public void close()
{    release();}
0
public BytesInput decompress(BytesInput bytes, int uncompressedSize) throws IOException
{    decompressor.reset();    byte[] inputBytes = bytes.toByteArray();    decompressor.setInput(inputBytes, 0, inputBytes.length);    byte[] output = new byte[uncompressedSize];    decompressor.decompress(output, 0, uncompressedSize);    return BytesInput.from(output);}
0
public void decompress(ByteBuffer input, int compressedSize, ByteBuffer output, int uncompressedSize) throws IOException
{    decompressor.reset();    byte[] inputBytes = new byte[compressedSize];    input.position(0);    input.get(inputBytes);    decompressor.setInput(inputBytes, 0, inputBytes.length);    byte[] outputBytes = new byte[uncompressedSize];    decompressor.decompress(outputBytes, 0, uncompressedSize);    output.clear();    output.put(outputBytes);}
0
public void release()
{    DirectCodecPool.INSTANCE.returnDecompressor(decompressor);}
0
public BytesInput decompress(BytesInput compressedBytes, int uncompressedSize) throws IOException
{    return extraDecompressor.decompress(compressedBytes, uncompressedSize);}
0
public void decompress(ByteBuffer input, int compressedSize, ByteBuffer output, int uncompressedSize) throws IOException
{    output.clear();    try {        DECOMPRESS_METHOD.invoke(decompressor, (ByteBuffer) input.limit(compressedSize), (ByteBuffer) output.limit(uncompressedSize));    } catch (IllegalAccessException e) {        throw new DirectCodecPool.ParquetCompressionCodecException(e);    } catch (InvocationTargetException e) {        throw new DirectCodecPool.ParquetCompressionCodecException(e);    }    output.position(uncompressedSize);}
0
public void release()
{    DirectCodecPool.INSTANCE.returnDirectDecompressor(decompressor);    extraDecompressor.release();}
0
public void decompress(ByteBuffer input, int compressedSize, ByteBuffer output, int uncompressedSize) throws IOException
{    Preconditions.checkArgument(compressedSize == uncompressedSize, "Non-compressed data did not have matching compressed and uncompressed sizes.");    output.clear();    output.put((ByteBuffer) input.duplicate().position(0).limit(compressedSize));}
0
public BytesInput decompress(BytesInput bytes, int uncompressedSize) throws IOException
{    return bytes;}
0
public void release()
{}
0
public BytesInput decompress(BytesInput bytes, int uncompressedSize) throws IOException
{    return extraDecompressor.decompress(bytes, uncompressedSize);}
0
public void decompress(ByteBuffer src, int compressedSize, ByteBuffer dst, int uncompressedSize) throws IOException
{    dst.clear();    int size = Snappy.uncompress(src, dst);    dst.limit(size);}
0
public void release()
{}
0
public BytesInput compress(BytesInput bytes) throws IOException
{    int maxOutputSize = Snappy.maxCompressedLength((int) bytes.size());    ByteBuffer bufferIn = bytes.toByteBuffer();    outgoing = ensure(outgoing, maxOutputSize);    final int size;    if (bufferIn.isDirect()) {        size = Snappy.compress(bufferIn, outgoing);    } else {                this.incoming = ensure(this.incoming, (int) bytes.size());        this.incoming.put(bufferIn);        this.incoming.flip();        size = Snappy.compress(this.incoming, outgoing);    }    outgoing.limit(size);    return BytesInput.from(outgoing);}
0
public CompressionCodecName getCodecName()
{    return CompressionCodecName.SNAPPY;}
0
public void release()
{    outgoing = DirectCodecFactory.this.release(outgoing);    incoming = DirectCodecFactory.this.release(incoming);}
0
public BytesInput compress(BytesInput bytes) throws IOException
{    return bytes;}
0
public CompressionCodecName getCodecName()
{    return CompressionCodecName.UNCOMPRESSED;}
0
public void release()
{}
0
public Object makeObject() throws Exception
{    return codec.createCompressor();}
0
public Object makeObject() throws Exception
{    return codec.createDecompressor();}
0
public Object makeObject() throws Exception
{    return CREATE_DIRECT_DECOMPRESSOR_METHOD.invoke(DIRECT_DECOMPRESSION_CODEC_CLASS);}
0
public Object borrowDirectDecompressor()
{    Preconditions.checkArgument(supportDirectDecompressor, "Tried to get a direct Decompressor from a non-direct codec.");    try {        return directDecompressorPool.borrowObject();    } catch (Exception e) {        throw new ParquetCompressionCodecException(e);    }}
0
public boolean supportsDirectDecompression()
{    return supportDirectDecompressor;}
0
public Decompressor borrowDecompressor()
{    return borrow(decompressorPool);}
0
public Compressor borrowCompressor()
{    return borrow(compressorPool);}
0
public CodecPool codec(CompressionCodec codec)
{    CodecPool pools = codecs.get(codec);    if (pools == null) {        synchronized (this) {            pools = codecs.get(codec);            if (pools == null) {                pools = new CodecPool(codec);                codecs.put(codec, pools);            }        }    }    return pools;}
0
private void returnToPool(Object obj, Map<Class<?>, GenericObjectPool> pools)
{    try {        GenericObjectPool pool = pools.get(obj.getClass());        if (pool == null) {            throw new IllegalStateException("Received unexpected compressor or decompressor, " + "cannot be returned to any available pool: " + obj.getClass().getSimpleName());        }        pool.returnObject(obj);    } catch (Exception e) {        throw new ParquetCompressionCodecException(e);    }}
0
public T borrow(GenericObjectPool pool)
{    try {        return (T) pool.borrowObject();    } catch (Exception e) {        throw new ParquetCompressionCodecException(e);    }}
0
public void returnCompressor(Compressor compressor)
{    returnToPool(compressor, cPools);}
0
public void returnDecompressor(Decompressor decompressor)
{    returnToPool(decompressor, dePools);}
0
public void returnDirectDecompressor(Object decompressor)
{    returnToPool(decompressor, directDePools);}
0
public static void setSchema(Job job, MessageType schema)
{    GroupWriteSupport.setSchema(schema, ContextUtil.getConfiguration(job));}
0
public static MessageType getSchema(Job job)
{    return GroupWriteSupport.getSchema(ContextUtil.getConfiguration(job));}
0
public static Builder builder(Path file)
{    return new Builder(file);}
0
public static Builder builder(OutputFile file)
{    return new Builder(file);}
0
public Builder withType(MessageType type)
{    this.type = type;    return this;}
0
public Builder withExtraMetaData(Map<String, String> extraMetaData)
{    this.extraMetaData = extraMetaData;    return this;}
0
protected Builder self()
{    return this;}
0
protected WriteSupport<Group> getWriteSupport(Configuration conf)
{    return new GroupWriteSupport(type, extraMetaData);}
0
public org.apache.parquet.hadoop.api.ReadSupport.ReadContext init(Configuration configuration, Map<String, String> keyValueMetaData, MessageType fileSchema)
{    String partialSchemaString = configuration.get(ReadSupport.PARQUET_READ_SCHEMA);    MessageType requestedProjection = getSchemaForRead(fileSchema, partialSchemaString);    return new ReadContext(requestedProjection);}
0
public RecordMaterializer<Group> prepareForRead(Configuration configuration, Map<String, String> keyValueMetaData, MessageType fileSchema, org.apache.parquet.hadoop.api.ReadSupport.ReadContext readContext)
{    return new GroupRecordConverter(readContext.getRequestedSchema());}
0
public static void setSchema(MessageType schema, Configuration configuration)
{    configuration.set(PARQUET_EXAMPLE_SCHEMA, schema.toString());}
0
public static MessageType getSchema(Configuration configuration)
{    return parseMessageType(checkNotNull(configuration.get(PARQUET_EXAMPLE_SCHEMA), PARQUET_EXAMPLE_SCHEMA));}
0
public String getName()
{    return "example";}
0
public org.apache.parquet.hadoop.api.WriteSupport.WriteContext init(Configuration configuration)
{        if (schema == null) {        schema = getSchema(configuration);    }    return new WriteContext(schema, this.extraMetaData);}
0
public void prepareForWrite(RecordConsumer recordConsumer)
{    groupWriter = new GroupWriter(recordConsumer, schema);}
0
public void write(Group record)
{    groupWriter.write(record);}
0
public Path getFile()
{    return file;}
0
public ParquetMetadata getParquetMetadata()
{    return parquetMetadata;}
0
public String toString()
{    return "Footer{" + file + ", " + parquetMetadata + "}";}
0
public void close() throws IOException
{    if (reader != null) {        reader.close();    }}
0
public Void getCurrentKey() throws IOException, InterruptedException
{    return null;}
0
public T getCurrentValue() throws IOException, InterruptedException
{    return currentValue;}
0
public float getProgress() throws IOException, InterruptedException
{    return (float) current / total;}
0
public void initialize(ParquetFileReader reader, ParquetReadOptions options)
{        Configuration conf = new Configuration();    if (options instanceof HadoopReadOptions) {        conf = ((HadoopReadOptions) options).getConf();    }    for (String property : options.getPropertyNames()) {        conf.set(property, options.getProperty(property));    }        this.reader = reader;    FileMetaData parquetFileMetadata = reader.getFooter().getFileMetaData();    this.fileSchema = parquetFileMetadata.getSchema();    Map<String, String> fileMetadata = parquetFileMetadata.getKeyValueMetaData();    ReadSupport.ReadContext readContext = readSupport.init(new InitContext(conf, toSetMultiMap(fileMetadata), fileSchema));    this.columnIOFactory = new ColumnIOFactory(parquetFileMetadata.getCreatedBy());    this.requestedSchema = readContext.getRequestedSchema();    this.columnCount = requestedSchema.getPaths().size();    this.recordConverter = readSupport.prepareForRead(conf, fileMetadata, fileSchema, readContext);    this.strictTypeChecking = options.isEnabled(STRICT_TYPE_CHECKING, true);    this.total = reader.getFilteredRecordCount();    this.unmaterializableRecordCounter = new UnmaterializableRecordCounter(options, total);    this.filterRecords = options.useRecordFilter();    reader.setRequestedSchema(requestedSchema);    }
1
public void initialize(ParquetFileReader reader, Configuration configuration) throws IOException
{        this.reader = reader;    FileMetaData parquetFileMetadata = reader.getFooter().getFileMetaData();    this.fileSchema = parquetFileMetadata.getSchema();    Map<String, String> fileMetadata = parquetFileMetadata.getKeyValueMetaData();    ReadSupport.ReadContext readContext = readSupport.init(new InitContext(configuration, toSetMultiMap(fileMetadata), fileSchema));    this.columnIOFactory = new ColumnIOFactory(parquetFileMetadata.getCreatedBy());    this.requestedSchema = readContext.getRequestedSchema();    this.columnCount = requestedSchema.getPaths().size();    this.recordConverter = readSupport.prepareForRead(configuration, fileMetadata, fileSchema, readContext);    this.strictTypeChecking = configuration.getBoolean(STRICT_TYPE_CHECKING, true);    this.total = reader.getFilteredRecordCount();    this.unmaterializableRecordCounter = new UnmaterializableRecordCounter(configuration, total);    this.filterRecords = configuration.getBoolean(RECORD_FILTERING_ENABLED, true);    reader.setRequestedSchema(requestedSchema);    }
1
public boolean nextKeyValue() throws IOException, InterruptedException
{    boolean recordFound = false;    while (!recordFound) {                if (current >= total) {            return false;        }        try {            checkRead();            current++;            try {                currentValue = recordReader.read();            } catch (RecordMaterializationException e) {                                unmaterializableRecordCounter.incErrors(e);                                continue;            }            if (recordReader.shouldSkipCurrentRecord()) {                                                continue;            }            if (currentValue == null) {                                current = totalCountLoadedSoFar;                                continue;            }            recordFound = true;                    } catch (RuntimeException e) {            throw new ParquetDecodingException(format("Can not read value at %d in block %d in file %s", current, currentBlock, reader.getPath()), e);        }    }    return true;}
1
private static Map<K, Set<V>> toSetMultiMap(Map<K, V> map)
{    Map<K, Set<V>> setMultiMap = new HashMap<K, Set<V>>();    for (Map.Entry<K, V> entry : map.entrySet()) {        Set<V> set = new HashSet<V>();        set.add(entry.getValue());        setMultiMap.put(entry.getKey(), Collections.unmodifiableSet(set));    }    return Collections.unmodifiableMap(setMultiMap);}
0
public ParquetMetadata getFooter()
{    return parquetFileWriter.getFooter();}
0
private void initStore()
{    pageStore = new ColumnChunkPageWriteStore(compressor, schema, props.getAllocator(), props.getColumnIndexTruncateLength(), props.getPageWriteChecksumEnabled());    columnStore = props.newColumnWriteStore(schema, pageStore);    MessageColumnIO columnIO = new ColumnIOFactory(validating).getColumnIO(schema);    this.recordConsumer = columnIO.getRecordWriter(columnStore);    writeSupport.prepareForWrite(recordConsumer);}
0
public void close() throws IOException, InterruptedException
{    if (!closed) {        flushRowGroupToStore();        FinalizedWriteContext finalWriteContext = writeSupport.finalizeWrite();        Map<String, String> finalMetadata = new HashMap<String, String>(extraMetaData);        String modelName = writeSupport.getName();        if (modelName != null) {            finalMetadata.put(ParquetWriter.OBJECT_MODEL_NAME_PROP, modelName);        }        finalMetadata.putAll(finalWriteContext.getExtraMetaData());        parquetFileWriter.end(finalMetadata);        closed = true;    }}
0
public void write(T value) throws IOException, InterruptedException
{    writeSupport.write(value);    ++recordCount;    checkBlockSizeReached();}
0
public long getDataSize()
{    return lastRowGroupEndPos + columnStore.getBufferedSize();}
0
private void checkBlockSizeReached() throws IOException
{    if (recordCount >= recordCountForNextMemCheck) {                long memSize = columnStore.getBufferedSize();        long recordSize = memSize / recordCount;                if (memSize > (nextRowGroupSize - 2 * recordSize)) {                        flushRowGroupToStore();            initStore();            recordCountForNextMemCheck = min(max(MINIMUM_RECORD_COUNT_FOR_CHECK, recordCount / 2), MAXIMUM_RECORD_COUNT_FOR_CHECK);            this.lastRowGroupEndPos = parquetFileWriter.getPos();        } else {            recordCountForNextMemCheck = min(            max(MINIMUM_RECORD_COUNT_FOR_CHECK, (recordCount + (long) (nextRowGroupSize / ((float) recordSize))) / 2),             recordCount + MAXIMUM_RECORD_COUNT_FOR_CHECK);                    }    }}
1
private void flushRowGroupToStore() throws IOException
{    recordConsumer.flush();        if (columnStore.getAllocatedSize() > (3 * rowGroupSizeThreshold)) {            }    if (recordCount > 0) {        parquetFileWriter.startBlock(recordCount);        columnStore.flush();        pageStore.flushToFileWriter(parquetFileWriter);        recordCount = 0;        parquetFileWriter.endBlock();        this.nextRowGroupSize = Math.min(parquetFileWriter.getNextRowGroupSize(), rowGroupSizeThreshold);    }    columnStore = null;    pageStore = null;}
1
 long getRowGroupSizeThreshold()
{    return rowGroupSizeThreshold;}
0
 void setRowGroupSizeThreshold(long rowGroupSizeThreshold)
{    this.rowGroupSizeThreshold = rowGroupSizeThreshold;}
0
 MessageType getSchema()
{    return this.schema;}
0
public boolean removeEldestEntry(final Map.Entry<K, V> eldest)
{    boolean result = size() > maxSize;    if (result) {        if (LOG.isDebugEnabled()) {                    }    }    return result;}
1
public V remove(final K key)
{    V oldValue = cacheMap.remove(key);    if (oldValue != null) {            }    return oldValue;}
1
public void put(final K key, final V newValue)
{    if (newValue == null || !newValue.isCurrent(key)) {        if (LOG.isWarnEnabled()) {                    }        return;    }    V oldValue = cacheMap.get(key);    if (oldValue != null && oldValue.isNewerThan(newValue)) {        if (LOG.isWarnEnabled()) {                    }        return;    }        oldValue = cacheMap.put(key, newValue);    if (LOG.isDebugEnabled()) {        if (oldValue == null) {                    } else {                    }    }}
1
public void clear()
{    cacheMap.clear();}
0
public V getCurrentValue(final K key)
{    V value = cacheMap.get(key);        if (value != null && !value.isCurrent(key)) {                remove(key);        return null;    }    return value;}
1
public int size()
{    return cacheMap.size();}
0
public void set(T object)
{    this.object = object;}
0
public T get()
{    return object;}
0
public RecordReader<Void, Container<V>> getRecordReader(InputSplit split, JobConf job, Reporter reporter) throws IOException
{    return new RecordReaderWrapper<V>(split, job, reporter);}
0
public InputSplit[] getSplits(JobConf job, int numSplits) throws IOException
{    if (isTaskSideMetaData(job)) {        return super.getSplits(job, numSplits);    }    List<Footer> footers = getFooters(job);    List<ParquetInputSplit> splits = realInputFormat.getSplits(job, footers);    if (splits == null) {        return null;    }    InputSplit[] resultSplits = new InputSplit[splits.size()];    int i = 0;    for (ParquetInputSplit split : splits) {        resultSplits[i++] = new ParquetInputSplitWrapper(split);    }    return resultSplits;}
0
public List<Footer> getFooters(JobConf job) throws IOException
{    return realInputFormat.getFooters(job, asList(super.listStatus(job)));}
0
public void close() throws IOException
{    realReader.close();}
0
public Void createKey()
{    return null;}
0
public Container<V> createValue()
{    return valueContainer;}
0
public long getPos() throws IOException
{    return (long) (splitLen * getProgress());}
0
public float getProgress() throws IOException
{    try {        return realReader.getProgress();    } catch (InterruptedException e) {        Thread.interrupted();        throw new IOException(e);    }}
0
public boolean next(Void key, Container<V> value) throws IOException
{    if (eof) {        return false;    }    if (firstRecord) {                firstRecord = false;        return true;    }    try {        if (realReader.nextKeyValue()) {            if (value != null)                value.set(realReader.getCurrentValue());            return true;        }    } catch (InterruptedException e) {        throw new IOException(e);    }        eof = true;    return false;}
0
public static boolean isTaskSideMetaData(JobConf job)
{    return job.getBoolean(ParquetInputFormat.TASK_SIDE_METADATA, TRUE);}
0
public long getLength() throws IOException
{    return realSplit.getLength();}
0
public String[] getLocations() throws IOException
{    return realSplit.getLocations();}
0
public void readFields(DataInput in) throws IOException
{    realSplit = new ParquetInputSplit();    realSplit.readFields(in);}
0
public void write(DataOutput out) throws IOException
{    realSplit.write(out);}
0
public static void setWriteSupportClass(Configuration configuration, Class<?> writeSupportClass)
{    configuration.set(ParquetOutputFormat.WRITE_SUPPORT_CLASS, writeSupportClass.getName());}
0
public static void setBlockSize(Configuration configuration, int blockSize)
{    configuration.setInt(ParquetOutputFormat.BLOCK_SIZE, blockSize);}
0
public static void setPageSize(Configuration configuration, int pageSize)
{    configuration.setInt(ParquetOutputFormat.PAGE_SIZE, pageSize);}
0
public static void setCompression(Configuration configuration, CompressionCodecName compression)
{    configuration.set(ParquetOutputFormat.COMPRESSION, compression.name());}
0
public static void setEnableDictionary(Configuration configuration, boolean enableDictionary)
{    configuration.setBoolean(ParquetOutputFormat.ENABLE_DICTIONARY, enableDictionary);}
0
public static void setAsOutputFormat(JobConf jobConf)
{    jobConf.setOutputFormat(DeprecatedParquetOutputFormat.class);    jobConf.setOutputCommitter(MapredParquetOutputCommitter.class);}
0
private CompressionCodecName getCodec(final JobConf conf)
{    return CodecConfig.from(conf).getCodec();}
0
private static Path getDefaultWorkFile(JobConf conf, String name, String extension)
{    String file = getUniqueName(conf, name) + extension;    return new Path(getWorkOutputPath(conf), file);}
0
public RecordWriter<Void, V> getRecordWriter(FileSystem fs, JobConf conf, String name, Progressable progress) throws IOException
{    return new RecordWriterWrapper(realOutputFormat, fs, conf, name, progress);}
0
public void close(Reporter reporter) throws IOException
{    try {        realWriter.close(null);    } catch (InterruptedException e) {        Thread.interrupted();        throw new IOException(e);    }}
0
public void write(Void key, V value) throws IOException
{    try {        realWriter.write(key, value);    } catch (InterruptedException e) {        Thread.interrupted();        throw new IOException(e);    }}
0
public void commitJob(JobContext jobContext) throws IOException
{    super.commitJob(jobContext);    Configuration conf = ContextUtil.getConfiguration(jobContext);    Path outputPath = FileOutputFormat.getOutputPath(new JobConf(conf));    ParquetOutputCommitter.writeMetaDataFile(conf, outputPath);}
0
private void checkRatio(float ratio)
{    if (ratio <= 0 || ratio > 1) {        throw new IllegalArgumentException("The configured memory pool ratio " + ratio + " is " + "not between 0 and 1.");    }}
0
 synchronized void addWriter(InternalParquetRecordWriter writer, Long allocation)
{    Long oldValue = writerList.get(writer);    if (oldValue == null) {        writerList.put(writer, allocation);    } else {        throw new IllegalArgumentException("[BUG] The Parquet Memory Manager should not add an " + "instance of InternalParquetRecordWriter more than once. The Manager already contains " + "the writer: " + writer);    }    updateAllocation();}
0
 synchronized void removeWriter(InternalParquetRecordWriter writer)
{    if (writerList.containsKey(writer)) {        writerList.remove(writer);    }    if (!writerList.isEmpty()) {        updateAllocation();    }}
0
private void updateAllocation()
{    long totalAllocations = 0;    for (Long allocation : writerList.values()) {        totalAllocations += allocation;    }    if (totalAllocations <= totalMemoryPool) {        scale = 1.0;    } else {        scale = (double) totalMemoryPool / totalAllocations;                for (Runnable callBack : callBacks.values()) {                        callBack.run();        }    }    int maxColCount = 0;    for (InternalParquetRecordWriter w : writerList.keySet()) {        maxColCount = Math.max(w.getSchema().getColumns().size(), maxColCount);    }    for (Map.Entry<InternalParquetRecordWriter, Long> entry : writerList.entrySet()) {        long newSize = (long) Math.floor(entry.getValue() * scale);        if (scale < 1.0 && minMemoryAllocation > 0 && newSize < minMemoryAllocation) {            throw new ParquetRuntimeException(String.format("New Memory allocation %d bytes" + " is smaller than the minimum allocation size of %d bytes.", newSize, minMemoryAllocation)) {            };        }        entry.getKey().setRowGroupSizeThreshold(newSize);            }}
1
 long getTotalMemoryPool()
{    return totalMemoryPool;}
0
 Map<InternalParquetRecordWriter, Long> getWriterList()
{    return writerList;}
0
 float getMemoryPoolRatio()
{    return memoryPoolRatio;}
0
public void registerScaleCallBack(String callBackName, Runnable callBack)
{    Preconditions.checkNotNull(callBackName, "callBackName");    Preconditions.checkNotNull(callBack, "callBack");    if (callBacks.containsKey(callBackName)) {        throw new IllegalArgumentException("The callBackName " + callBackName + " is duplicated and has been registered already.");    } else {        callBacks.put(callBackName, callBack);    }}
0
 Map<String, Runnable> getScaleCallBacks()
{    return Collections.unmodifiableMap(callBacks);}
0
 double getScale()
{    return scale;}
0
public void setPath(String path)
{    this.path = path;}
0
public String getPath()
{    return path;}
0
public long getRowCount()
{    return rowCount;}
0
public void setRowCount(long rowCount)
{    this.rowCount = rowCount;}
0
public long getTotalByteSize()
{    return totalByteSize;}
0
public void setTotalByteSize(long totalByteSize)
{    this.totalByteSize = totalByteSize;}
0
public void addColumn(ColumnChunkMetaData column)
{    columns.add(column);}
0
public List<ColumnChunkMetaData> getColumns()
{    return Collections.unmodifiableList(columns);}
0
public long getStartingPos()
{    return getColumns().get(0).getStartingPos();}
0
public String toString()
{    return "BlockMetaData{" + rowCount + ", " + totalByteSize + " " + columns + "}";}
0
public long getCompressedSize()
{    long totalSize = 0;    for (ColumnChunkMetaData col : getColumns()) {        totalSize += col.getTotalSize();    }    return totalSize;}
0
public static ColumnChunkMetaData get(ColumnPath path, PrimitiveTypeName type, CompressionCodecName codec, Set<Encoding> encodings, long firstDataPage, long dictionaryPageOffset, long valueCount, long totalSize, long totalUncompressedSize)
{    return get(path, type, codec, null, encodings, new BooleanStatistics(), firstDataPage, dictionaryPageOffset, valueCount, totalSize, totalUncompressedSize);}
0
public static ColumnChunkMetaData get(ColumnPath path, PrimitiveTypeName type, CompressionCodecName codec, Set<Encoding> encodings, Statistics statistics, long firstDataPage, long dictionaryPageOffset, long valueCount, long totalSize, long totalUncompressedSize)
{    return get(path, type, codec, null, encodings, statistics, firstDataPage, dictionaryPageOffset, valueCount, totalSize, totalUncompressedSize);}
0
public static ColumnChunkMetaData get(ColumnPath path, PrimitiveTypeName type, CompressionCodecName codec, EncodingStats encodingStats, Set<Encoding> encodings, Statistics statistics, long firstDataPage, long dictionaryPageOffset, long valueCount, long totalSize, long totalUncompressedSize)
{    return get(path, Types.optional(type).named("fake_type"), codec, encodingStats, encodings, statistics, firstDataPage, dictionaryPageOffset, valueCount, totalSize, totalUncompressedSize);}
0
public static ColumnChunkMetaData get(ColumnPath path, PrimitiveType type, CompressionCodecName codec, EncodingStats encodingStats, Set<Encoding> encodings, Statistics statistics, long firstDataPage, long dictionaryPageOffset, long valueCount, long totalSize, long totalUncompressedSize)
{        if (positiveLongFitsInAnInt(firstDataPage) && positiveLongFitsInAnInt(dictionaryPageOffset) && positiveLongFitsInAnInt(valueCount) && positiveLongFitsInAnInt(totalSize) && positiveLongFitsInAnInt(totalUncompressedSize)) {        return new IntColumnChunkMetaData(path, type, codec, encodingStats, encodings, statistics, firstDataPage, dictionaryPageOffset, valueCount, totalSize, totalUncompressedSize);    } else {        return new LongColumnChunkMetaData(path, type, codec, encodingStats, encodings, statistics, firstDataPage, dictionaryPageOffset, valueCount, totalSize, totalUncompressedSize);    }}
0
public long getStartingPos()
{    long dictionaryPageOffset = getDictionaryPageOffset();    long firstDataPageOffset = getFirstDataPageOffset();    if (dictionaryPageOffset > 0 && dictionaryPageOffset < firstDataPageOffset) {                return dictionaryPageOffset;    }    return firstDataPageOffset;}
0
protected static boolean positiveLongFitsInAnInt(long value)
{    return (value >= 0) && (value + Integer.MIN_VALUE <= Integer.MAX_VALUE);}
0
public CompressionCodecName getCodec()
{    return properties.getCodec();}
0
public ColumnPath getPath()
{    return properties.getPath();}
0
public PrimitiveTypeName getType()
{    return properties.getType();}
0
public PrimitiveType getPrimitiveType()
{    return properties.getPrimitiveType();}
0
public IndexReference getColumnIndexReference()
{    return columnIndexReference;}
0
public void setColumnIndexReference(IndexReference indexReference)
{    this.columnIndexReference = indexReference;}
0
public IndexReference getOffsetIndexReference()
{    return offsetIndexReference;}
0
public void setOffsetIndexReference(IndexReference offsetIndexReference)
{    this.offsetIndexReference = offsetIndexReference;}
0
public Set<Encoding> getEncodings()
{    return properties.getEncodings();}
0
public EncodingStats getEncodingStats()
{    return encodingStats;}
0
public String toString()
{    return "ColumnMetaData{" + properties.toString() + ", " + getFirstDataPageOffset() + "}";}
0
private int positiveLongToInt(long value)
{    if (!ColumnChunkMetaData.positiveLongFitsInAnInt(value)) {        throw new IllegalArgumentException("value should be positive and fit in an int: " + value);    }    return (int) (value + Integer.MIN_VALUE);}
0
private long intToPositiveLong(int value)
{    return (long) value - Integer.MIN_VALUE;}
0
public long getFirstDataPageOffset()
{    return intToPositiveLong(firstDataPage);}
0
public long getDictionaryPageOffset()
{    return intToPositiveLong(dictionaryPageOffset);}
0
public long getValueCount()
{    return intToPositiveLong(valueCount);}
0
public long getTotalUncompressedSize()
{    return intToPositiveLong(totalUncompressedSize);}
0
public long getTotalSize()
{    return intToPositiveLong(totalSize);}
0
public Statistics getStatistics()
{    return statistics;}
0
public long getFirstDataPageOffset()
{    return firstDataPageOffset;}
0
public long getDictionaryPageOffset()
{    return dictionaryPageOffset;}
0
public long getValueCount()
{    return valueCount;}
0
public long getTotalUncompressedSize()
{    return totalUncompressedSize;}
0
public long getTotalSize()
{    return totalSize;}
0
public Statistics getStatistics()
{    return statistics;}
0
public static ColumnChunkProperties get(ColumnPath path, PrimitiveTypeName type, CompressionCodecName codec, Set<Encoding> encodings)
{    return get(path, new PrimitiveType(Type.Repetition.OPTIONAL, type, ""), codec, encodings);}
0
public static ColumnChunkProperties get(ColumnPath path, PrimitiveType type, CompressionCodecName codec, Set<Encoding> encodings)
{    return properties.canonicalize(new ColumnChunkProperties(codec, path, type, encodings));}
0
public CompressionCodecName getCodec()
{    return codec;}
0
public ColumnPath getPath()
{    return path;}
0
public PrimitiveTypeName getType()
{    return type.getPrimitiveTypeName();}
0
public PrimitiveType getPrimitiveType()
{    return type;}
0
public Set<Encoding> getEncodings()
{    return encodings;}
0
public boolean equals(Object obj)
{    if (obj instanceof ColumnChunkProperties) {        ColumnChunkProperties other = (ColumnChunkProperties) obj;        return other.codec == codec && other.path.equals(path) && other.type.equals(type) && equals(other.encodings, encodings);    }    return false;}
0
private boolean equals(Set<Encoding> a, Set<Encoding> b)
{    return a.size() == b.size() && a.containsAll(b);}
0
public int hashCode()
{    return codec.hashCode() ^ path.hashCode() ^ type.hashCode() ^ Arrays.hashCode(encodings.toArray());}
0
public String toString()
{    return codec + " " + path + " " + type + "  " + encodings;}
0
public static EncodingList getEncodingList(List<Encoding> encodings)
{    return encodingLists.canonicalize(new EncodingList(encodings));}
0
public boolean equals(Object obj)
{    if (obj instanceof EncodingList) {        List<org.apache.parquet.column.Encoding> other = ((EncodingList) obj).encodings;        final int size = other.size();        if (size != encodings.size()) {            return false;        }        for (int i = 0; i < size; i++) {            if (!other.get(i).equals(encodings.get(i))) {                return false;            }        }        return true;    }    return false;}
0
public int hashCode()
{    int result = 1;    for (org.apache.parquet.column.Encoding element : encodings) result = 31 * result + (element == null ? 0 : element.hashCode());    return result;}
0
public List<Encoding> toList()
{    return encodings;}
0
public Iterator<Encoding> iterator()
{    return encodings.iterator();}
0
public int size()
{    return encodings.size();}
0
public MessageType getSchema()
{    return schema;}
0
public String toString()
{    return "FileMetaData{schema: " + schema + ", metadata: " + keyValueMetaData + "}";}
0
public Map<String, String> getKeyValueMetaData()
{    return keyValueMetaData;}
0
public String getCreatedBy()
{    return createdBy;}
0
public MessageType getSchema()
{    return schema;}
0
public String toString()
{    return "GlobalMetaData{schema: " + schema + ", metadata: " + keyValueMetaData + "}";}
0
public Map<String, Set<String>> getKeyValueMetaData()
{    return keyValueMetaData;}
0
public Set<String> getCreatedBy()
{    return createdBy;}
0
public FileMetaData merge()
{    String createdByString = createdBy.size() == 1 ? createdBy.iterator().next() : createdBy.toString();    Map<String, String> mergedKeyValues = new HashMap<String, String>();    for (Entry<String, Set<String>> entry : keyValueMetaData.entrySet()) {        if (entry.getValue().size() > 1) {            throw new RuntimeException("could not merge metadata: key " + entry.getKey() + " has conflicting values: " + entry.getValue());        }        mergedKeyValues.put(entry.getKey(), entry.getValue().iterator().next());    }    return new FileMetaData(schema, mergedKeyValues, createdByString);}
0
public static String toJSON(ParquetMetadata parquetMetaData)
{    return toJSON(parquetMetaData, false);}
0
public static String toPrettyJSON(ParquetMetadata parquetMetaData)
{    return toJSON(parquetMetaData, true);}
0
private static String toJSON(ParquetMetadata parquetMetaData, boolean isPrettyPrint)
{    StringWriter stringWriter = new StringWriter();    try {        if (isPrettyPrint) {            objectMapper.writerWithDefaultPrettyPrinter().writeValue(stringWriter, parquetMetaData);        } else {            objectMapper.writeValue(stringWriter, parquetMetaData);        }    } catch (JsonGenerationException e) {        throw new RuntimeException(e);    } catch (JsonMappingException e) {        throw new RuntimeException(e);    } catch (IOException e) {        throw new RuntimeException(e);    }    return stringWriter.toString();}
0
public static ParquetMetadata fromJSON(String json)
{    try {        return objectMapper.readValue(new StringReader(json), ParquetMetadata.class);    } catch (JsonParseException e) {        throw new RuntimeException(e);    } catch (JsonMappingException e) {        throw new RuntimeException(e);    } catch (IOException e) {        throw new RuntimeException(e);    }}
0
public List<BlockMetaData> getBlocks()
{    return blocks;}
0
public FileMetaData getFileMetaData()
{    return fileMetaData;}
0
public String toString()
{    return "ParquetMetaData{" + fileMetaData + ", blocks: " + blocks + "}";}
0
public static List<Footer> readAllFootersInParallelUsingSummaryFiles(Configuration configuration, List<FileStatus> partFiles) throws IOException
{    return readAllFootersInParallelUsingSummaryFiles(configuration, partFiles, false);}
0
private static MetadataFilter filter(boolean skipRowGroups)
{    return skipRowGroups ? SKIP_ROW_GROUPS : NO_FILTER;}
0
public static List<Footer> readAllFootersInParallelUsingSummaryFiles(final Configuration configuration, final Collection<FileStatus> partFiles, final boolean skipRowGroups) throws IOException
{        Set<Path> parents = new HashSet<Path>();    for (FileStatus part : partFiles) {        parents.add(part.getPath().getParent());    }        List<Callable<Map<Path, Footer>>> summaries = new ArrayList<Callable<Map<Path, Footer>>>();    for (final Path path : parents) {        summaries.add(() -> {            ParquetMetadata mergedMetadata = readSummaryMetadata(configuration, path, skipRowGroups);            if (mergedMetadata != null) {                final List<Footer> footers;                if (skipRowGroups) {                    footers = new ArrayList<Footer>();                    for (FileStatus f : partFiles) {                        footers.add(new Footer(f.getPath(), mergedMetadata));                    }                } else {                    footers = footersFromSummaryFile(path, mergedMetadata);                }                Map<Path, Footer> map = new HashMap<Path, Footer>();                for (Footer footer : footers) {                                        footer = new Footer(new Path(path, footer.getFile().getName()), footer.getParquetMetadata());                    map.put(footer.getFile(), footer);                }                return map;            } else {                return Collections.emptyMap();            }        });    }    Map<Path, Footer> cache = new HashMap<Path, Footer>();    try {        List<Map<Path, Footer>> footersFromSummaries = runAllInParallel(configuration.getInt(PARQUET_READ_PARALLELISM, 5), summaries);        for (Map<Path, Footer> footers : footersFromSummaries) {            cache.putAll(footers);        }    } catch (ExecutionException e) {        throw new IOException("Error reading summaries", e);    }        List<Footer> result = new ArrayList<Footer>(partFiles.size());    List<FileStatus> toRead = new ArrayList<FileStatus>();    for (FileStatus part : partFiles) {        Footer f = cache.get(part.getPath());        if (f != null) {            result.add(f);        } else {            toRead.add(part);        }    }    if (toRead.size() > 0) {                        result.addAll(readAllFootersInParallel(configuration, toRead, skipRowGroups));    }    return result;}
1
private static List<T> runAllInParallel(int parallelism, List<Callable<T>> toRun) throws ExecutionException
{        ExecutorService threadPool = Executors.newFixedThreadPool(parallelism);    try {        List<Future<T>> futures = new ArrayList<Future<T>>();        for (Callable<T> callable : toRun) {            futures.add(threadPool.submit(callable));        }        List<T> result = new ArrayList<T>(toRun.size());        for (Future<T> future : futures) {            try {                result.add(future.get());            } catch (InterruptedException e) {                throw new RuntimeException("The thread was interrupted", e);            }        }        return result;    } finally {        threadPool.shutdownNow();    }}
1
public static List<Footer> readAllFootersInParallel(final Configuration configuration, List<FileStatus> partFiles) throws IOException
{    return readAllFootersInParallel(configuration, partFiles, false);}
0
public static List<Footer> readAllFootersInParallel(final Configuration configuration, List<FileStatus> partFiles, final boolean skipRowGroups) throws IOException
{    List<Callable<Footer>> footers = new ArrayList<Callable<Footer>>();    for (final FileStatus currentFile : partFiles) {        footers.add(() -> {            try {                return new Footer(currentFile.getPath(), readFooter(configuration, currentFile, filter(skipRowGroups)));            } catch (IOException e) {                throw new IOException("Could not read footer for file " + currentFile, e);            }        });    }    try {        return runAllInParallel(configuration.getInt(PARQUET_READ_PARALLELISM, 5), footers);    } catch (ExecutionException e) {        throw new IOException("Could not read footer: " + e.getMessage(), e.getCause());    }}
0
public static List<Footer> readAllFootersInParallel(Configuration configuration, FileStatus fileStatus, boolean skipRowGroups) throws IOException
{    List<FileStatus> statuses = listFiles(configuration, fileStatus);    return readAllFootersInParallel(configuration, statuses, skipRowGroups);}
0
public static List<Footer> readAllFootersInParallel(Configuration configuration, FileStatus fileStatus) throws IOException
{    return readAllFootersInParallel(configuration, fileStatus, false);}
0
public static List<Footer> readFooters(Configuration configuration, Path path) throws IOException
{    return readFooters(configuration, status(configuration, path));}
0
private static FileStatus status(Configuration configuration, Path path) throws IOException
{    return path.getFileSystem(configuration).getFileStatus(path);}
0
public static List<Footer> readFooters(Configuration configuration, FileStatus pathStatus) throws IOException
{    return readFooters(configuration, pathStatus, false);}
0
public static List<Footer> readFooters(Configuration configuration, FileStatus pathStatus, boolean skipRowGroups) throws IOException
{    List<FileStatus> files = listFiles(configuration, pathStatus);    return readAllFootersInParallelUsingSummaryFiles(configuration, files, skipRowGroups);}
0
private static List<FileStatus> listFiles(Configuration conf, FileStatus fileStatus) throws IOException
{    if (fileStatus.isDir()) {        FileSystem fs = fileStatus.getPath().getFileSystem(conf);        FileStatus[] list = fs.listStatus(fileStatus.getPath(), HiddenFileFilter.INSTANCE);        List<FileStatus> result = new ArrayList<FileStatus>();        for (FileStatus sub : list) {            result.addAll(listFiles(conf, sub));        }        return result;    } else {        return Arrays.asList(fileStatus);    }}
0
public static List<Footer> readSummaryFile(Configuration configuration, FileStatus summaryStatus) throws IOException
{    final Path parent = summaryStatus.getPath().getParent();    ParquetMetadata mergedFooters = readFooter(configuration, summaryStatus, filter(false));    return footersFromSummaryFile(parent, mergedFooters);}
0
 static ParquetMetadata readSummaryMetadata(Configuration configuration, Path basePath, boolean skipRowGroups) throws IOException
{    Path metadataFile = new Path(basePath, PARQUET_METADATA_FILE);    Path commonMetaDataFile = new Path(basePath, PARQUET_COMMON_METADATA_FILE);    FileSystem fileSystem = basePath.getFileSystem(configuration);    if (skipRowGroups && fileSystem.exists(commonMetaDataFile)) {                        return readFooter(configuration, commonMetaDataFile, filter(skipRowGroups));    } else if (fileSystem.exists(metadataFile)) {                return readFooter(configuration, metadataFile, filter(skipRowGroups));    } else {        return null;    }}
1
 static List<Footer> footersFromSummaryFile(final Path parent, ParquetMetadata mergedFooters)
{    Map<Path, ParquetMetadata> footers = new HashMap<Path, ParquetMetadata>();    List<BlockMetaData> blocks = mergedFooters.getBlocks();    for (BlockMetaData block : blocks) {        String path = block.getPath();        Path fullPath = new Path(parent, path);        ParquetMetadata current = footers.get(fullPath);        if (current == null) {            current = new ParquetMetadata(mergedFooters.getFileMetaData(), new ArrayList<BlockMetaData>());            footers.put(fullPath, current);        }        current.getBlocks().add(block);    }    List<Footer> result = new ArrayList<Footer>();    for (Entry<Path, ParquetMetadata> entry : footers.entrySet()) {        result.add(new Footer(entry.getKey(), entry.getValue()));    }    return result;}
0
public static final ParquetMetadata readFooter(Configuration configuration, Path file) throws IOException
{    return readFooter(configuration, file, NO_FILTER);}
0
public static ParquetMetadata readFooter(Configuration configuration, Path file, MetadataFilter filter) throws IOException
{    return readFooter(HadoopInputFile.fromPath(file, configuration), filter);}
0
public static final ParquetMetadata readFooter(Configuration configuration, FileStatus file) throws IOException
{    return readFooter(configuration, file, NO_FILTER);}
0
public static final ParquetMetadata readFooter(Configuration configuration, FileStatus file, MetadataFilter filter) throws IOException
{    return readFooter(HadoopInputFile.fromStatus(file, configuration), filter);}
0
public static final ParquetMetadata readFooter(InputFile file, MetadataFilter filter) throws IOException
{    ParquetReadOptions options;    if (file instanceof HadoopInputFile) {        options = HadoopReadOptions.builder(((HadoopInputFile) file).getConfiguration()).withMetadataFilter(filter).build();    } else {        options = ParquetReadOptions.builder().withMetadataFilter(filter).build();    }    try (SeekableInputStream in = file.newStream()) {        return readFooter(file, options, in);    }}
0
private static final ParquetMetadata readFooter(InputFile file, ParquetReadOptions options, SeekableInputStream f) throws IOException
{    ParquetMetadataConverter converter = new ParquetMetadataConverter(options);    return readFooter(file, options, f, converter);}
0
private static final ParquetMetadata readFooter(InputFile file, ParquetReadOptions options, SeekableInputStream f, ParquetMetadataConverter converter) throws IOException
{    long fileLen = file.getLength();    String filePath = file.toString();        int FOOTER_LENGTH_SIZE = 4;    if (fileLen < MAGIC.length + FOOTER_LENGTH_SIZE + MAGIC.length) {                throw new RuntimeException(filePath + " is not a Parquet file (too small length: " + fileLen + ")");    }    long footerLengthIndex = fileLen - FOOTER_LENGTH_SIZE - MAGIC.length;        f.seek(footerLengthIndex);    int footerLength = readIntLittleEndian(f);    byte[] magic = new byte[MAGIC.length];    f.readFully(magic);    if (!Arrays.equals(MAGIC, magic)) {        throw new RuntimeException(filePath + " is not a Parquet file. expected magic number at tail " + Arrays.toString(MAGIC) + " but found " + Arrays.toString(magic));    }    long footerIndex = footerLengthIndex - footerLength;        if (footerIndex < MAGIC.length || footerIndex >= footerLengthIndex) {        throw new RuntimeException("corrupted file: the footer index is not within the file: " + footerIndex);    }    f.seek(footerIndex);            ByteBuffer footerBytesBuffer = ByteBuffer.allocate(footerLength);    f.readFully(footerBytesBuffer);        footerBytesBuffer.flip();    InputStream footerBytesStream = ByteBufferInputStream.wrap(footerBytesBuffer);    return converter.readParquetMetadata(footerBytesStream, options.getMetadataFilter());}
1
public static ParquetFileReader open(Configuration conf, Path file) throws IOException
{    return new ParquetFileReader(HadoopInputFile.fromPath(file, conf), HadoopReadOptions.builder(conf).build());}
0
public static ParquetFileReader open(Configuration conf, Path file, MetadataFilter filter) throws IOException
{    return open(HadoopInputFile.fromPath(file, conf), HadoopReadOptions.builder(conf).withMetadataFilter(filter).build());}
0
public static ParquetFileReader open(Configuration conf, Path file, ParquetMetadata footer) throws IOException
{    return new ParquetFileReader(conf, file, footer);}
0
public static ParquetFileReader open(InputFile file) throws IOException
{    return new ParquetFileReader(file, ParquetReadOptions.builder().build());}
0
public static ParquetFileReader open(InputFile file, ParquetReadOptions options) throws IOException
{    return new ParquetFileReader(file, options);}
0
private static List<T> listWithNulls(int size)
{    return Stream.generate(() -> (T) null).limit(size).collect(Collectors.toCollection(ArrayList<T>::new));}
0
public ParquetMetadata getFooter()
{    if (footer == null) {        try {                        this.footer = readFooter(file, options, f, converter);        } catch (IOException e) {            throw new ParquetDecodingException("Unable to read file footer", e);        }    }    return footer;}
0
public FileMetaData getFileMetaData()
{    if (fileMetaData != null) {        return fileMetaData;    }    return getFooter().getFileMetaData();}
0
public long getRecordCount()
{    long total = 0;    for (BlockMetaData block : blocks) {        total += block.getRowCount();    }    return total;}
0
 long getFilteredRecordCount()
{    if (!options.useColumnIndexFilter()) {        return getRecordCount();    }    long total = 0;    for (int i = 0, n = blocks.size(); i < n; ++i) {        total += getRowRanges(i).rowCount();    }    return total;}
0
public Path getPath()
{    return new Path(file.toString());}
0
public String getFile()
{    return file.toString();}
0
private List<BlockMetaData> filterRowGroups(List<BlockMetaData> blocks) throws IOException
{        List<RowGroupFilter.FilterLevel> levels = new ArrayList<>();    if (options.useStatsFilter()) {        levels.add(STATISTICS);    }    if (options.useDictionaryFilter()) {        levels.add(DICTIONARY);    }    FilterCompat.Filter recordFilter = options.getRecordFilter();    if (recordFilter != null) {        return RowGroupFilter.filterRowGroups(levels, recordFilter, blocks, this);    }    return blocks;}
0
public List<BlockMetaData> getRowGroups()
{    return blocks;}
0
public void setRequestedSchema(MessageType projection)
{    paths.clear();    for (ColumnDescriptor col : projection.getColumns()) {        paths.put(ColumnPath.get(col.getPath()), col);    }}
0
public void appendTo(ParquetFileWriter writer) throws IOException
{    writer.appendRowGroups(f, blocks, true);}
0
public PageReadStore readNextRowGroup() throws IOException
{    if (currentBlock == blocks.size()) {        return null;    }    BlockMetaData block = blocks.get(currentBlock);    if (block.getRowCount() == 0) {        throw new RuntimeException("Illegal row group of 0 rows");    }    this.currentRowGroup = new ColumnChunkPageReadStore(block.getRowCount());        List<ConsecutivePartList> allParts = new ArrayList<ConsecutivePartList>();    ConsecutivePartList currentParts = null;    for (ColumnChunkMetaData mc : block.getColumns()) {        ColumnPath pathKey = mc.getPath();        BenchmarkCounter.incrementTotalBytes(mc.getTotalSize());        ColumnDescriptor columnDescriptor = paths.get(pathKey);        if (columnDescriptor != null) {            long startingPos = mc.getStartingPos();                        if (currentParts == null || currentParts.endPos() != startingPos) {                currentParts = new ConsecutivePartList(startingPos);                allParts.add(currentParts);            }            currentParts.addChunk(new ChunkDescriptor(columnDescriptor, mc, startingPos, (int) mc.getTotalSize()));        }    }        ChunkListBuilder builder = new ChunkListBuilder();    for (ConsecutivePartList consecutiveChunks : allParts) {        consecutiveChunks.readAll(f, builder);    }    for (Chunk chunk : builder.build()) {        currentRowGroup.addColumn(chunk.descriptor.col, chunk.readAllPages());    }        if (nextDictionaryReader != null) {        nextDictionaryReader.setRowGroup(currentRowGroup);    }    advanceToNextBlock();    return currentRowGroup;}
0
public PageReadStore readNextFilteredRowGroup() throws IOException
{    if (currentBlock == blocks.size()) {        return null;    }    if (!options.useColumnIndexFilter()) {        return readNextRowGroup();    }    BlockMetaData block = blocks.get(currentBlock);    if (block.getRowCount() == 0) {        throw new RuntimeException("Illegal row group of 0 rows");    }    ColumnIndexStore ciStore = getColumnIndexStore(currentBlock);    RowRanges rowRanges = getRowRanges(currentBlock);    long rowCount = rowRanges.rowCount();    if (rowCount == 0) {                advanceToNextBlock();        return readNextFilteredRowGroup();    }    if (rowCount == block.getRowCount()) {                return readNextRowGroup();    }    this.currentRowGroup = new ColumnChunkPageReadStore(rowRanges);        ChunkListBuilder builder = new ChunkListBuilder();    List<ConsecutivePartList> allParts = new ArrayList<ConsecutivePartList>();    ConsecutivePartList currentParts = null;    for (ColumnChunkMetaData mc : block.getColumns()) {        ColumnPath pathKey = mc.getPath();        ColumnDescriptor columnDescriptor = paths.get(pathKey);        if (columnDescriptor != null) {            OffsetIndex offsetIndex = ciStore.getOffsetIndex(mc.getPath());            OffsetIndex filteredOffsetIndex = filterOffsetIndex(offsetIndex, rowRanges, block.getRowCount());            for (OffsetRange range : calculateOffsetRanges(filteredOffsetIndex, mc, offsetIndex.getOffset(0))) {                BenchmarkCounter.incrementTotalBytes(range.getLength());                long startingPos = range.getOffset();                                if (currentParts == null || currentParts.endPos() != startingPos) {                    currentParts = new ConsecutivePartList(startingPos);                    allParts.add(currentParts);                }                ChunkDescriptor chunkDescriptor = new ChunkDescriptor(columnDescriptor, mc, startingPos, (int) range.getLength());                currentParts.addChunk(chunkDescriptor);                builder.setOffsetIndex(chunkDescriptor, filteredOffsetIndex);            }        }    }        for (ConsecutivePartList consecutiveChunks : allParts) {        consecutiveChunks.readAll(f, builder);    }    for (Chunk chunk : builder.build()) {        currentRowGroup.addColumn(chunk.descriptor.col, chunk.readAllPages());    }        if (nextDictionaryReader != null) {        nextDictionaryReader.setRowGroup(currentRowGroup);    }    advanceToNextBlock();    return currentRowGroup;}
0
private ColumnIndexStore getColumnIndexStore(int blockIndex)
{    ColumnIndexStore ciStore = blockIndexStores.get(blockIndex);    if (ciStore == null) {        ciStore = ColumnIndexStoreImpl.create(this, blocks.get(blockIndex), paths.keySet());        blockIndexStores.set(blockIndex, ciStore);    }    return ciStore;}
0
private RowRanges getRowRanges(int blockIndex)
{    RowRanges rowRanges = blockRowRanges.get(blockIndex);    if (rowRanges == null) {        rowRanges = ColumnIndexFilter.calculateRowRanges(options.getRecordFilter(), getColumnIndexStore(blockIndex), paths.keySet(), blocks.get(blockIndex).getRowCount());        blockRowRanges.set(blockIndex, rowRanges);    }    return rowRanges;}
0
public boolean skipNextRowGroup()
{    return advanceToNextBlock();}
0
private boolean advanceToNextBlock()
{    if (currentBlock == blocks.size()) {        return false;    }        ++currentBlock;    this.nextDictionaryReader = null;    return true;}
0
public DictionaryPageReadStore getNextDictionaryReader()
{    if (nextDictionaryReader == null && currentBlock < blocks.size()) {        this.nextDictionaryReader = getDictionaryReader(blocks.get(currentBlock));    }    return nextDictionaryReader;}
0
public DictionaryPageReader getDictionaryReader(BlockMetaData block)
{    return new DictionaryPageReader(this, block);}
0
 DictionaryPage readDictionary(ColumnChunkMetaData meta) throws IOException
{    if (!meta.getEncodings().contains(Encoding.PLAIN_DICTIONARY) && !meta.getEncodings().contains(Encoding.RLE_DICTIONARY)) {        return null;    }        if (f.getPos() != meta.getStartingPos()) {        f.seek(meta.getStartingPos());    }    PageHeader pageHeader = Util.readPageHeader(f);    if (!pageHeader.isSetDictionary_page_header()) {                return null;    }    DictionaryPage compressedPage = readCompressedDictionary(pageHeader, f);    BytesInputDecompressor decompressor = options.getCodecFactory().getDecompressor(meta.getCodec());    return new DictionaryPage(decompressor.decompress(compressedPage.getBytes(), compressedPage.getUncompressedSize()), compressedPage.getDictionarySize(), compressedPage.getEncoding());}
0
private DictionaryPage readCompressedDictionary(PageHeader pageHeader, SeekableInputStream fin) throws IOException
{    DictionaryPageHeader dictHeader = pageHeader.getDictionary_page_header();    int uncompressedPageSize = pageHeader.getUncompressed_page_size();    int compressedPageSize = pageHeader.getCompressed_page_size();    byte[] dictPageBytes = new byte[compressedPageSize];    fin.readFully(dictPageBytes);    BytesInput bin = BytesInput.from(dictPageBytes);    return new DictionaryPage(bin, uncompressedPageSize, dictHeader.getNum_values(), converter.getEncoding(dictHeader.getEncoding()));}
0
public ColumnIndex readColumnIndex(ColumnChunkMetaData column) throws IOException
{    IndexReference ref = column.getColumnIndexReference();    if (ref == null) {        return null;    }    f.seek(ref.getOffset());    return ParquetMetadataConverter.fromParquetColumnIndex(column.getPrimitiveType(), Util.readColumnIndex(f));}
0
public OffsetIndex readOffsetIndex(ColumnChunkMetaData column) throws IOException
{    IndexReference ref = column.getOffsetIndexReference();    if (ref == null) {        return null;    }    f.seek(ref.getOffset());    return ParquetMetadataConverter.fromParquetOffsetIndex(Util.readOffsetIndex(f));}
0
public void close() throws IOException
{    try {        if (f != null) {            f.close();        }    } finally {        options.getCodecFactory().release();    }}
0
 void add(ChunkDescriptor descriptor, List<ByteBuffer> buffers, SeekableInputStream f)
{    ChunkData data = map.get(descriptor);    if (data == null) {        data = new ChunkData();        map.put(descriptor, data);    }    data.buffers.addAll(buffers);    lastDescriptor = descriptor;    this.f = f;}
0
 void setOffsetIndex(ChunkDescriptor descriptor, OffsetIndex offsetIndex)
{    ChunkData data = map.get(descriptor);    if (data == null) {        data = new ChunkData();        map.put(descriptor, data);    }    data.offsetIndex = offsetIndex;}
0
 List<Chunk> build()
{    List<Chunk> chunks = new ArrayList<>();    for (Entry<ChunkDescriptor, ChunkData> entry : map.entrySet()) {        ChunkDescriptor descriptor = entry.getKey();        ChunkData data = entry.getValue();        if (descriptor.equals(lastDescriptor)) {                        chunks.add(new WorkaroundChunk(lastDescriptor, data.buffers, f, data.offsetIndex));        } else {            chunks.add(new Chunk(descriptor, data.buffers, data.offsetIndex));        }    }    return chunks;}
0
protected PageHeader readPageHeader() throws IOException
{    return Util.readPageHeader(stream);}
0
private void verifyCrc(int referenceCrc, byte[] bytes, String exceptionMsg)
{    crc.reset();    crc.update(bytes);    if (crc.getValue() != ((long) referenceCrc & 0xffffffffL)) {        throw new ParquetDecodingException(exceptionMsg);    }}
0
public ColumnChunkPageReader readAllPages() throws IOException
{    List<DataPage> pagesInChunk = new ArrayList<DataPage>();    DictionaryPage dictionaryPage = null;    PrimitiveType type = getFileMetaData().getSchema().getType(descriptor.col.getPath()).asPrimitiveType();    long valuesCountReadSoFar = 0;    int dataPageCountReadSoFar = 0;    while (hasMorePages(valuesCountReadSoFar, dataPageCountReadSoFar)) {        PageHeader pageHeader = readPageHeader();        int uncompressedPageSize = pageHeader.getUncompressed_page_size();        int compressedPageSize = pageHeader.getCompressed_page_size();        final BytesInput pageBytes;        switch(pageHeader.type) {            case DICTIONARY_PAGE:                                if (dictionaryPage != null) {                    throw new ParquetDecodingException("more than one dictionary page in column " + descriptor.col);                }                pageBytes = this.readAsBytesInput(compressedPageSize);                if (options.usePageChecksumVerification() && pageHeader.isSetCrc()) {                    verifyCrc(pageHeader.getCrc(), pageBytes.toByteArray(), "could not verify dictionary page integrity, CRC checksum verification failed");                }                DictionaryPageHeader dicHeader = pageHeader.getDictionary_page_header();                dictionaryPage = new DictionaryPage(pageBytes, uncompressedPageSize, dicHeader.getNum_values(), converter.getEncoding(dicHeader.getEncoding()));                                if (pageHeader.isSetCrc()) {                    dictionaryPage.setCrc(pageHeader.getCrc());                }                break;            case DATA_PAGE:                DataPageHeader dataHeaderV1 = pageHeader.getData_page_header();                pageBytes = this.readAsBytesInput(compressedPageSize);                if (options.usePageChecksumVerification() && pageHeader.isSetCrc()) {                    verifyCrc(pageHeader.getCrc(), pageBytes.toByteArray(), "could not verify page integrity, CRC checksum verification failed");                }                DataPageV1 dataPageV1 = new DataPageV1(pageBytes, dataHeaderV1.getNum_values(), uncompressedPageSize, converter.fromParquetStatistics(getFileMetaData().getCreatedBy(), dataHeaderV1.getStatistics(), type), converter.getEncoding(dataHeaderV1.getRepetition_level_encoding()), converter.getEncoding(dataHeaderV1.getDefinition_level_encoding()), converter.getEncoding(dataHeaderV1.getEncoding()));                                if (pageHeader.isSetCrc()) {                    dataPageV1.setCrc(pageHeader.getCrc());                }                pagesInChunk.add(dataPageV1);                valuesCountReadSoFar += dataHeaderV1.getNum_values();                ++dataPageCountReadSoFar;                break;            case DATA_PAGE_V2:                DataPageHeaderV2 dataHeaderV2 = pageHeader.getData_page_header_v2();                int dataSize = compressedPageSize - dataHeaderV2.getRepetition_levels_byte_length() - dataHeaderV2.getDefinition_levels_byte_length();                pagesInChunk.add(new DataPageV2(dataHeaderV2.getNum_rows(), dataHeaderV2.getNum_nulls(), dataHeaderV2.getNum_values(), this.readAsBytesInput(dataHeaderV2.getRepetition_levels_byte_length()), this.readAsBytesInput(dataHeaderV2.getDefinition_levels_byte_length()), converter.getEncoding(dataHeaderV2.getEncoding()), this.readAsBytesInput(dataSize), uncompressedPageSize, converter.fromParquetStatistics(getFileMetaData().getCreatedBy(), dataHeaderV2.getStatistics(), type), dataHeaderV2.isIs_compressed()));                valuesCountReadSoFar += dataHeaderV2.getNum_values();                ++dataPageCountReadSoFar;                break;            default:                                stream.skipFully(compressedPageSize);                break;        }    }    if (offsetIndex == null && valuesCountReadSoFar != descriptor.metadata.getValueCount()) {                throw new IOException("Expected " + descriptor.metadata.getValueCount() + " values in column chunk at " + getPath() + " offset " + descriptor.metadata.getFirstDataPageOffset() + " but got " + valuesCountReadSoFar + " values instead over " + pagesInChunk.size() + " pages ending at file offset " + (descriptor.fileOffset + stream.position()));    }    BytesInputDecompressor decompressor = options.getCodecFactory().getDecompressor(descriptor.metadata.getCodec());    return new ColumnChunkPageReader(decompressor, pagesInChunk, dictionaryPage, offsetIndex, blocks.get(currentBlock).getRowCount());}
1
private boolean hasMorePages(long valuesCountReadSoFar, int dataPageCountReadSoFar)
{    return offsetIndex == null ? valuesCountReadSoFar < descriptor.metadata.getValueCount() : dataPageCountReadSoFar < offsetIndex.getPageCount();}
0
public BytesInput readAsBytesInput(int size) throws IOException
{    return BytesInput.from(stream.sliceBuffers(size));}
0
protected PageHeader readPageHeader() throws IOException
{    PageHeader pageHeader;        stream.mark(8192);    try {        pageHeader = Util.readPageHeader(stream);    } catch (IOException e) {                                                        stream.reset();                        pageHeader = Util.readPageHeader(new SequenceInputStream(stream, f));    }    return pageHeader;}
1
public BytesInput readAsBytesInput(int size) throws IOException
{    int available = stream.available();    if (size > available) {                                        int missingBytes = size - available;                List<ByteBuffer> buffers = new ArrayList<>();        buffers.addAll(stream.sliceBuffers(available));        ByteBuffer lastBuffer = ByteBuffer.allocate(missingBytes);        f.readFully(lastBuffer);        buffers.add(lastBuffer);        return BytesInput.from(buffers);    }    return super.readAsBytesInput(size);}
1
public int hashCode()
{    return col.hashCode();}
0
public boolean equals(Object obj)
{    if (this == obj) {        return true;    } else if (obj instanceof ChunkDescriptor) {        return col.equals(((ChunkDescriptor) obj).col);    } else {        return false;    }}
0
public void addChunk(ChunkDescriptor descriptor)
{    chunks.add(descriptor);    length += descriptor.size;}
0
public void readAll(SeekableInputStream f, ChunkListBuilder builder) throws IOException
{    List<Chunk> result = new ArrayList<Chunk>(chunks.size());    f.seek(offset);    int fullAllocations = length / options.getMaxAllocationSize();    int lastAllocationSize = length % options.getMaxAllocationSize();    int numAllocations = fullAllocations + (lastAllocationSize > 0 ? 1 : 0);    List<ByteBuffer> buffers = new ArrayList<>(numAllocations);    for (int i = 0; i < fullAllocations; i += 1) {        buffers.add(options.getAllocator().allocate(options.getMaxAllocationSize()));    }    if (lastAllocationSize > 0) {        buffers.add(options.getAllocator().allocate(lastAllocationSize));    }    for (ByteBuffer buffer : buffers) {        f.readFully(buffer);        buffer.flip();    }        BenchmarkCounter.incrementBytesRead(length);    ByteBufferInputStream stream = ByteBufferInputStream.wrap(buffers);    for (int i = 0; i < chunks.size(); i++) {        ChunkDescriptor descriptor = chunks.get(i);        builder.add(descriptor, stream.sliceBuffers(descriptor.size), f);    }}
0
public long endPos()
{    return offset + length;}
0
 STATE start() throws IOException
{    return error();}
0
 STATE startBlock() throws IOException
{    return error();}
0
 STATE startColumn() throws IOException
{    return error();}
0
 STATE write() throws IOException
{    return error();}
0
 STATE endColumn() throws IOException
{    return error();}
0
 STATE endBlock() throws IOException
{    return error();}
0
 STATE end() throws IOException
{    return error();}
0
private final STATE error() throws IOException
{    throw new IOException("The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: " + this.name());}
0
 STATE start()
{    return STARTED;}
0
 STATE startBlock()
{    return BLOCK;}
0
 STATE end()
{    return ENDED;}
0
 STATE startColumn()
{    return COLUMN;}
0
 STATE endBlock()
{    return STARTED;}
0
 STATE endColumn()
{    return BLOCK;}
0
 STATE write()
{    return this;}
0
public void start() throws IOException
{    state = state.start();        out.write(MAGIC);}
1
public void startBlock(long recordCount) throws IOException
{    state = state.startBlock();            alignment.alignForRowGroup(out);    currentBlock = new BlockMetaData();    currentRecordCount = recordCount;    currentColumnIndexes = new ArrayList<>();    currentOffsetIndexes = new ArrayList<>();}
1
public void startColumn(ColumnDescriptor descriptor, long valueCount, CompressionCodecName compressionCodecName) throws IOException
{    state = state.startColumn();    encodingStatsBuilder.clear();    currentEncodings = new HashSet<Encoding>();    currentChunkPath = ColumnPath.get(descriptor.getPath());    currentChunkType = descriptor.getPrimitiveType();    currentChunkCodec = compressionCodecName;    currentChunkValueCount = valueCount;    currentChunkFirstDataPage = out.getPos();    compressedLength = 0;    uncompressedLength = 0;        currentStatistics = null;    columnIndexBuilder = ColumnIndexBuilder.getBuilder(currentChunkType, columnIndexTruncateLength);    offsetIndexBuilder = OffsetIndexBuilder.getBuilder();    firstPageOffset = -1;}
0
public void writeDictionaryPage(DictionaryPage dictionaryPage) throws IOException
{    state = state.write();        currentChunkDictionaryPageOffset = out.getPos();    int uncompressedSize = dictionaryPage.getUncompressedSize();        int compressedPageSize = (int) dictionaryPage.getBytes().size();    if (pageWriteChecksumEnabled) {        crc.reset();        crc.update(dictionaryPage.getBytes().toByteArray());        metadataConverter.writeDictionaryPageHeader(uncompressedSize, compressedPageSize, dictionaryPage.getDictionarySize(), dictionaryPage.getEncoding(), (int) crc.getValue(), out);    } else {        metadataConverter.writeDictionaryPageHeader(uncompressedSize, compressedPageSize, dictionaryPage.getDictionarySize(), dictionaryPage.getEncoding(), out);    }    long headerSize = out.getPos() - currentChunkDictionaryPageOffset;    this.uncompressedLength += uncompressedSize + headerSize;    this.compressedLength += compressedPageSize + headerSize;        dictionaryPage.getBytes().writeAllTo(out);    encodingStatsBuilder.addDictEncoding(dictionaryPage.getEncoding());    currentEncodings.add(dictionaryPage.getEncoding());}
1
public void writeDataPage(int valueCount, int uncompressedPageSize, BytesInput bytes, Encoding rlEncoding, Encoding dlEncoding, Encoding valuesEncoding) throws IOException
{    state = state.write();        offsetIndexBuilder = OffsetIndexBuilder.getNoOpBuilder();    columnIndexBuilder = ColumnIndexBuilder.getNoOpBuilder();    long beforeHeader = out.getPos();        int compressedPageSize = (int) bytes.size();    metadataConverter.writeDataPageV1Header(uncompressedPageSize, compressedPageSize, valueCount, rlEncoding, dlEncoding, valuesEncoding, out);    long headerSize = out.getPos() - beforeHeader;    this.uncompressedLength += uncompressedPageSize + headerSize;    this.compressedLength += compressedPageSize + headerSize;        bytes.writeAllTo(out);    encodingStatsBuilder.addDataEncoding(valuesEncoding);    currentEncodings.add(rlEncoding);    currentEncodings.add(dlEncoding);    currentEncodings.add(valuesEncoding);}
1
public void writeDataPage(int valueCount, int uncompressedPageSize, BytesInput bytes, Statistics statistics, Encoding rlEncoding, Encoding dlEncoding, Encoding valuesEncoding) throws IOException
{        offsetIndexBuilder = OffsetIndexBuilder.getNoOpBuilder();    columnIndexBuilder = ColumnIndexBuilder.getNoOpBuilder();    innerWriteDataPage(valueCount, uncompressedPageSize, bytes, statistics, rlEncoding, dlEncoding, valuesEncoding);}
0
public void writeDataPage(int valueCount, int uncompressedPageSize, BytesInput bytes, Statistics statistics, long rowCount, Encoding rlEncoding, Encoding dlEncoding, Encoding valuesEncoding) throws IOException
{    long beforeHeader = out.getPos();    innerWriteDataPage(valueCount, uncompressedPageSize, bytes, statistics, rlEncoding, dlEncoding, valuesEncoding);    offsetIndexBuilder.add((int) (out.getPos() - beforeHeader), rowCount);}
0
private void innerWriteDataPage(int valueCount, int uncompressedPageSize, BytesInput bytes, Statistics statistics, Encoding rlEncoding, Encoding dlEncoding, Encoding valuesEncoding) throws IOException
{    state = state.write();    long beforeHeader = out.getPos();    if (firstPageOffset == -1) {        firstPageOffset = beforeHeader;    }        int compressedPageSize = (int) bytes.size();    if (pageWriteChecksumEnabled) {        crc.reset();        crc.update(bytes.toByteArray());        metadataConverter.writeDataPageV1Header(uncompressedPageSize, compressedPageSize, valueCount, rlEncoding, dlEncoding, valuesEncoding, (int) crc.getValue(), out);    } else {        metadataConverter.writeDataPageV1Header(uncompressedPageSize, compressedPageSize, valueCount, rlEncoding, dlEncoding, valuesEncoding, out);    }    long headerSize = out.getPos() - beforeHeader;    this.uncompressedLength += uncompressedPageSize + headerSize;    this.compressedLength += compressedPageSize + headerSize;        bytes.writeAllTo(out);        if (currentStatistics == null) {        currentStatistics = statistics.copy();    } else {        currentStatistics.mergeStatistics(statistics);    }    columnIndexBuilder.add(statistics);    encodingStatsBuilder.addDataEncoding(valuesEncoding);    currentEncodings.add(rlEncoding);    currentEncodings.add(dlEncoding);    currentEncodings.add(valuesEncoding);}
1
 void writeColumnChunk(ColumnDescriptor descriptor, long valueCount, CompressionCodecName compressionCodecName, DictionaryPage dictionaryPage, BytesInput bytes, long uncompressedTotalPageSize, long compressedTotalPageSize, Statistics<?> totalStats, ColumnIndexBuilder columnIndexBuilder, OffsetIndexBuilder offsetIndexBuilder, Set<Encoding> rlEncodings, Set<Encoding> dlEncodings, List<Encoding> dataEncodings) throws IOException
{    startColumn(descriptor, valueCount, compressionCodecName);    state = state.write();    if (dictionaryPage != null) {        writeDictionaryPage(dictionaryPage);    }        long headersSize = bytes.size() - compressedTotalPageSize;    this.uncompressedLength += uncompressedTotalPageSize + headersSize;    this.compressedLength += compressedTotalPageSize + headersSize;        firstPageOffset = out.getPos();    bytes.writeAllTo(out);    encodingStatsBuilder.addDataEncodings(dataEncodings);    if (rlEncodings.isEmpty()) {        encodingStatsBuilder.withV2Pages();    }    currentEncodings.addAll(rlEncodings);    currentEncodings.addAll(dlEncodings);    currentEncodings.addAll(dataEncodings);    currentStatistics = totalStats;    this.columnIndexBuilder = columnIndexBuilder;    this.offsetIndexBuilder = offsetIndexBuilder;    endColumn();}
1
public void endColumn() throws IOException
{    state = state.endColumn();        if (columnIndexBuilder.getMinMaxSize() > columnIndexBuilder.getPageCount() * MAX_STATS_SIZE) {        currentColumnIndexes.add(null);    } else {        currentColumnIndexes.add(columnIndexBuilder.build());    }    currentOffsetIndexes.add(offsetIndexBuilder.build(firstPageOffset));    currentBlock.addColumn(ColumnChunkMetaData.get(currentChunkPath, currentChunkType, currentChunkCodec, encodingStatsBuilder.build(), currentEncodings, currentStatistics, currentChunkFirstDataPage, currentChunkDictionaryPageOffset, currentChunkValueCount, compressedLength, uncompressedLength));    this.currentBlock.setTotalByteSize(currentBlock.getTotalByteSize() + uncompressedLength);    this.uncompressedLength = 0;    this.compressedLength = 0;    columnIndexBuilder = null;    offsetIndexBuilder = null;}
1
public void endBlock() throws IOException
{    state = state.endBlock();        currentBlock.setRowCount(currentRecordCount);    blocks.add(currentBlock);    columnIndexes.add(currentColumnIndexes);    offsetIndexes.add(currentOffsetIndexes);    currentColumnIndexes = null;    currentOffsetIndexes = null;    currentBlock = null;}
1
public void appendFile(Configuration conf, Path file) throws IOException
{    ParquetFileReader.open(conf, file).appendTo(this);}
0
public void appendFile(InputFile file) throws IOException
{    try (ParquetFileReader reader = ParquetFileReader.open(file)) {        reader.appendTo(this);    }}
0
public void appendRowGroups(FSDataInputStream file, List<BlockMetaData> rowGroups, boolean dropColumns) throws IOException
{    appendRowGroups(HadoopStreams.wrap(file), rowGroups, dropColumns);}
0
public void appendRowGroups(SeekableInputStream file, List<BlockMetaData> rowGroups, boolean dropColumns) throws IOException
{    for (BlockMetaData block : rowGroups) {        appendRowGroup(file, block, dropColumns);    }}
0
public void appendRowGroup(FSDataInputStream from, BlockMetaData rowGroup, boolean dropColumns) throws IOException
{    appendRowGroup(HadoopStreams.wrap(from), rowGroup, dropColumns);}
0
public void appendRowGroup(SeekableInputStream from, BlockMetaData rowGroup, boolean dropColumns) throws IOException
{    startBlock(rowGroup.getRowCount());    Map<String, ColumnChunkMetaData> columnsToCopy = new HashMap<String, ColumnChunkMetaData>();    for (ColumnChunkMetaData chunk : rowGroup.getColumns()) {        columnsToCopy.put(chunk.getPath().toDotString(), chunk);    }    List<ColumnChunkMetaData> columnsInOrder = new ArrayList<ColumnChunkMetaData>();    for (ColumnDescriptor descriptor : schema.getColumns()) {        String path = ColumnPath.get(descriptor.getPath()).toDotString();        ColumnChunkMetaData chunk = columnsToCopy.remove(path);        if (chunk != null) {            columnsInOrder.add(chunk);        } else {            throw new IllegalArgumentException(String.format("Missing column '%s', cannot copy row group: %s", path, rowGroup));        }    }        if (!dropColumns && !columnsToCopy.isEmpty()) {        throw new IllegalArgumentException(String.format("Columns cannot be copied (missing from target schema): %s", Strings.join(columnsToCopy.keySet(), ", ")));    }        long start = -1;    long length = 0;    long blockUncompressedSize = 0L;    for (int i = 0; i < columnsInOrder.size(); i += 1) {        ColumnChunkMetaData chunk = columnsInOrder.get(i);                long newChunkStart = out.getPos() + length;                if (start < 0) {                        start = chunk.getStartingPos();        }        length += chunk.getTotalSize();        if ((i + 1) == columnsInOrder.size() || columnsInOrder.get(i + 1).getStartingPos() != (start + length)) {                        copy(from, out, start, length);                        start = -1;            length = 0;        }                        currentColumnIndexes.add(null);        currentOffsetIndexes.add(null);        currentBlock.addColumn(ColumnChunkMetaData.get(chunk.getPath(), chunk.getPrimitiveType(), chunk.getCodec(), chunk.getEncodingStats(), chunk.getEncodings(), chunk.getStatistics(), newChunkStart, newChunkStart, chunk.getValueCount(), chunk.getTotalSize(), chunk.getTotalUncompressedSize()));        blockUncompressedSize += chunk.getTotalUncompressedSize();    }    currentBlock.setTotalByteSize(blockUncompressedSize);    endBlock();}
0
private static void copy(SeekableInputStream from, PositionOutputStream to, long start, long length) throws IOException
{        from.seek(start);    long bytesCopied = 0;    byte[] buffer = COPY_BUFFER.get();    while (bytesCopied < length) {        long bytesLeft = length - bytesCopied;        int bytesRead = from.read(buffer, 0, (buffer.length < bytesLeft ? buffer.length : (int) bytesLeft));        if (bytesRead < 0) {            throw new IllegalArgumentException("Unexpected end of input file at " + start + bytesCopied);        }        to.write(buffer, 0, bytesRead);        bytesCopied += bytesRead;    }}
1
public void end(Map<String, String> extraMetaData) throws IOException
{    state = state.end();    serializeColumnIndexes(columnIndexes, blocks, out);    serializeOffsetIndexes(offsetIndexes, blocks, out);        this.footer = new ParquetMetadata(new FileMetaData(schema, extraMetaData, Version.FULL_VERSION), blocks);    serializeFooter(footer, out);    out.close();}
1
private static void serializeColumnIndexes(List<List<ColumnIndex>> columnIndexes, List<BlockMetaData> blocks, PositionOutputStream out) throws IOException
{        for (int bIndex = 0, bSize = blocks.size(); bIndex < bSize; ++bIndex) {        List<ColumnChunkMetaData> columns = blocks.get(bIndex).getColumns();        List<ColumnIndex> blockColumnIndexes = columnIndexes.get(bIndex);        for (int cIndex = 0, cSize = columns.size(); cIndex < cSize; ++cIndex) {            ColumnChunkMetaData column = columns.get(cIndex);            org.apache.parquet.format.ColumnIndex columnIndex = ParquetMetadataConverter.toParquetColumnIndex(column.getPrimitiveType(), blockColumnIndexes.get(cIndex));            if (columnIndex == null) {                continue;            }            long offset = out.getPos();            Util.writeColumnIndex(columnIndex, out);            column.setColumnIndexReference(new IndexReference(offset, (int) (out.getPos() - offset)));        }    }}
1
private static void serializeOffsetIndexes(List<List<OffsetIndex>> offsetIndexes, List<BlockMetaData> blocks, PositionOutputStream out) throws IOException
{        for (int bIndex = 0, bSize = blocks.size(); bIndex < bSize; ++bIndex) {        List<ColumnChunkMetaData> columns = blocks.get(bIndex).getColumns();        List<OffsetIndex> blockOffsetIndexes = offsetIndexes.get(bIndex);        for (int cIndex = 0, cSize = columns.size(); cIndex < cSize; ++cIndex) {            OffsetIndex offsetIndex = blockOffsetIndexes.get(cIndex);            if (offsetIndex == null) {                continue;            }            ColumnChunkMetaData column = columns.get(cIndex);            long offset = out.getPos();            Util.writeOffsetIndex(ParquetMetadataConverter.toParquetOffsetIndex(offsetIndex), out);            column.setOffsetIndexReference(new IndexReference(offset, (int) (out.getPos() - offset)));        }    }}
1
private static void serializeFooter(ParquetMetadata footer, PositionOutputStream out) throws IOException
{    long footerIndex = out.getPos();    org.apache.parquet.format.FileMetaData parquetMetadata = metadataConverter.toParquetMetadata(CURRENT_VERSION, footer);    writeFileMetaData(parquetMetadata, out);        BytesUtils.writeIntLittleEndian(out, (int) (out.getPos() - footerIndex));    out.write(MAGIC);}
1
public ParquetMetadata getFooter()
{    Preconditions.checkState(state == STATE.ENDED, "Cannot return unfinished footer.");    return footer;}
0
public static ParquetMetadata mergeMetadataFiles(List<Path> files, Configuration conf) throws IOException
{    Preconditions.checkArgument(!files.isEmpty(), "Cannot merge an empty list of metadata");    GlobalMetaData globalMetaData = null;    List<BlockMetaData> blocks = new ArrayList<BlockMetaData>();    for (Path p : files) {        ParquetMetadata pmd = ParquetFileReader.readFooter(conf, p, ParquetMetadataConverter.NO_FILTER);        FileMetaData fmd = pmd.getFileMetaData();        globalMetaData = mergeInto(fmd, globalMetaData, true);        blocks.addAll(pmd.getBlocks());    }        return new ParquetMetadata(globalMetaData.merge(), blocks);}
0
public static void writeMergedMetadataFile(List<Path> files, Path outputPath, Configuration conf) throws IOException
{    ParquetMetadata merged = mergeMetadataFiles(files, conf);    writeMetadataFile(outputPath, merged, outputPath.getFileSystem(conf));}
0
public static void writeMetadataFile(Configuration configuration, Path outputPath, List<Footer> footers) throws IOException
{    writeMetadataFile(configuration, outputPath, footers, JobSummaryLevel.ALL);}
0
public static void writeMetadataFile(Configuration configuration, Path outputPath, List<Footer> footers, JobSummaryLevel level) throws IOException
{    Preconditions.checkArgument(level == JobSummaryLevel.ALL || level == JobSummaryLevel.COMMON_ONLY, "Unsupported level: " + level);    FileSystem fs = outputPath.getFileSystem(configuration);    outputPath = outputPath.makeQualified(fs);    ParquetMetadata metadataFooter = mergeFooters(outputPath, footers);    if (level == JobSummaryLevel.ALL) {        writeMetadataFile(outputPath, metadataFooter, fs, PARQUET_METADATA_FILE);    }    metadataFooter.getBlocks().clear();    writeMetadataFile(outputPath, metadataFooter, fs, PARQUET_COMMON_METADATA_FILE);}
0
private static void writeMetadataFile(Path outputPathRoot, ParquetMetadata metadataFooter, FileSystem fs, String parquetMetadataFile) throws IOException
{    Path metaDataPath = new Path(outputPathRoot, parquetMetadataFile);    writeMetadataFile(metaDataPath, metadataFooter, fs);}
0
private static void writeMetadataFile(Path outputPath, ParquetMetadata metadataFooter, FileSystem fs) throws IOException
{    PositionOutputStream metadata = HadoopStreams.wrap(fs.create(outputPath));    metadata.write(MAGIC);    serializeFooter(metadataFooter, metadata);    metadata.close();}
0
 static ParquetMetadata mergeFooters(Path root, List<Footer> footers)
{    String rootPath = root.toUri().getPath();    GlobalMetaData fileMetaData = null;    List<BlockMetaData> blocks = new ArrayList<BlockMetaData>();    for (Footer footer : footers) {        String footerPath = footer.getFile().toUri().getPath();        if (!footerPath.startsWith(rootPath)) {            throw new ParquetEncodingException(footerPath + " invalid: all the files must be contained in the root " + root);        }        footerPath = footerPath.substring(rootPath.length());        while (footerPath.startsWith("/")) {            footerPath = footerPath.substring(1);        }        fileMetaData = mergeInto(footer.getParquetMetadata().getFileMetaData(), fileMetaData);        for (BlockMetaData block : footer.getParquetMetadata().getBlocks()) {            block.setPath(footerPath);            blocks.add(block);        }    }    return new ParquetMetadata(fileMetaData.merge(), blocks);}
0
public long getPos() throws IOException
{    return out.getPos();}
0
public long getNextRowGroupSize() throws IOException
{    return alignment.nextRowGroupSize(out);}
0
 static GlobalMetaData getGlobalMetaData(List<Footer> footers)
{    return getGlobalMetaData(footers, true);}
0
 static GlobalMetaData getGlobalMetaData(List<Footer> footers, boolean strict)
{    GlobalMetaData fileMetaData = null;    for (Footer footer : footers) {        ParquetMetadata currentMetadata = footer.getParquetMetadata();        fileMetaData = mergeInto(currentMetadata.getFileMetaData(), fileMetaData, strict);    }    return fileMetaData;}
0
 static GlobalMetaData mergeInto(FileMetaData toMerge, GlobalMetaData mergedMetadata)
{    return mergeInto(toMerge, mergedMetadata, true);}
0
 static GlobalMetaData mergeInto(FileMetaData toMerge, GlobalMetaData mergedMetadata, boolean strict)
{    MessageType schema = null;    Map<String, Set<String>> newKeyValues = new HashMap<String, Set<String>>();    Set<String> createdBy = new HashSet<String>();    if (mergedMetadata != null) {        schema = mergedMetadata.getSchema();        newKeyValues.putAll(mergedMetadata.getKeyValueMetaData());        createdBy.addAll(mergedMetadata.getCreatedBy());    }    if ((schema == null && toMerge.getSchema() != null) || (schema != null && !schema.equals(toMerge.getSchema()))) {        schema = mergeInto(toMerge.getSchema(), schema, strict);    }    for (Entry<String, String> entry : toMerge.getKeyValueMetaData().entrySet()) {        Set<String> values = newKeyValues.get(entry.getKey());        if (values == null) {            values = new LinkedHashSet<String>();            newKeyValues.put(entry.getKey(), values);        }        values.add(entry.getValue());    }    createdBy.add(toMerge.getCreatedBy());    return new GlobalMetaData(schema, newKeyValues, createdBy);}
0
 static MessageType mergeInto(MessageType toMerge, MessageType mergedSchema)
{    return mergeInto(toMerge, mergedSchema, true);}
0
 static MessageType mergeInto(MessageType toMerge, MessageType mergedSchema, boolean strict)
{    if (mergedSchema == null) {        return toMerge;    }    return mergedSchema.union(toMerge, strict);}
0
public static NoAlignment get(long rowGroupSize)
{    return new NoAlignment(rowGroupSize);}
0
public void alignForRowGroup(PositionOutputStream out)
{}
0
public long nextRowGroupSize(PositionOutputStream out)
{    return rowGroupSize;}
0
public static PaddingAlignment get(long dfsBlockSize, long rowGroupSize, int maxPaddingSize)
{    return new PaddingAlignment(dfsBlockSize, rowGroupSize, maxPaddingSize);}
0
public void alignForRowGroup(PositionOutputStream out) throws IOException
{    long remaining = dfsBlockSize - (out.getPos() % dfsBlockSize);    if (isPaddingNeeded(remaining)) {                for (; remaining > 0; remaining -= zeros.length) {            out.write(zeros, 0, (int) Math.min((long) zeros.length, remaining));        }    }}
1
public long nextRowGroupSize(PositionOutputStream out) throws IOException
{    if (maxPaddingSize <= 0) {        return rowGroupSize;    }    long remaining = dfsBlockSize - (out.getPos() % dfsBlockSize);    if (isPaddingNeeded(remaining)) {        return rowGroupSize;    }    return Math.min(remaining, rowGroupSize);}
0
protected boolean isPaddingNeeded(long remaining)
{    return (remaining <= maxPaddingSize);}
0
public static void setTaskSideMetaData(Job job, boolean taskSideMetadata)
{    ContextUtil.getConfiguration(job).setBoolean(TASK_SIDE_METADATA, taskSideMetadata);}
0
public static boolean isTaskSideMetaData(Configuration configuration)
{    return configuration.getBoolean(TASK_SIDE_METADATA, TRUE);}
0
public static void setReadSupportClass(Job job, Class<?> readSupportClass)
{    ContextUtil.getConfiguration(job).set(READ_SUPPORT_CLASS, readSupportClass.getName());}
0
public static void setUnboundRecordFilter(Job job, Class<? extends UnboundRecordFilter> filterClass)
{    Configuration conf = ContextUtil.getConfiguration(job);    checkArgument(getFilterPredicate(conf) == null, "You cannot provide an UnboundRecordFilter after providing a FilterPredicate");    conf.set(UNBOUND_RECORD_FILTER, filterClass.getName());}
0
public static Class<?> getUnboundRecordFilter(Configuration configuration)
{    return ConfigurationUtil.getClassFromConfig(configuration, UNBOUND_RECORD_FILTER, UnboundRecordFilter.class);}
0
private static UnboundRecordFilter getUnboundRecordFilterInstance(Configuration configuration)
{    Class<?> clazz = ConfigurationUtil.getClassFromConfig(configuration, UNBOUND_RECORD_FILTER, UnboundRecordFilter.class);    if (clazz == null) {        return null;    }    try {        UnboundRecordFilter unboundRecordFilter = (UnboundRecordFilter) clazz.newInstance();        if (unboundRecordFilter instanceof Configurable) {            ((Configurable) unboundRecordFilter).setConf(configuration);        }        return unboundRecordFilter;    } catch (InstantiationException e) {        throw new BadConfigurationException("could not instantiate unbound record filter class", e);    } catch (IllegalAccessException e) {        throw new BadConfigurationException("could not instantiate unbound record filter class", e);    }}
0
public static void setReadSupportClass(JobConf conf, Class<?> readSupportClass)
{    conf.set(READ_SUPPORT_CLASS, readSupportClass.getName());}
0
public static Class<?> getReadSupportClass(Configuration configuration)
{    return ConfigurationUtil.getClassFromConfig(configuration, READ_SUPPORT_CLASS, ReadSupport.class);}
0
public static void setFilterPredicate(Configuration configuration, FilterPredicate filterPredicate)
{    checkArgument(getUnboundRecordFilter(configuration) == null, "You cannot provide a FilterPredicate after providing an UnboundRecordFilter");    configuration.set(FILTER_PREDICATE + ".human.readable", filterPredicate.toString());    try {        SerializationUtil.writeObjectToConfAsBase64(FILTER_PREDICATE, filterPredicate, configuration);    } catch (IOException e) {        throw new RuntimeException(e);    }}
0
private static FilterPredicate getFilterPredicate(Configuration configuration)
{    try {        return SerializationUtil.readObjectFromConfAsBase64(FILTER_PREDICATE, configuration);    } catch (IOException e) {        throw new RuntimeException(e);    }}
0
public static Filter getFilter(Configuration conf)
{    return FilterCompat.get(getFilterPredicate(conf), getUnboundRecordFilterInstance(conf));}
0
public RecordReader<Void, T> createRecordReader(InputSplit inputSplit, TaskAttemptContext taskAttemptContext) throws IOException, InterruptedException
{    Configuration conf = ContextUtil.getConfiguration(taskAttemptContext);    ReadSupport<T> readSupport = getReadSupport(conf);    return new ParquetRecordReader<T>(readSupport, getFilter(conf));}
0
 ReadSupport<T> getReadSupport(Configuration configuration)
{    return getReadSupportInstance(readSupportClass == null ? (Class<? extends ReadSupport<T>>) getReadSupportClass(configuration) : readSupportClass);}
0
public static ReadSupport<T> getReadSupportInstance(Configuration configuration)
{    return getReadSupportInstance((Class<? extends ReadSupport<T>>) getReadSupportClass(configuration));}
0
 static ReadSupport<T> getReadSupportInstance(Class<? extends ReadSupport<T>> readSupportClass)
{    try {        return readSupportClass.newInstance();    } catch (InstantiationException e) {        throw new BadConfigurationException("could not instantiate read support class", e);    } catch (IllegalAccessException e) {        throw new BadConfigurationException("could not instantiate read support class", e);    }}
0
protected boolean isSplitable(JobContext context, Path filename)
{    return ContextUtil.getConfiguration(context).getBoolean(SPLIT_FILES, true);}
0
public List<InputSplit> getSplits(JobContext jobContext) throws IOException
{    Configuration configuration = ContextUtil.getConfiguration(jobContext);    List<InputSplit> splits = new ArrayList<InputSplit>();    if (isTaskSideMetaData(configuration)) {                for (InputSplit split : super.getSplits(jobContext)) {            Preconditions.checkArgument(split instanceof FileSplit, "Cannot wrap non-FileSplit: " + split);            splits.add(ParquetInputSplit.from((FileSplit) split));        }        return splits;    } else {        splits.addAll(getSplits(configuration, getFooters(jobContext)));    }    return splits;}
0
public List<ParquetInputSplit> getSplits(Configuration configuration, List<Footer> footers) throws IOException
{    boolean strictTypeChecking = configuration.getBoolean(STRICT_TYPE_CHECKING, true);    final long maxSplitSize = configuration.getLong("mapred.max.split.size", Long.MAX_VALUE);    final long minSplitSize = Math.max(getFormatMinSplitSize(), configuration.getLong("mapred.min.split.size", 0L));    if (maxSplitSize < 0 || minSplitSize < 0) {        throw new ParquetDecodingException("maxSplitSize or minSplitSize should not be negative: maxSplitSize = " + maxSplitSize + "; minSplitSize = " + minSplitSize);    }    GlobalMetaData globalMetaData = ParquetFileWriter.getGlobalMetaData(footers, strictTypeChecking);    ReadContext readContext = getReadSupport(configuration).init(new InitContext(configuration, globalMetaData.getKeyValueMetaData(), globalMetaData.getSchema()));    return new ClientSideMetadataSplitStrategy().getSplits(configuration, footers, maxSplitSize, minSplitSize, readContext);}
0
protected List<FileStatus> listStatus(JobContext jobContext) throws IOException
{    return getAllFileRecursively(super.listStatus(jobContext), ContextUtil.getConfiguration(jobContext));}
0
private static List<FileStatus> getAllFileRecursively(List<FileStatus> files, Configuration conf) throws IOException
{    List<FileStatus> result = new ArrayList<FileStatus>();    for (FileStatus file : files) {        if (file.isDir()) {            Path p = file.getPath();            FileSystem fs = p.getFileSystem(conf);            staticAddInputPathRecursively(result, fs, p, HiddenFileFilter.INSTANCE);        } else {            result.add(file);        }    }        return result;}
1
private static void staticAddInputPathRecursively(List<FileStatus> result, FileSystem fs, Path path, PathFilter inputFilter) throws IOException
{    for (FileStatus stat : fs.listStatus(path, inputFilter)) {        if (stat.isDir()) {            staticAddInputPathRecursively(result, fs, stat.getPath(), inputFilter);        } else {            result.add(stat);        }    }}
0
public List<Footer> getFooters(JobContext jobContext) throws IOException
{    List<FileStatus> statuses = listStatus(jobContext);    if (statuses.isEmpty()) {        return Collections.emptyList();    }    Configuration config = ContextUtil.getConfiguration(jobContext);            Map<FileStatusWrapper, Footer> footersMap = new LinkedHashMap<FileStatusWrapper, Footer>();    Set<FileStatus> missingStatuses = new HashSet<FileStatus>();    Map<Path, FileStatusWrapper> missingStatusesMap = new HashMap<Path, FileStatusWrapper>(missingStatuses.size());    if (footersCache == null) {        footersCache = new LruCache<FileStatusWrapper, FootersCacheValue>(Math.max(statuses.size(), MIN_FOOTER_CACHE_SIZE));    }    for (FileStatus status : statuses) {        FileStatusWrapper statusWrapper = new FileStatusWrapper(status);        FootersCacheValue cacheEntry = footersCache.getCurrentValue(statusWrapper);        if (LOG.isDebugEnabled()) {                    }        if (cacheEntry != null) {            footersMap.put(statusWrapper, cacheEntry.getFooter());        } else {            footersMap.put(statusWrapper, null);            missingStatuses.add(status);            missingStatusesMap.put(status.getPath(), statusWrapper);        }    }        if (!missingStatuses.isEmpty()) {        List<Footer> newFooters = getFooters(config, missingStatuses);        for (Footer newFooter : newFooters) {                                                            FileStatusWrapper fileStatus = missingStatusesMap.get(newFooter.getFile());            footersCache.put(fileStatus, new FootersCacheValue(fileStatus, newFooter));        }    }    List<Footer> footers = new ArrayList<Footer>(statuses.size());    for (Entry<FileStatusWrapper, Footer> footerEntry : footersMap.entrySet()) {        Footer footer = footerEntry.getValue();        if (footer == null) {                        footers.add(footersCache.getCurrentValue(footerEntry.getKey()).getFooter());        } else {            footers.add(footer);        }    }    return footers;}
1
public List<Footer> getFooters(Configuration configuration, List<FileStatus> statuses) throws IOException
{    return getFooters(configuration, (Collection<FileStatus>) statuses);}
0
public List<Footer> getFooters(Configuration configuration, Collection<FileStatus> statuses) throws IOException
{        boolean taskSideMetaData = isTaskSideMetaData(configuration);    return ParquetFileReader.readAllFootersInParallelUsingSummaryFiles(configuration, statuses, taskSideMetaData);}
1
public GlobalMetaData getGlobalMetaData(JobContext jobContext) throws IOException
{    return ParquetFileWriter.getGlobalMetaData(getFooters(jobContext));}
0
public boolean isCurrent(FileStatusWrapper key)
{    long currentModTime = key.getModificationTime();    boolean isCurrent = modificationTime >= currentModTime;    if (LOG.isDebugEnabled() && !isCurrent) {            }    return isCurrent;}
1
public Footer getFooter()
{    return footer;}
0
public boolean isNewerThan(FootersCacheValue otherValue)
{    return otherValue == null || modificationTime > otherValue.modificationTime;}
0
public Path getPath()
{    return footer.getFile();}
0
public long getModificationTime()
{    return status.getModificationTime();}
0
public int hashCode()
{    return status.hashCode();}
0
public boolean equals(Object other)
{    return other instanceof FileStatusWrapper && status.equals(((FileStatusWrapper) other).status);}
0
public String toString()
{    return status.getPath().toString();}
0
public int compare(BlockLocation b1, BlockLocation b2)
{    return Long.signum(b1.getOffset() - b2.getOffset());}
0
private long getHDFSBlockEndingPosition(int hdfsBlockIndex)
{    BlockLocation hdfsBlock = hdfsBlocks[hdfsBlockIndex];    return hdfsBlock.getOffset() + hdfsBlock.getLength() - 1;}
0
private boolean checkBelongingToANewHDFSBlock(BlockMetaData rowGroupMetadata)
{    boolean isNewHdfsBlock = false;    long rowGroupMidPoint = rowGroupMetadata.getStartingPos() + (rowGroupMetadata.getCompressedSize() / 2);        while (rowGroupMidPoint > getHDFSBlockEndingPosition(currentMidPointHDFSBlockIndex)) {        isNewHdfsBlock = true;        currentMidPointHDFSBlockIndex++;        if (currentMidPointHDFSBlockIndex >= hdfsBlocks.length)            throw new ParquetDecodingException("the row group is not in hdfs blocks in the file: midpoint of row groups is " + rowGroupMidPoint + ", the end of the hdfs block is " + getHDFSBlockEndingPosition(currentMidPointHDFSBlockIndex - 1));    }    while (rowGroupMetadata.getStartingPos() > getHDFSBlockEndingPosition(currentStartHdfsBlockIndex)) {        currentStartHdfsBlockIndex++;        if (currentStartHdfsBlockIndex >= hdfsBlocks.length)            throw new ParquetDecodingException("The row group does not start in this file: row group offset is " + rowGroupMetadata.getStartingPos() + " but the end of hdfs blocks of file is " + getHDFSBlockEndingPosition(currentStartHdfsBlockIndex));    }    return isNewHdfsBlock;}
0
public BlockLocation getCurrentBlock()
{    return hdfsBlocks[currentStartHdfsBlockIndex];}
0
private void addRowGroup(BlockMetaData rowGroup)
{    this.rowGroups.add(rowGroup);    this.compressedByteSize += rowGroup.getCompressedSize();}
0
public long getCompressedByteSize()
{    return compressedByteSize;}
0
public List<BlockMetaData> getRowGroups()
{    return rowGroups;}
0
 int getRowGroupCount()
{    return rowGroups.size();}
0
public ParquetInputSplit getParquetInputSplit(FileStatus fileStatus, String requestedSchema, Map<String, String> readSupportMetadata) throws IOException
{    MessageType requested = MessageTypeParser.parseMessageType(requestedSchema);    long length = 0;    for (BlockMetaData block : this.getRowGroups()) {        List<ColumnChunkMetaData> columns = block.getColumns();        for (ColumnChunkMetaData column : columns) {            if (requested.containsPath(column.getPath().toArray())) {                length += column.getTotalSize();            }        }    }    BlockMetaData lastRowGroup = this.getRowGroups().get(this.getRowGroupCount() - 1);    long end = lastRowGroup.getStartingPos() + lastRowGroup.getTotalByteSize();    long[] rowGroupOffsets = new long[this.getRowGroupCount()];    for (int i = 0; i < rowGroupOffsets.length; i++) {        rowGroupOffsets[i] = this.getRowGroups().get(i).getStartingPos();    }    return new ParquetInputSplit(fileStatus.getPath(), hdfsBlock.getOffset(), end, length, hdfsBlock.getHosts(), rowGroupOffsets);}
0
 List<ParquetInputSplit> getSplits(Configuration configuration, List<Footer> footers, long maxSplitSize, long minSplitSize, ReadContext readContext) throws IOException
{    List<ParquetInputSplit> splits = new ArrayList<ParquetInputSplit>();    Filter filter = ParquetInputFormat.getFilter(configuration);    long rowGroupsDropped = 0;    long totalRowGroups = 0;    for (Footer footer : footers) {        final Path file = footer.getFile();                FileSystem fs = file.getFileSystem(configuration);        FileStatus fileStatus = fs.getFileStatus(file);        ParquetMetadata parquetMetaData = footer.getParquetMetadata();        List<BlockMetaData> blocks = parquetMetaData.getBlocks();        List<BlockMetaData> filteredBlocks;        totalRowGroups += blocks.size();        filteredBlocks = RowGroupFilter.filterRowGroups(filter, blocks, parquetMetaData.getFileMetaData().getSchema());        rowGroupsDropped += blocks.size() - filteredBlocks.size();        if (filteredBlocks.isEmpty()) {            continue;        }        BlockLocation[] fileBlockLocations = fs.getFileBlockLocations(fileStatus, 0, fileStatus.getLen());        splits.addAll(generateSplits(filteredBlocks, fileBlockLocations, fileStatus, readContext.getRequestedSchema().toString(), readContext.getReadSupportMetadata(), minSplitSize, maxSplitSize));    }    if (rowGroupsDropped > 0 && totalRowGroups > 0) {        int percentDropped = (int) ((((double) rowGroupsDropped) / totalRowGroups) * 100);            } else {            }    return splits;}
1
 static List<ParquetInputSplit> generateSplits(List<BlockMetaData> rowGroupBlocks, BlockLocation[] hdfsBlocksArray, FileStatus fileStatus, String requestedSchema, Map<String, String> readSupportMetadata, long minSplitSize, long maxSplitSize) throws IOException
{    List<SplitInfo> splitRowGroups = generateSplitInfo(rowGroupBlocks, hdfsBlocksArray, minSplitSize, maxSplitSize);        List<ParquetInputSplit> resultSplits = new ArrayList<ParquetInputSplit>();    for (SplitInfo splitInfo : splitRowGroups) {        ParquetInputSplit split = splitInfo.getParquetInputSplit(fileStatus, requestedSchema, readSupportMetadata);        resultSplits.add(split);    }    return resultSplits;}
0
 static List<SplitInfo> generateSplitInfo(List<BlockMetaData> rowGroupBlocks, BlockLocation[] hdfsBlocksArray, long minSplitSize, long maxSplitSize)
{    List<SplitInfo> splitRowGroups;    if (maxSplitSize < minSplitSize || maxSplitSize < 0 || minSplitSize < 0) {        throw new ParquetDecodingException("maxSplitSize and minSplitSize should be positive and max should be greater or equal to the minSplitSize: maxSplitSize = " + maxSplitSize + "; minSplitSize is " + minSplitSize);    }    HDFSBlocks hdfsBlocks = new HDFSBlocks(hdfsBlocksArray);    hdfsBlocks.checkBelongingToANewHDFSBlock(rowGroupBlocks.get(0));    SplitInfo currentSplit = new SplitInfo(hdfsBlocks.getCurrentBlock());        splitRowGroups = new ArrayList<SplitInfo>();        checkSorted(rowGroupBlocks);    for (BlockMetaData rowGroupMetadata : rowGroupBlocks) {        if ((hdfsBlocks.checkBelongingToANewHDFSBlock(rowGroupMetadata) && currentSplit.getCompressedByteSize() >= minSplitSize && currentSplit.getCompressedByteSize() > 0) || currentSplit.getCompressedByteSize() >= maxSplitSize) {                                    splitRowGroups.add(currentSplit);            currentSplit = new SplitInfo(hdfsBlocks.getCurrentBlock());        }        currentSplit.addRowGroup(rowGroupMetadata);    }    if (currentSplit.getRowGroupCount() > 0) {        splitRowGroups.add(currentSplit);    }    return splitRowGroups;}
0
private static void checkSorted(List<BlockMetaData> rowGroupBlocks)
{    long previousOffset = 0L;    for (BlockMetaData rowGroup : rowGroupBlocks) {        long currentOffset = rowGroup.getStartingPos();        if (currentOffset < previousOffset) {            throw new ParquetDecodingException("row groups are not sorted: previous row groups starts at " + previousOffset + ", current row group starts at " + currentOffset);        }    }}
0
private static long end(List<BlockMetaData> blocks, String requestedSchema)
{    MessageType requested = MessageTypeParser.parseMessageType(requestedSchema);    long length = 0;    for (BlockMetaData block : blocks) {        List<ColumnChunkMetaData> columns = block.getColumns();        for (ColumnChunkMetaData column : columns) {            if (requested.containsPath(column.getPath().toArray())) {                length += column.getTotalSize();            }        }    }    return length;}
0
private static long[] offsets(List<BlockMetaData> blocks)
{    long[] offsets = new long[blocks.size()];    for (int i = 0; i < offsets.length; i++) {        offsets[i] = blocks.get(i).getStartingPos();    }    return offsets;}
0
public List<BlockMetaData> getBlocks()
{    throw new UnsupportedOperationException("Splits no longer have row group metadata, see PARQUET-234");}
0
 static ParquetInputSplit from(FileSplit split) throws IOException
{    return new ParquetInputSplit(split.getPath(), split.getStart(), split.getStart() + split.getLength(), split.getLength(), split.getLocations(), null);}
0
 static ParquetInputSplit from(org.apache.hadoop.mapred.FileSplit split) throws IOException
{    return new ParquetInputSplit(split.getPath(), split.getStart(), split.getStart() + split.getLength(), split.getLength(), split.getLocations(), null);}
0
 String getRequestedSchema()
{    throw new UnsupportedOperationException("Splits no longer have the requested schema, see PARQUET-234");}
0
public String getFileSchema()
{    throw new UnsupportedOperationException("Splits no longer have the file schema, see PARQUET-234");}
0
public long getEnd()
{    return end;}
0
public Map<String, String> getExtraMetadata()
{    throw new UnsupportedOperationException("Splits no longer have file metadata, see PARQUET-234");}
0
 Map<String, String> getReadSupportMetadata()
{    throw new UnsupportedOperationException("Splits no longer have read-support metadata, see PARQUET-234");}
0
public long[] getRowGroupOffsets()
{    return rowGroupOffsets;}
0
public String toString()
{    String hosts;    try {        hosts = Arrays.toString(getLocations());    } catch (Exception e) {                hosts = "(" + e + ")";    }    return this.getClass().getSimpleName() + "{" + "part: " + getPath() + " start: " + getStart() + " end: " + getEnd() + " length: " + getLength() + " hosts: " + hosts + (rowGroupOffsets == null ? "" : (" row groups: " + Arrays.toString(rowGroupOffsets))) + "}";}
0
public void readFields(DataInput hin) throws IOException
{    byte[] bytes = readArray(hin);    DataInputStream in = new DataInputStream(new GZIPInputStream(new ByteArrayInputStream(bytes)));    super.readFields(in);    this.end = in.readLong();    if (in.readBoolean()) {        this.rowGroupOffsets = new long[in.readInt()];        for (int i = 0; i < rowGroupOffsets.length; i++) {            rowGroupOffsets[i] = in.readLong();        }    }    in.close();}
0
public void write(DataOutput hout) throws IOException
{    ByteArrayOutputStream baos = new ByteArrayOutputStream();    DataOutputStream out = new DataOutputStream(new GZIPOutputStream(baos));    super.write(out);    out.writeLong(end);    out.writeBoolean(rowGroupOffsets != null);    if (rowGroupOffsets != null) {        out.writeInt(rowGroupOffsets.length);        for (long o : rowGroupOffsets) {            out.writeLong(o);        }    }    out.close();    writeArray(hout, baos.toByteArray());}
0
private static void writeArray(DataOutput out, byte[] bytes) throws IOException
{    out.writeInt(bytes.length);    out.write(bytes, 0, bytes.length);}
0
private static byte[] readArray(DataInput in) throws IOException
{    int len = in.readInt();    byte[] bytes = new byte[len];    in.readFully(bytes);    return bytes;}
0
public void commitJob(JobContext jobContext) throws IOException
{    super.commitJob(jobContext);    Configuration configuration = ContextUtil.getConfiguration(jobContext);    writeMetaDataFile(configuration, outputPath);}
0
public static void writeMetaDataFile(Configuration configuration, Path outputPath)
{    JobSummaryLevel level = ParquetOutputFormat.getJobSummaryLevel(configuration);    if (level == JobSummaryLevel.NONE) {        return;    }    try {        final FileSystem fileSystem = outputPath.getFileSystem(configuration);        FileStatus outputStatus = fileSystem.getFileStatus(outputPath);        List<Footer> footers;        switch(level) {            case ALL:                                footers = ParquetFileReader.readAllFootersInParallel(configuration, outputStatus, false);                break;            case COMMON_ONLY:                                footers = ParquetFileReader.readAllFootersInParallel(configuration, outputStatus, true);                break;            default:                throw new IllegalArgumentException("Unrecognized job summary level: " + level);        }                if (footers.isEmpty()) {            return;        }        try {            ParquetFileWriter.writeMetadataFile(configuration, outputPath, footers, level);        } catch (Exception e) {                        final Path metadataPath = new Path(outputPath, ParquetFileWriter.PARQUET_METADATA_FILE);            try {                if (fileSystem.exists(metadataPath)) {                    fileSystem.delete(metadataPath, true);                }            } catch (Exception e2) {                            }            try {                final Path commonMetadataPath = new Path(outputPath, ParquetFileWriter.PARQUET_COMMON_METADATA_FILE);                if (fileSystem.exists(commonMetadataPath)) {                    fileSystem.delete(commonMetadataPath, true);                }            } catch (Exception e2) {                            }        }    } catch (Exception e) {            }}
1
public static JobSummaryLevel getJobSummaryLevel(Configuration conf)
{    String level = conf.get(JOB_SUMMARY_LEVEL);    String deprecatedFlag = conf.get(ENABLE_JOB_SUMMARY);    if (deprecatedFlag != null) {            }    if (level != null && deprecatedFlag != null) {            }    if (level != null) {        return JobSummaryLevel.valueOf(level.toUpperCase());    }    if (deprecatedFlag != null) {        return Boolean.valueOf(deprecatedFlag) ? JobSummaryLevel.ALL : JobSummaryLevel.NONE;    }    return JobSummaryLevel.ALL;}
1
public static void setWriteSupportClass(Job job, Class<?> writeSupportClass)
{    getConfiguration(job).set(WRITE_SUPPORT_CLASS, writeSupportClass.getName());}
0
public static void setWriteSupportClass(JobConf job, Class<?> writeSupportClass)
{    job.set(WRITE_SUPPORT_CLASS, writeSupportClass.getName());}
0
public static Class<?> getWriteSupportClass(Configuration configuration)
{    final String className = configuration.get(WRITE_SUPPORT_CLASS);    if (className == null) {        return null;    }    final Class<?> writeSupportClass = ConfigurationUtil.getClassFromConfig(configuration, WRITE_SUPPORT_CLASS, WriteSupport.class);    return writeSupportClass;}
0
public static void setBlockSize(Job job, int blockSize)
{    getConfiguration(job).setInt(BLOCK_SIZE, blockSize);}
0
public static void setPageSize(Job job, int pageSize)
{    getConfiguration(job).setInt(PAGE_SIZE, pageSize);}
0
public static void setDictionaryPageSize(Job job, int pageSize)
{    getConfiguration(job).setInt(DICTIONARY_PAGE_SIZE, pageSize);}
0
public static void setCompression(Job job, CompressionCodecName compression)
{    getConfiguration(job).set(COMPRESSION, compression.name());}
0
public static void setEnableDictionary(Job job, boolean enableDictionary)
{    getConfiguration(job).setBoolean(ENABLE_DICTIONARY, enableDictionary);}
0
public static boolean getEnableDictionary(JobContext jobContext)
{    return getEnableDictionary(getConfiguration(jobContext));}
0
public static int getBlockSize(JobContext jobContext)
{    return getBlockSize(getConfiguration(jobContext));}
0
public static int getPageSize(JobContext jobContext)
{    return getPageSize(getConfiguration(jobContext));}
0
public static int getDictionaryPageSize(JobContext jobContext)
{    return getDictionaryPageSize(getConfiguration(jobContext));}
0
public static CompressionCodecName getCompression(JobContext jobContext)
{    return getCompression(getConfiguration(jobContext));}
0
public static boolean isCompressionSet(JobContext jobContext)
{    return isCompressionSet(getConfiguration(jobContext));}
0
public static void setValidation(JobContext jobContext, boolean validating)
{    setValidation(getConfiguration(jobContext), validating);}
0
public static boolean getValidation(JobContext jobContext)
{    return getValidation(getConfiguration(jobContext));}
0
public static boolean getEnableDictionary(Configuration configuration)
{    return configuration.getBoolean(ENABLE_DICTIONARY, ParquetProperties.DEFAULT_IS_DICTIONARY_ENABLED);}
0
public static int getMinRowCountForPageSizeCheck(Configuration configuration)
{    return configuration.getInt(MIN_ROW_COUNT_FOR_PAGE_SIZE_CHECK, ParquetProperties.DEFAULT_MINIMUM_RECORD_COUNT_FOR_CHECK);}
0
public static int getMaxRowCountForPageSizeCheck(Configuration configuration)
{    return configuration.getInt(MAX_ROW_COUNT_FOR_PAGE_SIZE_CHECK, ParquetProperties.DEFAULT_MAXIMUM_RECORD_COUNT_FOR_CHECK);}
0
public static boolean getEstimatePageSizeCheck(Configuration configuration)
{    return configuration.getBoolean(ESTIMATE_PAGE_SIZE_CHECK, ParquetProperties.DEFAULT_ESTIMATE_ROW_COUNT_FOR_PAGE_SIZE_CHECK);}
0
public static int getBlockSize(Configuration configuration)
{    return configuration.getInt(BLOCK_SIZE, DEFAULT_BLOCK_SIZE);}
0
public static long getLongBlockSize(Configuration configuration)
{    return configuration.getLong(BLOCK_SIZE, DEFAULT_BLOCK_SIZE);}
0
public static int getPageSize(Configuration configuration)
{    return configuration.getInt(PAGE_SIZE, ParquetProperties.DEFAULT_PAGE_SIZE);}
0
public static int getDictionaryPageSize(Configuration configuration)
{    return configuration.getInt(DICTIONARY_PAGE_SIZE, ParquetProperties.DEFAULT_DICTIONARY_PAGE_SIZE);}
0
public static WriterVersion getWriterVersion(Configuration configuration)
{    String writerVersion = configuration.get(WRITER_VERSION, ParquetProperties.DEFAULT_WRITER_VERSION.toString());    return WriterVersion.fromString(writerVersion);}
0
public static CompressionCodecName getCompression(Configuration configuration)
{    return CodecConfig.getParquetCompressionCodec(configuration);}
0
public static boolean isCompressionSet(Configuration configuration)
{    return CodecConfig.isParquetCompressionSet(configuration);}
0
public static void setValidation(Configuration configuration, boolean validating)
{    configuration.setBoolean(VALIDATION, validating);}
0
public static boolean getValidation(Configuration configuration)
{    return configuration.getBoolean(VALIDATION, false);}
0
private CompressionCodecName getCodec(TaskAttemptContext taskAttemptContext)
{    return CodecConfig.from(taskAttemptContext).getCodec();}
0
public static void setMaxPaddingSize(JobContext jobContext, int maxPaddingSize)
{    setMaxPaddingSize(getConfiguration(jobContext), maxPaddingSize);}
0
public static void setMaxPaddingSize(Configuration conf, int maxPaddingSize)
{    conf.setInt(MAX_PADDING_BYTES, maxPaddingSize);}
0
private static int getMaxPaddingSize(Configuration conf)
{    return conf.getInt(MAX_PADDING_BYTES, ParquetWriter.MAX_PADDING_SIZE_DEFAULT);}
0
public static void setColumnIndexTruncateLength(JobContext jobContext, int length)
{    setColumnIndexTruncateLength(getConfiguration(jobContext), length);}
0
public static void setColumnIndexTruncateLength(Configuration conf, int length)
{    conf.setInt(COLUMN_INDEX_TRUNCATE_LENGTH, length);}
0
private static int getColumnIndexTruncateLength(Configuration conf)
{    return conf.getInt(COLUMN_INDEX_TRUNCATE_LENGTH, ParquetProperties.DEFAULT_COLUMN_INDEX_TRUNCATE_LENGTH);}
0
public static void setPageRowCountLimit(JobContext jobContext, int rowCount)
{    setPageRowCountLimit(getConfiguration(jobContext), rowCount);}
0
public static void setPageRowCountLimit(Configuration conf, int rowCount)
{    conf.setInt(PAGE_ROW_COUNT_LIMIT, rowCount);}
0
private static int getPageRowCountLimit(Configuration conf)
{    return conf.getInt(PAGE_ROW_COUNT_LIMIT, ParquetProperties.DEFAULT_PAGE_ROW_COUNT_LIMIT);}
0
public static void setPageWriteChecksumEnabled(JobContext jobContext, boolean val)
{    setPageWriteChecksumEnabled(getConfiguration(jobContext), val);}
0
public static void setPageWriteChecksumEnabled(Configuration conf, boolean val)
{    conf.setBoolean(PAGE_WRITE_CHECKSUM_ENABLED, val);}
0
public static boolean getPageWriteChecksumEnabled(Configuration conf)
{    return conf.getBoolean(PAGE_WRITE_CHECKSUM_ENABLED, ParquetProperties.DEFAULT_PAGE_WRITE_CHECKSUM_ENABLED);}
0
public RecordWriter<Void, T> getRecordWriter(TaskAttemptContext taskAttemptContext) throws IOException, InterruptedException
{    return getRecordWriter(taskAttemptContext, Mode.CREATE);}
0
public RecordWriter<Void, T> getRecordWriter(TaskAttemptContext taskAttemptContext, Mode mode) throws IOException, InterruptedException
{    final Configuration conf = getConfiguration(taskAttemptContext);    CompressionCodecName codec = getCodec(taskAttemptContext);    String extension = codec.getExtension() + ".parquet";    Path file = getDefaultWorkFile(taskAttemptContext, extension);    return getRecordWriter(conf, file, codec, mode);}
0
public RecordWriter<Void, T> getRecordWriter(TaskAttemptContext taskAttemptContext, Path file) throws IOException, InterruptedException
{    return getRecordWriter(taskAttemptContext, file, Mode.CREATE);}
0
public RecordWriter<Void, T> getRecordWriter(TaskAttemptContext taskAttemptContext, Path file, Mode mode) throws IOException, InterruptedException
{    return getRecordWriter(getConfiguration(taskAttemptContext), file, getCodec(taskAttemptContext), mode);}
0
public RecordWriter<Void, T> getRecordWriter(Configuration conf, Path file, CompressionCodecName codec) throws IOException, InterruptedException
{    return getRecordWriter(conf, file, codec, Mode.CREATE);}
0
public RecordWriter<Void, T> getRecordWriter(Configuration conf, Path file, CompressionCodecName codec, Mode mode) throws IOException, InterruptedException
{    final WriteSupport<T> writeSupport = getWriteSupport(conf);    ParquetProperties props = ParquetProperties.builder().withPageSize(getPageSize(conf)).withDictionaryPageSize(getDictionaryPageSize(conf)).withDictionaryEncoding(getEnableDictionary(conf)).withWriterVersion(getWriterVersion(conf)).estimateRowCountForPageSizeCheck(getEstimatePageSizeCheck(conf)).withMinRowCountForPageSizeCheck(getMinRowCountForPageSizeCheck(conf)).withMaxRowCountForPageSizeCheck(getMaxRowCountForPageSizeCheck(conf)).withColumnIndexTruncateLength(getColumnIndexTruncateLength(conf)).withPageRowCountLimit(getPageRowCountLimit(conf)).withPageWriteChecksumEnabled(getPageWriteChecksumEnabled(conf)).build();    long blockSize = getLongBlockSize(conf);    int maxPaddingSize = getMaxPaddingSize(conf);    boolean validating = getValidation(conf);    if (LOG.isInfoEnabled()) {                                                                                                            }    WriteContext init = writeSupport.init(conf);    ParquetFileWriter w = new ParquetFileWriter(HadoopOutputFile.fromPath(file, conf), init.getSchema(), mode, blockSize, maxPaddingSize, props.getColumnIndexTruncateLength(), props.getPageWriteChecksumEnabled());    w.start();    float maxLoad = conf.getFloat(ParquetOutputFormat.MEMORY_POOL_RATIO, MemoryManager.DEFAULT_MEMORY_POOL_RATIO);    long minAllocation = conf.getLong(ParquetOutputFormat.MIN_MEMORY_ALLOCATION, MemoryManager.DEFAULT_MIN_MEMORY_ALLOCATION);    synchronized (ParquetOutputFormat.class) {        if (memoryManager == null) {            memoryManager = new MemoryManager(maxLoad, minAllocation);        }    }    if (memoryManager.getMemoryPoolRatio() != maxLoad) {            }    return new ParquetRecordWriter<T>(w, writeSupport, init.getSchema(), init.getExtraMetaData(), blockSize, codec, validating, props, memoryManager, conf);}
1
public WriteSupport<T> getWriteSupport(Configuration configuration)
{    if (writeSupport != null)        return writeSupport;    Class<?> writeSupportClass = getWriteSupportClass(configuration);    try {        return (WriteSupport<T>) checkNotNull(writeSupportClass, "writeSupportClass").newInstance();    } catch (InstantiationException e) {        throw new BadConfigurationException("could not instantiate write support class: " + writeSupportClass, e);    } catch (IllegalAccessException e) {        throw new BadConfigurationException("could not instantiate write support class: " + writeSupportClass, e);    }}
0
public OutputCommitter getOutputCommitter(TaskAttemptContext context) throws IOException
{    if (committer == null) {        Path output = getOutputPath(context);        committer = new ParquetOutputCommitter(output, context);    }    return committer;}
0
public static synchronized MemoryManager getMemoryManager()
{    return memoryManager;}
0
public T read() throws IOException
{    try {        if (reader != null && reader.nextKeyValue()) {            return reader.getCurrentValue();        } else {            initReader();            return reader == null ? null : read();        }    } catch (InterruptedException e) {        throw new IOException(e);    }}
0
private void initReader() throws IOException
{    if (reader != null) {        reader.close();        reader = null;    }    if (filesIterator.hasNext()) {        InputFile file = filesIterator.next();        ParquetFileReader fileReader = ParquetFileReader.open(file, options);        reader = new InternalParquetRecordReader<>(readSupport, options.getRecordFilter());        reader.initialize(fileReader, options);    }}
0
public void close() throws IOException
{    if (reader != null) {        reader.close();    }}
0
public static Builder<T> read(InputFile file) throws IOException
{    return new Builder<>(file);}
0
public static Builder<T> builder(ReadSupport<T> readSupport, Path path)
{    return new Builder<>(readSupport, path);}
0
public Builder<T> withConf(Configuration conf)
{    this.conf = checkNotNull(conf, "conf");            this.optionsBuilder = HadoopReadOptions.builder(conf);    if (filter != null) {        optionsBuilder.withRecordFilter(filter);    }    return this;}
0
public Builder<T> withFilter(Filter filter)
{    this.filter = filter;    optionsBuilder.withRecordFilter(filter);    return this;}
0
public Builder<T> useSignedStringMinMax(boolean useSignedStringMinMax)
{    optionsBuilder.useSignedStringMinMax(useSignedStringMinMax);    return this;}
0
public Builder<T> useSignedStringMinMax()
{    optionsBuilder.useSignedStringMinMax();    return this;}
0
public Builder<T> useStatsFilter(boolean useStatsFilter)
{    optionsBuilder.useStatsFilter(useStatsFilter);    return this;}
0
public Builder<T> useStatsFilter()
{    optionsBuilder.useStatsFilter();    return this;}
0
public Builder<T> useDictionaryFilter(boolean useDictionaryFilter)
{    optionsBuilder.useDictionaryFilter(useDictionaryFilter);    return this;}
0
public Builder<T> useDictionaryFilter()
{    optionsBuilder.useDictionaryFilter();    return this;}
0
public Builder<T> useRecordFilter(boolean useRecordFilter)
{    optionsBuilder.useRecordFilter(useRecordFilter);    return this;}
0
public Builder<T> useRecordFilter()
{    optionsBuilder.useRecordFilter();    return this;}
0
public Builder<T> useColumnIndexFilter(boolean useColumnIndexFilter)
{    optionsBuilder.useColumnIndexFilter(useColumnIndexFilter);    return this;}
0
public Builder<T> useColumnIndexFilter()
{    optionsBuilder.useColumnIndexFilter();    return this;}
0
public Builder<T> usePageChecksumVerification(boolean usePageChecksumVerification)
{    optionsBuilder.usePageChecksumVerification(usePageChecksumVerification);    return this;}
0
public Builder<T> usePageChecksumVerification()
{    optionsBuilder.usePageChecksumVerification();    return this;}
0
public Builder<T> withFileRange(long start, long end)
{    optionsBuilder.withRange(start, end);    return this;}
0
public Builder<T> withCodecFactory(CompressionCodecFactory codecFactory)
{    optionsBuilder.withCodecFactory(codecFactory);    return this;}
0
public Builder<T> set(String key, String value)
{    optionsBuilder.set(key, value);    return this;}
0
protected ReadSupport<T> getReadSupport()
{        Preconditions.checkArgument(readSupport != null, "[BUG] Classes that extend Builder should override getReadSupport()");    return readSupport;}
0
public ParquetReader<T> build() throws IOException
{    ParquetReadOptions options = optionsBuilder.build();    if (path != null) {        FileSystem fs = path.getFileSystem(conf);        FileStatus stat = fs.getFileStatus(path);        if (stat.isFile()) {            return new ParquetReader<>(Collections.singletonList((InputFile) HadoopInputFile.fromStatus(stat, conf)), options, getReadSupport());        } else {            List<InputFile> files = new ArrayList<>();            for (FileStatus fileStatus : fs.listStatus(path, HiddenFileFilter.INSTANCE)) {                files.add(HadoopInputFile.fromStatus(fileStatus, conf));            }            return new ParquetReader<T>(files, options, getReadSupport());        }    } else {        return new ParquetReader<>(Collections.singletonList(file), options, getReadSupport());    }}
0
public void close() throws IOException
{    internalReader.close();}
0
public Void getCurrentKey() throws IOException, InterruptedException
{    return null;}
0
public T getCurrentValue() throws IOException, InterruptedException
{    return internalReader.getCurrentValue();}
0
public float getProgress() throws IOException, InterruptedException
{    return internalReader.getProgress();}
0
public void initialize(InputSplit inputSplit, TaskAttemptContext context) throws IOException, InterruptedException
{    if (ContextUtil.hasCounterMethod(context)) {        BenchmarkCounter.initCounterFromContext(context);    } else {            }    initializeInternalReader(toParquetSplit(inputSplit), ContextUtil.getConfiguration(context));}
1
public void initialize(InputSplit inputSplit, Configuration configuration, Reporter reporter) throws IOException, InterruptedException
{    BenchmarkCounter.initCounterFromReporter(reporter, configuration);    initializeInternalReader(toParquetSplit(inputSplit), configuration);}
0
private void initializeInternalReader(ParquetInputSplit split, Configuration configuration) throws IOException
{    Path path = split.getPath();    long[] rowGroupOffsets = split.getRowGroupOffsets();        ParquetReadOptions.Builder optionsBuilder = HadoopReadOptions.builder(configuration);    if (rowGroupOffsets != null) {        optionsBuilder.withOffsets(rowGroupOffsets);    } else {        optionsBuilder.withRange(split.getStart(), split.getEnd());    }        ParquetFileReader reader = ParquetFileReader.open(HadoopInputFile.fromPath(path, configuration), optionsBuilder.build());    if (rowGroupOffsets != null) {                List<BlockMetaData> blocks = reader.getFooter().getBlocks();        if (blocks.size() != rowGroupOffsets.length) {            throw new IllegalStateException("All of the offsets in the split should be found in the file." + " expected: " + Arrays.toString(rowGroupOffsets) + " found: " + blocks);        }    }    if (!reader.getRowGroups().isEmpty()) {        checkDeltaByteArrayProblem(reader.getFooter().getFileMetaData(), configuration, reader.getRowGroups().get(0));    }    internalReader.initialize(reader, configuration);}
0
private void checkDeltaByteArrayProblem(FileMetaData meta, Configuration conf, BlockMetaData block)
{        if (conf.getBoolean(ParquetInputFormat.SPLIT_FILES, true)) {                Set<Encoding> encodings = new HashSet<Encoding>();        for (ColumnChunkMetaData column : block.getColumns()) {            encodings.addAll(column.getEncodings());        }        for (Encoding encoding : encodings) {            if (CorruptDeltaByteArrays.requiresSequentialReads(meta.getCreatedBy(), encoding)) {                throw new ParquetDecodingException("Cannot read data due to " + "PARQUET-246: to read safely, set " + SPLIT_FILES + " to false");            }        }    }}
0
public boolean nextKeyValue() throws IOException, InterruptedException
{    return internalReader.nextKeyValue();}
0
private ParquetInputSplit toParquetSplit(InputSplit split) throws IOException
{    if (split instanceof ParquetInputSplit) {        return (ParquetInputSplit) split;    } else if (split instanceof FileSplit) {        return ParquetInputSplit.from((FileSplit) split);    } else if (split instanceof org.apache.hadoop.mapred.FileSplit) {        return ParquetInputSplit.from((org.apache.hadoop.mapred.FileSplit) split);    } else {        throw new IllegalArgumentException("Invalid split (not a FileSplit or ParquetInputSplit): " + split);    }}
0
public void close(TaskAttemptContext context) throws IOException, InterruptedException
{    try {        internalWriter.close();        } finally {        if (codecFactory != null) {            codecFactory.release();        }        if (memoryManager != null) {            memoryManager.removeWriter(internalWriter);        }    }}
0
public void write(Void key, T value) throws IOException, InterruptedException
{    internalWriter.write(value);}
0
public void write(T object) throws IOException
{    try {        writer.write(object);    } catch (InterruptedException e) {        throw new IOException(e);    }}
0
public void close() throws IOException
{    try {        writer.close();    } catch (InterruptedException e) {        throw new IOException(e);    } finally {                codecFactory.release();    }}
0
public ParquetMetadata getFooter()
{    return writer.getFooter();}
0
public long getDataSize()
{    return writer.getDataSize();}
0
public SELF withConf(Configuration conf)
{    this.conf = conf;    return self();}
0
public SELF withWriteMode(ParquetFileWriter.Mode mode)
{    this.mode = mode;    return self();}
0
public SELF withCompressionCodec(CompressionCodecName codecName)
{    this.codecName = codecName;    return self();}
0
public SELF withRowGroupSize(int rowGroupSize)
{    this.rowGroupSize = rowGroupSize;    return self();}
0
public SELF withPageSize(int pageSize)
{    encodingPropsBuilder.withPageSize(pageSize);    return self();}
0
public SELF withPageRowCountLimit(int rowCount)
{    encodingPropsBuilder.withPageRowCountLimit(rowCount);    return self();}
0
public SELF withDictionaryPageSize(int dictionaryPageSize)
{    encodingPropsBuilder.withDictionaryPageSize(dictionaryPageSize);    return self();}
0
public SELF withMaxPaddingSize(int maxPaddingSize)
{    this.maxPaddingSize = maxPaddingSize;    return self();}
0
public SELF enableDictionaryEncoding()
{    encodingPropsBuilder.withDictionaryEncoding(true);    return self();}
0
public SELF withDictionaryEncoding(boolean enableDictionary)
{    encodingPropsBuilder.withDictionaryEncoding(enableDictionary);    return self();}
0
public SELF enableValidation()
{    this.enableValidation = true;    return self();}
0
public SELF withValidation(boolean enableValidation)
{    this.enableValidation = enableValidation;    return self();}
0
public SELF withWriterVersion(WriterVersion version)
{    encodingPropsBuilder.withWriterVersion(version);    return self();}
0
public SELF enablePageWriteChecksum()
{    encodingPropsBuilder.withPageWriteChecksumEnabled(true);    return self();}
0
public SELF withPageWriteChecksumEnabled(boolean enablePageWriteChecksum)
{    encodingPropsBuilder.withPageWriteChecksumEnabled(enablePageWriteChecksum);    return self();}
0
public SELF config(String property, String value)
{    conf.set(property, value);    return self();}
0
public ParquetWriter<T> build() throws IOException
{    if (file != null) {        return new ParquetWriter<>(file, mode, getWriteSupport(conf), codecName, rowGroupSize, enableValidation, conf, maxPaddingSize, encodingPropsBuilder.build());    } else {        return new ParquetWriter<>(HadoopOutputFile.fromPath(path, conf), mode, getWriteSupport(conf), codecName, rowGroupSize, enableValidation, conf, maxPaddingSize, encodingPropsBuilder.build());    }}
0
public static void main(String[] args) throws Exception
{    if (args.length != 1) {        System.err.println("usage PrintFooter <path>");        return;    }    Path path = new Path(new URI(args[0]));    final Configuration configuration = new Configuration();    final FileSystem fs = path.getFileSystem(configuration);    FileStatus fileStatus = fs.getFileStatus(path);    Path summary = new Path(fileStatus.getPath(), PARQUET_METADATA_FILE);    if (fileStatus.isDir() && fs.exists(summary)) {        System.out.println("reading summary file");        FileStatus summaryStatus = fs.getFileStatus(summary);        List<Footer> readSummaryFile = ParquetFileReader.readSummaryFile(configuration, summaryStatus);        for (Footer footer : readSummaryFile) {            add(footer.getParquetMetadata());        }    } else {        List<FileStatus> statuses;        if (fileStatus.isDir()) {            System.out.println("listing files in " + fileStatus.getPath());            statuses = Arrays.asList(fs.listStatus(fileStatus.getPath(), HiddenFileFilter.INSTANCE));        } else {            statuses = new ArrayList<FileStatus>();            statuses.add(fileStatus);        }        System.out.println("opening " + statuses.size() + " files");        int i = 0;        ExecutorService threadPool = Executors.newFixedThreadPool(5);        try {            long t0 = System.currentTimeMillis();            Deque<Future<ParquetMetadata>> footers = new LinkedBlockingDeque<Future<ParquetMetadata>>();            for (final FileStatus currentFile : statuses) {                footers.add(threadPool.submit(() -> {                    try {                        return ParquetFileReader.readFooter(configuration, currentFile, NO_FILTER);                    } catch (Exception e) {                        throw new ParquetDecodingException("could not read footer", e);                    }                }));            }            int previousPercent = 0;            int n = 60;            System.out.print("0% [");            for (int j = 0; j < n; j++) {                System.out.print(" ");            }            System.out.print("] 100%");            for (int j = 0; j < n + 6; j++) {                System.out.print('\b');            }            while (!footers.isEmpty()) {                Future<ParquetMetadata> futureFooter = footers.removeFirst();                if (!futureFooter.isDone()) {                    footers.addLast(futureFooter);                    continue;                }                ParquetMetadata footer = futureFooter.get();                int currentPercent = (++i * n / statuses.size());                while (currentPercent > previousPercent) {                    System.out.print("*");                    previousPercent++;                }                add(footer);            }            System.out.println("");            long t1 = System.currentTimeMillis();            System.out.println("read all footers in " + (t1 - t0) + " ms");        } finally {            threadPool.shutdownNow();        }    }    Set<Entry<ColumnDescriptor, ColStats>> entries = stats.entrySet();    long total = 0;    long totalUnc = 0;    for (Entry<ColumnDescriptor, ColStats> entry : entries) {        ColStats colStats = entry.getValue();        total += colStats.allStats.total;        totalUnc += colStats.uncStats.total;    }    for (Entry<ColumnDescriptor, ColStats> entry : entries) {        ColStats colStats = entry.getValue();        System.out.println(entry.getKey() + " " + percent(colStats.allStats.total, total) + "% of all space " + colStats);    }    System.out.println("number of blocks: " + blockCount);    System.out.println("total data size: " + humanReadable(total) + " (raw " + humanReadable(totalUnc) + ")");    System.out.println("total record: " + humanReadable(recordCount));    System.out.println("average block size: " + humanReadable(total / blockCount) + " (raw " + humanReadable(totalUnc / blockCount) + ")");    System.out.println("average record count: " + humanReadable(recordCount / blockCount));}
0
private static void add(ParquetMetadata footer)
{    for (BlockMetaData blockMetaData : footer.getBlocks()) {        ++blockCount;        MessageType schema = footer.getFileMetaData().getSchema();        recordCount += blockMetaData.getRowCount();        List<ColumnChunkMetaData> columns = blockMetaData.getColumns();        for (ColumnChunkMetaData columnMetaData : columns) {            ColumnDescriptor desc = schema.getColumnDescription(columnMetaData.getPath().toArray());            add(desc, columnMetaData.getValueCount(), columnMetaData.getTotalSize(), columnMetaData.getTotalUncompressedSize(), columnMetaData.getEncodings(), columnMetaData.getStatistics());        }    }}
0
private static void printTotalString(String message, long total, long totalUnc)
{    System.out.println("total " + message + ": " + humanReadable(total) + " (raw " + humanReadable(totalUnc) + " saved " + percentComp(totalUnc, total) + "%)");}
0
private static float percentComp(long raw, long compressed)
{    return percent(raw - compressed, raw);}
0
private static float percent(long numerator, long denominator)
{    return ((float) ((numerator) * 1000 / denominator)) / 10;}
0
private static String humanReadable(long size)
{    if (size < 1000) {        return String.valueOf(size);    }    long currentSize = size;    long previousSize = size * 1000;    int count = 0;    String[] unit = { "", "K", "M", "G", "T", "P" };    while (currentSize >= 1000) {        previousSize = currentSize;        currentSize = currentSize / 1000;        ++count;    }    return ((float) previousSize / 1000) + unit[count];}
0
public void add(long length)
{    min = Math.min(length, min);    max = Math.max(length, max);    total += length;}
0
public String toString(int blocks)
{    return "min: " + humanReadable(min) + " max: " + humanReadable(max) + " average: " + humanReadable(total / blocks) + " total: " + humanReadable(total);}
0
public void add(long valueCount, long size, long uncSize, Collection<Encoding> encodings, Statistics colValuesStats)
{    ++blocks;    valueCountStats.add(valueCount);    allStats.add(size);    uncStats.add(uncSize);    this.encodings.addAll(encodings);    this.colValuesStats = colValuesStats;}
0
public String toString()
{    long raw = uncStats.total;    long compressed = allStats.total;    return encodings + " " + allStats.toString(blocks) + " (raw data: " + humanReadable(raw) + (raw == 0 ? "" : " saving " + (raw - compressed) * 100 / raw + "%") + ")\n" + "  values: " + valueCountStats.toString(blocks) + "\n" + "  uncompressed: " + uncStats.toString(blocks) + "\n" + "  column values statistics: " + colValuesStats.toString();}
0
private static void add(ColumnDescriptor desc, long valueCount, long size, long uncSize, Collection<Encoding> encodings, Statistics colValuesStats)
{    ColStats colStats = stats.get(desc);    if (colStats == null) {        colStats = new ColStats();        stats.put(desc, colStats);    }    colStats.add(valueCount, size, uncSize, encodings, colValuesStats);}
0
public void incErrors(RecordMaterializationException cause) throws ParquetDecodingException
{    numErrors++;        if (numErrors > 0 && errorThreshold <= 0) {                throw new ParquetDecodingException("Error while decoding records", cause);    }    double errRate = numErrors / (double) totalNumRecords;    if (errRate > errorThreshold) {        String message = String.format("Decoding error rate of at least %s/%s crosses configured threshold of %s", numErrors, totalNumRecords, errorThreshold);                throw new ParquetDecodingException(message, cause);    }}
1
private static float getFloat(ParquetReadOptions options, String key, float defaultValue)
{    String value = options.getProperty(key);    if (value != null) {        return Float.valueOf(value);    } else {        return defaultValue;    }}
0
public static Class<?> getClassFromConfig(Configuration configuration, String configName, Class<?> assignableFrom)
{    final String className = configuration.get(configName);    if (className == null) {        return null;    }    try {        final Class<?> foundClass = configuration.getClassByName(className);        if (!assignableFrom.isAssignableFrom(foundClass)) {            throw new BadConfigurationException("class " + className + " set in job conf at " + configName + " is not a subclass of " + assignableFrom.getCanonicalName());        }        return foundClass;    } catch (ClassNotFoundException e) {        throw new BadConfigurationException("could not instantiate class " + className + " set in job conf at " + configName, e);    }}
0
public static JobContext newJobContext(Configuration conf, JobID jobId)
{    try {        return (JobContext) JOB_CONTEXT_CONSTRUCTOR.newInstance(conf, jobId);    } catch (InstantiationException e) {        throw new IllegalArgumentException("Can't instantiate JobContext", e);    } catch (IllegalAccessException e) {        throw new IllegalArgumentException("Can't instantiate JobContext", e);    } catch (InvocationTargetException e) {        throw new IllegalArgumentException("Can't instantiate JobContext", e);    }}
0
public static TaskAttemptContext newTaskAttemptContext(Configuration conf, TaskAttemptID taskAttemptId)
{    try {        return (TaskAttemptContext) TASK_CONTEXT_CONSTRUCTOR.newInstance(conf, taskAttemptId);    } catch (InstantiationException e) {        throw new IllegalArgumentException("Can't instantiate TaskAttemptContext", e);    } catch (IllegalAccessException e) {        throw new IllegalArgumentException("Can't instantiate TaskAttemptContext", e);    } catch (InvocationTargetException e) {        throw new IllegalArgumentException("Can't instantiate TaskAttemptContext", e);    }}
0
public static Counter newGenericCounter(String name, String displayName, long value)
{    try {        return (Counter) GENERIC_COUNTER_CONSTRUCTOR.newInstance(name, displayName, value);    } catch (InstantiationException e) {        throw new IllegalArgumentException("Can't instantiate Counter", e);    } catch (IllegalAccessException e) {        throw new IllegalArgumentException("Can't instantiate Counter", e);    } catch (InvocationTargetException e) {        throw new IllegalArgumentException("Can't instantiate Counter", e);    }}
0
public static Configuration getConfiguration(JobContext context)
{    try {        return (Configuration) GET_CONFIGURATION_METHOD.invoke(context);    } catch (IllegalAccessException e) {        throw new IllegalArgumentException("Can't invoke method", e);    } catch (InvocationTargetException e) {        throw new IllegalArgumentException("Can't invoke method", e);    }}
0
public static Counter getCounter(TaskAttemptContext context, String groupName, String counterName)
{    Method counterMethod = findCounterMethod(context);    return (Counter) invoke(counterMethod, context, groupName, counterName);}
0
public static boolean hasCounterMethod(TaskAttemptContext context)
{    return findCounterMethod(context) != null;}
0
private static Method findCounterMethod(TaskAttemptContext context)
{    if (context != null) {        if (COUNTER_METHODS_BY_CLASS.containsKey(context.getClass())) {            return COUNTER_METHODS_BY_CLASS.get(context.getClass());        }        try {            Method method = context.getClass().getMethod("getCounter", String.class, String.class);            if (method.getReturnType().isAssignableFrom(Counter.class)) {                COUNTER_METHODS_BY_CLASS.put(context.getClass(), method);                return method;            }        } catch (NoSuchMethodException e) {            return null;        }    }    return null;}
0
private static Object invoke(Method method, Object obj, Object... args)
{    try {        return method.invoke(obj, args);    } catch (IllegalAccessException e) {        throw new IllegalArgumentException("Can't invoke method " + method.getName(), e);    } catch (InvocationTargetException e) {        throw new IllegalArgumentException("Can't invoke method " + method.getName(), e);    }}
0
public static void incrementCounter(Counter counter, long increment)
{    invoke(INCREMENT_COUNTER_METHOD, counter, increment);}
0
public static void initCounterFromContext(TaskAttemptContext context)
{    counterLoader = new MapReduceCounterLoader(context);    loadCounters();}
0
public static void initCounterFromReporter(Reporter reporter, Configuration configuration)
{    counterLoader = new MapRedCounterLoader(reporter, configuration);    loadCounters();}
0
private static void loadCounters()
{    bytesReadCounter = getCounterWhenFlagIsSet(COUNTER_GROUP_NAME, BYTES_READ_COUNTER_NAME, ENABLE_BYTES_READ_COUNTER);    totalBytesCounter = getCounterWhenFlagIsSet(COUNTER_GROUP_NAME, BYTES_TOTAL_COUNTER_NAME, ENABLE_BYTES_TOTAL_COUNTER);    timeCounter = getCounterWhenFlagIsSet(COUNTER_GROUP_NAME, TIME_READ_COUNTER_NAME, ENABLE_TIME_READ_COUNTER);}
0
private static ICounter getCounterWhenFlagIsSet(String groupName, String counterName, String counterFlag)
{    return counterLoader.getCounterByNameAndFlag(groupName, counterName, counterFlag);}
0
public static void incrementTotalBytes(long val)
{    totalBytesCounter.increment(val);}
0
public static long getTotalBytes()
{    return totalBytesCounter.getCount();}
0
public static void incrementBytesRead(long val)
{    bytesReadCounter.increment(val);}
0
public static long getBytesRead()
{    return bytesReadCounter.getCount();}
0
public static void incrementTime(long val)
{    timeCounter.increment(val);}
0
public static long getTime()
{    return timeCounter.getCount();}
0
public void increment(long val)
{}
0
public long getCount()
{    return 0;}
0
public void increment(long val)
{    adaptee.increment(val);}
0
public long getCount()
{    return adaptee.getCounter();}
0
public ICounter getCounterByNameAndFlag(String groupName, String counterName, String counterFlag)
{    if (conf.getBoolean(counterFlag, true)) {        Counters.Counter counter = reporter.getCounter(groupName, counterName);        if (counter != null) {            return new MapRedCounterAdapter(reporter.getCounter(groupName, counterName));        }    }    return new BenchmarkCounter.NullCounter();}
0
public void increment(long val)
{    ContextUtil.incrementCounter(adaptee, val);}
0
public long getCount()
{        return adaptee.getValue();}
0
public ICounter getCounterByNameAndFlag(String groupName, String counterName, String counterFlag)
{    if (ContextUtil.getConfiguration(context).getBoolean(counterFlag, true)) {        return new MapReduceCounterAdapter(ContextUtil.getCounter(context, groupName, counterName));    } else {        return new BenchmarkCounter.NullCounter();    }}
0
public long getPos() throws IOException
{    return stream.getPos();}
0
public void seek(long newPos) throws IOException
{    stream.seek(newPos);}
0
public void readFully(byte[] bytes) throws IOException
{    stream.readFully(bytes, 0, bytes.length);}
0
public void readFully(byte[] bytes, int start, int len) throws IOException
{    stream.readFully(bytes);}
0
public void close() throws IOException
{    stream.close();}
0
public long getPos() throws IOException
{    return stream.getPos();}
0
public void seek(long newPos) throws IOException
{    stream.seek(newPos);}
0
public void readFully(byte[] bytes, int start, int len) throws IOException
{    stream.readFully(bytes);}
0
public int read(ByteBuffer buf) throws IOException
{    return stream.read(buf);}
0
public void readFully(ByteBuffer buf) throws IOException
{    readFully(reader, buf);}
0
public int read(ByteBuffer buf) throws IOException
{    return stream.read(buf);}
0
public static void readFully(Reader reader, ByteBuffer buf) throws IOException
{        while (buf.hasRemaining()) {        int readCount = reader.read(buf);        if (readCount == -1) {                        throw new EOFException("Reached the end of stream. Still have: " + buf.remaining() + " bytes left");        }    }}
0
public static CompressionCodecFactory newFactory(int sizeHint)
{    return new CodecFactory(new Configuration(), sizeHint);}
0
public static CompressionCodecFactory newFactory(Configuration conf, int sizeHint)
{    return new CodecFactory(conf, sizeHint);}
0
public static CompressionCodecFactory newDirectFactory(Configuration conf, ByteBufferAllocator allocator, int sizeHint)
{    return CodecFactory.createDirectCodecFactory(conf, allocator, sizeHint);}
0
public static HadoopInputFile fromPath(Path path, Configuration conf) throws IOException
{    FileSystem fs = path.getFileSystem(conf);    return new HadoopInputFile(fs, fs.getFileStatus(path), conf);}
0
public static HadoopInputFile fromStatus(FileStatus stat, Configuration conf) throws IOException
{    FileSystem fs = stat.getPath().getFileSystem(conf);    return new HadoopInputFile(fs, stat, conf);}
0
public Configuration getConfiguration()
{    return conf;}
0
public long getLength()
{    return stat.getLen();}
0
public SeekableInputStream newStream() throws IOException
{    return HadoopStreams.wrap(fs.open(stat.getPath()));}
0
public String toString()
{    return stat.getPath().toString();}
0
public static Set<String> getBlockFileSystems()
{    return BLOCK_FS_SCHEMES;}
0
private static boolean supportsBlockSize(FileSystem fs)
{    return BLOCK_FS_SCHEMES.contains(fs.getUri().getScheme());}
0
public static HadoopOutputFile fromPath(Path path, Configuration conf) throws IOException
{    FileSystem fs = path.getFileSystem(conf);    return new HadoopOutputFile(fs, fs.makeQualified(path), conf);}
0
public Configuration getConfiguration()
{    return conf;}
0
public PositionOutputStream create(long blockSizeHint) throws IOException
{    return HadoopStreams.wrap(fs.create(path, false, /* do not overwrite */    DFS_BUFFER_SIZE_DEFAULT, fs.getDefaultReplication(path), Math.max(fs.getDefaultBlockSize(path), blockSizeHint)));}
0
public PositionOutputStream createOrOverwrite(long blockSizeHint) throws IOException
{    return HadoopStreams.wrap(fs.create(path, true, /* overwrite if exists */    DFS_BUFFER_SIZE_DEFAULT, fs.getDefaultReplication(path), Math.max(fs.getDefaultBlockSize(path), blockSizeHint)));}
0
public boolean supportsBlockSize()
{    return supportsBlockSize(fs);}
0
public long defaultBlockSize()
{    return fs.getDefaultBlockSize(path);}
0
public String toString()
{    return path.toString();}
0
public long getPos() throws IOException
{    return wrapped.getPos();}
0
public void write(int b) throws IOException
{    wrapped.write(b);}
0
public void write(byte[] b) throws IOException
{    wrapped.write(b);}
0
public void write(byte[] b, int off, int len) throws IOException
{    wrapped.write(b, off, len);}
0
public void sync() throws IOException
{    wrapped.hsync();}
0
public void flush() throws IOException
{    wrapped.flush();}
0
public void close() throws IOException
{    wrapped.close();}
0
public static SeekableInputStream wrap(FSDataInputStream stream)
{    Preconditions.checkNotNull(stream, "Cannot wrap a null input stream");    if (byteBufferReadableClass != null && h2SeekableConstructor != null && byteBufferReadableClass.isInstance(stream.getWrappedStream())) {        try {            return h2SeekableConstructor.newInstance(stream);        } catch (InstantiationException e) {                        return new H1SeekableInputStream(stream);        } catch (IllegalAccessException e) {                        return new H1SeekableInputStream(stream);        } catch (InvocationTargetException e) {            throw new ParquetDecodingException("Could not instantiate H2SeekableInputStream", e.getTargetException());        }    } else {        return new H1SeekableInputStream(stream);    }}
1
private static Class<?> getReadableClass()
{    try {        return Class.forName("org.apache.hadoop.fs.ByteBufferReadable");    } catch (ClassNotFoundException e) {        return null;    } catch (NoClassDefFoundError e) {        return null;    }}
0
private static Class<SeekableInputStream> getH2SeekableClass()
{    try {        return (Class<SeekableInputStream>) Class.forName("org.apache.parquet.hadoop.util.H2SeekableInputStream");    } catch (ClassNotFoundException e) {        return null;    } catch (NoClassDefFoundError e) {        return null;    }}
0
private static Constructor<SeekableInputStream> getH2SeekableConstructor()
{    Class<SeekableInputStream> h2SeekableClass = getH2SeekableClass();    if (h2SeekableClass != null) {        try {            return h2SeekableClass.getConstructor(FSDataInputStream.class);        } catch (NoSuchMethodException e) {            return null;        }    }    return null;}
0
public static PositionOutputStream wrap(FSDataOutputStream stream)
{    Preconditions.checkNotNull(stream, "Cannot wrap a null output stream");    return new HadoopPositionOutputStream(stream);}
0
public boolean accept(Path p)
{    final char c = p.getName().charAt(0);    return c != '.' && c != '_';}
0
public static void writeObjectToConfAsBase64(String key, Object obj, Configuration conf) throws IOException
{    try (ByteArrayOutputStream baos = new ByteArrayOutputStream()) {        try (GZIPOutputStream gos = new GZIPOutputStream(baos);            ObjectOutputStream oos = new ObjectOutputStream(gos)) {            oos.writeObject(obj);        }        conf.set(key, new String(Base64.getMimeEncoder().encode(baos.toByteArray()), StandardCharsets.UTF_8));    }}
0
public static T readObjectFromConfAsBase64(String key, Configuration conf) throws IOException
{    String b64 = conf.get(key);    if (b64 == null) {        return null;    }    byte[] bytes = Base64.getMimeDecoder().decode(b64.getBytes(StandardCharsets.UTF_8));    try (ByteArrayInputStream bais = new ByteArrayInputStream(bytes);        GZIPInputStream gis = new GZIPInputStream(bais);        ObjectInputStream ois = new ObjectInputStream(gis)) {        return (T) ois.readObject();    } catch (ClassNotFoundException e) {        throw new IOException("Could not read object from config with key " + key, e);    } catch (ClassCastException e) {        throw new IOException("Could not cast object read from config with key " + key, e);    }}
0
public String getProperty(String property)
{    String value = super.getProperty(property);    if (value != null) {        return value;    }    return conf.get(property);}
0
public Configuration getConf()
{    return conf;}
0
public static Builder builder(Configuration conf)
{    return new Builder(conf);}
0
public ParquetReadOptions build()
{    return new HadoopReadOptions(useSignedStringMinMax, useStatsFilter, useDictionaryFilter, useRecordFilter, useColumnIndexFilter, usePageChecksumVerification, recordFilter, metadataFilter, codecFactory, allocator, maxAllocationSize, properties, conf);}
0
public long getOffset()
{    return offset;}
0
public int getLength()
{    return length;}
0
public boolean useSignedStringMinMax()
{    return useSignedStringMinMax;}
0
public boolean useStatsFilter()
{    return useStatsFilter;}
0
public boolean useDictionaryFilter()
{    return useDictionaryFilter;}
0
public boolean useRecordFilter()
{    return useRecordFilter;}
0
public boolean useColumnIndexFilter()
{    return useColumnIndexFilter;}
0
public boolean usePageChecksumVerification()
{    return usePageChecksumVerification;}
0
public FilterCompat.Filter getRecordFilter()
{    return recordFilter;}
0
public ParquetMetadataConverter.MetadataFilter getMetadataFilter()
{    return metadataFilter;}
0
public CompressionCodecFactory getCodecFactory()
{    return codecFactory;}
0
public ByteBufferAllocator getAllocator()
{    return allocator;}
0
public int getMaxAllocationSize()
{    return maxAllocationSize;}
0
public Set<String> getPropertyNames()
{    return properties.keySet();}
0
public String getProperty(String property)
{    return properties.get(property);}
0
public boolean isEnabled(String property, boolean defaultValue)
{    if (properties.containsKey(property)) {        return Boolean.valueOf(properties.get(property));    } else {        return defaultValue;    }}
0
public static Builder builder()
{    return new Builder();}
0
public Builder useSignedStringMinMax(boolean useSignedStringMinMax)
{    this.useSignedStringMinMax = useSignedStringMinMax;    return this;}
0
public Builder useSignedStringMinMax()
{    this.useSignedStringMinMax = true;    return this;}
0
public Builder useStatsFilter(boolean useStatsFilter)
{    this.useStatsFilter = useStatsFilter;    return this;}
0
public Builder useStatsFilter()
{    this.useStatsFilter = true;    return this;}
0
public Builder useDictionaryFilter(boolean useDictionaryFilter)
{    this.useDictionaryFilter = useDictionaryFilter;    return this;}
0
public Builder useDictionaryFilter()
{    this.useDictionaryFilter = true;    return this;}
0
public Builder useRecordFilter(boolean useRecordFilter)
{    this.useRecordFilter = useRecordFilter;    return this;}
0
public Builder useRecordFilter()
{    this.useRecordFilter = true;    return this;}
0
public Builder useColumnIndexFilter(boolean useColumnIndexFilter)
{    this.useColumnIndexFilter = useColumnIndexFilter;    return this;}
0
public Builder useColumnIndexFilter()
{    return useColumnIndexFilter(true);}
0
public Builder usePageChecksumVerification(boolean usePageChecksumVerification)
{    this.usePageChecksumVerification = usePageChecksumVerification;    return this;}
0
public Builder usePageChecksumVerification()
{    return usePageChecksumVerification(true);}
0
public Builder withRecordFilter(FilterCompat.Filter rowGroupFilter)
{    this.recordFilter = rowGroupFilter;    return this;}
0
public Builder withRange(long start, long end)
{    this.metadataFilter = ParquetMetadataConverter.range(start, end);    return this;}
0
public Builder withOffsets(long... rowGroupOffsets)
{    this.metadataFilter = ParquetMetadataConverter.offsets(rowGroupOffsets);    return this;}
0
public Builder withMetadataFilter(ParquetMetadataConverter.MetadataFilter metadataFilter)
{    this.metadataFilter = metadataFilter;    return this;}
0
public Builder withCodecFactory(CompressionCodecFactory codecFactory)
{    this.codecFactory = codecFactory;    return this;}
0
public Builder withAllocator(ByteBufferAllocator allocator)
{    this.allocator = allocator;    return this;}
0
public Builder withMaxAllocationInBytes(int allocationSizeInBytes)
{    this.maxAllocationSize = allocationSizeInBytes;    return this;}
0
public Builder withPageChecksumVerification(boolean val)
{    this.usePageChecksumVerification = val;    return this;}
0
public Builder set(String key, String value)
{    properties.put(key, value);    return this;}
0
public Builder copy(ParquetReadOptions options)
{    useSignedStringMinMax(options.useSignedStringMinMax);    useStatsFilter(options.useStatsFilter);    useDictionaryFilter(options.useDictionaryFilter);    useRecordFilter(options.useRecordFilter);    withRecordFilter(options.recordFilter);    withMetadataFilter(options.metadataFilter);    withCodecFactory(options.codecFactory);    withAllocator(options.allocator);    withPageChecksumVerification(options.usePageChecksumVerification);    for (Map.Entry<String, String> keyValue : options.properties.entrySet()) {        set(keyValue.getKey(), keyValue.getValue());    }    return this;}
0
public ParquetReadOptions build()
{    return new ParquetReadOptions(useSignedStringMinMax, useStatsFilter, useDictionaryFilter, useRecordFilter, useColumnIndexFilter, usePageChecksumVerification, recordFilter, metadataFilter, codecFactory, allocator, maxAllocationSize, properties);}
0
protected Path writeDirect(String type, DirectWriter writer) throws IOException
{    return writeDirect(MessageTypeParser.parseMessageType(type), writer);}
0
protected Path writeDirect(String type, DirectWriter writer, Map<String, String> metadata) throws IOException
{    return writeDirect(MessageTypeParser.parseMessageType(type), writer, metadata);}
0
protected Path writeDirect(MessageType type, DirectWriter writer) throws IOException
{    return writeDirect(type, writer, new HashMap<String, String>());}
0
protected Path writeDirect(MessageType type, DirectWriter writer, Map<String, String> metadata) throws IOException
{    File temp = tempDir.newFile(UUID.randomUUID().toString());    temp.deleteOnExit();    temp.delete();    Path path = new Path(temp.getPath());    ParquetWriter<Void> parquetWriter = new ParquetWriter<Void>(path, new DirectWriteSupport(type, writer, metadata));    parquetWriter.write(null);    parquetWriter.close();    return path;}
0
public WriteContext init(Configuration configuration)
{    return new WriteContext(type, metadata);}
0
public void prepareForWrite(RecordConsumer recordConsumer)
{    this.recordConsumer = recordConsumer;}
0
public void write(Void record)
{    writer.write(recordConsumer);}
0
public static Collection<Object[]> getParameters()
{    List<PrimitiveTypeName> types = Arrays.asList(PrimitiveTypeName.BOOLEAN, PrimitiveTypeName.INT32, PrimitiveTypeName.INT64, PrimitiveTypeName.INT96, PrimitiveTypeName.FLOAT, PrimitiveTypeName.DOUBLE, PrimitiveTypeName.BINARY, PrimitiveTypeName.FIXED_LEN_BYTE_ARRAY);    List<CompressionCodecName> codecs;    String codecList = System.getenv("TEST_CODECS");    if (codecList != null) {        codecs = new ArrayList<CompressionCodecName>();        for (String codec : codecList.split(",")) {            codecs.add(CompressionCodecName.valueOf(codec.toUpperCase(Locale.ENGLISH)));        }    } else {                codecs = Arrays.asList(CompressionCodecName.UNCOMPRESSED);    }    System.err.println("Testing codecs: " + codecs);    List<Object[]> parameters = new ArrayList<Object[]>();    for (PrimitiveTypeName type : types) {        for (CompressionCodecName codec : codecs) {            parameters.add(new Object[] { type, codec });        }    }    return parameters;}
0
public static void initialize() throws IOException
{    Random random = new Random(RANDOM_SEED);    intGenerator = new RandomValues.IntGenerator(random.nextLong());    longGenerator = new RandomValues.LongGenerator(random.nextLong());    int96Generator = new RandomValues.Int96Generator(random.nextLong());    floatGenerator = new RandomValues.FloatGenerator(random.nextLong());    doubleGenerator = new RandomValues.DoubleGenerator(random.nextLong());    binaryGenerator = new RandomValues.BinaryGenerator(random.nextLong());    fixedBinaryGenerator = new RandomValues.FixedGenerator(random.nextLong(), FIXED_LENGTH);}
0
public void testFileEncodingsWithoutDictionary() throws Exception
{    final boolean DISABLE_DICTIONARY = false;    List<?> randomValues;    randomValues = generateRandomValues(this.paramTypeName, RECORD_COUNT);    /* Run an encoding test per each writer version.     * This loop will make sure to test future writer versions added to WriterVersion enum.     */    for (WriterVersion writerVersion : WriterVersion.values()) {        System.out.println(String.format("Testing %s/%s/%s encodings using ROW_GROUP_SIZE=%d PAGE_SIZE=%d", writerVersion, this.paramTypeName, this.compression, TEST_ROW_GROUP_SIZE, TEST_PAGE_SIZE));        Path parquetFile = createTempFile();        writeValuesToFile(parquetFile, this.paramTypeName, randomValues, TEST_ROW_GROUP_SIZE, TEST_PAGE_SIZE, DISABLE_DICTIONARY, writerVersion);        PageGroupValidator.validatePages(parquetFile, randomValues);    }}
0
public void testFileEncodingsWithDictionary() throws Exception
{    final boolean ENABLE_DICTIONARY = true;    List<?> dictionaryValues = generateDictionaryValues(this.paramTypeName, RECORD_COUNT);    /* Run an encoding test per each writer version.     * This loop will make sure to test future writer versions added to WriterVersion enum.     */    for (WriterVersion writerVersion : WriterVersion.values()) {        System.out.println(String.format("Testing %s/%s/%s + DICTIONARY encodings using ROW_GROUP_SIZE=%d PAGE_SIZE=%d", writerVersion, this.paramTypeName, this.compression, TEST_ROW_GROUP_SIZE, TEST_PAGE_SIZE));        Path parquetFile = createTempFile();        writeValuesToFile(parquetFile, this.paramTypeName, dictionaryValues, TEST_ROW_GROUP_SIZE, TEST_PAGE_SIZE, ENABLE_DICTIONARY, writerVersion);        PageGroupValidator.validatePages(parquetFile, dictionaryValues);    }}
0
private Path createTempFile() throws IOException
{    File tempFile = tempFolder.newFile();    tempFile.delete();    return new Path(tempFile.getAbsolutePath());}
0
private void writeValuesToFile(Path file, PrimitiveTypeName type, List<?> values, int rowGroupSize, int pageSize, boolean enableDictionary, WriterVersion version) throws IOException
{    MessageType schema;    if (type == PrimitiveTypeName.FIXED_LEN_BYTE_ARRAY) {        schema = Types.buildMessage().required(type).length(FIXED_LENGTH).named("field").named("test");    } else {        schema = Types.buildMessage().required(type).named("field").named("test");    }    SimpleGroupFactory message = new SimpleGroupFactory(schema);    GroupWriteSupport.setSchema(schema, configuration);    ParquetWriter<Group> writer = ExampleParquetWriter.builder(file).withCompressionCodec(compression).withRowGroupSize(rowGroupSize).withPageSize(pageSize).withDictionaryPageSize(TEST_DICT_PAGE_SIZE).withDictionaryEncoding(enableDictionary).withWriterVersion(version).withConf(configuration).build();    for (Object o : values) {        switch(type) {            case BOOLEAN:                writer.write(message.newGroup().append("field", (Boolean) o));                break;            case INT32:                writer.write(message.newGroup().append("field", (Integer) o));                break;            case INT64:                writer.write(message.newGroup().append("field", (Long) o));                break;            case FLOAT:                writer.write(message.newGroup().append("field", (Float) o));                break;            case DOUBLE:                writer.write(message.newGroup().append("field", (Double) o));                break;            case INT96:            case BINARY:            case FIXED_LEN_BYTE_ARRAY:                writer.write(message.newGroup().append("field", (Binary) o));                break;            default:                throw new IllegalArgumentException("Unknown type name: " + type);        }    }    writer.close();}
0
private List<?> generateRandomValues(PrimitiveTypeName type, int count)
{    List<Object> values = new ArrayList<Object>();    for (int i = 0; i < count; i++) {        Object value;        switch(type) {            case BOOLEAN:                value = (intGenerator.nextValue() % 2 == 0) ? true : false;                break;            case INT32:                value = intGenerator.nextValue();                break;            case INT64:                value = longGenerator.nextValue();                break;            case FLOAT:                value = floatGenerator.nextValue();                break;            case DOUBLE:                value = doubleGenerator.nextValue();                break;            case INT96:                value = int96Generator.nextBinaryValue();                break;            case BINARY:                value = binaryGenerator.nextBinaryValue();                break;            case FIXED_LEN_BYTE_ARRAY:                value = fixedBinaryGenerator.nextBinaryValue();                break;            default:                throw new IllegalArgumentException("Unknown type name: " + type);        }        values.add(value);    }    return values;}
0
private List<?> generateDictionaryValues(PrimitiveTypeName type, int count)
{    final int DICT_VALUES_SIZE = 100;    final List<?> DICT_BINARY_VALUES = generateRandomValues(PrimitiveTypeName.BINARY, DICT_VALUES_SIZE);    final List<?> DICT_INT96_VALUES = generateRandomValues(PrimitiveTypeName.INT96, DICT_VALUES_SIZE);    final List<?> DICT_FIXED_LEN_VALUES = generateRandomValues(PrimitiveTypeName.FIXED_LEN_BYTE_ARRAY, DICT_VALUES_SIZE);    List<Object> values = new ArrayList<Object>();    for (int i = 0; i < count; i++) {        int dictValue = i % DICT_VALUES_SIZE;        Object value;        switch(type) {            case BOOLEAN:                value = (i % 2 == 0) ? true : false;                break;            case INT32:                value = dictValue;                break;            case INT64:                value = (long) dictValue;                break;            case FLOAT:                value = (float) dictValue;                break;            case DOUBLE:                value = (double) dictValue;                break;            case INT96:                value = DICT_INT96_VALUES.get(dictValue);                break;            case BINARY:                value = DICT_BINARY_VALUES.get(dictValue);                break;            case FIXED_LEN_BYTE_ARRAY:                value = DICT_FIXED_LEN_VALUES.get(dictValue);                break;            default:                throw new IllegalArgumentException("Unknown type name: " + type);        }        values.add(value);    }    return values;}
0
public static void validatePages(Path file, List<?> expectedValues) throws IOException
{    List<PageReadStore> blockReaders = readBlocksFromFile(file);    MessageType fileSchema = readSchemaFromFile(file);    int rowGroupID = 0;    int rowsRead = 0;    for (PageReadStore pageReadStore : blockReaders) {        for (ColumnDescriptor columnsDesc : fileSchema.getColumns()) {            List<DataPage> pageGroup = getPageGroupForColumn(pageReadStore, columnsDesc);            DictionaryPage dictPage = reusableCopy(getDictionaryPageForColumn(pageReadStore, columnsDesc));            List<?> expectedRowGroupValues = expectedValues.subList(rowsRead, (int) (rowsRead + pageReadStore.getRowCount()));            validateFirstToLast(rowGroupID, dictPage, pageGroup, columnsDesc, expectedRowGroupValues);            validateLastToFirst(rowGroupID, dictPage, pageGroup, columnsDesc, expectedRowGroupValues);        }        rowsRead += pageReadStore.getRowCount();        rowGroupID++;    }}
0
private static DictionaryPage reusableCopy(DictionaryPage dict)
{    if (dict == null) {        return null;    }    try {        return new DictionaryPage(BytesInput.from(dict.getBytes().toByteArray()), dict.getDictionarySize(), dict.getEncoding());    } catch (IOException e) {        throw new ParquetDecodingException("Cannot read dictionary", e);    }}
0
private static DataPage reusableCopy(DataPage page)
{    return page.accept(new DataPage.Visitor<DataPage>() {        @Override        public DataPage visit(DataPageV1 data) {            try {                return new DataPageV1(BytesInput.from(data.getBytes().toByteArray()), data.getValueCount(), data.getUncompressedSize(), data.getStatistics(), data.getRlEncoding(), data.getDlEncoding(), data.getValueEncoding());            } catch (IOException e) {                throw new ParquetDecodingException("Cannot read data", e);            }        }        @Override        public DataPage visit(DataPageV2 data) {            try {                return new DataPageV2(data.getRowCount(), data.getNullCount(), data.getValueCount(), BytesInput.from(data.getRepetitionLevels().toByteArray()), BytesInput.from(data.getDefinitionLevels().toByteArray()), data.getDataEncoding(), BytesInput.from(data.getData().toByteArray()), data.getUncompressedSize(), data.getStatistics(), data.isCompressed());            } catch (IOException e) {                throw new ParquetDecodingException("Cannot read data", e);            }        }    });}
0
public DataPage visit(DataPageV1 data)
{    try {        return new DataPageV1(BytesInput.from(data.getBytes().toByteArray()), data.getValueCount(), data.getUncompressedSize(), data.getStatistics(), data.getRlEncoding(), data.getDlEncoding(), data.getValueEncoding());    } catch (IOException e) {        throw new ParquetDecodingException("Cannot read data", e);    }}
0
public DataPage visit(DataPageV2 data)
{    try {        return new DataPageV2(data.getRowCount(), data.getNullCount(), data.getValueCount(), BytesInput.from(data.getRepetitionLevels().toByteArray()), BytesInput.from(data.getDefinitionLevels().toByteArray()), data.getDataEncoding(), BytesInput.from(data.getData().toByteArray()), data.getUncompressedSize(), data.getStatistics(), data.isCompressed());    } catch (IOException e) {        throw new ParquetDecodingException("Cannot read data", e);    }}
0
private static void validateFirstToLast(int rowGroupID, DictionaryPage dictPage, List<DataPage> pageGroup, ColumnDescriptor desc, List<?> expectedValues)
{    int rowsRead = 0, pageID = 0;    for (DataPage page : pageGroup) {        List<?> expectedPageValues = expectedValues.subList(rowsRead, rowsRead + page.getValueCount());        PageValuesValidator.validateValuesForPage(rowGroupID, pageID, dictPage, page, desc, expectedPageValues);        rowsRead += page.getValueCount();        pageID++;    }}
0
private static void validateLastToFirst(int rowGroupID, DictionaryPage dictPage, List<DataPage> pageGroup, ColumnDescriptor desc, List<?> expectedValues)
{    int rowsLeft = expectedValues.size();    for (int pageID = pageGroup.size() - 1; pageID >= 0; pageID--) {        DataPage page = pageGroup.get(pageID);        int offset = rowsLeft - page.getValueCount();        List<?> expectedPageValues = expectedValues.subList(offset, offset + page.getValueCount());        PageValuesValidator.validateValuesForPage(rowGroupID, pageID, dictPage, page, desc, expectedPageValues);        rowsLeft -= page.getValueCount();    }}
0
private static DictionaryPage getDictionaryPageForColumn(PageReadStore pageReadStore, ColumnDescriptor columnDescriptor)
{    PageReader pageReader = pageReadStore.getPageReader(columnDescriptor);    return pageReader.readDictionaryPage();}
0
private static List<DataPage> getPageGroupForColumn(PageReadStore pageReadStore, ColumnDescriptor columnDescriptor)
{    PageReader pageReader = pageReadStore.getPageReader(columnDescriptor);    List<DataPage> pageGroup = new ArrayList<DataPage>();    DataPage page;    while ((page = pageReader.readPage()) != null) {        pageGroup.add(reusableCopy(page));    }    return pageGroup;}
0
private static MessageType readSchemaFromFile(Path file) throws IOException
{    ParquetMetadata metadata = ParquetFileReader.readFooter(configuration, file, ParquetMetadataConverter.NO_FILTER);    return metadata.getFileMetaData().getSchema();}
0
private static List<PageReadStore> readBlocksFromFile(Path file) throws IOException
{    List<PageReadStore> rowGroups = new ArrayList<PageReadStore>();    ParquetMetadata metadata = ParquetFileReader.readFooter(configuration, file, ParquetMetadataConverter.NO_FILTER);    ParquetFileReader fileReader = new ParquetFileReader(configuration, metadata.getFileMetaData(), file, metadata.getBlocks(), metadata.getFileMetaData().getSchema().getColumns());    PageReadStore group;    while ((group = fileReader.readNextRowGroup()) != null) {        rowGroups.add(group);    }    return rowGroups;}
0
public void validateNextValue(Object value)
{    assertEquals(String.format("Value from page is different than expected, ROW_GROUP_ID=%d PAGE_ID=%d VALUE_POS=%d", rowGroupID, pageID, currentPos), expectedValues.get(currentPos++), value);}
0
public static void validateValuesForPage(int rowGroupID, int pageID, DictionaryPage dictPage, DataPage page, ColumnDescriptor columnDesc, List<?> expectedValues)
{    TestStatistics.SingletonPageReader pageReader = new TestStatistics.SingletonPageReader(dictPage, page);    PrimitiveConverter converter = getConverter(rowGroupID, pageID, columnDesc.getType(), expectedValues);    ColumnReaderImpl column = new ColumnReaderImpl(columnDesc, pageReader, converter, null);    for (int i = 0; i < pageReader.getTotalValueCount(); i += 1) {        column.writeCurrentValueToConverter();        column.consume();    }}
0
private static PrimitiveConverter getConverter(final int rowGroupID, final int pageID, PrimitiveTypeName type, final List<?> expectedValues)
{    return type.convert(new PrimitiveType.PrimitiveTypeNameConverter<PrimitiveConverter, RuntimeException>() {        @Override        public PrimitiveConverter convertFLOAT(PrimitiveTypeName primitiveTypeName) throws RuntimeException {            final PageValuesValidator validator = new PageValuesValidator(rowGroupID, pageID, expectedValues);            return new PrimitiveConverter() {                @Override                public void addFloat(float value) {                    validator.validateNextValue(value);                }            };        }        @Override        public PrimitiveConverter convertDOUBLE(PrimitiveTypeName primitiveTypeName) throws RuntimeException {            final PageValuesValidator validator = new PageValuesValidator(rowGroupID, pageID, expectedValues);            return new PrimitiveConverter() {                @Override                public void addDouble(double value) {                    validator.validateNextValue(value);                }            };        }        @Override        public PrimitiveConverter convertINT32(PrimitiveTypeName primitiveTypeName) throws RuntimeException {            final PageValuesValidator validator = new PageValuesValidator(rowGroupID, pageID, expectedValues);            return new PrimitiveConverter() {                @Override                public void addInt(int value) {                    validator.validateNextValue(value);                }            };        }        @Override        public PrimitiveConverter convertINT64(PrimitiveTypeName primitiveTypeName) throws RuntimeException {            final PageValuesValidator validator = new PageValuesValidator(rowGroupID, pageID, expectedValues);            return new PrimitiveConverter() {                @Override                public void addLong(long value) {                    validator.validateNextValue(value);                }            };        }        @Override        public PrimitiveConverter convertINT96(PrimitiveTypeName primitiveTypeName) throws RuntimeException {            return convertBINARY(primitiveTypeName);        }        @Override        public PrimitiveConverter convertFIXED_LEN_BYTE_ARRAY(PrimitiveTypeName primitiveTypeName) throws RuntimeException {            return convertBINARY(primitiveTypeName);        }        @Override        public PrimitiveConverter convertBOOLEAN(PrimitiveTypeName primitiveTypeName) throws RuntimeException {            final PageValuesValidator validator = new PageValuesValidator(rowGroupID, pageID, expectedValues);            return new PrimitiveConverter() {                @Override                public void addBoolean(boolean value) {                    validator.validateNextValue(value);                }            };        }        @Override        public PrimitiveConverter convertBINARY(PrimitiveTypeName primitiveTypeName) throws RuntimeException {            final PageValuesValidator validator = new PageValuesValidator(rowGroupID, pageID, expectedValues);            return new PrimitiveConverter() {                @Override                public void addBinary(Binary value) {                    validator.validateNextValue(value);                }            };        }    });}
0
public PrimitiveConverter convertFLOAT(PrimitiveTypeName primitiveTypeName) throws RuntimeException
{    final PageValuesValidator validator = new PageValuesValidator(rowGroupID, pageID, expectedValues);    return new PrimitiveConverter() {        @Override        public void addFloat(float value) {            validator.validateNextValue(value);        }    };}
0
public void addFloat(float value)
{    validator.validateNextValue(value);}
0
public PrimitiveConverter convertDOUBLE(PrimitiveTypeName primitiveTypeName) throws RuntimeException
{    final PageValuesValidator validator = new PageValuesValidator(rowGroupID, pageID, expectedValues);    return new PrimitiveConverter() {        @Override        public void addDouble(double value) {            validator.validateNextValue(value);        }    };}
0
public void addDouble(double value)
{    validator.validateNextValue(value);}
0
public PrimitiveConverter convertINT32(PrimitiveTypeName primitiveTypeName) throws RuntimeException
{    final PageValuesValidator validator = new PageValuesValidator(rowGroupID, pageID, expectedValues);    return new PrimitiveConverter() {        @Override        public void addInt(int value) {            validator.validateNextValue(value);        }    };}
0
public void addInt(int value)
{    validator.validateNextValue(value);}
0
public PrimitiveConverter convertINT64(PrimitiveTypeName primitiveTypeName) throws RuntimeException
{    final PageValuesValidator validator = new PageValuesValidator(rowGroupID, pageID, expectedValues);    return new PrimitiveConverter() {        @Override        public void addLong(long value) {            validator.validateNextValue(value);        }    };}
0
public void addLong(long value)
{    validator.validateNextValue(value);}
0
public PrimitiveConverter convertINT96(PrimitiveTypeName primitiveTypeName) throws RuntimeException
{    return convertBINARY(primitiveTypeName);}
0
public PrimitiveConverter convertFIXED_LEN_BYTE_ARRAY(PrimitiveTypeName primitiveTypeName) throws RuntimeException
{    return convertBINARY(primitiveTypeName);}
0
public PrimitiveConverter convertBOOLEAN(PrimitiveTypeName primitiveTypeName) throws RuntimeException
{    final PageValuesValidator validator = new PageValuesValidator(rowGroupID, pageID, expectedValues);    return new PrimitiveConverter() {        @Override        public void addBoolean(boolean value) {            validator.validateNextValue(value);        }    };}
0
public void addBoolean(boolean value)
{    validator.validateNextValue(value);}
0
public PrimitiveConverter convertBINARY(PrimitiveTypeName primitiveTypeName) throws RuntimeException
{    final PageValuesValidator validator = new PageValuesValidator(rowGroupID, pageID, expectedValues);    return new PrimitiveConverter() {        @Override        public void addBinary(Binary value) {            validator.validateNextValue(value);        }    };}
0
public void addBinary(Binary value)
{    validator.validateNextValue(value);}
0
public void testApplyRowGroupFilters()
{    List<BlockMetaData> blocks = new ArrayList<BlockMetaData>();    IntStatistics stats1 = new IntStatistics();    stats1.setMinMax(10, 100);    stats1.setNumNulls(4);    BlockMetaData b1 = makeBlockFromStats(stats1, 301);    blocks.add(b1);    IntStatistics stats2 = new IntStatistics();    stats2.setMinMax(8, 102);    stats2.setNumNulls(0);    BlockMetaData b2 = makeBlockFromStats(stats2, 302);    blocks.add(b2);    IntStatistics stats3 = new IntStatistics();    stats3.setMinMax(100, 102);    stats3.setNumNulls(12);    BlockMetaData b3 = makeBlockFromStats(stats3, 303);    blocks.add(b3);    IntStatistics stats4 = new IntStatistics();    stats4.setMinMax(0, 0);    stats4.setNumNulls(304);    BlockMetaData b4 = makeBlockFromStats(stats4, 304);    blocks.add(b4);    IntStatistics stats5 = new IntStatistics();    stats5.setMinMax(50, 50);    stats5.setNumNulls(7);    BlockMetaData b5 = makeBlockFromStats(stats5, 305);    blocks.add(b5);    IntStatistics stats6 = new IntStatistics();    stats6.setMinMax(0, 0);    stats6.setNumNulls(12);    BlockMetaData b6 = makeBlockFromStats(stats6, 306);    blocks.add(b6);    MessageType schema = MessageTypeParser.parseMessageType("message Document { optional int32 foo; }");    IntColumn foo = intColumn("foo");    List<BlockMetaData> filtered = RowGroupFilter.filterRowGroups(FilterCompat.get(eq(foo, 50)), blocks, schema);    assertEquals(Arrays.asList(b1, b2, b5), filtered);    filtered = RowGroupFilter.filterRowGroups(FilterCompat.get(notEq(foo, 50)), blocks, schema);    assertEquals(Arrays.asList(b1, b2, b3, b4, b5, b6), filtered);    filtered = RowGroupFilter.filterRowGroups(FilterCompat.get(eq(foo, null)), blocks, schema);    assertEquals(Arrays.asList(b1, b3, b4, b5, b6), filtered);    filtered = RowGroupFilter.filterRowGroups(FilterCompat.get(notEq(foo, null)), blocks, schema);    assertEquals(Arrays.asList(b1, b2, b3, b5, b6), filtered);    filtered = RowGroupFilter.filterRowGroups(FilterCompat.get(eq(foo, 0)), blocks, schema);    assertEquals(Arrays.asList(b6), filtered);}
0
private static Binary toBinary(String decimalWithoutScale, int byteCount)
{    return toBinary(new BigInteger(decimalWithoutScale), byteCount);}
0
private static Binary toBinary(BigInteger decimalWithoutScale, int byteCount)
{    byte[] src = decimalWithoutScale.toByteArray();    if (src.length > byteCount) {        throw new IllegalArgumentException("Too large decimal value for byte count " + byteCount);    }    byte[] dest = new byte[byteCount];    System.arraycopy(src, 0, dest, dest.length - src.length, src.length);    return Binary.fromConstantByteArray(dest);}
0
private static void writeData(SimpleGroupFactory f, ParquetWriter<Group> writer) throws IOException
{    for (int i = 0; i < nElements; i++) {        int index = i % ALPHABET.length();        Group group = f.newGroup().append("binary_field", ALPHABET.substring(index, index + 1)).append("single_value_field", "sharp").append("fixed_field", DECIMAL_VALUES[i % DECIMAL_VALUES.length]).append("int32_field", intValues[i % intValues.length]).append("int64_field", longValues[i % longValues.length]).append("double_field", toDouble(intValues[i % intValues.length])).append("float_field", toFloat(intValues[i % intValues.length])).append("plain_int32_field", i).append("fallback_binary_field", i < (nElements / 2) ? ALPHABET.substring(index, index + 1) : UUID.randomUUID().toString()).append("int96_field", INT96_VALUES[i % INT96_VALUES.length]);                if (index % 10 > 0) {            group.append("optional_single_value_field", "sharp");        }        writer.write(group);    }    writer.close();}
0
public static void prepareFile() throws IOException
{    cleanup();    prepareFile(PARQUET_1_0, FILE_V1);    prepareFile(PARQUET_2_0, FILE_V2);}
0
private static void prepareFile(WriterVersion version, Path file) throws IOException
{    GroupWriteSupport.setSchema(schema, conf);    SimpleGroupFactory f = new SimpleGroupFactory(schema);    ParquetWriter<Group> writer = ExampleParquetWriter.builder(file).withWriterVersion(version).withCompressionCodec(GZIP).withRowGroupSize(1024 * 1024).withPageSize(1024).enableDictionaryEncoding().withDictionaryPageSize(2 * 1024).withConf(conf).build();    writeData(f, writer);}
0
public static void cleanup() throws IOException
{    deleteFile(FILE_V1);    deleteFile(FILE_V2);}
0
private static void deleteFile(Path file) throws IOException
{    FileSystem fs = file.getFileSystem(conf);    if (fs.exists(file)) {        fs.delete(file, true);    }}
0
public static Object[] params()
{    return new Object[] { PARQUET_1_0, PARQUET_2_0 };}
0
public void setUp() throws Exception
{    reader = ParquetFileReader.open(conf, file);    ParquetMetadata meta = reader.getFooter();    ccmd = meta.getBlocks().get(0).getColumns();    dictionaries = reader.getDictionaryReader(meta.getBlocks().get(0));}
0
public void tearDown() throws Exception
{    reader.close();}
0
public void testDictionaryEncodedColumns() throws Exception
{    switch(version) {        case PARQUET_1_0:            testDictionaryEncodedColumnsV1();            break;        case PARQUET_2_0:            testDictionaryEncodedColumnsV2();            break;    }}
0
private void testDictionaryEncodedColumnsV1() throws Exception
{    Set<String> dictionaryEncodedColumns = new HashSet<String>(Arrays.asList("binary_field", "single_value_field", "optional_single_value_field", "int32_field", "int64_field", "double_field", "float_field", "int96_field"));    for (ColumnChunkMetaData column : ccmd) {        String name = column.getPath().toDotString();        if (dictionaryEncodedColumns.contains(name)) {            assertTrue("Column should be dictionary encoded: " + name, column.getEncodings().contains(Encoding.PLAIN_DICTIONARY));            assertFalse("Column should not have plain data pages" + name, column.getEncodings().contains(Encoding.PLAIN));        } else {            assertTrue("Column should have plain encoding: " + name, column.getEncodings().contains(Encoding.PLAIN));            if (name.startsWith("fallback")) {                assertTrue("Column should have some dictionary encoding: " + name, column.getEncodings().contains(Encoding.PLAIN_DICTIONARY));            } else {                assertFalse("Column should have no dictionary encoding: " + name, column.getEncodings().contains(Encoding.PLAIN_DICTIONARY));            }        }    }}
0
private void testDictionaryEncodedColumnsV2() throws Exception
{    Set<String> dictionaryEncodedColumns = new HashSet<String>(Arrays.asList("binary_field", "single_value_field", "optional_single_value_field", "fixed_field", "int32_field", "int64_field", "double_field", "float_field", "int96_field"));    for (ColumnChunkMetaData column : ccmd) {        EncodingStats encStats = column.getEncodingStats();        String name = column.getPath().toDotString();        if (dictionaryEncodedColumns.contains(name)) {            assertTrue("Column should have dictionary pages: " + name, encStats.hasDictionaryPages());            assertTrue("Column should have dictionary encoded pages: " + name, encStats.hasDictionaryEncodedPages());            assertFalse("Column should not have non-dictionary encoded pages: " + name, encStats.hasNonDictionaryEncodedPages());        } else {            assertTrue("Column should have non-dictionary encoded pages: " + name, encStats.hasNonDictionaryEncodedPages());            if (name.startsWith("fallback")) {                assertTrue("Column should have dictionary pages: " + name, encStats.hasDictionaryPages());                assertTrue("Column should have dictionary encoded pages: " + name, encStats.hasDictionaryEncodedPages());            } else {                assertFalse("Column should not have dictionary pages: " + name, encStats.hasDictionaryPages());                assertFalse("Column should not have dictionary encoded pages: " + name, encStats.hasDictionaryEncodedPages());            }        }    }}
0
public void testEqBinary() throws Exception
{    BinaryColumn b = binaryColumn("binary_field");    FilterPredicate pred = eq(b, Binary.fromString("c"));    assertFalse("Should not drop block for lower case letters", canDrop(pred, ccmd, dictionaries));    assertTrue("Should drop block for upper case letters", canDrop(eq(b, Binary.fromString("A")), ccmd, dictionaries));    assertFalse("Should not drop block for null", canDrop(eq(b, null), ccmd, dictionaries));}
0
public void testEqFixed() throws Exception
{    BinaryColumn b = binaryColumn("fixed_field");        if (version == PARQUET_2_0) {        assertTrue("Should drop block for -2", canDrop(eq(b, toBinary("-2", 17)), ccmd, dictionaries));    }    assertFalse("Should not drop block for -1", canDrop(eq(b, toBinary("-1", 17)), ccmd, dictionaries));    assertFalse("Should not drop block for null", canDrop(eq(b, null), ccmd, dictionaries));}
0
public void testEqInt96() throws Exception
{    BinaryColumn b = binaryColumn("int96_field");        assertFalse("Should not drop block for -2", canDrop(eq(b, toBinary("-2", 12)), ccmd, dictionaries));    assertFalse("Should not drop block for -1", canDrop(eq(b, toBinary("-1", 12)), ccmd, dictionaries));    assertFalse("Should not drop block for null", canDrop(eq(b, null), ccmd, dictionaries));}
0
public void testNotEqBinary() throws Exception
{    BinaryColumn sharp = binaryColumn("single_value_field");    BinaryColumn sharpAndNull = binaryColumn("optional_single_value_field");    BinaryColumn b = binaryColumn("binary_field");    assertTrue("Should drop block with only the excluded value", canDrop(notEq(sharp, Binary.fromString("sharp")), ccmd, dictionaries));    assertFalse("Should not drop block with any other value", canDrop(notEq(sharp, Binary.fromString("applause")), ccmd, dictionaries));    assertFalse("Should not drop block with only the excluded value and null", canDrop(notEq(sharpAndNull, Binary.fromString("sharp")), ccmd, dictionaries));    assertFalse("Should not drop block with any other value", canDrop(notEq(sharpAndNull, Binary.fromString("applause")), ccmd, dictionaries));    assertFalse("Should not drop block with a known value", canDrop(notEq(b, Binary.fromString("x")), ccmd, dictionaries));    assertFalse("Should not drop block with a known value", canDrop(notEq(b, Binary.fromString("B")), ccmd, dictionaries));    assertFalse("Should not drop block for null", canDrop(notEq(b, null), ccmd, dictionaries));}
0
public void testLtInt() throws Exception
{    IntColumn i32 = intColumn("int32_field");    int lowest = Integer.MAX_VALUE;    for (int value : intValues) {        lowest = Math.min(lowest, value);    }    assertTrue("Should drop: < lowest value", canDrop(lt(i32, lowest), ccmd, dictionaries));    assertFalse("Should not drop: < (lowest value + 1)", canDrop(lt(i32, lowest + 1), ccmd, dictionaries));    assertFalse("Should not drop: contains matching values", canDrop(lt(i32, Integer.MAX_VALUE), ccmd, dictionaries));}
0
public void testLtFixed() throws Exception
{    BinaryColumn fixed = binaryColumn("fixed_field");        if (version == PARQUET_2_0) {        assertTrue("Should drop: < lowest value", canDrop(lt(fixed, DECIMAL_VALUES[0]), ccmd, dictionaries));    }    assertFalse("Should not drop: < 2nd lowest value", canDrop(lt(fixed, DECIMAL_VALUES[1]), ccmd, dictionaries));}
0
public void testLtEqLong() throws Exception
{    LongColumn i64 = longColumn("int64_field");    long lowest = Long.MAX_VALUE;    for (long value : longValues) {        lowest = Math.min(lowest, value);    }    assertTrue("Should drop: <= lowest - 1", canDrop(ltEq(i64, lowest - 1), ccmd, dictionaries));    assertFalse("Should not drop: <= lowest", canDrop(ltEq(i64, lowest), ccmd, dictionaries));    assertFalse("Should not drop: contains matching values", canDrop(ltEq(i64, Long.MAX_VALUE), ccmd, dictionaries));}
0
public void testGtFloat() throws Exception
{    FloatColumn f = floatColumn("float_field");    float highest = Float.MIN_VALUE;    for (int value : intValues) {        highest = Math.max(highest, toFloat(value));    }    assertTrue("Should drop: > highest value", canDrop(gt(f, highest), ccmd, dictionaries));    assertFalse("Should not drop: > (highest value - 1.0)", canDrop(gt(f, highest - 1.0f), ccmd, dictionaries));    assertFalse("Should not drop: contains matching values", canDrop(gt(f, Float.MIN_VALUE), ccmd, dictionaries));}
0
public void testGtEqDouble() throws Exception
{    DoubleColumn d = doubleColumn("double_field");    double highest = Double.MIN_VALUE;    for (int value : intValues) {        highest = Math.max(highest, toDouble(value));    }    assertTrue("Should drop: >= highest + 0.00000001", canDrop(gtEq(d, highest + 0.00000001), ccmd, dictionaries));    assertFalse("Should not drop: >= highest", canDrop(gtEq(d, highest), ccmd, dictionaries));    assertFalse("Should not drop: contains matching values", canDrop(gtEq(d, Double.MIN_VALUE), ccmd, dictionaries));}
0
public void testAnd() throws Exception
{    BinaryColumn col = binaryColumn("binary_field");        FilterPredicate B = eq(col, Binary.fromString("B"));    FilterPredicate C = eq(col, Binary.fromString("C"));        FilterPredicate x = eq(col, Binary.fromString("x"));    FilterPredicate y = eq(col, Binary.fromString("y"));    assertTrue("Should drop when either predicate must be false", canDrop(and(B, y), ccmd, dictionaries));    assertTrue("Should drop when either predicate must be false", canDrop(and(x, C), ccmd, dictionaries));    assertTrue("Should drop when either predicate must be false", canDrop(and(B, C), ccmd, dictionaries));    assertFalse("Should not drop when either predicate could be true", canDrop(and(x, y), ccmd, dictionaries));}
0
public void testOr() throws Exception
{    BinaryColumn col = binaryColumn("binary_field");        FilterPredicate B = eq(col, Binary.fromString("B"));    FilterPredicate C = eq(col, Binary.fromString("C"));        FilterPredicate x = eq(col, Binary.fromString("x"));    FilterPredicate y = eq(col, Binary.fromString("y"));    assertFalse("Should not drop when one predicate could be true", canDrop(or(B, y), ccmd, dictionaries));    assertFalse("Should not drop when one predicate could be true", canDrop(or(x, C), ccmd, dictionaries));    assertTrue("Should drop when both predicates must be false", canDrop(or(B, C), ccmd, dictionaries));    assertFalse("Should not drop when one predicate could be true", canDrop(or(x, y), ccmd, dictionaries));}
0
public void testUdp() throws Exception
{    InInt32UDP dropabble = new InInt32UDP(ImmutableSet.of(42));    InInt32UDP undroppable = new InInt32UDP(ImmutableSet.of(205));    assertTrue("Should drop block for non-matching UDP", canDrop(userDefined(intColumn("int32_field"), dropabble), ccmd, dictionaries));    assertFalse("Should not drop block for matching UDP", canDrop(userDefined(intColumn("int32_field"), undroppable), ccmd, dictionaries));}
0
public void testInverseUdp() throws Exception
{    InInt32UDP droppable = new InInt32UDP(ImmutableSet.of(42));    InInt32UDP undroppable = new InInt32UDP(ImmutableSet.of(205));    Set<Integer> allValues = ImmutableSet.copyOf(Arrays.asList(ArrayUtils.toObject(intValues)));    InInt32UDP completeMatch = new InInt32UDP(allValues);    FilterPredicate inverse = LogicalInverseRewriter.rewrite(not(userDefined(intColumn("int32_field"), droppable)));    FilterPredicate inverse1 = LogicalInverseRewriter.rewrite(not(userDefined(intColumn("int32_field"), undroppable)));    FilterPredicate inverse2 = LogicalInverseRewriter.rewrite(not(userDefined(intColumn("int32_field"), completeMatch)));    assertFalse("Should not drop block for inverse of non-matching UDP", canDrop(inverse, ccmd, dictionaries));    assertFalse("Should not drop block for inverse of UDP with some matches", canDrop(inverse1, ccmd, dictionaries));    assertTrue("Should drop block for inverse of UDP with all matches", canDrop(inverse2, ccmd, dictionaries));}
0
public void testColumnWithoutDictionary() throws Exception
{    IntColumn plain = intColumn("plain_int32_field");    DictionaryPageReadStore dictionaryStore = mock(DictionaryPageReadStore.class);    assertFalse("Should never drop block using plain encoding", canDrop(eq(plain, -10), ccmd, dictionaryStore));    assertFalse("Should never drop block using plain encoding", canDrop(lt(plain, -10), ccmd, dictionaryStore));    assertFalse("Should never drop block using plain encoding", canDrop(ltEq(plain, -10), ccmd, dictionaryStore));    assertFalse("Should never drop block using plain encoding", canDrop(gt(plain, nElements + 10), ccmd, dictionaryStore));    assertFalse("Should never drop block using plain encoding", canDrop(gtEq(plain, nElements + 10), ccmd, dictionaryStore));    assertFalse("Should never drop block using plain encoding", canDrop(notEq(plain, nElements + 10), ccmd, dictionaryStore));    verifyZeroInteractions(dictionaryStore);}
0
public void testColumnWithDictionaryAndPlainEncodings() throws Exception
{    IntColumn plain = intColumn("fallback_binary_field");    DictionaryPageReadStore dictionaryStore = mock(DictionaryPageReadStore.class);    assertFalse("Should never drop block using plain encoding", canDrop(eq(plain, -10), ccmd, dictionaryStore));    assertFalse("Should never drop block using plain encoding", canDrop(lt(plain, -10), ccmd, dictionaryStore));    assertFalse("Should never drop block using plain encoding", canDrop(ltEq(plain, -10), ccmd, dictionaryStore));    assertFalse("Should never drop block using plain encoding", canDrop(gt(plain, nElements + 10), ccmd, dictionaryStore));    assertFalse("Should never drop block using plain encoding", canDrop(gtEq(plain, nElements + 10), ccmd, dictionaryStore));    assertFalse("Should never drop block using plain encoding", canDrop(notEq(plain, nElements + 10), ccmd, dictionaryStore));    verifyZeroInteractions(dictionaryStore);}
0
public void testEqMissingColumn() throws Exception
{    BinaryColumn b = binaryColumn("missing_column");    assertTrue("Should drop block for non-null query", canDrop(eq(b, Binary.fromString("any")), ccmd, dictionaries));    assertFalse("Should not drop block null query", canDrop(eq(b, null), ccmd, dictionaries));}
0
public void testNotEqMissingColumn() throws Exception
{    BinaryColumn b = binaryColumn("missing_column");    assertFalse("Should not drop block for non-null query", canDrop(notEq(b, Binary.fromString("any")), ccmd, dictionaries));    assertTrue("Should not drop block null query", canDrop(notEq(b, null), ccmd, dictionaries));}
0
public void testLtMissingColumn() throws Exception
{    BinaryColumn b = binaryColumn("missing_column");    assertTrue("Should drop block for any non-null query", canDrop(lt(b, Binary.fromString("any")), ccmd, dictionaries));}
0
public void testLtEqMissingColumn() throws Exception
{    BinaryColumn b = binaryColumn("missing_column");    assertTrue("Should drop block for any non-null query", canDrop(ltEq(b, Binary.fromString("any")), ccmd, dictionaries));}
0
public void testGtMissingColumn() throws Exception
{    BinaryColumn b = binaryColumn("missing_column");    assertTrue("Should drop block for any non-null query", canDrop(gt(b, Binary.fromString("any")), ccmd, dictionaries));}
0
public void testGtEqMissingColumn() throws Exception
{    BinaryColumn b = binaryColumn("missing_column");    assertTrue("Should drop block for any non-null query", canDrop(gtEq(b, Binary.fromString("any")), ccmd, dictionaries));}
0
public void testUdpMissingColumn() throws Exception
{    InInt32UDP nullRejecting = new InInt32UDP(ImmutableSet.of(42));    InInt32UDP nullAccepting = new InInt32UDP(Sets.newHashSet((Integer) null));    IntColumn fake = intColumn("missing_column");    assertTrue("Should drop block for null rejecting udp", canDrop(userDefined(fake, nullRejecting), ccmd, dictionaries));    assertFalse("Should not drop block for null accepting udp", canDrop(userDefined(fake, nullAccepting), ccmd, dictionaries));}
0
public void testInverseUdpMissingColumn() throws Exception
{    InInt32UDP nullRejecting = new InInt32UDP(ImmutableSet.of(42));    InInt32UDP nullAccepting = new InInt32UDP(Sets.newHashSet((Integer) null));    IntColumn fake = intColumn("missing_column");    assertTrue("Should drop block for null accepting udp", canDrop(LogicalInverseRewriter.rewrite(not(userDefined(fake, nullAccepting))), ccmd, dictionaries));    assertFalse("Should not drop block for null rejecting udp", canDrop(LogicalInverseRewriter.rewrite(not(userDefined(fake, nullRejecting))), ccmd, dictionaries));}
0
public boolean keep(Integer value)
{    return ints.contains(value);}
0
public boolean canDrop(Statistics<Integer> statistics)
{    return false;}
0
public boolean inverseCanDrop(Statistics<Integer> statistics)
{    return false;}
0
private static double toDouble(int value)
{    return (value * 1.0);}
0
private static float toFloat(int value)
{    return (float) (value * 2.0);}
0
public Double getLon()
{    return lon;}
0
public Double getLat()
{    return lat;}
0
public boolean equals(Object o)
{    if (this == o)        return true;    if (o == null || getClass() != o.getClass())        return false;    Location location = (Location) o;    if (lat != null ? !lat.equals(location.lat) : location.lat != null)        return false;    if (lon != null ? !lon.equals(location.lon) : location.lon != null)        return false;    return true;}
0
public int hashCode()
{    int result = lon != null ? lon.hashCode() : 0;    result = 31 * result + (lat != null ? lat.hashCode() : 0);    return result;}
0
public String toString()
{    return "Location [lon=" + lon + ", lat=" + lat + "]";}
0
public long getNumber()
{    return number;}
0
public String getKind()
{    return kind;}
0
public boolean equals(Object o)
{    if (this == o)        return true;    if (o == null || getClass() != o.getClass())        return false;    PhoneNumber that = (PhoneNumber) o;    if (number != that.number)        return false;    if (kind != null ? !kind.equals(that.kind) : that.kind != null)        return false;    return true;}
0
public int hashCode()
{    int result = (int) (number ^ (number >>> 32));    result = 31 * result + (kind != null ? kind.hashCode() : 0);    return result;}
0
public String toString()
{    return "PhoneNumber [number=" + number + ", kind=" + kind + "]";}
0
public long getId()
{    return id;}
0
public String getName()
{    return name;}
0
public List<PhoneNumber> getPhoneNumbers()
{    return phoneNumbers;}
0
public Location getLocation()
{    return location;}
0
public boolean equals(Object o)
{    if (this == o)        return true;    if (o == null || getClass() != o.getClass())        return false;    User user = (User) o;    if (id != user.id)        return false;    if (location != null ? !location.equals(user.location) : user.location != null)        return false;    if (name != null ? !name.equals(user.name) : user.name != null)        return false;    if (phoneNumbers != null ? !phoneNumbers.equals(user.phoneNumbers) : user.phoneNumbers != null)        return false;    return true;}
0
public int hashCode()
{    int result = (int) (id ^ (id >>> 32));    result = 31 * result + (name != null ? name.hashCode() : 0);    result = 31 * result + (phoneNumbers != null ? phoneNumbers.hashCode() : 0);    result = 31 * result + (location != null ? location.hashCode() : 0);    return result;}
0
public String toString()
{    return "User [id=" + id + ", name=" + name + ", phoneNumbers=" + phoneNumbers + ", location=" + location + "]";}
0
public static SimpleGroup groupFromUser(User user)
{    SimpleGroup root = new SimpleGroup(schema);    root.append("id", user.getId());    if (user.getName() != null) {        root.append("name", user.getName());    }    if (user.getPhoneNumbers() != null) {        Group phoneNumbers = root.addGroup("phoneNumbers");        for (PhoneNumber number : user.getPhoneNumbers()) {            Group phone = phoneNumbers.addGroup("phone");            phone.append("number", number.getNumber());            if (number.getKind() != null) {                phone.append("kind", number.getKind());            }        }    }    if (user.getLocation() != null) {        Group location = root.addGroup("location");        if (user.getLocation().getLon() != null) {            location.append("lon", user.getLocation().getLon());        }        if (user.getLocation().getLat() != null) {            location.append("lat", user.getLocation().getLat());        }    }    return root;}
0
private static User userFromGroup(Group root)
{    return new User(getLong(root, "id"), getString(root, "name"), getPhoneNumbers(getGroup(root, "phoneNumbers")), getLocation(getGroup(root, "location")));}
0
private static List<PhoneNumber> getPhoneNumbers(Group phoneNumbers)
{    if (phoneNumbers == null) {        return null;    }    List<PhoneNumber> list = new ArrayList<>();    for (int i = 0, n = phoneNumbers.getFieldRepetitionCount("phone"); i < n; ++i) {        Group phone = phoneNumbers.getGroup("phone", i);        list.add(new PhoneNumber(getLong(phone, "number"), getString(phone, "kind")));    }    return list;}
0
private static Location getLocation(Group location)
{    if (location == null) {        return null;    }    return new Location(getDouble(location, "lon"), getDouble(location, "lat"));}
0
private static boolean isNull(Group group, String field)
{    int repetition = group.getFieldRepetitionCount(field);    if (repetition == 0) {        return true;    } else if (repetition == 1) {        return false;    }    throw new AssertionError("Invalid repetitionCount " + repetition + " for field " + field + " in group " + group);}
0
private static Long getLong(Group group, String field)
{    return isNull(group, field) ? null : group.getLong(field, 0);}
0
private static String getString(Group group, String field)
{    return isNull(group, field) ? null : group.getString(field, 0);}
0
private static Double getDouble(Group group, String field)
{    return isNull(group, field) ? null : group.getDouble(field, 0);}
0
private static Group getGroup(Group group, String field)
{    return isNull(group, field) ? null : group.getGroup(field, 0);}
0
public static File writeToFile(List<User> users) throws IOException
{    File f = File.createTempFile("phonebook", ".parquet");    f.deleteOnExit();    if (!f.delete()) {        throw new IOException("couldn't delete tmp file" + f);    }    writeToFile(f, users);    return f;}
0
public static void writeToFile(File f, List<User> users) throws IOException
{    write(ExampleParquetWriter.builder(new Path(f.getAbsolutePath())), users);}
0
public static void write(ParquetWriter.Builder<Group, ?> builder, List<User> users) throws IOException
{    builder.config(GroupWriteSupport.PARQUET_EXAMPLE_SCHEMA, schema.toString());    try (ParquetWriter<Group> writer = builder.build()) {        for (User u : users) {            writer.write(groupFromUser(u));        }    }}
0
private static ParquetReader<Group> createReader(Path file, Filter filter) throws IOException
{    Configuration conf = new Configuration();    GroupWriteSupport.setSchema(schema, conf);    return ParquetReader.builder(new GroupReadSupport(), file).withConf(conf).withFilter(filter).build();}
0
public static List<Group> readFile(File f, Filter filter) throws IOException
{    ParquetReader<Group> reader = createReader(new Path(f.getAbsolutePath()), filter);    Group current;    List<Group> users = new ArrayList<Group>();    current = reader.read();    while (current != null) {        users.add(current);        current = reader.read();    }    return users;}
0
public static List<User> readUsers(ParquetReader.Builder<Group> builder) throws IOException
{    ParquetReader<Group> reader = builder.set(GroupWriteSupport.PARQUET_EXAMPLE_SCHEMA, schema.toString()).build();    List<User> users = new ArrayList<>();    for (Group group = reader.read(); group != null; group = reader.read()) {        users.add(userFromGroup(group));    }    return users;}
0
public static void main(String[] args) throws IOException
{    File f = new File(args[0]);    writeToFile(f, TestRecordLevelFilters.makeUsers());}
0
public static List<User> makeUsers()
{    List<User> users = new ArrayList<User>();    users.add(new User(17, null, null, null));    users.add(new User(18, "bob", null, null));    users.add(new User(19, "alice", new ArrayList<PhoneNumber>(), null));    users.add(new User(20, "thing1", Arrays.asList(new PhoneNumber(5555555555L, null)), null));    users.add(new User(27, "thing2", Arrays.asList(new PhoneNumber(1111111111L, "home")), null));    users.add(new User(28, "popular", Arrays.asList(new PhoneNumber(1111111111L, "home"), new PhoneNumber(2222222222L, null), new PhoneNumber(3333333333L, "mobile")), null));    users.add(new User(30, null, Arrays.asList(new PhoneNumber(1111111111L, "home")), null));    for (int i = 100; i < 200; i++) {        Location location = null;        if (i % 3 == 1) {            location = new Location((double) i, (double) i * 2);        }        if (i % 3 == 2) {            location = new Location((double) i, null);        }        users.add(new User(i, "p" + i, Arrays.asList(new PhoneNumber(i, "cell")), location));    }    return users;}
0
public static void setup() throws IOException
{    users = makeUsers();    phonebookFile = PhoneBookWriter.writeToFile(users);}
0
private static List<Group> getExpected(UserFilter f)
{    List<Group> expected = new ArrayList<Group>();    for (User u : users) {        if (f.keep(u)) {            expected.add(PhoneBookWriter.groupFromUser(u));        }    }    return expected;}
0
private static void assertFilter(List<Group> found, UserFilter f)
{    List<Group> expected = getExpected(f);    assertEquals(expected.size(), found.size());    Iterator<Group> expectedIter = expected.iterator();    Iterator<Group> foundIter = found.iterator();    while (expectedIter.hasNext()) {        assertEquals(expectedIter.next().toString(), foundIter.next().toString());    }}
0
public void testNoFilter() throws Exception
{    List<Group> found = PhoneBookWriter.readFile(phonebookFile, FilterCompat.NOOP);    assertFilter(found, new UserFilter() {        @Override        public boolean keep(User u) {            return true;        }    });}
0
public boolean keep(User u)
{    return true;}
0
public void testAllFilter() throws Exception
{    BinaryColumn name = binaryColumn("name");    FilterPredicate pred = eq(name, Binary.fromString("no matches"));    List<Group> found = PhoneBookWriter.readFile(phonebookFile, FilterCompat.get(pred));    assertEquals(new ArrayList<Group>(), found);}
0
public void testNameNotNull() throws Exception
{    BinaryColumn name = binaryColumn("name");    FilterPredicate pred = notEq(name, null);    List<Group> found = PhoneBookWriter.readFile(phonebookFile, FilterCompat.get(pred));    assertFilter(found, new UserFilter() {        @Override        public boolean keep(User u) {            return u.getName() != null;        }    });}
0
public boolean keep(User u)
{    return u.getName() != null;}
0
public boolean keep(Binary value)
{    if (value == null) {        return false;    }    return value.toStringUsingUTF8().startsWith("p");}
0
public boolean canDrop(Statistics<Binary> statistics)
{    return false;}
0
public boolean inverseCanDrop(Statistics<Binary> statistics)
{    return false;}
0
public boolean keep(Long value)
{    if (value == null) {        return false;    }    return hSet.contains(value);}
0
public boolean canDrop(Statistics<Long> statistics)
{    return false;}
0
public boolean inverseCanDrop(Statistics<Long> statistics)
{    return false;}
0
public void testNameNotStartWithP() throws Exception
{    BinaryColumn name = binaryColumn("name");    FilterPredicate pred = not(userDefined(name, StartWithP.class));    List<Group> found = PhoneBookWriter.readFile(phonebookFile, FilterCompat.get(pred));    assertFilter(found, new UserFilter() {        @Override        public boolean keep(User u) {            return u.getName() == null || !u.getName().startsWith("p");        }    });}
0
public boolean keep(User u)
{    return u.getName() == null || !u.getName().startsWith("p");}
0
public void testUserDefinedByInstance() throws Exception
{    LongColumn name = longColumn("id");    final HashSet<Long> h = new HashSet<Long>();    h.add(20L);    h.add(27L);    h.add(28L);    FilterPredicate pred = userDefined(name, new SetInFilter(h));    List<Group> found = PhoneBookWriter.readFile(phonebookFile, FilterCompat.get(pred));    assertFilter(found, new UserFilter() {        @Override        public boolean keep(User u) {            return u != null && h.contains(u.getId());        }    });}
0
public boolean keep(User u)
{    return u != null && h.contains(u.getId());}
0
public void testComplex() throws Exception
{    BinaryColumn name = binaryColumn("name");    DoubleColumn lon = doubleColumn("location.lon");    DoubleColumn lat = doubleColumn("location.lat");    FilterPredicate pred = or(and(gt(lon, 150.0), notEq(lat, null)), eq(name, Binary.fromString("alice")));    List<Group> found = PhoneBookWriter.readFile(phonebookFile, FilterCompat.get(pred));    assertFilter(found, new UserFilter() {        @Override        public boolean keep(User u) {            String name = u.getName();            Double lat = null;            Double lon = null;            if (u.getLocation() != null) {                lat = u.getLocation().getLat();                lon = u.getLocation().getLon();            }            return (lon != null && lon > 150.0 && lat != null) || "alice".equals(name);        }    });}
0
public boolean keep(User u)
{    String name = u.getName();    Double lat = null;    Double lon = null;    if (u.getLocation() != null) {        lat = u.getLocation().getLat();        lon = u.getLocation().getLon();    }    return (lon != null && lon > 150.0 && lat != null) || "alice".equals(name);}
0
private static ColumnChunkMetaData getIntColumnMeta(org.apache.parquet.column.statistics.Statistics<?> stats, long valueCount)
{    return ColumnChunkMetaData.get(ColumnPath.get("int", "column"), PrimitiveTypeName.INT32, CompressionCodecName.GZIP, new HashSet<Encoding>(Arrays.asList(Encoding.PLAIN)), stats, 0L, 0L, valueCount, 0L, 0L);}
0
private static ColumnChunkMetaData getDoubleColumnMeta(org.apache.parquet.column.statistics.Statistics<?> stats, long valueCount)
{    return ColumnChunkMetaData.get(ColumnPath.get("double", "column"), PrimitiveTypeName.DOUBLE, CompressionCodecName.GZIP, new HashSet<Encoding>(Arrays.asList(Encoding.PLAIN)), stats, 0L, 0L, valueCount, 0L, 0L);}
0
public void testEqNonNull()
{    assertTrue(canDrop(eq(intColumn, 9), columnMetas));    assertFalse(canDrop(eq(intColumn, 10), columnMetas));    assertFalse(canDrop(eq(intColumn, 100), columnMetas));    assertTrue(canDrop(eq(intColumn, 101), columnMetas));        assertTrue(canDrop(eq(intColumn, 0), nullColumnMetas));    assertTrue(canDrop(eq(missingColumn, fromString("any")), columnMetas));    assertFalse(canDrop(eq(intColumn, 50), missingMinMaxColumnMetas));    assertFalse(canDrop(eq(doubleColumn, 50.0), missingMinMaxColumnMetas));}
0
public void testEqNull()
{    IntStatistics statsNoNulls = new IntStatistics();    statsNoNulls.setMinMax(10, 100);    statsNoNulls.setNumNulls(0);    IntStatistics statsSomeNulls = new IntStatistics();    statsSomeNulls.setMinMax(10, 100);    statsSomeNulls.setNumNulls(3);    assertTrue(canDrop(eq(intColumn, null), Arrays.asList(getIntColumnMeta(statsNoNulls, 177L), getDoubleColumnMeta(doubleStats, 177L))));    assertFalse(canDrop(eq(intColumn, null), Arrays.asList(getIntColumnMeta(statsSomeNulls, 177L), getDoubleColumnMeta(doubleStats, 177L))));    assertFalse(canDrop(eq(missingColumn, null), columnMetas));    assertFalse(canDrop(eq(intColumn, null), missingMinMaxColumnMetas));    assertFalse(canDrop(eq(doubleColumn, null), missingMinMaxColumnMetas));}
0
public void testNotEqNonNull()
{    assertFalse(canDrop(notEq(intColumn, 9), columnMetas));    assertFalse(canDrop(notEq(intColumn, 10), columnMetas));    assertFalse(canDrop(notEq(intColumn, 100), columnMetas));    assertFalse(canDrop(notEq(intColumn, 101), columnMetas));    IntStatistics allSevens = new IntStatistics();    allSevens.setMinMax(7, 7);    assertTrue(canDrop(notEq(intColumn, 7), Arrays.asList(getIntColumnMeta(allSevens, 177L), getDoubleColumnMeta(doubleStats, 177L))));    allSevens.setNumNulls(100L);    assertFalse(canDrop(notEq(intColumn, 7), Arrays.asList(getIntColumnMeta(allSevens, 177L), getDoubleColumnMeta(doubleStats, 177L))));    allSevens.setNumNulls(177L);    assertFalse(canDrop(notEq(intColumn, 7), Arrays.asList(getIntColumnMeta(allSevens, 177L), getDoubleColumnMeta(doubleStats, 177L))));    assertFalse(canDrop(notEq(missingColumn, fromString("any")), columnMetas));    assertFalse(canDrop(notEq(intColumn, 50), missingMinMaxColumnMetas));    assertFalse(canDrop(notEq(doubleColumn, 50.0), missingMinMaxColumnMetas));}
0
public void testNotEqNull()
{    IntStatistics statsNoNulls = new IntStatistics();    statsNoNulls.setMinMax(10, 100);    statsNoNulls.setNumNulls(0);    IntStatistics statsSomeNulls = new IntStatistics();    statsSomeNulls.setMinMax(10, 100);    statsSomeNulls.setNumNulls(3);    IntStatistics statsAllNulls = new IntStatistics();    statsAllNulls.setMinMax(0, 0);    statsAllNulls.setNumNulls(177);    assertFalse(canDrop(notEq(intColumn, null), Arrays.asList(getIntColumnMeta(statsNoNulls, 177L), getDoubleColumnMeta(doubleStats, 177L))));    assertFalse(canDrop(notEq(intColumn, null), Arrays.asList(getIntColumnMeta(statsSomeNulls, 177L), getDoubleColumnMeta(doubleStats, 177L))));    assertTrue(canDrop(notEq(intColumn, null), Arrays.asList(getIntColumnMeta(statsAllNulls, 177L), getDoubleColumnMeta(doubleStats, 177L))));    assertTrue(canDrop(notEq(missingColumn, null), columnMetas));    assertFalse(canDrop(notEq(intColumn, null), missingMinMaxColumnMetas));    assertFalse(canDrop(notEq(doubleColumn, null), missingMinMaxColumnMetas));}
0
public void testLt()
{    assertTrue(canDrop(lt(intColumn, 9), columnMetas));    assertTrue(canDrop(lt(intColumn, 10), columnMetas));    assertFalse(canDrop(lt(intColumn, 100), columnMetas));    assertFalse(canDrop(lt(intColumn, 101), columnMetas));    assertTrue(canDrop(lt(intColumn, 0), nullColumnMetas));    assertTrue(canDrop(lt(intColumn, 7), nullColumnMetas));    assertTrue(canDrop(lt(missingColumn, fromString("any")), columnMetas));    assertFalse(canDrop(lt(intColumn, 0), missingMinMaxColumnMetas));    assertFalse(canDrop(lt(doubleColumn, 0.0), missingMinMaxColumnMetas));}
0
public void testLtEq()
{    assertTrue(canDrop(ltEq(intColumn, 9), columnMetas));    assertFalse(canDrop(ltEq(intColumn, 10), columnMetas));    assertFalse(canDrop(ltEq(intColumn, 100), columnMetas));    assertFalse(canDrop(ltEq(intColumn, 101), columnMetas));    assertTrue(canDrop(ltEq(intColumn, 0), nullColumnMetas));    assertTrue(canDrop(ltEq(intColumn, 7), nullColumnMetas));    assertTrue(canDrop(ltEq(missingColumn, fromString("any")), columnMetas));    assertFalse(canDrop(ltEq(intColumn, -1), missingMinMaxColumnMetas));    assertFalse(canDrop(ltEq(doubleColumn, -0.1), missingMinMaxColumnMetas));}
0
public void testGt()
{    assertFalse(canDrop(gt(intColumn, 9), columnMetas));    assertFalse(canDrop(gt(intColumn, 10), columnMetas));    assertTrue(canDrop(gt(intColumn, 100), columnMetas));    assertTrue(canDrop(gt(intColumn, 101), columnMetas));    assertTrue(canDrop(gt(intColumn, 0), nullColumnMetas));    assertTrue(canDrop(gt(intColumn, 7), nullColumnMetas));    assertTrue(canDrop(gt(missingColumn, fromString("any")), columnMetas));    assertFalse(canDrop(gt(intColumn, 0), missingMinMaxColumnMetas));    assertFalse(canDrop(gt(doubleColumn, 0.0), missingMinMaxColumnMetas));}
0
public void testGtEq()
{    assertFalse(canDrop(gtEq(intColumn, 9), columnMetas));    assertFalse(canDrop(gtEq(intColumn, 10), columnMetas));    assertFalse(canDrop(gtEq(intColumn, 100), columnMetas));    assertTrue(canDrop(gtEq(intColumn, 101), columnMetas));    assertTrue(canDrop(gtEq(intColumn, 0), nullColumnMetas));    assertTrue(canDrop(gtEq(intColumn, 7), nullColumnMetas));    assertTrue(canDrop(gtEq(missingColumn, fromString("any")), columnMetas));    assertFalse(canDrop(gtEq(intColumn, 1), missingMinMaxColumnMetas));    assertFalse(canDrop(gtEq(doubleColumn, 0.1), missingMinMaxColumnMetas));}
0
public void testAnd()
{    FilterPredicate yes = eq(intColumn, 9);    FilterPredicate no = eq(doubleColumn, 50D);    assertTrue(canDrop(and(yes, yes), columnMetas));    assertTrue(canDrop(and(yes, no), columnMetas));    assertTrue(canDrop(and(no, yes), columnMetas));    assertFalse(canDrop(and(no, no), columnMetas));}
0
public void testOr()
{    FilterPredicate yes = eq(intColumn, 9);    FilterPredicate no = eq(doubleColumn, 50D);    assertTrue(canDrop(or(yes, yes), columnMetas));    assertFalse(canDrop(or(yes, no), columnMetas));    assertFalse(canDrop(or(no, yes), columnMetas));    assertFalse(canDrop(or(no, no), columnMetas));}
0
public boolean keep(Integer value)
{    if (value == null) {        return true;    }    throw new RuntimeException("this method should not be called with value != null");}
0
public boolean canDrop(Statistics<Integer> statistics)
{    return statistics.getMin() == 7 && statistics.getMax() == 7;}
0
public boolean inverseCanDrop(Statistics<Integer> statistics)
{    return statistics.getMin() == 8 && statistics.getMax() == 8;}
0
public boolean keep(Integer value)
{    if (value == null) {        return false;    }    throw new RuntimeException("this method should not be called with value != null");}
0
public boolean keep(Double value)
{    if (value == null) {        return true;    }    throw new RuntimeException("this method should not be called with value != null");}
0
public boolean canDrop(Statistics<Double> statistics)
{    return statistics.getMin() <= 0.0;}
0
public boolean inverseCanDrop(Statistics<Double> statistics)
{    return statistics.getMin() > 0.0;}
0
public void testUdp()
{    FilterPredicate pred = userDefined(intColumn, SevensAndEightsUdp.class);    FilterPredicate invPred = LogicalInverseRewriter.rewrite(not(userDefined(intColumn, SevensAndEightsUdp.class)));    FilterPredicate udpDropMissingColumn = userDefined(missingColumn2, DropNullUdp.class);    FilterPredicate invUdpDropMissingColumn = LogicalInverseRewriter.rewrite(not(userDefined(missingColumn2, DropNullUdp.class)));    FilterPredicate udpKeepMissingColumn = userDefined(missingColumn2, SevensAndEightsUdp.class);    FilterPredicate invUdpKeepMissingColumn = LogicalInverseRewriter.rewrite(not(userDefined(missingColumn2, SevensAndEightsUdp.class)));    FilterPredicate allPositivePred = userDefined(doubleColumn, AllPositiveUdp.class);    IntStatistics seven = new IntStatistics();    seven.setMinMax(7, 7);    IntStatistics eight = new IntStatistics();    eight.setMinMax(8, 8);    IntStatistics neither = new IntStatistics();    neither.setMinMax(1, 2);    assertTrue(canDrop(pred, Arrays.asList(getIntColumnMeta(seven, 177L), getDoubleColumnMeta(doubleStats, 177L))));    assertFalse(canDrop(pred, Arrays.asList(getIntColumnMeta(eight, 177L), getDoubleColumnMeta(doubleStats, 177L))));    assertFalse(canDrop(pred, Arrays.asList(getIntColumnMeta(neither, 177L), getDoubleColumnMeta(doubleStats, 177L))));    assertFalse(canDrop(invPred, Arrays.asList(getIntColumnMeta(seven, 177L), getDoubleColumnMeta(doubleStats, 177L))));    assertTrue(canDrop(invPred, Arrays.asList(getIntColumnMeta(eight, 177L), getDoubleColumnMeta(doubleStats, 177L))));    assertFalse(canDrop(invPred, Arrays.asList(getIntColumnMeta(neither, 177L), getDoubleColumnMeta(doubleStats, 177L))));        assertTrue(canDrop(udpDropMissingColumn, Arrays.asList(getIntColumnMeta(seven, 177L), getDoubleColumnMeta(doubleStats, 177L))));    assertTrue(canDrop(udpDropMissingColumn, Arrays.asList(getIntColumnMeta(eight, 177L), getDoubleColumnMeta(doubleStats, 177L))));    assertTrue(canDrop(udpDropMissingColumn, Arrays.asList(getIntColumnMeta(neither, 177L), getDoubleColumnMeta(doubleStats, 177L))));        assertFalse(canDrop(invUdpDropMissingColumn, Arrays.asList(getIntColumnMeta(seven, 177L), getDoubleColumnMeta(doubleStats, 177L))));    assertFalse(canDrop(invUdpDropMissingColumn, Arrays.asList(getIntColumnMeta(eight, 177L), getDoubleColumnMeta(doubleStats, 177L))));    assertFalse(canDrop(invUdpDropMissingColumn, Arrays.asList(getIntColumnMeta(neither, 177L), getDoubleColumnMeta(doubleStats, 177L))));        assertFalse(canDrop(udpKeepMissingColumn, Arrays.asList(getIntColumnMeta(seven, 177L), getDoubleColumnMeta(doubleStats, 177L))));    assertFalse(canDrop(udpKeepMissingColumn, Arrays.asList(getIntColumnMeta(eight, 177L), getDoubleColumnMeta(doubleStats, 177L))));    assertFalse(canDrop(udpKeepMissingColumn, Arrays.asList(getIntColumnMeta(neither, 177L), getDoubleColumnMeta(doubleStats, 177L))));        assertTrue(canDrop(invUdpKeepMissingColumn, Arrays.asList(getIntColumnMeta(seven, 177L), getDoubleColumnMeta(doubleStats, 177L))));    assertTrue(canDrop(invUdpKeepMissingColumn, Arrays.asList(getIntColumnMeta(eight, 177L), getDoubleColumnMeta(doubleStats, 177L))));    assertTrue(canDrop(invUdpKeepMissingColumn, Arrays.asList(getIntColumnMeta(neither, 177L), getDoubleColumnMeta(doubleStats, 177L))));    assertFalse(canDrop(allPositivePred, missingMinMaxColumnMetas));}
0
public void testClearExceptionForNots()
{    List<ColumnChunkMetaData> columnMetas = Arrays.asList(getDoubleColumnMeta(new DoubleStatistics(), 0L), getIntColumnMeta(new IntStatistics(), 0L));    FilterPredicate pred = and(not(eq(doubleColumn, 12.0)), eq(intColumn, 17));    try {        canDrop(pred, columnMetas);        fail("This should throw");    } catch (IllegalArgumentException e) {        assertEquals("This predicate contains a not! Did you forget to run this predicate through LogicalInverseRewriter?" + " not(eq(double.column, 12.0))", e.getMessage());    }}
0
public void createDataFile() throws Exception
{    File file = temp.newFile("test.parquet");    this.path = new Path(file.toString());    MessageType type = Types.buildMessage().required(INT64).named("id").required(BINARY).as(UTF8).named("data").named("test");    SimpleGroupFactory factory = new SimpleGroupFactory(type);    ParquetWriter<Group> writer = ExampleParquetWriter.builder(path).withWriteMode(ParquetFileWriter.Mode.OVERWRITE).withType(type).build();    try {        for (long i = 0; i < 1000; i += 1) {            Group g = factory.newGroup();            g.add(0, i);            g.add(1, "data-" + i);            writer.write(g);        }    } finally {        writer.close();    }}
0
public void testNormalFilter() throws Exception
{    assertEquals(500, countFilteredRecords(path, lt(longColumn("id"), 500L)));}
0
public void testSimpleMissingColumnFilter() throws Exception
{    assertEquals(0, countFilteredRecords(path, lt(longColumn("missing"), 500L)));}
0
public void testAndMissingColumnFilter() throws Exception
{        assertEquals(500, countFilteredRecords(path, and(lt(longColumn("id"), 500L), eq(binaryColumn("missing"), null))));    assertEquals(500, countFilteredRecords(path, and(lt(longColumn("id"), 500L), notEq(binaryColumn("missing"), fromString("any")))));    assertEquals(500, countFilteredRecords(path, and(eq(binaryColumn("missing"), null), lt(longColumn("id"), 500L))));    assertEquals(500, countFilteredRecords(path, and(notEq(binaryColumn("missing"), fromString("any")), lt(longColumn("id"), 500L))));        assertEquals(0, countFilteredRecords(path, and(lt(longColumn("id"), 500L), eq(binaryColumn("missing"), fromString("any")))));    assertEquals(0, countFilteredRecords(path, and(lt(longColumn("id"), 500L), notEq(binaryColumn("missing"), null))));    assertEquals(0, countFilteredRecords(path, and(lt(longColumn("id"), 500L), lt(doubleColumn("missing"), 33.33))));    assertEquals(0, countFilteredRecords(path, and(lt(longColumn("id"), 500L), ltEq(doubleColumn("missing"), 33.33))));    assertEquals(0, countFilteredRecords(path, and(lt(longColumn("id"), 500L), gt(doubleColumn("missing"), 33.33))));    assertEquals(0, countFilteredRecords(path, and(lt(longColumn("id"), 500L), gtEq(doubleColumn("missing"), 33.33))));    assertEquals(0, countFilteredRecords(path, and(eq(binaryColumn("missing"), fromString("any")), lt(longColumn("id"), 500L))));    assertEquals(0, countFilteredRecords(path, and(notEq(binaryColumn("missing"), null), lt(longColumn("id"), 500L))));    assertEquals(0, countFilteredRecords(path, and(lt(doubleColumn("missing"), 33.33), lt(longColumn("id"), 500L))));    assertEquals(0, countFilteredRecords(path, and(ltEq(doubleColumn("missing"), 33.33), lt(longColumn("id"), 500L))));    assertEquals(0, countFilteredRecords(path, and(gt(doubleColumn("missing"), 33.33), lt(longColumn("id"), 500L))));    assertEquals(0, countFilteredRecords(path, and(gtEq(doubleColumn("missing"), 33.33), lt(longColumn("id"), 500L))));}
0
public void testOrMissingColumnFilter() throws Exception
{        assertEquals(500, countFilteredRecords(path, or(lt(longColumn("id"), 500L), eq(binaryColumn("missing"), fromString("any")))));    assertEquals(500, countFilteredRecords(path, or(lt(longColumn("id"), 500L), notEq(binaryColumn("missing"), null))));    assertEquals(500, countFilteredRecords(path, or(lt(longColumn("id"), 500L), lt(doubleColumn("missing"), 33.33))));    assertEquals(500, countFilteredRecords(path, or(lt(longColumn("id"), 500L), ltEq(doubleColumn("missing"), 33.33))));    assertEquals(500, countFilteredRecords(path, or(lt(longColumn("id"), 500L), gt(doubleColumn("missing"), 33.33))));    assertEquals(500, countFilteredRecords(path, or(lt(longColumn("id"), 500L), gtEq(doubleColumn("missing"), 33.33))));    assertEquals(500, countFilteredRecords(path, or(eq(binaryColumn("missing"), fromString("any")), lt(longColumn("id"), 500L))));    assertEquals(500, countFilteredRecords(path, or(notEq(binaryColumn("missing"), null), lt(longColumn("id"), 500L))));    assertEquals(500, countFilteredRecords(path, or(lt(doubleColumn("missing"), 33.33), lt(longColumn("id"), 500L))));    assertEquals(500, countFilteredRecords(path, or(ltEq(doubleColumn("missing"), 33.33), lt(longColumn("id"), 500L))));    assertEquals(500, countFilteredRecords(path, or(gt(doubleColumn("missing"), 33.33), lt(longColumn("id"), 500L))));    assertEquals(500, countFilteredRecords(path, or(gtEq(doubleColumn("missing"), 33.33), lt(longColumn("id"), 500L))));        assertEquals(1000, countFilteredRecords(path, or(lt(longColumn("id"), 500L), eq(binaryColumn("missing"), null))));    assertEquals(1000, countFilteredRecords(path, or(lt(longColumn("id"), 500L), notEq(binaryColumn("missing"), fromString("any")))));    assertEquals(1000, countFilteredRecords(path, or(eq(binaryColumn("missing"), null), lt(longColumn("id"), 500L))));    assertEquals(1000, countFilteredRecords(path, or(notEq(binaryColumn("missing"), fromString("any")), lt(longColumn("id"), 500L))));}
0
public static long countFilteredRecords(Path path, FilterPredicate pred) throws IOException
{    ParquetReader<Group> reader = ParquetReader.builder(new GroupReadSupport(), path).withFilter(FilterCompat.get(pred)).build();    long count = 0;    try {        while (reader.read() != null) {            count += 1;        }    } finally {        reader.close();    }    return count;}
0
public void testPageHeader() throws IOException
{    ByteArrayOutputStream out = new ByteArrayOutputStream();    PageType type = PageType.DATA_PAGE;    int compSize = 10;    int uncSize = 20;    PageHeader pageHeader = new PageHeader(type, uncSize, compSize);    writePageHeader(pageHeader, out);    PageHeader readPageHeader = readPageHeader(new ByteArrayInputStream(out.toByteArray()));    assertEquals(pageHeader, readPageHeader);}
0
public void testSchemaConverter()
{    ParquetMetadataConverter parquetMetadataConverter = new ParquetMetadataConverter();    List<SchemaElement> parquetSchema = parquetMetadataConverter.toParquetSchema(Paper.schema);    MessageType schema = parquetMetadataConverter.fromParquetSchema(parquetSchema, null);    assertEquals(Paper.schema, schema);}
0
public void testSchemaConverterDecimal()
{    ParquetMetadataConverter parquetMetadataConverter = new ParquetMetadataConverter();    List<SchemaElement> schemaElements = parquetMetadataConverter.toParquetSchema(Types.buildMessage().required(PrimitiveTypeName.BINARY).as(OriginalType.DECIMAL).precision(9).scale(2).named("aBinaryDecimal").optional(PrimitiveTypeName.FIXED_LEN_BYTE_ARRAY).length(4).as(OriginalType.DECIMAL).precision(9).scale(2).named("aFixedDecimal").named("Message"));    List<SchemaElement> expected = Lists.newArrayList(new SchemaElement("Message").setNum_children(2), new SchemaElement("aBinaryDecimal").setRepetition_type(FieldRepetitionType.REQUIRED).setType(Type.BYTE_ARRAY).setConverted_type(ConvertedType.DECIMAL).setLogicalType(LogicalType.DECIMAL(new DecimalType(2, 9))).setPrecision(9).setScale(2), new SchemaElement("aFixedDecimal").setRepetition_type(FieldRepetitionType.OPTIONAL).setType(Type.FIXED_LEN_BYTE_ARRAY).setType_length(4).setConverted_type(ConvertedType.DECIMAL).setLogicalType(LogicalType.DECIMAL(new DecimalType(2, 9))).setPrecision(9).setScale(2));    Assert.assertEquals(expected, schemaElements);}
0
public void testLogicalTypesBackwardCompatibleWithConvertedTypes()
{    ParquetMetadataConverter parquetMetadataConverter = new ParquetMetadataConverter();    MessageType expected = Types.buildMessage().required(PrimitiveTypeName.BINARY).as(OriginalType.DECIMAL).precision(9).scale(2).named("aBinaryDecimal").named("Message");    List<SchemaElement> parquetSchema = parquetMetadataConverter.toParquetSchema(expected);            parquetSchema.get(1).setLogicalType(null);    MessageType schema = parquetMetadataConverter.fromParquetSchema(parquetSchema, null);    assertEquals(expected, schema);}
0
public void testIncompatibleLogicalAndConvertedTypes()
{    ParquetMetadataConverter parquetMetadataConverter = new ParquetMetadataConverter();    MessageType schema = Types.buildMessage().required(PrimitiveTypeName.BINARY).as(OriginalType.DECIMAL).precision(9).scale(2).named("aBinary").named("Message");    MessageType expected = Types.buildMessage().required(PrimitiveTypeName.BINARY).as(LogicalTypeAnnotation.jsonType()).named("aBinary").named("Message");    List<SchemaElement> parquetSchema = parquetMetadataConverter.toParquetSchema(schema);        parquetSchema.get(1).setConverted_type(ConvertedType.JSON);    MessageType actual = parquetMetadataConverter.fromParquetSchema(parquetSchema, null);    assertEquals(expected, actual);}
0
public void testTimeLogicalTypes()
{    ParquetMetadataConverter parquetMetadataConverter = new ParquetMetadataConverter();    MessageType expected = Types.buildMessage().required(PrimitiveTypeName.INT64).as(timestampType(false, MILLIS)).named("aTimestampNonUtcMillis").required(PrimitiveTypeName.INT64).as(timestampType(true, MILLIS)).named("aTimestampUtcMillis").required(PrimitiveTypeName.INT64).as(timestampType(false, MICROS)).named("aTimestampNonUtcMicros").required(PrimitiveTypeName.INT64).as(timestampType(true, MICROS)).named("aTimestampUtcMicros").required(PrimitiveTypeName.INT64).as(timestampType(false, NANOS)).named("aTimestampNonUtcNanos").required(PrimitiveTypeName.INT64).as(timestampType(true, NANOS)).named("aTimestampUtcNanos").required(PrimitiveTypeName.INT32).as(timeType(false, MILLIS)).named("aTimeNonUtcMillis").required(PrimitiveTypeName.INT32).as(timeType(true, MILLIS)).named("aTimeUtcMillis").required(PrimitiveTypeName.INT64).as(timeType(false, MICROS)).named("aTimeNonUtcMicros").required(PrimitiveTypeName.INT64).as(timeType(true, MICROS)).named("aTimeUtcMicros").required(PrimitiveTypeName.INT64).as(timeType(false, NANOS)).named("aTimeNonUtcNanos").required(PrimitiveTypeName.INT64).as(timeType(true, NANOS)).named("aTimeUtcNanos").named("Message");    List<SchemaElement> parquetSchema = parquetMetadataConverter.toParquetSchema(expected);    MessageType schema = parquetMetadataConverter.fromParquetSchema(parquetSchema, null);    assertEquals(expected, schema);}
0
public void testLogicalToConvertedTypeConversion()
{    ParquetMetadataConverter parquetMetadataConverter = new ParquetMetadataConverter();    assertEquals(ConvertedType.UTF8, parquetMetadataConverter.convertToConvertedType(stringType()));    assertEquals(ConvertedType.ENUM, parquetMetadataConverter.convertToConvertedType(enumType()));    assertEquals(ConvertedType.INT_8, parquetMetadataConverter.convertToConvertedType(intType(8, true)));    assertEquals(ConvertedType.INT_16, parquetMetadataConverter.convertToConvertedType(intType(16, true)));    assertEquals(ConvertedType.INT_32, parquetMetadataConverter.convertToConvertedType(intType(32, true)));    assertEquals(ConvertedType.INT_64, parquetMetadataConverter.convertToConvertedType(intType(64, true)));    assertEquals(ConvertedType.UINT_8, parquetMetadataConverter.convertToConvertedType(intType(8, false)));    assertEquals(ConvertedType.UINT_16, parquetMetadataConverter.convertToConvertedType(intType(16, false)));    assertEquals(ConvertedType.UINT_32, parquetMetadataConverter.convertToConvertedType(intType(32, false)));    assertEquals(ConvertedType.UINT_64, parquetMetadataConverter.convertToConvertedType(intType(64, false)));    assertEquals(ConvertedType.DECIMAL, parquetMetadataConverter.convertToConvertedType(decimalType(8, 16)));    assertEquals(ConvertedType.TIMESTAMP_MILLIS, parquetMetadataConverter.convertToConvertedType(timestampType(true, MILLIS)));    assertEquals(ConvertedType.TIMESTAMP_MICROS, parquetMetadataConverter.convertToConvertedType(timestampType(true, MICROS)));    assertNull(parquetMetadataConverter.convertToConvertedType(timestampType(true, NANOS)));    assertNull(parquetMetadataConverter.convertToConvertedType(timestampType(false, MILLIS)));    assertNull(parquetMetadataConverter.convertToConvertedType(timestampType(false, MICROS)));    assertNull(parquetMetadataConverter.convertToConvertedType(timestampType(false, NANOS)));    assertEquals(ConvertedType.TIME_MILLIS, parquetMetadataConverter.convertToConvertedType(timeType(true, MILLIS)));    assertEquals(ConvertedType.TIME_MICROS, parquetMetadataConverter.convertToConvertedType(timeType(true, MICROS)));    assertNull(parquetMetadataConverter.convertToConvertedType(timeType(true, NANOS)));    assertNull(parquetMetadataConverter.convertToConvertedType(timeType(false, MILLIS)));    assertNull(parquetMetadataConverter.convertToConvertedType(timeType(false, MICROS)));    assertNull(parquetMetadataConverter.convertToConvertedType(timeType(false, NANOS)));    assertEquals(ConvertedType.DATE, parquetMetadataConverter.convertToConvertedType(dateType()));    assertEquals(ConvertedType.INTERVAL, parquetMetadataConverter.convertToConvertedType(LogicalTypeAnnotation.IntervalLogicalTypeAnnotation.getInstance()));    assertEquals(ConvertedType.JSON, parquetMetadataConverter.convertToConvertedType(jsonType()));    assertEquals(ConvertedType.BSON, parquetMetadataConverter.convertToConvertedType(bsonType()));    assertEquals(ConvertedType.LIST, parquetMetadataConverter.convertToConvertedType(listType()));    assertEquals(ConvertedType.MAP, parquetMetadataConverter.convertToConvertedType(mapType()));    assertEquals(ConvertedType.MAP_KEY_VALUE, parquetMetadataConverter.convertToConvertedType(LogicalTypeAnnotation.MapKeyValueTypeAnnotation.getInstance()));}
0
public void testEnumEquivalence()
{    ParquetMetadataConverter parquetMetadataConverter = new ParquetMetadataConverter();    for (org.apache.parquet.column.Encoding encoding : org.apache.parquet.column.Encoding.values()) {        assertEquals(encoding, parquetMetadataConverter.getEncoding(parquetMetadataConverter.getEncoding(encoding)));    }    for (org.apache.parquet.format.Encoding encoding : org.apache.parquet.format.Encoding.values()) {        assertEquals(encoding, parquetMetadataConverter.getEncoding(parquetMetadataConverter.getEncoding(encoding)));    }    for (Repetition repetition : Repetition.values()) {        assertEquals(repetition, parquetMetadataConverter.fromParquetRepetition(parquetMetadataConverter.toParquetRepetition(repetition)));    }    for (FieldRepetitionType repetition : FieldRepetitionType.values()) {        assertEquals(repetition, parquetMetadataConverter.toParquetRepetition(parquetMetadataConverter.fromParquetRepetition(repetition)));    }    for (PrimitiveTypeName primitiveTypeName : PrimitiveTypeName.values()) {        assertEquals(primitiveTypeName, parquetMetadataConverter.getPrimitive(parquetMetadataConverter.getType(primitiveTypeName)));    }    for (Type type : Type.values()) {        assertEquals(type, parquetMetadataConverter.getType(parquetMetadataConverter.getPrimitive(type)));    }    for (OriginalType original : OriginalType.values()) {        assertEquals(original, parquetMetadataConverter.getLogicalTypeAnnotation(parquetMetadataConverter.convertToConvertedType(LogicalTypeAnnotation.fromOriginalType(original, null)), null).toOriginalType());    }    for (ConvertedType converted : ConvertedType.values()) {        assertEquals(converted, parquetMetadataConverter.convertToConvertedType(parquetMetadataConverter.getLogicalTypeAnnotation(converted, null)));    }}
0
private FileMetaData metadata(long... sizes)
{    List<SchemaElement> schema = emptyList();    List<RowGroup> rowGroups = new ArrayList<RowGroup>();    long offset = 0;    for (long size : sizes) {        ColumnChunk columnChunk = new ColumnChunk(offset);        columnChunk.setMeta_data(new ColumnMetaData(INT32, Collections.<org.apache.parquet.format.Encoding>emptyList(), Collections.<String>emptyList(), UNCOMPRESSED, 10l, size * 2, size, offset));        rowGroups.add(new RowGroup(Arrays.asList(columnChunk), size, 1));        offset += size;    }    return new FileMetaData(1, schema, sizes.length, rowGroups);}
0
private FileMetaData filter(FileMetaData md, long start, long end)
{    return filterFileMetaDataByMidpoint(new FileMetaData(md), new ParquetMetadataConverter.RangeMetadataFilter(start, end));}
0
private FileMetaData find(FileMetaData md, Long... blockStart)
{    return filterFileMetaDataByStart(new FileMetaData(md), new ParquetMetadataConverter.OffsetMetadataFilter(Sets.newHashSet((Long[]) blockStart)));}
0
private FileMetaData find(FileMetaData md, long blockStart)
{    return filterFileMetaDataByStart(new FileMetaData(md), new ParquetMetadataConverter.OffsetMetadataFilter(Sets.newHashSet(blockStart)));}
0
private void verifyMD(FileMetaData md, long... offsets)
{    assertEquals(offsets.length, md.row_groups.size());    for (int i = 0; i < offsets.length; i++) {        long offset = offsets[i];        RowGroup rowGroup = md.getRow_groups().get(i);        assertEquals(offset, getOffset(rowGroup));    }}
0
private void verifyAllFilters(FileMetaData md, long splitWidth)
{    Set<Long> offsetsFound = new TreeSet<Long>();    for (long start = 0; start < fileSize(md); start += splitWidth) {        FileMetaData filtered = filter(md, start, start + splitWidth);        for (RowGroup rg : filtered.getRow_groups()) {            long o = getOffset(rg);            if (offsetsFound.contains(o)) {                fail("found the offset twice: " + o);            } else {                offsetsFound.add(o);            }        }    }    if (offsetsFound.size() != md.row_groups.size()) {        fail("missing row groups, " + "found: " + offsetsFound + "\nexpected " + md.getRow_groups());    }}
0
private long fileSize(FileMetaData md)
{    long size = 0;    for (RowGroup rg : md.getRow_groups()) {        size += rg.total_byte_size;    }    return size;}
0
public void testFilterMetaData()
{    verifyMD(filter(metadata(50, 50, 50), 0, 50), 0);    verifyMD(filter(metadata(50, 50, 50), 50, 100), 50);    verifyMD(filter(metadata(50, 50, 50), 100, 150), 100);        verifyMD(filter(metadata(50, 50, 50), 25, 75), 0);        verifyMD(filter(metadata(50, 50, 50), 26, 75));        verifyMD(filter(metadata(50, 50, 50), 26, 76), 50);    verifyAllFilters(metadata(50, 50, 50), 10);    verifyAllFilters(metadata(50, 50, 50), 51);        verifyAllFilters(metadata(50, 50, 50), 25);    verifyAllFilters(metadata(50, 50, 50), 24);    verifyAllFilters(metadata(50, 50, 50), 26);    verifyAllFilters(metadata(50, 50, 50), 110);    verifyAllFilters(metadata(10, 50, 500), 110);    verifyAllFilters(metadata(10, 50, 500), 10);    verifyAllFilters(metadata(10, 50, 500), 600);    verifyAllFilters(metadata(11, 9, 10), 10);    verifyAllFilters(metadata(11, 9, 10), 9);    verifyAllFilters(metadata(11, 9, 10), 8);}
0
public void testFindRowGroups()
{    verifyMD(find(metadata(50, 50, 50), 0), 0);    verifyMD(find(metadata(50, 50, 50), 50), 50);    verifyMD(find(metadata(50, 50, 50), 100), 100);    verifyMD(find(metadata(50, 50, 50), 0L, 50L), 0, 50);    verifyMD(find(metadata(50, 50, 50), 0L, 50L, 100L), 0, 50, 100);    verifyMD(find(metadata(50, 50, 50), 50L, 100L), 50, 100);        verifyMD(find(metadata(50, 50, 50), 10));}
0
public void randomTestFilterMetaData()
{            Random random = new Random(42);    for (int j = 0; j < 100; j++) {        long[] rgs = new long[random.nextInt(50)];        for (int i = 0; i < rgs.length; i++) {                        rgs[i] = random.nextInt(10000) + 1;        }                int splitSize = random.nextInt(10000) + 1;        try {            verifyAllFilters(metadata(rgs), splitSize);        } catch (AssertionError e) {            throw (AssertionError) new AssertionError("fail verifyAllFilters(metadata(" + Arrays.toString(rgs) + "), " + splitSize + ")").initCause(e);        }    }}
0
public void testNullFieldMetadataDebugLogging()
{    MessageType schema = parseMessageType("message test { optional binary some_null_field; }");    org.apache.parquet.hadoop.metadata.FileMetaData fileMetaData = new org.apache.parquet.hadoop.metadata.FileMetaData(schema, new HashMap<String, String>(), null);    List<BlockMetaData> blockMetaDataList = new ArrayList<BlockMetaData>();    BlockMetaData blockMetaData = new BlockMetaData();    blockMetaData.addColumn(createColumnChunkMetaData());    blockMetaDataList.add(blockMetaData);    ParquetMetadata metadata = new ParquetMetadata(fileMetaData, blockMetaDataList);    ParquetMetadata.toJSON(metadata);}
0
public void testMetadataToJson()
{    ParquetMetadata metadata = new ParquetMetadata(null, null);    assertEquals("{\"fileMetaData\":null,\"blocks\":null}", ParquetMetadata.toJSON(metadata));    assertEquals("{\n" + "  \"fileMetaData\" : null,\n" + "  \"blocks\" : null\n" + "}", ParquetMetadata.toPrettyJSON(metadata));}
0
private ColumnChunkMetaData createColumnChunkMetaData()
{    Set<org.apache.parquet.column.Encoding> e = new HashSet<org.apache.parquet.column.Encoding>();    PrimitiveTypeName t = PrimitiveTypeName.BINARY;    ColumnPath p = ColumnPath.get("foo");    CompressionCodecName c = CompressionCodecName.GZIP;    BinaryStatistics s = new BinaryStatistics();    ColumnChunkMetaData md = ColumnChunkMetaData.get(p, t, c, e, s, 0, 0, 0, 0, 0);    return md;}
0
public void testEncodingsCache()
{    ParquetMetadataConverter parquetMetadataConverter = new ParquetMetadataConverter();    List<org.apache.parquet.format.Encoding> formatEncodingsCopy1 = Arrays.asList(org.apache.parquet.format.Encoding.BIT_PACKED, org.apache.parquet.format.Encoding.RLE_DICTIONARY, org.apache.parquet.format.Encoding.DELTA_LENGTH_BYTE_ARRAY);    List<org.apache.parquet.format.Encoding> formatEncodingsCopy2 = Arrays.asList(org.apache.parquet.format.Encoding.BIT_PACKED, org.apache.parquet.format.Encoding.RLE_DICTIONARY, org.apache.parquet.format.Encoding.DELTA_LENGTH_BYTE_ARRAY);    Set<org.apache.parquet.column.Encoding> expected = new HashSet<org.apache.parquet.column.Encoding>();    expected.add(org.apache.parquet.column.Encoding.BIT_PACKED);    expected.add(org.apache.parquet.column.Encoding.RLE_DICTIONARY);    expected.add(org.apache.parquet.column.Encoding.DELTA_LENGTH_BYTE_ARRAY);    Set<org.apache.parquet.column.Encoding> res1 = parquetMetadataConverter.fromFormatEncodings(formatEncodingsCopy1);    Set<org.apache.parquet.column.Encoding> res2 = parquetMetadataConverter.fromFormatEncodings(formatEncodingsCopy1);    Set<org.apache.parquet.column.Encoding> res3 = parquetMetadataConverter.fromFormatEncodings(formatEncodingsCopy2);        assertEquals(expected, res1);    assertEquals(expected, res2);    assertEquals(expected, res3);        assertSame(res1, res2);    assertSame(res1, res3);        assertEquals("java.util.Collections$UnmodifiableSet", res1.getClass().getName());    assertEquals("java.util.Collections$UnmodifiableSet", res2.getClass().getName());    assertEquals("java.util.Collections$UnmodifiableSet", res3.getClass().getName());}
0
public void testBinaryStatsV1()
{    testBinaryStats(StatsHelper.V1);}
0
public void testBinaryStatsV2()
{    testBinaryStats(StatsHelper.V2);}
0
private void testBinaryStats(StatsHelper helper)
{        BinaryStatistics stats = new BinaryStatistics();    stats.incrementNumNulls(3004);    byte[] min = new byte[904];    byte[] max = new byte[2388];    stats.updateStats(Binary.fromConstantByteArray(min));    stats.updateStats(Binary.fromConstantByteArray(max));    long totalLen = min.length + max.length;    Assert.assertFalse("Should not be smaller than min + max size", stats.isSmallerThan(totalLen));    Assert.assertTrue("Should be smaller than min + max size + 1", stats.isSmallerThan(totalLen + 1));    org.apache.parquet.format.Statistics formatStats = helper.toParquetStatistics(stats);    assertFalse("Min should not be set", formatStats.isSetMin());    assertFalse("Max should not be set", formatStats.isSetMax());    if (helper == StatsHelper.V2) {        Assert.assertArrayEquals("Min_value should match", min, formatStats.getMin_value());        Assert.assertArrayEquals("Max_value should match", max, formatStats.getMax_value());    }    Assert.assertEquals("Num nulls should match", 3004, formatStats.getNull_count());        stats.setMinMaxFromBytes(max, max);    formatStats = helper.toParquetStatistics(stats);    Assert.assertFalse("Min should not be set", formatStats.isSetMin());    Assert.assertFalse("Max should not be set", formatStats.isSetMax());    Assert.assertFalse("Min_value should not be set", formatStats.isSetMin_value());    Assert.assertFalse("Max_value should not be set", formatStats.isSetMax_value());    Assert.assertFalse("Num nulls should not be set", formatStats.isSetNull_count());    Statistics roundTripStats = ParquetMetadataConverter.fromParquetStatisticsInternal(Version.FULL_VERSION, formatStats, new PrimitiveType(Repetition.OPTIONAL, PrimitiveTypeName.BINARY, ""), ParquetMetadataConverter.SortOrder.SIGNED);    Assert.assertTrue(roundTripStats.isEmpty());}
0
public void testIntegerStatsV1()
{    testIntegerStats(StatsHelper.V1);}
0
public void testIntegerStatsV2()
{    testIntegerStats(StatsHelper.V2);}
0
private void testIntegerStats(StatsHelper helper)
{        IntStatistics stats = new IntStatistics();    stats.incrementNumNulls(3004);    int min = Integer.MIN_VALUE;    int max = Integer.MAX_VALUE;    stats.updateStats(min);    stats.updateStats(max);    org.apache.parquet.format.Statistics formatStats = helper.toParquetStatistics(stats);    Assert.assertEquals("Min should match", min, BytesUtils.bytesToInt(formatStats.getMin()));    Assert.assertEquals("Max should match", max, BytesUtils.bytesToInt(formatStats.getMax()));    Assert.assertEquals("Num nulls should match", 3004, formatStats.getNull_count());}
0
public void testLongStatsV1()
{    testLongStats(StatsHelper.V1);}
0
public void testLongStatsV2()
{    testLongStats(StatsHelper.V2);}
0
private void testLongStats(StatsHelper helper)
{        LongStatistics stats = new LongStatistics();    stats.incrementNumNulls(3004);    long min = Long.MIN_VALUE;    long max = Long.MAX_VALUE;    stats.updateStats(min);    stats.updateStats(max);    org.apache.parquet.format.Statistics formatStats = helper.toParquetStatistics(stats);    Assert.assertEquals("Min should match", min, BytesUtils.bytesToLong(formatStats.getMin()));    Assert.assertEquals("Max should match", max, BytesUtils.bytesToLong(formatStats.getMax()));    Assert.assertEquals("Num nulls should match", 3004, formatStats.getNull_count());}
0
public void testFloatStatsV1()
{    testFloatStats(StatsHelper.V1);}
0
public void testFloatStatsV2()
{    testFloatStats(StatsHelper.V2);}
0
private void testFloatStats(StatsHelper helper)
{        FloatStatistics stats = new FloatStatistics();    stats.incrementNumNulls(3004);    float min = Float.MIN_VALUE;    float max = Float.MAX_VALUE;    stats.updateStats(min);    stats.updateStats(max);    org.apache.parquet.format.Statistics formatStats = helper.toParquetStatistics(stats);    Assert.assertEquals("Min should match", min, Float.intBitsToFloat(BytesUtils.bytesToInt(formatStats.getMin())), 0.000001);    Assert.assertEquals("Max should match", max, Float.intBitsToFloat(BytesUtils.bytesToInt(formatStats.getMax())), 0.000001);    Assert.assertEquals("Num nulls should match", 3004, formatStats.getNull_count());}
0
public void testDoubleStatsV1()
{    testDoubleStats(StatsHelper.V1);}
0
public void testDoubleStatsV2()
{    testDoubleStats(StatsHelper.V2);}
0
private void testDoubleStats(StatsHelper helper)
{        DoubleStatistics stats = new DoubleStatistics();    stats.incrementNumNulls(3004);    double min = Double.MIN_VALUE;    double max = Double.MAX_VALUE;    stats.updateStats(min);    stats.updateStats(max);    org.apache.parquet.format.Statistics formatStats = helper.toParquetStatistics(stats);    Assert.assertEquals("Min should match", min, Double.longBitsToDouble(BytesUtils.bytesToLong(formatStats.getMin())), 0.000001);    Assert.assertEquals("Max should match", max, Double.longBitsToDouble(BytesUtils.bytesToLong(formatStats.getMax())), 0.000001);    Assert.assertEquals("Num nulls should match", 3004, formatStats.getNull_count());}
0
public void testBooleanStatsV1()
{    testBooleanStats(StatsHelper.V1);}
0
public void testBooleanStatsV2()
{    testBooleanStats(StatsHelper.V2);}
0
private void testBooleanStats(StatsHelper helper)
{        BooleanStatistics stats = new BooleanStatistics();    stats.incrementNumNulls(3004);    boolean min = Boolean.FALSE;    boolean max = Boolean.TRUE;    stats.updateStats(min);    stats.updateStats(max);    org.apache.parquet.format.Statistics formatStats = helper.toParquetStatistics(stats);    Assert.assertEquals("Min should match", min, BytesUtils.bytesToBool(formatStats.getMin()));    Assert.assertEquals("Max should match", max, BytesUtils.bytesToBool(formatStats.getMax()));    Assert.assertEquals("Num nulls should match", 3004, formatStats.getNull_count());}
0
public void testIgnoreStatsWithSignedSortOrder()
{    ParquetMetadataConverter converter = new ParquetMetadataConverter();    BinaryStatistics stats = new BinaryStatistics();    stats.incrementNumNulls();    stats.updateStats(Binary.fromString("A"));    stats.incrementNumNulls();    stats.updateStats(Binary.fromString("z"));    stats.incrementNumNulls();    PrimitiveType binaryType = Types.required(PrimitiveTypeName.BINARY).as(OriginalType.UTF8).named("b");    Statistics convertedStats = converter.fromParquetStatistics(Version.FULL_VERSION, StatsHelper.V1.toParquetStatistics(stats), binaryType);    Assert.assertFalse("Stats should not include min/max: " + convertedStats, convertedStats.hasNonNullValue());    Assert.assertTrue("Stats should have null count: " + convertedStats, convertedStats.isNumNullsSet());    Assert.assertEquals("Stats should have 3 nulls: " + convertedStats, 3L, convertedStats.getNumNulls());}
0
public void testStillUseStatsWithSignedSortOrderIfSingleValueV1()
{    testStillUseStatsWithSignedSortOrderIfSingleValue(StatsHelper.V1);}
0
public void testStillUseStatsWithSignedSortOrderIfSingleValueV2()
{    testStillUseStatsWithSignedSortOrderIfSingleValue(StatsHelper.V2);}
0
private void testStillUseStatsWithSignedSortOrderIfSingleValue(StatsHelper helper)
{    ParquetMetadataConverter converter = new ParquetMetadataConverter();    BinaryStatistics stats = new BinaryStatistics();    stats.incrementNumNulls();    stats.updateStats(Binary.fromString("A"));    stats.incrementNumNulls();    stats.updateStats(Binary.fromString("A"));    stats.incrementNumNulls();    PrimitiveType binaryType = Types.required(PrimitiveTypeName.BINARY).as(OriginalType.UTF8).named("b");    Statistics convertedStats = converter.fromParquetStatistics(Version.FULL_VERSION, ParquetMetadataConverter.toParquetStatistics(stats), binaryType);    Assert.assertFalse("Stats should not be empty: " + convertedStats, convertedStats.isEmpty());    Assert.assertArrayEquals("min == max: " + convertedStats, convertedStats.getMaxBytes(), convertedStats.getMinBytes());}
0
public void testUseStatsWithSignedSortOrderV1()
{    testUseStatsWithSignedSortOrder(StatsHelper.V1);}
0
public void testUseStatsWithSignedSortOrderV2()
{    testUseStatsWithSignedSortOrder(StatsHelper.V2);}
0
private void testUseStatsWithSignedSortOrder(StatsHelper helper)
{        Configuration conf = new Configuration();    conf.setBoolean("parquet.strings.signed-min-max.enabled", true);    ParquetMetadataConverter converter = new ParquetMetadataConverter(conf);    BinaryStatistics stats = new BinaryStatistics();    stats.incrementNumNulls();    stats.updateStats(Binary.fromString("A"));    stats.incrementNumNulls();    stats.updateStats(Binary.fromString("z"));    stats.incrementNumNulls();    PrimitiveType binaryType = Types.required(PrimitiveTypeName.BINARY).as(OriginalType.UTF8).named("b");    Statistics convertedStats = converter.fromParquetStatistics(Version.FULL_VERSION, helper.toParquetStatistics(stats), binaryType);    Assert.assertFalse("Stats should not be empty", convertedStats.isEmpty());    Assert.assertTrue(convertedStats.isNumNullsSet());    Assert.assertEquals("Should have 3 nulls", 3, convertedStats.getNumNulls());    if (helper == StatsHelper.V1) {        assertFalse("Min-max should be null for V1 stats", convertedStats.hasNonNullValue());    } else {        Assert.assertEquals("Should have correct min (unsigned sort)", Binary.fromString("A"), convertedStats.genericGetMin());        Assert.assertEquals("Should have correct max (unsigned sort)", Binary.fromString("z"), convertedStats.genericGetMax());    }}
0
public void testMissingValuesFromStats()
{    ParquetMetadataConverter converter = new ParquetMetadataConverter();    PrimitiveType type = Types.required(PrimitiveTypeName.INT32).named("test_int32");    org.apache.parquet.format.Statistics formatStats = new org.apache.parquet.format.Statistics();    Statistics<?> stats = converter.fromParquetStatistics(Version.FULL_VERSION, formatStats, type);    assertFalse(stats.isNumNullsSet());    assertFalse(stats.hasNonNullValue());    assertTrue(stats.isEmpty());    assertEquals(-1, stats.getNumNulls());    formatStats.clear();    formatStats.setMin(BytesUtils.intToBytes(-100));    formatStats.setMax(BytesUtils.intToBytes(100));    stats = converter.fromParquetStatistics(Version.FULL_VERSION, formatStats, type);    assertFalse(stats.isNumNullsSet());    assertTrue(stats.hasNonNullValue());    assertFalse(stats.isEmpty());    assertEquals(-1, stats.getNumNulls());    assertEquals(-100, stats.genericGetMin());    assertEquals(100, stats.genericGetMax());    formatStats.clear();    formatStats.setNull_count(2000);    stats = converter.fromParquetStatistics(Version.FULL_VERSION, formatStats, type);    assertTrue(stats.isNumNullsSet());    assertFalse(stats.hasNonNullValue());    assertFalse(stats.isEmpty());    assertEquals(2000, stats.getNumNulls());}
0
public void testSkippedV2Stats()
{    testSkippedV2Stats(Types.optional(PrimitiveTypeName.FIXED_LEN_BYTE_ARRAY).length(12).as(OriginalType.INTERVAL).named(""), new BigInteger("12345678"), new BigInteger("12345679"));    testSkippedV2Stats(Types.optional(PrimitiveTypeName.INT96).named(""), new BigInteger("-75687987"), new BigInteger("45367657"));}
0
private void testSkippedV2Stats(PrimitiveType type, Object min, Object max)
{    Statistics<?> stats = createStats(type, min, max);    org.apache.parquet.format.Statistics statistics = ParquetMetadataConverter.toParquetStatistics(stats);    assertFalse(statistics.isSetMin());    assertFalse(statistics.isSetMax());    assertFalse(statistics.isSetMin_value());    assertFalse(statistics.isSetMax_value());}
0
public void testV2OnlyStats()
{    testV2OnlyStats(Types.optional(PrimitiveTypeName.INT32).as(OriginalType.UINT_8).named(""), 0x7F, 0x80);    testV2OnlyStats(Types.optional(PrimitiveTypeName.INT32).as(OriginalType.UINT_16).named(""), 0x7FFF, 0x8000);    testV2OnlyStats(Types.optional(PrimitiveTypeName.INT32).as(OriginalType.UINT_32).named(""), 0x7FFFFFFF, 0x80000000);    testV2OnlyStats(Types.optional(PrimitiveTypeName.INT64).as(OriginalType.UINT_64).named(""), 0x7FFFFFFFFFFFFFFFL, 0x8000000000000000L);    testV2OnlyStats(Types.optional(PrimitiveTypeName.BINARY).as(OriginalType.DECIMAL).precision(6).named(""), new BigInteger("-765875"), new BigInteger("876856"));    testV2OnlyStats(Types.optional(PrimitiveTypeName.FIXED_LEN_BYTE_ARRAY).length(14).as(OriginalType.DECIMAL).precision(7).named(""), new BigInteger("-6769643"), new BigInteger("9864675"));}
0
private void testV2OnlyStats(PrimitiveType type, Object min, Object max)
{    Statistics<?> stats = createStats(type, min, max);    org.apache.parquet.format.Statistics statistics = ParquetMetadataConverter.toParquetStatistics(stats);    assertFalse(statistics.isSetMin());    assertFalse(statistics.isSetMax());    assertEquals(ByteBuffer.wrap(stats.getMinBytes()), statistics.min_value);    assertEquals(ByteBuffer.wrap(stats.getMaxBytes()), statistics.max_value);}
0
public void testV2StatsEqualMinMax()
{    testV2StatsEqualMinMax(Types.optional(PrimitiveTypeName.INT32).as(OriginalType.UINT_8).named(""), 93, 93);    testV2StatsEqualMinMax(Types.optional(PrimitiveTypeName.INT32).as(OriginalType.UINT_16).named(""), -5892, -5892);    testV2StatsEqualMinMax(Types.optional(PrimitiveTypeName.INT32).as(OriginalType.UINT_32).named(""), 234998934, 234998934);    testV2StatsEqualMinMax(Types.optional(PrimitiveTypeName.INT64).as(OriginalType.UINT_64).named(""), -2389943895984985L, -2389943895984985L);    testV2StatsEqualMinMax(Types.optional(PrimitiveTypeName.BINARY).as(OriginalType.DECIMAL).precision(6).named(""), new BigInteger("823749"), new BigInteger("823749"));    testV2StatsEqualMinMax(Types.optional(PrimitiveTypeName.FIXED_LEN_BYTE_ARRAY).length(14).as(OriginalType.DECIMAL).precision(7).named(""), new BigInteger("-8752832"), new BigInteger("-8752832"));    testV2StatsEqualMinMax(Types.optional(PrimitiveTypeName.INT96).named(""), new BigInteger("81032984"), new BigInteger("81032984"));}
0
private void testV2StatsEqualMinMax(PrimitiveType type, Object min, Object max)
{    Statistics<?> stats = createStats(type, min, max);    org.apache.parquet.format.Statistics statistics = ParquetMetadataConverter.toParquetStatistics(stats);    assertEquals(ByteBuffer.wrap(stats.getMinBytes()), statistics.min);    assertEquals(ByteBuffer.wrap(stats.getMaxBytes()), statistics.max);    assertEquals(ByteBuffer.wrap(stats.getMinBytes()), statistics.min_value);    assertEquals(ByteBuffer.wrap(stats.getMaxBytes()), statistics.max_value);}
0
private static Statistics<?> createStats(PrimitiveType type, T min, T max)
{    Class<?> c = min.getClass();    if (c == Integer.class) {        return createStatsTyped(type, (Integer) min, (Integer) max);    } else if (c == Long.class) {        return createStatsTyped(type, (Long) min, (Long) max);    } else if (c == BigInteger.class) {        return createStatsTyped(type, (BigInteger) min, (BigInteger) max);    }    fail("Not implemented");    return null;}
0
private static Statistics<?> createStatsTyped(PrimitiveType type, int min, int max)
{    Statistics<?> stats = Statistics.createStats(type);    stats.updateStats(max);    stats.updateStats(min);    assertEquals(min, stats.genericGetMin());    assertEquals(max, stats.genericGetMax());    return stats;}
0
private static Statistics<?> createStatsTyped(PrimitiveType type, long min, long max)
{    Statistics<?> stats = Statistics.createStats(type);    stats.updateStats(max);    stats.updateStats(min);    assertEquals(min, stats.genericGetMin());    assertEquals(max, stats.genericGetMax());    return stats;}
0
private static Statistics<?> createStatsTyped(PrimitiveType type, BigInteger min, BigInteger max)
{    Statistics<?> stats = Statistics.createStats(type);    Binary minBinary = Binary.fromConstantByteArray(min.toByteArray());    Binary maxBinary = Binary.fromConstantByteArray(max.toByteArray());    stats.updateStats(maxBinary);    stats.updateStats(minBinary);    assertEquals(minBinary, stats.genericGetMin());    assertEquals(maxBinary, stats.genericGetMax());    return stats;}
0
public org.apache.parquet.format.Statistics toParquetStatistics(Statistics<?> stats)
{    org.apache.parquet.format.Statistics statistics = ParquetMetadataConverter.toParquetStatistics(stats);    statistics.unsetMin_value();    statistics.unsetMax_value();    return statistics;}
0
public org.apache.parquet.format.Statistics toParquetStatistics(Statistics<?> stats)
{    return ParquetMetadataConverter.toParquetStatistics(stats);}
0
public void testColumnOrders() throws IOException
{    MessageType schema = parseMessageType("message test {" +     "  optional binary binary_col;" + "  optional group map_col (MAP) {" + "    repeated group map (MAP_KEY_VALUE) {" +     "        required binary key (UTF8);" + "        optional group list_col (LIST) {" + "          repeated group list {" +     "            optional int96 array_element;" + "          }" + "        }" + "    }" + "  }" + "}");    org.apache.parquet.hadoop.metadata.FileMetaData fileMetaData = new org.apache.parquet.hadoop.metadata.FileMetaData(schema, new HashMap<String, String>(), null);    ParquetMetadata metadata = new ParquetMetadata(fileMetaData, new ArrayList<BlockMetaData>());    ParquetMetadataConverter converter = new ParquetMetadataConverter();    FileMetaData formatMetadata = converter.toParquetMetadata(1, metadata);    List<org.apache.parquet.format.ColumnOrder> columnOrders = formatMetadata.getColumn_orders();    assertEquals(3, columnOrders.size());    for (org.apache.parquet.format.ColumnOrder columnOrder : columnOrders) {        assertTrue(columnOrder.isSetTYPE_ORDER());    }            columnOrders.get(1).clear();    MessageType resultSchema = converter.fromParquetMetadata(formatMetadata).getFileMetaData().getSchema();    List<ColumnDescriptor> columns = resultSchema.getColumns();    assertEquals(3, columns.size());    assertEquals(ColumnOrder.typeDefined(), columns.get(0).getPrimitiveType().columnOrder());    assertEquals(ColumnOrder.undefined(), columns.get(1).getPrimitiveType().columnOrder());    assertEquals(ColumnOrder.undefined(), columns.get(2).getPrimitiveType().columnOrder());}
0
public void testOffsetIndexConversion()
{    OffsetIndexBuilder builder = OffsetIndexBuilder.getBuilder();    builder.add(1000, 10000, 0);    builder.add(22000, 12000, 100);    OffsetIndex offsetIndex = ParquetMetadataConverter.fromParquetOffsetIndex(ParquetMetadataConverter.toParquetOffsetIndex(builder.build(100000)));    assertEquals(2, offsetIndex.getPageCount());    assertEquals(101000, offsetIndex.getOffset(0));    assertEquals(10000, offsetIndex.getCompressedPageSize(0));    assertEquals(0, offsetIndex.getFirstRowIndex(0));    assertEquals(122000, offsetIndex.getOffset(1));    assertEquals(12000, offsetIndex.getCompressedPageSize(1));    assertEquals(100, offsetIndex.getFirstRowIndex(1));}
0
public void testColumnIndexConversion()
{    PrimitiveType type = Types.required(PrimitiveTypeName.INT64).named("test_int64");    ColumnIndexBuilder builder = ColumnIndexBuilder.getBuilder(type, Integer.MAX_VALUE);    Statistics<?> stats = Statistics.createStats(type);    stats.incrementNumNulls(16);    stats.updateStats(-100l);    stats.updateStats(100l);    builder.add(stats);    stats = Statistics.createStats(type);    stats.incrementNumNulls(111);    builder.add(stats);    stats = Statistics.createStats(type);    stats.updateStats(200l);    stats.updateStats(500l);    builder.add(stats);    org.apache.parquet.format.ColumnIndex parquetColumnIndex = ParquetMetadataConverter.toParquetColumnIndex(type, builder.build());    ColumnIndex columnIndex = ParquetMetadataConverter.fromParquetColumnIndex(type, parquetColumnIndex);    assertEquals(BoundaryOrder.ASCENDING, columnIndex.getBoundaryOrder());    assertTrue(Arrays.asList(false, true, false).equals(columnIndex.getNullPages()));    assertTrue(Arrays.asList(16l, 111l, 0l).equals(columnIndex.getNullCounts()));    assertTrue(Arrays.asList(ByteBuffer.wrap(BytesUtils.longToBytes(-100l)), ByteBuffer.allocate(0), ByteBuffer.wrap(BytesUtils.longToBytes(200l))).equals(columnIndex.getMinValues()));    assertTrue(Arrays.asList(ByteBuffer.wrap(BytesUtils.longToBytes(100l)), ByteBuffer.allocate(0), ByteBuffer.wrap(BytesUtils.longToBytes(500l))).equals(columnIndex.getMaxValues()));    assertNull("Should handle null column index", ParquetMetadataConverter.toParquetColumnIndex(Types.required(PrimitiveTypeName.INT32).named("test_int32"), null));    assertNull("Should ignore unsupported types", ParquetMetadataConverter.toParquetColumnIndex(Types.required(PrimitiveTypeName.INT96).named("test_int96"), columnIndex));    assertNull("Should ignore unsupported types", ParquetMetadataConverter.fromParquetColumnIndex(Types.required(PrimitiveTypeName.FIXED_LEN_BYTE_ARRAY).length(12).as(OriginalType.INTERVAL).named("test_interval"), parquetColumnIndex));}
0
public void testReadingCodecs() throws IOException
{    shouldUseParquetFlagToSetCodec("gzip", CompressionCodecName.GZIP);    shouldUseHadoopFlagToSetCodec(CompressionCodecName.GZIP.getHadoopCompressionCodecClassName(), CompressionCodecName.GZIP);    shouldUseParquetFlagToSetCodec("snappy", CompressionCodecName.SNAPPY);    shouldUseHadoopFlagToSetCodec(CompressionCodecName.SNAPPY.getHadoopCompressionCodecClassName(), CompressionCodecName.SNAPPY);        shouldUseHadoopFlagToSetCodec("unexistedCodec", CompressionCodecName.UNCOMPRESSED);        shouldUseHadoopFlagToSetCodec("org.apache.hadoop.io.compress.DefaultCodec", CompressionCodecName.UNCOMPRESSED);}
0
public void shouldUseParquetFlagToSetCodec(String codecNameStr, CompressionCodecName expectedCodec) throws IOException
{        Job job = new Job();    Configuration conf = job.getConfiguration();    conf.set(ParquetOutputFormat.COMPRESSION, codecNameStr);    TaskAttemptContext task = ContextUtil.newTaskAttemptContext(conf, new TaskAttemptID(new TaskID(new JobID("test", 1), false, 1), 1));    Assert.assertEquals(CodecConfig.from(task).getCodec(), expectedCodec);        JobConf jobConf = new JobConf();    jobConf.set(ParquetOutputFormat.COMPRESSION, codecNameStr);    Assert.assertEquals(CodecConfig.from(jobConf).getCodec(), expectedCodec);}
0
public void shouldUseHadoopFlagToSetCodec(String codecClassStr, CompressionCodecName expectedCodec) throws IOException
{        Job job = new Job();    Configuration conf = job.getConfiguration();    conf.setBoolean("mapred.output.compress", true);    conf.set("mapred.output.compression.codec", codecClassStr);    TaskAttemptContext task = ContextUtil.newTaskAttemptContext(conf, new TaskAttemptID(new TaskID(new JobID("test", 1), false, 1), 1));    Assert.assertEquals(expectedCodec, CodecConfig.from(task).getCodec());        JobConf jobConf = new JobConf();    jobConf.setBoolean("mapred.output.compress", true);    jobConf.set("mapred.output.compression.codec", codecClassStr);    Assert.assertEquals(CodecConfig.from(jobConf).getCodec(), expectedCodec);}
0
public void setUp()
{    conf = new Configuration();    jobConf = new JobConf();    writeSchema = "message example {\n" + "required int32 line;\n" + "required binary content;\n" + "}";    readSchema = "message example {\n" + "required int32 line;\n" + "required binary content;\n" + "}";}
0
private void runMapReduceJob(CompressionCodecName codec) throws IOException, ClassNotFoundException, InterruptedException
{    final FileSystem fileSystem = parquetPath.getFileSystem(conf);    fileSystem.delete(parquetPath, true);    fileSystem.delete(outputPath, true);    {        writeJob = new Job(conf, "write");        TextInputFormat.addInputPath(writeJob, inputPath);        writeJob.setInputFormatClass(TextInputFormat.class);        writeJob.setNumReduceTasks(0);        ExampleOutputFormat.setCompression(writeJob, codec);        ExampleOutputFormat.setOutputPath(writeJob, parquetPath);        writeJob.setOutputFormatClass(ExampleOutputFormat.class);        writeJob.setMapperClass(ReadMapper.class);        ExampleOutputFormat.setSchema(writeJob, MessageTypeParser.parseMessageType(writeSchema));        writeJob.submit();        waitForJob(writeJob);    }    {        jobConf.set(ReadSupport.PARQUET_READ_SCHEMA, readSchema);        jobConf.set(ParquetInputFormat.READ_SUPPORT_CLASS, GroupReadSupport.class.getCanonicalName());        jobConf.setInputFormat(MyDeprecatedInputFormat.class);        MyDeprecatedInputFormat.setInputPaths(jobConf, parquetPath);        jobConf.setOutputFormat(org.apache.hadoop.mapred.TextOutputFormat.class);        org.apache.hadoop.mapred.TextOutputFormat.setOutputPath(jobConf, outputPath);        jobConf.setMapperClass(DeprecatedWriteMapper.class);        jobConf.setNumReduceTasks(0);        mapRedJob = JobClient.runJob(jobConf);    }}
0
public void testReadWriteWithCountDeprecated() throws Exception
{    runMapReduceJob(CompressionCodecName.GZIP);    assertTrue(mapRedJob.getCounters().getGroup("parquet").getCounterForName("bytesread").getValue() > 0L);    assertTrue(mapRedJob.getCounters().getGroup("parquet").getCounterForName("bytestotal").getValue() > 0L);    assertTrue(mapRedJob.getCounters().getGroup("parquet").getCounterForName("bytesread").getValue() == mapRedJob.getCounters().getGroup("parquet").getCounterForName("bytestotal").getValue());}
0
public void testReadWriteWithoutCounter() throws Exception
{    jobConf.set("parquet.benchmark.time.read", "false");    jobConf.set("parquet.benchmark.bytes.total", "false");    jobConf.set("parquet.benchmark.bytes.read", "false");    runMapReduceJob(CompressionCodecName.GZIP);    assertEquals(mapRedJob.getCounters().getGroup("parquet").getCounterForName("bytesread").getValue(), 0L);    assertEquals(mapRedJob.getCounters().getGroup("parquet").getCounterForName("bytestotal").getValue(), 0L);    assertEquals(mapRedJob.getCounters().getGroup("parquet").getCounterForName("timeread").getValue(), 0L);}
0
private void waitForJob(Job job) throws InterruptedException, IOException
{    while (!job.isComplete()) {        System.out.println("waiting for job " + job.getJobName());        sleep(100);    }    System.out.println("status for job " + job.getJobName() + ": " + (job.isSuccessful() ? "SUCCESS" : "FAILURE"));    if (!job.isSuccessful()) {        throw new RuntimeException("job failed " + job.getJobName());    }}
0
protected void setup(Context context) throws IOException, InterruptedException
{    factory = new SimpleGroupFactory(GroupWriteSupport.getSchema(ContextUtil.getConfiguration(context)));}
0
protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException
{    Group group = factory.newGroup().append("line", (int) key.get()).append("content", value.toString());    context.write(null, group);}
0
public void map(Void aVoid, Container<Group> valueContainer, OutputCollector<LongWritable, Text> longWritableTextOutputCollector, Reporter reporter) throws IOException
{    Group value = valueContainer.get();    longWritableTextOutputCollector.collect(new LongWritable(value.getInteger("line", 0)), new Text(value.getString("content", 0)));}
0
public void close() throws IOException
{}
0
public void configure(JobConf entries)
{}
0
public void setUp()
{    conf = new Configuration();    jobConf = new JobConf();    writeSchema = "message example {\n" + "required int32 line;\n" + "required binary content;\n" + "}";}
0
private void runMapReduceJob(CompressionCodecName codec) throws IOException, ClassNotFoundException, InterruptedException
{    final FileSystem fileSystem = parquetPath.getFileSystem(conf);    fileSystem.delete(parquetPath, true);    fileSystem.delete(outputPath, true);    {        jobConf.setInputFormat(TextInputFormat.class);        TextInputFormat.addInputPath(jobConf, inputPath);        jobConf.setNumReduceTasks(0);        jobConf.setOutputFormat(DeprecatedParquetOutputFormat.class);        DeprecatedParquetOutputFormat.setCompression(jobConf, codec);        DeprecatedParquetOutputFormat.setOutputPath(jobConf, parquetPath);        DeprecatedParquetOutputFormat.setWriteSupportClass(jobConf, GroupWriteSupport.class);        GroupWriteSupport.setSchema(MessageTypeParser.parseMessageType(writeSchema), jobConf);        jobConf.setMapperClass(DeprecatedMapper.class);        mapRedJob = JobClient.runJob(jobConf);    }}
0
public void testReadWrite() throws Exception
{    runMapReduceJob(CompressionCodecName.GZIP);    assert (mapRedJob.isSuccessful());}
0
public void configure(JobConf job)
{    factory = new SimpleGroupFactory(GroupWriteSupport.getSchema(job));}
0
public void map(LongWritable key, Text value, OutputCollector<Void, Group> outputCollector, Reporter reporter) throws IOException
{    Group group = factory.newGroup().append("line", (int) key.get()).append("content", value.toString());    outputCollector.collect(null, group);}
0
public void close()
{}
0
public void testInitWithoutSpecifyingRequestSchema() throws Exception
{    GroupReadSupport s = new GroupReadSupport();    Configuration configuration = new Configuration();    Map<String, String> keyValueMetaData = new HashMap<String, String>();    MessageType fileSchema = MessageTypeParser.parseMessageType(fullSchemaStr);    ReadSupport.ReadContext context = s.init(configuration, keyValueMetaData, fileSchema);    assertEquals(context.getRequestedSchema(), fileSchema);}
0
public void testInitWithPartialSchema()
{    GroupReadSupport s = new GroupReadSupport();    Configuration configuration = new Configuration();    Map<String, String> keyValueMetaData = new HashMap<String, String>();    MessageType fileSchema = MessageTypeParser.parseMessageType(fullSchemaStr);    MessageType partialSchema = MessageTypeParser.parseMessageType(partialSchemaStr);    configuration.set(ReadSupport.PARQUET_READ_SCHEMA, partialSchemaStr);    ReadSupport.ReadContext context = s.init(configuration, keyValueMetaData, fileSchema);    assertEquals(context.getRequestedSchema(), partialSchema);}
0
public void setUp()
{    conf = new Configuration();    writeSchema = "message example {\n" + "required int32 line;\n" + "required binary content;\n" + "}";    readSchema = "message example {\n" + "required int32 line;\n" + "required binary content;\n" + "}";    partialSchema = "message example {\n" + "required int32 line;\n" + "}";    readMapperClass = ReadMapper.class;    writeMapperClass = WriteMapper.class;}
0
public void write(Group record)
{    super.write(record);    ++count;}
0
public org.apache.parquet.hadoop.api.WriteSupport.FinalizedWriteContext finalizeWrite()
{    Map<String, String> extraMetadata = new HashMap<String, String>();    extraMetadata.put("my.count", String.valueOf(count));    return new FinalizedWriteContext(extraMetadata);}
0
public org.apache.parquet.hadoop.api.ReadSupport.ReadContext init(InitContext context)
{    Set<String> counts = context.getKeyValueMetadata().get("my.count");    assertTrue("counts: " + counts, counts.size() > 0);    return super.init(context);}
0
protected void setup(org.apache.hadoop.mapreduce.Mapper<LongWritable, Text, Void, Group>.Context context) throws java.io.IOException, InterruptedException
{    factory = new SimpleGroupFactory(GroupWriteSupport.getSchema(ContextUtil.getConfiguration(context)));}
0
protected void map(LongWritable key, Text value, Mapper<LongWritable, Text, Void, Group>.Context context) throws java.io.IOException, InterruptedException
{    Group group = factory.newGroup().append("line", (int) key.get()).append("content", value.toString());    context.write(null, group);}
0
protected void map(Void key, Group value, Mapper<Void, Group, LongWritable, Text>.Context context) throws IOException, InterruptedException
{    context.write(new LongWritable(value.getInteger("line", 0)), new Text(value.getString("content", 0)));}
0
protected void map(Void key, Group value, Mapper<Void, Group, LongWritable, Text>.Context context) throws IOException, InterruptedException
{    context.write(new LongWritable(value.getInteger("line", 0)), new Text("dummy"));}
0
private void runMapReduceJob(CompressionCodecName codec) throws IOException, ClassNotFoundException, InterruptedException
{    runMapReduceJob(codec, Collections.<String, String>emptyMap());}
0
private void runMapReduceJob(CompressionCodecName codec, Map<String, String> extraConf) throws IOException, ClassNotFoundException, InterruptedException
{    Configuration conf = new Configuration(this.conf);    for (Map.Entry<String, String> entry : extraConf.entrySet()) {        conf.set(entry.getKey(), entry.getValue());    }    final FileSystem fileSystem = parquetPath.getFileSystem(conf);    fileSystem.delete(parquetPath, true);    fileSystem.delete(outputPath, true);    {        writeJob = new Job(conf, "write");        TextInputFormat.addInputPath(writeJob, inputPath);        writeJob.setInputFormatClass(TextInputFormat.class);        writeJob.setNumReduceTasks(0);        ParquetOutputFormat.setCompression(writeJob, codec);        ParquetOutputFormat.setOutputPath(writeJob, parquetPath);        writeJob.setOutputFormatClass(ParquetOutputFormat.class);        writeJob.setMapperClass(readMapperClass);        ParquetOutputFormat.setWriteSupportClass(writeJob, MyWriteSupport.class);        GroupWriteSupport.setSchema(MessageTypeParser.parseMessageType(writeSchema), writeJob.getConfiguration());        writeJob.submit();        waitForJob(writeJob);    }    {        conf.set(ReadSupport.PARQUET_READ_SCHEMA, readSchema);        readJob = new Job(conf, "read");        readJob.setInputFormatClass(ParquetInputFormat.class);        ParquetInputFormat.setReadSupportClass(readJob, MyReadSupport.class);        ParquetInputFormat.setInputPaths(readJob, parquetPath);        readJob.setOutputFormatClass(TextOutputFormat.class);        TextOutputFormat.setOutputPath(readJob, outputPath);        readJob.setMapperClass(writeMapperClass);        readJob.setNumReduceTasks(0);        readJob.submit();        waitForJob(readJob);    }}
0
private void testReadWrite(CompressionCodecName codec) throws IOException, ClassNotFoundException, InterruptedException
{    testReadWrite(codec, Collections.<String, String>emptyMap());}
0
private void testReadWrite(CompressionCodecName codec, Map<String, String> conf) throws IOException, ClassNotFoundException, InterruptedException
{    runMapReduceJob(codec, conf);    final BufferedReader in = new BufferedReader(new FileReader(new File(inputPath.toString())));    final BufferedReader out = new BufferedReader(new FileReader(new File(outputPath.toString(), "part-m-00000")));    String lineIn;    String lineOut = null;    int lineNumber = 0;    while ((lineIn = in.readLine()) != null && (lineOut = out.readLine()) != null) {        ++lineNumber;        lineOut = lineOut.substring(lineOut.indexOf("\t") + 1);        assertEquals("line " + lineNumber, lineIn, lineOut);    }    assertNull("line " + lineNumber, out.readLine());    assertNull("line " + lineNumber, lineIn);    in.close();    out.close();}
0
public void testReadWrite() throws IOException, ClassNotFoundException, InterruptedException
{        testReadWrite(CompressionCodecName.GZIP);    testReadWrite(CompressionCodecName.UNCOMPRESSED);    testReadWrite(CompressionCodecName.SNAPPY);}
0
public void testReadWriteTaskSideMD() throws IOException, ClassNotFoundException, InterruptedException
{    testReadWrite(CompressionCodecName.UNCOMPRESSED, new HashMap<String, String>() {        {            put("parquet.task.side.metadata", "true");        }    });}
0
public void testReadWriteTaskSideMDAggressiveFilter() throws IOException, ClassNotFoundException, InterruptedException
{    Configuration conf = new Configuration();        ParquetInputFormat.setFilterPredicate(conf, FilterApi.eq(FilterApi.intColumn("line"), -1000));    final String fpString = conf.get(ParquetInputFormat.FILTER_PREDICATE);    runMapReduceJob(CompressionCodecName.UNCOMPRESSED, new HashMap<String, String>() {        {            put("parquet.task.side.metadata", "true");            put(ParquetInputFormat.FILTER_PREDICATE, fpString);        }    });    File file = new File(outputPath.toString(), "part-m-00000");    List<String> lines = Files.readAllLines(file.toPath(), StandardCharsets.UTF_8);    assertTrue(lines.isEmpty());}
0
public void testReadWriteFilter() throws IOException, ClassNotFoundException, InterruptedException
{    Configuration conf = new Configuration();            ParquetInputFormat.setFilterPredicate(conf, FilterApi.lt(FilterApi.intColumn("line"), 500));    final String fpString = conf.get(ParquetInputFormat.FILTER_PREDICATE);    runMapReduceJob(CompressionCodecName.UNCOMPRESSED, new HashMap<String, String>() {        {            put("parquet.task.side.metadata", "true");            put(ParquetInputFormat.FILTER_PREDICATE, fpString);        }    });    File file = new File(inputPath.toString());    List<String> expected = Files.readAllLines(file.toPath(), StandardCharsets.UTF_8);        int size = 0;    Iterator<String> iter = expected.iterator();    while (iter.hasNext()) {        String next = iter.next();        if (size < 500) {            size += next.length();            continue;        }        iter.remove();    }        File file2 = new File(outputPath.toString(), "part-m-00000");    List<String> found = Files.readAllLines(file2.toPath(), StandardCharsets.UTF_8);    StringBuilder sbFound = new StringBuilder();    for (String line : found) {        sbFound.append(line.split("\t", -1)[1]);        sbFound.append("\n");    }    sbFound.deleteCharAt(sbFound.length() - 1);    assertEquals(Strings.join(expected, "\n"), sbFound.toString());}
0
public void testProjection() throws Exception
{    readSchema = partialSchema;    writeMapperClass = PartialWriteMapper.class;    runMapReduceJob(CompressionCodecName.GZIP);}
0
private static long value(Job job, String groupName, String name) throws Exception
{        Method getGroup = org.apache.hadoop.mapreduce.Counters.class.getMethod("getGroup", String.class);        Method findCounter = org.apache.hadoop.mapreduce.CounterGroup.class.getMethod("findCounter", String.class);        Method getValue = org.apache.hadoop.mapreduce.Counter.class.getMethod("getValue");    CounterGroup group = (CounterGroup) getGroup.invoke(job.getCounters(), groupName);    Counter counter = (Counter) findCounter.invoke(group, name);    return (Long) getValue.invoke(counter);}
0
public void testReadWriteWithCounter() throws Exception
{    runMapReduceJob(CompressionCodecName.GZIP);    assertTrue(value(readJob, "parquet", "bytesread") > 0L);    assertTrue(value(readJob, "parquet", "bytestotal") > 0L);    assertTrue(value(readJob, "parquet", "bytesread") == value(readJob, "parquet", "bytestotal"));}
0
public void testReadWriteWithoutCounter() throws Exception
{    conf.set("parquet.benchmark.time.read", "false");    conf.set("parquet.benchmark.bytes.total", "false");    conf.set("parquet.benchmark.bytes.read", "false");    runMapReduceJob(CompressionCodecName.GZIP);    assertTrue(value(readJob, "parquet", "bytesread") == 0L);    assertTrue(value(readJob, "parquet", "bytestotal") == 0L);    assertTrue(value(readJob, "parquet", "timeread") == 0L);}
0
private void waitForJob(Job job) throws InterruptedException, IOException
{    while (!job.isComplete()) {                sleep(100);    }        if (!job.isSuccessful()) {        throw new RuntimeException("job failed " + job.getJobName());    }}
1
public void testConversionBig()
{    long big = (long) Integer.MAX_VALUE + 1;    ColumnChunkMetaData md = newMD(big);    assertTrue(md instanceof IntColumnChunkMetaData);    assertEquals(big, md.getFirstDataPageOffset());}
0
public void testConversionSmall()
{    long small = 1;    ColumnChunkMetaData md = newMD(small);    assertTrue(md instanceof IntColumnChunkMetaData);    assertEquals(small, md.getFirstDataPageOffset());}
0
public void testConversionVeryBig()
{    long veryBig = (long) Integer.MAX_VALUE * 3;    ColumnChunkMetaData md = newMD(veryBig);    assertTrue(md instanceof LongColumnChunkMetaData);    assertEquals(veryBig, md.getFirstDataPageOffset());}
0
public void testConversionNeg()
{    long neg = -1;    ColumnChunkMetaData md = newMD(neg);    assertTrue(md instanceof LongColumnChunkMetaData);    assertEquals(neg, md.getFirstDataPageOffset());}
0
private ColumnChunkMetaData newMD(long big)
{    Set<Encoding> e = new HashSet<Encoding>();    PrimitiveTypeName t = BINARY;    ColumnPath p = ColumnPath.get("foo");    CompressionCodecName c = CompressionCodecName.GZIP;    BinaryStatistics s = new BinaryStatistics();    ColumnChunkMetaData md = ColumnChunkMetaData.get(p, t, c, e, s, big, 0, 0, 0, 0);    return md;}
0
 PositionOutputStream out()
{    return out;}
0
public PositionOutputStream create(long blockSizeHint) throws IOException
{    return out = file.create(blockSizeHint);}
0
public PositionOutputStream createOrOverwrite(long blockSizeHint) throws IOException
{    return out = file.createOrOverwrite(blockSizeHint);}
0
public boolean supportsBlockSize()
{    return file.supportsBlockSize();}
0
public long defaultBlockSize()
{    return file.defaultBlockSize();}
0
public void initConfiguration()
{    this.conf = new Configuration();}
0
public void test() throws Exception
{    Path file = new Path("target/test/TestColumnChunkPageWriteStore/test.parquet");    Path root = file.getParent();    FileSystem fs = file.getFileSystem(conf);    if (fs.exists(root)) {        fs.delete(root, true);    }    fs.mkdirs(root);    MessageType schema = MessageTypeParser.parseMessageType("message test { repeated binary bar; }");    ColumnDescriptor col = schema.getColumns().get(0);    Encoding dataEncoding = PLAIN;    int valueCount = 10;    int d = 1;    int r = 2;    int v = 3;    BytesInput definitionLevels = BytesInput.fromInt(d);    BytesInput repetitionLevels = BytesInput.fromInt(r);    Statistics<?> statistics = Statistics.getBuilderForReading(Types.required(PrimitiveTypeName.BINARY).named("test_binary")).build();    BytesInput data = BytesInput.fromInt(v);    int rowCount = 5;    int nullCount = 1;    statistics.incrementNumNulls(nullCount);    statistics.setMinMaxFromBytes(new byte[] { 0, 1, 2 }, new byte[] { 0, 1, 2, 3 });    long pageOffset;    long pageSize;    {        OutputFileForTesting outputFile = new OutputFileForTesting(file, conf);        ParquetFileWriter writer = new ParquetFileWriter(outputFile, schema, Mode.CREATE, ParquetWriter.DEFAULT_BLOCK_SIZE, ParquetWriter.MAX_PADDING_SIZE_DEFAULT);        writer.start();        writer.startBlock(rowCount);        pageOffset = outputFile.out().getPos();        {            ColumnChunkPageWriteStore store = new ColumnChunkPageWriteStore(compressor(GZIP), schema, new HeapByteBufferAllocator(), Integer.MAX_VALUE);            PageWriter pageWriter = store.getPageWriter(col);            pageWriter.writePageV2(rowCount, nullCount, valueCount, repetitionLevels, definitionLevels, dataEncoding, data, statistics);            store.flushToFileWriter(writer);            pageSize = outputFile.out().getPos() - pageOffset;        }        writer.endBlock();        writer.end(new HashMap<String, String>());    }    {        ParquetMetadata footer = ParquetFileReader.readFooter(conf, file, NO_FILTER);        ParquetFileReader reader = new ParquetFileReader(conf, footer.getFileMetaData(), file, footer.getBlocks(), schema.getColumns());        PageReadStore rowGroup = reader.readNextRowGroup();        PageReader pageReader = rowGroup.getPageReader(col);        DataPageV2 page = (DataPageV2) pageReader.readPage();        assertEquals(rowCount, page.getRowCount());        assertEquals(nullCount, page.getNullCount());        assertEquals(valueCount, page.getValueCount());        assertEquals(d, intValue(page.getDefinitionLevels()));        assertEquals(r, intValue(page.getRepetitionLevels()));        assertEquals(dataEncoding, page.getDataEncoding());        assertEquals(v, intValue(page.getData()));                ColumnChunkMetaData column = footer.getBlocks().get(0).getColumns().get(0);        ColumnIndex columnIndex = reader.readColumnIndex(column);        assertArrayEquals(statistics.getMinBytes(), columnIndex.getMinValues().get(0).array());        assertArrayEquals(statistics.getMaxBytes(), columnIndex.getMaxValues().get(0).array());        assertEquals(statistics.getNumNulls(), columnIndex.getNullCounts().get(0).longValue());        assertFalse(columnIndex.getNullPages().get(0));        OffsetIndex offsetIndex = reader.readOffsetIndex(column);        assertEquals(1, offsetIndex.getPageCount());        assertEquals(pageSize, offsetIndex.getCompressedPageSize(0));        assertEquals(0, offsetIndex.getFirstRowIndex(0));        assertEquals(pageOffset, offsetIndex.getOffset(0));        reader.close();    }}
0
private int intValue(BytesInput in) throws IOException
{    ByteArrayOutputStream baos = new ByteArrayOutputStream();    in.writeAllTo(baos);    LittleEndianDataInputStream os = new LittleEndianDataInputStream(new ByteArrayInputStream(baos.toByteArray()));    int i = os.readInt();    os.close();    return i;}
0
public void testColumnOrderV1() throws IOException
{    ParquetFileWriter mockFileWriter = Mockito.mock(ParquetFileWriter.class);    InOrder inOrder = inOrder(mockFileWriter);    MessageType schema = Types.buildMessage().required(BINARY).as(UTF8).named("a_string").required(INT32).named("an_int").required(INT64).named("a_long").required(FLOAT).named("a_float").required(DOUBLE).named("a_double").named("order_test");    BytesInput fakeData = BytesInput.fromInt(34);    int fakeCount = 3;    BinaryStatistics fakeStats = new BinaryStatistics();            ColumnChunkPageWriteStore store = new ColumnChunkPageWriteStore(compressor(UNCOMPRESSED), schema, new HeapByteBufferAllocator(), Integer.MAX_VALUE);    for (ColumnDescriptor col : schema.getColumns()) {        PageWriter pageWriter = store.getPageWriter(col);        pageWriter.writePage(fakeData, fakeCount, fakeStats, RLE, RLE, PLAIN);    }        store.flushToFileWriter(mockFileWriter);    for (ColumnDescriptor col : schema.getColumns()) {        inOrder.verify(mockFileWriter).writeColumnChunk(eq(col), eq((long) fakeCount), eq(UNCOMPRESSED), isNull(DictionaryPage.class), any(), eq(fakeData.size()), eq(fakeData.size()), eq(fakeStats),         same(ColumnIndexBuilder.getNoOpBuilder()),         same(OffsetIndexBuilder.getNoOpBuilder()), any(), any(), any());    }}
0
private CodecFactory.BytesCompressor compressor(CompressionCodecName codec)
{    return new CodecFactory(conf, pageSize).getCompressor(codec);}
0
public static Collection<Object[]> params()
{    return Arrays.asList(new Object[] { FILE_V1 }, new Object[] { FILE_V2 });}
0
private static List<User> generateData(int rowCount)
{    List<User> users = new ArrayList<>();    List<String> names = generateNames(rowCount);    for (int i = 0; i < rowCount; ++i) {        users.add(new User(i, names.get(i), generatePhoneNumbers(), generateLocation(i, rowCount)));    }    return users;}
0
private static List<String> generateNames(int rowCount)
{    List<String> list = new ArrayList<>();        list.add("anderson");    list.add("anderson");    list.add("miller");    list.add("miller");    list.add("miller");    list.add("thomas");    list.add("thomas");    list.add("williams");    int nullCount = rowCount / 100;    String alphabet = "aabcdeefghiijklmnoopqrstuuvwxyz";    int maxLength = 8;    for (int i = rowCount - list.size() - nullCount; i >= 0; --i) {        int l = RANDOM.nextInt(maxLength);        StringBuilder builder = new StringBuilder(l);        for (int j = 0; j < l; ++j) {            builder.append(alphabet.charAt(RANDOM.nextInt(alphabet.length())));        }        list.add(builder.toString());    }    Collections.sort(list, (str1, str2) -> -str1.compareTo(str2));        for (int i = 0; i < nullCount; ++i) {        list.add(RANDOM.nextInt(list.size()), null);    }    return list;}
0
private static List<PhoneNumber> generatePhoneNumbers()
{    int length = RANDOM.nextInt(5) - 1;    if (length < 0) {        return null;    }    List<PhoneNumber> phoneNumbers = new ArrayList<>(length);    for (int i = 0; i < length; ++i) {                long number = Math.abs(RANDOM.nextLong() % 900000) + 100000;        phoneNumbers.add(new PhoneNumber(number, PHONE_KINDS[RANDOM.nextInt(PHONE_KINDS.length)]));    }    return phoneNumbers;}
0
private static Location generateLocation(int id, int rowCount)
{    if (RANDOM.nextDouble() < 0.01) {        return null;    }    double lat = RANDOM.nextDouble() * 90.0 - (id < rowCount / 2 ? 90.0 : 0.0);    double lon = RANDOM.nextDouble() * 90.0 - (id < rowCount / 4 || id >= 3 * rowCount / 4 ? 90.0 : 0.0);    return new Location(RANDOM.nextDouble() < 0.01 ? null : lat, RANDOM.nextDouble() < 0.01 ? null : lon);}
0
private static Path createTempFile()
{    try {        return new Path(Files.createTempFile("test-ci_", ".parquet").toAbsolutePath().toString());    } catch (IOException e) {        throw new AssertionError("Unable to create temporary file", e);    }}
0
private List<User> readUsers(FilterPredicate filter, boolean useOtherFiltering) throws IOException
{    return readUsers(FilterCompat.get(filter), useOtherFiltering, true);}
0
private List<User> readUsers(FilterPredicate filter, boolean useOtherFiltering, boolean useColumnIndexFilter) throws IOException
{    return readUsers(FilterCompat.get(filter), useOtherFiltering, useColumnIndexFilter);}
0
private List<User> readUsers(Filter filter, boolean useOtherFiltering) throws IOException
{    return readUsers(filter, useOtherFiltering, true);}
0
private List<User> readUsers(Filter filter, boolean useOtherFiltering, boolean useColumnIndexFilter) throws IOException
{    return PhoneBookWriter.readUsers(ParquetReader.builder(new GroupReadSupport(), file).withFilter(filter).useDictionaryFilter(useOtherFiltering).useStatsFilter(useOtherFiltering).useRecordFilter(useOtherFiltering).useColumnIndexFilter(useColumnIndexFilter));}
0
private static void assertContains(Stream<User> expected, List<User> actual)
{    Iterator<User> expIt = expected.iterator();    if (!expIt.hasNext()) {        return;    }    User exp = expIt.next();    for (User act : actual) {        if (act.equals(exp)) {            if (!expIt.hasNext()) {                break;            }            exp = expIt.next();        }    }    assertFalse("Not all expected elements are in the actual list. E.g.: " + exp, expIt.hasNext());}
0
public static void createFile() throws IOException
{        int pageSize = DATA.size() / 10;        int rowGroupSize = pageSize * 6 * 5;    PhoneBookWriter.write(ExampleParquetWriter.builder(FILE_V1).withWriteMode(OVERWRITE).withRowGroupSize(rowGroupSize).withPageSize(pageSize).withWriterVersion(WriterVersion.PARQUET_1_0), DATA);    PhoneBookWriter.write(ExampleParquetWriter.builder(FILE_V2).withWriteMode(OVERWRITE).withRowGroupSize(rowGroupSize).withPageSize(pageSize).withWriterVersion(WriterVersion.PARQUET_2_0), DATA);}
0
public static void deleteFile() throws IOException
{    FILE_V1.getFileSystem(new Configuration()).delete(FILE_V1, false);    FILE_V2.getFileSystem(new Configuration()).delete(FILE_V2, false);}
0
public void testSimpleFiltering() throws IOException
{    assertCorrectFiltering(record -> record.getId() == 1234, eq(longColumn("id"), 1234l));    assertCorrectFiltering(record -> "miller".equals(record.getName()), eq(binaryColumn("name"), Binary.fromString("miller")));    assertCorrectFiltering(record -> record.getName() == null, eq(binaryColumn("name"), null));}
0
public void testNoFiltering() throws IOException
{        assertEquals(DATA, readUsers(FilterCompat.NOOP, false));    assertEquals(DATA, readUsers(FilterCompat.NOOP, true));        assertEquals(DATA.stream().filter(user -> user.getId() == 1234).collect(Collectors.toList()), readUsers(eq(longColumn("id"), 1234l), true, false));    assertEquals(DATA.stream().filter(user -> "miller".equals(user.getName())).collect(Collectors.toList()), readUsers(eq(binaryColumn("name"), Binary.fromString("miller")), true, false));    assertEquals(DATA.stream().filter(user -> user.getName() == null).collect(Collectors.toList()), readUsers(eq(binaryColumn("name"), null), true, false));        assertEquals(DATA, readUsers(eq(longColumn("id"), 1234l), false, false));    assertEquals(DATA, readUsers(eq(binaryColumn("name"), Binary.fromString("miller")), false, false));    assertEquals(DATA, readUsers(eq(binaryColumn("name"), null), false, false));}
0
public void testComplexFiltering() throws IOException
{    assertCorrectFiltering(record -> {        Location loc = record.getLocation();        Double lat = loc == null ? null : loc.getLat();        Double lon = loc == null ? null : loc.getLon();        return lat != null && lon != null && 37 <= lat && lat <= 70 && -21 <= lon && lon <= 35;    }, and(and(gtEq(doubleColumn("location.lat"), 37.0), ltEq(doubleColumn("location.lat"), 70.0)), and(gtEq(doubleColumn("location.lon"), -21.0), ltEq(doubleColumn("location.lon"), 35.0))));    assertCorrectFiltering(record -> {        Location loc = record.getLocation();        return loc == null || (loc.getLat() == null && loc.getLon() == null);    }, and(eq(doubleColumn("location.lat"), null), eq(doubleColumn("location.lon"), null)));    assertCorrectFiltering(record -> {        String name = record.getName();        return name != null && name.compareTo("thomas") < 0 && record.getId() <= 3 * DATA.size() / 4;    }, and(lt(binaryColumn("name"), Binary.fromString("thomas")), ltEq(longColumn("id"), 3l * DATA.size() / 4)));}
0
private static boolean isStartingWithVowel(String str)
{    if (str == null || str.isEmpty()) {        return false;    }    switch(str.charAt(0)) {        case 'a':        case 'e':        case 'i':        case 'o':        case 'u':            return true;        default:            return false;    }}
0
public boolean keep(Binary value)
{    return value != null && isStartingWithVowel(value.toStringUsingUTF8());}
0
public boolean canDrop(Statistics<Binary> statistics)
{    Comparator<Binary> cmp = statistics.getComparator();    Binary min = statistics.getMin();    Binary max = statistics.getMax();    return cmp.compare(max, A) < 0 || (cmp.compare(min, B) >= 0 && cmp.compare(max, E) < 0) || (cmp.compare(min, F) >= 0 && cmp.compare(max, I) < 0) || (cmp.compare(min, J) >= 0 && cmp.compare(max, O) < 0) || (cmp.compare(min, P) >= 0 && cmp.compare(max, U) < 0) || cmp.compare(min, V) >= 0;}
0
public boolean inverseCanDrop(Statistics<Binary> statistics)
{    Comparator<Binary> cmp = statistics.getComparator();    Binary min = statistics.getMin();    Binary max = statistics.getMax();    return (cmp.compare(min, A) >= 0 && cmp.compare(max, B) < 0) || (cmp.compare(min, E) >= 0 && cmp.compare(max, F) < 0) || (cmp.compare(min, I) >= 0 && cmp.compare(max, J) < 0) || (cmp.compare(min, O) >= 0 && cmp.compare(max, P) < 0) || (cmp.compare(min, U) >= 0 && cmp.compare(max, V) < 0);}
0
public boolean keep(Long value)
{        return value % divisor == 0;}
0
public boolean canDrop(Statistics<Long> statistics)
{    long min = statistics.getMin();    long max = statistics.getMax();    return min % divisor != 0 && max % divisor != 0 && min / divisor == max / divisor;}
0
public boolean inverseCanDrop(Statistics<Long> statistics)
{    long min = statistics.getMin();    long max = statistics.getMax();    return min == max && min % divisor == 0;}
0
public void testUDF() throws IOException
{    assertCorrectFiltering(record -> NameStartsWithVowel.isStartingWithVowel(record.getName()) || record.getId() % 234 == 0, or(userDefined(binaryColumn("name"), NameStartsWithVowel.class), userDefined(longColumn("id"), new IsDivisibleBy(234))));    assertCorrectFiltering(record -> !(NameStartsWithVowel.isStartingWithVowel(record.getName()) || record.getId() % 234 == 0), not(or(userDefined(binaryColumn("name"), NameStartsWithVowel.class), userDefined(longColumn("id"), new IsDivisibleBy(234)))));}
0
public void testFilteringWithMissingColumns() throws IOException
{        assertEquals(DATA, readUsers(notEq(binaryColumn("not-existing-binary"), Binary.EMPTY), true));    assertCorrectFiltering(record -> record.getId() == 1234, and(eq(longColumn("id"), 1234l), eq(longColumn("not-existing-long"), null)));    assertCorrectFiltering(record -> "miller".equals(record.getName()), and(eq(binaryColumn("name"), Binary.fromString("miller")), invert(userDefined(binaryColumn("not-existing-binary"), NameStartsWithVowel.class))));        assertEquals(emptyList(), readUsers(lt(longColumn("not-existing-long"), 0l), true));    assertCorrectFiltering(record -> "miller".equals(record.getName()), or(eq(binaryColumn("name"), Binary.fromString("miller")), gtEq(binaryColumn("not-existing-binary"), Binary.EMPTY)));    assertCorrectFiltering(record -> record.getId() == 1234, or(eq(longColumn("id"), 1234l), userDefined(longColumn("not-existing-long"), new IsDivisibleBy(1))));}
0
private Path writeSimpleParquetFile(Configuration conf, CompressionCodecName compression) throws IOException
{    File file = tempFolder.newFile();    file.delete();    Path path = new Path(file.toURI());    for (int i = 0; i < PAGE_SIZE; i++) {        colAPage1Bytes[i] = (byte) i;        colAPage2Bytes[i] = (byte) -i;        colBPage1Bytes[i] = (byte) (i + 100);        colBPage2Bytes[i] = (byte) (i - 100);    }    ParquetFileWriter writer = new ParquetFileWriter(conf, schemaSimple, path, ParquetWriter.DEFAULT_BLOCK_SIZE, ParquetWriter.MAX_PADDING_SIZE_DEFAULT);    writer.start();    writer.startBlock(numRecordsLargeFile);    CodecFactory codecFactory = new CodecFactory(conf, PAGE_SIZE);    CodecFactory.BytesCompressor compressor = codecFactory.getCompressor(compression);    ColumnChunkPageWriteStore writeStore = new ColumnChunkPageWriteStore(compressor, schemaSimple, new HeapByteBufferAllocator(), Integer.MAX_VALUE, ParquetOutputFormat.getPageWriteChecksumEnabled(conf));    PageWriter pageWriter = writeStore.getPageWriter(colADesc);    pageWriter.writePage(BytesInput.from(colAPage1Bytes), numRecordsLargeFile / 2, numRecordsLargeFile / 2, EMPTY_STATS_INT32, Encoding.RLE, Encoding.RLE, Encoding.PLAIN);    pageWriter.writePage(BytesInput.from(colAPage2Bytes), numRecordsLargeFile / 2, numRecordsLargeFile / 2, EMPTY_STATS_INT32, Encoding.RLE, Encoding.RLE, Encoding.PLAIN);    pageWriter = writeStore.getPageWriter(colBDesc);    pageWriter.writePage(BytesInput.from(colBPage1Bytes), numRecordsLargeFile / 2, numRecordsLargeFile / 2, EMPTY_STATS_INT32, Encoding.RLE, Encoding.RLE, Encoding.PLAIN);    pageWriter.writePage(BytesInput.from(colBPage2Bytes), numRecordsLargeFile / 2, numRecordsLargeFile / 2, EMPTY_STATS_INT32, Encoding.RLE, Encoding.RLE, Encoding.PLAIN);    writeStore.flushToFileWriter(writer);    writer.endBlock();    writer.end(new HashMap<>());    codecFactory.release();    return path;}
0
private Path writeNestedWithNullsSampleParquetFile(Configuration conf, boolean dictionaryEncoding, CompressionCodecName compression) throws IOException
{    File file = tempFolder.newFile();    file.delete();    Path path = new Path(file.toURI());    try (ParquetWriter<Group> writer = ExampleParquetWriter.builder(path).withConf(conf).withWriteMode(ParquetFileWriter.Mode.OVERWRITE).withCompressionCodec(compression).withDictionaryEncoding(dictionaryEncoding).withType(schemaNestedWithNulls).withPageWriteChecksumEnabled(ParquetOutputFormat.getPageWriteChecksumEnabled(conf)).build()) {        GroupFactory groupFactory = new SimpleGroupFactory(schemaNestedWithNulls);        Random rand = new Random(42);        for (int i = 0; i < numRecordsNestedWithNullsFile; i++) {            Group group = groupFactory.newGroup();            if (rand.nextDouble() > nullRatio) {                                if (rand.nextDouble() > 0.5) {                    group.addGroup("c").append("id", (long) i).addGroup("d").append("val", rand.nextInt() % 10);                } else {                    group.addGroup("c").append("id", (long) i).addGroup("d").append("val", rand.nextInt() % 10).append("val", rand.nextInt() % 10).append("val", rand.nextInt() % 10);                }            }            writer.write(group);        }    }    return path;}
0
public void testWriteOnVerifyOff() throws IOException
{    Configuration conf = new Configuration();    conf.setBoolean(ParquetOutputFormat.PAGE_WRITE_CHECKSUM_ENABLED, true);    conf.setBoolean(ParquetInputFormat.PAGE_VERIFY_CHECKSUM_ENABLED, false);    Path path = writeSimpleParquetFile(conf, CompressionCodecName.UNCOMPRESSED);    try (ParquetFileReader reader = getParquetFileReader(path, conf, Arrays.asList(colADesc, colBDesc))) {        PageReadStore pageReadStore = reader.readNextRowGroup();        DataPageV1 colAPage1 = readNextPage(colADesc, pageReadStore);        assertCrcSetAndCorrect(colAPage1, colAPage1Bytes);        assertCorrectContent(colAPage1.getBytes().toByteArray(), colAPage1Bytes);        DataPageV1 colAPage2 = readNextPage(colADesc, pageReadStore);        assertCrcSetAndCorrect(colAPage2, colAPage2Bytes);        assertCorrectContent(colAPage2.getBytes().toByteArray(), colAPage2Bytes);        DataPageV1 colBPage1 = readNextPage(colBDesc, pageReadStore);        assertCrcSetAndCorrect(colBPage1, colBPage1Bytes);        assertCorrectContent(colBPage1.getBytes().toByteArray(), colBPage1Bytes);        DataPageV1 colBPage2 = readNextPage(colBDesc, pageReadStore);        assertCrcSetAndCorrect(colBPage2, colBPage2Bytes);        assertCorrectContent(colBPage2.getBytes().toByteArray(), colBPage2Bytes);    }}
0
public void testWriteOffVerifyOff() throws IOException
{    Configuration conf = new Configuration();    conf.setBoolean(ParquetOutputFormat.PAGE_WRITE_CHECKSUM_ENABLED, false);    conf.setBoolean(ParquetInputFormat.PAGE_VERIFY_CHECKSUM_ENABLED, false);    Path path = writeSimpleParquetFile(conf, CompressionCodecName.UNCOMPRESSED);    try (ParquetFileReader reader = getParquetFileReader(path, conf, Arrays.asList(colADesc, colBDesc))) {        PageReadStore pageReadStore = reader.readNextRowGroup();        assertCrcNotSet(readNextPage(colADesc, pageReadStore));        assertCrcNotSet(readNextPage(colADesc, pageReadStore));        assertCrcNotSet(readNextPage(colBDesc, pageReadStore));        assertCrcNotSet(readNextPage(colBDesc, pageReadStore));    }}
0
public void testWriteOffVerifyOn() throws IOException
{    Configuration conf = new Configuration();    conf.setBoolean(ParquetOutputFormat.PAGE_WRITE_CHECKSUM_ENABLED, false);    conf.setBoolean(ParquetInputFormat.PAGE_VERIFY_CHECKSUM_ENABLED, true);    Path path = writeSimpleParquetFile(conf, CompressionCodecName.UNCOMPRESSED);    try (ParquetFileReader reader = getParquetFileReader(path, conf, Arrays.asList(colADesc, colBDesc))) {        PageReadStore pageReadStore = reader.readNextRowGroup();        assertCorrectContent(readNextPage(colADesc, pageReadStore).getBytes().toByteArray(), colAPage1Bytes);        assertCorrectContent(readNextPage(colADesc, pageReadStore).getBytes().toByteArray(), colAPage2Bytes);        assertCorrectContent(readNextPage(colBDesc, pageReadStore).getBytes().toByteArray(), colBPage1Bytes);        assertCorrectContent(readNextPage(colBDesc, pageReadStore).getBytes().toByteArray(), colBPage2Bytes);    }}
0
public void testWriteOnVerifyOn() throws IOException
{    Configuration conf = new Configuration();    conf.setBoolean(ParquetOutputFormat.PAGE_WRITE_CHECKSUM_ENABLED, true);    conf.setBoolean(ParquetInputFormat.PAGE_VERIFY_CHECKSUM_ENABLED, true);    Path path = writeSimpleParquetFile(conf, CompressionCodecName.UNCOMPRESSED);    try (ParquetFileReader reader = getParquetFileReader(path, conf, Arrays.asList(colADesc, colBDesc))) {        PageReadStore pageReadStore = reader.readNextRowGroup();        DataPageV1 colAPage1 = readNextPage(colADesc, pageReadStore);        assertCrcSetAndCorrect(colAPage1, colAPage1Bytes);        assertCorrectContent(colAPage1.getBytes().toByteArray(), colAPage1Bytes);        DataPageV1 colAPage2 = readNextPage(colADesc, pageReadStore);        assertCrcSetAndCorrect(colAPage2, colAPage2Bytes);        assertCorrectContent(colAPage2.getBytes().toByteArray(), colAPage2Bytes);        DataPageV1 colBPage1 = readNextPage(colBDesc, pageReadStore);        assertCrcSetAndCorrect(colBPage1, colBPage1Bytes);        assertCorrectContent(colBPage1.getBytes().toByteArray(), colBPage1Bytes);        DataPageV1 colBPage2 = readNextPage(colBDesc, pageReadStore);        assertCrcSetAndCorrect(colBPage2, colBPage2Bytes);        assertCorrectContent(colBPage2.getBytes().toByteArray(), colBPage2Bytes);    }}
0
public void testCorruptedPage() throws IOException
{    Configuration conf = new Configuration();    conf.setBoolean(ParquetOutputFormat.PAGE_WRITE_CHECKSUM_ENABLED, true);    Path path = writeSimpleParquetFile(conf, CompressionCodecName.UNCOMPRESSED);    InputFile inputFile = HadoopInputFile.fromPath(path, conf);    try (SeekableInputStream inputStream = inputFile.newStream()) {        int fileLen = (int) inputFile.getLength();        byte[] fileBytes = new byte[fileLen];        inputStream.readFully(fileBytes);        inputStream.close();                                fileBytes[fileLen / 8]++;        fileBytes[fileLen / 8 + ((fileLen / 4) * 3)]++;        OutputFile outputFile = HadoopOutputFile.fromPath(path, conf);        try (PositionOutputStream outputStream = outputFile.createOrOverwrite(1024 * 1024)) {            outputStream.write(fileBytes);            outputStream.close();                                    conf.setBoolean(ParquetInputFormat.PAGE_VERIFY_CHECKSUM_ENABLED, false);            try (ParquetFileReader reader = getParquetFileReader(path, conf, Arrays.asList(colADesc, colBDesc))) {                PageReadStore pageReadStore = reader.readNextRowGroup();                DataPageV1 colAPage1 = readNextPage(colADesc, pageReadStore);                assertFalse("Data in page was not corrupted", Arrays.equals(colAPage1.getBytes().toByteArray(), colAPage1Bytes));                readNextPage(colADesc, pageReadStore);                readNextPage(colBDesc, pageReadStore);                DataPageV1 colBPage2 = readNextPage(colBDesc, pageReadStore);                assertFalse("Data in page was not corrupted", Arrays.equals(colBPage2.getBytes().toByteArray(), colBPage2Bytes));            }                        conf.setBoolean(ParquetInputFormat.PAGE_VERIFY_CHECKSUM_ENABLED, true);            try (ParquetFileReader reader = getParquetFileReader(path, conf, Arrays.asList(colADesc, colBDesc))) {                                assertVerificationFailed(reader);            }        }    }}
0
public void testCompression() throws IOException
{    Configuration conf = new Configuration();    conf.setBoolean(ParquetOutputFormat.PAGE_WRITE_CHECKSUM_ENABLED, true);    conf.setBoolean(ParquetInputFormat.PAGE_VERIFY_CHECKSUM_ENABLED, true);    Path path = writeSimpleParquetFile(conf, CompressionCodecName.SNAPPY);    try (ParquetFileReader reader = getParquetFileReader(path, conf, Arrays.asList(colADesc, colBDesc))) {        PageReadStore pageReadStore = reader.readNextRowGroup();        DataPageV1 colAPage1 = readNextPage(colADesc, pageReadStore);        assertCrcSetAndCorrect(colAPage1, snappy(colAPage1Bytes));        assertCorrectContent(colAPage1.getBytes().toByteArray(), colAPage1Bytes);        DataPageV1 colAPage2 = readNextPage(colADesc, pageReadStore);        assertCrcSetAndCorrect(colAPage2, snappy(colAPage2Bytes));        assertCorrectContent(colAPage2.getBytes().toByteArray(), colAPage2Bytes);        DataPageV1 colBPage1 = readNextPage(colBDesc, pageReadStore);        assertCrcSetAndCorrect(colBPage1, snappy(colBPage1Bytes));        assertCorrectContent(colBPage1.getBytes().toByteArray(), colBPage1Bytes);        DataPageV1 colBPage2 = readNextPage(colBDesc, pageReadStore);        assertCrcSetAndCorrect(colBPage2, snappy(colBPage2Bytes));        assertCorrectContent(colBPage2.getBytes().toByteArray(), colBPage2Bytes);    }}
0
public void testNestedWithNulls() throws IOException
{    Configuration conf = new Configuration();            conf.setBoolean(ParquetOutputFormat.PAGE_WRITE_CHECKSUM_ENABLED, false);    conf.setBoolean(ParquetInputFormat.PAGE_VERIFY_CHECKSUM_ENABLED, false);    Path refPath = writeNestedWithNullsSampleParquetFile(conf, false, CompressionCodecName.SNAPPY);    try (ParquetFileReader refReader = getParquetFileReader(refPath, conf, Arrays.asList(colCIdDesc, colDValDesc))) {        PageReadStore refPageReadStore = refReader.readNextRowGroup();        byte[] colCIdPageBytes = readNextPage(colCIdDesc, refPageReadStore).getBytes().toByteArray();        byte[] colDValPageBytes = readNextPage(colDValDesc, refPageReadStore).getBytes().toByteArray();                conf.setBoolean(ParquetOutputFormat.PAGE_WRITE_CHECKSUM_ENABLED, true);        conf.setBoolean(ParquetInputFormat.PAGE_VERIFY_CHECKSUM_ENABLED, true);        Path path = writeNestedWithNullsSampleParquetFile(conf, false, CompressionCodecName.SNAPPY);        try (ParquetFileReader reader = getParquetFileReader(path, conf, Arrays.asList(colCIdDesc, colDValDesc))) {            PageReadStore pageReadStore = reader.readNextRowGroup();            DataPageV1 colCIdPage = readNextPage(colCIdDesc, pageReadStore);            assertCrcSetAndCorrect(colCIdPage, snappy(colCIdPageBytes));            assertCorrectContent(colCIdPage.getBytes().toByteArray(), colCIdPageBytes);            DataPageV1 colDValPage = readNextPage(colDValDesc, pageReadStore);            assertCrcSetAndCorrect(colDValPage, snappy(colDValPageBytes));            assertCorrectContent(colDValPage.getBytes().toByteArray(), colDValPageBytes);        }    }}
0
public void testDictionaryEncoding() throws IOException
{    Configuration conf = new Configuration();            conf.setBoolean(ParquetOutputFormat.PAGE_WRITE_CHECKSUM_ENABLED, false);    conf.setBoolean(ParquetInputFormat.PAGE_VERIFY_CHECKSUM_ENABLED, false);    Path refPath = writeNestedWithNullsSampleParquetFile(conf, true, CompressionCodecName.SNAPPY);    try (ParquetFileReader refReader = getParquetFileReader(refPath, conf, Collections.singletonList(colDValDesc))) {        PageReadStore refPageReadStore = refReader.readNextRowGroup();                byte[] dictPageBytes = readDictPage(colDValDesc, refPageReadStore).getBytes().toByteArray();        byte[] colDValPageBytes = readNextPage(colDValDesc, refPageReadStore).getBytes().toByteArray();                conf.setBoolean(ParquetOutputFormat.PAGE_WRITE_CHECKSUM_ENABLED, true);        conf.setBoolean(ParquetInputFormat.PAGE_VERIFY_CHECKSUM_ENABLED, true);        Path path = writeNestedWithNullsSampleParquetFile(conf, true, CompressionCodecName.SNAPPY);        try (ParquetFileReader reader = getParquetFileReader(path, conf, Collections.singletonList(colDValDesc))) {            PageReadStore pageReadStore = reader.readNextRowGroup();            DictionaryPage dictPage = readDictPage(colDValDesc, pageReadStore);            assertCrcSetAndCorrect(dictPage, snappy(dictPageBytes));            assertCorrectContent(dictPage.getBytes().toByteArray(), dictPageBytes);            DataPageV1 colDValPage = readNextPage(colDValDesc, pageReadStore);            assertCrcSetAndCorrect(colDValPage, snappy(colDValPageBytes));            assertCorrectContent(colDValPage.getBytes().toByteArray(), colDValPageBytes);        }    }}
0
private byte[] snappy(byte[] bytes) throws IOException
{    SnappyCompressor compressor = new SnappyCompressor();    compressor.reset();    compressor.setInput(bytes, 0, bytes.length);    compressor.finish();    byte[] buffer = new byte[bytes.length * 2];    int compressedSize = compressor.compress(buffer, 0, buffer.length);    return Arrays.copyOfRange(buffer, 0, compressedSize);}
0
private ParquetFileReader getParquetFileReader(Path path, Configuration conf, List<ColumnDescriptor> columns) throws IOException
{    ParquetMetadata footer = ParquetFileReader.readFooter(conf, path);    return new ParquetFileReader(conf, footer.getFileMetaData(), path, footer.getBlocks(), columns);}
0
private DictionaryPage readDictPage(ColumnDescriptor colDesc, PageReadStore pageReadStore)
{    return pageReadStore.getPageReader(colDesc).readDictionaryPage();}
0
private DataPageV1 readNextPage(ColumnDescriptor colDesc, PageReadStore pageReadStore)
{    return (DataPageV1) pageReadStore.getPageReader(colDesc).readPage();}
0
private void assertCorrectContent(byte[] pageBytes, byte[] referenceBytes)
{    assertArrayEquals("Read page content was different from expected page content", referenceBytes, pageBytes);}
0
private void assertCrcSetAndCorrect(Page page, byte[] referenceBytes)
{    assertTrue("Checksum was not set in page", page.getCrc().isPresent());    int crcFromPage = page.getCrc().getAsInt();    crc.reset();    crc.update(referenceBytes);    assertEquals("Checksum found in page did not match calculated reference checksum", crc.getValue(), (long) crcFromPage & 0xffffffffL);}
0
private void assertCrcNotSet(Page page)
{    assertFalse("Checksum was set in page", page.getCrc().isPresent());}
0
private void assertVerificationFailed(ParquetFileReader reader)
{    try {        reader.readNextRowGroup();        fail("Expected checksum verification exception to be thrown");    } catch (Exception e) {        assertTrue("Thrown exception is of incorrect type", e instanceof ParquetDecodingException);        assertTrue("Did not catch checksum verification ParquetDecodingException", e.getMessage().contains("CRC checksum verification failed"));    }}
0
private void test(int size, CompressionCodecName codec, boolean useOnHeapCompression, Decompression decomp)
{    ByteBuffer rawBuf = null;    ByteBuffer outBuf = null;    ByteBufferAllocator allocator = null;    try {        allocator = new DirectByteBufferAllocator();        final CodecFactory codecFactory = CodecFactory.createDirectCodecFactory(new Configuration(), allocator, pageSize);        rawBuf = allocator.allocate(size);        final byte[] rawArr = new byte[size];        outBuf = allocator.allocate(size * 2);        final Random r = new Random();        final byte[] random = new byte[1024];        int pos = 0;        while (pos < size) {            r.nextBytes(random);            rawBuf.put(random);            System.arraycopy(random, 0, rawArr, pos, random.length);            pos += random.length;        }        rawBuf.flip();        final DirectCodecFactory.BytesCompressor c = codecFactory.getCompressor(codec);        final CodecFactory.BytesDecompressor d = codecFactory.getDecompressor(codec);        final BytesInput compressed;        if (useOnHeapCompression) {            compressed = c.compress(BytesInput.from(rawArr));        } else {            compressed = c.compress(BytesInput.from(rawBuf));        }        switch(decomp) {            case OFF_HEAP:                {                    final ByteBuffer buf = compressed.toByteBuffer();                    final ByteBuffer b = allocator.allocate(buf.capacity());                    try {                        b.put(buf);                        b.flip();                        d.decompress(b, (int) compressed.size(), outBuf, size);                        for (int i = 0; i < size; i++) {                            Assert.assertTrue("Data didn't match at " + i, outBuf.get(i) == rawBuf.get(i));                        }                    } finally {                        allocator.release(b);                    }                    break;                }            case OFF_HEAP_BYTES_INPUT:                {                    final ByteBuffer buf = compressed.toByteBuffer();                    final ByteBuffer b = allocator.allocate(buf.limit());                    try {                        b.put(buf);                        b.flip();                        final BytesInput input = d.decompress(BytesInput.from(b), size);                        Assert.assertArrayEquals(String.format("While testing codec %s", codec), input.toByteArray(), rawArr);                    } finally {                        allocator.release(b);                    }                    break;                }            case ON_HEAP:                {                    final byte[] buf = compressed.toByteArray();                    final BytesInput input = d.decompress(BytesInput.from(buf), size);                    Assert.assertArrayEquals(input.toByteArray(), rawArr);                    break;                }        }    } catch (Exception e) {        final String msg = String.format("Failure while testing Codec: %s, OnHeapCompressionInput: %s, Decompression Mode: %s, Data Size: %d", codec.name(), useOnHeapCompression, decomp.name(), size);        System.out.println(msg);        throw new RuntimeException(msg, e);    } finally {        if (rawBuf != null) {            allocator.release(rawBuf);        }        if (outBuf != null) {            allocator.release(rawBuf);        }    }}
0
public void createDirectFactoryWithHeapAllocatorFails()
{    String errorMsg = "Test failed, creation of a direct codec factory should have failed when passed a non-direct allocator.";    try {        CodecFactory.createDirectCodecFactory(new Configuration(), new HeapByteBufferAllocator(), 0);        throw new RuntimeException(errorMsg);    } catch (IllegalStateException ex) {                Assert.assertTrue("Missing expected error message.", ex.getMessage().contains("A DirectCodecFactory requires a direct buffer allocator be provided."));    } catch (Exception ex) {        throw new RuntimeException(errorMsg + " Failed with the wrong error.");    }}
0
public void compressionCodecs() throws Exception
{    final int[] sizes = { 4 * 1024, 1 * 1024 * 1024 };    final boolean[] comp = { true, false };    Set<CompressionCodecName> codecsToSkip = new HashSet<>();        codecsToSkip.add(LZO);        codecsToSkip.add(LZ4);        codecsToSkip.add(ZSTD);    for (final int size : sizes) {        for (final boolean useOnHeapComp : comp) {            for (final Decompression decomp : Decompression.values()) {                for (final CompressionCodecName codec : CompressionCodecName.values()) {                    if (codecsToSkip.contains(codec)) {                        continue;                    }                    test(size, codec, useOnHeapComp, decomp);                }            }        }    }}
0
public void setUp()
{    blocks = new ArrayList<BlockMetaData>();    for (int i = 0; i < 10; i++) {        blocks.add(newBlock(i * 10, 10));    }    schema = MessageTypeParser.parseMessageType("message doc { required binary foo; }");    fileMetaData = new FileMetaData(schema, new HashMap<String, String>(), "parquet-mr");}
0
public void testThrowExceptionWhenMaxSplitSizeIsSmallerThanMinSplitSize() throws IOException
{    try {        generateSplitByMinMaxSize(50, 49);        fail("should throw exception when max split size is smaller than the min split size");    } catch (ParquetDecodingException e) {        assertEquals("maxSplitSize and minSplitSize should be positive and max should be greater or equal to the minSplitSize: maxSplitSize = 49; minSplitSize is 50", e.getMessage());    }}
0
public void testThrowExceptionWhenMaxSplitSizeIsNegative() throws IOException
{    try {        generateSplitByMinMaxSize(-100, -50);        fail("should throw exception when max split size is negative");    } catch (ParquetDecodingException e) {        assertEquals("maxSplitSize and minSplitSize should be positive and max should be greater or equal to the minSplitSize: maxSplitSize = -50; minSplitSize is -100", e.getMessage());    }}
0
public void testGetFilter() throws IOException
{    IntColumn intColumn = intColumn("foo");    FilterPredicate p = or(eq(intColumn, 7), eq(intColumn, 12));    Configuration conf = new Configuration();    ParquetInputFormat.setFilterPredicate(conf, p);    Filter read = ParquetInputFormat.getFilter(conf);    assertTrue(read instanceof FilterPredicateCompat);    assertEquals(p, ((FilterPredicateCompat) read).getFilterPredicate());    conf = new Configuration();    ParquetInputFormat.setFilterPredicate(conf, not(p));    read = ParquetInputFormat.getFilter(conf);    assertTrue(read instanceof FilterPredicateCompat);    assertEquals(and(notEq(intColumn, 7), notEq(intColumn, 12)), ((FilterPredicateCompat) read).getFilterPredicate());    assertEquals(FilterCompat.NOOP, ParquetInputFormat.getFilter(new Configuration()));}
0
public void testGenerateSplitsAlignedWithHDFSBlock() throws IOException
{    withHDFSBlockSize(50, 50);    List<ParquetInputSplit> splits = generateSplitByMinMaxSize(50, 50);    shouldSplitBlockSizeBe(splits, 5, 5);    shouldSplitLocationBe(splits, 0, 1);    shouldSplitLengthBe(splits, 50, 50);    splits = generateSplitByMinMaxSize(0, Long.MAX_VALUE);    shouldSplitBlockSizeBe(splits, 5, 5);    shouldSplitLocationBe(splits, 0, 1);    shouldSplitLengthBe(splits, 50, 50);}
0
public void testRowGroupNotAlignToHDFSBlock() throws IOException
{        withHDFSBlockSize(51, 51);    List<ParquetInputSplit> splits = generateSplitByMinMaxSize(50, 50);    shouldSplitBlockSizeBe(splits, 5, 5);        shouldSplitLocationBe(splits, 0, 0);    shouldSplitLengthBe(splits, 50, 50);        withHDFSBlockSize(49, 49);    splits = generateSplitByMinMaxSize(50, 50);    shouldSplitBlockSizeBe(splits, 5, 5);    shouldSplitLocationBe(splits, 0, 1);    shouldSplitLengthBe(splits, 50, 50);    /*    aaaa bbbbb c    for the 5th row group, the midpoint is 45, but the end of first hdfsBlock is 44, therefore a new split(b) will be created    for 9th group, the mid point is 85, the end of second block is 88, so it's considered mainly in the 2nd hdfs block, and therefore inserted as    a row group of split b     */    withHDFSBlockSize(44, 44, 44);    splits = generateSplitByMinMaxSize(40, 50);    shouldSplitBlockSizeBe(splits, 4, 5, 1);    shouldSplitLocationBe(splits, 0, 0, 2);    shouldSplitLengthBe(splits, 40, 50, 10);}
0
public void testGenerateSplitsNotAlignedWithHDFSBlock() throws IOException, InterruptedException
{    withHDFSBlockSize(50, 50);    List<ParquetInputSplit> splits = generateSplitByMinMaxSize(55, 56);    shouldSplitBlockSizeBe(splits, 6, 4);    shouldSplitLocationBe(splits, 0, 1);    shouldSplitLengthBe(splits, 60, 40);    withHDFSBlockSize(51, 51);    splits = generateSplitByMinMaxSize(55, 56);    shouldSplitBlockSizeBe(splits, 6, 4);        shouldSplitLocationBe(splits, 0, 1);    shouldSplitLengthBe(splits, 60, 40);    withHDFSBlockSize(49, 49, 49);    splits = generateSplitByMinMaxSize(55, 56);    shouldSplitBlockSizeBe(splits, 6, 4);    shouldSplitLocationBe(splits, 0, 1);    shouldSplitLengthBe(splits, 60, 40);}
0
public void testGenerateSplitsSmallerThanMaxSizeAndAlignToHDFS() throws Exception
{    withHDFSBlockSize(50, 50);    List<ParquetInputSplit> splits = generateSplitByMinMaxSize(18, 30);    shouldSplitBlockSizeBe(splits, 3, 2, 3, 2);    shouldSplitLocationBe(splits, 0, 0, 1, 1);    shouldSplitLengthBe(splits, 30, 20, 30, 20);    /*    aaabb cccdd         */    withHDFSBlockSize(51, 51);    splits = generateSplitByMinMaxSize(18, 30);    shouldSplitBlockSizeBe(splits, 3, 2, 3, 2);        shouldSplitLocationBe(splits, 0, 0, 0, 1);    shouldSplitLengthBe(splits, 30, 20, 30, 20);    /*    aaabb cccdd     */    withHDFSBlockSize(49, 49, 49);    splits = generateSplitByMinMaxSize(18, 30);    shouldSplitBlockSizeBe(splits, 3, 2, 3, 2);    shouldSplitLocationBe(splits, 0, 0, 1, 1);    shouldSplitLengthBe(splits, 30, 20, 30, 20);}
0
public void testGenerateSplitsCrossHDFSBlockBoundaryToSatisfyMinSize() throws Exception
{    withHDFSBlockSize(50, 50);    List<ParquetInputSplit> splits = generateSplitByMinMaxSize(25, 30);    shouldSplitBlockSizeBe(splits, 3, 3, 3, 1);    shouldSplitLocationBe(splits, 0, 0, 1, 1);    shouldSplitLengthBe(splits, 30, 30, 30, 10);}
0
public void testMultipleRowGroupsInABlockToAlignHDFSBlock() throws Exception
{    withHDFSBlockSize(50, 50);    List<ParquetInputSplit> splits = generateSplitByMinMaxSize(10, 18);    shouldSplitBlockSizeBe(splits, 2, 2, 1, 2, 2, 1);    shouldSplitLocationBe(splits, 0, 0, 0, 1, 1, 1);    shouldSplitLengthBe(splits, 20, 20, 10, 20, 20, 10);    /*    aabbc ddeef    notice the first byte of split d is in the first hdfs block:    when adding the 6th row group, although the first byte of it is in the first hdfs block    , but the mid point of the row group is in the second hdfs block, there for a new split(d) is created including that row group     */    withHDFSBlockSize(51, 51);    splits = generateSplitByMinMaxSize(10, 18);    shouldSplitBlockSizeBe(splits, 2, 2, 1, 2, 2, 1);        shouldSplitLocationBe(splits, 0, 0, 0, 0, 1, 1);    shouldSplitLengthBe(splits, 20, 20, 10, 20, 20, 10);    /*    aabbc ddeef    same as the case where block sizes are 50 50     */    withHDFSBlockSize(49, 49);    splits = generateSplitByMinMaxSize(10, 18);    shouldSplitBlockSizeBe(splits, 2, 2, 1, 2, 2, 1);    shouldSplitLocationBe(splits, 0, 0, 0, 1, 1, 1);    shouldSplitLengthBe(splits, 20, 20, 10, 20, 20, 10);}
0
public RecordFilter bind(Iterable<ColumnReader> readers)
{    return null;}
0
public void testOnlyOneKindOfFilterSupported() throws Exception
{    IntColumn foo = intColumn("foo");    FilterPredicate p = or(eq(foo, 10), eq(foo, 11));    Job job = new Job();    Configuration conf = job.getConfiguration();    ParquetInputFormat.setUnboundRecordFilter(job, DummyUnboundRecordFilter.class);    try {        ParquetInputFormat.setFilterPredicate(conf, p);        fail("this should throw");    } catch (IllegalArgumentException e) {        assertEquals("You cannot provide a FilterPredicate after providing an UnboundRecordFilter", e.getMessage());    }    job = new Job();    conf = job.getConfiguration();    ParquetInputFormat.setFilterPredicate(conf, p);    try {        ParquetInputFormat.setUnboundRecordFilter(job, DummyUnboundRecordFilter.class);        fail("this should throw");    } catch (IllegalArgumentException e) {        assertEquals("You cannot provide an UnboundRecordFilter after providing a FilterPredicate", e.getMessage());    }}
0
public static BlockMetaData makeBlockFromStats(IntStatistics stats, long valueCount)
{    BlockMetaData blockMetaData = new BlockMetaData();    ColumnChunkMetaData column = ColumnChunkMetaData.get(ColumnPath.get("foo"), PrimitiveTypeName.INT32, CompressionCodecName.GZIP, new HashSet<Encoding>(Arrays.asList(Encoding.PLAIN)), stats, 100l, 100l, valueCount, 100l, 100l);    blockMetaData.addColumn(column);    blockMetaData.setTotalByteSize(200l);    blockMetaData.setRowCount(valueCount);    return blockMetaData;}
0
public void testFooterCacheValueIsCurrent() throws IOException, InterruptedException
{    File tempFile = getTempFile();    FileSystem fs = FileSystem.getLocal(new Configuration());    ParquetInputFormat.FootersCacheValue cacheValue = getDummyCacheValue(tempFile, fs);    assertTrue(tempFile.setLastModified(tempFile.lastModified() + 5000));    assertFalse(cacheValue.isCurrent(new ParquetInputFormat.FileStatusWrapper(fs.getFileStatus(new Path(tempFile.getAbsolutePath())))));}
0
public void testFooterCacheValueIsNewer() throws IOException
{    File tempFile = getTempFile();    FileSystem fs = FileSystem.getLocal(new Configuration());    ParquetInputFormat.FootersCacheValue cacheValue = getDummyCacheValue(tempFile, fs);    assertTrue(cacheValue.isNewerThan(null));    assertFalse(cacheValue.isNewerThan(cacheValue));    assertTrue(tempFile.setLastModified(tempFile.lastModified() + 5000));    ParquetInputFormat.FootersCacheValue newerCacheValue = getDummyCacheValue(tempFile, fs);    assertTrue(newerCacheValue.isNewerThan(cacheValue));    assertFalse(cacheValue.isNewerThan(newerCacheValue));}
0
public void testDeprecatedConstructorOfParquetInputSplit() throws Exception
{    withHDFSBlockSize(50, 50);    List<ParquetInputSplit> splits = generateSplitByDeprecatedConstructor(50, 50);    shouldSplitBlockSizeBe(splits, 5, 5);    shouldOneSplitRowGroupOffsetBe(splits.get(0), 0, 10, 20, 30, 40);    shouldOneSplitRowGroupOffsetBe(splits.get(1), 50, 60, 70, 80, 90);    shouldSplitLengthBe(splits, 50, 50);    shouldSplitStartBe(splits, 0, 50);}
0
public void testGetFootersReturnsInPredictableOrder() throws IOException
{    File tempDir = Files.createTempDir();    tempDir.deleteOnExit();        int numFiles = 10;    String url = "";    for (int i = 0; i < numFiles; i++) {        File file = new File(tempDir, String.format("part-%05d.parquet", i));        createParquetFile(file);        if (i > 0) {            url += ",";        }        url += "file:" + file.getAbsolutePath();    }    Job job = new Job();    FileInputFormat.setInputPaths(job, url);    List<Footer> footers = new ParquetInputFormat<Object>().getFooters(job);    for (int i = 0; i < numFiles; i++) {        Footer footer = footers.get(i);        File file = new File(tempDir, String.format("part-%05d.parquet", i));        assertEquals("file:" + file.getAbsolutePath(), footer.getFile().toString());    }}
0
private void createParquetFile(File file) throws IOException
{    Path path = new Path(file.toURI());    Configuration configuration = new Configuration();    MessageType schema = MessageTypeParser.parseMessageType("message m { required group a {required binary b;}}");    String[] columnPath = { "a", "b" };    ColumnDescriptor c1 = schema.getColumnDescription(columnPath);    byte[] bytes1 = { 0, 1, 2, 3 };    byte[] bytes2 = { 2, 3, 4, 5 };    CompressionCodecName codec = CompressionCodecName.UNCOMPRESSED;    BinaryStatistics stats = new BinaryStatistics();    ParquetFileWriter w = new ParquetFileWriter(configuration, schema, path);    w.start();    w.startBlock(3);    w.startColumn(c1, 5, codec);    w.writeDataPage(2, 4, BytesInput.from(bytes1), stats, BIT_PACKED, BIT_PACKED, PLAIN);    w.writeDataPage(3, 4, BytesInput.from(bytes1), stats, BIT_PACKED, BIT_PACKED, PLAIN);    w.endColumn();    w.endBlock();    w.startBlock(4);    w.startColumn(c1, 7, codec);    w.writeDataPage(7, 4, BytesInput.from(bytes2), stats, BIT_PACKED, BIT_PACKED, PLAIN);    w.endColumn();    w.endBlock();    w.end(new HashMap<String, String>());}
0
private File getTempFile() throws IOException
{    File tempFile = File.createTempFile("footer_", ".txt");    tempFile.deleteOnExit();    return tempFile;}
0
private ParquetInputFormat.FootersCacheValue getDummyCacheValue(File file, FileSystem fs) throws IOException
{    Path path = new Path(file.getPath());    FileStatus status = fs.getFileStatus(path);    ParquetInputFormat.FileStatusWrapper statusWrapper = new ParquetInputFormat.FileStatusWrapper(status);    ParquetMetadata mockMetadata = mock(ParquetMetadata.class);    ParquetInputFormat.FootersCacheValue cacheValue = new ParquetInputFormat.FootersCacheValue(statusWrapper, new Footer(path, mockMetadata));    assertTrue(cacheValue.isCurrent(statusWrapper));    return cacheValue;}
0
private List<ParquetInputSplit> generateSplitByMinMaxSize(long min, long max) throws IOException
{    return ClientSideMetadataSplitStrategy.generateSplits(blocks, hdfsBlocks, fileStatus, schema.toString(), extramd, min, max);}
0
private List<ParquetInputSplit> generateSplitByDeprecatedConstructor(long min, long max) throws IOException
{    List<ParquetInputSplit> splits = new ArrayList<ParquetInputSplit>();    List<ClientSideMetadataSplitStrategy.SplitInfo> splitInfos = ClientSideMetadataSplitStrategy.generateSplitInfo(blocks, hdfsBlocks, min, max);    for (ClientSideMetadataSplitStrategy.SplitInfo splitInfo : splitInfos) {        ParquetInputSplit split = new ParquetInputSplit(fileStatus.getPath(), splitInfo.hdfsBlock.getOffset(), splitInfo.hdfsBlock.getLength(), splitInfo.hdfsBlock.getHosts(), splitInfo.rowGroups, schema.toString(), null, null, extramd);        splits.add(split);    }    return splits;}
0
private void shouldSplitStartBe(List<ParquetInputSplit> splits, long... offsets)
{    assertEquals(message(splits), offsets.length, splits.size());    for (int i = 0; i < offsets.length; i++) {        assertEquals(message(splits) + i, offsets[i], splits.get(i).getStart());    }}
0
private void shouldSplitBlockSizeBe(List<ParquetInputSplit> splits, int... sizes)
{    assertEquals(message(splits), sizes.length, splits.size());    for (int i = 0; i < sizes.length; i++) {        assertEquals(message(splits) + i, sizes[i], splits.get(i).getRowGroupOffsets().length);    }}
0
private void shouldSplitLocationBe(List<ParquetInputSplit> splits, int... locations) throws IOException
{    assertEquals(message(splits), locations.length, splits.size());    for (int i = 0; i < locations.length; i++) {        int loc = locations[i];        ParquetInputSplit split = splits.get(i);        assertEquals(message(splits) + i, "[foo" + loc + ".datanode, bar" + loc + ".datanode]", Arrays.toString(split.getLocations()));    }}
0
private void shouldOneSplitRowGroupOffsetBe(ParquetInputSplit split, int... rowGroupOffsets)
{    assertEquals(split.toString(), rowGroupOffsets.length, split.getRowGroupOffsets().length);    for (int i = 0; i < rowGroupOffsets.length; i++) {        assertEquals(split.toString(), rowGroupOffsets[i], split.getRowGroupOffsets()[i]);    }}
0
private String message(List<ParquetInputSplit> splits)
{    return String.valueOf(splits) + " " + Arrays.toString(hdfsBlocks) + "\n";}
0
private void shouldSplitLengthBe(List<ParquetInputSplit> splits, int... lengths)
{    assertEquals(message(splits), lengths.length, splits.size());    for (int i = 0; i < lengths.length; i++) {        assertEquals(message(splits) + i, lengths[i], splits.get(i).getLength());    }}
0
private void withHDFSBlockSize(long... blockSizes)
{    hdfsBlocks = new BlockLocation[blockSizes.length];    long offset = 0;    for (int i = 0; i < blockSizes.length; i++) {        long blockSize = blockSizes[i];        hdfsBlocks[i] = new BlockLocation(new String[0], new String[] { "foo" + i + ".datanode", "bar" + i + ".datanode" }, offset, blockSize);        offset += blockSize;    }    fileStatus = new FileStatus(offset, false, 2, 50, 0, new Path("hdfs://foo.namenode:1234/bar"));}
0
private BlockMetaData newBlock(long start, long compressedBlockSize)
{    BlockMetaData blockMetaData = new BlockMetaData();        long uncompressedSize = compressedBlockSize * 2;    ColumnChunkMetaData column = ColumnChunkMetaData.get(ColumnPath.get("foo"), PrimitiveTypeName.BINARY, CompressionCodecName.GZIP, new HashSet<Encoding>(Arrays.asList(Encoding.PLAIN)), new BinaryStatistics(), start, 0l, 0l, compressedBlockSize, uncompressedSize);    blockMetaData.addColumn(column);    blockMetaData.setTotalByteSize(uncompressedSize);    return blockMetaData;}
0
protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException
{        String line = value.toString();    for (int i = 0; i < line.length(); i += 1) {        Group group = GROUP_FACTORY.newGroup();        group.add(0, Binary.fromString(UUID.randomUUID().toString()));        group.add(1, Binary.fromString(line.substring(i, i + 1)));        context.write(null, group);    }}
0
public static void setBytesReadCounter(Counter bytesRead)
{    bytesReadCounter = bytesRead;}
0
protected void map(Void key, Group value, Context context) throws IOException, InterruptedException
{        setBytesReadCounter(ContextUtil.getCounter(context, "parquet", "bytesread"));}
0
public void testProjectionSize() throws Exception
{        Assume.assumeTrue(org.apache.hadoop.mapreduce.JobContext.class.isInterface());    File inputFile = temp.newFile();    FileOutputStream out = new FileOutputStream(inputFile);    out.write(FILE_CONTENT.getBytes("UTF-8"));    out.close();    File tempFolder = temp.newFolder();    tempFolder.delete();    Path tempPath = new Path(tempFolder.toURI());    File outputFolder = temp.newFile();    outputFolder.delete();    Configuration conf = new Configuration();        conf.set("parquet.read.schema", Types.buildMessage().required(BINARY).as(UTF8).named("char").named("FormatTestObject").toString());        conf.set("parquet.enable.summary-metadata", "false");    conf.set("parquet.example.schema", PARQUET_TYPE.toString());    {        Job writeJob = new Job(conf, "write");        writeJob.setInputFormatClass(TextInputFormat.class);        TextInputFormat.addInputPath(writeJob, new Path(inputFile.toString()));        writeJob.setOutputFormatClass(ExampleOutputFormat.class);        writeJob.setMapperClass(Writer.class);                writeJob.setNumReduceTasks(0);        ParquetOutputFormat.setBlockSize(writeJob, 10240);        ParquetOutputFormat.setPageSize(writeJob, 512);        ParquetOutputFormat.setDictionaryPageSize(writeJob, 1024);        ParquetOutputFormat.setEnableDictionary(writeJob, true);                ParquetOutputFormat.setMaxPaddingSize(writeJob, 1023);        ParquetOutputFormat.setOutputPath(writeJob, tempPath);        waitForJob(writeJob);    }    long bytesWritten = 0;    FileSystem fs = FileSystem.getLocal(conf);    for (FileStatus file : fs.listStatus(tempPath)) {        bytesWritten += file.getLen();    }    long bytesRead;    {        Job readJob = new Job(conf, "read");        readJob.setInputFormatClass(ExampleInputFormat.class);        TextInputFormat.addInputPath(readJob, tempPath);        readJob.setOutputFormatClass(TextOutputFormat.class);        readJob.setMapperClass(Reader.class);                readJob.setNumReduceTasks(0);        TextOutputFormat.setOutputPath(readJob, new Path(outputFolder.toString()));        waitForJob(readJob);        bytesRead = Reader.bytesReadCounter.getValue();    }    Assert.assertTrue("Should read less than 10% of the input file size", bytesRead < (bytesWritten / 10));}
0
private void waitForJob(Job job) throws Exception
{    job.submit();    while (!job.isComplete()) {        sleep(100);    }    if (!job.isSuccessful()) {        throw new RuntimeException("job failed " + job.getJobName());    }}
0
protected boolean isSplitable(JobContext context, Path filename)
{    return false;}
0
protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException
{        String line = value.toString();    for (int i = 0; i < line.length(); i += 1) {        Group group = GROUP_FACTORY.newGroup();        group.add(0, Binary.fromString(UUID.randomUUID().toString()));        group.add(1, Binary.fromString(line.substring(i, i + 1)));        context.write(null, group);    }}
0
protected void map(Void key, Group value, Context context) throws IOException, InterruptedException
{    context.write(null, new Text(value.getString("char", 0)));}
0
public void testBasicBehaviorWithPadding() throws Exception
{    HadoopOutputFile.getBlockFileSystems().add("file");    File inputFile = temp.newFile();    FileOutputStream out = new FileOutputStream(inputFile);    out.write(FILE_CONTENT.getBytes("UTF-8"));    out.close();    File tempFolder = temp.newFolder();    tempFolder.delete();    Path tempPath = new Path(tempFolder.toURI());    File outputFolder = temp.newFile();    outputFolder.delete();    Configuration conf = new Configuration();        conf.set("dfs.block.size", "1024");    conf.set("dfs.blocksize", "1024");    conf.set("dfs.blockSize", "1024");    conf.set("fs.local.block.size", "1024");        conf.set("fs.file.impl.disable.cache", "true");        conf.set("parquet.enable.summary-metadata", "false");    conf.set("parquet.example.schema", PARQUET_TYPE.toString());    {        Job writeJob = new Job(conf, "write");        writeJob.setInputFormatClass(TextInputFormat.class);        TextInputFormat.addInputPath(writeJob, new Path(inputFile.toString()));        writeJob.setOutputFormatClass(ParquetOutputFormat.class);        writeJob.setMapperClass(Writer.class);                writeJob.setNumReduceTasks(0);        ParquetOutputFormat.setWriteSupportClass(writeJob, GroupWriteSupport.class);        ParquetOutputFormat.setBlockSize(writeJob, 1024);        ParquetOutputFormat.setPageSize(writeJob, 512);        ParquetOutputFormat.setDictionaryPageSize(writeJob, 512);        ParquetOutputFormat.setEnableDictionary(writeJob, true);                ParquetOutputFormat.setMaxPaddingSize(writeJob, 1023);        ParquetOutputFormat.setOutputPath(writeJob, tempPath);        waitForJob(writeJob);    }        File parquetFile = getDataFile(tempFolder);    ParquetMetadata footer = ParquetFileReader.readFooter(conf, new Path(parquetFile.toString()), ParquetMetadataConverter.NO_FILTER);    for (BlockMetaData block : footer.getBlocks()) {        Assert.assertTrue("Block should start at a multiple of the block size", block.getStartingPos() % 1024 == 0);    }    {        Job readJob = new Job(conf, "read");        readJob.setInputFormatClass(NoSplits.class);        ParquetInputFormat.setReadSupportClass(readJob, GroupReadSupport.class);        TextInputFormat.addInputPath(readJob, tempPath);        readJob.setOutputFormatClass(TextOutputFormat.class);        readJob.setMapperClass(Reader.class);                readJob.setNumReduceTasks(0);        TextOutputFormat.setOutputPath(readJob, new Path(outputFolder.toString()));        waitForJob(readJob);    }    File dataFile = getDataFile(outputFolder);    Assert.assertNotNull("Should find a data file", dataFile);    StringBuilder contentBuilder = new StringBuilder();    for (String line : Files.readAllLines(dataFile.toPath(), StandardCharsets.UTF_8)) {        contentBuilder.append(line);    }    String reconstructed = contentBuilder.toString();    Assert.assertEquals("Should match written file content", FILE_CONTENT, reconstructed);    HadoopOutputFile.getBlockFileSystems().remove("file");}
0
private void waitForJob(Job job) throws Exception
{    job.submit();    while (!job.isComplete()) {        sleep(100);    }    if (!job.isSuccessful()) {        throw new RuntimeException("job failed " + job.getJobName());    }}
0
private File getDataFile(File location)
{    File[] files = location.listFiles();    File dataFile = null;    if (files != null) {        for (File file : files) {            if (file.getName().startsWith("part-")) {                dataFile = file;                break;            }        }    }    return dataFile;}
0
public boolean isCurrent(String key)
{    return current;}
0
public void setCurrent(boolean current)
{    this.current = current;}
0
public boolean isNewerThan(SimpleValue otherValue)
{    return newerThan;}
0
public void testMaxSize()
{    LruCache<String, SimpleValue> cache = new LruCache<String, SimpleValue>(1);    String oldKey = DEFAULT_KEY;    String newKey = oldKey + "_new";    SimpleValue oldValue = new SimpleValue(true, true);    cache.put(oldKey, oldValue);    assertEquals(oldValue, cache.getCurrentValue(oldKey));    assertEquals(1, cache.size());    SimpleValue newValue = new SimpleValue(true, true);    cache.put(newKey, newValue);    assertNull(cache.getCurrentValue(oldKey));    assertEquals(newValue, cache.getCurrentValue(newKey));    assertEquals(1, cache.size());}
0
public void testOlderValueIsIgnored()
{    LruCache<String, SimpleValue> cache = new LruCache<String, SimpleValue>(1);    SimpleValue currentValue = new SimpleValue(true, true);    SimpleValue notAsCurrentValue = new SimpleValue(true, false);    cache.put(DEFAULT_KEY, currentValue);    cache.put(DEFAULT_KEY, notAsCurrentValue);    assertEquals("The existing value in the cache was overwritten", currentValue, cache.getCurrentValue(DEFAULT_KEY));}
0
public void testOutdatedValueIsIgnored()
{    LruCache<String, SimpleValue> cache = new LruCache<String, SimpleValue>(1);    SimpleValue outdatedValue = new SimpleValue(false, true);    cache.put(DEFAULT_KEY, outdatedValue);    assertEquals(0, cache.size());    assertNull(cache.getCurrentValue(DEFAULT_KEY));}
0
public void testCurrentValueOverwritesExisting()
{    LruCache<String, SimpleValue> cache = new LruCache<String, SimpleValue>(1);    SimpleValue currentValue = new SimpleValue(true, true);    SimpleValue notAsCurrentValue = new SimpleValue(true, false);    cache.put(DEFAULT_KEY, notAsCurrentValue);    assertEquals(1, cache.size());    cache.put(DEFAULT_KEY, currentValue);    assertEquals(1, cache.size());    assertEquals("The existing value in the cache was NOT overwritten", currentValue, cache.getCurrentValue(DEFAULT_KEY));}
0
public void testGetOutdatedValueReturnsNull()
{    LruCache<String, SimpleValue> cache = new LruCache<String, SimpleValue>(1);    SimpleValue value = new SimpleValue(true, true);    cache.put(DEFAULT_KEY, value);    assertEquals(1, cache.size());    assertEquals(value, cache.getCurrentValue(DEFAULT_KEY));    value.setCurrent(false);    assertNull("The value should not be current anymore", cache.getCurrentValue(DEFAULT_KEY));    assertEquals(0, cache.size());}
0
public void testRemove()
{    LruCache<String, SimpleValue> cache = new LruCache<String, SimpleValue>(1);    SimpleValue value = new SimpleValue(true, true);    cache.put(DEFAULT_KEY, value);    assertEquals(1, cache.size());    assertEquals(value, cache.getCurrentValue(DEFAULT_KEY));        assertEquals(value, cache.remove(DEFAULT_KEY));    assertNull(cache.getCurrentValue(DEFAULT_KEY));    assertEquals(0, cache.size());}
0
public void testClear()
{    LruCache<String, SimpleValue> cache = new LruCache<String, SimpleValue>(2);    String key1 = DEFAULT_KEY + 1;    String key2 = DEFAULT_KEY + 2;    SimpleValue value = new SimpleValue(true, true);    cache.put(key1, value);    cache.put(key2, value);    assertEquals(value, cache.getCurrentValue(key1));    assertEquals(value, cache.getCurrentValue(key2));    assertEquals(2, cache.size());    cache.clear();    assertNull(cache.getCurrentValue(key1));    assertNull(cache.getCurrentValue(key2));    assertEquals(0, cache.size());}
0
public void setUp() throws Exception
{    parquetOutputFormat = new ParquetOutputFormat(new GroupWriteSupport());    GroupWriteSupport.setSchema(MessageTypeParser.parseMessageType(writeSchema), conf);    expectedPoolSize = Math.round((double) ManagementFactory.getMemoryMXBean().getHeapMemoryUsage().getMax() * MemoryManager.DEFAULT_MEMORY_POOL_RATIO);    long rowGroupSize = expectedPoolSize / 2;    conf.setLong(ParquetOutputFormat.BLOCK_SIZE, rowGroupSize);        createWriter(0).close(null);}
0
public void testMemoryManagerUpperLimit()
{                long poolSize = ParquetOutputFormat.getMemoryManager().getTotalMemoryPool();    Assert.assertTrue("Pool size should be within 10% of the expected value" + " (expected = " + expectedPoolSize + " actual = " + poolSize + ")", Math.abs(expectedPoolSize - poolSize) < (long) (expectedPoolSize * 0.10));}
0
public void testMemoryManager() throws Exception
{    long poolSize = ParquetOutputFormat.getMemoryManager().getTotalMemoryPool();    long rowGroupSize = poolSize / 2;    conf.setLong(ParquetOutputFormat.BLOCK_SIZE, rowGroupSize);    Assert.assertTrue("Pool should hold 2 full row groups", (2 * rowGroupSize) <= poolSize);    Assert.assertTrue("Pool should not hold 3 full row groups", poolSize < (3 * rowGroupSize));    Assert.assertEquals("Allocations should start out at 0", 0, getTotalAllocation());    RecordWriter writer1 = createWriter(1);    Assert.assertTrue("Allocations should never exceed pool size", getTotalAllocation() <= poolSize);    Assert.assertEquals("First writer should be limited by row group size", rowGroupSize, getTotalAllocation());    RecordWriter writer2 = createWriter(2);    Assert.assertTrue("Allocations should never exceed pool size", getTotalAllocation() <= poolSize);    Assert.assertEquals("Second writer should be limited by row group size", 2 * rowGroupSize, getTotalAllocation());    RecordWriter writer3 = createWriter(3);    Assert.assertTrue("Allocations should never exceed pool size", getTotalAllocation() <= poolSize);    writer1.close(null);    Assert.assertTrue("Allocations should never exceed pool size", getTotalAllocation() <= poolSize);    Assert.assertEquals("Allocations should be increased to the row group size", 2 * rowGroupSize, getTotalAllocation());    writer2.close(null);    Assert.assertTrue("Allocations should never exceed pool size", getTotalAllocation() <= poolSize);    Assert.assertEquals("Allocations should be increased to the row group size", rowGroupSize, getTotalAllocation());    writer3.close(null);    Assert.assertEquals("Allocations should be increased to the row group size", 0, getTotalAllocation());}
0
public void testReallocationCallback() throws Exception
{        long poolSize = ParquetOutputFormat.getMemoryManager().getTotalMemoryPool();    long rowGroupSize = poolSize / 2;    conf.setLong(ParquetOutputFormat.BLOCK_SIZE, rowGroupSize);    Assert.assertTrue("Pool should hold 2 full row groups", (2 * rowGroupSize) <= poolSize);    Assert.assertTrue("Pool should not hold 3 full row groups", poolSize < (3 * rowGroupSize));    Runnable callback = () -> counter++;        ParquetOutputFormat.getMemoryManager().registerScaleCallBack("increment-test-counter", callback);    try {        ParquetOutputFormat.getMemoryManager().registerScaleCallBack("increment-test-counter", callback);        Assert.fail("Duplicated registering callback should throw duplicates exception.");    } catch (IllegalArgumentException e) {        }        RecordWriter writer1 = createWriter(1);    RecordWriter writer2 = createWriter(2);    RecordWriter writer3 = createWriter(3);    writer1.close(null);    writer2.close(null);    writer3.close(null);        Assert.assertEquals("Allocations should be adjusted once", 1, counter);    Assert.assertEquals("Should not allow duplicate callbacks", 1, ParquetOutputFormat.getMemoryManager().getScaleCallBacks().size());}
0
private RecordWriter createWriter(int index) throws Exception
{    File file = temp.newFile(String.valueOf(index) + ".parquet");    if (!file.delete()) {        throw new RuntimeException("Could not delete file: " + file);    }    RecordWriter writer = parquetOutputFormat.getRecordWriter(conf, new Path(file.toString()), CompressionCodecName.UNCOMPRESSED);    return writer;}
0
private long getTotalAllocation()
{    Set<InternalParquetRecordWriter> writers = ParquetOutputFormat.getMemoryManager().getWriterList().keySet();    long total = 0;    for (InternalParquetRecordWriter writer : writers) {        total += writer.getRowGroupSizeThreshold();    }    return total;}
0
private static void writeFile(File out, Configuration conf, boolean useSchema2) throws IOException
{    if (!useSchema2) {        GroupWriteSupport.setSchema(schema, conf);    } else {        GroupWriteSupport.setSchema(schema2, conf);    }    SimpleGroupFactory f = new SimpleGroupFactory(schema);    Map<String, String> extraMetaData = new HashMap<String, String>();    extraMetaData.put("schema_num", useSchema2 ? "2" : "1");    ParquetWriter<Group> writer = ExampleParquetWriter.builder(new Path(out.getAbsolutePath())).withConf(conf).withExtraMetaData(extraMetaData).build();    for (int i = 0; i < 1000; i++) {        Group g = f.newGroup().append("binary_field", "test" + i).append("int32_field", i).append("int64_field", (long) i).append("boolean_field", i % 2 == 0).append("float_field", (float) i).append("double_field", (double) i).append("flba_field", "foo");        if (!useSchema2) {            g = g.append("int96_field", Binary.fromConstantByteArray(new byte[12]));        }        writer.write(g);    }    writer.close();}
0
private WrittenFileInfo writeFiles(boolean mixedSchemas) throws Exception
{    WrittenFileInfo info = new WrittenFileInfo();    Configuration conf = new Configuration();    info.conf = conf;    File root1 = new File(temp.getRoot(), "out1");    File root2 = new File(temp.getRoot(), "out2");    Path rootPath1 = new Path(root1.getAbsolutePath());    Path rootPath2 = new Path(root2.getAbsolutePath());    for (int i = 0; i < 10; i++) {        writeFile(new File(root1, i + ".parquet"), conf, true);    }    List<Footer> footers = ParquetFileReader.readFooters(conf, rootPath1.getFileSystem(conf).getFileStatus(rootPath1), false);    ParquetFileWriter.writeMetadataFile(conf, rootPath1, footers, JobSummaryLevel.ALL);    for (int i = 0; i < 7; i++) {        writeFile(new File(root2, i + ".parquet"), conf, !mixedSchemas);    }    footers = ParquetFileReader.readFooters(conf, rootPath2.getFileSystem(conf).getFileStatus(rootPath2), false);    ParquetFileWriter.writeMetadataFile(conf, rootPath2, footers, JobSummaryLevel.ALL);    info.commonMetaPath1 = new Path(new File(root1, ParquetFileWriter.PARQUET_COMMON_METADATA_FILE).getAbsolutePath());    info.commonMetaPath2 = new Path(new File(root2, ParquetFileWriter.PARQUET_COMMON_METADATA_FILE).getAbsolutePath());    info.metaPath1 = new Path(new File(root1, ParquetFileWriter.PARQUET_METADATA_FILE).getAbsolutePath());    info.metaPath2 = new Path(new File(root2, ParquetFileWriter.PARQUET_METADATA_FILE).getAbsolutePath());    return info;}
0
public void testMergeMetadataFiles() throws Exception
{    WrittenFileInfo info = writeFiles(false);    ParquetMetadata commonMeta1 = ParquetFileReader.readFooter(info.conf, info.commonMetaPath1, ParquetMetadataConverter.NO_FILTER);    ParquetMetadata commonMeta2 = ParquetFileReader.readFooter(info.conf, info.commonMetaPath2, ParquetMetadataConverter.NO_FILTER);    ParquetMetadata meta1 = ParquetFileReader.readFooter(info.conf, info.metaPath1, ParquetMetadataConverter.NO_FILTER);    ParquetMetadata meta2 = ParquetFileReader.readFooter(info.conf, info.metaPath2, ParquetMetadataConverter.NO_FILTER);    assertTrue(commonMeta1.getBlocks().isEmpty());    assertTrue(commonMeta2.getBlocks().isEmpty());    assertEquals(commonMeta1.getFileMetaData().getSchema(), commonMeta2.getFileMetaData().getSchema());    assertFalse(meta1.getBlocks().isEmpty());    assertFalse(meta2.getBlocks().isEmpty());    assertEquals(meta1.getFileMetaData().getSchema(), meta2.getFileMetaData().getSchema());    assertEquals(commonMeta1.getFileMetaData().getKeyValueMetaData(), commonMeta2.getFileMetaData().getKeyValueMetaData());    assertEquals(meta1.getFileMetaData().getKeyValueMetaData(), meta2.getFileMetaData().getKeyValueMetaData());        Path mergedOut = new Path(new File(temp.getRoot(), "merged_meta").getAbsolutePath());    Path mergedCommonOut = new Path(new File(temp.getRoot(), "merged_common_meta").getAbsolutePath());    ParquetFileWriter.writeMergedMetadataFile(Arrays.asList(info.metaPath1, info.metaPath2), mergedOut, info.conf);    ParquetFileWriter.writeMergedMetadataFile(Arrays.asList(info.commonMetaPath1, info.commonMetaPath2), mergedCommonOut, info.conf);    ParquetMetadata mergedMeta = ParquetFileReader.readFooter(info.conf, mergedOut, ParquetMetadataConverter.NO_FILTER);    ParquetMetadata mergedCommonMeta = ParquetFileReader.readFooter(info.conf, mergedCommonOut, ParquetMetadataConverter.NO_FILTER);        assertEquals(meta1.getBlocks().size() + meta2.getBlocks().size(), mergedMeta.getBlocks().size());    assertTrue(mergedCommonMeta.getBlocks().isEmpty());    assertEquals(meta1.getFileMetaData().getSchema(), mergedMeta.getFileMetaData().getSchema());    assertEquals(commonMeta1.getFileMetaData().getSchema(), mergedCommonMeta.getFileMetaData().getSchema());    assertEquals(meta1.getFileMetaData().getKeyValueMetaData(), mergedMeta.getFileMetaData().getKeyValueMetaData());    assertEquals(commonMeta1.getFileMetaData().getKeyValueMetaData(), mergedCommonMeta.getFileMetaData().getKeyValueMetaData());}
0
public void testThrowsWhenIncompatible() throws Exception
{    WrittenFileInfo info = writeFiles(true);    Path mergedOut = new Path(new File(temp.getRoot(), "merged_meta").getAbsolutePath());    Path mergedCommonOut = new Path(new File(temp.getRoot(), "merged_common_meta").getAbsolutePath());    try {        ParquetFileWriter.writeMergedMetadataFile(Arrays.asList(info.metaPath1, info.metaPath2), mergedOut, info.conf);        fail("this should throw");    } catch (RuntimeException e) {        boolean eq1 = e.getMessage().equals("could not merge metadata: key schema_num has conflicting values: [2, 1]");        boolean eq2 = e.getMessage().equals("could not merge metadata: key schema_num has conflicting values: [1, 2]");        assertEquals(eq1 || eq2, true);    }    try {        ParquetFileWriter.writeMergedMetadataFile(Arrays.asList(info.commonMetaPath1, info.commonMetaPath2), mergedCommonOut, info.conf);        fail("this should throw");    } catch (RuntimeException e) {        boolean eq1 = e.getMessage().equals("could not merge metadata: key schema_num has conflicting values: [2, 1]");        boolean eq2 = e.getMessage().equals("could not merge metadata: key schema_num has conflicting values: [1, 2]");        assertEquals(eq1 || eq2, true);    }}
0
private String getString(int minSize, int maxSize)
{    int size = random.nextInt(maxSize - minSize) + minSize;    StringBuilder builder = new StringBuilder(size);    for (int i = 0; i < size; ++i) {        builder.append(ALPHABET.charAt(random.nextInt(ALPHABET.length())));    }    return builder.toString();}
0
public Group get()
{    Group group = factory.newGroup();    group.add("id", random.nextInt());    group.add("name", getString(NAME_MIN_SIZE, NAME_MAX_SIZE));    Group phoneNumbers = group.addGroup("phone_numbers");    for (int i = 0, n = random.nextInt(PHONE_NUMBERS_MAX_SIZE); i < n; ++i) {        Group phoneNumber = phoneNumbers.addGroup(0);        phoneNumber.add(0, random.nextLong() % (MAX_PHONE_NUMBER - MIN_PHONE_NUMBER) + MIN_PHONE_NUMBER);    }    if (random.nextDouble() >= COMMENT_NULL_RATIO) {        group.add("comment", getString(0, COMMENT_MAX_SIZE));    }    return group;}
0
public static void createTmpDir()
{    tmpDir = new Path(Files.createTempDir().getAbsolutePath().toString());}
0
public static void deleteTmpDir() throws IOException
{    tmpDir.getFileSystem(new Configuration()).delete(tmpDir, true);}
0
private Path writeFile(Iterable<Group> data) throws IOException
{    Path file = new Path(tmpDir, "testMultipleReadWrite_" + UUID.randomUUID() + ".parquet");    try (ParquetWriter<Group> writer = ExampleParquetWriter.builder(file).config(GroupWriteSupport.PARQUET_EXAMPLE_SCHEMA, SCHEMA.toString()).build()) {        for (Group group : data) {            writer.write(group);        }    }    return file;}
0
private void validateFile(Path file, List<Group> data) throws IOException
{    try (ParquetReader<Group> reader = ParquetReader.builder(new GroupReadSupport(), file).build()) {        for (Group group : data) {            assertEquals(group.toString(), reader.read().toString());        }    }}
0
private void validateFile(Path file, Filter filter, Stream<Group> data) throws IOException
{    try (ParquetReader<Group> reader = ParquetReader.builder(new GroupReadSupport(), file).withFilter(filter).build()) {        for (Iterator<Group> it = data.iterator(); it.hasNext(); ) {            assertEquals(it.next().toString(), reader.read().toString());        }    }}
0
private void validateFileWithIdFilter(Path file, List<Group> data) throws IOException
{    validateFile(file, FilterCompat.get(eq(intColumn("id"), 0)), data.stream().filter(group -> group.getInteger("id", 0) == 0));}
0
private void validateFileWithCommentFilter(Path file, List<Group> data) throws IOException
{    validateFile(file, FilterCompat.get(eq(binaryColumn("comment"), null)), data.stream().filter(group -> group.getFieldRepetitionCount("comment") == 0));}
0
private void validateFileWithComplexFilter(Path file, List<Group> data) throws IOException
{    Binary binaryValueB = fromString("b");    Filter filter = FilterCompat.get(and(gtEq(intColumn("id"), 0), and(lt(binaryColumn("name"), binaryValueB), notEq(binaryColumn("comment"), null))));    Predicate<Group> predicate = group -> group.getInteger("id", 0) >= 0 && BINARY_COMPARATOR.compare(group.getBinary("name", 0), binaryValueB) < 0 && group.getFieldRepetitionCount("comment") > 0;    validateFile(file, filter, data.stream().filter(predicate));}
0
public void testWriteRead() throws Throwable
{        List<List<Group>> data = new ArrayList<>();    for (int i = 0; i < 10; ++i) {        data.add(Stream.generate(new DataGenerator(i)).limit(10000 - i * 1000).collect(Collectors.toList()));    }        List<Future<Path>> futureFiles = new ArrayList<>();    ExecutorService exec = Executors.newFixedThreadPool(6);    for (List<Group> d : data) {        futureFiles.add(exec.submit(() -> {            Path file = writeFile(d);            validateFile(file, d);            return file;        }));    }    List<Path> files = new ArrayList<>();    for (Future<Path> future : futureFiles) {        try {            files.add(future.get());        } catch (ExecutionException e) {            throw e.getCause();        }    }        List<Future<?>> futures = new ArrayList<>();    for (int i = 0; i < 10; ++i) {        Path file = files.get(i);        List<Group> d = data.get(i);        futures.add(exec.submit(() -> {            validateFileWithIdFilter(file, d);            return null;        }));        futures.add(exec.submit(() -> {            validateFileWithCommentFilter(file, d);            return null;        }));        futures.add(exec.submit(() -> {            validateFileWithComplexFilter(file, d);            return null;        }));    }    for (Future<?> future : futures) {        try {            future.get();        } catch (ExecutionException e) {            throw e.getCause();        }    }}
0
public void testWriteMode() throws Exception
{    File testFile = temp.newFile();    MessageType schema = MessageTypeParser.parseMessageType("message m { required group a {required binary b;} required group " + "c { required int64 d; }}");    Configuration conf = new Configuration();    ParquetFileWriter writer = null;    boolean exceptionThrown = false;    Path path = new Path(testFile.toURI());    try {        writer = new ParquetFileWriter(conf, schema, path, ParquetFileWriter.Mode.CREATE);    } catch (IOException ioe1) {        exceptionThrown = true;    }    assertTrue(exceptionThrown);    exceptionThrown = false;    try {        writer = new ParquetFileWriter(conf, schema, path, OVERWRITE);    } catch (IOException ioe2) {        exceptionThrown = true;    }    assertTrue(!exceptionThrown);    testFile.delete();}
0
public void testWriteRead() throws Exception
{    File testFile = temp.newFile();    testFile.delete();    Path path = new Path(testFile.toURI());    Configuration configuration = new Configuration();    ParquetFileWriter w = new ParquetFileWriter(configuration, SCHEMA, path);    w.start();    w.startBlock(3);    w.startColumn(C1, 5, CODEC);    long c1Starts = w.getPos();    w.writeDataPage(2, 4, BytesInput.from(BYTES1), EMPTY_STATS, BIT_PACKED, BIT_PACKED, PLAIN);    w.writeDataPage(3, 4, BytesInput.from(BYTES1), EMPTY_STATS, BIT_PACKED, BIT_PACKED, PLAIN);    w.endColumn();    long c1Ends = w.getPos();    w.startColumn(C2, 6, CODEC);    long c2Starts = w.getPos();    w.writeDataPage(2, 4, BytesInput.from(BYTES2), EMPTY_STATS, BIT_PACKED, BIT_PACKED, PLAIN);    w.writeDataPage(3, 4, BytesInput.from(BYTES2), EMPTY_STATS, BIT_PACKED, BIT_PACKED, PLAIN);    w.writeDataPage(1, 4, BytesInput.from(BYTES2), EMPTY_STATS, BIT_PACKED, BIT_PACKED, PLAIN);    w.endColumn();    long c2Ends = w.getPos();    w.endBlock();    w.startBlock(4);    w.startColumn(C1, 7, CODEC);    w.writeDataPage(7, 4, BytesInput.from(BYTES3), EMPTY_STATS, BIT_PACKED, BIT_PACKED, PLAIN);    w.endColumn();    w.startColumn(C2, 8, CODEC);    w.writeDataPage(8, 4, BytesInput.from(BYTES4), EMPTY_STATS, BIT_PACKED, BIT_PACKED, PLAIN);    w.endColumn();    w.endBlock();    w.end(new HashMap<String, String>());    ParquetMetadata readFooter = ParquetFileReader.readFooter(configuration, path);    assertEquals("footer: " + readFooter, 2, readFooter.getBlocks().size());    assertEquals(c1Ends - c1Starts, readFooter.getBlocks().get(0).getColumns().get(0).getTotalSize());    assertEquals(c2Ends - c2Starts, readFooter.getBlocks().get(0).getColumns().get(1).getTotalSize());    assertEquals(c2Ends - c1Starts, readFooter.getBlocks().get(0).getTotalByteSize());    HashSet<Encoding> expectedEncoding = new HashSet<Encoding>();    expectedEncoding.add(PLAIN);    expectedEncoding.add(BIT_PACKED);    assertEquals(expectedEncoding, readFooter.getBlocks().get(0).getColumns().get(0).getEncodings());    {                ParquetFileReader r = new ParquetFileReader(configuration, readFooter.getFileMetaData(), path, Arrays.asList(readFooter.getBlocks().get(0)), Arrays.asList(SCHEMA.getColumnDescription(PATH1)));        PageReadStore pages = r.readNextRowGroup();        assertEquals(3, pages.getRowCount());        validateContains(SCHEMA, pages, PATH1, 2, BytesInput.from(BYTES1));        validateContains(SCHEMA, pages, PATH1, 3, BytesInput.from(BYTES1));        assertNull(r.readNextRowGroup());    }    {                ParquetFileReader r = new ParquetFileReader(configuration, readFooter.getFileMetaData(), path, readFooter.getBlocks(), Arrays.asList(SCHEMA.getColumnDescription(PATH1), SCHEMA.getColumnDescription(PATH2)));        PageReadStore pages = r.readNextRowGroup();        assertEquals(3, pages.getRowCount());        validateContains(SCHEMA, pages, PATH1, 2, BytesInput.from(BYTES1));        validateContains(SCHEMA, pages, PATH1, 3, BytesInput.from(BYTES1));        validateContains(SCHEMA, pages, PATH2, 2, BytesInput.from(BYTES2));        validateContains(SCHEMA, pages, PATH2, 3, BytesInput.from(BYTES2));        validateContains(SCHEMA, pages, PATH2, 1, BytesInput.from(BYTES2));        pages = r.readNextRowGroup();        assertEquals(4, pages.getRowCount());        validateContains(SCHEMA, pages, PATH1, 7, BytesInput.from(BYTES3));        validateContains(SCHEMA, pages, PATH2, 8, BytesInput.from(BYTES4));        assertNull(r.readNextRowGroup());    }    PrintFooter.main(new String[] { path.toString() });}
0
public void testAlignmentWithPadding() throws Exception
{    File testFile = temp.newFile();    Path path = new Path(testFile.toURI());    Configuration conf = new Configuration();        conf.setBoolean(ParquetOutputFormat.PAGE_WRITE_CHECKSUM_ENABLED, false);        ParquetFileWriter w = new ParquetFileWriter(conf, SCHEMA, path, 120, 60);    w.start();    w.startBlock(3);    w.startColumn(C1, 5, CODEC);    long c1Starts = w.getPos();    w.writeDataPage(2, 4, BytesInput.from(BYTES1), EMPTY_STATS, BIT_PACKED, BIT_PACKED, PLAIN);    w.writeDataPage(3, 4, BytesInput.from(BYTES1), EMPTY_STATS, BIT_PACKED, BIT_PACKED, PLAIN);    w.endColumn();    long c1Ends = w.getPos();    w.startColumn(C2, 6, CODEC);    long c2Starts = w.getPos();    w.writeDataPage(2, 4, BytesInput.from(BYTES2), EMPTY_STATS, BIT_PACKED, BIT_PACKED, PLAIN);    w.writeDataPage(3, 4, BytesInput.from(BYTES2), EMPTY_STATS, BIT_PACKED, BIT_PACKED, PLAIN);    w.writeDataPage(1, 4, BytesInput.from(BYTES2), EMPTY_STATS, BIT_PACKED, BIT_PACKED, PLAIN);    w.endColumn();    long c2Ends = w.getPos();    w.endBlock();        long firstRowGroupEnds = w.getPos();    w.startBlock(4);    w.startColumn(C1, 7, CODEC);    w.writeDataPage(7, 4, BytesInput.from(BYTES3), EMPTY_STATS, BIT_PACKED, BIT_PACKED, PLAIN);    w.endColumn();    w.startColumn(C2, 8, CODEC);    w.writeDataPage(8, 4, BytesInput.from(BYTES4), EMPTY_STATS, BIT_PACKED, BIT_PACKED, PLAIN);    w.endColumn();    w.endBlock();    long secondRowGroupEnds = w.getPos();    w.end(new HashMap<String, String>());    FileSystem fs = path.getFileSystem(conf);    long fileLen = fs.getFileStatus(path).getLen();    FSDataInputStream data = fs.open(path);        data.seek(fileLen - 8);    long footerLen = BytesUtils.readIntLittleEndian(data);    long startFooter = fileLen - footerLen - 8;    assertEquals("Footer should start after second row group without padding", secondRowGroupEnds, startFooter);    ParquetMetadata readFooter = ParquetFileReader.readFooter(conf, path);    assertEquals("footer: " + readFooter, 2, readFooter.getBlocks().size());    assertEquals(c1Ends - c1Starts, readFooter.getBlocks().get(0).getColumns().get(0).getTotalSize());    assertEquals(c2Ends - c2Starts, readFooter.getBlocks().get(0).getColumns().get(1).getTotalSize());    assertEquals(c2Ends - c1Starts, readFooter.getBlocks().get(0).getTotalByteSize());    HashSet<Encoding> expectedEncoding = new HashSet<Encoding>();    expectedEncoding.add(PLAIN);    expectedEncoding.add(BIT_PACKED);    assertEquals(expectedEncoding, readFooter.getBlocks().get(0).getColumns().get(0).getEncodings());        assertEquals("First row group should start after magic", 4, readFooter.getBlocks().get(0).getStartingPos());    assertTrue("First row group should end before the block size (120)", firstRowGroupEnds < 120);    assertEquals("Second row group should start at the block size", 120, readFooter.getBlocks().get(1).getStartingPos());    {                ParquetFileReader r = new ParquetFileReader(conf, readFooter.getFileMetaData(), path, Arrays.asList(readFooter.getBlocks().get(0)), Arrays.asList(SCHEMA.getColumnDescription(PATH1)));        PageReadStore pages = r.readNextRowGroup();        assertEquals(3, pages.getRowCount());        validateContains(SCHEMA, pages, PATH1, 2, BytesInput.from(BYTES1));        validateContains(SCHEMA, pages, PATH1, 3, BytesInput.from(BYTES1));        assertNull(r.readNextRowGroup());    }    {                ParquetFileReader r = new ParquetFileReader(conf, readFooter.getFileMetaData(), path, readFooter.getBlocks(), Arrays.asList(SCHEMA.getColumnDescription(PATH1), SCHEMA.getColumnDescription(PATH2)));        PageReadStore pages = r.readNextRowGroup();        assertEquals(3, pages.getRowCount());        validateContains(SCHEMA, pages, PATH1, 2, BytesInput.from(BYTES1));        validateContains(SCHEMA, pages, PATH1, 3, BytesInput.from(BYTES1));        validateContains(SCHEMA, pages, PATH2, 2, BytesInput.from(BYTES2));        validateContains(SCHEMA, pages, PATH2, 3, BytesInput.from(BYTES2));        validateContains(SCHEMA, pages, PATH2, 1, BytesInput.from(BYTES2));        pages = r.readNextRowGroup();        assertEquals(4, pages.getRowCount());        validateContains(SCHEMA, pages, PATH1, 7, BytesInput.from(BYTES3));        validateContains(SCHEMA, pages, PATH2, 8, BytesInput.from(BYTES4));        assertNull(r.readNextRowGroup());    }    PrintFooter.main(new String[] { path.toString() });}
0
public void testAlignmentWithNoPaddingNeeded() throws Exception
{    File testFile = temp.newFile();    Path path = new Path(testFile.toURI());    Configuration conf = new Configuration();        conf.setBoolean(ParquetOutputFormat.PAGE_WRITE_CHECKSUM_ENABLED, false);        ParquetFileWriter w = new ParquetFileWriter(conf, SCHEMA, path, 100, 50);    w.start();    w.startBlock(3);    w.startColumn(C1, 5, CODEC);    long c1Starts = w.getPos();    w.writeDataPage(2, 4, BytesInput.from(BYTES1), EMPTY_STATS, BIT_PACKED, BIT_PACKED, PLAIN);    w.writeDataPage(3, 4, BytesInput.from(BYTES1), EMPTY_STATS, BIT_PACKED, BIT_PACKED, PLAIN);    w.endColumn();    long c1Ends = w.getPos();    w.startColumn(C2, 6, CODEC);    long c2Starts = w.getPos();    w.writeDataPage(2, 4, BytesInput.from(BYTES2), EMPTY_STATS, BIT_PACKED, BIT_PACKED, PLAIN);    w.writeDataPage(3, 4, BytesInput.from(BYTES2), EMPTY_STATS, BIT_PACKED, BIT_PACKED, PLAIN);    w.writeDataPage(1, 4, BytesInput.from(BYTES2), EMPTY_STATS, BIT_PACKED, BIT_PACKED, PLAIN);    w.endColumn();    long c2Ends = w.getPos();    w.endBlock();        long firstRowGroupEnds = w.getPos();    w.startBlock(4);    w.startColumn(C1, 7, CODEC);    w.writeDataPage(7, 4, BytesInput.from(BYTES3), EMPTY_STATS, BIT_PACKED, BIT_PACKED, PLAIN);    w.endColumn();    w.startColumn(C2, 8, CODEC);    w.writeDataPage(8, 4, BytesInput.from(BYTES4), EMPTY_STATS, BIT_PACKED, BIT_PACKED, PLAIN);    w.endColumn();    w.endBlock();    long secondRowGroupEnds = w.getPos();    w.end(new HashMap<String, String>());    FileSystem fs = path.getFileSystem(conf);    long fileLen = fs.getFileStatus(path).getLen();    FSDataInputStream data = fs.open(path);        data.seek(fileLen - 8);    long footerLen = BytesUtils.readIntLittleEndian(data);    long startFooter = fileLen - footerLen - 8;    assertEquals("Footer should start after second row group without padding", secondRowGroupEnds, startFooter);    ParquetMetadata readFooter = ParquetFileReader.readFooter(conf, path);    assertEquals("footer: " + readFooter, 2, readFooter.getBlocks().size());    assertEquals(c1Ends - c1Starts, readFooter.getBlocks().get(0).getColumns().get(0).getTotalSize());    assertEquals(c2Ends - c2Starts, readFooter.getBlocks().get(0).getColumns().get(1).getTotalSize());    assertEquals(c2Ends - c1Starts, readFooter.getBlocks().get(0).getTotalByteSize());    HashSet<Encoding> expectedEncoding = new HashSet<Encoding>();    expectedEncoding.add(PLAIN);    expectedEncoding.add(BIT_PACKED);    assertEquals(expectedEncoding, readFooter.getBlocks().get(0).getColumns().get(0).getEncodings());        assertEquals("First row group should start after magic", 4, readFooter.getBlocks().get(0).getStartingPos());    assertTrue("First row group should end before the block size (120)", firstRowGroupEnds > 100);    assertEquals("Second row group should start after no padding", 109, readFooter.getBlocks().get(1).getStartingPos());    {                ParquetFileReader r = new ParquetFileReader(conf, readFooter.getFileMetaData(), path, Arrays.asList(readFooter.getBlocks().get(0)), Arrays.asList(SCHEMA.getColumnDescription(PATH1)));        PageReadStore pages = r.readNextRowGroup();        assertEquals(3, pages.getRowCount());        validateContains(SCHEMA, pages, PATH1, 2, BytesInput.from(BYTES1));        validateContains(SCHEMA, pages, PATH1, 3, BytesInput.from(BYTES1));        assertNull(r.readNextRowGroup());    }    {                ParquetFileReader r = new ParquetFileReader(conf, readFooter.getFileMetaData(), path, readFooter.getBlocks(), Arrays.asList(SCHEMA.getColumnDescription(PATH1), SCHEMA.getColumnDescription(PATH2)));        PageReadStore pages = r.readNextRowGroup();        assertEquals(3, pages.getRowCount());        validateContains(SCHEMA, pages, PATH1, 2, BytesInput.from(BYTES1));        validateContains(SCHEMA, pages, PATH1, 3, BytesInput.from(BYTES1));        validateContains(SCHEMA, pages, PATH2, 2, BytesInput.from(BYTES2));        validateContains(SCHEMA, pages, PATH2, 3, BytesInput.from(BYTES2));        validateContains(SCHEMA, pages, PATH2, 1, BytesInput.from(BYTES2));        pages = r.readNextRowGroup();        assertEquals(4, pages.getRowCount());        validateContains(SCHEMA, pages, PATH1, 7, BytesInput.from(BYTES3));        validateContains(SCHEMA, pages, PATH2, 8, BytesInput.from(BYTES4));        assertNull(r.readNextRowGroup());    }    PrintFooter.main(new String[] { path.toString() });}
0
public void testConvertToThriftStatistics() throws Exception
{    long[] longArray = new long[] { 39L, 99L, 12L, 1000L, 65L, 542L, 2533461316L, -253346131996L, Long.MAX_VALUE, Long.MIN_VALUE };    LongStatistics parquetMRstats = new LongStatistics();    for (long l : longArray) {        parquetMRstats.updateStats(l);    }    final String createdBy = "parquet-mr version 1.8.0 (build d4d5a07ec9bd262ca1e93c309f1d7d4a74ebda4c)";    Statistics thriftStats = org.apache.parquet.format.converter.ParquetMetadataConverter.toParquetStatistics(parquetMRstats);    LongStatistics convertedBackStats = (LongStatistics) org.apache.parquet.format.converter.ParquetMetadataConverter.fromParquetStatistics(createdBy, thriftStats, PrimitiveTypeName.INT64);    assertEquals(parquetMRstats.getMax(), convertedBackStats.getMax());    assertEquals(parquetMRstats.getMin(), convertedBackStats.getMin());    assertEquals(parquetMRstats.getNumNulls(), convertedBackStats.getNumNulls());}
0
public void testWriteReadStatistics() throws Exception
{        Assume.assumeTrue(!shouldIgnoreStatistics(Version.FULL_VERSION, BINARY));    File testFile = temp.newFile();    testFile.delete();    Path path = new Path(testFile.toURI());    Configuration configuration = new Configuration();    configuration.setBoolean("parquet.strings.signed-min-max.enabled", true);    MessageType schema = MessageTypeParser.parseMessageType("message m { required group a {required binary b (UTF8);} required group c { required int64 d; }}");    String[] path1 = { "a", "b" };    ColumnDescriptor c1 = schema.getColumnDescription(path1);    String[] path2 = { "c", "d" };    ColumnDescriptor c2 = schema.getColumnDescription(path2);    byte[] bytes1 = { 0, 1, 2, 3 };    byte[] bytes2 = { 1, 2, 3, 4 };    byte[] bytes3 = { 2, 3, 4, 5 };    byte[] bytes4 = { 3, 4, 5, 6 };    CompressionCodecName codec = CompressionCodecName.UNCOMPRESSED;    BinaryStatistics statsB1C1P1 = new BinaryStatistics();    BinaryStatistics statsB1C1P2 = new BinaryStatistics();    LongStatistics statsB1C2P1 = new LongStatistics();    LongStatistics statsB1C2P2 = new LongStatistics();    BinaryStatistics statsB2C1P1 = new BinaryStatistics();    LongStatistics statsB2C2P1 = new LongStatistics();    statsB1C1P1.setMinMax(Binary.fromString("s"), Binary.fromString("z"));    statsB1C1P2.setMinMax(Binary.fromString("a"), Binary.fromString("b"));    statsB1C2P1.setMinMax(2l, 10l);    statsB1C2P2.setMinMax(-6l, 4l);    statsB2C1P1.setMinMax(Binary.fromString("d"), Binary.fromString("e"));    statsB2C2P1.setMinMax(11l, 122l);    ParquetFileWriter w = new ParquetFileWriter(configuration, schema, path);    w.start();    w.startBlock(3);    w.startColumn(c1, 5, codec);    w.writeDataPage(2, 4, BytesInput.from(bytes1), statsB1C1P1, BIT_PACKED, BIT_PACKED, PLAIN);    w.writeDataPage(3, 4, BytesInput.from(bytes1), statsB1C1P2, BIT_PACKED, BIT_PACKED, PLAIN);    w.endColumn();    w.startColumn(c2, 6, codec);    w.writeDataPage(3, 4, BytesInput.from(bytes2), statsB1C2P1, BIT_PACKED, BIT_PACKED, PLAIN);    w.writeDataPage(1, 4, BytesInput.from(bytes2), statsB1C2P2, BIT_PACKED, BIT_PACKED, PLAIN);    w.endColumn();    w.endBlock();    w.startBlock(4);    w.startColumn(c1, 7, codec);    w.writeDataPage(7, 4, BytesInput.from(bytes3), statsB2C1P1, BIT_PACKED, BIT_PACKED, PLAIN);    w.endColumn();    w.startColumn(c2, 8, codec);    w.writeDataPage(8, 4, BytesInput.from(bytes4), statsB2C2P1, BIT_PACKED, BIT_PACKED, PLAIN);    w.endColumn();    w.endBlock();    w.end(new HashMap<String, String>());    ParquetMetadata readFooter = ParquetFileReader.readFooter(configuration, path);    for (BlockMetaData block : readFooter.getBlocks()) {        for (ColumnChunkMetaData col : block.getColumns()) {            col.getPath();        }    }        BinaryStatistics bs1 = new BinaryStatistics();    bs1.setMinMax(Binary.fromString("a"), Binary.fromString("z"));    LongStatistics ls1 = new LongStatistics();    ls1.setMinMax(-6l, 10l);    BinaryStatistics bs2 = new BinaryStatistics();    bs2.setMinMax(Binary.fromString("d"), Binary.fromString("e"));    LongStatistics ls2 = new LongStatistics();    ls2.setMinMax(11l, 122l);    {                BinaryStatistics bsout = (BinaryStatistics) readFooter.getBlocks().get(0).getColumns().get(0).getStatistics();        String str = new String(bsout.getMaxBytes());        String str2 = new String(bsout.getMinBytes());        TestUtils.assertStatsValuesEqual(bs1, readFooter.getBlocks().get(0).getColumns().get(0).getStatistics());        TestUtils.assertStatsValuesEqual(ls1, readFooter.getBlocks().get(0).getColumns().get(1).getStatistics());    }    {                TestUtils.assertStatsValuesEqual(bs2, readFooter.getBlocks().get(1).getColumns().get(0).getStatistics());        TestUtils.assertStatsValuesEqual(ls2, readFooter.getBlocks().get(1).getColumns().get(1).getStatistics());    }}
0
public void testMetaDataFile() throws Exception
{    File testDir = temp.newFolder();    Path testDirPath = new Path(testDir.toURI());    Configuration configuration = new Configuration();    final FileSystem fs = testDirPath.getFileSystem(configuration);    enforceEmptyDir(configuration, testDirPath);    MessageType schema = MessageTypeParser.parseMessageType("message m { required group a {required binary b;} required group c { required int64 d; }}");    createFile(configuration, new Path(testDirPath, "part0"), schema);    createFile(configuration, new Path(testDirPath, "part1"), schema);    createFile(configuration, new Path(testDirPath, "part2"), schema);    FileStatus outputStatus = fs.getFileStatus(testDirPath);    List<Footer> footers = ParquetFileReader.readFooters(configuration, outputStatus, false);    validateFooters(footers);    ParquetFileWriter.writeMetadataFile(configuration, testDirPath, footers, JobSummaryLevel.ALL);    footers = ParquetFileReader.readFooters(configuration, outputStatus, false);    validateFooters(footers);    footers = ParquetFileReader.readFooters(configuration, fs.getFileStatus(new Path(testDirPath, "part0")), false);    assertEquals(1, footers.size());    final FileStatus metadataFile = fs.getFileStatus(new Path(testDirPath, ParquetFileWriter.PARQUET_METADATA_FILE));    final FileStatus metadataFileLight = fs.getFileStatus(new Path(testDirPath, ParquetFileWriter.PARQUET_COMMON_METADATA_FILE));    final List<Footer> metadata = ParquetFileReader.readSummaryFile(configuration, metadataFile);    validateFooters(metadata);    footers = ParquetFileReader.readAllFootersInParallelUsingSummaryFiles(configuration, Arrays.asList(fs.listStatus(testDirPath, HiddenFileFilter.INSTANCE)), false);    validateFooters(footers);    fs.delete(metadataFile.getPath(), false);    fs.delete(metadataFileLight.getPath(), false);    footers = ParquetFileReader.readAllFootersInParallelUsingSummaryFiles(configuration, Arrays.asList(fs.listStatus(testDirPath)), false);    validateFooters(footers);}
0
public void testWriteReadStatisticsAllNulls() throws Exception
{        Assume.assumeTrue(!shouldIgnoreStatistics(Version.FULL_VERSION, BINARY));    File testFile = temp.newFile();    testFile.delete();    writeSchema = "message example {\n" + "required binary content (UTF8);\n" + "}";    Path path = new Path(testFile.toURI());    MessageType schema = MessageTypeParser.parseMessageType(writeSchema);    Configuration configuration = new Configuration();    configuration.setBoolean("parquet.strings.signed-min-max.enabled", true);    GroupWriteSupport.setSchema(schema, configuration);    ParquetWriter<Group> writer = new ParquetWriter<Group>(path, configuration, new GroupWriteSupport());    Group r1 = new SimpleGroup(schema);    writer.write(r1);    writer.close();    ParquetMetadata readFooter = ParquetFileReader.readFooter(configuration, path);        org.apache.parquet.column.statistics.Statistics stats = readFooter.getBlocks().get(0).getColumns().get(0).getStatistics();    assertFalse("is empty: " + stats, stats.isEmpty());        assertEquals("nulls: " + stats, 1, stats.getNumNulls());}
0
private void validateFooters(final List<Footer> metadata)
{        assertEquals(String.valueOf(metadata), 3, metadata.size());    for (Footer footer : metadata) {        final File file = new File(footer.getFile().toUri());        assertTrue(file.getName(), file.getName().startsWith("part"));        assertTrue(file.getPath(), file.exists());        final ParquetMetadata parquetMetadata = footer.getParquetMetadata();        assertEquals(2, parquetMetadata.getBlocks().size());        final Map<String, String> keyValueMetaData = parquetMetadata.getFileMetaData().getKeyValueMetaData();        assertEquals("bar", keyValueMetaData.get("foo"));        assertEquals(footer.getFile().getName(), keyValueMetaData.get(footer.getFile().getName()));    }}
1
private void createFile(Configuration configuration, Path path, MessageType schema) throws IOException
{    String[] path1 = { "a", "b" };    ColumnDescriptor c1 = schema.getColumnDescription(path1);    String[] path2 = { "c", "d" };    ColumnDescriptor c2 = schema.getColumnDescription(path2);    byte[] bytes1 = { 0, 1, 2, 3 };    byte[] bytes2 = { 1, 2, 3, 4 };    byte[] bytes3 = { 2, 3, 4, 5 };    byte[] bytes4 = { 3, 4, 5, 6 };    CompressionCodecName codec = CompressionCodecName.UNCOMPRESSED;    BinaryStatistics stats1 = new BinaryStatistics();    BinaryStatistics stats2 = new BinaryStatistics();    ParquetFileWriter w = new ParquetFileWriter(configuration, schema, path);    w.start();    w.startBlock(3);    w.startColumn(c1, 5, codec);    w.writeDataPage(2, 4, BytesInput.from(bytes1), stats1, BIT_PACKED, BIT_PACKED, PLAIN);    w.writeDataPage(3, 4, BytesInput.from(bytes1), stats1, BIT_PACKED, BIT_PACKED, PLAIN);    w.endColumn();    w.startColumn(c2, 6, codec);    w.writeDataPage(2, 4, BytesInput.from(bytes2), stats2, BIT_PACKED, BIT_PACKED, PLAIN);    w.writeDataPage(3, 4, BytesInput.from(bytes2), stats2, BIT_PACKED, BIT_PACKED, PLAIN);    w.writeDataPage(1, 4, BytesInput.from(bytes2), stats2, BIT_PACKED, BIT_PACKED, PLAIN);    w.endColumn();    w.endBlock();    w.startBlock(4);    w.startColumn(c1, 7, codec);    w.writeDataPage(7, 4, BytesInput.from(bytes3), stats1, BIT_PACKED, BIT_PACKED, PLAIN);    w.endColumn();    w.startColumn(c2, 8, codec);    w.writeDataPage(8, 4, BytesInput.from(bytes4), stats2, BIT_PACKED, BIT_PACKED, PLAIN);    w.endColumn();    w.endBlock();    final HashMap<String, String> extraMetaData = new HashMap<String, String>();    extraMetaData.put("foo", "bar");    extraMetaData.put(path.getName(), path.getName());    w.end(extraMetaData);}
0
private void validateContains(MessageType schema, PageReadStore pages, String[] path, int values, BytesInput bytes) throws IOException
{    PageReader pageReader = pages.getPageReader(schema.getColumnDescription(path));    DataPage page = pageReader.readPage();    assertEquals(values, page.getValueCount());    assertArrayEquals(bytes.toByteArray(), ((DataPageV1) page).getBytes().toByteArray());}
0
public void testMergeMetadata()
{    FileMetaData md1 = new FileMetaData(new MessageType("root1", new PrimitiveType(REPEATED, BINARY, "a"), new PrimitiveType(OPTIONAL, BINARY, "b")), new HashMap<String, String>(), "test");    FileMetaData md2 = new FileMetaData(new MessageType("root2", new PrimitiveType(REQUIRED, BINARY, "c")), new HashMap<String, String>(), "test2");    GlobalMetaData merged = ParquetFileWriter.mergeInto(md2, ParquetFileWriter.mergeInto(md1, null));    assertEquals(merged.getSchema(), new MessageType("root1", new PrimitiveType(REPEATED, BINARY, "a"), new PrimitiveType(OPTIONAL, BINARY, "b"), new PrimitiveType(REQUIRED, BINARY, "c")));}
0
public void testMergeFooters()
{    List<BlockMetaData> oneBlocks = new ArrayList<BlockMetaData>();    oneBlocks.add(new BlockMetaData());    oneBlocks.add(new BlockMetaData());    List<BlockMetaData> twoBlocks = new ArrayList<BlockMetaData>();    twoBlocks.add(new BlockMetaData());    List<BlockMetaData> expected = new ArrayList<BlockMetaData>();    expected.addAll(oneBlocks);    expected.addAll(twoBlocks);    Footer one = new Footer(new Path("file:/tmp/output/one.parquet"), new ParquetMetadata(new FileMetaData(new MessageType("root1", new PrimitiveType(REPEATED, BINARY, "a"), new PrimitiveType(OPTIONAL, BINARY, "b")), new HashMap<String, String>(), "test"), oneBlocks));    Footer two = new Footer(new Path("/tmp/output/two.parquet"), new ParquetMetadata(new FileMetaData(new MessageType("root2", new PrimitiveType(REQUIRED, BINARY, "c")), new HashMap<String, String>(), "test2"), twoBlocks));    List<Footer> footers = new ArrayList<Footer>();    footers.add(one);    footers.add(two);    ParquetMetadata merged = ParquetFileWriter.mergeFooters(new Path("/tmp"), footers);    assertEquals(new MessageType("root1", new PrimitiveType(REPEATED, BINARY, "a"), new PrimitiveType(OPTIONAL, BINARY, "b"), new PrimitiveType(REQUIRED, BINARY, "c")), merged.getFileMetaData().getSchema());    assertEquals("Should have all blocks", expected, merged.getBlocks());}
0
public void testWriteMetadataFileWithRelativeOutputPath() throws IOException
{    Configuration conf = new Configuration();    FileSystem fs = FileSystem.get(conf);    Path relativeRoot = new Path("target/_test_relative");    Path qualifiedRoot = fs.makeQualified(relativeRoot);    ParquetMetadata mock = Mockito.mock(ParquetMetadata.class);    FileMetaData fileMetaData = new FileMetaData(new MessageType("root1", new PrimitiveType(REPEATED, BINARY, "a")), new HashMap<String, String>(), "test");    Mockito.when(mock.getFileMetaData()).thenReturn(fileMetaData);    List<Footer> footers = new ArrayList<Footer>();    Footer footer = new Footer(new Path(qualifiedRoot, "one"), mock);    footers.add(footer);        ParquetFileWriter.writeMetadataFile(conf, relativeRoot, footers, JobSummaryLevel.ALL);}
0
public void testColumnIndexWriteRead() throws Exception
{    File testFile = temp.newFile();    testFile.delete();    Path path = new Path(testFile.toURI());    Configuration configuration = new Configuration();    ParquetFileWriter w = new ParquetFileWriter(configuration, SCHEMA, path);    w.start();    w.startBlock(4);    w.startColumn(C1, 7, CODEC);    w.writeDataPage(7, 4, BytesInput.from(BYTES3), EMPTY_STATS, BIT_PACKED, BIT_PACKED, PLAIN);    w.endColumn();    w.startColumn(C2, 8, CODEC);    w.writeDataPage(8, 4, BytesInput.from(BYTES4), EMPTY_STATS, BIT_PACKED, BIT_PACKED, PLAIN);    w.endColumn();    w.endBlock();    w.startBlock(4);    w.startColumn(C1, 5, CODEC);    long c1p1Starts = w.getPos();    w.writeDataPage(2, 4, BytesInput.from(BYTES1), statsC1(null, Binary.fromString("aaa")), 1, BIT_PACKED, BIT_PACKED, PLAIN);    long c1p2Starts = w.getPos();    w.writeDataPage(3, 4, BytesInput.from(BYTES1), statsC1(Binary.fromString("bbb"), Binary.fromString("ccc")), 3, BIT_PACKED, BIT_PACKED, PLAIN);    w.endColumn();    long c1Ends = w.getPos();    w.startColumn(C2, 6, CODEC);    long c2p1Starts = w.getPos();    w.writeDataPage(2, 4, BytesInput.from(BYTES2), statsC2(117l, 100l), 1, BIT_PACKED, BIT_PACKED, PLAIN);    long c2p2Starts = w.getPos();    w.writeDataPage(3, 4, BytesInput.from(BYTES2), statsC2(null, null, null), 2, BIT_PACKED, BIT_PACKED, PLAIN);    long c2p3Starts = w.getPos();    w.writeDataPage(1, 4, BytesInput.from(BYTES2), statsC2(0l), 1, BIT_PACKED, BIT_PACKED, PLAIN);    w.endColumn();    long c2Ends = w.getPos();    w.endBlock();    w.startBlock(4);    w.startColumn(C1, 7, CODEC);    w.writeDataPage(7, 4, BytesInput.from(BYTES3),     statsC1(Binary.fromConstantByteArray(new byte[(int) MAX_STATS_SIZE]), Binary.fromConstantByteArray(new byte[1])), 4, BIT_PACKED, BIT_PACKED, PLAIN);    w.endColumn();    w.startColumn(C2, 8, CODEC);    w.writeDataPage(8, 4, BytesInput.from(BYTES4), EMPTY_STATS, BIT_PACKED, BIT_PACKED, PLAIN);    w.endColumn();    w.endBlock();    w.end(new HashMap<String, String>());    try (ParquetFileReader reader = new ParquetFileReader(HadoopInputFile.fromPath(path, configuration), ParquetReadOptions.builder().build())) {        ParquetMetadata footer = reader.getFooter();        assertEquals(3, footer.getBlocks().size());        BlockMetaData blockMeta = footer.getBlocks().get(1);        assertEquals(2, blockMeta.getColumns().size());        ColumnIndex columnIndex = reader.readColumnIndex(blockMeta.getColumns().get(0));        assertEquals(BoundaryOrder.ASCENDING, columnIndex.getBoundaryOrder());        assertTrue(Arrays.asList(1l, 0l).equals(columnIndex.getNullCounts()));        assertTrue(Arrays.asList(false, false).equals(columnIndex.getNullPages()));        List<ByteBuffer> minValues = columnIndex.getMinValues();        assertEquals(2, minValues.size());        List<ByteBuffer> maxValues = columnIndex.getMaxValues();        assertEquals(2, maxValues.size());        assertEquals("aaa", new String(minValues.get(0).array(), StandardCharsets.UTF_8));        assertEquals("aaa", new String(maxValues.get(0).array(), StandardCharsets.UTF_8));        assertEquals("bbb", new String(minValues.get(1).array(), StandardCharsets.UTF_8));        assertEquals("ccc", new String(maxValues.get(1).array(), StandardCharsets.UTF_8));        columnIndex = reader.readColumnIndex(blockMeta.getColumns().get(1));        assertEquals(BoundaryOrder.DESCENDING, columnIndex.getBoundaryOrder());        assertTrue(Arrays.asList(0l, 3l, 0l).equals(columnIndex.getNullCounts()));        assertTrue(Arrays.asList(false, true, false).equals(columnIndex.getNullPages()));        minValues = columnIndex.getMinValues();        assertEquals(3, minValues.size());        maxValues = columnIndex.getMaxValues();        assertEquals(3, maxValues.size());        assertEquals(100, BytesUtils.bytesToLong(minValues.get(0).array()));        assertEquals(117, BytesUtils.bytesToLong(maxValues.get(0).array()));        assertEquals(0, minValues.get(1).array().length);        assertEquals(0, maxValues.get(1).array().length);        assertEquals(0, BytesUtils.bytesToLong(minValues.get(2).array()));        assertEquals(0, BytesUtils.bytesToLong(maxValues.get(2).array()));        OffsetIndex offsetIndex = reader.readOffsetIndex(blockMeta.getColumns().get(0));        assertEquals(2, offsetIndex.getPageCount());        assertEquals(c1p1Starts, offsetIndex.getOffset(0));        assertEquals(c1p2Starts, offsetIndex.getOffset(1));        assertEquals(c1p2Starts - c1p1Starts, offsetIndex.getCompressedPageSize(0));        assertEquals(c1Ends - c1p2Starts, offsetIndex.getCompressedPageSize(1));        assertEquals(0, offsetIndex.getFirstRowIndex(0));        assertEquals(1, offsetIndex.getFirstRowIndex(1));        offsetIndex = reader.readOffsetIndex(blockMeta.getColumns().get(1));        assertEquals(3, offsetIndex.getPageCount());        assertEquals(c2p1Starts, offsetIndex.getOffset(0));        assertEquals(c2p2Starts, offsetIndex.getOffset(1));        assertEquals(c2p3Starts, offsetIndex.getOffset(2));        assertEquals(c2p2Starts - c2p1Starts, offsetIndex.getCompressedPageSize(0));        assertEquals(c2p3Starts - c2p2Starts, offsetIndex.getCompressedPageSize(1));        assertEquals(c2Ends - c2p3Starts, offsetIndex.getCompressedPageSize(2));        assertEquals(0, offsetIndex.getFirstRowIndex(0));        assertEquals(1, offsetIndex.getFirstRowIndex(1));        assertEquals(3, offsetIndex.getFirstRowIndex(2));        assertNull(reader.readColumnIndex(footer.getBlocks().get(2).getColumns().get(0)));    }}
0
private org.apache.parquet.column.statistics.Statistics<?> statsC1(Binary... values)
{    org.apache.parquet.column.statistics.Statistics<?> stats = org.apache.parquet.column.statistics.Statistics.createStats(C1.getPrimitiveType());    for (Binary value : values) {        if (value == null) {            stats.incrementNumNulls();        } else {            stats.updateStats(value);        }    }    return stats;}
0
private org.apache.parquet.column.statistics.Statistics<?> statsC2(Long... values)
{    org.apache.parquet.column.statistics.Statistics<?> stats = org.apache.parquet.column.statistics.Statistics.createStats(C2.getPrimitiveType());    for (Long value : values) {        if (value == null) {            stats.incrementNumNulls();        } else {            stats.updateStats(value);        }    }    return stats;}
0
public void testDefault() throws Exception
{    Configuration conf = new Configuration();        assertEquals(JobSummaryLevel.ALL, ParquetOutputFormat.getJobSummaryLevel(conf));}
0
public void testDeprecatedStillWorks() throws Exception
{    Configuration conf = new Configuration();    conf.set(ParquetOutputFormat.ENABLE_JOB_SUMMARY, "true");    assertEquals(JobSummaryLevel.ALL, ParquetOutputFormat.getJobSummaryLevel(conf));    conf.set(ParquetOutputFormat.ENABLE_JOB_SUMMARY, "false");    assertEquals(JobSummaryLevel.NONE, ParquetOutputFormat.getJobSummaryLevel(conf));}
0
public void testLevelParses() throws Exception
{    Configuration conf = new Configuration();    conf.set(ParquetOutputFormat.JOB_SUMMARY_LEVEL, "all");    assertEquals(JobSummaryLevel.ALL, ParquetOutputFormat.getJobSummaryLevel(conf));    conf.set(ParquetOutputFormat.JOB_SUMMARY_LEVEL, "common_only");    assertEquals(JobSummaryLevel.COMMON_ONLY, ParquetOutputFormat.getJobSummaryLevel(conf));    conf.set(ParquetOutputFormat.JOB_SUMMARY_LEVEL, "none");    assertEquals(JobSummaryLevel.NONE, ParquetOutputFormat.getJobSummaryLevel(conf));}
0
public void testLevelTakesPrecedence() throws Exception
{    Configuration conf = new Configuration();    conf.set(ParquetOutputFormat.JOB_SUMMARY_LEVEL, "common_only");    conf.set(ParquetOutputFormat.ENABLE_JOB_SUMMARY, "false");    assertEquals(JobSummaryLevel.COMMON_ONLY, ParquetOutputFormat.getJobSummaryLevel(conf));}
0
public void test() throws Exception
{    Configuration conf = new Configuration();    Path root = new Path("target/tests/TestParquetWriter/");    enforceEmptyDir(conf, root);    MessageType schema = parseMessageType("message test { " + "required binary binary_field; " + "required int32 int32_field; " + "required int64 int64_field; " + "required boolean boolean_field; " + "required float float_field; " + "required double double_field; " + "required fixed_len_byte_array(3) flba_field; " + "required int96 int96_field; " + "} ");    GroupWriteSupport.setSchema(schema, conf);    SimpleGroupFactory f = new SimpleGroupFactory(schema);    Map<String, Encoding> expected = new HashMap<String, Encoding>();    expected.put("10-" + PARQUET_1_0, PLAIN_DICTIONARY);    expected.put("1000-" + PARQUET_1_0, PLAIN);    expected.put("10-" + PARQUET_2_0, RLE_DICTIONARY);    expected.put("1000-" + PARQUET_2_0, DELTA_BYTE_ARRAY);    for (int modulo : asList(10, 1000)) {        for (WriterVersion version : WriterVersion.values()) {            Path file = new Path(root, version.name() + "_" + modulo);            ParquetWriter<Group> writer = new ParquetWriter<Group>(file, new GroupWriteSupport(), UNCOMPRESSED, 1024, 1024, 512, true, false, version, conf);            for (int i = 0; i < 1000; i++) {                writer.write(f.newGroup().append("binary_field", "test" + (i % modulo)).append("int32_field", 32).append("int64_field", 64l).append("boolean_field", true).append("float_field", 1.0f).append("double_field", 2.0d).append("flba_field", "foo").append("int96_field", Binary.fromConstantByteArray(new byte[12])));            }            writer.close();            ParquetReader<Group> reader = ParquetReader.builder(new GroupReadSupport(), file).withConf(conf).build();            for (int i = 0; i < 1000; i++) {                Group group = reader.read();                assertEquals("test" + (i % modulo), group.getBinary("binary_field", 0).toStringUsingUTF8());                assertEquals(32, group.getInteger("int32_field", 0));                assertEquals(64l, group.getLong("int64_field", 0));                assertEquals(true, group.getBoolean("boolean_field", 0));                assertEquals(1.0f, group.getFloat("float_field", 0), 0.001);                assertEquals(2.0d, group.getDouble("double_field", 0), 0.001);                assertEquals("foo", group.getBinary("flba_field", 0).toStringUsingUTF8());                assertEquals(Binary.fromConstantByteArray(new byte[12]), group.getInt96("int96_field", 0));            }            reader.close();            ParquetMetadata footer = readFooter(conf, file, NO_FILTER);            for (BlockMetaData blockMetaData : footer.getBlocks()) {                for (ColumnChunkMetaData column : blockMetaData.getColumns()) {                    if (column.getPath().toDotString().equals("binary_field")) {                        String key = modulo + "-" + version;                        Encoding expectedEncoding = expected.get(key);                        assertTrue(key + ":" + column.getEncodings() + " should contain " + expectedEncoding, column.getEncodings().contains(expectedEncoding));                    }                }            }            assertEquals("Object model property should be example", "example", footer.getFileMetaData().getKeyValueMetaData().get(ParquetWriter.OBJECT_MODEL_NAME_PROP));        }    }}
0
public void testBadWriteSchema() throws IOException
{    final File file = temp.newFile("test.parquet");    file.delete();    TestUtils.assertThrows("Should reject a schema with an empty group", InvalidSchemaException.class, (Callable<Void>) () -> {        ExampleParquetWriter.builder(new Path(file.toString())).withType(Types.buildMessage().addField(new GroupType(REQUIRED, "invalid_group")).named("invalid_message")).build();        return null;    });    Assert.assertFalse("Should not create a file when schema is rejected", file.exists());}
0
public void testNullValuesWithPageRowLimit() throws IOException
{    MessageType schema = Types.buildMessage().optionalList().optionalElement(BINARY).as(stringType()).named("str_list").named("msg");    final int recordCount = 100;    Configuration conf = new Configuration();    GroupWriteSupport.setSchema(schema, conf);    GroupFactory factory = new SimpleGroupFactory(schema);    Group listNull = factory.newGroup();    File file = temp.newFile();    file.delete();    Path path = new Path(file.getAbsolutePath());    try (ParquetWriter<Group> writer = ExampleParquetWriter.builder(path).withPageRowCountLimit(10).withConf(conf).build()) {        for (int i = 0; i < recordCount; ++i) {            writer.write(listNull);        }    }    try (ParquetReader<Group> reader = ParquetReader.builder(new GroupReadSupport(), path).build()) {        int readRecordCount = 0;        for (Group group = reader.read(); group != null; group = reader.read()) {            assertEquals(listNull.toString(), group.toString());            ++readRecordCount;        }        assertEquals("Number of written records should be equal to the read one", recordCount, readRecordCount);    }}
0
public void createSourceData() throws IOException
{    this.file1 = newTemp();    this.file2 = newTemp();    ParquetWriter<Group> writer1 = ExampleParquetWriter.builder(file1).withType(FILE_SCHEMA).build();    ParquetWriter<Group> writer2 = ExampleParquetWriter.builder(file2).withType(FILE_SCHEMA).build();    for (int i = 0; i < FILE_SIZE; i += 1) {        Group group1 = GROUP_FACTORY.newGroup();        group1.add("id", i);        group1.add("string", UUID.randomUUID().toString());        writer1.write(group1);        file1content.add(group1);        Group group2 = GROUP_FACTORY.newGroup();        group2.add("id", FILE_SIZE + i);        group2.add("string", UUID.randomUUID().toString());        writer2.write(group2);        file2content.add(group2);    }    writer1.close();    writer2.close();}
0
public void testBasicBehavior() throws IOException
{    Path combinedFile = newTemp();    ParquetFileWriter writer = new ParquetFileWriter(CONF, FILE_SCHEMA, combinedFile);    writer.start();    writer.appendFile(CONF, file1);    writer.appendFile(CONF, file2);    writer.end(EMPTY_METADATA);    LinkedList<Group> expected = new LinkedList<Group>();    expected.addAll(file1content);    expected.addAll(file2content);    ParquetReader<Group> reader = ParquetReader.builder(new GroupReadSupport(), combinedFile).build();    Group next;    while ((next = reader.read()) != null) {        Group expectedNext = expected.removeFirst();                Assert.assertEquals("Each id should match", expectedNext.getInteger("id", 0), next.getInteger("id", 0));        Assert.assertEquals("Each string should match", expectedNext.getString("string", 0), next.getString("string", 0));    }    Assert.assertEquals("All records should be present", 0, expected.size());}
0
public void testMergedMetadata() throws IOException
{    Path combinedFile = newTemp();    ParquetFileWriter writer = new ParquetFileWriter(CONF, FILE_SCHEMA, combinedFile);    writer.start();    writer.appendFile(CONF, file1);    writer.appendFile(CONF, file2);    writer.end(EMPTY_METADATA);    ParquetMetadata combinedFooter = ParquetFileReader.readFooter(CONF, combinedFile, NO_FILTER);    ParquetMetadata f1Footer = ParquetFileReader.readFooter(CONF, file1, NO_FILTER);    ParquetMetadata f2Footer = ParquetFileReader.readFooter(CONF, file2, NO_FILTER);    LinkedList<BlockMetaData> expectedRowGroups = new LinkedList<BlockMetaData>();    expectedRowGroups.addAll(f1Footer.getBlocks());    expectedRowGroups.addAll(f2Footer.getBlocks());    Assert.assertEquals("Combined should have the right number of row groups", expectedRowGroups.size(), combinedFooter.getBlocks().size());    long nextStart = 4;    for (BlockMetaData rowGroup : combinedFooter.getBlocks()) {        BlockMetaData expected = expectedRowGroups.removeFirst();        Assert.assertEquals("Row count should match", expected.getRowCount(), rowGroup.getRowCount());        Assert.assertEquals("Compressed size should match", expected.getCompressedSize(), rowGroup.getCompressedSize());        Assert.assertEquals("Total size should match", expected.getTotalByteSize(), rowGroup.getTotalByteSize());        Assert.assertEquals("Start pos should be at the last row group's end", nextStart, rowGroup.getStartingPos());        assertColumnsEquivalent(expected.getColumns(), rowGroup.getColumns());        nextStart = rowGroup.getStartingPos() + rowGroup.getTotalByteSize();    }}
0
public void assertColumnsEquivalent(List<ColumnChunkMetaData> expected, List<ColumnChunkMetaData> actual)
{    Assert.assertEquals("Should have the expected columns", expected.size(), actual.size());    for (int i = 0; i < actual.size(); i += 1) {        ColumnChunkMetaData current = actual.get(i);        if (i != 0) {            ColumnChunkMetaData previous = actual.get(i - 1);            long expectedStart = previous.getStartingPos() + previous.getTotalSize();            Assert.assertEquals("Should start after the previous column", expectedStart, current.getStartingPos());        }        assertColumnMetadataEquivalent(expected.get(i), current);    }}
0
public void assertColumnMetadataEquivalent(ColumnChunkMetaData expected, ColumnChunkMetaData actual)
{    Assert.assertEquals("Should be the expected column", expected.getPath(), expected.getPath());    Assert.assertEquals("Primitive type should not change", expected.getType(), actual.getType());    Assert.assertEquals("Compression codec should not change", expected.getCodec(), actual.getCodec());    Assert.assertEquals("Data encodings should not change", expected.getEncodings(), actual.getEncodings());    Assert.assertEquals("Statistics should not change", expected.getStatistics(), actual.getStatistics());    Assert.assertEquals("Uncompressed size should not change", expected.getTotalUncompressedSize(), actual.getTotalUncompressedSize());    Assert.assertEquals("Compressed size should not change", expected.getTotalSize(), actual.getTotalSize());    Assert.assertEquals("Number of values should not change", expected.getValueCount(), actual.getValueCount());}
0
public void testAllowDroppingColumns() throws IOException
{    MessageType droppedColumnSchema = Types.buildMessage().required(BINARY).as(UTF8).named("string").named("AppendTest");    Path droppedColumnFile = newTemp();    ParquetFileWriter writer = new ParquetFileWriter(CONF, droppedColumnSchema, droppedColumnFile);    writer.start();    writer.appendFile(CONF, file1);    writer.appendFile(CONF, file2);    writer.end(EMPTY_METADATA);    LinkedList<Group> expected = new LinkedList<Group>();    expected.addAll(file1content);    expected.addAll(file2content);    ParquetMetadata footer = ParquetFileReader.readFooter(CONF, droppedColumnFile, NO_FILTER);    for (BlockMetaData rowGroup : footer.getBlocks()) {        Assert.assertEquals("Should have only the string column", 1, rowGroup.getColumns().size());    }    ParquetReader<Group> reader = ParquetReader.builder(new GroupReadSupport(), droppedColumnFile).build();    Group next;    while ((next = reader.read()) != null) {        Group expectedNext = expected.removeFirst();        Assert.assertEquals("Each string should match", expectedNext.getString("string", 0), next.getString("string", 0));    }    Assert.assertEquals("All records should be present", 0, expected.size());}
0
public void testFailDroppingColumns() throws IOException
{    MessageType droppedColumnSchema = Types.buildMessage().required(BINARY).as(UTF8).named("string").named("AppendTest");    final ParquetMetadata footer = ParquetFileReader.readFooter(CONF, file1, NO_FILTER);    final FSDataInputStream incoming = file1.getFileSystem(CONF).open(file1);    Path droppedColumnFile = newTemp();    final ParquetFileWriter writer = new ParquetFileWriter(CONF, droppedColumnSchema, droppedColumnFile);    writer.start();    TestUtils.assertThrows("Should complain that id column is dropped", IllegalArgumentException.class, (Callable<Void>) () -> {        writer.appendRowGroups(incoming, footer.getBlocks(), false);        return null;    });}
0
public void testFailMissingColumn() throws IOException
{    MessageType fileSchema = Types.buildMessage().required(INT32).named("id").required(BINARY).as(UTF8).named("string").required(FLOAT).named("value").named("AppendTest");    Path missingColumnFile = newTemp();    final ParquetFileWriter writer = new ParquetFileWriter(CONF, fileSchema, missingColumnFile);    writer.start();    TestUtils.assertThrows("Should complain that value column is missing", IllegalArgumentException.class, (Callable<Void>) () -> {        writer.appendFile(CONF, file1);        return null;    });}
0
private Path newTemp() throws IOException
{    File file = temp.newFile();    Preconditions.checkArgument(file.delete(), "Could not remove temp file");    return new Path(file.toString());}
0
public void test() throws Exception
{    Configuration conf = new Configuration();    Path root = new Path("target/tests/TestParquetWriter/");    FileSystem fs = root.getFileSystem(conf);    if (fs.exists(root)) {        fs.delete(root, true);    }    fs.mkdirs(root);    MessageType schema = parseMessageType("message test { " + "required binary binary_field; " + "required int32 int32_field; " + "required int64 int64_field; " + "required boolean boolean_field; " + "required float float_field; " + "required double double_field; " + "required fixed_len_byte_array(3) flba_field; " + "required int96 int96_field; " + "optional binary null_field; " + "} ");    GroupWriteSupport.setSchema(schema, conf);    SimpleGroupFactory f = new SimpleGroupFactory(schema);    Map<String, Encoding> expected = new HashMap<String, Encoding>();    expected.put("10-" + PARQUET_1_0, PLAIN_DICTIONARY);    expected.put("1000-" + PARQUET_1_0, PLAIN);    expected.put("10-" + PARQUET_2_0, RLE_DICTIONARY);    expected.put("1000-" + PARQUET_2_0, DELTA_BYTE_ARRAY);    for (int modulo : asList(10, 1000)) {        for (WriterVersion version : WriterVersion.values()) {            Path file = new Path(root, version.name() + "_" + modulo);            ParquetWriter<Group> writer = new ParquetWriter<Group>(file, new GroupWriteSupport(), UNCOMPRESSED, 1024, 1024, 512, true, false, version, conf);            for (int i = 0; i < 1000; i++) {                writer.write(f.newGroup().append("binary_field", "test" + (i % modulo)).append("int32_field", 32).append("int64_field", 64l).append("boolean_field", true).append("float_field", 1.0f).append("double_field", 2.0d).append("flba_field", "foo").append("int96_field", Binary.fromConstantByteArray(new byte[12])));            }            writer.close();            ParquetReader<Group> reader = ParquetReader.builder(new GroupReadSupport(), file).withConf(conf).build();            for (int i = 0; i < 1000; i++) {                Group group = reader.read();                assertEquals("test" + (i % modulo), group.getBinary("binary_field", 0).toStringUsingUTF8());                assertEquals(32, group.getInteger("int32_field", 0));                assertEquals(64l, group.getLong("int64_field", 0));                assertEquals(true, group.getBoolean("boolean_field", 0));                assertEquals(1.0f, group.getFloat("float_field", 0), 0.001);                assertEquals(2.0d, group.getDouble("double_field", 0), 0.001);                assertEquals("foo", group.getBinary("flba_field", 0).toStringUsingUTF8());                assertEquals(Binary.fromConstantByteArray(new byte[12]), group.getInt96("int96_field", 0));                assertEquals(0, group.getFieldRepetitionCount("null_field"));            }            reader.close();            ParquetMetadata footer = readFooter(conf, file, NO_FILTER);            for (BlockMetaData blockMetaData : footer.getBlocks()) {                for (ColumnChunkMetaData column : blockMetaData.getColumns()) {                    if (column.getPath().toDotString().equals("binary_field")) {                        String key = modulo + "-" + version;                        Encoding expectedEncoding = expected.get(key);                        assertTrue(key + ":" + column.getEncodings() + " should contain " + expectedEncoding, column.getEncodings().contains(expectedEncoding));                    }                }            }        }    }}
0
private static void writeData(ParquetWriter<Group> writer) throws IOException
{    SimpleGroupFactory f = new SimpleGroupFactory(SCHEMA);    for (int i = 0; i < NUM_RECORDS; i += 1) {        int index = i % ALPHABET.length();        Group group = f.newGroup().append("dict_binary_field", ALPHABET.substring(index, index + 1)).append("plain_int32_field", i).append("fallback_binary_field", i < (NUM_RECORDS / 2) ? ALPHABET.substring(index, index + 1) : UUID.randomUUID().toString());        writer.write(group);    }}
0
public void testReadWrite() throws Exception
{    File file = temp.newFile("encoding-stats.parquet");    assertTrue(file.delete());    Path path = new Path(file.toString());    ParquetWriter<Group> writer = ExampleParquetWriter.builder(path).withWriterVersion(PARQUET_1_0).withPageSize(    1024).enableDictionaryEncoding().withDictionaryPageSize(2 * 1024).withConf(CONF).withType(SCHEMA).build();    writeData(writer);    writer.close();    ParquetFileReader reader = ParquetFileReader.open(CONF, path);    assertEquals("Should have one row group", 1, reader.getRowGroups().size());    BlockMetaData rowGroup = reader.getRowGroups().get(0);    ColumnChunkMetaData dictColumn = rowGroup.getColumns().get(0);    EncodingStats dictStats = dictColumn.getEncodingStats();    assertNotNull("Dict column should have non-null encoding stats", dictStats);    assertTrue("Dict column should have a dict page", dictStats.hasDictionaryPages());    assertTrue("Dict column should have dict-encoded pages", dictStats.hasDictionaryEncodedPages());    assertFalse("Dict column should not have non-dict pages", dictStats.hasNonDictionaryEncodedPages());    ColumnChunkMetaData plainColumn = rowGroup.getColumns().get(1);    EncodingStats plainStats = plainColumn.getEncodingStats();    assertNotNull("Plain column should have non-null encoding stats", plainStats);    assertFalse("Plain column should not have a dict page", plainStats.hasDictionaryPages());    assertFalse("Plain column should not have dict-encoded pages", plainStats.hasDictionaryEncodedPages());    assertTrue("Plain column should have non-dict pages", plainStats.hasNonDictionaryEncodedPages());    ColumnChunkMetaData fallbackColumn = rowGroup.getColumns().get(2);    EncodingStats fallbackStats = fallbackColumn.getEncodingStats();    assertNotNull("Fallback column should have non-null encoding stats", fallbackStats);    assertTrue("Fallback column should have a dict page", fallbackStats.hasDictionaryPages());    assertTrue("Fallback column should have dict-encoded pages", fallbackStats.hasDictionaryEncodedPages());    assertTrue("Fallback column should have non-dict pages", fallbackStats.hasNonDictionaryEncodedPages());}
0
public void TestSnappy() throws IOException
{        SnappyCompressor compressor = new SnappyCompressor();    SnappyDecompressor decompressor = new SnappyDecompressor();    TestSnappy(compressor, decompressor, "");    TestSnappy(compressor, decompressor, "FooBar");    TestSnappy(compressor, decompressor, "FooBar1", "FooBar2");    TestSnappy(compressor, decompressor, "FooBar");    TestSnappy(compressor, decompressor, "a", "blahblahblah", "abcdef");    TestSnappy(compressor, decompressor, "");    TestSnappy(compressor, decompressor, "FooBar");}
0
public void TestSnappyStream() throws IOException
{    SnappyCodec codec = new SnappyCodec();    codec.setConf(new Configuration());    int blockSize = 1024;    int inputSize = blockSize * 1024;    byte[] input = new byte[inputSize];    for (int i = 0; i < inputSize; ++i) {        input[i] = (byte) i;    }    ByteArrayOutputStream compressedStream = new ByteArrayOutputStream();    CompressionOutputStream compressor = codec.createOutputStream(compressedStream);    int bytesCompressed = 0;    while (bytesCompressed < inputSize) {        int len = Math.min(inputSize - bytesCompressed, blockSize);        compressor.write(input, bytesCompressed, len);        bytesCompressed += len;    }    compressor.finish();    byte[] rawCompressed = Snappy.compress(input);    byte[] codecCompressed = compressedStream.toByteArray();            assertArrayEquals(rawCompressed, codecCompressed);    ByteArrayInputStream inputStream = new ByteArrayInputStream(codecCompressed);    CompressionInputStream decompressor = codec.createInputStream(inputStream);    byte[] codecDecompressed = new byte[inputSize];    int bytesDecompressed = 0;    int numBytes;    while ((numBytes = decompressor.read(codecDecompressed, bytesDecompressed, blockSize)) != 0) {        bytesDecompressed += numBytes;        if (bytesDecompressed == inputSize)            break;    }    byte[] rawDecompressed = Snappy.uncompress(rawCompressed);    assertArrayEquals(input, rawDecompressed);    assertArrayEquals(input, codecDecompressed);}
0
private void TestSnappy(SnappyCompressor compressor, SnappyDecompressor decompressor, String... strings) throws IOException
{    compressor.reset();    decompressor.reset();    int uncompressedSize = 0;    for (String s : strings) {        uncompressedSize += s.length();    }    byte[] uncompressedData = new byte[uncompressedSize];    int len = 0;    for (String s : strings) {        byte[] tmp = s.getBytes();        System.arraycopy(tmp, 0, uncompressedData, len, s.length());        len += s.length();    }    assert (compressor.needsInput());    compressor.setInput(uncompressedData, 0, len);    assert (compressor.needsInput());    compressor.finish();    assert (!compressor.needsInput());    assert (!compressor.finished() || uncompressedSize == 0);    byte[] compressedData = new byte[1000];    int compressedSize = compressor.compress(compressedData, 0, 1000);    assert (compressor.finished());    assert (!decompressor.finished());    assert (decompressor.needsInput());    decompressor.setInput(compressedData, 0, compressedSize);    assert (!decompressor.finished());    byte[] decompressedData = new byte[uncompressedSize];    int decompressedSize = decompressor.decompress(decompressedData, 0, uncompressedSize);    assert (decompressor.finished());    assertEquals(uncompressedSize, decompressedSize);    assertArrayEquals(uncompressedData, decompressedData);}
0
public static void enforceEmptyDir(Configuration conf, Path path) throws IOException
{    FileSystem fs = path.getFileSystem(conf);    if (fs.exists(path)) {        if (!fs.delete(path, true)) {            throw new IOException("can not delete path " + path);        }    }    if (!fs.mkdirs(path)) {        throw new IOException("can not create path " + path);    }}
0
public static void assertThrows(String message, Class<? extends Exception> expected, Callable callable)
{    try {        callable.call();        Assert.fail("No exception was thrown (" + message + "), expected: " + expected.getName());    } catch (Exception actual) {        try {            Assert.assertEquals(message, expected, actual.getClass());        } catch (AssertionError e) {            e.addSuppressed(actual);            throw e;        }    }}
0
public static void assertStatsValuesEqual(Statistics<?> stats1, Statistics<?> stats2)
{    assertStatsValuesEqual(null, stats1, stats2);}
0
public static void assertStatsValuesEqual(String message, Statistics<?> expected, Statistics<?> actual)
{    if (expected == actual) {        return;    }    if (expected == null || actual == null) {        Assert.assertEquals(expected, actual);    }    Assert.assertThat(actual, CoreMatchers.instanceOf(expected.getClass()));    Assert.assertArrayEquals(message, expected.getMaxBytes(), actual.getMaxBytes());    Assert.assertArrayEquals(message, expected.getMinBytes(), actual.getMinBytes());    Assert.assertEquals(message, expected.getNumNulls(), actual.getNumNulls());}
0
public synchronized int read(byte[] b, int off, int len)
{    if (current < lengths.length) {        if (len <= lengths[current]) {                        int bytesRead = super.read(b, off, len);            lengths[current] -= bytesRead;            return bytesRead;        } else {            int bytesRead = super.read(b, off, lengths[current]);            current += 1;            return bytesRead;        }    } else {        return super.read(b, off, len);    }}
0
public int read(long position, byte[] buffer, int offset, int length) throws IOException
{    seek(position);    return read(buffer, offset, length);}
0
public void readFully(long position, byte[] buffer, int offset, int length) throws IOException
{    throw new UnsupportedOperationException("Not actually supported.");}
0
public void readFully(long position, byte[] buffer) throws IOException
{    throw new UnsupportedOperationException("Not actually supported.");}
0
public void seek(long pos) throws IOException
{    this.pos = (int) pos;}
0
public long getPos() throws IOException
{    return this.pos;}
0
public boolean seekToNewSource(long targetPos) throws IOException
{    seek(targetPos);    return true;}
0
public int read(ByteBuffer buf) throws IOException
{            byte[] temp = new byte[buf.remaining()];    int bytesRead = stream.read(temp, 0, temp.length);    if (bytesRead > 0) {        buf.put(temp, 0, bytesRead);    }    return bytesRead;}
0
public void testHeapReadFullySmallBuffer() throws Exception
{    ByteBuffer readBuffer = ByteBuffer.allocate(8);    FSDataInputStream hadoopStream = new FSDataInputStream(new MockHadoopInputStream());    MockBufferReader reader = new MockBufferReader(hadoopStream);    H2SeekableInputStream.readFully(reader, readBuffer);    Assert.assertEquals(8, readBuffer.position());    Assert.assertEquals(8, readBuffer.limit());    H2SeekableInputStream.readFully(reader, readBuffer);    Assert.assertEquals(8, readBuffer.position());    Assert.assertEquals(8, readBuffer.limit());    readBuffer.flip();    Assert.assertEquals("Buffer contents should match", ByteBuffer.wrap(TEST_ARRAY, 0, 8), readBuffer);}
0
public void testHeapReadFullyLargeBuffer() throws Exception
{    final ByteBuffer readBuffer = ByteBuffer.allocate(20);    FSDataInputStream hadoopStream = new FSDataInputStream(new MockHadoopInputStream());    final MockBufferReader reader = new MockBufferReader(hadoopStream);    TestUtils.assertThrows("Should throw EOFException", EOFException.class, () -> {        H2SeekableInputStream.readFully(reader, readBuffer);        return null;    });                            Assert.assertEquals(10, readBuffer.position());    Assert.assertEquals(20, readBuffer.limit());}
0
public void testHeapReadFullyJustRight() throws Exception
{    ByteBuffer readBuffer = ByteBuffer.allocate(10);    FSDataInputStream hadoopStream = new FSDataInputStream(new MockHadoopInputStream());    MockBufferReader reader = new MockBufferReader(hadoopStream);        H2SeekableInputStream.readFully(reader, readBuffer);    Assert.assertEquals(10, readBuffer.position());    Assert.assertEquals(10, readBuffer.limit());        H2SeekableInputStream.readFully(reader, readBuffer);    Assert.assertEquals(10, readBuffer.position());    Assert.assertEquals(10, readBuffer.limit());    readBuffer.flip();    Assert.assertEquals("Buffer contents should match", ByteBuffer.wrap(TEST_ARRAY), readBuffer);}
0
public void testHeapReadFullySmallReads() throws Exception
{    ByteBuffer readBuffer = ByteBuffer.allocate(10);    FSDataInputStream hadoopStream = new FSDataInputStream(new MockHadoopInputStream(2, 3, 3));    MockBufferReader reader = new MockBufferReader(hadoopStream);    H2SeekableInputStream.readFully(reader, readBuffer);    Assert.assertEquals(10, readBuffer.position());    Assert.assertEquals(10, readBuffer.limit());    H2SeekableInputStream.readFully(reader, readBuffer);    Assert.assertEquals(10, readBuffer.position());    Assert.assertEquals(10, readBuffer.limit());    readBuffer.flip();    Assert.assertEquals("Buffer contents should match", ByteBuffer.wrap(TEST_ARRAY), readBuffer);}
0
public void testHeapReadFullyPosition() throws Exception
{    ByteBuffer readBuffer = ByteBuffer.allocate(10);    readBuffer.position(3);    readBuffer.mark();    FSDataInputStream hadoopStream = new FSDataInputStream(new MockHadoopInputStream(2, 3, 3));    MockBufferReader reader = new MockBufferReader(hadoopStream);    H2SeekableInputStream.readFully(reader, readBuffer);    Assert.assertEquals(10, readBuffer.position());    Assert.assertEquals(10, readBuffer.limit());    H2SeekableInputStream.readFully(reader, readBuffer);    Assert.assertEquals(10, readBuffer.position());    Assert.assertEquals(10, readBuffer.limit());    readBuffer.reset();    Assert.assertEquals("Buffer contents should match", ByteBuffer.wrap(TEST_ARRAY, 0, 7), readBuffer);}
0
public void testHeapReadFullyLimit() throws Exception
{    ByteBuffer readBuffer = ByteBuffer.allocate(10);    readBuffer.limit(7);    FSDataInputStream hadoopStream = new FSDataInputStream(new MockHadoopInputStream(2, 3, 3));    MockBufferReader reader = new MockBufferReader(hadoopStream);    H2SeekableInputStream.readFully(reader, readBuffer);    Assert.assertEquals(7, readBuffer.position());    Assert.assertEquals(7, readBuffer.limit());    H2SeekableInputStream.readFully(reader, readBuffer);    Assert.assertEquals(7, readBuffer.position());    Assert.assertEquals(7, readBuffer.limit());    readBuffer.flip();    Assert.assertEquals("Buffer contents should match", ByteBuffer.wrap(TEST_ARRAY, 0, 7), readBuffer);    readBuffer.position(7);    readBuffer.limit(10);    H2SeekableInputStream.readFully(reader, readBuffer);    Assert.assertEquals(10, readBuffer.position());    Assert.assertEquals(10, readBuffer.limit());    readBuffer.flip();    Assert.assertEquals("Buffer contents should match", ByteBuffer.wrap(TEST_ARRAY), readBuffer);}
0
public void testHeapReadFullyPositionAndLimit() throws Exception
{    ByteBuffer readBuffer = ByteBuffer.allocate(10);    readBuffer.position(3);    readBuffer.limit(7);    readBuffer.mark();    FSDataInputStream hadoopStream = new FSDataInputStream(new MockHadoopInputStream(2, 3, 3));    MockBufferReader reader = new MockBufferReader(hadoopStream);    H2SeekableInputStream.readFully(reader, readBuffer);    Assert.assertEquals(7, readBuffer.position());    Assert.assertEquals(7, readBuffer.limit());    H2SeekableInputStream.readFully(reader, readBuffer);    Assert.assertEquals(7, readBuffer.position());    Assert.assertEquals(7, readBuffer.limit());    readBuffer.reset();    Assert.assertEquals("Buffer contents should match", ByteBuffer.wrap(TEST_ARRAY, 0, 4), readBuffer);    readBuffer.position(7);    readBuffer.limit(10);    H2SeekableInputStream.readFully(reader, readBuffer);    Assert.assertEquals(10, readBuffer.position());    Assert.assertEquals(10, readBuffer.limit());    readBuffer.reset();    Assert.assertEquals("Buffer contents should match", ByteBuffer.wrap(TEST_ARRAY, 0, 7), readBuffer);}
0
public void testDirectReadFullySmallBuffer() throws Exception
{    ByteBuffer readBuffer = ByteBuffer.allocateDirect(8);    FSDataInputStream hadoopStream = new FSDataInputStream(new MockHadoopInputStream());    MockBufferReader reader = new MockBufferReader(hadoopStream);    H2SeekableInputStream.readFully(reader, readBuffer);    Assert.assertEquals(8, readBuffer.position());    Assert.assertEquals(8, readBuffer.limit());    H2SeekableInputStream.readFully(reader, readBuffer);    Assert.assertEquals(8, readBuffer.position());    Assert.assertEquals(8, readBuffer.limit());    readBuffer.flip();    Assert.assertEquals("Buffer contents should match", ByteBuffer.wrap(TEST_ARRAY, 0, 8), readBuffer);}
0
public void testDirectReadFullyLargeBuffer() throws Exception
{    final ByteBuffer readBuffer = ByteBuffer.allocateDirect(20);    FSDataInputStream hadoopStream = new FSDataInputStream(new MockHadoopInputStream());    final MockBufferReader reader = new MockBufferReader(hadoopStream);    TestUtils.assertThrows("Should throw EOFException", EOFException.class, () -> {        H2SeekableInputStream.readFully(reader, readBuffer);        return null;    });                            Assert.assertEquals(10, readBuffer.position());    Assert.assertEquals(20, readBuffer.limit());}
0
public void testDirectReadFullyJustRight() throws Exception
{    ByteBuffer readBuffer = ByteBuffer.allocateDirect(10);    FSDataInputStream hadoopStream = new FSDataInputStream(new MockHadoopInputStream());    MockBufferReader reader = new MockBufferReader(hadoopStream);        H2SeekableInputStream.readFully(reader, readBuffer);    Assert.assertEquals(10, readBuffer.position());    Assert.assertEquals(10, readBuffer.limit());        H2SeekableInputStream.readFully(reader, readBuffer);    Assert.assertEquals(10, readBuffer.position());    Assert.assertEquals(10, readBuffer.limit());    readBuffer.flip();    Assert.assertEquals("Buffer contents should match", ByteBuffer.wrap(TEST_ARRAY), readBuffer);}
0
public void testDirectReadFullySmallReads() throws Exception
{    ByteBuffer readBuffer = ByteBuffer.allocateDirect(10);    FSDataInputStream hadoopStream = new FSDataInputStream(new MockHadoopInputStream(2, 3, 3));    MockBufferReader reader = new MockBufferReader(hadoopStream);    H2SeekableInputStream.readFully(reader, readBuffer);    Assert.assertEquals(10, readBuffer.position());    Assert.assertEquals(10, readBuffer.limit());    H2SeekableInputStream.readFully(reader, readBuffer);    Assert.assertEquals(10, readBuffer.position());    Assert.assertEquals(10, readBuffer.limit());    readBuffer.flip();    Assert.assertEquals("Buffer contents should match", ByteBuffer.wrap(TEST_ARRAY), readBuffer);}
0
public void testDirectReadFullyPosition() throws Exception
{    ByteBuffer readBuffer = ByteBuffer.allocateDirect(10);    readBuffer.position(3);    readBuffer.mark();    FSDataInputStream hadoopStream = new FSDataInputStream(new MockHadoopInputStream(2, 3, 3));    MockBufferReader reader = new MockBufferReader(hadoopStream);    H2SeekableInputStream.readFully(reader, readBuffer);    Assert.assertEquals(10, readBuffer.position());    Assert.assertEquals(10, readBuffer.limit());    H2SeekableInputStream.readFully(reader, readBuffer);    Assert.assertEquals(10, readBuffer.position());    Assert.assertEquals(10, readBuffer.limit());    readBuffer.reset();    Assert.assertEquals("Buffer contents should match", ByteBuffer.wrap(TEST_ARRAY, 0, 7), readBuffer);}
0
public void testDirectReadFullyLimit() throws Exception
{    ByteBuffer readBuffer = ByteBuffer.allocateDirect(10);    readBuffer.limit(7);    FSDataInputStream hadoopStream = new FSDataInputStream(new MockHadoopInputStream(2, 3, 3));    H2SeekableInputStream.Reader reader = new MockBufferReader(hadoopStream);    H2SeekableInputStream.readFully(reader, readBuffer);    Assert.assertEquals(7, readBuffer.position());    Assert.assertEquals(7, readBuffer.limit());    H2SeekableInputStream.readFully(reader, readBuffer);    Assert.assertEquals(7, readBuffer.position());    Assert.assertEquals(7, readBuffer.limit());    readBuffer.flip();    Assert.assertEquals("Buffer contents should match", ByteBuffer.wrap(TEST_ARRAY, 0, 7), readBuffer);    readBuffer.position(7);    readBuffer.limit(10);    H2SeekableInputStream.readFully(reader, readBuffer);    Assert.assertEquals(10, readBuffer.position());    Assert.assertEquals(10, readBuffer.limit());    readBuffer.flip();    Assert.assertEquals("Buffer contents should match", ByteBuffer.wrap(TEST_ARRAY), readBuffer);}
0
public void testDirectReadFullyPositionAndLimit() throws Exception
{    ByteBuffer readBuffer = ByteBuffer.allocateDirect(10);    readBuffer.position(3);    readBuffer.limit(7);    readBuffer.mark();    FSDataInputStream hadoopStream = new FSDataInputStream(new MockHadoopInputStream(2, 3, 3));    MockBufferReader reader = new MockBufferReader(hadoopStream);    H2SeekableInputStream.readFully(reader, readBuffer);    Assert.assertEquals(7, readBuffer.position());    Assert.assertEquals(7, readBuffer.limit());    H2SeekableInputStream.readFully(reader, readBuffer);    Assert.assertEquals(7, readBuffer.position());    Assert.assertEquals(7, readBuffer.limit());    readBuffer.reset();    Assert.assertEquals("Buffer contents should match", ByteBuffer.wrap(TEST_ARRAY, 0, 4), readBuffer);    readBuffer.position(7);    readBuffer.limit(10);    H2SeekableInputStream.readFully(reader, readBuffer);    Assert.assertEquals(10, readBuffer.position());    Assert.assertEquals(10, readBuffer.limit());    readBuffer.reset();    Assert.assertEquals("Buffer contents should match", ByteBuffer.wrap(TEST_ARRAY, 0, 7), readBuffer);}
0
public void testReadWriteObjectToConfAsBase64() throws Exception
{    Map<Integer, String> anObject = new HashMap<Integer, String>();    anObject.put(7, "seven");    anObject.put(8, "eight");    Configuration conf = new Configuration();    SerializationUtil.writeObjectToConfAsBase64("anobject", anObject, conf);    Map<Integer, String> copy = SerializationUtil.readObjectFromConfAsBase64("anobject", conf);    assertEquals(anObject, copy);    try {        Set<String> bad = SerializationUtil.readObjectFromConfAsBase64("anobject", conf);        fail("This should throw a ClassCastException");    } catch (ClassCastException e) {    }    conf = new Configuration();    Object nullObj = null;    SerializationUtil.writeObjectToConfAsBase64("anobject", null, conf);    Object copyObj = SerializationUtil.readObjectFromConfAsBase64("anobject", conf);    assertEquals(nullObj, copyObj);}
0
public void readObjectFromConfAsBase64UnsetKey() throws Exception
{    assertNull(SerializationUtil.readObjectFromConfAsBase64("non-existant-key", new Configuration()));}
0
public boolean shouldGenerateNull()
{    return (random.nextInt(10) == 0);}
0
public int randomInt()
{    return random.nextInt();}
0
public int randomPositiveInt(int maximum)
{        return random.nextInt(Math.abs(maximum) + 1);}
0
public long randomLong()
{    return random.nextLong();}
0
public long randomLong(long maximum)
{    return randomLong() % maximum;}
0
public float randomFloat()
{    return random.nextFloat();}
0
public float randomFloat(float maximum)
{    return random.nextFloat() % maximum;}
0
public double randomDouble()
{    return random.nextDouble();}
0
public double randomDouble(double maximum)
{    return random.nextDouble() % maximum;}
0
public BigInteger randomInt96()
{    return new BigInteger(95, random);}
0
public BigInteger randomInt96(BigInteger maximum)
{    BigInteger result;    while ((result = randomInt96()).compareTo(maximum) > 0) ;    return result;}
0
public char randomLetter()
{    return ALPHABET.charAt(randomPositiveInt(ALPHABET.length() - 1));}
0
public String randomString(int maxLength)
{    return randomFixedLengthString(randomPositiveInt(maxLength));}
0
public String randomFixedLengthString(int length)
{    StringBuilder builder = new StringBuilder();    for (int index = 0; index < length; index++) {        builder.append(randomLetter());    }    return builder.toString();}
0
public Binary asReusedBinary(byte[] data)
{    int length = Math.min(data.length, bufferLength);    System.arraycopy(data, 0, buffer, 0, length);    return Binary.fromReusedByteArray(data, 0, length);}
0
public Integer nextValue()
{    return (minimum + randomPositiveInt(range));}
0
public Integer nextValue()
{    return super.nextValue() & mask;}
0
public Integer nextValue()
{    return randomInt();}
0
public Long nextValue()
{    return (minimum + randomLong(range));}
0
public Long nextValue()
{    return randomLong();}
0
public BigInteger nextValue()
{    return (minimum.add(randomInt96(range)));}
0
public Binary nextBinaryValue()
{    return asReusedBinary(nextValue().toByteArray());}
0
public Float nextValue()
{    return (minimum + randomFloat(range));}
0
public Float nextValue()
{    return randomFloat();}
0
public Double nextValue()
{    return (minimum + randomDouble(range));}
0
public Double nextValue()
{    return randomDouble();}
0
public String nextValue()
{    int stringLength = randomPositiveInt(15) + 1;    return randomString(stringLength);}
0
public Binary nextBinaryValue()
{    return asReusedBinary(nextValue().getBytes());}
0
public Binary nextValue()
{        int length = 5 + randomPositiveInt(buffer.length - 5);    for (int index = 0; index < length; index++) {        buffer[index] = (byte) randomInt();    }    return Binary.fromReusedByteArray(buffer, 0, length);}
0
public Binary nextBinaryValue()
{    return nextValue();}
0
public Binary nextValue()
{    for (int index = 0; index < buffer.length; index++) {        buffer[index] = (byte) randomInt();    }    return Binary.fromReusedByteArray(buffer);}
0
public Binary nextBinaryValue()
{    return nextValue();}
0
public T minimum()
{    return this.minimum;}
0
public T maximum()
{    return this.maximum;}
0
public static void writeAndTest(WriteContext context) throws IOException
{        Configuration configuration = new Configuration();    GroupWriteSupport.setSchema(context.schema, configuration);    GroupWriteSupport groupWriteSupport = new GroupWriteSupport();        final int blockSize = context.blockSize;    final int pageSize = context.pageSize;    final int dictionaryPageSize = pageSize;    final boolean enableDictionary = context.enableDictionary;    final boolean enableValidation = context.enableValidation;    ParquetProperties.WriterVersion writerVersion = context.version;    CompressionCodecName codec = CompressionCodecName.UNCOMPRESSED;    ParquetWriter<Group> writer = new ParquetWriter<Group>(context.fsPath, groupWriteSupport, codec, blockSize, pageSize, dictionaryPageSize, enableDictionary, enableValidation, writerVersion, configuration);    context.write(writer);    writer.close();    context.test();    context.path.delete();}
0
public DictionaryPage readDictionaryPage()
{    return dict;}
0
public long getTotalValueCount()
{    return data.getValueCount();}
0
public DataPage readPage()
{    return data;}
0
private static Statistics<T> getStatisticsFromPageHeader(DataPage page)
{    return page.accept(new DataPage.Visitor<Statistics<T>>() {        @Override        @SuppressWarnings("unchecked")        public Statistics<T> visit(DataPageV1 dataPageV1) {            return (Statistics<T>) dataPageV1.getStatistics();        }        @Override        @SuppressWarnings("unchecked")        public Statistics<T> visit(DataPageV2 dataPageV2) {            return (Statistics<T>) dataPageV2.getStatistics();        }    });}
0
public Statistics<T> visit(DataPageV1 dataPageV1)
{    return (Statistics<T>) dataPageV1.getStatistics();}
0
public Statistics<T> visit(DataPageV2 dataPageV2)
{    return (Statistics<T>) dataPageV2.getStatistics();}
0
public void validate(T value)
{    if (hasNonNull) {        assertTrue("min should be <= all values", comparator.compare(min, value) <= 0);        assertTrue("min should be >= all values", comparator.compare(max, value) >= 0);    }}
0
private static PrimitiveConverter getValidatingConverter(final DataPage page, PrimitiveTypeName type)
{    return type.convert(new PrimitiveType.PrimitiveTypeNameConverter<PrimitiveConverter, RuntimeException>() {        @Override        public PrimitiveConverter convertFLOAT(PrimitiveTypeName primitiveTypeName) {            final StatsValidator<Float> validator = new StatsValidator<Float>(page);            return new PrimitiveConverter() {                @Override                public void addFloat(float value) {                    validator.validate(value);                }            };        }        @Override        public PrimitiveConverter convertDOUBLE(PrimitiveTypeName primitiveTypeName) {            final StatsValidator<Double> validator = new StatsValidator<Double>(page);            return new PrimitiveConverter() {                @Override                public void addDouble(double value) {                    validator.validate(value);                }            };        }        @Override        public PrimitiveConverter convertINT32(PrimitiveTypeName primitiveTypeName) {            final StatsValidator<Integer> validator = new StatsValidator<Integer>(page);            return new PrimitiveConverter() {                @Override                public void addInt(int value) {                    validator.validate(value);                }            };        }        @Override        public PrimitiveConverter convertINT64(PrimitiveTypeName primitiveTypeName) {            final StatsValidator<Long> validator = new StatsValidator<Long>(page);            return new PrimitiveConverter() {                @Override                public void addLong(long value) {                    validator.validate(value);                }            };        }        @Override        public PrimitiveConverter convertBOOLEAN(PrimitiveTypeName primitiveTypeName) {            final StatsValidator<Boolean> validator = new StatsValidator<Boolean>(page);            return new PrimitiveConverter() {                @Override                public void addBoolean(boolean value) {                    validator.validate(value);                }            };        }        @Override        public PrimitiveConverter convertINT96(PrimitiveTypeName primitiveTypeName) {            return convertBINARY(primitiveTypeName);        }        @Override        public PrimitiveConverter convertFIXED_LEN_BYTE_ARRAY(PrimitiveTypeName primitiveTypeName) {            return convertBINARY(primitiveTypeName);        }        @Override        public PrimitiveConverter convertBINARY(PrimitiveTypeName primitiveTypeName) {            final StatsValidator<Binary> validator = new StatsValidator<Binary>(page);            return new PrimitiveConverter() {                @Override                public void addBinary(Binary value) {                    validator.validate(value);                }            };        }    });}
0
public PrimitiveConverter convertFLOAT(PrimitiveTypeName primitiveTypeName)
{    final StatsValidator<Float> validator = new StatsValidator<Float>(page);    return new PrimitiveConverter() {        @Override        public void addFloat(float value) {            validator.validate(value);        }    };}
0
public void addFloat(float value)
{    validator.validate(value);}
0
public PrimitiveConverter convertDOUBLE(PrimitiveTypeName primitiveTypeName)
{    final StatsValidator<Double> validator = new StatsValidator<Double>(page);    return new PrimitiveConverter() {        @Override        public void addDouble(double value) {            validator.validate(value);        }    };}
0
public void addDouble(double value)
{    validator.validate(value);}
0
public PrimitiveConverter convertINT32(PrimitiveTypeName primitiveTypeName)
{    final StatsValidator<Integer> validator = new StatsValidator<Integer>(page);    return new PrimitiveConverter() {        @Override        public void addInt(int value) {            validator.validate(value);        }    };}
0
public void addInt(int value)
{    validator.validate(value);}
0
public PrimitiveConverter convertINT64(PrimitiveTypeName primitiveTypeName)
{    final StatsValidator<Long> validator = new StatsValidator<Long>(page);    return new PrimitiveConverter() {        @Override        public void addLong(long value) {            validator.validate(value);        }    };}
0
public void addLong(long value)
{    validator.validate(value);}
0
public PrimitiveConverter convertBOOLEAN(PrimitiveTypeName primitiveTypeName)
{    final StatsValidator<Boolean> validator = new StatsValidator<Boolean>(page);    return new PrimitiveConverter() {        @Override        public void addBoolean(boolean value) {            validator.validate(value);        }    };}
0
public void addBoolean(boolean value)
{    validator.validate(value);}
0
public PrimitiveConverter convertINT96(PrimitiveTypeName primitiveTypeName)
{    return convertBINARY(primitiveTypeName);}
0
public PrimitiveConverter convertFIXED_LEN_BYTE_ARRAY(PrimitiveTypeName primitiveTypeName)
{    return convertBINARY(primitiveTypeName);}
0
public PrimitiveConverter convertBINARY(PrimitiveTypeName primitiveTypeName)
{    final StatsValidator<Binary> validator = new StatsValidator<Binary>(page);    return new PrimitiveConverter() {        @Override        public void addBinary(Binary value) {            validator.validate(value);        }    };}
0
public void addBinary(Binary value)
{    validator.validate(value);}
0
public void validate(MessageType schema, PageReadStore store)
{    for (ColumnDescriptor desc : schema.getColumns()) {        PageReader reader = store.getPageReader(desc);        DictionaryPage dict = reader.readDictionaryPage();        DataPage page;        while ((page = reader.readPage()) != null) {            validateStatsForPage(page, dict, desc);        }    }}
0
private void validateStatsForPage(DataPage page, DictionaryPage dict, ColumnDescriptor desc)
{    SingletonPageReader reader = new SingletonPageReader(dict, page);    PrimitiveConverter converter = getValidatingConverter(page, desc.getType());    Statistics<?> stats = getStatisticsFromPageHeader(page);    assertEquals("Statistics does not use the proper comparator", desc.getPrimitiveType().comparator().getClass(), stats.comparator().getClass());    if (stats.isEmpty()) {                        System.err.println(String.format("No stats written for page=%s col=%s", page, Arrays.toString(desc.getPath())));        return;    }    long numNulls = 0;    ColumnReaderImpl column = new ColumnReaderImpl(desc, reader, converter, null);    for (int i = 0; i < reader.getTotalValueCount(); i += 1) {        if (column.getCurrentDefinitionLevel() >= desc.getMaxDefinitionLevel()) {            column.writeCurrentValueToConverter();        } else {            numNulls += 1;        }        column.consume();    }    Assert.assertEquals(numNulls, stats.getNumNulls());    System.err.println(String.format("Validated stats min=%s max=%s nulls=%d for page=%s col=%s", stats.minAsString(), stats.maxAsString(), stats.getNumNulls(), page, Arrays.toString(desc.getPath())));}
0
private static MessageType buildSchema(long seed)
{    Random random = new Random(seed);    int fixedBinaryLength = random.nextInt(21) + 1;    int fixedPrecision = calculatePrecision(fixedBinaryLength);    int fixedScale = fixedPrecision / 4;    int binaryPrecision = calculatePrecision(16);    int binaryScale = binaryPrecision / 4;    return new MessageType("schema", new PrimitiveType(OPTIONAL, INT32, "i32"), new PrimitiveType(OPTIONAL, INT64, "i64"), new PrimitiveType(OPTIONAL, INT96, "i96"), new PrimitiveType(OPTIONAL, FLOAT, "sngl"), new PrimitiveType(OPTIONAL, DOUBLE, "dbl"), new PrimitiveType(OPTIONAL, BINARY, "strings"), new PrimitiveType(OPTIONAL, BINARY, "binary"), new PrimitiveType(OPTIONAL, FIXED_LEN_BYTE_ARRAY, fixedBinaryLength, "fixed-binary"), new PrimitiveType(REQUIRED, INT32, "unconstrained-i32"), new PrimitiveType(REQUIRED, INT64, "unconstrained-i64"), new PrimitiveType(REQUIRED, FLOAT, "unconstrained-sngl"), new PrimitiveType(REQUIRED, DOUBLE, "unconstrained-dbl"), Types.optional(INT32).as(OriginalType.INT_8).named("int8"), Types.optional(INT32).as(OriginalType.UINT_8).named("uint8"), Types.optional(INT32).as(OriginalType.INT_16).named("int16"), Types.optional(INT32).as(OriginalType.UINT_16).named("uint16"), Types.optional(INT32).as(OriginalType.INT_32).named("int32"), Types.optional(INT32).as(OriginalType.UINT_32).named("uint32"), Types.optional(INT64).as(OriginalType.INT_64).named("int64"), Types.optional(INT64).as(OriginalType.UINT_64).named("uint64"), Types.optional(INT32).as(OriginalType.DECIMAL).precision(9).scale(2).named("decimal-int32"), Types.optional(INT64).as(OriginalType.DECIMAL).precision(18).scale(4).named("decimal-int64"), Types.optional(FIXED_LEN_BYTE_ARRAY).length(fixedBinaryLength).as(OriginalType.DECIMAL).precision(fixedPrecision).scale(fixedScale).named("decimal-fixed"), Types.optional(BINARY).as(OriginalType.DECIMAL).precision(binaryPrecision).scale(binaryScale).named("decimal-binary"), Types.optional(BINARY).as(OriginalType.UTF8).named("utf8"), Types.optional(BINARY).as(OriginalType.ENUM).named("enum"), Types.optional(BINARY).as(OriginalType.JSON).named("json"), Types.optional(BINARY).as(OriginalType.BSON).named("bson"), Types.optional(INT32).as(OriginalType.DATE).named("date"), Types.optional(INT32).as(OriginalType.TIME_MILLIS).named("time-millis"), Types.optional(INT64).as(OriginalType.TIME_MICROS).named("time-micros"), Types.optional(INT64).as(OriginalType.TIMESTAMP_MILLIS).named("timestamp-millis"), Types.optional(INT64).as(OriginalType.TIMESTAMP_MICROS).named("timestamp-micros"), Types.optional(FIXED_LEN_BYTE_ARRAY).length(12).as(OriginalType.INTERVAL).named("interval"));}
0
private static int calculatePrecision(int byteCnt)
{    String maxValue = BigInteger.valueOf(2L).pow(8 * byteCnt - 1).toString();    return maxValue.length() - 1;}
0
public void write(ParquetWriter<Group> writer) throws IOException
{    for (int index = 0; index < recordCount; index++) {        Group group = new SimpleGroup(super.schema);        for (int column = 0, columnCnt = schema.getFieldCount(); column < columnCnt; ++column) {            Type type = schema.getType(column);            RandomValueGenerator<?> generator = randomGenerators.get(column);            if (type.isRepetition(OPTIONAL) && generator.shouldGenerateNull()) {                continue;            }            switch(type.asPrimitiveType().getPrimitiveTypeName()) {                case BINARY:                case FIXED_LEN_BYTE_ARRAY:                case INT96:                    group.append(type.getName(), ((RandomBinaryBase<?>) generator).nextBinaryValue());                    break;                case INT32:                    group.append(type.getName(), (Integer) generator.nextValue());                    break;                case INT64:                    group.append(type.getName(), (Long) generator.nextValue());                    break;                case FLOAT:                    group.append(type.getName(), (Float) generator.nextValue());                    break;                case DOUBLE:                    group.append(type.getName(), (Double) generator.nextValue());                    break;                case BOOLEAN:                    group.append(type.getName(), (Boolean) generator.nextValue());                    break;            }        }        writer.write(group);    }}
0
public void test() throws IOException
{    Configuration configuration = new Configuration();    ParquetMetadata metadata = ParquetFileReader.readFooter(configuration, super.fsPath, ParquetMetadataConverter.NO_FILTER);    ParquetFileReader reader = new ParquetFileReader(configuration, metadata.getFileMetaData(), super.fsPath, metadata.getBlocks(), metadata.getFileMetaData().getSchema().getColumns());    PageStatsValidator validator = new PageStatsValidator();    PageReadStore pageReadStore;    while ((pageReadStore = reader.readNextRowGroup()) != null) {        validator.validate(metadata.getFileMetaData().getSchema(), pageReadStore);    }}
0
public void testStatistics() throws IOException
{    File file = folder.newFile("test_file.parquet");    file.delete();    System.out.println(String.format("RANDOM SEED: %s", RANDOM_SEED));    Random random = new Random(RANDOM_SEED);    int blockSize = (random.nextInt(54) + 10) * MEGABYTE;    int pageSize = (random.nextInt(10) + 1) * MEGABYTE;    List<DataContext> contexts = Arrays.asList(new DataContext(random.nextLong(), file, blockSize, pageSize, false, ParquetProperties.WriterVersion.PARQUET_1_0), new DataContext(random.nextLong(), file, blockSize, pageSize, true, ParquetProperties.WriterVersion.PARQUET_1_0), new DataContext(random.nextLong(), file, blockSize, pageSize, false, ParquetProperties.WriterVersion.PARQUET_2_0), new DataContext(random.nextLong(), file, blockSize, pageSize, true, ParquetProperties.WriterVersion.PARQUET_2_0));    for (DataContext test : contexts) {        DataGenerationContext.writeAndTest(test);    }}
0
private void init(final JobConf job)
{    final String plan = HiveConf.getVar(job, HiveConf.ConfVars.PLAN);    if (mrwork == null && plan != null && plan.length() > 0) {        mrwork = Utilities.getMapRedWork(job);        pathToPartitionInfo.clear();        for (final Map.Entry<String, PartitionDesc> entry : mrwork.getPathToPartitionInfo().entrySet()) {            pathToPartitionInfo.put(new Path(entry.getKey()).toUri().getPath().toString(), entry.getValue());        }    }}
0
private void pushProjectionsAndFilters(final JobConf jobConf, final String splitPath, final String splitPathWithNoSchema)
{    if (mrwork == null) {                return;    } else if (mrwork.getPathToAliases() == null) {                return;    }    final ArrayList<String> aliases = new ArrayList<String>();    final Iterator<Entry<String, ArrayList<String>>> iterator = mrwork.getPathToAliases().entrySet().iterator();    while (iterator.hasNext()) {        final Entry<String, ArrayList<String>> entry = iterator.next();        final String key = new Path(entry.getKey()).toUri().getPath();        if (splitPath.equals(key) || splitPathWithNoSchema.equals(key)) {            final ArrayList<String> list = entry.getValue();            for (final String val : list) {                aliases.add(val);            }        }    }    for (final String alias : aliases) {        final Operator<? extends Serializable> op = mrwork.getAliasToWork().get(alias);        if (op != null && op instanceof TableScanOperator) {            final TableScanOperator tableScan = (TableScanOperator) op;                        final ArrayList<Integer> list = tableScan.getNeededColumnIDs();            if (list != null) {                ColumnProjectionUtils.appendReadColumnIDs(jobConf, list);            } else {                ColumnProjectionUtils.setFullyReadColumns(jobConf);            }            pushFilters(jobConf, tableScan);        }    }}
1
private void pushFilters(final JobConf jobConf, final TableScanOperator tableScan)
{    final TableScanDesc scanDesc = tableScan.getConf();    if (scanDesc == null) {                return;    }        Utilities.setColumnNameList(jobConf, tableScan);        final ExprNodeDesc filterExpr = scanDesc.getFilterExpr();    if (filterExpr == null) {                return;    }    final String filterText = filterExpr.getExprString();    final String filterExprSerialized = Utilities.serializeExpression(filterExpr);    jobConf.set(TableScanDesc.FILTER_TEXT_CONF_STR, filterText);    jobConf.set(TableScanDesc.FILTER_EXPR_CONF_STR, filterExprSerialized);}
1
public JobConf pushProjectionsAndFilters(JobConf jobConf, Path path) throws IOException
{    init(jobConf);    final JobConf cloneJobConf = new JobConf(jobConf);    final PartitionDesc part = pathToPartitionInfo.get(path.toString());    if ((part != null) && (part.getTableDesc() != null)) {        Utilities.copyTableJobPropertiesToConf(part.getTableDesc(), cloneJobConf);    }    pushProjectionsAndFilters(cloneJobConf, path.toString(), path.toUri().toString());    return cloneJobConf;}
0
private void init(final JobConf job)
{    final String plan = HiveConf.getVar(job, HiveConf.ConfVars.PLAN);    if (mapWork == null && plan != null && plan.length() > 0) {        mapWork = Utilities.getMapWork(job);        pathToPartitionInfo.clear();        for (final Map.Entry<String, PartitionDesc> entry : mapWork.getPathToPartitionInfo().entrySet()) {            pathToPartitionInfo.put(new Path(entry.getKey()).toUri().getPath().toString(), entry.getValue());        }    }}
0
private void pushProjectionsAndFilters(final JobConf jobConf, final String splitPath, final String splitPathWithNoSchema)
{    if (mapWork == null) {                return;    } else if (mapWork.getPathToAliases() == null) {                return;    }    final ArrayList<String> aliases = new ArrayList<String>();    final Iterator<Entry<String, ArrayList<String>>> iterator = mapWork.getPathToAliases().entrySet().iterator();    while (iterator.hasNext()) {        final Entry<String, ArrayList<String>> entry = iterator.next();        final String key = new Path(entry.getKey()).toUri().getPath();        if (splitPath.equals(key) || splitPathWithNoSchema.equals(key)) {            final ArrayList<String> list = entry.getValue();            for (final String val : list) {                aliases.add(val);            }        }    }    for (final String alias : aliases) {        final Operator<? extends Serializable> op = mapWork.getAliasToWork().get(alias);        if (op != null && op instanceof TableScanOperator) {            final TableScanOperator tableScan = (TableScanOperator) op;                        final ArrayList<Integer> list = tableScan.getNeededColumnIDs();            if (list != null) {                ColumnProjectionUtils.appendReadColumnIDs(jobConf, list);            } else {                ColumnProjectionUtils.setFullyReadColumns(jobConf);            }            pushFilters(jobConf, tableScan);        }    }}
1
private void pushFilters(final JobConf jobConf, final TableScanOperator tableScan)
{    final TableScanDesc scanDesc = tableScan.getConf();    if (scanDesc == null) {                return;    }        Utilities.setColumnNameList(jobConf, tableScan);        final ExprNodeDesc filterExpr = scanDesc.getFilterExpr();    if (filterExpr == null) {                return;    }    final String filterText = filterExpr.getExprString();    final String filterExprSerialized = Utilities.serializeExpression(filterExpr);    jobConf.set(TableScanDesc.FILTER_TEXT_CONF_STR, filterText);    jobConf.set(TableScanDesc.FILTER_EXPR_CONF_STR, filterExprSerialized);}
1
public JobConf pushProjectionsAndFilters(JobConf jobConf, Path path) throws IOException
{    init(jobConf);    final JobConf cloneJobConf = new JobConf(jobConf);    final PartitionDesc part = pathToPartitionInfo.get(path.toString());    if ((part != null) && (part.getTableDesc() != null)) {        Utilities.copyTableJobPropertiesToConf(part.getTableDesc(), cloneJobConf);    }    pushProjectionsAndFilters(cloneJobConf, path.toString(), path.toUri().toString());    return cloneJobConf;}
0
public HiveBinding create()
{    Class<? extends HiveBinding> bindingClazz = create(HiveBindingFactory.class.getClassLoader());    try {        return bindingClazz.newInstance();    } catch (Exception e) {        throw new HiveBindingInstantiationError("Unexpected error creating instance" + " of " + bindingClazz.getCanonicalName(), e);    }}
0
 Class<? extends HiveBinding> create(ClassLoader classLoader)
{            Class hiveVersionInfo;    try {        hiveVersionInfo = Class.forName(HIVE_VERSION_CLASS_NAME, true, classLoader);    } catch (ClassNotFoundException e) {                return Hive010Binding.class;    }    return createInternal(hiveVersionInfo);}
1
 Class<? extends HiveBinding> createInternal(Class hiveVersionInfo)
{    String hiveVersion;    try {        Method getVersionMethod = hiveVersionInfo.getMethod(HIVE_VERSION_METHOD_NAME, (Class[]) null);        String rawVersion = (String) getVersionMethod.invoke(null, (Object[]) null);                hiveVersion = trimVersion(rawVersion);    } catch (Exception e) {        throw new UnexpectedHiveVersionProviderError("Unexpected error whilst " + "determining Hive version", e);    }    if (hiveVersion.equalsIgnoreCase(HIVE_VERSION_UNKNOWN)) {                return createBindingForUnknownVersion();    }    if (hiveVersion.startsWith(HIVE_VERSION_010)) {                return Hive010Binding.class;    } else if (hiveVersion.startsWith(HIVE_VERSION_011)) {                return Hive010Binding.class;    } else if (hiveVersion.startsWith(HIVE_VERSION_013)) {        throw new HiveBindingInstantiationError("Hive 0.13 contains native Parquet support " + "and the parquet-hive jars from the parquet project should not be included " + "in Hive's classpath.");    }            return Hive012Binding.class;}
1
private Class<? extends HiveBinding> createBindingForUnknownVersion()
{    try {        Class<?> utilitiesClass = Class.forName(HIVE_UTILITIES_CLASS_NAME);        for (Method method : utilitiesClass.getDeclaredMethods()) {            if (HIVE_012_INDICATOR_UTILITIES_GETMAPWORK.equals(method.getName())) {                                return Hive012Binding.class;            }        }                return Hive010Binding.class;    } catch (ClassNotFoundException e) {                return LATEST_BINDING;    }}
1
private static String trimVersion(String s)
{    if (s == null) {        return HIVE_VERSION_NULL;    }    return s.trim();}
0
public void setup()
{    hiveBindingFactory = new HiveBindingFactory();}
0
public void testMissingHiveVersionInfoClass()
{    Assert.assertEquals(Hive010Binding.class, hiveBindingFactory.create(new NoopClassLoader()));}
0
public void testNoHiveVersion()
{    hiveBindingFactory.createInternal(NoHiveVersion.class);}
0
public void testBlankHiveVersion()
{    hiveBindingFactory.createInternal(BlankHiveVersion.class);    Assert.assertEquals(Hive012Binding.class, hiveBindingFactory.createInternal(BlankHiveVersion.class));}
0
public void testUnknownHiveVersion()
{    hiveBindingFactory.createInternal(BlankHiveVersion.class);        Assert.assertEquals(Hive012Binding.class, hiveBindingFactory.createInternal(BlankHiveVersion.class));}
0
public void testNullHiveVersion()
{    hiveBindingFactory.createInternal(NullHiveVersion.class);    Assert.assertEquals(Hive012Binding.class, hiveBindingFactory.createInternal(NullHiveVersion.class));}
0
public void testHive010()
{    Assert.assertEquals(Hive010Binding.class, hiveBindingFactory.createInternal(Hive010Version.class));}
0
public void testHive010WithSpaces()
{    Assert.assertEquals(Hive010Binding.class, hiveBindingFactory.createInternal(Hive010VersionWithSpaces.class));}
0
public void testHive011()
{    Assert.assertEquals(Hive010Binding.class, hiveBindingFactory.createInternal(Hive011Version.class));}
0
public void testHive012()
{    Assert.assertEquals(Hive012Binding.class, hiveBindingFactory.createInternal(Hive012Version.class));}
0
public void testHive013()
{    hiveBindingFactory.createInternal(Hive013Version.class);}
0
public Class<?> loadClass(String name) throws ClassNotFoundException
{    throw new ClassNotFoundException(name);}
0
public static String getVersion()
{    return "";}
0
public static String getVersion()
{    return HiveBindingFactory.HIVE_VERSION_UNKNOWN;}
0
public static String getVersion()
{    return null;}
0
public static String getVersion()
{    return HiveBindingFactory.HIVE_VERSION_010;}
0
public static String getVersion()
{    return " " + HiveBindingFactory.HIVE_VERSION_010 + " ";}
0
public static String getVersion()
{    return HiveBindingFactory.HIVE_VERSION_011;}
0
public static String getVersion()
{    return HiveBindingFactory.HIVE_VERSION_012;}
0
public static String getVersion()
{    return HiveBindingFactory.HIVE_VERSION_013;}
0
public List<String> getColumns(final String columns)
{    final List<String> result = (List<String>) StringUtils.getStringCollection(columns);    result.removeAll(virtualColumns);    return result;}
0
public Converter getConverter(final int fieldIndex)
{    return converters[fieldIndex];}
0
public void start()
{    if (isMap) {        mapPairContainer = new Writable[2];    }}
0
public void end()
{    if (isMap) {        currentValue = new ArrayWritable(Writable.class, mapPairContainer);    }    parent.add(index, currentValue);}
0
protected void set(final int index, final Writable value)
{    if (index != 0 && mapPairContainer == null || index > 1) {        throw new ParquetDecodingException("Repeated group can only have one or two fields for maps." + " Not allowed to set for the index : " + index);    }    if (isMap) {        mapPairContainer[index] = value;    } else {        currentValue = value;    }}
0
protected void add(final int index, final Writable value)
{    set(index, value);}
0
public final ArrayWritable getCurrentArray()
{    final Writable[] writableArr;    if (this.rootMap != null) {                writableArr = this.rootMap;    } else {        writableArr = new Writable[currentArr.length];    }    for (int i = 0; i < currentArr.length; i++) {        final Object obj = currentArr[i];        if (obj instanceof List) {            final List<?> objList = (List<?>) obj;            final ArrayWritable arr = new ArrayWritable(Writable.class, objList.toArray(new Writable[objList.size()]));            writableArr[i] = arr;        } else {            writableArr[i] = (Writable) obj;        }    }    return new ArrayWritable(Writable.class, writableArr);}
0
protected final void set(final int index, final Writable value)
{    currentArr[index] = value;}
0
public Converter getConverter(final int fieldIndex)
{    return converters[fieldIndex];}
0
public void start()
{    for (int i = 0; i < currentArr.length; i++) {        currentArr[i] = null;    }}
0
public void end()
{    if (parent != null) {        parent.set(index, getCurrentArray());    }}
0
protected void add(final int index, final Writable value)
{    if (currentArr[index] != null) {        final Object obj = currentArr[index];        if (obj instanceof List) {            final List<Writable> list = (List<Writable>) obj;            list.add(value);        } else {            throw new IllegalStateException("This should be a List: " + obj);        }    } else {                                        final List<Writable> buffer = new ArrayList<Writable>();        buffer.add(value);        currentArr[index] = (Object) buffer;    }}
0
public ArrayWritable getCurrentRecord()
{    return root.getCurrentArray();}
0
public GroupConverter getRootConverter()
{    return root;}
0
private Class<?> getType()
{    return _type;}
0
public static Converter getNewConverter(final Class<?> type, final int index, final HiveGroupConverter parent)
{    for (final ETypeConverter eConverter : values()) {        if (eConverter.getType() == type) {            return eConverter.getConverter(type, index, parent);        }    }    throw new IllegalArgumentException("Converter not found ... for type : " + type);}
0
 Converter getConverter(final Class<?> type, final int index, final HiveGroupConverter parent)
{    return new PrimitiveConverter() {        @Override        public void addDouble(final double value) {            parent.set(index, new DoubleWritable(value));        }    };}
0
public void addDouble(final double value)
{    parent.set(index, new DoubleWritable(value));}
0
 Converter getConverter(final Class<?> type, final int index, final HiveGroupConverter parent)
{    return new PrimitiveConverter() {        @Override        public void addBoolean(final boolean value) {            parent.set(index, new BooleanWritable(value));        }    };}
0
public void addBoolean(final boolean value)
{    parent.set(index, new BooleanWritable(value));}
0
 Converter getConverter(final Class<?> type, final int index, final HiveGroupConverter parent)
{    return new PrimitiveConverter() {        @Override        public void addFloat(final float value) {            parent.set(index, new FloatWritable(value));        }    };}
0
public void addFloat(final float value)
{    parent.set(index, new FloatWritable(value));}
0
 Converter getConverter(final Class<?> type, final int index, final HiveGroupConverter parent)
{    return new PrimitiveConverter() {        @Override        public void addInt(final int value) {            parent.set(index, new IntWritable(value));        }    };}
0
public void addInt(final int value)
{    parent.set(index, new IntWritable(value));}
0
 Converter getConverter(final Class<?> type, final int index, final HiveGroupConverter parent)
{    return new PrimitiveConverter() {        @Override        public void addLong(final long value) {            parent.set(index, new LongWritable(value));        }    };}
0
public void addLong(final long value)
{    parent.set(index, new LongWritable(value));}
0
 Converter getConverter(final Class<?> type, final int index, final HiveGroupConverter parent)
{    return new PrimitiveConverter() {                @Override        public void addDouble(final double value) {            parent.set(index, new DoubleWritable(value));        }    };}
0
public void addDouble(final double value)
{    parent.set(index, new DoubleWritable(value));}
0
 Converter getConverter(final Class<?> type, final int index, final HiveGroupConverter parent)
{    return new PrimitiveConverter() {        private Binary[] dictBinary;        private String[] dict;        @Override        public boolean hasDictionarySupport() {            return true;        }        @Override        public void setDictionary(Dictionary dictionary) {            dictBinary = new Binary[dictionary.getMaxId() + 1];            dict = new String[dictionary.getMaxId() + 1];            for (int i = 0; i <= dictionary.getMaxId(); i++) {                Binary binary = dictionary.decodeToBinary(i);                dictBinary[i] = binary;                dict[i] = binary.toStringUsingUTF8();            }        }        @Override        public void addValueFromDictionary(int dictionaryId) {            parent.set(index, new DicBinaryWritable(dictBinary[dictionaryId], dict[dictionaryId]));        }        @Override        public void addBinary(Binary value) {            parent.set(index, new BinaryWritable(value));        }    };}
0
public boolean hasDictionarySupport()
{    return true;}
0
public void setDictionary(Dictionary dictionary)
{    dictBinary = new Binary[dictionary.getMaxId() + 1];    dict = new String[dictionary.getMaxId() + 1];    for (int i = 0; i <= dictionary.getMaxId(); i++) {        Binary binary = dictionary.decodeToBinary(i);        dictBinary[i] = binary;        dict[i] = binary.toStringUsingUTF8();    }}
0
public void addValueFromDictionary(int dictionaryId)
{    parent.set(index, new DicBinaryWritable(dictBinary[dictionaryId], dict[dictionaryId]));}
0
public void addBinary(Binary value)
{    parent.set(index, new BinaryWritable(value));}
0
protected static Converter getConverterFromDescription(final Type type, final int index, final HiveGroupConverter parent)
{    if (type == null) {        return null;    }    if (type.isPrimitive()) {        return ETypeConverter.getNewConverter(type.asPrimitiveType().getPrimitiveTypeName().javaType, index, parent);    } else {        if (type.asGroupType().getRepetition() == Repetition.REPEATED) {            return new ArrayWritableGroupConverter(type.asGroupType(), parent, index);        } else {            return new DataWritableGroupConverter(type.asGroupType(), parent, index);        }    }}
0
public static MessageType convert(final List<String> columnNames, final List<TypeInfo> columnTypes)
{    final MessageType schema = new MessageType("hive_schema", convertTypes(columnNames, columnTypes));    return schema;}
0
private static Type[] convertTypes(final List<String> columnNames, final List<TypeInfo> columnTypes)
{    if (columnNames.size() != columnTypes.size()) {        throw new IllegalStateException("Mismatched Hive columns and types. Hive columns names" + " found : " + columnNames + " . And Hive types found : " + columnTypes);    }    final Type[] types = new Type[columnNames.size()];    for (int i = 0; i < columnNames.size(); ++i) {        types[i] = convertType(columnNames.get(i), columnTypes.get(i));    }    return types;}
0
private static Type convertType(final String name, final TypeInfo typeInfo)
{    return convertType(name, typeInfo, Repetition.OPTIONAL);}
0
private static Type convertType(final String name, final TypeInfo typeInfo, final Repetition repetition)
{    if (typeInfo.getCategory().equals(Category.PRIMITIVE)) {        if (typeInfo.equals(TypeInfoFactory.stringTypeInfo)) {            return new PrimitiveType(repetition, PrimitiveTypeName.BINARY, name);        } else if (typeInfo.equals(TypeInfoFactory.intTypeInfo) || typeInfo.equals(TypeInfoFactory.shortTypeInfo) || typeInfo.equals(TypeInfoFactory.byteTypeInfo)) {            return new PrimitiveType(repetition, PrimitiveTypeName.INT32, name);        } else if (typeInfo.equals(TypeInfoFactory.longTypeInfo)) {            return new PrimitiveType(repetition, PrimitiveTypeName.INT64, name);        } else if (typeInfo.equals(TypeInfoFactory.doubleTypeInfo)) {            return new PrimitiveType(repetition, PrimitiveTypeName.DOUBLE, name);        } else if (typeInfo.equals(TypeInfoFactory.floatTypeInfo)) {            return new PrimitiveType(repetition, PrimitiveTypeName.FLOAT, name);        } else if (typeInfo.equals(TypeInfoFactory.booleanTypeInfo)) {            return new PrimitiveType(repetition, PrimitiveTypeName.BOOLEAN, name);        } else if (typeInfo.equals(TypeInfoFactory.binaryTypeInfo)) {                        throw new UnsupportedOperationException("Binary type not implemented");        } else if (typeInfo.equals(TypeInfoFactory.timestampTypeInfo)) {            throw new UnsupportedOperationException("Timestamp type not implemented");        } else if (typeInfo.equals(TypeInfoFactory.voidTypeInfo)) {            throw new UnsupportedOperationException("Void type not implemented");        } else if (typeInfo.equals(TypeInfoFactory.unknownTypeInfo)) {            throw new UnsupportedOperationException("Unknown type not implemented");        } else {            throw new IllegalArgumentException("Unknown type: " + typeInfo);        }    } else if (typeInfo.getCategory().equals(Category.LIST)) {        return convertArrayType(name, (ListTypeInfo) typeInfo);    } else if (typeInfo.getCategory().equals(Category.STRUCT)) {        return convertStructType(name, (StructTypeInfo) typeInfo);    } else if (typeInfo.getCategory().equals(Category.MAP)) {        return convertMapType(name, (MapTypeInfo) typeInfo);    } else if (typeInfo.getCategory().equals(Category.UNION)) {        throw new UnsupportedOperationException("Union type not implemented");    } else {        throw new IllegalArgumentException("Unknown type: " + typeInfo);    }}
0
private static GroupType convertArrayType(final String name, final ListTypeInfo typeInfo)
{    final TypeInfo subType = typeInfo.getListElementTypeInfo();    return listWrapper(name, listType(), new GroupType(Repetition.REPEATED, ParquetHiveSerDe.ARRAY.toString(), convertType("array_element", subType)));}
0
private static GroupType convertStructType(final String name, final StructTypeInfo typeInfo)
{    final List<String> columnNames = typeInfo.getAllStructFieldNames();    final List<TypeInfo> columnTypes = typeInfo.getAllStructFieldTypeInfos();    return new GroupType(Repetition.OPTIONAL, name, convertTypes(columnNames, columnTypes));}
0
private static GroupType convertMapType(final String name, final MapTypeInfo typeInfo)
{    final Type keyType = convertType(ParquetHiveSerDe.MAP_KEY.toString(), typeInfo.getMapKeyTypeInfo(), Repetition.REQUIRED);    final Type valueType = convertType(ParquetHiveSerDe.MAP_VALUE.toString(), typeInfo.getMapValueTypeInfo());    return ConversionPatterns.mapType(Repetition.OPTIONAL, name, keyType, valueType);}
0
private static GroupType listWrapper(final String name, final LogicalTypeAnnotation logicalTypeAnnotation, final GroupType groupType)
{    return Types.optionalGroup().addField(groupType).as(logicalTypeAnnotation).named(name);}
0
public org.apache.hadoop.mapred.RecordReader<Void, ArrayWritable> getRecordReader(final org.apache.hadoop.mapred.InputSplit split, final org.apache.hadoop.mapred.JobConf job, final org.apache.hadoop.mapred.Reporter reporter) throws IOException
{    try {        return (RecordReader<Void, ArrayWritable>) new ParquetRecordReaderWrapper(realInput, split, job, reporter);    } catch (final InterruptedException e) {        throw new RuntimeException("Cannot create a RecordReaderWrapper", e);    }}
0
public void checkOutputSpecs(final FileSystem ignored, final JobConf job) throws IOException
{    realOutputFormat.checkOutputSpecs(ShimLoader.getHadoopShims().getHCatShim().createJobContext(job, null));}
0
public RecordWriter<Void, ArrayWritable> getRecordWriter(final FileSystem ignored, final JobConf job, final String name, final Progressable progress) throws IOException
{    throw new RuntimeException("Should never be used");}
0
public FileSinkOperator.RecordWriter getHiveRecordWriter(final JobConf jobConf, final Path finalOutPath, final Class<? extends Writable> valueClass, final boolean isCompressed, final Properties tableProperties, final Progressable progress) throws IOException
{        final String columnNameProperty = tableProperties.getProperty(IOConstants.COLUMNS);    final String columnTypeProperty = tableProperties.getProperty(IOConstants.COLUMNS_TYPES);    List<String> columnNames;    List<TypeInfo> columnTypes;    if (columnNameProperty.length() == 0) {        columnNames = new ArrayList<String>();    } else {        columnNames = Arrays.asList(columnNameProperty.split(","));    }    if (columnTypeProperty.length() == 0) {        columnTypes = new ArrayList<TypeInfo>();    } else {        columnTypes = TypeInfoUtils.getTypeInfosFromTypeString(columnTypeProperty);    }    DataWritableWriteSupport.setSchema(HiveSchemaConverter.convert(columnNames, columnTypes), jobConf);    return getParquerRecordWriterWrapper(realOutputFormat, jobConf, finalOutPath.toString(), progress);}
1
protected ParquetRecordWriterWrapper getParquerRecordWriterWrapper(ParquetOutputFormat<ArrayWritable> realOutputFormat, JobConf jobConf, String finalOutPath, Progressable progress) throws IOException
{    return new ParquetRecordWriterWrapper(realOutputFormat, jobConf, finalOutPath.toString(), progress);}
0
private static List<String> getColumns(final String columns)
{    return (new HiveBindingFactory()).create().getColumns(columns);}
0
public org.apache.parquet.hadoop.api.ReadSupport.ReadContext init(final Configuration configuration, final Map<String, String> keyValueMetaData, final MessageType fileSchema)
{    final String columns = configuration.get(IOConstants.COLUMNS);    final Map<String, String> contextMetadata = new HashMap<String, String>();    if (columns != null) {        final List<String> listColumns = getColumns(columns);        final List<Type> typeListTable = new ArrayList<Type>();        for (final String col : listColumns) {                        if (fileSchema.containsField(col)) {                typeListTable.add(fileSchema.getType(col));            } else {                                typeListTable.add(new PrimitiveType(Repetition.OPTIONAL, PrimitiveTypeName.BINARY, col));            }        }        MessageType tableSchema = new MessageType(TABLE_SCHEMA, typeListTable);        contextMetadata.put(HIVE_SCHEMA_KEY, tableSchema.toString());        MessageType requestedSchemaByUser = tableSchema;        final List<Integer> indexColumnsWanted = ColumnProjectionUtils.getReadColumnIDs(configuration);        final List<Type> typeListWanted = new ArrayList<Type>();        for (final Integer idx : indexColumnsWanted) {            typeListWanted.add(tableSchema.getType(listColumns.get(idx)));        }        requestedSchemaByUser = resolveSchemaAccess(new MessageType(fileSchema.getName(), typeListWanted), fileSchema, configuration);        return new ReadContext(requestedSchemaByUser, contextMetadata);    } else {        contextMetadata.put(HIVE_SCHEMA_KEY, fileSchema.toString());        return new ReadContext(fileSchema, contextMetadata);    }}
0
public RecordMaterializer<ArrayWritable> prepareForRead(final Configuration configuration, final Map<String, String> keyValueMetaData, final MessageType fileSchema, final org.apache.parquet.hadoop.api.ReadSupport.ReadContext readContext)
{    final Map<String, String> metadata = readContext.getReadSupportMetadata();    if (metadata == null) {        throw new IllegalStateException("ReadContext not initialized properly. " + "Don't know the Hive Schema.");    }    final MessageType tableSchema = resolveSchemaAccess(MessageTypeParser.parseMessageType(metadata.get(HIVE_SCHEMA_KEY)), fileSchema, configuration);    return new DataWritableRecordConverter(readContext.getRequestedSchema(), tableSchema);}
0
private MessageType resolveSchemaAccess(MessageType requestedSchema, MessageType fileSchema, Configuration configuration)
{    if (configuration.getBoolean(PARQUET_COLUMN_INDEX_ACCESS, false)) {        final List<String> listColumns = getColumns(configuration.get(IOConstants.COLUMNS));        List<Type> requestedTypes = new ArrayList<Type>();        for (Type t : requestedSchema.getFields()) {            int index = listColumns.indexOf(t.getName());            requestedTypes.add(fileSchema.getType(index));        }        requestedSchema = new MessageType(requestedSchema.getName(), requestedTypes);    }    return requestedSchema;}
0
public void close() throws IOException
{    if (realReader != null) {        realReader.close();    }}
0
public Void createKey()
{    return null;}
0
public ArrayWritable createValue()
{    return valueObj;}
0
public long getPos() throws IOException
{    return (long) (splitLen * getProgress());}
0
public float getProgress() throws IOException
{    if (realReader == null) {        return 1f;    } else {        try {            return realReader.getProgress();        } catch (final InterruptedException e) {            throw new IOException(e);        }    }}
0
public boolean next(final Void key, final ArrayWritable value) throws IOException
{    if (eof) {        return false;    }    try {        if (firstRecord) {                        firstRecord = false;        } else if (!realReader.nextKeyValue()) {                        eof = true;            return false;        }        final ArrayWritable tmpCurValue = realReader.getCurrentValue();        if (value != tmpCurValue) {            final Writable[] arrValue = value.get();            final Writable[] arrCurrent = tmpCurValue.get();            if (value != null && arrValue.length == arrCurrent.length) {                System.arraycopy(arrCurrent, 0, arrValue, 0, arrCurrent.length);            } else {                if (arrValue.length != arrCurrent.length) {                    throw new IOException("DeprecatedParquetHiveInput : size of object differs. Value" + " size :  " + arrValue.length + ", Current Object size : " + arrCurrent.length);                } else {                    throw new IOException("DeprecatedParquetHiveInput can not support RecordReaders that" + " don't return same key & value & value is null");                }            }        }        return true;    } catch (final InterruptedException e) {        throw new IOException(e);    }}
0
protected ParquetInputSplit getSplit(final InputSplit oldSplit, final JobConf conf) throws IOException
{    if (oldSplit instanceof FileSplit) {        FileSplit fileSplit = (FileSplit) oldSplit;        final long splitStart = fileSplit.getStart();        final long splitLength = fileSplit.getLength();        final Path finalPath = fileSplit.getPath();        final JobConf cloneJob = hiveBinding.pushProjectionsAndFilters(conf, finalPath.getParent());        final ParquetMetadata parquetMetadata = ParquetFileReader.readFooter(cloneJob, finalPath, SKIP_ROW_GROUPS);        final FileMetaData fileMetaData = parquetMetadata.getFileMetaData();        final ReadContext readContext = new DataWritableReadSupport().init(cloneJob, fileMetaData.getKeyValueMetaData(), fileMetaData.getSchema());        schemaSize = MessageTypeParser.parseMessageType(readContext.getReadSupportMetadata().get(DataWritableReadSupport.HIVE_SCHEMA_KEY)).getFieldCount();        return new ParquetInputSplit(finalPath, splitStart, splitStart + splitLength, splitLength, fileSplit.getLocations(), null);    } else {        throw new IllegalArgumentException("Unknown split type: " + oldSplit);    }}
0
public String getTypeName()
{    return "map<" + keyInspector.getTypeName() + "," + valueInspector.getTypeName() + ">";}
0
public Category getCategory()
{    return Category.MAP;}
0
public ObjectInspector getMapKeyObjectInspector()
{    return keyInspector;}
0
public ObjectInspector getMapValueObjectInspector()
{    return valueInspector;}
0
public Map<?, ?> getMap(final Object data)
{    if (data == null) {        return null;    }    if (data instanceof ArrayWritable) {        final Writable[] mapContainer = ((ArrayWritable) data).get();        if (mapContainer == null || mapContainer.length == 0) {            return null;        }        final Writable[] mapArray = ((ArrayWritable) mapContainer[0]).get();        final Map<Writable, Writable> map = new HashMap<Writable, Writable>();        for (final Writable obj : mapArray) {            final ArrayWritable mapObj = (ArrayWritable) obj;            final Writable[] arr = mapObj.get();            map.put(arr[0], arr[1]);        }        return map;    }    if (data instanceof Map) {        return (Map) data;    }    throw new UnsupportedOperationException("Cannot inspect " + data.getClass().getCanonicalName());}
0
public int getMapSize(final Object data)
{    if (data == null) {        return -1;    }    if (data instanceof ArrayWritable) {        final Writable[] mapContainer = ((ArrayWritable) data).get();        if (mapContainer == null || mapContainer.length == 0) {            return -1;        } else {            return ((ArrayWritable) mapContainer[0]).get().length;        }    }    if (data instanceof Map) {        return ((Map) data).size();    }    throw new UnsupportedOperationException("Cannot inspect " + data.getClass().getCanonicalName());}
0
public Object create()
{    Map<Object, Object> m = new HashMap<Object, Object>();    return m;}
0
public Object put(Object map, Object key, Object value)
{    Map<Object, Object> m = (HashMap<Object, Object>) map;    m.put(key, value);    return m;}
0
public Object remove(Object map, Object key)
{    Map<Object, Object> m = (HashMap<Object, Object>) map;    m.remove(key);    return m;}
0
public Object clear(Object map)
{    Map<Object, Object> m = (HashMap<Object, Object>) map;    m.clear();    return m;}
0
public int hashCode()
{    final int prime = 31;    int result = 1;    result = prime * result + ((keyInspector == null) ? 0 : keyInspector.hashCode());    result = prime * result + ((valueInspector == null) ? 0 : valueInspector.hashCode());    return result;}
0
public boolean equals(Object obj)
{    if (this == obj) {        return true;    }    if (obj == null) {        return false;    }    if (getClass() != obj.getClass()) {        return false;    }    final AbstractParquetMapInspector other = (AbstractParquetMapInspector) obj;    if (keyInspector == null) {        if (other.keyInspector != null) {            return false;        }    } else if (!keyInspector.equals(other.keyInspector)) {        return false;    }    if (valueInspector == null) {        if (other.valueInspector != null) {            return false;        }    } else if (!valueInspector.equals(other.valueInspector)) {        return false;    }    return true;}
0
private ObjectInspector getObjectInspector(final TypeInfo typeInfo)
{    if (typeInfo.equals(TypeInfoFactory.doubleTypeInfo)) {        return PrimitiveObjectInspectorFactory.writableDoubleObjectInspector;    } else if (typeInfo.equals(TypeInfoFactory.booleanTypeInfo)) {        return PrimitiveObjectInspectorFactory.writableBooleanObjectInspector;    } else if (typeInfo.equals(TypeInfoFactory.floatTypeInfo)) {        return PrimitiveObjectInspectorFactory.writableFloatObjectInspector;    } else if (typeInfo.equals(TypeInfoFactory.intTypeInfo)) {        return PrimitiveObjectInspectorFactory.writableIntObjectInspector;    } else if (typeInfo.equals(TypeInfoFactory.longTypeInfo)) {        return PrimitiveObjectInspectorFactory.writableLongObjectInspector;    } else if (typeInfo.equals(TypeInfoFactory.stringTypeInfo)) {        return ParquetPrimitiveInspectorFactory.parquetStringInspector;    } else if (typeInfo.getCategory().equals(Category.STRUCT)) {        return new ArrayWritableObjectInspector((StructTypeInfo) typeInfo);    } else if (typeInfo.getCategory().equals(Category.LIST)) {        final TypeInfo subTypeInfo = ((ListTypeInfo) typeInfo).getListElementTypeInfo();        return new ParquetHiveArrayInspector(getObjectInspector(subTypeInfo));    } else if (typeInfo.getCategory().equals(Category.MAP)) {        final TypeInfo keyTypeInfo = ((MapTypeInfo) typeInfo).getMapKeyTypeInfo();        final TypeInfo valueTypeInfo = ((MapTypeInfo) typeInfo).getMapValueTypeInfo();        if (keyTypeInfo.equals(TypeInfoFactory.stringTypeInfo) || keyTypeInfo.equals(TypeInfoFactory.byteTypeInfo) || keyTypeInfo.equals(TypeInfoFactory.shortTypeInfo)) {            return new DeepParquetHiveMapInspector(getObjectInspector(keyTypeInfo), getObjectInspector(valueTypeInfo));        } else {            return new StandardParquetHiveMapInspector(getObjectInspector(keyTypeInfo), getObjectInspector(valueTypeInfo));        }    } else if (typeInfo.equals(TypeInfoFactory.timestampTypeInfo)) {        throw new UnsupportedOperationException("timestamp not implemented yet");    } else if (typeInfo.equals(TypeInfoFactory.byteTypeInfo)) {        return ParquetPrimitiveInspectorFactory.parquetByteInspector;    } else if (typeInfo.equals(TypeInfoFactory.shortTypeInfo)) {        return ParquetPrimitiveInspectorFactory.parquetShortInspector;    } else {        throw new IllegalArgumentException("Unknown field info: " + typeInfo);    }}
0
public Category getCategory()
{    return Category.STRUCT;}
0
public String getTypeName()
{    return typeInfo.getTypeName();}
0
public List<? extends StructField> getAllStructFieldRefs()
{    return fields;}
0
public Object getStructFieldData(final Object data, final StructField fieldRef)
{    if (data == null) {        return null;    }    if (data instanceof ArrayWritable) {        final ArrayWritable arr = (ArrayWritable) data;        return arr.get()[((StructFieldImpl) fieldRef).getIndex()];    }        if (data instanceof List) {        return ((List) data).get(((StructFieldImpl) fieldRef).getIndex());    }    throw new UnsupportedOperationException("Cannot inspect " + data.getClass().getCanonicalName());}
0
public StructField getStructFieldRef(final String name)
{    return fieldsByName.get(name);}
0
public List<Object> getStructFieldsDataAsList(final Object data)
{    if (data == null) {        return null;    }    if (data instanceof ArrayWritable) {        final ArrayWritable arr = (ArrayWritable) data;        final Object[] arrWritable = arr.get();        return new ArrayList<Object>(Arrays.asList(arrWritable));    }    throw new UnsupportedOperationException("Cannot inspect " + data.getClass().getCanonicalName());}
0
public Object create()
{    final ArrayList<Object> list = new ArrayList<Object>(fields.size());    for (int i = 0; i < fields.size(); ++i) {        list.add(null);    }    return list;}
0
public Object setStructFieldData(Object struct, StructField field, Object fieldValue)
{    final ArrayList<Object> list = (ArrayList<Object>) struct;    list.set(((StructFieldImpl) field).getIndex(), fieldValue);    return list;}
0
public boolean equals(Object obj)
{    if (obj == null) {        return false;    }    if (getClass() != obj.getClass()) {        return false;    }    final ArrayWritableObjectInspector other = (ArrayWritableObjectInspector) obj;    if (this.typeInfo != other.typeInfo && (this.typeInfo == null || !this.typeInfo.equals(other.typeInfo))) {        return false;    }    return true;}
0
public int hashCode()
{    int hash = 5;    hash = 29 * hash + (this.typeInfo != null ? this.typeInfo.hashCode() : 0);    return hash;}
0
public String getFieldComment()
{    return "";}
0
public String getFieldName()
{    return name;}
0
public int getIndex()
{    return index;}
0
public ObjectInspector getFieldObjectInspector()
{    return inspector;}
0
public Object getMapValueElement(final Object data, final Object key)
{    if (data == null || key == null) {        return null;    }    if (data instanceof ArrayWritable) {        final Writable[] mapContainer = ((ArrayWritable) data).get();        if (mapContainer == null || mapContainer.length == 0) {            return null;        }        final Writable[] mapArray = ((ArrayWritable) mapContainer[0]).get();        for (final Writable obj : mapArray) {            final ArrayWritable mapObj = (ArrayWritable) obj;            final Writable[] arr = mapObj.get();            if (key.equals(arr[0]) || key.equals(((PrimitiveObjectInspector) keyInspector).getPrimitiveJavaObject(arr[0])) || key.equals(((PrimitiveObjectInspector) keyInspector).getPrimitiveWritableObject(arr[0]))) {                return arr[1];            }        }        return null;    }    if (data instanceof Map) {        final Map<?, ?> map = (Map<?, ?>) data;        if (map.containsKey(key)) {            return map.get(key);        }        for (final Map.Entry<?, ?> entry : map.entrySet()) {            if (key.equals(((PrimitiveObjectInspector) keyInspector).getPrimitiveJavaObject(entry.getKey())) || key.equals(((PrimitiveObjectInspector) keyInspector).getPrimitiveWritableObject(entry.getKey()))) {                return entry.getValue();            }        }        return null;    }    throw new UnsupportedOperationException("Cannot inspect " + data.getClass().getCanonicalName());}
0
public String getTypeName()
{    return "array<" + arrayElementInspector.getTypeName() + ">";}
0
public Category getCategory()
{    return Category.LIST;}
0
public ObjectInspector getListElementObjectInspector()
{    return arrayElementInspector;}
0
public Object getListElement(final Object data, final int index)
{    if (data == null) {        return null;    }    if (data instanceof ArrayWritable) {        final Writable[] listContainer = ((ArrayWritable) data).get();        if (listContainer == null || listContainer.length == 0) {            return null;        }        final Writable subObj = listContainer[0];        if (subObj == null) {            return null;        }        if (index >= 0 && index < ((ArrayWritable) subObj).get().length) {            return ((ArrayWritable) subObj).get()[index];        } else {            return null;        }    }    throw new UnsupportedOperationException("Cannot inspect " + data.getClass().getCanonicalName());}
0
public int getListLength(final Object data)
{    if (data == null) {        return -1;    }    if (data instanceof ArrayWritable) {        final Writable[] listContainer = ((ArrayWritable) data).get();        if (listContainer == null || listContainer.length == 0) {            return -1;        }        final Writable subObj = listContainer[0];        if (subObj == null) {            return 0;        }        return ((ArrayWritable) subObj).get().length;    }    throw new UnsupportedOperationException("Cannot inspect " + data.getClass().getCanonicalName());}
0
public List<?> getList(final Object data)
{    if (data == null) {        return null;    }    if (data instanceof ArrayWritable) {        final Writable[] listContainer = ((ArrayWritable) data).get();        if (listContainer == null || listContainer.length == 0) {            return null;        }        final Writable subObj = listContainer[0];        if (subObj == null) {            return null;        }        final Writable[] array = ((ArrayWritable) subObj).get();        final List<Writable> list = new ArrayList<Writable>();        for (final Writable obj : array) {            list.add(obj);        }        return list;    }    throw new UnsupportedOperationException("Cannot inspect " + data.getClass().getCanonicalName());}
0
public Object create(final int size)
{    final ArrayList<Object> result = new ArrayList<Object>(size);    for (int i = 0; i < size; ++i) {        result.add(null);    }    return result;}
0
public Object set(final Object list, final int index, final Object element)
{    final ArrayList l = (ArrayList) list;    l.set(index, element);    return list;}
0
public Object resize(final Object list, final int newSize)
{    final ArrayList l = (ArrayList) list;    l.ensureCapacity(newSize);    while (l.size() < newSize) {        l.add(null);    }    while (l.size() > newSize) {        l.remove(l.size() - 1);    }    return list;}
0
public boolean equals(final Object o)
{    if (o == null || o.getClass() != getClass()) {        return false;    } else if (o == this) {        return true;    } else {        final ObjectInspector other = ((ParquetHiveArrayInspector) o).arrayElementInspector;        return other.equals(arrayElementInspector);    }}
0
public int hashCode()
{    int hash = 3;    hash = 29 * hash + (this.arrayElementInspector != null ? this.arrayElementInspector.hashCode() : 0);    return hash;}
0
public final void initialize(final Configuration conf, final Properties tbl) throws SerDeException
{    final TypeInfo rowTypeInfo;    final List<String> columnNames;    final List<TypeInfo> columnTypes;        final String columnNameProperty = tbl.getProperty(IOConstants.COLUMNS);    final String columnTypeProperty = tbl.getProperty(IOConstants.COLUMNS_TYPES);    if (columnNameProperty.length() == 0) {        columnNames = new ArrayList<String>();    } else {        columnNames = Arrays.asList(columnNameProperty.split(","));    }    if (columnTypeProperty.length() == 0) {        columnTypes = new ArrayList<TypeInfo>();    } else {        columnTypes = TypeInfoUtils.getTypeInfosFromTypeString(columnTypeProperty);    }    if (columnNames.size() != columnTypes.size()) {        throw new IllegalArgumentException("ParquetHiveSerde initialization failed. Number of column " + "name and column type differs. columnNames = " + columnNames + ", columnTypes = " + columnTypes);    }        rowTypeInfo = TypeInfoFactory.getStructTypeInfo(columnNames, columnTypes);    this.objInspector = new ArrayWritableObjectInspector((StructTypeInfo) rowTypeInfo);        stats = new SerDeStats();    serializedSize = 0;    deserializedSize = 0;    status = LAST_OPERATION.UNKNOWN;}
0
public Object deserialize(final Writable blob) throws SerDeException
{    status = LAST_OPERATION.DESERIALIZE;    deserializedSize = 0;    if (blob instanceof ArrayWritable) {        deserializedSize = ((ArrayWritable) blob).get().length;        return blob;    } else {        return null;    }}
0
public ObjectInspector getObjectInspector() throws SerDeException
{    return objInspector;}
0
public Class<? extends Writable> getSerializedClass()
{    return ArrayWritable.class;}
0
public Writable serialize(final Object obj, final ObjectInspector objInspector) throws SerDeException
{    if (!objInspector.getCategory().equals(Category.STRUCT)) {        throw new SerDeException("Cannot serialize " + objInspector.getCategory() + ". Can only serialize a struct");    }    final ArrayWritable serializeData = createStruct(obj, (StructObjectInspector) objInspector);    serializedSize = serializeData.get().length;    status = LAST_OPERATION.SERIALIZE;    return serializeData;}
0
private ArrayWritable createStruct(final Object obj, final StructObjectInspector inspector) throws SerDeException
{    final List<? extends StructField> fields = inspector.getAllStructFieldRefs();    final Writable[] arr = new Writable[fields.size()];    for (int i = 0; i < fields.size(); i++) {        final StructField field = fields.get(i);        final Object subObj = inspector.getStructFieldData(obj, field);        final ObjectInspector subInspector = field.getFieldObjectInspector();        arr[i] = createObject(subObj, subInspector);    }    return new ArrayWritable(Writable.class, arr);}
0
private Writable createMap(final Object obj, final MapObjectInspector inspector) throws SerDeException
{    final Map<?, ?> sourceMap = inspector.getMap(obj);    final ObjectInspector keyInspector = inspector.getMapKeyObjectInspector();    final ObjectInspector valueInspector = inspector.getMapValueObjectInspector();    final List<ArrayWritable> array = new ArrayList<ArrayWritable>();    if (sourceMap != null) {        for (final Entry<?, ?> keyValue : sourceMap.entrySet()) {            final Writable key = createObject(keyValue.getKey(), keyInspector);            final Writable value = createObject(keyValue.getValue(), valueInspector);            if (key != null) {                Writable[] arr = new Writable[2];                arr[0] = key;                arr[1] = value;                array.add(new ArrayWritable(Writable.class, arr));            }        }    }    if (array.size() > 0) {        final ArrayWritable subArray = new ArrayWritable(ArrayWritable.class, array.toArray(new ArrayWritable[array.size()]));        return new ArrayWritable(Writable.class, new Writable[] { subArray });    } else {        return null;    }}
0
private ArrayWritable createArray(final Object obj, final ListObjectInspector inspector) throws SerDeException
{    final List<?> sourceArray = inspector.getList(obj);    final ObjectInspector subInspector = inspector.getListElementObjectInspector();    final List<Writable> array = new ArrayList<Writable>();    if (sourceArray != null) {        for (final Object curObj : sourceArray) {            final Writable newObj = createObject(curObj, subInspector);            if (newObj != null) {                array.add(newObj);            }        }    }    if (array.size() > 0) {        final ArrayWritable subArray = new ArrayWritable(array.get(0).getClass(), array.toArray(new Writable[array.size()]));        return new ArrayWritable(Writable.class, new Writable[] { subArray });    } else {        return null;    }}
0
private Writable createPrimitive(final Object obj, final PrimitiveObjectInspector inspector) throws SerDeException
{    if (obj == null) {        return null;    }    switch(inspector.getPrimitiveCategory()) {        case VOID:            return null;        case BOOLEAN:            return new BooleanWritable(((BooleanObjectInspector) inspector).get(obj) ? Boolean.TRUE : Boolean.FALSE);        case BYTE:            return new ByteWritable((byte) ((ByteObjectInspector) inspector).get(obj));        case DOUBLE:            return new DoubleWritable(((DoubleObjectInspector) inspector).get(obj));        case FLOAT:            return new FloatWritable(((FloatObjectInspector) inspector).get(obj));        case INT:            return new IntWritable(((IntObjectInspector) inspector).get(obj));        case LONG:            return new LongWritable(((LongObjectInspector) inspector).get(obj));        case SHORT:            return new ShortWritable((short) ((ShortObjectInspector) inspector).get(obj));        case STRING:            return new BinaryWritable(Binary.fromString(((StringObjectInspector) inspector).getPrimitiveJavaObject(obj)));        default:            throw new SerDeException("Unknown primitive : " + inspector.getPrimitiveCategory());    }}
0
private Writable createObject(final Object obj, final ObjectInspector inspector) throws SerDeException
{    switch(inspector.getCategory()) {        case STRUCT:            return createStruct(obj, (StructObjectInspector) inspector);        case LIST:            return createArray(obj, (ListObjectInspector) inspector);        case MAP:            return createMap(obj, (MapObjectInspector) inspector);        case PRIMITIVE:            return createPrimitive(obj, (PrimitiveObjectInspector) inspector);        default:            throw new SerDeException("Unknown data type" + inspector.getCategory());    }}
0
public SerDeStats getSerDeStats()
{        assert (status != LAST_OPERATION.UNKNOWN);    if (status == LAST_OPERATION.SERIALIZE) {        stats.setRawDataSize(serializedSize);    } else {        stats.setRawDataSize(deserializedSize);    }    return stats;}
0
public Object getPrimitiveWritableObject(final Object o)
{    return o == null ? null : new ByteWritable(get(o));}
0
public Object create(final byte val)
{    return new ByteWritable(val);}
0
public Object set(final Object o, final byte val)
{    ((ByteWritable) o).set(val);    return o;}
0
public byte get(Object o)
{        if (o instanceof IntWritable) {        return (byte) ((IntWritable) o).get();    }    return ((ByteWritable) o).get();}
0
public Object getPrimitiveWritableObject(final Object o)
{    return o == null ? null : new ShortWritable(get(o));}
0
public Object create(final short val)
{    return new ShortWritable(val);}
0
public Object set(final Object o, final short val)
{    ((ShortWritable) o).set(val);    return o;}
0
public short get(Object o)
{        if (o instanceof IntWritable) {        return (short) ((IntWritable) o).get();    }    return ((ShortWritable) o).get();}
0
public Object getMapValueElement(final Object data, final Object key)
{    if (data == null || key == null) {        return null;    }    if (data instanceof ArrayWritable) {        final Writable[] mapContainer = ((ArrayWritable) data).get();        if (mapContainer == null || mapContainer.length == 0) {            return null;        }        final Writable[] mapArray = ((ArrayWritable) mapContainer[0]).get();        for (final Writable obj : mapArray) {            final ArrayWritable mapObj = (ArrayWritable) obj;            final Writable[] arr = mapObj.get();            if (key.equals(arr[0])) {                return arr[1];            }        }        return null;    }    if (data instanceof Map) {        return ((Map) data).get(key);    }    throw new UnsupportedOperationException("Cannot inspect " + data.getClass().getCanonicalName());}
0
public void set(BigDecimal value)
{    value = value.stripTrailingZeros();    if (value.compareTo(BigDecimal.ZERO) == 0) {                        value = BigDecimal.ZERO;    }    set(value.unscaledValue().toByteArray(), value.scale());}
0
public void set(final BigDecimalWritable writable)
{    set(writable.getBigDecimal());}
0
public void set(final byte[] bytes, final int scale)
{    this.internalStorage = bytes;    this.scale = scale;}
0
public void setFromBytes(final byte[] bytes, int offset, final int length)
{    LazyBinaryUtils.readVInt(bytes, offset, vInt);    scale = vInt.value;    offset += vInt.length;    LazyBinaryUtils.readVInt(bytes, offset, vInt);    offset += vInt.length;    if (internalStorage.length != vInt.value) {        internalStorage = new byte[vInt.value];    }    System.arraycopy(bytes, offset, internalStorage, 0, vInt.value);}
0
public BigDecimal getBigDecimal()
{    return new BigDecimal(new BigInteger(internalStorage), scale);}
0
public void readFields(final DataInput in) throws IOException
{    scale = WritableUtils.readVInt(in);    final int byteArrayLen = WritableUtils.readVInt(in);    if (internalStorage.length != byteArrayLen) {        internalStorage = new byte[byteArrayLen];    }    in.readFully(internalStorage);}
0
public void write(final DataOutput out) throws IOException
{    WritableUtils.writeVInt(out, scale);    WritableUtils.writeVInt(out, internalStorage.length);    out.write(internalStorage);}
0
public int compareTo(final BigDecimalWritable that)
{    return getBigDecimal().compareTo(that.getBigDecimal());}
0
public void writeToByteStream(final Output byteStream)
{    LazyBinaryUtils.writeVInt(byteStream, scale);    LazyBinaryUtils.writeVInt(byteStream, internalStorage.length);    byteStream.write(internalStorage, 0, internalStorage.length);}
0
public String toString()
{    return getBigDecimal().toString();}
0
public boolean equals(final Object other)
{    if (other == null || !(other instanceof BigDecimalWritable)) {        return false;    }    final BigDecimalWritable bdw = (BigDecimalWritable) other;        return getBigDecimal().compareTo(bdw.getBigDecimal()) == 0;}
0
public int hashCode()
{    return getBigDecimal().hashCode();}
0
public Binary getBinary()
{    return binary;}
0
public byte[] getBytes()
{    return binary.getBytes();}
0
public String getString()
{    return binary.toStringUsingUTF8();}
0
public void readFields(DataInput input) throws IOException
{    byte[] bytes = new byte[input.readInt()];    input.readFully(bytes);    binary = Binary.fromConstantByteArray(bytes);}
0
public void write(DataOutput output) throws IOException
{    output.writeInt(binary.length());    binary.writeTo(output);}
0
public int hashCode()
{    return binary == null ? 0 : binary.hashCode();}
0
public boolean equals(Object obj)
{    if (obj instanceof BinaryWritable) {        final BinaryWritable other = (BinaryWritable) obj;        return binary.equals(other.binary);    }    return false;}
0
public String getString()
{    return string;}
0
public void write(final ArrayWritable arr)
{    if (arr == null) {        return;    }    recordConsumer.startMessage();    writeData(arr, schema);    recordConsumer.endMessage();}
0
private void writeData(final ArrayWritable arr, final GroupType type)
{    if (arr == null) {        return;    }    final int fieldCount = type.getFieldCount();    Writable[] values = arr.get();    for (int field = 0; field < fieldCount; ++field) {        final Type fieldType = type.getType(field);        final String fieldName = fieldType.getName();        final Writable value = values[field];        if (value == null) {            continue;        }        recordConsumer.startField(fieldName, field);        if (fieldType.isPrimitive()) {            writePrimitive(value);        } else {            recordConsumer.startGroup();            if (value instanceof ArrayWritable) {                if (fieldType.asGroupType().getRepetition().equals(Type.Repetition.REPEATED)) {                    writeArray((ArrayWritable) value, fieldType.asGroupType());                } else {                    writeData((ArrayWritable) value, fieldType.asGroupType());                }            } else if (value != null) {                throw new ParquetEncodingException("This should be an ArrayWritable or MapWritable: " + value);            }            recordConsumer.endGroup();        }        recordConsumer.endField(fieldName, field);    }}
0
private void writeArray(final ArrayWritable array, final GroupType type)
{    if (array == null) {        return;    }    final Writable[] subValues = array.get();    final int fieldCount = type.getFieldCount();    for (int field = 0; field < fieldCount; ++field) {        final Type subType = type.getType(field);        recordConsumer.startField(subType.getName(), field);        for (int i = 0; i < subValues.length; ++i) {            final Writable subValue = subValues[i];            if (subValue != null) {                if (subType.isPrimitive()) {                    if (subValue instanceof ArrayWritable) {                                                writePrimitive(((ArrayWritable) subValue).get()[field]);                    } else {                        writePrimitive(subValue);                    }                } else {                    if (!(subValue instanceof ArrayWritable)) {                        throw new RuntimeException("This should be a ArrayWritable: " + subValue);                    } else {                        recordConsumer.startGroup();                        writeData((ArrayWritable) subValue, subType.asGroupType());                        recordConsumer.endGroup();                    }                }            }        }        recordConsumer.endField(subType.getName(), field);    }}
0
private void writePrimitive(final Writable value)
{    if (value == null) {        return;    }    if (value instanceof DoubleWritable) {        recordConsumer.addDouble(((DoubleWritable) value).get());    } else if (value instanceof BooleanWritable) {        recordConsumer.addBoolean(((BooleanWritable) value).get());    } else if (value instanceof FloatWritable) {        recordConsumer.addFloat(((FloatWritable) value).get());    } else if (value instanceof IntWritable) {        recordConsumer.addInteger(((IntWritable) value).get());    } else if (value instanceof LongWritable) {        recordConsumer.addLong(((LongWritable) value).get());    } else if (value instanceof ShortWritable) {        recordConsumer.addInteger(((ShortWritable) value).get());    } else if (value instanceof ByteWritable) {        recordConsumer.addInteger(((ByteWritable) value).get());    } else if (value instanceof BigDecimalWritable) {        throw new UnsupportedOperationException("BigDecimal writing not implemented");    } else if (value instanceof BinaryWritable) {        recordConsumer.addBinary(((BinaryWritable) value).getBinary());    } else {        throw new IllegalArgumentException("Unknown value type: " + value + " " + value.getClass());    }}
0
public static void setSchema(final MessageType schema, final Configuration configuration)
{    configuration.set(PARQUET_HIVE_SCHEMA, schema.toString());}
0
public static MessageType getSchema(final Configuration configuration)
{    return MessageTypeParser.parseMessageType(configuration.get(PARQUET_HIVE_SCHEMA));}
0
public WriteContext init(final Configuration configuration)
{    schema = getSchema(configuration);    return new WriteContext(schema, new HashMap<String, String>());}
0
public void prepareForWrite(final RecordConsumer recordConsumer)
{    writer = new DataWritableWriter(recordConsumer, schema);}
0
public void write(final ArrayWritable record)
{    writer.write(record);}
0
public void close(final Reporter reporter) throws IOException
{    try {        realWriter.close(taskContext);    } catch (final InterruptedException e) {        throw new IOException(e);    }}
0
public void write(final Void key, final ArrayWritable value) throws IOException
{    try {        realWriter.write(key, value);    } catch (final InterruptedException e) {        throw new IOException(e);    }}
0
public void close(final boolean abort) throws IOException
{    close(null);}
0
public void write(final Writable w) throws IOException
{    write(null, (ArrayWritable) w);}
0
public Text getPrimitiveWritableObject(final Object o)
{    if (o == null) {        return null;    }    if (o instanceof BinaryWritable) {        return new Text(((BinaryWritable) o).getBytes());    }    if (o instanceof Text) {        return (Text) o;    }    if (o instanceof String) {        return new Text((String) o);    }    throw new UnsupportedOperationException("Cannot inspect " + o.getClass().getCanonicalName());}
0
public String getPrimitiveJavaObject(final Object o)
{    if (o == null) {        return null;    }    if (o instanceof BinaryWritable) {        return ((BinaryWritable) o).getString();    }    if (o instanceof Text) {        return ((Text) o).toString();    }    if (o instanceof String) {        return (String) o;    }    throw new UnsupportedOperationException("Cannot inspect " + o.getClass().getCanonicalName());}
0
public Object set(final Object o, final Text text)
{    return new BinaryWritable(text == null ? null : Binary.fromReusedByteArray(text.getBytes()));}
0
public Object set(final Object o, final String string)
{    return new BinaryWritable(string == null ? null : Binary.fromString(string));}
0
public Object create(final Text text)
{    if (text == null) {        return null;    }    return text.toString();}
0
public Object create(final String string)
{    return string;}
0
public Object getMapValueElement(Object o, Object o1)
{    throw new UnsupportedOperationException("Should not be called");}
0
public void setUp()
{    inspector = new TestableAbstractParquetMapInspector(PrimitiveObjectInspectorFactory.javaIntObjectInspector, PrimitiveObjectInspectorFactory.javaIntObjectInspector);}
0
public void testNullMap()
{    assertEquals("Wrong size", -1, inspector.getMapSize(null));    assertNull("Should be null", inspector.getMap(null));}
0
public void testNullContainer()
{    final ArrayWritable map = new ArrayWritable(ArrayWritable.class, null);    assertEquals("Wrong size", -1, inspector.getMapSize(map));    assertNull("Should be null", inspector.getMap(map));}
0
public void testEmptyContainer()
{    final ArrayWritable map = new ArrayWritable(ArrayWritable.class, new ArrayWritable[0]);    assertEquals("Wrong size", -1, inspector.getMapSize(map));    assertNull("Should be null", inspector.getMap(map));}
0
public void testRegularMap()
{    final Writable[] entry1 = new Writable[] { new IntWritable(0), new IntWritable(1) };    final Writable[] entry2 = new Writable[] { new IntWritable(2), new IntWritable(3) };    final ArrayWritable internalMap = new ArrayWritable(ArrayWritable.class, new Writable[] { new ArrayWritable(Writable.class, entry1), new ArrayWritable(Writable.class, entry2) });    final ArrayWritable map = new ArrayWritable(ArrayWritable.class, new Writable[] { internalMap });    final Map<Writable, Writable> expected = new HashMap<Writable, Writable>();    expected.put(new IntWritable(0), new IntWritable(1));    expected.put(new IntWritable(2), new IntWritable(3));    assertEquals("Wrong size", 2, inspector.getMapSize(map));    assertEquals("Wrong result of inspection", expected, inspector.getMap(map));}
0
public void testHashMap()
{    final Map<Writable, Writable> map = new HashMap<Writable, Writable>();    map.put(new IntWritable(0), new IntWritable(1));    map.put(new IntWritable(2), new IntWritable(3));    map.put(new IntWritable(4), new IntWritable(5));    map.put(new IntWritable(6), new IntWritable(7));    assertEquals("Wrong size", 4, inspector.getMapSize(map));    assertEquals("Wrong result of inspection", map, inspector.getMap(map));}
0
public void setUp()
{    inspector = new DeepParquetHiveMapInspector(ParquetPrimitiveInspectorFactory.parquetShortInspector, PrimitiveObjectInspectorFactory.javaIntObjectInspector);}
0
public void testNullMap()
{    assertNull("Should be null", inspector.getMapValueElement(null, new ShortWritable((short) 0)));}
0
public void testNullContainer()
{    final ArrayWritable map = new ArrayWritable(ArrayWritable.class, null);    assertNull("Should be null", inspector.getMapValueElement(map, new ShortWritable((short) 0)));}
0
public void testEmptyContainer()
{    final ArrayWritable map = new ArrayWritable(ArrayWritable.class, new ArrayWritable[0]);    assertNull("Should be null", inspector.getMapValueElement(map, new ShortWritable((short) 0)));}
0
public void testRegularMap()
{    final Writable[] entry1 = new Writable[] { new IntWritable(0), new IntWritable(1) };    final Writable[] entry2 = new Writable[] { new IntWritable(2), new IntWritable(3) };    final ArrayWritable internalMap = new ArrayWritable(ArrayWritable.class, new Writable[] { new ArrayWritable(Writable.class, entry1), new ArrayWritable(Writable.class, entry2) });    final ArrayWritable map = new ArrayWritable(ArrayWritable.class, new Writable[] { internalMap });    assertEquals("Wrong result of inspection", new IntWritable(1), inspector.getMapValueElement(map, new IntWritable(0)));    assertEquals("Wrong result of inspection", new IntWritable(3), inspector.getMapValueElement(map, new IntWritable(2)));    assertEquals("Wrong result of inspection", new IntWritable(1), inspector.getMapValueElement(map, new ShortWritable((short) 0)));    assertEquals("Wrong result of inspection", new IntWritable(3), inspector.getMapValueElement(map, new ShortWritable((short) 2)));}
0
public void testHashMap()
{    final Map<Writable, Writable> map = new HashMap<Writable, Writable>();    map.put(new IntWritable(0), new IntWritable(1));    map.put(new IntWritable(2), new IntWritable(3));    map.put(new IntWritable(4), new IntWritable(5));    map.put(new IntWritable(6), new IntWritable(7));    assertEquals("Wrong result of inspection", new IntWritable(1), inspector.getMapValueElement(map, new IntWritable(0)));    assertEquals("Wrong result of inspection", new IntWritable(3), inspector.getMapValueElement(map, new IntWritable(2)));    assertEquals("Wrong result of inspection", new IntWritable(5), inspector.getMapValueElement(map, new IntWritable(4)));    assertEquals("Wrong result of inspection", new IntWritable(7), inspector.getMapValueElement(map, new IntWritable(6)));    assertEquals("Wrong result of inspection", new IntWritable(1), inspector.getMapValueElement(map, new ShortWritable((short) 0)));    assertEquals("Wrong result of inspection", new IntWritable(3), inspector.getMapValueElement(map, new ShortWritable((short) 2)));    assertEquals("Wrong result of inspection", new IntWritable(5), inspector.getMapValueElement(map, new ShortWritable((short) 4)));    assertEquals("Wrong result of inspection", new IntWritable(7), inspector.getMapValueElement(map, new ShortWritable((short) 6)));}
0
public void setUp()
{    inspector = new ParquetHiveArrayInspector(PrimitiveObjectInspectorFactory.javaIntObjectInspector);}
0
public void testNullArray()
{    assertEquals("Wrong size", -1, inspector.getListLength(null));    assertNull("Should be null", inspector.getList(null));    assertNull("Should be null", inspector.getListElement(null, 0));}
0
public void testNullContainer()
{    final ArrayWritable list = new ArrayWritable(ArrayWritable.class, null);    assertEquals("Wrong size", -1, inspector.getListLength(list));    assertNull("Should be null", inspector.getList(list));    assertNull("Should be null", inspector.getListElement(list, 0));}
0
public void testEmptyContainer()
{    final ArrayWritable list = new ArrayWritable(ArrayWritable.class, new ArrayWritable[0]);    assertEquals("Wrong size", -1, inspector.getListLength(list));    assertNull("Should be null", inspector.getList(list));    assertNull("Should be null", inspector.getListElement(list, 0));}
0
public void testRegularList()
{    final ArrayWritable internalList = new ArrayWritable(Writable.class, new Writable[] { new IntWritable(3), new IntWritable(5), new IntWritable(1) });    final ArrayWritable list = new ArrayWritable(ArrayWritable.class, new ArrayWritable[] { internalList });    final List<Writable> expected = new ArrayList<Writable>();    expected.add(new IntWritable(3));    expected.add(new IntWritable(5));    expected.add(new IntWritable(1));    assertEquals("Wrong size", 3, inspector.getListLength(list));    assertEquals("Wrong result of inspection", expected, inspector.getList(list));    for (int i = 0; i < expected.size(); ++i) {        assertEquals("Wrong result of inspection", expected.get(i), inspector.getListElement(list, i));    }    assertNull("Should be null", inspector.getListElement(list, 3));}
0
public void setUp()
{    inspector = new StandardParquetHiveMapInspector(PrimitiveObjectInspectorFactory.javaIntObjectInspector, PrimitiveObjectInspectorFactory.javaIntObjectInspector);}
0
public void testNullMap()
{    assertNull("Should be null", inspector.getMapValueElement(null, new IntWritable(0)));}
0
public void testNullContainer()
{    final ArrayWritable map = new ArrayWritable(ArrayWritable.class, null);    assertNull("Should be null", inspector.getMapValueElement(map, new IntWritable(0)));}
0
public void testEmptyContainer()
{    final ArrayWritable map = new ArrayWritable(ArrayWritable.class, new ArrayWritable[0]);    assertNull("Should be null", inspector.getMapValueElement(map, new IntWritable(0)));}
0
public void testRegularMap()
{    final Writable[] entry1 = new Writable[] { new IntWritable(0), new IntWritable(1) };    final Writable[] entry2 = new Writable[] { new IntWritable(2), new IntWritable(3) };    final ArrayWritable internalMap = new ArrayWritable(ArrayWritable.class, new Writable[] { new ArrayWritable(Writable.class, entry1), new ArrayWritable(Writable.class, entry2) });    final ArrayWritable map = new ArrayWritable(ArrayWritable.class, new Writable[] { internalMap });    assertEquals("Wrong result of inspection", new IntWritable(1), inspector.getMapValueElement(map, new IntWritable(0)));    assertEquals("Wrong result of inspection", new IntWritable(3), inspector.getMapValueElement(map, new IntWritable(2)));    assertNull("Wrong result of inspection", inspector.getMapValueElement(map, new ShortWritable((short) 0)));    assertNull("Wrong result of inspection", inspector.getMapValueElement(map, new ShortWritable((short) 2)));}
0
public void testHashMap()
{    final Map<Writable, Writable> map = new HashMap<Writable, Writable>();    map.put(new IntWritable(0), new IntWritable(1));    map.put(new IntWritable(2), new IntWritable(3));    map.put(new IntWritable(4), new IntWritable(5));    map.put(new IntWritable(6), new IntWritable(7));    assertEquals("Wrong result of inspection", new IntWritable(1), inspector.getMapValueElement(map, new IntWritable(0)));    assertEquals("Wrong result of inspection", new IntWritable(3), inspector.getMapValueElement(map, new IntWritable(2)));    assertEquals("Wrong result of inspection", new IntWritable(5), inspector.getMapValueElement(map, new IntWritable(4)));    assertEquals("Wrong result of inspection", new IntWritable(7), inspector.getMapValueElement(map, new IntWritable(6)));    assertNull("Wrong result of inspection", inspector.getMapValueElement(map, new ShortWritable((short) 0)));    assertNull("Wrong result of inspection", inspector.getMapValueElement(map, new ShortWritable((short) 2)));    assertNull("Wrong result of inspection", inspector.getMapValueElement(map, new ShortWritable((short) 4)));    assertNull("Wrong result of inspection", inspector.getMapValueElement(map, new ShortWritable((short) 6)));}
0
private List<String> createHiveColumnsFrom(final String columnNamesStr)
{    List<String> columnNames;    if (columnNamesStr.length() == 0) {        columnNames = new ArrayList<String>();    } else {        columnNames = Arrays.asList(columnNamesStr.split(","));    }    return columnNames;}
0
private List<TypeInfo> createHiveTypeInfoFrom(final String columnsTypeStr)
{    List<TypeInfo> columnTypes;    if (columnsTypeStr.length() == 0) {        columnTypes = new ArrayList<TypeInfo>();    } else {        columnTypes = TypeInfoUtils.getTypeInfosFromTypeString(columnsTypeStr);    }    return columnTypes;}
0
private void testConversion(final String columnNamesStr, final String columnsTypeStr, final String expectedSchema) throws Exception
{    final List<String> columnNames = createHiveColumnsFrom(columnNamesStr);    final List<TypeInfo> columnTypes = createHiveTypeInfoFrom(columnsTypeStr);    final MessageType messageTypeFound = HiveSchemaConverter.convert(columnNames, columnTypes);    final MessageType expectedMT = MessageTypeParser.parseMessageType(expectedSchema);    assertEquals("converting " + columnNamesStr + ": " + columnsTypeStr + " to " + expectedSchema, expectedMT, messageTypeFound);}
0
public void testSimpleType() throws Exception
{    testConversion("a,b,c", "int,double,boolean", "message hive_schema {\n" + "  optional int32 a;\n" + "  optional double b;\n" + "  optional boolean c;\n" + "}\n");}
0
public void testArray() throws Exception
{    testConversion("arrayCol", "array<int>", "message hive_schema {\n" + "  optional group arrayCol (LIST) {\n" + "    repeated group bag {\n" + "      optional int32 array_element;\n" + "    }\n" + "  }\n" + "}\n");}
0
public void testStruct() throws Exception
{    testConversion("structCol", "struct<a:int,b:double,c:boolean>", "message hive_schema {\n" + "  optional group structCol {\n" + "    optional int32 a;\n" + "    optional double b;\n" + "    optional boolean c;\n" + "  }\n" + "}\n");}
0
public void testMap() throws Exception
{    testConversion("mapCol", "map<string,string>", "message hive_schema {\n" + "  optional group mapCol (MAP) {\n" + "    repeated group map (MAP_KEY_VALUE) {\n" + "      required binary key;\n" + "      optional binary value;\n" + "    }\n" + "  }\n" + "}\n");}
0
public void testMapOriginalType() throws Exception
{    final String hiveColumnTypes = "map<string,string>";    final String hiveColumnNames = "mapCol";    final List<String> columnNames = createHiveColumnsFrom(hiveColumnNames);    final List<TypeInfo> columnTypes = createHiveTypeInfoFrom(hiveColumnTypes);    final MessageType messageTypeFound = HiveSchemaConverter.convert(columnNames, columnTypes);        assertEquals(1, messageTypeFound.getFieldCount());    org.apache.parquet.schema.Type topLevel = messageTypeFound.getFields().get(0);    assertEquals("mapCol", topLevel.getName());    assertEquals(OriginalType.MAP, topLevel.getOriginalType());    assertEquals(Repetition.OPTIONAL, topLevel.getRepetition());    assertEquals(1, topLevel.asGroupType().getFieldCount());    org.apache.parquet.schema.Type secondLevel = topLevel.asGroupType().getFields().get(0);        assertEquals("map", secondLevel.getName());    assertEquals(OriginalType.MAP_KEY_VALUE, secondLevel.getOriginalType());    assertEquals(Repetition.REPEATED, secondLevel.getRepetition());}
0
public void testDefaultConstructor()
{    new MapredParquetInputFormat();}
0
public void testConstructorWithParquetInputFormat()
{    new MapredParquetInputFormat((ParquetInputFormat<ArrayWritable>) mock(ParquetInputFormat.class));}
0
public void testConstructor()
{    new MapredParquetOutputFormat();}
0
public void testConstructorWithFormat()
{    new MapredParquetOutputFormat((ParquetOutputFormat<ArrayWritable>) mock(ParquetOutputFormat.class));}
0
public void testGetRecordWriterThrowsException()
{    try {        new MapredParquetOutputFormat().getRecordWriter(null, null, null, null);        fail("should throw runtime exception.");    } catch (Exception e) {        assertEquals("Should never be used", e.getMessage());    }}
0
public void testGetHiveRecordWriter() throws IOException
{    Properties tableProps = new Properties();    tableProps.setProperty("columns", "foo,bar");    tableProps.setProperty("columns.types", "int:int");    final Progressable mockProgress = mock(Progressable.class);    final ParquetOutputFormat<ArrayWritable> outputFormat = (ParquetOutputFormat<ArrayWritable>) mock(ParquetOutputFormat.class);    JobConf jobConf = new JobConf();    try {        new MapredParquetOutputFormat(outputFormat) {            @Override            protected ParquetRecordWriterWrapper getParquerRecordWriterWrapper(ParquetOutputFormat<ArrayWritable> realOutputFormat, JobConf jobConf, String finalOutPath, Progressable progress) throws IOException {                assertEquals(outputFormat, realOutputFormat);                assertNotNull(jobConf.get(DataWritableWriteSupport.PARQUET_HIVE_SCHEMA));                assertEquals("/foo", finalOutPath.toString());                assertEquals(mockProgress, progress);                throw new RuntimeException("passed tests");            }        }.getHiveRecordWriter(jobConf, new Path("/foo"), null, false, tableProps, mockProgress);        fail("should throw runtime exception.");    } catch (RuntimeException e) {        assertEquals("passed tests", e.getMessage());    }}
0
protected ParquetRecordWriterWrapper getParquerRecordWriterWrapper(ParquetOutputFormat<ArrayWritable> realOutputFormat, JobConf jobConf, String finalOutPath, Progressable progress) throws IOException
{    assertEquals(outputFormat, realOutputFormat);    assertNotNull(jobConf.get(DataWritableWriteSupport.PARQUET_HIVE_SCHEMA));    assertEquals("/foo", finalOutPath.toString());    assertEquals(mockProgress, progress);    throw new RuntimeException("passed tests");}
0
public void testParquetHiveSerDe() throws Throwable
{    try {                System.out.println("test: testParquetHiveSerDe");        final ParquetHiveSerDe serDe = new ParquetHiveSerDe();        final Configuration conf = new Configuration();        final Properties tbl = createProperties();        serDe.initialize(conf, tbl);                final Writable[] arr = new Writable[8];        arr[0] = new ByteWritable((byte) 123);        arr[1] = new ShortWritable((short) 456);        arr[2] = new IntWritable(789);        arr[3] = new LongWritable(1000l);        arr[4] = new DoubleWritable((double) 5.3);        arr[5] = new BinaryWritable(Binary.fromString("hive and hadoop and parquet. Big family."));        final Writable[] mapContainer = new Writable[1];        final Writable[] map = new Writable[3];        for (int i = 0; i < 3; ++i) {            final Writable[] pair = new Writable[2];            pair[0] = new BinaryWritable(Binary.fromString("key_" + i));            pair[1] = new IntWritable(i);            map[i] = new ArrayWritable(Writable.class, pair);        }        mapContainer[0] = new ArrayWritable(Writable.class, map);        arr[6] = new ArrayWritable(Writable.class, mapContainer);        final Writable[] arrayContainer = new Writable[1];        final Writable[] array = new Writable[5];        for (int i = 0; i < 5; ++i) {            array[i] = new BinaryWritable(Binary.fromString("elem_" + i));        }        arrayContainer[0] = new ArrayWritable(Writable.class, array);        arr[7] = new ArrayWritable(Writable.class, arrayContainer);        final ArrayWritable arrWritable = new ArrayWritable(Writable.class, arr);                deserializeAndSerializeLazySimple(serDe, arrWritable);        System.out.println("test: testParquetHiveSerDe - OK");    } catch (final Throwable e) {        e.printStackTrace();        throw e;    }}
0
private void deserializeAndSerializeLazySimple(final ParquetHiveSerDe serDe, final ArrayWritable t) throws SerDeException
{        final StructObjectInspector oi = (StructObjectInspector) serDe.getObjectInspector();        final Object row = serDe.deserialize(t);    assertEquals("deserialization gives the wrong object class", row.getClass(), ArrayWritable.class);    assertEquals("size correct after deserialization", serDe.getSerDeStats().getRawDataSize(), t.get().length);    assertEquals("deserialization gives the wrong object", t, row);        final ArrayWritable serializedArr = (ArrayWritable) serDe.serialize(row, oi);    assertEquals("size correct after serialization", serDe.getSerDeStats().getRawDataSize(), serializedArr.get().length);    assertTrue("serialized object should be equal to starting object", arrayWritableEquals(t, serializedArr));}
0
private Properties createProperties()
{    final Properties tbl = new Properties();        tbl.setProperty("columns", "abyte,ashort,aint,along,adouble,astring,amap,alist");    tbl.setProperty("columns.types", "tinyint:smallint:int:bigint:double:string:map<string,int>:array<string>");    tbl.setProperty(org.apache.hadoop.hive.serde.serdeConstants.SERIALIZATION_NULL_FORMAT, "NULL");    return tbl;}
0
public static boolean arrayWritableEquals(final ArrayWritable a1, final ArrayWritable a2)
{    final Writable[] a1Arr = a1.get();    final Writable[] a2Arr = a2.get();    if (a1Arr.length != a2Arr.length) {        return false;    }    for (int i = 0; i < a1Arr.length; ++i) {        if (a1Arr[i] instanceof ArrayWritable) {            if (!(a2Arr[i] instanceof ArrayWritable)) {                return false;            }            if (!arrayWritableEquals((ArrayWritable) a1Arr[i], (ArrayWritable) a2Arr[i])) {                return false;            }        } else {            if (!a1Arr[i].equals(a2Arr[i])) {                return false;            }        }    }    return true;}
0
public static BigDecimal binaryToDecimal(Binary value, int precision, int scale)
{    /*     * Precision <= 18 checks for the max number of digits for an unscaled long,     * else treat with big integer conversion     */    if (precision <= 18) {        ByteBuffer buffer = value.toByteBuffer();        byte[] bytes = buffer.array();        int start = buffer.arrayOffset() + buffer.position();        int end = buffer.arrayOffset() + buffer.limit();        long unscaled = 0L;        int i = start;        while (i < end) {            unscaled = (unscaled << 8 | bytes[i] & 0xff);            i++;        }        int bits = 8 * (end - start);        long unscaledNew = (unscaled << (64 - bits)) >> (64 - bits);        if (unscaledNew <= -pow(10, 18) || unscaledNew >= pow(10, 18)) {            return new BigDecimal(unscaledNew);        } else {            return BigDecimal.valueOf(unscaledNew / pow(10, scale));        }    } else {        return new BigDecimal(new BigInteger(value.getBytes()), scale);    }}
0
public Converter getConverter(int fieldIndex)
{    if (fieldIndex != 0) {        throw new IllegalArgumentException("maps have only one field. can't reach " + fieldIndex);    }    return keyValue;}
0
public final void start()
{    buffer.clear();}
0
public void end()
{    parent.add(new LinkedHashMap<String, Object>(buffer));}
0
public Iterator<java.util.Map.Entry<String, Object>> iterator()
{    return entries.iterator();}
0
public int size()
{    return entries.size();}
0
public Tuple put(String key, Object value)
{    entries.add(new SimpleImmutableEntry<String, Object>(key, value));    return null;}
0
public void clear()
{    entries.clear();}
0
public Set<java.util.Map.Entry<String, Object>> entrySet()
{    return entrySet;}
0
 void add(Object value)
{    currentKey = value;}
0
 void add(Object value)
{    currentValue = value;}
0
public Converter getConverter(int fieldIndex)
{    if (fieldIndex == 0) {        return keyConverter;    } else if (fieldIndex == 1) {        return valueConverter;    }    throw new IllegalArgumentException("only the key (0) and value (1) fields expected: " + fieldIndex);}
0
public final void start()
{    currentKey = null;    currentValue = null;}
0
public void end()
{    buffer.put(currentKey.toString(), currentValue);    currentKey = null;    currentValue = null;}
0
public final void addBinary(Binary value)
{    currentKey = value.toStringUsingUTF8();}
0
 void add(Object value)
{    TupleConverter.this.set(index, value);}
0
private Type getType(boolean columnIndexAccess, String alias, int index)
{    if (columnIndexAccess) {        if (index < parquetSchema.getFieldCount()) {            return parquetSchema.getType(index);        }    } else {        return parquetSchema.getType(parquetSchema.getFieldIndex(alias));    }    return null;}
0
 static Converter newConverter(FieldSchema pigField, Type type, final ParentValueContainer parent, boolean elephantBirdCompatible, boolean columnIndexAccess)
{    try {        switch(pigField.type) {            case DataType.BAG:                return new BagConverter(type.asGroupType(), pigField, parent, elephantBirdCompatible, columnIndexAccess);            case DataType.MAP:                return new MapConverter(type.asGroupType(), pigField, parent, elephantBirdCompatible, columnIndexAccess);            case DataType.TUPLE:                return new TupleConverter(type.asGroupType(), pigField.schema, elephantBirdCompatible, columnIndexAccess) {                    @Override                    public void end() {                        super.end();                        parent.add(this.currentTuple);                    }                };            case DataType.CHARARRAY:                                return new FieldStringConverter(parent, type.getLogicalTypeAnnotation() instanceof LogicalTypeAnnotation.StringLogicalTypeAnnotation);            case DataType.BYTEARRAY:                return new FieldByteArrayConverter(parent);            case DataType.INTEGER:                return new FieldIntegerConverter(parent);            case DataType.BOOLEAN:                if (elephantBirdCompatible) {                    return new FieldIntegerConverter(parent);                } else {                    return new FieldBooleanConverter(parent);                }            case DataType.FLOAT:                return new FieldFloatConverter(parent);            case DataType.DOUBLE:                return new FieldDoubleConverter(parent);            case DataType.LONG:                return new FieldLongConverter(parent);            case DataType.BIGDECIMAL:                return new FieldBigDecimalConverter(type, parent);            default:                throw new TupleConversionException("unsupported pig type: " + pigField);        }    } catch (FrontendException e) {        throw new TupleConversionException("error while preparing converter for:\n" + pigField + "\n" + type, e);    } catch (RuntimeException e) {        throw new TupleConversionException("error while preparing converter for:\n" + pigField + "\n" + type, e);    }}
0
public void end()
{    super.end();    parent.add(this.currentTuple);}
0
public Converter getConverter(int fieldIndex)
{    return converters[fieldIndex];}
0
public final void start()
{    currentTuple = TF.newTuple(schemaSize);    if (elephantBirdCompatible) {        try {            int i = 0;            for (Type field : parquetSchema.getFields()) {                if (field.isPrimitive() && field.isRepetition(Repetition.OPTIONAL)) {                    PrimitiveType primitiveType = field.asPrimitiveType();                    switch(primitiveType.getPrimitiveTypeName()) {                        case INT32:                            currentTuple.set(i, I32_ZERO);                            break;                        case INT64:                            currentTuple.set(i, I64_ZERO);                            break;                        case FLOAT:                            currentTuple.set(i, FLOAT_ZERO);                            break;                        case DOUBLE:                            currentTuple.set(i, DOUBLE_ZERO);                            break;                        case BOOLEAN:                            currentTuple.set(i, I32_ZERO);                            break;                    }                }                ++i;            }        } catch (ExecException e) {            throw new RuntimeException(e);        }    }}
0
 final void set(int fieldIndex, Object value)
{    try {        currentTuple.set(fieldIndex, value);    } catch (ExecException e) {        throw new TupleConversionException("Could not set " + value + " to current tuple " + currentTuple + " at " + fieldIndex, e);    }}
0
public void end()
{}
0
public final Tuple getCurrentTuple()
{    return currentTuple;}
0
public final void addBinary(Binary value)
{    parent.add(value.toStringUsingUTF8());}
0
public boolean hasDictionarySupport()
{    return dictionarySupport;}
0
public void setDictionary(Dictionary dictionary)
{    dict = new String[dictionary.getMaxId() + 1];    for (int i = 0; i <= dictionary.getMaxId(); i++) {        dict[i] = dictionary.decodeToBinary(i).toStringUsingUTF8();    }}
0
public void addValueFromDictionary(int dictionaryId)
{    parent.add(dict[dictionaryId]);}
0
public void addLong(long value)
{    parent.add(Long.toString(value));}
0
public void addInt(int value)
{    parent.add(Integer.toString(value));}
0
public void addFloat(float value)
{    parent.add(Float.toString(value));}
0
public void addDouble(double value)
{    parent.add(Double.toString(value));}
0
public void addBoolean(boolean value)
{    parent.add(Boolean.toString(value));}
0
public final void addBinary(Binary value)
{    parent.add(new DataByteArray(value.getBytes()));}
0
public final void addDouble(double value)
{    parent.add(value);}
0
public void addLong(long value)
{    parent.add((double) value);}
0
public void addInt(int value)
{    parent.add((double) value);}
0
public void addFloat(float value)
{    parent.add((double) value);}
0
public void addBoolean(boolean value)
{    parent.add(value ? 1.0d : 0.0d);}
0
public void addBinary(Binary value)
{    parent.add(Double.parseDouble(value.toStringUsingUTF8()));}
0
public final void addFloat(float value)
{    parent.add(value);}
0
public void addLong(long value)
{    parent.add((float) value);}
0
public void addInt(int value)
{    parent.add((float) value);}
0
public void addDouble(double value)
{    parent.add((float) value);}
0
public void addBoolean(boolean value)
{    parent.add(value ? 1.0f : 0.0f);}
0
public void addBinary(Binary value)
{    parent.add(Float.parseFloat(value.toStringUsingUTF8()));}
0
public final void addLong(long value)
{    parent.add(value);}
0
public void addInt(int value)
{    parent.add((long) value);}
0
public void addFloat(float value)
{    parent.add((long) value);}
0
public void addDouble(double value)
{    parent.add((long) value);}
0
public void addBoolean(boolean value)
{    parent.add(value ? 1L : 0L);}
0
public void addBinary(Binary value)
{    parent.add(Long.parseLong(value.toStringUsingUTF8()));}
0
public final void addBoolean(boolean value)
{    parent.add(value ? 1 : 0);}
0
public final void addInt(int value)
{    parent.add(value);}
0
public void addLong(long value)
{    parent.add((int) value);}
0
public void addFloat(float value)
{    parent.add((int) value);}
0
public void addDouble(double value)
{    parent.add((int) value);}
0
public void addBinary(Binary value)
{    parent.add(Integer.parseInt(value.toStringUsingUTF8()));}
0
public final void addBoolean(boolean value)
{    parent.add(value);}
0
public final void addInt(int value)
{    parent.add(value != 0);}
0
public void addLong(long value)
{    parent.add(value != 0);}
0
public void addFloat(float value)
{    parent.add(value != 0);}
0
public void addDouble(double value)
{    parent.add(value != 0);}
0
public void addBinary(Binary value)
{    parent.add(Boolean.parseBoolean(value.toStringUsingUTF8()));}
0
public final void addBinary(Binary value)
{    int precision = primitiveType.asPrimitiveType().getDecimalMetadata().getPrecision();    int scale = primitiveType.asPrimitiveType().getDecimalMetadata().getScale();    BigDecimal finaldecimal = DecimalUtils.binaryToDecimal(value, precision, scale);    parent.add(finaldecimal);}
0
 void add(Object value)
{    buffer.add(TF.newTuple(value));}
0
 void add(Object value)
{    buffer.add((Tuple) value);}
0
public Converter getConverter(int fieldIndex)
{    if (fieldIndex != 0) {        throw new IllegalArgumentException("bags have only one field. can't reach " + fieldIndex);    }    return child;}
0
public final void start()
{    buffer.clear();}
0
public void end()
{    parent.add(new NonSpillableDataBag(new ArrayList<Tuple>(buffer)));}
0
public Tuple getCurrentRecord()
{    return root.getCurrentTuple();}
0
public GroupConverter getRootConverter()
{    return root;}
0
public void setLocation(String location, Job job) throws IOException
{    if (LOG.isDebugEnabled()) {        String jobToString = String.format("job[id=%s, name=%s]", job.getJobID(), job.getJobName());            }    setInput(location, job);}
1
private void setInput(String location, Job job) throws IOException
{    this.setLocationHasBeenCalled = true;    this.location = location;    setInputPaths(job, location);        if (UDFContext.getUDFContext().isFrontend()) {        storeInUDFContext(PARQUET_COLUMN_INDEX_ACCESS, Boolean.toString(columnIndexAccess));    }    schema = PigSchemaConverter.parsePigSchema(getPropertyFromUDFContext(PARQUET_PIG_SCHEMA));    requiredFieldList = PigSchemaConverter.deserializeRequiredFieldList(getPropertyFromUDFContext(PARQUET_PIG_REQUIRED_FIELDS));    columnIndexAccess = Boolean.parseBoolean(getPropertyFromUDFContext(PARQUET_COLUMN_INDEX_ACCESS));    initSchema(job);    if (UDFContext.getUDFContext().isFrontend()) {                storeInUDFContext(PARQUET_PIG_SCHEMA, pigSchemaToString(schema));        storeInUDFContext(PARQUET_PIG_REQUIRED_FIELDS, serializeRequiredFieldList(requiredFieldList));    }        getConfiguration(job).set(PARQUET_PIG_SCHEMA, pigSchemaToString(schema));    getConfiguration(job).set(PARQUET_PIG_REQUIRED_FIELDS, serializeRequiredFieldList(requiredFieldList));    getConfiguration(job).set(PARQUET_COLUMN_INDEX_ACCESS, Boolean.toString(columnIndexAccess));    FilterPredicate filterPredicate = (FilterPredicate) getFromUDFContext(ParquetInputFormat.FILTER_PREDICATE);    if (filterPredicate != null) {        ParquetInputFormat.setFilterPredicate(getConfiguration(job), filterPredicate);    }}
0
public InputFormat<Void, Tuple> getInputFormat() throws IOException
{        return getParquetInputFormat();}
1
private void checkSetLocationHasBeenCalled()
{    if (!setLocationHasBeenCalled) {        throw new IllegalStateException("setLocation() must be called first");    }}
0
public RecordReader<Void, Tuple> createRecordReader(InputSplit inputSplit, TaskAttemptContext taskAttemptContext) throws IOException, InterruptedException
{        inputFormatCache.remove(location);    return super.createRecordReader(inputSplit, taskAttemptContext);}
0
private ParquetInputFormat<Tuple> getParquetInputFormat() throws ParserException
{    checkSetLocationHasBeenCalled();    if (parquetInputFormat == null) {                Reference<ParquetInputFormat<Tuple>> ref = inputFormatCache.get(location);        parquetInputFormat = ref == null ? null : ref.get();        if (parquetInputFormat == null) {            parquetInputFormat = new UnregisteringParquetInputFormat(location);            inputFormatCache.put(location, new SoftReference<ParquetInputFormat<Tuple>>(parquetInputFormat));        }    }    return parquetInputFormat;}
0
public void prepareToRead(@SuppressWarnings("rawtypes") RecordReader reader, PigSplit split) throws IOException
{        this.reader = reader;}
1
public Tuple getNext() throws IOException
{    try {        if (reader.nextKeyValue()) {            return (Tuple) reader.getCurrentValue();        } else {            return null;        }    } catch (InterruptedException e) {        Thread.interrupted();        throw new ParquetDecodingException("Interrupted", e);    }}
0
public String[] getPartitionKeys(String location, Job job) throws IOException
{    if (LOG.isDebugEnabled()) {        String jobToString = String.format("job[id=%s, name=%s]", job.getJobID(), job.getJobName());            }    setInput(location, job);    return null;}
1
public ResourceSchema getSchema(String location, Job job) throws IOException
{    if (LOG.isDebugEnabled()) {        String jobToString = String.format("job[id=%s, name=%s]", job.getJobID(), job.getJobName());            }    setInput(location, job);    return new ResourceSchema(schema);}
1
private void initSchema(Job job) throws IOException
{    if (schema != null) {        return;    }    if (schema == null && requestedSchema != null) {                schema = requestedSchema;    }    if (schema == null) {                final GlobalMetaData globalMetaData = getParquetInputFormat().getGlobalMetaData(job);        schema = getPigSchemaFromMultipleFiles(globalMetaData.getSchema(), globalMetaData.getKeyValueMetaData());    }    if (isElephantBirdCompatible(job)) {        convertToElephantBirdCompatibleSchema(schema);    }}
0
private void convertToElephantBirdCompatibleSchema(Schema schema)
{    if (schema == null) {        return;    }    for (FieldSchema fieldSchema : schema.getFields()) {        if (fieldSchema.type == DataType.BOOLEAN) {            fieldSchema.type = DataType.INTEGER;        }        convertToElephantBirdCompatibleSchema(fieldSchema.schema);    }}
0
private boolean isElephantBirdCompatible(Job job)
{    return getConfiguration(job).getBoolean(TupleReadSupport.PARQUET_PIG_ELEPHANT_BIRD_COMPATIBLE, false);}
0
public ResourceStatistics getStatistics(String location, Job job) throws IOException
{    if (LOG.isDebugEnabled()) {        String jobToString = String.format("job[id=%s, name=%s]", job.getJobID(), job.getJobName());            }    /* We need to call setInput since setLocation is not       guaranteed to be called before this */    setInput(location, job);    long length = 0;    try {        for (InputSplit split : getParquetInputFormat().getSplits(job)) {            length += split.getLength();        }    } catch (InterruptedException e) {                return null;    }    ResourceStatistics stats = new ResourceStatistics();        stats.setmBytes(length / 1024 / 1024);    return stats;}
1
public void setPartitionFilter(Expression expression) throws IOException
{    }
1
public List<OperatorSet> getFeatures()
{    return asList(LoadPushDown.OperatorSet.PROJECTION);}
0
protected String getPropertyFromUDFContext(String key)
{    UDFContext udfContext = UDFContext.getUDFContext();    return udfContext.getUDFProperties(this.getClass(), new String[] { signature }).getProperty(key);}
0
protected Object getFromUDFContext(String key)
{    UDFContext udfContext = UDFContext.getUDFContext();    return udfContext.getUDFProperties(this.getClass(), new String[] { signature }).get(key);}
0
protected void storeInUDFContext(String key, Object value)
{    UDFContext udfContext = UDFContext.getUDFContext();    java.util.Properties props = udfContext.getUDFProperties(this.getClass(), new String[] { signature });    props.put(key, value);}
0
public RequiredFieldResponse pushProjection(RequiredFieldList requiredFieldList) throws FrontendException
{    this.requiredFieldList = requiredFieldList;    if (requiredFieldList == null)        return null;    schema = getSchemaFromRequiredFieldList(schema, requiredFieldList.getFields());    storeInUDFContext(PARQUET_PIG_SCHEMA, pigSchemaToString(schema));    storeInUDFContext(PARQUET_PIG_REQUIRED_FIELDS, serializeRequiredFieldList(requiredFieldList));    return new RequiredFieldResponse(true);}
0
public void setUDFContextSignature(String signature)
{    this.signature = signature;}
0
private Schema getSchemaFromRequiredFieldList(Schema schema, List<RequiredField> fieldList) throws FrontendException
{    Schema s = new Schema();    for (RequiredField rf : fieldList) {        FieldSchema f;        try {            f = schema.getField(rf.getAlias()).clone();        } catch (CloneNotSupportedException e) {            throw new FrontendException("Clone not supported for the fieldschema", e);        }        if (rf.getSubFields() == null) {            s.add(f);        } else {            Schema innerSchema = getSchemaFromRequiredFieldList(f.schema, rf.getSubFields());            if (innerSchema == null) {                return null;            } else {                f.schema = innerSchema;                s.add(f);            }        }    }    return s;}
0
public List<String> getPredicateFields(String s, Job job) throws IOException
{    if (!job.getConfiguration().getBoolean(ENABLE_PREDICATE_FILTER_PUSHDOWN, DEFAULT_PREDICATE_PUSHDOWN_ENABLED)) {        return null;    }    List<String> fields = new ArrayList<String>();    for (FieldSchema field : schema.getFields()) {        switch(field.type) {            case DataType.BOOLEAN:            case DataType.INTEGER:            case DataType.LONG:            case DataType.FLOAT:            case DataType.DOUBLE:            case DataType.CHARARRAY:                fields.add(field.alias);                break;            default:                                break;        }    }    return fields;}
0
public List<Expression.OpType> getSupportedExpressionTypes()
{    OpType[] supportedTypes = { OpType.OP_EQ, OpType.OP_NE, OpType.OP_GT, OpType.OP_GE, OpType.OP_LT, OpType.OP_LE, OpType.OP_AND, OpType.OP_OR,     OpType.OP_NOT };    return Arrays.asList(supportedTypes);}
0
public void setPushdownPredicate(Expression e) throws IOException
{        FilterPredicate pred = buildFilter(e);        storeInUDFContext(ParquetInputFormat.FILTER_PREDICATE, pred);}
1
private FilterPredicate buildFilter(Expression e)
{    OpType op = e.getOpType();    if (e instanceof BinaryExpression) {        Expression lhs = ((BinaryExpression) e).getLhs();        Expression rhs = ((BinaryExpression) e).getRhs();        switch(op) {            case OP_AND:                return and(buildFilter(lhs), buildFilter(rhs));            case OP_OR:                return or(buildFilter(lhs), buildFilter(rhs));            case OP_BETWEEN:                BetweenExpression between = (BetweenExpression) rhs;                return and(buildFilter(OpType.OP_GE, (Column) lhs, (Const) between.getLower()), buildFilter(OpType.OP_LE, (Column) lhs, (Const) between.getUpper()));            case OP_IN:                FilterPredicate current = null;                for (Object value : ((InExpression) rhs).getValues()) {                    FilterPredicate next = buildFilter(OpType.OP_EQ, (Column) lhs, (Const) value);                    if (current != null) {                        current = or(current, next);                    } else {                        current = next;                    }                }                return current;        }        if (lhs instanceof Column && rhs instanceof Const) {            return buildFilter(op, (Column) lhs, (Const) rhs);        } else if (lhs instanceof Const && rhs instanceof Column) {            return buildFilter(op, (Column) rhs, (Const) lhs);        }    } else if (e instanceof UnaryExpression && op == OpType.OP_NOT) {        return LogicalInverseRewriter.rewrite(not(buildFilter(((UnaryExpression) e).getExpression())));    }    throw new RuntimeException("Could not build filter for expression: " + e);}
0
private FilterPredicate buildFilter(OpType op, Column col, Const value)
{    String name = col.getName();    try {        FieldSchema f = schema.getField(name);        switch(f.type) {            case DataType.BOOLEAN:                Operators.BooleanColumn boolCol = booleanColumn(name);                switch(op) {                    case OP_EQ:                        return eq(boolCol, getValue(value, boolCol.getColumnType()));                    case OP_NE:                        return notEq(boolCol, getValue(value, boolCol.getColumnType()));                    default:                        throw new RuntimeException("Operation " + op + " not supported for boolean column: " + name);                }            case DataType.INTEGER:                Operators.IntColumn intCol = intColumn(name);                return op(op, intCol, value);            case DataType.LONG:                Operators.LongColumn longCol = longColumn(name);                return op(op, longCol, value);            case DataType.FLOAT:                Operators.FloatColumn floatCol = floatColumn(name);                return op(op, floatCol, value);            case DataType.DOUBLE:                Operators.DoubleColumn doubleCol = doubleColumn(name);                return op(op, doubleCol, value);            case DataType.CHARARRAY:                Operators.BinaryColumn binaryCol = binaryColumn(name);                return op(op, binaryCol, value);            default:                throw new RuntimeException("Unsupported type " + f.type + " for field: " + name);        }    } catch (FrontendException e) {        throw new RuntimeException("Error processing pushdown for column:" + col, e);    }}
0
private static FilterPredicate op(Expression.OpType op, COL col, Const valueExpr)
{    C value = getValue(valueExpr, col.getColumnType());    switch(op) {        case OP_EQ:            return eq(col, value);        case OP_NE:            return notEq(col, value);        case OP_GT:            return gt(col, value);        case OP_GE:            return gtEq(col, value);        case OP_LT:            return lt(col, value);        case OP_LE:            return ltEq(col, value);    }    return null;}
0
private static C getValue(Const valueExpr, Class<C> type)
{    Object value = valueExpr.getValue();    if (value instanceof String) {        value = Binary.fromString((String) value);    }    return type.cast(value);}
0
private Properties getProperties()
{    UDFContext udfc = UDFContext.getUDFContext();    Properties p = udfc.getUDFProperties(this.getClass(), new String[] { signature });    return p;}
0
private Schema getSchema()
{    try {        final String schemaString = getProperties().getProperty(SCHEMA);        if (schemaString == null) {            throw new ParquetEncodingException("Can not store relation in Parquet as the schema is unknown");        }        return Utils.getSchemaFromString(schemaString);    } catch (ParserException e) {        throw new ParquetEncodingException("can not get schema from context", e);    }}
0
public void setStoreFuncUDFContextSignature(String signature)
{    super.setStoreFuncUDFContextSignature(signature);    this.signature = signature;}
0
public void checkSchema(ResourceSchema s) throws IOException
{    getProperties().setProperty(SCHEMA, s.toString());}
0
public OutputFormat<Void, Tuple> getOutputFormat() throws IOException
{    Schema pigSchema = getSchema();    return new ParquetOutputFormat<Tuple>(new TupleWriteSupport(pigSchema));}
0
public void prepareToWrite(RecordWriter recordWriter) throws IOException
{    this.recordWriter = recordWriter;}
0
public void putNext(Tuple tuple) throws IOException
{    try {        this.recordWriter.write(null, tuple);    } catch (InterruptedException e) {        Thread.interrupted();        throw new ParquetEncodingException("Interrupted while writing", e);    }}
0
public void setStoreLocation(String location, Job job) throws IOException
{    FileOutputFormat.setOutputPath(job, new Path(location));}
0
public void storeSchema(ResourceSchema schema, String location, Job job) throws IOException
{}
0
public void storeStatistics(ResourceStatistics resourceStatistics, String location, Job job) throws IOException
{}
0
public static PigMetaData fromMetaData(Map<String, String> keyValueMetaData)
{    if (keyValueMetaData.containsKey(PIG_SCHEMA)) {        return new PigMetaData(keyValueMetaData.get(PIG_SCHEMA));    }    return null;}
0
public static Set<String> getPigSchemas(Map<String, Set<String>> keyValueMetaData)
{    return keyValueMetaData.get(PIG_SCHEMA);}
0
public void setPigSchema(String pigSchema)
{    this.pigSchema = pigSchema;}
0
public String getPigSchema()
{    return pigSchema;}
0
public void addToMetaData(Map<String, String> map)
{    map.put(PIG_SCHEMA, pigSchema);}
0
public static Schema parsePigSchema(String pigSchemaString)
{    try {        return pigSchemaString == null ? null : Utils.getSchemaFromString(pigSchemaString);    } catch (ParserException e) {        throw new SchemaConversionException("could not parse Pig schema: " + pigSchemaString, e);    }}
0
public List<Type> filterTupleSchema(GroupType schemaToFilter, Schema pigSchema, RequiredFieldList requiredFieldsList)
{    List<Type> newFields = new ArrayList<Type>();    List<Pair<FieldSchema, Integer>> indexedFields = new ArrayList<Pair<FieldSchema, Integer>>();    try {        if (requiredFieldsList == null) {            int index = 0;            for (FieldSchema fs : pigSchema.getFields()) {                indexedFields.add(new Pair<FieldSchema, Integer>(fs, index++));            }        } else {            for (RequiredField rf : requiredFieldsList.getFields()) {                indexedFields.add(new Pair<FieldSchema, Integer>(pigSchema.getField(rf.getAlias()), rf.getIndex()));            }        }        for (Pair<FieldSchema, Integer> p : indexedFields) {            FieldSchema fieldSchema = pigSchema.getField(p.first.alias);            if (p.second < schemaToFilter.getFieldCount()) {                Type type = schemaToFilter.getFields().get(p.second);                newFields.add(filter(type, fieldSchema));            }        }    } catch (FrontendException e) {        throw new RuntimeException("Failed to filter requested fields", e);    }    return newFields;}
0
public List<Type> filterTupleSchema(GroupType schemaToFilter, Schema requestedPigSchema, RequiredFieldList requiredFieldsList)
{    List<FieldSchema> fields = requestedPigSchema.getFields();    List<Type> newFields = new ArrayList<Type>();    for (int i = 0; i < fields.size(); i++) {        FieldSchema fieldSchema = fields.get(i);        String name = name(fieldSchema.alias, "field_" + i);        if (schemaToFilter.containsField(name)) {            newFields.add(filter(schemaToFilter.getType(name), fieldSchema));        }    }    return newFields;}
0
 static String pigSchemaToString(Schema pigSchema)
{    final String pigSchemaString = pigSchema.toString();    return pigSchemaString.substring(1, pigSchemaString.length() - 1);}
0
public static RequiredFieldList deserializeRequiredFieldList(String requiredFieldString)
{    if (requiredFieldString == null) {        return null;    }    try {        return (RequiredFieldList) ObjectSerializer.deserialize(requiredFieldString);    } catch (IOException e) {        throw new RuntimeException("Failed to deserialize pushProjection", e);    }}
0
 static String serializeRequiredFieldList(RequiredFieldList requiredFieldList)
{    try {        return ObjectSerializer.serialize(requiredFieldList);    } catch (IOException e) {        throw new RuntimeException("Failed to searlize required fields.", e);    }}
0
public Schema convert(MessageType parquetSchema)
{    return convertFields(parquetSchema.getFields());}
0
public Schema convertField(Type parquetType)
{    return convertFields(Arrays.asList(parquetType));}
0
private Schema convertFields(List<Type> parquetFields)
{    List<FieldSchema> fields = new ArrayList<Schema.FieldSchema>();    for (Type parquetType : parquetFields) {        try {            FieldSchema innerfieldSchema = getFieldSchema(parquetType);            if (parquetType.isRepetition(Repetition.REPEATED)) {                Schema bagSchema = new Schema(Arrays.asList(innerfieldSchema));                fields.add(new FieldSchema(null, bagSchema, DataType.BAG));            } else {                fields.add(innerfieldSchema);            }        } catch (FrontendException fe) {            throw new SchemaConversionException("can't convert " + parquetType, fe);        }    }    return new Schema(fields);}
0
private FieldSchema getSimpleFieldSchema(final String fieldName, Type parquetType) throws FrontendException
{    final PrimitiveTypeName parquetPrimitiveTypeName = parquetType.asPrimitiveType().getPrimitiveTypeName();    final LogicalTypeAnnotation logicalTypeAnnotation = parquetType.getLogicalTypeAnnotation();    return parquetPrimitiveTypeName.convert(new PrimitiveTypeNameConverter<Schema.FieldSchema, FrontendException>() {        @Override        public FieldSchema convertFLOAT(PrimitiveTypeName primitiveTypeName) throws FrontendException {            return new FieldSchema(fieldName, null, DataType.FLOAT);        }        @Override        public FieldSchema convertDOUBLE(PrimitiveTypeName primitiveTypeName) throws FrontendException {            return new FieldSchema(fieldName, null, DataType.DOUBLE);        }        @Override        public FieldSchema convertINT32(PrimitiveTypeName primitiveTypeName) throws FrontendException {            return new FieldSchema(fieldName, null, DataType.INTEGER);        }        @Override        public FieldSchema convertINT64(PrimitiveTypeName primitiveTypeName) throws FrontendException {            return new FieldSchema(fieldName, null, DataType.LONG);        }        @Override        public FieldSchema convertINT96(PrimitiveTypeName primitiveTypeName) throws FrontendException {                        return new FieldSchema(fieldName, null, DataType.BYTEARRAY);        }        @Override        public FieldSchema convertFIXED_LEN_BYTE_ARRAY(PrimitiveTypeName primitiveTypeName) throws FrontendException {            if (logicalTypeAnnotation instanceof LogicalTypeAnnotation.DecimalLogicalTypeAnnotation) {                return new FieldSchema(fieldName, null, DataType.BIGDECIMAL);            } else {                return new FieldSchema(fieldName, null, DataType.BYTEARRAY);            }        }        @Override        public FieldSchema convertBOOLEAN(PrimitiveTypeName primitiveTypeName) throws FrontendException {            return new FieldSchema(fieldName, null, DataType.BOOLEAN);        }        @Override        public FieldSchema convertBINARY(PrimitiveTypeName primitiveTypeName) throws FrontendException {            if (logicalTypeAnnotation instanceof LogicalTypeAnnotation.StringLogicalTypeAnnotation) {                return new FieldSchema(fieldName, null, DataType.CHARARRAY);            } else {                return new FieldSchema(fieldName, null, DataType.BYTEARRAY);            }        }    });}
1
public FieldSchema convertFLOAT(PrimitiveTypeName primitiveTypeName) throws FrontendException
{    return new FieldSchema(fieldName, null, DataType.FLOAT);}
0
public FieldSchema convertDOUBLE(PrimitiveTypeName primitiveTypeName) throws FrontendException
{    return new FieldSchema(fieldName, null, DataType.DOUBLE);}
0
public FieldSchema convertINT32(PrimitiveTypeName primitiveTypeName) throws FrontendException
{    return new FieldSchema(fieldName, null, DataType.INTEGER);}
0
public FieldSchema convertINT64(PrimitiveTypeName primitiveTypeName) throws FrontendException
{    return new FieldSchema(fieldName, null, DataType.LONG);}
0
public FieldSchema convertINT96(PrimitiveTypeName primitiveTypeName) throws FrontendException
{        return new FieldSchema(fieldName, null, DataType.BYTEARRAY);}
1
public FieldSchema convertFIXED_LEN_BYTE_ARRAY(PrimitiveTypeName primitiveTypeName) throws FrontendException
{    if (logicalTypeAnnotation instanceof LogicalTypeAnnotation.DecimalLogicalTypeAnnotation) {        return new FieldSchema(fieldName, null, DataType.BIGDECIMAL);    } else {        return new FieldSchema(fieldName, null, DataType.BYTEARRAY);    }}
0
public FieldSchema convertBOOLEAN(PrimitiveTypeName primitiveTypeName) throws FrontendException
{    return new FieldSchema(fieldName, null, DataType.BOOLEAN);}
0
public FieldSchema convertBINARY(PrimitiveTypeName primitiveTypeName) throws FrontendException
{    if (logicalTypeAnnotation instanceof LogicalTypeAnnotation.StringLogicalTypeAnnotation) {        return new FieldSchema(fieldName, null, DataType.CHARARRAY);    } else {        return new FieldSchema(fieldName, null, DataType.BYTEARRAY);    }}
0
private FieldSchema getComplexFieldSchema(String fieldName, Type parquetType) throws FrontendException
{    GroupType parquetGroupType = parquetType.asGroupType();    LogicalTypeAnnotation logicalTypeAnnotation = parquetGroupType.getLogicalTypeAnnotation();    if (logicalTypeAnnotation != null) {        try {            return logicalTypeAnnotation.accept(new LogicalTypeAnnotation.LogicalTypeAnnotationVisitor<FieldSchema>() {                @Override                public Optional<FieldSchema> visit(LogicalTypeAnnotation.MapLogicalTypeAnnotation mapLogicalType) {                    try {                                                if (parquetGroupType.getFieldCount() != 1 || parquetGroupType.getType(0).isPrimitive()) {                            throw new SchemaConversionException("Invalid map type " + parquetGroupType);                        }                        GroupType mapKeyValType = parquetGroupType.getType(0).asGroupType();                        if (!mapKeyValType.isRepetition(Repetition.REPEATED) || (mapKeyValType.getLogicalTypeAnnotation() != null && !mapKeyValType.getLogicalTypeAnnotation().equals(LogicalTypeAnnotation.MapKeyValueTypeAnnotation.getInstance())) || mapKeyValType.getFieldCount() != 2) {                            throw new SchemaConversionException("Invalid map type " + parquetGroupType);                        }                                                Type valueType = mapKeyValType.getType(1);                        Schema s = convertField(valueType);                        s.getField(0).alias = null;                        return of(new FieldSchema(fieldName, s, DataType.MAP));                    } catch (FrontendException e) {                        throw new FrontendExceptionWrapper(e);                    }                }                @Override                public Optional<FieldSchema> visit(LogicalTypeAnnotation.ListLogicalTypeAnnotation listLogicalType) {                    try {                        Type type = parquetGroupType.getType(0);                        if (parquetGroupType.getFieldCount() != 1 || type.isPrimitive()) {                                                        Schema primitiveSchema = new Schema(getSimpleFieldSchema(parquetGroupType.getFieldName(0), type));                            Schema tupleSchema = new Schema(new FieldSchema(ARRAY_VALUE_NAME, primitiveSchema, DataType.TUPLE));                            return of(new FieldSchema(fieldName, tupleSchema, DataType.BAG));                        }                        GroupType tupleType = parquetGroupType.getType(0).asGroupType();                        if (!tupleType.isRepetition(Repetition.REPEATED)) {                            throw new SchemaConversionException("Invalid list type " + parquetGroupType);                        }                        Schema tupleSchema = new Schema(new FieldSchema(tupleType.getName(), convertFields(tupleType.getFields()), DataType.TUPLE));                        return of(new FieldSchema(fieldName, tupleSchema, DataType.BAG));                    } catch (FrontendException e) {                        throw new FrontendExceptionWrapper(e);                    }                }            }).orElseThrow(() -> new SchemaConversionException("Unexpected original type for " + parquetType + ": " + logicalTypeAnnotation));        } catch (FrontendExceptionWrapper e) {            throw e.frontendException;        }    } else {                return new FieldSchema(fieldName, convertFields(parquetGroupType.getFields()), DataType.TUPLE);    }}
0
public Optional<FieldSchema> visit(LogicalTypeAnnotation.MapLogicalTypeAnnotation mapLogicalType)
{    try {                if (parquetGroupType.getFieldCount() != 1 || parquetGroupType.getType(0).isPrimitive()) {            throw new SchemaConversionException("Invalid map type " + parquetGroupType);        }        GroupType mapKeyValType = parquetGroupType.getType(0).asGroupType();        if (!mapKeyValType.isRepetition(Repetition.REPEATED) || (mapKeyValType.getLogicalTypeAnnotation() != null && !mapKeyValType.getLogicalTypeAnnotation().equals(LogicalTypeAnnotation.MapKeyValueTypeAnnotation.getInstance())) || mapKeyValType.getFieldCount() != 2) {            throw new SchemaConversionException("Invalid map type " + parquetGroupType);        }                Type valueType = mapKeyValType.getType(1);        Schema s = convertField(valueType);        s.getField(0).alias = null;        return of(new FieldSchema(fieldName, s, DataType.MAP));    } catch (FrontendException e) {        throw new FrontendExceptionWrapper(e);    }}
0
public Optional<FieldSchema> visit(LogicalTypeAnnotation.ListLogicalTypeAnnotation listLogicalType)
{    try {        Type type = parquetGroupType.getType(0);        if (parquetGroupType.getFieldCount() != 1 || type.isPrimitive()) {                        Schema primitiveSchema = new Schema(getSimpleFieldSchema(parquetGroupType.getFieldName(0), type));            Schema tupleSchema = new Schema(new FieldSchema(ARRAY_VALUE_NAME, primitiveSchema, DataType.TUPLE));            return of(new FieldSchema(fieldName, tupleSchema, DataType.BAG));        }        GroupType tupleType = parquetGroupType.getType(0).asGroupType();        if (!tupleType.isRepetition(Repetition.REPEATED)) {            throw new SchemaConversionException("Invalid list type " + parquetGroupType);        }        Schema tupleSchema = new Schema(new FieldSchema(tupleType.getName(), convertFields(tupleType.getFields()), DataType.TUPLE));        return of(new FieldSchema(fieldName, tupleSchema, DataType.BAG));    } catch (FrontendException e) {        throw new FrontendExceptionWrapper(e);    }}
0
private FieldSchema getFieldSchema(Type parquetType) throws FrontendException
{    final String fieldName = parquetType.getName();    if (parquetType.isPrimitive()) {        return getSimpleFieldSchema(fieldName, parquetType);    } else {        return getComplexFieldSchema(fieldName, parquetType);    }}
0
public MessageType convert(Schema pigSchema)
{    return new MessageType("pig_schema", convertTypes(pigSchema));}
0
private Type[] convertTypes(Schema pigSchema)
{    List<FieldSchema> fields = pigSchema.getFields();    Type[] types = new Type[fields.size()];    for (int i = 0; i < types.length; i++) {        types[i] = convert(fields.get(i), i);    }    return types;}
0
private Type convert(FieldSchema fieldSchema, String defaultAlias)
{    String name = name(fieldSchema.alias, defaultAlias);    return convertWithName(fieldSchema, name);}
0
private Type convertWithName(FieldSchema fieldSchema, String name)
{    try {        switch(fieldSchema.type) {            case DataType.BAG:                return convertBag(name, fieldSchema);            case DataType.TUPLE:                return convertTuple(name, fieldSchema, Repetition.OPTIONAL);            case DataType.MAP:                return convertMap(name, fieldSchema);            case DataType.BOOLEAN:                return primitive(name, PrimitiveTypeName.BOOLEAN);            case DataType.CHARARRAY:                return primitive(name, PrimitiveTypeName.BINARY, stringType());            case DataType.INTEGER:                return primitive(name, PrimitiveTypeName.INT32);            case DataType.LONG:                return primitive(name, PrimitiveTypeName.INT64);            case DataType.FLOAT:                return primitive(name, PrimitiveTypeName.FLOAT);            case DataType.DOUBLE:                return primitive(name, PrimitiveTypeName.DOUBLE);            case DataType.DATETIME:                throw new UnsupportedOperationException();            case DataType.BYTEARRAY:                return primitive(name, PrimitiveTypeName.BINARY);            default:                throw new SchemaConversionException("Unknown type " + fieldSchema.type + " " + DataType.findTypeName(fieldSchema.type));        }    } catch (FrontendException e) {        throw new SchemaConversionException("can't convert " + fieldSchema, e);    }}
0
private Type convert(FieldSchema fieldSchema, int index)
{    return convert(fieldSchema, "field_" + index);}
0
private GroupType convertBag(String name, FieldSchema fieldSchema) throws FrontendException
{    FieldSchema innerField = fieldSchema.schema.getField(0);    return ConversionPatterns.listType(Repetition.OPTIONAL, name, convertTuple(name(innerField.alias, "bag"), innerField, Repetition.REPEATED));}
0
private String name(String fieldAlias, String defaultName)
{    return fieldAlias == null ? defaultName : fieldAlias;}
0
private Type primitive(String name, PrimitiveTypeName primitive, LogicalTypeAnnotation logicalTypeAnnotation)
{    return Types.primitive(primitive, Repetition.OPTIONAL).as(logicalTypeAnnotation).named(name);}
0
private PrimitiveType primitive(String name, PrimitiveTypeName primitive)
{    return Types.primitive(primitive, Repetition.OPTIONAL).named(name);}
0
private GroupType convertMap(String alias, FieldSchema fieldSchema)
{    Schema innerSchema = fieldSchema.schema;    if (innerSchema == null || innerSchema.size() != 1) {        throw new SchemaConversionException("Invalid map Schema, schema should contain exactly one field: " + fieldSchema);    }    FieldSchema innerField = null;    try {        innerField = innerSchema.getField(0);    } catch (FrontendException fe) {        throw new SchemaConversionException("Invalid map schema, cannot infer innerschema: ", fe);    }    Type convertedValue = convertWithName(innerField, "value");    return ConversionPatterns.stringKeyMapType(Repetition.OPTIONAL, alias, name(innerField.alias, "map"), convertedValue);}
0
private GroupType convertTuple(String alias, FieldSchema field, Repetition repetition)
{    return new GroupType(repetition, alias, convertTypes(field.schema));}
0
public MessageType filter(MessageType schemaToFilter, Schema requestedPigSchema)
{    return filter(schemaToFilter, requestedPigSchema, null);}
0
private Type filter(Type type, FieldSchema fieldSchema)
{    if (LOG.isDebugEnabled())            try {        switch(fieldSchema.type) {            case DataType.BAG:                return filterBag(type.asGroupType(), fieldSchema);            case DataType.MAP:                return filterMap(type.asGroupType(), fieldSchema);            case DataType.TUPLE:                return filterTuple(type.asGroupType(), fieldSchema);            default:                return type;        }    } catch (FrontendException e) {        throw new SchemaConversionException("can't filter " + type + " with " + fieldSchema, e);    } catch (RuntimeException e) {        throw new RuntimeException("can't filter " + type + " with " + fieldSchema, e);    }}
1
private Type filterTuple(GroupType tupleType, FieldSchema tupleFieldSchema) throws FrontendException
{    if (LOG.isDebugEnabled())            return tupleType.withNewFields(columnAccess.filterTupleSchema(tupleType, tupleFieldSchema.schema, null));}
1
private Type filterMap(GroupType mapType, FieldSchema mapFieldSchema) throws FrontendException
{    if (LOG.isDebugEnabled())            if (mapType.getFieldCount() != 1) {        throw new RuntimeException("not unwrapping the right type, this should be a Map: " + mapType);    }    GroupType nested = mapType.getType(0).asGroupType();    if (nested.getFieldCount() != 2) {        throw new RuntimeException("this should be a Map Key/Value: " + mapType);    }    FieldSchema innerField = mapFieldSchema.schema.getField(0);    return mapType.withNewFields(nested.withNewFields(nested.getType(0), filter(nested.getType(1), innerField)));}
1
private Type filterBag(GroupType bagType, FieldSchema bagFieldSchema) throws FrontendException
{    if (LOG.isDebugEnabled())            if (bagType.getFieldCount() != 1) {        throw new RuntimeException("not unwrapping the right type, this should be a Bag: " + bagType);    }    Type nested = bagType.getType(0);    FieldSchema innerField = bagFieldSchema.schema.getField(0);    if (nested.isPrimitive() || nested.getLogicalTypeAnnotation() instanceof LogicalTypeAnnotation.MapLogicalTypeAnnotation || nested.getLogicalTypeAnnotation() instanceof LogicalTypeAnnotation.ListLogicalTypeAnnotation) {                innerField = innerField.schema.getField(0);    }    return bagType.withNewFields(filter(nested, innerField));}
1
public void add(Schema schema, DataBag bag)
{    super.add(bag);    size.add(bag.size());    FieldSchema field = getField(schema, 0);    if (bag.size() > 0 && content == null) {        content = new FieldSummaryData();        content.setName(getName(field));    }    for (Tuple tuple : bag) {        content.add(getSchema(field), tuple);    }}
0
public void merge(SummaryData other)
{    super.merge(other);    BagSummaryData otherBagSummary = (BagSummaryData) other;    size.merge(otherBagSummary.size);    content = merge(content, otherBagSummary.content);}
0
public FieldSummaryData getContent()
{    return content;}
0
public void setContent(FieldSummaryData content)
{    this.content = content;}
0
public ValueStat getSize()
{    return size;}
0
public void setSize(ValueStat size)
{    this.size = size;}
0
public void add()
{    ++count;}
0
public String getValue()
{    return value;}
0
public void setValue(String value)
{    this.value = value;}
0
public int getCount()
{    return count;}
0
public void setCount(int count)
{    this.count = count;}
0
public void add(int countToAdd)
{    this.count += countToAdd;}
0
public void add(String value)
{    if (values != null) {        EnumValueCount enumValueCount = values.get(value);        if (enumValueCount == null) {            enumValueCount = new EnumValueCount(value);            values.put(value, enumValueCount);        }        enumValueCount.add();        checkValues();    }}
0
public void merge(EnumStat other)
{    if (values != null) {        if (other.values == null) {            values = null;            return;        }        for (EnumValueCount otherValue : other.getValues()) {            EnumValueCount myValue = values.get(otherValue.value);            if (myValue == null) {                values.put(otherValue.value, otherValue);            } else {                myValue.add(otherValue.count);            }        }        checkValues();    }}
0
private void checkValues()
{    if (values.size() > MAX_COUNT) {        values = null;    }}
0
public Collection<EnumValueCount> getValues()
{    return values == null ? null : values.values();}
0
public void setValues(Collection<EnumValueCount> values)
{    if (values == null) {        this.values = null;    } else if (this.values != null) {        for (EnumValueCount value : values) {            this.values.put(value.getValue(), value);        }    }}
0
public void merge(SummaryData other)
{    super.merge(other);    FieldSummaryData otherFieldSummaryData = (FieldSummaryData) other;    if (otherFieldSummaryData.name != null) {        setName(otherFieldSummaryData.name);    }    bag = merge(bag, otherFieldSummaryData.bag);    tuple = merge(tuple, otherFieldSummaryData.tuple);    map = merge(map, otherFieldSummaryData.map);    string = merge(string, otherFieldSummaryData.string);    number = merge(number, otherFieldSummaryData.number);    nullCount += otherFieldSummaryData.nullCount;    unknown += otherFieldSummaryData.unknown;    error += otherFieldSummaryData.error;}
0
public void add(Schema schema, Object o)
{    super.add(o);    if (o == null) {        ++nullCount;    } else if (o instanceof DataBag) {        if (bag == null) {            bag = new BagSummaryData();        }        bag.add(schema, (DataBag) o);    } else if (o instanceof Tuple) {        if (tuple == null) {            tuple = new TupleSummaryData();        }        tuple.addTuple(schema, (Tuple) o);    } else if (o instanceof Map<?, ?>) {        if (map == null) {            map = new MapSummaryData();        }        map.add(schema, (Map<?, ?>) o);    } else if (o instanceof String) {        if (string == null) {            string = new StringSummaryData();        }        string.add((String) o);    } else if (o instanceof Number) {        if (number == null) {            number = new NumberSummaryData();        }        number.add((Number) o);    } else {        ++unknown;    }}
0
public void addError()
{    ++error;}
0
public BagSummaryData getBag()
{    return bag;}
0
public void setBag(BagSummaryData bag)
{    this.bag = bag;}
0
public TupleSummaryData getTuple()
{    return tuple;}
0
public void setTuple(TupleSummaryData tuple)
{    this.tuple = tuple;}
0
public MapSummaryData getMap()
{    return map;}
0
public void setMap(MapSummaryData map)
{    this.map = map;}
0
public StringSummaryData getString()
{    return string;}
0
public void setString(StringSummaryData string)
{    this.string = string;}
0
public NumberSummaryData getNumber()
{    return number;}
0
public void setNumber(NumberSummaryData number)
{    this.number = number;}
0
public Long getNull()
{    return nullCount == 0 ? null : nullCount;}
0
public void setNull(long nullCnt)
{    this.nullCount = nullCnt;}
0
public Long getUnknown()
{    return unknown == 0 ? null : unknown;}
0
public void setUnknown(long unknown)
{    this.unknown = unknown;}
0
public Long getError()
{    return error == 0 ? null : error;}
0
public void setError(long error)
{    this.error = error;}
0
public void setName(String name)
{    if (this.name != null && !this.name.equals(name)) {        throw new IllegalStateException("name mismatch " + this.name + " expected, got " + name);    }    this.name = name;}
0
public String getName()
{    return name;}
0
public void add(Schema schema, Map<?, ?> m)
{    super.add(m);    size.add(m.size());    FieldSchema field = getField(schema, 0);    if (m.size() > 0 && key == null) {        key = new FieldSummaryData();        key.setName(getName(field));        value = new FieldSummaryData();        value.setName(getName(field));    }    for (Map.Entry<?, ?> entry : m.entrySet()) {        key.add(null, entry.getKey());        value.add(getSchema(field), entry.getValue());    }}
0
public void merge(SummaryData other)
{    super.merge(other);    MapSummaryData otherMapSummaryData = (MapSummaryData) other;    size.merge(otherMapSummaryData.size);    key = merge(key, otherMapSummaryData.key);    value = merge(value, otherMapSummaryData.value);}
0
public FieldSummaryData getKey()
{    return key;}
0
public void setKey(FieldSummaryData key)
{    this.key = key;}
0
public FieldSummaryData getValue()
{    return value;}
0
public void setValue(FieldSummaryData value)
{    this.value = value;}
0
public ValueStat getSize()
{    return size;}
0
public void setSize(ValueStat size)
{    this.size = size;}
0
public void add(Number n)
{    super.add(n);    value.add(n.doubleValue());}
0
public void merge(SummaryData other)
{    super.merge(other);    value.merge(((NumberSummaryData) other).value);}
0
public ValueStat getValue()
{    return value;}
0
public void setValue(ValueStat value)
{    this.value = value;}
0
public void add(String s)
{    super.add(s);    size.add(s.length());    values.add(s);}
0
public void merge(SummaryData other)
{    super.merge(other);    StringSummaryData stringSummaryData = (StringSummaryData) other;    size.merge(stringSummaryData.size);    values.merge(stringSummaryData.values);}
0
public ValueStat getSize()
{    return size;}
0
public void setSize(ValueStat size)
{    this.size = size;}
0
public Collection<EnumValueCount> getValues()
{    Collection<EnumValueCount> values2 = values.getValues();    if (values2 == null) {        return null;    }    List<EnumValueCount> list = new ArrayList<EnumValueCount>(values2);    Collections.sort(list, new Comparator<EnumValueCount>() {        @Override        public int compare(EnumValueCount o1, EnumValueCount o2) {            return o2.getCount() - o1.getCount();        }    });    return list;}
0
public int compare(EnumValueCount o1, EnumValueCount o2)
{    return o2.getCount() - o1.getCount();}
0
public void setValues(Collection<EnumValueCount> values)
{    this.values.setValues(values);}
0
public Tuple exec(Tuple t) throws IOException
{    return new JSONTuple(sumUp(getInputSchema(), t));}
0
public Tuple exec(Tuple t) throws IOException
{    return new JSONTuple(merge(t));}
0
public String exec(Tuple t) throws IOException
{    return SummaryData.toPrettyJSON(merge(t));}
0
public void readFields(DataInput dataInput) throws IOException
{    throw new UnsupportedOperationException();}
0
public void write(DataOutput dataOutput) throws IOException
{    Tuple t = TF.newTuple(json());    t.write(dataOutput);}
0
public int compareTo(Object o)
{    throw new UnsupportedOperationException();}
0
public void append(Object o)
{    throw new UnsupportedOperationException();}
0
public Object get(int i) throws ExecException
{    if (i == 0) {        return json();    }    throw new ExecException();}
0
private String json()
{    return SummaryData.toJSON(data);}
0
public List<Object> getAll()
{    return new ArrayList<Object>(Arrays.asList(json()));}
0
public long getMemorySize()
{        return 100;}
0
public byte getType(int i) throws ExecException
{    if (i == 0) {        return DataType.CHARARRAY;    }    throw new ExecException("size is 1");}
0
public boolean isNull(int i) throws ExecException
{    if (i == 0) {        return false;    }    throw new ExecException("size is 1");}
0
public void reference(Tuple t)
{    throw new UnsupportedOperationException();}
0
public void set(int i, Object o) throws ExecException
{    throw new UnsupportedOperationException();}
0
public int size()
{    return 1;}
0
public String toDelimitedString(String delim) throws ExecException
{    return json();}
0
public Iterator<Object> iterator()
{    return getAll().iterator();}
0
private static TupleSummaryData getData(Tuple tuple) throws ExecException
{    if (tuple instanceof JSONTuple) {        return ((JSONTuple) tuple).data;    } else {        return SummaryData.fromJSON((String) tuple.get(0), TupleSummaryData.class);    }}
0
private static TupleSummaryData merge(Tuple t) throws IOException
{    TupleSummaryData summaryData = new TupleSummaryData();    DataBag bag = (DataBag) t.get(0);    for (Tuple tuple : bag) {        summaryData.merge(getData(tuple));    }    return summaryData;}
0
private static TupleSummaryData sumUp(Schema schema, Tuple t) throws ExecException
{    TupleSummaryData summaryData = new TupleSummaryData();    DataBag bag = (DataBag) t.get(0);    for (Tuple tuple : bag) {        summaryData.addTuple(schema, tuple);    }    return summaryData;}
0
public String exec(Tuple t) throws IOException
{    return SummaryData.toPrettyJSON(sumUp(getInputSchema(), t));}
0
public String getInitial()
{    return Initial.class.getName();}
0
public String getIntermed()
{    return Intermediate.class.getName();}
0
public String getFinal()
{    return Final.class.getName();}
0
public static String toJSON(SummaryData summaryData)
{    return toJSON(summaryData, objectMapper);}
0
public static String toPrettyJSON(SummaryData summaryData)
{    return toJSON(summaryData, prettyObjectMapper);}
0
private static String toJSON(SummaryData summaryData, ObjectMapper mapper)
{    StringWriter stringWriter = new StringWriter();    try {        mapper.writeValue(stringWriter, summaryData);    } catch (JsonGenerationException e) {        throw new RuntimeException(e);    } catch (JsonMappingException e) {        throw new RuntimeException(e);    } catch (IOException e) {        throw new RuntimeException(e);    }    return stringWriter.toString();}
0
public static T fromJSON(String json, Class<T> clazz)
{    try {        return objectMapper.readValue(new StringReader(json), clazz);    } catch (JsonParseException e) {        throw new RuntimeException(e);    } catch (JsonMappingException e) {        throw new RuntimeException(e);    } catch (IOException e) {        throw new RuntimeException(e);    }}
0
public static T merge(T s1, T s2)
{    if (s1 == null) {        return s2;    } else if (s2 == null) {        return s1;    } else {        s1.merge(s2);        return s1;    }}
0
protected FieldSchema getField(Schema schema, int i)
{    try {        if (schema == null || i >= schema.size()) {            return null;        }        FieldSchema field = schema.getField(i);        return field;    } catch (FrontendException e) {        throw new RuntimeException(e);    }}
0
protected Schema getSchema(FieldSchema field)
{    return field == null ? null : field.schema;}
0
protected String getName(FieldSchema field)
{    return field == null ? null : field.alias;}
0
public void add(Object o)
{    ++count;}
0
public void merge(SummaryData other)
{    this.count += other.count;}
0
public long getCount()
{    return count;}
0
public void setCount(long count)
{    this.count = count;}
0
public String toString()
{    return toJSON(this);}
0
public void addTuple(Schema schema, Tuple tuple)
{    super.add(tuple);    int tupleSize = tuple.size();    size.add(tupleSize);    ensureSize(tupleSize);    for (int i = 0; i < tupleSize; i++) {        FieldSummaryData fieldSummaryData = fields.get(i);        try {            FieldSchema field = getField(schema, i);            fieldSummaryData.setName(getName(field));            Object o = tuple.get(i);            fieldSummaryData.add(getSchema(field), o);        } catch (ExecException e) {            LOG.log(Level.WARNING, "Can't get value from tuple", e);            fieldSummaryData.addError();        }    }}
0
private void ensureSize(int sizeToEnsure)
{    while (fields.size() < sizeToEnsure) {        fields.add(new FieldSummaryData());    }}
0
public void merge(SummaryData other)
{    super.merge(other);    TupleSummaryData otherTupleSummaryData = (TupleSummaryData) other;    size.merge(otherTupleSummaryData.size);    ensureSize(otherTupleSummaryData.fields.size());    for (int i = 0; i < otherTupleSummaryData.fields.size(); i++) {        fields.get(i).merge(otherTupleSummaryData.fields.get(i));    }}
0
public List<FieldSummaryData> getFields()
{    return fields;}
0
public void setFields(List<FieldSummaryData> fields)
{    this.fields = fields;}
0
public ValueStat getSize()
{    return size;}
0
public void setSize(ValueStat size)
{    this.size = size;}
0
public void add(double v)
{    total += v;    min = Math.min(min, v);    max = Math.max(max, v);}
0
public void merge(ValueStat other)
{    total += other.total;    min = Math.min(min, other.min);    max = Math.max(max, other.max);}
0
public double getTotal()
{    return total;}
0
public void setTotal(double total)
{    this.total = total;}
0
public double getMin()
{    return min;}
0
public void setMin(double min)
{    this.min = min;}
0
public double getMax()
{    return max;}
0
public void setMax(double max)
{    this.max = max;}
0
 static Schema getPigSchema(Configuration configuration)
{    return parsePigSchema(configuration.get(PARQUET_PIG_SCHEMA));}
0
 static RequiredFieldList getRequiredFields(Configuration configuration)
{    String requiredFieldString = configuration.get(PARQUET_PIG_REQUIRED_FIELDS);    if (requiredFieldString == null) {        return null;    }    try {        return (RequiredFieldList) ObjectSerializer.deserialize(requiredFieldString);    } catch (IOException iOException) {        throw new RuntimeException("Failed to deserialize pushProjection");    }}
0
 static Schema getPigSchemaFromMultipleFiles(MessageType fileSchema, Map<String, Set<String>> keyValueMetaData)
{    Set<String> pigSchemas = PigMetaData.getPigSchemas(keyValueMetaData);    if (pigSchemas == null) {        return pigSchemaConverter.convert(fileSchema);    }    Schema mergedPigSchema = null;    for (String pigSchemaString : pigSchemas) {        try {            mergedPigSchema = union(mergedPigSchema, parsePigSchema(pigSchemaString));        } catch (FrontendException e) {            throw new ParquetDecodingException("can not merge " + pigSchemaString + " into " + mergedPigSchema, e);        }    }    return mergedPigSchema;}
0
 static Schema getPigSchemaFromFile(MessageType fileSchema, Map<String, String> keyValueMetaData)
{    PigMetaData pigMetaData = PigMetaData.fromMetaData(keyValueMetaData);    if (pigMetaData == null) {        return pigSchemaConverter.convert(fileSchema);    }    return parsePigSchema(pigMetaData.getPigSchema());}
0
private static Schema union(Schema merged, Schema pigSchema) throws FrontendException
{    List<FieldSchema> fields = new ArrayList<Schema.FieldSchema>();    if (merged == null) {        return pigSchema;    }        for (FieldSchema fieldSchema : merged.getFields()) {        FieldSchema newFieldSchema = pigSchema.getField(fieldSchema.alias);        if (newFieldSchema == null) {            fields.add(fieldSchema);        } else {            fields.add(union(fieldSchema, newFieldSchema));        }    }        for (FieldSchema newFieldSchema : pigSchema.getFields()) {        FieldSchema oldFieldSchema = merged.getField(newFieldSchema.alias);        if (oldFieldSchema == null) {            fields.add(newFieldSchema);        }    }    return new Schema(fields);}
0
private static FieldSchema union(FieldSchema mergedFieldSchema, FieldSchema newFieldSchema)
{    if (!mergedFieldSchema.alias.equals(newFieldSchema.alias) || mergedFieldSchema.type != newFieldSchema.type) {        throw new IncompatibleSchemaModificationException("Incompatible Pig schema change: " + mergedFieldSchema + " can not accept");    }    try {        return new FieldSchema(mergedFieldSchema.alias, union(mergedFieldSchema.schema, newFieldSchema.schema), mergedFieldSchema.type);    } catch (FrontendException e) {        throw new SchemaConversionException(e);    }}
0
public ReadContext init(InitContext initContext)
{    Schema pigSchema = getPigSchema(initContext.getConfiguration());    RequiredFieldList requiredFields = getRequiredFields(initContext.getConfiguration());    boolean columnIndexAccess = initContext.getConfiguration().getBoolean(PARQUET_COLUMN_INDEX_ACCESS, false);    if (pigSchema == null) {        return new ReadContext(initContext.getFileSchema());    } else {                MessageType parquetRequestedSchema = new PigSchemaConverter(columnIndexAccess).filter(initContext.getFileSchema(), pigSchema, requiredFields);        return new ReadContext(parquetRequestedSchema);    }}
0
public static TupleWriteSupport fromPigSchema(String pigSchemaString) throws ParserException
{    return new TupleWriteSupport(Utils.getSchemaFromString(pigSchemaString));}
0
public String getName()
{    return "pig";}
0
public Schema getPigSchema()
{    return rootPigSchema;}
0
public MessageType getParquetSchema()
{    return rootSchema;}
0
public WriteContext init(Configuration configuration)
{    Map<String, String> extraMetaData = new HashMap<String, String>();    new PigMetaData(rootPigSchema).addToMetaData(extraMetaData);    return new WriteContext(rootSchema, extraMetaData);}
0
public void prepareForWrite(RecordConsumer recordConsumer)
{    this.recordConsumer = recordConsumer;}
0
public void write(Tuple t)
{    try {        recordConsumer.startMessage();        writeTuple(rootSchema, rootPigSchema, t);        recordConsumer.endMessage();    } catch (ExecException e) {        throw new RuntimeException(e);    } catch (FrontendException e) {        throw new RuntimeException(e);    }}
0
private void writeTuple(GroupType schema, Schema pigSchema, Tuple t) throws ExecException, FrontendException
{    List<Type> fields = schema.getFields();    List<FieldSchema> pigFields = pigSchema.getFields();    assert fields.size() == pigFields.size();    for (int i = 0; i < fields.size(); i++) {        if (t.isNull(i)) {            continue;        }        Type fieldType = fields.get(i);        recordConsumer.startField(fieldType.getName(), i);        FieldSchema pigType = pigFields.get(i);        switch(pigType.type) {            case DataType.BAG:                Type bagType = fieldType.asGroupType().getType(0);                FieldSchema pigBagInnerType = pigType.schema.getField(0);                DataBag bag = (DataBag) t.get(i);                recordConsumer.startGroup();                if (bag.size() > 0) {                    recordConsumer.startField(bagType.getName(), 0);                    for (Tuple tuple : bag) {                        if (bagType.isPrimitive()) {                            writeValue(bagType, pigBagInnerType, tuple, 0);                        } else {                            recordConsumer.startGroup();                            writeTuple(bagType.asGroupType(), pigBagInnerType.schema, tuple);                            recordConsumer.endGroup();                        }                    }                    recordConsumer.endField(bagType.getName(), 0);                }                recordConsumer.endGroup();                break;            case DataType.MAP:                Type mapType = fieldType.asGroupType().getType(0);                FieldSchema pigMapInnerType = pigType.schema.getField(0);                                @SuppressWarnings("unchecked")                Map<String, Object> map = (Map<String, Object>) t.get(i);                recordConsumer.startGroup();                if (map.size() > 0) {                    recordConsumer.startField(mapType.getName(), 0);                    Set<Entry<String, Object>> entrySet = map.entrySet();                    for (Entry<String, Object> entry : entrySet) {                        recordConsumer.startGroup();                        Schema keyValueSchema = new Schema(Arrays.asList(new FieldSchema("key", DataType.CHARARRAY), new FieldSchema("value", pigMapInnerType.schema, pigMapInnerType.type)));                        writeTuple(mapType.asGroupType(), keyValueSchema, TF.newTuple(Arrays.asList(entry.getKey(), entry.getValue())));                        recordConsumer.endGroup();                    }                    recordConsumer.endField(mapType.getName(), 0);                }                recordConsumer.endGroup();                break;            default:                writeValue(fieldType, pigType, t, i);                break;        }        recordConsumer.endField(fieldType.getName(), i);    }}
0
private void writeValue(Type type, FieldSchema pigType, Tuple t, int i)
{    try {        if (type.isPrimitive()) {            switch(type.asPrimitiveType().getPrimitiveTypeName()) {                                case BINARY:                    byte[] bytes;                    if (pigType.type == DataType.BYTEARRAY) {                        bytes = ((DataByteArray) t.get(i)).get();                    } else if (pigType.type == DataType.CHARARRAY) {                        bytes = ((String) t.get(i)).getBytes("UTF-8");                    } else {                        throw new UnsupportedOperationException("can not convert from " + DataType.findTypeName(pigType.type) + " to BINARY ");                    }                    recordConsumer.addBinary(Binary.fromReusedByteArray(bytes));                    break;                case BOOLEAN:                    recordConsumer.addBoolean((Boolean) t.get(i));                    break;                case INT32:                    recordConsumer.addInteger(((Number) t.get(i)).intValue());                    break;                case INT64:                    recordConsumer.addLong(((Number) t.get(i)).longValue());                    break;                case DOUBLE:                    recordConsumer.addDouble(((Number) t.get(i)).doubleValue());                    break;                case FLOAT:                    recordConsumer.addFloat(((Number) t.get(i)).floatValue());                    break;                default:                    throw new UnsupportedOperationException(type.asPrimitiveType().getPrimitiveTypeName().name());            }        } else {            assert pigType.type == DataType.TUPLE;            recordConsumer.startGroup();            writeTuple(type.asGroupType(), pigType.schema, (Tuple) t.get(i));            recordConsumer.endGroup();        }    } catch (Exception e) {        throw new ParquetEncodingException("can not write value at " + i + " in tuple " + t + " from type '" + pigType + "' to type '" + type + "'", e);    }}
0
public static void main(String[] args) throws Exception
{    StringBuilder schemaString = new StringBuilder("a0: chararray");    for (int i = 1; i < COLUMN_COUNT; i++) {        schemaString.append(", a" + i + ": chararray");    }    String out = "target/PerfTest";    {        PigServer pigServer = new PigServer(ExecType.LOCAL);        Data data = Storage.resetData(pigServer);        Collection<Tuple> list = new ArrayList<Tuple>();        for (int i = 0; i < ROW_COUNT; i++) {            Tuple tuple = TupleFactory.getInstance().newTuple(COLUMN_COUNT);            for (int j = 0; j < COLUMN_COUNT; j++) {                tuple.set(j, "a" + i + "_" + j);            }            list.add(tuple);        }        data.set("in", schemaString.toString(), list);        pigServer.setBatchOn();        pigServer.registerQuery("A = LOAD 'in' USING mock.Storage();");        pigServer.deleteFile(out);        pigServer.registerQuery("Store A into '" + out + "' using " + ParquetStorer.class.getName() + "();");        if (pigServer.executeBatch().get(0).getStatus() != JOB_STATUS.COMPLETED) {            throw new RuntimeException("Job failed", pigServer.executeBatch().get(0).getException());        }    }    load(out, 1);    load(out, 2);    load(out, 3);    load(out, 4);    load(out, 5);    load(out, 10);    load(out, 20);    load(out, 50);    System.out.println(results);}
0
private static void load(String out, int colsToLoad) throws ExecException, IOException
{    long t0 = System.currentTimeMillis();    StringBuilder schemaString = new StringBuilder("a0: chararray");    for (int i = 1; i < colsToLoad; i++) {        schemaString.append(", a" + i + ": chararray");    }    PigServer pigServer = new PigServer(ExecType.LOCAL);    pigServer.registerQuery("B = LOAD '" + out + "' USING " + ParquetLoader.class.getName() + "('" + schemaString + "');");    pigServer.registerQuery("C = FOREACH (GROUP B ALL) GENERATE COUNT(B);");    Iterator<Tuple> it = pigServer.openIterator("C");    if (!it.hasNext()) {        throw new RuntimeException("Job failed: no tuple to read");    }    Long count = (Long) it.next().get(0);    assertEquals(ROW_COUNT, count.longValue());    long t1 = System.currentTimeMillis();    results.append((t1 - t0) + " ms to read " + colsToLoad + " columns\n");}
0
public static void main(String[] args) throws Exception
{    StringBuilder results = new StringBuilder();    String out = "target/PerfTest2";    File outDir = new File(out);    if (outDir.exists()) {        clean(outDir);    }    write(out);    for (int i = 0; i < 2; i++) {        load(out, 1, results);        load(out, 2, results);        load(out, 3, results);        load(out, 4, results);        load(out, 5, results);        load(out, 10, results);        load(out, 20, results);        load(out, 50, results);        results.append("\n");    }    System.out.println(results);}
0
public static void write(String out) throws IOException, ParserException, InterruptedException, ExecException
{    {        StringBuilder schemaString = new StringBuilder("a0: chararray");        for (int i = 1; i < COLUMN_COUNT; i++) {            schemaString.append(", a" + i + ": chararray");        }        String location = out;        String schema = schemaString.toString();        StoreFuncInterface storer = new ParquetStorer();        Job job = new Job(conf);        storer.setStoreFuncUDFContextSignature("sig");        String absPath = storer.relToAbsPathForStoreLocation(location, new Path(new File(".").getAbsoluteFile().toURI()));        storer.setStoreLocation(absPath, job);        storer.checkSchema(new ResourceSchema(Utils.getSchemaFromString(schema)));                @SuppressWarnings("unchecked")        OutputFormat<Void, Tuple> outputFormat = storer.getOutputFormat();                JobContext jobContext = ContextUtil.newJobContext(ContextUtil.getConfiguration(job), new JobID("jt", jobid++));        outputFormat.checkOutputSpecs(jobContext);        if (schema != null) {            ResourceSchema resourceSchema = new ResourceSchema(Utils.getSchemaFromString(schema));            storer.checkSchema(resourceSchema);            if (storer instanceof StoreMetadata) {                ((StoreMetadata) storer).storeSchema(resourceSchema, absPath, job);            }        }        TaskAttemptContext taskAttemptContext = ContextUtil.newTaskAttemptContext(ContextUtil.getConfiguration(job), new TaskAttemptID("jt", jobid, true, 1, 0));        RecordWriter<Void, Tuple> recordWriter = outputFormat.getRecordWriter(taskAttemptContext);        storer.prepareToWrite(recordWriter);        for (int i = 0; i < ROW_COUNT; i++) {            Tuple tuple = TupleFactory.getInstance().newTuple(COLUMN_COUNT);            for (int j = 0; j < COLUMN_COUNT; j++) {                tuple.set(j, "a" + i + "_" + j);            }            storer.putNext(tuple);        }        recordWriter.close(taskAttemptContext);        OutputCommitter outputCommitter = outputFormat.getOutputCommitter(taskAttemptContext);        outputCommitter.commitTask(taskAttemptContext);        outputCommitter.commitJob(jobContext);    }}
0
 static void clean(File outDir)
{    if (outDir.isDirectory()) {        File[] listFiles = outDir.listFiles();        for (File file : listFiles) {            clean(file);        }    }    outDir.delete();}
0
 static void load(String out, int colsToLoad, StringBuilder results) throws Exception
{    StringBuilder schemaString = new StringBuilder("a0: chararray");    for (int i = 1; i < colsToLoad; i++) {        schemaString.append(", a" + i + ": chararray");    }    long t0 = System.currentTimeMillis();    Job job = new Job(conf);    int loadjobId = jobid++;    LoadFunc loadFunc = new ParquetLoader(schemaString.toString());    loadFunc.setUDFContextSignature("sigLoader" + loadjobId);    String absPath = loadFunc.relativeToAbsolutePath(out, new Path(new File(".").getAbsoluteFile().toURI()));    loadFunc.setLocation(absPath, job);        @SuppressWarnings("unchecked")    InputFormat<Void, Tuple> inputFormat = loadFunc.getInputFormat();    JobContext jobContext = ContextUtil.newJobContext(ContextUtil.getConfiguration(job), new JobID("jt", loadjobId));    List<InputSplit> splits = inputFormat.getSplits(jobContext);    int i = 0;    int taskid = 0;    for (InputSplit split : splits) {        TaskAttemptContext taskAttemptContext = ContextUtil.newTaskAttemptContext(ContextUtil.getConfiguration(job), new TaskAttemptID("jt", loadjobId, true, taskid++, 0));        RecordReader<Void, Tuple> recordReader = inputFormat.createRecordReader(split, taskAttemptContext);        loadFunc.prepareToRead(recordReader, null);        recordReader.initialize(split, taskAttemptContext);        Tuple t;        while ((t = loadFunc.getNext()) != null) {            if (DEBUG)                System.out.println(t);            ++i;        }    }    assertEquals(ROW_COUNT, i);    long t1 = System.currentTimeMillis();    results.append((t1 - t0) + " ms to read " + colsToLoad + " columns\n");}
0
public static void main(String[] args) throws Exception
{    StringBuilder results = new StringBuilder();    String out = "target/PerfTestReadAllCols";    File outDir = new File(out);    if (outDir.exists()) {        PerfTest2.clean(outDir);    }    PerfTest2.write(out);    for (int i = 0; i < 5; i++) {        PerfTest2.load(out, PerfTest2.COLUMN_COUNT, results);        results.append("\n");    }    System.out.println(results);}
0
public static Tuple t(Object... objects)
{    return tf.newTuple(Arrays.asList(objects));}
0
public static DataBag b(Tuple... tuples)
{    return bf.newDefaultBag(Arrays.asList(tuples));}
0
public static Map<String, Object> m(Object... objects)
{    Map<String, Object> m = new HashMap<String, Object>();    for (int i = 0; i < objects.length; i += 2) {        m.put((String) objects[i], objects[i + 1]);    }    return m;}
0
public void testEvalFunc() throws IOException
{    Summary summary = new Summary();    String result = summary.exec(t(TEST_BAG));    validate(result, 1);}
0
public void testAlgebraic() throws IOException
{    Summary.Initial initial = new Summary.Initial();    Summary.Intermediate intermediate1 = new Summary.Intermediate();    Summary.Intermediate intermediate2 = new Summary.Intermediate();    Summary.Final finall = new Summary.Final();    DataBag combinedRedIn = bf.newDefaultBag();    for (int r = 0; r < 5; r++) {        DataBag combinedMapOut = bf.newDefaultBag();        for (int m = 0; m < 5; m++) {            DataBag mapOut = bf.newDefaultBag();            for (Tuple t : TEST_BAG) {                Tuple exec = initial.exec(t(b(t)));                mapOut.add(exec);            }            Tuple exec = intermediate1.exec(t(mapOut));            validate((String) exec.get(0), 1);            combinedMapOut.add(exec);        }        combinedRedIn.add(intermediate2.exec(t(combinedMapOut)));    }    String result = finall.exec(t(combinedRedIn));    validate(result, 5 * 5);}
0
private void validate(String result, int factor) throws IOException
{    TupleSummaryData s = SummaryData.fromJSON(result, TupleSummaryData.class);        assertEquals(9 * factor, s.getCount());    assertEquals(1 * factor, s.getFields().get(0).getNull().longValue());    assertEquals(7 * factor, s.getFields().get(0).getBag().getCount());    assertEquals(18 * factor, s.getFields().get(0).getBag().getContent().getTuple().getFields().get(0).getCount());    MapSummaryData map = s.getFields().get(0).getBag().getContent().getTuple().getFields().get(1).getMap();    assertEquals(2 * factor, map.getCount());    assertEquals(3 * factor, map.getKey().getCount());}
0
public void testPigScript() throws Exception
{    PigServer pigServer = new PigServer(ExecType.LOCAL);    Data data = Storage.resetData(pigServer);    List<Tuple> list = new ArrayList<Tuple>();    for (int i = 0; i < 1002; i++) {        list.add(t("a", "b" + i, 1l, b(t("a", m("foo", "bar")))));    }    data.set("in", "a:chararray, a1:chararray, b:int, c:{t:(a2:chararray, b2:[])}", list);    pigServer.registerQuery("A = LOAD 'in' USING mock.Storage();");    pigServer.registerQuery("B = FOREACH (GROUP A ALL) GENERATE " + Summary.class.getName() + "(A);");    pigServer.registerQuery("STORE B INTO 'out' USING mock.Storage();");    System.out.println(data.get("out").get(0).get(0));    TupleSummaryData s = SummaryData.fromJSON((String) data.get("out").get(0).get(0), TupleSummaryData.class);    System.out.println(s);}
0
public void testMaxIsZero() throws Exception
{    PigServer pigServer = new PigServer(ExecType.LOCAL);    Data data = Storage.resetData(pigServer);    List<Tuple> list = new ArrayList<Tuple>();    for (int i = 0; i < 10; i++) {        list.add(t("a", i - 9));    }    data.set("in", "a:chararray, b:int", list);    pigServer.registerQuery("A = LOAD 'in' USING mock.Storage();");    pigServer.registerQuery("B = FOREACH (GROUP A ALL) GENERATE " + Summary.class.getName() + "(A);");    pigServer.registerQuery("STORE B INTO 'out' USING mock.Storage();");    TupleSummaryData s = SummaryData.fromJSON((String) data.get("out").get(0).get(0), TupleSummaryData.class);    System.out.println(s);    assertEquals(0, s.getFields().get(1).getNumber().getValue().getMax(), 0);}
0
private void testDecimalConversion(double value, int precision, int scale, String stringValue)
{    String originalString = Double.toString(value);    BigDecimal originalValue = new BigDecimal(originalString);    BigDecimal convertedValue = DecimalUtils.binaryToDecimal(Binary.fromByteArray(originalValue.unscaledValue().toByteArray()), precision, scale);    assertEquals(stringValue, convertedValue.toString());}
0
private void testDecimalConversion(int value, int precision, int scale, String stringValue)
{    String originalString = Integer.toString(value);    BigDecimal originalValue = new BigDecimal(originalString);    BigDecimal convertedValue = DecimalUtils.binaryToDecimal(Binary.fromByteArray(originalValue.unscaledValue().toByteArray()), precision, scale);    assertEquals(stringValue, convertedValue.toString());}
0
private void testDecimalConversion(long value, int precision, int scale, String stringValue)
{    String originalString = Long.toString(value);    BigDecimal originalValue = new BigDecimal(originalString);    BigDecimal convertedValue = DecimalUtils.binaryToDecimal(Binary.fromByteArray(originalValue.unscaledValue().toByteArray()), precision, scale);    assertEquals(stringValue, convertedValue.toString());}
0
public void testBinaryToDecimal() throws Exception
{                testDecimalConversion(Long.MAX_VALUE, 19, 0, "9223372036854775807");    testDecimalConversion(Long.MIN_VALUE, 19, 0, "-9223372036854775808");    testDecimalConversion(0L, 0, 0, "0.0");        testDecimalConversion(Integer.MAX_VALUE, 10, 0, "2147483647");    testDecimalConversion(Integer.MIN_VALUE, 10, 0, "-2147483648");    testDecimalConversion(0, 0, 0, "0.0");        testDecimalConversion(12345678912345678d, 17, 0, "12345678912345678");    testDecimalConversion(123456789123456.78, 17, 2, "123456789123456.78");    testDecimalConversion(0.12345678912345678, 17, 17, "0.12345678912345678");    testDecimalConversion(-0.000102, 6, 6, "-0.000102");}
0
public void testSchema() throws Exception
{    String location = "garbage";    ParquetLoader pLoader = new ParquetLoader("a:chararray, " + "b:{t:(c:chararray, d:chararray)}, " + "p:[(q:chararray, r:chararray)]");    Job job = new Job();    pLoader.getSchema(location, job);    RequiredFieldList list = new RequiredFieldList();    RequiredField field = new RequiredField("a", 0, null, DataType.CHARARRAY);    list.add(field);    field = new RequiredField("b", 0, Arrays.asList(new RequiredField("t", 0, Arrays.asList(new RequiredField("d", 1, null, DataType.CHARARRAY)), DataType.TUPLE)), DataType.BAG);    list.add(field);    pLoader.pushProjection(list);    pLoader.setLocation(location, job);    assertEquals("{a: chararray,b: {t: (d: chararray)}}", TupleReadSupport.getPigSchema(job.getConfiguration()).toString());}
0
public void testProjectionPushdown() throws Exception
{    PigServer pigServer = new PigServer(ExecType.LOCAL);    pigServer.setValidateEachStatement(true);    String out = "target/out";    int rows = 10;    Data data = Storage.resetData(pigServer);    List<Tuple> list = new ArrayList<Tuple>();    for (int i = 0; i < rows; i++) {        list.add(Storage.tuple(i, "a" + i, i * 2));    }    data.set("in", "i:int, a:chararray, b:int", list);    pigServer.setBatchOn();    pigServer.registerQuery("A = LOAD 'in' USING mock.Storage();");    pigServer.deleteFile(out);    pigServer.registerQuery("Store A into '" + out + "' using " + ParquetStorer.class.getName() + "();");    pigServer.executeBatch();    List<Tuple> expectedList = new ArrayList<Tuple>();    for (int i = 0; i < rows; i++) {        expectedList.add(Storage.tuple("a" + i));    }    pigServer.registerQuery("C = LOAD '" + out + "' using " + ParquetLoader.class.getName() + "();");    pigServer.registerQuery("D = foreach C generate a;");    pigServer.registerQuery("Store D into 'out' using mock.Storage();");    pigServer.executeBatch();    List<Tuple> actualList = data.get("out");    pigServer.registerQuery("C = LOAD '" + out + "' using " + ParquetLoader.class.getName() + "('a:chararray, b:int');");    Assert.assertEquals("{a: chararray,b: int}", pigServer.dumpSchema("C").toString());    try {        pigServer.registerQuery("D = foreach C generate i;");        Assert.fail("Frontend Exception expected");    } catch (FrontendException fe) {    }                pigServer = new PigServer(ExecType.LOCAL);    data = Storage.resetData(pigServer);    pigServer.setBatchOn();    pigServer.registerQuery("C = LOAD '" + out + "' using " + ParquetLoader.class.getName() + "('a:chararray, b:int');");    pigServer.registerQuery("D = foreach C generate a;");    pigServer.registerQuery("Store D into 'out' using mock.Storage();");    pigServer.executeBatch();    actualList = data.get("out");    Assert.assertEquals(expectedList, actualList);}
0
public void testNullPadding() throws Exception
{    PigServer pigServer = new PigServer(ExecType.LOCAL);    pigServer.setValidateEachStatement(true);    String out = "target/out";    int rows = 10;    Data data = Storage.resetData(pigServer);    List<Tuple> list = new ArrayList<Tuple>();    for (int i = 0; i < rows; i++) {        list.add(Storage.tuple(i, "a" + i, i * 2));    }    data.set("in", "i:int, a:chararray, b:int", list);    pigServer.setBatchOn();    pigServer.registerQuery("A = LOAD 'in' USING mock.Storage();");    pigServer.deleteFile(out);    pigServer.registerQuery("Store A into '" + out + "' using " + ParquetStorer.class.getName() + "();");    pigServer.executeBatch();        pigServer.registerQuery("C = LOAD '" + out + "' using " + ParquetLoader.class.getName() + "('i:int, a:chararray, b:int, n1:int, n2:chararray');");    pigServer.registerQuery("STORE C into 'out' using mock.Storage();");    pigServer.executeBatch();    List<Tuple> actualList = data.get("out");    assertEquals(rows, actualList.size());    for (Tuple t : actualList) {        assertTrue(t.isNull(3));        assertTrue(t.isNull(4));    }        pigServer.registerQuery("D = LOAD '" + out + "' using " + ParquetLoader.class.getName() + "('n1:int, a:chararray, n2:chararray, b:int');");    pigServer.registerQuery("STORE D into 'out2' using mock.Storage();");    pigServer.executeBatch();    actualList = data.get("out2");    assertEquals(rows, actualList.size());    for (Tuple t : actualList) {        assertTrue(t.isNull(0));        assertTrue(t.isNull(2));    }}
0
public void testReqestedSchemaColumnPruning() throws Exception
{    PigServer pigServer = new PigServer(ExecType.LOCAL);    pigServer.setValidateEachStatement(true);    String out = "target/out";    int rows = 10;    Data data = Storage.resetData(pigServer);    List<Tuple> list = new ArrayList<Tuple>();    for (int i = 0; i < rows; i++) {        list.add(Storage.tuple(i, "a" + i, i * 2));    }    data.set("in", "i:int, a:chararray, b:int", list);    pigServer.setBatchOn();    pigServer.registerQuery("A = LOAD 'in' USING mock.Storage();");    pigServer.deleteFile(out);    pigServer.registerQuery("Store A into '" + out + "' using " + ParquetStorer.class.getName() + "();");    pigServer.executeBatch();        pigServer.registerQuery("C = LOAD '" + out + "' using " + ParquetLoader.class.getName() + "('i:int, a:chararray, b:int, n1:int, n2:chararray');");    pigServer.registerQuery("G = foreach C generate n1,b,n2,i;");    pigServer.registerQuery("STORE G into 'out' using mock.Storage();");    pigServer.executeBatch();    List<Tuple> actualList = data.get("out");    assertEquals(rows, actualList.size());    for (Tuple t : actualList) {        assertEquals(4, t.size());        assertTrue(t.isNull(0));        assertTrue(t.isNull(2));    }}
0
public void testTypePersuasion() throws Exception
{    Properties p = new Properties();    p.setProperty(STRICT_TYPE_CHECKING, Boolean.FALSE.toString());    PigServer pigServer = new PigServer(ExecType.LOCAL, p);    pigServer.setValidateEachStatement(true);    String out = "target/out";    int rows = 10;    Data data = Storage.resetData(pigServer);    List<Tuple> list = new ArrayList<Tuple>();    for (int i = 0; i < rows; i++) {        list.add(Storage.tuple(i, (long) i, (float) i, (double) i, Integer.toString(i), Boolean.TRUE));    }    data.set("in", "i:int, l:long, f:float, d:double, s:chararray, b:boolean", list);    pigServer.setBatchOn();    pigServer.registerQuery("A = LOAD 'in' USING mock.Storage();");    pigServer.deleteFile(out);    pigServer.registerQuery("Store A into '" + out + "' using " + ParquetStorer.class.getName() + "();");    pigServer.executeBatch();    List<Tuple> actualList = null;    byte[] types = { INTEGER, LONG, FLOAT, DOUBLE, CHARARRAY, BOOLEAN };        for (int i = 0; i < types.length; i++) {        String query = "B = LOAD '" + out + "' using " + ParquetLoader.class.getName() + "('i:" + DataType.findTypeName(types[i % types.length]) + "," + "  l:" + DataType.findTypeName(types[(i + 1) % types.length]) + "," + "  f:" + DataType.findTypeName(types[(i + 2) % types.length]) + "," + "  d:" + DataType.findTypeName(types[(i + 3) % types.length]) + "," + "  s:" + DataType.findTypeName(types[(i + 4) % types.length]) + "," + "  b:" + DataType.findTypeName(types[(i + 5) % types.length]) + "');";        System.out.println("Query: " + query);        pigServer.registerQuery(query);        pigServer.registerQuery("STORE B into 'out" + i + "' using mock.Storage();");        pigServer.executeBatch();        actualList = data.get("out" + i);        assertEquals(rows, actualList.size());        for (Tuple t : actualList) {            assertTrue(t.getType(0) == types[i % types.length]);            assertTrue(t.getType(1) == types[(i + 1) % types.length]);            assertTrue(t.getType(2) == types[(i + 2) % types.length]);            assertTrue(t.getType(3) == types[(i + 3) % types.length]);            assertTrue(t.getType(4) == types[(i + 4) % types.length]);            assertTrue(t.getType(5) == types[(i + 5) % types.length]);        }    }}
0
public void testColumnIndexAccess() throws Exception
{    PigServer pigServer = new PigServer(ExecType.LOCAL);    pigServer.setValidateEachStatement(true);    String out = "target/out";    int rows = 10;    Data data = Storage.resetData(pigServer);    List<Tuple> list = new ArrayList<Tuple>();    for (int i = 0; i < rows; i++) {        list.add(Storage.tuple(i, i * 1.0, i * 2L, "v" + i));    }    data.set("in", "c1:int, c2:double, c3:long, c4:chararray", list);    pigServer.setBatchOn();    pigServer.registerQuery("A = LOAD 'in' USING mock.Storage();");    pigServer.deleteFile(out);    pigServer.registerQuery("Store A into '" + out + "' using " + ParquetStorer.class.getName() + "();");    pigServer.executeBatch();        pigServer.registerQuery("B = LOAD '" + out + "' using " + ParquetLoader.class.getName() + "('n1:int, n2:double, n3:long, n4:chararray', 'true');");    pigServer.registerQuery("STORE B into 'out' using mock.Storage();");    pigServer.executeBatch();    List<Tuple> actualList = data.get("out");    assertEquals(rows, actualList.size());    for (int i = 0; i < rows; i++) {        Tuple t = actualList.get(i);        assertEquals(4, t.size());        assertEquals(i, t.get(0));        assertEquals(i * 1.0, t.get(1));        assertEquals(i * 2L, t.get(2));        assertEquals("v" + i, t.get(3));    }}
0
public void testColumnIndexAccessProjection() throws Exception
{    PigServer pigServer = new PigServer(ExecType.LOCAL);    pigServer.setValidateEachStatement(true);    String out = "target/out";    int rows = 10;    Data data = Storage.resetData(pigServer);    List<Tuple> list = new ArrayList<Tuple>();    for (int i = 0; i < rows; i++) {        list.add(Storage.tuple(i, i * 1.0, i * 2L, "v" + i));    }    data.set("in", "c1:int, c2:double, c3:long, c4:chararray", list);    pigServer.setBatchOn();    pigServer.registerQuery("A = LOAD 'in' USING mock.Storage();");    pigServer.deleteFile(out);    pigServer.registerQuery("Store A into '" + out + "' using " + ParquetStorer.class.getName() + "();");    pigServer.executeBatch();    pigServer.registerQuery("B = LOAD '" + out + "' using " + ParquetLoader.class.getName() + "('n1:int, n2:double, n3:long, n4:chararray', 'true');");    pigServer.registerQuery("C = foreach B generate n1, n3;");    pigServer.registerQuery("STORE C into 'out' using mock.Storage();");    pigServer.executeBatch();    List<Tuple> actualList = data.get("out");    assertEquals(rows, actualList.size());    for (int i = 0; i < rows; i++) {        Tuple t = actualList.get(i);        assertEquals(2, t.size());        assertEquals(i, t.get(0));        assertEquals(i * 2L, t.get(1));    }}
0
public void testPredicatePushdown() throws Exception
{    Configuration conf = new Configuration();    conf.setBoolean(ParquetLoader.ENABLE_PREDICATE_FILTER_PUSHDOWN, true);    PigServer pigServer = new PigServer(ExecType.LOCAL, conf);    pigServer.setValidateEachStatement(true);    String out = "target/out";    String out2 = "target/out2";    int rows = 10;    Data data = Storage.resetData(pigServer);    List<Tuple> list = new ArrayList<Tuple>();    for (int i = 0; i < rows; i++) {        list.add(Storage.tuple(i, i * 1.0, i * 2L, "v" + i));    }    data.set("in", "c1:int, c2:double, c3:long, c4:chararray", list);    pigServer.setBatchOn();    pigServer.registerQuery("A = LOAD 'in' USING mock.Storage();");    pigServer.deleteFile(out);    pigServer.registerQuery("Store A into '" + out + "' using " + ParquetStorer.class.getName() + "();");    pigServer.executeBatch();    pigServer.deleteFile(out2);    pigServer.registerQuery("B = LOAD '" + out + "' using " + ParquetLoader.class.getName() + "('c1:int, c2:double, c3:long, c4:chararray');");    pigServer.registerQuery("C = FILTER B by c1 == 1 or c1 == 5;");    pigServer.registerQuery("STORE C into '" + out2 + "' using mock.Storage();");    List<ExecJob> jobs = pigServer.executeBatch();    long recordsRead = jobs.get(0).getStatistics().getInputStats().get(0).getNumberRecords();    assertEquals(2, recordsRead);}
0
public void testStorer() throws ExecException, Exception
{    String out = "target/out";    int rows = 1000;    Properties props = new Properties();    props.setProperty("parquet.compression", "uncompressed");    props.setProperty("parquet.page.size", "1000");    PigServer pigServer = new PigServer(ExecType.LOCAL, props);    Data data = Storage.resetData(pigServer);    Collection<Tuple> list = new ArrayList<Tuple>();    for (int i = 0; i < rows; i++) {        list.add(tuple("a" + i));    }    data.set("in", "a:chararray", list);    pigServer.setBatchOn();    pigServer.registerQuery("A = LOAD 'in' USING mock.Storage();");    pigServer.deleteFile(out);    pigServer.registerQuery("Store A into '" + out + "' using " + ParquetStorer.class.getName() + "();");    if (pigServer.executeBatch().get(0).getStatus() != JOB_STATUS.COMPLETED) {        throw new RuntimeException("Job failed", pigServer.executeBatch().get(0).getException());    }    pigServer.registerQuery("B = LOAD '" + out + "' USING " + ParquetLoader.class.getName() + "();");    pigServer.registerQuery("Store B into 'out' using mock.Storage();");    if (pigServer.executeBatch().get(0).getStatus() != JOB_STATUS.COMPLETED) {        throw new RuntimeException("Job failed", pigServer.executeBatch().get(0).getException());    }    List<Tuple> result = data.get("out");    assertEquals(rows, result.size());    int i = 0;    for (Tuple tuple : result) {        assertEquals("a" + i, tuple.get(0));        ++i;    }}
0
public void testMultipleSchema() throws ExecException, Exception
{    String out = "target/out";    int rows = 1000;    Properties props = new Properties();    props.setProperty("parquet.compression", "uncompressed");    props.setProperty("parquet.page.size", "1000");    PigServer pigServer = new PigServer(ExecType.LOCAL, props);    Data data = Storage.resetData(pigServer);    Collection<Tuple> list1 = new ArrayList<Tuple>();    for (int i = 0; i < rows; i++) {        list1.add(tuple("a" + i));    }    Collection<Tuple> list2 = new ArrayList<Tuple>();    for (int i = 0; i < rows; i++) {        list2.add(tuple("b" + i));    }    data.set("a", "a:chararray", list1);    data.set("b", "b:chararray", list2);    pigServer.setBatchOn();    pigServer.registerQuery("A = LOAD 'a' USING mock.Storage();");    pigServer.registerQuery("B = LOAD 'b' USING mock.Storage();");    pigServer.deleteFile(out);    pigServer.registerQuery("Store A into '" + out + "/a' using " + ParquetStorer.class.getName() + "();");    pigServer.registerQuery("Store B into '" + out + "/b' using " + ParquetStorer.class.getName() + "();");    if (pigServer.executeBatch().get(0).getStatus() != JOB_STATUS.COMPLETED) {        throw new RuntimeException("Job failed", pigServer.executeBatch().get(0).getException());    }    pigServer.registerQuery("B = LOAD '" + out + "/*' USING " + ParquetLoader.class.getName() + "();");    pigServer.registerQuery("Store B into 'out' using mock.Storage();");    if (pigServer.executeBatch().get(0).getStatus() != JOB_STATUS.COMPLETED) {        throw new RuntimeException("Job failed", pigServer.executeBatch().get(0).getException());    }    List<Tuple> result = data.get("out");    final Schema schema = data.getSchema("out");    assertEquals(2, schema.size());        int ai;    int bi;    if ("a".equals(schema.getField(0).alias)) {        ai = 0;        bi = 1;        assertEquals("a", schema.getField(0).alias);        assertEquals("b", schema.getField(1).alias);    } else {        ai = 1;        bi = 0;        assertEquals("b", schema.getField(0).alias);        assertEquals("a", schema.getField(1).alias);    }    assertEquals(rows * 2, result.size());    int a = 0;    int b = 0;    for (Tuple tuple : result) {        String fa = (String) tuple.get(ai);        String fb = (String) tuple.get(bi);        if (fa != null) {            assertEquals("a" + a, fa);            ++a;        }        if (fb != null) {            assertEquals("b" + b, fb);            ++b;        }    }}
0
public void testStorerCompressed() throws ExecException, Exception
{    String out = "target/out";    int rows = 1000;    Properties props = new Properties();    props.setProperty("parquet.compression", "gzip");    props.setProperty("parquet.page.size", "1000");    PigServer pigServer = new PigServer(ExecType.LOCAL, props);    Data data = Storage.resetData(pigServer);    Collection<Tuple> list = new ArrayList<Tuple>();    for (int i = 0; i < rows; i++) {        list.add(Storage.tuple("a" + i));    }    data.set("in", "a:chararray", list);    pigServer.setBatchOn();    pigServer.registerQuery("A = LOAD 'in' USING mock.Storage();");    pigServer.deleteFile(out);    pigServer.registerQuery("Store A into '" + out + "' using " + ParquetStorer.class.getName() + "();");    if (pigServer.executeBatch().get(0).getStatus() != JOB_STATUS.COMPLETED) {        throw new RuntimeException("Job failed", pigServer.executeBatch().get(0).getException());    }    pigServer.registerQuery("B = LOAD '" + out + "' USING " + ParquetLoader.class.getName() + "();");    pigServer.registerQuery("Store B into 'out' using mock.Storage();");    if (pigServer.executeBatch().get(0).getStatus() != JOB_STATUS.COMPLETED) {        throw new RuntimeException("Job failed", pigServer.executeBatch().get(0).getException());    }    List<Tuple> result = data.get("out");    assertEquals(rows, result.size());    int i = 0;    for (Tuple tuple : result) {        assertEquals("a" + i, tuple.get(0));        ++i;    }}
0
public void testComplexSchema() throws ExecException, Exception
{    String out = "target/out";    PigServer pigServer = new PigServer(ExecType.LOCAL);    Data data = Storage.resetData(pigServer);    Collection<Tuple> list = new ArrayList<Tuple>();    for (int i = 0; i < 1000; i++) {        list.add(tuple("a" + i, bag(tuple("o", "b"))));    }    for (int i = 10; i < 2000; i++) {        list.add(tuple("a" + i, bag(tuple("o", "b"), tuple("o", "b"), tuple("o", "b"), tuple("o", "b"))));    }    for (int i = 20; i < 3000; i++) {        list.add(tuple("a" + i, bag(tuple("o", "b"), tuple("o", null), tuple(null, "b"), tuple(null, null))));    }    for (int i = 30; i < 4000; i++) {        list.add(tuple("a" + i, null));    }    Collections.shuffle((List<?>) list);    data.set("in", "a:chararray, b:{t:(c:chararray, d:chararray)}", list);    pigServer.setBatchOn();    pigServer.registerQuery("A = LOAD 'in' USING mock.Storage();");    pigServer.deleteFile(out);    pigServer.registerQuery("Store A into '" + out + "' using " + ParquetStorer.class.getName() + "();");    if (pigServer.executeBatch().get(0).getStatus() != JOB_STATUS.COMPLETED) {        throw new RuntimeException("Job failed", pigServer.executeBatch().get(0).getException());    }    {        pigServer.registerQuery("B = LOAD '" + out + "' USING " + ParquetLoader.class.getName() + "();");        pigServer.registerQuery("Store B into 'out' using mock.Storage();");        if (pigServer.executeBatch().get(0).getStatus() != JOB_STATUS.COMPLETED) {            throw new RuntimeException("Job failed", pigServer.executeBatch().get(0).getException());        }        List<Tuple> result = data.get("out");        assertEquals(list, result);        final Schema schema = data.getSchema("out");        assertEquals("{a:chararray, b:{t:(c:chararray, d:chararray)}}".replaceAll(" ", ""), schema.toString().replaceAll(" ", ""));    }    {        pigServer.registerQuery("C = LOAD '" + out + "' USING " + ParquetLoader.class.getName() + "('a:chararray');");        pigServer.registerQuery("Store C into 'out2' using mock.Storage();");        if (pigServer.executeBatch().get(0).getStatus() != JOB_STATUS.COMPLETED) {            throw new RuntimeException("Job failed", pigServer.executeBatch().get(0).getException());        }        final Function<Tuple, Object> grabFirstColumn = new Function<Tuple, Object>() {            @Override            public Object apply(Tuple input) {                try {                    return input.get(0);                } catch (ExecException e) {                    throw new RuntimeException(e);                }            }        };        List<Tuple> result2 = data.get("out2");                Object[] result2int = Collections2.transform(result2, grabFirstColumn).toArray();        Object[] input2int = Collections2.transform(list, grabFirstColumn).toArray();        assertArrayEquals(input2int, result2int);    }}
0
public Object apply(Tuple input)
{    try {        return input.get(0);    } catch (ExecException e) {        throw new RuntimeException(e);    }}
0
private void testPigConversion(String pigSchemaString) throws Exception
{    Schema pigSchema = Utils.getSchemaFromString(pigSchemaString);    MessageType parquetSchema = pigSchemaConverter.convert(pigSchema);    Schema convertedSchema = pigSchemaConverter.convert(parquetSchema);    assertEquals(pigSchema, convertedSchema);}
0
public void testSimpleBag() throws Exception
{    testPigConversion("b:{t:(a:int)}");}
0
public void testMultiBag() throws Exception
{    testPigConversion("x:int, b:{t:(a:int,b:chararray)}");}
0
public void testMapSimple() throws Exception
{    testPigConversion("b:[(c:int)]");}
0
public void testMapTuple() throws Exception
{    testPigConversion("a:chararray, b:[(c:chararray, d:chararray)]");}
0
public void testMapOfList() throws Exception
{    testPigConversion("a:map[{bag: (a:int)}]");}
0
public void testListsOfPrimitive() throws Exception
{    for (Type.Repetition repetition : Type.Repetition.values()) {        for (Type.Repetition valueRepetition : Type.Repetition.values()) {            for (PrimitiveType.PrimitiveTypeName primitiveTypeName : PrimitiveType.PrimitiveTypeName.values()) {                if (primitiveTypeName != PrimitiveType.PrimitiveTypeName.INT96) {                                        Types.PrimitiveBuilder<PrimitiveType> value = Types.primitive(primitiveTypeName, valueRepetition);                    if (primitiveTypeName == PrimitiveType.PrimitiveTypeName.FIXED_LEN_BYTE_ARRAY)                        value.length(1);                    GroupType type = Types.buildGroup(repetition).addField(value.named("b")).as(OriginalType.LIST).named("a");                                        pigSchemaConverter.convertField(type);                }            }        }    }}
0
private void testConversion(String pigSchemaString, String schemaString) throws Exception
{    Schema pigSchema = Utils.getSchemaFromString(pigSchemaString);    MessageType schema = pigSchemaConverter.convert(pigSchema);    MessageType expectedMT = MessageTypeParser.parseMessageType(schemaString);    assertEquals("converting " + pigSchemaString + " to " + schemaString, expectedMT, schema);    MessageType filtered = pigSchemaConverter.filter(schema, pigSchema, null);    assertEquals("converting " + pigSchemaString + " to " + schemaString + " and filtering", schema.toString(), filtered.toString());}
0
public void testTupleBag() throws Exception
{    testConversion("a:chararray, b:{t:(c:chararray, d:chararray)}", "message pig_schema {\n" + "  optional binary a (UTF8);\n" + "  optional group b (LIST) {\n" + "    repeated group t {\n" + "      optional binary c (UTF8);\n" + "      optional binary d (UTF8);\n" + "    }\n" + "  }\n" + "}\n");}
0
public void testTupleBagWithAnonymousInnerField() throws Exception
{    testConversion("a:chararray, b:{(c:chararray, d:chararray)}", "message pig_schema {\n" + "  optional binary a (UTF8);\n" + "  optional group b (LIST) {\n" +     "    repeated group bag {\n" + "      optional binary c (UTF8);\n" + "      optional binary d (UTF8);\n" + "    }\n" + "  }\n" + "}\n");}
0
public void testMap() throws Exception
{    testConversion("a:chararray, b:[(c:chararray, d:chararray)]", "message pig_schema {\n" + "  optional binary a (UTF8);\n" + "  optional group b (MAP) {\n" + "    repeated group map (MAP_KEY_VALUE) {\n" + "      required binary key (UTF8);\n" + "      optional group value {\n" + "        optional binary c (UTF8);\n" + "        optional binary d (UTF8);\n" + "      }\n" + "    }\n" + "  }\n" + "}\n");}
0
public void testMap2() throws Exception
{    testConversion("a:map[int]", "message pig_schema {\n" + "  optional group a (MAP) {\n" + "    repeated group map (MAP_KEY_VALUE) {\n" + "      required binary key (UTF8);\n" + "      optional int32 value;" + "    }\n" + "  }\n" + "}\n");}
0
public void testMap3() throws Exception
{    testConversion("a:map[map[int]]", "message pig_schema {\n" + "  optional group a (MAP) {\n" + "    repeated group map (MAP_KEY_VALUE) {\n" + "      required binary key (UTF8);\n" + "      optional group value (MAP) {\n" + "        repeated group map (MAP_KEY_VALUE) {\n" + "          required binary key (UTF8);\n" + "          optional int32 value;\n" + "        }\n" + "      }\n" + "    }\n" + "  }\n" + "}\n");}
0
public void testMap4() throws Exception
{    testConversion("a:map[bag{(a:int)}]", "message pig_schema {\n" + "  optional group a (MAP) {\n" + "    repeated group map (MAP_KEY_VALUE) {\n" + "      required binary key (UTF8);\n" + "      optional group value (LIST) {\n" + "        repeated group bag {\n" + "          optional int32 a;\n" + "        }\n" + "      }\n" + "    }\n" + "  }\n" + "}\n");}
0
public void testListOfPrimitiveIsABag() throws Exception
{    testFixedConversion("message pig_schema {\n" + "  optional group a (LIST) {\n" + "    repeated binary b (UTF8);\n" + "  }\n" + "}\n", "a:{" + PigSchemaConverter.ARRAY_VALUE_NAME + ":(b: chararray)}");}
0
private void testFixedConversion(String schemaString, String pigSchemaString) throws Exception
{    Schema expectedPigSchema = Utils.getSchemaFromString(pigSchemaString);    MessageType parquetSchema = MessageTypeParser.parseMessageType(schemaString);    Schema pigSchema = pigSchemaConverter.convert(parquetSchema);    assertEquals("converting " + schemaString + " to " + pigSchemaString, expectedPigSchema, pigSchema);}
0
public void testMapWithFixed() throws Exception
{    testFixedConversion("message pig_schema {\n" + "  optional binary a;\n" + "  optional group b (MAP) {\n" + "    repeated group map (MAP_KEY_VALUE) {\n" + "      required binary key;\n" + "      optional group value {\n" + "        optional fixed_len_byte_array(5) c;\n" + "        optional fixed_len_byte_array(7) d;\n" + "      }\n" + "    }\n" + "  }\n" + "}\n", "a:bytearray, b:[(c:bytearray, d:bytearray)]");}
0
public void testMapWithFixedWithoutOriginalType() throws Exception
{    testFixedConversion("message spark_schema {\n" + "  optional binary a;\n" + "  optional group b (MAP) {\n" + "    repeated group map {\n" + "      required binary key;\n" + "      optional group value {\n" + "        optional fixed_len_byte_array(5) c;\n" + "        optional fixed_len_byte_array(7) d;\n" + "      }\n" + "    }\n" + "  }\n" + "}\n", "a:bytearray, b:[(c:bytearray, d:bytearray)]");}
0
public void testInt96() throws Exception
{    testFixedConversion("message spark_schema {\n" + "  optional int96 datetime;\n" + "}", "datetime:bytearray");}
0
public void testAnonymousField() throws Exception
{    testConversion("a:chararray, int", "message pig_schema {\n" + "  optional binary a (UTF8);\n" + "  optional int32 val_0;\n" + "}\n");}
0
public void testSchemaEvolution()
{    Map<String, Set<String>> map = new LinkedHashMap<String, Set<String>>();    map.put("pig.schema", new LinkedHashSet<String>(Arrays.asList("a:int, b:int, c:int, d:int, e:int, f:int", "aa:int, aaa:int, b:int, c:int, ee:int")));    Schema result = getPigSchemaFromMultipleFiles(new MessageType("file_schema", new PrimitiveType(OPTIONAL, INT32, "a")), map);    assertEquals("a: int,b: int,c: int,d: int,e: int,f: int,aa: int,aaa: int,ee: int", pigSchemaToString(result));}
0
public void testArtSchema() throws ExecException, ParserException
{    String pigSchemaString = "DocId:long, " + "Links:(Backward:{(long)}, Forward:{(long)}), " + "Name:{(Language:{(Code:chararray,Country:chararray)}, Url:chararray)}";    SimpleGroup g = new SimpleGroup(getMessageType(pigSchemaString));    g.add("DocId", 1l);    Group links = g.addGroup("Links");    links.addGroup("Backward").addGroup("bag").add(0, 1l);    links.addGroup("Forward").addGroup("bag").add(0, 1l);    Group name = g.addGroup("Name").addGroup("bag");    name.addGroup("Language").addGroup("bag").append("Code", "en").append("Country", "US");    name.add("Url", "http://foo/bar");    testFromGroups(pigSchemaString, Arrays.<Group>asList(g));}
0
public void testBags() throws ExecException, ParserException
{    String pigSchemaString = "a: {(b: chararray)}";    SimpleGroup g = new SimpleGroup(getMessageType(pigSchemaString));    Group addGroup = g.addGroup("a");    addGroup.addGroup("bag").append("b", "foo");    addGroup.addGroup("bag").append("b", "bar");    testFromGroups(pigSchemaString, Arrays.<Group>asList(g));}
0
public void testMaps() throws ExecException, ParserException
{    String pigSchemaString = "a: [(b: chararray)]";    SimpleGroup g = new SimpleGroup(getMessageType(pigSchemaString));    Group map = g.addGroup("a");    map.addGroup("map").append("key", "foo").addGroup("value").append("b", "foo");    map.addGroup("map").append("key", "bar").addGroup("value").append("b", "bar");    testFromGroups(pigSchemaString, Arrays.<Group>asList(g));}
0
public void testComplexSchema() throws Exception
{    String pigSchemaString = "a:chararray, b:{t:(c:chararray, d:chararray)}";    Tuple t0 = tuple("a" + 0, bag(tuple("o", "b"), tuple("o1", "b1")));    Tuple t1 = tuple("a" + 1, bag(tuple("o", "b"), tuple("o", "b"), tuple("o", "b"), tuple("o", "b")));    Tuple t2 = tuple("a" + 2, bag(tuple("o", "b"), tuple("o", null), tuple(null, "b"), tuple(null, null)));    Tuple t3 = tuple("a" + 3, null);    testFromTuple(pigSchemaString, Arrays.asList(t0, t1, t2, t3));}
0
public void testMapSchema() throws Exception
{    String pigSchemaString = "a:chararray, b:[(c:chararray, d:chararray)]";    Tuple t0 = tuple("a" + 0, new HashMap() {        {            put("foo", tuple("o", "b"));        }    });    Tuple t1 = tuple("a" + 1, new HashMap() {        {            put("foo", tuple("o", "b"));            put("foo", tuple("o", "b"));            put("foo", tuple("o", "b"));            put("foo", tuple("o", "b"));        }    });    Tuple t2 = tuple("a" + 2, new HashMap() {        {            put("foo", tuple("o", "b"));            put("foo", tuple("o", null));            put("foo", tuple(null, "b"));            put("foo", tuple(null, null));        }    });    Tuple t3 = tuple("a" + 3, null);    testFromTuple(pigSchemaString, Arrays.asList(t0, t1, t2, t3));}
0
private void testFromTuple(String pigSchemaString, List<Tuple> input) throws Exception
{    List<Tuple> tuples = new ArrayList<Tuple>();    RecordMaterializer<Tuple> recordConsumer = newPigRecordConsumer(pigSchemaString);    TupleWriteSupport tupleWriter = newTupleWriter(pigSchemaString, recordConsumer);    for (Tuple tuple : input) {                tupleWriter.write(tuple);        tuples.add(recordConsumer.getCurrentRecord());    }    assertEquals(input.size(), tuples.size());    for (int i = 0; i < input.size(); i++) {        Tuple in = input.get(i);        Tuple out = tuples.get(i);        assertEquals(in.toString(), out.toString());    }}
1
private void testFromGroups(String pigSchemaString, List<Group> input) throws ParserException
{    List<Tuple> tuples = new ArrayList<Tuple>();    MessageType schema = getMessageType(pigSchemaString);    RecordMaterializer<Tuple> pigRecordConsumer = newPigRecordConsumer(pigSchemaString);    GroupWriter groupWriter = new GroupWriter(new RecordConsumerLoggingWrapper(new ConverterConsumer(pigRecordConsumer.getRootConverter(), schema)), schema);    for (Group group : input) {        groupWriter.write(group);        final Tuple tuple = pigRecordConsumer.getCurrentRecord();        tuples.add(tuple);            }    List<Group> groups = new ArrayList<Group>();    GroupRecordConverter recordConsumer = new GroupRecordConverter(schema);    TupleWriteSupport tupleWriter = newTupleWriter(pigSchemaString, recordConsumer);    for (Tuple t : tuples) {                tupleWriter.write(t);        groups.add(recordConsumer.getCurrentRecord());    }    assertEquals(input.size(), groups.size());    for (int i = 0; i < input.size(); i++) {        Group in = input.get(i);                Group out = groups.get(i);        assertEquals(in.toString(), out.toString());    }}
1
private TupleWriteSupport newTupleWriter(String pigSchemaString, RecordMaterializer<T> recordConsumer) throws ParserException
{    TupleWriteSupport tupleWriter = TupleWriteSupport.fromPigSchema(pigSchemaString);    tupleWriter.init(null);    tupleWriter.prepareForWrite(new ConverterConsumer(recordConsumer.getRootConverter(), tupleWriter.getParquetSchema()));    return tupleWriter;}
0
private Map<String, String> pigMetaData(String pigSchemaString)
{    Map<String, String> map = new HashMap<String, String>();    new PigMetaData(pigSchemaString).addToMetaData(map);    return map;}
0
private RecordMaterializer<Tuple> newPigRecordConsumer(String pigSchemaString) throws ParserException
{    TupleReadSupport tupleReadSupport = new TupleReadSupport();    final Configuration configuration = new Configuration(false);    MessageType parquetSchema = getMessageType(pigSchemaString);    final Map<String, String> pigMetaData = pigMetaData(pigSchemaString);    Map<String, Set<String>> globalMetaData = new HashMap<String, Set<String>>();    for (Entry<String, String> entry : pigMetaData.entrySet()) {        globalMetaData.put(entry.getKey(), new HashSet<String>(Arrays.asList(entry.getValue())));    }    configuration.set(PARQUET_PIG_SCHEMA, pigSchemaString);    final ReadContext init = tupleReadSupport.init(new InitContext(configuration, globalMetaData, parquetSchema));    return tupleReadSupport.prepareForRead(configuration, pigMetaData, parquetSchema, init);}
0
private MessageType getMessageType(String pigSchemaString) throws ParserException
{    Schema pigSchema = Utils.getSchemaFromString(pigSchemaString);    return new PigSchemaConverter().convert(pigSchema);}
0
public static void main(String[] args) throws Exception
{    String pigSchema = pigSchema(false, false);    String pigSchemaProjected = pigSchema(true, false);    String pigSchemaNoString = pigSchema(true, true);    MessageType schema = new PigSchemaConverter().convert(Utils.getSchemaFromString(pigSchema));    MemPageStore memPageStore = new MemPageStore(0);    ColumnWriteStoreV1 columns = new ColumnWriteStoreV1(memPageStore, ParquetProperties.builder().withPageSize(50 * 1024 * 1024).withDictionaryEncoding(false).build());    write(memPageStore, columns, schema, pigSchema);    columns.flush();    read(memPageStore, pigSchema, pigSchemaProjected, pigSchemaNoString);    System.out.println(columns.getBufferedSize() + " bytes used total");    System.out.println("max col size: " + columns.maxColMemSize() + " bytes");}
0
private static String pigSchema(boolean projected, boolean noStrings)
{    StringBuilder sb = new StringBuilder();    for (int i = 0; i < TOP_LEVEL_COLS; i++) {        if (i != 0) {            sb.append(", ");        }        sb.append("i" + i + ":(");        if (!noStrings) {            for (int j = 0; j < (projected ? 2 : 4); j++) {                if (j != 0) {                    sb.append(", ");                }                sb.append("j" + j + ":chararray");            }            sb.append(", ");        }        for (int k = 0; k < (projected ? 2 : 4); k++) {            if (k != 0) {                sb.append(", ");            }            sb.append("k" + k + ":long");        }        for (int l = 0; l < (projected ? 1 : 2); l++) {            sb.append(", ");            sb.append("l" + l + ":{t:(v:int)}");        }        sb.append(")");    }    return sb.toString();}
0
private static Tuple tuple() throws ExecException
{    TupleFactory tf = TupleFactory.getInstance();    Tuple t = tf.newTuple(TOP_LEVEL_COLS);    for (int i = 0; i < TOP_LEVEL_COLS; i++) {        Tuple ti = tf.newTuple(10);        for (int j = 0; j < 4; j++) {            ti.set(j, "foo" + i + "," + j);        }        for (int k = 0; k < 4; k++) {            ti.set(4 + k, (long) k);        }        for (int l = 0; l < 2; l++) {            DataBag bag = new NonSpillableDataBag();            for (int m = 0; m < 10; m++) {                bag.add(tf.newTuple((Object) new Integer(m)));            }            ti.set(8 + l, bag);        }        t.set(i, ti);    }    return t;}
0
private static void read(PageReadStore columns, String pigSchemaString, String pigSchemaProjected, String pigSchemaProjectedNoStrings) throws ParserException
{    read(columns, pigSchemaString, "read all");    read(columns, pigSchemaProjected, "read projected");    read(columns, pigSchemaProjectedNoStrings, "read projected no Strings");}
0
private static void read(PageReadStore columns, String pigSchemaString, String message) throws ParserException
{    System.out.println(message);    MessageColumnIO columnIO = newColumnFactory(pigSchemaString);    TupleReadSupport tupleReadSupport = new TupleReadSupport();    Map<String, String> pigMetaData = pigMetaData(pigSchemaString);    MessageType schema = new PigSchemaConverter().convert(Utils.getSchemaFromString(pigSchemaString));    ReadContext init = tupleReadSupport.init(null, pigMetaData, schema);    RecordMaterializer<Tuple> recordConsumer = tupleReadSupport.prepareForRead(null, pigMetaData, schema, init);    RecordReader<Tuple> recordReader = columnIO.getRecordReader(columns, recordConsumer);                    read(recordReader, 10000, pigSchemaString);    read(recordReader, 10000, pigSchemaString);    read(recordReader, 10000, pigSchemaString);    read(recordReader, 10000, pigSchemaString);    read(recordReader, 10000, pigSchemaString);    read(recordReader, 100000, pigSchemaString);    read(recordReader, 1000000, pigSchemaString);    System.out.println();}
0
private static Map<String, String> pigMetaData(String pigSchemaString)
{    Map<String, String> map = new HashMap<String, String>();    new PigMetaData(pigSchemaString).addToMetaData(map);    return map;}
0
private static void write(MemPageStore memPageStore, ColumnWriteStoreV1 columns, MessageType schema, String pigSchemaString) throws ExecException, ParserException
{    MessageColumnIO columnIO = newColumnFactory(pigSchemaString);    TupleWriteSupport tupleWriter = TupleWriteSupport.fromPigSchema(pigSchemaString);    tupleWriter.init(null);    tupleWriter.prepareForWrite(columnIO.getRecordWriter(columns));    write(memPageStore, tupleWriter, 10000);    write(memPageStore, tupleWriter, 10000);    write(memPageStore, tupleWriter, 10000);    write(memPageStore, tupleWriter, 10000);    write(memPageStore, tupleWriter, 10000);    write(memPageStore, tupleWriter, 100000);    write(memPageStore, tupleWriter, 1000000);    System.out.println();}
0
private static MessageColumnIO newColumnFactory(String pigSchemaString) throws ParserException
{    MessageType schema = new PigSchemaConverter().convert(Utils.getSchemaFromString(pigSchemaString));    return new ColumnIOFactory().getColumnIO(schema);}
0
private static void read(RecordReader<Tuple> recordReader, int count, String pigSchemaString) throws ParserException
{    long t0 = System.currentTimeMillis();    Tuple tuple = null;    for (int i = 0; i < count; i++) {        tuple = recordReader.read();    }    if (tuple == null) {        throw new RuntimeException();    }    long t1 = System.currentTimeMillis();    long t = t1 - t0;        float err = (float) 100 * 2 / t;    System.out.printf("read %,9d recs in %,5d ms at %,9d rec/s err: %3.2f%%\n", count, t, t == 0 ? 0 : count * 1000 / t, err);}
0
private static void write(MemPageStore memPageStore, TupleWriteSupport tupleWriter, int count) throws ExecException
{    Tuple tu = tuple();    long t0 = System.currentTimeMillis();    for (int i = 0; i < count; i++) {        tupleWriter.write(tu);    }    long t1 = System.currentTimeMillis();    long t = t1 - t0;    memPageStore.addRowCount(count);    System.out.printf("written %,9d recs in %,5d ms at %,9d rec/s\n", count, t, t == 0 ? 0 : count * 1000 / t);}
0
public Converter getConverter(int fieldIndex)
{    return converters[fieldIndex];}
0
public void start()
{}
0
public void end()
{    parent.add(myBuilder.build());    myBuilder.clear();}
0
private Converter newMessageConverter(final Message.Builder parentBuilder, final Descriptors.FieldDescriptor fieldDescriptor, Type parquetType)
{    boolean isRepeated = fieldDescriptor.isRepeated();    ParentValueContainer parent;    if (isRepeated) {        parent = new ParentValueContainer() {            @Override            public void add(Object value) {                parentBuilder.addRepeatedField(fieldDescriptor, value);            }        };    } else {        parent = new ParentValueContainer() {            @Override            public void add(Object value) {                parentBuilder.setField(fieldDescriptor, value);            }        };    }    LogicalTypeAnnotation logicalTypeAnnotation = parquetType.getLogicalTypeAnnotation();    if (logicalTypeAnnotation == null) {        return newScalarConverter(parent, parentBuilder, fieldDescriptor, parquetType);    }    return logicalTypeAnnotation.accept(new LogicalTypeAnnotation.LogicalTypeAnnotationVisitor<Converter>() {        @Override        public Optional<Converter> visit(LogicalTypeAnnotation.ListLogicalTypeAnnotation listLogicalType) {            return of(new ListConverter(parentBuilder, fieldDescriptor, parquetType));        }        @Override        public Optional<Converter> visit(LogicalTypeAnnotation.MapLogicalTypeAnnotation mapLogicalType) {            return of(new MapConverter(parentBuilder, fieldDescriptor, parquetType));        }    }).orElseGet(() -> newScalarConverter(parent, parentBuilder, fieldDescriptor, parquetType));}
0
public void add(Object value)
{    parentBuilder.addRepeatedField(fieldDescriptor, value);}
0
public void add(Object value)
{    parentBuilder.setField(fieldDescriptor, value);}
0
public Optional<Converter> visit(LogicalTypeAnnotation.ListLogicalTypeAnnotation listLogicalType)
{    return of(new ListConverter(parentBuilder, fieldDescriptor, parquetType));}
0
public Optional<Converter> visit(LogicalTypeAnnotation.MapLogicalTypeAnnotation mapLogicalType)
{    return of(new MapConverter(parentBuilder, fieldDescriptor, parquetType));}
0
private Converter newScalarConverter(ParentValueContainer pvc, Message.Builder parentBuilder, Descriptors.FieldDescriptor fieldDescriptor, Type parquetType)
{    JavaType javaType = fieldDescriptor.getJavaType();    switch(javaType) {        case STRING:            return new ProtoStringConverter(pvc);        case FLOAT:            return new ProtoFloatConverter(pvc);        case DOUBLE:            return new ProtoDoubleConverter(pvc);        case BOOLEAN:            return new ProtoBooleanConverter(pvc);        case BYTE_STRING:            return new ProtoBinaryConverter(pvc);        case ENUM:            return new ProtoEnumConverter(pvc, fieldDescriptor);        case INT:            return new ProtoIntConverter(pvc);        case LONG:            return new ProtoLongConverter(pvc);        case MESSAGE:            {                Message.Builder subBuilder = parentBuilder.newBuilderForField(fieldDescriptor);                return new ProtoMessageConverter(pvc, subBuilder, parquetType.asGroupType());            }    }    throw new UnsupportedOperationException(String.format("Cannot convert type: %s" + " (Parquet type: %s) ", javaType, parquetType));}
0
public Message.Builder getBuilder()
{    return myBuilder;}
0
private Map<Binary, Descriptors.EnumValueDescriptor> makeLookupStructure(Descriptors.FieldDescriptor enumFieldType)
{    Descriptors.EnumDescriptor enumType = enumFieldType.getEnumType();    Map<Binary, Descriptors.EnumValueDescriptor> lookupStructure = new HashMap<Binary, Descriptors.EnumValueDescriptor>();    List<Descriptors.EnumValueDescriptor> enumValues = enumType.getValues();    for (Descriptors.EnumValueDescriptor value : enumValues) {        String name = value.getName();        lookupStructure.put(Binary.fromString(name), enumType.findValueByName(name));    }    return lookupStructure;}
0
private Descriptors.EnumValueDescriptor translateEnumValue(Binary binaryValue)
{    Descriptors.EnumValueDescriptor protoValue = enumLookup.get(binaryValue);    if (protoValue == null) {        Set<Binary> knownValues = enumLookup.keySet();        String msg = "Illegal enum value \"" + binaryValue + "\"" + " in protocol buffer \"" + fieldType.getFullName() + "\"" + " legal values are: \"" + knownValues + "\"";        throw new InvalidRecordException(msg);    }    return protoValue;}
0
public final void addBinary(Binary binaryValue)
{    Descriptors.EnumValueDescriptor protoValue = translateEnumValue(binaryValue);    parent.add(protoValue);}
0
public void addValueFromDictionary(int dictionaryId)
{    parent.add(dict[dictionaryId]);}
0
public boolean hasDictionarySupport()
{    return true;}
0
public void setDictionary(Dictionary dictionary)
{    dict = new Descriptors.EnumValueDescriptor[dictionary.getMaxId() + 1];    for (int i = 0; i <= dictionary.getMaxId(); i++) {        Binary binaryValue = dictionary.decodeToBinary(i);        dict[i] = translateEnumValue(binaryValue);    }}
0
public void addBinary(Binary binary)
{    ByteString byteString = ByteString.copyFrom(binary.toByteBuffer());    parent.add(byteString);}
0
public final void addBoolean(boolean value)
{    parent.add(value);}
0
public void addDouble(double value)
{    parent.add(value);}
0
public void addFloat(float value)
{    parent.add(value);}
0
public void addInt(int value)
{    parent.add(value);}
0
public void addLong(long value)
{    parent.add(value);}
0
public void addBinary(Binary binary)
{    String str = binary.toStringUsingUTF8();    parent.add(str);}
0
public Converter getConverter(int fieldIndex)
{    if (fieldIndex > 0) {        throw new ParquetDecodingException("Unexpected multiple fields in the LIST wrapper");    }    return new GroupConverter() {        @Override        public Converter getConverter(int fieldIndex) {            return converter;        }        @Override        public void start() {        }        @Override        public void end() {        }    };}
0
public Converter getConverter(int fieldIndex)
{    return converter;}
0
public void start()
{}
0
public void end()
{}
0
public void start()
{}
0
public void end()
{}
0
public Converter getConverter(int fieldIndex)
{    if (fieldIndex > 0) {        throw new ParquetDecodingException("Unexpected multiple fields in the MAP wrapper");    }    return converter;}
0
public void start()
{}
0
public void end()
{}
0
public static void setRequestedProjection(Job job, String requestedProjection)
{    ProtoReadSupport.setRequestedProjection(ContextUtil.getConfiguration(job), requestedProjection);}
0
public static void setProtobufClass(Job job, Class<? extends Message> protoClass)
{    ProtoWriteSupport.setSchema(ContextUtil.getConfiguration(job), protoClass);}
0
public static Builder<T> builder(Path file)
{    return ParquetReader.builder(new ProtoReadSupport(), file);}
0
public static void setRequestedProjection(Configuration configuration, String requestedProjection)
{    configuration.set(PB_REQUESTED_PROJECTION, requestedProjection);}
0
public static void setProtobufClass(Configuration configuration, String protobufClass)
{    configuration.set(PB_CLASS, protobufClass);}
0
public ReadContext init(InitContext context)
{    String requestedProjectionString = context.getConfiguration().get(PB_REQUESTED_PROJECTION);    if (requestedProjectionString != null && !requestedProjectionString.trim().isEmpty()) {        MessageType requestedProjection = getSchemaForRead(context.getFileSchema(), requestedProjectionString);                return new ReadContext(requestedProjection);    } else {        MessageType fileSchema = context.getFileSchema();                return new ReadContext(fileSchema);    }}
1
public RecordMaterializer<T> prepareForRead(Configuration configuration, Map<String, String> keyValueMetaData, MessageType fileSchema, ReadContext readContext)
{    String headerProtoClass = keyValueMetaData.get(PB_CLASS);    String configuredProtoClass = configuration.get(PB_CLASS);    if (configuredProtoClass != null) {                headerProtoClass = configuredProtoClass;    }    if (headerProtoClass == null) {        throw new RuntimeException("I Need parameter " + PB_CLASS + " with Protocol Buffer class");    }        MessageType requestedSchema = readContext.getRequestedSchema();    Class<? extends Message> protobufClass = Protobufs.getProtobufClass(headerProtoClass);    return new ProtoRecordMaterializer(requestedSchema, protobufClass);}
1
public void add(Object a)
{    throw new RuntimeException("Should never happen");}
0
public void start()
{    reusedBuilder.clear();    super.start();}
0
public void end()
{}
0
public T getCurrentRecord()
{    if (buildBefore) {        return (T) this.reusedBuilder.build();    } else {        return (T) this.reusedBuilder;    }}
0
public void setBuildBefore(boolean buildBefore)
{    this.buildBefore = buildBefore;}
0
public T getCurrentRecord()
{    return root.getCurrentRecord();}
0
public GroupConverter getRootConverter()
{    return root;}
0
public MessageType convert(Class<? extends Message> protobufClass)
{        Descriptors.Descriptor descriptor = Protobufs.getMessageDescriptor(protobufClass);    MessageType messageType = convertFields(Types.buildMessage(), descriptor.getFields()).named(descriptor.getFullName());        return messageType;}
1
private GroupBuilder<T> convertFields(GroupBuilder<T> groupBuilder, List<FieldDescriptor> fieldDescriptors)
{    for (FieldDescriptor fieldDescriptor : fieldDescriptors) {        groupBuilder = addField(fieldDescriptor, groupBuilder).id(fieldDescriptor.getNumber()).named(fieldDescriptor.getName());    }    return groupBuilder;}
0
private Type.Repetition getRepetition(FieldDescriptor descriptor)
{    if (descriptor.isRequired()) {        return Type.Repetition.REQUIRED;    } else if (descriptor.isRepeated()) {        return Type.Repetition.REPEATED;    } else {        return Type.Repetition.OPTIONAL;    }}
0
private Builder<? extends Builder<?, GroupBuilder<T>>, GroupBuilder<T>> addField(FieldDescriptor descriptor, final GroupBuilder<T> builder)
{    if (descriptor.getJavaType() == JavaType.MESSAGE) {        return addMessageField(descriptor, builder);    }    ParquetType parquetType = getParquetType(descriptor);    if (descriptor.isRepeated() && parquetSpecsCompliant) {                return addRepeatedPrimitive(parquetType.primitiveType, parquetType.logicalTypeAnnotation, builder);    }    return builder.primitive(parquetType.primitiveType, getRepetition(descriptor)).as(parquetType.logicalTypeAnnotation);}
0
private Builder<? extends Builder<?, GroupBuilder<T>>, GroupBuilder<T>> addRepeatedPrimitive(PrimitiveTypeName primitiveType, LogicalTypeAnnotation logicalTypeAnnotation, final GroupBuilder<T> builder)
{    return builder.group(Type.Repetition.OPTIONAL).as(listType()).group(Type.Repetition.REPEATED).primitive(primitiveType, Type.Repetition.REQUIRED).as(logicalTypeAnnotation).named("element").named("list");}
0
private GroupBuilder<GroupBuilder<T>> addRepeatedMessage(FieldDescriptor descriptor, GroupBuilder<T> builder)
{    GroupBuilder<GroupBuilder<GroupBuilder<GroupBuilder<T>>>> result = builder.group(Type.Repetition.OPTIONAL).as(listType()).group(Type.Repetition.REPEATED).group(Type.Repetition.OPTIONAL);    convertFields(result, descriptor.getMessageType().getFields());    return result.named("element").named("list");}
0
private GroupBuilder<GroupBuilder<T>> addMessageField(FieldDescriptor descriptor, final GroupBuilder<T> builder)
{    if (descriptor.isMapField() && parquetSpecsCompliant) {                return addMapField(descriptor, builder);    }    if (descriptor.isRepeated() && parquetSpecsCompliant) {                return addRepeatedMessage(descriptor, builder);    }        GroupBuilder<GroupBuilder<T>> group = builder.group(getRepetition(descriptor));    convertFields(group, descriptor.getMessageType().getFields());    return group;}
0
private GroupBuilder<GroupBuilder<T>> addMapField(FieldDescriptor descriptor, final GroupBuilder<T> builder)
{    List<FieldDescriptor> fields = descriptor.getMessageType().getFields();    if (fields.size() != 2) {        throw new UnsupportedOperationException("Expected two fields for the map (key/value), but got: " + fields);    }    ParquetType mapKeyParquetType = getParquetType(fields.get(0));    GroupBuilder<GroupBuilder<GroupBuilder<T>>> group = builder.group(Type.Repetition.OPTIONAL).as(    mapType()).group(    Type.Repetition.REPEATED).primitive(mapKeyParquetType.primitiveType, Type.Repetition.REQUIRED).as(mapKeyParquetType.logicalTypeAnnotation).named("key");    return addField(fields.get(1), group).named("value").named("key_value");}
0
private ParquetType getParquetType(FieldDescriptor fieldDescriptor)
{    JavaType javaType = fieldDescriptor.getJavaType();    switch(javaType) {        case INT:            return ParquetType.of(INT32);        case LONG:            return ParquetType.of(INT64);        case DOUBLE:            return ParquetType.of(DOUBLE);        case BOOLEAN:            return ParquetType.of(BOOLEAN);        case FLOAT:            return ParquetType.of(FLOAT);        case STRING:            return ParquetType.of(BINARY, stringType());        case ENUM:            return ParquetType.of(BINARY, enumType());        case BYTE_STRING:            return ParquetType.of(BINARY);        default:            throw new UnsupportedOperationException("Cannot convert Protocol Buffer: unknown type " + javaType);    }}
0
public static ParquetType of(PrimitiveTypeName primitiveType, LogicalTypeAnnotation logicalTypeAnnotation)
{    return new ParquetType(primitiveType, logicalTypeAnnotation);}
0
public static ParquetType of(PrimitiveTypeName primitiveType)
{    return of(primitiveType, null);}
0
public String getName()
{    return "protobuf";}
0
public static void setSchema(Configuration configuration, Class<? extends Message> protoClass)
{    configuration.setClass(PB_CLASS_WRITE, protoClass, Message.class);}
0
public static void setWriteSpecsCompliant(Configuration configuration, boolean writeSpecsCompliant)
{    configuration.setBoolean(PB_SPECS_COMPLIANT_WRITE, writeSpecsCompliant);}
0
public void write(T record)
{    recordConsumer.startMessage();    try {        messageWriter.writeTopLevelMessage(record);    } catch (RuntimeException e) {        Message m = (record instanceof Message.Builder) ? ((Message.Builder) record).build() : (Message) record;                throw e;    }    recordConsumer.endMessage();}
1
public void prepareForWrite(RecordConsumer recordConsumer)
{    this.recordConsumer = recordConsumer;}
0
public WriteContext init(Configuration configuration)
{        if (protoMessage == null) {        Class<? extends Message> pbClass = configuration.getClass(PB_CLASS_WRITE, null, Message.class);        if (pbClass != null) {            protoMessage = pbClass;        } else {            String msg = "Protocol buffer class not specified.";            String hint = " Please use method ProtoParquetOutputFormat.setProtobufClass(...) or other similar method.";            throw new BadConfigurationException(msg + hint);        }    }    writeSpecsCompliant = configuration.getBoolean(PB_SPECS_COMPLIANT_WRITE, writeSpecsCompliant);    MessageType rootSchema = new ProtoSchemaConverter(writeSpecsCompliant).convert(protoMessage);    Descriptor messageDescriptor = Protobufs.getMessageDescriptor(protoMessage);    validatedMapping(messageDescriptor, rootSchema);    this.messageWriter = new MessageWriter(messageDescriptor, rootSchema);    Map<String, String> extraMetaData = new HashMap<String, String>();    extraMetaData.put(ProtoReadSupport.PB_CLASS, protoMessage.getName());    extraMetaData.put(ProtoReadSupport.PB_DESCRIPTOR, serializeDescriptor(protoMessage));    extraMetaData.put(PB_SPECS_COMPLIANT_WRITE, String.valueOf(writeSpecsCompliant));    return new WriteContext(rootSchema, extraMetaData);}
0
 void setFieldName(String fieldName)
{    this.fieldName = fieldName;}
0
 void setIndex(int index)
{    this.index = index;}
0
 void writeRawValue(Object value)
{}
0
 void writeField(Object value)
{    recordConsumer.startField(fieldName, index);    writeRawValue(value);    recordConsumer.endField(fieldName, index);}
0
private FieldWriter createWriter(FieldDescriptor fieldDescriptor, Type type)
{    switch(fieldDescriptor.getJavaType()) {        case STRING:            return new StringWriter();        case MESSAGE:            return createMessageWriter(fieldDescriptor, type);        case INT:            return new IntWriter();        case LONG:            return new LongWriter();        case FLOAT:            return new FloatWriter();        case DOUBLE:            return new DoubleWriter();        case ENUM:            return new EnumWriter();        case BOOLEAN:            return new BooleanWriter();        case BYTE_STRING:            return new BinaryWriter();    }        return unknownType(fieldDescriptor);}
0
private FieldWriter createMessageWriter(FieldDescriptor fieldDescriptor, Type type)
{    if (fieldDescriptor.isMapField() && writeSpecsCompliant) {        return createMapWriter(fieldDescriptor, type);    }    return new MessageWriter(fieldDescriptor.getMessageType(), getGroupType(type));}
0
private GroupType getGroupType(Type type)
{    LogicalTypeAnnotation logicalTypeAnnotation = type.getLogicalTypeAnnotation();    if (logicalTypeAnnotation == null) {        return type.asGroupType();    }    return logicalTypeAnnotation.accept(new LogicalTypeAnnotation.LogicalTypeAnnotationVisitor<GroupType>() {        @Override        public Optional<GroupType> visit(LogicalTypeAnnotation.ListLogicalTypeAnnotation listLogicalType) {            return ofNullable(type.asGroupType().getType("list").asGroupType().getType("element").asGroupType());        }        @Override        public Optional<GroupType> visit(LogicalTypeAnnotation.MapLogicalTypeAnnotation mapLogicalType) {            return ofNullable(type.asGroupType().getType("key_value").asGroupType().getType("value").asGroupType());        }    }).orElse(type.asGroupType());}
0
public Optional<GroupType> visit(LogicalTypeAnnotation.ListLogicalTypeAnnotation listLogicalType)
{    return ofNullable(type.asGroupType().getType("list").asGroupType().getType("element").asGroupType());}
0
public Optional<GroupType> visit(LogicalTypeAnnotation.MapLogicalTypeAnnotation mapLogicalType)
{    return ofNullable(type.asGroupType().getType("key_value").asGroupType().getType("value").asGroupType());}
0
private MapWriter createMapWriter(FieldDescriptor fieldDescriptor, Type type)
{    List<FieldDescriptor> fields = fieldDescriptor.getMessageType().getFields();    if (fields.size() != 2) {        throw new UnsupportedOperationException("Expected two fields for the map (key/value), but got: " + fields);    }        FieldDescriptor keyProtoField = fields.get(0);    FieldWriter keyWriter = createWriter(keyProtoField, type);    keyWriter.setFieldName(keyProtoField.getName());    keyWriter.setIndex(0);        FieldDescriptor valueProtoField = fields.get(1);    FieldWriter valueWriter = createWriter(valueProtoField, type);    valueWriter.setFieldName(valueProtoField.getName());    valueWriter.setIndex(1);    return new MapWriter(keyWriter, valueWriter);}
0
 void writeTopLevelMessage(Object value)
{    writeAllFields((MessageOrBuilder) value);}
0
 final void writeRawValue(Object value)
{    recordConsumer.startGroup();    writeAllFields((MessageOrBuilder) value);    recordConsumer.endGroup();}
0
 final void writeField(Object value)
{    recordConsumer.startField(fieldName, index);    writeRawValue(value);    recordConsumer.endField(fieldName, index);}
0
private void writeAllFields(MessageOrBuilder pb)
{        Map<FieldDescriptor, Object> changedPbFields = pb.getAllFields();    for (Map.Entry<FieldDescriptor, Object> entry : changedPbFields.entrySet()) {        FieldDescriptor fieldDescriptor = entry.getKey();        if (fieldDescriptor.isExtension()) {                        throw new UnsupportedOperationException("Cannot convert Protobuf message with extension field(s)");        }        int fieldIndex = fieldDescriptor.getIndex();        fieldWriters[fieldIndex].writeField(entry.getValue());    }}
0
 final void writeRawValue(Object value)
{    throw new UnsupportedOperationException("Array has no raw value");}
0
 final void writeField(Object value)
{    recordConsumer.startField(fieldName, index);    recordConsumer.startGroup();    List<?> list = (List<?>) value;        recordConsumer.startField("list", 0);    for (Object listEntry : list) {        recordConsumer.startGroup();                recordConsumer.startField("element", 0);        fieldWriter.writeRawValue(listEntry);        recordConsumer.endField("element", 0);        recordConsumer.endGroup();    }    recordConsumer.endField("list", 0);    recordConsumer.endGroup();    recordConsumer.endField(fieldName, index);}
0
 final void writeRawValue(Object value)
{    throw new UnsupportedOperationException("Array has no raw value");}
0
 final void writeField(Object value)
{    recordConsumer.startField(fieldName, index);    List<?> list = (List<?>) value;    for (Object listEntry : list) {        fieldWriter.writeRawValue(listEntry);    }    recordConsumer.endField(fieldName, index);}
0
private void validatedMapping(Descriptor descriptor, GroupType parquetSchema)
{    List<FieldDescriptor> allFields = descriptor.getFields();    for (FieldDescriptor fieldDescriptor : allFields) {        String fieldName = fieldDescriptor.getName();        int fieldIndex = fieldDescriptor.getIndex();        int parquetIndex = parquetSchema.getFieldIndex(fieldName);        if (fieldIndex != parquetIndex) {            String message = "FieldIndex mismatch name=" + fieldName + ": " + fieldIndex + " != " + parquetIndex;            throw new IncompatibleSchemaModificationException(message);        }    }}
0
 final void writeRawValue(Object value)
{    Binary binaryString = Binary.fromString((String) value);    recordConsumer.addBinary(binaryString);}
0
 final void writeRawValue(Object value)
{    recordConsumer.addInteger((Integer) value);}
0
 final void writeRawValue(Object value)
{    recordConsumer.addLong((Long) value);}
0
 final void writeRawValue(Object value)
{    recordConsumer.startGroup();        recordConsumer.startField("key_value", 0);    for (Message msg : (Collection<Message>) value) {        recordConsumer.startGroup();        final Descriptor descriptorForType = msg.getDescriptorForType();        final FieldDescriptor keyDesc = descriptorForType.findFieldByName("key");        final FieldDescriptor valueDesc = descriptorForType.findFieldByName("value");        keyWriter.writeField(msg.getField(keyDesc));        valueWriter.writeField(msg.getField(valueDesc));        recordConsumer.endGroup();    }    recordConsumer.endField("key_value", 0);    recordConsumer.endGroup();}
0
 final void writeRawValue(Object value)
{    recordConsumer.addFloat((Float) value);}
0
 final void writeRawValue(Object value)
{    recordConsumer.addDouble((Double) value);}
0
 final void writeRawValue(Object value)
{    Binary binary = Binary.fromString(((Descriptors.EnumValueDescriptor) value).getName());    recordConsumer.addBinary(binary);}
0
 final void writeRawValue(Object value)
{    recordConsumer.addBoolean((Boolean) value);}
0
 final void writeRawValue(Object value)
{    ByteString byteString = (ByteString) value;    Binary binary = Binary.fromConstantByteArray(byteString.toByteArray());    recordConsumer.addBinary(binary);}
0
private FieldWriter unknownType(FieldDescriptor fieldDescriptor)
{    String exceptionMsg = "Unknown type with descriptor \"" + fieldDescriptor + "\" and type \"" + fieldDescriptor.getJavaType() + "\".";    throw new InvalidRecordException(exceptionMsg);}
0
private String serializeDescriptor(Class<? extends Message> protoClass)
{    Descriptor descriptor = Protobufs.getMessageDescriptor(protoClass);    DescriptorProtos.DescriptorProto asProto = descriptor.toProto();    return TextFormat.printToString(asProto);}
0
public void testInputOutput() throws Exception
{    TestProtobuf.IOFormatMessage input;    {        TestProtobuf.IOFormatMessage.Builder msg = TestProtobuf.IOFormatMessage.newBuilder();        msg.setOptionalDouble(666);        msg.addRepeatedString("Msg1");        msg.addRepeatedString("Msg2");        msg.getMsgBuilder().setSomeId(323);        input = msg.build();    }    List<Message> result = runMRJobs(input);    assertEquals(1, result.size());    TestProtobuf.IOFormatMessage output = (TestProtobuf.IOFormatMessage) result.get(0);    assertEquals(666, output.getOptionalDouble(), 0.00001);    assertEquals(323, output.getMsg().getSomeId());    assertEquals("Msg1", output.getRepeatedString(0));    assertEquals("Msg2", output.getRepeatedString(1));    assertEquals(input, output);}
0
public void testProto3InputOutput() throws Exception
{    TestProto3.IOFormatMessage input;    {        TestProto3.IOFormatMessage.Builder msg = TestProto3.IOFormatMessage.newBuilder();        msg.setOptionalDouble(666);        msg.addRepeatedString("Msg1");        msg.addRepeatedString("Msg2");        msg.getMsgBuilder().setSomeId(323);        input = msg.build();    }    List<Message> result = runMRJobs(input);    assertEquals(1, result.size());    TestProto3.IOFormatMessage output = (TestProto3.IOFormatMessage) result.get(0);    assertEquals(666, output.getOptionalDouble(), 0.00001);    assertEquals(323, output.getMsg().getSomeId());    assertEquals("Msg1", output.getRepeatedString(0));    assertEquals("Msg2", output.getRepeatedString(1));    assertEquals(input, output);}
0
public void testProjection() throws Exception
{    TestProtobuf.Document.Builder writtenDocument = TestProtobuf.Document.newBuilder();    writtenDocument.setDocId(12345);    writtenDocument.addNameBuilder().setUrl("http://goout.cz/");    Path outputPath = new WriteUsingMR().write(writtenDocument.build());        ReadUsingMR reader = new ReadUsingMR();    String projection = "message Document {required int64 DocId; }";    reader.setRequestedProjection(projection);    List<Message> output = reader.read(outputPath);    TestProtobuf.Document readDocument = (TestProtobuf.Document) output.get(0);        assertTrue(readDocument.hasDocId());    assertTrue("Found data outside projection.", readDocument.getNameCount() == 0);}
0
public void testProto3Projection() throws Exception
{    TestProto3.Document.Builder writtenDocument = TestProto3.Document.newBuilder();    writtenDocument.setDocId(12345);    writtenDocument.addNameBuilder().setUrl("http://goout.cz/");    Path outputPath = new WriteUsingMR().write(writtenDocument.build());        ReadUsingMR reader = new ReadUsingMR();    String projection = "message Document {optional int64 DocId; }";    reader.setRequestedProjection(projection);    List<Message> output = reader.read(outputPath);    TestProto3.Document readDocument = (TestProto3.Document) output.get(0);        assertTrue(readDocument.getDocId() == 12345);    assertTrue(readDocument.getNameCount() == 0);    assertTrue("Found data outside projection.", readDocument.getNameCount() == 0);}
0
public void testCustomProtoClass() throws Exception
{    FirstCustomClassMessage.Builder inputMessage;    inputMessage = FirstCustomClassMessage.newBuilder();    inputMessage.setString("writtenString");    Path outputPath = new WriteUsingMR().write(new Message[] { inputMessage.build() });    ReadUsingMR readUsingMR = new ReadUsingMR();    String customClass = SecondCustomClassMessage.class.getName();    ProtoReadSupport.setProtobufClass(readUsingMR.getConfiguration(), customClass);    List<Message> result = readUsingMR.read(outputPath);    assertEquals(1, result.size());    Message msg = result.get(0);    assertFalse("Class from header returned.", msg instanceof FirstCustomClassMessage);    assertTrue("Custom class was not used", msg instanceof SecondCustomClassMessage);    String stringValue;    stringValue = ((SecondCustomClassMessage) msg).getString();    assertEquals("writtenString", stringValue);}
0
public void testProto3CustomProtoClass() throws Exception
{    TestProto3.FirstCustomClassMessage.Builder inputMessage;    inputMessage = TestProto3.FirstCustomClassMessage.newBuilder();    inputMessage.setString("writtenString");    Path outputPath = new WriteUsingMR().write(new Message[] { inputMessage.build() });    ReadUsingMR readUsingMR = new ReadUsingMR();    String customClass = TestProto3.SecondCustomClassMessage.class.getName();    ProtoReadSupport.setProtobufClass(readUsingMR.getConfiguration(), customClass);    List<Message> result = readUsingMR.read(outputPath);    assertEquals(1, result.size());    Message msg = result.get(0);    assertFalse("Class from header returned.", msg instanceof TestProto3.FirstCustomClassMessage);    assertTrue("Custom class was not used", msg instanceof TestProto3.SecondCustomClassMessage);    String stringValue;    stringValue = ((TestProto3.SecondCustomClassMessage) msg).getString();    assertEquals("writtenString", stringValue);}
0
public void testRepeatedIntMessageClass() throws Exception
{    TestProtobuf.RepeatedIntMessage msgEmpty = TestProtobuf.RepeatedIntMessage.newBuilder().build();    TestProtobuf.RepeatedIntMessage msgNonEmpty = TestProtobuf.RepeatedIntMessage.newBuilder().addRepeatedInt(1).addRepeatedInt(2).build();    Path outputPath = new WriteUsingMR().write(msgEmpty, msgNonEmpty);    ReadUsingMR readUsingMR = new ReadUsingMR();    String customClass = TestProtobuf.RepeatedIntMessage.class.getName();    ProtoReadSupport.setProtobufClass(readUsingMR.getConfiguration(), customClass);    List<Message> result = readUsingMR.read(outputPath);    assertEquals(2, result.size());    assertEquals(msgEmpty, result.get(0));    assertEquals(msgNonEmpty, result.get(1));}
0
public void testRepeatedIntMessageClassSchemaCompliant() throws Exception
{    TestProtobuf.RepeatedIntMessage msgEmpty = TestProtobuf.RepeatedIntMessage.newBuilder().build();    TestProtobuf.RepeatedIntMessage msgNonEmpty = TestProtobuf.RepeatedIntMessage.newBuilder().addRepeatedInt(1).addRepeatedInt(2).build();    Configuration conf = new Configuration();    ProtoWriteSupport.setWriteSpecsCompliant(conf, true);    Path outputPath = new WriteUsingMR(conf).write(msgEmpty, msgNonEmpty);    ReadUsingMR readUsingMR = new ReadUsingMR();    String customClass = TestProtobuf.RepeatedIntMessage.class.getName();    ProtoReadSupport.setProtobufClass(readUsingMR.getConfiguration(), customClass);    List<Message> result = readUsingMR.read(outputPath);    assertEquals(2, result.size());    assertEquals(msgEmpty, result.get(0));    assertEquals(msgNonEmpty, result.get(1));}
0
public void testMapIntMessageClass() throws Exception
{    TestProtobuf.MapIntMessage msgEmpty = TestProtobuf.MapIntMessage.newBuilder().build();    TestProtobuf.MapIntMessage msgNonEmpty = TestProtobuf.MapIntMessage.newBuilder().putMapInt(1, 123).putMapInt(2, 234).build();    Path outputPath = new WriteUsingMR().write(msgEmpty, msgNonEmpty);    ReadUsingMR readUsingMR = new ReadUsingMR();    String customClass = TestProtobuf.MapIntMessage.class.getName();    ProtoReadSupport.setProtobufClass(readUsingMR.getConfiguration(), customClass);    List<Message> result = readUsingMR.read(outputPath);    assertEquals(2, result.size());    assertEquals(msgEmpty, result.get(0));    assertEquals(msgNonEmpty, result.get(1));}
0
public void testMapIntMessageClassSchemaCompliant() throws Exception
{    TestProtobuf.MapIntMessage msgEmpty = TestProtobuf.MapIntMessage.newBuilder().build();    TestProtobuf.MapIntMessage msgNonEmpty = TestProtobuf.MapIntMessage.newBuilder().putMapInt(1, 123).putMapInt(2, 234).build();    Configuration conf = new Configuration();    ProtoWriteSupport.setWriteSpecsCompliant(conf, true);    Path outputPath = new WriteUsingMR(conf).write(msgEmpty, msgNonEmpty);    ReadUsingMR readUsingMR = new ReadUsingMR(conf);    String customClass = TestProtobuf.MapIntMessage.class.getName();    ProtoReadSupport.setProtobufClass(readUsingMR.getConfiguration(), customClass);    List<Message> result = readUsingMR.read(outputPath);    assertEquals(2, result.size());    assertEquals(msgEmpty, result.get(0));    assertEquals(msgNonEmpty, result.get(1));}
0
public void testRepeatedInnerMessageClass() throws Exception
{    TestProtobuf.RepeatedInnerMessage msgEmpty = TestProtobuf.RepeatedInnerMessage.newBuilder().build();    TestProtobuf.RepeatedInnerMessage msgNonEmpty = TestProtobuf.RepeatedInnerMessage.newBuilder().addRepeatedInnerMessage(TestProtobuf.InnerMessage.newBuilder().setOne("one").build()).addRepeatedInnerMessage(TestProtobuf.InnerMessage.newBuilder().setTwo("two").build()).build();    Path outputPath = new WriteUsingMR().write(msgEmpty, msgNonEmpty);    ReadUsingMR readUsingMR = new ReadUsingMR();    String customClass = TestProtobuf.RepeatedInnerMessage.class.getName();    ProtoReadSupport.setProtobufClass(readUsingMR.getConfiguration(), customClass);    List<Message> result = readUsingMR.read(outputPath);    assertEquals(2, result.size());    assertEquals(msgEmpty, result.get(0));    assertEquals(msgNonEmpty, result.get(1));}
0
public void testRepeatedInnerMessageClassSchemaCompliant() throws Exception
{    TestProtobuf.RepeatedInnerMessage msgEmpty = TestProtobuf.RepeatedInnerMessage.newBuilder().build();    TestProtobuf.RepeatedInnerMessage msgNonEmpty = TestProtobuf.RepeatedInnerMessage.newBuilder().addRepeatedInnerMessage(TestProtobuf.InnerMessage.newBuilder().setOne("one").build()).addRepeatedInnerMessage(TestProtobuf.InnerMessage.newBuilder().setTwo("two").build()).build();    Configuration conf = new Configuration();    ProtoWriteSupport.setWriteSpecsCompliant(conf, true);    Path outputPath = new WriteUsingMR(conf).write(msgEmpty, msgNonEmpty);    ReadUsingMR readUsingMR = new ReadUsingMR(conf);    String customClass = TestProtobuf.RepeatedInnerMessage.class.getName();    ProtoReadSupport.setProtobufClass(readUsingMR.getConfiguration(), customClass);    List<Message> result = readUsingMR.read(outputPath);    assertEquals(2, result.size());    assertEquals(msgEmpty, result.get(0));    assertEquals(msgNonEmpty, result.get(1));}
0
public static List<Message> runMRJobs(Message... messages) throws Exception
{    Path outputPath = new WriteUsingMR().write(messages);    List<Message> result = new ReadUsingMR().read(outputPath);    return result;}
0
public void testAllTypes() throws Exception
{    SchemaConverterAllDatatypes.Builder data;    data = SchemaConverterAllDatatypes.newBuilder();    data.setOptionalBool(true);    data.setOptionalBytes(ByteString.copyFrom("someText", "UTF-8"));    data.setOptionalDouble(0.577);    data.setOptionalFloat(3.1415f);    data.setOptionalEnum(SchemaConverterAllDatatypes.TestEnum.FIRST);    data.setOptionalFixed32(1000 * 1000 * 1);    data.setOptionalFixed64(1000 * 1000 * 1000 * 2);    data.setOptionalInt32(1000 * 1000 * 3);    data.setOptionalInt64(1000L * 1000 * 1000 * 4);    data.setOptionalSFixed32(1000 * 1000 * 5);    data.setOptionalSFixed64(1000L * 1000 * 1000 * 6);    data.setOptionalSInt32(1000 * 1000 * 56);    data.setOptionalSInt64(1000L * 1000 * 1000 * 7);    data.setOptionalString("Good Will Hunting");    data.setOptionalUInt32(1000 * 1000 * 8);    data.setOptionalUInt64(1000L * 1000 * 1000 * 9);    data.getOptionalMessageBuilder().setSomeId(1984);    data.getPbGroupBuilder().setGroupInt(1492);    SchemaConverterAllDatatypes dataBuilt = data.build();    data.clear();    List<TestProtobuf.SchemaConverterAllDatatypes> result;    result = testData(dataBuilt);        SchemaConverterAllDatatypes o = result.get(0);    assertEquals("Good Will Hunting", o.getOptionalString());    assertEquals(true, o.getOptionalBool());    assertEquals(ByteString.copyFrom("someText", "UTF-8"), o.getOptionalBytes());    assertEquals(0.577, o.getOptionalDouble(), 0.00001);    assertEquals(3.1415f, o.getOptionalFloat(), 0.00001);    assertEquals(SchemaConverterAllDatatypes.TestEnum.FIRST, o.getOptionalEnum());    assertEquals(1000 * 1000 * 1, o.getOptionalFixed32());    assertEquals(1000 * 1000 * 1000 * 2, o.getOptionalFixed64());    assertEquals(1000 * 1000 * 3, o.getOptionalInt32());    assertEquals(1000L * 1000 * 1000 * 4, o.getOptionalInt64());    assertEquals(1000 * 1000 * 5, o.getOptionalSFixed32());    assertEquals(1000L * 1000 * 1000 * 6, o.getOptionalSFixed64());    assertEquals(1000 * 1000 * 56, o.getOptionalSInt32());    assertEquals(1000L * 1000 * 1000 * 7, o.getOptionalSInt64());    assertEquals(1000 * 1000 * 8, o.getOptionalUInt32());    assertEquals(1000L * 1000 * 1000 * 9, o.getOptionalUInt64());    assertEquals(1984, o.getOptionalMessage().getSomeId());    assertEquals(1492, o.getPbGroup().getGroupInt());}
0
public void testProto3AllTypes() throws Exception
{    TestProto3.SchemaConverterAllDatatypes.Builder data;    data = TestProto3.SchemaConverterAllDatatypes.newBuilder();    data.setOptionalBool(true);    data.setOptionalBytes(ByteString.copyFrom("someText", "UTF-8"));    data.setOptionalDouble(0.577);    data.setOptionalFloat(3.1415f);    data.setOptionalEnum(TestProto3.SchemaConverterAllDatatypes.TestEnum.FIRST);    data.setOptionalFixed32(1000 * 1000 * 1);    data.setOptionalFixed64(1000 * 1000 * 1000 * 2);    data.setOptionalInt32(1000 * 1000 * 3);    data.setOptionalInt64(1000L * 1000 * 1000 * 4);    data.setOptionalSFixed32(1000 * 1000 * 5);    data.setOptionalSFixed64(1000L * 1000 * 1000 * 6);    data.setOptionalSInt32(1000 * 1000 * 56);    data.setOptionalSInt64(1000L * 1000 * 1000 * 7);    data.setOptionalString("Good Will Hunting");    data.setOptionalUInt32(1000 * 1000 * 8);    data.setOptionalUInt64(1000L * 1000 * 1000 * 9);    data.getOptionalMessageBuilder().setSomeId(1984);    TestProto3.SchemaConverterAllDatatypes dataBuilt = data.build();    data.clear();    List<TestProto3.SchemaConverterAllDatatypes> result;    result = testData(dataBuilt);        TestProto3.SchemaConverterAllDatatypes o = result.get(0);    assertEquals("Good Will Hunting", o.getOptionalString());    assertEquals(true, o.getOptionalBool());    assertEquals(ByteString.copyFrom("someText", "UTF-8"), o.getOptionalBytes());    assertEquals(0.577, o.getOptionalDouble(), 0.00001);    assertEquals(3.1415f, o.getOptionalFloat(), 0.00001);    assertEquals(TestProto3.SchemaConverterAllDatatypes.TestEnum.FIRST, o.getOptionalEnum());    assertEquals(1000 * 1000 * 1, o.getOptionalFixed32());    assertEquals(1000 * 1000 * 1000 * 2, o.getOptionalFixed64());    assertEquals(1000 * 1000 * 3, o.getOptionalInt32());    assertEquals(1000L * 1000 * 1000 * 4, o.getOptionalInt64());    assertEquals(1000 * 1000 * 5, o.getOptionalSFixed32());    assertEquals(1000L * 1000 * 1000 * 6, o.getOptionalSFixed64());    assertEquals(1000 * 1000 * 56, o.getOptionalSInt32());    assertEquals(1000L * 1000 * 1000 * 7, o.getOptionalSInt64());    assertEquals(1000 * 1000 * 8, o.getOptionalUInt32());    assertEquals(1000L * 1000 * 1000 * 9, o.getOptionalUInt64());    assertEquals(1984, o.getOptionalMessage().getSomeId());}
0
public void testAllTypesMultiple() throws Exception
{    int count = 100;    SchemaConverterAllDatatypes[] input = new SchemaConverterAllDatatypes[count];    for (int i = 0; i < count; i++) {        SchemaConverterAllDatatypes.Builder d = SchemaConverterAllDatatypes.newBuilder();        if (i % 2 != 0)            d.setOptionalBool(true);        if (i % 3 != 0)            d.setOptionalBytes(ByteString.copyFrom("someText " + i, "UTF-8"));        if (i % 4 != 0)            d.setOptionalDouble(0.577 * i);        if (i % 5 != 0)            d.setOptionalFloat(3.1415f * i);        if (i % 6 != 0)            d.setOptionalEnum(SchemaConverterAllDatatypes.TestEnum.FIRST);        if (i % 7 != 0)            d.setOptionalFixed32(1000 * i * 1);        if (i % 8 != 0)            d.setOptionalFixed64(1000 * i * 1000 * 2);        if (i % 9 != 0)            d.setOptionalInt32(1000 * i * 3);        if (i % 2 != 1)            d.setOptionalSFixed32(1000 * i * 5);        if (i % 3 != 1)            d.setOptionalSFixed64(1000 * i * 1000 * 6);        if (i % 4 != 1)            d.setOptionalSInt32(1000 * i * 56);        if (i % 5 != 1)            d.setOptionalSInt64(1000 * i * 1000 * 7);        if (i % 6 != 1)            d.setOptionalString("Good Will Hunting " + i);        if (i % 7 != 1)            d.setOptionalUInt32(1000 * i * 8);        if (i % 8 != 1)            d.setOptionalUInt64(1000 * i * 1000 * 9);        if (i % 9 != 1)            d.getOptionalMessageBuilder().setSomeId(1984 * i);        if (i % 2 != 1)            d.getPbGroupBuilder().setGroupInt(1492 * i);        if (i % 3 != 1)            d.setOptionalInt64(1000 * i * 1000 * 4);        input[i] = d.build();    }    List<TestProtobuf.SchemaConverterAllDatatypes> result;    result = testData(input);        assertEquals("Good Will Hunting 0", result.get(0).getOptionalString());    assertEquals("Good Will Hunting 90", result.get(90).getOptionalString());}
0
public void testProto3AllTypesMultiple() throws Exception
{    int count = 100;    TestProto3.SchemaConverterAllDatatypes[] input = new TestProto3.SchemaConverterAllDatatypes[count];    for (int i = 0; i < count; i++) {        TestProto3.SchemaConverterAllDatatypes.Builder d = TestProto3.SchemaConverterAllDatatypes.newBuilder();        if (i % 2 != 0)            d.setOptionalBool(true);        if (i % 3 != 0)            d.setOptionalBytes(ByteString.copyFrom("someText " + i, "UTF-8"));        if (i % 4 != 0)            d.setOptionalDouble(0.577 * i);        if (i % 5 != 0)            d.setOptionalFloat(3.1415f * i);        if (i % 6 != 0)            d.setOptionalEnum(TestProto3.SchemaConverterAllDatatypes.TestEnum.FIRST);        if (i % 7 != 0)            d.setOptionalFixed32(1000 * i * 1);        if (i % 8 != 0)            d.setOptionalFixed64(1000 * i * 1000 * 2);        if (i % 9 != 0)            d.setOptionalInt32(1000 * i * 3);        if (i % 2 != 1)            d.setOptionalSFixed32(1000 * i * 5);        if (i % 3 != 1)            d.setOptionalSFixed64(1000 * i * 1000 * 6);        if (i % 4 != 1)            d.setOptionalSInt32(1000 * i * 56);        if (i % 5 != 1)            d.setOptionalSInt64(1000 * i * 1000 * 7);        if (i % 6 != 1)            d.setOptionalString("Good Will Hunting " + i);        if (i % 7 != 1)            d.setOptionalUInt32(1000 * i * 8);        if (i % 8 != 1)            d.setOptionalUInt64(1000 * i * 1000 * 9);        if (i % 9 != 1)            d.getOptionalMessageBuilder().setSomeId(1984 * i);        if (i % 3 != 1)            d.setOptionalInt64(1000 * i * 1000 * 4);        input[i] = d.build();    }    List<TestProto3.SchemaConverterAllDatatypes> result;    result = testData(input);        assertEquals("Good Will Hunting 0", result.get(0).getOptionalString());    assertEquals("Good Will Hunting 90", result.get(90).getOptionalString());}
0
public void testDefaults() throws Exception
{    SchemaConverterAllDatatypes.Builder data;    data = SchemaConverterAllDatatypes.newBuilder();    List<SchemaConverterAllDatatypes> result = testData(data.build());    SchemaConverterAllDatatypes message = result.get(0);    assertEquals("", message.getOptionalString());    assertEquals(false, message.getOptionalBool());    assertEquals(0, message.getOptionalFixed32());}
0
public void testProto3Defaults() throws Exception
{    TestProto3.SchemaConverterAllDatatypes.Builder data;    data = TestProto3.SchemaConverterAllDatatypes.newBuilder();    List<TestProto3.SchemaConverterAllDatatypes> result = testData(data.build());    TestProto3.SchemaConverterAllDatatypes message = result.get(0);    assertEquals("", message.getOptionalString());    assertEquals(false, message.getOptionalBool());    assertEquals(0, message.getOptionalFixed32());}
0
public void testRepeatedMessages() throws Exception
{    TestProtobuf.TopMessage.Builder top = TestProtobuf.TopMessage.newBuilder();    top.addInnerBuilder().setOne("First inner");    top.addInnerBuilder().setTwo("Second inner");    top.addInnerBuilder().setThree("Third inner");    TestProtobuf.TopMessage result = testData(top.build()).get(0);    assertEquals(3, result.getInnerCount());    TestProtobuf.InnerMessage first = result.getInner(0);    TestProtobuf.InnerMessage second = result.getInner(1);    TestProtobuf.InnerMessage third = result.getInner(2);    assertEquals("First inner", first.getOne());    assertFalse(first.hasTwo());    assertFalse(first.hasThree());    assertEquals("Second inner", second.getTwo());    assertFalse(second.hasOne());    assertFalse(second.hasThree());    assertEquals("Third inner", third.getThree());    assertFalse(third.hasOne());    assertFalse(third.hasTwo());}
0
public void testProto3RepeatedMessages() throws Exception
{    TestProto3.TopMessage.Builder top = TestProto3.TopMessage.newBuilder();    top.addInnerBuilder().setOne("First inner");    top.addInnerBuilder().setTwo("Second inner");    top.addInnerBuilder().setThree("Third inner");    TestProto3.TopMessage result = testData(top.build()).get(0);    assertEquals(3, result.getInnerCount());    TestProto3.InnerMessage first = result.getInner(0);    TestProto3.InnerMessage second = result.getInner(1);    TestProto3.InnerMessage third = result.getInner(2);    assertEquals("First inner", first.getOne());    assertTrue(first.getTwo().isEmpty());    assertTrue(first.getThree().isEmpty());    assertEquals("Second inner", second.getTwo());    assertTrue(second.getOne().isEmpty());    assertTrue(second.getThree().isEmpty());    assertEquals("Third inner", third.getThree());    assertTrue(third.getOne().isEmpty());    assertTrue(third.getTwo().isEmpty());}
0
public void testRepeatedInt() throws Exception
{    TestProtobuf.RepeatedIntMessage.Builder top = TestProtobuf.RepeatedIntMessage.newBuilder();    top.addRepeatedInt(1);    top.addRepeatedInt(2);    top.addRepeatedInt(3);    TestProtobuf.RepeatedIntMessage result = testData(top.build()).get(0);    assertEquals(3, result.getRepeatedIntCount());    assertEquals(1, result.getRepeatedInt(0));    assertEquals(2, result.getRepeatedInt(1));    assertEquals(3, result.getRepeatedInt(2));}
0
public void testProto3RepeatedInt() throws Exception
{    TestProto3.RepeatedIntMessage.Builder top = TestProto3.RepeatedIntMessage.newBuilder();    top.addRepeatedInt(1);    top.addRepeatedInt(2);    top.addRepeatedInt(3);    TestProto3.RepeatedIntMessage result = testData(top.build()).get(0);    assertEquals(3, result.getRepeatedIntCount());    assertEquals(1, result.getRepeatedInt(0));    assertEquals(2, result.getRepeatedInt(1));    assertEquals(3, result.getRepeatedInt(2));}
0
public void testLargeProtobufferFieldId() throws Exception
{    TestProtobuf.HighIndexMessage.Builder builder = TestProtobuf.HighIndexMessage.newBuilder();    builder.addRepeatedInt(1);    builder.addRepeatedInt(2);    testData(builder.build());}
0
public void testProto3LargeProtobufferFieldId() throws Exception
{    TestProto3.HighIndexMessage.Builder builder = TestProto3.HighIndexMessage.newBuilder();    builder.addRepeatedInt(1);    builder.addRepeatedInt(2);    testData(builder.build());}
0
private void testConversion(Class<? extends Message> pbClass, String parquetSchemaString, boolean parquetSpecsCompliant) throws Exception
{    ProtoSchemaConverter protoSchemaConverter = new ProtoSchemaConverter(parquetSpecsCompliant);    MessageType schema = protoSchemaConverter.convert(pbClass);    MessageType expectedMT = MessageTypeParser.parseMessageType(parquetSchemaString);    assertEquals(expectedMT.toString(), schema.toString());}
0
private void testConversion(Class<? extends Message> pbClass, String parquetSchemaString) throws Exception
{    testConversion(pbClass, parquetSchemaString, true);}
0
public void testConvertAllDatatypes() throws Exception
{    String expectedSchema = "message TestProtobuf.SchemaConverterAllDatatypes {\n" + "  optional double optionalDouble = 1;\n" + "  optional float optionalFloat = 2;\n" + "  optional int32 optionalInt32 = 3;\n" + "  optional int64 optionalInt64 = 4;\n" + "  optional int32 optionalUInt32 = 5;\n" + "  optional int64 optionalUInt64 = 6;\n" + "  optional int32 optionalSInt32 = 7;\n" + "  optional int64 optionalSInt64 = 8;\n" + "  optional int32 optionalFixed32 = 9;\n" + "  optional int64 optionalFixed64 = 10;\n" + "  optional int32 optionalSFixed32 = 11;\n" + "  optional int64 optionalSFixed64 = 12;\n" + "  optional boolean optionalBool = 13;\n" + "  optional binary optionalString (UTF8) = 14;\n" + "  optional binary optionalBytes = 15;\n" + "  optional group optionalMessage = 16 {\n" + "    optional int32 someId = 3;\n" + "  }\n" + "  optional group pbgroup = 17 {\n" + "    optional int32 groupInt = 2;\n" + "  }\n" + " optional binary optionalEnum (ENUM)  = 18;" + "}";    testConversion(TestProtobuf.SchemaConverterAllDatatypes.class, expectedSchema);}
0
public void testProto3ConvertAllDatatypes() throws Exception
{    String expectedSchema = "message TestProto3.SchemaConverterAllDatatypes {\n" + "  optional double optionalDouble = 1;\n" + "  optional float optionalFloat = 2;\n" + "  optional int32 optionalInt32 = 3;\n" + "  optional int64 optionalInt64 = 4;\n" + "  optional int32 optionalUInt32 = 5;\n" + "  optional int64 optionalUInt64 = 6;\n" + "  optional int32 optionalSInt32 = 7;\n" + "  optional int64 optionalSInt64 = 8;\n" + "  optional int32 optionalFixed32 = 9;\n" + "  optional int64 optionalFixed64 = 10;\n" + "  optional int32 optionalSFixed32 = 11;\n" + "  optional int64 optionalSFixed64 = 12;\n" + "  optional boolean optionalBool = 13;\n" + "  optional binary optionalString (UTF8) = 14;\n" + "  optional binary optionalBytes = 15;\n" + "  optional group optionalMessage = 16 {\n" + "    optional int32 someId = 3;\n" + "  }\n" + "  optional binary optionalEnum (ENUM) = 18;" + "  optional int32 someInt32 = 19;" + "  optional binary someString (UTF8) = 20;" + "  optional group optionalMap (MAP) = 21 {\n" + "    repeated group key_value {\n" + "      required int64 key;\n" + "      optional group value {\n" + "        optional int32 someId = 3;\n" + "      }\n" + "    }\n" + "  }\n" + "}";    testConversion(TestProto3.SchemaConverterAllDatatypes.class, expectedSchema);}
0
public void testConvertRepetition() throws Exception
{    String expectedSchema = "message TestProtobuf.SchemaConverterRepetition {\n" + "  optional int32 optionalPrimitive = 1;\n" + "  required int32 requiredPrimitive = 2;\n" + "  optional group repeatedPrimitive (LIST) = 3 {\n" + "    repeated group list {\n" + "      required int32 element;\n" + "    }\n" + "  }\n" + "  optional group optionalMessage = 7 {\n" + "    optional int32 someId = 3;\n" + "  }\n" + "  required group requiredMessage = 8 {\n" + "    optional int32 someId= 3;\n" + "  }\n" + "  optional group repeatedMessage (LIST) = 9 {\n" + "    repeated group list {\n" + "      optional group element {\n" + "        optional int32 someId = 3;\n" + "      }\n" + "    }\n" + "  }" + "}";    testConversion(TestProtobuf.SchemaConverterRepetition.class, expectedSchema);}
0
public void testProto3ConvertRepetition() throws Exception
{    String expectedSchema = "message TestProto3.SchemaConverterRepetition {\n" + "  optional int32 optionalPrimitive = 1;\n" + "  optional group repeatedPrimitive (LIST) = 3 {\n" + "    repeated group list {\n" + "      required int32 element;\n" + "    }\n" + "  }\n" + "  optional group optionalMessage = 7 {\n" + "    optional int32 someId = 3;\n" + "  }\n" + "  optional group repeatedMessage (LIST) = 9 {\n" + "    repeated group list {\n" + "      optional group element {\n" + "        optional int32 someId = 3;\n" + "      }\n" + "    }\n" + "  }\n" + "}";    testConversion(TestProto3.SchemaConverterRepetition.class, expectedSchema);}
0
public void testConvertRepeatedIntMessage() throws Exception
{    String expectedSchema = "message TestProtobuf.RepeatedIntMessage {\n" + "  optional group repeatedInt (LIST) = 1 {\n" + "    repeated group list {\n" + "      required int32 element;\n" + "      }\n" + "    }\n" + "  }\n" + "}";    testConversion(TestProtobuf.RepeatedIntMessage.class, expectedSchema);}
0
public void testConvertRepeatedIntMessageNonSpecsCompliant() throws Exception
{    String expectedSchema = "message TestProtobuf.RepeatedIntMessage {\n" + "  repeated int32 repeatedInt = 1;\n" + "}";    testConversion(TestProtobuf.RepeatedIntMessage.class, expectedSchema, false);}
0
public void testProto3ConvertRepeatedIntMessage() throws Exception
{    String expectedSchema = "message TestProto3.RepeatedIntMessage {\n" + "  optional group repeatedInt (LIST) = 1 {\n" + "    repeated group list {\n" + "      required int32 element;\n" + "      }\n" + "    }\n" + "  }\n" + "}";    testConversion(TestProto3.RepeatedIntMessage.class, expectedSchema);}
0
public void testProto3ConvertRepeatedIntMessageNonSpecsCompliant() throws Exception
{    String expectedSchema = "message TestProto3.RepeatedIntMessage {\n" + "  repeated int32 repeatedInt = 1;\n" + "}";    testConversion(TestProto3.RepeatedIntMessage.class, expectedSchema, false);}
0
public void testConvertRepeatedInnerMessage() throws Exception
{    String expectedSchema = "message TestProtobuf.RepeatedInnerMessage {\n" + "  optional group repeatedInnerMessage (LIST) = 1 {\n" + "    repeated group list {\n" + "      optional group element {\n" + "        optional binary one (UTF8) = 1;\n" + "        optional binary two (UTF8) = 2;\n" + "        optional binary three (UTF8) = 3;\n" + "      }\n" + "    }\n" + "  }\n" + "}";    testConversion(TestProtobuf.RepeatedInnerMessage.class, expectedSchema);}
0
public void testConvertRepeatedInnerMessageNonSpecsCompliant() throws Exception
{    String expectedSchema = "message TestProtobuf.RepeatedInnerMessage {\n" + "  repeated group repeatedInnerMessage = 1 {\n" + "    optional binary one (UTF8) = 1;\n" + "    optional binary two (UTF8) = 2;\n" + "    optional binary three (UTF8) = 3;\n" + "  }\n" + "}";    testConversion(TestProtobuf.RepeatedInnerMessage.class, expectedSchema, false);}
0
public void testProto3ConvertRepeatedInnerMessage() throws Exception
{    String expectedSchema = "message TestProto3.RepeatedInnerMessage {\n" + "  optional group repeatedInnerMessage (LIST) = 1 {\n" + "    repeated group list {\n" + "      optional group element {\n" + "        optional binary one (UTF8) = 1;\n" + "        optional binary two (UTF8) = 2;\n" + "        optional binary three (UTF8) = 3;\n" + "      }\n" + "    }\n" + "  }\n" + "}";    testConversion(TestProto3.RepeatedInnerMessage.class, expectedSchema);}
0
public void testProto3ConvertRepeatedInnerMessageNonSpecsCompliant() throws Exception
{    String expectedSchema = "message TestProto3.RepeatedInnerMessage {\n" + "  repeated group repeatedInnerMessage = 1 {\n" + "    optional binary one (UTF8) = 1;\n" + "    optional binary two (UTF8) = 2;\n" + "    optional binary three (UTF8) = 3;\n" + "  }\n" + "}";    testConversion(TestProto3.RepeatedInnerMessage.class, expectedSchema, false);}
0
public void testConvertMapIntMessage() throws Exception
{    String expectedSchema = "message TestProtobuf.MapIntMessage {\n" + "  optional group mapInt (MAP) = 1 {\n" + "    repeated group key_value {\n" + "      required int32 key;\n" + "      optional int32 value;\n" + "    }\n" + "  }\n" + "}";    testConversion(TestProtobuf.MapIntMessage.class, expectedSchema);}
0
public void testConvertMapIntMessageNonSpecsCompliant() throws Exception
{    String expectedSchema = "message TestProtobuf.MapIntMessage {\n" + "  repeated group mapInt = 1 {\n" + "    optional int32 key = 1;\n" + "    optional int32 value = 2;\n" + "  }\n" + "}";    testConversion(TestProtobuf.MapIntMessage.class, expectedSchema, false);}
0
public void testProto3ConvertMapIntMessage() throws Exception
{    String expectedSchema = "message TestProto3.MapIntMessage {\n" + "  optional group mapInt (MAP) = 1 {\n" + "    repeated group key_value {\n" + "      required int32 key;\n" + "      optional int32 value;\n" + "    }\n" + "  }\n" + "}";    testConversion(TestProto3.MapIntMessage.class, expectedSchema);}
0
public void testProto3ConvertMapIntMessageNonSpecsCompliant() throws Exception
{    String expectedSchema = "message TestProto3.MapIntMessage {\n" + "  repeated group mapInt = 1 {\n" + "    optional int32 key = 1;\n" + "    optional int32 value = 2;\n" + "  }\n" + "}";    testConversion(TestProto3.MapIntMessage.class, expectedSchema, false);}
0
private ProtoWriteSupport<T> createReadConsumerInstance(Class<T> cls, RecordConsumer readConsumerMock)
{    return createReadConsumerInstance(cls, readConsumerMock, new Configuration());}
0
private ProtoWriteSupport<T> createReadConsumerInstance(Class<T> cls, RecordConsumer readConsumerMock, Configuration conf)
{    ProtoWriteSupport support = new ProtoWriteSupport(cls);    support.init(conf);    support.prepareForWrite(readConsumerMock);    return support;}
0
public void testSimplestMessage() throws Exception
{    RecordConsumer readConsumerMock = Mockito.mock(RecordConsumer.class);    ProtoWriteSupport instance = createReadConsumerInstance(TestProtobuf.InnerMessage.class, readConsumerMock);    TestProtobuf.InnerMessage.Builder msg = TestProtobuf.InnerMessage.newBuilder();    msg.setOne("oneValue");    instance.write(msg.build());    InOrder inOrder = Mockito.inOrder(readConsumerMock);    inOrder.verify(readConsumerMock).startMessage();    inOrder.verify(readConsumerMock).startField("one", 0);    inOrder.verify(readConsumerMock).addBinary(Binary.fromString("oneValue"));    inOrder.verify(readConsumerMock).endField("one", 0);    inOrder.verify(readConsumerMock).endMessage();    Mockito.verifyNoMoreInteractions(readConsumerMock);}
0
public void testProto3SimplestMessage() throws Exception
{    RecordConsumer readConsumerMock = Mockito.mock(RecordConsumer.class);    ProtoWriteSupport instance = createReadConsumerInstance(TestProto3.InnerMessage.class, readConsumerMock);    TestProto3.InnerMessage.Builder msg = TestProto3.InnerMessage.newBuilder();    msg.setOne("oneValue");    instance.write(msg.build());    InOrder inOrder = Mockito.inOrder(readConsumerMock);    inOrder.verify(readConsumerMock).startMessage();    inOrder.verify(readConsumerMock).startField("one", 0);    inOrder.verify(readConsumerMock).addBinary(Binary.fromString("oneValue"));    inOrder.verify(readConsumerMock).endField("one", 0);    inOrder.verify(readConsumerMock).endMessage();    Mockito.verifyNoMoreInteractions(readConsumerMock);}
0
public void testRepeatedIntMessageSpecsCompliant() throws Exception
{    RecordConsumer readConsumerMock = Mockito.mock(RecordConsumer.class);    Configuration conf = new Configuration();    ProtoWriteSupport.setWriteSpecsCompliant(conf, true);    ProtoWriteSupport instance = createReadConsumerInstance(TestProtobuf.RepeatedIntMessage.class, readConsumerMock, conf);    TestProtobuf.RepeatedIntMessage.Builder msg = TestProtobuf.RepeatedIntMessage.newBuilder();    msg.addRepeatedInt(1323);    msg.addRepeatedInt(54469);    instance.write(msg.build());    InOrder inOrder = Mockito.inOrder(readConsumerMock);    inOrder.verify(readConsumerMock).startMessage();    inOrder.verify(readConsumerMock).startField("repeatedInt", 0);    inOrder.verify(readConsumerMock).startGroup();    inOrder.verify(readConsumerMock).startField("list", 0);    inOrder.verify(readConsumerMock).startGroup();    inOrder.verify(readConsumerMock).startField("element", 0);    inOrder.verify(readConsumerMock).addInteger(1323);    inOrder.verify(readConsumerMock).endField("element", 0);    inOrder.verify(readConsumerMock).endGroup();    inOrder.verify(readConsumerMock).startGroup();    inOrder.verify(readConsumerMock).startField("element", 0);    inOrder.verify(readConsumerMock).addInteger(54469);    inOrder.verify(readConsumerMock).endField("element", 0);    inOrder.verify(readConsumerMock).endGroup();    inOrder.verify(readConsumerMock).endField("list", 0);    inOrder.verify(readConsumerMock).endGroup();    inOrder.verify(readConsumerMock).endField("repeatedInt", 0);    inOrder.verify(readConsumerMock).endMessage();    Mockito.verifyNoMoreInteractions(readConsumerMock);}
0
public void testRepeatedIntMessage() throws Exception
{    RecordConsumer readConsumerMock = Mockito.mock(RecordConsumer.class);    ProtoWriteSupport instance = createReadConsumerInstance(TestProtobuf.RepeatedIntMessage.class, readConsumerMock);    TestProtobuf.RepeatedIntMessage.Builder msg = TestProtobuf.RepeatedIntMessage.newBuilder();    msg.addRepeatedInt(1323);    msg.addRepeatedInt(54469);    instance.write(msg.build());    InOrder inOrder = Mockito.inOrder(readConsumerMock);    inOrder.verify(readConsumerMock).startMessage();    inOrder.verify(readConsumerMock).startField("repeatedInt", 0);    inOrder.verify(readConsumerMock).addInteger(1323);    inOrder.verify(readConsumerMock).addInteger(54469);    inOrder.verify(readConsumerMock).endField("repeatedInt", 0);    inOrder.verify(readConsumerMock).endMessage();    Mockito.verifyNoMoreInteractions(readConsumerMock);}
0
public void testRepeatedIntMessageEmptySpecsCompliant() throws Exception
{    RecordConsumer readConsumerMock = Mockito.mock(RecordConsumer.class);    Configuration conf = new Configuration();    ProtoWriteSupport.setWriteSpecsCompliant(conf, true);    ProtoWriteSupport instance = createReadConsumerInstance(TestProtobuf.RepeatedIntMessage.class, readConsumerMock, conf);    TestProtobuf.RepeatedIntMessage.Builder msg = TestProtobuf.RepeatedIntMessage.newBuilder();    instance.write(msg.build());    InOrder inOrder = Mockito.inOrder(readConsumerMock);    inOrder.verify(readConsumerMock).startMessage();    inOrder.verify(readConsumerMock).endMessage();    Mockito.verifyNoMoreInteractions(readConsumerMock);}
0
public void testRepeatedIntMessageEmpty() throws Exception
{    RecordConsumer readConsumerMock = Mockito.mock(RecordConsumer.class);    ProtoWriteSupport instance = createReadConsumerInstance(TestProtobuf.RepeatedIntMessage.class, readConsumerMock);    TestProtobuf.RepeatedIntMessage.Builder msg = TestProtobuf.RepeatedIntMessage.newBuilder();    instance.write(msg.build());    InOrder inOrder = Mockito.inOrder(readConsumerMock);    inOrder.verify(readConsumerMock).startMessage();    inOrder.verify(readConsumerMock).endMessage();    Mockito.verifyNoMoreInteractions(readConsumerMock);}
0
public void testProto3RepeatedIntMessageSpecsCompliant() throws Exception
{    RecordConsumer readConsumerMock = Mockito.mock(RecordConsumer.class);    Configuration conf = new Configuration();    ProtoWriteSupport.setWriteSpecsCompliant(conf, true);    ProtoWriteSupport instance = createReadConsumerInstance(TestProto3.RepeatedIntMessage.class, readConsumerMock, conf);    TestProto3.RepeatedIntMessage.Builder msg = TestProto3.RepeatedIntMessage.newBuilder();    msg.addRepeatedInt(1323);    msg.addRepeatedInt(54469);    instance.write(msg.build());    InOrder inOrder = Mockito.inOrder(readConsumerMock);    inOrder.verify(readConsumerMock).startMessage();    inOrder.verify(readConsumerMock).startField("repeatedInt", 0);    inOrder.verify(readConsumerMock).startGroup();    inOrder.verify(readConsumerMock).startField("list", 0);    inOrder.verify(readConsumerMock).startGroup();    inOrder.verify(readConsumerMock).startField("element", 0);    inOrder.verify(readConsumerMock).addInteger(1323);    inOrder.verify(readConsumerMock).endField("element", 0);    inOrder.verify(readConsumerMock).endGroup();    inOrder.verify(readConsumerMock).startGroup();    inOrder.verify(readConsumerMock).startField("element", 0);    inOrder.verify(readConsumerMock).addInteger(54469);    inOrder.verify(readConsumerMock).endField("element", 0);    inOrder.verify(readConsumerMock).endGroup();    inOrder.verify(readConsumerMock).endField("list", 0);    inOrder.verify(readConsumerMock).endGroup();    inOrder.verify(readConsumerMock).endField("repeatedInt", 0);    inOrder.verify(readConsumerMock).endMessage();    Mockito.verifyNoMoreInteractions(readConsumerMock);}
0
public void testProto3RepeatedIntMessage() throws Exception
{    RecordConsumer readConsumerMock = Mockito.mock(RecordConsumer.class);    ProtoWriteSupport instance = createReadConsumerInstance(TestProto3.RepeatedIntMessage.class, readConsumerMock);    TestProto3.RepeatedIntMessage.Builder msg = TestProto3.RepeatedIntMessage.newBuilder();    msg.addRepeatedInt(1323);    msg.addRepeatedInt(54469);    instance.write(msg.build());    InOrder inOrder = Mockito.inOrder(readConsumerMock);    inOrder.verify(readConsumerMock).startMessage();    inOrder.verify(readConsumerMock).startField("repeatedInt", 0);    inOrder.verify(readConsumerMock).addInteger(1323);    inOrder.verify(readConsumerMock).addInteger(54469);    inOrder.verify(readConsumerMock).endField("repeatedInt", 0);    inOrder.verify(readConsumerMock).endMessage();    Mockito.verifyNoMoreInteractions(readConsumerMock);}
0
public void testProto3RepeatedIntMessageEmptySpecsCompliant() throws Exception
{    RecordConsumer readConsumerMock = Mockito.mock(RecordConsumer.class);    Configuration conf = new Configuration();    ProtoWriteSupport.setWriteSpecsCompliant(conf, true);    ProtoWriteSupport instance = createReadConsumerInstance(TestProtobuf.RepeatedIntMessage.class, readConsumerMock, conf);    TestProtobuf.RepeatedIntMessage.Builder msg = TestProtobuf.RepeatedIntMessage.newBuilder();    instance.write(msg.build());    InOrder inOrder = Mockito.inOrder(readConsumerMock);    inOrder.verify(readConsumerMock).startMessage();    inOrder.verify(readConsumerMock).endMessage();    Mockito.verifyNoMoreInteractions(readConsumerMock);}
0
public void testProto3RepeatedIntMessageEmpty() throws Exception
{    RecordConsumer readConsumerMock = Mockito.mock(RecordConsumer.class);    ProtoWriteSupport instance = createReadConsumerInstance(TestProtobuf.RepeatedIntMessage.class, readConsumerMock);    TestProtobuf.RepeatedIntMessage.Builder msg = TestProtobuf.RepeatedIntMessage.newBuilder();    instance.write(msg.build());    InOrder inOrder = Mockito.inOrder(readConsumerMock);    inOrder.verify(readConsumerMock).startMessage();    inOrder.verify(readConsumerMock).endMessage();    Mockito.verifyNoMoreInteractions(readConsumerMock);}
0
public void testMapIntMessageSpecsCompliant() throws Exception
{    RecordConsumer readConsumerMock = Mockito.mock(RecordConsumer.class);    Configuration conf = new Configuration();    ProtoWriteSupport.setWriteSpecsCompliant(conf, true);    ProtoWriteSupport instance = createReadConsumerInstance(TestProtobuf.MapIntMessage.class, readConsumerMock, conf);    TestProtobuf.MapIntMessage.Builder msg = TestProtobuf.MapIntMessage.newBuilder();    msg.putMapInt(123, 1);    msg.putMapInt(234, 2);    instance.write(msg.build());    InOrder inOrder = Mockito.inOrder(readConsumerMock);    inOrder.verify(readConsumerMock).startMessage();    inOrder.verify(readConsumerMock).startField("mapInt", 0);    inOrder.verify(readConsumerMock).startGroup();    inOrder.verify(readConsumerMock).startField("key_value", 0);    inOrder.verify(readConsumerMock).startGroup();    inOrder.verify(readConsumerMock).startField("key", 0);    inOrder.verify(readConsumerMock).addInteger(123);    inOrder.verify(readConsumerMock).endField("key", 0);    inOrder.verify(readConsumerMock).startField("value", 1);    inOrder.verify(readConsumerMock).addInteger(1);    inOrder.verify(readConsumerMock).endField("value", 1);    inOrder.verify(readConsumerMock).endGroup();    inOrder.verify(readConsumerMock).startGroup();    inOrder.verify(readConsumerMock).startField("key", 0);    inOrder.verify(readConsumerMock).addInteger(234);    inOrder.verify(readConsumerMock).endField("key", 0);    inOrder.verify(readConsumerMock).startField("value", 1);    inOrder.verify(readConsumerMock).addInteger(2);    inOrder.verify(readConsumerMock).endField("value", 1);    inOrder.verify(readConsumerMock).endGroup();    inOrder.verify(readConsumerMock).endField("key_value", 0);    inOrder.verify(readConsumerMock).endGroup();    inOrder.verify(readConsumerMock).endField("mapInt", 0);    inOrder.verify(readConsumerMock).endMessage();    Mockito.verifyNoMoreInteractions(readConsumerMock);}
0
public void testMapIntMessage() throws Exception
{    RecordConsumer readConsumerMock = Mockito.mock(RecordConsumer.class);    ProtoWriteSupport instance = createReadConsumerInstance(TestProtobuf.MapIntMessage.class, readConsumerMock);    TestProtobuf.MapIntMessage.Builder msg = TestProtobuf.MapIntMessage.newBuilder();    msg.putMapInt(123, 1);    msg.putMapInt(234, 2);    instance.write(msg.build());    InOrder inOrder = Mockito.inOrder(readConsumerMock);    inOrder.verify(readConsumerMock).startMessage();    inOrder.verify(readConsumerMock).startField("mapInt", 0);    inOrder.verify(readConsumerMock).startGroup();    inOrder.verify(readConsumerMock).startField("key", 0);    inOrder.verify(readConsumerMock).addInteger(123);    inOrder.verify(readConsumerMock).endField("key", 0);    inOrder.verify(readConsumerMock).startField("value", 1);    inOrder.verify(readConsumerMock).addInteger(1);    inOrder.verify(readConsumerMock).endField("value", 1);    inOrder.verify(readConsumerMock).endGroup();    inOrder.verify(readConsumerMock).startGroup();    inOrder.verify(readConsumerMock).startField("key", 0);    inOrder.verify(readConsumerMock).addInteger(234);    inOrder.verify(readConsumerMock).endField("key", 0);    inOrder.verify(readConsumerMock).startField("value", 1);    inOrder.verify(readConsumerMock).addInteger(2);    inOrder.verify(readConsumerMock).endField("value", 1);    inOrder.verify(readConsumerMock).endGroup();    inOrder.verify(readConsumerMock).endField("mapInt", 0);    inOrder.verify(readConsumerMock).endMessage();    Mockito.verifyNoMoreInteractions(readConsumerMock);}
0
public void testMapIntMessageEmptySpecsCompliant() throws Exception
{    RecordConsumer readConsumerMock = Mockito.mock(RecordConsumer.class);    Configuration conf = new Configuration();    ProtoWriteSupport.setWriteSpecsCompliant(conf, true);    ProtoWriteSupport instance = createReadConsumerInstance(TestProtobuf.MapIntMessage.class, readConsumerMock, conf);    TestProtobuf.MapIntMessage.Builder msg = TestProtobuf.MapIntMessage.newBuilder();    instance.write(msg.build());    InOrder inOrder = Mockito.inOrder(readConsumerMock);    inOrder.verify(readConsumerMock).startMessage();    inOrder.verify(readConsumerMock).endMessage();    Mockito.verifyNoMoreInteractions(readConsumerMock);}
0
public void testMapIntMessageEmpty() throws Exception
{    RecordConsumer readConsumerMock = Mockito.mock(RecordConsumer.class);    ProtoWriteSupport instance = createReadConsumerInstance(TestProtobuf.MapIntMessage.class, readConsumerMock);    TestProtobuf.MapIntMessage.Builder msg = TestProtobuf.MapIntMessage.newBuilder();    instance.write(msg.build());    InOrder inOrder = Mockito.inOrder(readConsumerMock);    inOrder.verify(readConsumerMock).startMessage();    inOrder.verify(readConsumerMock).endMessage();    Mockito.verifyNoMoreInteractions(readConsumerMock);}
0
public void testProto3MapIntMessageSpecsCompliant() throws Exception
{    RecordConsumer readConsumerMock = Mockito.mock(RecordConsumer.class);    Configuration conf = new Configuration();    ProtoWriteSupport.setWriteSpecsCompliant(conf, true);    ProtoWriteSupport instance = createReadConsumerInstance(TestProto3.MapIntMessage.class, readConsumerMock, conf);    TestProto3.MapIntMessage.Builder msg = TestProto3.MapIntMessage.newBuilder();    msg.putMapInt(123, 1);    msg.putMapInt(234, 2);    instance.write(msg.build());    InOrder inOrder = Mockito.inOrder(readConsumerMock);    inOrder.verify(readConsumerMock).startMessage();    inOrder.verify(readConsumerMock).startField("mapInt", 0);    inOrder.verify(readConsumerMock).startGroup();    inOrder.verify(readConsumerMock).startField("key_value", 0);    inOrder.verify(readConsumerMock).startGroup();    inOrder.verify(readConsumerMock).startField("key", 0);    inOrder.verify(readConsumerMock).addInteger(123);    inOrder.verify(readConsumerMock).endField("key", 0);    inOrder.verify(readConsumerMock).startField("value", 1);    inOrder.verify(readConsumerMock).addInteger(1);    inOrder.verify(readConsumerMock).endField("value", 1);    inOrder.verify(readConsumerMock).endGroup();    inOrder.verify(readConsumerMock).startGroup();    inOrder.verify(readConsumerMock).startField("key", 0);    inOrder.verify(readConsumerMock).addInteger(234);    inOrder.verify(readConsumerMock).endField("key", 0);    inOrder.verify(readConsumerMock).startField("value", 1);    inOrder.verify(readConsumerMock).addInteger(2);    inOrder.verify(readConsumerMock).endField("value", 1);    inOrder.verify(readConsumerMock).endGroup();    inOrder.verify(readConsumerMock).endField("key_value", 0);    inOrder.verify(readConsumerMock).endGroup();    inOrder.verify(readConsumerMock).endField("mapInt", 0);    inOrder.verify(readConsumerMock).endMessage();    Mockito.verifyNoMoreInteractions(readConsumerMock);}
0
public void testProto3MapIntMessage() throws Exception
{    RecordConsumer readConsumerMock = Mockito.mock(RecordConsumer.class);    ProtoWriteSupport instance = createReadConsumerInstance(TestProto3.MapIntMessage.class, readConsumerMock);    TestProto3.MapIntMessage.Builder msg = TestProto3.MapIntMessage.newBuilder();    msg.putMapInt(123, 1);    msg.putMapInt(234, 2);    instance.write(msg.build());    InOrder inOrder = Mockito.inOrder(readConsumerMock);    inOrder.verify(readConsumerMock).startMessage();    inOrder.verify(readConsumerMock).startField("mapInt", 0);    inOrder.verify(readConsumerMock).startGroup();    inOrder.verify(readConsumerMock).startField("key", 0);    inOrder.verify(readConsumerMock).addInteger(123);    inOrder.verify(readConsumerMock).endField("key", 0);    inOrder.verify(readConsumerMock).startField("value", 1);    inOrder.verify(readConsumerMock).addInteger(1);    inOrder.verify(readConsumerMock).endField("value", 1);    inOrder.verify(readConsumerMock).endGroup();    inOrder.verify(readConsumerMock).startGroup();    inOrder.verify(readConsumerMock).startField("key", 0);    inOrder.verify(readConsumerMock).addInteger(234);    inOrder.verify(readConsumerMock).endField("key", 0);    inOrder.verify(readConsumerMock).startField("value", 1);    inOrder.verify(readConsumerMock).addInteger(2);    inOrder.verify(readConsumerMock).endField("value", 1);    inOrder.verify(readConsumerMock).endGroup();    inOrder.verify(readConsumerMock).endField("mapInt", 0);    inOrder.verify(readConsumerMock).endMessage();    Mockito.verifyNoMoreInteractions(readConsumerMock);}
0
public void testProto3MapIntMessageEmptySpecsCompliant() throws Exception
{    RecordConsumer readConsumerMock = Mockito.mock(RecordConsumer.class);    Configuration conf = new Configuration();    ProtoWriteSupport.setWriteSpecsCompliant(conf, true);    ProtoWriteSupport instance = createReadConsumerInstance(TestProto3.MapIntMessage.class, readConsumerMock, conf);    TestProto3.MapIntMessage.Builder msg = TestProto3.MapIntMessage.newBuilder();    instance.write(msg.build());    InOrder inOrder = Mockito.inOrder(readConsumerMock);    inOrder.verify(readConsumerMock).startMessage();    inOrder.verify(readConsumerMock).endMessage();    Mockito.verifyNoMoreInteractions(readConsumerMock);}
0
public void testProto3MapIntMessageEmpty() throws Exception
{    RecordConsumer readConsumerMock = Mockito.mock(RecordConsumer.class);    ProtoWriteSupport instance = createReadConsumerInstance(TestProto3.MapIntMessage.class, readConsumerMock);    TestProto3.MapIntMessage.Builder msg = TestProto3.MapIntMessage.newBuilder();    instance.write(msg.build());    InOrder inOrder = Mockito.inOrder(readConsumerMock);    inOrder.verify(readConsumerMock).startMessage();    inOrder.verify(readConsumerMock).endMessage();    Mockito.verifyNoMoreInteractions(readConsumerMock);}
0
public void testRepeatedInnerMessageMessage_message() throws Exception
{    RecordConsumer readConsumerMock = Mockito.mock(RecordConsumer.class);    ProtoWriteSupport instance = createReadConsumerInstance(TestProtobuf.TopMessage.class, readConsumerMock);    TestProtobuf.TopMessage.Builder msg = TestProtobuf.TopMessage.newBuilder();    msg.addInnerBuilder().setOne("one").setTwo("two");    instance.write(msg.build());    InOrder inOrder = Mockito.inOrder(readConsumerMock);    inOrder.verify(readConsumerMock).startMessage();    inOrder.verify(readConsumerMock).startField("inner", 0);    inOrder.verify(readConsumerMock).startGroup();    inOrder.verify(readConsumerMock).startField("one", 0);    inOrder.verify(readConsumerMock).addBinary(Binary.fromConstantByteArray("one".getBytes()));    inOrder.verify(readConsumerMock).endField("one", 0);    inOrder.verify(readConsumerMock).startField("two", 1);    inOrder.verify(readConsumerMock).addBinary(Binary.fromConstantByteArray("two".getBytes()));    inOrder.verify(readConsumerMock).endField("two", 1);    inOrder.verify(readConsumerMock).endGroup();    inOrder.verify(readConsumerMock).endField("inner", 0);    inOrder.verify(readConsumerMock).endMessage();    Mockito.verifyNoMoreInteractions(readConsumerMock);}
0
public void testRepeatedInnerMessageSpecsCompliantMessage_message() throws Exception
{    RecordConsumer readConsumerMock = Mockito.mock(RecordConsumer.class);    Configuration conf = new Configuration();    ProtoWriteSupport.setWriteSpecsCompliant(conf, true);    ProtoWriteSupport instance = createReadConsumerInstance(TestProtobuf.TopMessage.class, readConsumerMock, conf);    TestProtobuf.TopMessage.Builder msg = TestProtobuf.TopMessage.newBuilder();    msg.addInnerBuilder().setOne("one").setTwo("two");    instance.write(msg.build());    InOrder inOrder = Mockito.inOrder(readConsumerMock);    inOrder.verify(readConsumerMock).startMessage();    inOrder.verify(readConsumerMock).startField("inner", 0);    inOrder.verify(readConsumerMock).startGroup();    inOrder.verify(readConsumerMock).startField("list", 0);    inOrder.verify(readConsumerMock).startGroup();    inOrder.verify(readConsumerMock).startField("element", 0);    inOrder.verify(readConsumerMock).startGroup();    inOrder.verify(readConsumerMock).startField("one", 0);    inOrder.verify(readConsumerMock).addBinary(Binary.fromConstantByteArray("one".getBytes()));    inOrder.verify(readConsumerMock).endField("one", 0);    inOrder.verify(readConsumerMock).startField("two", 1);    inOrder.verify(readConsumerMock).addBinary(Binary.fromConstantByteArray("two".getBytes()));    inOrder.verify(readConsumerMock).endField("two", 1);    inOrder.verify(readConsumerMock).endGroup();    inOrder.verify(readConsumerMock).endField("element", 0);    inOrder.verify(readConsumerMock).endGroup();    inOrder.verify(readConsumerMock).endField("list", 0);    inOrder.verify(readConsumerMock).endGroup();    inOrder.verify(readConsumerMock).endField("inner", 0);    inOrder.verify(readConsumerMock).endMessage();    Mockito.verifyNoMoreInteractions(readConsumerMock);}
0
public void testProto3RepeatedInnerMessageMessage_message() throws Exception
{    RecordConsumer readConsumerMock = Mockito.mock(RecordConsumer.class);    ;    ProtoWriteSupport instance = createReadConsumerInstance(TestProto3.TopMessage.class, readConsumerMock);    TestProto3.TopMessage.Builder msg = TestProto3.TopMessage.newBuilder();    msg.addInnerBuilder().setOne("one").setTwo("two");    instance.write(msg.build());    InOrder inOrder = Mockito.inOrder(readConsumerMock);    inOrder.verify(readConsumerMock).startMessage();    inOrder.verify(readConsumerMock).startField("inner", 0);    inOrder.verify(readConsumerMock).startGroup();    inOrder.verify(readConsumerMock).startField("one", 0);    inOrder.verify(readConsumerMock).addBinary(Binary.fromConstantByteArray("one".getBytes()));    inOrder.verify(readConsumerMock).endField("one", 0);    inOrder.verify(readConsumerMock).startField("two", 1);    inOrder.verify(readConsumerMock).addBinary(Binary.fromConstantByteArray("two".getBytes()));    inOrder.verify(readConsumerMock).endField("two", 1);    inOrder.verify(readConsumerMock).endGroup();    inOrder.verify(readConsumerMock).endField("inner", 0);    inOrder.verify(readConsumerMock).endMessage();    Mockito.verifyNoMoreInteractions(readConsumerMock);}
0
public void testProto3RepeatedInnerMessageSpecsCompliantMessage_message() throws Exception
{    RecordConsumer readConsumerMock = Mockito.mock(RecordConsumer.class);    Configuration conf = new Configuration();    ProtoWriteSupport.setWriteSpecsCompliant(conf, true);    ProtoWriteSupport instance = createReadConsumerInstance(TestProto3.TopMessage.class, readConsumerMock, conf);    TestProto3.TopMessage.Builder msg = TestProto3.TopMessage.newBuilder();    msg.addInnerBuilder().setOne("one").setTwo("two");    instance.write(msg.build());    InOrder inOrder = Mockito.inOrder(readConsumerMock);    inOrder.verify(readConsumerMock).startMessage();    inOrder.verify(readConsumerMock).startField("inner", 0);    inOrder.verify(readConsumerMock).startGroup();    inOrder.verify(readConsumerMock).startField("list", 0);    inOrder.verify(readConsumerMock).startGroup();    inOrder.verify(readConsumerMock).startField("element", 0);    inOrder.verify(readConsumerMock).startGroup();    inOrder.verify(readConsumerMock).startField("one", 0);    inOrder.verify(readConsumerMock).addBinary(Binary.fromConstantByteArray("one".getBytes()));    inOrder.verify(readConsumerMock).endField("one", 0);    inOrder.verify(readConsumerMock).startField("two", 1);    inOrder.verify(readConsumerMock).addBinary(Binary.fromConstantByteArray("two".getBytes()));    inOrder.verify(readConsumerMock).endField("two", 1);    inOrder.verify(readConsumerMock).endGroup();    inOrder.verify(readConsumerMock).endField("element", 0);    inOrder.verify(readConsumerMock).endGroup();    inOrder.verify(readConsumerMock).endField("list", 0);    inOrder.verify(readConsumerMock).endGroup();    inOrder.verify(readConsumerMock).endField("inner", 0);    inOrder.verify(readConsumerMock).endMessage();    Mockito.verifyNoMoreInteractions(readConsumerMock);}
0
public void testRepeatedInnerMessageSpecsCompliantMessage_scalar() throws Exception
{    RecordConsumer readConsumerMock = Mockito.mock(RecordConsumer.class);    Configuration conf = new Configuration();    ProtoWriteSupport.setWriteSpecsCompliant(conf, true);    ProtoWriteSupport instance = createReadConsumerInstance(TestProtobuf.TopMessage.class, readConsumerMock, conf);    TestProtobuf.TopMessage.Builder msg = TestProtobuf.TopMessage.newBuilder();    msg.addInnerBuilder().setOne("one");    msg.addInnerBuilder().setTwo("two");    instance.write(msg.build());    InOrder inOrder = Mockito.inOrder(readConsumerMock);    inOrder.verify(readConsumerMock).startMessage();    inOrder.verify(readConsumerMock).startField("inner", 0);    inOrder.verify(readConsumerMock).startGroup();    inOrder.verify(readConsumerMock).startField("list", 0);        inOrder.verify(readConsumerMock).startGroup();    inOrder.verify(readConsumerMock).startField("element", 0);    inOrder.verify(readConsumerMock).startGroup();    inOrder.verify(readConsumerMock).startField("one", 0);    inOrder.verify(readConsumerMock).addBinary(Binary.fromConstantByteArray("one".getBytes()));    inOrder.verify(readConsumerMock).endField("one", 0);    inOrder.verify(readConsumerMock).endGroup();    inOrder.verify(readConsumerMock).endField("element", 0);    inOrder.verify(readConsumerMock).endGroup();        inOrder.verify(readConsumerMock).startGroup();    inOrder.verify(readConsumerMock).startField("element", 0);    inOrder.verify(readConsumerMock).startGroup();    inOrder.verify(readConsumerMock).startField("two", 1);    inOrder.verify(readConsumerMock).addBinary(Binary.fromConstantByteArray("two".getBytes()));    inOrder.verify(readConsumerMock).endField("two", 1);    inOrder.verify(readConsumerMock).endGroup();    inOrder.verify(readConsumerMock).endField("element", 0);    inOrder.verify(readConsumerMock).endGroup();    inOrder.verify(readConsumerMock).endField("list", 0);    inOrder.verify(readConsumerMock).endGroup();    inOrder.verify(readConsumerMock).endField("inner", 0);    inOrder.verify(readConsumerMock).endMessage();    Mockito.verifyNoMoreInteractions(readConsumerMock);}
0
public void testRepeatedInnerMessageMessage_scalar() throws Exception
{    RecordConsumer readConsumerMock = Mockito.mock(RecordConsumer.class);    ProtoWriteSupport instance = createReadConsumerInstance(TestProtobuf.TopMessage.class, readConsumerMock);    TestProtobuf.TopMessage.Builder msg = TestProtobuf.TopMessage.newBuilder();    msg.addInnerBuilder().setOne("one");    msg.addInnerBuilder().setTwo("two");    instance.write(msg.build());    InOrder inOrder = Mockito.inOrder(readConsumerMock);    inOrder.verify(readConsumerMock).startMessage();    inOrder.verify(readConsumerMock).startField("inner", 0);        inOrder.verify(readConsumerMock).startGroup();    inOrder.verify(readConsumerMock).startField("one", 0);    inOrder.verify(readConsumerMock).addBinary(Binary.fromConstantByteArray("one".getBytes()));    inOrder.verify(readConsumerMock).endField("one", 0);    inOrder.verify(readConsumerMock).endGroup();        inOrder.verify(readConsumerMock).startGroup();    inOrder.verify(readConsumerMock).startField("two", 1);    inOrder.verify(readConsumerMock).addBinary(Binary.fromConstantByteArray("two".getBytes()));    inOrder.verify(readConsumerMock).endField("two", 1);    inOrder.verify(readConsumerMock).endGroup();    inOrder.verify(readConsumerMock).endField("inner", 0);    inOrder.verify(readConsumerMock).endMessage();    Mockito.verifyNoMoreInteractions(readConsumerMock);}
0
public void testProto3RepeatedInnerMessageMessage_scalar() throws Exception
{    RecordConsumer readConsumerMock = Mockito.mock(RecordConsumer.class);    ProtoWriteSupport instance = createReadConsumerInstance(TestProto3.TopMessage.class, readConsumerMock);    TestProto3.TopMessage.Builder msg = TestProto3.TopMessage.newBuilder();    msg.addInnerBuilder().setOne("one");    msg.addInnerBuilder().setTwo("two");    instance.write(msg.build());    InOrder inOrder = Mockito.inOrder(readConsumerMock);    inOrder.verify(readConsumerMock).startMessage();    inOrder.verify(readConsumerMock).startField("inner", 0);        inOrder.verify(readConsumerMock).startGroup();    inOrder.verify(readConsumerMock).startField("one", 0);    inOrder.verify(readConsumerMock).addBinary(Binary.fromConstantByteArray("one".getBytes()));    inOrder.verify(readConsumerMock).endField("one", 0);    inOrder.verify(readConsumerMock).endGroup();        inOrder.verify(readConsumerMock).startGroup();    inOrder.verify(readConsumerMock).startField("two", 1);    inOrder.verify(readConsumerMock).addBinary(Binary.fromConstantByteArray("two".getBytes()));    inOrder.verify(readConsumerMock).endField("two", 1);    inOrder.verify(readConsumerMock).endGroup();    inOrder.verify(readConsumerMock).endField("inner", 0);    inOrder.verify(readConsumerMock).endMessage();    Mockito.verifyNoMoreInteractions(readConsumerMock);}
0
public void testProto3RepeatedInnerMessageSpecsCompliantMessage_scalar() throws Exception
{    RecordConsumer readConsumerMock = Mockito.mock(RecordConsumer.class);    Configuration conf = new Configuration();    ProtoWriteSupport.setWriteSpecsCompliant(conf, true);    ProtoWriteSupport instance = createReadConsumerInstance(TestProto3.TopMessage.class, readConsumerMock, conf);    TestProto3.TopMessage.Builder msg = TestProto3.TopMessage.newBuilder();    msg.addInnerBuilder().setOne("one");    msg.addInnerBuilder().setTwo("two");    instance.write(msg.build());    InOrder inOrder = Mockito.inOrder(readConsumerMock);    inOrder.verify(readConsumerMock).startMessage();    inOrder.verify(readConsumerMock).startField("inner", 0);    inOrder.verify(readConsumerMock).startGroup();    inOrder.verify(readConsumerMock).startField("list", 0);        inOrder.verify(readConsumerMock).startGroup();    inOrder.verify(readConsumerMock).startField("element", 0);    inOrder.verify(readConsumerMock).startGroup();    inOrder.verify(readConsumerMock).startField("one", 0);    inOrder.verify(readConsumerMock).addBinary(Binary.fromConstantByteArray("one".getBytes()));    inOrder.verify(readConsumerMock).endField("one", 0);    inOrder.verify(readConsumerMock).endGroup();    inOrder.verify(readConsumerMock).endField("element", 0);    inOrder.verify(readConsumerMock).endGroup();        inOrder.verify(readConsumerMock).startGroup();    inOrder.verify(readConsumerMock).startField("element", 0);    inOrder.verify(readConsumerMock).startGroup();    inOrder.verify(readConsumerMock).startField("two", 1);    inOrder.verify(readConsumerMock).addBinary(Binary.fromConstantByteArray("two".getBytes()));    inOrder.verify(readConsumerMock).endField("two", 1);    inOrder.verify(readConsumerMock).endGroup();    inOrder.verify(readConsumerMock).endField("element", 0);    inOrder.verify(readConsumerMock).endGroup();    inOrder.verify(readConsumerMock).endField("list", 0);    inOrder.verify(readConsumerMock).endGroup();    inOrder.verify(readConsumerMock).endField("inner", 0);    inOrder.verify(readConsumerMock).endMessage();    Mockito.verifyNoMoreInteractions(readConsumerMock);}
0
public void testOptionalInnerMessage() throws Exception
{    RecordConsumer readConsumerMock = Mockito.mock(RecordConsumer.class);    ProtoWriteSupport instance = createReadConsumerInstance(TestProtobuf.MessageA.class, readConsumerMock);    TestProtobuf.MessageA.Builder msg = TestProtobuf.MessageA.newBuilder();    msg.getInnerBuilder().setOne("one");    instance.write(msg.build());    InOrder inOrder = Mockito.inOrder(readConsumerMock);    inOrder.verify(readConsumerMock).startMessage();    inOrder.verify(readConsumerMock).startField("inner", 0);    inOrder.verify(readConsumerMock).startGroup();    inOrder.verify(readConsumerMock).startField("one", 0);    inOrder.verify(readConsumerMock).addBinary(Binary.fromConstantByteArray("one".getBytes()));    inOrder.verify(readConsumerMock).endField("one", 0);    inOrder.verify(readConsumerMock).endGroup();    inOrder.verify(readConsumerMock).endField("inner", 0);    inOrder.verify(readConsumerMock).endMessage();    Mockito.verifyNoMoreInteractions(readConsumerMock);}
0
public void testProto3OptionalInnerMessage() throws Exception
{    RecordConsumer readConsumerMock = Mockito.mock(RecordConsumer.class);    ProtoWriteSupport instance = createReadConsumerInstance(TestProto3.MessageA.class, readConsumerMock);    TestProto3.MessageA.Builder msg = TestProto3.MessageA.newBuilder();    msg.getInnerBuilder().setOne("one");    instance.write(msg.build());    InOrder inOrder = Mockito.inOrder(readConsumerMock);    inOrder.verify(readConsumerMock).startMessage();    inOrder.verify(readConsumerMock).startField("inner", 0);    inOrder.verify(readConsumerMock).startGroup();    inOrder.verify(readConsumerMock).startField("one", 0);    inOrder.verify(readConsumerMock).addBinary(Binary.fromConstantByteArray("one".getBytes()));    inOrder.verify(readConsumerMock).endField("one", 0);    inOrder.verify(readConsumerMock).endGroup();    inOrder.verify(readConsumerMock).endField("inner", 0);    inOrder.verify(readConsumerMock).endMessage();    Mockito.verifyNoMoreInteractions(readConsumerMock);}
0
public void testMessageWithExtensions() throws Exception
{    RecordConsumer readConsumerMock = Mockito.mock(RecordConsumer.class);    ProtoWriteSupport instance = createReadConsumerInstance(TestProtobuf.Vehicle.class, readConsumerMock);    TestProtobuf.Vehicle.Builder msg = TestProtobuf.Vehicle.newBuilder();    msg.setHorsePower(300);            msg.setExtension(TestProtobuf.Airplane.wingSpan, 50);    instance.write(msg.build());}
0
public static Path someTemporaryFilePath() throws IOException
{    File tmp = File.createTempFile("ParquetProtobuf_unitTest", ".tmp");    tmp.deleteOnExit();    tmp.delete();    return new Path(tmp.getPath());}
0
public static List<T> writeAndRead(T... records) throws IOException
{    Class<? extends Message> cls = inferRecordsClass(records);    Path file = writeMessages(cls, records);    return readMessages(file);}
0
public static Class<? extends Message> inferRecordsClass(MessageOrBuilder[] records)
{    Class<? extends Message> cls = null;    for (MessageOrBuilder record : records) {        Class<? extends Message> recordClass;        if (record instanceof Message.Builder) {            recordClass = ((Message.Builder) record).build().getClass();        } else if (record instanceof Message) {            recordClass = ((Message) record).getClass();        } else {            throw new RuntimeException("Illegal class " + record);        }        if (cls == null) {            cls = recordClass;        } else if (!cls.equals(recordClass)) {            throw new RuntimeException("Class mismatch :" + cls + " and " + recordClass);        }    }    return cls;}
0
public static List<T> testData(T... messages) throws IOException
{    checkSameBuilderInstance(messages);    List<MessageOrBuilder> input = cloneList(messages);    List<MessageOrBuilder> output = (List<MessageOrBuilder>) writeAndRead(messages);    List<Message> outputAsMessages = asMessages(output);    assertEquals("The protocol buffers are not same:\n", asMessages(input), outputAsMessages);    return (List<T>) outputAsMessages;}
0
private static List<MessageOrBuilder> cloneList(MessageOrBuilder[] messages)
{    List<MessageOrBuilder> result = new ArrayList<MessageOrBuilder>();    for (MessageOrBuilder mob : messages) {        result.add(asMessage(mob));    }    return result;}
0
public static List<Message> asMessages(List<MessageOrBuilder> mobs)
{    List<Message> result = new ArrayList<Message>();    for (MessageOrBuilder messageOrBuilder : mobs) {        result.add(asMessage(messageOrBuilder));    }    return result;}
0
public static Message asMessage(MessageOrBuilder mob)
{    Message message;    if (mob instanceof Message.Builder) {        message = ((Message.Builder) mob).build();    } else {        message = (Message) mob;    }    return message;}
0
private static void checkSameBuilderInstance(MessageOrBuilder[] messages)
{    for (int i = 0; i < messages.length; i++) {        MessageOrBuilder firstMessage = messages[i];        boolean isBuilder = firstMessage instanceof Message.Builder;        if (isBuilder) {            for (int j = 0; j < messages.length; j++) {                MessageOrBuilder secondMessage = messages[j];                if (i != j) {                    boolean isSame = secondMessage == firstMessage;                    if (isSame) {                        fail("Data contains two references to same instance." + secondMessage);                    }                }            }        }    }}
0
public static List<T> readMessages(Path file) throws IOException
{    ProtoParquetReader<T> reader = new ProtoParquetReader<T>(file);    List<T> result = new ArrayList<T>();    boolean hasNext = true;    while (hasNext) {        T item = reader.read();        if (item == null) {            hasNext = false;        } else {            assertNotNull(item);                        result.add((T) asMessage(item).toBuilder());        }    }    reader.close();    return result;}
0
public static Path writeMessages(MessageOrBuilder... records) throws IOException
{    return writeMessages(inferRecordsClass(records), records);}
0
public static Path writeMessages(Class<? extends Message> cls, MessageOrBuilder... records) throws IOException
{    Path file = someTemporaryFilePath();    ProtoParquetWriter<MessageOrBuilder> writer = new ProtoParquetWriter<MessageOrBuilder>(file, cls);    for (MessageOrBuilder record : records) {        writer.write(record);    }    writer.close();    return file;}
0
public void setRequestedProjection(String projection)
{    this.projection = projection;}
0
public Configuration getConfiguration()
{    return conf;}
0
protected void map(Void key, MessageOrBuilder value, Context context)
{    Message clone = ((Message.Builder) value).build();    outputMessages.add(clone);}
0
public List<Message> read(Path parquetPath) throws Exception
{    synchronized (ReadUsingMR.class) {        outputMessages = new ArrayList<Message>();        final Job job = new Job(conf, "read");        job.setInputFormatClass(ProtoParquetInputFormat.class);        ProtoParquetInputFormat.setInputPaths(job, parquetPath);        if (projection != null) {            ProtoParquetInputFormat.setRequestedProjection(job, projection);        }        job.setMapperClass(ReadingMapper.class);        job.setNumReduceTasks(0);        job.setOutputFormatClass(NullOutputFormat.class);        WriteUsingMR.waitForJob(job);        List<Message> result = Collections.unmodifiableList(outputMessages);        outputMessages = null;        return result;    }}
0
public Configuration getConfiguration()
{    return conf;}
0
public void run(Context context) throws IOException, InterruptedException
{    if (inputMessages == null || inputMessages.size() == 0) {        throw new RuntimeException("No mock data given");    } else {        for (Message msg : inputMessages) {            context.write(null, msg);                    }    }}
1
public Path write(Message... messages) throws Exception
{    synchronized (WriteUsingMR.class) {        outputPath = TestUtils.someTemporaryFilePath();        Path inputPath = TestUtils.someTemporaryFilePath();        FileSystem fileSystem = inputPath.getFileSystem(conf);        fileSystem.create(inputPath);        inputMessages = Collections.unmodifiableList(Arrays.asList(messages));        final Job job = new Job(conf, "write");                TextInputFormat.addInputPath(job, inputPath);        job.setInputFormatClass(TextInputFormat.class);        job.setMapperClass(WritingMapper.class);        job.setNumReduceTasks(0);        job.setOutputFormatClass(ProtoParquetOutputFormat.class);        ProtoParquetOutputFormat.setOutputPath(job, outputPath);        ProtoParquetOutputFormat.setProtobufClass(job, TestUtils.inferRecordsClass(messages));        waitForJob(job);        inputMessages = null;        return outputPath;    }}
0
 static void waitForJob(Job job) throws Exception
{    job.submit();    while (!job.isComplete()) {                sleep(50);    }        if (!job.isSuccessful()) {        throw new RuntimeException("job failed " + job.getJobName());    }}
1
public static void setScroogeClass(Configuration configuration, Class<? extends ThriftStruct> thriftClass)
{    ScroogeWriteSupport.setScroogeClass(configuration, thriftClass);}
0
public static Class<? extends ThriftStruct> getScroogeClass(Configuration configuration)
{    return ScroogeWriteSupport.getScroogeClass(configuration);}
0
public void sinkConfInit(FlowProcess<JobConf> fp, Tap<JobConf, RecordReader, OutputCollector> tap, JobConf jobConf)
{    DeprecatedParquetOutputFormat.setAsOutputFormat(jobConf);    ParquetOutputFormat.setWriteSupportClass(jobConf, ScroogeWriteSupport.class);    ScroogeWriteSupport.setScroogeClass(jobConf, this.config.getKlass());}
0
public void sourceConfInit(FlowProcess<JobConf> fp, Tap<JobConf, RecordReader, OutputCollector> tap, JobConf jobConf)
{    super.sourceConfInit(fp, tap, jobConf);    jobConf.setInputFormat(DeprecatedParquetInputFormat.class);    ParquetInputFormat.setReadSupportClass(jobConf, ScroogeReadSupport.class);    ThriftReadSupport.setRecordConverterClass(jobConf, ScroogeRecordConverter.class);}
0
protected MessageType getProjectedSchema(FieldProjectionFilter fieldProjectionFilter)
{    ThriftType.StructType thriftStruct = new ScroogeStructConverter().convert(thriftClass);    return new ThriftSchemaConverter(fieldProjectionFilter).convert(thriftStruct);}
0
public T readOneRecord(TProtocol protocol) throws TException
{    return codec.decode(protocol);}
0
private static ThriftStructCodec<?> getCodec(Class<?> klass)
{    Class<?> companionClass;    try {        companionClass = Class.forName(klass.getName() + "$");        Object companionObject = companionClass.getField("MODULE$").get(null);        return (ThriftStructCodec<?>) companionObject;    } catch (Exception t) {        if (t instanceof InterruptedException)            Thread.currentThread().interrupt();        throw new RuntimeException("Unable to create ThriftStructCodec", t);    }}
0
public ThriftType.StructType convert(Class scroogeClass)
{    return convertStructFromClass(scroogeClass);}
0
private static String mapKeyName(String fieldName)
{    return fieldName + "_map_key";}
0
private static String mapValueName(String fieldName)
{    return fieldName + "_map_value";}
0
private static String listElemName(String fieldName)
{    return fieldName + "_list_elem";}
0
private static String setElemName(String fieldName)
{    return fieldName + "_set_elem";}
0
private Class getCompanionClass(Class klass)
{    try {        return Class.forName(klass.getName() + "$");    } catch (ClassNotFoundException e) {        throw new ScroogeSchemaConversionException("Can not find companion object for scrooge class " + klass, e);    }}
0
private ThriftType.StructType convertStructFromClass(Class klass)
{    return convertCompanionClassToStruct(getCompanionClass(klass));}
0
private ThriftType.StructType convertCompanionClassToStruct(Class<?> companionClass)
{    ThriftStructCodec<?> companionObject;    try {        companionObject = (ThriftStructCodec<?>) companionClass.getField("MODULE$").get(null);    } catch (NoSuchFieldException e) {        throw new ScroogeSchemaConversionException("Can not get ThriftStructCodec from companion object of " + companionClass.getName(), e);    } catch (IllegalAccessException e) {        throw new ScroogeSchemaConversionException("Can not get ThriftStructCodec from companion object of " + companionClass.getName(), e);    }        List<ThriftField> children = new LinkedList<ThriftField>();    Iterable<ThriftStructFieldInfo> scroogeFields = getFieldInfos(companionObject);    for (ThriftStructFieldInfo field : scroogeFields) {        children.add(toThriftField(field));    }    StructOrUnionType structOrUnionType = isUnion(companionObject.getClass()) ? StructOrUnionType.UNION : StructOrUnionType.STRUCT;    return new ThriftType.StructType(children, structOrUnionType);}
0
private Iterable<ThriftStructFieldInfo> getFieldInfos(ThriftStructCodec<?> c)
{    Class<? extends ThriftStructCodec> klass = c.getClass();    if (isUnion(klass)) {                return getFieldInfosForUnion(klass);    } else {                try {            Object r = klass.getMethod("fieldInfos").invoke(c);            return JavaConversions$.MODULE$.asJavaIterable((scala.collection.Iterable<ThriftStructFieldInfo>) r);        } catch (ClassCastException e) {            throw new ScroogeSchemaConversionException("can not get field Info from: " + c.toString(), e);        } catch (InvocationTargetException e) {            throw new ScroogeSchemaConversionException("can not get field Info from: " + c.toString(), e);        } catch (NoSuchMethodException e) {            throw new ScroogeSchemaConversionException("can not get field Info from: " + c.toString(), e);        } catch (IllegalAccessException e) {            throw new ScroogeSchemaConversionException("can not get field Info from: " + c.toString(), e);        }    }}
0
private Iterable<ThriftStructFieldInfo> getFieldInfosForUnion(Class klass)
{    ArrayList<ThriftStructFieldInfo> fields = new ArrayList<ThriftStructFieldInfo>();    for (Field f : klass.getDeclaredFields()) {        if (f.getType().equals(Manifest.class)) {            Class unionClass = (Class) ((ParameterizedType) f.getGenericType()).getActualTypeArguments()[0];            Class companionUnionClass = getCompanionClass(unionClass);            try {                Object companionUnionObj = companionUnionClass.getField("MODULE$").get(null);                ThriftStructFieldInfo info = (ThriftStructFieldInfo) companionUnionClass.getMethod("fieldInfo").invoke(companionUnionObj);                fields.add(info);            } catch (NoSuchFieldException e) {                throw new ScroogeSchemaConversionException("can not find fieldInfo for " + unionClass, e);            } catch (InvocationTargetException e) {                throw new ScroogeSchemaConversionException("can not find fieldInfo for " + unionClass, e);            } catch (NoSuchMethodException e) {                throw new ScroogeSchemaConversionException("can not find fieldInfo for " + unionClass, e);            } catch (IllegalAccessException e) {                throw new ScroogeSchemaConversionException("can not find fieldInfo for " + unionClass, e);            }        }    }    return fields;}
0
public ThriftField toThriftField(ThriftStructFieldInfo scroogeField)
{    Requirement requirement = getRequirementType(scroogeField);    String fieldName = scroogeField.tfield().name;    short fieldId = scroogeField.tfield().id;    byte thriftTypeByte = scroogeField.tfield().type;    ThriftTypeID typeId = ThriftTypeID.fromByte(thriftTypeByte);    ThriftType thriftType;    switch(typeId) {        case BOOL:            thriftType = new ThriftType.BoolType();            break;        case BYTE:            thriftType = new ThriftType.ByteType();            break;        case DOUBLE:            thriftType = new ThriftType.DoubleType();            break;        case I16:            thriftType = new ThriftType.I16Type();            break;        case I32:            thriftType = new ThriftType.I32Type();            break;        case I64:            thriftType = new ThriftType.I64Type();            break;        case STRING:            ThriftType.StringType stringType = new ThriftType.StringType();                        if (!String.class.equals(scroogeField.manifest().runtimeClass())) {                stringType.setBinary(true);            }            thriftType = stringType;            break;        case STRUCT:            thriftType = convertStructTypeField(scroogeField);            break;        case MAP:            thriftType = convertMapTypeField(scroogeField, requirement);            break;        case SET:            thriftType = convertSetTypeField(scroogeField, requirement);            break;        case LIST:            thriftType = convertListTypeField(scroogeField, requirement);            break;        case ENUM:            thriftType = convertEnumTypeField(scroogeField);            break;        case STOP:        case VOID:        default:            throw new IllegalArgumentException("can't convert type " + typeId);    }    return new ThriftField(fieldName, fieldId, requirement, thriftType);}
0
private ThriftType convertSetTypeField(ThriftStructFieldInfo f, Requirement requirement)
{    return convertSetTypeField(f.tfield().name, f.valueManifest().get(), requirement);}
0
private ThriftType convertSetTypeField(String fieldName, Manifest<?> valueManifest, Requirement requirement)
{    String elemName = setElemName(fieldName);    ThriftType elementType = convertClassToThriftType(elemName, requirement, valueManifest);            ThriftField elementField = generateFieldWithoutId(elemName, requirement, elementType);    return new ThriftType.SetType(elementField);}
0
private ThriftType convertListTypeField(ThriftStructFieldInfo f, Requirement requirement)
{    return convertListTypeField(f.tfield().name, f.valueManifest().get(), requirement);}
0
private ThriftType convertListTypeField(String fieldName, Manifest<?> valueManifest, Requirement requirement)
{    String elemName = listElemName(fieldName);    ThriftType elementType = convertClassToThriftType(elemName, requirement, valueManifest);    ThriftField elementField = generateFieldWithoutId(elemName, requirement, elementType);    return new ThriftType.ListType(elementField);}
0
private ThriftType convertMapTypeField(ThriftStructFieldInfo f, Requirement requirement)
{    return convertMapTypeField(f.tfield().name, f.keyManifest().get(), f.valueManifest().get(), requirement);}
0
private ThriftType convertMapTypeField(String fieldName, Manifest<?> keyManifest, Manifest<?> valueManifest, Requirement requirement)
{    String keyName = mapKeyName(fieldName);    String valueName = mapValueName(fieldName);    ThriftType keyType = convertClassToThriftType(keyName, requirement, keyManifest);    ThriftField keyField = generateFieldWithoutId(keyName, requirement, keyType);    ThriftType valueType = convertClassToThriftType(valueName, requirement, valueManifest);    ThriftField valueField = generateFieldWithoutId(valueName, requirement, valueType);    return new ThriftType.MapType(keyField, valueField);}
0
private ThriftField generateFieldWithoutId(String fieldName, Requirement requirement, ThriftType thriftType)
{    return new ThriftField(fieldName, (short) 1, requirement, thriftType);}
0
private ThriftType convertClassToThriftType(String name, Requirement requirement, Manifest<?> typeManifest)
{    Class typeClass = typeManifest.runtimeClass();    if (typeManifest.runtimeClass() == boolean.class) {        return new ThriftType.BoolType();    } else if (typeClass == byte.class) {        return new ThriftType.ByteType();    } else if (typeClass == double.class) {        return new ThriftType.DoubleType();    } else if (typeClass == short.class) {        return new ThriftType.I16Type();    } else if (typeClass == int.class) {        return new ThriftType.I32Type();    } else if (typeClass == long.class) {        return new ThriftType.I64Type();    } else if (typeClass == String.class) {        return new ThriftType.StringType();    } else if (typeClass == ByteBuffer.class) {        return new ThriftType.StringType();    } else if (typeClass == scala.collection.Seq.class) {        Manifest<?> a = typeManifest.typeArguments().apply(0);        return convertListTypeField(name, a, requirement);    } else if (typeClass == scala.collection.Set.class) {        Manifest<?> setElementManifest = typeManifest.typeArguments().apply(0);        return convertSetTypeField(name, setElementManifest, requirement);    } else if (typeClass == scala.collection.Map.class) {        List<Manifest<?>> ms = JavaConversions.seqAsJavaList(typeManifest.typeArguments());        Manifest keyManifest = ms.get(0);        Manifest valueManifest = ms.get(1);        return convertMapTypeField(name, keyManifest, valueManifest, requirement);    } else if (com.twitter.scrooge.ThriftEnum.class.isAssignableFrom(typeClass)) {        return convertEnumTypeField(typeClass, name);    } else {        return convertStructFromClass(typeClass);    }}
0
private ThriftType convertStructTypeField(ThriftStructFieldInfo f)
{    return convertStructFromClass(f.manifest().runtimeClass());}
0
private List getEnumList(String enumName) throws ClassNotFoundException, IllegalAccessException, NoSuchFieldException, NoSuchMethodException, InvocationTargetException
{        enumName += "$";    Class companionObjectClass = Class.forName(enumName);    Object cObject = companionObjectClass.getField("MODULE$").get(null);    Method listMethod = companionObjectClass.getMethod("list", new Class[] {});    Object result = listMethod.invoke(cObject, null);    return JavaConversions.seqAsJavaList((Seq) result);}
0
public ThriftType convertEnumTypeField(ThriftStructFieldInfo f)
{    return convertEnumTypeField(f.manifest().runtimeClass(), f.tfield().name);}
0
private ThriftType convertEnumTypeField(Class enumClass, String fieldName)
{    List<ThriftType.EnumValue> enumValues = new ArrayList<ThriftType.EnumValue>();    String enumName = enumClass.getName();    try {        List enumCollection = getEnumList(enumName);        for (Object enumObj : enumCollection) {            ScroogeEnumDesc enumDesc = ScroogeEnumDesc.fromEnum(enumObj);            enumValues.add(new ThriftType.EnumValue(enumDesc.id, enumDesc.originalName));        }        return new ThriftType.EnumType(enumValues);    } catch (RuntimeException e) {        throw new ScroogeSchemaConversionException("Can not convert enum field " + fieldName, e);    } catch (NoSuchMethodException e) {        throw new ScroogeSchemaConversionException("Can not convert enum field " + fieldName, e);    } catch (IllegalAccessException e) {        throw new ScroogeSchemaConversionException("Can not convert enum field " + fieldName, e);    } catch (NoSuchFieldException e) {        throw new ScroogeSchemaConversionException("Can not convert enum field " + fieldName, e);    } catch (InvocationTargetException e) {        throw new ScroogeSchemaConversionException("Can not convert enum field " + fieldName, e);    } catch (ClassNotFoundException e) {        throw new ScroogeSchemaConversionException("Can not convert enum field " + fieldName, e);    }}
0
private boolean isUnion(Class klass)
{    for (Field f : klass.getDeclaredFields()) {        if (f.getName().equals("Union"))            return true;    }    return false;}
0
private Requirement getRequirementType(ThriftStructFieldInfo f)
{    if (f.isOptional() && !f.isRequired()) {        return OPTIONAL;    } else if (f.isRequired() && !f.isOptional()) {        return REQUIRED;    } else if (!f.isOptional() && !f.isRequired()) {        return DEFAULT;    } else {        throw new ScroogeSchemaConversionException("can not determine requirement type for : " + f.toString() + ", isOptional=" + f.isOptional() + ", isRequired=" + f.isRequired());    }}
0
public static ScroogeEnumDesc fromEnum(Object rawScroogeEnum) throws NoSuchMethodException, InvocationTargetException, IllegalAccessException
{    Class enumClass = rawScroogeEnum.getClass();    Method valueMethod = enumClass.getMethod("value", new Class[] {});    Method originalNameMethod = enumClass.getMethod("originalName", new Class[] {});    ScroogeEnumDesc result = new ScroogeEnumDesc();    result.id = (Integer) valueMethod.invoke(rawScroogeEnum, null);    result.originalName = (String) originalNameMethod.invoke(rawScroogeEnum, null);    return result;}
0
public static void setScroogeClass(Configuration configuration, Class<? extends ThriftStruct> thriftClass)
{    AbstractThriftWriteSupport.setGenericThriftClass(configuration, thriftClass);}
0
public static Class<? extends ThriftStruct> getScroogeClass(Configuration configuration)
{    return (Class<? extends ThriftStruct>) AbstractThriftWriteSupport.getGenericThriftClass(configuration);}
0
public String getName()
{    return "scrooge";}
0
protected StructType getThriftStruct()
{    ScroogeStructConverter schemaConverter = new ScroogeStructConverter();    return schemaConverter.convert(thriftClass);}
0
public void write(T record)
{    try {        record.write(parquetWriteProtocol);    } catch (TException e) {        throw new ParquetEncodingException(e);    }}
0
public void testWritePrimitveThriftReadScrooge() throws Exception
{    RequiredPrimitiveFixture toWrite = new RequiredPrimitiveFixture(true, (byte) 2, (short) 3, 4, (long) 5, 6.0, "7");    toWrite.setInfo_string("it's info");    verifyScroogeRead(thriftRecords(toWrite), org.apache.parquet.scrooge.test.RequiredPrimitiveFixture.class, "RequiredPrimitiveFixture(true,2,3,4,5,6.0,7,Some(it's info))\n", "**");}
0
public void testNestedReadingInScrooge() throws Exception
{    Map<String, org.apache.parquet.thrift.test.Phone> phoneMap = new HashMap<String, Phone>();    phoneMap.put("key1", new org.apache.parquet.thrift.test.Phone("111", "222"));    org.apache.parquet.thrift.test.TestPersonWithAllInformation toWrite = new org.apache.parquet.thrift.test.TestPersonWithAllInformation(new org.apache.parquet.thrift.test.Name("first"), new Address("my_street", "my_zip"), phoneMap);    toWrite.setInfo("my_info");    String expected = "TestPersonWithAllInformation(Name(first,None),None,Address(my_street,my_zip),None,Some(my_info),Map(key1 -> Phone(111,222)),None,None)\n";    verifyScroogeRead(thriftRecords(toWrite), TestPersonWithAllInformation.class, expected, "**");    String expectedProjected = "TestPersonWithAllInformation(Name(first,None),None,Address(my_street,my_zip),None,Some(my_info),Map(),None,None)\n";    verifyScroogeRead(thriftRecords(toWrite), TestPersonWithAllInformation.class, expectedProjected, "address/*;info;name/first_name");}
0
public void operate(FlowProcess flowProcess, FunctionCall functionCall)
{    Object record = functionCall.getArguments().getObject(0);    Tuple result = new Tuple();    result.add(record.toString());    functionCall.getOutputCollector().add(result);}
0
public void verifyScroogeRead(List<TBase> recordsToWrite, Class<T> readClass, String expectedStr, String projectionFilter) throws Exception
{    Configuration conf = new Configuration();    deleteIfExist(PARQUET_PATH);    deleteIfExist(TXT_OUTPUT_PATH);    final Path parquetFile = new Path(PARQUET_PATH);    writeParquetFile(recordsToWrite, conf, parquetFile);    Scheme sourceScheme = new ParquetScroogeScheme(new Config().withRecordClass(readClass).withProjectionString(projectionFilter));    Tap source = new Hfs(sourceScheme, PARQUET_PATH);    Scheme sinkScheme = new TextLine(new Fields("first", "last"));    Tap sink = new Hfs(sinkScheme, TXT_OUTPUT_PATH);    Pipe assembly = new Pipe("namecp");    assembly = new Each(assembly, new ObjectToStringFunction());    Flow flow = new HadoopFlowConnector().connect("namecp", source, sink, assembly);    flow.complete();    String result = FileUtils.readFileToString(new File(TXT_OUTPUT_PATH + "/part-00000"));    assertEquals(expectedStr, result);}
0
private void writeParquetFile(List<TBase> recordsToWrite, Configuration conf, Path parquetFile) throws IOException, InterruptedException, org.apache.thrift.TException
{        final TProtocolFactory protocolFactory = new TCompactProtocol.Factory();    final TaskAttemptID taskId = new TaskAttemptID("local", 0, true, 0, 0);    Class writeClass = recordsToWrite.get(0).getClass();    final ThriftToParquetFileWriter w = new ThriftToParquetFileWriter(parquetFile, ContextUtil.newTaskAttemptContext(conf, taskId), protocolFactory, writeClass);    final ByteArrayOutputStream baos = new ByteArrayOutputStream();    final TProtocol protocol = protocolFactory.getProtocol(new TIOStreamTransport(baos));    for (TBase recordToWrite : recordsToWrite) {        recordToWrite.write(protocol);    }    w.write(new BytesWritable(baos.toByteArray()));    w.close();}
0
private List<TBase> thriftRecords(TBase... records)
{    List<TBase> result = new ArrayList<TBase>();    for (TBase record : records) {        result.add(record);    }    return result;}
0
private void deleteIfExist(String path) throws IOException
{    Path p = new Path(path);    Configuration conf = new Configuration();    final FileSystem fs = p.getFileSystem(conf);    if (fs.exists(p)) {        fs.delete(p, true);    }}
0
public void testWriteThenRead() throws Exception
{    doWrite();    doRead();}
0
private void doWrite() throws Exception
{    Path path = new Path(parquetOutputPath);    final FileSystem fs = path.getFileSystem(new Configuration());    if (fs.exists(path))        fs.delete(path, true);    Scheme sourceScheme = new TextLine(new Fields("first", "last"));    Tap source = new Hfs(sourceScheme, txtInputPath);    Scheme sinkScheme = new ParquetScroogeScheme<Name>(Name.class);    Tap sink = new Hfs(sinkScheme, parquetOutputPath);    Pipe assembly = new Pipe("namecp");    assembly = new Each(assembly, new PackThriftFunction());    Flow flow = new HadoopFlowConnector().connect("namecp", source, sink, assembly);    flow.complete();}
0
private void doRead() throws Exception
{    Path path = new Path(txtOutputPath);    final FileSystem fs = path.getFileSystem(new Configuration());    if (fs.exists(path))        fs.delete(path, true);    Scheme sourceScheme = new ParquetScroogeScheme<Name>(Name.class);    Tap source = new Hfs(sourceScheme, parquetOutputPath);    Scheme sinkScheme = new TextLine(new Fields("first", "last"));    Tap sink = new Hfs(sinkScheme, txtOutputPath);    Pipe assembly = new Pipe("namecp");    assembly = new Each(assembly, new UnpackThriftFunction());    Flow flow = new HadoopFlowConnector().connect("namecp", source, sink, assembly);    flow.complete();    String result = FileUtils.readFileToString(new File(txtOutputPath + "/part-00000"));    assertEquals("0\tAlice\tPractice\n15\tBob\tHope\n24\tCharlie\tHorse\n", result);}
0
public void operate(FlowProcess flowProcess, FunctionCall functionCall)
{    TupleEntry arguments = functionCall.getArguments();    Tuple result = new Tuple();    Name name = Name$.MODULE$.apply(arguments.getString(0), Option.apply(arguments.getString(1)));    result.add(name);    functionCall.getOutputCollector().add(result);}
0
public void operate(FlowProcess flowProcess, FunctionCall functionCall)
{    TupleEntry arguments = functionCall.getArguments();    Tuple result = new Tuple();    Name name = (Name) arguments.getObject(0);    result.add(name.firstName());    result.add(name.lastName().get());    functionCall.getOutputCollector().add(result);}
0
public void testScroogeBinaryEncoding() throws Exception
{    StringAndBinary expected = new StringAndBinary.Immutable("test", ByteBuffer.wrap(new byte[] { -123, 20, 33 }));    File temp = tempDir.newFile(UUID.randomUUID().toString());    temp.deleteOnExit();    temp.delete();    Path path = new Path(temp.getPath());    ParquetWriter<StringAndBinary> writer = new ParquetWriter<StringAndBinary>(path, new Configuration(), new ScroogeWriteSupport<StringAndBinary>(StringAndBinary.class));    writer.write(expected);    writer.close();        ParquetReader<org.apache.parquet.thrift.test.binary.StringAndBinary> reader = ThriftParquetReader.<org.apache.parquet.thrift.test.binary.StringAndBinary>build(path).withThriftClass(org.apache.parquet.thrift.test.binary.StringAndBinary.class).build();    org.apache.parquet.thrift.test.binary.StringAndBinary record = reader.read();    reader.close();    Assert.assertEquals("String should match after serialization round trip", "test", record.s);    Assert.assertEquals("ByteBuffer should match after serialization round trip", ByteBuffer.wrap(new byte[] { -123, 20, 33 }), record.b);}
0
public void testScroogeBinaryDecoding() throws Exception
{    StringAndBinary expected = new StringAndBinary.Immutable("test", ByteBuffer.wrap(new byte[] { -123, 20, 33 }));    File temp = tempDir.newFile(UUID.randomUUID().toString());    temp.deleteOnExit();    temp.delete();    Path path = new Path(temp.getPath());    ParquetWriter<StringAndBinary> writer = new ParquetWriter<StringAndBinary>(path, new Configuration(), new ScroogeWriteSupport<StringAndBinary>(StringAndBinary.class));    writer.write(expected);    writer.close();    Configuration conf = new Configuration();    conf.set("parquet.thrift.converter.class", ScroogeRecordConverter.class.getName());    ParquetReader<StringAndBinary> reader = ParquetReader.<StringAndBinary>builder(new ScroogeReadSupport(), path).withConf(conf).build();    StringAndBinary record = reader.read();    reader.close();    Assert.assertEquals("String should match after serialization round trip", "test", record.s());    Assert.assertEquals("ByteBuffer should match after serialization round trip", ByteBuffer.wrap(new byte[] { -123, 20, 33 }), record.b());}
0
private void shouldConvertConsistentlyWithThriftStructConverter(Class scroogeClass) throws ClassNotFoundException
{    Class<? extends TBase<?, ?>> thriftClass = (Class<? extends TBase<?, ?>>) Class.forName(scroogeClass.getName().replaceFirst("org.apache.parquet.scrooge.test", "org.apache.parquet.thrift.test"));    ThriftType.StructType structFromThriftSchemaConverter = ThriftSchemaConverter.toStructType(thriftClass);    ThriftType.StructType structFromScroogeSchemaConverter = new ScroogeStructConverter().convert(scroogeClass);    assertEquals(toParquetSchema(structFromThriftSchemaConverter), toParquetSchema(structFromScroogeSchemaConverter));}
0
private MessageType toParquetSchema(ThriftType.StructType struct)
{    return ThriftSchemaConverter.convertWithoutProjection(struct);}
0
public void testConvertPrimitiveMapKey() throws Exception
{    shouldConvertConsistentlyWithThriftStructConverter(TestMapPrimitiveKey.class);}
0
public void testBinary() throws Exception
{    shouldConvertConsistentlyWithThriftStructConverter(StringAndBinary.class);}
0
public void testUnion() throws Exception
{    shouldConvertConsistentlyWithThriftStructConverter(TestUnion.class);}
0
public void testConvertPrimitiveMapValue() throws Exception
{    shouldConvertConsistentlyWithThriftStructConverter(TestMapPrimitiveValue.class);}
0
public void testConvertPrimitiveList() throws Exception
{    shouldConvertConsistentlyWithThriftStructConverter(TestListPrimitive.class);}
0
public void testConvertPrimitiveSet() throws Exception
{    shouldConvertConsistentlyWithThriftStructConverter(TestSetPrimitive.class);}
0
public void testConvertEnum() throws Exception
{    shouldConvertConsistentlyWithThriftStructConverter(TestFieldOfEnum.class);}
0
public void testMapBinary() throws Exception
{    shouldConvertConsistentlyWithThriftStructConverter(TestMapBinary.class);}
0
public void testMapComplex() throws Exception
{    shouldConvertConsistentlyWithThriftStructConverter(TestMapComplex.class);}
0
public void testConvertStruct() throws Exception
{    shouldConvertConsistentlyWithThriftStructConverter(TestPersonWithAllInformation.class);}
0
public void testDefaultFields() throws Exception
{    shouldConvertConsistentlyWithThriftStructConverter(AddressWithStreetWithDefaultRequirement.class);}
0
public void testConvertOptionalPrimitiveMap() throws Exception
{    shouldConvertConsistentlyWithThriftStructConverter(TestOptionalMap.class);}
0
public void testConvertNestedList() throws Exception
{    shouldConvertConsistentlyWithThriftStructConverter(NestedList.class);}
0
public void testConvertListNestMap() throws Exception
{    shouldConvertConsistentlyWithThriftStructConverter(ListNestMap.class);}
0
public void testConvertListNestEnum() throws Exception
{    shouldConvertConsistentlyWithThriftStructConverter(ListNestEnum.class);}
0
public void testConvertMapNestList() throws Exception
{    shouldConvertConsistentlyWithThriftStructConverter(MapNestList.class);}
0
public void testConvertMapNestMap() throws Exception
{    shouldConvertConsistentlyWithThriftStructConverter(MapNestMap.class);}
0
public void testConvertMapNestSet() throws Exception
{    shouldConvertConsistentlyWithThriftStructConverter(MapNestSet.class);}
0
public void testConvertListNestSet() throws Exception
{    shouldConvertConsistentlyWithThriftStructConverter(ListNestSet.class);}
0
public void testConvertSetNestSet() throws Exception
{    shouldConvertConsistentlyWithThriftStructConverter(SetNestSet.class);}
0
public void testConvertSetNestList() throws Exception
{    shouldConvertConsistentlyWithThriftStructConverter(SetNestList.class);}
0
public void testConvertSetNestMap() throws Exception
{    shouldConvertConsistentlyWithThriftStructConverter(SetNestMap.class);}
0
public void setupJob(Job job, Path path) throws Exception
{    job.setInputFormatClass(ParquetScroogeInputFormat.class);    ParquetScroogeInputFormat.setInputPaths(job, path);    ParquetScroogeInputFormat.setThriftClass(job.getConfiguration(), StructWithUnionV2.class);    ThriftReadSupport.setRecordConverterClass(job.getConfiguration(), ScroogeRecordConverter.class);    job.setMapperClass(ReadMapper.class);    job.setNumReduceTasks(0);    job.setOutputFormatClass(NullOutputFormat.class);}
0
protected void assertEqualsExcepted(List<org.apache.parquet.thrift.test.compat.StructWithUnionV2> expected, List<Object> found) throws Exception
{    List<StructWithUnionV2> scroogeExpected = new ArrayList<StructWithUnionV2>();    for (org.apache.parquet.thrift.test.compat.StructWithUnionV2 tbase : expected) {        ByteArrayOutputStream baos = new ByteArrayOutputStream();        TProtocol out = new Factory().getProtocol(new TIOStreamTransport(baos));        tbase.write(out);        TProtocol in = new Factory().getProtocol(new TIOStreamTransport(new ByteArrayInputStream(baos.toByteArray())));        scroogeExpected.add(StructWithUnionV2$.MODULE$.decode(in));    }    assertEquals(scroogeExpected, found);}
0
public static void setGenericThriftClass(Configuration configuration, Class<?> thriftClass)
{    configuration.set(PARQUET_THRIFT_CLASS, thriftClass.getName());}
0
public static Class getGenericThriftClass(Configuration configuration)
{    final String thriftClassName = configuration.get(PARQUET_THRIFT_CLASS);    if (thriftClassName == null) {        throw new BadConfigurationException("the thrift class conf is missing in job conf at " + PARQUET_THRIFT_CLASS);    }    try {        @SuppressWarnings("unchecked")        Class thriftClass = Class.forName(thriftClassName);        return thriftClass;    } catch (ClassNotFoundException e) {        throw new BadConfigurationException("the class " + thriftClassName + " in job conf at " + PARQUET_THRIFT_CLASS + " could not be found", e);    }}
0
protected void init(Class<T> thriftClass)
{    this.thriftClass = thriftClass;    this.thriftStruct = getThriftStruct();    this.schema = ThriftSchemaConverter.convertWithoutProjection(thriftStruct);    final Map<String, String> extraMetaData = new ThriftMetaData(thriftClass.getName(), thriftStruct).toExtraMetaData();        if (isPigLoaded() && TBase.class.isAssignableFrom(thriftClass)) {        new PigMetaData(new ThriftToPig((Class<? extends TBase<?, ?>>) thriftClass).toSchema()).addToMetaData(extraMetaData);    }    this.writeContext = new WriteContext(schema, extraMetaData);}
0
protected boolean isPigLoaded()
{    try {        Class.forName("org.apache.pig.impl.logicalLayer.schema.Schema");        return true;    } catch (ClassNotFoundException e) {                return false;    }}
1
public WriteContext init(Configuration configuration)
{    if (writeContext == null) {        init(getGenericThriftClass(configuration));    }    return writeContext;}
0
public void prepareForWrite(RecordConsumer recordConsumer)
{    final MessageColumnIO columnIO = new ColumnIOFactory().getColumnIO(schema);    this.parquetWriteProtocol = new ParquetWriteProtocol(recordConsumer, columnIO, thriftStruct);}
0
public static void setThriftClass(Job job, Class<? extends TBase<?, ?>> thriftClass)
{    TBaseWriteSupport.setThriftClass(ContextUtil.getConfiguration(job), thriftClass);}
0
public static Class<? extends TBase<?, ?>> getThriftClass(Job job)
{    return TBaseWriteSupport.getThriftClass(ContextUtil.getConfiguration(job));}
0
public static void setTProtocolClass(Job job, Class<U> tProtocolClass)
{    ThriftBytesWriteSupport.setTProtocolClass(ContextUtil.getConfiguration(job), tProtocolClass);}
0
public static void setThriftClass(JobConf conf, Class<T> klass)
{    conf.set(ThriftReadSupport.THRIFT_READ_CLASS_KEY, klass.getName());}
0
public static void setThriftClass(Configuration conf, Class<T> klass)
{    conf.set(ThriftReadSupport.THRIFT_READ_CLASS_KEY, klass.getName());}
0
public static void setThriftClass(Job job, Class<? extends TBase<?, ?>> thriftClass)
{    ThriftWriteSupport.setThriftClass(ContextUtil.getConfiguration(job), thriftClass);}
0
public static Class<? extends TBase<?, ?>> getThriftClass(Job job)
{    return ThriftWriteSupport.getThriftClass(ContextUtil.getConfiguration(job));}
0
public static void setThriftClass(Configuration configuration, Class<U> thriftClass)
{    AbstractThriftWriteSupport.setGenericThriftClass(configuration, thriftClass);}
0
public static Class<? extends TBase<?, ?>> getThriftClass(Configuration configuration)
{    return (Class<? extends TBase<?, ?>>) AbstractThriftWriteSupport.getGenericThriftClass(configuration);}
0
public String getName()
{    return "thrift";}
0
protected StructType getThriftStruct()
{    return ThriftSchemaConverter.toStructType(thriftClass);}
0
public void write(T record)
{    try {        record.write(parquetWriteProtocol);    } catch (TException e) {        throw new ParquetEncodingException(e);    }}
0
public static void setTProtocolClass(Configuration conf, Class<U> tProtocolClass)
{    conf.set(PARQUET_PROTOCOL_CLASS, tProtocolClass.getName());}
0
public static Class<TProtocolFactory> getTProtocolFactoryClass(Configuration conf)
{    final String tProtocolClassName = conf.get(PARQUET_PROTOCOL_CLASS);    if (tProtocolClassName == null) {        throw new BadConfigurationException("the protocol class conf is missing in job conf at " + PARQUET_PROTOCOL_CLASS);    }    try {        @SuppressWarnings("unchecked")        Class<TProtocolFactory> tProtocolFactoryClass = (Class<TProtocolFactory>) Class.forName(tProtocolClassName + "$Factory");        return tProtocolFactoryClass;    } catch (ClassNotFoundException e) {        throw new BadConfigurationException("the Factory for class " + tProtocolClassName + " in job conf at " + PARQUET_PROTOCOL_CLASS + " could not be found", e);    }}
0
public String getName()
{    return "thrift";}
0
public WriteContext init(Configuration configuration)
{    if (this.protocolFactory == null) {        try {            this.protocolFactory = getTProtocolFactoryClass(configuration).newInstance();        } catch (InstantiationException e) {            throw new RuntimeException(e);        } catch (IllegalAccessException e) {            throw new RuntimeException(e);        }    }    if (thriftClass != null) {        TBaseWriteSupport.setThriftClass(configuration, thriftClass);    } else {        thriftClass = TBaseWriteSupport.getThriftClass(configuration);    }    this.thriftStruct = ThriftSchemaConverter.toStructType(thriftClass);    this.schema = ThriftSchemaConverter.convertWithoutProjection(thriftStruct);    if (buffered) {        readToWrite = new BufferedProtocolReadToWrite(thriftStruct, errorHandler);    } else {        readToWrite = new ProtocolReadToWrite();    }    return thriftWriteSupport.init(configuration);}
0
private TProtocol protocol(BytesWritable record)
{    TProtocol protocol = protocolFactory.getProtocol(new TIOStreamTransport(new ByteArrayInputStream(record.getBytes())));    /* Reduce the chance of OOM when data is corrupted. When readBinary is called on TBinaryProtocol, it reads the length of the binary first,     so if the data is corrupted, it could read a big integer as the length of the binary and therefore causes OOM to happen.     Currently this fix only applies to TBinaryProtocol which has the setReadLength defined (thrift 0.7).      */    if (SET_READ_LENGTH != null && protocol instanceof TBinaryProtocol) {        try {            SET_READ_LENGTH.invoke(protocol, new Object[] { record.getLength() });        } catch (IllegalAccessException | IllegalArgumentException | InvocationTargetException e) {                        SET_READ_LENGTH = null;        }    }    return protocol;}
1
public void prepareForWrite(RecordConsumer recordConsumer)
{    final MessageColumnIO columnIO = new ColumnIOFactory().getColumnIO(schema);    this.parquetWriteProtocol = new ParquetWriteProtocol(recordConsumer, columnIO, thriftStruct);    thriftWriteSupport.prepareForWrite(recordConsumer);}
0
public void write(BytesWritable record)
{    try {        readToWrite.readOne(protocol(record), parquetWriteProtocol);    } catch (TException e) {        throw new ParquetEncodingException(e);    }}
0
public static void setRecordConverterClass(JobConf conf, Class<?> klass)
{    setRecordConverterClass((Configuration) conf, klass);}
0
public static void setRecordConverterClass(Configuration conf, Class<?> klass)
{    conf.set(RECORD_CONVERTER_CLASS_KEY, klass.getName());}
0
public static void setProjectionPushdown(JobConf jobConf, String projectionString)
{    jobConf.set(THRIFT_COLUMN_FILTER_KEY, projectionString);}
0
public static void setStrictFieldProjectionFilter(Configuration conf, String semicolonDelimitedGlobs)
{    conf.set(STRICT_THRIFT_COLUMN_FILTER_KEY, semicolonDelimitedGlobs);}
0
public static FieldProjectionFilter getFieldProjectionFilter(Configuration conf)
{    String deprecated = conf.get(THRIFT_COLUMN_FILTER_KEY);    String strict = conf.get(STRICT_THRIFT_COLUMN_FILTER_KEY);    if (Strings.isNullOrEmpty(deprecated) && Strings.isNullOrEmpty(strict)) {        return null;    }    if (!Strings.isNullOrEmpty(deprecated) && !Strings.isNullOrEmpty(strict)) {        throw new ThriftProjectionException("You cannot provide both " + THRIFT_COLUMN_FILTER_KEY + " and " + STRICT_THRIFT_COLUMN_FILTER_KEY + "! " + THRIFT_COLUMN_FILTER_KEY + " is deprecated.");    }    if (!Strings.isNullOrEmpty(deprecated)) {                return new DeprecatedFieldProjectionFilter(deprecated);    }    return StrictFieldProjectionFilter.fromSemicolonDelimitedString(strict);}
1
public org.apache.parquet.hadoop.api.ReadSupport.ReadContext init(InitContext context)
{    final Configuration configuration = context.getConfiguration();    final MessageType fileMessageType = context.getFileSchema();    MessageType requestedProjection = fileMessageType;    String partialSchemaString = configuration.get(ReadSupport.PARQUET_READ_SCHEMA);    FieldProjectionFilter projectionFilter = getFieldProjectionFilter(configuration);    if (partialSchemaString != null && projectionFilter != null) {        throw new ThriftProjectionException(String.format("You cannot provide both a partial schema and field projection filter." + "Only one of (%s, %s, %s) should be set.", PARQUET_READ_SCHEMA, STRICT_THRIFT_COLUMN_FILTER_KEY, THRIFT_COLUMN_FILTER_KEY));    }        if (partialSchemaString != null) {        requestedProjection = getSchemaForRead(fileMessageType, partialSchemaString);    } else if (projectionFilter != null) {        try {            initThriftClassFromMultipleFiles(context.getKeyValueMetadata(), configuration);            requestedProjection = getProjectedSchema(projectionFilter);        } catch (ClassNotFoundException e) {            throw new ThriftProjectionException("can not find thriftClass from configuration", e);        }    }    MessageType schemaForRead = getSchemaForRead(fileMessageType, requestedProjection);    return new ReadContext(schemaForRead);}
0
protected MessageType getProjectedSchema(FieldProjectionFilter fieldProjectionFilter)
{    return new ThriftSchemaConverter(fieldProjectionFilter).convert((Class<TBase<?, ?>>) thriftClass);}
0
private void initThriftClassFromMultipleFiles(Map<String, Set<String>> fileMetadata, Configuration conf) throws ClassNotFoundException
{    if (thriftClass != null) {        return;    }    String className = conf.get(THRIFT_READ_CLASS_KEY, null);    if (className == null) {        Set<String> names = ThriftMetaData.getThriftClassNames(fileMetadata);        if (names == null || names.size() != 1) {            throw new ParquetDecodingException("Could not read file as the Thrift class is not provided and could not be resolved from the file: " + names);        }        className = names.iterator().next();    }    thriftClass = (Class<T>) Class.forName(className);}
0
private void initThriftClass(ThriftMetaData metadata, Configuration conf) throws ClassNotFoundException
{    if (thriftClass != null) {        return;    }    String className = conf.get(THRIFT_READ_CLASS_KEY, null);    if (className == null) {        if (metadata == null) {            throw new ParquetDecodingException("Could not read file as the Thrift class is not provided and could not be resolved from the file");        }        thriftClass = (Class<T>) metadata.getThriftClass();    } else {        thriftClass = (Class<T>) Class.forName(className);    }}
0
public RecordMaterializer<T> prepareForRead(Configuration configuration, Map<String, String> keyValueMetaData, MessageType fileSchema, org.apache.parquet.hadoop.api.ReadSupport.ReadContext readContext)
{    ThriftMetaData thriftMetaData = ThriftMetaData.fromExtraMetaData(keyValueMetaData);    try {        initThriftClass(thriftMetaData, configuration);    } catch (ClassNotFoundException e) {        throw new RuntimeException("Cannot find Thrift object class for metadata: " + thriftMetaData, e);    }        if (thriftMetaData == null) {        thriftMetaData = ThriftMetaData.fromThriftClass(thriftClass);    }    String converterClassName = configuration.get(RECORD_CONVERTER_CLASS_KEY, RECORD_CONVERTER_DEFAULT);    return getRecordConverterInstance(converterClassName, thriftClass, readContext.getRequestedSchema(), thriftMetaData.getDescriptor(), configuration);}
0
private static ThriftRecordConverter<T> getRecordConverterInstance(String converterClassName, Class<T> thriftClass, MessageType requestedSchema, StructType descriptor, Configuration conf)
{    Class<ThriftRecordConverter<T>> converterClass;    try {        converterClass = (Class<ThriftRecordConverter<T>>) Class.forName(converterClassName);    } catch (ClassNotFoundException e) {        throw new RuntimeException("Cannot find Thrift converter class: " + converterClassName, e);    }    try {                try {            Constructor<ThriftRecordConverter<T>> constructor = converterClass.getConstructor(Class.class, MessageType.class, StructType.class, Configuration.class);            return constructor.newInstance(thriftClass, requestedSchema, descriptor, conf);        } catch (IllegalAccessException e) {                } catch (NoSuchMethodException e) {                }        Constructor<ThriftRecordConverter<T>> constructor = converterClass.getConstructor(Class.class, MessageType.class, StructType.class);        return constructor.newInstance(thriftClass, requestedSchema, descriptor);    } catch (InstantiationException e) {        throw new RuntimeException("Failed to construct Thrift converter class: " + converterClassName, e);    } catch (InvocationTargetException e) {        throw new RuntimeException("Failed to construct Thrift converter class: " + converterClassName, e);    } catch (IllegalAccessException e) {        throw new RuntimeException("Cannot access constructor for Thrift converter class: " + converterClassName, e);    } catch (NoSuchMethodException e) {        throw new RuntimeException("Cannot find constructor for Thrift converter class: " + converterClassName, e);    }}
0
public void write(BytesWritable bytes) throws IOException, InterruptedException
{    recordWriter.write(null, bytes);}
0
public void close() throws IOException
{    try {        recordWriter.close(taskAttemptContext);    } catch (InterruptedException e) {        Thread.interrupted();        throw new IOException("The thread was interrupted", e);    }}
0
public static void setThriftClass(Configuration configuration, Class<U> thriftClass)
{    TBaseWriteSupport.setThriftClass(configuration, thriftClass);}
0
public static Class<? extends TBase<?, ?>> getThriftClass(Configuration configuration)
{    return TBaseWriteSupport.getThriftClass(configuration);}
0
public String getName()
{    return writeSupport.getName();}
0
public WriteContext init(Configuration configuration)
{    return this.writeSupport.init(configuration);}
0
public void prepareForWrite(RecordConsumer recordConsumer)
{    this.writeSupport.prepareForWrite(recordConsumer);}
0
public void write(T record)
{    this.writeSupport.write(record);}
0
public void write(TProtocol out) throws TException
{    out.writeFieldStop();    out.writeStructEnd();}
0
public String toDebugString()
{    return ")";}
0
public void write(TProtocol out) throws TException
{    out.writeFieldEnd();}
0
public String toDebugString()
{    return ";";}
0
public void write(TProtocol out) throws TException
{    out.writeMapEnd();}
0
public String toDebugString()
{    return "]";}
0
public void write(TProtocol out) throws TException
{    out.writeListEnd();}
0
public String toDebugString()
{    return "}";}
0
public void write(TProtocol out) throws TException
{    out.writeSetEnd();}
0
public String toDebugString()
{    return "*}";}
0
public void readOne(TProtocol in, TProtocol out) throws TException
{    List<Action> buffer = new LinkedList<Action>();    try {        boolean hasFieldsIgnored = readOneStruct(in, buffer, thriftType);        if (hasFieldsIgnored) {            notifyRecordHasFieldIgnored();        }    } catch (Exception e) {        throw new SkippableException(error("Error while reading", buffer), e);    }    try {        for (Action a : buffer) {            a.write(out);        }    } catch (Exception e) {        throw new TException(error("Can not write record", buffer), e);    }}
0
private void notifyRecordHasFieldIgnored()
{    if (errorHandler != null) {        errorHandler.handleRecordHasFieldIgnored();    }}
0
private void notifyIgnoredFieldsOfRecord(TField field)
{    if (errorHandler != null) {        errorHandler.handleFieldIgnored(field);    }}
0
private String error(String message, List<Action> buffer)
{    StringBuilder sb = new StringBuilder(message).append(": ");    for (Action action : buffer) {        sb.append(action.toDebugString());    }    return sb.toString();}
0
private boolean readOneValue(TProtocol in, byte type, List<Action> buffer, ThriftType expectedType) throws TException
{    if (expectedType != null && expectedType.getType().getSerializedThriftType() != type) {        throw new DecodingSchemaMismatchException("the data type does not match the expected thrift structure: expected " + expectedType + " got " + typeName(type));    }    boolean hasFieldsIgnored = false;    switch(type) {        case TType.LIST:            hasFieldsIgnored = readOneList(in, buffer, (ListType) expectedType);            break;        case TType.MAP:            hasFieldsIgnored = readOneMap(in, buffer, (MapType) expectedType);            break;        case TType.SET:            hasFieldsIgnored = readOneSet(in, buffer, (SetType) expectedType);            break;        case TType.STRUCT:            hasFieldsIgnored = readOneStruct(in, buffer, (StructType) expectedType);            break;        case TType.STOP:            break;        case TType.BOOL:            final boolean bool = in.readBool();            writeBoolAction(buffer, bool);            break;        case TType.BYTE:            final byte b = in.readByte();            writeByteAction(buffer, b);            break;        case TType.DOUBLE:            final double d = in.readDouble();            writeDoubleAction(buffer, d);            break;        case TType.I16:            final short s = in.readI16();            writeShortAction(buffer, s);            break;                case TType.ENUM:        case TType.I32:            final int i = in.readI32();            checkEnum(expectedType, i);            writeIntAction(buffer, i);            break;        case TType.I64:            final long l = in.readI64();            writeLongAction(buffer, l);            break;        case TType.STRING:            final ByteBuffer bin = in.readBinary();            writeStringAction(buffer, bin);            break;        case TType.VOID:            break;        default:            throw new TException("Unknown type: " + type);    }    return hasFieldsIgnored;}
0
private void writeStringAction(List<Action> buffer, final ByteBuffer bin)
{    buffer.add(new Action() {        @Override        public void write(TProtocol out) throws TException {            out.writeBinary(bin);        }        @Override        public String toDebugString() {            return String.valueOf(bin);        }    });}
0
public void write(TProtocol out) throws TException
{    out.writeBinary(bin);}
0
public String toDebugString()
{    return String.valueOf(bin);}
0
private void writeLongAction(List<Action> buffer, final long l)
{    buffer.add(new Action() {        @Override        public void write(TProtocol out) throws TException {            out.writeI64(l);        }        @Override        public String toDebugString() {            return String.valueOf(l);        }    });}
0
public void write(TProtocol out) throws TException
{    out.writeI64(l);}
0
public String toDebugString()
{    return String.valueOf(l);}
0
private void writeIntAction(List<Action> buffer, final int i)
{    buffer.add(new Action() {        @Override        public void write(TProtocol out) throws TException {            out.writeI32(i);        }        @Override        public String toDebugString() {            return String.valueOf(i);        }    });}
0
public void write(TProtocol out) throws TException
{    out.writeI32(i);}
0
public String toDebugString()
{    return String.valueOf(i);}
0
private void writeShortAction(List<Action> buffer, final short s)
{    buffer.add(new Action() {        @Override        public void write(TProtocol out) throws TException {            out.writeI16(s);        }        @Override        public String toDebugString() {            return String.valueOf(s);        }    });}
0
public void write(TProtocol out) throws TException
{    out.writeI16(s);}
0
public String toDebugString()
{    return String.valueOf(s);}
0
private void writeDoubleAction(List<Action> buffer, final double d)
{    buffer.add(new Action() {        @Override        public void write(TProtocol out) throws TException {            out.writeDouble(d);        }        @Override        public String toDebugString() {            return String.valueOf(d);        }    });}
0
public void write(TProtocol out) throws TException
{    out.writeDouble(d);}
0
public String toDebugString()
{    return String.valueOf(d);}
0
private void writeByteAction(List<Action> buffer, final byte b)
{    buffer.add(new Action() {        @Override        public void write(TProtocol out) throws TException {            out.writeByte(b);        }        @Override        public String toDebugString() {            return String.valueOf(b);        }    });}
0
public void write(TProtocol out) throws TException
{    out.writeByte(b);}
0
public String toDebugString()
{    return String.valueOf(b);}
0
private void writeBoolAction(List<Action> buffer, final boolean bool)
{    buffer.add(new Action() {        @Override        public void write(TProtocol out) throws TException {            out.writeBool(bool);        }        @Override        public String toDebugString() {            return String.valueOf(bool);        }    });}
0
public void write(TProtocol out) throws TException
{    out.writeBool(bool);}
0
public String toDebugString()
{    return String.valueOf(bool);}
0
private String typeName(byte type)
{    try {        return ThriftTypeID.fromByte(type).name();    } catch (RuntimeException e) {        return String.valueOf(type);    }}
0
private boolean readOneStruct(TProtocol in, List<Action> buffer, StructType type) throws TException
{    final TStruct struct = in.readStructBegin();    buffer.add(new Action() {        @Override        public void write(TProtocol out) throws TException {            out.writeStructBegin(struct);        }        @Override        public String toDebugString() {            return "(";        }    });    TField field;    boolean hasFieldsIgnored = false;    int childFieldsPresent = 0;    while ((field = in.readFieldBegin()).type != TType.STOP) {        final TField currentField = field;        ThriftField expectedField;        if ((expectedField = type.getChildById(field.id)) == null) {            handleUnrecognizedField(field, type, in);            hasFieldsIgnored |= true;            continue;        }        childFieldsPresent++;        buffer.add(new Action() {            @Override            public void write(TProtocol out) throws TException {                out.writeFieldBegin(currentField);            }            @Override            public String toDebugString() {                return "f=" + currentField.id + "<t=" + typeName(currentField.type) + ">: ";            }        });        hasFieldsIgnored |= readOneValue(in, field.type, buffer, expectedField.getType());        in.readFieldEnd();        buffer.add(FIELD_END);    }        assertUnionHasExactlyOneChild(type, childFieldsPresent);    in.readStructEnd();    buffer.add(STRUCT_END);    return hasFieldsIgnored;}
0
public void write(TProtocol out) throws TException
{    out.writeStructBegin(struct);}
0
public String toDebugString()
{    return "(";}
0
public void write(TProtocol out) throws TException
{    out.writeFieldBegin(currentField);}
0
public String toDebugString()
{    return "f=" + currentField.id + "<t=" + typeName(currentField.type) + ">: ";}
0
private void handleUnrecognizedField(TField field, StructType type, TProtocol in) throws TException
{    switch(type.getStructOrUnionType()) {        case STRUCT:                        notifyIgnoredFieldsOfRecord(field);                        new ProtocolReadToWrite().readOneValue(in, new NullProtocol(), field.type);            break;        case UNION:                        throw new DecodingSchemaMismatchException("Unrecognized union member with id: " + field.id + " for struct:\n" + type);        case UNKNOWN:            throw unknownStructOrUnion(type);        default:            throw unrecognizedStructOrUnion(type.getStructOrUnionType());    }}
0
private void assertUnionHasExactlyOneChild(StructType type, int childFieldsPresent)
{    switch(type.getStructOrUnionType()) {        case STRUCT:                        break;        case UNION:                        if (childFieldsPresent != 1) {                if (childFieldsPresent == 0) {                    throw new DecodingSchemaMismatchException("Cannot write a TUnion with no set value in :\n" + type);                } else {                    throw new DecodingSchemaMismatchException("Cannot write a TUnion with more than 1 set value in :\n" + type);                }            }            break;        case UNKNOWN:            throw unknownStructOrUnion(type);        default:            throw unrecognizedStructOrUnion(type.getStructOrUnionType());    }}
0
private static ShouldNeverHappenException unrecognizedStructOrUnion(StructOrUnionType type)
{    return new ShouldNeverHappenException("Unrecognized StructOrUnionType: " + type);}
0
private static ShouldNeverHappenException unknownStructOrUnion(StructType type)
{    return new ShouldNeverHappenException("This should never happen! " + "Don't know if this field is a union, was the deprecated constructor of StructType used?\n" + type);}
0
private boolean readOneMap(TProtocol in, List<Action> buffer, MapType mapType) throws TException
{    final TMap map = in.readMapBegin();    buffer.add(new Action() {        @Override        public void write(TProtocol out) throws TException {            out.writeMapBegin(map);        }        @Override        public String toDebugString() {            return "<k=" + map.keyType + ", v=" + map.valueType + ", s=" + map.size + ">[";        }    });    boolean hasFieldIgnored = false;    for (int i = 0; i < map.size; i++) {        hasFieldIgnored |= readOneValue(in, map.keyType, buffer, mapType.getKey().getType());        hasFieldIgnored |= readOneValue(in, map.valueType, buffer, mapType.getValue().getType());    }    in.readMapEnd();    buffer.add(MAP_END);    return hasFieldIgnored;}
0
public void write(TProtocol out) throws TException
{    out.writeMapBegin(map);}
0
public String toDebugString()
{    return "<k=" + map.keyType + ", v=" + map.valueType + ", s=" + map.size + ">[";}
0
private boolean readOneSet(TProtocol in, List<Action> buffer, SetType expectedType) throws TException
{    final TSet set = in.readSetBegin();    buffer.add(new Action() {        @Override        public void write(TProtocol out) throws TException {            out.writeSetBegin(set);        }        @Override        public String toDebugString() {            return "<e=" + set.elemType + ", s=" + set.size + ">{*";        }    });    boolean hasFieldsIgnored = readCollectionElements(in, set.size, set.elemType, buffer, expectedType.getValues().getType());    in.readSetEnd();    buffer.add(SET_END);    return hasFieldsIgnored;}
0
public void write(TProtocol out) throws TException
{    out.writeSetBegin(set);}
0
public String toDebugString()
{    return "<e=" + set.elemType + ", s=" + set.size + ">{*";}
0
private boolean readOneList(TProtocol in, List<Action> buffer, ListType expectedType) throws TException
{    final TList list = in.readListBegin();    buffer.add(new Action() {        @Override        public void write(TProtocol out) throws TException {            out.writeListBegin(list);        }        @Override        public String toDebugString() {            return "<e=" + list.elemType + ", s=" + list.size + ">{";        }    });    boolean hasFieldsIgnored = readCollectionElements(in, list.size, list.elemType, buffer, expectedType.getValues().getType());    in.readListEnd();    buffer.add(LIST_END);    return hasFieldsIgnored;}
0
public void write(TProtocol out) throws TException
{    out.writeListBegin(list);}
0
public String toDebugString()
{    return "<e=" + list.elemType + ", s=" + list.size + ">{";}
0
private boolean readCollectionElements(TProtocol in, final int size, final byte elemType, List<Action> buffer, ThriftType expectedType) throws TException
{    boolean hasFieldIgnored = false;    for (int i = 0; i < size; i++) {        hasFieldIgnored |= readOneValue(in, elemType, buffer, expectedType);    }    return hasFieldIgnored;}
0
private void checkEnum(ThriftType expectedType, int i)
{    if (expectedType.getType() == ThriftTypeID.ENUM) {        ThriftType.EnumType expectedEnumType = (ThriftType.EnumType) expectedType;        if (expectedEnumType.getEnumValueById(i) == null) {            throw new DecodingSchemaMismatchException("can not find index " + i + " in enum " + expectedType);        }    }}
0
public void writeMessageBegin(TMessage tMessage) throws TException
{}
0
public void writeMessageEnd() throws TException
{}
0
public void writeStructBegin(TStruct tStruct) throws TException
{}
0
public void writeStructEnd() throws TException
{}
0
public void writeFieldBegin(TField tField) throws TException
{}
0
public void writeFieldEnd() throws TException
{}
0
public void writeFieldStop() throws TException
{}
0
public void writeMapBegin(TMap tMap) throws TException
{}
0
public void writeMapEnd() throws TException
{}
0
public void writeListBegin(TList tList) throws TException
{}
0
public void writeListEnd() throws TException
{}
0
public void writeSetBegin(TSet tSet) throws TException
{}
0
public void writeSetEnd() throws TException
{}
0
public void writeBool(boolean b) throws TException
{}
0
public void writeByte(byte b) throws TException
{}
0
public void writeI16(short i) throws TException
{}
0
public void writeI32(int i) throws TException
{}
0
public void writeI64(long l) throws TException
{}
0
public void writeDouble(double v) throws TException
{}
0
public void writeString(String s) throws TException
{}
0
public void writeBinary(ByteBuffer byteBuffer) throws TException
{}
0
public TMessage readMessageBegin() throws TException
{    return null;}
0
public void readMessageEnd() throws TException
{}
0
public TStruct readStructBegin() throws TException
{    return null;}
0
public void readStructEnd() throws TException
{}
0
public TField readFieldBegin() throws TException
{    return null;}
0
public void readFieldEnd() throws TException
{}
0
public TMap readMapBegin() throws TException
{    return null;}
0
public void readMapEnd() throws TException
{}
0
public TList readListBegin() throws TException
{    return null;}
0
public void readListEnd() throws TException
{}
0
public TSet readSetBegin() throws TException
{    return null;}
0
public void readSetEnd() throws TException
{}
0
public boolean readBool() throws TException
{    return false;}
0
public byte readByte() throws TException
{    return 0;}
0
public short readI16() throws TException
{    return 0;}
0
public int readI32() throws TException
{    return 0;}
0
public long readI64() throws TException
{    return 0;}
0
public double readDouble() throws TException
{    return 0;}
0
public String readString() throws TException
{    return null;}
0
public ByteBuffer readBinary() throws TException
{    return null;}
0
public boolean isKeep()
{    return false;}
0
public Keep asKeep()
{    throw new ShouldNeverHappenException("asKeep called on " + this);}
0
public boolean isDrop()
{    return false;}
0
public Drop asDrop()
{    throw new ShouldNeverHappenException("asDrop called on " + this);}
0
public boolean isSentinelUnion()
{    return false;}
0
public SentinelUnion asSentinelUnion()
{    throw new ShouldNeverHappenException("asSentinelUnion called on " + this);}
0
public FieldsPath path()
{    return path;}
0
public boolean isKeep()
{    return true;}
0
public Keep asKeep()
{    return this;}
0
public Type getType()
{    return type;}
0
public boolean isSentinelUnion()
{    return true;}
0
public SentinelUnion asSentinelUnion()
{    return this;}
0
public Type getType()
{    return type;}
0
public boolean isDrop()
{    return true;}
0
public Drop asDrop()
{    return this;}
0
public void handleRecordHasFieldIgnored()
{}
0
public void handleFieldIgnored(TField field)
{}
0
public boolean keep(FieldsPath path)
{    if (found) {        return false;    }    found = true;    return true;}
0
public void assertNoUnmatchedPatterns() throws ThriftProjectionException
{}
0
private String getClassInfo()
{    final Class<? extends ParquetProtocol> clazz = getClass();    final Method enclosingMethod = clazz.getEnclosingMethod();    if (enclosingMethod != null) {        return clazz.getName() + " in " + enclosingMethod.toGenericString();    }    return clazz.getName();}
0
private TException exception()
{    String message = name == null ? "in " + getClassInfo() : "when we expected " + name + " in " + getClassInfo();    return new TException(new UnsupportedOperationException(new Exception().getStackTrace()[1].getMethodName() + " was called " + message));}
0
public void writeMessageBegin(TMessage message) throws TException
{    throw exception();}
0
public void writeMessageEnd() throws TException
{    throw exception();}
0
public void writeStructBegin(TStruct struct) throws TException
{    throw exception();}
0
public void writeStructEnd() throws TException
{    throw exception();}
0
public void writeFieldBegin(TField field) throws TException
{    throw exception();}
0
public void writeFieldEnd() throws TException
{    throw exception();}
0
public void writeFieldStop() throws TException
{    throw exception();}
0
public void writeMapBegin(TMap map) throws TException
{    throw exception();}
0
public void writeMapEnd() throws TException
{    throw exception();}
0
public void writeListBegin(TList list) throws TException
{    throw exception();}
0
public void writeListEnd() throws TException
{    throw exception();}
0
public void writeSetBegin(TSet set) throws TException
{    throw exception();}
0
public void writeSetEnd() throws TException
{    throw exception();}
0
public void writeBool(boolean b) throws TException
{    throw exception();}
0
public void writeByte(byte b) throws TException
{    throw exception();}
0
public void writeI16(short i16) throws TException
{    throw exception();}
0
public void writeI32(int i32) throws TException
{    throw exception();}
0
public void writeI64(long i64) throws TException
{    throw exception();}
0
public void writeDouble(double dub) throws TException
{    throw exception();}
0
public void writeString(String str) throws TException
{    throw exception();}
0
public void writeBinary(ByteBuffer buf) throws TException
{    throw exception();}
0
public TMessage readMessageBegin() throws TException
{    throw exception();}
0
public void readMessageEnd() throws TException
{    throw exception();}
0
public TStruct readStructBegin() throws TException
{    throw exception();}
0
public void readStructEnd() throws TException
{    throw exception();}
0
public TField readFieldBegin() throws TException
{    throw exception();}
0
public void readFieldEnd() throws TException
{    throw exception();}
0
public TMap readMapBegin() throws TException
{    throw exception();}
0
public void readMapEnd() throws TException
{    throw exception();}
0
public TList readListBegin() throws TException
{    throw exception();}
0
public void readListEnd() throws TException
{    throw exception();}
0
public TSet readSetBegin() throws TException
{    throw exception();}
0
public void readSetEnd() throws TException
{    throw exception();}
0
public boolean readBool() throws TException
{    throw exception();}
0
public byte readByte() throws TException
{    throw exception();}
0
public short readI16() throws TException
{    throw exception();}
0
public int readI32() throws TException
{    throw exception();}
0
public long readI64() throws TException
{    throw exception();}
0
public double readDouble() throws TException
{    throw exception();}
0
public String readString() throws TException
{    throw exception();}
0
public ByteBuffer readBinary() throws TException
{    throw exception();}
0
public void add(TProtocol p)
{    events.addLast(p);}
0
public void addAll(Collection<TProtocol> events)
{    this.events.addAll(events);}
0
public void clear()
{    this.events.clear();}
0
private TProtocol next()
{    return events.removeFirst();}
0
public TMessage readMessageBegin() throws TException
{        return next().readMessageBegin();}
1
public void readMessageEnd() throws TException
{        next().readMessageEnd();}
1
public TStruct readStructBegin() throws TException
{        return next().readStructBegin();}
1
public void readStructEnd() throws TException
{        next().readStructEnd();}
1
public TField readFieldBegin() throws TException
{        return next().readFieldBegin();}
1
public void readFieldEnd() throws TException
{        next().readFieldEnd();}
1
public TMap readMapBegin() throws TException
{        return next().readMapBegin();}
1
public void readMapEnd() throws TException
{        next().readMapEnd();}
1
public TList readListBegin() throws TException
{        return next().readListBegin();}
1
public void readListEnd() throws TException
{        next().readListEnd();}
1
public TSet readSetBegin() throws TException
{        return next().readSetBegin();}
1
public void readSetEnd() throws TException
{        next().readSetEnd();}
1
public boolean readBool() throws TException
{        return next().readBool();}
1
public byte readByte() throws TException
{        return next().readByte();}
1
public short readI16() throws TException
{        return next().readI16();}
1
public int readI32() throws TException
{        return next().readI32();}
1
public long readI64() throws TException
{        return next().readI64();}
1
public double readDouble() throws TException
{        return next().readDouble();}
1
public String readString() throws TException
{        return next().readString();}
1
public ByteBuffer readBinary() throws TException
{        return next().readBinary();}
1
 void start()
{    this.returnClause.start();}
0
 void end()
{    this.returnClause.end();}
0
public void writeI32(int i32) throws TException
{    start();    EnumValue value = type.getEnumValueById(i32);    if (value == null) {        throw new ParquetEncodingException("Can not find enum value of index " + i32 + " for field:" + columnIO.toString());    }    recordConsumer.addBinary(Binary.fromString(value.getName()));    end();}
0
public void start()
{}
0
public void end()
{    ++consumedRecords;    if (consumedRecords == size) {        currentProtocol = ListWriteProtocol.this;        consumedRecords = 0;    }}
0
private void startListWrapper()
{    start();    recordConsumer.startGroup();    if (size > 0) {        recordConsumer.startField(listContent.getType().getName(), 0);        currentProtocol = contentProtocol;    }}
0
private void endListWrapper()
{    if (size > 0) {        recordConsumer.endField(listContent.getType().getName(), 0);    }    recordConsumer.endGroup();    end();}
0
public void writeListBegin(TList list) throws TException
{    size = list.size;    startListWrapper();}
0
public void writeListEnd() throws TException
{    endListWrapper();}
0
public void writeSetBegin(TSet set) throws TException
{    size = set.size;    startListWrapper();}
0
public void writeSetEnd() throws TException
{    endListWrapper();}
0
public void start()
{    recordConsumer.startGroup();    recordConsumer.startField(key.getName(), key.getIndex());}
0
public void end()
{    recordConsumer.endField(key.getName(), key.getIndex());    currentProtocol = valueProtocol;}
0
public void start()
{    recordConsumer.startField(value.getName(), value.getIndex());}
0
public void end()
{    consumed++;    recordConsumer.endField(value.getName(), value.getIndex());    recordConsumer.endGroup();    if (consumed == countToConsume) {        currentProtocol = MapWriteProtocol.this;        consumed = 0;    } else {        currentProtocol = keyProtocol;    }}
0
public void writeMapBegin(TMap map) throws TException
{    start();    recordConsumer.startGroup();    countToConsume = map.size;    if (countToConsume > 0) {        recordConsumer.startField(mapContent.getType().getName(), 0);        currentProtocol = keyProtocol;    }}
0
public void writeMapEnd() throws TException
{    if (countToConsume > 0) {        recordConsumer.endField(mapContent.getType().getName(), 0);    }    recordConsumer.endGroup();    end();}
0
public void writeBool(boolean b) throws TException
{    start();    recordConsumer.addBoolean(b);    end();}
0
public void writeByte(byte b) throws TException
{    start();    recordConsumer.addInteger(b);    end();}
0
public void writeI16(short i16) throws TException
{    start();    recordConsumer.addInteger(i16);    end();}
0
public void writeI32(int i32) throws TException
{    start();    recordConsumer.addInteger(i32);    end();}
0
public void writeI64(long i64) throws TException
{    start();    recordConsumer.addLong(i64);    end();}
0
public void writeDouble(double dub) throws TException
{    start();    recordConsumer.addDouble(dub);    end();}
0
public void writeString(String str) throws TException
{    start();    writeStringToRecordConsumer(str);    end();}
0
public void writeBinary(ByteBuffer buf) throws TException
{    start();    writeBinaryToRecordConsumer(buf);    end();}
0
public void start()
{}
0
public void end()
{    currentProtocol = StructWriteProtocol.this;}
0
public void writeStructBegin(TStruct struct) throws TException
{    start();    recordConsumer.startGroup();}
0
public void writeStructEnd() throws TException
{    recordConsumer.endGroup();    end();}
0
public void writeFieldBegin(TField field) throws TException
{    if (field.type == TType.STOP) {        return;    }    try {        currentType = thriftFieldIdToParquetField[field.id];        if (currentType == null) {            throw new ParquetEncodingException("field " + field.id + " was not found in " + thriftType + " and " + schema.getType());        }        final int index = currentType.getIndex();        recordConsumer.startField(currentType.getName(), index);        currentProtocol = children[index];    } catch (ArrayIndexOutOfBoundsException e) {        throw new ParquetEncodingException("field " + field.id + " was not found in " + thriftType + " and " + schema.getType());    }}
0
public void writeFieldStop() throws TException
{}
0
public void writeFieldEnd() throws TException
{    recordConsumer.endField(currentType.getName(), currentType.getIndex());}
0
public void writeStructBegin(TStruct struct) throws TException
{    recordConsumer.startMessage();}
0
public void writeStructEnd() throws TException
{    recordConsumer.endMessage();}
0
private String toString(TStruct struct)
{    return "<TStruct name:" + struct.name + ">";}
0
private String toString(TList list)
{    return "<TList elemType:" + list.elemType + " size:" + list.size + ">";}
0
private String toString(TMap map)
{    return "<TMap keyType:" + map.keyType + " valueType:" + map.valueType + " size:" + map.size + ">";}
0
public void writeMessageBegin(TMessage message) throws TException
{        currentProtocol.writeMessageBegin(message);}
1
public void writeMessageEnd() throws TException
{        currentProtocol.writeMessageEnd();}
1
public void writeStructBegin(TStruct struct) throws TException
{    if (LOG.isDebugEnabled())            currentProtocol.writeStructBegin(struct);}
1
public void writeStructEnd() throws TException
{        currentProtocol.writeStructEnd();}
1
public void writeFieldBegin(TField field) throws TException
{        currentProtocol.writeFieldBegin(field);}
1
public void writeFieldEnd() throws TException
{        currentProtocol.writeFieldEnd();}
1
public void writeFieldStop() throws TException
{        currentProtocol.writeFieldStop();}
1
public void writeMapBegin(TMap map) throws TException
{    if (LOG.isDebugEnabled())            currentProtocol.writeMapBegin(map);}
1
public void writeMapEnd() throws TException
{        currentProtocol.writeMapEnd();}
1
public void writeListBegin(TList list) throws TException
{    if (LOG.isDebugEnabled())            currentProtocol.writeListBegin(list);}
1
public void writeListEnd() throws TException
{        currentProtocol.writeListEnd();}
1
public void writeSetBegin(TSet set) throws TException
{        currentProtocol.writeSetBegin(set);}
1
public void writeSetEnd() throws TException
{        currentProtocol.writeSetEnd();}
1
public void writeBool(boolean b) throws TException
{        currentProtocol.writeBool(b);}
1
public void writeByte(byte b) throws TException
{        currentProtocol.writeByte(b);}
1
public void writeI16(short i16) throws TException
{        currentProtocol.writeI16(i16);}
1
public void writeI32(int i32) throws TException
{        currentProtocol.writeI32(i32);}
1
public void writeI64(long i64) throws TException
{        currentProtocol.writeI64(i64);}
1
public void writeDouble(double dub) throws TException
{        currentProtocol.writeDouble(dub);}
1
public void writeString(String str) throws TException
{        currentProtocol.writeString(str);}
1
public void writeBinary(ByteBuffer buf) throws TException
{        currentProtocol.writeBinary(buf);}
1
private void writeBinaryToRecordConsumer(ByteBuffer buf)
{    recordConsumer.addBinary(Binary.fromReusedByteArray(buf.array(), buf.position(), buf.limit() - buf.position()));}
0
private void writeStringToRecordConsumer(String str)
{    recordConsumer.addBinary(Binary.fromString(str));}
0
private TProtocol getProtocol(ThriftField field, ColumnIO columnIO, Events returnClause)
{    TProtocol p;    final ThriftType type = field.getType();    switch(type.getType()) {        case STOP:        case VOID:        default:            throw new UnsupportedOperationException("can't convert type of " + field);        case BOOL:        case BYTE:        case DOUBLE:        case I16:        case I32:        case I64:        case STRING:            p = new PrimitiveWriteProtocol((PrimitiveColumnIO) columnIO, returnClause);            break;        case STRUCT:            p = new StructWriteProtocol((GroupColumnIO) columnIO, (StructType) type, returnClause);            break;        case MAP:            p = new MapWriteProtocol((GroupColumnIO) columnIO, (MapType) type, returnClause);            break;        case SET:            p = new ListWriteProtocol((GroupColumnIO) columnIO, ((SetType) type).getValues(), returnClause);            break;        case LIST:            p = new ListWriteProtocol((GroupColumnIO) columnIO, ((ListType) type).getValues(), returnClause);            break;        case ENUM:            p = new EnumWriteProtocol((PrimitiveColumnIO) columnIO, (EnumType) type, returnClause);            break;    }    return p;}
0
public OutputFormat<Void, Tuple> getOutputFormat() throws IOException
{    return new ParquetOutputFormat<Tuple>(new TupleToThriftWriteSupport(className));}
0
public void prepareToWrite(RecordWriter recordWriter) throws IOException
{    this.recordWriter = recordWriter;}
0
public void putNext(Tuple tuple) throws IOException
{    try {        this.recordWriter.write(null, tuple);    } catch (InterruptedException e) {        throw new ParquetEncodingException("Interrupted while writing", e);    }}
0
public void setStoreLocation(String location, Job job) throws IOException
{    FileOutputFormat.setOutputPath(job, new Path(location));}
0
public String getName()
{    return "thrift";}
0
public WriteContext init(Configuration configuration)
{    try {        Class<?> clazz = configuration.getClassByName(className).asSubclass(TBase.class);        thriftWriteSupport = new ThriftWriteSupport(clazz);        pigToThrift = new PigToThrift(clazz);        return thriftWriteSupport.init(configuration);    } catch (ClassNotFoundException e) {        throw new BadConfigurationException("The thrift class name was not found: " + className, e);    } catch (ClassCastException e) {        throw new BadConfigurationException("The thrift class name should extend TBase: " + className, e);    }}
0
public void prepareForWrite(RecordConsumer recordConsumer)
{    thriftWriteSupport.prepareForWrite(recordConsumer);}
0
public void write(Tuple t)
{    thriftWriteSupport.write(pigToThrift.getThriftObject(t));}
0
public Void visit(ThriftType.MapType mapType, Void v)
{    dummyEvents.add(new ParquetProtocol("readMapBegin()") {        @Override        public TMap readMapBegin() throws TException {            return new TMap();        }    });    dummyEvents.add(new ParquetProtocol("readMapEnd()") {        @Override        public void readMapEnd() throws TException {        }    });    return null;}
0
public TMap readMapBegin() throws TException
{    return new TMap();}
0
public void readMapEnd() throws TException
{}
0
public Void visit(final ThriftType.SetType setType, Void v)
{    dummyEvents.add(new ParquetProtocol("readSetBegin()") {        @Override        public TSet readSetBegin() throws TException {            return new TSet();        }    });    dummyEvents.add(new ParquetProtocol("readSetEnd()") {        @Override        public void readSetEnd() throws TException {        }    });    return null;}
0
public TSet readSetBegin() throws TException
{    return new TSet();}
0
public void readSetEnd() throws TException
{}
0
public Void visit(final ThriftType.ListType listType, Void v)
{    dummyEvents.add(new ParquetProtocol("readListBegin()") {        @Override        public TList readListBegin() throws TException {            return new TList();        }    });    dummyEvents.add(new ParquetProtocol("readListEnd()") {        @Override        public void readListEnd() throws TException {        }    });    return null;}
0
public TList readListBegin() throws TException
{    return new TList();}
0
public void readListEnd() throws TException
{}
0
public Void visit(ThriftType.StructType structType, Void v)
{    dummyEvents.add(new StructBeginProtocol("struct"));    List<ThriftField> children = structType.getChildren();    for (ThriftField child : children) {        dummyEvents.add(new ReadFieldBeginProtocol(child));                child.getType().accept(this, null);        dummyEvents.add(DefaultProtocolEventsGenerator.READ_FIELD_END);    }    dummyEvents.add(DefaultProtocolEventsGenerator.READ_FIELD_STOP);    dummyEvents.add(DefaultProtocolEventsGenerator.READ_STRUCT_END);    return null;}
0
public Void visit(ThriftType.EnumType enumType, Void v)
{    dummyEvents.add(new ParquetProtocol("readI32() enum") {        @Override        public int readI32() throws TException {            return 0;        }    });    return null;}
0
public int readI32() throws TException
{    return 0;}
0
public Void visit(ThriftType.BoolType boolType, Void v)
{    dummyEvents.add(new ParquetProtocol("readBool()") {        @Override        public boolean readBool() throws TException {            return false;        }    });    return null;}
0
public boolean readBool() throws TException
{    return false;}
0
public Void visit(ThriftType.ByteType byteType, Void v)
{    dummyEvents.add(new ParquetProtocol("readByte() int") {        @Override        public byte readByte() throws TException {            return (byte) 0;        }    });    return null;}
0
public byte readByte() throws TException
{    return (byte) 0;}
0
public Void visit(ThriftType.DoubleType doubleType, Void v)
{    dummyEvents.add(new ParquetProtocol("readDouble()") {        @Override        public double readDouble() throws TException {            return 0.0;        }    });    return null;}
0
public double readDouble() throws TException
{    return 0.0;}
0
public Void visit(ThriftType.I16Type i16Type, Void v)
{    dummyEvents.add(new ParquetProtocol("readI16()") {        @Override        public short readI16() throws TException {            return (short) 0;        }    });    return null;}
0
public short readI16() throws TException
{    return (short) 0;}
0
public Void visit(ThriftType.I32Type i32Type, Void v)
{    dummyEvents.add(new ParquetProtocol("readI32()") {        @Override        public int readI32() throws TException {            return 0;        }    });    return null;}
0
public int readI32() throws TException
{    return 0;}
0
public Void visit(ThriftType.I64Type i64Type, Void v)
{    dummyEvents.add(new ParquetProtocol("readI64()") {        @Override        public long readI64() throws TException {            return 0;        }    });    return null;}
0
public long readI64() throws TException
{    return 0;}
0
public Void visit(ThriftType.StringType stringType, Void v)
{    dummyEvents.add(new StringProtocol(""));    return null;}
0
public List<ParquetProtocol> getEvents()
{    return dummyEvents;}
0
public TStruct readStructBegin() throws TException
{    return new TStruct(structName);}
0
public String readString() throws TException
{    return str;}
0
public ByteBuffer readBinary() throws TException
{    return ByteBuffer.wrap("str".getBytes());}
0
public TField readFieldBegin() throws TException
{    return stop;}
0
public void readStructEnd() throws TException
{}
0
public void readFieldEnd() throws TException
{}
0
public List<TProtocol> createProtocolEventsForField(ThriftField missingField)
{    TProtocol fieldBegin = new ReadFieldBeginProtocol(missingField);    createdEvents.add(fieldBegin);    DefaultEventsVisitor dummyCreatorvisitor = new DefaultEventsVisitor();    missingField.getType().accept(dummyCreatorvisitor, null);    createdEvents.addAll(dummyCreatorvisitor.getEvents());    createdEvents.add(READ_FIELD_END);    return createdEvents;}
0
public List<TProtocol> amendMissingRequiredFields(StructType recordThriftType) throws TException
{    Iterator<TProtocol> protocolIter = rootEvents.iterator();    checkStruct(protocolIter, recordThriftType);    return fixedEvents;}
0
private TProtocol acceptProtocol(TProtocol p)
{    this.fixedEvents.add(p);    return p;}
0
private void checkStruct(Iterator<TProtocol> eventIter, ThriftType.StructType thriftStructType) throws TException
{    TStruct tStruct = acceptProtocol(eventIter.next()).readStructBegin();    List<ThriftField> childrenFields = thriftStructType.getChildren();    Set<Short> includedFieldsIds = new HashSet<Short>();    while (true) {        TProtocol next = eventIter.next();        TField field = next.readFieldBegin();        if (isStopField(field))            break;        acceptProtocol(next);        includedFieldsIds.add(field.id);        ThriftField fieldDefinition = thriftStructType.getChildById(field.id);        checkField(field.type, eventIter, fieldDefinition);        acceptProtocol(eventIter.next()).readFieldEnd();    }    for (ThriftField requiredField : childrenFields) {        if (!isRequired(requiredField)) {            continue;        }        if (!includedFieldsIds.contains(requiredField.getFieldId())) {            fixedEvents.addAll(new DefaultProtocolEventsGenerator().createProtocolEventsForField(requiredField));        }    }    acceptProtocol(DefaultProtocolEventsGenerator.READ_FIELD_STOP);    acceptProtocol(eventIter.next()).readStructEnd();}
0
private void checkField(byte type, Iterator<TProtocol> eventIter, ThriftField fieldDefinition) throws TException
{    switch(type) {        case TType.STRUCT:            checkStruct(eventIter, (ThriftType.StructType) fieldDefinition.getType());            return;        case TType.LIST:            checkList(eventIter, fieldDefinition);            return;        case TType.MAP:            checkMap(eventIter, fieldDefinition);            return;        case TType.SET:            checkSet(eventIter, fieldDefinition);            return;    }    checkPrimitiveField(type, eventIter);}
0
private void checkSet(Iterator<TProtocol> eventIter, ThriftField setFieldDefinition) throws TException
{    TSet thriftSet = acceptProtocol(eventIter.next()).readSetBegin();    ThriftField elementFieldDefinition = ((ThriftType.SetType) setFieldDefinition.getType()).getValues();    int setSize = thriftSet.size;    for (int i = 0; i < setSize; i++) {        checkField(thriftSet.elemType, eventIter, elementFieldDefinition);    }    acceptProtocol(eventIter.next()).readSetEnd();}
0
private void checkMap(Iterator<TProtocol> eventIter, ThriftField mapFieldForWriting) throws TException
{    TMap thriftMap = acceptProtocol(eventIter.next()).readMapBegin();    ThriftField keyFieldForWriting = ((ThriftType.MapType) mapFieldForWriting.getType()).getKey();    ThriftField valueFieldForWriting = ((ThriftType.MapType) mapFieldForWriting.getType()).getValue();    int mapSize = thriftMap.size;    for (int i = 0; i < mapSize; i++) {                checkField(thriftMap.keyType, eventIter, keyFieldForWriting);                checkField(thriftMap.valueType, eventIter, valueFieldForWriting);    }    acceptProtocol(eventIter.next()).readMapEnd();}
0
private void checkList(Iterator<TProtocol> eventIter, ThriftField listFieldUsedForWriting) throws TException
{    ThriftField valueFieldForWriting = ((ThriftType.ListType) listFieldUsedForWriting.getType()).getValues();    TList thriftList = acceptProtocol(eventIter.next()).readListBegin();    int listSize = thriftList.size;    for (int i = 0; i < listSize; i++) {        checkField(thriftList.elemType, eventIter, valueFieldForWriting);    }    acceptProtocol(eventIter.next()).readListEnd();}
0
private void checkPrimitiveField(byte type, Iterator<TProtocol> eventIter) throws TException
{    acceptProtocol(eventIter.next());}
0
private boolean isStopField(TField field)
{    return field.type == TType.STOP;}
0
private boolean isRequired(ThriftField requiredField)
{    return requiredField.getRequirement() == ThriftField.Requirement.REQUIRED;}
0
public TField readFieldBegin() throws TException
{    return new TField(field.getName(), thriftType, field.getFieldId());}
0
public boolean matches(String path)
{    if (this.pattern.matches(path)) {        this.hasMatchingPath = true;        return true;    } else {        return false;    }}
0
public boolean keep(FieldsPath path)
{    if (filterPatterns.size() == 0)        return true;    for (PathGlobPatternStatus pattern : filterPatterns) {        if (pattern.matches(path.toDelimitedString("/")))            return true;    }    return false;}
0
public void assertNoUnmatchedPatterns() throws ThriftProjectionException
{    List<PathGlobPattern> unmatched = new LinkedList<PathGlobPattern>();    for (PathGlobPatternStatus p : filterPatterns) {        if (!p.hasMatchingPath) {            unmatched.add(p.pattern);        }    }    if (!unmatched.isEmpty()) {        StringBuilder message = new StringBuilder("The following projection patterns did not match any columns in this schema:\n");        for (PathGlobPattern p : unmatched) {            message.append(p);            message.append('\n');        }        throw new ThriftProjectionException(message.toString());    }}
0
public static Pattern compile(String globPattern)
{    return new GlobPattern(globPattern).compiled();}
0
private static void error(String message, String pattern, int pos)
{    throw new PatternSyntaxException(message, pattern, pos);}
0
public Pattern compiled()
{    return compiled;}
0
public boolean matches(CharSequence s)
{    return compiled.matcher(s).matches();}
0
public void set(String glob)
{    StringBuilder regex = new StringBuilder();    int setOpen = 0;    int curlyOpen = 0;    int len = glob.length();    hasWildcard = false;    for (int i = 0; i < len; i++) {        char c = glob.charAt(i);        switch(c) {            case BACKSLASH:                if (++i >= len) {                    error("Missing escaped character", glob, i);                }                regex.append(c).append(glob.charAt(i));                continue;            case '.':            case '$':            case '(':            case ')':            case '|':            case '+':                                regex.append(BACKSLASH);                break;            case '*':                if (i + 1 < len && glob.charAt(i + 1) == '*') {                    regex.append('.');                    i++;                    break;                }                regex.append("[^" + PATH_SEPARATOR + "]");                hasWildcard = true;                break;            case '?':                regex.append('.');                hasWildcard = true;                continue;            case             '{':                                regex.append("(?:");                curlyOpen++;                hasWildcard = true;                continue;            case ',':                regex.append(curlyOpen > 0 ? '|' : c);                continue;            case '}':                if (curlyOpen > 0) {                                        curlyOpen--;                    regex.append(")");                    continue;                }                break;            case '[':                if (setOpen > 0) {                    error("Unclosed character class", glob, i);                }                setOpen++;                hasWildcard = true;                break;            case             '^':                if (setOpen == 0) {                    regex.append(BACKSLASH);                }                break;            case             '!':                regex.append(setOpen > 0 && '[' == glob.charAt(i - 1) ? '^' : '!');                continue;            case ']':                                                                setOpen = 0;                break;            default:        }        regex.append(c);    }    if (setOpen > 0) {        error("Unclosed character class", glob, len);    }    if (curlyOpen > 0) {        error("Unclosed group", glob, len);    }    compiled = Pattern.compile(regex.toString());}
0
public String toString()
{    return compiled.toString();}
0
public boolean hasWildcard()
{    return hasWildcard;}
0
public boolean keep(FieldsPath path)
{    return true;}
0
public void assertNoUnmatchedPatterns() throws ThriftProjectionException
{}
0
public FieldsPath push(ThriftField f)
{    ArrayList<ThriftField> copy = new ArrayList<ThriftField>(fields);    copy.add(f);    return new FieldsPath(copy);}
0
public String toDelimitedString(String delim)
{    StringBuilder delimited = new StringBuilder();    for (int i = 0; i < fields.size(); i++) {        ThriftField currentField = fields.get(i);        if (i > 0) {            ThriftField previousField = fields.get(i - 1);            if (FieldsPath.isKeyFieldOfMap(currentField, previousField)) {                delimited.append("key");                delimited.append(delim);                continue;            } else if (FieldsPath.isValueFieldOfMap(currentField, previousField)) {                delimited.append("value");                delimited.append(delim);                continue;            }        }        delimited.append(currentField.getName()).append(delim);    }    if (delimited.length() == 0) {        return "";    } else {        return delimited.substring(0, delimited.length() - 1);    }}
0
public String toString()
{    return toDelimitedString(".");}
0
private static boolean isValueFieldOfMap(ThriftField currentField, ThriftField previousField)
{    ThriftType previousType = previousField.getType();    return previousType instanceof ThriftType.MapType && ((ThriftType.MapType) previousType).getValue() == currentField;}
0
private static boolean isKeyFieldOfMap(ThriftField currentField, ThriftField previousField)
{    ThriftType previousType = previousField.getType();    return previousType instanceof ThriftType.MapType && ((ThriftType.MapType) previousType).getKey() == currentField;}
0
 static List<String> parseSemicolonDelimitedString(String columnsToKeepGlobs)
{    String[] splits = columnsToKeepGlobs.split(GLOB_SEPARATOR);    List<String> globs = new ArrayList<String>();    for (String s : splits) {        if (!s.isEmpty()) {            globs.add(s);        }    }    if (globs.isEmpty()) {        throw new ThriftProjectionException(String.format("Semicolon delimited string '%s' contains 0 glob strings", columnsToKeepGlobs));    }    return globs;}
0
public static StrictFieldProjectionFilter fromSemicolonDelimitedString(String columnsToKeepGlobs)
{    return new StrictFieldProjectionFilter(parseSemicolonDelimitedString(columnsToKeepGlobs));}
0
public boolean keep(FieldsPath path)
{    return keep(path.toDelimitedString("."));}
0
 boolean keep(String path)
{    WildcardPath match = null;        for (WildcardPathStatus wp : columnsToKeep) {        if (wp.matches(path)) {            if (match != null && !match.getParentGlobPath().equals(wp.getWildcardPath().getParentGlobPath())) {                String message = "Field path: '%s' matched more than one glob path pattern. First match: " + "'%s' (when expanded to '%s') second match:'%s' (when expanded to '%s')";                warn(String.format(message, path, match.getParentGlobPath(), match.getOriginalPattern(), wp.getWildcardPath().getParentGlobPath(), wp.getWildcardPath().getOriginalPattern()));            } else {                match = wp.getWildcardPath();            }        }    }    return match != null;}
0
protected void warn(String warning)
{    }
1
private List<WildcardPath> getUnmatchedPatterns()
{    List<WildcardPath> unmatched = new ArrayList<WildcardPath>();    for (WildcardPathStatus wp : columnsToKeep) {        if (!wp.hasMatched()) {            unmatched.add(wp.getWildcardPath());        }    }    return unmatched;}
0
public void assertNoUnmatchedPatterns() throws ThriftProjectionException
{    List<WildcardPath> unmatched = getUnmatchedPatterns();    if (!unmatched.isEmpty()) {        StringBuilder message = new StringBuilder("The following projection patterns did not match any columns in this schema:\n");        for (WildcardPath wp : unmatched) {            message.append(String.format("Pattern: '%s' (when expanded to '%s')", wp.getParentGlobPath(), wp.getOriginalPattern()));            message.append('\n');        }        throw new ThriftProjectionException(message.toString());    }}
0
public boolean matches(String path)
{    boolean matches = wildcardPath.matches(path);    this.hasMatched = hasMatched || matches;    return matches;}
0
public WildcardPath getWildcardPath()
{    return wildcardPath;}
0
public boolean hasMatched()
{    return hasMatched;}
0
public void readOne(TProtocol in, TProtocol out) throws TException
{    readOneStruct(in, out);}
0
 void readOneValue(TProtocol in, TProtocol out, byte type) throws TException
{    switch(type) {        case TType.LIST:            readOneList(in, out);            break;        case TType.MAP:            readOneMap(in, out);            break;        case TType.SET:            readOneSet(in, out);            break;        case TType.STRUCT:            readOneStruct(in, out);            break;        case TType.STOP:            break;        case TType.BOOL:            out.writeBool(in.readBool());            break;        case TType.BYTE:            out.writeByte(in.readByte());            break;        case TType.DOUBLE:            out.writeDouble(in.readDouble());            break;        case TType.I16:            out.writeI16(in.readI16());            break;                case TType.ENUM:        case TType.I32:            out.writeI32(in.readI32());            break;        case TType.I64:            out.writeI64(in.readI64());            break;        case TType.STRING:            out.writeBinary(in.readBinary());            break;        case TType.VOID:            break;        default:            throw new TException("Unknown type: " + type);    }}
0
private void readOneStruct(TProtocol in, TProtocol out) throws TException
{    final TStruct struct = in.readStructBegin();    out.writeStructBegin(struct);    TField field;    while ((field = in.readFieldBegin()).type != TType.STOP) {        out.writeFieldBegin(field);        readOneValue(in, out, field.type);        in.readFieldEnd();        out.writeFieldEnd();    }    out.writeFieldStop();    in.readStructEnd();    out.writeStructEnd();}
0
private void readOneMap(TProtocol in, TProtocol out) throws TException
{    final TMap map = in.readMapBegin();    out.writeMapBegin(map);    for (int i = 0; i < map.size; i++) {        readOneValue(in, out, map.keyType);        readOneValue(in, out, map.valueType);    }    in.readMapEnd();    out.writeMapEnd();}
0
private void readOneSet(TProtocol in, TProtocol out) throws TException
{    final TSet set = in.readSetBegin();    out.writeSetBegin(set);    readCollectionElements(in, out, set.size, set.elemType);    in.readSetEnd();    out.writeSetEnd();}
0
private void readOneList(TProtocol in, TProtocol out) throws TException
{    final TList list = in.readListBegin();    out.writeListBegin(list);    readCollectionElements(in, out, list.size, list.elemType);    in.readListEnd();    out.writeListEnd();}
0
private void readCollectionElements(TProtocol in, TProtocol out, final int size, final byte elemType) throws TException
{    for (int i = 0; i < size; i++) {        readOneValue(in, out, elemType);    }}
0
public CompatibilityReport checkCompatibility(ThriftType.StructType oldStruct, ThriftType.StructType newStruct)
{    CompatibleCheckerVisitor visitor = new CompatibleCheckerVisitor();    newStruct.accept(visitor, new State(oldStruct, new FieldsPath()));    return visitor.getReport();}
0
public boolean isCompatible()
{    return isCompatible;}
0
public boolean hasEmptyStruct()
{    return hasEmptyStruct;}
0
public void fail(String message)
{    messages.add(message);    isCompatible = false;}
0
public void emptyStruct(String message)
{    messages.add(message);    hasEmptyStruct = true;}
0
public List<String> getMessages()
{    return messages;}
0
public String prettyMessages()
{    return Strings.join(messages, "\n");}
0
public String toString()
{    return "CompatibilityReport{" + "isCompatible=" + isCompatible + ", hasEmptyStruct=" + hasEmptyStruct + ", messages=\n" + prettyMessages() + '}';}
0
public CompatibilityReport getReport()
{    return report;}
0
public Void visit(ThriftType.MapType mapType, State state)
{    ThriftType.MapType oldMapType = ((ThriftType.MapType) state.oldType);    ThriftField oldKeyField = oldMapType.getKey();    ThriftField newKeyField = mapType.getKey();    ThriftField newValueField = mapType.getValue();    ThriftField oldValueField = oldMapType.getValue();    checkField(oldKeyField, newKeyField, state.path);    checkField(oldValueField, newValueField, state.path);    return null;}
0
public Void visit(ThriftType.SetType setType, State state)
{    ThriftType.SetType oldSetType = ((ThriftType.SetType) state.oldType);    ThriftField oldField = oldSetType.getValues();    ThriftField newField = setType.getValues();    checkField(oldField, newField, state.path);    return null;}
0
public Void visit(ThriftType.ListType listType, State state)
{    ThriftType.ListType currentOldType = ((ThriftType.ListType) state.oldType);    ThriftField oldField = currentOldType.getValues();    ThriftField newField = listType.getValues();    checkField(oldField, newField, state.path);    return null;}
0
public void incompatible(String message, FieldsPath path)
{    report.fail("at " + path + ":" + message);}
0
private void checkField(ThriftField oldField, ThriftField newField, FieldsPath path)
{    if (!newField.getType().getType().equals(oldField.getType().getType())) {        incompatible("type is not compatible: " + oldField.getType().getType() + " vs " + newField.getType().getType(), path);        return;    }    if (!newField.getName().equals(oldField.getName())) {        incompatible("field names are different: " + oldField.getName() + " vs " + newField.getName(), path);        return;    }    if (firstIsMoreRestirctive(newField.getRequirement(), oldField.getRequirement())) {        incompatible("new field is more restrictive: " + newField.getName(), path);        return;    }    newField.getType().accept(this, new State(oldField.getType(), path.push(newField)));}
0
private boolean firstIsMoreRestirctive(ThriftField.Requirement firstReq, ThriftField.Requirement secReq)
{    if (firstReq == ThriftField.Requirement.REQUIRED && secReq != ThriftField.Requirement.REQUIRED) {        return true;    } else {        return false;    }}
0
public Void visit(ThriftType.StructType newStruct, State state)
{    ThriftType.StructType oldStructType = ((ThriftType.StructType) state.oldType);    short oldMaxId = 0;    if (newStruct.getChildren().isEmpty()) {        report.emptyStruct("encountered an empty struct: " + state.path);    }    for (ThriftField oldField : oldStructType.getChildren()) {        short fieldId = oldField.getFieldId();        if (fieldId > oldMaxId) {            oldMaxId = fieldId;        }        ThriftField newField = newStruct.getChildById(fieldId);        if (newField == null) {            incompatible("can not find index in new Struct: " + fieldId + " in " + newStruct, state.path);            return null;        }        checkField(oldField, newField, state.path);    }        for (ThriftField newField : newStruct.getChildren()) {                if (newField.getRequirement() != ThriftField.Requirement.REQUIRED)                        continue;        short newFieldId = newField.getFieldId();        if (newFieldId > oldMaxId) {            incompatible("new required field " + newField.getName() + " is added", state.path);            return null;        }        if (newFieldId < oldMaxId && oldStructType.getChildById(newFieldId) == null) {            incompatible("new required field " + newField.getName() + " is added", state.path);            return null;        }    }    return null;}
0
public Void visit(EnumType enumType, State state)
{    return null;}
0
public Void visit(BoolType boolType, State state)
{    return null;}
0
public Void visit(ByteType byteType, State state)
{    return null;}
0
public Void visit(DoubleType doubleType, State state)
{    return null;}
0
public Void visit(I16Type i16Type, State state)
{    return null;}
0
public Void visit(I32Type i32Type, State state)
{    return null;}
0
public Void visit(I64Type i64Type, State state)
{    return null;}
0
public Void visit(StringType stringType, State state)
{    return null;}
0
public static void main(String[] args) throws Exception
{    LinkedList<String> arguments = new LinkedList<String>(Arrays.asList(args));    String operator = arguments.pollFirst();    if (operator.equals("generate-json")) {                generateJson(arguments);    }    if (operator.equals("compare-json")) {        compareJson(arguments);    }}
0
private static void compareJson(LinkedList<String> arguments) throws IOException
{    String oldJsonPath = arguments.pollFirst();    String newJsonPath = arguments.pollFirst();    File oldJsonFile = new File(oldJsonPath);    checkExist(oldJsonFile);    File newJsonFile = new File(newJsonPath);    checkExist(newJsonFile);    ObjectMapper mapper = new ObjectMapper();    ThriftType.StructType oldStruct = mapper.readValue(oldJsonFile, ThriftType.StructType.class);    ThriftType.StructType newStruct = mapper.readValue(newJsonFile, ThriftType.StructType.class);    CompatibilityReport report = new CompatibilityChecker().checkCompatibility(oldStruct, newStruct);    if (!report.isCompatible) {        System.err.println("schema not compatible");        System.err.println(report.getMessages());        System.exit(1);    }    if (report.hasEmptyStruct()) {        System.err.println("schema contains empty struct");        System.err.println(report.getMessages());        System.exit(1);    }    System.out.println("[success] schema is compatible");}
0
private static void checkExist(File f)
{    if (!f.exists())        throw new RuntimeException("can not find file " + f);}
0
private static void generateJson(LinkedList<String> arguments) throws ClassNotFoundException, IOException
{    String catName = arguments.pollFirst();    String className = arguments.pollFirst();    String storedPath = arguments.pollFirst();    File storeDir = new File(storedPath);    ThriftType.StructType structType = ThriftSchemaConverter.toStructType((Class<? extends TBase<?, ?>>) Class.forName(className));    ObjectMapper mapper = new ObjectMapper();    String fileName = catName + ".json";    mapper.writerWithDefaultPrettyPrinter().writeValue(new File(storeDir, fileName), structType);}
0
 static T fromJSON(String json, Class<T> clzz)
{    try {        return om.readValue(json, clzz);    } catch (IOException e) {        throw new RuntimeException(e);    }}
0
 static String toJSON(Object o)
{    try (final StringWriter sw = new StringWriter()) {        om.writeValue(sw, o);        return sw.toString();    } catch (IOException e) {        throw new RuntimeException(e);    }}
0
public byte getRequirement()
{    return requirement;}
0
public static Requirement fromType(byte fieldRequirementType)
{    for (Requirement req : Requirement.values()) {        if (req.requirement == fieldRequirementType) {            return req;        }    }    throw new RuntimeException("Unknown requirement " + fieldRequirementType);}
0
public String getName()
{    return name;}
0
public short getFieldId()
{    return fieldId;}
0
public ThriftType getType()
{    return type;}
0
public Requirement getRequirement()
{    return requirement;}
0
public String toString()
{    return JSON.toJSON(this);}
0
public boolean equals(Object o)
{    if (this == o)        return true;    if (!(o instanceof ThriftField))        return false;    ThriftField that = (ThriftField) o;    if (fieldId != that.fieldId)        return false;    if (!name.equals(that.name))        return false;    if (requirement != that.requirement)        return false;    if (!type.equals(that.type))        return false;    return true;}
0
public int hashCode()
{    int result = name.hashCode();    result = 31 * result + (int) fieldId;    result = 31 * result + requirement.hashCode();    result = 31 * result + type.hashCode();    return result;}
0
public boolean equals(Object o)
{    if (this == o)        return true;    if (!(o instanceof ThriftType))        return false;    ThriftType that = (ThriftType) o;    if (type != that.type)        return false;    return true;}
0
public int hashCode()
{    return type != null ? type.hashCode() : 0;}
0
public static ThriftType fromJSON(String json)
{    return JSON.fromJSON(json, ThriftType.class);}
0
public String toJSON()
{    return JSON.toJSON(this);}
0
public String toString()
{    return toJSON();}
0
public final void visit(EnumType enumType)
{    throw new IllegalArgumentException("Expected complex type");}
0
public final void visit(BoolType boolType)
{    throw new IllegalArgumentException("Expected complex type");}
0
public final void visit(ByteType byteType)
{    throw new IllegalArgumentException("Expected complex type");}
0
public final void visit(DoubleType doubleType)
{    throw new IllegalArgumentException("Expected complex type");}
0
public final void visit(I16Type i16Type)
{    throw new IllegalArgumentException("Expected complex type");}
0
public final void visit(I32Type i32Type)
{    throw new IllegalArgumentException("Expected complex type");}
0
public final void visit(I64Type i64Type)
{    throw new IllegalArgumentException("Expected complex type");}
0
public final void visit(StringType stringType)
{    throw new IllegalArgumentException("Expected complex type");}
0
public List<ThriftField> getChildren()
{    return children;}
0
public ThriftField getChildById(short id)
{    if (id >= childById.length) {        return null;    } else {        return childById[id];    }}
0
public StructOrUnionType getStructOrUnionType()
{    return structOrUnionType;}
0
public R accept(StateVisitor<R, S> visitor, S state)
{    return visitor.visit(this, state);}
0
public void accept(TypeVisitor visitor)
{    visitor.visit(this);}
0
public boolean equals(Object o)
{    if (this == o)        return true;    if (o == null || getClass() != o.getClass())        return false;    StructType that = (StructType) o;    if (!Arrays.equals(childById, that.childById))        return false;    return true;}
0
public int hashCode()
{    int result = childById != null ? Arrays.hashCode(childById) : 0;    return result;}
0
public ThriftField getKey()
{    return key;}
0
public ThriftField getValue()
{    return value;}
0
public R accept(StateVisitor<R, S> visitor, S state)
{    return visitor.visit(this, state);}
0
public void accept(TypeVisitor visitor)
{    visitor.visit(this);}
0
public boolean equals(Object o)
{    if (this == o)        return true;    if (!(o instanceof MapType))        return false;    if (!super.equals(o))        return false;    MapType mapType = (MapType) o;    if (!key.equals(mapType.key))        return false;    if (!value.equals(mapType.value))        return false;    return true;}
0
public int hashCode()
{    int result = super.hashCode();    result = 31 * result + key.hashCode();    result = 31 * result + value.hashCode();    return result;}
0
public ThriftField getValues()
{    return values;}
0
public R accept(StateVisitor<R, S> visitor, S state)
{    return visitor.visit(this, state);}
0
public void accept(TypeVisitor visitor)
{    visitor.visit(this);}
0
public boolean equals(Object o)
{    if (this == o)        return true;    if (!(o instanceof SetType))        return false;    if (!super.equals(o))        return false;    SetType setType = (SetType) o;    if (!values.equals(setType.values))        return false;    return true;}
0
public int hashCode()
{    int result = super.hashCode();    result = 31 * result + values.hashCode();    return result;}
0
public ThriftField getValues()
{    return values;}
0
public R accept(StateVisitor<R, S> visitor, S state)
{    return visitor.visit(this, state);}
0
public void accept(TypeVisitor visitor)
{    visitor.visit(this);}
0
public boolean equals(Object o)
{    if (this == o)        return true;    if (!(o instanceof ListType))        return false;    if (!super.equals(o))        return false;    ListType listType = (ListType) o;    if (!values.equals(listType.values))        return false;    return true;}
0
public int hashCode()
{    int result = super.hashCode();    result = 31 * result + values.hashCode();    return result;}
0
public int getId()
{    return id;}
0
public String getName()
{    return name;}
0
public boolean equals(Object o)
{    if (this == o)        return true;    if (!(o instanceof EnumValue))        return false;    EnumValue enumValue = (EnumValue) o;    if (id != enumValue.id)        return false;    if (name != null ? !name.equals(enumValue.name) : enumValue.name != null)        return false;    return true;}
0
public int hashCode()
{    int result = id;    result = 31 * result + (name != null ? name.hashCode() : 0);    return result;}
0
public Iterable<EnumValue> getValues()
{    return new Iterable<EnumValue>() {        @Override        public Iterator<EnumValue> iterator() {            return values.iterator();        }    };}
0
public Iterator<EnumValue> iterator()
{    return values.iterator();}
0
public EnumValue getEnumValueById(int id)
{    prepareEnumLookUp();    return idEnumLookup.get(id);}
0
private void prepareEnumLookUp()
{    if (idEnumLookup == null) {        idEnumLookup = new HashMap<Integer, EnumValue>();        for (EnumValue value : values) {            idEnumLookup.put(value.getId(), value);        }    }}
0
public R accept(StateVisitor<R, S> visitor, S state)
{    return visitor.visit(this, state);}
0
public void accept(TypeVisitor visitor)
{    visitor.visit(this);}
0
public boolean equals(Object o)
{    if (this == o)        return true;    if (!(o instanceof EnumType))        return false;    if (!super.equals(o))        return false;    EnumType enumType = (EnumType) o;    if (!values.equals(enumType.values))        return false;    return true;}
0
public int hashCode()
{    int result = super.hashCode();    result = 31 * result + values.hashCode();    return result;}
0
public R accept(StateVisitor<R, S> visitor, S state)
{    return visitor.visit(this, state);}
0
public void accept(TypeVisitor visitor)
{    visitor.visit(this);}
0
public R accept(StateVisitor<R, S> visitor, S state)
{    return visitor.visit(this, state);}
0
public void accept(TypeVisitor visitor)
{    visitor.visit(this);}
0
public R accept(StateVisitor<R, S> visitor, S state)
{    return visitor.visit(this, state);}
0
public void accept(TypeVisitor visitor)
{    visitor.visit(this);}
0
public R accept(StateVisitor<R, S> visitor, S state)
{    return visitor.visit(this, state);}
0
public void accept(TypeVisitor visitor)
{    visitor.visit(this);}
0
public R accept(StateVisitor<R, S> visitor, S state)
{    return visitor.visit(this, state);}
0
public void accept(TypeVisitor visitor)
{    visitor.visit(this);}
0
public R accept(StateVisitor<R, S> visitor, S state)
{    return visitor.visit(this, state);}
0
public void accept(TypeVisitor visitor)
{    visitor.visit(this);}
0
public boolean isBinary()
{    return binary;}
0
public void setBinary(boolean binary)
{    this.binary = binary;}
0
public R accept(StateVisitor<R, S> visitor, S state)
{    return visitor.visit(this, state);}
0
public void accept(TypeVisitor visitor)
{    visitor.visit(this);}
0
public ThriftTypeID getType()
{    return this.type;}
0
public byte getThriftType()
{    return thriftType;}
0
public boolean isComplex()
{    return complex;}
0
public Class<? extends ThriftType> getType()
{    return clss;}
0
public static ThriftTypeID fromByte(byte type)
{    return types[type];}
0
public byte getSerializedThriftType()
{    return serializedThriftType;}
0
public T readOneRecord(TProtocol protocol) throws TException
{    try {        T thriftObject = thriftClass.newInstance();        thriftObject.read(protocol);        return thriftObject;    } catch (InstantiationException e) {        throw new ParquetDecodingException("Could not instantiate Thrift " + thriftClass, e);    } catch (IllegalAccessException e) {        throw new ParquetDecodingException("Thrift class or constructor not public " + thriftClass, e);    }}
0
public Class<?> getThriftClass()
{    if (thriftClass == null) {        thriftClass = getThriftClass(thriftClassName);    }    return thriftClass;}
0
public static Class<?> getThriftClass(String thriftClassName)
{    try {        Class<?> thriftClass = Class.forName(thriftClassName);        return thriftClass;    } catch (ClassNotFoundException e) {        throw new BadConfigurationException("Could not instantiate thrift class " + thriftClassName, e);    }}
0
public StructType getDescriptor()
{    return descriptor;}
0
public static ThriftMetaData fromExtraMetaData(Map<String, String> extraMetaData)
{    final String thriftClassName = extraMetaData.get(THRIFT_CLASS);    final String thriftDescriptorString = extraMetaData.get(THRIFT_DESCRIPTOR);    if (thriftClassName == null || thriftDescriptorString == null) {        return null;    }    final StructType descriptor = parseDescriptor(thriftDescriptorString);    return new ThriftMetaData(thriftClassName, descriptor);}
0
public static ThriftMetaData fromThriftClass(Class<?> thriftClass)
{    if (thriftClass != null && TBase.class.isAssignableFrom(thriftClass)) {        Class<? extends TBase<?, ?>> tClass = (Class<? extends TBase<?, ?>>) thriftClass;        StructType descriptor = new ThriftSchemaConverter().toStructType(tClass);        return new ThriftMetaData(thriftClass.getName(), descriptor);    }    return null;}
0
private static StructType parseDescriptor(String json)
{    try {        return (StructType) ThriftType.fromJSON(json);    } catch (RuntimeException e) {        throw new BadConfigurationException("Could not read the thrift descriptor " + json, e);    }}
0
public Map<String, String> toExtraMetaData()
{    final Map<String, String> map = new HashMap<String, String>();    map.put(THRIFT_CLASS, getThriftClass().getName());    map.put(THRIFT_DESCRIPTOR, descriptor.toJSON());    return map;}
0
public static Set<String> getThriftClassNames(Map<String, Set<String>> fileMetadata)
{    return fileMetadata.get(THRIFT_CLASS);}
0
public String toString()
{    return String.format("ThriftMetaData(thriftClassName: %s, descriptor: %s)", thriftClassName, descriptor);}
0
public static Builder<T> build(Path file)
{    return new Builder<T>(file);}
0
public Builder<T> withConf(Configuration conf)
{    this.conf = checkNotNull(conf, "conf");    return this;}
0
public Builder<T> withFilter(Filter filter)
{    this.filter = checkNotNull(filter, "filter");    return this;}
0
public Builder<T> withThriftClass(Class<T> thriftClass)
{    this.thriftClass = checkNotNull(thriftClass, "thriftClass");    return this;}
0
public ParquetReader<T> build() throws IOException
{    ReadSupport<T> readSupport;    if (thriftClass != null) {        readSupport = new ThriftReadSupport<T>(thriftClass);    } else {        readSupport = new ThriftReadSupport<T>();    }    return ParquetReader.builder(readSupport, file).withConf(conf).withFilter(filter).build();}
0
public void readFieldEnd() throws TException
{}
0
private void startField()
{    events.add(readFieldBegin);}
0
private void endField()
{    events.add(readFieldEnd);}
0
public TField readFieldBegin() throws TException
{    return new TField(field.getName(), thriftType, field.getFieldId());}
0
public void addBinary(Binary value)
{    startField();    delegate.addBinary(value);    endField();}
0
public void addBoolean(boolean value)
{    startField();    delegate.addBoolean(value);    endField();}
0
public void addDouble(double value)
{    startField();    delegate.addDouble(value);    endField();}
0
public void addFloat(float value)
{    startField();    delegate.addFloat(value);    endField();}
0
public void addInt(int value)
{    startField();    delegate.addInt(value);    endField();}
0
public void addLong(long value)
{    startField();    delegate.addLong(value);    endField();}
0
public TField readFieldBegin() throws TException
{    return new TField(field.getName(), field.getType().getType().getThriftType(), field.getFieldId());}
0
public Converter getConverter(int fieldIndex)
{    return delegate.getConverter(fieldIndex);}
0
public void start()
{    events.add(readFieldBegin);    delegate.start();}
0
public void end()
{    delegate.end();    events.add(readFieldEnd);}
0
public Converter getConverter(int fieldIndex)
{    return delegate.getConverter(fieldIndex);}
0
public void start()
{    delegate.start();}
0
public void end()
{    delegate.end();    ++count;}
0
public void startCounting()
{    count = 0;}
0
public int getCount()
{    return count;}
0
public void addBinary(Binary value)
{    delegate.addBinary(value);    ++count;}
0
public void addBoolean(boolean value)
{    delegate.addBoolean(value);    ++count;}
0
public void addDouble(double value)
{    delegate.addDouble(value);    ++count;}
0
public void addFloat(float value)
{    delegate.addFloat(value);    ++count;}
0
public void addInt(int value)
{    delegate.addInt(value);    ++count;}
0
public void addLong(long value)
{    delegate.addLong(value);    ++count;}
0
public void startCounting()
{    count = 0;}
0
public int getCount()
{    return count;}
0
public void addBoolean(final boolean value)
{    events.add(new ParquetProtocol("readBool()") {        @Override        public boolean readBool() throws TException {            return value;        }    });}
0
public boolean readBool() throws TException
{    return value;}
0
public void addDouble(final double value)
{    events.add(new ParquetProtocol("readDouble()") {        @Override        public double readDouble() throws TException {            return value;        }    });}
0
public double readDouble() throws TException
{    return value;}
0
public void addFloat(final float value)
{        events.add(new ParquetProtocol("readDouble() float") {        @Override        public double readDouble() throws TException {            return value;        }    });}
0
public double readDouble() throws TException
{    return value;}
0
public void addInt(final int value)
{        switch(type) {        case BYTE:            events.add(new ParquetProtocol("readByte() int") {                @Override                public byte readByte() throws TException {                    return (byte) value;                }            });            break;        case I16:            events.add(new ParquetProtocol("readI16()") {                @Override                public short readI16() throws TException {                    return (short) value;                }            });            break;        case I32:            events.add(new ParquetProtocol("readI32()") {                @Override                public int readI32() throws TException {                    return value;                }            });            break;        default:            throw new UnsupportedOperationException("not convertible type " + type);    }}
0
public byte readByte() throws TException
{    return (byte) value;}
0
public short readI16() throws TException
{    return (short) value;}
0
public int readI32() throws TException
{    return value;}
0
public void addLong(final long value)
{    events.add(new ParquetProtocol("readI64()") {        @Override        public long readI64() throws TException {            return value;        }    });}
0
public long readI64() throws TException
{    return value;}
0
public void addBinary(final Binary value)
{    events.add(new ParquetProtocol("readString() binary") {        @Override        public String readString() throws TException {            return value.toStringUsingUTF8();        }        @Override        public ByteBuffer readBinary() throws TException {            return value.toByteBuffer();        }    });}
0
public String readString() throws TException
{    return value.toStringUsingUTF8();}
0
public ByteBuffer readBinary() throws TException
{    return value.toByteBuffer();}
0
public void addBinary(final Binary value)
{    final Integer id = enumLookup.get(value);    if (id == null) {        throw new ParquetDecodingException("Unrecognized enum value: " + value.toStringUsingUTF8() + " known values: " + enumLookup + " in " + this.field);    }    events.add(new ParquetProtocol("readI32() enum") {        @Override        public int readI32() throws TException {            return id;        }    });}
0
public int readI32() throws TException
{    return id;}
0
public Converter getConverter(int fieldIndex)
{    if (fieldIndex != 0) {        throw new IllegalArgumentException("lists have only one field. can't reach " + fieldIndex);    }    return child;}
0
public void start()
{    child.startCounting();}
0
public void readMapEnd() throws TException
{}
0
public void end()
{    final int count = child.getCount();    parentEvents.add(new ParquetProtocol("readMapBegin()") {        @Override        public TMap readMapBegin() throws TException {            return new TMap(keyType, valueType, count);        }    });    parentEvents.addAll(mapEvents);    mapEvents.clear();    parentEvents.add(readMapEnd);}
0
public TMap readMapBegin() throws TException
{    return new TMap(keyType, valueType, count);}
0
public Converter getConverter(int fieldIndex)
{    switch(fieldIndex) {        case 0:            return keyConverter;        case 1:            return valueConverter;        default:            throw new IllegalArgumentException("only key (0) and value (1) are supported. got " + fieldIndex);    }}
0
public void start()
{}
0
public void end()
{}
0
public void readSetEnd() throws TException
{}
0
 void collectionStart(final int count, final byte type)
{    parentEvents.add(new ParquetProtocol("readSetBegin()") {        @Override        public TSet readSetBegin() throws TException {            return new TSet(type, count);        }    });}
0
public TSet readSetBegin() throws TException
{    return new TSet(type, count);}
0
 void collectionEnd()
{    parentEvents.add(readSetEnd);}
0
public void readListEnd() throws TException
{}
0
 void collectionStart(final int count, final byte type)
{    parentEvents.add(new ParquetProtocol("readListBegin()") {        @Override        public TList readListBegin() throws TException {            return new TList(type, count);        }    });}
0
public TList readListBegin() throws TException
{    return new TList(type, count);}
0
 void collectionEnd()
{    parentEvents.add(readListEnd);}
0
public Converter getConverter(int fieldIndex)
{    if (fieldIndex != 0) {        throw new IllegalArgumentException("lists have only one field. can't reach " + fieldIndex);    }    return child;}
0
public void start()
{    childCounter.startCounting();}
0
public void end()
{    int count = childCounter.getCount();    if (elementConverter != null) {        count -= elementConverter.getNullElementCount();    }    collectionStart(count, valuesType.getThriftType());    parentEvents.addAll(listEvents);    listEvents.clear();    collectionEnd();}
0
public Converter getConverter(int fieldIndex)
{    Preconditions.checkArgument(fieldIndex == 0, "Illegal field index: %s", fieldIndex);    return elementConverter;}
0
public void start()
{    elementEvents.clear();}
0
public void end()
{    if (elementEvents.size() > 0) {        listEvents.addAll(elementEvents);    } else {        nullElementCount += 1;    }}
0
public int getNullElementCount()
{    return nullElementCount;}
0
public Converter getConverter(int fieldIndex)
{    return converters[fieldIndex];}
0
public TStruct readStructBegin() throws TException
{    return tStruct;}
0
public void start()
{    events.add(readStructBegin);}
0
public TField readFieldBegin() throws TException
{    return stop;}
0
public void readStructEnd() throws TException
{}
0
public void end()
{    events.add(readFieldStop);    events.add(readStructEnd);}
0
private boolean hasMissingRequiredFieldInGroupType(GroupType requested, GroupType fullSchema)
{    for (Type field : fullSchema.getFields()) {        if (requested.containsField(field.getName())) {            Type requestedType = requested.getType(field.getName());                        if (!field.isPrimitive()) {                if (hasMissingRequiredFieldInGroupType(requestedType.asGroupType(), field.asGroupType())) {                    return true;                } else {                                        continue;                }            }        } else {            if (field.getRepetition() == Type.Repetition.REQUIRED) {                                return true;            } else {                                continue;            }        }    }    return false;}
0
public T getCurrentRecord()
{    try {        if (missingRequiredFieldsInProjection) {            List<TProtocol> fixedEvents = new ProtocolEventsAmender(rootEvents).amendMissingRequiredFields(thriftType);            protocol.addAll(fixedEvents);        } else {            protocol.addAll(rootEvents);        }        rootEvents.clear();        return thriftReader.readOneRecord(protocol);    } catch (TException e) {        protocol.clear();        rootEvents.clear();        throw new RecordMaterializationException("Could not read thrift object from protocol", e);    }}
0
public void skipCurrentRecord()
{    rootEvents.clear();}
0
public GroupConverter getRootConverter()
{    return structConverter;}
0
private Converter newConverter(List<TProtocol> events, Type type, ThriftField field)
{    switch(field.getType().getType()) {        case LIST:            return new ListConverter(events, type.asGroupType(), field);        case SET:            return new SetConverter(events, type.asGroupType(), field);        case MAP:            return new MapConverter(events, type.asGroupType(), field);        case STRUCT:            return new StructConverter(events, type.asGroupType(), field);        case STRING:            return new FieldStringConverter(events, field);        case ENUM:            return new FieldEnumConverter(events, field);        default:            return new FieldPrimitiveConverter(events, field);    }}
0
public MessageType convert(Class<? extends TBase<?, ?>> thriftClass)
{    return convert(toStructType(thriftClass));}
0
public MessageType convert(StructType struct)
{    MessageType messageType = ThriftSchemaConvertVisitor.convert(struct, fieldProjectionFilter, true);    fieldProjectionFilter.assertNoUnmatchedPatterns();    return messageType;}
0
public static MessageType convertWithoutProjection(StructType struct)
{    return ThriftSchemaConvertVisitor.convert(struct, FieldProjectionFilter.ALL_COLUMNS, false);}
0
public static StructOrUnionType structOrUnionType(Class<T> klass)
{    return TUnion.class.isAssignableFrom(klass) ? StructOrUnionType.UNION : StructOrUnionType.STRUCT;}
0
public static ThriftType.StructType toStructType(Class<? extends TBase<?, ?>> thriftClass)
{    final TStructDescriptor struct = TStructDescriptor.getInstance(thriftClass);    return toStructType(struct);}
0
private static StructType toStructType(TStructDescriptor struct)
{    List<Field> fields = struct.getFields();    List<ThriftField> children = new ArrayList<ThriftField>(fields.size());    for (Field field : fields) {        Requirement req = field.getFieldMetaData() == null ? Requirement.OPTIONAL : Requirement.fromType(field.getFieldMetaData().requirementType);        children.add(toThriftField(field.getName(), field, req));    }    return new StructType(children, structOrUnionType(struct.getThriftClass()));}
0
 static boolean isListElementType(Type repeatedType, ThriftField thriftElement)
{    if (repeatedType.isPrimitive() || (repeatedType.asGroupType().getFieldCount() != 1) || (repeatedType.asGroupType().getType(0).isRepetition(REPEATED))) {                return true;    } else if (thriftElement != null && thriftElement.getType() instanceof StructType) {        Set<String> fieldNames = new HashSet<String>();        for (ThriftField field : ((StructType) thriftElement.getType()).getChildren()) {            fieldNames.add(field.getName());        }                return fieldNames.contains(repeatedType.asGroupType().getFieldName(0));    }    return false;}
0
private static ThriftField toThriftField(String name, Field field, ThriftField.Requirement requirement)
{    ThriftType type;    switch(ThriftTypeID.fromByte(field.getType())) {        case STOP:        case VOID:        default:            throw new UnsupportedOperationException("can't convert type of " + field);        case BOOL:            type = new BoolType();            break;        case BYTE:            type = new ByteType();            break;        case DOUBLE:            type = new DoubleType();            break;        case I16:            type = new I16Type();            break;        case I32:            type = new I32Type();            break;        case I64:            type = new I64Type();            break;        case STRING:            StringType stringType = new StringType();            FieldMetaData fieldMetaData = field.getFieldMetaData();                        if (fieldMetaData != null && fieldMetaData.valueMetaData.isBinary()) {                stringType.setBinary(true);            }            type = stringType;            break;        case STRUCT:            type = toStructType(field.gettStructDescriptor());            break;        case MAP:            final Field mapKeyField = field.getMapKeyField();            final Field mapValueField = field.getMapValueField();            type = new ThriftType.MapType(toThriftField(mapKeyField.getName(), mapKeyField, requirement), toThriftField(mapValueField.getName(), mapValueField, requirement));            break;        case SET:            final Field setElemField = field.getSetElemField();            type = new ThriftType.SetType(toThriftField(setElemField.getName(), setElemField, requirement));            break;        case LIST:            final Field listElemField = field.getListElemField();            type = new ThriftType.ListType(toThriftField(listElemField.getName(), listElemField, requirement));            break;        case ENUM:            Collection<TEnum> enumValues = field.getEnumValues();            List<EnumValue> values = new ArrayList<ThriftType.EnumValue>();            for (TEnum tEnum : enumValues) {                values.add(new EnumValue(tEnum.getValue(), tEnum.toString()));            }            type = new EnumType(values);            break;    }    return new ThriftField(name, field.getId(), requirement, type);}
0
public static MessageType convert(StructType struct, FieldProjectionFilter filter)
{    return convert(struct, filter, true);}
0
public static MessageType convert(StructType struct, FieldProjectionFilter filter, boolean keepOneOfEachUnion)
{    State state = new State(new FieldsPath(), REPEATED, "ParquetSchema");    ConvertedField converted = struct.accept(new ThriftSchemaConvertVisitor(filter, true, keepOneOfEachUnion), state);    if (!converted.isKeep()) {        throw new ThriftProjectionException("No columns have been selected");    }    return new MessageType(state.name, converted.asKeep().getType().asGroupType().getFields());}
0
public FieldProjectionFilter getFieldProjectionFilter()
{    return fieldProjectionFilter;}
0
public ConvertedField visit(MapType mapType, State state)
{    ThriftField keyField = mapType.getKey();    ThriftField valueField = mapType.getValue();    State keyState = new State(state.path.push(keyField), REQUIRED, "key");                State valueState = new State(state.path.push(valueField), OPTIONAL, "value");    ConvertedField convertedKey = keyField.getType().accept(this, keyState);    ConvertedField convertedValue = valueField.getType().accept(this, valueState);    if (!convertedKey.isKeep()) {        if (convertedValue.isKeep()) {            throw new ThriftProjectionException("Cannot select only the values of a map, you must keep the keys as well: " + state.path);        }                return new Drop(state.path);    }        if (doProjection) {        ConvertedField fullConvKey = keyField.getType().accept(new ThriftSchemaConvertVisitor(FieldProjectionFilter.ALL_COLUMNS, false, keepOneOfEachUnion), keyState);        if (!fullConvKey.asKeep().getType().equals(convertedKey.asKeep().getType())) {            throw new ThriftProjectionException("Cannot select only a subset of the fields in a map key, " + "for path " + state.path);        }    }    if (convertedValue.isKeep()) {                Type mapField = mapType(state.repetition, state.name, convertedKey.asKeep().getType(), convertedValue.asKeep().getType());        return new Keep(state.path, mapField);    }        ConvertedField sentinelValue = valueField.getType().accept(new ThriftSchemaConvertVisitor(new KeepOnlyFirstPrimitiveFilter(), true, keepOneOfEachUnion), valueState);    Type mapField = mapType(state.repetition, state.name, convertedKey.asKeep().getType(),     sentinelValue.asKeep().getType());    return new Keep(state.path, mapField);}
0
private ConvertedField visitListLike(ThriftField listLike, State state, boolean isSet)
{    State childState = new State(state.path, REPEATED, state.name + "_tuple");    ConvertedField converted = listLike.getType().accept(this, childState);    if (converted.isKeep()) {                if (isSet && doProjection) {            ConvertedField fullConv = listLike.getType().accept(new ThriftSchemaConvertVisitor(FieldProjectionFilter.ALL_COLUMNS, false, keepOneOfEachUnion), childState);            if (!converted.asKeep().getType().equals(fullConv.asKeep().getType())) {                throw new ThriftProjectionException("Cannot select only a subset of the fields in a set, " + "for path " + state.path);            }        }        return new Keep(state.path, listType(state.repetition, state.name, converted.asKeep().getType()));    }    return new Drop(state.path);}
0
public ConvertedField visit(SetType setType, State state)
{    return visitListLike(setType.getValues(), state, true);}
0
public ConvertedField visit(ListType listType, State state)
{    return visitListLike(listType.getValues(), state, false);}
0
public ConvertedField visit(StructType structType, State state)
{                final boolean needsToKeepOneOfEachUnion = keepOneOfEachUnion && isUnion(structType.getStructOrUnionType());    boolean hasSentinelUnionColumns = false;    boolean hasNonSentinelUnionColumns = false;    List<Type> convertedChildren = new ArrayList<Type>();    for (ThriftField child : structType.getChildren()) {        State childState = new State(state.path.push(child), getRepetition(child), child.getName());        ConvertedField converted = child.getType().accept(this, childState);        if (!converted.isKeep() && needsToKeepOneOfEachUnion) {                                                                                    ConvertedField firstPrimitive = child.getType().accept(new ThriftSchemaConvertVisitor(new KeepOnlyFirstPrimitiveFilter(), true, keepOneOfEachUnion), childState);            convertedChildren.add(firstPrimitive.asKeep().getType().withId(child.getFieldId()));            hasSentinelUnionColumns = true;        }        if (converted.isSentinelUnion()) {                        if (childState.repetition == REQUIRED) {                                convertedChildren.add(converted.asSentinelUnion().getType().withId(child.getFieldId()));                hasSentinelUnionColumns = true;            }        } else if (converted.isKeep()) {                        convertedChildren.add(converted.asKeep().getType().withId(child.getFieldId()));            hasNonSentinelUnionColumns = true;        }    }    if (!hasNonSentinelUnionColumns && hasSentinelUnionColumns) {                return new SentinelUnion(state.path, new GroupType(state.repetition, state.name, convertedChildren));    }    if (hasNonSentinelUnionColumns) {                return new Keep(state.path, new GroupType(state.repetition, state.name, convertedChildren));    } else {                return new Drop(state.path);    }}
0
private ConvertedField visitPrimitiveType(PrimitiveTypeName type, State state)
{    return visitPrimitiveType(type, null, state);}
0
private ConvertedField visitPrimitiveType(PrimitiveTypeName type, LogicalTypeAnnotation orig, State state)
{    PrimitiveBuilder<PrimitiveType> b = primitive(type, state.repetition);    if (orig != null) {        b = b.as(orig);    }    if (fieldProjectionFilter.keep(state.path)) {        return new Keep(state.path, b.named(state.name));    } else {        return new Drop(state.path);    }}
0
public ConvertedField visit(EnumType enumType, State state)
{    return visitPrimitiveType(BINARY, enumType(), state);}
0
public ConvertedField visit(BoolType boolType, State state)
{    return visitPrimitiveType(BOOLEAN, state);}
0
public ConvertedField visit(ByteType byteType, State state)
{    return visitPrimitiveType(INT32, state);}
0
public ConvertedField visit(DoubleType doubleType, State state)
{    return visitPrimitiveType(DOUBLE, state);}
0
public ConvertedField visit(I16Type i16Type, State state)
{    return visitPrimitiveType(INT32, state);}
0
public ConvertedField visit(I32Type i32Type, State state)
{    return visitPrimitiveType(INT32, state);}
0
public ConvertedField visit(I64Type i64Type, State state)
{    return visitPrimitiveType(INT64, state);}
0
public ConvertedField visit(StringType stringType, State state)
{    return stringType.isBinary() ? visitPrimitiveType(BINARY, state) : visitPrimitiveType(BINARY, stringType(), state);}
0
private static boolean isUnion(StructOrUnionType s)
{    switch(s) {        case STRUCT:            return false;        case UNION:            return true;        case UNKNOWN:            throw new ShouldNeverHappenException("Encountered UNKNOWN StructOrUnionType");        default:            throw new ShouldNeverHappenException("Unrecognized type: " + s);    }}
0
private Type.Repetition getRepetition(ThriftField thriftField)
{    switch(thriftField.getRequirement()) {        case REQUIRED:            return REQUIRED;        case OPTIONAL:            return OPTIONAL;        case DEFAULT:            return OPTIONAL;        default:            throw new IllegalArgumentException("unknown requirement type: " + thriftField.getRequirement());    }}
0
public void testUnannotatedListOfPrimitives() throws Exception
{    Path test = writeDirect("message UnannotatedListOfPrimitives {" + "  repeated int32 list_of_ints;" + "}", new DirectWriter() {        @Override        public void write(RecordConsumer rc) {            rc.startMessage();            rc.startField("list_of_ints", 0);            rc.addInteger(34);            rc.addInteger(35);            rc.addInteger(36);            rc.endField("list_of_ints", 0);            rc.endMessage();        }    });}
0
public void write(RecordConsumer rc)
{    rc.startMessage();    rc.startField("list_of_ints", 0);    rc.addInteger(34);    rc.addInteger(35);    rc.addInteger(36);    rc.endField("list_of_ints", 0);    rc.endMessage();}
0
public void testUnannotatedListOfGroups() throws Exception
{    Path test = writeDirect("message UnannotatedListOfGroups {" + "  repeated group list_of_points {" + "    required float x;" + "    required float y;" + "  }" + "}", new DirectWriter() {        @Override        public void write(RecordConsumer rc) {            rc.startMessage();            rc.startField("list_of_points", 0);            rc.startGroup();            rc.startField("x", 0);            rc.addFloat(1.0f);            rc.endField("x", 0);            rc.startField("y", 1);            rc.addFloat(1.0f);            rc.endField("y", 1);            rc.endGroup();            rc.startGroup();            rc.startField("x", 0);            rc.addFloat(2.0f);            rc.endField("x", 0);            rc.startField("y", 1);            rc.addFloat(2.0f);            rc.endField("y", 1);            rc.endGroup();            rc.endField("list_of_points", 0);            rc.endMessage();        }    });}
0
public void write(RecordConsumer rc)
{    rc.startMessage();    rc.startField("list_of_points", 0);    rc.startGroup();    rc.startField("x", 0);    rc.addFloat(1.0f);    rc.endField("x", 0);    rc.startField("y", 1);    rc.addFloat(1.0f);    rc.endField("y", 1);    rc.endGroup();    rc.startGroup();    rc.startField("x", 0);    rc.addFloat(2.0f);    rc.endField("x", 0);    rc.startField("y", 1);    rc.addFloat(2.0f);    rc.endField("y", 1);    rc.endGroup();    rc.endField("list_of_points", 0);    rc.endMessage();}
0
public void testRepeatedPrimitiveInList() throws Exception
{    Path test = writeDirect("message RepeatedPrimitiveInList {" + "  required group list_of_ints (LIST) {" + "    repeated int32 array;" + "  }" + "}", new DirectWriter() {        @Override        public void write(RecordConsumer rc) {            rc.startMessage();            rc.startField("list_of_ints", 0);            rc.startGroup();            rc.startField("array", 0);            rc.addInteger(34);            rc.addInteger(35);            rc.addInteger(36);            rc.endField("array", 0);            rc.endGroup();            rc.endField("list_of_ints", 0);            rc.endMessage();        }    });    ListOfInts expected = new ListOfInts(Lists.newArrayList(34, 35, 36));    ListOfInts actual = reader(test, ListOfInts.class).read();    Assert.assertEquals("Should read record correctly", expected, actual);}
0
public void write(RecordConsumer rc)
{    rc.startMessage();    rc.startField("list_of_ints", 0);    rc.startGroup();    rc.startField("array", 0);    rc.addInteger(34);    rc.addInteger(35);    rc.addInteger(36);    rc.endField("array", 0);    rc.endGroup();    rc.endField("list_of_ints", 0);    rc.endMessage();}
0
public void testMultiFieldGroupInList() throws Exception
{        Path test = writeDirect("message MultiFieldGroupInList {" + "  optional group locations (LIST) {" + "    repeated group element {" + "      required double latitude;" + "      required double longitude;" + "    }" + "  }" + "}", new DirectWriter() {        @Override        public void write(RecordConsumer rc) {            rc.startMessage();            rc.startField("locations", 0);            rc.startGroup();            rc.startField("element", 0);            rc.startGroup();            rc.startField("latitude", 0);            rc.addDouble(0.0);            rc.endField("latitude", 0);            rc.startField("longitude", 1);            rc.addDouble(0.0);            rc.endField("longitude", 1);            rc.endGroup();            rc.startGroup();            rc.startField("latitude", 0);            rc.addDouble(0.0);            rc.endField("latitude", 0);            rc.startField("longitude", 1);            rc.addDouble(180.0);            rc.endField("longitude", 1);            rc.endGroup();            rc.endField("element", 0);            rc.endGroup();            rc.endField("locations", 0);            rc.endMessage();        }    });    ListOfLocations expected = new ListOfLocations();    expected.addToLocations(new Location(0.0, 0.0));    expected.addToLocations(new Location(0.0, 180.0));    assertReaderContains(reader(test, ListOfLocations.class), expected);}
0
public void write(RecordConsumer rc)
{    rc.startMessage();    rc.startField("locations", 0);    rc.startGroup();    rc.startField("element", 0);    rc.startGroup();    rc.startField("latitude", 0);    rc.addDouble(0.0);    rc.endField("latitude", 0);    rc.startField("longitude", 1);    rc.addDouble(0.0);    rc.endField("longitude", 1);    rc.endGroup();    rc.startGroup();    rc.startField("latitude", 0);    rc.addDouble(0.0);    rc.endField("latitude", 0);    rc.startField("longitude", 1);    rc.addDouble(180.0);    rc.endField("longitude", 1);    rc.endGroup();    rc.endField("element", 0);    rc.endGroup();    rc.endField("locations", 0);    rc.endMessage();}
0
public void testSingleFieldGroupInList() throws Exception
{            Path test = writeDirect("message SingleFieldGroupInList {" + "  optional group single_element_groups (LIST) {" + "    repeated group single_element_group {" + "      required int64 count;" + "    }" + "  }" + "}", new DirectWriter() {        @Override        public void write(RecordConsumer rc) {            rc.startMessage();            rc.startField("single_element_groups", 0);            rc.startGroup();                        rc.startField("single_element_group", 0);            rc.startGroup();            rc.startField("count", 0);            rc.addLong(1234L);            rc.endField("count", 0);            rc.endGroup();            rc.startGroup();            rc.startField("count", 0);            rc.addLong(2345L);            rc.endField("count", 0);            rc.endGroup();                        rc.endField("single_element_group", 0);            rc.endGroup();            rc.endField("single_element_groups", 0);            rc.endMessage();        }    });            ListOfSingleElementGroups expectedOldBehavior = new ListOfSingleElementGroups();    expectedOldBehavior.addToSingle_element_groups(new SingleElementGroup(1234L));    expectedOldBehavior.addToSingle_element_groups(new SingleElementGroup(2345L));    assertReaderContains(reader(test, ListOfSingleElementGroups.class), expectedOldBehavior);        ListOfCounts expectedNewBehavior = new ListOfCounts();    expectedNewBehavior.addToSingle_element_groups(1234L);    expectedNewBehavior.addToSingle_element_groups(2345L);    assertReaderContains(reader(test, ListOfCounts.class), expectedNewBehavior);}
0
public void write(RecordConsumer rc)
{    rc.startMessage();    rc.startField("single_element_groups", 0);    rc.startGroup();        rc.startField("single_element_group", 0);    rc.startGroup();    rc.startField("count", 0);    rc.addLong(1234L);    rc.endField("count", 0);    rc.endGroup();    rc.startGroup();    rc.startField("count", 0);    rc.addLong(2345L);    rc.endField("count", 0);    rc.endGroup();        rc.endField("single_element_group", 0);    rc.endGroup();    rc.endField("single_element_groups", 0);    rc.endMessage();}
0
public void testNewOptionalGroupInList() throws Exception
{    Path test = writeDirect("message NewOptionalGroupInList {" + "  optional group locations (LIST) {" + "    repeated group list {" + "      optional group element {" + "        required double latitude;" + "        required double longitude;" + "      }" + "    }" + "  }" + "}", new DirectWriter() {        @Override        public void write(RecordConsumer rc) {            rc.startMessage();            rc.startField("locations", 0);            rc.startGroup();                        rc.startField("list", 0);                                    rc.startGroup();            rc.startField("element", 0);            rc.startGroup();            rc.startField("latitude", 0);            rc.addDouble(0.0);            rc.endField("latitude", 0);            rc.startField("longitude", 1);            rc.addDouble(0.0);            rc.endField("longitude", 1);            rc.endGroup();            rc.endField("element", 0);                        rc.endGroup();                                    rc.startGroup();                        rc.endGroup();                                    rc.startGroup();            rc.startField("element", 0);            rc.startGroup();            rc.startField("latitude", 0);            rc.addDouble(0.0);            rc.endField("latitude", 0);            rc.startField("longitude", 1);            rc.addDouble(180.0);            rc.endField("longitude", 1);            rc.endGroup();            rc.endField("element", 0);                        rc.endGroup();                        rc.endField("list", 0);            rc.endGroup();            rc.endField("locations", 0);            rc.endMessage();        }    });    ListOfLocations expected = new ListOfLocations();    expected.addToLocations(new Location(0.0, 0.0));            expected.addToLocations(new Location(0.0, 180.0));    try {        assertReaderContains(reader(test, ListOfLocations.class), expected);        fail("Should fail: locations are optional and not ignored");    } catch (RuntimeException e) {                assertTrue(e.getCause().getCause().getMessage().contains("locations"));    }    assertReaderContains(readerIgnoreNulls(test, ListOfLocations.class), expected);}
0
public void write(RecordConsumer rc)
{    rc.startMessage();    rc.startField("locations", 0);    rc.startGroup();        rc.startField("list", 0);            rc.startGroup();    rc.startField("element", 0);    rc.startGroup();    rc.startField("latitude", 0);    rc.addDouble(0.0);    rc.endField("latitude", 0);    rc.startField("longitude", 1);    rc.addDouble(0.0);    rc.endField("longitude", 1);    rc.endGroup();    rc.endField("element", 0);        rc.endGroup();            rc.startGroup();        rc.endGroup();            rc.startGroup();    rc.startField("element", 0);    rc.startGroup();    rc.startField("latitude", 0);    rc.addDouble(0.0);    rc.endField("latitude", 0);    rc.startField("longitude", 1);    rc.addDouble(180.0);    rc.endField("longitude", 1);    rc.endGroup();    rc.endField("element", 0);        rc.endGroup();        rc.endField("list", 0);    rc.endGroup();    rc.endField("locations", 0);    rc.endMessage();}
0
public void testNewRequiredGroupInList() throws Exception
{    Path test = writeDirect("message NewRequiredGroupInList {" + "  optional group locations (LIST) {" + "    repeated group list {" + "      required group element {" + "        required double latitude;" + "        required double longitude;" + "      }" + "    }" + "  }" + "}", new DirectWriter() {        @Override        public void write(RecordConsumer rc) {            rc.startMessage();            rc.startField("locations", 0);            rc.startGroup();                        rc.startField("list", 0);                                    rc.startGroup();            rc.startField("element", 0);            rc.startGroup();            rc.startField("latitude", 0);            rc.addDouble(0.0);            rc.endField("latitude", 0);            rc.startField("longitude", 1);            rc.addDouble(180.0);            rc.endField("longitude", 1);            rc.endGroup();            rc.endField("element", 0);                        rc.endGroup();                                    rc.startGroup();            rc.startField("element", 0);            rc.startGroup();            rc.startField("latitude", 0);            rc.addDouble(0.0);            rc.endField("latitude", 0);            rc.startField("longitude", 1);            rc.addDouble(0.0);            rc.endField("longitude", 1);            rc.endGroup();            rc.endField("element", 0);                        rc.endGroup();                        rc.endField("list", 0);            rc.endGroup();            rc.endField("locations", 0);            rc.endMessage();        }    });    ListOfLocations expected = new ListOfLocations();    expected.addToLocations(new Location(0.0, 180.0));    expected.addToLocations(new Location(0.0, 0.0));    assertReaderContains(reader(test, ListOfLocations.class), expected);}
0
public void write(RecordConsumer rc)
{    rc.startMessage();    rc.startField("locations", 0);    rc.startGroup();        rc.startField("list", 0);            rc.startGroup();    rc.startField("element", 0);    rc.startGroup();    rc.startField("latitude", 0);    rc.addDouble(0.0);    rc.endField("latitude", 0);    rc.startField("longitude", 1);    rc.addDouble(180.0);    rc.endField("longitude", 1);    rc.endGroup();    rc.endField("element", 0);        rc.endGroup();            rc.startGroup();    rc.startField("element", 0);    rc.startGroup();    rc.startField("latitude", 0);    rc.addDouble(0.0);    rc.endField("latitude", 0);    rc.startField("longitude", 1);    rc.addDouble(0.0);    rc.endField("longitude", 1);    rc.endGroup();    rc.endField("element", 0);        rc.endGroup();        rc.endField("list", 0);    rc.endGroup();    rc.endField("locations", 0);    rc.endMessage();}
0
public void testAvroCompatRequiredGroupInList() throws Exception
{    Path test = writeDirect("message AvroCompatRequiredGroupInList {" + "  optional group locations (LIST) {" + "    repeated group array {" + "      required group element {" + "        required double latitude;" + "        required double longitude;" + "      }" + "    }" + "  }" + "}", new DirectWriter() {        @Override        public void write(RecordConsumer rc) {            rc.startMessage();            rc.startField("locations", 0);            rc.startGroup();                        rc.startField("array", 0);                                    rc.startGroup();            rc.startField("element", 0);            rc.startGroup();            rc.startField("latitude", 0);            rc.addDouble(90.0);            rc.endField("latitude", 0);            rc.startField("longitude", 1);            rc.addDouble(180.0);            rc.endField("longitude", 1);            rc.endGroup();            rc.endField("element", 0);                        rc.endGroup();                                    rc.startGroup();            rc.startField("element", 0);            rc.startGroup();            rc.startField("latitude", 0);            rc.addDouble(-90.0);            rc.endField("latitude", 0);            rc.startField("longitude", 1);            rc.addDouble(0.0);            rc.endField("longitude", 1);            rc.endGroup();            rc.endField("element", 0);                        rc.endGroup();                        rc.endField("array", 0);            rc.endGroup();            rc.endField("locations", 0);            rc.endMessage();        }    });    ListOfLocations expected = new ListOfLocations();    expected.addToLocations(new Location(90.0, 180.0));    expected.addToLocations(new Location(-90.0, 0.0));    assertReaderContains(reader(test, ListOfLocations.class), expected);}
0
public void write(RecordConsumer rc)
{    rc.startMessage();    rc.startField("locations", 0);    rc.startGroup();        rc.startField("array", 0);            rc.startGroup();    rc.startField("element", 0);    rc.startGroup();    rc.startField("latitude", 0);    rc.addDouble(90.0);    rc.endField("latitude", 0);    rc.startField("longitude", 1);    rc.addDouble(180.0);    rc.endField("longitude", 1);    rc.endGroup();    rc.endField("element", 0);        rc.endGroup();            rc.startGroup();    rc.startField("element", 0);    rc.startGroup();    rc.startField("latitude", 0);    rc.addDouble(-90.0);    rc.endField("latitude", 0);    rc.startField("longitude", 1);    rc.addDouble(0.0);    rc.endField("longitude", 1);    rc.endGroup();    rc.endField("element", 0);        rc.endGroup();        rc.endField("array", 0);    rc.endGroup();    rc.endField("locations", 0);    rc.endMessage();}
0
public void testAvroCompatListInList() throws Exception
{    Path test = writeDirect("message AvroCompatListInList {" + "  optional group listOfLists (LIST) {" + "    repeated group array (LIST) {" + "      repeated int32 array;" + "    }" + "  }" + "}", new DirectWriter() {        @Override        public void write(RecordConsumer rc) {            rc.startMessage();            rc.startField("locations", 0);            rc.startGroup();                        rc.startField("array", 0);            rc.startGroup();                        rc.startField("array", 0);                        rc.addInteger(34);            rc.addInteger(35);            rc.addInteger(36);                        rc.endField("array", 0);            rc.endGroup();                        rc.startGroup();            rc.endGroup();            rc.startGroup();                        rc.startField("array", 0);                        rc.addInteger(32);            rc.addInteger(33);            rc.addInteger(34);                        rc.endField("array", 0);            rc.endGroup();                        rc.endField("array", 0);            rc.endGroup();            rc.endField("locations", 0);            rc.endMessage();        }    });    ListOfLists expected = new ListOfLists();    expected.addToListOfLists(Arrays.asList(34, 35, 36));    expected.addToListOfLists(Arrays.<Integer>asList());    expected.addToListOfLists(Arrays.asList(32, 33, 34));        assertReaderContains(reader(test, ListOfLists.class), expected);}
0
public void write(RecordConsumer rc)
{    rc.startMessage();    rc.startField("locations", 0);    rc.startGroup();        rc.startField("array", 0);    rc.startGroup();        rc.startField("array", 0);        rc.addInteger(34);    rc.addInteger(35);    rc.addInteger(36);        rc.endField("array", 0);    rc.endGroup();        rc.startGroup();    rc.endGroup();    rc.startGroup();        rc.startField("array", 0);        rc.addInteger(32);    rc.addInteger(33);    rc.addInteger(34);        rc.endField("array", 0);    rc.endGroup();        rc.endField("array", 0);    rc.endGroup();    rc.endField("locations", 0);    rc.endMessage();}
0
public void testThriftCompatListInList() throws Exception
{    Path test = writeDirect("message ThriftCompatListInList {" + "  optional group listOfLists (LIST) {" + "    repeated group listOfLists_tuple (LIST) {" + "      repeated int32 listOfLists_tuple_tuple;" + "    }" + "  }" + "}", new DirectWriter() {        @Override        public void write(RecordConsumer rc) {            rc.startMessage();            rc.startField("locations", 0);            rc.startGroup();                        rc.startField("listOfLists_tuple", 0);            rc.startGroup();                        rc.startField("listOfLists_tuple_tuple", 0);                        rc.addInteger(34);            rc.addInteger(35);            rc.addInteger(36);                        rc.endField("listOfLists_tuple_tuple", 0);            rc.endGroup();                        rc.startGroup();            rc.endGroup();            rc.startGroup();                        rc.startField("listOfLists_tuple_tuple", 0);                        rc.addInteger(32);            rc.addInteger(33);            rc.addInteger(34);                        rc.endField("listOfLists_tuple_tuple", 0);            rc.endGroup();                        rc.endField("listOfLists_tuple", 0);            rc.endGroup();            rc.endField("locations", 0);            rc.endMessage();        }    });    ListOfLists expected = new ListOfLists();    expected.addToListOfLists(Arrays.asList(34, 35, 36));    expected.addToListOfLists(Arrays.<Integer>asList());    expected.addToListOfLists(Arrays.asList(32, 33, 34));        assertReaderContains(reader(test, ListOfLists.class), expected);}
0
public void write(RecordConsumer rc)
{    rc.startMessage();    rc.startField("locations", 0);    rc.startGroup();        rc.startField("listOfLists_tuple", 0);    rc.startGroup();        rc.startField("listOfLists_tuple_tuple", 0);        rc.addInteger(34);    rc.addInteger(35);    rc.addInteger(36);        rc.endField("listOfLists_tuple_tuple", 0);    rc.endGroup();        rc.startGroup();    rc.endGroup();    rc.startGroup();        rc.startField("listOfLists_tuple_tuple", 0);        rc.addInteger(32);    rc.addInteger(33);    rc.addInteger(34);        rc.endField("listOfLists_tuple_tuple", 0);    rc.endGroup();        rc.endField("listOfLists_tuple", 0);    rc.endGroup();    rc.endField("locations", 0);    rc.endMessage();}
0
public void testOldThriftCompatRequiredGroupInList() throws Exception
{    Path test = writeDirect("message OldThriftCompatRequiredGroupInList {" + "  optional group locations (LIST) {" + "    repeated group locations_tuple {" + "      required group element {" + "        required double latitude;" + "        required double longitude;" + "      }" + "    }" + "  }" + "}", new DirectWriter() {        @Override        public void write(RecordConsumer rc) {            rc.startMessage();            rc.startField("locations", 0);            rc.startGroup();                        rc.startField("locations_tuple", 0);                                    rc.startGroup();            rc.startField("element", 0);            rc.startGroup();            rc.startField("latitude", 0);            rc.addDouble(0.0);            rc.endField("latitude", 0);            rc.startField("longitude", 1);            rc.addDouble(180.0);            rc.endField("longitude", 1);            rc.endGroup();            rc.endField("element", 0);                        rc.endGroup();                                    rc.startGroup();            rc.startField("element", 0);            rc.startGroup();            rc.startField("latitude", 0);            rc.addDouble(0.0);            rc.endField("latitude", 0);            rc.startField("longitude", 1);            rc.addDouble(0.0);            rc.endField("longitude", 1);            rc.endGroup();            rc.endField("element", 0);                        rc.endGroup();                        rc.endField("locations_tuple", 0);            rc.endGroup();            rc.endField("locations", 0);            rc.endMessage();        }    });    ListOfLocations expected = new ListOfLocations();    expected.addToLocations(new Location(0.0, 180.0));    expected.addToLocations(new Location(0.0, 0.0));    assertReaderContains(reader(test, ListOfLocations.class), expected);}
0
public void write(RecordConsumer rc)
{    rc.startMessage();    rc.startField("locations", 0);    rc.startGroup();        rc.startField("locations_tuple", 0);            rc.startGroup();    rc.startField("element", 0);    rc.startGroup();    rc.startField("latitude", 0);    rc.addDouble(0.0);    rc.endField("latitude", 0);    rc.startField("longitude", 1);    rc.addDouble(180.0);    rc.endField("longitude", 1);    rc.endGroup();    rc.endField("element", 0);        rc.endGroup();            rc.startGroup();    rc.startField("element", 0);    rc.startGroup();    rc.startField("latitude", 0);    rc.addDouble(0.0);    rc.endField("latitude", 0);    rc.startField("longitude", 1);    rc.addDouble(0.0);    rc.endField("longitude", 1);    rc.endGroup();    rc.endField("element", 0);        rc.endGroup();        rc.endField("locations_tuple", 0);    rc.endGroup();    rc.endField("locations", 0);    rc.endMessage();}
0
public void testHiveCompatOptionalGroupInList() throws Exception
{    Path test = writeDirect("message HiveCompatOptionalGroupInList {" + "  optional group locations (LIST) {" + "    repeated group bag {" + "      optional group element {" + "        required double latitude;" + "        required double longitude;" + "      }" + "    }" + "  }" + "}", new DirectWriter() {        @Override        public void write(RecordConsumer rc) {            rc.startMessage();            rc.startField("locations", 0);            rc.startGroup();                        rc.startField("bag", 0);                                    rc.startGroup();            rc.startField("element", 0);            rc.startGroup();            rc.startField("latitude", 0);            rc.addDouble(0.0);            rc.endField("latitude", 0);            rc.startField("longitude", 1);            rc.addDouble(180.0);            rc.endField("longitude", 1);            rc.endGroup();            rc.endField("element", 0);                        rc.endGroup();                                    rc.startGroup();            rc.startField("element", 0);            rc.startGroup();            rc.startField("latitude", 0);            rc.addDouble(0.0);            rc.endField("latitude", 0);            rc.startField("longitude", 1);            rc.addDouble(0.0);            rc.endField("longitude", 1);            rc.endGroup();            rc.endField("element", 0);                        rc.endGroup();                        rc.endField("bag", 0);            rc.endGroup();            rc.endField("locations", 0);            rc.endMessage();        }    });    ListOfLocations expected = new ListOfLocations();    expected.addToLocations(new Location(0.0, 180.0));    expected.addToLocations(new Location(0.0, 0.0));    try {        assertReaderContains(reader(test, ListOfLocations.class), expected);        fail("Should fail: locations are optional and not ignored");    } catch (RuntimeException e) {                assertTrue(e.getCause().getCause().getMessage().contains("locations"));    }    assertReaderContains(readerIgnoreNulls(test, ListOfLocations.class), expected);}
0
public void write(RecordConsumer rc)
{    rc.startMessage();    rc.startField("locations", 0);    rc.startGroup();        rc.startField("bag", 0);            rc.startGroup();    rc.startField("element", 0);    rc.startGroup();    rc.startField("latitude", 0);    rc.addDouble(0.0);    rc.endField("latitude", 0);    rc.startField("longitude", 1);    rc.addDouble(180.0);    rc.endField("longitude", 1);    rc.endGroup();    rc.endField("element", 0);        rc.endGroup();            rc.startGroup();    rc.startField("element", 0);    rc.startGroup();    rc.startField("latitude", 0);    rc.addDouble(0.0);    rc.endField("latitude", 0);    rc.startField("longitude", 1);    rc.addDouble(0.0);    rc.endField("longitude", 1);    rc.endGroup();    rc.endField("element", 0);        rc.endGroup();        rc.endField("bag", 0);    rc.endGroup();    rc.endField("locations", 0);    rc.endMessage();}
0
public ParquetReader<T> reader(Path file, Class<T> thriftClass) throws IOException
{    return ThriftParquetReader.<T>build(file).withThriftClass(thriftClass).build();}
0
public ParquetReader<T> readerIgnoreNulls(Path file, Class<T> thriftClass) throws IOException
{    Configuration conf = new Configuration();    conf.setBoolean(ThriftRecordConverter.IGNORE_NULL_LIST_ELEMENTS, true);    return ThriftParquetReader.<T>build(file).withThriftClass(thriftClass).withConf(conf).build();}
0
public void assertReaderContains(ParquetReader<T> reader, T... expected) throws IOException
{    T record;    List<T> actual = Lists.newArrayList();    while ((record = reader.read()) != null) {        actual.add(record);    }    Assert.assertEquals("Should match exepected records", Lists.newArrayList(expected), actual);}
0
public void testBinary() throws IOException
{    StringAndBinary expected = new StringAndBinary("test", ByteBuffer.wrap(new byte[] { -123, 20, 33 }));    File temp = tempDir.newFile(UUID.randomUUID().toString());    temp.deleteOnExit();    temp.delete();    Path path = new Path(temp.getPath());    ThriftParquetWriter<StringAndBinary> writer = new ThriftParquetWriter<StringAndBinary>(path, StringAndBinary.class, CompressionCodecName.SNAPPY);    writer.write(expected);    writer.close();    ParquetReader<StringAndBinary> reader = ThriftParquetReader.<StringAndBinary>build(path).withThriftClass(StringAndBinary.class).build();    StringAndBinary record = reader.read();    reader.close();    assertSchema(ParquetFileReader.readFooter(new Configuration(), path));    assertEquals("Should match after serialization round trip", expected, record);}
0
private void assertSchema(ParquetMetadata parquetMetadata)
{    List<Type> fields = parquetMetadata.getFileMetaData().getSchema().getFields();    assertEquals(2, fields.size());    assertEquals(Types.required(PrimitiveType.PrimitiveTypeName.BINARY).as(OriginalType.UTF8).id(1).named("s"), fields.get(0));    assertEquals(Types.required(PrimitiveType.PrimitiveTypeName.BINARY).id(2).named("b"), fields.get(1));}
0
protected void setup(Context context) throws IOException, InterruptedException
{    records = new ArrayList<Object>();}
0
protected void map(Void key, T value, Context context) throws IOException, InterruptedException
{    records.add(value);}
0
public static StructWithAStructThatLooksLikeUnionV2 makeValid(int i)
{    AStructThatLooksLikeUnionV2 validUnion = new AStructThatLooksLikeUnionV2();    switch(i % 3) {        case 0:            validUnion.setALong(new ALong(17L));            break;        case 1:            validUnion.setANewBool(new ABool(false));            break;        case 2:            validUnion.setAString(new AString("bar"));            break;    }    return new StructWithAStructThatLooksLikeUnionV2("foo" + i, validUnion);}
0
public static StructWithUnionV2 makeExpectedValid(int i)
{    UnionV2 validUnion = new UnionV2();    switch(i % 3) {        case 0:            validUnion.setALong(new ALong(17L));            break;        case 1:            validUnion.setANewBool(new ABool(false));            break;        case 2:            validUnion.setAString(new AString("bar"));            break;    }    return new StructWithUnionV2("foo" + i, validUnion);}
0
public static StructWithAStructThatLooksLikeUnionV2 makeInvalid(int i)
{    AStructThatLooksLikeUnionV2 invalid = new AStructThatLooksLikeUnionV2();    if (i % 2 == 0) {                invalid.setALong(new ALong(18l));        invalid.setANewBool(new ABool(false));    } else {        }    return new StructWithAStructThatLooksLikeUnionV2("foo" + i, invalid);}
0
protected void setupJob(Job job, Path path) throws Exception
{    job.setInputFormatClass(ParquetThriftInputFormat.class);    ParquetThriftInputFormat.setInputPaths(job, path);    ParquetThriftInputFormat.setThriftClass(job.getConfiguration(), StructWithUnionV2.class);    job.setMapperClass(ReadMapper.class);    job.setNumReduceTasks(0);    job.setOutputFormatClass(NullOutputFormat.class);}
0
protected void assertEqualsExcepted(List<StructWithUnionV2> expected, List<Object> found) throws Exception
{    assertEquals(expected, found);}
0
private Path writeFileWithCorruptRecords(int numCorrupt, List<StructWithUnionV2> collectExpectedRecords) throws Exception
{                Path outputPath = new Path(new File(tempDir.getRoot(), "corrupt_out").getAbsolutePath());    ParquetWriter<StructWithAStructThatLooksLikeUnionV2> writer = new ThriftParquetWriter<StructWithAStructThatLooksLikeUnionV2>(outputPath, StructWithAStructThatLooksLikeUnionV2.class, CompressionCodecName.UNCOMPRESSED);    int numRecords = 0;    for (int i = 0; i < 100; i++) {        StructWithAStructThatLooksLikeUnionV2 valid = makeValid(numRecords);        StructWithUnionV2 expected = makeExpectedValid(numRecords);        numRecords++;        collectExpectedRecords.add(expected);        writer.write(valid);    }    for (int i = 0; i < numCorrupt; i++) {        writer.write(makeInvalid(numRecords++));    }    for (int i = 0; i < 100; i++) {        StructWithAStructThatLooksLikeUnionV2 valid = makeValid(numRecords);        StructWithUnionV2 expected = makeExpectedValid(numRecords);        numRecords++;        collectExpectedRecords.add(expected);        writer.write(valid);    }    writer.close();    return outputPath;}
0
private void readFile(Path path, Configuration conf, String name) throws Exception
{    Job job = new Job(conf, name);    setupJob(job, path);    waitForJob(job);}
0
public void testDefaultsToNoTolerance() throws Exception
{    ArrayList<StructWithUnionV2> expected = new ArrayList<StructWithUnionV2>();    try {        readFile(writeFileWithCorruptRecords(1, expected), new Configuration(), "testDefaultsToNoTolerance");        fail("This should throw");    } catch (RuntimeException e) {                assertEquals(100, ReadMapper.records.size());        assertEqualsExcepted(expected.subList(0, 100), ReadMapper.records);    }}
0
public void testCanTolerateBadRecords() throws Exception
{    Configuration conf = new Configuration();    conf.setFloat(UnmaterializableRecordCounter.BAD_RECORD_THRESHOLD_CONF_KEY, 0.1f);    List<StructWithUnionV2> expected = new ArrayList<StructWithUnionV2>();    readFile(writeFileWithCorruptRecords(4, expected), conf, "testCanTolerateBadRecords");    assertEquals(200, ReadMapper.records.size());    assertEqualsExcepted(expected, ReadMapper.records);}
0
public void testThrowsWhenTooManyBadRecords() throws Exception
{    Configuration conf = new Configuration();    conf.setFloat(UnmaterializableRecordCounter.BAD_RECORD_THRESHOLD_CONF_KEY, 0.1f);    ArrayList<StructWithUnionV2> expected = new ArrayList<StructWithUnionV2>();    try {        readFile(writeFileWithCorruptRecords(300, expected), conf, "testThrowsWhenTooManyBadRecords");        fail("This should throw");    } catch (RuntimeException e) {                assertEquals(100, ReadMapper.records.size());        assertEqualsExcepted(expected.subList(0, 100), ReadMapper.records);    }}
0
public static AddressBook nextAddressbook(int i)
{    final ArrayList<Person> persons = new ArrayList<Person>();    for (int j = 0; j < i % 3; j++) {        final ArrayList<PhoneNumber> phones = new ArrayList<PhoneNumber>();        for (int k = 0; k < i % 4; k++) {            phones.add(new PhoneNumber("12345" + i));        }        persons.add(new Person(new Name("John" + i, "Roberts"), i, "John@example.com" + i, phones));    }    AddressBook a = new AddressBook(persons);    return a;}
0
public void run(org.apache.hadoop.mapreduce.Mapper<LongWritable, Text, Void, AddressBook>.Context context) throws IOException, InterruptedException
{    for (int i = 0; i < 10; i++) {        AddressBook a = TestInputOutputFormat.nextAddressbook(i);        context.write(null, a);    }}
0
protected void map(Void key, AddressBook value, Mapper<Void, Group, LongWritable, Text>.Context context) throws IOException, InterruptedException
{    context.write(null, new Text(value.toString()));}
0
public void testReadWrite() throws Exception
{    final Configuration conf = new Configuration();    final Path inputPath = new Path("src/test/java/org/apache/parquet/hadoop/thrift/TestInputOutputFormat.java");    final Path parquetPath = new Path("target/test/thrift/TestInputOutputFormat/parquet");    final Path outputPath = new Path("target/test/thrift/TestInputOutputFormat/out");    final FileSystem fileSystem = parquetPath.getFileSystem(conf);    fileSystem.delete(parquetPath, true);    fileSystem.delete(outputPath, true);    {        final Job job = new Job(conf, "write");                TextInputFormat.addInputPath(job, inputPath);        job.setInputFormatClass(TextInputFormat.class);        job.setMapperClass(TestInputOutputFormat.MyMapper.class);        job.setNumReduceTasks(0);        job.setOutputFormatClass(ParquetThriftOutputFormat.class);        ParquetThriftOutputFormat.setCompression(job, CompressionCodecName.GZIP);        ParquetThriftOutputFormat.setOutputPath(job, parquetPath);        ParquetThriftOutputFormat.setThriftClass(job, AddressBook.class);        waitForJob(job);    }    {        final Job job = new Job(conf, "read");        job.setInputFormatClass(ParquetThriftInputFormat.class);        ParquetThriftInputFormat.setInputPaths(job, parquetPath);        job.setMapperClass(TestInputOutputFormat.MyMapper2.class);        job.setNumReduceTasks(0);        job.setOutputFormatClass(TextOutputFormat.class);        TextOutputFormat.setOutputPath(job, outputPath);        waitForJob(job);    }    final BufferedReader out = new BufferedReader(new FileReader(new File(outputPath.toString(), "part-m-00000")));    String lineOut = null;    int lineNumber = 0;    while ((lineOut = out.readLine()) != null) {        lineOut = lineOut.substring(lineOut.indexOf("\t") + 1);        AddressBook a = nextAddressbook(lineNumber);        assertEquals("line " + lineNumber, a.toString(), lineOut);        ++lineNumber;    }    assertNull("line " + lineNumber, out.readLine());    out.close();}
0
protected void map(LongWritable key, Text value, org.apache.hadoop.mapreduce.Mapper<LongWritable, Text, Void, StructV1>.Context context) throws IOException, InterruptedException
{    context.write(null, new StructV1(value.toString() + 1));}
0
protected void map(LongWritable key, Text value, org.apache.hadoop.mapreduce.Mapper<LongWritable, Text, Void, StructV2>.Context context) throws IOException, InterruptedException
{    final StructV2 s = new StructV2(value.toString() + 2);    s.setAge("undetermined");    context.write(null, s);}
0
protected void map(LongWritable key, Text value, org.apache.hadoop.mapreduce.Mapper<LongWritable, Text, Void, StructV3>.Context context) throws IOException, InterruptedException
{    final StructV3 s = new StructV3(value.toString() + 3);    s.setAge("average");    s.setGender("unavailable");    context.write(null, s);}
0
protected void map(LongWritable key, StructV3 value, org.apache.hadoop.mapreduce.Mapper<LongWritable, Text, Void, Text>.Context context) throws IOException, InterruptedException
{    context.write(null, new Text(value.toString()));}
0
public void testSchemaEvolution() throws Exception
{    final Configuration conf = new Configuration();    final Path inputPath = new Path("target/test/thrift/schema_evolution/in");    final Path parquetPath = new Path("target/test/thrift/schema_evolution/parquet");    final Path outputPath = new Path("target/test/thrift/schema_evolution/out");    final FileSystem fileSystem = parquetPath.getFileSystem(conf);    fileSystem.delete(inputPath, true);    final FSDataOutputStream in = fileSystem.create(inputPath);    in.writeUTF("Alice\nBob\nCharles\n");    in.close();    fileSystem.delete(parquetPath, true);    fileSystem.delete(outputPath, true);    {        write(conf, inputPath, new Path(parquetPath, "V1"), TestInputOutputFormat.SchemaEvolutionMapper1.class, StructV1.class);        write(conf, inputPath, new Path(parquetPath, "V2"), TestInputOutputFormat.SchemaEvolutionMapper2.class, StructV2.class);        write(conf, inputPath, new Path(parquetPath, "V3"), TestInputOutputFormat.SchemaEvolutionMapper3.class, StructV3.class);    }    {        final Job job = new Job(conf, "read");        job.setInputFormatClass(ParquetThriftInputFormat.class);        ParquetThriftInputFormat.setInputPaths(job, new Path(parquetPath, "*"));        ParquetThriftInputFormat.setThriftClass(job.getConfiguration(), StructV3.class);        job.setMapperClass(TestInputOutputFormat.SchemaEvolutionReadMapper.class);        job.setNumReduceTasks(0);        job.setOutputFormatClass(TextOutputFormat.class);        TextOutputFormat.setOutputPath(job, outputPath);        waitForJob(job);    }    read(outputPath + "/part-m-00000", 3);    read(outputPath + "/part-m-00001", 3);    read(outputPath + "/part-m-00002", 3);}
0
private void read(String outputPath, int expected) throws FileNotFoundException, IOException
{    final BufferedReader out = new BufferedReader(new FileReader(new File(outputPath.toString())));    String lineOut = null;    int lineNumber = 0;    while ((lineOut = out.readLine()) != null) {        lineOut = lineOut.substring(lineOut.indexOf("\t") + 1);        System.out.println(lineOut);        ++lineNumber;    }    out.close();    Assert.assertEquals(expected, lineNumber);}
0
private void write(final Configuration conf, final Path inputPath, final Path parquetPath, Class<? extends Mapper> mapperClass, Class<? extends TBase<?, ?>> outputClass) throws IOException, Exception
{    final Job job = new Job(conf, "write");        TextInputFormat.addInputPath(job, inputPath);    job.setInputFormatClass(TextInputFormat.class);    job.setMapperClass(mapperClass);    job.setNumReduceTasks(0);    job.setOutputFormatClass(ParquetThriftOutputFormat.class);    ParquetThriftOutputFormat.setCompression(job, CompressionCodecName.GZIP);    ParquetThriftOutputFormat.setOutputPath(job, parquetPath);    ParquetThriftOutputFormat.setThriftClass(job, outputClass);    waitForJob(job);}
0
public static void waitForJob(Job job) throws Exception
{    job.submit();    while (!job.isComplete()) {                sleep(100);    }        if (!job.isSuccessful()) {        throw new RuntimeException("job failed " + job.getJobName());    }}
1
public void testThriftOptionalFieldsWithReadProjectionUsingParquetSchema() throws Exception
{        Configuration conf = new Configuration();    final String readProjectionSchema = "message AddressBook {\n" + "  optional group persons {\n" + "    repeated group persons_tuple {\n" + "      required group name {\n" + "        optional binary first_name;\n" + "        optional binary last_name;\n" + "      }\n" + "      optional int32 id;\n" + "    }\n" + "  }\n" + "}";    conf.set(ReadSupport.PARQUET_READ_SCHEMA, readProjectionSchema);    TBase toWrite = new AddressBook(Arrays.asList(new Person(new Name("Bob", "Roberts"), 0, "bob.roberts@example.com", Arrays.asList(new PhoneNumber("1234567890")))));    TBase toRead = new AddressBook(Arrays.asList(new Person(new Name("Bob", "Roberts"), 0, null, null)));    shouldDoProjection(conf, toWrite, toRead, AddressBook.class);}
0
public void testPullingInRequiredStructWithFilter() throws Exception
{    final String projectionFilterDesc = "persons/{id};persons/email";    TBase toWrite = new AddressBook(Arrays.asList(new Person(new Name("Bob", "Roberts"), 0, "bob.roberts@example.com", Arrays.asList(new PhoneNumber("1234567890")))));            TBase toRead = new AddressBook(Arrays.asList(new Person(new Name("", ""), 0, "bob.roberts@example.com", null)));    shouldDoProjectionWithThriftColumnFilter(projectionFilterDesc, toWrite, toRead, AddressBook.class);}
0
public void testReorderdOptionalFields() throws Exception
{    final String projectionFilter = "**";    StructWithReorderedOptionalFields toWrite = new StructWithReorderedOptionalFields();    toWrite.setFieldOne(1);    toWrite.setFieldTwo(2);    toWrite.setFieldThree(3);    shouldDoProjectionWithThriftColumnFilter(projectionFilter, toWrite, toWrite, StructWithReorderedOptionalFields.class);}
0
public void testProjectOutOptionalFields() throws Exception
{    final String projectionFilterDesc = "persons/name/*";    TBase toWrite = new AddressBook(Arrays.asList(new Person(new Name("Bob", "Roberts"), 0, "bob.roberts@example.com", Arrays.asList(new PhoneNumber("1234567890")))));        TBase toRead = new AddressBook(Arrays.asList(new Person(new Name("Bob", "Roberts"), 0, null, null)));    shouldDoProjectionWithThriftColumnFilter(projectionFilterDesc, toWrite, toRead, AddressBook.class);}
0
public void testPullInRequiredMaps() throws Exception
{    String filter = "name";    Map<String, String> mapValue = new HashMap<String, String>();    mapValue.put("a", "1");    mapValue.put("b", "2");    RequiredMapFixture toWrite = new RequiredMapFixture(mapValue);    toWrite.setName("testName");    RequiredMapFixture toRead = new RequiredMapFixture(new HashMap<String, String>());    toRead.setName("testName");    shouldDoProjectionWithThriftColumnFilter(filter, toWrite, toRead, RequiredMapFixture.class);}
0
public void testDropMapValuePrimitive() throws Exception
{    String filter = "mavalue/key";    Map<String, String> mapValue = new HashMap<String, String>();    mapValue.put("a", "1");    mapValue.put("b", "2");    RequiredMapFixture toWrite = new RequiredMapFixture(mapValue);    toWrite.setName("testName");            Map<String, String> readValue = new HashMap<String, String>();    readValue.put("a", "1");    readValue.put("b", "2");    RequiredMapFixture toRead = new RequiredMapFixture(readValue);    shouldDoProjectionWithThriftColumnFilter(filter, toWrite, toRead, RequiredMapFixture.class);}
0
private StructV4WithExtracStructField makeStructV4WithExtracStructField(String id)
{    StructV4WithExtracStructField sv4 = new StructV4WithExtracStructField();    StructV3 sv3 = new StructV3();    sv3.setAge("age " + id);    sv3.setGender("gender" + id);    sv3.setName("inner name " + id);    sv4.setAge("outer age " + id);    sv4.setAddedStruct(sv3);    sv4.setGender("outer gender " + id);    sv4.setName("outer name " + id);    return sv4;}
0
public void testDropMapValueStruct() throws Exception
{    String filter = "reqMap/key";    Map<String, StructV4WithExtracStructField> mapValue = new HashMap<String, StructV4WithExtracStructField>();    StructV4WithExtracStructField v1 = makeStructV4WithExtracStructField("1");    StructV4WithExtracStructField v2 = makeStructV4WithExtracStructField("2");    mapValue.put("key 1", v1);    mapValue.put("key 2", v2);    MapWithStructValue toWrite = new MapWithStructValue(mapValue);        HashMap<String, StructV4WithExtracStructField> readValue = new HashMap<String, StructV4WithExtracStructField>();    readValue.put("key 1", new StructV4WithExtracStructField("outer name 1"));    readValue.put("key 2", new StructV4WithExtracStructField("outer name 2"));    MapWithStructValue toRead = new MapWithStructValue(readValue);    shouldDoProjectionWithThriftColumnFilter(filter, toWrite, toRead, MapWithStructValue.class);}
0
public void testDropMapValueNestedPrim() throws Exception
{    String filter = "reqMap/key";    Map<String, Map<String, String>> mapValue = new HashMap<String, Map<String, String>>();    Map<String, String> innerValue1 = new HashMap<String, String>();    innerValue1.put("inner key (1, 1)", "inner (1, 1)");    innerValue1.put("inner key (1, 2)", "inner (1, 2)");    Map<String, String> innerValue2 = new HashMap<String, String>();    innerValue2.put("inner key (2, 1)", "inner (2, 1)");    innerValue2.put("inner key (2, 2)", "inner (2, 2)");    mapValue.put("outer key 1", innerValue1);    mapValue.put("outer key 2", innerValue2);    MapWithPrimMapValue toWrite = new MapWithPrimMapValue(mapValue);    Map<String, Map<String, String>> expected = new HashMap<String, Map<String, String>>();    Map<String, String> expectedInnerValue1 = new HashMap<String, String>();    expectedInnerValue1.put("inner key (1, 1)", "inner (1, 1)");    expectedInnerValue1.put("inner key (1, 2)", "inner (1, 2)");    Map<String, String> expectedInnerValue2 = new HashMap<String, String>();    expectedInnerValue2.put("inner key (2, 1)", "inner (2, 1)");    expectedInnerValue2.put("inner key (2, 2)", "inner (2, 2)");    expected.put("outer key 1", expectedInnerValue1);    expected.put("outer key 2", expectedInnerValue2);    MapWithPrimMapValue toRead = new MapWithPrimMapValue(expected);    shouldDoProjectionWithThriftColumnFilter(filter, toWrite, toRead, MapWithPrimMapValue.class);}
0
public void testDropMapValueNestedStruct() throws Exception
{    String filter = "reqMap/key";    Map<String, Map<String, StructV4WithExtracStructField>> mapValue = new HashMap<String, Map<String, StructV4WithExtracStructField>>();    Map<String, StructV4WithExtracStructField> innerValue1 = new HashMap<String, StructV4WithExtracStructField>();    innerValue1.put("inner key (1, 1)", makeStructV4WithExtracStructField("inner (1, 1)"));    innerValue1.put("inner key (1, 2)", makeStructV4WithExtracStructField("inner (1, 2)"));    Map<String, StructV4WithExtracStructField> innerValue2 = new HashMap<String, StructV4WithExtracStructField>();    innerValue2.put("inner key (2, 1)", makeStructV4WithExtracStructField("inner (2, 1)"));    innerValue2.put("inner key (2, 2)", makeStructV4WithExtracStructField("inner (2, 2)"));    mapValue.put("outer key 1", innerValue1);    mapValue.put("outer key 2", innerValue2);    MapWithStructMapValue toWrite = new MapWithStructMapValue(mapValue);    Map<String, Map<String, StructV4WithExtracStructField>> expected = new HashMap<String, Map<String, StructV4WithExtracStructField>>();    Map<String, StructV4WithExtracStructField> expectedInnerValue1 = new HashMap<String, StructV4WithExtracStructField>();    expectedInnerValue1.put("inner key (1, 1)", new StructV4WithExtracStructField("outer name inner (1, 1)"));    expectedInnerValue1.put("inner key (1, 2)", new StructV4WithExtracStructField("outer name inner (1, 2)"));    Map<String, StructV4WithExtracStructField> expectedInnerValue2 = new HashMap<String, StructV4WithExtracStructField>();    expectedInnerValue2.put("inner key (2, 1)", new StructV4WithExtracStructField("outer name inner (2, 1)"));    expectedInnerValue2.put("inner key (2, 2)", new StructV4WithExtracStructField("outer name inner (2, 2)"));    expected.put("outer key 1", expectedInnerValue1);    expected.put("outer key 2", expectedInnerValue2);    MapWithStructMapValue toRead = new MapWithStructMapValue(expected);    shouldDoProjectionWithThriftColumnFilter(filter, toWrite, toRead, MapWithStructMapValue.class);}
0
public void testPullInRequiredLists() throws Exception
{    String filter = "info";    RequiredListFixture toWrite = new RequiredListFixture(Arrays.asList(new org.apache.parquet.thrift.test.Name("first_name")));    toWrite.setInfo("test_info");    RequiredListFixture toRead = new RequiredListFixture(new ArrayList<org.apache.parquet.thrift.test.Name>());    toRead.setInfo("test_info");    shouldDoProjectionWithThriftColumnFilter(filter, toWrite, toRead, RequiredListFixture.class);}
0
public void testPullInRequiredSets() throws Exception
{    String filter = "info";    RequiredSetFixture toWrite = new RequiredSetFixture(new HashSet<org.apache.parquet.thrift.test.Name>(Arrays.asList(new org.apache.parquet.thrift.test.Name("first_name"))));    toWrite.setInfo("test_info");    RequiredSetFixture toRead = new RequiredSetFixture(new HashSet<org.apache.parquet.thrift.test.Name>());    toRead.setInfo("test_info");    shouldDoProjectionWithThriftColumnFilter(filter, toWrite, toRead, RequiredSetFixture.class);}
0
public void testPullInPrimitiveValues() throws Exception
{    String filter = "info_string";    RequiredPrimitiveFixture toWrite = new RequiredPrimitiveFixture(true, (byte) 2, (short) 3, 4, (long) 5, (double) 6.0, "7");    toWrite.setInfo_string("it's info");    RequiredPrimitiveFixture toRead = new RequiredPrimitiveFixture(false, (byte) 0, (short) 0, 0, (long) 0, (double) 0.0, "");    toRead.setInfo_string("it's info");    shouldDoProjectionWithThriftColumnFilter(filter, toWrite, toRead, RequiredPrimitiveFixture.class);}
0
private void shouldDoProjectionWithThriftColumnFilter(String filterDesc, TBase toWrite, TBase toRead, Class<? extends TBase<?, ?>> thriftClass) throws Exception
{    Configuration conf = new Configuration();    conf.set(ThriftReadSupport.THRIFT_COLUMN_FILTER_KEY, filterDesc);    shouldDoProjection(conf, toWrite, toRead, thriftClass);}
0
private void shouldDoProjection(Configuration conf, T recordToWrite, T exptectedReadResult, Class<? extends TBase<?, ?>> thriftClass) throws Exception
{    final Path parquetFile = new Path("target/test/TestParquetToThriftReadWriteAndProjection/file.parquet");    final FileSystem fs = parquetFile.getFileSystem(conf);    if (fs.exists(parquetFile)) {        fs.delete(parquetFile, true);    }        final TProtocolFactory protocolFactory = new TCompactProtocol.Factory();    final TaskAttemptID taskId = new TaskAttemptID("local", 0, true, 0, 0);    final ThriftToParquetFileWriter w = new ThriftToParquetFileWriter(parquetFile, ContextUtil.newTaskAttemptContext(conf, taskId), protocolFactory, thriftClass);    final ByteArrayOutputStream baos = new ByteArrayOutputStream();    final TProtocol protocol = protocolFactory.getProtocol(new TIOStreamTransport(baos));    recordToWrite.write(protocol);    w.write(new BytesWritable(baos.toByteArray()));    w.close();    final ParquetThriftInputFormat<T> parquetThriftInputFormat = new ParquetThriftInputFormat<T>();    final Job job = new Job(conf, "read");    job.setInputFormatClass(ParquetThriftInputFormat.class);    ParquetThriftInputFormat.setInputPaths(job, parquetFile);    final JobID jobID = new JobID("local", 1);    List<InputSplit> splits = parquetThriftInputFormat.getSplits(ContextUtil.newJobContext(ContextUtil.getConfiguration(job), jobID));    T readValue = null;    for (InputSplit split : splits) {        TaskAttemptContext taskAttemptContext = ContextUtil.newTaskAttemptContext(ContextUtil.getConfiguration(job), new TaskAttemptID(new TaskID(jobID, true, 1), 0));        final RecordReader<Void, T> reader = parquetThriftInputFormat.createRecordReader(split, taskAttemptContext);        reader.initialize(split, taskAttemptContext);        if (reader.nextKeyValue()) {            readValue = reader.getCurrentValue();                    }    }    assertEquals(exptectedReadResult, readValue);}
1
public void testWriteFile() throws IOException, InterruptedException, TException
{    final AddressBook a = new AddressBook(Arrays.asList(new Person(new Name("Bob", "Roberts"), 0, "bob.roberts@example.com", Arrays.asList(new PhoneNumber("1234567890")))));    final Path fileToCreate = createFile(a);    ParquetReader<Group> reader = createRecordReader(fileToCreate);    Group g = null;    int i = 0;    while ((g = reader.read()) != null) {        assertEquals(a.persons.size(), g.getFieldRepetitionCount("persons"));        assertEquals(a.persons.get(0).email, g.getGroup("persons", 0).getGroup(0, 0).getString("email", 0));                ++i;    }    assertEquals("read 1 record", 1, i);}
0
public void testWriteStatistics() throws Exception
{        IntStatistics intStatsSmall = new IntStatistics();    intStatsSmall.setMinMax(2, 100);    LongStatistics longStatsSmall = new LongStatistics();    longStatsSmall.setMinMax(-17l, 287L);    DoubleStatistics doubleStatsSmall = new DoubleStatistics();    doubleStatsSmall.setMinMax(-15.55d, 9.63d);    BinaryStatistics binaryStatsSmall = new BinaryStatistics();    binaryStatsSmall.setMinMax(Binary.fromString("as"), Binary.fromString("world"));    BooleanStatistics boolStats = new BooleanStatistics();    boolStats.setMinMax(false, true);        Path p = createFile(new RequiredPrimitiveFixture(false, (byte) 32, (short) 32, 2, 90l, -15.55d, "as"), new RequiredPrimitiveFixture(false, (byte) 100, (short) 100, 100, 287l, -9.0d, "world"), new RequiredPrimitiveFixture(true, (byte) 2, (short) 2, 9, -17l, 9.63d, "hello"));    final Configuration configuration = new Configuration();    configuration.setBoolean("parquet.strings.signed-min-max.enabled", true);    final FileSystem fs = p.getFileSystem(configuration);    FileStatus fileStatus = fs.getFileStatus(p);    ParquetMetadata footer = ParquetFileReader.readFooter(configuration, p);    for (BlockMetaData bmd : footer.getBlocks()) {        for (ColumnChunkMetaData cmd : bmd.getColumns()) {            switch(cmd.getType()) {                case INT32:                    TestUtils.assertStatsValuesEqual(intStatsSmall, cmd.getStatistics());                    break;                case INT64:                    TestUtils.assertStatsValuesEqual(longStatsSmall, cmd.getStatistics());                    break;                case DOUBLE:                    TestUtils.assertStatsValuesEqual(doubleStatsSmall, cmd.getStatistics());                    break;                case BOOLEAN:                    TestUtils.assertStatsValuesEqual(boolStats, cmd.getStatistics());                    break;                case BINARY:                                        if (cmd.getPath().toString() == "[test_string]")                        TestUtils.assertStatsValuesEqual(binaryStatsSmall, cmd.getStatistics());                    break;            }        }    }        IntStatistics intStatsLarge = new IntStatistics();    intStatsLarge.setMinMax(-Integer.MAX_VALUE, Integer.MAX_VALUE);    LongStatistics longStatsLarge = new LongStatistics();    longStatsLarge.setMinMax(-Long.MAX_VALUE, Long.MAX_VALUE);    DoubleStatistics doubleStatsLarge = new DoubleStatistics();    doubleStatsLarge.setMinMax(-Double.MAX_VALUE, Double.MAX_VALUE);    BinaryStatistics binaryStatsLarge = new BinaryStatistics();    binaryStatsLarge.setMinMax(Binary.fromString("some small string"), Binary.fromString("some very large string here to test in this function"));        Path p_large = createFile(new RequiredPrimitiveFixture(false, (byte) 2, (short) 32, -Integer.MAX_VALUE, -Long.MAX_VALUE, -Double.MAX_VALUE, "some small string"), new RequiredPrimitiveFixture(false, (byte) 100, (short) 100, Integer.MAX_VALUE, Long.MAX_VALUE, Double.MAX_VALUE, "some very large string here to test in this function"), new RequiredPrimitiveFixture(true, (byte) 2, (short) 2, 9, -17l, 9.63d, "hello"));        final Configuration configuration_large = new Configuration();    configuration.setBoolean("parquet.strings.signed-min-max.enabled", true);    final FileSystem fs_large = p_large.getFileSystem(configuration_large);    FileStatus fileStatus_large = fs_large.getFileStatus(p_large);    ParquetMetadata footer_large = ParquetFileReader.readFooter(configuration_large, p_large);    for (BlockMetaData bmd : footer_large.getBlocks()) {        for (ColumnChunkMetaData cmd : bmd.getColumns()) {            switch(cmd.getType()) {                case INT32:                                        if (cmd.getPath().toString() == "[test_i32]")                        TestUtils.assertStatsValuesEqual(intStatsLarge, cmd.getStatistics());                    break;                case INT64:                    TestUtils.assertStatsValuesEqual(longStatsLarge, cmd.getStatistics());                    break;                case DOUBLE:                    TestUtils.assertStatsValuesEqual(doubleStatsLarge, cmd.getStatistics());                    break;                case BOOLEAN:                    TestUtils.assertStatsValuesEqual(boolStats, cmd.getStatistics());                    break;                case BINARY:                                        if (cmd.getPath().toString() == "[test_string]")                        TestUtils.assertStatsValuesEqual(binaryStatsLarge, cmd.getStatistics());                    break;            }        }    }}
0
public void testWriteFileListOfMap() throws IOException, InterruptedException, TException
{    Map<String, String> map1 = new HashMap<String, String>();    map1.put("key11", "value11");    map1.put("key12", "value12");    Map<String, String> map2 = new HashMap<String, String>();    map2.put("key21", "value21");    final TestMapInList listMap = new TestMapInList("listmap", Arrays.asList(map1, map2));    final Path fileToCreate = createFile(listMap);    ParquetReader<Group> reader = createRecordReader(fileToCreate);    Group g = null;    while ((g = reader.read()) != null) {        assertEquals(listMap.names.size(), g.getGroup("names", 0).getFieldRepetitionCount("names_tuple"));        assertEquals(listMap.names.get(0).size(), g.getGroup("names", 0).getGroup("names_tuple", 0).getFieldRepetitionCount("map"));        assertEquals(listMap.names.get(1).size(), g.getGroup("names", 0).getGroup("names_tuple", 1).getFieldRepetitionCount("map"));    }}
0
public void testWriteFileMapOfList() throws IOException, InterruptedException, TException
{    Map<String, List<String>> map = new HashMap<String, List<String>>();    map.put("key", Arrays.asList("val1", "val2"));    final TestListInMap mapList = new TestListInMap("maplist", map);    final Path fileToCreate = createFile(mapList);    ParquetReader<Group> reader = createRecordReader(fileToCreate);    Group g = null;    while ((g = reader.read()) != null) {        assertEquals("key", g.getGroup("names", 0).getGroup("map", 0).getBinary("key", 0).toStringUsingUTF8());        assertEquals(map.get("key").size(), g.getGroup("names", 0).getGroup("map", 0).getGroup("value", 0).getFieldRepetitionCount(0));    }}
0
public void testWriteFileMapOfLists() throws IOException, InterruptedException, TException
{    Map<List<String>, List<String>> map = new HashMap<List<String>, List<String>>();    map.put(Arrays.asList("key1", "key2"), Arrays.asList("val1", "val2"));    final TestListsInMap mapList = new TestListsInMap("maplists", map);    final Path fileToCreate = createFile(mapList);    ParquetReader<Group> reader = createRecordReader(fileToCreate);    Group g = null;    while ((g = reader.read()) != null) {        assertEquals("key1", g.getGroup("names", 0).getGroup("map", 0).getGroup("key", 0).getBinary("key_tuple", 0).toStringUsingUTF8());        assertEquals("key2", g.getGroup("names", 0).getGroup("map", 0).getGroup("key", 0).getBinary("key_tuple", 1).toStringUsingUTF8());        assertEquals("val1", g.getGroup("names", 0).getGroup("map", 0).getGroup("value", 0).getBinary("value_tuple", 0).toStringUsingUTF8());        assertEquals("val2", g.getGroup("names", 0).getGroup("map", 0).getGroup("value", 0).getBinary("value_tuple", 1).toStringUsingUTF8());    }}
0
private ParquetReader<Group> createRecordReader(Path parquetFilePath) throws IOException
{    Configuration configuration = new Configuration(true);    GroupReadSupport readSupport = new GroupReadSupport();    ParquetMetadata readFooter = ParquetFileReader.readFooter(configuration, parquetFilePath);    MessageType schema = readFooter.getFileMetaData().getSchema();    readSupport.init(configuration, null, schema);    return new ParquetReader<Group>(parquetFilePath, readSupport);}
0
private Path createFile(T... tObjs) throws IOException, InterruptedException, TException
{    final Path fileToCreate = new Path("target/test/TestThriftToParquetFileWriter/" + tObjs[0].getClass() + ".parquet");        Configuration conf = new Configuration();    final FileSystem fs = fileToCreate.getFileSystem(conf);    if (fs.exists(fileToCreate)) {        fs.delete(fileToCreate, true);    }    TProtocolFactory protocolFactory = new TCompactProtocol.Factory();    TaskAttemptID taskId = new TaskAttemptID("local", 0, true, 0, 0);    ThriftToParquetFileWriter w = new ThriftToParquetFileWriter(fileToCreate, ContextUtil.newTaskAttemptContext(conf, taskId), protocolFactory, (Class<? extends TBase<?, ?>>) tObjs[0].getClass());    for (T tObj : tObjs) {        final ByteArrayOutputStream baos = new ByteArrayOutputStream();        final TProtocol protocol = protocolFactory.getProtocol(new TIOStreamTransport(baos));        tObj.write(protocol);        w.write(new BytesWritable(baos.toByteArray()));    }    w.close();    return fileToCreate;}
1
public void testStorer() throws ExecException, Exception
{    String out = "target/out";    int rows = 1000;    Properties props = new Properties();    props.setProperty("parquet.compression", "uncompressed");    props.setProperty("parquet.page.size", "1000");    PigServer pigServer = new PigServer(ExecType.LOCAL, props);    Data data = Storage.resetData(pigServer);    Collection<Tuple> list = new ArrayList<Tuple>();    for (int i = 0; i < rows; i++) {        list.add(tuple("bob", "roberts" + i));    }    data.set("in", "fn:chararray, ln:chararray", list);    pigServer.deleteFile(out);    pigServer.setBatchOn();    pigServer.registerQuery("A = LOAD 'in' USING mock.Storage();");    pigServer.registerQuery("Store A into '" + out + "' using " + ParquetThriftStorer.class.getName() + "('" + Name.class.getName() + "');");    execBatch(pigServer);    pigServer.registerQuery("B = LOAD '" + out + "' USING " + ParquetLoader.class.getName() + "();");    pigServer.registerQuery("Store B into 'out' using mock.Storage();");    execBatch(pigServer);    List<Tuple> result = data.get("out");    assertEquals(rows, result.size());    int i = 0;    for (Tuple tuple : result) {        assertEquals(tuple("bob", "roberts" + i), tuple);        ++i;    }}
0
private void execBatch(PigServer pigServer) throws IOException
{    if (pigServer.executeBatch().get(0).getStatus() != JOB_STATUS.COMPLETED) {        throw new RuntimeException("Job failed", pigServer.executeBatch().get(0).getException());    }}
0
public void testRecursiveGlob()
{    PathGlobPattern g = new PathGlobPattern("a/**/b");    assertFalse(g.matches("a/b"));    assertTrue(g.matches("a/asd/b"));    assertTrue(g.matches("a/asd/ss/b"));    g = new PathGlobPattern("a/**");    assertTrue(g.matches("a/as"));    assertTrue(g.matches("a/asd/b"));    assertTrue(g.matches("a/asd/ss/b"));}
0
public void testStandardGlob()
{    PathGlobPattern g = new PathGlobPattern("a/*");    assertTrue(g.matches("a/as"));    assertFalse(g.matches("a/asd/b"));    assertFalse(g.matches("a/asd/ss/b"));    g = new PathGlobPattern("a/{bb,cc}/d");    assertTrue(g.matches("a/bb/d"));    assertTrue(g.matches("a/cc/d"));    assertFalse(g.matches("a/cc/bb/d"));    assertFalse(g.matches("a/d"));}
0
public void testFieldsPath()
{    StructType person = ThriftSchemaConverter.toStructType(Person.class);    List<String> paths = PrimitivePathVisitor.visit(person, ".");    assertEquals(Arrays.asList("name.first_name", "name.last_name", "id", "email", "phones.number", "phones.type"), paths);    paths = PrimitivePathVisitor.visit(person, "/");    assertEquals(Arrays.asList("name/first_name", "name/last_name", "id", "email", "phones/number", "phones/type"), paths);    StructType structInMap = ThriftSchemaConverter.toStructType(TestStructInMap.class);    paths = PrimitivePathVisitor.visit(structInMap, ".");    assertEquals(Arrays.asList("name", "names.key", "names.value.name.first_name", "names.value.name.last_name", "names.value.phones.key", "names.value.phones.value", "name_to_id.key", "name_to_id.value"), paths);    paths = PrimitivePathVisitor.visit(structInMap, "/");    assertEquals(Arrays.asList("name", "names/key", "names/value/name/first_name", "names/value/name/last_name", "names/value/phones/key", "names/value/phones/value", "name_to_id/key", "name_to_id/value"), paths);}
0
public static List<String> visit(StructType s, String delim)
{    PrimitivePathVisitor v = new PrimitivePathVisitor(delim);    return s.accept(v, new FieldsPath());}
0
public List<String> visit(MapType mapType, FieldsPath path)
{    List<String> ret = new ArrayList<String>();    ThriftField key = mapType.getKey();    ThriftField value = mapType.getValue();    ret.addAll(key.getType().accept(this, path.push(key)));    ret.addAll(value.getType().accept(this, path.push(value)));    return ret;}
0
public List<String> visit(SetType setType, FieldsPath path)
{    return setType.getValues().getType().accept(this, path);}
0
public List<String> visit(ListType listType, FieldsPath path)
{    return listType.getValues().getType().accept(this, path);}
0
public List<String> visit(StructType structType, FieldsPath path)
{    List<String> ret = new ArrayList<String>();    for (ThriftField child : structType.getChildren()) {        ret.addAll(child.getType().accept(this, path.push(child)));    }    return ret;}
0
private List<String> visitPrimitive(FieldsPath path)
{    return Arrays.asList(path.toDelimitedString(delim));}
0
public List<String> visit(EnumType enumType, FieldsPath path)
{    return visitPrimitive(path);}
0
public List<String> visit(BoolType boolType, FieldsPath path)
{    return visitPrimitive(path);}
0
public List<String> visit(ByteType byteType, FieldsPath path)
{    return visitPrimitive(path);}
0
public List<String> visit(DoubleType doubleType, FieldsPath path)
{    return visitPrimitive(path);}
0
public List<String> visit(I16Type i16Type, FieldsPath path)
{    return visitPrimitive(path);}
0
public List<String> visit(I32Type i32Type, FieldsPath path)
{    return visitPrimitive(path);}
0
public List<String> visit(I64Type i64Type, FieldsPath path)
{    return visitPrimitive(path);}
0
public List<String> visit(StringType stringType, FieldsPath path)
{    return visitPrimitive(path);}
0
public void testFromSemicolonDelimitedString()
{    List<String> globs = StrictFieldProjectionFilter.parseSemicolonDelimitedString(";x.y.z;*.a.b.c*;;foo;;;;bar;");    assertEquals(Arrays.asList("x.y.z", "*.a.b.c*", "foo", "bar"), globs);    try {        StrictFieldProjectionFilter.parseSemicolonDelimitedString(";;");        fail("this should throw");    } catch (ThriftProjectionException e) {        assertEquals("Semicolon delimited string ';;' contains 0 glob strings", e.getMessage());    }}
0
private static void assertMatches(StrictFieldProjectionFilter filter, String... strings)
{    for (String s : strings) {        if (!filter.keep(s)) {            fail(String.format("String '%s' was expected to match", s));        }    }}
0
private static void assertDoesNotMatch(StrictFieldProjectionFilter filter, String... strings)
{    for (String s : strings) {        if (filter.keep(s)) {            fail(String.format("String '%s' was not expected to match", s));        }    }}
0
public void testProjection()
{    StrictFieldProjectionFilter filter = StrictFieldProjectionFilter.fromSemicolonDelimitedString("home.phone_number;home.address;work.address.zip;base_info;*.average;a.b.c.pre{x,y,z{a,b,c}}post");    assertMatches(filter, "home.phone_number", "home.address", "work.address.zip", "base_info", "foo.average", "bar.x.y.z.average", "base_info.nested.field", "a.b.c.prexpost", "a.b.c.prezapost");    assertDoesNotMatch(filter, "home2.phone_number", "home2.address", "work.address", "base_info2", "foo_average", "bar.x.y.z_average", "base_info_nested.field", "hi", "average", "a.b.c.pre{x,y,z{a,b,c}}post", "");}
0
public void testIsStrict()
{    StrictFieldProjectionFilter filter = StrictFieldProjectionFilter.fromSemicolonDelimitedString("home.phone_number;a.b.c.pre{x,y,z{a,b,c}}post;bar.*.average");    assertMatches(filter, "home.phone_number", "bar.foo.average", "a.b.c.prexpost", "a.b.c.prezcpost");    assertDoesNotMatch(filter, "hello");    try {        filter.assertNoUnmatchedPatterns();        fail("this should throw");    } catch (ThriftProjectionException e) {        String expectedMessage = "The following projection patterns did not match any columns in this schema:\n" + "Pattern: 'a.b.c.pre{x,y,z{a,b,c}}post' (when expanded to 'a.b.c.preypost')\n" + "Pattern: 'a.b.c.pre{x,y,z{a,b,c}}post' (when expanded to 'a.b.c.prezapost')\n" + "Pattern: 'a.b.c.pre{x,y,z{a,b,c}}post' (when expanded to 'a.b.c.prezbpost')\n";        assertEquals(expectedMessage, e.getMessage());    }}
0
public void testWarnWhenMultiplePatternsMatch()
{    StrictFieldProjectionFilter filter = createMockBuilder(StrictFieldProjectionFilter.class).withConstructor(Arrays.asList("a.b.c.{x_average,z_average}", "a.*_average")).addMockedMethod("warn").createMock();        filter.warn("Field path: 'a.b.c.x_average' matched more than one glob path pattern. " + "First match: 'a.b.c.{x_average,z_average}' (when expanded to 'a.b.c.x_average') " + "second match:'a.*_average' (when expanded to 'a.*_average')");    filter.warn("Field path: 'a.b.c.z_average' matched more than one glob path pattern. " + "First match: 'a.b.c.{x_average,z_average}' (when expanded to 'a.b.c.z_average') " + "second match:'a.*_average' (when expanded to 'a.*_average')");    replay(filter);    assertMatches(filter, "a.b.c.x_average", "a.b.c.z_average", "a.other.w_average");    assertDoesNotMatch(filter, "hello");    verify(filter);}
0
public void testAddOptionalField()
{    verifyCompatible(StructV1.class, StructV2.class, true);}
0
public void testRemoveOptionalField()
{    verifyCompatible(StructV2.class, StructV1.class, false);}
0
public void testRenameField()
{    verifyCompatible(StructV1.class, RenameStructV1.class, false);}
0
public void testTypeChange()
{    verifyCompatible(StructV1.class, TypeChangeStructV1.class, false);}
0
public void testReuirementChange()
{        verifyCompatible(StructV1.class, OptionalStructV1.class, true);    verifyCompatible(StructV1.class, DefaultStructV1.class, true);        verifyCompatible(OptionalStructV1.class, StructV1.class, false);    verifyCompatible(DefaultStructV1.class, StructV1.class, false);}
0
public void testAddRequiredField()
{    verifyCompatible(StructV1.class, AddRequiredStructV1.class, false);}
0
public void testMap()
{        verifyCompatible(MapStructV1.class, MapStructV2.class, true);    verifyCompatible(MapValueStructV1.class, MapValueStructV2.class, true);        verifyCompatible(MapStructV2.class, MapStructV1.class, false);    verifyCompatible(MapValueStructV2.class, MapValueStructV1.class, false);        verifyCompatible(MapStructV2.class, MapAddRequiredStructV1.class, false);}
0
public void testSet()
{    verifyCompatible(SetStructV2.class, SetStructV1.class, false);    verifyCompatible(SetStructV1.class, SetStructV2.class, true);}
0
public void testList()
{    verifyCompatible(ListStructV2.class, ListStructV1.class, false);    verifyCompatible(ListStructV1.class, ListStructV2.class, true);}
0
public void testEmptyStruct()
{    CompatibilityReport report = getCompatibilityReport(NestedEmptyStruct.class, NestedEmptyStruct.class);    assertEquals("encountered an empty struct: required_empty\nencountered an empty struct: optional_empty", report.prettyMessages());    assertTrue(report.hasEmptyStruct());}
0
private ThriftType.StructType struct(Class thriftClass)
{    return ThriftSchemaConverter.toStructType(thriftClass);}
0
private CompatibilityReport getCompatibilityReport(Class oldClass, Class newClass)
{    CompatibilityChecker checker = new CompatibilityChecker();    CompatibilityReport report = checker.checkCompatibility(struct(oldClass), struct(newClass));    return report;}
0
private void verifyCompatible(Class oldClass, Class newClass, boolean expectCompatible)
{    CompatibilityReport report = getCompatibilityReport(oldClass, newClass);    assertEquals(expectCompatible, report.isCompatible());}
0
public void testWriteUnionInfo() throws Exception
{    StructType st = new StructType(new LinkedList<ThriftField>(), null);    assertEquals("{\n" + "  \"id\" : \"STRUCT\",\n" + "  \"children\" : [ ],\n" + "  \"structOrUnionType\" : \"STRUCT\"\n" + "}", st.toJSON());    st = new StructType(new LinkedList<ThriftField>(), StructOrUnionType.UNION);    assertEquals("{\n" + "  \"id\" : \"STRUCT\",\n" + "  \"children\" : [ ],\n" + "  \"structOrUnionType\" : \"UNION\"\n" + "}", st.toJSON());    st = new StructType(new LinkedList<ThriftField>(), StructOrUnionType.STRUCT);    assertEquals("{\n" + "  \"id\" : \"STRUCT\",\n" + "  \"children\" : [ ],\n" + "  \"structOrUnionType\" : \"STRUCT\"\n" + "}", st.toJSON());}
0
public void testParseUnionInfo() throws Exception
{    StructType st = (StructType) StructType.fromJSON("{\"id\": \"STRUCT\", \"children\":[], \"structOrUnionType\": \"UNION\"}");    assertEquals(st.getStructOrUnionType(), StructOrUnionType.UNION);    st = (StructType) StructType.fromJSON("{\"id\": \"STRUCT\", \"children\":[], \"structOrUnionType\": \"STRUCT\"}");    assertEquals(st.getStructOrUnionType(), StructOrUnionType.STRUCT);    st = (StructType) StructType.fromJSON("{\"id\": \"STRUCT\", \"children\":[]}");    assertEquals(st.getStructOrUnionType(), StructOrUnionType.STRUCT);    st = (StructType) StructType.fromJSON("{\"id\": \"STRUCT\", \"children\":[], \"structOrUnionType\": \"UNKNOWN\"}");    assertEquals(st.getStructOrUnionType(), StructOrUnionType.UNKNOWN);}
0
public void testList() throws TException
{    final List<String> names = new ArrayList<String>();    names.add("John");    names.add("Jack");    final TestNameList o = new TestNameList("name", names);    validate(o);}
0
public void testSet() throws TException
{    final Set<String> names = new HashSet<String>();    names.add("John");    names.add("Jack");    final TestNameSet o = new TestNameSet("name", names);    validate(o);}
0
public void testReadEmpty() throws Exception
{    AddressBook expected = new AddressBook();    validate(expected);}
0
public void testOneOfEach() throws TException
{    final List<Byte> bytes = new ArrayList<Byte>();    bytes.add((byte) 1);    final List<Short> shorts = new ArrayList<Short>();    shorts.add((short) 1);    final List<Long> longs = new ArrayList<Long>();    longs.add((long) 1);    OneOfEach a = new OneOfEach(true, false, (byte) 8, (short) 16, (int) 32, (long) 64, (double) 1234, "string", "å", false, ByteBuffer.wrap("a".getBytes()), bytes, shorts, longs);    validate(a);}
0
public void testRead() throws Exception
{    final PhoneNumber phoneNumber = new PhoneNumber("5555555555");    phoneNumber.type = MOBILE;    List<Person> persons = Arrays.asList(new Person(new Name("john", "johson"), 1, "john@johnson.org", Arrays.asList(phoneNumber)), new Person(new Name("jack", "jackson"), 2, "jack@jackson.org", Arrays.asList(new PhoneNumber("5555555556"))));    AddressBook expected = new AddressBook(persons);    validate(expected);}
0
public void testMap() throws Exception
{    final Map<String, String> map = new HashMap<String, String>();    map.put("foo", "bar");    TestMap testMap = new TestMap("map_name", map);    validate(testMap);}
0
public void testStructInMap() throws Exception
{    final Map<String, TestPerson> map = new HashMap<String, TestPerson>();    map.put("foo", new TestPerson(new TestName("john", "johnson"), new HashMap<TestPhoneType, String>()));    final Map<String, Integer> stringToIntMap = Collections.singletonMap("bar", 10);    TestStructInMap testMap = new TestStructInMap("map_name", map, stringToIntMap);    validate(testMap);}
0
private void validate(T expected) throws TException
{    @SuppressWarnings("unchecked")    final Class<T> thriftClass = (Class<T>) expected.getClass();    final MemPageStore memPageStore = new MemPageStore(1);    final ThriftSchemaConverter schemaConverter = new ThriftSchemaConverter();    final MessageType schema = schemaConverter.convert(thriftClass);        final MessageColumnIO columnIO = new ColumnIOFactory(true).getColumnIO(schema);    final ColumnWriteStoreV1 columns = new ColumnWriteStoreV1(memPageStore, ParquetProperties.builder().withPageSize(10000).withDictionaryEncoding(false).build());    final RecordConsumer recordWriter = columnIO.getRecordWriter(columns);    final StructType thriftType = schemaConverter.toStructType(thriftClass);    ParquetWriteProtocol parquetWriteProtocol = new ParquetWriteProtocol(recordWriter, columnIO, thriftType);    expected.write(parquetWriteProtocol);    recordWriter.flush();    columns.flush();    ThriftRecordConverter<T> converter = new TBaseRecordConverter<T>(thriftClass, schema, thriftType);    final RecordReader<T> recordReader = columnIO.getRecordReader(memPageStore, converter);    final T result = recordReader.read();    assertEquals(expected, result);}
1
public void testMap() throws Exception
{    String[] expectations = { "startMessage()", "startField(name, 0)", "addBinary(map_name)", "endField(name, 0)", "startField(names, 1)", "startGroup()", "startField(map, 0)", "startGroup()", "startField(key, 0)", "addBinary(foo)", "endField(key, 0)", "startField(value, 1)", "addBinary(bar)", "endField(value, 1)", "endGroup()", "startGroup()", "startField(key, 0)", "addBinary(foo2)", "endField(key, 0)", "startField(value, 1)", "addBinary(bar2)", "endField(value, 1)", "endGroup()", "endField(map, 0)", "endGroup()", "endField(names, 1)", "endMessage()" };    String[] expectationsAlt = { "startMessage()", "startField(name, 0)", "addBinary(map_name)", "endField(name, 0)", "startField(names, 1)", "startGroup()", "startField(map, 0)", "startGroup()", "startField(key, 0)", "addBinary(foo2)", "endField(key, 0)", "startField(value, 1)", "addBinary(bar2)", "endField(value, 1)", "endGroup()", "startGroup()", "startField(key, 0)", "addBinary(foo)", "endField(key, 0)", "startField(value, 1)", "addBinary(bar)", "endField(value, 1)", "endGroup()", "endField(map, 0)", "endGroup()", "endField(names, 1)", "endMessage()" };    final Map<String, String> map = new TreeMap<String, String>();    map.put("foo", "bar");    map.put("foo2", "bar2");    TestMap testMap = new TestMap("map_name", map);    try {        validatePig(expectations, testMap);    } catch (ComparisonFailure e) {                                validatePig(expectationsAlt, testMap);    }    validateThrift(expectations, testMap);}
0
public void testMapInSet() throws Exception
{    String[] pigExpectations = { "startMessage()", "startField(name, 0)", "addBinary(top)", "endField(name, 0)",     "startField(names, 1)", "startGroup()",     "startField(t, 0)", "startGroup()",     "startField(names_tuple, 0)", "startGroup()",     "startField(map, 0)", "startGroup()",     "startField(key, 0)", "addBinary(foo)", "endField(key, 0)",     "startField(value, 1)", "addBinary(bar)", "endField(value, 1)", "endGroup()", "endField(map, 0)", "endGroup()", "endField(names_tuple, 0)", "endGroup()", "endField(t, 0)", "endGroup()", "endField(names, 1)", "endMessage()" };    final Set<Map<String, String>> set = new HashSet<Map<String, String>>();    final Map<String, String> map = new HashMap<String, String>();    map.put("foo", "bar");    set.add(map);    TestMapInSet o = new TestMapInSet("top", set);    validatePig(pigExpectations, o);    String[] expectationsThrift = { "startMessage()", "startField(name, 0)", "addBinary(top)", "endField(name, 0)",     "startField(names, 1)", "startGroup()",     "startField(names_tuple, 0)", "startGroup()",     "startField(map, 0)", "startGroup()",     "startField(key, 0)", "addBinary(foo)", "endField(key, 0)",     "startField(value, 1)", "addBinary(bar)", "endField(value, 1)", "endGroup()", "endField(map, 0)", "endGroup()", "endField(names_tuple, 0)", "endGroup()", "endField(names, 1)", "endMessage()" };    validateThrift(expectationsThrift, o);}
0
public void testNameList() throws TException
{    final List<String> names = new ArrayList<String>();    names.add("John");    names.add("Jack");    final TestNameList o = new TestNameList("name", names);    String[] pigExpectations = { "startMessage()", "startField(name, 0)", "addBinary(name)", "endField(name, 0)", "startField(names, 1)", "startGroup()", "startField(t, 0)", "startGroup()", "startField(names_tuple, 0)", "addBinary(John)", "endField(names_tuple, 0)", "endGroup()", "startGroup()", "startField(names_tuple, 0)", "addBinary(Jack)", "endField(names_tuple, 0)", "endGroup()", "endField(t, 0)", "endGroup()", "endField(names, 1)", "endMessage()" };    validatePig(pigExpectations, o);    String[] expectations = { "startMessage()", "startField(name, 0)", "addBinary(name)", "endField(name, 0)", "startField(names, 1)", "startGroup()", "startField(names_tuple, 0)", "addBinary(John)", "addBinary(Jack)", "endField(names_tuple, 0)", "endGroup()", "endField(names, 1)", "endMessage()" };    validateThrift(expectations, o);}
0
public void testStructInMap() throws Exception
{    String[] expectations = { "startMessage()", "startField(name, 0)", "addBinary(map_name)", "endField(name, 0)", "startField(names, 1)", "startGroup()", "startField(map, 0)", "startGroup()", "startField(key, 0)", "addBinary(foo)", "endField(key, 0)", "startField(value, 1)", "startGroup()", "startField(name, 0)", "startGroup()", "startField(first_name, 0)", "addBinary(john)", "endField(first_name, 0)", "startField(last_name, 1)", "addBinary(johnson)", "endField(last_name, 1)", "endGroup()", "endField(name, 0)", "startField(phones, 1)", "startGroup()", "endGroup()", "endField(phones, 1)", "endGroup()", "endField(value, 1)", "endGroup()", "endField(map, 0)", "endGroup()", "endField(names, 1)", "startField(name_to_id, 2)", "startGroup()", "startField(map, 0)", "startGroup()", "startField(key, 0)", "addBinary(bar)", "endField(key, 0)", "startField(value, 1)", "addInt(10)", "endField(value, 1)", "endGroup()", "endField(map, 0)", "endGroup()", "endField(name_to_id, 2)", "endMessage()" };    final Map<String, TestPerson> map = new HashMap<String, TestPerson>();    map.put("foo", new TestPerson(new TestName("john", "johnson"), new HashMap<TestPhoneType, String>()));    final Map<String, Integer> stringToIntMap = Collections.singletonMap("bar", 10);    TestStructInMap testMap = new TestStructInMap("map_name", map, stringToIntMap);    validatePig(expectations, testMap);    validateThrift(expectations, testMap);}
0
public void testProtocolEmptyAdressBook() throws Exception
{    String[] expectations = { "startMessage()", "startField(persons, 0)", "startGroup()", "endGroup()", "endField(persons, 0)", "endMessage()" };    AddressBook a = new AddressBook(new ArrayList<Person>());    validatePig(expectations, a);    validateThrift(expectations, a);}
0
public void testProtocolAddressBook() throws Exception
{    String[] expectations = {     "startMessage()",     "startField(persons, 0)", "startGroup()",     "startField(t, 0)", "startGroup()", "startField(name, 0)",     "startGroup()", "startField(first_name, 0)", "addBinary(Bob)", "endField(first_name, 0)", "startField(last_name, 1)", "addBinary(Roberts)", "endField(last_name, 1)", "endGroup()", "endField(name, 0)", "startField(id, 1)", "addInt(1)", "endField(id, 1)", "startField(email, 2)", "addBinary(bob@roberts.com)", "endField(email, 2)", "startField(phones, 3)", "startGroup()", "startField(t, 0)", "startGroup()", "startField(number, 0)", "addBinary(555 999 9999)", "endField(number, 0)", "endGroup()", "startGroup()", "startField(number, 0)", "addBinary(555 999 9998)", "endField(number, 0)", "startField(type, 1)", "addBinary(HOME)", "endField(type, 1)", "endGroup()", "endField(t, 0)", "endGroup()", "endField(phones, 3)", "endGroup()", "startGroup()", "startField(name, 0)", "startGroup()", "startField(first_name, 0)", "addBinary(Dick)", "endField(first_name, 0)", "startField(last_name, 1)", "addBinary(Richardson)", "endField(last_name, 1)", "endGroup()", "endField(name, 0)", "startField(id, 1)", "addInt(2)", "endField(id, 1)", "startField(email, 2)", "addBinary(dick@richardson.com)", "endField(email, 2)", "startField(phones, 3)", "startGroup()", "startField(t, 0)", "startGroup()", "startField(number, 0)", "addBinary(555 999 9997)", "endField(number, 0)", "endGroup()", "startGroup()", "startField(number, 0)", "addBinary(555 999 9996)", "endField(number, 0)", "endGroup()", "endField(t, 0)", "endGroup()", "endField(phones, 3)", "endGroup()", "endField(t, 0)", "endGroup()", "endField(persons, 0)", "endMessage()" };    ArrayList<Person> persons = new ArrayList<Person>();    final PhoneNumber phoneNumber = new PhoneNumber("555 999 9998");    phoneNumber.type = PhoneType.HOME;    persons.add(new Person(new Name("Bob", "Roberts"), 1, "bob@roberts.com", Arrays.asList(new PhoneNumber("555 999 9999"), phoneNumber)));    persons.add(new Person(new Name("Dick", "Richardson"), 2, "dick@richardson.com", Arrays.asList(new PhoneNumber("555 999 9997"), new PhoneNumber("555 999 9996"))));    AddressBook a = new AddressBook(persons);    validatePig(expectations, a);        String[] expectationsThrift = Arrays.copyOf(expectations, expectations.length, String[].class);    expectationsThrift[3] = "startField(persons_tuple, 0)";    expectationsThrift[23] = "startField(phones_tuple, 0)";    expectationsThrift[37] = "endField(phones_tuple, 0)";    expectationsThrift[60] = "startField(phones_tuple, 0)";    expectationsThrift[71] = "endField(phones_tuple, 0)";    expectationsThrift[75] = "endField(persons_tuple, 0)";    validateThrift(expectationsThrift, a);}
0
public void testOneOfEach() throws TException
{    String[] expectations = { "startMessage()", "startField(im_true, 0)", "addInt(1)", "endField(im_true, 0)", "startField(im_false, 1)", "addInt(0)", "endField(im_false, 1)", "startField(a_bite, 2)", "addInt(8)", "endField(a_bite, 2)", "startField(integer16, 3)", "addInt(16)", "endField(integer16, 3)", "startField(integer32, 4)", "addInt(32)", "endField(integer32, 4)", "startField(integer64, 5)", "addLong(64)", "endField(integer64, 5)", "startField(double_precision, 6)", "addDouble(1234.0)", "endField(double_precision, 6)", "startField(some_characters, 7)", "addBinary(string)", "endField(some_characters, 7)", "startField(zomg_unicode, 8)", "addBinary(å)", "endField(zomg_unicode, 8)", "startField(what_who, 9)", "addInt(0)", "endField(what_who, 9)", "startField(base64, 10)", "addBinary(a)", "endField(base64, 10)", "startField(byte_list, 11)", "startGroup()", "endGroup()", "endField(byte_list, 11)", "startField(i16_list, 12)", "startGroup()", "endGroup()", "endField(i16_list, 12)", "startField(i64_list, 13)", "startGroup()", "endGroup()", "endField(i64_list, 13)", "endMessage()" };    OneOfEach a = new OneOfEach(true, false, (byte) 8, (short) 16, (int) 32, (long) 64, (double) 1234, "string", "å", false, ByteBuffer.wrap("a".getBytes()), new ArrayList<Byte>(), new ArrayList<Short>(), new ArrayList<Long>());    validatePig(expectations, a);    String[] thriftExpectations = Arrays.copyOf(expectations, expectations.length, String[].class);        thriftExpectations[2] = "addBoolean(true)";    thriftExpectations[5] = "addBoolean(false)";    thriftExpectations[29] = "addBoolean(false)";    validateThrift(thriftExpectations, a);}
0
private void validateThrift(String[] expectations, TBase<?, ?> a) throws TException
{    final ThriftSchemaConverter thriftSchemaConverter = new ThriftSchemaConverter();        final Class<TBase<?, ?>> class1 = (Class<TBase<?, ?>>) a.getClass();    final MessageType schema = thriftSchemaConverter.convert(class1);        final StructType structType = thriftSchemaConverter.toStructType(class1);    ExpectationValidatingRecordConsumer recordConsumer = new ExpectationValidatingRecordConsumer(new ArrayDeque<String>(Arrays.asList(expectations)));    final MessageColumnIO columnIO = new ColumnIOFactory().getColumnIO(schema);    ParquetWriteProtocol p = new ParquetWriteProtocol(new RecordConsumerLoggingWrapper(recordConsumer), columnIO, structType);    a.write(p);}
1
private MessageType validatePig(String[] expectations, TBase<?, ?> a)
{    ThriftToPig<TBase<?, ?>> thriftToPig = new ThriftToPig(a.getClass());    ExpectationValidatingRecordConsumer recordConsumer = new ExpectationValidatingRecordConsumer(new ArrayDeque<String>(Arrays.asList(expectations)));    Schema pigSchema = thriftToPig.toSchema();        MessageType schema = new PigSchemaConverter().convert(pigSchema);        TupleWriteSupport tupleWriteSupport = new TupleWriteSupport(pigSchema);    tupleWriteSupport.init(null);    tupleWriteSupport.prepareForWrite(recordConsumer);    final Tuple pigTuple = thriftToPig.getPigTuple(a);        tupleWriteSupport.write(pigTuple);    return schema;}
1
public void testOneOfEach() throws Exception
{    OneOfEach a = new OneOfEach(true, false, (byte) 8, (short) 16, (int) 32, (long) 64, (double) 1234, "string", "å", false, ByteBuffer.wrap("a".getBytes()), new ArrayList<Byte>(), new ArrayList<Short>(), new ArrayList<Long>());    writeReadCompare(a);}
0
public void testWriteRead() throws Exception
{    ArrayList<Person> persons = new ArrayList<Person>();    final PhoneNumber phoneNumber = new PhoneNumber("555 999 9998");    phoneNumber.type = PhoneType.HOME;    persons.add(new Person(new Name("Bob", "Roberts"), 1, "bob@roberts.com", Arrays.asList(new PhoneNumber("555 999 9999"), phoneNumber)));    persons.add(new Person(new Name("Dick", "Richardson"), 2, "dick@richardson.com", Arrays.asList(new PhoneNumber("555 999 9997"), new PhoneNumber("555 999 9996"))));    AddressBook a = new AddressBook(persons);    writeReadCompare(a);}
0
public void testEmptyStruct() throws Exception
{    AddressBook a = new AddressBook();    writeReadCompare(a);}
0
public void testMapSet() throws Exception
{    final Set<Map<String, String>> set = new HashSet<Map<String, String>>();    final Map<String, String> map = new HashMap<String, String>();    map.put("foo", "bar");    set.add(map);    TestMapInSet a = new TestMapInSet("top", set);    writeReadCompare(a);}
0
private void writeReadCompare(TBase<?, ?> a) throws TException, InstantiationException, IllegalAccessException
{    ProtocolPipe[] pipes = { new ProtocolReadToWrite(), new BufferedProtocolReadToWrite(ThriftSchemaConverter.toStructType((Class<TBase<?, ?>>) a.getClass())) };    for (ProtocolPipe p : pipes) {        final ByteArrayOutputStream in = new ByteArrayOutputStream();        final ByteArrayOutputStream out = new ByteArrayOutputStream();        a.write(protocol(in));        p.readOne(protocol(new ByteArrayInputStream(in.toByteArray())), protocol(out));        TBase<?, ?> b = a.getClass().newInstance();        b.read(protocol(new ByteArrayInputStream(out.toByteArray())));        assertEquals(p.getClass().getSimpleName(), a, b);    }}
0
public void testIncompatibleSchemaRecord() throws Exception
{        CountingErrorHandler countingHandler = new CountingErrorHandler();    BufferedProtocolReadToWrite p = new BufferedProtocolReadToWrite(ThriftSchemaConverter.toStructType(AddressBook.class), countingHandler);    final ByteArrayOutputStream in = new ByteArrayOutputStream();    final ByteArrayOutputStream out = new ByteArrayOutputStream();    OneOfEach a = new OneOfEach(true, false, (byte) 8, (short) 16, (int) 32, (long) 64, (double) 1234, "string", "å", false, ByteBuffer.wrap("a".getBytes()), new ArrayList<Byte>(), new ArrayList<Short>(), new ArrayList<Long>());    a.write(protocol(in));    try {        p.readOne(protocol(new ByteArrayInputStream(in.toByteArray())), protocol(out));        fail("this should throw");    } catch (SkippableException e) {        Throwable cause = e.getCause();        assertTrue(cause instanceof DecodingSchemaMismatchException);        assertTrue(cause.getMessage().contains("the data type does not match the expected thrift structure"));        assertTrue(cause.getMessage().contains("got BOOL"));    }    assertEquals(0, countingHandler.recordCountOfMissingFields);    assertEquals(0, countingHandler.fieldIgnoredCount);}
0
public void testUnrecognizedUnionMemberSchema() throws Exception
{    CountingErrorHandler countingHandler = new CountingErrorHandler();    BufferedProtocolReadToWrite p = new BufferedProtocolReadToWrite(ThriftSchemaConverter.toStructType(StructWithUnionV1.class), countingHandler);    final ByteArrayOutputStream in = new ByteArrayOutputStream();    final ByteArrayOutputStream out = new ByteArrayOutputStream();    StructWithUnionV1 validUnion = new StructWithUnionV1("a valid struct", UnionV1.aLong(new ALong(17L)));    StructWithUnionV2 invalidUnion = new StructWithUnionV2("a struct with new union member", UnionV2.aNewBool(new ABool(true)));    validUnion.write(protocol(in));    invalidUnion.write(protocol(in));    ByteArrayInputStream baos = new ByteArrayInputStream(in.toByteArray());        p.readOne(protocol(baos), protocol(out));    try {        p.readOne(protocol(baos), protocol(out));        fail("this should throw");    } catch (SkippableException e) {        Throwable cause = e.getCause();        assertEquals(DecodingSchemaMismatchException.class, cause.getClass());        assertTrue(cause.getMessage().startsWith("Unrecognized union member with id: 3 for struct:"));    }    assertEquals(0, countingHandler.recordCountOfMissingFields);    assertEquals(0, countingHandler.fieldIgnoredCount);}
0
public void testUnionWithExtraOrNoValues() throws Exception
{    CountingErrorHandler countingHandler = new CountingErrorHandler();    BufferedProtocolReadToWrite p = new BufferedProtocolReadToWrite(ThriftSchemaConverter.toStructType(StructWithUnionV2.class), countingHandler);    ByteArrayOutputStream in = new ByteArrayOutputStream();    final ByteArrayOutputStream out = new ByteArrayOutputStream();    StructWithUnionV2 validUnion = new StructWithUnionV2("a valid struct", UnionV2.aLong(new ALong(17L)));    StructWithAStructThatLooksLikeUnionV2 allMissing = new StructWithAStructThatLooksLikeUnionV2("all missing", new AStructThatLooksLikeUnionV2());    AStructThatLooksLikeUnionV2 extra = new AStructThatLooksLikeUnionV2();    extra.setALong(new ALong(18L));    extra.setANewBool(new ABool(false));    StructWithAStructThatLooksLikeUnionV2 hasExtra = new StructWithAStructThatLooksLikeUnionV2("has extra", new AStructThatLooksLikeUnionV2(extra));    validUnion.write(protocol(in));    allMissing.write(protocol(in));    ByteArrayInputStream baos = new ByteArrayInputStream(in.toByteArray());        p.readOne(protocol(baos), protocol(out));    try {        p.readOne(protocol(baos), protocol(out));        fail("this should throw");    } catch (SkippableException e) {        Throwable cause = e.getCause();        assertEquals(DecodingSchemaMismatchException.class, cause.getClass());        assertTrue(cause.getMessage().startsWith("Cannot write a TUnion with no set value in"));    }    assertEquals(0, countingHandler.recordCountOfMissingFields);    assertEquals(0, countingHandler.fieldIgnoredCount);    in = new ByteArrayOutputStream();    validUnion.write(protocol(in));    hasExtra.write(protocol(in));    baos = new ByteArrayInputStream(in.toByteArray());        p.readOne(protocol(baos), protocol(out));    try {        p.readOne(protocol(baos), protocol(out));        fail("this should throw");    } catch (SkippableException e) {        Throwable cause = e.getCause();        assertEquals(DecodingSchemaMismatchException.class, cause.getClass());        assertTrue(cause.getMessage().startsWith("Cannot write a TUnion with more than 1 set value in"));    }    assertEquals(0, countingHandler.recordCountOfMissingFields);    assertEquals(0, countingHandler.fieldIgnoredCount);}
0
public void testUnionWithStructWithUnknownField() throws Exception
{    CountingErrorHandler countingHandler = new CountingErrorHandler();    BufferedProtocolReadToWrite p = new BufferedProtocolReadToWrite(ThriftSchemaConverter.toStructType(UnionV3.class), countingHandler);    ByteArrayOutputStream in = new ByteArrayOutputStream();    final ByteArrayOutputStream out = new ByteArrayOutputStream();    UnionV3 validUnion = UnionV3.aStruct(new StructV1("a valid struct"));    StructV2 structV2 = new StructV2("a valid struct");    structV2.setAge("a valid age");    UnionThatLooksLikeUnionV3 unionWithUnknownStructField = UnionThatLooksLikeUnionV3.aStruct(structV2);    validUnion.write(protocol(in));    unionWithUnknownStructField.write(protocol(in));    ByteArrayInputStream baos = new ByteArrayInputStream(in.toByteArray());        p.readOne(protocol(baos), protocol(out));    p.readOne(protocol(baos), protocol(out));    assertEquals(1, countingHandler.recordCountOfMissingFields);    assertEquals(1, countingHandler.fieldIgnoredCount);    in = new ByteArrayOutputStream();    validUnion.write(protocol(in));    unionWithUnknownStructField.write(protocol(in));    baos = new ByteArrayInputStream(in.toByteArray());        p.readOne(protocol(baos), protocol(out));    p.readOne(protocol(baos), protocol(out));    assertEquals(2, countingHandler.recordCountOfMissingFields);    assertEquals(2, countingHandler.fieldIgnoredCount);}
0
public void testEnumMissingSchema() throws Exception
{    CountingErrorHandler countingHandler = new CountingErrorHandler();    BufferedProtocolReadToWrite p = new BufferedProtocolReadToWrite(ThriftSchemaConverter.toStructType(StructWithEnum.class), countingHandler);    final ByteArrayOutputStream in = new ByteArrayOutputStream();    final ByteArrayOutputStream out = new ByteArrayOutputStream();    StructWithMoreEnum enumDefinedInOldDefinition = new StructWithMoreEnum(NumberEnumWithMoreValue.THREE);    StructWithMoreEnum extraEnumDefinedInNewDefinition = new StructWithMoreEnum(NumberEnumWithMoreValue.FOUR);    enumDefinedInOldDefinition.write(protocol(in));    extraEnumDefinedInNewDefinition.write(protocol(in));    ByteArrayInputStream baos = new ByteArrayInputStream(in.toByteArray());        p.readOne(protocol(baos), protocol(out));    try {        p.readOne(protocol(baos), protocol(out));        fail("this should throw");    } catch (SkippableException e) {        Throwable cause = e.getCause();        assertEquals(DecodingSchemaMismatchException.class, cause.getClass());        assertTrue(cause.getMessage().contains("can not find index 4 in enum"));    }    assertEquals(0, countingHandler.recordCountOfMissingFields);    assertEquals(0, countingHandler.fieldIgnoredCount);}
0
public void testMissingFieldHandling() throws Exception
{    CountingErrorHandler countingHandler = new CountingErrorHandler() {        @Override        public void handleFieldIgnored(TField field) {            assertEquals(field.id, 4);            fieldIgnoredCount++;        }    };    BufferedProtocolReadToWrite structForRead = new BufferedProtocolReadToWrite(ThriftSchemaConverter.toStructType(StructV3.class), countingHandler);        final ByteArrayOutputStream in = new ByteArrayOutputStream();    StructV4WithExtracStructField dataWithNewSchema = new StructV4WithExtracStructField("name");    dataWithNewSchema.setAge("10");    dataWithNewSchema.setGender("male");    StructV3 structV3 = new StructV3("name");    structV3.setAge("10");    dataWithNewSchema.setAddedStruct(structV3);    dataWithNewSchema.write(protocol(in));        final ByteArrayOutputStream out = new ByteArrayOutputStream();    structForRead.readOne(protocol(new ByteArrayInputStream(in.toByteArray())), protocol(out));        assertEquals(1, countingHandler.recordCountOfMissingFields);    assertEquals(1, countingHandler.fieldIgnoredCount);    StructV4WithExtracStructField b = StructV4WithExtracStructField.class.newInstance();    b.read(protocol(new ByteArrayInputStream(out.toByteArray())));    assertEquals(dataWithNewSchema.getName(), b.getName());    assertEquals(dataWithNewSchema.getAge(), b.getAge());    assertEquals(dataWithNewSchema.getGender(), b.getGender());    assertEquals(null, b.getAddedStruct());}
0
public void handleFieldIgnored(TField field)
{    assertEquals(field.id, 4);    fieldIgnoredCount++;}
0
public void TestExtraFieldWhenFieldIndexIsNotStartFromZero() throws Exception
{    CountingErrorHandler countingHandler = new CountingErrorHandler() {        @Override        public void handleFieldIgnored(TField field) {            assertEquals(3, field.id);            fieldIgnoredCount++;        }    };    BufferedProtocolReadToWrite structForRead = new BufferedProtocolReadToWrite(ThriftSchemaConverter.toStructType(StructWithIndexStartsFrom4.class), countingHandler);        final ByteArrayOutputStream in = new ByteArrayOutputStream();    StructWithExtraField dataWithNewExtraField = new StructWithExtraField(new Phone("111", "222"), new Phone("333", "444"));    dataWithNewExtraField.write(protocol(in));        final ByteArrayOutputStream out = new ByteArrayOutputStream();    structForRead.readOne(protocol(new ByteArrayInputStream(in.toByteArray())), protocol(out));    assertEquals(1, countingHandler.recordCountOfMissingFields);    assertEquals(1, countingHandler.fieldIgnoredCount);}
0
public void handleFieldIgnored(TField field)
{    assertEquals(3, field.id);    fieldIgnoredCount++;}
0
private TCompactProtocol protocol(OutputStream to)
{    return new TCompactProtocol(new TIOStreamTransport(to));}
0
private TCompactProtocol protocol(InputStream from)
{    return new TCompactProtocol(new TIOStreamTransport(from));}
0
public void handleRecordHasFieldIgnored()
{    recordCountOfMissingFields++;}
0
public void handleFieldIgnored(TField field)
{    fieldIgnoredCount++;}
0
public void testToStringDoesNotThrow()
{    StructType descriptor = new StructType(new ArrayList<ThriftField>(), StructOrUnionType.STRUCT);    ThriftMetaData tmd = new ThriftMetaData("non existent class!!!", descriptor);    assertEquals("ThriftMetaData(thriftClassName: non existent class!!!, descriptor: {\n" + "  \"id\" : \"STRUCT\",\n" + "  \"children\" : [ ],\n" + "  \"structOrUnionType\" : \"STRUCT\"\n" + "})", tmd.toString());    tmd = new ThriftMetaData("non existent class!!!", null);    assertEquals("ThriftMetaData(thriftClassName: non existent class!!!, descriptor: null)", tmd.toString());}
0
public void testWriteRead() throws IOException
{    Configuration configuration = new Configuration();    Path f = new Path("target/test/TestThriftParquetReaderWriter");    FileSystem fs = f.getFileSystem(configuration);    if (fs.exists(f)) {        fs.delete(f, true);    }    AddressBook original = new AddressBook(Arrays.asList(new Person(new Name("Bob", "Roberts"), 1, "bob@roberts.com", Arrays.asList(new PhoneNumber("5555555555")))));    {                ThriftParquetWriter<AddressBook> thriftParquetWriter = new ThriftParquetWriter<AddressBook>(f, AddressBook.class, CompressionCodecName.UNCOMPRESSED);        thriftParquetWriter.write(original);        thriftParquetWriter.close();    }    {                ThriftParquetReader<AddressBook> thriftParquetReader = new ThriftParquetReader<AddressBook>(f, AddressBook.class);        AddressBook read = thriftParquetReader.read();        Assert.assertEquals(original, read);        thriftParquetReader.close();    }    {                ThriftParquetReader<AddressBook> thriftParquetReader = new ThriftParquetReader<AddressBook>(f);        AddressBook read = thriftParquetReader.read();        Assert.assertEquals(original, read);        thriftParquetReader.close();    }}
0
public void testUnknownEnumThrowsGoodException() throws Exception
{    EnumType et = new EnumType(Arrays.asList(new EnumValue(77, "hello")));    ThriftField field = new ThriftField("name", (short) 1, Requirement.REQUIRED, et);    ArrayList<TProtocol> events = new ArrayList<TProtocol>();    FieldEnumConverter conv = new FieldEnumConverter(events, field);    conv.addBinary(Binary.fromString("hello"));    assertEquals(1, events.size());    assertEquals(77, events.get(0).readI32());    try {        conv.addBinary(Binary.fromString("FAKE_ENUM_VALUE"));        fail("this should throw");    } catch (ParquetDecodingException e) {        assertEquals("Unrecognized enum value: FAKE_ENUM_VALUE known values: {Binary{\"hello\"}=77} in {\n" + "  \"name\" : \"name\",\n" + "  \"fieldId\" : 1,\n" + "  \"requirement\" : \"REQUIRED\",\n" + "  \"type\" : {\n" + "    \"id\" : \"ENUM\",\n" + "    \"values\" : [ {\n" + "      \"id\" : 77,\n" + "      \"name\" : \"hello\"\n" + "    } ]\n" + "  }\n" + "}", e.getMessage());    }}
0
public void constructorDoesNotRequireStructOrUnionTypeMeta() throws Exception
{    String jsonWithNoStructOrUnionMeta = Strings.join(Files.readAllLines(new File("src/test/resources/org/apache/parquet/thrift/StructWithUnionV1NoStructOrUnionMeta.json").toPath(), StandardCharsets.UTF_8), "\n");    StructType noStructOrUnionMeta = (StructType) ThriftType.fromJSON(jsonWithNoStructOrUnionMeta);        new ThriftRecordConverter<StructWithUnionV1>(new ThriftReader<StructWithUnionV1>() {        @Override        public StructWithUnionV1 readOneRecord(TProtocol protocol) throws TException {            return null;        }    }, "name", new ThriftSchemaConverter().convert(StructWithUnionV1.class), noStructOrUnionMeta);}
0
public StructWithUnionV1 readOneRecord(TProtocol protocol) throws TException
{    return null;}
0
public void testToMessageType() throws Exception
{    String expected = "message ParquetSchema {\n" + "  optional group persons (LIST) = 1 {\n" + "    repeated group persons_tuple {\n" + "      required group name = 1 {\n" + "        optional binary first_name (UTF8) = 1;\n" + "        optional binary last_name (UTF8) = 2;\n" + "      }\n" + "      optional int32 id = 2;\n" + "      optional binary email (UTF8) = 3;\n" + "      optional group phones (LIST) = 4 {\n" + "        repeated group phones_tuple {\n" + "          optional binary number (UTF8) = 1;\n" + "          optional binary type (ENUM) = 2;\n" + "        }\n" + "      }\n" + "    }\n" + "  }\n" + "}";    ThriftSchemaConverter schemaConverter = new ThriftSchemaConverter();    final MessageType converted = schemaConverter.convert(AddressBook.class);    assertEquals(MessageTypeParser.parseMessageType(expected), converted);}
0
public void testToProjectedThriftType()
{    shouldGetProjectedSchema("name/first_name", "name.first_name", "message ParquetSchema {" + "  required group name = 1 {" + "    optional binary first_name (UTF8) = 1;" + "  }}", Person.class);    shouldGetProjectedSchema("name/first_name;name/last_name", "name.first_name;name.last_name", "message ParquetSchema {" + "  required group name = 1 {" + "    optional binary first_name (UTF8) = 1;" + "    optional binary last_name (UTF8) = 2;" + "  }}", Person.class);    shouldGetProjectedSchema("name/{first,last}_name;", "name.{first,last}_name;", "message ParquetSchema {" + "  required group name = 1 {" + "    optional binary first_name (UTF8) = 1;" + "    optional binary last_name (UTF8) = 2;" + "  }}", Person.class);    shouldGetProjectedSchema("name/*", "name", "message ParquetSchema {" + "  required group name = 1 {" + "    optional binary first_name (UTF8) = 1;" + "    optional binary last_name (UTF8) = 2;" + "  }" + "}", Person.class);    shouldGetProjectedSchema("*/*_name", "*.*_name", "message ParquetSchema {" + "  required group name = 1 {" + "    optional binary first_name (UTF8) = 1;" + "    optional binary last_name (UTF8) = 2;" + "  }" + "}", Person.class);    shouldGetProjectedSchema("name/first_*", "name.first_*", "message ParquetSchema {" + "  required group name = 1 {" + "    optional binary first_name (UTF8) = 1;" + "  }" + "}", Person.class);    shouldGetProjectedSchema("*/*", "*.*", "message ParquetSchema {" + "  required group name = 1 {" + "  optional binary first_name (UTF8) = 1;" + "  optional binary last_name (UTF8) = 2;" + "} " + "  optional group phones (LIST) = 4 {" + "    repeated group phones_tuple {" + "      optional binary number (UTF8) = 1;" + "      optional binary type (ENUM) = 2;" + "    }" + "}}", Person.class);}
0
public void testProjectMapThriftType()
{        shouldGetProjectedSchema("name;names/key*;names/value/**", "name;names.key*;names.value", "message ParquetSchema {\n" + "  optional binary name (UTF8) = 1;\n" + "  optional group names (MAP) = 2 {\n" + "    repeated group map (MAP_KEY_VALUE) {\n" + "      required binary key (UTF8);\n" + "      optional group value {\n" + "        optional group name = 1 {\n" + "          optional binary first_name (UTF8) = 1;\n" + "          optional binary last_name (UTF8) = 2;\n" + "        }\n" + "        optional group phones (MAP) = 2 {\n" + "          repeated group map (MAP_KEY_VALUE) {\n" + "            required binary key (ENUM);\n" + "            optional binary value (UTF8);\n" + "          }\n" + "        }\n" + "      }\n" + "    }\n" + "  }\n" + "}", TestStructInMap.class);        shouldGetProjectedSchema("name;names/key;names/value/name/*", "name;names.key;names.value.name", "message ParquetSchema {\n" + "  optional binary name (UTF8) = 1;\n" + "  optional group names (MAP) = 2 {\n" + "    repeated group map (MAP_KEY_VALUE) {\n" + "      required binary key (UTF8);\n" + "      optional group value {\n" + "        optional group name = 1 {\n" + "          optional binary first_name (UTF8) = 1;\n" + "          optional binary last_name (UTF8) = 2;\n" + "        }\n" + "      }\n" + "    }\n" + "  }\n" + "}", TestStructInMap.class);}
0
public void testProjectOnlyKeyInMap()
{    shouldGetProjectedSchema("name;names/key", "name;names.key", "message ParquetSchema {\n" + "  optional binary name (UTF8) = 1;\n" + "  optional group names (MAP) = 2 {\n" + "    repeated group map (MAP_KEY_VALUE) {\n" + "      required binary key (UTF8);\n" + "      optional group value {\n" + "        optional group name = 1 {\n" + "          optional binary first_name (UTF8) = 1;\n" + "        }\n" + "      }" + "    }\n" + "  }\n" + "}", TestStructInMap.class);}
0
private void shouldThrowWhenProjectionFilterMatchesNothing(String filters, String unmatchedFilter, Class<? extends TBase<?, ?>> thriftClass)
{    try {        getDeprecatedFilteredSchema(filters, thriftClass);        fail("should throw projection exception when filter matches nothing");    } catch (ThriftProjectionException e) {        assertEquals("The following projection patterns did not match any columns in this schema:\n" + unmatchedFilter + "\n", e.getMessage());    }}
0
private void shouldThrowWhenNoColumnsAreSelected(String filters, Class<? extends TBase<?, ?>> thriftClass)
{    try {        getDeprecatedFilteredSchema(filters, thriftClass);        fail("should throw projection exception when no columns are selected");    } catch (ThriftProjectionException e) {        assertEquals("No columns have been selected", e.getMessage());    }}
0
public void testThrowWhenNoColumnsAreSelected()
{    shouldThrowWhenNoColumnsAreSelected("non_existing", TestStructInMap.class);}
0
public void testThrowWhenProjectionFilterMatchesNothing()
{    shouldThrowWhenProjectionFilterMatchesNothing("name;non_existing", "non_existing", TestStructInMap.class);    shouldThrowWhenProjectionFilterMatchesNothing("**;non_existing", "non_existing", TestStructInMap.class);    shouldThrowWhenProjectionFilterMatchesNothing("**;names/non_existing", "names/non_existing", TestStructInMap.class);    shouldThrowWhenProjectionFilterMatchesNothing("**;names/non_existing;non_existing", "names/non_existing\nnon_existing", TestStructInMap.class);}
0
public void testProjectOnlyValueInMap()
{    try {        getDeprecatedFilteredSchema("name;names/value/**", TestStructInMap.class);        fail("this should throw");    } catch (ThriftProjectionException e) {        assertEquals("Cannot select only the values of a map, you must keep the keys as well: names", e.getMessage());    }    try {        getStrictFilteredSchema("name;names.value", TestStructInMap.class);        fail("this should throw");    } catch (ThriftProjectionException e) {        assertEquals("Cannot select only the values of a map, you must keep the keys as well: names", e.getMessage());    }}
0
private void doTestPartialKeyProjection(String deprecated, String strict)
{    try {        getDeprecatedFilteredSchema(deprecated, MapStructV2.class);        fail("this should throw");    } catch (ThriftProjectionException e) {        assertEquals("Cannot select only a subset of the fields in a map key, for path map1", e.getMessage());    }    try {        getStrictFilteredSchema(strict, MapStructV2.class);        fail("this should throw");    } catch (ThriftProjectionException e) {        assertEquals("Cannot select only a subset of the fields in a map key, for path map1", e.getMessage());    }}
0
public void testPartialKeyProjection()
{    doTestPartialKeyProjection("map1/key/age", "map1.key.age");    doTestPartialKeyProjection("map1/key/age;map1/value/**", "map1.{key.age,value}");}
0
public void testSetPartialProjection()
{    try {        getDeprecatedFilteredSchema("set1/age", SetStructV2.class);        fail("this should throw");    } catch (ThriftProjectionException e) {        assertEquals("Cannot select only a subset of the fields in a set, for path set1", e.getMessage());    }    try {        getStrictFilteredSchema("set1.age", SetStructV2.class);        fail("this should throw");    } catch (ThriftProjectionException e) {        assertEquals("Cannot select only a subset of the fields in a set, for path set1", e.getMessage());    }}
0
public void testConvertStructCreatedViaDeprecatedConstructor()
{    String expected = "message ParquetSchema {\n" + "  required binary a (UTF8) = 1;\n" + "  required binary b (UTF8) = 2;\n" + "}\n";    ThriftSchemaConverter converter = new ThriftSchemaConverter();    StructType structType = new StructType(Arrays.asList(new ThriftField("a", (short) 1, REQUIRED, new ThriftType.StringType()), new ThriftField("b", (short) 2, REQUIRED, new ThriftType.StringType())));    final MessageType converted = converter.convert(structType);    assertEquals(MessageTypeParser.parseMessageType(expected), converted);}
0
public static void shouldGetProjectedSchema(String deprecatedFilterDesc, String strictFilterDesc, String expectedSchemaStr, Class<? extends TBase<?, ?>> thriftClass)
{    MessageType depRequestedSchema = getDeprecatedFilteredSchema(deprecatedFilterDesc, thriftClass);    MessageType strictRequestedSchema = getStrictFilteredSchema(strictFilterDesc, thriftClass);    MessageType expectedSchema = parseMessageType(expectedSchemaStr);    assertEquals(expectedSchema, depRequestedSchema);    assertEquals(expectedSchema, strictRequestedSchema);}
0
private static MessageType getDeprecatedFilteredSchema(String filterDesc, Class<? extends TBase<?, ?>> thriftClass)
{    DeprecatedFieldProjectionFilter fieldProjectionFilter = new DeprecatedFieldProjectionFilter(filterDesc);    return new ThriftSchemaConverter(fieldProjectionFilter).convert(thriftClass);}
0
private static MessageType getStrictFilteredSchema(String semicolonDelimitedString, Class<? extends TBase<?, ?>> thriftClass)
{    StrictFieldProjectionFilter fieldProjectionFilter = StrictFieldProjectionFilter.fromSemicolonDelimitedString(semicolonDelimitedString);    return new ThriftSchemaConverter(fieldProjectionFilter).convert(thriftClass);}
0
public void testToThriftType() throws Exception
{    final StructType converted = ThriftSchemaConverter.toStructType(AddressBook.class);    final String json = converted.toJSON();    final ThriftType fromJSON = StructType.fromJSON(json);    assertEquals(json, fromJSON.toJSON());}
0
public void testTopLevelUnions()
{        shouldGetProjectedSchema("aLong/**", "aLong", "message ParquetSchema {\n" + "  optional group aString = 1 {\n" + "    required binary s (UTF8) = 1;\n" + "  }\n" + "  optional group aLong = 2 {\n" + "    required int64 l = 1;\n" + "  }\n" + "  optional group aNewBool = 3 {\n" + "    required boolean b = 1;\n" + "  }\n" + "}", UnionV2.class);            shouldGetProjectedSchema("aNewBool/**", "aNewBool", "message ParquetSchema {\n" + "  optional group structV3 = 1 {\n" + "    required binary name (UTF8) = 1;\n" + "  }\n" + "  optional group structV4 = 2 {\n" + "    required binary name (UTF8) = 1;\n" + "  }\n" + "  optional group aNewBool = 3 {\n" + "    required boolean b = 1;\n" + "  }\n" + "}", UnionOfStructs.class);}
0
public void optionalUnionShouldBeDropped()
{    shouldGetProjectedSchema("name", "name", "message ParquetSchema {\n" + "  required binary name (UTF8) = 1;\n" + "}", StructWithOptionalUnionOfStructs.class);}
0
public void optionalUnionInRequiredStructShouldBeDropped()
{    shouldGetProjectedSchema("name", "name", "message ParquetSchema {\n" + "  required binary name (UTF8) = 1;\n" + "}", OptionalInsideRequired.class);}
0
public void requiredUnionInsideOptionalStructShouldBeDropped()
{        shouldGetProjectedSchema("name", "name", "message ParquetSchema {\n" + "  required binary name (UTF8) = 1;\n" + "}", RequiredInsideOptional.class);}
0
public void requiredUnionInsideOptionalStructShouldBeKeptIfParentSelected()
{    shouldGetProjectedSchema("aStruct/name", "aStruct.name", "message ParquetSchema {\n" + "  optional group aStruct = 2 {\n" + "    required binary name (UTF8) = 1;\n" + "    required group aUnion = 2 {\n" + "      optional group structV3 = 1 {\n" + "        required binary name (UTF8) = 1;\n" + "      }\n" + "      optional group structV4 = 2 {\n" + "        required binary name (UTF8) = 1;\n" + "      }\n" + "      optional group aNewBool = 3 {\n" + "        required boolean b = 1;\n" + "      }\n" + "    }\n" + "  }\n" + "}", RequiredInsideOptional.class);}
0
public void selectingOneUnionMemberKeepsSentinels()
{    shouldGetProjectedSchema("aUnion/structV4/addedStruct/gender", "aUnion.structV4.addedStruct.gender", "message ParquetSchema {\n" + "  optional group aUnion = 2 {\n" + "    optional group structV3 = 1 {\n" + "      required binary name (UTF8) = 1;\n" + "    }\n" + "    optional group structV4 = 2 {\n" + "      optional group addedStruct = 4 {\n" + "        optional binary gender (UTF8) = 3;\n" + "      }\n" + "    }\n" + "    optional group aNewBool = 3 {\n" + "      required boolean b = 1;\n" + "    }\n" + "  }\n" + "}", StructWithOptionalUnionOfStructs.class);}
0
public void testUnionInsideUnion()
{    shouldGetProjectedSchema("structV3/age", "structV3.age", "message ParquetSchema {\n" + "  optional group structV3 = 1 {\n" + "    optional binary age (UTF8) = 2;\n" + "  }\n" + "  optional group unionOfStructs = 2 {\n" + "    optional group structV3 = 1 {\n" + "      required binary name (UTF8) = 1;\n" + "    }\n" + "    optional group structV4 = 2 {\n" + "      required binary name (UTF8) = 1;\n" + "    }\n" + "    optional group aNewBool = 3 {\n" + "      required boolean b = 1;\n" + "    }\n" + "  }\n" + "  optional group aLong = 3 {\n" + "    required int64 l = 1;\n" + "  }\n" + "}", NestedUnion.class);    shouldGetProjectedSchema("unionOfStructs/structV4/addedStruct/gender", "unionOfStructs.structV4.addedStruct.gender", "message ParquetSchema {\n" + "  optional group structV3 = 1 {\n" + "    required binary name (UTF8) = 1;\n" + "  }\n" + "  optional group unionOfStructs = 2 {\n" + "    optional group structV3 = 1 {\n" + "      required binary name (UTF8) = 1;\n" + "    }\n" + "    optional group structV4 = 2 {\n" + "      optional group addedStruct = 4 {\n" + "        optional binary gender (UTF8) = 3;\n" + "      }\n" + "    }\n" + "    optional group aNewBool = 3 {\n" + "      required boolean b = 1;\n" + "    }\n" + "  }\n" + "  optional group aLong = 3 {\n" + "    required int64 l = 1;\n" + "  }\n" + "}\n", NestedUnion.class);    shouldGetProjectedSchema("unionV2/aLong/**", "unionV2.aLong", "message ParquetSchema {\n" + "  optional group nestedUnion = 1 {\n" + "    optional group structV3 = 1 {\n" + "      required binary name (UTF8) = 1;\n" + "    }\n" + "    optional group unionOfStructs = 2 {\n" + "      optional group structV3 = 1 {\n" + "        required binary name (UTF8) = 1;\n" + "      }\n" + "      optional group structV4 = 2 {\n" + "        required binary name (UTF8) = 1;\n" + "      }\n" + "      optional group aNewBool = 3 {\n" + "        required boolean b = 1;\n" + "      }\n" + "    }\n" + "    optional group aLong = 3 {\n" + "      required int64 l = 1;\n" + "    }\n" + "  }\n" + "  optional group unionV2 = 2 {\n" + "    optional group aString = 1 {\n" + "      required binary s (UTF8) = 1;\n" + "    }\n" + "    optional group aLong = 2 {\n" + "      required int64 l = 1;\n" + "    }\n" + "    optional group aNewBool = 3 {\n" + "      required boolean b = 1;\n" + "    }\n" + "  }\n" + "}", NestedNestedUnion.class);}
0
public void testListOfUnions()
{                    shouldGetProjectedSchema("optListUnion/structV3/age", "optListUnion.structV3.age", "message ParquetSchema {\n" + "  optional group optListUnion (LIST) = 1 {\n" + "    repeated group optListUnion_tuple {\n" + "      optional group structV3 = 1 {\n" + "        optional binary age (UTF8) = 2;\n" + "      }\n" + "      optional group structV4 = 2 {\n" + "        required binary name (UTF8) = 1;\n" + "      }\n" + "      optional group aNewBool = 3 {\n" + "        required boolean b = 1;\n" + "      }\n" + "    }\n" + "  }\n" + "}", ListOfUnions.class);            shouldGetProjectedSchema("reqListUnion/structV3/age", "reqListUnion.structV3.age", "message ParquetSchema {\n" + "  required group reqListUnion (LIST) = 2 {\n" + "    repeated group reqListUnion_tuple {\n" + "      optional group structV3 = 1 {\n" + "        optional binary age (UTF8) = 2;\n" + "      }\n" + "      optional group structV4 = 2 {\n" + "        required binary name (UTF8) = 1;\n" + "      }\n" + "      optional group aNewBool = 3 {\n" + "        required boolean b = 1;\n" + "      }\n" + "    }\n" + "  }\n" + "}", ListOfUnions.class);}
0
public void testMapWithUnionKey()
{    shouldGetProjectedSchema("optMapWithUnionKey/key/**", "optMapWithUnionKey.key", "message ParquetSchema {\n" + "  optional group optMapWithUnionKey (MAP) = 1 {\n" + "    repeated group map (MAP_KEY_VALUE) {\n" + "      required group key {\n" + "        optional group structV3 = 1 {\n" + "          required binary name (UTF8) = 1;\n" + "          optional binary age (UTF8) = 2;\n" + "          optional binary gender (UTF8) = 3;\n" + "        }\n" + "        optional group structV4 = 2 {\n" + "          required binary name (UTF8) = 1;\n" + "          optional binary age (UTF8) = 2;\n" + "          optional binary gender (UTF8) = 3;\n" + "          optional group addedStruct = 4 {\n" + "            required binary name (UTF8) = 1;\n" + "            optional binary age (UTF8) = 2;\n" + "            optional binary gender (UTF8) = 3;\n" + "          }\n" + "        }\n" + "        optional group aNewBool = 3 {\n" + "          required boolean b = 1;\n" + "        }\n" + "      }\n" + "      optional group value {\n" + "        required binary name (UTF8) = 1;\n" + "      } " + "    }\n" + "  }\n" + "}", MapWithUnionKey.class);    shouldGetProjectedSchema("optMapWithUnionKey/key/**;optMapWithUnionKey/value/gender", "optMapWithUnionKey.{key,value.gender}", "message ParquetSchema {\n" + "  optional group optMapWithUnionKey (MAP) = 1 {\n" + "    repeated group map (MAP_KEY_VALUE) {\n" + "      required group key {\n" + "        optional group structV3 = 1 {\n" + "          required binary name (UTF8) = 1;\n" + "          optional binary age (UTF8) = 2;\n" + "          optional binary gender (UTF8) = 3;\n" + "        }\n" + "        optional group structV4 = 2 {\n" + "          required binary name (UTF8) = 1;\n" + "          optional binary age (UTF8) = 2;\n" + "          optional binary gender (UTF8) = 3;\n" + "          optional group addedStruct = 4 {\n" + "            required binary name (UTF8) = 1;\n" + "            optional binary age (UTF8) = 2;\n" + "            optional binary gender (UTF8) = 3;\n" + "          }\n" + "        }\n" + "        optional group aNewBool = 3 {\n" + "          required boolean b = 1;\n" + "        }\n" + "      }\n" + "      optional group value {\n" + "        optional binary gender (UTF8) = 3;\n" + "      }\n" + "    }\n" + "  }\n" + "}", MapWithUnionKey.class);}
0
public void testMapWithUnionValue()
{    shouldGetProjectedSchema("optMapWithUnionValue/key/**;optMapWithUnionValue/value/structV4/addedStruct/gender", "optMapWithUnionValue.{key,value.structV4.addedStruct.gender}", "message ParquetSchema {\n" + "  optional group optMapWithUnionValue (MAP) = 1 {\n" + "    repeated group map (MAP_KEY_VALUE) {\n" + "      required group key {\n" + "        required binary name (UTF8) = 1;\n" + "        optional binary age (UTF8) = 2;\n" + "        optional binary gender (UTF8) = 3;\n" + "      }\n" + "      optional group value {\n" + "        optional group structV3 = 1 {\n" + "          required binary name (UTF8) = 1;\n" + "        }\n" + "        optional group structV4 = 2 {\n" + "          optional group addedStruct = 4 {\n" + "            optional binary gender (UTF8) = 3;\n" + "          }\n" + "        }\n" + "        optional group aNewBool = 3 {\n" + "          required boolean b = 1;\n" + "        }\n" + "      }\n" + "    }\n" + "  }\n" + "}", MapWithUnionValue.class);}
0
public void testMessyNestedUnions()
{    shouldGetProjectedSchema("reqStructWithUnionV2/name", "reqStructWithUnionV2.name", "message ParquetSchema {\n" + "  required group reqUnionOfStructs = 2 {\n" + "    optional group structV3 = 1 {\n" + "      required binary name (UTF8) = 1;\n" + "    }\n" + "    optional group structV4 = 2 {\n" + "      required binary name (UTF8) = 1;\n" + "    }\n" + "    optional group aNewBool = 3 {\n" + "      required boolean b = 1;\n" + "    }\n" + "  }\n" + "  required group reqNestedUnion = 5 {\n" + "    optional group structV3 = 1 {\n" + "      required binary name (UTF8) = 1;\n" + "    }\n" + "    optional group unionOfStructs = 2 {\n" + "      optional group structV3 = 1 {\n" + "        required binary name (UTF8) = 1;\n" + "      }\n" + "      optional group structV4 = 2 {\n" + "        required binary name (UTF8) = 1;\n" + "      }\n" + "      optional group aNewBool = 3 {\n" + "        required boolean b = 1;\n" + "      }\n" + "    }\n" + "    optional group aLong = 3 {\n" + "      required int64 l = 1;\n" + "    }\n" + "  }\n" + "  required group reqStructWithUnionV2 = 8 {\n" + "    required binary name (UTF8) = 1;\n" + "    required group aUnion = 2 {\n" + "      optional group aString = 1 {\n" + "        required binary s (UTF8) = 1;\n" + "      }\n" + "      optional group aLong = 2 {\n" + "        required int64 l = 1;\n" + "      }\n" + "      optional group aNewBool = 3 {\n" + "        required boolean b = 1;\n" + "      }\n" + "    }\n" + "  }\n" + "  required group reqUnionStructUnion = 11 {\n" + "    optional group structV3 = 1 {\n" + "      required binary name (UTF8) = 1;\n" + "    }\n" + "    optional group structWithUnionOfStructs = 2 {\n" + "      required binary name (UTF8) = 1;\n" + "      required group aUnion = 2 {\n" + "        optional group structV3 = 1 {\n" + "          required binary name (UTF8) = 1;\n" + "        }\n" + "        optional group structV4 = 2 {\n" + "          required binary name (UTF8) = 1;\n" + "        }\n" + "        optional group aNewBool = 3 {\n" + "          required boolean b = 1;\n" + "        }\n" + "      }\n" + "    }\n" + "    optional group aLong = 3 {\n" + "      required int64 l = 1;\n" + "    }\n" + "  }\n" + "}", StructWithNestedUnion.class);}
0
public void testMap() throws Exception
{    Map<String, String> map = new TreeMap<String, String>();    map.put("foo", "bar");    map.put("foo2", "bar2");    TestMap testMap = new TestMap("map_name", map);    validateSameTupleAsEB(testMap);}
0
public void testMapInSet() throws Exception
{    final Set<Map<String, String>> set = new HashSet<Map<String, String>>();    final Map<String, String> map = new HashMap<String, String>();    map.put("foo", "bar");    set.add(map);    TestMapInSet o = new TestMapInSet("top", set);    validateSameTupleAsEB(o);}
0
public void testStructInMap() throws Exception
{    final Map<String, TestPerson> map = new HashMap<String, TestPerson>();    map.put("foo", new TestPerson(new TestName("john", "johnson"), new HashMap<TestPhoneType, String>()));    final Map<String, Integer> stringToIntMap = Collections.singletonMap("bar", 10);    TestStructInMap testMap = new TestStructInMap("map_name", map, stringToIntMap);    validateSameTupleAsEB(testMap);}
0
public void testProtocolEmptyAdressBook() throws Exception
{    AddressBook a = new AddressBook(new ArrayList<Person>());    validateSameTupleAsEB(a);}
0
public void testProtocolAddressBook() throws Exception
{    ArrayList<Person> persons = new ArrayList<Person>();    final PhoneNumber phoneNumber = new PhoneNumber("555 999 9998");    phoneNumber.type = PhoneType.HOME;    persons.add(new Person(new Name("Bob", "Roberts"), 1, "bob@roberts.com", Arrays.asList(new PhoneNumber("555 999 9999"), phoneNumber)));    persons.add(new Person(new Name("Dick", "Richardson"), 2, "dick@richardson.com", Arrays.asList(new PhoneNumber("555 999 9997"), new PhoneNumber("555 999 9996"))));    AddressBook a = new AddressBook(persons);    validateSameTupleAsEB(a);}
0
public void testOneOfEach() throws Exception
{    OneOfEach a = new OneOfEach(true, false, (byte) 8, (short) 16, (int) 32, (long) 64, (double) 1234, "string", "å", false, ByteBuffer.wrap("a".getBytes()), new ArrayList<Byte>(), new ArrayList<Short>(), new ArrayList<Long>());    validateSameTupleAsEB(a);}
0
public void testStringList() throws Exception
{    final List<String> names = new ArrayList<String>();    names.add("John");    names.add("Jack");    TestNameList o = new TestNameList("name", names);    validateSameTupleAsEB(o);}
0
public static void validateSameTupleAsEB(T o) throws TException
{    final ThriftSchemaConverter thriftSchemaConverter = new ThriftSchemaConverter();    @SuppressWarnings("unchecked")    final Class<T> class1 = (Class<T>) o.getClass();    final MessageType schema = thriftSchemaConverter.convert(class1);    final StructType structType = ThriftSchemaConverter.toStructType(class1);    final ThriftToPig<T> thriftToPig = new ThriftToPig<T>(class1);    final Schema pigSchema = thriftToPig.toSchema();    final TupleRecordMaterializer tupleRecordConverter = new TupleRecordMaterializer(schema, pigSchema, true);    RecordConsumer recordConsumer = new ConverterConsumer(tupleRecordConverter.getRootConverter(), schema);    final MessageColumnIO columnIO = new ColumnIOFactory().getColumnIO(schema);    ParquetWriteProtocol p = new ParquetWriteProtocol(new RecordConsumerLoggingWrapper(recordConsumer), columnIO, structType);    o.write(p);    final Tuple t = tupleRecordConverter.getCurrentRecord();    final Tuple expected = thriftToPig.getPigTuple(o);    assertEquals(expected.toString(), t.toString());    final MessageType filtered = new PigSchemaConverter().filter(schema, pigSchema);    assertEquals(schema.toString(), filtered.toString());}
0
public Options getOptions()
{    return null;}
0
public boolean supportsExtraArgs()
{    return true;}
0
public void execute(CommandLine options) throws Exception
{    String[] args = options.getArgs();    if (args.length < min) {        throw new MissingArgumentException("missing required arguments");    }    if (args.length > max) {        throw new UnrecognizedOptionException("unknown extra argument \"" + args[max] + "\"");    }}
0
public String[] getUsageDescription()
{    return USAGE;}
0
public String getCommandDescription()
{    return "Prints the content of a Parquet file. The output contains only the data, no metadata is displayed";}
0
public Options getOptions()
{    return OPTIONS;}
0
public void execute(CommandLine options) throws Exception
{    super.execute(options);    String[] args = options.getArgs();    String input = args[0];    ParquetReader<SimpleRecord> reader = null;    try {        PrintWriter writer = new PrintWriter(Main.out, true);        reader = ParquetReader.builder(new SimpleReadSupport(), new Path(input)).build();        ParquetMetadata metadata = ParquetFileReader.readFooter(new Configuration(), new Path(input));        JsonRecordFormatter.JsonGroupFormatter formatter = JsonRecordFormatter.fromSchema(metadata.getFileMetaData().getSchema());        for (SimpleRecord value = reader.read(); value != null; value = reader.read()) {            if (options.hasOption('j')) {                writer.write(formatter.formatRecord(value));            } else {                value.prettyPrint(writer);            }            writer.println();        }    } finally {        if (reader != null) {            try {                reader.close();            } catch (Exception ex) {            }        }    }}
0
public String[] getUsageDescription()
{    return USAGE;}
0
public String getCommandDescription()
{    return "Prints the column and offset indexes of a Parquet file.";}
0
public Options getOptions()
{    return OPTIONS;}
0
public void execute(CommandLine options) throws Exception
{    super.execute(options);    String[] args = options.getArgs();    InputFile in = HadoopInputFile.fromPath(new Path(args[0]), new Configuration());    PrintWriter out = new PrintWriter(Main.out, true);    String rowGroupValue = options.getOptionValue("r");    Set<String> indexes = new HashSet<>();    if (rowGroupValue != null) {        indexes.addAll(Arrays.asList(rowGroupValue.split("\\s*,\\s*")));    }    boolean showColumnIndex = options.hasOption("i");    boolean showOffsetIndex = options.hasOption("o");    if (!showColumnIndex && !showOffsetIndex) {        showColumnIndex = true;        showOffsetIndex = true;    }    try (ParquetFileReader reader = ParquetFileReader.open(in)) {        boolean firstBlock = true;        int rowGroupIndex = 0;        for (BlockMetaData block : reader.getFooter().getBlocks()) {            if (!indexes.isEmpty() && !indexes.contains(Integer.toString(rowGroupIndex))) {                ++rowGroupIndex;                continue;            }            if (!firstBlock) {                out.println();                firstBlock = false;            }            out.format("row group %d:%n", rowGroupIndex);            for (ColumnChunkMetaData column : getColumns(block, options)) {                String path = column.getPath().toDotString();                if (showColumnIndex) {                    out.format("column index for column %s:%n", path);                    ColumnIndex columnIndex = reader.readColumnIndex(column);                    if (columnIndex == null) {                        out.println("NONE");                    } else {                        out.println(columnIndex);                    }                }                if (showOffsetIndex) {                    out.format("offset index for column %s:%n", path);                    OffsetIndex offsetIndex = reader.readOffsetIndex(column);                    if (offsetIndex == null) {                        out.println("NONE");                    } else {                        out.println(offsetIndex);                    }                }            }            ++rowGroupIndex;        }    }}
0
private static List<ColumnChunkMetaData> getColumns(BlockMetaData block, CommandLine options)
{    List<ColumnChunkMetaData> columns = block.getColumns();    String pathValue = options.getOptionValue("c");    if (pathValue == null) {        return columns;    }    String[] paths = pathValue.split("\\s*,\\s*");    Map<String, ColumnChunkMetaData> pathMap = new HashMap<>();    for (ColumnChunkMetaData column : columns) {        pathMap.put(column.getPath().toDotString(), column);    }    List<ColumnChunkMetaData> filtered = new ArrayList<>();    for (String path : paths) {        ColumnChunkMetaData column = pathMap.get(path);        if (column != null) {            filtered.add(column);        }    }    return filtered;}
0
public Options getOptions()
{    return OPTIONS;}
0
public String[] getUsageDescription()
{    return USAGE;}
0
public String getCommandDescription()
{    return "Prints the content and metadata of a Parquet file";}
0
public void execute(CommandLine options) throws Exception
{    super.execute(options);    String[] args = options.getArgs();    String input = args[0];    Configuration conf = new Configuration();    Path inpath = new Path(input);    ParquetMetadata metaData = ParquetFileReader.readFooter(conf, inpath, NO_FILTER);    MessageType schema = metaData.getFileMetaData().getSchema();    boolean showmd = !options.hasOption('m');    boolean showdt = !options.hasOption('d');    boolean cropoutput = !options.hasOption('n');    Set<String> showColumns = null;    if (options.hasOption('c')) {        String[] cols = options.getOptionValues('c');        showColumns = new HashSet<String>(Arrays.asList(cols));    }    PrettyPrintWriter out = prettyPrintWriter(cropoutput);    dump(out, metaData, schema, inpath, showmd, showdt, showColumns);}
0
public static void dump(PrettyPrintWriter out, ParquetMetadata meta, MessageType schema, Path inpath, boolean showmd, boolean showdt, Set<String> showColumns) throws IOException
{    Configuration conf = new Configuration();    List<BlockMetaData> blocks = meta.getBlocks();    List<ColumnDescriptor> columns = schema.getColumns();    if (showColumns != null) {        columns = new ArrayList<ColumnDescriptor>();        for (ColumnDescriptor column : schema.getColumns()) {            String path = Joiner.on('.').skipNulls().join(column.getPath());            if (showColumns.contains(path)) {                columns.add(column);            }        }    }    ParquetFileReader freader = null;    if (showmd) {        try {            long group = 0;            for (BlockMetaData block : blocks) {                if (group != 0)                    out.println();                out.format("row group %d%n", group++);                out.rule('-');                List<ColumnChunkMetaData> ccmds = block.getColumns();                if (showColumns != null) {                    ccmds = new ArrayList<ColumnChunkMetaData>();                    for (ColumnChunkMetaData ccmd : block.getColumns()) {                        String path = Joiner.on('.').skipNulls().join(ccmd.getPath().toArray());                        if (showColumns.contains(path)) {                            ccmds.add(ccmd);                        }                    }                }                MetadataUtils.showDetails(out, ccmds);                List<BlockMetaData> rblocks = Collections.singletonList(block);                freader = new ParquetFileReader(conf, meta.getFileMetaData(), inpath, rblocks, columns);                PageReadStore store = freader.readNextRowGroup();                while (store != null) {                    out.incrementTabLevel();                    for (ColumnDescriptor column : columns) {                        out.println();                        dump(out, store, column);                    }                    out.decrementTabLevel();                    store = freader.readNextRowGroup();                }                out.flushColumns();            }        } finally {            if (freader != null) {                freader.close();            }        }    }    if (showdt) {        boolean first = true;        for (ColumnDescriptor column : columns) {            if (!first || showmd)                out.println();            first = false;            out.format("%s %s%n", column.getType(), Joiner.on('.').skipNulls().join(column.getPath()));            out.rule('-');            try {                long page = 1;                long total = blocks.size();                long offset = 1;                freader = new ParquetFileReader(conf, meta.getFileMetaData(), inpath, blocks, Collections.singletonList(column));                PageReadStore store = freader.readNextRowGroup();                while (store != null) {                    ColumnReadStoreImpl crstore = new ColumnReadStoreImpl(store, new DumpGroupConverter(), schema, meta.getFileMetaData().getCreatedBy());                    dump(out, crstore, column, page++, total, offset);                    offset += store.getRowCount();                    store = freader.readNextRowGroup();                }                out.flushColumns();            } finally {                out.flushColumns();                if (freader != null) {                    freader.close();                }            }        }    }}
0
private static boolean verifyCrc(int referenceCrc, byte[] bytes)
{    crc.reset();    crc.update(bytes);    return crc.getValue() == ((long) referenceCrc & 0xffffffffL);}
0
public static void dump(final PrettyPrintWriter out, PageReadStore store, ColumnDescriptor column) throws IOException
{    PageReader reader = store.getPageReader(column);    long vc = reader.getTotalValueCount();    int rmax = column.getMaxRepetitionLevel();    int dmax = column.getMaxDefinitionLevel();    out.format("%s TV=%d RL=%d DL=%d", Joiner.on('.').skipNulls().join(column.getPath()), vc, rmax, dmax);    DictionaryPage dict = reader.readDictionaryPage();    if (dict != null) {        out.format(" DS:%d", dict.getDictionarySize());        out.format(" DE:%s", dict.getEncoding());    }    out.println();    out.rule('-');    DataPage page = reader.readPage();    for (long count = 0; page != null; count++) {        out.format("page %d:", count);        page.accept(new Visitor<Void>() {            @Override            public Void visit(DataPageV1 pageV1) {                out.format(" DLE:%s", pageV1.getDlEncoding());                out.format(" RLE:%s", pageV1.getRlEncoding());                out.format(" VLE:%s", pageV1.getValueEncoding());                Statistics<?> statistics = pageV1.getStatistics();                if (statistics != null) {                    out.format(" ST:[%s]", statistics);                } else {                    out.format(" ST:[none]");                }                if (pageV1.getCrc().isPresent()) {                    try {                        out.format(" CRC:%s", verifyCrc(pageV1.getCrc().getAsInt(), pageV1.getBytes().toByteArray()) ? "[verified]" : "[PAGE CORRUPT]");                    } catch (IOException e) {                        out.format(" CRC:[error getting page bytes]");                    }                } else {                    out.format(" CRC:[none]");                }                return null;            }            @Override            public Void visit(DataPageV2 pageV2) {                out.format(" DLE:RLE");                out.format(" RLE:RLE");                out.format(" VLE:%s", pageV2.getDataEncoding());                Statistics<?> statistics = pageV2.getStatistics();                if (statistics != null) {                    out.format(" ST:[%s]", statistics);                } else {                    out.format(" ST:[none]");                }                return null;            }        });        out.format(" SZ:%d", page.getUncompressedSize());        out.format(" VC:%d", page.getValueCount());        out.println();        page = reader.readPage();    }}
0
public Void visit(DataPageV1 pageV1)
{    out.format(" DLE:%s", pageV1.getDlEncoding());    out.format(" RLE:%s", pageV1.getRlEncoding());    out.format(" VLE:%s", pageV1.getValueEncoding());    Statistics<?> statistics = pageV1.getStatistics();    if (statistics != null) {        out.format(" ST:[%s]", statistics);    } else {        out.format(" ST:[none]");    }    if (pageV1.getCrc().isPresent()) {        try {            out.format(" CRC:%s", verifyCrc(pageV1.getCrc().getAsInt(), pageV1.getBytes().toByteArray()) ? "[verified]" : "[PAGE CORRUPT]");        } catch (IOException e) {            out.format(" CRC:[error getting page bytes]");        }    } else {        out.format(" CRC:[none]");    }    return null;}
0
public Void visit(DataPageV2 pageV2)
{    out.format(" DLE:RLE");    out.format(" RLE:RLE");    out.format(" VLE:%s", pageV2.getDataEncoding());    Statistics<?> statistics = pageV2.getStatistics();    if (statistics != null) {        out.format(" ST:[%s]", statistics);    } else {        out.format(" ST:[none]");    }    return null;}
0
public static void dump(PrettyPrintWriter out, ColumnReadStoreImpl crstore, ColumnDescriptor column, long page, long total, long offset) throws IOException
{    int dmax = column.getMaxDefinitionLevel();    ColumnReader creader = crstore.getColumnReader(column);    out.format("*** row group %d of %d, values %d to %d ***%n", page, total, offset, offset + creader.getTotalValueCount() - 1);    for (long i = 0, e = creader.getTotalValueCount(); i < e; ++i) {        int rlvl = creader.getCurrentRepetitionLevel();        int dlvl = creader.getCurrentDefinitionLevel();        out.format("value %d: R:%d D:%d V:", offset + i, rlvl, dlvl);        if (dlvl == dmax) {            PrimitiveStringifier stringifier = column.getPrimitiveType().stringifier();            switch(column.getType()) {                case FIXED_LEN_BYTE_ARRAY:                case INT96:                case BINARY:                    out.print(stringifier.stringify(creader.getBinary()));                    break;                case BOOLEAN:                    out.print(stringifier.stringify(creader.getBoolean()));                    break;                case DOUBLE:                    out.print(stringifier.stringify(creader.getDouble()));                    break;                case FLOAT:                    out.print(stringifier.stringify(creader.getFloat()));                    break;                case INT32:                    out.print(stringifier.stringify(creader.getInteger()));                    break;                case INT64:                    out.print(stringifier.stringify(creader.getLong()));                    break;            }        } else {            out.format("<null>");        }        out.println();        creader.consume();    }}
0
public static String binaryToString(Binary value)
{    byte[] data = value.getBytesUnsafe();    if (data == null)        return null;    try {        CharBuffer buffer = UTF8_DECODER.decode(value.toByteBuffer());        return buffer.toString();    } catch (Exception ex) {    }    return "<bytes...>";}
0
public static BigInteger binaryToBigInteger(Binary value)
{    byte[] data = value.getBytesUnsafe();    if (data == null)        return null;    return new BigInteger(data);}
0
private static PrettyPrintWriter prettyPrintWriter(boolean cropOutput)
{    PrettyPrintWriter.Builder builder = PrettyPrintWriter.stdoutPrettyPrinter().withAutoColumn().withWhitespaceHandler(WhiteSpaceHandler.ELIMINATE_NEWLINES).withColumnPadding(1).withMaxBufferedLines(1000000).withFlushOnTab();    if (cropOutput) {        builder.withAutoCrop();    }    return builder.build();}
0
public void start()
{}
0
public void end()
{}
0
public Converter getConverter(int fieldIndex)
{    return new DumpConverter();}
0
public GroupConverter asGroupConverter()
{    return new DumpGroupConverter();}
0
public Options getOptions()
{    return OPTIONS;}
0
public String[] getUsageDescription()
{    return USAGE;}
0
public String getCommandDescription()
{    return "Prints the first n record of the Parquet file";}
0
public void execute(CommandLine options) throws Exception
{    super.execute(options);    long num = DEFAULT;    if (options.hasOption('n')) {        num = Long.parseLong(options.getOptionValue('n'));    }    String[] args = options.getArgs();    String input = args[0];    ParquetReader<SimpleRecord> reader = null;    try {        PrintWriter writer = new PrintWriter(Main.out, true);        reader = ParquetReader.builder(new SimpleReadSupport(), new Path(input)).build();        for (SimpleRecord value = reader.read(); value != null && num-- > 0; value = reader.read()) {            value.prettyPrint(writer);            writer.println();        }    } finally {        if (reader != null) {            try {                reader.close();            } catch (Exception ex) {            }        }    }}
0
public String[] getUsageDescription()
{    return USAGE;}
0
public String getCommandDescription()
{    return "Merges multiple Parquet files into one. " + "The command doesn't merge row groups, just places one after the other. " + "When used to merge many small files, the resulting file will still contain small row groups, " + "which usually leads to bad query performance.";}
0
public void execute(CommandLine options) throws Exception
{        List<String> args = options.getArgList();    List<Path> inputFiles = getInputFiles(args.subList(0, args.size() - 1));    Path outputFile = new Path(args.get(args.size() - 1));        FileMetaData mergedMeta = mergedMetadata(inputFiles);    PrintWriter out = new PrintWriter(Main.out, true);        ParquetFileWriter writer = new ParquetFileWriter(conf, mergedMeta.getSchema(), outputFile, ParquetFileWriter.Mode.CREATE);    writer.start();    boolean tooSmallFilesMerged = false;    for (Path input : inputFiles) {        if (input.getFileSystem(conf).getFileStatus(input).getLen() < TOO_SMALL_FILE_THRESHOLD) {            out.format("Warning: file %s is too small, length: %d\n", input, input.getFileSystem(conf).getFileStatus(input).getLen());            tooSmallFilesMerged = true;        }        writer.appendFile(HadoopInputFile.fromPath(input, conf));    }    if (tooSmallFilesMerged) {        out.println("Warning: you merged too small files. " + "Although the size of the merged file is bigger, it STILL contains small row groups, thus you don't have the advantage of big row groups, " + "which usually leads to bad query performance!");    }    writer.end(mergedMeta.getKeyValueMetaData());}
0
private FileMetaData mergedMetadata(List<Path> inputFiles) throws IOException
{    return ParquetFileWriter.mergeMetadataFiles(inputFiles, conf).getFileMetaData();}
0
private List<Path> getInputFiles(List<String> input) throws IOException
{    List<Path> inputFiles = null;    if (input.size() == 1) {        Path p = new Path(input.get(0));        FileSystem fs = p.getFileSystem(conf);        FileStatus status = fs.getFileStatus(p);        if (status.isDir()) {            inputFiles = getInputFilesFromDirectory(status);        }    } else {        inputFiles = parseInputFiles(input);    }    checkParquetFiles(inputFiles);    return inputFiles;}
0
private void checkParquetFiles(List<Path> inputFiles) throws IOException
{    if (inputFiles == null || inputFiles.size() <= 1) {        throw new IllegalArgumentException("Not enough files to merge");    }    for (Path inputFile : inputFiles) {        FileSystem fs = inputFile.getFileSystem(conf);        FileStatus status = fs.getFileStatus(inputFile);        if (status.isDir()) {            throw new IllegalArgumentException("Illegal parquet file: " + inputFile.toUri());        }    }}
0
private List<Path> getInputFilesFromDirectory(FileStatus partitionDir) throws IOException
{    FileSystem fs = partitionDir.getPath().getFileSystem(conf);    FileStatus[] inputFiles = fs.listStatus(partitionDir.getPath(), HiddenFileFilter.INSTANCE);    List<Path> input = new ArrayList<Path>();    for (FileStatus f : inputFiles) {        input.add(f.getPath());    }    return input;}
0
private List<Path> parseInputFiles(List<String> input)
{    List<Path> inputFiles = new ArrayList<Path>();    for (String name : input) {        inputFiles.add(new Path(name));    }    return inputFiles;}
0
 static void showDetails(PrettyPrintWriter out, ParquetMetadata meta, boolean showOriginalTypes)
{    showDetails(out, meta.getFileMetaData(), showOriginalTypes);    long i = 1;    for (BlockMetaData bmeta : meta.getBlocks()) {        out.println();        showDetails(out, bmeta, i++);    }}
0
 static void showDetails(PrettyPrintWriter out, FileMetaData meta, boolean showOriginalTypes)
{    out.format("creator: %s%n", meta.getCreatedBy());    Map<String, String> extra = meta.getKeyValueMetaData();    if (extra != null) {        for (Map.Entry<String, String> entry : meta.getKeyValueMetaData().entrySet()) {            out.print("extra: ");            out.incrementTabLevel();            out.format("%s = %s%n", entry.getKey(), entry.getValue());            out.decrementTabLevel();        }    }    out.println();    out.format("file schema: %s%n", meta.getSchema().getName());    out.rule('-');    showDetails(out, meta.getSchema(), showOriginalTypes);}
0
private static void showDetails(PrettyPrintWriter out, BlockMetaData meta, Long num)
{    long rows = meta.getRowCount();    long tbs = meta.getTotalByteSize();    long offset = meta.getStartingPos();    out.format("row group%s: RC:%d TS:%d OFFSET:%d%n", (num == null ? "" : " " + num), rows, tbs, offset);    out.rule('-');    showDetails(out, meta.getColumns());}
0
 static void showDetails(PrettyPrintWriter out, List<ColumnChunkMetaData> ccmeta)
{    Map<String, Object> chunks = new LinkedHashMap<String, Object>();    for (ColumnChunkMetaData cmeta : ccmeta) {        String[] path = cmeta.getPath().toArray();        Map<String, Object> current = chunks;        for (int i = 0; i < path.length - 1; ++i) {            String next = path[i];            if (!current.containsKey(next)) {                current.put(next, new LinkedHashMap<String, Object>());            }            current = (Map<String, Object>) current.get(next);        }        current.put(path[path.length - 1], cmeta);    }    showColumnChunkDetails(out, chunks, 0);}
0
private static void showColumnChunkDetails(PrettyPrintWriter out, Map<String, Object> current, int depth)
{    for (Map.Entry<String, Object> entry : current.entrySet()) {        String name = Strings.repeat(".", depth) + entry.getKey();        Object value = entry.getValue();        if (value instanceof Map) {            out.println(name + ": ");            showColumnChunkDetails(out, (Map<String, Object>) value, depth + 1);        } else {            out.print(name + ": ");            showDetails(out, (ColumnChunkMetaData) value, false);        }    }}
0
private static void showDetails(PrettyPrintWriter out, ColumnChunkMetaData meta, boolean name)
{    long doff = meta.getDictionaryPageOffset();    long foff = meta.getFirstDataPageOffset();    long tsize = meta.getTotalSize();    long usize = meta.getTotalUncompressedSize();    long count = meta.getValueCount();    double ratio = usize / (double) tsize;    String encodings = Joiner.on(',').skipNulls().join(meta.getEncodings());    if (name) {        String path = Joiner.on('.').skipNulls().join(meta.getPath());        out.format("%s: ", path);    }    out.format(" %s", meta.getType());    out.format(" %s", meta.getCodec());    out.format(" DO:%d", doff);    out.format(" FPO:%d", foff);    out.format(" SZ:%d/%d/%.2f", tsize, usize, ratio);    out.format(" VC:%d", count);    if (!encodings.isEmpty())        out.format(" ENC:%s", encodings);    Statistics<?> stats = meta.getStatistics();    if (stats != null) {        out.format(" ST:[%s]", stats);    } else {        out.format(" ST:[none]");    }    out.println();}
0
 static void showDetails(PrettyPrintWriter out, MessageType type, boolean showOriginalTypes)
{    List<String> cpath = new ArrayList<String>();    for (Type ftype : type.getFields()) {        showDetails(out, ftype, 0, type, cpath, showOriginalTypes);    }}
0
private static void showDetails(PrettyPrintWriter out, GroupType type, int depth, MessageType container, List<String> cpath, boolean showOriginalTypes)
{    String name = Strings.repeat(".", depth) + type.getName();    Repetition rep = type.getRepetition();    int fcount = type.getFieldCount();    out.format("%s: %s F:%d%n", name, rep, fcount);    cpath.add(type.getName());    for (Type ftype : type.getFields()) {        showDetails(out, ftype, depth + 1, container, cpath, showOriginalTypes);    }    cpath.remove(cpath.size() - 1);}
0
private static void showDetails(PrettyPrintWriter out, PrimitiveType type, int depth, MessageType container, List<String> cpath, boolean showOriginalTypes)
{    String name = Strings.repeat(".", depth) + type.getName();    Repetition rep = type.getRepetition();    PrimitiveTypeName ptype = type.getPrimitiveTypeName();    out.format("%s: %s %s", name, rep, ptype);    if (showOriginalTypes) {        OriginalType otype;        try {            otype = type.getOriginalType();        } catch (Exception e) {            otype = null;        }        if (otype != null)            out.format(" O:%s", otype);    } else {        LogicalTypeAnnotation ltype = type.getLogicalTypeAnnotation();        if (ltype != null)            out.format(" L:%s", ltype);    }    if (container != null) {        cpath.add(type.getName());        String[] paths = cpath.toArray(new String[cpath.size()]);        cpath.remove(cpath.size() - 1);        ColumnDescriptor desc = container.getColumnDescription(paths);        int defl = desc.getMaxDefinitionLevel();        int repl = desc.getMaxRepetitionLevel();        out.format(" R:%d D:%d", repl, defl);    }    out.println();}
0
private static void showDetails(PrettyPrintWriter out, Type type, int depth, MessageType container, List<String> cpath, boolean showOriginalTypes)
{    if (type instanceof GroupType) {        showDetails(out, type.asGroupType(), depth, container, cpath, showOriginalTypes);        return;    } else if (type instanceof PrimitiveType) {        showDetails(out, type.asPrimitiveType(), depth, container, cpath, showOriginalTypes);        return;    }}
0
public static Map<String, Command> allCommands()
{    Map<String, Command> results = new LinkedHashMap<String, Command>();    for (Map.Entry<String, Class<? extends Command>> entry : registry.entrySet()) {        try {            results.put(entry.getKey(), entry.getValue().newInstance());        } catch (Exception ex) {        }    }    return results;}
0
public static Command getCommandByName(String name)
{    Class<? extends Command> clazz = registry.get(name);    if (clazz == null) {        return null;    }    try {        return clazz.newInstance();    } catch (Exception ex) {        return null;    }}
0
public Options getOptions()
{    return OPTIONS;}
0
public String[] getUsageDescription()
{    return USAGE;}
0
public String getCommandDescription()
{    return "Prints the count of rows in Parquet file(s)";}
0
public void execute(CommandLine options) throws Exception
{    super.execute(options);    String[] args = options.getArgs();    String input = args[0];    out = new PrintWriter(Main.out, true);    inputPath = new Path(input);    conf = new Configuration();    inputFileStatuses = inputPath.getFileSystem(conf).globStatus(inputPath);    long rowCount = 0;    for (FileStatus fs : inputFileStatuses) {        long fileRowCount = 0;        for (Footer f : ParquetFileReader.readFooters(conf, fs, false)) {            for (BlockMetaData b : f.getParquetMetadata().getBlocks()) {                rowCount += b.getRowCount();                fileRowCount += b.getRowCount();            }        }        if (options.hasOption('d')) {            out.format("%s row count: %d\n", fs.getPath().getName(), fileRowCount);        }    }    out.format("Total RowCount: %d", rowCount);    out.println();}
0
public String[] getUsageDescription()
{    return USAGE;}
0
public String getCommandDescription()
{    return "Prints the metadata of Parquet file(s)";}
0
public Options getOptions()
{    return OPTIONS;}
0
public void execute(CommandLine options) throws Exception
{    super.execute(options);    String[] args = options.getArgs();    String input = args[0];    boolean showOriginalTypes = options.hasOption('o');    Configuration conf = new Configuration();    Path inputPath = new Path(input);    FileStatus inputFileStatus = inputPath.getFileSystem(conf).getFileStatus(inputPath);    List<Footer> footers = ParquetFileReader.readFooters(conf, inputFileStatus, false);    PrettyPrintWriter out = PrettyPrintWriter.stdoutPrettyPrinter().withAutoColumn().withWhitespaceHandler(WhiteSpaceHandler.COLLAPSE_WHITESPACE).withColumnPadding(1).build();    for (Footer f : footers) {        out.format("file: %s%n", f.getFile());        MetadataUtils.showDetails(out, f.getParquetMetadata(), showOriginalTypes);        out.flushColumns();    }}
0
public String[] getUsageDescription()
{    return USAGE;}
0
public String getCommandDescription()
{    return "Prints the schema of Parquet file(s)";}
0
public Options getOptions()
{    return OPTIONS;}
0
public void execute(CommandLine options) throws Exception
{    super.execute(options);    String[] args = options.getArgs();    String input = args[0];    Configuration conf = new Configuration();    ParquetMetadata metaData;    Path path = new Path(input);    FileSystem fs = path.getFileSystem(conf);    Path file;    if (fs.isDirectory(path)) {        FileStatus[] statuses = fs.listStatus(path, HiddenFileFilter.INSTANCE);        if (statuses.length == 0) {            throw new RuntimeException("Directory " + path.toString() + " is empty");        }        file = statuses[0].getPath();    } else {        file = path;    }    metaData = ParquetFileReader.readFooter(conf, file, NO_FILTER);    MessageType schema = metaData.getFileMetaData().getSchema();    Main.out.println(schema);    if (options.hasOption('d')) {        boolean showOriginalTypes = options.hasOption('o');        PrettyPrintWriter out = PrettyPrintWriter.stdoutPrettyPrinter().build();        MetadataUtils.showDetails(out, metaData, showOriginalTypes);    }}
0
public Options getOptions()
{    return OPTIONS;}
0
public String[] getUsageDescription()
{    return USAGE;}
0
public String getCommandDescription()
{    return "Prints the size of Parquet file(s)";}
0
public void execute(CommandLine options) throws Exception
{    super.execute(options);    String[] args = options.getArgs();    String input = args[0];    out = new PrintWriter(Main.out, true);    inputPath = new Path(input);    conf = new Configuration();    inputFileStatuses = inputPath.getFileSystem(conf).globStatus(inputPath);    long size = 0;    for (FileStatus fs : inputFileStatuses) {        long fileSize = 0;        for (Footer f : ParquetFileReader.readFooters(conf, fs, false)) {            for (BlockMetaData b : f.getParquetMetadata().getBlocks()) {                size += (options.hasOption('u') ? b.getTotalByteSize() : b.getCompressedSize());                fileSize += (options.hasOption('u') ? b.getTotalByteSize() : b.getCompressedSize());            }        }        if (options.hasOption('d')) {            if (options.hasOption('p')) {                out.format("%s: %s\n", fs.getPath().getName(), getPrettySize(fileSize));            } else {                out.format("%s: %d bytes\n", fs.getPath().getName(), fileSize);            }        }    }    if (options.hasOption('p')) {        out.format("Total Size: %s", getPrettySize(size));    } else {        out.format("Total Size: %d bytes", size);    }    out.println();}
0
public String getPrettySize(long bytes)
{    if (bytes / ONE_KB < 1) {        return String.format("%d", bytes) + " bytes";    }    if (bytes / ONE_MB < 1) {        return String.format("%.3f", bytes / ONE_KB) + " KB";    }    if (bytes / ONE_GB < 1) {        return String.format("%.3f", bytes / ONE_MB) + " MB";    }    if (bytes / ONE_TB < 1) {        return String.format("%.3f", bytes / ONE_GB) + " GB";    }    if (bytes / ONE_PB < 1) {        return String.format("%.3f", bytes / ONE_TB) + " TB";    }    return String.format("%.3f", bytes / ONE_PB) + " PB";}
0
protected Object formatResults(List<Object> listOfValues)
{    if (super.typeInfo.getRepetition() == Type.Repetition.REPEATED) {        return listOfValues;    } else {        return listOfValues.get(SINGLE_VALUE);    }}
0
private Map<String, JsonRecordFormatter> buildWriters(GroupType groupSchema)
{    Map<String, JsonRecordFormatter> writers = new LinkedHashMap<String, JsonRecordFormatter>();    for (Type type : groupSchema.getFields()) {        if (type.isPrimitive()) {            writers.put(type.getName(), new JsonPrimitiveWriter(type));        } else {            writers.put(type.getName(), new JsonGroupFormatter((GroupType) type));        }    }    return writers;}
0
private Object add(SimpleRecord record)
{    return formatEntries(collateEntries(record));}
0
private Map<String, List<Object>> collateEntries(SimpleRecord record)
{    Map<String, List<Object>> collatedEntries = new LinkedHashMap<String, List<Object>>();    for (SimpleRecord.NameValue value : record.getValues()) {        if (collatedEntries.containsKey(value.getName())) {            collatedEntries.get(value.getName()).add(value.getValue());        } else {            List<Object> newResultListForKey = new ArrayList<Object>();            newResultListForKey.add(value.getValue());            collatedEntries.put(value.getName(), newResultListForKey);        }    }    return collatedEntries;}
0
private Object formatEntries(Map<String, List<Object>> entries)
{    Map<String, Object> results = new LinkedHashMap<String, Object>();    for (Map.Entry<String, List<Object>> entry : entries.entrySet()) {        JsonRecordFormatter formatter = formatters.get(entry.getKey());        results.put(entry.getKey(), formatter.formatResults(entry.getValue()));    }    return results;}
0
protected Object formatResults(List<SimpleRecord> values)
{    if (super.typeInfo.getRepetition() == Type.Repetition.REPEATED) {        List<Object> results = new ArrayList<Object>();        for (SimpleRecord object : values) {            results.add(add(object));        }        return results;    } else {        return add(values.get(SINGLE_VALUE));    }}
0
public String formatRecord(SimpleRecord value) throws IOException
{    ObjectMapper mapper = new ObjectMapper();    return mapper.writeValueAsString(add(value));}
0
public static JsonGroupFormatter fromSchema(MessageType messageType)
{    return new JsonGroupFormatter(messageType);}
0
public static void mergeOptionsInto(Options opt, Options opts)
{    if (opts == null) {        return;    }    Collection<Option> all = opts.getOptions();    if (all != null && !all.isEmpty()) {        for (Option o : all) {            opt.addOption(o);        }    }}
0
public static Options mergeOptions(Options opt, Options... opts)
{    Options results = new Options();    mergeOptionsInto(results, opt);    for (Options o : opts) {        mergeOptionsInto(results, o);    }    return results;}
0
public static void showUsage(HelpFormatter format, PrintWriter err, String name, Command command)
{    Options options = mergeOptions(OPTIONS, command.getOptions());    String[] usage = command.getUsageDescription();    String ustr = name + " [option...]";    if (usage != null && usage.length >= 1) {        ustr = ustr + " " + usage[0];    }    format.printWrapped(err, WIDTH, name + ":\n" + command.getCommandDescription());    format.printUsage(err, WIDTH, ustr);    format.printWrapped(err, WIDTH, LEFT_PAD, "where option is one of:");    format.printOptions(err, WIDTH, options, LEFT_PAD, DESC_PAD);    if (usage != null && usage.length >= 2) {        for (int i = 1; i < usage.length; ++i) {            format.printWrapped(err, WIDTH, LEFT_PAD, usage[i]);        }    }}
0
public static void showUsage(String name, Command command)
{    HelpFormatter format = new HelpFormatter();    PrintWriter err = new PrintWriter(Main.err, true);    Options options = command.getOptions();    showUsage(format, err, "parquet-" + name, command);}
0
public static void showUsage()
{    HelpFormatter format = new HelpFormatter();    PrintWriter err = new PrintWriter(Main.err, true);    Map<String, Command> all = Registry.allCommands();    boolean first = true;    for (Map.Entry<String, Command> entry : all.entrySet()) {        String name = entry.getKey();        Command command = entry.getValue();        if (!first)            err.println();        first = false;        showUsage(format, err, "parquet-tools " + name, command);    }}
0
public static void die(String message, boolean usage)
{    die(message, usage, null, null);}
0
public static void die(Throwable th, boolean usage)
{    die(th, usage, null, null);}
0
public static void die(String message, boolean usage, String name, Command command)
{    if (message != null) {        Main.err.println(message);        Main.err.println();    }    if (usage) {        if (name == null && command == null) {            showUsage();        } else {            showUsage(name, command);        }    }    System.exit(1);}
0
public static void die(Throwable th, boolean usage, String name, Command command)
{    die(th.toString(), usage, name, command);}
0
public static void main(String[] args)
{    Main.out = System.out;    Main.err = System.err;    PrintStream VoidStream = new PrintStream(new OutputStream() {        @Override        public void write(int b) throws IOException {        }        @Override        public void write(byte[] b) throws IOException {        }        @Override        public void write(byte[] b, int off, int len) throws IOException {        }        @Override        public void flush() throws IOException {        }        @Override        public void close() throws IOException {        }    });    System.setOut(VoidStream);    System.setErr(VoidStream);    if (args.length == 0) {        die("No command specified", true, null, null);    }    String name = args[0];    if ("-h".equals(name) || "--help".equals(name)) {        showUsage();        System.exit(0);    }    Command command = Registry.getCommandByName(name);    if (command == null) {        die("Unknown command: " + name, true, null, null);    }    boolean debug = false;    try {        String[] cargs = Arrays.copyOfRange(args, 1, args.length);        Options opts = mergeOptions(OPTIONS, command.getOptions());        boolean extra = command.supportsExtraArgs();        CommandLineParser parser = new PosixParser();        CommandLine cmd = parser.parse(opts == null ? new Options() : opts, cargs, extra);        if (cmd.hasOption('h')) {            showUsage(name, command);            System.exit(0);        }        if (cmd.hasOption("no-color")) {            System.setProperty("DISABLE_COLORS", "true");        }        debug = cmd.hasOption("debug");        command.execute(cmd);    } catch (ParseException ex) {        if (debug)            ex.printStackTrace(Main.err);        die("Invalid arguments: " + ex.getMessage(), true, name, command);    } catch (Throwable th) {        if (debug)            th.printStackTrace(Main.err);        die(th, false, name, command);    }}
0
public void write(int b) throws IOException
{}
0
public void write(byte[] b) throws IOException
{}
0
public void write(byte[] b, int off, int len) throws IOException
{}
0
public void flush() throws IOException
{}
0
public void close() throws IOException
{}
0
protected Object toJsonObject()
{    Object[] result = new Object[values.size()];    for (int i = 0; i < values.size(); i++) {        result[i] = toJsonValue(values.get(i).getValue());    }    return result;}
0
public void start()
{    record = new SimpleListRecord();}
0
protected Object toJsonObject()
{    Map<String, Object> result = Maps.newLinkedHashMap();    for (NameValue value : values) {        String key = null;        Object val = null;        for (NameValue kv : ((SimpleRecord) value.getValue()).values) {            String kvName = kv.getName();            Object kvValue = kv.getValue();            if (kvName.equals("key")) {                key = keyToString(kvValue);            } else if (kvName.equals("value")) {                val = toJsonValue(kvValue);            }        }        result.put(key, val);    }    return result;}
0
 String keyToString(Object kvValue)
{    if (kvValue == null) {        return "null";    }    Class<?> type = kvValue.getClass();    if (type.isArray()) {        if (type.getComponentType() == boolean.class) {            return Arrays.toString((boolean[]) kvValue);        } else if (type.getComponentType() == byte.class) {            return new BinaryNode((byte[]) kvValue).asText();        } else if (type.getComponentType() == char.class) {            return Arrays.toString((char[]) kvValue);        } else if (type.getComponentType() == double.class) {            return Arrays.toString((double[]) kvValue);        } else if (type.getComponentType() == float.class) {            return Arrays.toString((float[]) kvValue);        } else if (type.getComponentType() == int.class) {            return Arrays.toString((int[]) kvValue);        } else if (type.getComponentType() == long.class) {            return Arrays.toString((long[]) kvValue);        } else if (type.getComponentType() == short.class) {            return Arrays.toString((short[]) kvValue);        } else {            return Arrays.toString((Object[]) kvValue);        }    } else {        return String.valueOf(kvValue);    }}
0
public void start()
{    record = new SimpleMapRecord();}
0
public RecordMaterializer<SimpleRecord> prepareForRead(Configuration conf, Map<String, String> metaData, MessageType schema, ReadContext context)
{    return new SimpleRecordMaterializer(schema);}
0
public ReadContext init(InitContext context)
{    return new ReadContext(context.getFileSchema());}
0
public void add(String name, Object value)
{    values.add(new NameValue(name, value));}
0
public List<NameValue> getValues()
{    return Collections.unmodifiableList(values);}
0
public String toString()
{    return values.toString();}
0
public void prettyPrint()
{    prettyPrint(new PrintWriter(System.out, true));}
0
public void prettyPrint(PrintWriter out)
{    prettyPrint(out, 0);}
0
public void prettyPrint(PrintWriter out, int depth)
{    for (NameValue value : values) {        out.print(Strings.repeat(".", depth));        out.print(value.getName());        Object val = value.getValue();        if (val == null) {            out.print(" = ");            out.print("<null>");        } else if (byte[].class == val.getClass()) {            out.print(" = ");            out.print(new BinaryNode((byte[]) val).asText());        } else if (short[].class == val.getClass()) {            out.print(" = ");            out.print(Arrays.toString((short[]) val));        } else if (int[].class == val.getClass()) {            out.print(" = ");            out.print(Arrays.toString((int[]) val));        } else if (long[].class == val.getClass()) {            out.print(" = ");            out.print(Arrays.toString((long[]) val));        } else if (float[].class == val.getClass()) {            out.print(" = ");            out.print(Arrays.toString((float[]) val));        } else if (double[].class == val.getClass()) {            out.print(" = ");            out.print(Arrays.toString((double[]) val));        } else if (boolean[].class == val.getClass()) {            out.print(" = ");            out.print(Arrays.toString((boolean[]) val));        } else if (val.getClass().isArray()) {            out.print(" = ");            out.print(Arrays.deepToString((Object[]) val));        } else if (SimpleRecord.class.isAssignableFrom(val.getClass())) {            out.println(":");            ((SimpleRecord) val).prettyPrint(out, depth + 1);            continue;        } else {            out.print(" = ");            out.print(String.valueOf(val));        }        out.println();    }}
0
public void prettyPrintJson(PrintWriter out) throws IOException
{    ObjectMapper mapper = new ObjectMapper();    out.write(mapper.writeValueAsString(this.toJsonObject()));}
0
protected Object toJsonObject()
{    Map<String, Object> result = Maps.newLinkedHashMap();    for (NameValue value : values) {        result.put(value.getName(), toJsonValue(value.getValue()));    }    return result;}
0
protected static Object toJsonValue(Object val)
{    if (SimpleRecord.class.isAssignableFrom(val.getClass())) {        return ((SimpleRecord) val).toJsonObject();    } else if (byte[].class == val.getClass()) {        return new BinaryNode((byte[]) val);    } else {        return val;    }}
0
public String toString()
{    return name + ": " + value;}
0
public String getName()
{    return name;}
0
public Object getValue()
{    return value;}
0
private Converter createConverter(Type field)
{    LogicalTypeAnnotation ltype = field.getLogicalTypeAnnotation();    if (field.isPrimitive()) {        if (ltype != null) {            return ltype.accept(new LogicalTypeAnnotation.LogicalTypeAnnotationVisitor<Converter>() {                @Override                public Optional<Converter> visit(LogicalTypeAnnotation.StringLogicalTypeAnnotation stringLogicalType) {                    return of(new StringConverter(field.getName()));                }                @Override                public Optional<Converter> visit(LogicalTypeAnnotation.DecimalLogicalTypeAnnotation decimalLogicalType) {                    int scale = decimalLogicalType.getScale();                    return of(new DecimalConverter(field.getName(), scale));                }            }).orElse(new SimplePrimitiveConverter(field.getName()));        }        return new SimplePrimitiveConverter(field.getName());    }    GroupType groupType = field.asGroupType();    if (ltype != null) {        return ltype.accept(new LogicalTypeAnnotation.LogicalTypeAnnotationVisitor<Converter>() {            @Override            public Optional<Converter> visit(LogicalTypeAnnotation.MapLogicalTypeAnnotation mapLogicalType) {                return of(new SimpleMapRecordConverter(groupType, field.getName(), SimpleRecordConverter.this));            }            @Override            public Optional<Converter> visit(LogicalTypeAnnotation.ListLogicalTypeAnnotation listLogicalType) {                return of(new SimpleListRecordConverter(groupType, field.getName(), SimpleRecordConverter.this));            }        }).orElse(new SimpleRecordConverter(groupType, field.getName(), this));    }    return new SimpleRecordConverter(groupType, field.getName(), this);}
0
public Optional<Converter> visit(LogicalTypeAnnotation.StringLogicalTypeAnnotation stringLogicalType)
{    return of(new StringConverter(field.getName()));}
0
public Optional<Converter> visit(LogicalTypeAnnotation.DecimalLogicalTypeAnnotation decimalLogicalType)
{    int scale = decimalLogicalType.getScale();    return of(new DecimalConverter(field.getName(), scale));}
0
public Optional<Converter> visit(LogicalTypeAnnotation.MapLogicalTypeAnnotation mapLogicalType)
{    return of(new SimpleMapRecordConverter(groupType, field.getName(), SimpleRecordConverter.this));}
0
public Optional<Converter> visit(LogicalTypeAnnotation.ListLogicalTypeAnnotation listLogicalType)
{    return of(new SimpleListRecordConverter(groupType, field.getName(), SimpleRecordConverter.this));}
0
public Converter getConverter(int fieldIndex)
{    return converters[fieldIndex];}
0
 SimpleRecord getCurrentRecord()
{    return record;}
0
public void start()
{    record = new SimpleRecord();}
0
public void end()
{    if (parent != null) {        parent.getCurrentRecord().add(name, record);    }}
0
public void addBinary(Binary value)
{    record.add(name, value.getBytes());}
0
public void addBoolean(boolean value)
{    record.add(name, value);}
0
public void addDouble(double value)
{    record.add(name, value);}
0
public void addFloat(float value)
{    record.add(name, value);}
0
public void addInt(int value)
{    record.add(name, value);}
0
public void addLong(long value)
{    record.add(name, value);}
0
public void addBinary(Binary value)
{    record.add(name, value.toStringUsingUTF8());}
0
public void addBinary(Binary value)
{    record.add(name, new BigDecimal(new BigInteger(value.getBytes()), scale));}
0
public void addInt(int value)
{    record.add(name, BigDecimal.valueOf(value).movePointLeft(scale));}
0
public void addLong(long value)
{    record.add(name, BigDecimal.valueOf(value).movePointLeft(scale));}
0
public SimpleRecord getCurrentRecord()
{    return root.getCurrentRecord();}
0
public GroupConverter getRootConverter()
{    return root;}
0
public static void showDetails(PrettyPrintWriter out, ParquetMetadata meta)
{    showDetails(out, meta.getFileMetaData());    long i = 1;    for (BlockMetaData bmeta : meta.getBlocks()) {        out.println();        showDetails(out, bmeta, i++);    }}
0
public static void showDetails(PrettyPrintWriter out, FileMetaData meta)
{    out.format("creator: %s%n", meta.getCreatedBy());    Map<String, String> extra = meta.getKeyValueMetaData();    if (extra != null) {        for (Map.Entry<String, String> entry : meta.getKeyValueMetaData().entrySet()) {            out.print("extra: ");            out.incrementTabLevel();            out.format("%s = %s%n", entry.getKey(), entry.getValue());            out.decrementTabLevel();        }    }    out.println();    out.format("file schema: %s%n", meta.getSchema().getName());    out.rule('-');    showDetails(out, meta.getSchema());}
0
public static void showDetails(PrettyPrintWriter out, BlockMetaData meta)
{    showDetails(out, meta, null);}
0
private static void showDetails(PrettyPrintWriter out, BlockMetaData meta, Long num)
{    long rows = meta.getRowCount();    long tbs = meta.getTotalByteSize();    long offset = meta.getStartingPos();    out.format("row group%s: RC:%d TS:%d OFFSET:%d%n", (num == null ? "" : " " + num), rows, tbs, offset);    out.rule('-');    showDetails(out, meta.getColumns());}
0
public static void showDetails(PrettyPrintWriter out, List<ColumnChunkMetaData> ccmeta)
{    Map<String, Object> chunks = new LinkedHashMap<String, Object>();    for (ColumnChunkMetaData cmeta : ccmeta) {        String[] path = cmeta.getPath().toArray();        Map<String, Object> current = chunks;        for (int i = 0; i < path.length - 1; ++i) {            String next = path[i];            if (!current.containsKey(next)) {                current.put(next, new LinkedHashMap<String, Object>());            }            current = (Map<String, Object>) current.get(next);        }        current.put(path[path.length - 1], cmeta);    }    showColumnChunkDetails(out, chunks, 0);}
0
private static void showColumnChunkDetails(PrettyPrintWriter out, Map<String, Object> current, int depth)
{    for (Map.Entry<String, Object> entry : current.entrySet()) {        String name = Strings.repeat(".", depth) + entry.getKey();        Object value = entry.getValue();        if (value instanceof Map) {            out.println(name + ": ");            showColumnChunkDetails(out, (Map<String, Object>) value, depth + 1);        } else {            out.print(name + ": ");            showDetails(out, (ColumnChunkMetaData) value, false);        }    }}
0
public static void showDetails(PrettyPrintWriter out, ColumnChunkMetaData meta)
{    showDetails(out, meta, true);}
0
private static void showDetails(PrettyPrintWriter out, ColumnChunkMetaData meta, boolean name)
{    long doff = meta.getDictionaryPageOffset();    long foff = meta.getFirstDataPageOffset();    long tsize = meta.getTotalSize();    long usize = meta.getTotalUncompressedSize();    long count = meta.getValueCount();    double ratio = usize / (double) tsize;    String encodings = Joiner.on(',').skipNulls().join(meta.getEncodings());    if (name) {        String path = Joiner.on('.').skipNulls().join(meta.getPath());        out.format("%s: ", path);    }    out.format(" %s", meta.getType());    out.format(" %s", meta.getCodec());    out.format(" DO:%d", doff);    out.format(" FPO:%d", foff);    out.format(" SZ:%d/%d/%.2f", tsize, usize, ratio);    out.format(" VC:%d", count);    if (!encodings.isEmpty())        out.format(" ENC:%s", encodings);    Statistics<?> stats = meta.getStatistics();    if (stats != null) {        out.format(" ST:[%s]", stats);    } else {        out.format(" ST:[none]");    }    out.println();}
0
public static void showDetails(PrettyPrintWriter out, ColumnDescriptor desc)
{    String path = Joiner.on(".").skipNulls().join(desc.getPath());    PrimitiveTypeName type = desc.getType();    int defl = desc.getMaxDefinitionLevel();    int repl = desc.getMaxRepetitionLevel();    out.format("column desc: %s T:%s R:%d D:%d%n", path, type, repl, defl);}
0
public static void showDetails(PrettyPrintWriter out, MessageType type)
{    List<String> cpath = new ArrayList<String>();    for (Type ftype : type.getFields()) {        showDetails(out, ftype, 0, type, cpath);    }}
0
public static void showDetails(PrettyPrintWriter out, GroupType type)
{    showDetails(out, type, 0, null, null);}
0
public static void showDetails(PrettyPrintWriter out, PrimitiveType type)
{    showDetails(out, type, 0, null, null);}
0
public static void showDetails(PrettyPrintWriter out, Type type)
{    showDetails(out, type, 0, null, null);}
0
private static void showDetails(PrettyPrintWriter out, GroupType type, int depth, MessageType container, List<String> cpath)
{    String name = Strings.repeat(".", depth) + type.getName();    Repetition rep = type.getRepetition();    int fcount = type.getFieldCount();    out.format("%s: %s F:%d%n", name, rep, fcount);    cpath.add(type.getName());    for (Type ftype : type.getFields()) {        showDetails(out, ftype, depth + 1, container, cpath);    }    cpath.remove(cpath.size() - 1);}
0
private static void showDetails(PrettyPrintWriter out, PrimitiveType type, int depth, MessageType container, List<String> cpath)
{    String name = Strings.repeat(".", depth) + type.getName();    OriginalType otype = type.getOriginalType();    Repetition rep = type.getRepetition();    PrimitiveTypeName ptype = type.getPrimitiveTypeName();    out.format("%s: %s %s", name, rep, ptype);    if (otype != null)        out.format(" O:%s", otype);    if (container != null) {        cpath.add(type.getName());        String[] paths = cpath.toArray(new String[cpath.size()]);        cpath.remove(cpath.size() - 1);        ColumnDescriptor desc = container.getColumnDescription(paths);        int defl = desc.getMaxDefinitionLevel();        int repl = desc.getMaxRepetitionLevel();        out.format(" R:%d D:%d", repl, defl);    }    out.println();}
0
private static void showDetails(PrettyPrintWriter out, Type type, int depth, MessageType container, List<String> cpath)
{    if (type instanceof GroupType) {        showDetails(out, type.asGroupType(), depth, container, cpath);        return;    } else if (type instanceof PrimitiveType) {        showDetails(out, type.asPrimitiveType(), depth, container, cpath);        return;    }}
0
public void setTabLevel(int level)
{    this.tabLevel = level;    this.tabs = Strings.repeat(" ", tabWidth * level);    if (flushOnTab)        flushColumns();}
0
public void incrementTabLevel()
{    setTabLevel(tabLevel + 1);}
0
public void decrementTabLevel()
{    if (tabLevel == 0) {        return;    }    setTabLevel(tabLevel - 1);}
0
private int determineNumColumns()
{    int max = 0;    for (Line line : buffer) {        int num = line.countCharacter(columnSeparator);        if (num > max) {            max = num;        }    }    return max > maxColumns ? maxColumns : max;}
0
private int[] determineColumnWidths()
{    int columns = determineNumColumns();    if (columns == 0) {        return null;    }    int[] widths = new int[columns];    for (Line line : buffer) {        for (int last = 0, idx = 0; last < line.length() && idx < columns; ++idx) {            int pos = line.indexOf(columnSeparator, last);            if (pos < 0)                break;            int wid = pos - last + 1 + columnPadding;            if (wid > widths[idx]) {                widths[idx] = wid;            }            last = line.firstNonWhiteSpace(idx + 1);        }    }    return widths;}
0
private Line toColumns(int[] widths, Line line) throws IOException
{    int last = 0;    for (int i = 0; i < widths.length; ++i) {        int width = widths[i];        int idx = line.indexOf(columnSeparator, last);        if (idx < 0)            break;        if ((idx + 1) <= width) {            line.spaceOut(width - (idx + 1), idx + 1);        }        last = line.firstNonWhiteSpace(idx + 1);    }    return line;}
0
public void flushColumns()
{    flushColumns(false);}
0
private void flushColumns(boolean preserveLast)
{    int size = buffer.size();    int[] widths = null;    if (autoColumn) {        widths = determineColumnWidths();    }    StringBuilder builder = new StringBuilder();    try {        for (int i = 0; i < size - 1; ++i) {            Line line = buffer.get(i);            if (widths != null) {                line = toColumns(widths, line);            }            fixupLine(line);            builder.setLength(0);            line.toString(builder);            super.out.append(builder.toString());            super.out.append(LINE_SEP);        }        if (!preserveLast) {            Line line = buffer.get(size - 1);            if (widths != null) {                line = toColumns(widths, line);            }            fixupLine(line);            builder.setLength(0);            line.toString(builder);            super.out.append(builder.toString());        }        super.out.flush();    } catch (IOException ex) {    }    Line addback = null;    if (preserveLast) {        addback = buffer.get(size - 1);    }    buffer.clear();    if (addback != null)        buffer.add(addback);    else        buffer.add(new Line());}
0
private void flushIfNeeded()
{    flushIfNeeded(false);}
0
private void flushIfNeeded(boolean preserveLast)
{    if (!autoColumn || buffer.size() > maxBufferedLines) {        flushColumns(preserveLast);    }}
0
private void appendToCurrent(String s)
{    int size = buffer.size();    Line value = buffer.get(size - 1);    if (value.isEmpty()) {        value.append(tabs());    }    value.append(span(s));}
0
private void fixupLine(Line line)
{    if (autoCrop) {        line.trimTo(consoleWidth, appendToLongLine);    }}
0
private void print(String s, boolean mayHaveNewlines)
{    if (s == null) {        appendToCurrent("null");        return;    }    if (s.isEmpty()) {        return;    }    if (LINE_SEP.equals(s)) {        buffer.add(new Line());        flushIfNeeded();        return;    }    if (whiteSpaceHandler != null) {        boolean endswith = s.endsWith(LINE_SEP);        switch(whiteSpaceHandler) {            case ELIMINATE_NEWLINES:                s = s.replaceAll("\\r\\n|\\r|\\n", " ");                break;            case COLLAPSE_WHITESPACE:                s = s.replaceAll("\\s+", " ");                break;        }        mayHaveNewlines = endswith;        if (endswith)            s = s + LINE_SEP;    }    if (!mayHaveNewlines) {        appendToCurrent(s);        return;    }    String[] lines = s.split("\\r?\\n", -1);    appendToCurrent(lines[0]);    for (int i = 1; i < lines.length; ++i) {        String value = lines[i];        if (value.isEmpty()) {            buffer.add(new Line());        } else {            Line line = new Line();            line.append(tabs());            line.append(span(value, true));            buffer.add(line);        }    }    resetColor();    flushIfNeeded(true);}
0
public void print(String s)
{    print(s, true);}
0
public void println()
{    print(LINE_SEP, true);    flushIfNeeded();}
0
public void println(String x)
{    print(x);    println();}
0
public void print(boolean b)
{    print(String.valueOf(b), false);}
0
public void print(char c)
{    print(String.valueOf(c), false);}
0
public void print(int i)
{    print(String.valueOf(i), false);}
0
public void print(long l)
{    print(String.valueOf(l), false);}
0
public void print(float f)
{    print(String.valueOf(f), false);}
0
public void print(double d)
{    print(String.valueOf(d), false);}
0
public void print(char[] s)
{    print(String.valueOf(s), true);}
0
public void print(Object obj)
{    print(String.valueOf(obj), true);}
0
public PrintWriter printf(String format, Object... args)
{    return printf(formatter.locale(), format, args);}
0
public PrintWriter printf(Locale l, String format, Object... args)
{    formatter.format(l, format, args);    String results = formatString.toString();    formatString.setLength(0);    print(results);    flushIfNeeded();    return this;}
0
public PrintWriter format(String format, Object... args)
{    return printf(format, args);}
0
public PrintWriter format(Locale l, String format, Object... args)
{    return printf(l, format, args);}
0
public PrintWriter append(char c)
{    print(c);    return this;}
0
public PrintWriter append(CharSequence csq)
{    if (csq == null) {        print("null");        return this;    }    return append(csq, 0, csq.length());}
0
public PrintWriter append(CharSequence csq, int start, int end)
{    if (csq == null) {        print("null");        return this;    }    print(csq.subSequence(start, end).toString());    return this;}
0
public void println(boolean x)
{    print(x);    println();}
0
public void println(char x)
{    print(x);    println();}
0
public void println(int x)
{    print(x);    println();}
0
public void println(long x)
{    print(x);    println();}
0
public void println(float x)
{    print(x);    println();}
0
public void println(double x)
{    print(x);    println();}
0
public void println(char[] x)
{    print(x);    println();}
0
public void println(Object x)
{    print(x);    println();}
0
public void rule(char c)
{    if (tabs.length() >= consoleWidth)        return;    int width = consoleWidth;    if (width == Integer.MAX_VALUE) {        width = 100;    }    println(Strings.repeat(String.valueOf(c), width - tabs.length()));}
0
public PrettyPrintWriter iff(boolean predicate)
{    if (!predicate && acceptColorModification) {        resetColor();    } else {        acceptColorModification = false;    }    return this;}
0
public PrettyPrintWriter otherwise()
{    acceptColorModification = false;    return this;}
0
public PrettyPrintWriter black()
{    if (!acceptColorModification)        return this;    colorForeground = FG_COLOR_BLACK;    return this;}
0
public PrettyPrintWriter red()
{    if (!acceptColorModification)        return this;    colorForeground = FG_COLOR_RED;    return this;}
0
public PrettyPrintWriter green()
{    if (!acceptColorModification)        return this;    colorForeground = FG_COLOR_GREEN;    return this;}
0
public PrettyPrintWriter yellow()
{    if (!acceptColorModification)        return this;    colorForeground = FG_COLOR_YELLOW;    return this;}
0
public PrettyPrintWriter blue()
{    if (!acceptColorModification)        return this;    colorForeground = FG_COLOR_BLUE;    return this;}
0
public PrettyPrintWriter magenta()
{    if (!acceptColorModification)        return this;    colorForeground = FG_COLOR_MAGENTA;    return this;}
0
public PrettyPrintWriter cyan()
{    if (!acceptColorModification)        return this;    colorForeground = FG_COLOR_CYAN;    return this;}
0
public PrettyPrintWriter white()
{    if (!acceptColorModification)        return this;    colorForeground = FG_COLOR_WHITE;    return this;}
0
public PrettyPrintWriter bgblack()
{    if (!acceptColorModification)        return this;    colorBackground = BG_COLOR_BLACK;    return this;}
0
public PrettyPrintWriter bgred()
{    if (!acceptColorModification)        return this;    colorBackground = BG_COLOR_RED;    return this;}
0
public PrettyPrintWriter bggreen()
{    if (!acceptColorModification)        return this;    colorBackground = BG_COLOR_GREEN;    return this;}
0
public PrettyPrintWriter bgyellow()
{    if (!acceptColorModification)        return this;    colorBackground = BG_COLOR_YELLOW;    return this;}
0
public PrettyPrintWriter bgblue()
{    if (!acceptColorModification)        return this;    colorBackground = BG_COLOR_BLUE;    return this;}
0
public PrettyPrintWriter bgmagenta()
{    if (!acceptColorModification)        return this;    colorBackground = BG_COLOR_MAGENTA;    return this;}
0
public PrettyPrintWriter bgcyan()
{    if (!acceptColorModification)        return this;    colorBackground = BG_COLOR_CYAN;    return this;}
0
public PrettyPrintWriter bgwhite()
{    if (!acceptColorModification)        return this;    colorBackground = BG_COLOR_WHITE;    return this;}
0
public PrettyPrintWriter bold()
{    if (!acceptColorModification)        return this;    colorMode = MODE_BOLD;    return this;}
0
public PrettyPrintWriter blink()
{    if (!acceptColorModification)        return this;    colorMode = MODE_BLINK;    return this;}
0
public PrettyPrintWriter concealed()
{    if (!acceptColorModification)        return this;    colorMode = MODE_CONCEALED;    return this;}
0
public PrettyPrintWriter off()
{    if (!acceptColorModification)        return this;    colorMode = MODE_OFF;    return this;}
0
public PrettyPrintWriter underscore()
{    if (!acceptColorModification)        return this;    colorMode = MODE_UNDER;    return this;}
0
public PrettyPrintWriter reverse()
{    if (!acceptColorModification)        return this;    colorMode = MODE_REVERSE;    return this;}
0
public static Builder stdoutPrettyPrinter()
{    return new Builder(Main.out).withAutoFlush();}
0
public static Builder stderrPrettyPrinter()
{    return new Builder(Main.err).withAutoFlush();}
0
public static Builder newPrettyPrinter(OutputStream out)
{    return new Builder(out);}
0
public Builder withAutoFlush()
{    this.autoFlush = true;    return this;}
0
public Builder withAutoCrop()
{    return withAutoCrop(DEFAULT_WIDTH);}
0
public Builder withAutoCrop(int consoleWidth)
{    return withAutoCrop(consoleWidth, DEFAULT_APPEND);}
0
public Builder withAutoCrop(int consoleWidth, String appendToLong)
{    return withAutoCrop(consoleWidth, mkspan(appendToLong));}
0
public Builder withAutoCrop(int consoleWidth, Span appendToLong)
{    this.consoleWidth = consoleWidth;    this.appendToLongLine = appendToLong;    this.autoCrop = true;    return this;}
0
public Builder withTabSize(int tabWidth)
{    this.tabWidth = tabWidth;    return this;}
0
public Builder withAutoColumn()
{    return withAutoColumn(DEFAULT_COLUMN_SEP);}
0
public Builder withAutoColumn(char columnSeparator)
{    return withAutoColumn(columnSeparator, DEFAULT_MAX_COLUMNS);}
0
public Builder withAutoColumn(char columnSeparator, int maxColumns)
{    this.autoColumn = true;    this.columnSeparator = columnSeparator;    this.maxColumns = maxColumns;    return this;}
0
public Builder withColumnPadding(int columnPadding)
{    this.columnPadding = columnPadding;    return this;}
0
public Builder withWhitespaceHandler(WhiteSpaceHandler whiteSpaceHandler)
{    this.whiteSpaceHandler = whiteSpaceHandler;    return this;}
0
public Builder withMaxBufferedLines(long maxBufferedLines)
{    this.maxBufferedLines = maxBufferedLines;    return this;}
0
public Builder withFlushOnTab()
{    this.flushOnTab = true;    return this;}
0
public PrettyPrintWriter build()
{    return new PrettyPrintWriter(out, autoFlush, autoColumn, autoCrop, appendToLongLine, consoleWidth, tabWidth, columnSeparator, maxColumns, columnPadding, maxBufferedLines, flushOnTab, whiteSpaceHandler);}
0
private Span tabs()
{    return new Span(tabs);}
0
private Span span(String span)
{    return span(span, false);}
0
private void resetColor()
{    acceptColorModification = true;    colorMode = null;    colorForeground = null;    colorBackground = null;}
0
public static Span mkspan(String span)
{    return new Span(span);}
0
public static Span mkspan(String span, String color)
{    return mkspan(span, null, color, null);}
0
public static Span mkspan(String span, String colorMode, String colorForeground, String colorBackground)
{    if (DEFAULT_COLORS > 0 && (colorMode != null || colorForeground != null || colorBackground != null)) {        String color = "\u001B[" + Joiner.on(';').skipNulls().join(colorMode, colorForeground, colorBackground) + "m";        return new Span(span, color);    } else {        return mkspan(span);    }}
0
private Span span(String span, boolean keepColor)
{    Span result;    if (DEFAULT_COLORS > 0 && (colorMode != null || colorForeground != null || colorBackground != null)) {        result = mkspan(span, colorMode, colorForeground, colorBackground);    } else {        result = mkspan(span);    }    if (!keepColor) {        resetColor();    }    return result;}
0
public void append(Span span)
{    length += span.length();    if (spans.isEmpty()) {        spans.add(span);        return;    }    Span last = spans.get(spans.size() - 1);    if (last.canAppend(span)) {        last.append(span);    } else {        spans.add(span);    }}
0
public boolean isEmpty()
{    return length == 0;}
0
public int length()
{    return length;}
0
public int indexOf(char ch, int start)
{    int offset = 0;    for (Span span : spans) {        if (start > span.length()) {            start -= span.length();            continue;        }        int idx = span.indexOf(ch, start);        if (idx >= 0)            return offset + idx;        offset += span.length() - start;        start = 0;    }    return -1;}
0
public void spaceOut(int width, int start)
{    for (Span span : spans) {        if (start > span.length()) {            start -= span.length();            continue;        }        span.spaceOut(width, start);        return;    }}
0
public int firstNonWhiteSpace(int start)
{    return start;}
0
public int countCharacter(char ch)
{    int result = 0;    for (Span span : spans) {        result += span.countCharacter(ch);    }    return result;}
0
public void trimTo(int width, Span appendToLongLine)
{    int i = 0;    int remaining = width;    for (i = 0; i < spans.size(); ++i) {        Span next = spans.get(i);        if (next.length() > remaining) {            ++i;            next.trimTo(remaining, appendToLongLine);            break;        }        remaining -= next.length();    }    for (; i < spans.size(); ++i) {        spans.remove(i);    }}
0
public void toString(StringBuilder builder)
{    for (Span span : spans) {        span.toString(builder);    }}
0
public int length()
{    return span.length();}
0
public boolean isEmpty()
{    return span.isEmpty();}
0
public int indexOf(char ch, int start)
{    return span.indexOf(ch, start);}
0
public void spaceOut(int width, int start)
{    int removeTo = start;    while (removeTo < span.length() && Character.isWhitespace(span.charAt(removeTo))) {        removeTo++;    }    span = span.substring(0, start) + Strings.repeat(" ", width) + span.substring(removeTo);}
0
public int countCharacter(char ch)
{    int result = 0;    for (int i = 0; i < span.length(); ++i) {        if (span.charAt(i) == ch) {            result++;        }    }    return result;}
0
public void trimTo(int width, Span appendToLongLine)
{    if (appendToLongLine != null && !appendToLongLine.isEmpty()) {        int shortten = appendToLongLine.length();        if (shortten > width)            shortten = width;        span = span.substring(0, width - shortten) + appendToLongLine;    } else {        span = span.substring(0, width + 1);    }}
0
public String toString()
{    StringBuilder builder = new StringBuilder();    toString(builder);    return builder.toString();}
0
public void toString(StringBuilder builder)
{    if (color != null)        builder.append(color);    builder.append(span);    if (color != null)        builder.append(RESET);}
0
public void append(Span other)
{    span = span + other.span;}
0
public boolean canAppend(Span other)
{    if (color == null && other == null)        return true;    if (color == null && other != null)        return false;    return color.equals(other);}
0
private List<T> array(T... objects)
{    return Arrays.asList(objects);}
0
private Map.Entry<String, T> entry(final String key, final T value)
{    return new Map.Entry<String, T>() {        @Override        public String getKey() {            return key;        }        @Override        public T getValue() {            return value;        }        @Override        public T setValue(T value) {            throw new UnsupportedOperationException();        }    };}
0
public String getKey()
{    return key;}
0
public T getValue()
{    return value;}
0
public T setValue(T value)
{    throw new UnsupportedOperationException();}
0
private Map<String, ?> obj(Map.Entry<String, ?>... entries) throws IOException
{    Map<String, Object> entriesAsMap = new LinkedHashMap<String, Object>();    for (Map.Entry<String, ?> entry : entries) {        entriesAsMap.put(entry.getKey(), entry.getValue());    }    return entriesAsMap;}
0
private SimpleRecord.NameValue kv(String name, Object value)
{    return new SimpleRecord.NameValue(name, value);}
0
private String asJsonString(Object object) throws IOException
{    ObjectMapper mapper = new ObjectMapper();    return mapper.writeValueAsString(object);}
0
public void testFlatSchemaWithArrays() throws Exception
{    SimpleRecord simple = new SimpleRecord();    MessageType schema = new MessageType("schema", new PrimitiveType(Type.Repetition.REQUIRED, PrimitiveType.PrimitiveTypeName.BINARY, "reqd"), new PrimitiveType(Type.Repetition.OPTIONAL, PrimitiveType.PrimitiveTypeName.DOUBLE, "opt"), new PrimitiveType(Type.Repetition.REPEATED, PrimitiveType.PrimitiveTypeName.INT32, "odd"), new PrimitiveType(Type.Repetition.REPEATED, PrimitiveType.PrimitiveTypeName.INT64, "even"));    simple.values.add(kv("reqd", "a required value"));    simple.values.add(kv("opt", 1.2345));    simple.values.add(kv("odd", 1));    simple.values.add(kv("odd", 3));    simple.values.add(kv("odd", 5));    simple.values.add(kv("odd", 7));    simple.values.add(kv("odd", 9));    simple.values.add(kv("even", 2));    simple.values.add(kv("even", 4));    simple.values.add(kv("even", 6));    simple.values.add(kv("even", 8));    simple.values.add(kv("even", 10));    String expected = asJsonString(obj(entry("reqd", "a required value"), entry("opt", 1.2345), entry("odd", array(1, 3, 5, 7, 9)), entry("even", array(2, 4, 6, 8, 10))));    String actual = JsonRecordFormatter.fromSchema(schema).formatRecord(simple);    assertEquals(expected, actual);}
0
public void testNestedGrouping() throws Exception
{    SimpleRecord simple = new SimpleRecord();    MessageType schema = new MessageType("schema", new PrimitiveType(Type.Repetition.REPEATED, PrimitiveType.PrimitiveTypeName.BINARY, "flat-string"), new GroupType(Type.Repetition.OPTIONAL, "subgroup", new PrimitiveType(Type.Repetition.REQUIRED, PrimitiveType.PrimitiveTypeName.INT32, "flat-int"), new PrimitiveType(Type.Repetition.REPEATED, PrimitiveType.PrimitiveTypeName.BINARY, "string-list")));    SimpleRecord subgroup = new SimpleRecord();    subgroup.values.add(kv("flat-int", 12345));    subgroup.values.add(kv("string-list", "two"));    subgroup.values.add(kv("string-list", "four"));    subgroup.values.add(kv("string-list", "six"));    subgroup.values.add(kv("string-list", "eight"));    subgroup.values.add(kv("string-list", "ten"));    simple.values.add(kv("flat-string", "one"));    simple.values.add(kv("flat-string", "two"));    simple.values.add(kv("flat-string", "three"));    simple.values.add(kv("flat-string", "four"));    simple.values.add(kv("flat-string", "five"));    simple.values.add(kv("subgroup", subgroup));    String actual = JsonRecordFormatter.fromSchema(schema).formatRecord(simple);    String expected = asJsonString(obj(entry("flat-string", array("one", "two", "three", "four", "five")), entry("subgroup", obj(entry("flat-int", 12345), entry("string-list", array("two", "four", "six", "eight", "ten"))))));    assertEquals(expected, actual);}
0
public void testGroupList() throws Exception
{    SimpleRecord simple = new SimpleRecord();    MessageType schema = new MessageType("schema", new GroupType(Type.Repetition.REPEATED, "repeat-group", new PrimitiveType(Type.Repetition.REQUIRED, PrimitiveType.PrimitiveTypeName.INT64, "flat-int"), new PrimitiveType(Type.Repetition.REPEATED, PrimitiveType.PrimitiveTypeName.DOUBLE, "repeat-double")));    SimpleRecord repeatGroup = new SimpleRecord();    repeatGroup.values.add(kv("flat-int", 76543));    repeatGroup.values.add(kv("repeat-double", 1.2345));    repeatGroup.values.add(kv("repeat-double", 5.6789));    repeatGroup.values.add(kv("repeat-double", 10.11121314));    repeatGroup.values.add(kv("repeat-double", 0.4321));    repeatGroup.values.add(kv("repeat-double", 7.6543));    simple.values.add(kv("repeat-group", repeatGroup));    repeatGroup = new SimpleRecord();    repeatGroup.values.add(kv("flat-int", 12345));    repeatGroup.values.add(kv("repeat-double", 1.1));    repeatGroup.values.add(kv("repeat-double", 1.2));    repeatGroup.values.add(kv("repeat-double", 1.3));    repeatGroup.values.add(kv("repeat-double", 1.4));    repeatGroup.values.add(kv("repeat-double", 1.5));    simple.values.add(kv("repeat-group", repeatGroup));    repeatGroup = new SimpleRecord();    repeatGroup.values.add(kv("flat-int", 10293));    repeatGroup.values.add(kv("repeat-double", 9.5));    repeatGroup.values.add(kv("repeat-double", 9.4));    repeatGroup.values.add(kv("repeat-double", 9.3));    repeatGroup.values.add(kv("repeat-double", 9.2));    repeatGroup.values.add(kv("repeat-double", 9.1));    simple.values.add(kv("repeat-group", repeatGroup));    String actual = JsonRecordFormatter.fromSchema(schema).formatRecord(simple);    String expected = asJsonString(obj(entry("repeat-group", array(obj(entry("flat-int", 76543), entry("repeat-double", array(1.2345, 5.6789, 10.11121314, 0.4321, 7.6543))), obj(entry("flat-int", 12345), entry("repeat-double", array(1.1, 1.2, 1.3, 1.4, 1.5))), obj(entry("flat-int", 10293), entry("repeat-double", array(9.5, 9.4, 9.3, 9.2, 9.1)))))));    assertEquals(expected, actual);}
0
public String toString()
{    return "TestRecord {" + x + "," + y + "}";}
0
public void testBinary()
{    SimpleMapRecord r = new SimpleMapRecord();    Assert.assertEquals("null", r.keyToString(null));    Assert.assertEquals("[true, false, true]", r.keyToString(new boolean[] { true, false, true }));    Assert.assertEquals("[a, z]", r.keyToString(new char[] { 'a', 'z' }));    Assert.assertEquals("[1.0, 3.0]", r.keyToString(new double[] { 1.0, 3.0 }));    Assert.assertEquals("[2.0, 4.0]", r.keyToString(new float[] { 2.0f, 4.0f }));    Assert.assertEquals("[100, 999]", r.keyToString(new int[] { 100, 999 }));    Assert.assertEquals("[23, 37]", r.keyToString(new long[] { 23l, 37l }));    Assert.assertEquals("[-1, -2]", r.keyToString(new short[] { (short) -1, (short) -2 }));    Assert.assertEquals("dGVzdA==", r.keyToString("test".getBytes()));    Assert.assertEquals("TestRecord {222,333}", r.keyToString(new TestRecord(222, 333)));}
0
public String toString()
{    return "TestRecord {" + x + "," + y + "}";}
0
public void testBinary()
{    SimpleMapRecord r = new SimpleMapRecord();    Assert.assertEquals("null", r.keyToString(null));    Assert.assertEquals("true", r.keyToString(true));    Assert.assertEquals("a", r.keyToString('a'));    Assert.assertEquals("3.0", r.keyToString(3.0));    Assert.assertEquals("4.0", r.keyToString(4.0f));    Assert.assertEquals("100", r.keyToString(100));    Assert.assertEquals("37", r.keyToString(37l));    Assert.assertEquals("-1", r.keyToString((short) -1));    Assert.assertEquals("test", r.keyToString("test"));    Assert.assertEquals("123.123", r.keyToString(new BigDecimal("123.123")));}
0
public void testConverter() throws IOException
{    try (ParquetReader<SimpleRecord> reader = ParquetReader.builder(new SimpleReadSupport(), new Path(testFile().getAbsolutePath())).build()) {        for (SimpleRecord record = reader.read(); record != null; record = reader.read()) {            for (SimpleRecord.NameValue value : record.getValues()) {                switch(value.getName()) {                    case INT32_FIELD:                        Assert.assertEquals(32, value.getValue());                        break;                    case INT64_FIELD:                        Assert.assertEquals(64L, value.getValue());                        break;                    case FLOAT_FIELD:                        Assert.assertEquals(1.0f, value.getValue());                        break;                    case DOUBLE_FIELD:                        Assert.assertEquals(2.0d, value.getValue());                        break;                    case BINARY_FIELD:                        Assert.assertArrayEquals("foobar".getBytes(), (byte[]) value.getValue());                        break;                    case FIXED_LEN_BYTE_ARRAY_FIELD:                        Assert.assertArrayEquals(new byte[] { 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12 }, (byte[]) value.getValue());                        break;                }            }        }    }}
0
public void setUp() throws IOException
{    createTestParquetFile();}
0
private static MessageType createSchema()
{    return new MessageType("schema", new PrimitiveType(Type.Repetition.REQUIRED, PrimitiveType.PrimitiveTypeName.INT32, INT32_FIELD), new PrimitiveType(Type.Repetition.REQUIRED, PrimitiveType.PrimitiveTypeName.INT64, INT64_FIELD), new PrimitiveType(Type.Repetition.REQUIRED, PrimitiveType.PrimitiveTypeName.FLOAT, FLOAT_FIELD), new PrimitiveType(Type.Repetition.REQUIRED, PrimitiveType.PrimitiveTypeName.DOUBLE, DOUBLE_FIELD), new PrimitiveType(Type.Repetition.REQUIRED, PrimitiveType.PrimitiveTypeName.BINARY, BINARY_FIELD), new PrimitiveType(Type.Repetition.REQUIRED, PrimitiveType.PrimitiveTypeName.FIXED_LEN_BYTE_ARRAY, 12, FIXED_LEN_BYTE_ARRAY_FIELD));}
0
private void createTestParquetFile() throws IOException
{    Path fsPath = new Path(testFile().getPath());    Configuration conf = new Configuration();    MessageType schema = createSchema();    SimpleGroupFactory fact = new SimpleGroupFactory(schema);    GroupWriteSupport.setSchema(schema, conf);    try (ParquetWriter<Group> writer = new ParquetWriter<>(fsPath, new GroupWriteSupport(), CompressionCodecName.UNCOMPRESSED, 1024, 1024, 512, true, false, ParquetProperties.WriterVersion.PARQUET_2_0, conf)) {        writer.write(fact.newGroup().append(INT32_FIELD, 32).append(INT64_FIELD, 64L).append(FLOAT_FIELD, 1.0f).append(DOUBLE_FIELD, 2.0d).append(BINARY_FIELD, Binary.fromString("foobar")).append(FIXED_LEN_BYTE_ARRAY_FIELD, Binary.fromConstantByteArray(new byte[] { 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12 })));    }}
0
private File testFile()
{    return new File(this.tempFolder.getRoot(), getClass().getSimpleName() + ".parquet");}
0
